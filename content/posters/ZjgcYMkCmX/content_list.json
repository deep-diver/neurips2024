[{"type": "text", "text": "How does Inverse RL Scale to Large State Spaces? A Provably Efficient Approach ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Filippo Lazzati Mirco Mutti Alberto Maria Metelli Politecnico di Milano Technion Politecnico di Milano Milan, Italy Haifa, Israel Milan, Italy filippo.lazzati@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In online Inverse Reinforcement Learning (IRL), the learner can collect samples about the dynamics of the environment to improve its estimate of the reward function. Since IRL suffers from identifiability issues, many theoretical works on online IRL focus on estimating the entire set of rewards that explain the demonstrations, named the feasible reward set. However, none of the algorithms available in the literature can scale to problems with large state spaces. In this paper, we focus on the online IRL problem in Linear Markov Decision Processes (MDPs). We show that the structure offered by Linear MDPs is not sufficient for efficiently estimating the feasible set when the state space is large. As a consequence, we introduce the novel framework of rewards compatibility, which generalizes the notion of feasible set, and we develop CATY-IRL, a sample efficient algorithm whose complexity is independent of the cardinality of the state space in Linear MDPs. When restricted to the tabular setting, we demonstrate that CATY-IRL is minimax optimal up to logarithmic factors. As a by-product, we show that Reward-Free Exploration (RFE) enjoys the same worst-case rate, improving over the state-of-the-art lower bound. Finally, we devise a unifying framework for IRL and RFE that may be of independent interest. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse Reinforcement Learning (IRL) is the problem of inferring the reward function given demonstrations of an optimal behavior, i.e., from an expert agent. [49, 42]. Since its formulation, much of the research effort has been put into the design of efficient algorithms for solving the IRL problem [6, 4]. Indeed, the solution of the IRL problem opens the door to a variety of interesting applications, including Apprenticeship Learning (AL) [2, 1], reward design [16], interpretability of the expert\u2019s behavior [17], and transferability to new environments [15]. ", "page_idx": 0}, {"type": "text", "text": "Nowadays, the factor that most negatively impacts the adoption of IRL solutions in real-world applications is the intrinsic ill-posedness of its formulation. The IRL problem has been historically defined as the problem of recovering the reward function underlying the demonstrations [49, 42], even though mere demonstrations can be equivalently explained by a variety of rewards. In other words, the IRL problem is underconstrained, even in the limit of infinite demonstrations [42, 39]. ", "page_idx": 0}, {"type": "text", "text": "To overcome this weakness and to come up with a single reward function, three main approaches are commonly adopted in the literature. (i) The first approach consists of the use of a heuristic to select a specific reward function from the set of all the rewards that explain the demonstrations. Implicitly, these works re-define IRL as the problem of recovering the reward function explaining the demonstrations and complying with the heuristic. As an example, [42, 48] select the reward that maximizes some notion of margin, and [70] implicitly chooses the reward returned by the optimization algorithm among those that maximize the likelihood. However, these approaches may generate issues in applications [56, 15]. (ii) In the second approach, additional constraints beyond mere demonstrations are enforced to guarantee the uniqueness of the reward function to recover. In \u201creward identifiability\u201d works, the additional information commonly concerns some structure of the environment [25], or multiple demonstrations across various environments [5, 10]. In Reward Learning (ReL) works [19], demonstrations of optimal behavior are combined with other kinds of expert feedback, like comparisons [65]. (iii) As a third approach, recently, [39, 38] proposed the alternative formulation of IRL as the problem of recovering all the reward functions compatible with the demonstrations, i.e., the feasible reward set. In this manner, we are not subject to the limitations of the first approach, and we do not depend on additional information like in the second approach. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In practical applications, the chosen IRL formulation has to be tackled by algorithms that use a finite number of demonstrations and a limited knowledge of the dynamics of the environment. In the common online IRL scenario, the learner explores the (unknown) environment, and exploits this additional information to improve its performance on the IRL task [e.g., 39, 33, 38, 68, 31]. On this basis, the IRL approach (iii) based on the feasible set [39, 38] displays desirable properties since \u201cpostpones\u201d the choice of the heuristic and/or enforcement of additional constraints, with the advantage of analyzing the intrinsic complexity of the IRL problem only, without being obfuscated by other factors. In other words, this recent formulation of the IRL problem paves the way for the design and analysis of provably efficient IRL algorithms, endowed with solid theoretical guarantees. ", "page_idx": 1}, {"type": "text", "text": "However, the algorithms designed for learning the feasible set currently available in the literature [e.g., 39, 33, 38, 68, 31] struggle when attempting to scale them to IRL problems with large state spaces. This is apparent because their sample complexity exhibits an explicit dependence on the cardinality of the state space. This inevitably represents a major limitation since most real-world scenarios concern problems with large, or even continuous, state spaces [e.g., 15, 7, 40, 14]. ", "page_idx": 1}, {"type": "text", "text": "In this context, function approximation represents an essential tool to tackle the curse of dimensionality and enforce generalization [54, 41]. Linear Markov Decision Processes (MDPs) [23, 67] offer a simple but powerful structure, in which we assume the reward function and the transition model can be expressed as linear combinations of known features, that permits theoretical analysis of the sample complexity. Even though many extensions have been developed [64, 22, 13], the Linear MDPs framework typically represents one of the first function approximation settings to analyze when focusing on a novel problem, before moving to more complex settings [e.g., 63, 61]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to shed light on the challenges of scaling the feasible reward set to large-scale problems. Motivated by its limitations when dealing with large state spaces, we introduce the novel Rewards Compatibility framework. Being a generalization of the notion of feasible set, it allows us to define the new IRL Classification Problem, a fourth approach to cope with the ill-posedness of the IRL formulation. This permits the development of CATY-IRL (CompATibilitY for IRL), a provably efficient IRL algorithm for Linear MDPs characterized by large or even continuous state spaces. ", "page_idx": 1}, {"type": "text", "text": "Original Contributions. The main contributions of the current work can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove that the notion of feasible set can not be learned efficiently in MDPs with large/continuous state spaces, even under the structure enforced by Linear MDPs. Nevertheless, we show that this problem disappears under the assumption that the expert\u2019s policy is known, by providing a sample efficient algorithm for such setting (Section 3). \u2022 To overcome the need for knowing the expert\u2019s policy exactly, we propose Rewards Compatibility, a novel framework that formalizes the intuitive notion of compatibility of a reward function with expert demonstrations. It generalizes the feasible set and allows us to define an original learning setting, IRL classification, based on a new formulation of IRL classification task (Section 4). \u2022 For the newly-devised framework, we develop CATY-IRL (CompATibilitY for IRL), a new sample and computationally efficient IRL algorithm for both tabular and Linear MDPs. Remarkably, this CATY-IRL does not require the additional assumption that the expert\u2019s policy is known (Section 5). \u2022 In the tabular setting, we prove a tight minimax lower bound to the sample complexity of the IRL classification problem of $\\begin{array}{r}{\\Omega\\big(\\frac{H^{3}S A}{\\epsilon^{2}}\\Bar{(S+\\log\\frac{1}{\\delta})}\\big)}\\end{array}$ episodes, where $S$ and $A$ are the cardinalities of the state and action spaces, $H$ is the horizon, $\\epsilon$ the accuracy and $\\delta$ the failure probability. This bound is matched by CATY-IRL, up to logarithmic factors. Exploiting a similar construction, we show that a lower bound with the same rate holds also for the Reward-Free Exploration (RFE) problem, improving by an $H$ factor over the RFE state-of-the-art lower bound [21] (Section 6.1). \u2022 Finally, we formulate a novel Objective-Free Exploration (OFE) setting that isolates the challenges of exploration beyond Reinforcement Learning (RL), by unifying RFE and IRL (Section 6.2). Additional related works and the proofs of all the results are reported in Appendix A and B -E. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Given an integer $N\\in\\mathbb{N}$ , we define $[N]:=\\{1,\\dots,N\\}$ . Given sets $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , we denote $\\begin{array}{r}{\\mathcal{H}_{d}(\\mathcal{X},\\mathcal{Y})\\;:=\\;\\operatorname*{max}\\{\\operatorname*{sup}_{x\\in\\mathcal{X}}\\operatorname*{inf}_{y\\in\\mathcal{Y}}d(x,y),\\operatorname*{sup}_{y\\in\\mathcal{Y}}\\operatorname*{inf}_{x\\in\\mathcal{X}}d(y,x)\\}}\\end{array}$ their Hausdorff distance with inner distance $d$ . We denote by $\\Delta^{\\mathcal{X}}$ the probability simplex over $\\mathcal{X}$ , and by $\\Delta_{\\mathcal{Y}}^{\\mathcal{X}}$ the set of functions from $\\boldsymbol{\\wp}$ to $\\Delta^{\\mathcal{X}}$ . Sometimes, we denote the dot product between vectors $x,y$ as $\\langle x,y\\rangle:=x^{\\mathsf{T}}y$ . We employ $\\mathcal{O},\\Omega,\\Theta$ for the common asymptotic notation and $\\widetilde{\\mathcal{O}},\\widetilde{\\Omega},\\widetilde{\\Theta}$ to omit logarithmic terms. ", "page_idx": 2}, {"type": "text", "text": "Markov Decision Processes. A finite-horizon Markov  De cisi on Process (MDP) without reward spaces, $H\\in\\mathbb{N}$ is the ho $\\mathcal{M}:=(S,\\mathcal{A},H,d_{0},p)$ $d_{0}\\in\\Delta^{s}$ initial-s $\\boldsymbol{S}$ e di $\\boldsymbol{\\mathcal{A}}$ ibution, and $p\\in\\mathcal{P}:=\\Delta_{S\\times\\mathcal{A}\\times\\mathbb{I}}^{S}$ is the transition model. Given a (deterministic) reward function $r\\,\\in\\,\\Re:=\\,[-1,1]^{{\\mathcal{S}}\\times{\\mathcal{A}}\\times\\lbrack{H}\\rbrack}$ ,  we denote by $\\overline{{\\mathcal{M}}}:=\\mathcal{M}\\cup\\{r\\}$ the MDP obtained by pairing $\\mathcal{M}$ and $r$ . Each policy $\\pi\\in\\Pi:=\\Delta_{S\\times\\mathbb{I}}^{A}\\mathrm{{\\Omega}}_{[H]}$ induces in $\\overline{{\\mathcal{M}}}$ a state-action probability distribution $d^{p,\\pi}:=\\{d_{h}^{p,\\pi}\\}_{h\\in[\\![H]\\!]}$ (we omit $d_{0}$ for simplicity) that assigns, to each subset $\\mathcal{Z}\\subseteq\\mathcal{S}\\times\\mathcal{A}$ , the probability of being in $\\mathcal{Z}$ at stage $h\\,\\in\\,[\\![H]\\!]$ when playing $\\pi$ in $\\overline{{\\mathcal{M}}}$ . We denote with $S_{h}^{p,\\pi}$ the set of states supported by $d_{h}^{p,\\pi}$ for any action   at s tage $h$ , and with $S^{p,\\pi}$ the disjoint union of sets $\\{S_{h}^{p,\\pi}\\}_{h\\in\\mathbb{I}}$ . The $Q$ -function of policy $\\pi$ in MDP $\\overline{{\\mathcal{M}}}$ is defined at every $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ as $\\begin{array}{r}{Q_{h}^{\\pi}(s,\\bar{a};\\bar{p},r):=\\mathbb{E}_{p,\\pi}[\\sum_{t=h}^{H}r_{t}(s_{t},a_{t})|s_{h}=s,a_{h}=a]}\\end{array}$ and the optimal $Q$ -function as $\\begin{array}{r}{Q_{h}^{*}(s,a;p,r):=\\operatorname*{sup}_{\\pi\\in\\Pi}Q_{h}^{\\pi}(s,a;p,r)}\\end{array}$ , where the expectation $\\mathbb{E}_{p,\\pi}$ iawsn ecd  odtemhfepi nuoetp ettdih meo h-fneu cstnticootcino honaf s taipsc $\\pi$ $\\overline{{\\mathcal{M}}}$   \n$V$ $\\pi$ $(s,h)$ $\\begin{array}{r}{V_{h}^{\\pi}(s;p,r)\\::=\\:\\mathbb{E}_{p,\\pi}[\\sum_{t=h}^{H}r_{t}(s_{t},a_{t})|s_{h}\\:=\\:s],}\\end{array}$ $V$ $\\begin{array}{r}{V_{h}^{*}(s;p,r)\\;:=\\;\\operatorname*{sup}_{\\pi\\in\\Pi}V_{h}^{\\pi}(s;p,r)}\\end{array}$ $\\pi$ $J^{\\pi}(r;p)\\ :=\\ \\mathbb{E}_{s\\sim d_{0}}[V_{1}^{\\pi}(s;p,r)]$ , and the optimal utility as $J^{*}(\\bar{\\boldsymbol{r}};\\boldsymbol{p})\\;:=\\;\\mathbb{E}_{s\\sim d_{0}}[V_{1}^{*}(s;\\bar{\\boldsymbol{p}},\\boldsymbol{r})]$ . A forward (sampling) model of the environment permits to collect samples starting from $s\\sim d_{0}$ and following some policy. A generative (sampling) model consists in an oracle that, given an arbitrary state-action-stage triple $s,a,h$ in input, returns a sampled next state $s^{\\prime}\\sim p_{h}(\\cdot|s,a\\bar{)}$ . ", "page_idx": 2}, {"type": "text", "text": "Linear MDPs. Based on [23], we say that an MDP $\\overline{{\\mathcal{M}}}\\,=\\,(S,\\mathcal{A},H,d_{0},p,r)$ is a Linear MDP with a (known) feature map $\\phi:S\\times A\\rightarrow\\mathbb{R}^{d}$ , if for every $h\\in[H]$ , there exist $d\\in\\mathbb{N}$ unknown (signed) measures $\\mu_{h}=[\\mu_{h}^{1},\\cdot\\cdot\\cdot,\\mu_{h}^{d}]^{\\intercal}$ over $\\boldsymbol{S}$ and an unknown v e cto r $\\theta_{h}\\in\\mathbb{R}^{d}$ , such that for every $(s,{\\bar{a}})\\in S\\times A$ , we have $\\bar{p}_{h}\\dot{(\\cdot|s,a)}\\overset{\\cdot\\cdot-}{=}\\langle\\phi(s,a),\\mu_{h}(\\cdot)\\rangle$ and $r_{h}(s,a)=\\langle\\phi(s,a),\\theta_{h}\\rangle$ . Without loss of generality, we assume $\\|\\phi(s,a)\\|_{2}\\leqslant1$ for all $(s,a)\\in S\\times A$ , and $\\operatorname*{max}\\{\\|\\theta_{h}\\|_{2},\\||\\mu_{h}|(S)\\|_{2}\\}\\leqslant\\sqrt{d}.$ .1 $\\mathcal{M}$ is a Linear MDP without reward if its transition model satisfies the assumption described above. ", "page_idx": 2}, {"type": "text", "text": "BPI and RFE. In both Best-Policy Identification (BPI) [37] and Reward-Free Exploration (RFE) [21], the learner has to explore the unknown MDP to optimize a certain reward function. In BPI, the learner observes the reward function $r$ during exploration, and its goal is to output a policy $\\widehat{\\pi}$ such that, in the true MDP with transition model $p$ we have $\\mathbb{P}\\big(J^{*}(r;p)\\stackrel{\\smile}{-}J^{\\hat{\\pi}}(r;p)\\leqslant\\dot{\\epsilon}\\big)\\geqslant\\dot{1}-\\delta$ f opr every $\\epsilon,\\delta\\in(0,1)$ . RFE considers the setting in which the reward to optimize is revealed a posteriori of the exploration phase. Thus the goal of the agent in RFE is to compute an estimate $\\widehat{p}$ of the true dynamics $p$ so that $\\begin{array}{r}{\\mathbb{P}\\big(\\operatorname*{sup}_{r\\in\\Re}\\{J^{*}(\\breve{r};p)_{-}-J^{\\hat{\\pi}_{r}}(\\breve{r};p)\\}\\leqslant\\epsilon\\big)\\geqslant1-\\delta}\\end{array}$ for every $\\epsilon,\\delta\\in(0,1)$ , where ${\\hat{\\pi}}_{r}$ is the optimal policy in the MDP with $\\widehat{p}$ as t ransition model and $r$ as reward function. ", "page_idx": 2}, {"type": "text", "text": "Online IRL. We consider the online 2p IRL setting [39, 33, 68, 66, 53] in which, similarly to the online AL setting [53, 66], we are given a dataset $\\mathbf{\\bar{\\mathcal{D}}}^{E}=\\{(s_{1}^{i},a_{1}^{i},\\ldots,{s_{H-1}^{i}},a_{H-1}^{i},s_{H}^{i})\\}_{i\\in\\ensuremath{[\\tau^{E}]}}$ of $\\tau^{E}\\,\\in\\,\\mathbb{N}$ trajectories collected by executing the expert\u2019s policy $\\pi^{E}$ in a certain (unknown)  M DP $\\overline{{\\mathcal{M}}}=\\mathcal{M}\\cup\\{r^{E}\\}$ . We make the assumption that $\\pi^{E}$ is optimal under the true (unknown) reward $r^{E}$ in $\\overline{{\\mathcal{M}}}$ . Since the dynamics of $\\overline{{\\mathcal{M}}}$ is unknown, we are allowed to actively explore the environment through a forward model to collect a new state-action dataset $\\mathcal{D}$ . The goal is to use the latter and demonstrations in $\\mathcal{D}^{E}$ to estimate a reward function that makes the expert\u2019s policy $\\pi^{E}$ optimal. Sometimes, we will denote an IRL instance as $\\mathcal{M}\\cup\\{\\pi^{E}\\}$ , and a Linear IRL instance with recovered reward $r$ as an IRL instance in which $\\mathcal{M}\\cup\\{r\\}$ is a Linear MDP. ", "page_idx": 2}, {"type": "text", "text": "3 Limitations of the Feasible Set ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, after having characterized the feasible set formulation in Linear MDPs, we show that it suffers from statistical (and computational) inefficiency in problems with large state spaces, even under the Linear MDP assumption. We will provide a solution to these issues in Section 4. ", "page_idx": 3}, {"type": "text", "text": "The Feasible Set. According to the standard definition [e.g., 39, 33, 38, 68, 31], the feasible set contains the rewards that make the expert\u2019s policy $\\pi^{E}$ optimal, as defined below. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Feasible Set [31]). Let $\\mathcal{M}$ be an MDP without reward and let $\\pi^{E}$ be the expert\u2019s policy. The feasible set $\\mathcal{R}_{p,\\pi^{E}}$ of rewards compatible with $\\pi^{E}$ in $\\mathcal{M}$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{p,\\pi^{E}}:=\\{r\\in\\Re\\,|\\,J^{\\pi^{E}}(r;p)=J^{*}(r;p)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Without function approximation, the feasible set contains a variety of rewards for any deterministic policy. In Linear MDPs, due to the feature map, the feasible set might exhibit some degeneracy.3 Definition 3.1 can be adapted to Linear MDPs with feature map $\\phi$ as: $\\mathcal{R}_{\\phi,p,\\pi^{E}}:=\\{r\\in\\mathring{\\Re}\\,|\\,J^{\\pi^{E}}(r;p)=$ $J^{*}(r;p)\\,\\wedge\\,\\exists\\theta:\\mathbb{[}H]\\to\\mathbb{R}^{d},\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathbb{[}H]\\,:\\,r_{h}(s,a)=\\langle\\phi(s,a),\\theta_{h}\\rangle\\}$ . We omit $\\phi$ in $\\mathcal{R}_{\\phi,p,\\pi^{E}}$ for not a tion al simplicity. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1. Let $\\mathcal{M}$ be a Linear MDP without reward with a finite state space, and let $\\phi$ be $a$ feature mapping. Let $\\{\\Phi_{h}^{\\pi}\\}_{h\\in[\\![H]\\!]}$ and $\\{{\\overline{{\\Phi}}}_{h}\\}_{h\\in\\mathbb{I}^{H}\\mathbb{I}}$ be the sets of expert\u2019s and non-expert\u2019s features, defined for every $h\\in\\left[U\\right]$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi_{h}^{\\pi^{E}}:=\\big\\{\\phi(s,a^{E})\\,|\\,s\\in\\mathcal{S}_{h}^{p,\\pi^{E}},\\,a^{E}\\in\\mathcal{A}_{h}^{E}(s)\\big\\},\\qquad\\overline{{\\Phi}}_{h}:=\\big\\{\\phi(s,a)\\,|\\,s\\in\\mathcal{S}_{h}^{p,\\pi^{E}},\\,a\\in\\mathcal{A}\\cup\\mathcal{A}_{h}^{E}(s)\\big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A_{h}^{E}(s):=\\{a\\in\\mathcal{A}|\\pi_{h}^{E}(\\cdot|s)>0\\}$ for every $s\\in S$ . If for none of the $H$ pairs of sets $(\\Phi_{h}^{\\pi^{E}},\\overline{{\\Phi}}_{h})$ there exists a separating hyperplane, then $\\mathcal{R}_{p,\\pi^{E}}=\\{\\overline{{r}}\\}$ , with $\\overline{{r}}_{h}(s,a)=0\\,\\forall(s,a,h)\\in S\\times\\mathring{A}\\times\\mathbb{I}\\!\\!\\!\\parallel$ i.e., the feasible set with linear rewards in $\\phi$ contains only the reward function that assigns  zero reward everywhere. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, expert\u2019s actions must have the largest optimal $Q$ -value among all actions, and linearity imposes the \u201cseparability\u201d requirement. The result holds also for MDPs with linear rewards only. We exemplify Proposition 3.1 in Appendix B.1. ", "page_idx": 3}, {"type": "text", "text": "Learning the Feasible Set. In order to highlight the challenges of learning the feasible set with large-scale MDPs, based on [38, 31], we devise the following PAC requirement. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (PAC Algorithm). Let $\\epsilon,\\delta\\in(0,1)$ , and let A be an algorithm that collects $\\tau^{E}$ samples about $\\pi^{E}$ using a generative model, and $\\tau$ episodes from a Linear MDP without reward $\\mathcal{M}\\,=$ $(S,{\\mathcal{A}},H,d_{0},p)$ using a forward model. Let $\\hat{\\mathcal{R}}$ be the estimate of the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ outputted by $\\mathfrak{A}$ . Then, $\\mathfrak{A}$ is $(\\epsilon,\\delta)$ -PAC for IRL if ${}:\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}\\!\\left({\\mathcal{H}}_{d}({\\mathcal{R}}_{p,\\pi^{E}},{\\widehat{\\mathcal{R}}})\\leqslant\\epsilon\\right)\\geqslant1-\\delta$ , where $\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}$ is the probability measure induced by $\\mathfrak{A}$ in $\\mathcal{M}$ , and $\\begin{array}{r}{d(r,\\widehat{r})\\alpha\\operatorname*{sup}_{\\pi\\in\\Pi}\\sum_{h\\in[H]}\\mathbb{E}_{(s,a)\\sim d_{h}^{p,\\pi}(\\cdot,\\cdot)}\\left|r_{h}(s,a)-\\widehat{r}_{h}(s,a)\\right|}\\end{array}$ .4 The sample complexity is the pair $(\\tau^{E},\\tau)$ .p ", "page_idx": 3}, {"type": "text", "text": "It is worth noting that in Definition 3.2, we are considering a generative model for collecting samples from the expert\u2019s policy, which represents the easiest learning scenario. The following result shows that, even in this convenient setting, estimating the feasible set is statistically inefficient. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2 (Statistical Inefficiency). Let $\\mathcal{M}\\cup\\{\\pi^{E}\\}$ be a Linear IRL instance with finite state space $\\boldsymbol{S}$ and deterministic expert\u2019s policy, and let $\\epsilon,\\delta\\in(0,1)$ . If an algorithm $\\mathfrak{A}$ is $(\\epsilon,\\delta)$ -PAC, then $\\bar{\\boldsymbol{\\tau}}^{E}=\\Omega(\\boldsymbol{S})$ , where $S:=|S|$ is the cardinality of the state space. ", "page_idx": 3}, {"type": "text", "text": "In other words, even under the easiest learning conditions (i.e., generative model and deterministic expert), the sample complexity scales directly with the cardinality of the state space $S$ , thus, it is infeasible when $S$ is large or even infinite. Observe that this result extends to any class of MDPs that contains Linear MDPs. In Appendix B.2, we analyze if additional assumptions can drop the $\\Omega(S)$ dependence. Nevertheless, if $\\bar{\\pi}^{E}$ is known, it is possible to construct sample efficient algorithms. Algorithm 1 (whose pseudocode is presented in Appendix B.3), under the assumption that $\\pi^{E}$ is known, makes use of an inner RFE routine (Algorithm 1 of [62]) to recover the feasible set. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.3. Assume that $\\pi^{E}$ (along with its support $S^{p,\\pi^{E}}$ ) is known. Then, for any $\\epsilon,\\delta\\in(0,1)$ , Algorithm $^{\\,l}$ is $(\\epsilon,\\delta)$ -PAC for IRL with a number of episodes $\\tau$ upper bounded by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{5}d}{\\epsilon^{2}}\\Big(d+\\log\\frac{1}{\\delta}\\Big)\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Limitations of the Feasible Set. We can now conclude that the feasible set suffers from two main limitations. (i) Sample Inefficiency: If $\\pi^{E}$ is unknown, it requires a number of samples that depends on the cardinality of the state space (Theorem 3.2). (ii) Lack of Practical Implementability: It contains a continuum of rewards, thus, no practical algorithm can explicitly compute it. We will discuss in the next section how to overcome both these issues. ", "page_idx": 4}, {"type": "text", "text": "4 Rewards Compatibility ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present the main contribution of this work: Rewards Compatibility, a novel framework for IRL that allows us to conveniently rephrase the learning from demonstrations problem as a classification task. We anticipate that the presentation of the framework is completely general and independent of structural assumptions of the MDP (e.g., Linear MDP). ", "page_idx": 4}, {"type": "text", "text": "4.1 Compatible Rewards ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the following, for ease of presentation, we consider the exact setting, i.e., when $d_{0},\\,p$ , and $\\pi^{E}$ are known. In addition, we will drop the dependence on $p$ when clear from the context. ", "page_idx": 4}, {"type": "text", "text": "In IRL, an expert agent demonstrates policy $\\pi^{E}$ assumed optimal under some (unknown) reward function $r^{E}$ , i.e., $J^{\\bar{*}}(r^{E})=J^{\\pi^{E}}(r^{E})$ . The task is to recover a reward $r$ such that $J^{*}(r)=J^{\\pi^{E}}(r)$ . By definition, IRL tells us that $r^{E}$ makes the demonstrated policy $\\pi^{E}$ optimal, but what about other policies? We do not and cannot know. Since there are (infinite) rewards making $\\pi^{E}$ optimal (but they differ in the performance attributed to other policies) we realize that there are many rewards equally \u201ccompatible\u201d with $\\pi^{E}$ .5 Clearly, wih no additional information, we are unable to identify $r^{E}$ . ", "page_idx": 4}, {"type": "text", "text": "The feasible set considers only these rewards, i.e., $r\\in\\mathfrak{R}$ for which $J^{*}(r)=J^{\\pi^{E}}(r)$ , and it refuses all the others. This can be interpreted as the feasible set carrying out a classification of rewards based on a \u201chard\u201d notion of compatibility with demonstrations. In other words, rewards $r$ satisfying condition $J^{*}(r)=J^{\\pi^{E}}(r)$ are compatible with $\\pi^{E}$ , and the others are not. Nevertheless, our insight is that some rewards are \u201cmore\u201d compatible with $\\pi^{E}$ than others. ", "page_idx": 4}, {"type": "text", "text": "Example 4.1. Consider an MDP with one state and $H=1$ in which the expert has three actions: Eating a muffin $(M)$ , a cake $(C)$ , or some (bad) vegetable soup (S). The true reward $r^{E}$ assigns $r^{E}(\\bar{M})\\,=\\,+\\bar{1},r^{E}(C)\\,=\\,+0.99$ and $r^{E}(S)=-1$ , i.e., the expert has a (weak) preference for the muffin over the cake, while she hates the soup; thus, she will demonstrate $\\pi^{E}=\\dot{M}$ . Let $r_{g},r_{b}$ be: ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{g}(M)=+0.99,r_{g}(C)=+1,r_{g}(S)=-1,\\qquad r_{b}(M)=-1,r_{b}(C)=-1,r_{b}(S)=+1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, $r_{g}$ is \u201cmore\u201d compatible with $\\pi^{E}$ than $r_{b}$ , because it establishes that $M$ and $C$ are much better than $S$ , while reward $r_{b}$ reverses the preferences. Clearly, we make a small error if we model the preferences of the expert with $r_{g}$ instead of the true reward $\\dot{r}^{E}$ . However, the notion of feasible set is completely blind to the difference between $r_{g}$ and $r_{b}$ at modeling $r^{E}$ , and it refuses both of them. ", "page_idx": 4}, {"type": "text", "text": "We propose the following \u201csoft\u201d definition of (non)compatibility to capture this intuition.6 ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1 (Rewards (non)Compatibility). Let $\\mathcal{M}\\cup\\{\\pi^{E}\\}$ be an IRL instance, and let $r\\in\\mathfrak{R}$ be any reward. We define the (non)compatibility $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}:\\mathfrak{R}\\to\\mathbb{R}_{\\geqslant0}$ of reward $r$ w.r.t. $\\mathcal{M}\\cup\\{\\pi^{E}\\}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\overline{{{\\mathcal{C}}}}_{p,\\pi^{E}}(r):=J^{*}(r;p)-J^{\\pi^{E}}(r;p).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In words, the (non)compatibility of reward $r$ w.r.t. policy $\\pi^{E}$ in problem $\\mathcal{M}$ quantifies the suboptimality of $\\pi^{E}$ in the MDP ${\\mathcal{M}}\\cup\\{r\\}$ . By definition, rewards $r$ belonging to the feasible set (i.e., $r\\in\\mathcal{R}_{p,\\pi^{E}})$ ) satisfy $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)=0$ , i.e., they have zero non-compatibility with $\\pi^{E}$ in $\\mathcal{M}$ .7 ", "page_idx": 5}, {"type": "text", "text": "Example 4.1 (Continued). (Non)compatibility discriminates between $r_{g}$ and $r_{b}$ . Indeed, we have that $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r^{E})=0$ , $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r_{g})=0.01$ , and $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r_{b})=2$ . In words, reward $r_{g}$ suffers from very small (non)compatibility, while $r_{b}$ suffers from large (non)compatibility, thus we say that reward $r_{g}$ is more compatible with $\\pi^{E}$ than $r_{b}$ , as expected. ", "page_idx": 5}, {"type": "text", "text": "By definition of IRL, the true reward $r^{E}$ makes the observed $\\pi^{E}$ optimal, but reveals no information about the other policies. Thus, it is meaningful that $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}$ considers the suboptimality of $\\pi^{E}$ only, because demonstrations from $\\pi^{E}$ do not provide information about other policies, as illustrated below. ", "page_idx": 5}, {"type": "text", "text": "Example 4.2. Let $r_{b}^{\\prime}$ be such that $r_{b}^{\\prime}(M)=+0.99,r_{b}^{\\prime}(C)=-1,r_{b}^{\\prime}(S)=+1.$ . Clearly, $r_{b}^{\\prime}$ is much worse than $r_{g}$ at modeling $r^{E}$ , because it does not capture the fact that the expert appreciates the cake but she hates the soup. However, demonstrations from $\\pi^{E}$ alone do not provide information about $C$ or $S,$ , but only about $\\pi^{E}=M$ (i.e., the expert always eats the muffin). Thus, we have that $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r_{g})=\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(\\dot{r_{b}^{\\prime}})=0.01$ , i.e., $r_{g}$ and $r_{b}^{\\prime}$ are equally compatible with the given demonstrations. ", "page_idx": 5}, {"type": "text", "text": "For a discussion on comparing the (non)compatibility of different rewards, see Appendix C.4. ", "page_idx": 5}, {"type": "text", "text": "4.2 The IRL Classification Formulation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our goal is to overcome the limitations of the feasible set highlighted in Section 3. Drawing inspiration from the notion of \u201cmembership checker\u201d algorithm in [31], we propose a novel formulation of IRL. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.2 (IRL Classification Problem and IRL Algorithm). An IRL Classification Problem instance is made of a tuple $(\\mathcal{M},\\pi^{E},\\mathcal{R},\\Delta),$ , where $\\mathcal{M}$ is an MDP without reward, $\\pi^{E}$ is the expert\u2019s policy, $\\mathcal{R}\\subseteq\\mathfrak{R}$ is a set of rewards to classify, and $\\Delta\\in\\mathbb{R}_{\\geqslant0}$ is some threshold. The goal is to classify all and only the rewards $r\\in\\mathcal{R}$ based on their (non)compatibility with $\\pi^{E}$ in M w.r.t. $\\Delta$ . In symbols: ", "page_idx": 5}, {"type": "text", "text": "$\\forall r\\in\\mathcal{R}:\\ \\mathbf{if}\\ \\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)\\leqslant\\Delta$ then return True, else return False. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$A n$ IRL algorithm takes in input a reward $r\\in\\mathcal{R}$ and outputs a boolean saying whether $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)\\leqslant\\Delta$ . ", "page_idx": 5}, {"type": "text", "text": "Given $r\\in\\mathcal{R}$ , we output whether it makes the expert\u2019s policy $\\pi^{E}$ at most $\\Delta$ -suboptimal or not. Intuitively, we classify rewards in $\\mathcal{R}$ based on how good $\\pi^{\\dot{E}}$ performs w.r.t. them. A $\\Delta$ -(non)compatible reward guarantees that, among its $\\Delta$ -optimal policies, there is $\\pi^{E}$ , but the optimal policy might be different from $\\pi^{E}$ (see Appendix C.3 for how this relates to (forward) RL). Note that we allow for $\\mathcal{R}\\neq\\mathfrak{R}$ to manage scenarios in which we have some prior knowledge on $r^{E}$ , i.e., $r^{E}\\in\\mathcal{R}\\subset\\Re$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 4.1. Permitting non-zero (non)compatibility is equivalent to enlarging the feasible set. Let $\\mathcal{R}=\\Re$ , and define the set of rewards positively classified as ${\\mathcal{R}}_{\\Delta}$ , i.e., $\\mathcal{R}_{\\Delta}:=\\left\\{r\\in\\mathcal{R}\\,\\vert\\,\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)\\leqslant\\right.$ $\\Delta\\}$ . For any $\\Delta,\\Delta^{\\prime}$ s.t. $0\\leqslant\\Delta\\leqslant\\Delta^{\\prime}\\leqslant2H$ , we have: $\\mathcal{R}_{p,\\pi^{E}}=\\mathcal{R}_{0}\\subseteq\\mathcal{R}_{\\Delta}\\subseteq\\mathcal{R}_{\\Delta^{\\prime}}\\subseteq\\mathcal{R}_{2\\dot{H}}\\,=\\Re$ . ", "page_idx": 5}, {"type": "text", "text": "Discussion on Reward Compatibility. It should be remarked that: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "\u2022 The limits of the rewards compatibility framework are the same as the limits of the feasible set. We cannot identify $r^{E}$ from the feasible set or among the rewards with small (non)compatibility. As aforementioned, this is an inherent limit of IRL and cannot be overcome with a more refined objective formulation, unless further information on $r^{E}$ is available (e.g., preferences). \u2022 Rewards compatibility offers advantages over feasible set. Differently from the feasible set, as we will see in Section 5, it is possible to practically implement algorithms that solve the IRL classification problem, with guarantees of sample efficiency even when the state space is large. ", "page_idx": 5}, {"type": "text", "text": "4.3 A Learning Framework for Online IRL Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we combine the online IRL setting presented in Section 2 with the IRL classification problem of Definition 4.2. Intuitively, the performance of an algorithm depends on its accuracy at estimating the (non)compatibility of the rewards, as formalized by the following PAC requirement. ", "page_idx": 5}, {"type": "image", "img_path": "ZjgcYMkCmX/tmp/47822e8c9d0a73d51364b48cf21b021e84e309c66550c5e37f76518e72c596f5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: The axis represents (estimated) (non)compatibility values. (a) Rewards $r$ whose true (non)compatibility $\\overline{{\\mathcal{C}}}(\\boldsymbol{r}):=\\overline{{\\mathcal{C}}}_{\\boldsymbol{p},\\pi^{E}}(\\boldsymbol{r})$ is far from threshold $\\Delta$ by at least $\\epsilon$ , are correctly classified, while (b) in the opposite case, rewards can be mis-classified. (c) The red interval $[\\Delta-\\epsilon,\\Delta+\\epsilon]$ exemplifies the set of rewards $\\{r\\in\\mathcal{R}\\,|\\,|\\overline{{\\mathcal{C}}}(r)-\\Delta|\\leqslant\\epsilon\\}$ that are (potentially) mis-classified. The length of the interval reduces with $\\epsilon$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 4.3 (PAC Framework). Let $\\epsilon,\\delta\\in(0,1)$ , and let $\\mathcal{D}^{E}$ be a dataset of $\\gamma^{E}$ expert\u2019s trajectories. An algorithm $\\mathfrak{A}$ exploring for $\\tau$ episodes is $(\\epsilon,\\delta)$ -PAC for the IRL classification problem $i f$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{M},\\pi^{E},\\mathfrak{A}}\\left(\\operatorname*{sup}_{r\\in\\mathcal{R}}\\left\\vert\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)-\\widehat{\\mathcal{C}}(r)\\right\\vert\\leqslant\\epsilon\\right)\\geqslant1-\\delta,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbb{P}_{\\mathcal{M},\\pi^{E},\\mathfrak{V}}$ is the joint probability measure induced by $\\pi^{E}$ and $\\mathfrak{A}$ in $\\mathcal{M}$ , and $\\hat{c}$ is the estimate of $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}$ computed by $\\mathfrak{A}$ . The sample complexity is defined by the pair $(\\tau^{E},\\tau)$ . ", "page_idx": 6}, {"type": "text", "text": "Intuitively, our goal is to estimate the (non)compatibility of the rewards in $\\mathcal{R}$ with sufficient accuracy, so that, given a threshold $\\Delta\\geqslant0$ , we are able to classify \u201cmost\u201d of them correctly w.h.p. (with high probability). The concept is exemplified in Figure 2. Note that the estimation problem is independent of the threshold $\\Delta$ , which can be appropriately selected to cope with noise in the demonstrations, (unknown) expert suboptimality, or to manage the amount of \u201cfalse negatives\u201d and \u201cfalse positives\u201d. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.2. For $\\eta\\geqslant0$ , let $\\mathcal{R}_{\\eta}:=\\{r\\in\\mathcal{R}\\,|\\,\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)\\leqslant$ $\\eta\\}$ and $\\widehat{\\mathcal{R}}_{\\eta}\\;:=\\;\\{r\\;\\in\\;{\\mathcal R}\\,|\\,\\widehat{\\mathcal{C}}(r)\\;\\leqslant\\;\\eta\\}$ denote the sets of rewards  ppositively classif iepd using, respectively, the true (non)compatibility $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}$ and the estimate $\\hat{c}$ constructed by an $(\\epsilon,\\delta)$ -PAC algorithm. Then, with pro bpability $1-\\delta_{i}$ , it holds that: $\\hat{\\mathcal{R}}_{\\Delta-\\epsilon}\\,\\subseteq\\,\\mathcal{R}_{\\Delta}\\,\\subseteq\\,\\hat{\\mathcal{R}}_{\\Delta+\\epsilon}$ . Thus, we can trade-off the ampount of \u201cfalse ne gpatives\u201d (resp. \u201cfalse positives\u201d) by, e.g., choosing the threshold $\\Delta\\gets\\Delta+\\epsilon$ (resp. $\\Delta\\gets\\Delta-\\epsilon)$ . ", "page_idx": 6}, {"type": "text", "text": "5 CATY-IRL: ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A Provably Efficient Algorithm for IRL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present CATY-IRL (CompATibilitY for IRL), a provably efficient algorithm for solving the online IRL classification problem. We consider three different kinds of structure for the MDPs: tabular MDPs, tabular MDPs with linear rewards, and Linear MDPs. Similarly to RFE, our online IRL classification setting is made of two phases: $(i)$ an exploration phase, in which the algorithm explores the environment using the knowledge of $\\mathcal{R}$ and of the expert\u2019s dataset $\\mathcal{D}^{E}$ to collect samples about the dynamics of the MDP, and $(i i)$ a classification phase, in which it performs the classification of a reward $r\\,\\in\\,\\mathcal{R}$ without interactions with the environment. A flow-chart is reported in Figure 1 (pseudocode in Appendix D). ", "page_idx": 6}, {"type": "image", "img_path": "ZjgcYMkCmX/tmp/072fef17307051bacee04c06990274abe9f391b4122551a548ca7d3c3418ca1f.jpg", "img_caption": ["Figure 1: Flow-chart of CATY-IRL. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Exploration phase. The exploration phase collects a dataset $\\mathcal{D}$ in a way that depends on the structure of the MDP and of the set of rewards $\\mathcal{R}$ to be classified. Specifically, for Linear MDPs, ", "page_idx": 6}, {"type": "text", "text": "CATY-IRL executes RFLin [62]. Instead, for tabular MDPs (with or without linear reward), CATY-IRL instantiates either BPI-UCBVI [37] for each reward $r\\in\\mathcal{R}$ (when $|{\\mathcal{R}}|=\\Theta(1)$ , i.e., a \u201csmall\u201d constant w.r.t. to the size of the MDP, where \u201csmall\u201d depends on the size of the state space, see Appendix D.2) or RF-Express [37]. Note that CATY-IRL in this phase does not use the expert\u2019s dataset $\\dot{\\mathcal{D}}^{E}$ . ", "page_idx": 7}, {"type": "text", "text": "Classification phase. The classification performs the estimation $\\widehat{\\mathcal{C}}(\\boldsymbol{r})$ of the (non)compatibility term $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)$ for the single input reward $r\\,\\in\\,\\mathcal{R}$ by splitting it i ntpo two independent estimates: $\\hat{J}^{E}(r)\\approx J^{\\pi^{E}}(r;p)$ , which is computed with $\\mathcal{D}^{E}$ only, and $\\widehat{J}^{*}(r)\\approx J^{*}(r;p)$ , which is computed wpith $\\mathcal{D}$ only. Concerning $\\hat{J}^{E}(r)$ , when the reward is linea rp $r_{h}(s,a)\\,=\\,\\langle\\phi(s,a),\\theta_{h}\\rangle$ , CATY-IRL uses $\\mathcal{D}^{E}$ to construct an em ppirical estimate E \u00ab \u03c8p,\u03c0Eof the expert\u2019s expected feature count [6]. Otherwise, it directly estimates $\\widehat{d}^{E}\\approx d^{p,\\pi^{E}}$ tphe expert\u2019s occupancy measure. Such estimates can be used to derive $\\hat{J}^{E}(\\bar{r})$ straightfo rpwardly. Regarding $\\hat{J}^{*}(r)$ , CATY-IRL exploits the planning phase of the corresp onpding RFE (or BPI) algorithm adop tped at exploration phase.8 Finally, CATY-IRL applies the (potentially negative) input threshold $\\Delta$ to the difference $\\hat{J}^{*}(r)-\\hat{J}^{E}(r)$ to perform the classification. See Appendix $\\mathrm{D}$ for the full pseudo-code. Clearly, CA TpY-IRL c apn be implemented in practice, since it considers a single reward at a time instead of computing the full feasible set, and it is computationally efficient in linear MDPs, since it uses a computationally efficient algorithm as subroutine (see [62]). ", "page_idx": 7}, {"type": "text", "text": "Sample Efficiency. The next result analyzes the sample complexity (Definition 4.3) of CATY-IRL. Theorem 5.1 (Sample Complexity of CATY-IRL). Let $\\epsilon,\\delta\\in(0,1)$ . Then CATY-IRL is $(\\epsilon,\\delta)$ -PAC for IRL with a sample complexity upper bounded by: ", "page_idx": 7}, {"type": "text", "text": "Tabular MDPs: $\\begin{array}{r l}&{\\tau^{E}\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}S A}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\Big),\\quad\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}S A}{\\epsilon^{2}}\\Big(N+\\log\\frac{1}{\\delta}\\Big)\\Big),}\\\\ &{\\tau^{E}\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}d}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\Big),\\quad\\quad\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}S A}{\\epsilon^{2}}\\Big(N+\\log\\frac{1}{\\delta}\\Big)\\Big),}\\\\ &{\\tau^{E}\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}d}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\Big),\\quad\\quad\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{5}d}{\\epsilon^{2}}\\Big(d+\\log\\frac{1}{\\delta}\\Big)\\Big),}\\end{array}$ Tabular MDPs with linear rewards:   \nLinear MDPs:   \nwhere $N=0\\;i f|\\mathcal{R}|=\\Theta(1),$ , and $N=S$ otherwise. ", "page_idx": 7}, {"type": "text", "text": "Some observations are in order. We conjecture that the $d^{2}$ dependence when $|{\\mathcal{R}}|=\\Theta(1)$ is unavoidable in Linear MDPs because of the lower bound for BPI in [62]. In tabular MDPs with deterministic expert, one might use the results in [66] to reduce the rate of $\\tau^{E}$ from $\\tilde{\\mathcal{O}}(S A H^{3}\\log(\\delta^{-1})/\\epsilon^{2})$ to $\\tilde{\\mathcal{O}}(S H^{3/2}\\log(\\delta^{-1})/\\epsilon^{2})$ . Finally, note that the choice $\\Delta=\\epsilon$ allows us to  rpositively classify all the rrewards in the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ w.h.p. and, in this case, other rewards positively classified have true (non)compatibility at most $2\\epsilon$ w.h.p. In light of this result we conclude that rewards compatibility framework allows the practical development of sample efficient algorithms (e.g., CATY-IRL) in Linear MDPs with large/continuous state spaces. ", "page_idx": 7}, {"type": "text", "text": "6 Statistical Barriers and Objective-Free Exploration ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we show that CATY-IRL is minimax optimal for the number of exploration episodes in tabular MDPs, and that RFE and IRL share the same theoretical sample complexity. This allows us to formulate Objective-Free Exploration, a unifying setting for exploration problems. ", "page_idx": 7}, {"type": "text", "text": "6.1 The Theoretical Limits of IRL (and RFE) in the Tabular Setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In CATY-IRL, we use a minimax optimal RFE algorithm for exploration. However, this does not entail that CATY-IRL is minimax optimal for the IRL classification problem. There might exist another PAC algorithm with a sample complexity smaller than CATY-IRL. The following result states that, in the tabular setting, the bound in Theorem 5.1 is tight for the number of episodes $\\tau$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 6.1 (IRL Classification - Lower Bound). Let A be an $(\\epsilon,\\delta)$ -PAC algorithm for the IRL classification in tabular MDPs. Let $\\tau$ be the number of exploration episodes. Then, there exists an IRL classification instance such that: ", "page_idx": 8}, {"type": "equation", "text": "$$\ni f\\,|\\mathcal{R}|\\geqslant1:\\,\\tau\\geqslant\\Omega\\bigg(\\frac{H^{3}S A}{\\epsilon^{2}}\\log\\frac1\\delta\\bigg),\\qquad i f\\,\\mathcal{R}=\\mathfrak{R}:\\,\\tau\\geqslant\\Omega\\bigg(\\frac{H^{3}S A}{\\epsilon^{2}}\\Big(S+\\log\\frac1\\delta\\Big)\\bigg).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In both cases, the lower bound is matched by CATY-IRL, up to logarithmic factors. Note that CATY-IRL explores without using $\\mathcal{D}^{E}$ , thus, minimax optimality for $\\tau$ can be achieved without the knowledge of $\\mathcal{D}^{E}$ at exploration phase. As a by-product, we observe that a similar lower bound construction can be made also for RFE, leading to the following result. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.2 (RFE - Refined Lower Bound). Let $\\mathfrak{A}$ be an $(\\epsilon,\\delta)$ -PAC algorithm for RFE in tabular MDPs. Let $\\tau$ be the number of exploration episodes. Then, there exists an RFE instance such that: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tau\\geqslant\\Omega\\Biggl(\\frac{H^{3}S A}{\\epsilon^{2}}\\Bigl(S+\\log\\frac{1}{\\delta}\\Bigr)\\Biggr).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This bound improves the state-of-the-art RFE lower bound $\\begin{array}{r}{\\Omega\\big(\\frac{H^{3}S A}{\\epsilon^{2}.}\\big(\\frac{S}{H}{+}\\mathrm{log}\\,\\frac{1}{\\delta}\\big)\\big)}\\end{array}$ (obtained combining the bounds in [21] and [12]) by one $H$ factor, and it is matched by RF-Express [37]. ", "page_idx": 8}, {"type": "text", "text": "6.2 Objective-Free Exploration (OFE) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "What is the most efficient exploration strategy that can be performed in an unknown environment? It depends on the subsequent task that shall be solved. However, if the task is unknown at the exploration phase, we need a strategy that suffices for all the tasks that one might be interested in solving. Let us denote by $\\mathcal{F}$ the set of RL and IRL classification tasks. Since CATY-IRL is a sample efficient algorithm for the IRL classification problem, and it uses RFE as a subroutine, we conclude that the RFE exploration strategy is sufficient (and also minimax optimal in tabular MDPs) to obtain guarantees for class $\\mathcal{F}$ . Are there other problems for which RFE exploration suffices when the specific problem instance is revealed a posteriori of the exploration phase? We believe so, and in Appendix E, we identify two additional problems, i.e., Matching Performance (MP) and Imitation Learning from Observations alone (ILfO) [34], that represent potential candidates to belong to $\\mathcal{F}$ . ", "page_idx": 8}, {"type": "text", "text": "More in general, we formulate the Objective-Free Exploration $(O F E)$ problem as follows: ", "page_idx": 8}, {"type": "text", "text": "Definition 6.1 (Objective-Free Exploration). Given a tuple $(\\mathcal{M},\\mathcal{F},(\\epsilon,\\delta))$ , where $\\mathcal{M}$ is an unknown environment (e.g., MDP without reward), and $\\mathcal{F}$ is a certain class of tasks (e.g., all RL and IRL problems), the Objective-Free Exploration $(O F E)$ problem aims to find an exploration of the environment $\\mathcal{M}$ (e.g., RFE exploration) that permits to solve any task $f\\in\\mathcal F$ in an $(\\epsilon,\\delta)$ -correct manner. ", "page_idx": 8}, {"type": "text", "text": "This problem is called \u201cobjective-free\u201d because it does not require the knowledge of the specific \u201cobjective\u201d $f\\in\\mathcal F$ to be solved. In Appendix F, we describe a use case for OFE. We believe this is an interesting problem to be studied in future. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we have shown that the feasible set cannot be learned efficiently in problems with large/continuous state spaces even under the strong structure provided by Linear MDPs. For this reason, we have introduced the powerful framework of compatible rewards, which formalizes the intuitive notion of compatibility of a reward function with expert demonstrations, and it allows us to formulate the IRL problem as a classification task. In this context, we have devised CATY-IRL, a provably efficient IRL algorithm for Linear MDPs with large/continuous state spaces. Furthermore, in tabular MDPs, we have demonstrated the minimax optimality of CATY-IRL at exploration by presenting a novel lower bound to the IRL classification problem. As a by-product, our construction improves the current state-of-the-art lower bound for RFE. Finally, we have introduced OFE, a unifying problem setting for exploration problems, which generalizes both RFE and IRL. ", "page_idx": 8}, {"type": "text", "text": "Limitations. A limitation of our contributions concerns the adoption of the Linear MDP model, whose assumptions are overly strong to be consistently applied to real-world applications. Nevertheless, while the rewards compatibility framework is general and not tied to Linear MDPs, we believe that Linear MDPs represent an important initial step toward the development of provably efficient IRL algorithms with more general function approximation structures. Although a lower bound for Linear MDPs is missing, we believe that it represents an interesting direction for future works. Finally, we note that the empirical validation of the proposed algorithm is out of the scope of this work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Future Directions. Promising directions for future works concern the extension of the analysis of the rewards compatibility framework beyond Linear MDPs to general function approximation and to the offline setting. In addition, it might be fascinating to extend the notion of reward compatibility to other kinds of expert feedback (in the context of ReL), and to other IRL settings (e.g., suboptimal experts). Finally, we believe that OFE should be analysed in-depth given its practical importance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AI4REALNET has received funding from European Union\u2019s Horizon Europe Research and Innovation programme under the Grant Agreement No 101119527. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "Funded by the European Union - Next Generation EU within the project NRPP M4C2, Investment 1.,3 DD. 341 - 15 march 2022 - FAIR - Future Artificial Intelligence Research - Spoke 4 - PE00000013 - D53C22002380006. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Ng. An application of reinforcement learning to aerobatic helicopter flight. In Advances in Neural Information Processing Systems 19 (NeurIPS), 2006.   \n[2] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning 21 (ICML), 2004.   \n[3] Pieter Abbeel and Andrew Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In International Conference on Machine Learning 22 (ICML), 2005.   \n[4] Stephen Adams, Tyler Cody, and Peter A. Beling. A survey of inverse reinforcement learning. Artificial Intelligence Review, 55:4307\u20134346, 2022.   \n[5] Kareem Amin and Satinder Singh. Towards resolving unidentifiability in inverse reinforcement learning, 2016.   \n[6] Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and progress. Artificial Intelligence, 297:103500, 2018.   \n[7] Matt Barnes, Matthew Abueg, Oliver F. Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus Wulfmeier, and Shawn O\u2019Banion. Massively scalable inverse reinforcement learning in google maps, 2024.   \n[8] P. Dimitri Bertsekas. Convex Optimization Theory. Athena Scientific, 2009.   \n[9] Michael Bowling, John D. Martin, David Abel, and Will Dabney. Settling the reward hypothesis. In International Conference on Machine Learning 40 (ICML), 2023.   \n[10] Haoyang Cao, Samuel Cohen, and Lukasz Szpruch. Identifiability in inverse reinforcement learning. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 12362\u2013 12373, 2021.   \n[11] Gregory Dexter, Kevin Bello, and Jean Honorio. Inverse reinforcement learning in a continuous state space with formal guarantees. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 6972\u20136982, 2021.   \n[12] Omar Darwiche Domingues, Pierre M\u00e9nard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In International Conference on Algorithmic Learning Theory 32 (ALT), volume 132, pages 578\u2013598, 2021.   \n[13] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In International Conference on Machine Learning 38 (ICML), volume 139, pages 2826\u20132836, 2021.   \n[14] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning 33 (ICML), volume 48, pages 49\u201358, 2016.   \n[15] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations 5 (ICLR), 2017.   \n[16] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design. In Advances in Neural Information Processing Systems 30 (NeurIPS), 2017.   \n[17] Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. In Advances in Neural Information Processing Systems 29 (NeurIPS), 2016.   \n[18] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems 29 (NeurIPS), 2016.   \n[19] Hong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 4415\u20134426, 2020.   \n[20] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning 34 (ICML), volume 70, pages 1704\u20131713, 2017.   \n[21] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning 37 (ICML), volume 119, pages 4870\u20134879, 2020.   \n[22] Chi Jin, Qinghua Liu, and Sobhan Miryoosef.i Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 13406\u201313418, 2021.   \n[23] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory 33 (COLT), volume 125, pages 2137\u20132143, 2020.   \n[24] Emilie Kaufmann, Pierre M\u00e9nard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, and Michal Valko. Adaptive reward-free exploration. In International Conference on Algorithmic Learning Theory 32 (ALT), volume 132, pages 865\u2013891, 2021.   \n[25] Kuno Kim, Shivam Garg, Kirankumar Shiragur, and Stefano Ermon. Reward identification in inverse reinforcement learning. In International Conference on Machine Learning 38 (ICML), pages 5496\u20135505, 2021.   \n[26] Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon. Domain adaptive imitation learning. In International Conference on Machine Learning 37 (ICML), volume 119, pages 5286\u20135295, 2020.   \n[27] Edouard Klein, Matthieu Geist, Bilal Piot, and Olivier Pietquin. Inverse reinforcement learning through structured classification. In Advances in Neural Information Processing Systems 25 (NeurIPS), 2012.   \n[28] Abi Komanduru and Jean Honorio. On the correctness and sample complexity of inverse reinforcement learning. In Advances in Neural Information Processing Systems 32 (NeurIPS), 2019.   \n[29] Abi Komanduru and Jean Honorio. A lower bound for the sample complexity of inverse reinforcement learning. In International Conference on Machine Learning 38 (ICML), volume 139, pages 5676\u20135685, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[30] David M. Kreps. Notes On The Theory Of Choice. Westview Press, 1988. ", "page_idx": 11}, {"type": "text", "text": "[31] Filippo Lazzati, Mirco Mutti, and Alberto Maria Metelli. Offline inverse rl: New solution concepts and provably efficient algorithms. In International Conference on Machine Learning 41 (ICML), 2024.   \n[32] Gen Li, Yuling Yan, Yuxin Chen, and Jianqing Fan. Minimax-optimal reward-agnostic exploration in reinforcement learning, 2023.   \n[33] David Lindner, Andreas Krause, and Giorgia Ramponi. Active exploration for inverse reinforcement learning. In Advances in Neural Information Processing Systems 35 (NeurIPS), pages 5843\u20135853, 2022.   \n[34] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In IEEE International Conference on Robotics and Automation (ICRA), pages 1118\u20131125, 2018.   \n[35] Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Learning from demonstration: Provably efficient adversarial policy imitation with linear function approximation. In International Conference on Machine Learning 39 (ICML), volume 162, pages 14094\u201314138, 2022.   \n[36] Manuel Lopes, Francisco Melo, and Luis Montesano. Active learning for reward estimation in inverse reinforcement learning. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD), pages 31\u201346, 2009.   \n[37] Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Emilie Kaufmann, Edouard Leurent, and Michal Valko. Fast active learning for pure exploration in reinforcement learning. In International Conference on Machine Learning 38 (ICML), volume 139, pages 7599\u20137608, 2021.   \n[38] Alberto Maria Metelli, Filippo Lazzati, and Marcello Restelli. Towards theoretical understanding of inverse reinforcement learning. In International Conference on Machine Learning 40 (ICML), volume 202, pages 24555\u201324591, 2023.   \n[39] Alberto Maria Metelli, Giorgia Ramponi, Alessandro Concetti, and Marcello Restelli. Provably efficient learning of transferable rewards. In International Conference on Machine Learning 38 (ICML), volume 139, pages 7665\u20137676, 2021.   \n[40] Bernard Michini, Mark Cutler, and Jonathan P. How. Scalable reward learning from demonstration. In IEEE International Conference on Robotics and Automation (ICRA), pages 303\u2013308, 2013.   \n[41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013.   \n[42] Andrew Y. $\\mathrm{Ng}$ and Stuart J. Russell. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning 17 (ICML), pages 663\u2013670, 2000.   \n[43] Donald Ornstein. On the existence of stationary optimal strategies. Proceedings of the American Mathematical Society, 20(2):563\u2013569, 1969.   \n[44] Riccardo Poiani, Gabriele Curti, Alberto Maria Metelli, and Marcello Restelli. Inverse reinforcement learning with sub-optimal experts, 2024.   \n[45] Martin Lee Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1994.   \n[46] Nived Rajaraman, Yanjun Han, Lin Yang, Jingbo Liu, Jiantao Jiao, and Kannan Ramchandran. On the value of interaction and function approximation in imitation learning. In Advances in Neural Information Processing Systems 34 (NeurIPS), volume 34, pages 1325\u20131336, 2021.   \n[47] Nived Rajaraman, Lin Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamental limits of imitation learning. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 2914\u20132924, 2020.   \n[48] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In International Conference on Machine Learning 23 (ICML), pages 729\u2013736, 2006.   \n[49] Stuart Russell. Learning agents for uncertain environments (extended abstract). In Conference on Computational Learning Theory 11 (COLT), pages 101\u2013103, 1998.   \n[50] Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall, 3 edition, 2010.   \n[51] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather than assuming, human biases for reward inference. In International Conference on Machine Learning 36 (ICML), volume 97, pages 5670\u20135679, 2019.   \n[52] Mehran Shakerinava and Siamak Ravanbakhsh. Utility theory for sequential decision making. In International Conference on Machine Learning 39 (ICML), volume 162, pages 19616\u201319625, 2022.   \n[53] Lior Shani, Tom Zahavy, and Shie Mannor. Online apprenticeship learning. In AAAI Conference on Artificial Intelligence 36 (AAAI), pages 8240\u20138248, 2022.   \n[54] David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484\u2013503, 2016.   \n[55] Joar Skalse and Alessandro Abate. Misspecification in inverse reinforcement learning. In AAAI Conference on Artificial Intelligence 37 (AAAI), pages 15136\u201315143, 2023.   \n[56] Joar Max Viktor Skalse, Matthew Farrugia-Roberts, Stuart Russell, Alessandro Abate, and Adam Gleave. Invariance in policy optimisation and partial identifiability in reward learning. In International Conference on Machine Learning 40 (ICML), volume 202, pages 32033\u201332058, 2023.   \n[57] Wen Sun, Anirudh Vemula, Byron Boots, and Drew Bagnell. Provably efficient imitation learning from observation alone. In International Conference on Machine Learning 36 (ICML), volume 97, pages 6036\u20136045, 2019.   \n[58] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, 2018.   \n[59] Gokul Swamy, Nived Rajaraman, Matt Peng, Sanjiban Choudhury, J. Bagnell, Steven Z. Wu, Jiantao Jiao, and Kannan Ramchandran. Minimax optimal online imitation learning via replay estimation. In Advances in Neural Information Processing Systems 35 (NeruIPS), volume 35, pages 7077\u20137088, 2022.   \n[60] Umar Syed and Robert E Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural Information Processing Systems 20 (NeurIPS), 2007.   \n[61] Luca Viano, Stratis Skoulakis, and Volkan Cevher. Imitation learning in discounted linear mdps without exploration assumptions, 2024.   \n[62] Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-free RL is no harder than reward-aware RL in linear Markov decision processes. In International Conference on Machine Learning 39 (ICML), volume 162, pages 22430\u201322456, 2022.   \n[63] Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement learning with linear function approximation. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 17816\u201317826, 2020.   \n[64] Ruosong Wang, Ruslan Salakhutdinov, and Lin F. Yang. Reinforcement learning with general value function approximation: provably efficient approach via bounded eluder dimension. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 6123\u20136135, 2020.   \n[65] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F\u00fcrnkranz. A survey of preference-based reinforcement learning methods. Journal of Machine Learning Research, 18:1\u201346, 2017.   \n[66] Tian Xu, Ziniu Li, Yang Yu, and Zhimin Luo. Provably efficient adversarial imitation learning with unknown transitions. In Conference on Uncertainty in Artificial Intelligence 39 (UAI), volume 216, pages 2367\u20132378, 2023.   \n[67] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In International Conference on Machine Learning 36 (ICML), volume 97, pages 6995\u20137004, 2019.   \n[68] Lei Zhao, Mengdi Wang, and Yu Bai. Is inverse reinforcement learning harder than standard reinforcement learning? In International Conference on Machine Learning 41 (ICML), 2024.   \n[69] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In Conference on Learning Theory 34 (COLT), pages 4532\u20134576, 2021.   \n[70] Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence 23 (AAAI), pages 1433\u20131438, 2008. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we report and describe the literature that most relates to this paper. Theoretical works concerning the online IRL problem can be grouped in works that concern the feasible set, and works that do not. ", "page_idx": 14}, {"type": "text", "text": "Let us begin with works related to the feasible set. While the notion of feasible set has been introduced implicitly in [42], the first paper that analyses the sample complexity of estimating the feasible set in online IRL is [39]. Authors in [39] adopt the simple generative model in tabular MDPs, and devise two sample efficient algorithms. [33] focuses on the same problem as [39], but adopts a forward model in tabular MDPs. By adopting RFE exploration algorithms, they devise sample efficient algorithms. However, as remarked in [68], the learning framework considered in [33] suffers from a major issue. [38] builds upon [39] to construct the first minimax lower bound for the problem of estimating the fwehaesirbe saent d nagr e at hgee ncearradtiinvael itmy oodfe tl.h e Tshtaet el oawnedr  abctoiuonn ds ipsa cien st, oisr dtehre  ohfo $\\begin{array}{r}{\\Omega\\big(\\frac{H^{3}S A}{\\epsilon^{2}}(S+\\log\\frac{1}{\\delta})\\big)}\\end{array}$ $S$ $A$ $H$ $\\epsilon$ and $\\delta$ the failure probability. In addition, [38] develops US-IRL, an efficient algorithm whose sample complexity matches the lower bound. [44] analyze a setting analogous to that of [38], in which there is availability of a single optimal expert and multiple suboptimal experts with known suboptimality. [31] analyse the problem of estimating the feasible set when no active exploration of the environment is allowed, but the learner is given a batch dataset collected by some behavior policy $\\pi^{b}$ . Interestingly, [31] focuses on two novel learning targets that are suited for the offline setting, i.e., a subset and a superset of the feasible set. Authors in [31] demonstrate that such sets are the tightest learnable subset and superset of the feasible set, and propose a pessimistic algoroithm, PIRLO, to estimate them. [68] analyses the same offilne setting as [31], but instead of focusing on the notion of feasible set directly, it considers the notion of reward mapping, which considers reward functions as parametrized by their value and advantage functions, and whose image coincides with the feasible set. ", "page_idx": 14}, {"type": "text", "text": "With regards to online IRL works that do not consider the feasible set, we mention [36], which analyses an active learning framework for IRL. However, [36] assumes that the transition model is known, and its goal is to estimate the expert policy only. Works [28] and [29] provide, respectively, an upper bound and a lower bound to the sample complexity of IRL for $\\beta$ -strict separable problems in the tabular setting. However, both the setting considered and the bound obtained are fairly different from ours. Analogously, [11] provides a sample efficient IRL algorithm for $\\beta.$ -strict separable problems with continuous state space. However, their setting is different from ours since they assume that the system can be modelled using a basis of orthonormal functions. ", "page_idx": 14}, {"type": "text", "text": "A.1 Additional Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we collect additional related works that deserve to be mentioned. ", "page_idx": 14}, {"type": "text", "text": "Identifiability and Reward Learning. As aforementioned, the IRL problem is ill-posed, thus, to retrieve a single reward, additional constraints shall be imposed. [5] analyses the setting in which demonstrations of an optimal policy for the same reward function are provided across environments with different transition models. In this way, authors can reduce the experimental unidentifiability, and recover the state-only reward function. [10] and [26] concern reward identifiability but in entropy-regularized MDPs [70, 15]. Such setting is in some sense easier than the common IRL setting, because entropy-regularization permits a unique optimal policy for any reward function. [10] uses expert demonstrations from multiple transition models and multiple discount factors to retrieve the reward function, while [26] analyses properties of the dynamics of the MDP to increase the constraints. With regards to the more general field of Reward Learning (ReL), we mention [19], which introduces a framework that formalizes the constraints imposed by various kinds of human feedback (like demonstrations or preferences [65]). Intuitively, multiple feedbacks about the same reward represent additional constraints beyond mere demonstrations. [56] characterizes the partial identifiability of the reward function based on various reward learning data sources. ", "page_idx": 14}, {"type": "text", "text": "Linear MDPs and Extensions. As explained for instance in [23], since lower bounds to the sample complexity of various RL tasks in tabular MDPs depend explicitly on the cardinality state space $S$ , then we need to add structure to the problem if we want to develop efficient algorithms that scale to large state spaces. For this reason, the works [67, 23] analyze the Linear MDP model, which enforces some linearity constraints to the common MDP model. In this way, authors are able to provide efficient algorithms for RL in problems with large/continuous state spaces. However, there are other settings beyond Linear MDPs that are analysed in the RL literature. [20] introduces the notion of Bellman rank as complexity measure, and provides a sample efficient algorithm for problems with small Bellman rank. [64] analyzes general value function approximation when the function class has a low eluder dimension. [22] generalizes both the eluder dimension and Bellman rank complexity measures by defining the Bellman eluder dimension and providing a provably efficient algorithm. [13] introduces bilinear classes, a structural framework that, among the others, generalizes Linear MDPs. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Reward-Free Exploration (RFE) in Tabular and Linear MDPs. The RFE problem was introduced in [21], where authors provided a sample efficient algorithm and a lower bound for tabular MDPs. Later on, the state-of-the-art sample-efficient algorithms for RFE in tabular MDPs have been developed in [24, 37, 32]. It should be remarked that RFE requires more samples than common RL in tabular MDPs. [63] proposes a sample efficient algorithm for RFE in linear MDPs. [62] improves the algorithm of [63] and, interestingly, demonstrates that RFE is no harder than RL in Linear MDPs. ", "page_idx": 15}, {"type": "text", "text": "Online Apprenticeship Learning (AL). The first works that provide a theoretical analysis of the AL setting when the transition model is unknown are [3, 60]. Recently, [53] formulates the online AL problem, which closely resembles the online IRL problem. The main difference is that in online AL the ultimate goal is to imitate the expert, while in IRL is to recover a reward function. [66] improves the results in [53] by combining an RFE algorithm with an efficient algorithm for the estimation of the visitation distribution of the deterministic expert\u2019s policy in tabular MDPs, presented in [47]. We mention also [46, 59] for the sample complexity of estimating the expert\u2019s policy in problems with linear function approximation. In the context of Imitation Learning from Observation alone (ILfO) [34], the work [57] proposes a probably efficient algorithm for large-scale MDPs with unknown transition model. [35] provides an efficient AL algorithm based on GAIL [18] in Linear Kernel Episodic MDPs [69] with unknown transition model. ", "page_idx": 15}, {"type": "text", "text": "Others. We mention work [27], which considers a classification approach for IRL. However, this is fairly different from our IRL problem formulation in Section 4. ", "page_idx": 15}, {"type": "text", "text": "B Additional Results and Proofs for Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide additional results beyond those presented in Section 3, and then we report the missing proofs. Specifically, in Appendix B.1, we provide two numerical examples that explain Proposition 3.1, in Appendix B.2 we show that some additional regularity assumptions beyond the Linear MDP cannot remove the dependence on the cardinality of the state space in the sample complexity. In Appendix B.3, we report and describe the sample efficient algorithm mentioned in Section 3, while in Appendix B.4 we collect all the missing proofs of this section. ", "page_idx": 15}, {"type": "text", "text": "B.1 Some Examples for Proposition 3.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The following examples aim to explain Proposition 3.1 in a simple manner. ", "page_idx": 15}, {"type": "text", "text": "Example B.1 (Non-degenerate feasible set). Let $\\mathcal{M}=(S,\\mathcal{A},H,d_{0})\\cup\\{\\pi^{E}\\}$ be an IRL instance such that S \u201c ts1, s2u $,A\\,=\\,\\{a_{1},a_{2}\\},H\\,=\\,1,d_{0}(s_{1})\\,=\\,d_{0}(s_{2})\\,=\\,1/2,$ $l_{0}(s_{1})\\,=\\,d_{0}(s_{2})\\,=\\,\\mathrm{{1/2}},\\dot{\\pi}^{E}(\\acute{s}_{1})\\,=\\,\\pi^{E}(s_{2})\\,=\\,a_{1}$ . Consider the feature mapping $\\phi_{1}$ s.t. $\\phi_{1}(s,a)~=~\\mathbb{1}\\{a~=~a_{1}\\}$ for all $s\\ \\in\\ S$ . Then, we have $\\Phi^{\\pi^{E}}=\\{1\\}$ and ${\\overline{{\\Phi}}}=\\{0\\}$ . Clearly, these sets can be separated by any hyperplane $w\\in\\mathbb{R}_{>0}$ , since $\\:1\\cdot w\\;>\\;0\\cdot w$ , and so $\\mathcal{R}_{p,\\pi^{E}}\\ \\ne\\ \\{\\overline{{r}}\\}$ , with $\\overline{{r}}_{h}(s,a)\\;=\\;0~\\forall(s,a,h)\\;\\in\\;{\\cal S}\\;\\times\\;{\\cal A}\\;\\times\\;[{\\cal H}]$ . Actually, $\\mathcal{R}_{p,\\pi^{E}}=\\{r\\in\\Re\\,|\\,\\exists\\theta\\in(0,\\cdot1]:\\,r_{1}(s,a)=\\langle\\phi(s,a),\\theta\\rangle\\,\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A}\\}.$ ", "page_idx": 15}, {"type": "text", "text": "Example B.2 (Degenerate feasible set). Consider the same IRL instance as in the previous example, but this time consider the feature mapping $\\phi_{2}$ s.t. $\\phi_{2}(s_{1},a)=\\mathbb{1}\\{a=a_{1}\\}$ , and $\\phi_{2}(s_{2},a)=\\mathbb{1}\\{a=$ $a_{2}\\}$ . Then, we have $\\Phi^{\\pi^{E}}=\\{0,1\\}$ and $\\overline{{\\Phi}}=\\{0,1\\}$ . Clearly, the two sets coincide, thus they cannot be separated, and $\\mathcal{R}_{p,\\pi^{E}}=\\{\\overline{{r}}\\}$ , with $\\overline{{r}}_{h}(s,a)=\\dot{0}\\;\\forall(s,a,h)\\in S\\times A\\times\\lVert H\\rVert$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 Additional Regularity Assumptions of the State Space do not Make the Problem Learnable ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In tabular MDPs with small state space $\\boldsymbol{S}$ , collecting samples from every state $s\\in S$ is feasible, and it is exactly what previous works do: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Under the assumption that $\\pi^{E}$ is deterministic, [39, 38] collect one sample from every $(s,h)\\in{\\mathcal{S}}\\times\\mathbb{I}\\mathbb{H}]$ using a generative model, obtaining $\\pi^{E}$ exactly.   \n\u2022 If $\\pi^{E}$ is stochastic, under the assumption that all actions in the support of the expert\u2019s policy are played with probability at least $\\pi_{\\mathrm{min}}$ (see Assumption D.1 of [38]), both [38, 68] are able to learn the support of $\\pi^{E}$ exactly w.h.p. using ${\\alpha1/\\pi_{\\mathrm{min}}}$ samples in the online setting.9   \n\u2022 In the offilne setting, assuming that the occupancy measure of the expert\u2019s policy is at least $d_{\\mathrm{min}}$ in all reachable $(s,a)\\in S\\times A$ , then [31] learns the support of $\\tilde{\\pi}^{E}$ exactly w.h.p. using ${\\alpha1/d_{\\mathrm{min}}}$ episodes. ", "page_idx": 16}, {"type": "text", "text": "However, when $\\boldsymbol{S}$ is large, even under the Linear MDP assumption, this is not possible. In Section 3, we have formalized this fact with the following proposition: ", "page_idx": 16}, {"type": "text", "text": "Theorem 3.2 (Statistical Inefficiency). Let $\\mathcal{M}\\cup\\{\\pi^{E}\\}$ be a Linear IRL instance with finite state space $\\boldsymbol{S}$ and deterministic expert\u2019s policy, and let $\\epsilon,\\delta\\in(0,1)$ . If an algorithm $\\mathfrak{A}$ is $(\\epsilon,\\delta)$ -PAC, then $\\dot{\\boldsymbol{\\tau}}^{E}=\\Omega(\\boldsymbol{S})$ , where $S:=|S|$ is the cardinality of the state space. ", "page_idx": 16}, {"type": "text", "text": "Theorem 3.2 tells us that the Linear MDP assumption is too weak for the feasible set to be learnable using the PAC framework of Definition 3.2 with a number of samples independent of the cardinality of the state space. Therefore, we can try to introduce an additional assumption on the structure of the IRL problem $\\mathcal{M}\\cup\\{\\pi^{E}\\}$ and see whether it helps in alleviating the issue. Let us consider the following first assumption. ", "page_idx": 16}, {"type": "text", "text": "Assumption B.1. We assume a Lipschitz continuity property between features and states: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall(s,a,s^{\\prime})\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}:\\quad\\|\\phi(s,a)-\\phi(s^{\\prime},a)\\|_{2}\\leqslant L\\|s-s^{\\prime}\\|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for some $L>0$ and some distance $\\|\\cdot-\\cdot\\|\\,i n\\,{\\mathcal{S}}$ . ", "page_idx": 16}, {"type": "text", "text": "The intuition is that, based on the fact that in Linear MDPs the $Q$ -function of any policy $\\pi$ is linear in the feature mapping $Q_{h}^{\\pi}(\\cdot,\\cdot)=(\\phi(\\cdot,\\cdot),w_{h}^{\\pi})$ for some parameter vector $w_{h}^{\\pi}\\in\\dot{\\mathbb{R}}^{d}$ (see [23]), then if we are able to $\\epsilon_{}$ -cover the state space $\\boldsymbol{S}$ , we can approximate the $Q$ -function $Q_{h}^{\\pi}(s,\\cdot)$ in any $s\\in S$ with the $Q$ -function $Q_{h}^{\\pi}(s^{\\prime},\\cdot)$ of the closest point $s^{\\prime}$ int the covering, so that $|Q_{h}^{\\pi}(s,a)-Q_{h}^{\\pi}(s^{\\prime},a)|=$ $|(\\phi(s,a)-\\phi(s^{\\prime},a))^{\\top}\\bar{w}_{h}^{\\pi}|\\leqslant\\|\\phi(s,a)-\\phi(\\bar{s}^{\\prime},a)\\|_{2}\\|w_{h}^{\\pi}\\|_{2}\\leqslant L\\epsilon\\|\\bar{w}_{h}^{\\pi}\\|_{2}$ . However, this assumption is not sufficient. ", "page_idx": 16}, {"type": "text", "text": "Proposition B.1. Under the setting of Proposition 3.2, even under Assumption B.1, then an algorithm is $(\\epsilon,\\delta)$ -PAC only $\\boldsymbol{\\mathrm{i}}f\\tau^{E}=\\Omega(S)$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption B.1 fails because it does not provide any information about how the knowledge of the expert\u2019s policy at a state can be \u201ctransferred\u201d to other states, and thus we still need to sample almost all the states of Sp,\u03c0 to get an acceptable feasible set. ", "page_idx": 16}, {"type": "text", "text": "We devise another assumption to attempt to fix this issue. ", "page_idx": 16}, {"type": "text", "text": "Assumption B.2. We assume the following Lipschitz continuity property: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall(s,s^{\\prime})\\in\\mathcal{S}\\times\\mathcal{S}:\\quad\\|\\phi(s,\\pi_{h}^{E}(s))-\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))\\|_{2}\\leqslant L\\|s-s^{\\prime}\\|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for some $L>0$ and some distance $\\|\\cdot-\\cdot\\|\\,i n\\,{\\mathcal{S}}$ . ", "page_idx": 16}, {"type": "text", "text": "This assumption says that states that are close to each other cannot have the features corresponding to the expert\u2019s action too far away from each other. From a high-level point of view, it says that the features are \u201csomehow\u201d regular with $\\pi^{E}$ , so that when the expert lies in $s^{\\prime}$ which is really close to $s$ , then she plays an action which has the same \u201ceffect\u201d (i.e., same transition model and same reward, due to the Linear MDP assumption) as the expert\u2019s action in $s$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption B.2 is not comparable with Assumption B.1 since, on the one hand, it does not hold for all actions in $\\boldsymbol{\\mathcal{A}}$ , but only for those corresponding to $\\pi^{E}$ , but, on the other hand, provides information on how to transfer knowledge about $\\pi^{E}$ to neighbor states. ", "page_idx": 16}, {"type": "text", "text": "Let $\\begin{array}{r}{\\Delta^{\\prime}\\,:=\\,\\operatorname*{min}_{s\\in S,a,a^{\\prime}\\in A:\\phi(s,a)\\neq\\phi(s,a^{\\prime})}\\|\\phi(s,a)-\\phi(s,a^{\\prime})\\|_{2}}\\end{array}$ , i.e., the smallest non-zero distance between the features of different actions. Clearly, when $\\boldsymbol{S}$ is finite, since in Linear MDPs also $A:=|A|$ is finite, then $\\Delta^{\\prime}$ is finite too. So we can define a new quantity $\\Delta$ to be any number $0<\\Delta<\\Delta^{\\prime}$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition B.2. Under the setting of Proposition 3.2, under Assumption B.2, then a number of samples $\\tau^{E}\\;=\\;|\\mathcal{N}(\\frac{\\Delta}{2L};S,\\|\\cdot\\|)|$ is sufficient to recover $\\pi^{E}$ exactly in any $(s,h)\\,\\in\\,{\\mathcal{S}}$ , where $|\\mathcal{N}(\\frac{\\Delta}{2L};\\mathcal{S},\\|\\cdot\\|)|$ is the $\\Delta/(2L)$ -covering number of space $\\boldsymbol{S}$ w.r.t. distance $\\|\\cdot\\|$ . ", "page_idx": 17}, {"type": "text", "text": "Intuitively, by constructing a covering with a sufficiently small radius in the state space $\\boldsymbol{S}$ , then we are able to retrieve the exact expert\u2019s action in the neighborood of each state of the covering. Doing so, we are able to construct $\\epsilon$ -correct estimates of the feasible set. Of course, this is possible as long as $\\Delta^{\\prime}$ is not too small, and $L$ is not too large. When $\\boldsymbol{S}$ is infinitely large or continuous, it might be possible to construct feature mappings in which $\\Delta^{\\prime}\\to0$ , and so the approach would still require too many samples. ", "page_idx": 17}, {"type": "text", "text": "However, even for cases with finite and not too small $\\Delta^{\\prime}$ , the result in Proposition B.2 is not satisfactory, because it just allows to retrieve $\\pi^{E}$ under a stronger assumption than Linear MDPs, but not to perform an interesting learning process. We observe that the feasible set is an \u201cunstable\u201d concept, in the sense that, based on Proposition 3.1, changing the expert action in a single state might reduce the feasible set from a continuum of rewards to a singleton, or vice versa. ", "page_idx": 17}, {"type": "text", "text": "Remark B.1. If we want to be able to recover the exact feasible set efficiently, we need to recover the exact expert\u2019s policy almost everywhere. ", "page_idx": 17}, {"type": "text", "text": "B.3 Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "By exploiting an RFE algorithm as sub-routine like that of Algorithm 1 in [63] or Algorithm 1 in [62], we are able to construct estimates of the transition model $\\widehat{p}$ , that can be used to compute an \u201cempirical\u201d estimate of the feasible set $\\hat{\\mathcal{R}}\\approx\\mathcal{R}_{\\hat{p},\\pi^{E}}$ (since $\\phi$ and $\\pi^{E}$ are known). The algorithm is presented in Algorithm 1. ", "page_idx": 17}, {"type": "text", "text": "", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Algorithm 1: IRL for Linear MDPs (known expert\u2019s policy) Data: failure probability $\\delta>0$ , error tolerance $\\epsilon>0$ , expert policy $\\pi^{E}$ , all sets $\\mathcal{Z}\\subseteq S\\times\\mathbb{I}\\!H]$ that coincide with $S^{p,\\pi^{E}}$ almost everywhere based on measure dp,\u03c0E 1 D \u00d0 RFE_Exploration $(\\delta,\\epsilon)$ /\\* Various choices $\\ast/$ 2 for $h$ in $\\{H,H-1,\\ldots,2,1\\}$ do 3 $\\Lambda_{h}\\gets I+\\sum_{k=1}^{\\tau}\\phi(s_{h}^{k},a_{h}^{k})\\phi(s_{h}^{k},a_{h}^{k})\\mathsf{T}$ $\\widehat{\\mu}_{h}(\\cdot)\\gets\\Lambda_{h}^{-1}\\sum_{k=1}^{\\tau}\\phi(s_{h}^{k},a_{h}^{k})\\delta(\\cdot,s_{h+1}^{k})$ 5 end 6 $\\widehat{p}_{h}(\\cdot|s,a)\\gets\\langle\\phi(s,a),\\widehat{\\mu}_{h}(\\cdot)\\rangle$ for all $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ 7 $\\begin{array}{r l}&{p_{h}(\\cdot\\vert s,u)\\gets\\langle\\psi(s,u),\\mu_{h}(\\cdot)\\rangle\\,\\mathfrak{m}\\,\\mathfrak{a}\\mathfrak{m}\\,\\,\\mathfrak{a},\\mu)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\Vert^{\\mu}\\Vert}\\\\ &{\\widehat{\\mathcal{R}}\\gets\\left\\{\\widehat{r}\\in\\Re\\left\\vert\\exists\\mathcal{Z},\\forall(s,h)\\in\\mathcal{Z},\\forall a\\in\\mathcal{A}:\\,\\underbrace{\\mathbb{E}}_{a^{\\prime}\\sim\\pi_{h}^{E}(\\cdot\\vert s)}Q_{h}^{*}(s,a^{\\prime};\\widehat{p},\\widehat{r})\\geqslant Q_{h}^{*}(s,a;\\widehat{p},\\widehat{r})\\right\\}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Simply put, Algorithm 1 uses the dataset collected by an RFE algorithm to compute a least-squares estimate of the transition model $\\widehat{p},$ , and then it returns the feasible set defined according to it (recall that $\\phi$ and $\\pi^{E}$ are known). No tipce that this algorithm cannot be implemented in practice due to various reasons, like the presence of the Dirac delta $\\delta$ measure in the definition of some quantities (see Appendix B.4.3), and the fact that the feasible set is, potentially, a set containing infinite rewards. Nevertheless, Theorem 3.3 states that this algorithm is sample efficient. The proof of the theorem is provided in Appendix B.4.3. ", "page_idx": 17}, {"type": "text", "text": "It should be remarked that Algorithm 1 takes in input also the true support of the visit distribution of the expert policy $S^{p,\\pi^{E}}$ in case $\\boldsymbol{S}$ is finite, and all the possible sets $\\mathcal{Z}$ that agree with $S^{p,\\pi^{E}}$ a.e. based on the measure $d^{p,\\pi^{E}}$ in case $\\boldsymbol{S}$ is infinite. Intuitively, this set $(S^{p,\\pi^{E}})$ of $(s,h)$ pairs represents the domain in which $\\pi^{E}$ is defined. Indeed, since the expert in the true problem $p$ never visits pairs $(s^{\\prime},h^{\\prime})\\notin S^{p,\\pi^{E}}$ , its expert policy might reasonably be non well-defined there. When $\\boldsymbol{S}$ is infinite, we require all sets $\\mathcal{Z}$ because otherwise we cannot know which are the sets $S^{p,\\pi^{E}}\\backslash{\\mathcal{Z}}$ with zero measure, i.e., in which the reward can induce an optimal action different from the expert\u2019s one, since the overall contribution to the expected return is zero. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "The proof of Theorem 3.3 is obtained by using Algorithm 1 of [62] at Line 1 of Algorithm 1. In Appendix B.4.3, we demonstrate an upper bound also if we use Algorithm 1 in [63]. ", "page_idx": 18}, {"type": "text", "text": "B.4 Missing Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before diving into the proofs, we recall some important properties of the feasible set and of the Linear MDPs that will be useful in the proofs. First, we provide an explicit form for the feasible set presented at Definition 3.1. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.3 (Lemma E.1 in [31]). In the setting of Definition 3.1, if $\\boldsymbol{S}$ is finite, then the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ satisfies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{p,\\pi^{E}}=\\Big\\{r\\in\\mathfrak{R}\\Big|\\,\\forall(s,h)\\in\\mathcal{S}^{p,\\pi^{E}},\\forall a\\in\\mathcal{A}:\\,\\underset{a^{\\prime}\\sim\\pi_{h}^{E}(\\cdot\\vert s)}{\\mathbb{E}}Q_{h}^{*}(s,a^{\\prime};p,r)\\geqslant Q_{h}^{*}(s,a;p,r)\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that we have extended Lemma E.1 in [31] to consider stochastic expert policies (the extension is trivial). We can easily extend it to problems with large/continuous $\\boldsymbol{S}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma B.4 (Feasible Set Explicit). In the setting of Definition 3.1, then the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ satisfies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{p,\\pi^{E}}=\\biggr\\{r\\in\\Re\\left|\\,\\forall h\\in[H],\\exists\\overline{{S}}\\subseteq\\mathcal{S}_{h}^{p,\\pi^{E}}:d_{h}^{p,\\pi^{E}}(\\overline{{S}})=0\\wedge\\forall s\\notin\\overline{{S}},\\forall a\\in\\mathcal{A}:}\\\\ &{\\qquad\\qquad\\qquad\\mathbb{E}_{\\overline{{h}}}\\not\\simeq}Q_{h}^{*}(s,a^{\\prime};p,r)\\geqslant Q_{h}^{*}(s,a;p,r)\\biggr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Simply, Lemma B.4 improves on Lemma B.3 by allowing the reward to enforce the \u201cwrong\u201d action (i.e., different from the expert\u2019s action) in a subset with zero measure based on the visitation distribution. ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof is completely analogous to that of Lemma E.1 in [31]. We just need to observe that if set $\\bar{s}$ has zero measure (and the set of rewards $\\Re$ contains bounded rewards), then it does not affect the expected return. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Another useful property that we need is that the $Q$ -function is always linear in the feature map for any policy in Linear MDPs. ", "page_idx": 18}, {"type": "text", "text": "Proposition B.5 (Proposition 2.3 in [23]). For a Linear MDP, for any policy $\\pi$ , there exist weights $\\{w_{h}^{\\pi}\\}_{h\\in[\\![H]\\!]}$ such that, for any $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ , we have $Q_{h}^{\\pi}(s,a)=\\langle\\phi(s,a),w_{h}^{\\pi}\\rangle$ . ", "page_idx": 18}, {"type": "text", "text": "We can combine the results of Lemma B.4 and Proposition B.5 to obtain the following characterization of the feasible set in Linear MDPs. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.6. In the setting of Definition 3.1, the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ satisfies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathcal{R}_{p,\\pi^{E}}=\\Big\\{r\\in\\!\\mathfrak{R}\\Big|\\,\\exists\\{w_{h}\\}_{h\\in[H]},\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]\\!\\}:\\:r_{h}(s,a)=\\langle\\phi(s,a),\\theta_{h}\\rangle}\\\\ {\\quad\\wedge\\forall h\\in[H],\\exists\\overline{{\\mathcal{S}}}\\subseteq{S_{h}^{p,\\pi^{E}}:\\,d_{h}^{p,\\pi^{E}}(\\overline{{\\mathcal{S}}})}=0\\,\\wedge\\forall s\\notin\\overline{{\\mathcal{S}}},\\forall a^{E}\\in{A_{h}^{E}(s)}:}\\\\ {\\langle\\phi(s,a^{E}),w_{h}\\rangle=\\displaystyle\\operatorname*{max}_{a\\in\\mathcal{A}}\\!\\langle\\phi(s,a),w_{h}\\rangle\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\theta_{h}\\;:=\\;w_{h}\\,-\\,\\textstyle\\int_{\\mathcal{S}}\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\langle\\phi(s^{\\prime},a^{\\prime}),w_{h+1}\\rangle d\\mu_{h}(s^{\\prime})$ for all $h\\,\\in\\,\\[H]$ , and $A_{h}^{E}(s)\\;:=\\;\\{a\\;\\in$ $A|\\pi_{h}^{E}(a|s)>0\\}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. From [45], we know that in any MDP there exists an optimal policy. Therefore, thanks to Proposition B.5, we know that the optimal $Q$ -function $Q^{*}$ is linear in the feature map too. So, there ", "page_idx": 18}, {"type": "text", "text": "exist parameters $\\{w_{h}\\}_{h}$ such that, for any $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ , the optimal $Q$ -function can be rewritten as $Q_{h}^{*}(s,a)=\\langle\\phi(s,a),w_{h}\\rangle$ . From the Bellman equa t ion,  we know that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Q_{h}^{*}(s,a;p,r)=r_{h}(s,a)+\\int V_{h+1}^{*}(s^{\\prime};p,r)d p_{h}(s^{\\prime}|s,a)\\quad}}\\\\ &{}&{\\qquad=\\langle\\phi(s,a),\\theta_{h}\\rangle+\\langle\\phi(s,a),\\displaystyle\\int\\operatorname*{max}_{s}\\langle\\phi(s^{\\prime},a^{\\prime}),w_{h+1}\\rangle d\\mu_{h}(s^{\\prime})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By rearranging this equation, and removing the dot product with $\\phi(s,a)$ , we obtain that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\theta_{h}=w_{h}-\\int_{S}\\operatorname*{max}_{a^{\\prime}\\in\\cal A}\\langle\\phi(s^{\\prime},a^{\\prime}),w_{h+1}\\rangle d\\mu_{h}(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, this holds in any Linear MDP. If we desire to enforce the constraints in Lemma B.4, we simply have to impose the constraint on the optimal $Q$ -function using parameters $\\{w_{h}\\}_{h}$ outside some $\\overline{{S}}$ . This concludes the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "It is useful to introduce the following definitions. First we define the set of parameters that induce a $Q$ -function compatible with $\\pi^{E}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}_{p,\\pi^{E}}:=\\Big\\{w:\\|H\\|\\rightarrow\\mathbb{R}^{d}\\,\\Big|\\,\\forall h\\in\\|H\\|,\\exists\\overline{{\\mathcal{S}}}\\subseteq{\\mathcal{S}_{h}^{p,\\pi^{E}}}:d_{h}^{p,\\pi^{E}}(\\overline{{\\mathcal{S}}})=0\\,\\land\\,\\forall s\\notin\\overline{{\\mathcal{S}}},\\forall a^{E}\\in\\mathcal{A}_{h}^{E}(s):}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\langle\\phi(s,a^{E}),w_{h}\\rangle=\\displaystyle\\operatorname*{max}_{a\\in\\mathcal{A}}\\langle\\phi(s,a),w_{h}\\rangle\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we define the set of parameters of the reward function obtained by using $Q$ -functions parametrized by $w\\in\\mathcal{W}_{p,\\pi^{E}}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Theta_{p,\\pi^{E}}:=\\Big\\{\\theta:[H]\\rightarrow\\mathbb{R}^{d}\\,\\Big|\\,\\exists\\{w_{h}\\}_{h}\\in\\mathcal{W}_{p,\\pi^{E}}:\\,\\theta_{h}=w_{h}-\\int_{S}\\operatorname*{max}_{a^{\\prime}\\in A}\\langle\\phi(s^{\\prime},a^{\\prime}),w_{h+1}\\rangle d\\mu_{h}(s^{\\prime})\\Big\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Irrespective of the transition model $\\{\\mu_{h}\\}_{h}$ and the feature map $\\phi$ , we see that it is always possible to construct a surjective map from $\\Theta_{p,\\pi^{E}}$ to $\\mathcal{W}_{p,\\pi^{E}}$ (the map in the definition of $\\Theta_{p,\\pi^{E}}$ ). Thanks to these definitions, the feasible set can be rewritten as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}_{p,\\pi^{E}}=\\{r\\in\\Re\\,|\\,\\exists\\{\\theta_{h}\\}_{h}\\in\\Theta_{p,\\pi^{E}},\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathbb{I}\\mathbb{H}\\}:r_{h}(s,a)=\\langle\\phi(s,a),\\theta_{h}\\rangle\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We are now ready to provide the proofs of the various results of this section. ", "page_idx": 19}, {"type": "text", "text": "B.4.1 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition 3.1. Let $\\mathcal{M}$ be a Linear MDP without reward with a finite state space, and let $\\phi$ be $a$ feature mapping. Let $\\{\\Phi_{h}^{\\pi}\\}_{h\\in[\\![H]\\!]}$ and $\\{{\\overline{{\\Phi}}}_{h}\\}_{h\\in\\mathbb{I}^{\\mathbb{H}}\\mathbb{I}}$ be the sets of expert\u2019s and non-expert\u2019s features, defined for every $h\\in\\left[U\\right]$ as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Phi_{h}^{\\pi^{E}}:=\\big\\{\\phi(s,a^{E})\\,|\\,s\\in\\mathcal{S}_{h}^{p,\\pi^{E}},\\,a^{E}\\in\\mathcal{A}_{h}^{E}(s)\\big\\},\\qquad\\overline{{\\Phi}}_{h}:=\\big\\{\\phi(s,a)\\,|\\,s\\in\\mathcal{S}_{h}^{p,\\pi^{E}},\\,a\\in\\mathcal{A}\\cup\\mathcal{A}_{h}^{E}(s)\\big\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $A_{h}^{E}(s):=\\{a\\in A|\\pi_{h}^{E}(\\cdot|s)>0\\}$ for every $s\\in S$ . If for none of the $H$ pairs of sets $(\\Phi_{h}^{\\pi^{E}},\\overline{{\\Phi}}_{h})$ there exists a separating hyperplane, then $\\mathcal{R}_{p,\\pi^{E}}=\\{\\overline{{r}}\\}$ , with $\\overline{{r}}_{h}(s,a)=0\\,\\forall(s,a,h)\\in S\\times\\mathring{A}\\times\\mathbb{I}\\!\\!\\!\\parallel$ i.e., the feasible set with linear rewards in $\\phi$ contains only the reward function that assigns zero reward everywhere. ", "page_idx": 19}, {"type": "text", "text": "Proof. From [8], we recall that two sets $\\mathcal{V}_{1},\\mathcal{V}_{2}$ are separated by a hyperplane $H=\\{x|a^{\\mathsf{T}}x=b\\}$ if each lies in a different closed halfspace associated with $H$ , i.e., if either: ", "page_idx": 19}, {"type": "equation", "text": "$$\na^{\\mathsf{T}}y_{1}\\leqslant b\\leqslant a^{\\mathsf{T}}y_{2},\\quad\\forall y_{1}\\in\\mathcal{y}_{1},\\forall y_{2}\\in\\mathcal{y}_{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "or: ", "page_idx": 19}, {"type": "equation", "text": "$$\na^{\\mathsf{T}}y_{2}\\leqslant b\\leqslant a^{\\mathsf{T}}y_{1},\\quad\\forall y_{1}\\in\\mathcal{y}_{1},\\forall y_{2}\\in\\mathcal{y}_{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By definition of $\\mathcal{W}_{p,\\pi^{E}}$ , for each stage $h\\,\\in\\,[H]$ , we are looking for vectors $w_{h}\\,\\in\\,\\mathbb{R}^{d}$ such that $\\forall(s,h)\\in S^{p,\\pi^{E}}$ , it holds that: ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{h}^{\\mathsf{T}}\\phi(s,a)\\leqslant w_{h}^{\\mathsf{T}}\\phi(s,a^{E})\\quad\\forall a^{E}\\in\\mathcal{A}_{h}^{E}(s),\\forall a\\in\\mathcal{A}\\backslash\\mathcal{A}_{h}^{E}(s).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In words, for each $(s,h)\\in S^{p,\\pi^{E}}$ , we are looking for non-affine separating hyperplanes between features of expert and non-expert actions. However, since the hyperplane parameter $w_{h}$ is common to all states $\\bar{s}\\,\\in\\,\\bar{S}_{h}^{p,\\pi^{E}}$ , then it must separate expert from non-expert actions at all states. This is equivalent to finding the separating hyperplanes to the sets $\\Phi_{h}^{\\pi^{E}}$ and $\\overline{{\\Phi}}_{h}$ which contain all the points. Clearly, when the separating hyperplanes do not exist at all $\\ddot{h}\\in[H]$ , then the condition in $\\mathcal{W}_{p,\\pi^{E}}$ is satisfied by the zero vector alone. As a consequence, set $\\Theta_{p,\\pi^{E}}$ c ont ains only the zero vector, and so does Rp,\u03c0E. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Remark B.2. By using the result of Lemma B.4, we can easily convert Proposition 3.1 into a more general result by considering the impossibility of separating any pair of sets constructed by varying at will some subsets with zero measure. We will not provide such result explicitly. ", "page_idx": 20}, {"type": "text", "text": "B.4.2 Proofs of Proposition 3.2 and Appendix B.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the PAC framework of Definition 3.2, we have not specified formally the inner distance $d$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nd(r,\\widehat{r}):=\\frac{1}{M_{r,\\widehat{r}}}\\operatorname*{sup}_{\\pi\\in\\Pi}\\sum_{h\\in\\left[H\\right]}\\underset{(s,a)\\sim d_{h}^{p,\\pi}(\\cdot,\\cdot)}{\\mathbb{E}}|r_{h}(s,a)-\\widehat{r}_{h}(s,a)|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where: ", "page_idx": 20}, {"type": "equation", "text": "$$\nM_{r,\\hat{r}}:=\\operatorname*{max}\\{\\sqrt{d},\\operatorname*{max}_{h\\in[\\![H]\\!]}\\|\\theta_{h}\\|_{2},\\operatorname*{max}_{h\\in[\\![H]\\!]}\\|\\widehat\\theta_{h}\\|_{2}\\}/\\sqrt{d},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\{\\theta_{h}\\}_{h}$ and $\\{{\\widehat{\\theta}}_{h}\\}_{h}$ are the (unbounded) parameters of rewards $r$ and $\\widehat{r}$ . As explained in [31], such normalizationp term allows us to work with unbounded reward functi opns. In practice, we are relaxing the Linear MDP assumption presented in Section 2 about the boundedness of the parameters $\\theta$ of the rewards to avoid the issue described in [38] and [31]. We still assume that the feature mapping is bounded. Observe that this relaxation does not affect the results we present, which would hold even if we considered bounded parameters $\\theta$ . Indeed, as visible in the proofs, the instances do not need to be constructed with unbounded $\\theta$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem 3.2 (Statistical Inefficiency). Let $\\mathcal{M}\\cup\\{\\pi^{E}\\}$ be a Linear IRL instance with finite state space $\\boldsymbol{S}$ and deterministic expert\u2019s policy, and let \u03f5 $\\therefore\\delta\\in(0,1)$ . If an algorithm $\\mathfrak{A}$ is $(\\epsilon,\\delta)$ -PAC, then $\\dot{\\boldsymbol{\\tau}}^{E}=\\Omega(\\boldsymbol{S})$ , where $S:=|S|$ is the cardinality of the state space. ", "page_idx": 20}, {"type": "text", "text": "Proof. We construct two problem instances that lie at a finite Hausdorff distance, and show that, with less than $S$ calls to the sampling oracle, we are not able to discriminate between the two instances. ", "page_idx": 20}, {"type": "text", "text": "Let $\\boldsymbol{S}$ be the finite state space with cardinality $S$ , ${\\cal A}\\,=\\,\\{a_{1},a_{2}\\},{\\cal H}\\,=\\,1,d_{0}(s)\\,=\\,1/S\\,\\,\\,\\forall s\\,\\,\\in\\,S,$ $\\phi(s,a)\\,=\\,\\mathbb{1}\\{a\\,=\\,a_{1}\\}$ $\\pi_{2}^{E}(s)\\underline{{\\underline{{\\,}}}}=\\alpha_{1}\\,\\forall s\\in\\mathcal{S}\\backslash\\{\\overline{{s}}\\}$ ,n ad ncdo $\\pi_{2}^{E}({\\overline{{s}}})\\,=\\,a_{2}$ , eftoerr ma icneisrttiaci ne $\\overline{{s}}\\in S$ .s  pTohlei csieets $\\pi_{1}^{E}(s)\\,=\\,a_{1}\\,\\breve{\\forall}s\\,\\in\\,{\\cal S}$ ,t iabnlde with $\\pi_{1}^{E}$ is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta_{p,\\pi_{1}^{E}}=\\{\\theta\\in\\mathbb{R}\\,|\\,\\theta\\geqslant0\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $|\\pi_{1}^{E}(s,a_{1})\\geqslant Q^{\\pi_{1}^{E}}(s,a_{2})_{\\infty}\\iff r(s,a_{1})\\geqslant r(s,a_{2})\\iff\\phi(s,a_{1})\\theta\\geqslant\\phi(s,a_{2})\\theta\\iff$ $1\\cdot\\theta\\geqslant0\\cdot\\theta$ . Observe that, for $\\pi_{2}^{E}$ , due to the presence of $\\overline{s}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta_{p,\\pi_{2}^{E}}=\\{\\theta\\in\\mathbb{R}\\,|\\,\\theta=0\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $\\overline{s}$ imposes $\\theta\\leqslant0$ , and the other states impose $\\theta\\geqslant0$ . ", "page_idx": 20}, {"type": "text", "text": "Therefore, the Hausdorff distance between the two problems is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\mathcal{R}_{\\pi_{1}^{E}},\\mathcal{R}_{\\pi_{2}^{E}})=\\operatorname*{sup}_{\\theta\\ge0}\\frac{1}{\\operatorname*{max}\\{1,\\theta,0\\}}\\theta=\\operatorname*{sup}_{\\theta\\ge0}\\frac{1}{\\operatorname*{max}\\{1,\\theta\\}}\\theta=1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Obviously, we need a $\\Omega(S)$ samples to spot, if it exists, state $\\overline{s}$ , and thus distinguish between $\\mathcal{R}_{\\pi_{1}^{E}}$ and R\u03c0E . ", "page_idx": 20}, {"type": "text", "text": "Proposition B.1. Under the setting of Proposition 3.2, even under Assumption B.1, then an algorithm is $(\\epsilon,\\delta)$ -PAC only if $\\cdot\\boldsymbol{\\tau}^{E}=\\Omega(\\boldsymbol{S})$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. The same proof of Proposition 3.2 works here. ", "page_idx": 20}, {"type": "text", "text": "In particular, we now show that Assumption B.1 does not help. The Hausdorff distance between the instances in the proof of Proposition 3.2 can be written as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{H}(\\mathcal{R}_{\\pi_{1}^{K}},\\mathcal{R}_{\\pi_{2}^{K}})=}&{\\underset{\\theta_{1}\\geq0}{\\operatorname*{sup}}\\,\\underset{\\theta_{2}=0}{\\operatorname*{inf}}\\,\\frac{1}{\\underset{\\theta_{1}\\geq0}{\\operatorname*{max}}\\,\\underset{\\theta_{2}\\geq0}{\\operatorname*{max}}\\,\\underset{\\theta_{2}\\geq0}{\\operatorname*{mp}}}\\underset{s\\in\\Pi^{s}}{\\operatorname*{sup}}\\,\\underset{\\theta^{\\infty}\\sim\\pi(\\cdot|s)}{\\mathbb{E}}|r_{1}(s,a)-r_{2}(s,a)|}\\\\ &{=\\underset{\\theta_{1}\\geq0}{\\operatorname*{sup}}\\,\\underset{\\theta_{2}=0}{\\operatorname*{inf}}\\,\\frac{1}{\\underset{\\theta_{1}\\geq1}{\\operatorname*{sup}}\\,\\underset{\\theta\\in\\Pi^{s}}{\\operatorname*{sup}}\\,\\underset{\\theta\\in\\pi(\\cdot|s)}{\\operatorname*{max}}\\,\\underset{\\theta\\in\\pi(\\cdot|s)}{\\mathbb{E}}|\\phi(s,a)\\theta_{1}-\\phi(s,a)\\theta_{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\pm\\,\\phi(s^{\\prime},a)\\theta_{1}\\pm\\phi(s^{\\prime},a)\\theta_{2}|}\\\\ &{\\leqslant\\underset{\\pi\\in\\Pi^{s}}{\\operatorname*{sup}}\\,\\underset{s\\in\\pi(\\cdot|s)}{\\operatorname*{max}}\\,\\underset{\\theta^{\\infty}\\sim\\pi(\\cdot|s)}{\\mathbb{E}}\\,\\big|\\phi(s,a)-\\phi(s^{\\prime},a)\\big|+0}\\\\ &{\\qquad\\quad+\\underset{\\pi\\in\\Pi^{s}}{\\operatorname*{sup}}\\,\\underset{\\theta\\geq0}{\\operatorname*{sup}}\\,\\frac{1}{\\underset{\\theta_{1}\\geq0}{\\operatorname*{max}}\\,\\underset{\\theta_{2}=0}{\\operatorname*{suf}}\\,\\underset{\\theta^{\\infty}\\sim\\pi(\\cdot|s)}{\\mathbb{E}}}\\,\\big|\\phi(s^{\\prime},a)\\theta_{1}-\\phi(s^{\\prime},a)\\theta_{2}\\big|}\\\\ &{=\\underset{\\pi\\in\\Pi^{s}}{\\operatorname*{sup}}\\,\\underset{s\\in\\pi(\\cdot|s)}{\\operatorname*{sup}}\\,\\frac{1}{\\underset{\\theta}{\\operatorname*{sup}}}\\,\\big|\\phi(s,a)-\\phi(s^{\\prime},a\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $s^{\\prime}$ is the state in the covering closest to state $s$ ; while the first term can be bounded, the assumption does not help us with the second term. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proposition B.2. Under the setting of Proposition 3.2, under Assumption B.2, then a number of samples $\\tau^{E}\\;=\\;|\\mathcal{N}(\\frac{\\Delta}{2L};S,\\|\\cdot\\|)|$ is sufficient to recover $\\pi^{E}$ exactly in any $(s,h)\\,\\in\\,{\\mathcal{S}}$ , where $|\\mathcal{N}(\\frac{\\Delta}{2L};\\mathcal{S},\\|\\cdot\\|)|$ is the $\\Delta/(2L)$ -covering number of space $\\boldsymbol{S}$ w.r.t. distance $\\|\\cdot\\|$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. For any state $s\\in S^{p,\\pi^{E}}$ , by definition of covering $\\mathcal{N}(\\frac{\\Delta}{2L};\\mathcal{S},\\Vert\\cdot\\Vert)$ , there always exist another state $s^{\\prime}\\in\\mathcal{N}(\\frac{\\Delta}{2L};S,\\|\\cdot\\|)$ such that $\\begin{array}{r}{\\|s^{\\prime}-s\\|\\leqslant\\frac{\\Delta}{2L}}\\end{array}$ . By Assumption B.2 we know that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\phi(s,\\pi_{h}^{E}(s))-\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))\\|_{2}\\leqslant L\\|s^{\\prime}-s\\|\\leqslant\\frac{\\Delta}{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and since $\\pi_{h}^{E}(s^{\\prime})$ and thus $\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))$ is known, then the fact that $\\Delta$ is finite guarantees us that $\\pi_{h}^{E}(s)$ is equal to the action $a$ that minimizes the distance to $\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))$ . Notice that if, by contradiction, there were two actions $a_{1},a_{2}$ with $\\begin{array}{r}{\\|\\phi(s,a_{1})-\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))\\|_{2}\\leqslant\\frac{\\Delta}{2}}\\end{array}$ and $\\|\\phi(s,a_{2})-$ $\\begin{array}{r}{\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))\\|_{2}\\leqslant\\frac{\\Delta}{2}}\\end{array}$ , then by triangle inequality and finiteness of $\\Delta$ , we would have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta<\\|\\phi(s,a_{1})-\\phi(s,a_{2})\\|_{2}}\\\\ &{\\quad\\leqslant\\|\\phi(s,a_{1})-\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))\\|_{2}+\\|\\phi(s,a_{2})-\\phi(s^{\\prime},\\pi_{h}^{E}(s^{\\prime}))\\|_{2}}\\\\ &{\\quad\\leqslant\\displaystyle\\frac{\\Delta}{2}+\\frac{\\Delta}{2}=\\Delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is clearly a contradiction. ", "page_idx": 21}, {"type": "text", "text": "B.4.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The proof is based on deriving an upper bound to the Hausdorff distance between the true feasible set and its estimate. To do so, first, using the notation of [23], let us define the following quantities: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{h}(\\cdot|s,a):=\\langle\\phi(s,a),\\mu_{h}(\\cdot)\\rangle,}\\\\ &{\\widehat{\\mathbb{P}}_{h}(\\cdot|s,a):=\\phi(s,a)^{\\top}\\Lambda_{h}^{-1}\\displaystyle\\sum_{k=1}^{\\tau}\\phi(s_{h}^{k},a_{h}^{k})\\delta(\\cdot,s_{h+1}^{k}),}\\\\ &{\\overline{{\\mathbb{P}}}_{h}(\\cdot|s,a):=\\phi(s,a)^{\\top}\\Lambda_{h}^{-1}\\displaystyle\\sum_{k=1}^{\\tau}\\phi(s_{h}^{k},a_{h}^{k})\\mathbb{P}_{h}(\\cdot|s_{h}^{k},a_{h}^{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\delta(\\cdot,x)$ is the Dirac measure, and $(s_{h}^{k},a_{h}^{k})$ represents the state-action pair visited at stage $h$ of exploration episode $k\\in\\mathbb{[}\\tau]$ . In words, $\\mathbb{P}$ denotes the true transition model,P denotes the least squares estimate computed by Algorithm 1, and $\\overline{{\\mathbb{P}}}$ represents a bridge between  thpe two. As we will see, the core of the proof consists in upper bounding the term $\\big|\\left(\\mathbb{P}_{h}-\\widehat{\\mathbb{P}}\\right)V_{h+1}(s,a)\\big|$ at all $h\\in\\left[U\\right]$ and reachable $(s,a)\\in S\\times A$ , for all the bounded linear functions $V$ in class $\\nu$ , defined as: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{V}:=\\Big\\{V:\\mathcal{S}\\times\\mathbb{I}\\!H\\mathbb{I}\\to[-H,+H]\\,\\Big|\\,V(\\cdot)=\\operatorname*{max}_{a\\in\\mathcal{A}}\\phi(\\cdot,a)^{\\top}w,\\ \\|w\\|_{2}\\leqslant2H\\sqrt{d}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To achieve this goal, it will be useful to apply triangle inequality and to bound the following two terms separately: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\big(\\mathbb{P}_{h}-\\widehat{\\mathbb{P}}\\big)V_{h+1}(s,a)\\Big|\\leqslant\\Big|\\big(\\mathbb{P}_{h}-\\overline{{\\mathbb{P}}}\\big)V_{h+1}(s,a)\\Big|+\\Big|\\big(\\overline{{\\mathbb{P}}}_{h}-\\widehat{\\mathbb{P}}\\big)V_{h+1}(s,a)\\Big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma B.7 and Lemma B.8, which we now present, serve exactly this purpose. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.7. For any value function $V$ in the class $\\nu$ , for any $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathbb{I}\\mathbb{H}]$ , it holds that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Big|\\Big(\\overline{{\\mathbb{P}}}_{h}-\\mathbb{P}_{h}\\Big)V_{h+1}(s,a)\\Big|\\leqslant\\operatorname*{min}\\Big\\{H\\sqrt{d}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},2H\\Big\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\overline{{y}}_{k}-\\overline{{y}}_{k}\\right)\\overline{{y}}_{k+1}(x,u)=\\phi(s,q)T_{k-\\frac{1}{2}}\\sum_{q=0}^{k}\\phi(s,\\phi_{k}^{()},q)T_{k-1}(s,\\phi_{k}^{()},q)-\\overline{{y}}_{k}V_{k+1}(s,u)}\\\\ &{\\qquad\\qquad\\leq\\phi(s,q)T_{k-\\frac{1}{2}}\\sum_{\\substack{\\phi\\in\\mathcal{X}_{k}\\to\\mathcal{G}_{q}}}\\phi(s,q)T_{k}\\overline{{y}}_{k+1}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad=\\phi(s,q)T_{k-\\frac{1}{2}}\\sum_{\\substack{\\phi\\in\\mathcal{X}_{k}\\to\\mathcal{G}_{q}}}\\phi(s,q)T_{k-1}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad=\\phi(s,q)T_{k-\\frac{1}{2}}\\sum_{\\substack{\\phi\\in\\mathcal{X}_{k}\\to\\mathcal{G}_{q}}}\\phi(s,q)T_{k-1}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad=\\phi(s,q)T_{k-\\frac{1}{2}}\\sum_{\\substack{\\phi\\in\\mathcal{X}_{k}\\to\\mathcal{G}_{q}}}\\phi(s,q)T_{k}V_{k+1}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad\\geq\\phi(s,q)T_{k-\\frac{1}{2}}\\sum_{\\substack{\\phi\\in\\mathcal{X}_{k}\\to\\mathcal{G}_{q}}}\\phi(s,q)T_{k-1}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad\\quad-T_{k-\\frac{1}{2}}\\sum_{\\substack{\\phi\\in\\mathcal{X}_{k}\\to\\mathcal{G}_{q}}}\\phi(s,q)T_{k}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad\\geq\\phi(s,q)T_{k-\\frac{1}{2}}\\sum_{\\substack{\\phi\\in\\mathcal{X}_{k}\\to\\mathcal{G}_{q}}}\\phi(s,q)T_{k}V_{k+1}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\overline{{y}}_{k}(s,q)T_{k-1}(s,\\phi_{k}^{()},q)}\\\\ &{\\qquad\\qquad-\\overline{{y \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where at (1) we have defined vector $\\widetilde{w}_{h}:=\\int_{S}V_{h+1}(s^{\\prime})d\\mu_{h}(s^{\\prime})$ , at (2) we have used the definition of $\\Lambda_{h}$ , and at (3) we have recognized t hrat $\\phi(s_{h}^{k^{-}},a_{h}^{k})^{\\top}\\widetilde w_{h}=\\mathbb{P}_{h}V_{h+1}(s_{h}^{k},a_{h}^{k})$ . ", "page_idx": 22}, {"type": "text", "text": "By taking the absolute value, we can write: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\overline{{\\mathbb{P}}}_{h}-\\mathbb{P}_{h}\\right)V_{h+1}(s,a)\\Big|=\\left|\\phi(s,a)^{\\mathsf{T}}\\Lambda_{h}^{-1}\\widetilde{w}_{h}\\right|}&{}\\\\ {\\overset{(4)}{\\leqslant}\\left\\|\\widetilde{w}_{h}\\right\\|_{\\Lambda_{h}^{-1}}\\!\\left\\|\\phi(s,a)\\right\\|_{\\Lambda_{h}^{-1}}}&{}\\\\ {\\overset{(5)}{\\leqslant}\\left\\|\\widetilde{w}_{h}\\right\\|_{2}\\!\\left\\|\\phi(s,a)\\right\\|_{\\Lambda_{h}^{-1}}}&{}\\\\ {\\overset{(6)}{\\leqslant}H\\sqrt{d}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where at (4) we have applied Cauchy-Schwarz\u2019s inequality, at (5) we have bounded the quadratic form with the 2-norm and the largest eigenvector of the matrix, i.e., $\\|\\widetilde{w}_{h}\\|_{\\Lambda_{h}^{-1}}=\\sqrt{\\widetilde{w}_{h}^{\\intercal}\\Lambda_{h}^{-1}\\widetilde{w}_{h}}\\leqslant\\sqrt{\\sigma}\\|\\widetilde{w}_{h}\\|_{2}$ , where $\\sigma$ is the largest eigenvalue of matrix $\\Lambda_{h}^{-1}$ , and then we  hrave upper borunded $\\sigma\\leqslant1$ , sinc er 1 is the smallest eigenvalue of invertible matrix $\\Lambda_{h}$ (see [23]); finally, at (6) we have used the fact that $|V_{h+1}(\\cdot)|\\leqslant H$ , and so that $\\|\\widetilde{w}_{h}\\|_{2}=\\|\\int_{S}V_{h+1}(s^{\\prime})d\\mu_{h}(s^{\\prime})\\|_{2}\\leqslant H\\|\\mu_{h}(S)\\|_{2}\\leqslant H\\sqrt{d}$ . ", "page_idx": 22}, {"type": "text", "text": "The result follows by noticing that the quantity to bound cannot be larger than $2H$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma B.8. Let $\\delta\\in(0,1)$ . For any value function $V$ in the class $\\nu$ , for any $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]$ , with probability at least $1-\\delta/2$ , it holds that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\left(\\widehat{\\mathbb{P}}_{h}-\\overline{{\\mathbb{P}}}_{h}\\right)V_{h+1}(s,a)\\right|\\leqslant\\operatorname*{min}\\bigg\\{c H\\sqrt{d\\log\\left(1+\\tau\\right)+\\log\\frac{H}{\\delta}}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},2H\\bigg\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some constant $c$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We can write: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\biggl(\\widehat{\\mathbb{P}}_{h}-\\overline{{\\mathbb{P}}}_{h}\\biggr)V_{h+1}(s,a)\\biggr|=\\biggl|\\phi(s,a)^{\\top}\\Lambda_{h}^{-1}\\biggr\\rvert\\sum_{k=1}^{\\infty}\\phi(s_{h}^{\\top},a_{h}^{k})\\biggr[V_{h+1}(s_{h+1}^{k})-\\mathbb{P}_{h}V_{h+1}(s_{h}^{k},a_{h}^{k})\\biggr]\\biggr|}\\\\ &{\\qquad\\qquad\\stackrel{(1)}{\\leq}\\biggl|\\sum_{k=1}^{\\infty}\\phi(s_{h}^{\\top},a_{h}^{k})\\biggr[V_{h+1}(s_{h+1}^{k})-\\mathbb{P}_{h}V_{h+1}(s_{h}^{k},a_{h}^{k})\\biggr]\\biggr|_{{\\boldsymbol A}^{n}}\\biggr|\\phi(s,a)\\biggr|_{{\\boldsymbol A}_{h}^{-1}}}\\\\ &{\\qquad\\stackrel{(2)}{\\leq}\\sqrt{4H^{2}\\biggl(\\frac{d}{2}\\log(1+\\tau)+\\log\\frac{2N_{\\epsilon}}{\\delta}\\biggr)+8\\tau^{2}\\varepsilon^{2}}\\biggr|\\phi(s,a)\\biggr|_{{\\boldsymbol A}_{h}^{-1}}}\\\\ &{\\qquad\\stackrel{(9)}{\\leq}\\sqrt{4H^{2}\\biggl(\\frac{d}{2}\\log(1+\\tau)+2d\\log\\Big(1+\\frac{H\\sqrt{d}}{\\epsilon}\\Big)+\\log\\frac{1}{\\delta}\\Big)+8\\tau^{2}\\varepsilon^{2}}\\biggr|\\phi(s,a)\\biggr|_{{\\boldsymbol A}_{h}^{-1}}}\\\\ &{\\qquad\\stackrel{(4)}{\\leq}\\sqrt{4H^{2}\\biggl(\\frac{d}{2}\\log(1+\\tau)+2d\\log\\Big(1+4\\tau\\Big)+\\log\\frac{1}{\\delta}\\Big)+8H^{2}d}\\biggr|\\phi(s,a)\\biggr|_{{\\boldsymbol A}_{h}^{-1}}}\\\\ &{\\qquad\\stackrel{(5)}{\\leq}c H\\sqrt{d\\log(1+\\tau)+\\log\\frac{1}{\\delta}}\\biggr|\\phi(s,a)\\biggr|_{{\\boldsymbol A}_{h}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where at (1) we have applied Cauchy-Schwarz\u2019s inequality, at (2) we have applied Lemma B.13, at (3) we have upper bounded $\\mathcal{N}_{\\epsilon}$ using Lemma B.12, at (4), similarly to [62], unlike [23], we see that no union bound is needed (because there is no dependence on $\\Lambda$ ), thus by choosing $\\epsilon=H\\sqrt{d}/\\tau$ , we get the passage. Passage (5) follows for some constant $c$ . ", "page_idx": 23}, {"type": "text", "text": "The result follows by a union bound over $h\\in\\left[U\\right]$ , and by noticing that the quantity to bound cannot be larger than $2H$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "We are now ready to upper bound the Hausdorff distance using the two lemmas just presented. Recall that we work with unbounded rewards (parameters $\\theta$ ), and that the definition of inner distance $d$ is provided in Equation (1). ", "page_idx": 23}, {"type": "text", "text": "Lemma B.9. With probability at least $1-\\delta/2$ , the Hausdorff distance between the true feasible set $\\mathcal{R}_{p,\\pi^{E}}$ and its estimate $\\hat{\\mathcal{R}}$ returned by Algorithm $^{\\,I}$ can be upper bounded by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}(\\mathcal{R}_{p,\\pi^{E}},\\hat{\\mathcal{R}})\\leqslant4J^{*}(u;p),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $u_{h}(s,a)\\;\\;:=\\;\\;\\operatorname*{min}\\{\\beta\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},H\\}$ pfor all $(s,a,h)\\ \\in\\ S\\ \\times\\ A\\ \\times\\ \\mathbb{I}\\!\\!H].$ , and $\\beta\\ \\ :=$ $c H{\\sqrt{d\\log(1+\\tau)+\\log(H/\\delta)}}.$ for some absolute constant $c>0$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let us begin to bound the first branch of the Hausdorff distance. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r\\in\\mathcal{R}_{p,r}\\in\\mathcal{E}}{\\operatorname*{sup}}\\quad\\operatorname*{inf}\\,d(r,\\hat{r})=\\underset{r\\in\\mathcal{R}_{p,r}\\in\\mathcal{E}}{\\operatorname*{sup}}\\,\\frac{1}{r\\in\\hat{\\mathcal{R}}}\\frac{1}{M_{r}\\hat{r}}\\underset{\\pi\\in\\Pi_{h}\\in[H]}{\\operatorname*{sup}}\\underset{(s,a)\\sim d_{h}^{p,\\pi}(\\cdot,\\cdot)}{\\mathbb{E}}|r_{h}(s,a)-\\widehat{r}_{h}(s,a)|}\\\\ &{\\overset{(1)}{=}\\underset{r\\in\\mathcal{R}_{p,r}\\in\\mathcal{E}}{\\operatorname*{sup}}\\,\\frac{\\operatorname*{inf}}{M_{r}\\hat{r}}\\frac{1}{\\pi\\in\\Pi_{h}}\\underset{h\\in[H]}{\\sum}(s,a)\\underset{-d_{h}^{p,\\pi}(\\cdot,\\cdot)}{\\mathbb{E}}|Q_{h}^{*}(s,a;p,r)-\\mathbb{P}_{h}V_{h+1}^{*}(s,a;p,r)}\\\\ &{\\qquad-Q_{h}^{*}(s,a;\\hat{p},\\hat{r})+\\widehat{\\mathbb{P}}_{h}V_{h+1}^{*}(s,a;\\hat{p},\\hat{r})|}\\\\ &{\\overset{(2)}{\\leqslant}\\underset{r\\in\\mathcal{R}_{p,r}\\not\\in\\mathcal{E}}{\\operatorname*{sup}}\\,\\frac{1}{M_{r}\\tilde{r}}\\underset{\\pi\\in\\Pi_{h}\\in[H]}{\\operatorname*{sup}}\\underset{(s,a)\\sim d_{h}^{p,\\pi}(\\cdot,\\cdot)}{\\mathbb{E}}|Q_{h}^{*}(s,a;p,r)-\\mathbb{P}_{h}V_{h+1}^{*}(s,a;p,r)}\\\\ &{\\qquad-Q_{h}^{*}(s,a;\\hat{p},\\hat{r})+\\widehat{\\mathbb{P}}_{h}V_{h+1}^{*}(s,a;\\hat{p},\\hat{r})|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial t}\\displaystyle\\operatorname*{sup}_{r\\in\\mathbb{R}_{n}^{+},\\varepsilon^{\\prime}}\\frac{1}{M_{\\Gamma_{\\delta}^{\\prime}}\\tau\\in\\Pi_{h}}\\sum_{h\\in\\mathbb{Z}_{n}^{+},\\varepsilon^{\\prime}\\in\\mathcal{T}_{n^{+}}^{*}(\\varepsilon)}\\mathbb{E}_{h}^{*}(s,a;p,r)-\\mathbb{P}_{h}V_{h+1}^{*}(s,a;p,r)}\\\\ &{\\qquad-Q_{h}^{*}(s,a;p,r)+\\widehat{\\mathbb{P}}_{h}V_{h+1}^{*}(s,a;p,r)\\Big|}\\\\ &{=\\displaystyle\\operatorname*{sup}_{r\\in\\mathbb{R}_{n}^{+},\\varepsilon^{\\prime}\\in\\Pi_{h}}\\sum_{h\\in\\mathbb{Z}_{n}^{+},\\varepsilon^{\\prime}\\in\\mathcal{T}_{n^{+}}^{*}(\\varepsilon)}\\mathbb{E}_{h}^{*}(\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h})V_{h+1}^{*}(s,a;p,r)\\Big|}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\operatorname*{sup}_{r\\in\\mathbb{R}_{n}^{+},\\varepsilon^{\\prime}\\in\\mathcal{T}_{n}^{+}(\\varepsilon)}\\sum_{h\\in\\mathbb{Z}_{n}^{+},\\varepsilon^{\\prime}\\in\\mathcal{T}_{n}^{+}(\\varepsilon)}\\Big|\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h}\\Big)\\displaystyle\\frac{V_{h+1}^{*}(s,a;p,r)}{\\operatorname*{max}\\{1,\\operatorname*{max}_{h}\\|\\theta_{h}\\|_{2},\\varepsilon^{\\prime}\\in\\mathcal{T}_{n}^{+}\\}}\\Big|}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\operatorname*{sup}_{r\\in\\mathbb{R}_{n}^{+},\\varepsilon^{\\prime}\\in\\mathcal{T}_{n}^{+}(\\varepsilon)}\\sum_{h\\in\\mathbb{Z}_{n}^{+},\\varepsilon^{\\prime}\\in\\mathcal{T}_{n^{+}}^{*}(\\varepsilon)}\\Big|\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h}\\Big)V_{h+1}^{*}(s,a;p,\\frac{r}{K})\\Big|}\\\\ &{\\overset{(c)}{\\leq}\\displaystyle\\operatorname*{sup}_{s\\in\\mathbb{R}_{n}^{+},\\varepsilon^{\\prime}\\in\\Pi_{h\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where at (1) we have simply applied the Bellman optimality equation twice w.r.t. the reward function, at (2) we have upper bounded the infimum over the second set of rewards $\\hat{\\mathcal{R}}$ with the specific choice of rewardr P R provided by Lemma B.11, at (3) we use the prope rtpy of $\\widetilde{r}$ described in Lemma B.11,  art (4 ) pwe bring term $1/M_{r,\\tilde{r}}$ inside, and then we upper bound  itr by: $1/M_{r,\\tilde{r}}:=1/\\operatorname*{max}\\{\\sqrt{d},\\operatorname*{max}_{h}\\|\\theta_{h}\\|_{2},\\operatorname*{max}_{h}\\|\\widetilde{\\theta}_{h}\\|_{2}\\}/\\sqrt{d}\\leqslant1/\\operatorname*{max}\\{1,\\operatorname*{max}_{h}\\|\\theta_{h}\\|_{2}/\\sqrt{d}\\}$ , i.e., by simply rermoving one of the terms inside the mraximum operator at denominator. At (5) we define $K:=\\operatorname*{max}\\{1,\\operatorname*{max}_{h}\\|{\\theta_{h}}\\|_{2}/{\\sqrt{d}}\\}$ , and, since the value function is linear in the reward, we apply $K$ directly to the reward. At (6) we realize that the possible optimal value functions that can be constructed in $p$ using rewards in $\\mathcal{R}_{p,\\pi^{E}}$ normalized by $K$ are a subset of the value functions in class $\\nu$ , i.e., of all the possible optimal value functions with parameters $\\|w_{h}\\|_{2}\\leqslant2H{\\sqrt{d}}$ . This is not trivial since we are working with unbounded rewards $r$ , and thus their parameters $\\{\\theta_{h}\\}_{h}$ can be any. The normalization by $K$ permits this in the following manner. For any $h\\in\\left[U\\right]$ , we have $r_{h}(\\cdot,\\cdot)/K=$ $\\langle\\phi(\\cdot,\\cdot),\\theta_{h}/K\\rangle=\\langle\\phi(\\cdot,\\cdot),\\theta_{h}/\\operatorname*{max}\\{1,\\operatorname*{max}_{h^{\\prime}}\\|\\theta_{h^{\\prime}}\\|_{2}/\\sqrt{d}\\}\\rangle$ . Therefore, if $\\operatorname*{max}_{h^{\\prime}}\\|\\theta_{h^{\\prime}}\\|_{2}>\\sqrt{d}$ , then the normalization makes sure that $\\mathrm{max}h^{\\prime}$ $\\|\\theta_{h^{\\prime}}\\|_{2}=\\sqrt{d}$ , while if $\\operatorname*{max}_{h^{\\prime}}\\|\\theta_{h^{\\prime}}\\|_{2}\\leqslant\\sqrt{d}$ , then the normalization is by 1 and it has no effect. In this way, we see that value functions $\\begin{array}{r}{V_{h+1}^{*}(s,a;p,\\frac{r}{K})}\\end{array}$ can be created by a simple $r^{\\prime}$ with parameters $\\{\\theta_{h}^{\\prime}\\}_{h}$ with 2-norms bounded by $\\sqrt{d}$ . This guarantees that, since by hypothesis of Linear MDPs $\\|\\hat{\\phi}(\\cdot,\\cdot)\\|_{2}\\,\\leqslant\\,1$ , the value function never exceeds $H$ , and that the norm of the $Q$ -function parameters $\\{w_{h}^{\\pi}\\}_{h}$ for any policy $\\pi$ can be bounded as: $\\begin{array}{r}{\\|w_{h}^{\\pi}\\|_{2}\\leqslant\\|\\theta_{h}/K\\|_{2}+\\|\\int_{S}V_{h+1}^{\\pi}(s^{\\prime})d\\mu_{h}(s^{\\prime})\\|_{2}\\leqslant{\\sqrt{d}}+H\\|\\mu_{h}(S)\\|_{2}\\leqslant{\\sqrt{d}}+H{\\sqrt{d}}\\leqslant2H{\\sqrt{d}}}\\end{array}$ (similarly to Lemma B.1 of [23]). It should be remarked that class $\\nu$ is more general than the actual set of optimal value functions that can be obtained using $r\\in\\mathcal{R}_{p,\\pi^{E}}$ in $p$ , since such rewards induce optimal value functions for which the optimal action in $S^{p,\\pi^{E}}$ is always the expert\u2019s action/s $\\pi^{E}(s)$ . ", "page_idx": 24}, {"type": "text", "text": "Notice that the same derivation can be carried out also for the other branch of the Hausdorff distance, ending up with the same expression. Therefore, the last line is an upper bound to the Hausdorff distance: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}_{d}(\\mathcal{R}_{p,\\pi^{E}},\\widehat{\\mathcal{R}})\\leqslant\\underset{V\\in\\mathcal{V}}{\\operatorname*{sup}}\\underset{t\\in[H]}{\\operatorname*{sup}}\\sum_{i=1}^{r}(\\underset{h\\in[I]}{\\sum_{h=1}^{E}}\\underset{(\\cdot)}{\\boxed{(\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h})V_{h+1}(s,a)}}|\\left(\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h}\\right)V_{h+1}(s,a)|}\\\\ &{\\overset{(7)}{=}\\underset{\\pi\\in\\Pi_{h\\in[H]}}{\\operatorname*{sup}}\\underset{(s,a)\\sim d_{h}^{\\mathbb{C},\\pi^{E}}(\\cdot,\\cdot)}{\\sum_{k}}\\underset{V\\in\\mathcal{V}}{\\operatorname*{sup}}\\big[(\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h})V_{h+1}(s,a)\\big]}\\\\ &{=\\underset{\\pi\\in\\Pi_{h\\in[H]}}{\\operatorname*{sup}}\\underset{(s,a)\\sim d_{h}^{\\mathbb{C},\\pi^{E}}(\\cdot,\\cdot)}{\\sum_{k}}\\underset{V\\in\\mathcal{V}}{\\operatorname*{sup}}\\big[(\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h})V_{h+1}(s,a)\\pm\\overline{{\\mathbb{P}}}_{h}V_{h+1}(s,a)|}\\\\ &{\\overset{(5)}{\\lesssim}\\underset{\\pi\\in\\Pi_{h\\in[H]}}{\\operatorname*{sup}}\\sum_{i=1}^{r}\\underset{s,a)\\sim(\\cdot)}{\\underbrace{\\operatorname*{sup}}}\\underset{V\\in\\mathcal{V}}{\\operatorname*{sup}}\\big[(\\widehat{\\mathbb{P}}_{h}-\\mathbb{P}_{h})V_{h+1}(s,a)\\big]+\\big|(\\widehat{\\mathbb{P}}_{h}-\\overline{{\\mathbb{P}}}_{h})V_{h+1}(s,a)\\big|}\\\\ &{\\overset{(7)}{\\lesssim}\\underset{\\pi\\in\\Pi_{h\\in[H]}}{\\operatorname*{sup}}\\big(\\textnormal{s},a)\\underset{\\times d_{h}^{\\mathbb{C},\\pi^{E}}(\\cdot,\\cdot)}{\\sum_{k}}\\operatorname*{min}\\left\\lbrace c_{1}H\\sqrt{d\\log(1+\\tau)+\\log\\frac{H}{{\\delta}}}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},4H\\right\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leqslant4\\operatorname*{sup}_{n\\in\\Pi}\\displaystyle\\sum_{h\\in[H]}\\displaystyle(s,a)\\!\\!\\sim\\!\\!d_{h}^{p,\\pi}(.,.)\\operatorname*{min}\\Big\\{\\underbrace{c_{2}H\\sqrt{d\\log(1+\\tau)+\\log\\frac{H}{\\delta}}}_{=:\\beta}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},H\\Big\\}}\\\\ &{=4\\operatorname*{sup}_{\\pi\\in\\Pi}\\displaystyle\\sum_{h\\in[H]}\\Big(s,a)\\!\\!\\sim\\!\\!d_{h}^{p,\\pi}(.,.)\\underbrace{\\operatorname*{min}\\big\\{\\beta\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},H\\big\\}}_{=:u_{h}(s,a)}}\\\\ &{=4\\operatorname*{sup}_{\\pi\\in\\Pi}\\displaystyle\\sum_{h\\in[H]}\\sum_{(s,a)\\sim d_{h}^{p,\\pi}(.,.)}u_{h}(s,a)}\\\\ &{=4J^{\\ast}(u;p),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where at (7) we have noticed that class $\\nu$ contains the cartesian product of $H$ sets, one for each stage, and therefore the supremum can be brought inside the summation, at (8) we have applied triangle inequality, at (9) we have applied Lemma B.7 and Lemma B.8 and used some absolute constants $c_{1},c_{2}\\,>\\,0$ , and also the fact that for any numbers $x,y,w,z$ , we have $\\operatorname*{min}\\{x,y\\}+\\operatorname*{min}\\{w,z\\}\\leqslant$ $\\operatorname*{min}\\{x+w,y+z\\}$ . ", "page_idx": 25}, {"type": "text", "text": "To conclude the proof of the main theorem, we simply have to observe that any RFE algorithm provides a bound to $\\boldsymbol{J}^{*}(\\boldsymbol{u}^{\\prime};\\boldsymbol{p})$ for some $u^{\\prime}$ similar to $u$ . Depending on the RFE algorithm instantiated as sub-routine, the sample complexity of Algorithm 1 varies. ", "page_idx": 25}, {"type": "text", "text": "Theorem 3.3. Assume that $\\pi^{E}$ (along with its support $S^{p,\\pi^{E}},$ ) is known. Then, for any $\\epsilon,\\delta\\in(0,1)$ , Algorithm $^{\\,l}$ is $(\\epsilon,\\delta)$ -PAC for IRL with a number of episodes $\\tau$ upper bounded by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{5}d}{\\epsilon^{2}}\\Big(d+\\log\\frac{1}{\\delta}\\Big)\\Big).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. To get the result, we instantiate Algorithm 1 of [62] as RFE sub-routine. Simply, observe that [62] sets $\\beta^{\\prime}$ so that $\\beta^{\\prime}\\geqslant\\widetilde{\\beta}:=c^{\\prime}H\\sqrt{d\\log(1+d H\\tau)+\\log(H/\\delta)}\\geqslant\\beta$ . By Lemma B.9, we know that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}(\\mathcal{R}_{p,\\pi^{E}},\\widehat{\\mathcal{R}})\\leqslant4\\operatorname*{sup}_{\\pi\\in\\Pi}\\displaystyle\\sum_{h\\in[\\![H]\\!]}{}_{(s,a)\\sim d_{h}^{p,\\pi}(\\cdot,\\cdot)}\\operatorname*{min}\\big\\{\\beta\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},H\\big\\}}\\\\ &{\\qquad\\qquad\\leqslant2c_{1}\\beta^{\\prime}\\displaystyle\\sum_{h\\in[\\![H]\\!]}\\operatorname*{sup}_{\\pi\\in\\Pi\\,(s,a)\\sim d_{h}^{p,\\pi}(\\cdot,\\cdot)}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some absolute constant $c_{1}>0$ . It should be remarked that the quantity in the last line is, modulo $c_{1}$ , the quantity that [62] bound in the proof of their Theorem 1 using their algorithm. Specifically, by taking: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau\\leqslant\\tilde{\\mathcal{O}}\\bigg(\\frac{H^{5}d}{\\epsilon^{2}}\\Big(d+\\log\\frac{1}{\\delta}\\Big)+\\frac{H^{6}d^{9/2}}{\\epsilon}\\log^{4}\\frac{1}{\\delta}\\bigg),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and a union bound over the two events that hold w.p. $1-\\delta/2$ , and re-setting $\\epsilon\\gets c_{1}\\epsilon$ , we get the result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Notice that if we run Algorithm 1 of [63] for exploration instead of Algorithm 1 of [62], we obtain: ", "page_idx": 25}, {"type": "text", "text": "Theorem B.10. If we use Algorithm 1 of [63] at Line $^{\\,l}$ of Algorithm $^{\\,l}$ , then for any $\\epsilon,\\delta\\in(0,1)$ , such algorithm is $(\\epsilon,\\delta)$ -PAC for IRL with a number of episodes $\\tau$ upper bounded by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau\\leqslant\\tilde{\\mathcal{O}}\\bigg(\\frac{H^{6}d^{3}}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\bigg).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. By Lemma B.9, we know that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}(\\mathcal{R}_{p,\\pi^{E}},\\widehat{\\mathcal{R}})\\leqslant4J^{*}(u;p)}\\\\ &{\\qquad\\qquad\\qquad=4\\operatorname*{sup}_{\\pi\\in\\Pi}\\displaystyle\\sum_{h\\in[H]}\\underset{(s,a)\\sim d_{h}^{p,\\pi}(\\cdot,\\cdot)}{\\mathbb{E}}\\operatorname*{min}\\big\\{\\beta\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},H\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for $\\beta\\;:=\\;c H\\sqrt{d\\log(1+\\tau)+\\log(H/\\delta)}$ . Now, let us define, similarly to Appendix A of [63], the quantities $\\dot{u}_{h}^{\\prime}(s,a)\\,:=\\,\\operatorname*{min}\\{\\beta^{\\prime}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},H\\}$ for all $(s,a,h)\\,\\in\\,\\dot{\\cal S}\\,\\times\\,\\dot{\\cal A}\\,\\dot{\\times}\\,\\|H\\|$ , and $\\beta^{\\prime}:=$ $c^{\\prime}d H\\sqrt{\\log(d H/\\delta/\\epsilon)}$ for some absolute constant $c^{\\prime}>0$ . In addition, set the number of exploration episodes $\\tau$ to $\\tau\\,=\\,c^{\\prime\\prime}d^{3}H^{6}\\log(d H\\delta^{-1}\\epsilon^{-1})/\\epsilon^{2}$ , and notice that, for appropriate choices of $c^{\\prime},c^{\\prime\\prime}$ , it holds that: $\\begin{array}{r}{\\beta^{\\prime}\\,\\geqslant\\,c^{\\prime}d H\\sqrt{\\log(d H\\tau/\\delta)}\\,\\geqslant\\,\\beta\\,:=\\,c H\\sqrt{d\\log(1+\\tau)+\\log(1/\\delta)}}\\end{array}$ . This entails that $u_{h}^{\\prime}(s,a)\\geqslant u_{h}(s,a)$ at all $s,a,h$ , and so: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}(\\mathcal{R}_{p,\\pi^{E}},\\hat{\\mathcal{R}})\\leqslant c J^{*}(u^{\\prime};p)}\\\\ &{\\qquad\\qquad\\qquad\\quad=c H J^{*}(u^{\\prime}/H;p)}\\\\ &{\\qquad\\qquad\\quad\\overset{(1)}{\\leqslant}c_{1}H\\sqrt{\\frac{d^{3}H^{4}\\log\\frac{d\\tau H}{\\delta}}{\\tau}}}\\\\ &{\\qquad\\qquad\\quad\\overset{(2)}{\\leqslant}c_{2}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where at (1) we have applied Lemma 3.2 of [63] (reported in Lemma B.14 for simplicity) with some new constant $c_{1}>0$ , and at (2) we have simply replaced $\\tau$ with its value defined in Algorithm 1 of [63]. ", "page_idx": 26}, {"type": "text", "text": "The result follows by union bound between the two events that hold w.p. $1-\\delta/2$ to get $1-\\delta$ , and by noticing that $c_{2}$ is a constant, thus setting $\\epsilon\\gets c_{2}\\epsilon$ provides the result. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma B.11. Let $\\mathcal{R}_{p,\\pi^{E}}$ be the feasible set of policy $\\pi^{E}\\ w.r.t.$ transition models $p_{i}$ , and letR be its estimate constructed as in Algorithm $^{\\,l}$ using the true $\\pi^{E},S^{p,\\pi^{E}}$ (or sets $\\mathcal{Z}$ ) and some ${\\widehat{p}}.$ .  Fpor any reward $r\\in\\mathcal{R}_{p,\\pi^{E}}$ , the reward $\\widehat{r}$ such that, for all $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{r}_{h}(s,a)=r_{h}(s,a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}p_{h}(s^{\\prime}|s,a)V_{h+1}^{*}(s^{\\prime};p,r)-\\sum_{s^{\\prime}\\in\\mathcal{S}}\\widehat{p}_{h}(s^{\\prime}|s,a)V_{h+1}^{*}(s^{\\prime};\\widehat{p},\\widehat{r}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "belongs to $\\hat{\\mathcal{R}}$ . Moreover, observe that: $Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\\hat{p},\\hat{r})$ at all $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ . In addition , pfor any reward ${\\widehat{r}}\\in{\\widehat{\\mathcal{R}}},$ , it is possible to construct a re wpa rpd $r$ in analogous manner so that $r\\in\\mathcal{R}_{p,\\pi^{E}}$ , and such that $Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\\widehat{p},\\widehat{r})$ at all $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. First, we consider the case when $\\boldsymbol{S}$ is finite. By rearranging the terms in the definition of $\\Hat{r}$ , we see that, for all $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{r}_{h}(s,a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}\\widehat{p}_{h}(s^{\\prime}|s,a)V_{h+1}^{*}(s^{\\prime};\\widehat{p},\\widehat{r})=r_{h}(s,a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}p_{h}(s^{\\prime}|s,a)V_{h+1}^{*}(s^{\\prime};p,r),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which, by the Bellman optimality equation, entails that $Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\\hat{p},\\hat{r})$ . ", "page_idx": 26}, {"type": "text", "text": "We recall that $\\hat{\\mathcal{R}}$ is defined as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{R}}=\\big\\{\\widehat{r}\\in\\Re\\Big|\\,\\forall(s,h)\\in S^{p,\\pi^{E}},\\forall a\\in\\mathcal{A}:\\quad\\mathbb{E}_{\\mathbf{\\Phi}_{a^{\\prime}\\sim\\pi_{h}^{E}(\\cdot\\vert s)}}Q_{h}^{*}(s,a^{\\prime};\\widehat{p},\\widehat{r})\\geqslant Q_{h}^{*}(s,a;\\widehat{p},\\widehat{r})\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "while thanks to Lemma B.4, the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ can be written as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{p,\\pi^{E}}=\\big\\{r\\in\\mathfrak{R}\\,\\Big|\\,\\forall(s,h)\\in\\mathcal{S}^{p,\\pi^{E}},\\forall a\\in\\mathcal{A}:\\,\\underset{a^{\\prime}\\sim\\pi_{h}^{E}(\\cdot\\vert s)}{\\mathbb{E}}\\,Q_{h}^{*}(s,a^{\\prime};p,r)\\geqslant Q_{h}^{*}(s,a;p,r)\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It is clear that, if $Q_{h}^{*}(s,a;p,r)=Q_{h}^{*}(s,a;\\hat{p},\\hat{r})$ for all $(s,a,h)\\in S\\times A\\times\\mathbb{I}\\mathbb{I}\\mathbb{I}$ , then since $r\\in\\mathcal{R}_{p,\\pi^{E}}$ we necessarily have $\\widehat{r}\\in\\widehat{\\mathcal{R}}$ . ", "page_idx": 26}, {"type": "text", "text": "The proof of the opp posit e case is completely analogous. ", "page_idx": 26}, {"type": "text", "text": "In the case with infinite $\\boldsymbol{S}$ , notice that both the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ in Lemma B.4 and the definition of $\\hat{\\mathcal{R}}$ in Algorithm 1 make use of the same sets $\\mathcal{Z}$ . Thus, we simply make the choice of reward with spame $\\mathcal{Z}$ and proceed like in the finite case. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma B.12 (Covering Number of Class $\\nu$ ). Let $\\mathcal{V}$ be defined as in Equation (2), and define distance dist in $\\nu$ as $d i s t(V,V^{\\prime}):=\\operatorname*{sup}_{s\\in S}|V(s)-V^{\\prime}(s)|$ . Then, the $\\epsilon$ -covering number $|\\mathcal{N}(\\dot{\\epsilon};\\mathcal{V},d i s t)|$ of ", "page_idx": 26}, {"type": "text", "text": "set $\\mathcal{V}$ with distance dist can be bounded as: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log|{\\mathcal{N}}(\\epsilon;{\\mathcal{V}},d i s t)|\\leqslant d\\log\\Big(1+\\frac{4H\\sqrt{d}}{\\epsilon}\\Big).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof follows that of Lemma D.6 of [23], but is simpler because of the different form of $\\nu$ . ", "page_idx": 27}, {"type": "text", "text": "For any $V_{1},V_{2}\\in\\mathcal{V}$ parametrized by $w_{1},w_{2}$ , we write: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{dist}(V_{1},V_{2})=\\displaystyle\\operatorname*{sup}_{s\\in\\mathcal{S}}\\Big|\\operatorname*{max}_{a\\in\\mathcal{A}}\\langle\\phi(s,a),w_{1}\\rangle-\\operatorname*{max}_{a\\in\\mathcal{A}}\\langle\\phi(s,a),w_{2}\\rangle\\Big|}\\\\ {\\displaystyle\\overset{(1)}{\\leqslant}\\operatorname*{max}_{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\Big|\\phi(s,a)^{\\mathstrut}(w_{1}-w_{2})\\Big|}\\\\ {\\displaystyle\\overset{(2)}{\\leqslant}\\operatorname*{sup}_{\\phi\\in\\mathcal{S}}\\Big|\\phi^{\\mathstrut\\mathsf{T}}(w_{1}-w_{2})\\Big|}\\\\ {\\displaystyle\\overset{(3)}{=}\\lVert w_{1}-w_{2}\\rVert_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where at (1) we have used the common bound that the absolute difference of maxima is upper bounded by the maximum of the absolute difference of the two functions, at (2) we have used the fact that the feature map is always bounded by 1 in 2-norm, and at (3) we have recognized the dual norm of the 2-norm, i.e., itself. ", "page_idx": 27}, {"type": "text", "text": "If we construct an $\\epsilon$ -cover of $\\mathcal{W}:=\\{w\\in\\mathbb{R}^{d}\\,|\\,\\|w\\|_{2}\\leqslant2H\\sqrt{d}\\}$ w.r.t. the 2-norm, we get a covering number bounded by $|N(\\epsilon;\\mathcal{W},\\|\\cdot\\|_{2})|\\,\\leqslant\\,(1+4H\\sqrt{d}/\\epsilon)^{d}$ . Clearly, this value upper bounds the covering number of class $\\nu$ and the result follows. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma B.13 (Lemma D.4 of [23]). Let $\\{s_{k}\\}_{k=1}^{\\infty}$ be a stochastic process on state space $\\boldsymbol{S}$ with corresponding flitration $\\{\\mathcal{F}_{k}\\}_{k=0}^{\\infty}.\\_L e t\\,\\{\\phi_{k}\\}_{k=0}^{\\infty}$ be an $\\mathbb{R}^{d}$ -valued stochastic process where $\\phi_{k}\\in\\mathcal{F}_{k-1}$ , and $\\|\\phi_{k}\\|_{2}\\leqslant1$ . Let $\\begin{array}{r}{\\Lambda_{\\tau}=I+\\sum_{k=1}^{\\tau}\\phi_{k}\\phi_{k}^{\\intercal}}\\end{array}$ . Then, for any $\\delta>0$ , with probability at least $1-\\delta$ , for all $\\tau\\geqslant0$ , and any $V\\in\\mathcal{V}$ so th at $\\operatorname*{sup}_{s\\in{\\cal S}}|\\dot{V}(s)|\\leqslant H$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Big\\lVert\\sum_{k=1}^{\\tau}\\phi_{k}\\Big(V(s_{k})-\\mathbb{E}\\left[V(s_{k})|\\mathcal{F}_{k-1}\\right]\\Big)\\Big\\rVert_{\\Lambda_{\\tau}^{-1}}\\leqslant4H^{2}\\Big[\\frac{d}{2}\\log(1+\\tau)+\\log\\frac{\\mathcal{N}_{\\epsilon}}{\\delta}\\Big]+8\\tau^{2}\\epsilon^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathcal{N}_{\\epsilon}$ is the $\\epsilon$ -covering number of $\\nu$ with respect to the distance dis $\\begin{array}{r}{t(V,V^{\\prime}):=\\operatorname*{sup}_{s\\in S}|V(s)-}\\end{array}$ $V^{\\prime}(s)|$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma B.14 (Lemma 3.2 of [63]). With probability $1\\!-\\!\\delta/2,$ , for the function $u^{\\prime}$ defined as $u_{h}^{\\prime}(s,a):=$ $\\operatorname*{min}\\left\\{\\beta^{\\prime}\\|\\phi(s,a)\\|_{\\Lambda_{h}^{-1}},H\\right\\}$ (, with $\\beta^{\\prime}:=c^{\\prime}d H\\sqrt{\\log(d H\\delta^{-1}\\epsilon^{-1})}.$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\nJ^{*}(u^{\\prime}/H)\\leqslant c\\sqrt{\\frac{d^{3}H^{4}\\log\\frac{d\\tau H}{\\delta}}{\\tau}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some absolute constant $c>0$ . ", "page_idx": 27}, {"type": "text", "text": "C Additional Insights on Compatibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this appendix, we collect and describe additional insights to the notion of rewards compatibility introduced in Section 4. The appendix is organized in the following manner: Appendix C.1 provides a visual explanation to the notion of rewards compatibility, in Appendix C.2 we analyse a multiplicative alternative to the definition of rewards compatibility, and Appendix C.3 discusses the conditions under which a learned reward can be used for \u201cforward\u201d RL, by comparing rewards with small (non)compatibility with rewards learned in previous works. ", "page_idx": 27}, {"type": "text", "text": "C.1 A Visual Explanation for Rewards Compatibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this appendix, we aim to provide a visual intuition to the notion of rewards compatibility. For this reason, the reader should keep in mind Figure 3. ", "page_idx": 27}, {"type": "image", "img_path": "ZjgcYMkCmX/tmp/b4eda6acdebb2832d029b101a2db46090154bc1bcb60bc85f9f152e20a1465c2.jpg", "img_caption": ["Figure 3: In this figure, the point at the center represents the initial state $s_{0}=d_{0}$ of the environment $\\mathcal{M}$ , and each ray starting from it represents the occupancy measure $d^{p,\\pi}$ of some policy $\\pi$ . The figure aims to provide the intuition that policies with rays close to each other induce similar visit distributions (e.g., both point towards the same direction in some grid-world), and policies with rays far away from each other point toward very different directions (i.e., they have different occupancy measures). The red area in the right denotes the set of directions (occupancy measures $d^{p,\\pi}$ for some \u03c0) that are close in } \u00a8 }1 norm to the direction of the expert dp,\u03c0E. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "As explained in Section 4, even in the limit of infinite samples, i.e., even if we know $\\mathcal{M}\\;=$ $(S,\\bar{A,H},d_{0},p)\\cup\\{\\pi^{E}\\}$ exactly, and even if we assume that the expert is exactly optimal, i.e., $J^{*}(r^{E};p)-J^{\\pi^{E}}(r^{E};p)=0$ (where $r^{E}$ is the true reward optimized by the expert), then we still do not have idea of how other policies perform. Expert demonstrations only provide information about the performance of a single policy, $\\dot{\\pi}^{E}$ , w.r.t. to the reference $J^{\\ast}(r^{E};p)$ under the unknown $r^{E}$ , i.e., demonstrations say that $\\pi^{E}$ in $r^{\\dot{E}}$ performs as good as $J^{*}(r^{E};\\stackrel{.}{p})$ . But what about other policies? Demonstrations provide no information. ", "page_idx": 28}, {"type": "text", "text": "To see this, consider Figure 3, in which each line exemplifies the visitation distribution induced by some policy $\\pi\\in\\Pi$ , and the point in the middle represents the starting state $s_{0}\\,=\\,d_{0}$ . Intuitively, observing $d^{p,\\pi^{E}}$ along with knowing that $J^{\\pi^{E}}(r^{E};p)$ is good (i.e., because of expert demonstrations), does not tell us anything about the distribution $d^{p,\\overline{{\\pi}}}$ induced by some other policy $\\overline{{\\pi}}$ potentially arbitrarily different from $\\bar{d}^{p,\\pi^{E}}$ . Indeed, it might be the case that $J^{\\overline{{\\pi}}}(r^{E};p)$ is acceptable, or that it is as good as $J^{\\pi^{E}}(r^{E};p)$ , or that it is very bad. We cannot know from demonstrations only. ", "page_idx": 28}, {"type": "text", "text": "For this reason, if we consider the set of rewards with 0-(non)compatibility, i.e., the feasible reward set, we notice that it contains the rewards $r$ that make $\\overline{{\\pi}}$ optimal $\\bar{J}^{\\overline{{\\pi}}}(r;p)=J^{*}(r;p)$ , but also the rewards $r^{\\prime}$ that make $\\overline{{\\pi}}$ nearly optimal $J^{\\overline{{\\pi}}}(r^{\\prime};p)\\,\\approx\\,J^{*}(r^{\\bar{\\prime}};p)$ , and also the rewards $r^{\\prime\\prime}$ that make $\\overline{{\\pi}}$ a very bad-performing policy $J^{\\overline{{\\pi}}}(r^{\\prime\\prime};p)\\,\\ll\\,J^{*}(r^{\\prime\\prime};p)$ . Indeed, as long as both $r,r^{\\prime},r^{\\prime\\prime}$ make the direction pointed by $d^{p,\\pi^{E}}$ in Figure 3 a good direction, then they are in accordance with the constraint imposed by the demonstrations. The additional Degrees of Freedom (DoF) provided by policies beyond $\\pi^{E^{\\dagger}}$ (e.g., $\\overline{{\\pi}},\\ldots)$ permit the ill-posedness of IRL. ", "page_idx": 28}, {"type": "text", "text": "We said that expert demonstrations provide information just about the performance of a single policy, $\\pi^{E}$ . However, to be precise, in the context of IRL, this is not correct. Indeed, differently from the mere learning from demonstrations setting, in which we just assume that $\\pi^{E}$ is a very good-performing policy, in IRL we assume that the underlying problem is an MDP, i.e., that the expert agent is optimizing a reward function $r^{E}$ .10 This additional structure (i.e., that the underlying environment is indeed an MDP), makes sure that the performances of various directions $d^{p,\\pi}$ in Figure 3 are measured through a dot product with a fixed reward function $r$ , i.e.: ", "page_idx": 28}, {"type": "equation", "text": "$$\nJ^{\\pi}(r;p)=\\sum_{h\\in[\\![H]\\!]}\\langle d_{h}^{p,\\pi},r_{h}\\rangle.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For this reason, we have the guarantee that the directions in the red area surrounding $d^{p,\\pi^{E}}$ are almost as good as $d^{p,\\pi^{E}}$ . Indeed, for all policies $\\pi$ such that $\\begin{array}{r}{\\sum_{h\\in[H]}\\|d_{h}^{p,\\pi}-d_{h}^{p,\\pi^{E}}\\|_{1}\\leqslant\\epsilon}\\end{array}$ , i.e., for all policies ", "page_idx": 28}, {"type": "text", "text": "$\\epsilon$ -close to $\\pi^{E}$ in 1-norm, we can write: ", "page_idx": 29}, {"type": "equation", "text": "$$\n|J^{\\pi^{E}}(r^{E};p)-J^{\\pi}(r^{E};p)|=\\Big|\\sum_{h\\in[H]}\\langle d_{h}^{p,\\pi^{E}}-d_{h}^{p,\\pi},r_{h}^{E}\\rangle\\Big|\\leqslant\\sum_{h\\in[H]}\\|d_{h}^{p,\\pi}-d_{h}^{p,\\pi^{E}}\\|_{1}\\leqslant\\epsilon.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In other words, policies $\\pi$ and $\\pi^{E}$ have similar performances. ", "page_idx": 29}, {"type": "text", "text": "However, it should be remarked that, since we aim to recover the rewards explaining the expert\u2019s preferences, then we are guaranteed that policies close in 1-norm perform similarly under any reward function (by definition of 1-norm), and so we do not risk to incur in the error of representing $d^{p,\\pi^{E}}$ and a direction $d^{p,\\pi}$ inside the red area of Figure 3 with very different performances. ", "page_idx": 29}, {"type": "text", "text": "C.2 A Multiplicative Compatibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In Section 4, we have defined an additive notion of (non)compatibility, based on the difference of performance between $\\pi^{E}$ and $\\pi^{*}$ (the optimal policy). Here, we analyze a multiplicative notion of (non)compatibility, based on the ratio of the performances.11 ", "page_idx": 29}, {"type": "text", "text": "We make the following observation. Any reward $r\\in\\mathfrak{R}$ induces, in the considered environment $p$ , an ordering in the space of policies $\\Pi$ , based on the performance $J^{\\pi}(r;p)$ of each policy $\\pi\\in\\Pi$ . It is easy to notice that for any scaling and translation parameters $\\alpha\\in\\mathbb{R}_{>0},\\beta\\in\\mathbb{R}$ , the reward constructed as $\\dot{r}^{\\prime}(\\cdot,\\cdot)=\\alpha r(\\cdot,\\cdot)+\\beta$ induces the same ordering as $r$ in the space of policies.12 ", "page_idx": 29}, {"type": "text", "text": "For this reason, it seems desirable to use a notion of (non)compatibility such that rewards $r$ and $r^{\\prime}(\\cdot,\\cdot)\\,=\\,\\alpha r(\\cdot,\\cdot)\\,+\\,\\beta$ for some $\\alpha,\\beta$ , suffer from the same (non)compatibility w.r.t. some expert policy $\\pi^{E}$ . However, observe that, for the notion of compatibility $\\overline{{\\mathcal{C}}}$ in Definition 4.1, we have that, for any $r\\in\\mathfrak{R}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\overline{{\\mathscr{C}}}_{p,\\pi^{E}}(r+\\beta)=\\overline{{\\mathscr{C}}}_{p,\\pi^{E}}(r)\\qquad\\forall\\beta\\in\\mathbb{R},}&\\\\ &{\\overline{{\\mathscr{C}}}_{p,\\pi^{E}}(\\alpha r)=\\alpha\\overline{{\\mathscr{C}}}_{p,\\pi^{E}}(r)\\neq\\overline{{\\mathscr{C}}}_{p,\\pi^{E}}(r)\\qquad\\forall\\alpha\\in\\mathbb{R}_{>0}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Simply put, for the additive notion of (non)compatibility $\\overline{{\\mathcal{C}}}$ , the scale $(\\alpha)$ of a reward matters, and rescaling the reward modifies the (non)compatibility. ", "page_idx": 29}, {"type": "text", "text": "To solve this issue, one might introduce a multiplicative notion of compatibility $\\mathcal{F}$ (defined only for non-negative rewards and setting $\\mathcal{F}_{p,\\pi^{E}}(r)=0$ when the denominator is $0$ ): ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{F}_{p,\\pi^{E}}(r):=\\frac{J^{\\pi^{E}}(r;p)}{J^{*}(r;p)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Clearly, the larger ${\\mathcal{F}}_{p,\\pi^{E}}(r)$ , the closer is the performance of $\\pi^{E}$ to the optimal performance. Observe that, for this definition,we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{F}_{p,\\pi^{E}}(\\alpha r)=\\mathcal{F}_{p,\\pi^{E}}(r)}&{\\forall\\alpha\\in\\mathbb{R}_{>0}}\\\\ &{\\mathcal{F}_{p,\\pi^{E}}(r+\\beta)\\neq\\mathcal{F}_{p,\\pi^{E}}(r)}&{\\forall\\beta\\in\\mathbb{R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "i.e., this definition does not care about the scaling $\\alpha$ of the reward, but it is sensitive to the actual position $\\beta$ of that reward. ", "page_idx": 29}, {"type": "text", "text": "Therefore, both $\\overline{{\\mathcal{C}}}$ and $\\mathcal{F}$ suffer from some \u201crescaling\u201d issues. Is it possible to devise a notion of compatibility, i.e., a measure of suboptimality, for a policy, that is independent of both the scale $\\alpha$ and position $\\beta?$ Formally, we are looking for a function (notion of distance) $f:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}_{\\geq0}$ such that, for any $J_{1},J_{2}\\in\\mathbb{R}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(\\alpha J_{1}+\\beta,\\alpha J_{2}+\\beta)=f(J_{1},J_{2}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $\\alpha\\in\\mathbb{R}_{>0},\\beta\\in\\mathbb{R}$ . Unfortunately, this is not possible, since it is easy to show that all the functions $f$ of this kind are of the following type: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall J_{1},J_{2}\\in\\mathbb{R}\\times\\mathbb{R}:\\;f(J_{1},J_{2})=\\left\\{{K_{+}\\atop K_{0}}\\right.\\quad{\\mathrm{~if~}}J_{1}>J_{2}}\\\\ {\\left.{K_{-}\\atop K_{-}\\quad}\\right.\\quad{\\mathrm{~if~}}J_{1}<J_{2}}\\end{array}\\right.\\;,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "11E.g., see Theorem 7.2.7 in [45], which is inspired by [43].   \n12Indeed, simply observe that, for any $\\pi\\in\\Pi$ : $J^{\\pi}(r^{\\prime};p)=J^{\\pi}(\\alpha r+\\beta;p)=\\alpha J^{\\pi}(r;p)+\\beta$ . ", "page_idx": 29}, {"type": "text", "text": "for some reals $K_{+},K_{0},K_{-}$ . In words, any function $f$ that satisfies Equation (3) is able to express just an ordering between inputs $J_{1}$ and $J_{2}$ , but not an actual measure of sub-optimality/compatibility. ", "page_idx": 30}, {"type": "text", "text": "We conclude by stating that we prefer to use $\\overline{{\\mathcal{C}}}$ instead of $\\mathcal{F}$ for the following reasons: ", "page_idx": 30}, {"type": "text", "text": "\u2022 First, most RL literature prefers the additive notion of suboptimality towards the multiplicative one. \u2022 The additive notion of suboptimality is simpler to analyze w.r.t. the multiplicative one. ", "page_idx": 30}, {"type": "text", "text": "C.3 When can a learned reward be used for \u201cforward\u201d RL? ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this appendix, we exploit the intuition developed in Appendix C.1 to discuss under which conditions we can exploit demonstrations alone to recover a single reward that can be used for \u201cforward\u201d $R L$ , i.e., to recover a single reward $r$ for which we have the guarantee that any $\\epsilon$ -optimal policy $\\pi$ to $r$ in the true environment $p$ has similar performance in the same environment $p$ under the true reward $r^{E}$ , that is, policy $\\pi$ is an $f(\\epsilon)$ -optimal policy to $r^{E}$ in $p$ , for some function $f$ . ", "page_idx": 30}, {"type": "text", "text": "Applications of IRL range from Apprenticeship Learning (AL), to reward design, to interpretability of expert\u2019s preferences. Concerning AL, it is common to \u201cuse\u201d the reward $r$ learned through IRL to optimize our learning agent. But what properties $r$ should satisfy in order to obtain performance guarantees on our learning agent w.r.t. the true (unknown) $r^{E\\,}?$ We now list and analyze various plausible requirements. ", "page_idx": 30}, {"type": "text", "text": "\u2022 First, we might ask that, being $\\pi^{E}$ optimal w.r.t. $r^{E}$ , then $\\pi^{E}\\in\\arg\\operatorname*{max}_{\\pi}J^{\\pi}(r)$ , i.e., that the expert policy $\\pi^{E}$ is optimal under the learned reward $r$ . However, this requirement is not satisfactory for the following reason. Reward $r$ might induce more than one optimal policy (e.g., it might induce both $\\overline{{\\pi}}$ , $\\pi^{E}$ as optimal), and optimal policies other than $\\pi^{E}$ (e.g., $\\overline{{\\pi}}$ ) are not guaranteed to perform well under $r^{E}$ (actually, $\\overline{{\\pi}}$ can be any policy in $\\Pi$ ). Clearly, this is not satisfactory. Observe that there are rewards in the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ for which multiple policies are optimal (thus, not all the rewards in the feasible set are satisfactory).   \n\u2022 We might additionally ask that $\\pi^{E}$ is the unique optimal policy of reward $r$ (similarly to what happens in entropy-regularized MDPs [70, 15]). However, this is not satisfactory for the following reason. In practice, it is really difficult (almost impossible) to compute the optimal policy of a given reward. Thus, what is usually done in RL, is to settle for an $\\epsilon$ -optimal policy. Since any policy can be $\\epsilon_{}$ -optimal under reward $r$ , then no guarantee we can have for such policy w.r.t. $r^{E}$ .   \n\u2022 What if we ask that $\\pi^{E}$ is at least $\\epsilon$ -optimal under $r$ (i.e., the requirement provided by $\\epsilon$ -(non)compatible rewards)? Well, this is not satisfactory because optimal policies can be any, and because there might be other $\\epsilon$ -optimal policies that can perform arbitrarily bad under $r^{E}$ . ", "page_idx": 30}, {"type": "text", "text": "All the three requirements described above on $r$ do not provide guarantees that optimizing the considered reward $r$ provides a policy with satisfactory performance w.r.t. the true $r^{E}$ . However, as mentioned in Section 4 and in Appendix C.1, expert demonstrations do not provide any information about the performance of policies other than $\\pi^{\\bar{E}}$ under $r^{E}$ . ", "page_idx": 30}, {"type": "text", "text": "Remark C.1. If we want to be sure that an $\\epsilon$ -optimal policy $\\pi$ for the learned reward $r$ in $p$ is $i f$ $f(\\epsilon)$ -optimal for $r^{E}$ in $p$ (for some function $f_{.}$ ), then, clearly, we need that all the (at least) $\\epsilon$ -optimal policies under the learned $r$ have visitation distribution close to that of $\\pi^{E}$ in 1-norm (see Appendix C.1). ", "page_idx": 30}, {"type": "text", "text": "We stress that many IRL algorithms for AL, like max-margin [2], learn a reward function just as a mere mathematical tool to compute a policy $\\pi$ which is close in 1-norm $\\lVert d^{\\pi}-d^{\\pi^{E}}\\rVert_{1}$ to $\\pi^{E}$ . ", "page_idx": 30}, {"type": "text", "text": "A remark about works on the feasible set. If we look at recent works about the feasible set [38, 31, 68], it might seem that these works are able to provide guarantees between $r,r^{E}$ under distance $d^{a l l}$ (see Section 3.1 of [68]), defined as: ", "page_idx": 30}, {"type": "equation", "text": "$$\nd^{a l l}(r,r^{E}):=\\operatorname*{sup}_{\\pi\\in\\Pi}|J^{\\pi}(r)-J^{\\pi}(r^{E})|.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If $d^{a l l}(r,r^{E})$ is small, then the performance of any policy in $r$ , not just optimal policy or $\\epsilon$ -optimal policy, is similar also under $r^{\\ddagger}$ . In other words, if we use/optimize reward $r$ , then we have the guarantee that the performance of the retrieved policy under $r^{E}$ is more or less the same as its performance in $r$ . Therefore, clearly, rewards $r$ with small distance to $r^{E}\\ w.r.t.\\ d^{a l l}$ can be used for \u201cforward\u201d $R L$ . However, we have the following result: ", "page_idx": 31}, {"type": "text", "text": "Proposition C.1. Let $\\mathcal{M}\\,=\\,(S,\\mathcal{A},H,d_{0},p)$ be a known MDP without reward, and let $\\pi^{E}$ be $a$ known expert\u2019s policy. Let $r^{E}$ the true unknown reward optimized by the expert to construct $\\pi^{E}$ . Then, there does not exist a learning algorithm that receives in input the pair $(\\dot{\\mathcal{M}},\\pi^{E})$ and outputs $a$ single reward $r$ such that $d^{a l l}(r,r^{E})\\leqslant\\epsilon\\,w.p.\\,\\,1-\\delta$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. The proof is trivial. Indeed, since the feasible set $\\mathcal{R}_{p,\\pi^{E}}$ contains an infinite amount of reward functions along with $r^{E}$ , and the learning algorithm cannot discriminate $r^{E}$ inside $\\mathcal{R}_{p,\\pi^{E}}$ , then the best it can do is to output an arbitrary reward function $r\\in\\mathcal{R}_{p,\\pi^{E}}$ . However, since $\\mathcal{R}_{p,\\pi^{E}}$ contains, for any reward $r\\in\\mathcal{R}_{p,\\pi^{E}}$ , at least another reward $r^{\\prime}\\in\\mathcal{R}_{p,\\pi^{E}}$ such that $d^{a l l}(r,r^{\\prime})=c$ is finite and equal to some positive constant $c>0$ ,13 then we can simply construct the problem instance with $r^{E}:=r^{\\prime}$ to make the learning algorithm not able to output rewards that can be used for forward learning. ", "page_idx": 31}, {"type": "text", "text": "Nevertheless, [38, 31, 68] seem to provide sample efficient algorithms w.r.t. $d^{a l l}$ .14 By looking at Proposition C.1, we realize that this is clearly a contradiction. What is the right interpretation? ", "page_idx": 31}, {"type": "text", "text": "The trick is that the algorithms proposed in works [68, 38, 31] are not able to output a single reward $r$ which is close to $r^{E}$ w.r.t. $d^{a l l}$ , but, for any possible reward $\\bar{r}^{E}=r^{E}(V,A)$ parametrized $l^{15}$ by some value and advantage functions $V,A,$ , they are able to output a reward $r$ such that $d^{a l l}(r,r^{E}(V,A))$ is small. In other words, it is like if these works assume to know the $V,A$ parametrization of the true reward $r^{E}$ . Simply put, these works are able to output a reward $r$ that can be used for \u201cforward\u201d RL just under such assumption. Otherwise those algorithms do not provide such guarantee. ", "page_idx": 31}, {"type": "text", "text": "Conclusions. To sum up, we conclude that, in general, an arbitrary reward function with small (non)compatibility can not be used for \u201cforward\u201d learning (see Proposition C.1), because we cannot know given demonstrations alone whether the performances assigned by such reward to policies other than the expert policy are meaningful. In addition, for the same reason, we realize that also an arbitrary reward with zero (non)compatibility, i.e., an arbitrary reward in the feasible set, can not be used for \u201cforward\u201d learning. ", "page_idx": 31}, {"type": "text", "text": "C.4 Comparing the (non)compatibility of various rewards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Section 4, we said that rewards $r$ with smaller values of $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)$ are more compatible with $\\pi^{E}$ in $\\mathcal{M}=(S,\\mathcal{A},H,d_{0},p)$ . However, one might provide the following \u201ccounter-example\u201d: ", "page_idx": 31}, {"type": "text", "text": "Example C.1 (Question by Reviewer KyLX). Let $r^{1},r^{2}$ be two rewards such that $r_{h}^{2}(s,a)\\;=\\;$ $2r_{h}^{1}(s,a)\\geqslant0$ for all $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ . Then, clearly, $\\overline{{{\\mathcal{C}}}}_{p,\\pi^{E}}(r^{2})=2\\overline{{{\\mathcal{C}}}}_{p,\\pi^{E}}(r^{1})$ . Therefore, based on Section 4, we say that reward $r^{1}$ is more compatible than $r^{2}\\,_{W.}r t.\\ \\pi^{E}$ in $\\mathcal{M}$ . However, since $r^{2}$ is just $r^{1}$ re-scaled by a constant, the two MDPs $\\hat{\\mathcal{M}}\\cup\\{r^{1}\\}$ and $\\mathcal{M}\\cup\\{r^{2}\\}$ should be \u201cequivalent\u201d, thus, $r^{1}$ and $r^{2}$ should be, intuitively, equally compatible with $\\pi^{E}$ . ", "page_idx": 31}, {"type": "text", "text": "However, Example C.1 misleads the correct interpretation of the notion of reward function in MDPs, and in particular about the scale of the rewards. Let us explain better our point. ", "page_idx": 31}, {"type": "text", "text": "The MDP is a model, i.e., a simplified representation of reality, which is commonly applied to 2 different kinds of real-world scenarios: $(i)$ problems in which the agent (learner in RL or expert in IRL) actually receives some kind of scalar feedback from the environment, which can be modelled as a reward function; $(i i)$ problems in which the agent does not receive a feedback from the environment, but its objective, i.e., its structure of preferences among state-action trajectories (which trajectories are better than others), satisfies some axioms that permit to represent it through a scalar reward [52, 9] (this is referred to as the Reward Hypothesis in literature [58]). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "There is an enormous difference between scenario $(i)$ and scenario $(i i)$ . In $(i)$ the notion of $\\epsilon$ -optimal policy is well-defined for any fixed $\\epsilon>0$ , because the reward function is given and, thus, fixed. Instead, in $(i i)$ , the notion of reward function is a mere mathematical artifact used to represent preferences among trajectories, whose existence is guaranteed by a set of assumptions/axioms [52, 9, 58]. As Example C.1 shows, positive affine transformations of the reward do not affect the structure of preferences represented (see [52] or Section 16.2 of [50] or [30]). Therefore, in $(i i)$ , the notion of $\\epsilon$ -optimal policy is not well-defined, because rescaling a reward function $r$ to $k r$ changes the suboptimality of some policy $\\pi$ from $\\epsilon$ to $k\\epsilon$ . In other words, for fixed $\\epsilon>0$ , any policy can be made $\\epsilon$ -optimal by simply rescaling a reward $r$ to $k r$ for some small enough $k>0$ . ", "page_idx": 32}, {"type": "text", "text": "In IRL, this issue is even more influential because, although we are in setting $(i)$ , we have no idea on the scale of the true reward function. For this reason, our solution is to attach to any reward $r$ a notion of compatibility $\\overline{{\\boldsymbol{C}}}(\\boldsymbol{r})$ which implicitly contains information about the scale of the reward $r$ . Compatibilities of different rewards (e.g., $r^{1}$ and $r^{2}$ in Example C.1) cannot be compared unless the rewards have the same scale (e.g., $r^{1}$ and $r^{2}$ have different scales, thus their compatibilities shall not be compared). ", "page_idx": 32}, {"type": "text", "text": "It should be observed that in Appendix C.2 we discuss a notion of compatibility independent of the scale of the reward. However, we show that it suffers from major drawbacks that make the notion of compatibility introduced in the main paper (Definition 4.1) more suitable for the IRL problem. ", "page_idx": 32}, {"type": "text", "text": "In conclusion, to settle Example C.1, rewards $r^{1}$ and $r^{2}$ should not have the same compatibility, because they have different scales, and the notion of compatibility (i.e., suboptimality) is strictly connected to the scale of the reward. To carry out a fair comparison of compatibilities, one should rescale the compatibility of each reward based on the scale of the reward. ", "page_idx": 32}, {"type": "text", "text": "D Missing Proofs and Additional Results for Section 5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "This appendix is organized as follows. First, we report the full pseudo-code of CATY-IRL. Then, we provide the proof of Theorem 5.1 in Appendix D.2. ", "page_idx": 32}, {"type": "text", "text": "D.1 Algorithm ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we provide the extended version of CATY-IRL containing the explicit conditions under which we shall instantiate one BPI/RFE algorithm instead of another. ", "page_idx": 32}, {"type": "table", "img_path": "ZjgcYMkCmX/tmp/e8a86f9d6e5a88596376abcd80e25929563336e3328243729f6ed95e048d246b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Data: Failure probability $\\delta>0$ , target accuracy $\\epsilon>0$ , expert demonstrations $\\mathcal{D}^{E}$ , classification threshold $\\Delta\\in\\mathbb{R}$ , reward to classify $r\\in\\mathcal{R}$ , problem structure $\\imath\\in$ ttabular, linear rewards, Linear $\\mathrm{MDP}\\}$ , dataset $\\mathcal{D}$ // Estimate the expert $^,\\mathtt{s}$ performance $\\hat{J}^{E}(r)$ : 1 if $\\textit{\\i}=$ tabular then 2 $\\hat{d}^{E}\\gets$ empirical estimate of $d^{p,\\pi^{E}}$ from $\\mathcal{D}^{E}$ 3 $\\begin{array}{r}{\\hat{J}^{E}(r)\\gets\\sum_{h}\\langle\\hat{d}_{h}^{E},r_{h}\\rangle}\\end{array}$ 4 else 5 $\\hat{\\psi}^{E}\\gets$ empirical estimate of $\\psi^{p,\\pi^{E}}$ from $\\mathcal{D}^{E}$ 6 $\\begin{array}{r}{\\hat{J}^{E}(r)\\gets\\sum_{h}\\langle\\hat{\\psi}_{h}^{E},r_{h}\\rangle}\\end{array}$ 7 end // Estimate the optimal performance $\\hat{J}^{*}(r)$ : 8 if $\\i\\in$ ttabular, linear rewardsu then 9 if $|{\\mathcal{R}}|$ is a small constant then 10 $\\hat{J}^{*}(r)\\gets\\mathrm{BPI\\_Planning}(\\mathcal{D},r)$ /\\* Algorithm BPI-UCBVI [37] \\*/ 11 12 else 13 J\u02daprq \u00d0 RFE_PlanningpD, rq /\\* Algorithm RF-Express [37] \\*/ 14 15 end 16 else 17 $\\hat{J}^{*}(r)\\gets\\mathrm{RFE\\_Planning}(\\mathcal{D},r)$ /\\* Algorithm RFLin [62] \\*/ 18 19 end // Classify the reward: 20 $\\hat{\\mathcal{C}}(r)\\gets\\hat{J}^{*}(r)-\\hat{J}^{E}(r)$ 21  cplass $\\gets$ Tprue if $\\widehat{\\mathcal{C}}({\\boldsymbol{r}})\\leqslant\\Delta$ else False 22 return class ", "page_idx": 33}, {"type": "text", "text": "D.2 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Notice that, according to Definition 4.3, an algorithm is $(\\epsilon,\\delta)$ -PAC for IRL if it computes an estimate $\\epsilon$ -close to the true (non)compatibility w.h.p.. Such definition does not depend on the specific strategy adopted by the algorithm to actually classify the input reward using the computed estimate of (non)compatibility. ", "page_idx": 33}, {"type": "text", "text": "Before diving into the proof of Theorem 5.1, we make the following considerations. ", "page_idx": 33}, {"type": "text", "text": "In the common tabular MDPs setting without additional structure, we know that the expected utility $J^{\\pi}(r;p)$ of policy $\\pi$ under reward $r$ in environment with dynamics $p$ can computed as: ", "page_idx": 33}, {"type": "equation", "text": "$$\nJ^{\\pi}(r;p)=\\sum_{h\\in[\\![H]\\!]}\\langle r_{h},d_{h}^{p,\\pi}\\rangle,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $d_{h}^{p,\\pi}$ is the occupancy measure of policy $\\pi$ in $p$ . It should be remarked that both $r_{h}$ and $d_{h}^{p,\\pi}$ have $S A$ components for all $h\\in\\left[U\\right]$ . ", "page_idx": 33}, {"type": "text", "text": "In tabular MDPs with linear reward functions and in Linear MDPs, the reward function is linear in some feature map $\\phi$ , i.e.: ", "page_idx": 33}, {"type": "equation", "text": "$$\nr_{h}(\\cdot,\\cdot)=\\langle\\phi(\\cdot,\\cdot),\\theta_{h}\\rangle\\qquad\\forall h\\in\\[\\![H]\\!],\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\|\\phi(s,a)\\|_{2}\\leqslant1$ for all $(s,a)\\in S\\times A$ and $\\operatorname*{max}_{h}\\|\\theta_{h}\\|_{2}\\leqslant{\\sqrt{d}}$ . Using this decomposition, we can rewrite the expected utility $J^{\\pi}(r;p)$ as: ", "page_idx": 33}, {"type": "equation", "text": "$$\nJ^{\\pi}(r;p)=\\sum_{h\\in[\\![H]\\!]}\\langle r_{h},d_{h}^{p,\\pi}\\rangle\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{h\\in[H]}\\left<\\theta_{h}^{\\top}\\phi,d_{h}^{p,\\pi}\\right>}\\\\ &{=\\displaystyle\\sum_{h\\in[H]}\\theta_{h}^{\\top}\\!\\!\\!\\!\\!_{(s,a)\\sim d_{h}^{p,\\pi}}\\,\\phi(s,a)}\\\\ &{=\\displaystyle\\sum_{h\\in[H]}\\theta_{h}^{\\top}\\psi_{h}^{p,\\pi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we have defined the feature expectations $\\{\\psi_{h}^{p,\\pi}\\}_{h\\in[\\![H]\\!]}$ as $\\psi_{h}^{p,\\pi}:=\\mathbb{E}_{(s,a)\\sim d_{h}^{p,\\pi}}\\,\\phi(s,a)$ . Observe that vector $\\psi_{h}^{p,\\pi}$ has $d$ components instead of the $S A$ co mp onents of each $d_{h}^{p,\\pi}$ vector. ", "page_idx": 34}, {"type": "text", "text": "Since in our setting the IRL algorithm receives in input the reward function (or its parameter $\\theta\\in\\mathbb{R}^{d}$ ), to estimate the expected utility $J^{\\pi}(r;p)$ we must estimate the visit distributions $\\{d_{h}^{\\bar{p},\\pi}\\}_{h}$ or the feature expectations $\\{\\bar{\\psi_{h}^{\\bar{p},\\pi}}\\}_{h}$ . However, because of the different dimensionalities of such quantities $S A$ versus ), the estimates might require different amounts of samples. ", "page_idx": 34}, {"type": "text", "text": "Theorem 5.1 (Sample Complexity of CATY-IRL). Let $\\epsilon,\\delta\\in(0,1)$ . Then CATY-IRL is $(\\epsilon,\\delta)$ -PAC for IRL with a sample complexity upper bounded by: ", "page_idx": 34}, {"type": "text", "text": "Tabular MDPs: $\\begin{array}{r l}&{\\tau^{E}\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}S A}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\Big),\\quad\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}S A}{\\epsilon^{2}}\\Big(N+\\log\\frac{1}{\\delta}\\Big)\\Big),}\\\\ &{\\tau^{E}\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}d}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\Big),\\quad\\quad\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}S A}{\\epsilon^{2}}\\Big(N+\\log\\frac{1}{\\delta}\\Big)\\Big),}\\\\ &{\\tau^{E}\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}d}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\Big),\\quad\\quad\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{5}d}{\\epsilon^{2}}\\Big(d+\\log\\frac{1}{\\delta}\\Big)\\Big),}\\end{array}$   \nTabular MDPs with linear rewards:   \nLinear MDPs: ", "page_idx": 34}, {"type": "text", "text": "where $N=0\\;i f|\\mathcal{R}|=\\Theta(1),$ , and $N=S$ otherwise. ", "page_idx": 34}, {"type": "text", "text": "Proof. To prove the theorem, we aim to find a bound to the number of samples $\\tau^{E}$ such that the estimate $\\tilde{J}^{E}(r)\\,\\approx\\,J^{\\pi^{E}}(r;p)$ is $\\epsilon/2$ -correct with probability at least $1-\\delta/2$ . Next, similarly, we aim to bo upnd $\\tau$ so that $\\hat{J}^{*}(r)\\approx J^{*}(r;p)$ is $\\epsilon/2$ -correct with probability at least $1-\\delta/2$ . Then, the conclusion follows afte r pperforming a union bound and observing that, for any $r\\in\\mathcal{R}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)-\\widehat{\\mathcal{C}}(r)\\right|=\\Big|\\Big(J^{*}(r;p)-J^{\\pi^{E}}(r;p)\\Big)-\\Big(\\widehat{J}^{*}(r)-\\widehat{J}^{E}(r)\\Big)\\Big|}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\Big|J^{*}(r;p)-\\widehat{J}^{*}(r)\\Big|+\\Big|J^{\\pi^{E}}(r;p)-\\widehat{J}^{E}(r)\\Big|}\\\\ &{\\qquad\\qquad\\leqslant\\displaystyle\\frac{\\epsilon}{2}+\\frac{\\epsilon}{2}=\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Estimating $\\widehat{J}^{E}(r)\\approx J^{\\pi^{E}}(r;p)$ ", "page_idx": 34}, {"type": "text", "text": "To estimate $\\boldsymbol{J^{\\pi}}^{E}\\left(\\boldsymbol{r};p\\right)$ , CATY-IRL simply computes the empirical estimate of $\\{d_{h}^{p,\\pi^{E}}\\}$ in case of tabular MDPs, and the empirical estimate of $\\{\\psi_{h}^{p,\\pi^{E}}\\}$ in case of tabular MDPs with linear rewards and Linear MDPs. Notice that by empirical estimates we mean: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\widehat{d}_{h}^{E}(s,a):=\\frac{\\underset{i\\in[\\tau^{E}]}{\\sum}1\\{s_{h}^{i}=s\\wedge a_{h}^{i}=a\\}}{\\underset{i\\in[\\tau^{E}]}{\\sum}1\\{s_{h}^{i}=s\\}}\\qquad\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widehat{\\psi}_{h}^{E}:=\\frac{i\\!\\in\\![\\tau^{E}]}{\\tau^{E}}\\,\\qquad\\forall h\\in[H].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Concerning the estimate of the visit distribution $\\hat{d}^{E}$ , we can use the result of Lemma 6 in [53] (we are working with bounded rewards), to obtain tha tp: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{h\\in[H]}\\|d_{h}^{p,\\pi^{E}}-\\widehat{d}_{h}^{E}\\|_{1}\\leqslant\\sqrt{\\frac{S A H^{3}\\log\\frac{8S A H}{\\delta}}{2\\tau^{E}}}\\leqslant\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Solving w.r.t. $\\tau^{E}$ we get the bound on $\\tau^{E}$ . ", "page_idx": 35}, {"type": "text", "text": "In a completely analogous manner, we can bound the feature expectations as: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{h\\in[H]}\\|\\psi_{h}^{p,\\pi^{E}}-\\widehat{\\psi}_{h}^{E}\\|_{1}\\leqslant\\sqrt{\\frac{d H^{3}\\log\\frac{8d H}{\\delta}}{2\\tau^{E}}}\\leqslant\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Again, solving w.r.t. $\\tau^{E}$ we get the bound on $\\tau^{E}$ . ", "page_idx": 35}, {"type": "text", "text": "Estimating $\\widehat{J}^{*}(r)\\approx J^{*}(r;p)$ ", "page_idx": 35}, {"type": "text", "text": "Let us begin  with the case in which $\\mathcal{R}$ is large. As explained for instance in Definition 4 of [66], both algorithms RF-Express [37] and RFLin [62] satisfy the uniform policy evaluation property, i.e., they guarantee that, for any $\\epsilon,\\delta\\in(0,1)$ , after having explored for $\\begin{array}{r}{\\tau\\leqslant\\tilde{\\mathcal{O}}\\Big(\\frac{H^{3}S A}{\\epsilon^{2}}\\big(S+\\log\\frac{1}{\\delta}\\big)\\Big)}\\end{array}$ in case of RF-Express [37], and $\\begin{array}{r}{\\tau\\leqslant\\widetilde{\\mathcal{O}}\\Big(\\frac{H^{5}d}{\\epsilon^{2}}\\big(d+\\log\\frac{1}{\\delta}\\big)\\Big)}\\end{array}$ for the algorithm  in [62] (we omit linear terms in $1/\\epsilon)$ , they compute an estim ate ${\\widehat{p}}\\approx p$ of the true transition model such that: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{sup}_{r\\in\\mathfrak{R},\\pi\\in\\Pi}\\big|J^{\\pi}(r;p)-J^{\\pi}(r;\\widehat{p})\\big|\\leqslant\\epsilon\\Big)\\geqslant1-\\delta.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Clearly, if such property holds, then by computing the performance of the policy $\\widehat{\\pi}$ outputted by the RFE algorithm we are able to obtain an $\\epsilon/2$ -correct estimate of $J^{\\ast}(r;p)$ .16 ", "page_idx": 35}, {"type": "text", "text": "Concerning the case in which $|{\\mathcal{R}}|$ is a finite small constant, for tabular and tabular with linear rewards MDPs, we can simply use algorithm BPI-UCBVI of [37] as sub-routine, and run it as many times as there are rewards in $\\mathcal{R}$ . When $|{\\mathcal{R}}|$ is a small constant, we can proceed with a union bound over $\\mathcal{R}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{sup}_{r\\in\\Re,\\pi\\in\\Pi}\\big|J^{\\pi}(r;p)-J^{\\pi}(r;\\widehat{p})\\big|\\leqslant\\epsilon\\Big)\\geqslant1-\\sum_{r\\in\\mathcal{R}}\\mathbb{P}\\Big(\\operatorname*{sup}_{\\pi\\in\\Pi}\\big|J^{\\pi}(r;p)-J^{\\pi}(r;\\widehat{p})\\big|>\\epsilon\\Big)\\geqslant1-|\\mathcal{R}|\\delta.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This allows us to formally distinguish between small and large $|{\\mathcal{R}}|$ based on the following inequality: ", "page_idx": 35}, {"type": "equation", "text": "$$\nS+\\log\\frac{1}{\\delta}<\\log\\frac{|\\mathcal{R}|}{\\delta}\\implies S<\\log|\\mathcal{R}|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "E Missing Proofs and Additional Results for Section 6.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "This appendix is organized as follows. First, in Appendix E.1, we introduce two problems that share similarities with RFE and IRL, and we characterize the main differences among them. In addition, we enunciate a lower bound to the sample complexity that is common to some of these 4 problems. Next, in Appendix E.2, we provide the missing proofs. ", "page_idx": 35}, {"type": "text", "text": "E.1 Four Problems ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The 4 problems that we consider here are Reward-Free Exploration (RFE), Inverse Reinforcement Learning (IRL), Matching Performance (MP), and Imitation Learning from Demonstrations alone (ILfO). MP represents a novel generalization of RFE, while ILfO, introduced in [34], represents an exemplification of MP. Before enunciating the minimax lower bound, it is important to formally define each of these problems, as well as what we mean by learning in each problem. ", "page_idx": 35}, {"type": "text", "text": "E.1.1 Definition of the Problems ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In all the 4 problems, the learner is placed into an unknown MDP without reward $\\mathcal{M}\\;=\\;$ $(S,{\\mathcal{A}},H,d_{0},{\\bar{p}})$ , i.e., an environment whose dynamics $(d_{0},p)$ is unknown to the learner. For simplicity, w.l.o.g., we assume that there is a single initial state $s_{0}:=d_{0}$ . In each problem, the learner can explore the environment at will to collect samples about the dynamics $p$ , whose knowledge improves the performance of the agent at solving the task. However, at exploration phase, the learner does not know which is the specific task it has to solve. It just knows that the specific task belongs to a given set of tasks $\\mathfrak{T}$ (e.g., set of reward functions). The agent can use the knowledge of $\\mathfrak{T}$ to engage in a more efficient task-driven exploration. For any $\\epsilon,\\delta\\in(0,1)$ , the goal of the agent is to being able to ouputting, for any task in $\\mathfrak{T}$ a quantity $\\mathfrak{o}$ (e.g., a policy) that solves that specific task in an $\\epsilon$ -correct manner with probability at least $1-\\delta$ . The ultimate goal of exploration is to collect the least number of samples that permits $(\\epsilon,\\delta)$ -correctness for all the tasks in $\\mathfrak{T}$ . ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "Now, let us see what the quantities $\\mathfrak{T}$ and $\\mathfrak{o}$ represent in each of the 4 problems. In Table 1, we provide a sum up of the various definitions. ", "page_idx": 36}, {"type": "text", "text": "Reward-Free Exploration (RFE). In RFE, the learner receives a set of reward functions $\\mathfrak{T}=\\mathcal{R}\\subseteq$ $\\Re$ in input, and the goal is to exploit the information about $p$ collected at exploration phase to output, for any reward $r\\in\\mathcal{R}$ , an $\\epsilon_{\\mathrm{:}}$ -optimal policy $o=\\widehat{\\pi}_{r}$ w.p. $1-\\delta$ . When ${\\mathfrak{T}}=\\{\\bar{r}\\}$ is a singleton, the RFE problem is commonly termed the BPI problem .p In symbols, any RFE algorithm must guarantee that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{sup}_{r\\in\\mathcal{R}}J^{*}(r;p)-J^{\\hat{\\pi}_{r}}(r;p)\\leqslant\\epsilon\\Big)\\geqslant1-\\delta,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ${\\widehat{\\pi}}_{r}$ is the estimate of the algorithm for reward $r$ . ", "page_idx": 36}, {"type": "text", "text": "Inverse Reinforcement Learning (IRL). In IRL, the learner receives in input an occupancy $\\{d_{h}^{p,\\pi^{E}}\\}_{h\\in[\\![H]\\!]}$ and a set of reward functions $\\mathcal{R}\\subseteq\\mathfrak{R}$ : ${\\mathfrak{T}}=(d^{p,\\pi^{E}},{\\mathcal{R}})$ , but it does not know which specific rewar d i t will have to classify. Under the assumption that the occupancy measure $d^{p,\\pi^{E}}$ is known,18 the problem reduces to exploiting the information about $p$ collected at exploration phase to output, for any reward $r\\in\\mathcal{R}$ , an $\\epsilon$ -correct estimate $o=\\widehat{J}(\\boldsymbol{r})$ of the optimal utility $J^{*}(r)$ w.p. $1-\\delta$ . In symbols, under these conditions, any IRL algorithm  pmust guarantee that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\underset{r\\in\\mathcal{R}}{\\operatorname*{sup}}\\left|J^{*}(r;p)-\\widehat{J}(r)\\right|\\leqslant\\epsilon\\Big)\\geqslant1-\\delta,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\boldsymbol{\\hat{J}}(\\boldsymbol{r})$ is the estimate of the algorithm for reward $r$ . ", "page_idx": 36}, {"type": "text", "text": "Matching Performance (MP). In MP, the learner receives in input a set of reward functions $\\mathcal{R}\\subseteq\\mathfrak{R}$ and a measure of performance for each of them $\\overline{{J}}:\\mathcal{R}\\rightarrow\\mathbb{R}$ : $\\bar{\\mathfrak{T}}=(\\overline{{J}},\\mathcal{R})$ . For any $r\\in\\mathcal{R}$ , the utility $\\overline{{J}}(\\boldsymbol{r})$ represents a performance measure for which we aim to find the policy that achieves closest performance. Thus, in MP, the goal is to exploit the information about $p$ collected at exploration phase to output, for any reward $r\\,\\in\\,\\mathcal R$ , a policy $o\\;=\\;\\widehat{\\pi}_{r}$ such that, if we denote the policy with performance closest to $\\overline{{J}}(\\boldsymbol{r})$ by $\\begin{array}{r}{\\overline{{\\pi}}_{r}\\in\\arg\\operatorname*{min}_{\\pi}\\left|J^{\\pi}(r)-\\overline{{J}}(r)\\right|}\\end{array}$ , then the utility of policy ${\\widehat{\\pi}}_{r}$ is $\\epsilon$ -close to the utility of policy $\\overline{{\\pi}}_{r}$ w.p. $1-\\delta$ . In symbols, any MP algorithm must guarantee th atp: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{sup}_{r\\in\\mathcal{R}}\\big|J^{\\overline{{\\pi}}_{r}}(r;p)-J^{\\hat{\\pi}_{r}}(r;p)\\big|\\leqslant\\epsilon\\Big)\\geqslant1-\\delta,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{\\pi}}_{r}\\in\\arg\\operatorname*{min}_{\\pi}|J^{\\pi}(r)-\\overline{{J}}(r)|}\\end{array}$ , and ${\\widehat{\\pi}}_{r}$ is the estimate of the algorithm for reward $r$ . ", "page_idx": 36}, {"type": "text", "text": "Imitation Learning from Demonstrations alone (ILfO). In ILfO, the learner receives in input a set of state-only reward functions $\\mathcal{R}\\subset\\mathfrak{R}$ and a state-only occupancy measure $\\{\\overline{{d}}_{h}\\}_{h\\in[\\![H]\\!]}$ : $\\mathfrak{T}=(\\overline{{d}},\\mathcal{R})$ Under the assumption that $\\bar{d}$ does not leak any information about the true transition model $p$ , the goal is to exploit the information about $p$ collected at exploration phase to output, for any reward $r\\in\\mathcal{R}$ , a policy $o=\\widehat{\\pi}_{r}$ such that, if we denote the policy with performance closest to $\\begin{array}{r}{\\overline{{J}}(\\boldsymbol{r}):=\\sum_{h\\in[\\boldsymbol{H}]}\\langle r_{h},\\overline{{d}}_{h}\\rangle}\\end{array}$ by $\\begin{array}{r}{\\overline{{\\pi}}_{r}\\in\\arg\\operatorname*{min}_{\\pi}|J^{\\pi}(r)-\\overline{{J}}(r)|}\\end{array}$ , then the utility of policy ${\\widehat{\\pi}}_{r}$ is $\\epsilon_{}$ -close to the utility of policy $\\overline{{\\pi}}_{r}$ w.p. $1-\\delta$ . Simply put, ILfO, as defined in this manner, exe pmplifies the MP setting by providing a functional form to $\\overline{{J}}:\\mathcal{R}\\rightarrow\\mathbb{R}$ as an inner product between a certain state-only occupancy measure and the input reward. It should be remarked that the assumption made for ILfO is mild, because it is satisfied by the setting in which the expert and the learner have the same state space but different action spaces (or different dynamics). Indeed, in such case, the visit distribution $\\overline{{d}}$ of the expert would not leak any information about $p$ . In symbols, any ILfO algorithm must guarantee that: ", "page_idx": 36}, {"type": "table", "img_path": "ZjgcYMkCmX/tmp/f0f393eaba55490fdd853e2d7a4916fea71802276101ee158d3f03064e460037.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{sup}_{r\\in\\mathcal{R}}\\big|J^{\\overline{{\\pi}}_{r}}(r;p)-J^{\\hat{\\pi}_{r}}(r;p)\\big|\\leqslant\\epsilon\\Big)\\geqslant1-\\delta,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\overline{{\\pi}}_{r}\\,\\in\\,\\arg\\operatorname*{min}_{\\pi}\\,|J^{\\pi}(r)-\\overline{{J}}(r)|$ and $\\begin{array}{r}{\\overline{{J}}(\\boldsymbol{r}):=\\sum_{h\\in[\\boldsymbol{H}]}\\langle r_{h},\\overline{{d}}_{h}\\rangle.}\\end{array}$ , and ${\\widehat{\\pi}}_{r}$ is the estimate of the algorithm for reward $r$ . ", "page_idx": 37}, {"type": "text", "text": "E.1.2 Lower Bound ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We now present a minimax lower bound rate that is common to RFE, IRL, and MP. We report here the lower bounds presented in Section 6.1. ", "page_idx": 37}, {"type": "text", "text": "Theorem 6.1 (IRL Classification - Lower Bound). Let A be an $(\\epsilon,\\delta)$ -PAC algorithm for the IRL classification in tabular MDPs. Let $\\tau$ be the number of exploration episodes. Then, there exists an IRL classification instance such that: ", "page_idx": 37}, {"type": "equation", "text": "$$\ni f\\lvert\\mathcal{R}\\rvert\\geqslant1:\\ \\tau\\geqslant\\Omega\\biggl(\\frac{H^{3}S A}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\biggr),\\qquad i f\\mathscr{R}=\\Re:\\ \\tau\\geqslant\\Omega\\biggl(\\frac{H^{3}S A}{\\epsilon^{2}}\\Bigl(S+\\log\\frac{1}{\\delta}\\Bigr)\\biggr).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. The proof is similar to that of [38]. We split the proof in two parts, by considering two classes of difficult problem instances in Lemma E.2 and Lemma E.3. Next, we combine the two bounds through $\\operatorname*{max}\\{a,b\\}\\geqslant(a+b)/2$ for all $a,b\\geqslant0$ . For the proof, we will assume that the expert visit distribution is known. The obtained bound represents a lower bound to the more general setting in which it is unknown. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Theorem E.1 (RFE - Refined Lower Bound). Let $\\mathfrak{A}$ be an $(\\epsilon,\\delta)$ -PAC algorithm for RFE in tabular MDPs. Let $\\tau$ be the number of exploration episodes. Then, there exists an RFE instance such that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau\\geqslant\\Omega\\Biggl(\\frac{H^{3}S A}{\\epsilon^{2}}\\Bigl(S+\\log\\frac{1}{\\delta}\\Bigr)\\Biggr).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. The proof of this result is analogous to that of Theorem 6.1, and it employs Lemma E.2 and Lemma E.3. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Some observations are in order. First, since MP is a more general setting than RFE, then this lower bound is a lower bound for MP too. However, this is not guaranteed for ILfO. We observe that, while for RFE and IRL the bound is tight, for MP we cannot say so because we do not have the upper bound. Notice that, in case the expert state-only distribution $\\overline{{d}}$ was unknown at exploration phase, and revealed afterwards, then the lower bound of Theorem 6.1 holds for ILfO too, because we might a posteriori reveal the state-only distribution $\\overline{{d}}$ of the optimal policy, and thus, in such manner, ILfO would be reduced to RFE. ", "page_idx": 37}, {"type": "text", "text": "E.2 Missing proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lemma E.2. Let IRL and RFE be the learning problems defined as in Appendix E.1. Then, for each problem, any $(\\epsilon,\\delta)$ -PAC algorithm must collect at least the following number of exploration episodes: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau\\geqslant\\Omega\\mathopen{}\\mathclose\\bgroup\\left(\\frac{H^{3}S A}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\aftergroup\\egroup\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Observe that the proof for RFE is present in [12]. Thus, we have to prove just the result for IRL. For doing so, we will use both the results of [12] and [38]. Notice that for the sake of this proof we consider $\\bar{\\mathcal{R}}=\\{r\\}$ , that will reduce our problem to simple RL as, in order to compute the function $\\overline{{\\mathcal{C}}}_{p,\\pi^{E}}(r)$ , we just need to compute $J^{\\ast}(\\boldsymbol{r};\\boldsymbol{p})$ , being $\\boldsymbol{J^{\\pi}}^{E}(\\boldsymbol{r};p)$ known from the availability of $d^{p,\\pi^{E}}$ and $r$ . ", "page_idx": 38}, {"type": "image", "img_path": "ZjgcYMkCmX/tmp/36a9efa44883f7c9740c4c3f3908ca74e500d2de873c024a2a97066d4b1d89c0.jpg", "img_caption": ["Figure 4: Hard instances. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Instances Description The hard instances considered are exactly the same as [12], and are reported in Figure 4 for simplicity. The only difference is the presence of state $s_{E}$ , to which the expert\u2019s policy $\\pi^{E}$ brings, which is absorbing. Such state is needed to make the knowledge of the expert\u2019s visit distribution dp,\u03c0 useless at inferring information about the transition model in other parts of the state-action space. Based on [12], we describe such hard instances. Similarly to [12], we assume that $S\\geqslant7,A\\geqslant2$ , and there exists an integer $d$ such that $S=4+(A^{d}-1)/(A-1)$ , and we assume that $H\\geqslant3d$ . Note that [12] show how to relax the assumption on the existence of $d$ . ", "page_idx": 38}, {"type": "text", "text": "There are the initial state $s_{\\mathrm{w}}$ , from which the agent starts, and states $s_{g},s_{b}$ , respectively, the \u201cgood\u201d and \u201cbad\u201d states which are absorbing. Moreover, there is state $s_{E}$ , which is reached by the expert, and is absorbing. The remaining $S-4$ states are arranged in a full $A_{}$ -ary tree of depth $d-1$ with root $s_{\\mathrm{root}}$ . We denote by $\\overline{{H}}\\leqslant H-d$ a certain integer parameter, and by $\\mathcal{L}:=\\left\\{s_{1},s_{2},\\ldots,s_{L}\\right\\}$ the set of leaves of the tree. We define $\\mathcal{Z}:=\\{1+d,\\ldots,\\overline{{H}}+d\\}\\times\\mathcal{L}\\times\\mathcal{A}$ . For any $\\iota\\in{\\mathcal{Z}}$ , we define and MDP $\\mathcal{M}_{\\imath}$ as follows. In any state of the tree, i.e., in states $S\\backslash\\{s_{\\mathrm{w}},s_{g},s_{b},s_{E}\\}$ , the transitions are deterministic, and the $a$ -th action of a state brings to the $a$ -th child of that node. ", "page_idx": 38}, {"type": "text", "text": "The transitions from $s_{\\mathrm{w}}$ are given by ", "page_idx": 38}, {"type": "equation", "text": "$$\np_{h}(s_{\\mathrm{w}}|s_{\\mathrm{w}},a):=\\mathbb{1}\\{a=a_{\\mathrm{w}},h\\leqslant\\overline{{H}}\\}\\quad\\mathrm{and}\\quad p_{h}(s_{\\mathrm{root}}|s_{\\mathrm{w}},a):=1-p_{h}(s_{\\mathrm{w}}|s_{\\mathrm{w}},a).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In other words, action $a_{\\mathrm{w}}$ allows the agent to remain in the initial state $s_{\\mathrm{w}}$ up to stage $\\overline{H}$ . After stage $\\overline{H}$ , the agent is forced to leave $s_{\\mathrm{w}}$ and to traverse the tree down to the leaves. Action $a_{E}=\\pi_{1}^{E}(s_{\\mathrm{w}})$ is the only action that brings to state $s_{E}$ , which is absorbing. The transitions from any leaf $s_{i}\\in\\mathcal{L}$ are given, as in [12], by: ", "page_idx": 39}, {"type": "equation", "text": "$$\np_{h}(s_{g}|s_{i},a):=\\frac{1}{2}+\\Delta_{(h^{*},\\ell^{*},a^{*})}(h,s_{i},a)\\quad\\mathrm{and}\\quad p_{h}(s_{b}|s_{i},a):=\\frac{1}{2}-\\Delta_{(h^{*},\\ell^{*},a^{*})}(h,s_{i},a),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\Delta_{(h^{\\ast},\\ell^{\\ast},a^{\\ast})}(h,s_{i},a):=\\mathbb{1}\\{(h,s_{i},a)\\,=\\,(h^{\\ast},s_{\\ell^{\\ast}},a^{\\ast})\\}\\cdot\\epsilon^{\\prime}$ , for some $\\epsilon^{\\prime}\\in\\,[0,1/2]$ . For this reason, there exists a (single) leaf $\\ell^{*}$ where the agent can choose an action $a^{*}$ at stage $h^{*}$ to increase its probability of arriving to the good state $s_{g}$ , which provides higher reward. We define states $s_{g}$ and $s_{b}$ to be absorbing, i.e., they satisfy $p_{h}(s_{b}|\\bar{s}_{b},a):=\\bar{p}_{h}(s_{g}|s_{g},a):=1$ for any action $a$ . The reward function is state-only and is defined as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\forall a\\in{\\cal A},\\quad r_{h}(s,a):=\\mathbb{1}\\{s=s_{g},h\\geqslant\\overline{{{H}}}+d+1\\},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "so that even though the agent decides to stay at $s_{\\mathrm{w}}$ until stage $\\overline{H}$ , it does not lose any reward. Observe that state $s_{E}$ does not provide any reward, so that to estimate the (non)compatibility, any algorithm must provide a good estimate of the optimal performance. ", "page_idx": 39}, {"type": "text", "text": "Finally, we define a reference MDP $\\mathcal{M}_{0}$ which is an MDP of the above type but for which $\\Delta_{0}(h,s_{i},a)\\,:=\\,0\\$ for all $(h,s_{i},a)$ . For certain $\\epsilon^{\\prime}$ and $\\overline{H}$ to choose, we define the class $\\mathbb{M}$ to be the set $\\mathbb{M}:=\\{\\mathcal{M}_{0}\\}\\cup\\{\\mathcal{M}_{\\iota}\\}_{\\iota\\in\\mathcal{T}}$ . ", "page_idx": 39}, {"type": "text", "text": "Distance between problems We will prove the lower bound for instance $\\mathbf{\\mathcal{M}}_{0}$ . Observe that, in $\\mathcal{M}_{0}$ , the optimal utility is: ", "page_idx": 39}, {"type": "equation", "text": "$$\nJ_{0}^{*}=\\frac12(H-\\overline{{H}}-d),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "because there is no triple with additional bias towards $s_{g}$ . Instead, for any other $\\mathcal{M}_{\\tau}\\in\\mathbb{M}$ , the optimal utility is: ", "page_idx": 39}, {"type": "equation", "text": "$$\nJ_{\\imath}^{*}=(H-\\overline{{H}}-d)\\Big(\\frac{1}{2}+\\epsilon^{\\prime}\\Big).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, if we choose $\\epsilon^{\\prime}:=2\\epsilon/(H-\\overline{{H}}-d)$ , we have that, for any $\\iota\\in\\mathcal{T}$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|J_{0}^{*}-J_{\\iota}^{*}\\right|=2\\epsilon.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, in particular, for any estimate $\\widehat{J}\\in\\mathbb{R}$ we necessarily have $|J_{0}^{*}-\\widehat{J}|\\leqslant\\epsilon\\implies|J_{\\imath}^{*}-\\widehat{J}|>\\epsilon$ , and vice versa, i.e., we cannot provid ep an estimate $\\widehat{J}$ that is $\\epsilon$ -close to b otph $J_{0}^{*}$ and $J_{\\imath}^{*}$ . ", "page_idx": 39}, {"type": "text", "text": "Identifying the underlying problem Following [38 ], let us consider a generic $(\\epsilon,\\delta)$ -correct algorithm $\\mathfrak{A}$ that outputs the estimated optimal utility $\\hat{J}$ . Then, for all $\\imath\\in\\mathcal{T}$ , we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta\\geqslant\\underset{\\mathrm{all\\,problem\\,instances\\,}\\mathcal{M}}{\\operatorname*{sup}}\\,\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}\\left(\\left|J_{\\mathcal{M}}^{*}-\\hat{J}\\right|\\geqslant\\epsilon\\right)}\\\\ &{\\,\\,\\geqslant\\,\\underset{\\mathcal{M}\\in\\mathbb{N}}{\\operatorname*{sup}}\\,\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}\\left(\\left|J_{\\mathcal{M}}^{*}-\\hat{J}\\right|\\geqslant\\epsilon\\right)}\\\\ &{\\,\\,\\geqslant\\,\\underset{\\ell\\in\\{0,s\\}}{\\operatorname*{max}}\\,\\mathbb{P}_{\\mathcal{M}_{\\ell},\\mathfrak{A}}\\left(\\left|J_{\\ell}^{*}-\\hat{J}\\right|\\geqslant\\epsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For every $\\boldsymbol{\\imath}\\in\\mathcal{T}$ , we define the identification function $\\Psi_{\\i}$ as the index of the problem \u201crecognized\u201d by algorithm $\\mathfrak{A}$ . In symbols: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\Psi_{\\imath}:=\\underset{\\ell\\in\\{0,\\imath\\}}{\\arg\\operatorname*{min}}\\,\\Big|J_{\\ell}^{*}-\\widehat{J}\\Big|.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In words, given estimate $\\widehat{J}$ returned by algorithm $\\mathfrak{A}$ , the identification function $\\Psi_{\\i}$ returns the problem between $\\mathcal{M}_{0}$ and $\\mathcal{M}_{\\imath}$ w hpose optimal utility is closest to the estimate ${\\widehat{J}}.$ . For what we have seen in the previous paragraph, problems $\\mathcal{M}_{\\mathrm{0}}$ and $\\mathcal{M}_{\\imath}$ lie at a distance of at leas tp $2\\epsilon$ for all $\\iota\\in\\mathcal{T}$ . Therefore, for $\\bar{\\boldsymbol{\\jmath}}\\in\\{0,\\boldsymbol{\\imath}\\}$ , we have the following inclusion of events: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\{\\Psi_{\\i}\\neq j\\}\\subseteq\\{|J_{j}^{*}-\\widehat{J}|>\\epsilon\\}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "image", "img_path": "ZjgcYMkCmX/tmp/ad21e46919575ce287761874c1013d839602ad4b9d0375e6cd0c51ab51847176.jpg", "img_caption": ["Figure 5: Hard instances. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Thanks to this fact, we can continue lower bounding the probability as: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{\\ell\\in\\{0,\\iota\\}}\\mathbb{P}_{{M_{\\ell}},\\mathfrak{A}}\\!\\left(\\left|J_{\\ell}^{*}-\\hat{J}\\right|\\geqslant\\epsilon\\right)\\geqslant\\displaystyle\\operatorname*{max}_{\\ell\\in\\{0,\\iota\\}}\\mathbb{P}_{{M_{\\ell}},\\mathfrak{A}}\\!\\left\\{\\Psi_{\\iota}\\neq\\ell\\right\\}}&{}\\\\ {\\stackrel{(1)}{\\geqslant}\\displaystyle\\frac{1}{2}\\Bigg[\\mathbb{P}_{M_{0},\\mathfrak{A}}\\!\\left(\\Psi_{\\iota}\\neq0\\right)+\\mathbb{P}_{{M_{1}},\\mathfrak{A}}\\!\\left(\\Psi_{\\iota}\\neq\\iota\\right)\\Bigg]}&{}\\\\ {=\\displaystyle\\frac{1}{2}\\Bigg[\\mathbb{P}_{M_{0},\\mathfrak{A}}\\!\\left(\\Psi_{\\iota}\\neq0\\right)+\\mathbb{P}_{{M_{1}},\\mathfrak{A}}\\!\\left(\\Psi_{\\iota}=0\\right)\\Bigg]}&{}\\\\ {\\stackrel{(2)}{\\geqslant}\\displaystyle\\frac{1}{4}\\exp^{-\\mathrm{KL}(\\mathbb{P}_{M_{0},\\mathfrak{A}},\\mathbb{P}_{M_{1},\\mathfrak{A}})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where at (1) we have lower bounded the maximum with the average, i.e., $\\operatorname*{max}\\{a,b\\}\\geqslant(a+b)/2$ for all $a,b\\geqslant0$ , and at (2) we have applied the Bretagnolle-Huber\u2019s inequality [38]. ", "page_idx": 40}, {"type": "text", "text": "KL-divergence computation The proof can be concluded by upper bounding the KL divergence $\\mathrm{KL}(\\mathbb{P}_{\\mathcal{M}_{0},\\mathfrak{A}},\\mathbb{P}_{\\mathcal{M}_{\\tau},\\mathfrak{A}})$ as in the proof of Theorem 7 in [12], and then summing over all the $\\Theta({\\bar{S}}A H)$ instances to retrieve the result. ", "page_idx": 40}, {"type": "text", "text": "Lemma E.3. Let IRL and RFE be the learning problems defined as in Appendix E.1. For each problem, if the set of reward functions $\\mathcal{R}$ in input is $\\mathcal{R}=\\mathfrak{R}$ , then any $(\\epsilon,\\delta)$ -PAC algorithm must collect at least the following number of exploration episodes: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\tau\\geqslant\\Omega\\biggl(\\frac{H^{3}S^{2}A}{\\epsilon^{2}}\\biggr).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Instances description The hard instances that we use for the proof of this lemma are obtained by combining the hard instances in Lemma E.2 (i.e., the hard instances of [12]), with those in [38]. Specifically, this construction is based on the intuition described in [21] that, if we want to increase the sample complexity, we have to learn transitions also to $\\Theta(S)$ states, and not just from $\\Theta(S)$ states. Observe the presence of state $s_{E}$ (only for IRL), which plays the same role as in the proof of Lemma E.2. Any action in such state receives always reward $-1$ , thus it is meaningless for the estimate of the (non)compatibility, which reduces to the estimation of the optimal performance. In this manner, the expert distribution $d^{p,\\pi^{E}}$ does not provide additional information about the transition model of other portion of the state-action space. Therefore, in the following, we will present the lower bound construction as if such state did not exist. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "The hard instances are reported in Figure 5. Notice that they are exactly the same instances as those presented in the proof of Lemma E.2, with the difference that, from the $\\overline{S}$ leaves (differently from earlier, we now denote the number of leaves through $\\overline{S}$ instead of $L$ ), we do not reach just two states $s_{g},s_{b}$ , but we reach $\\Theta(S)$ absorbing states, i.e., $s_{1}^{\\prime},s_{2}^{\\prime},\\ldots,s_{\\overline{{S}}}^{\\prime}$ . The transitions from the leaves to such states is the same as in [38], and we report a description below. ", "page_idx": 41}, {"type": "text", "text": "Let us introduce the set $\\overline{{\\mathcal{Z}}}:=\\{s_{1},\\ldots,s_{\\overline{{S}}}\\}\\times\\mathcal{A}\\times\\{1+d,\\ldots,\\overline{{H}}+d\\}.$ . Let $\\bar{\\iota}:=\\left(s_{1},a_{1},1+d\\right)\\in\\overline{{\\mathcal{L}}}$ be a specific triple of set $\\mathcal{T}$ , and denote $\\mathcal{T}:=\\overline{{\\mathcal{T}}}\\backslash\\{\\bar{\\iota}\\}$ . Let us also introduce set $\\mathcal{V}:=\\{\\boldsymbol{v}\\in\\{-1,1\\}^{\\overline{{S}}}$ : $\\begin{array}{r}{\\sum_{j=1}^{\\overline{{S}}}v_{j}=0\\}}\\end{array}$ . Thanks to Lemma E.6 of [38] (that we report in Lemma E.4 for simplicity), we know that there exists a subset ${\\overline{{V}}}\\subseteq\\mathcal{V}$ (of transition models) with cardinality at least $2^{\\overline{{S}}/5}$ such that, for every pair $v,w\\in{\\overline{{\\mathcal{V}}}}$ with $v\\neq w$ , we have that $\\|v-w\\|_{1}\\geqslant\\overline{{S}}/16$ . In other words, we know that there exists a $\\overline{{S}}/16$ -packing of $\\mathcal{V}$ with cardinality at least $2^{\\overline{{S}}/5}$ . ", "page_idx": 41}, {"type": "text", "text": "Following [38], we denote by $\\pmb{v}=(\\boldsymbol{v}^{\\imath})_{\\imath\\in\\mathcal{I}}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ the generic vector of $\\overline{{\\nu}}^{\\mathcal{I}}$ . Now, for any $\\pmb{v}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ , for any triple $\\overline{{\\jmath}}\\in\\mathcal{T}$ , and for some parameter $\\epsilon^{\\prime}\\in[0,1/2]$ to choose, we construct problem instance $\\mathcal{M}_{v,\\bar{\\mathcal{I}}}$ as follows. ", "page_idx": 41}, {"type": "text", "text": "First of all, we define the transition model at triple $\\bar{\\iota}$ as: ", "page_idx": 41}, {"type": "equation", "text": "$$\np_{h_{\\overline{{{\\tau}}}}}(s_{i}^{\\prime}|s_{\\overline{{{\\tau}}}},a_{\\overline{{{\\tau}}}})=\\frac{1}{\\overline{{{S}}}}\\quad\\forall i\\in\\mathbb{[}\\overline{{{S}}}\\mathbb{]},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where observe that we use notation $\\boldsymbol{\\imath}=\\left(s_{\\iota},a_{\\iota},h_{\\iota}\\right)\\in\\overline{{\\mathcal{L}}}$ to denote triples in $\\overline{{\\mathcal{I}}}$ . Instead, for the generic triple $\\iota\\in\\mathcal{T}$ (including triple $j]$ ), the probability distribution of the next state is given by: ", "page_idx": 41}, {"type": "equation", "text": "$$\np_{h_{\\imath}}(s_{i}^{\\prime}|s_{\\imath},a_{\\imath})=\\frac{1}{\\overline{{S}}}+\\frac{\\epsilon^{\\prime}}{\\overline{{S}}}v_{i}^{\\imath}\\quad\\forall i\\in\\mathbb{[}\\overline{{S}}\\mathbb{]},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\pmb{v}_{i}^{\\imath}$ represents the $i$ -th component of the $\\imath$ -th vector in $\\pmb{v}$ . In words, the $i^{\\th}$ -th component of vector $\\pmb{v}^{\\imath}\\in\\overline{{\\mathcal{V}}}$ creates a bias of $\\epsilon^{\\prime}/\\overline{{S}}$ towards the next state $s_{i}^{\\prime}$ for all $i\\in[\\overline{{S}}]$ . Since $v^{\\iota}\\in\\overline{{\\mathcal{V}}}$ , then $p_{h_{\\imath}}(\\cdot|s_{\\imath},a_{\\imath})\\in\\Delta^{\\mathbb{I}\\overline{{S}}\\mathbb{I}}$ for all $\\iota\\in\\mathcal{T}$ . ", "page_idx": 41}, {"type": "text", "text": "We consider non-stationary reward functions. Specifically, all the rewards $r\\in\\mathfrak{R}$ that we consider assign reward 1 to both triples $\\bar{\\iota}$ and $\\bar{\\j}$ , i.e., $r_{h_{\\overline{{{\\tau}}}}}(\\bar{s_{\\overline{{{\\tau}}}}},a_{\\overline{{{\\tau}}}})=\\dot{1}$ and $r_{h_{\\overline{{{\\jmath}}}}}(s_{\\overline{{{\\jmath}}}},a_{\\overline{{{\\jmath}}}})=1$ . Next, for any other triple $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[\\![H]\\!]$ with state different from $s_{1}^{\\prime},s_{2}^{\\prime},\\ldots,s_{\\overline{{S}}}^{\\prime},$ , we assign reward 0. For states $s_{1}^{\\prime},s_{2}^{\\prime},\\ldots,s_{\\overline{{S}}}^{\\prime},$ , we consider state-only rewards whose value is always 0 in stages $[1,\\overline{{H}}+d]$ , and whose value is stationary and arbitrary afterwards. Intuitively, as in [12], forcing the reward to be 0 up $h=\\overline{{H}}+d$ guarantees that we cannot obtain a higher expected return $J$ by reaching the leaves states earlier (i.e., by exiting from $s_{\\mathrm{w}}$ before $\\overline{H}$ ). ", "page_idx": 41}, {"type": "text", "text": "Given the definition above, we construct the class of instances $\\mathbb{M}\\,:=\\,\\{\\mathcal{M}_{\\boldsymbol{v},\\imath}\\,:\\,\\imath\\,\\in\\,\\mathcal{Z},\\boldsymbol{v}\\,\\in\\,\\overline{{\\nu}}^{\\mathcal{Z}}\\}$ . Moreover, we will use the notation Mv \u0131w,\u0237 to denote the instance in which we replace the $\\textit{\\textbf{\\i}}$ component of $\\pmb{v}$ , i.e., $\\pmb{v}^{\\i}$ , with $w\\in\\mathcal{V}$ and $\\mathcal{M}_{v\\leftarrow0,\\jmath}$ the instance in which we replace the $\\imath$ component of $\\pmb{v}$ , i.e., $v^{\\i}$ , with the zero vector. Since we will always use this notation when substituting triple $\\jmath$ , i.e., we always use this notation in situations as Mv\u0237w,\u0237, then we omit the second parameter, and write just Mv \u0131w :\u201c Mv \u0131w,\u0237. ", "page_idx": 41}, {"type": "text", "text": "Distance between problems Consider an arbitrary problem instance $\\mathcal{M}_{\\pmb{v},\\imath}\\in\\mathbb{M}$ , for certain $\\imath\\in\\mathcal{T}$ and $\\pmb{v}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ . Let $r\\in\\mathfrak{R}$ be an arbitrary reward function that satisfies the constraints described earlier. Let $\\pi_{\\bar{\\imath}}\\in\\Pi$ be the deterministic policy that brings to triple \u0131. Then, its expected return is: ", "page_idx": 41}, {"type": "equation", "text": "$$\nJ^{\\pi_{\\overline{{{\\tau}}}}}(r;\\mathcal{M}_{v,\\imath})=1+\\frac{H-\\overline{{{H}}}-d}{\\overline{{{S}}}}\\sum_{i=1}^{\\overline{{{S}}}}r_{i},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $r_{i}:=r_{\\overline{{H}}+d+1}(s_{i}^{\\prime})$ for all $i\\in[\\overline{{S}}]$ . Let policy $\\pi_{\\imath}\\in\\Pi$ be the deterministic policy that brings to triple $\\imath$ . Then, its expected return is : ", "page_idx": 42}, {"type": "equation", "text": "$$\nJ^{\\pi_{*}}(r;\\mathcal{M}_{v,\\iota})=1+\\frac{H-\\overline{{H}}-d}{\\overline{{S}}}\\sum_{i=1}^{\\overline{{S}}}r_{i}+\\epsilon^{\\prime}\\frac{(H-\\overline{{H}}-d)}{\\overline{{S}}}\\sum_{i=1}^{\\overline{{S}}}v_{i}^{\\iota}r_{i}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Finally, let policy $\\pi_{\\mathcal{I}}\\in\\Pi$ be the deterministic policy that brings to any other triple $\\jmath\\in\\mathbb{Z}\\backslash\\{\\imath\\}$ . Then, its expected return is: ", "page_idx": 42}, {"type": "equation", "text": "$$\nJ^{\\pi_{2}}(r;\\mathcal{M}_{v,\\iota})=0+\\frac{H-\\overline{{H}}-d}{\\overline{{S}}}\\sum_{i=1}^{\\overline{{S}}}r_{i}+\\epsilon^{\\prime}\\frac{(H-\\overline{{H}}-d)}{\\overline{{S}}}\\sum_{i=1}^{\\overline{{S}}}v_{i}^{\\jmath}r_{i}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "It should be remarked that $\\begin{array}{r}{\\left(v,r\\right)=\\sum_{i\\in[\\overline{{S}}]}v_{i}r_{i}\\in[-\\overline{{S}},\\overline{{S}}]}\\end{array}$ for any $r\\in\\mathfrak{R}$ and $v\\in{\\overline{{\\mathcal{V}}}}$ , therefore, as long as: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\epsilon^{\\prime}(H-\\overline{{{H}}}-d)<1-\\epsilon^{\\prime}(H-\\overline{{{H}}}-d)-\\epsilon\\iff\\epsilon^{\\prime}<\\frac{1-\\epsilon}{2(H-\\overline{{{H}}}-d)},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "then any policy $\\pi_{\\boldsymbol{\\jmath}}$ is cannot be $\\epsilon$ -optimal in problem $\\mathcal{M}_{v,\\imath}$ , in which, thus, the optimal policy shall be searched for between $\\pi_{\\overline{{\\tau}}}$ and $\\pi_{\\imath}$ . ", "page_idx": 42}, {"type": "text", "text": "Now, consider an arbitrary pair $v,w\\in{\\overline{{\\mathcal{V}}}}$ such that $v\\neq w$ , and an arbitrary triple $\\imath\\in\\mathcal Z$ and vector $\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ . We now compare problem instances $\\mathcal{M}_{v\\leftarrow v}$ and $\\mathcal{M}_{v\\in\\mathbb{Z}_{w}}$ . Among all possible reward functions that satisfy the definition provided in the construction of the hard instances, we find reward $r^{\\prime}$ such that, in every component $i\\in[\\overline{{S}}]$ , satisfies: ", "page_idx": 42}, {"type": "equation", "text": "$$\nr_{i}^{\\prime}={\\left\\{\\begin{array}{l l}{+1}&{{\\mathrm{if}}\\ v_{i}=+1\\wedge w_{i}=-1}\\\\ {-1}&{{\\mathrm{if}}\\ v_{i}=-1\\wedge w_{i}=+1}\\\\ {0}&{{\\mathrm{if}}\\ v_{i}=w_{i}}\\end{array}\\right.}~.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For what we have seen before about class $\\overline{{\\nu}}$ , we know that $\\begin{array}{r}{\\|v-w\\|_{1}=\\sum_{i\\in[\\overline{{S}}]}|v_{i}-w_{i}|\\geqslant\\overline{{S}}/16}\\end{array}$ , thus, since $v,w\\in\\mathcal{V}$ , i.e., their components belong to $\\{-1,+1\\}$ , we know that there are at least $\\overline{{S}}/32$ components of $v,w$ that differ from each other. By using reward $r^{\\prime}$ , we have that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{\\overline{{S}}}\\displaystyle{v_{i}r_{i}^{\\prime}\\geqslant\\frac{\\overline{{S}}}{32}\\geqslant0},}\\\\ {\\displaystyle\\sum_{i=1}^{\\overline{{S}}}w_{i}r_{i}^{\\prime}\\leqslant-\\displaystyle\\frac{\\overline{{S}}}{32}\\leqslant0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "As a consequence, the expected returns of policies $\\pi_{\\overline{{{\\imath}}}}$ and $\\pi_{\\tau}$ in problems $\\mathcal{M}_{v\\leftarrow v}$ and $\\mathcal{M}_{v\\in\\mathbb{-}w}$ are: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle J^{\\pi_{\\tau}}(r^{\\prime};\\mathcal{M}_{v_{\\tau^{+}-v}^{\\tau}})=J^{\\pi_{\\tau}}(r^{\\prime};\\mathcal{M}_{v_{\\tau^{+}-w}^{\\tau}})=1+\\frac{H-\\overline{{H}}-d}{\\overline{{S}}}\\sum_{i=1}^{\\overline{{S}}}r_{i}^{\\prime},}\\\\ {\\displaystyle J^{\\pi_{\\tau}}(r^{\\prime};\\mathcal{M}_{v_{\\tau^{+}-v}^{\\tau}})\\!\\geqslant\\!1+\\frac{H-\\overline{{H}}-d}{\\overline{{S}}}\\sum_{i=1}^{\\overline{{S}}}r_{i}^{\\prime}\\!+\\!\\epsilon^{\\prime}\\frac{(H-\\overline{{H}}-d)}{32},}\\\\ {\\displaystyle J^{\\pi_{\\tau}}(r^{\\prime};\\mathcal{M}_{v_{\\tau^{+}-w}^{\\tau}})\\!\\leqslant\\!1+\\frac{H-\\overline{{H}}-d}{\\overline{{S}}}\\sum_{i=1}^{\\overline{{S}}}r_{i}^{\\prime}\\!-\\!\\epsilon^{\\prime}\\frac{(H-\\overline{{H}}-d)}{32},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "from which we infer that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v^{\\pm}\\!-\\!v})\\geqslant J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v_{-}^{\\pm}\\!-\\!v})=J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v_{-}^{\\pm}\\!-\\!w})\\geqslant J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v_{-}^{\\pm}\\!-\\!w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now, let us choose $\\epsilon^{\\prime}>64\\epsilon/(H-\\overline{{H}}-d)$ . To satisfy also the constraint in Equation (5), we can roughly assume $\\epsilon<1/256$ and set $\\epsilon^{\\prime}=65\\epsilon/(H-\\overline{{H}}-d)$ . Thanks to this choice, observe that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v\\leftarrow v})>J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v\\leftarrow v})+2\\epsilon,}\\\\ &{J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v\\leftarrow w})>J^{\\pi_{\\imath}}(r^{\\prime};\\mathcal{M}_{v\\leftarrow w})+2\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In words, policy $\\pi_{\\tau}$ is optimal in problem $\\mathcal{M}_{v\\leftarrow v}$ , and policy $\\pi_{\\overline{{{\\imath}}}}$ is worse than $2\\epsilon$ -suboptimal in such problem. In addition, observe that policy $\\pi_{\\overline{{{\\imath}}}}$ is\u00d0 optimal in problem $\\mathcal{M}_{v\\in-w}$ , and policy $\\pi_{\\tau}$ is worse than $2\\epsilon$ -suboptimal in such problem. We stress that any stochastic policy in-between $\\pi_{\\imath}$ and $\\pi_{\\overline{{{\\imath}}}}$ cannot be $\\epsilon$ -optimal for both problems. ", "page_idx": 43}, {"type": "text", "text": "To sum up, for the choice of $\\epsilon^{\\prime}$ made earlier, for arbitrary pairs of problems $\\mathcal{M}_{v\\leftarrow v}$ and $\\mathcal{M}_{v\\leftarrow w}$ we have seen that there exist rewards in for which a policy -optimal for probl\u00d0em $\\mathcal{M}_{\\imath,v}$ is \u00d0not $\\epsilon$ -optimal for problem $\\mathcal{M}_{\\imath,w}$ , and vice versa. ", "page_idx": 43}, {"type": "text", "text": "Identifying the underlying problem: RFE. We consider first RFE, and then IRL. ", "page_idx": 43}, {"type": "text", "text": "Let us consider an $(\\epsilon,\\delta)$ -correct algorithm $\\mathfrak{A}$ for RFE, that outputs, for any reward function $r\\in\\mathfrak{R}$ , a policy ${\\widehat{\\pi}}_{r}$ . For simplicity, we consider as output of Algorithm $\\mathfrak{A}$ a function $\\widehat{\\pi}:\\Re\\rightarrow\\Pi$ , that takes in input a  preward and outputs a policy. ", "page_idx": 43}, {"type": "text", "text": "For any $\\iota\\in\\mathcal{T}$ and $\\pmb{v}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ , we can lower bound the error probability as: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta\\geqslant\\underset{\\mathrm{all\\,problem\\,instances\\,},M}{\\operatorname*{sup}}\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}\\biggl(\\underset{r\\in\\mathfrak{R}}{\\operatorname*{sup}}J_{\\mathcal{M}}^{*}(r)-J_{\\mathcal{M}}^{\\hat{\\pi}_{r}}(r)\\geqslant\\epsilon\\biggr)}\\\\ &{\\stackrel{(1)}{\\geqslant}\\underset{\\mathcal{M}\\in\\mathbb{M}}{\\operatorname*{sup}}\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}\\biggl(\\underset{r\\in\\mathfrak{R}}{\\operatorname*{sup}}J_{\\mathcal{M}}^{*}(r)-J_{\\mathcal{M}}^{\\hat{\\pi}_{r}}(r)\\geqslant\\epsilon\\biggr)}\\\\ &{\\stackrel{(2)}{\\geqslant}\\underset{w\\in\\mathcal{V}}{\\operatorname*{max}}\\mathbb{P}_{\\mathcal{M}_{v_{\\varphi-w}^{\\pm}},\\mathfrak{A}}\\biggl(\\underset{r\\in\\mathfrak{R}}{\\operatorname*{sup}}J_{\\mathcal{M}_{v_{\\varphi-w}^{\\pm}}}^{*}(r)-J_{\\mathcal{M}_{v_{\\varphi-w}^{\\pm}}}^{\\hat{\\pi}_{r}}(r)\\geqslant\\epsilon\\biggr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where at (1) we have lower bounded by replacing all possible RFE problem instances with problem instances in $\\mathbb{M}$ , and at (2) we have lower bounded by replacing all instances in $\\mathbb{M}$ with just instances $\\{\\mathcal{M}_{v\\leftarrow w}:w\\in\\overline{{\\mathcal{V}}}\\}$ for the fixed triple $\\imath$ and vector $\\pmb{v}$ . ", "page_idx": 43}, {"type": "text", "text": "For every $\\iota\\in\\mathcal{Z}$ and $\\pmb{v}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ , we define the identification function $\\Psi_{\\boldsymbol{\\imath},\\boldsymbol{v}}$ as the index of the problem $w\\in{\\overline{{\\mathcal{V}}}}$ \u201crecognized\u201d by algorithm $\\mathfrak{A}$ . In symbols: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\Psi_{\\imath,v}:=\\operatorname*{arg\\,min}_{w\\in\\overline{{\\mathcal{V}}}}\\operatorname*{sup}_{r\\in\\Re}J_{\\mathcal{M}_{v\\leftarrow w}}^{*}(r)-J_{\\mathcal{M}_{v\\leftarrow w}}^{\\hat{\\pi}_{r}}(r).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In words, given estimate $\\widehat{\\pi}:\\Re\\rightarrow\\Pi$ returned by algorithm $\\mathfrak{A}$ , the identification function $\\Psi_{\\boldsymbol{\\imath},\\boldsymbol{v}}$ returns the problem in $\\{\\mathcal{M}_{v\\leftarrow w}:w\\in\\overline{{\\mathcal{V}}}\\}$ whose solution $\\pi:\\Re\\rightarrow\\Pi$ is closest to the estimate $\\widehat{\\pi}$ . For what we have seen in the previous paragraph, for any $v,w\\in{\\overline{{\\mathcal{V}}}}$ with $v\\neq w$ , for any fixed $\\iota\\in\\mathcal{T}$ apnd $\\pmb{v}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ , there exists a reward function $r^{\\prime}\\in\\mathfrak{R}$ such that no policy can have expected utility $\\epsilon$ -close to the optimal expected utility of both problems $\\mathcal{M}_{v\\leftarrow v}$ and $\\mathcal{M}_{v\\in-w}$ . Therefore, for $w\\in{\\overline{{\\mathcal{V}}}}$ , we have the following inclusion of events: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\{\\Psi_{\\iota,v}\\neq w\\}\\subseteq\\Big\\{\\operatorname*{sup}_{r\\in\\Re}J_{\\mathcal M_{v\\cdot\\iota_{w}}}^{*}(r)-J_{\\mathcal M_{v\\cdot\\iota_{w}}^{\\iota}}^{\\hat{\\pi}_{r}}(r)>\\epsilon\\Big\\}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We can continue to lower bound the probability as: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w\\in\\overline{{\\mathcal{V}}}}{\\operatorname*{max}}\\mathbb{P}_{\\mathcal{M}_{v_{\\ast-w}^{\\bot}},\\mathfrak{A}}\\Bigg(\\underset{r\\in\\mathfrak{P}}{\\operatorname*{sup}}J_{\\mathcal{M}_{v_{\\ast-w}^{\\bot}}}^{*}(r)-J_{\\mathcal{M}_{v_{\\ast-w}^{\\bot}}}^{\\widehat{n}_{r}}(r)\\geqslant\\epsilon\\Bigg)\\stackrel{(3)}{\\geqslant}\\frac{1}{|\\overline{{\\mathcal{V}}}|}\\sum_{w\\in\\overline{{\\mathcal{V}}}}\\mathbb{P}_{\\mathcal{M}_{v_{\\ast-w}^{\\bot}},\\mathfrak{A}}\\big(\\Psi_{\\iota,v}\\neq w\\big)}\\\\ &{\\qquad\\qquad\\qquad\\stackrel{(4)}{\\geqslant}1-\\underset{\\log|\\overline{{\\mathcal{V}}}|}{\\frac{1}{\\log|\\overline{{\\mathcal{V}}}|}}\\bigg(\\frac{1}{|\\overline{{\\mathcal{V}}}|}\\sum_{w\\in\\overline{{\\mathcal{V}}}}\\mathrm{KL}\\big(\\mathbb{P}_{\\mathcal{M}_{v_{\\ast-w}^{\\bot}},\\mathfrak{A}},\\mathbb{P}_{\\mathcal{M}_{v_{\\ast-0}^{\\bot}},\\mathfrak{A}}\\big)-\\log2\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where at (3) we have lower bounded the maximum over $\\overline{{\\mathcal{V}}}$ with the average, and at (4) we have applied, similary to [38], the Fano\u2019s inequality, reported in Theorem E.5 for simplicity. ", "page_idx": 43}, {"type": "text", "text": "Identifying the underlying problem: IRL. For IRL, it is possible to carry out a similar derivation. However, we remark that, now, the error is measured based on the expected utilities, and not on the policies. ", "page_idx": 43}, {"type": "text", "text": "Let us consider an $(\\epsilon,\\delta)$ -correct algorithm $\\mathfrak{A}$ for IRL, that outputs, for any reward function $r\\in\\mathfrak{R}$ , a utility ${\\widehat{J}}_{r}$ . For simplicity, we consider as output of Algorithm $\\mathfrak{A}$ a function $\\widehat{J}:\\Re\\rightarrow\\mathbb{R}$ , that takes in input a  preward and outputs a utility. ", "page_idx": 43}, {"type": "text", "text": "For any $\\iota\\in\\mathcal{T}$ and $\\pmb{v}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ , we can lower bound the error probability as: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta\\geqslant\\underset{\\mathrm{all\\,problem\\,instances\\,}}{\\operatorname*{sup}}\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}\\underset{r\\in\\mathfrak{R}}{\\operatorname*{sup}}\\left|J_{\\mathcal{M}}^{*}(r)-\\widehat{J}_{r}\\right|\\geqslant\\epsilon\\biggr)}\\\\ &{\\,\\,\\geqslant\\underset{\\mathcal{M}\\in\\mathbb{N}}{\\operatorname*{sup}}\\,\\mathbb{P}_{\\mathcal{M},\\mathfrak{A}}\\biggl(\\underset{r\\in\\mathfrak{R}}{\\operatorname*{sup}}\\left|J_{\\mathcal{M}}^{*}(r)-\\widehat{J}_{r}\\right|\\geqslant\\epsilon\\biggr)}\\\\ &{\\,\\,\\geqslant\\underset{w\\in\\mathcal{V}}{\\operatorname*{max}}\\,\\mathbb{P}_{\\mathcal{M}_{v^{\\pm}-w}},\\mathfrak{A}\\biggl(\\underset{r\\in\\mathfrak{R}}{\\operatorname*{sup}}\\left|J_{\\mathcal{M}_{v^{\\pm}-w}}^{*}(r)-\\widehat{J}_{r}\\right|\\geqslant\\epsilon\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For any $\\iota\\in\\mathcal{T}$ and $\\pmb{v}\\in\\overline{{\\mathcal{V}}}^{\\mathbb{Z}}$ , we define an identification function $\\Psi_{\\boldsymbol{\\imath},\\boldsymbol{v}}$ as: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\Psi_{\\imath,v}:=\\arg\\operatorname*{min}_{w\\in\\overline{{\\mathcal{V}}}}\\operatorname*{sup}_{r\\in\\Re}\\Big|J_{\\mathcal{M}_{v\\imath-w}}^{*}(r)-\\widehat{J}_{r}\\Big|,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and by a reasoning analogous to that for RFE, we can continue to lower bounding as: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w\\in\\overline{{\\mathcal{V}}}}{\\operatorname*{max}}\\mathbb{P}_{M_{v_{\\star\\leftarrow}^{\\perp}},\\mathfrak{A}}\\bigg(\\underset{r\\in\\mathfrak{R}}{\\operatorname*{sup}}\\left|J_{\\mathcal{M}_{v_{\\star\\leftarrow}^{\\perp}w}}^{*}(r)-\\widehat{J}_{r}\\right|\\geqslant\\epsilon\\bigg)\\geqslant\\frac{1}{|\\overline{{\\mathcal{V}}}|}\\sum_{w\\in\\overline{{\\mathcal{V}}}}\\mathbb{P}_{M_{v_{\\star\\leftarrow}^{\\perp}},\\mathfrak{A}}\\big(\\Psi_{\\iota,v}\\neq w\\big)}\\\\ &{\\qquad\\qquad\\qquad\\geqslant1-\\underset{\\log|\\overline{{\\mathcal{V}}}|}{\\operatorname*{lim}}\\bigg(\\frac{1}{|\\overline{{\\mathcal{V}}}|}\\sum_{w\\in\\overline{{\\mathcal{V}}}}\\mathrm{KL}(\\mathbb{P}_{M_{v_{\\star\\leftarrow}^{\\perp}w}},\\mathfrak{A},\\mathbb{P}_{M_{v_{\\star\\leftarrow}^{\\perp}},\\mathfrak{A}})-\\log2\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which represents the same lower bound obtained also for RFE. ", "page_idx": 44}, {"type": "text", "text": "KL-divergence computation The following derivation is analogous to that of [38]. To bound the KL-divergence term, for any $\\imath\\in\\mathcal{T}$ , we can write: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\mathbb{P}_{\\mathcal{M}_{v^{\\frac{\\lambda}{2}}w},\\mathfrak{M}},\\mathbb{P}_{\\mathcal{M}_{v^{\\frac{\\lambda}{2}}0},\\mathfrak{M}})\\overset{(1)}{=}\\underset{M_{v^{\\frac{\\lambda}{2}}w},\\mathfrak{M}}{\\mathbb{E}}\\left[N_{h_{1}}^{\\tau}(s_{1},a_{i})\\right]\\mathrm{KL}(p_{h_{1}}^{M_{v^{\\frac{\\lambda}{2}}w}}(\\cdot|s_{1},a_{1}),p_{h_{1}}^{M_{v^{\\frac{\\lambda}{2}}0}}(\\cdot|s_{i},a_{i}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(2)}{\\leqslant}2(\\epsilon^{\\prime})^{2}\\underset{M_{v^{\\frac{\\lambda}{2}}w},\\mathfrak{M}}{\\mathbb{E}}\\left[N_{h_{1}}^{\\tau}(s_{1},a_{1})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where at (1) we have applied Lemma E.7, and at (2) we have applied Lemma E.6 (having observed that the transition models differ in $\\imath$ and defined $\\begin{array}{r}{\\bar{N}_{h_{\\i}}^{\\tau}(s_{\\i},a_{\\i})=\\dot{\\sum_{t=1}^{\\tau}}\\,\\mathbb{1}\\big\\lbrace(s_{t},a_{t},h_{t})=\\bar{(s_{\\i},a_{\\i},h_{\\i})}\\big\\rbrace;}\\end{array}$ ). Plugging into Equation (6), we get: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\Bigg\\langle\\geqslant\\frac{1}{|\\overline{{\\mathcal{V}}}|}\\sum_{w\\in\\overline{{\\mathcal{V}}}}\\mathbb{P}_{M_{v^{\\pm}w},\\mathfrak{A}}\\left(\\Psi_{\\iota,v}\\neq w\\right)\\implies\\frac{1}{|\\overline{{\\mathcal{V}}}|}\\sum_{w\\in\\overline{{\\mathcal{V}}}}\\mathcal{M}_{v^{\\pm}w}\\mathbb{E}_{\\mathfrak{A}}\\left[N_{h_{1}}^{\\tau}(s_{1},a_{1})\\right]\\geqslant\\frac{(1-\\delta)\\log|\\overline{{\\mathcal{V}}}|-\\log2}{2(\\epsilon^{\\prime})^{2}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Notice that, since $|{\\overline{{\\mathcal{V}}}}|=\\Theta(e^{S})$ and $\\epsilon^{\\prime}=\\Theta(\\epsilon/H)$ , then this bound is in the order of $\\Omega\\big(\\frac{H^{2}S}{\\epsilon^{2}}\\big)$ . To get the additional $\\Omega({\\dot{S}}A H)$ dependence, we can make the same observation as in [38], i.e., that ince the derivation is carried out for every $\\iota\\in\\mathcal{T}$ and v P V , we can perform the summation over $\\textit{\\textbf{\\i}}$ and the average over $\\pmb{v}$ . By noticing that we get a guarantee on a mean under the uniform distribution of the instances of the sample complexity, we realize that there must exist one $v^{\\mathrm{hard}}\\in\\overline{{\\mathcal{V}}}$ for which it holds the desired $\\Omega\\left({\\frac{H^{3}S^{2}A}{\\epsilon^{2}}}\\right)$ dependency. ", "page_idx": 44}, {"type": "text", "text": "E.2.1 Technical Tools ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We report here some results from other works. The notation adopted is the same as the original works. ", "page_idx": 44}, {"type": "text", "text": "Lemma E.4 (Lemma E.6 of [38]). Let $\\begin{array}{r}{\\mathcal{V}=\\{v\\in\\{-1,1\\}^{D}:\\sum_{j=1}^{D}v_{j}=0\\}}\\end{array}$ . Then, the $\\frac{D}{16}$ -packing number of $\\nu$ w.r.t. the metric $\\begin{array}{r}{d(v,v^{\\prime})=\\sum_{j=1}^{D}|v_{j}-v_{j}^{\\prime}|}\\end{array}$ is low er bounded by $2^{\\frac{D}{5}}$ . ", "page_idx": 44}, {"type": "text", "text": "Theorem E.5. (Theorem E.2 of [38]) Let $\\mathbb{P}_{0},\\mathbb{P}_{1},\\ldots,\\mathbb{P}_{M}$ be probability measures on the same measurable space $(\\Omega,{\\mathcal{F}})$ , and let $A_{1},\\ldots,A_{M}\\in{\\mathcal{F}}$ be a partition of $\\Omega$ . Then, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{i=1}^{M}\\mathbb{P}_{i}(\\pmb{\\mathscr{A}}_{i}^{c})\\geqslant1-\\frac{\\frac{1}{M}\\sum_{i=1}^{M}D_{K L}(\\mathbb{P}_{i},\\mathbb{P}_{0})-\\log2}{\\log M},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Lemma E.6 (Lemma E.4 of [38]). Let $\\epsilon\\,\\in\\,[0,1/2]$ and $\\mathbf{v}\\;\\in\\;\\{-\\epsilon,\\epsilon\\}^{D}$ such that $\\textstyle\\sum_{i=1}^{d}v_{i}\\ =\\ 0$ Consider the two categorical distributions $\\begin{array}{r}{\\mathbb{P}=\\left(\\frac{1}{D},\\frac{1}{D},\\cdot\\cdot\\cdot,\\frac{1}{D}\\right)}\\end{array}$ and $\\begin{array}{r}{\\mathbb{P}=\\left(\\frac{1+v_{1}}{D},\\frac{1+v_{2}}{D},\\cdot\\cdot\\cdot,\\frac{1+v_{D}}{D}\\right)}\\end{array}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\nD_{K L}(\\mathbb{P},\\mathbb{Q})\\leqslant2\\epsilon^{2}\\qquad a n d\\qquad D_{K L}(\\mathbb{Q},\\mathbb{P})\\leqslant2\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Lemma E.7 (Lemma 5 of [12]). Let $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ be two MDPs that are identical except for their transition probabilities, denoted by $p_{h}$ and $p_{h}^{\\prime}$ , respectively. Assume that we have $\\forall(s a),\\,p_{h}(\\cdot|s,a)\\ll$ $p_{h}^{\\prime}(\\cdot|s,a)$ . Then, for any stopping time $\\tau$ with respect to $(\\mathcal{F}_{H}^{t})_{t\\geqslant1}$ that satisfies $\\mathbb{P}_{\\mathcal{M}}\\tau<\\infty=1$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\nK L\\Big(\\mathcal{P}_{\\mathcal{M}}^{I_{H}^{\\tau}},\\mathcal{P}_{\\mathcal{M}^{\\prime}}^{I_{H}^{\\tau}}\\Big)=\\sum_{s\\in\\mathcal{S}}\\sum_{a\\in\\mathcal{A}}\\sum_{h\\in[\\![H-1]\\!]}\\mathbb{E}\\left[N_{h,s,a}^{\\tau}\\right]\\!K L\\Big(p_{h}(\\cdot|s,a),p_{h}^{\\prime}(\\cdot|s,a)\\Big),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\begin{array}{r}{N_{h,s,a}^{\\tau}\\;:=\\;\\sum_{t=1}^{\\tau}\\mathbb{1}\\{(S_{h}^{t},A_{h}^{t})\\;=\\;(s,a)\\}}\\end{array}$ and $\\begin{array}{r}{I_{H}^{\\tau}:\\Omega\\to\\bigcup_{t\\geqslant1}\\mathcal{Z}_{H}^{t}\\,:\\,\\omega\\,\\mapsto\\,I_{H}^{\\tau(\\omega)}(\\omega)}\\end{array}$ is the random vector re presenting the history up to episode $\\tau$ . ", "page_idx": 45}, {"type": "text", "text": "F A Use Case for Objective-Free Exploration (OFE) ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Consider the following setting. You are given a certain MDP without reward $\\mathcal{M}=(S,\\mathcal{A},H,d_{0},p)$ , in which you do not know neither $d_{0}$ nor $p$ . Your job is to explore the environment to collect samples that allow you to construct estimates $\\hat{d}_{0}\\approx d_{0}$ and ${\\widehat{p}}\\approx p$ , that will be subsequently used to perform a task in a given class $\\mathcal{F}$ in an $(\\epsilon,\\delta)$ -c oprrect mann epr. Of course the number of samples should be as small as possible. How do you explore? It depends on which problems are contained in class $\\mathcal{F}$ . ", "page_idx": 45}, {"type": "text", "text": "A use case for OFE is the following. ", "page_idx": 45}, {"type": "text", "text": "Example F.1. Assume that we are given a single fixed environment (for instance, a warehouse), in which there are many tasks to do (e.g., labelling objects, putting stuff on the shelves, bringing products from one side to the other), and assume (it is reasonable) that it is desirable to have one robot for each task. To teach these robots how to behave, we decide to use RL. Since all the robots work in the same environment (warehouse), then the (unknown) transition model is the same. For this reason, an efficient exploration (potentially through RFE) is meaningful. However, we realize that some tasks are difficult to design (i.e., the rewards of such tasks). For these tasks, we prefer to use a human expert to exhibit demonstrations, and then use ReL (in particular, IRL), to learn the reward, that will be subsequently used for AL. To perform IRL nicely, the samples collected at the beginning shall be used. To sum up, we might be interested in performing multiple RL and IRL tasks in the same unknown MDP, and, for efficiency reasons, our exploration of the environment has to be performed only once (before) being given the tasks to solve. ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The Reviewer can find the list of contributions in Section 1, and the related content in the rest of the paper. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The Reviewer can find a \u201cLimitations\u201d paragraph in Section 7. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: The Reviewer can find the full set of complete proofs in the supplemental material. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 48}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 49}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The research conducted in the paper does not harm human subjects or participants, and there are no data-related concerns. In addition, we believe that there are no noteworthy potential harmful consequences of our research. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: This paper represents foundational research, and it is not tied to particular applications. In addition, we believe that there is no direct path to any negative applications. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 51}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 51}]