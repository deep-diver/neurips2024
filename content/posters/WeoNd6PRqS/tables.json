[{"figure_path": "WeoNd6PRqS/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of capabilities of different models. We include several representative methods here. Our OMG-LLaVA offers the most comprehensive capabilities, encompassing image-level, object-level, and pixel-level understanding and reasoning. Compared to [87; 35], OMG-LLaVA features an elegant and simple system architecture with only a single visual encoder.", "description": "This table compares the capabilities of various Multimodal Large Language Models (MLLMs), focusing on their ability to handle image-level, object-level, and pixel-level tasks.  It highlights the unique features of OMG-LLaVA, which is the proposed model in the paper, emphasizing its comprehensive capabilities and simpler architecture compared to other models.", "section": "2 Related Work"}, {"figure_path": "WeoNd6PRqS/tables/tables_3_1.jpg", "caption": "Table 1: Comparison of capabilities of different models. We include several representative methods here. Our OMG-LLaVA offers the most comprehensive capabilities, encompassing image-level, object-level, and pixel-level understanding and reasoning. Compared to [87; 35], OMG-LLaVA features an elegant and simple system architecture with only a single visual encoder.", "description": "This table compares the capabilities of various Multimodal Large Language Models (MLLMs) focusing on image-level, object-level, and pixel-level understanding and reasoning.  It highlights the unique capabilities of OMG-LLaVA, which achieves comprehensive capabilities with a simpler architecture than comparable models.", "section": "2 Related Work"}, {"figure_path": "WeoNd6PRqS/tables/tables_7_1.jpg", "caption": "Table 2: The comprehensive comparison of OMG-LLaVA and other MLLMs regarding pixel-level and object-level understanding and reasoning capability and performance. \"-\" indicates that the method does not handle this task. \u2020 indicates that the method used the GranD dataset [87] for pretraining, which is significantly larger than the datasets used by other methods.", "description": "This table compares the performance of OMG-LLaVA against other state-of-the-art multimodal large language models (MLLMs) on various tasks, including pixel-level and object-level understanding and reasoning.  It highlights OMG-LLaVA's comprehensive capabilities, showing its performance on multiple benchmarks (COCO, VIPSeg, refCOCO, refCOCO+, refCOCOg, GCG) and noting the number of visual encoders each model uses.  The table also indicates which models used the larger GranD dataset for pre-training.", "section": "4 Experiment"}, {"figure_path": "WeoNd6PRqS/tables/tables_7_2.jpg", "caption": "Table 3: Performance on referring expression segmentation datasets. The evaluation metric is cIoU. \"ft\" indicates finetuning on the referring expression datasets.", "description": "This table presents the performance of different models on three referring expression segmentation datasets: refCOCO, refCOCO+, and refCOCOg.  The models' performance is measured using the cIoU (Intersection over Union) metric. The table also indicates whether the model was fine-tuned (\"ft\") specifically on these datasets, showing the impact of dataset-specific training.  A higher cIoU score indicates better performance.", "section": "4 Experiment"}, {"figure_path": "WeoNd6PRqS/tables/tables_7_3.jpg", "caption": "Table 4: Performance on grounded conversation generation datasets. \"ft\" indicates finetuning on the GranDf [87] dataset. \u2020 indicates that the method used the GranD dataset [87] for pretraining.", "description": "This table compares the performance of different methods on the grounded conversation generation (GCG) task.  It shows the METEOR, CIDEr, AP50, and mIOU scores for each method.  The \"ft\" column indicates whether the method was fine-tuned on the GranDf dataset, and the \u2020 symbol indicates whether the method used the GranD dataset for pre-training.  The results demonstrate the relative performance of OMG-LLaVA and other models on this specific task.", "section": "4 Experiment"}, {"figure_path": "WeoNd6PRqS/tables/tables_7_4.jpg", "caption": "Table 5: Ablation study on RES and GCG datasets.", "description": "This table presents the ablation study results on the referring expression segmentation (RES) and grounded conversation generation (GCG) datasets.  It shows the impact of different modifications to the model architecture (perception prior embedding and object query input) on the performance metrics (cIoU, gIoU, METEOR, mIoU) for both tasks across multiple datasets (refCOCO, refCOCO+, refCOCOg).  The results demonstrate how these modifications improve the overall performance of the model. ", "section": "4 Experiment"}, {"figure_path": "WeoNd6PRqS/tables/tables_22_1.jpg", "caption": "Table 1: Comparison of capabilities of different models. We include several representative methods here. Our OMG-LLaVA offers the most comprehensive capabilities, encompassing image-level, object-level, and pixel-level understanding and reasoning. Compared to [87; 35], OMG-LLaVA features an elegant and simple system architecture with only a single visual encoder.", "description": "This table compares the capabilities of various Multimodal Large Language Models (MLLMs) focusing on their ability to handle image-level, object-level, and pixel-level tasks.  It highlights that OMG-LLaVA stands out by possessing all three levels of understanding and reasoning with a simpler system architecture than its counterparts (e.g., only one visual encoder).", "section": "2 Related Work"}, {"figure_path": "WeoNd6PRqS/tables/tables_22_2.jpg", "caption": "Table 7: Performance with different LLMs.", "description": "This table compares the performance of OMG-LLaVA using three different LLMs: Phi-3 3.8B, InternLM2-7B, and Qwen2-7B.  The performance metrics include cIoU and GIoU for refCOCO and refCOCO+ datasets, perception and reasoning scores for the MME dataset, and various other scores for SEED-Bench, POPE, AI2D, MMStar, and SQA benchmarks.  It shows how the choice of LLM affects the overall performance of the OMG-LLaVA model across different tasks and datasets.", "section": "4 Main Results"}, {"figure_path": "WeoNd6PRqS/tables/tables_23_1.jpg", "caption": "Table 8: Ablation study of projector for object-centric visual tokens.", "description": "This table presents the ablation study results focusing on the vision projector used for object-centric visual tokens. It compares different configurations, including using cross-attention and individual MLPs, and evaluates their impact on various metrics such as cIoU, gIoU, and METEOR across different datasets (refCOCO, refCOCO+, refCOCOg, and refCOCOg(C)). The results show the effect of different projector designs on the model's performance, helping to understand the contribution of the object-centric visual tokens in the proposed architecture.", "section": "4.2 Ablation and Analysis"}, {"figure_path": "WeoNd6PRqS/tables/tables_23_2.jpg", "caption": "Table 9: Ablation study on answer format of segmentation-based tasks. The first row represents the RES task using the fixed answer: \"Sure, it is [SEG].\" and the GCG task using \"<p> Expression </p> [SEG].\" The second row represents the segmentation tasks' answer format being unified as \"<p> Expression </p> [SEG].", "description": "This table shows the ablation study results for two different answer formats used in referring expression segmentation (RES) and grounded conversation generation (GCG) tasks.  The first row uses a fixed answer format, while the second row uses a more flexible format where the segmentation mask is enclosed within expression tags. The results show the impact of the answer format on the performance metrics (cIoU, gIoU, METEOR, AP50, mIoU).", "section": "A.2 More Detailed Ablation Studies"}, {"figure_path": "WeoNd6PRqS/tables/tables_23_3.jpg", "caption": "Table 3: Performance on referring expression segmentation datasets. The evaluation metric is cIoU. \"ft\" indicates finetuning on the referring expression datasets.", "description": "This table presents the performance of different methods on three referring expression segmentation datasets: refCOCO, refCOCO+, and refCOCOg.  The evaluation metric used is the cIoU (Intersection over Union). The table shows the performance with and without fine-tuning (ft) on the referring expression datasets, indicating whether the model was specifically trained for this task or used pre-trained weights.", "section": "4 Experiment"}]