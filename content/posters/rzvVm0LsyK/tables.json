[{"figure_path": "rzvVm0LsyK/tables/tables_8_1.jpg", "caption": "Table 1: Top-1 accuracy (%) for ImageNet classification by SwinTransformer.", "description": "This table presents the top-1 accuracy results for ImageNet classification using the Swin Transformer model.  The results are shown for two different epochs (200 and 300) and for three different optimizers: AdamW, AMSGrad, and ADOPT.  The table highlights the superior performance of ADOPT compared to the other optimizers in achieving higher accuracy.", "section": "5 Experiments"}, {"figure_path": "rzvVm0LsyK/tables/tables_8_2.jpg", "caption": "Table 2: Negative log-likelihood of NVAEs for MNIST density estimation. Lower is better.", "description": "This table presents the results of negative log-likelihood for training NVAE models on the MNIST dataset using two different optimizers: Adamax and ADOPT.  The negative log-likelihood is a metric used to evaluate the performance of generative models, with lower values indicating better performance.  The table shows the results at epochs 200 and 300, allowing for a comparison of performance over time.", "section": "5 Experiments"}, {"figure_path": "rzvVm0LsyK/tables/tables_14_1.jpg", "caption": "Table 3: Comparison of the problem settings between our analysis and other existing works.", "description": "This table compares various optimization algorithms (Adam, AMSGrad, and ADOPT) across different papers, focusing on the problem type (finite sum or general), smoothness assumptions (Lipschitz conditions on the gradient or true gradient), and gradient growth conditions (bounds on the gradient norm or variance).  The table highlights the differences in the assumptions used in each analysis, which can significantly influence the convergence results.", "section": "A Detailed Relationships to Existing Analyses"}, {"figure_path": "rzvVm0LsyK/tables/tables_14_2.jpg", "caption": "Table 4: Comparison of the convergence rate and imposed constraints on the hyperparameters between our analysis and other existing works. Please refer to the original papers for the definitions of \u03b3 and c.", "description": "This table compares the convergence rates and hyperparameter constraints derived by different studies on Adam and related adaptive gradient methods.  It contrasts the assumptions made (and their implications) for each analysis,  highlighting the differences in convergence guarantees obtained under varying conditions and constraints on hyperparameters beta1 and beta2.  The 'Ours' row represents the findings and constraints (or lack thereof) presented in this particular research paper.", "section": "A Detailed Relationships to Existing Analyses"}, {"figure_path": "rzvVm0LsyK/tables/tables_15_1.jpg", "caption": "Table 5: Recommended hyperparameters for the ADOPT algorithm", "description": "This table presents the recommended hyperparameter settings for the ADOPT algorithm.  It suggests using \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.9999, and \u03b5 = 1 \u00d7 10\u207b\u2076.  These values were experimentally determined to provide similar performance to Adam, but with a slightly larger \u03b5 value (1 \u00d7 10\u207b\u2076 vs. 1 \u00d7 10\u207b\u2078 for Adam) for better results.", "section": "D Recommendation of Hyperparameter Settings for ADOPT"}]