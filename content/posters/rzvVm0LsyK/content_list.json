[{"type": "text", "text": "ADOPT: Modified Adam Can Converge with Any $\\beta_{2}$ with the Optimal Rate ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shohei Taniguchi The University of Tokyo taniguchi@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Keno Harada The University of Tokyo keno.harada@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Gouki Minegishi The University of Tokyo minegishi@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Yuta Oshima The University of Tokyo yuta.oshima@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Seong Cheol Jeong The University of Tokyo jeong@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Go Nagahara The University of Tokyo nagaharago@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Tomoshi Iiyama The University of Tokyo iiyama@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Masahiro Suzuki The University of Tokyo masa@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Yusuke Iwasawa The University of Tokyo iwasawa@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Yutaka Matsuo The University of Tokyo matsuo@weblab.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., $\\beta_{2}$ , in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradi\u221aent method named ADOPT, which achieves the optimal convergence rate of $\\mathcal{O}(1/\\sqrt{T})$ with any choice of $\\beta_{2}$ without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/iShohei220/adopt. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic optimization algorithms, such as stochastic gradient descent (SGD), play a central role in deep learning. In particular, adaptive gradient methods based on exponential moving averages, such as Adam [Kingma and Ba, 2014], are widely used in practice. Despite the empirical success, it is ", "page_idx": 0}, {"type": "text", "text": "known that Adam does not converge in theory in general cases. For example, Reddi et al. [2018] show that Adam fails to converge to a correct solution in a simple example where the objective function at time $t$ is given as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nf_{t}\\left(\\theta\\right)=\\left\\{C\\theta,\\;\\;\\;\\mathrm{for}\\;t\\;\\mathrm{mod}\\;3=1\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $C\\,>\\,2$ and $\\theta\\in\\left[-1,1\\right]$ . In this online optimization setting, Adam converges to a wrong solution (i.e., $\\theta=1$ ) instead of the true solution (i.e., $\\theta=-1$ ) especially when the hyperparameter $\\beta_{2}$ is set to a small value. There have been several attempts to fix the non-convergent behavior of Adam [Reddi et al., 2018, Zou et al., 2019]. For example, AMSGrad [Reddi et al., 2018] ensures the convergence for online convex optimization by making slight modifications to the Adam algorithm. Subsequent studies [Chen et al., 2019, Zhou et al., 2018] show that AMSGrad also converges to a stationary point for smooth nonconvex stochastic optimization problems. However, the convergence proofs rely on the assumption that the gradient noise is uniformly bounded. This assumption is stronger than the one used for the analysis of vanilla SGD [Ghadimi and Lan, 2013, Bertsekas and Tsitsiklis, 2000, Khaled and Richt\u00e1rik, 2023], where the gradient variance is assumed to be uniformly bounded. In fact, the bounded noise assumption is often violated in practice. For example, when Gaussian noise is used in the gradient estimation (e.g., variational autoencoders [Kingma and Welling, 2014] and diffusion models [Ho et al., 2020, Song et al., 2021]), the stochastic gradient is no longer bounded. ", "page_idx": 1}, {"type": "text", "text": "Concurrently, Zhou et al. [2019] analyze the non-convergence of Adam in the problem described in Eq. (1) from the perspective of the correlation between the current gradient and the second moment estimate based on the exponential moving average. Specifically, they show that the non-convergence problem can be resolved by excluding the gradient of some recent steps from the calculation of the second moment estimate. Based on the analysis, they propose AdaShift, another variant of Adam. However, their theoretical analysis is limited to a single online convex problem described in Eq. (1), and the convergence of AdaShift for general nonconvex problems is unclear. ", "page_idx": 1}, {"type": "text", "text": "More recently, some works have demonstrated that Adam can converge by choosing $\\beta_{2}$ in a problemdependent manner [Shi et al., 2020, Zhang et al., 2022, Wang et al., 2022, Li et al., 2023, Wang et al., 2023]. However, tuning $\\beta_{2}$ for each specific problem is troublesome; hence developing algorithms with the problem-independent convergence guarantee is still important to safely apply adaptive gradient methods to a wide range of machine learning problems. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose an alternative approach to addressing the non-convergence problem of Adam without relying on the choice of $\\beta_{2}$ or strong assumptions such as the bounded noise assumption. To derive our algorithm, we first examine the case without momentum, analyzing the convergence bound of RMSprop for general smooth nonconvex optimization problems. Through the analysis, we uncover the fundamental cause of non-convergence, which stems from the correlation between the second moment estimate and the current gradient. This finding aligns with the results demonstrated by Zhou et al. [2019] for online convex optimization. This correlation can be easily eliminated by excluding the current gradient from the second moment estimate. ", "page_idx": 1}, {"type": "text", "text": "Subsequently, we extend our findings to the case where momentum is incorporated, as in Adam, and discover that the Adam-style momentum also contributes to non-convergence. To address it, we propose to change the order of the momentum update and the normalization by the second moment estimate. With this small adjustment, we successfully eliminate the non-convergence problem of Adam without relying on a specific hyperparameter choice and the bounded noise assumption. We provide theoretical evidence demonstrating \u221athat our derived algorithm, named ADOPT, can achieve convergence with the optimal rate of ${\\mathcal{O}}(1/{\\sqrt{T}})$ for smooth nonconvex optimization. ", "page_idx": 1}, {"type": "text", "text": "In our experiments, we begin by assessing the performance of ADOPT in a toy example where Adam typically fails to converge depending on the choice of $\\beta_{2}$ . This toy example is an extension of the one presented in Eq. (1) by Reddi et al. [2018], but we consider a scenario where AMSGrad is also hard to converge due to the dependence on the bounded noise assumption. Our results demonstrate that ADOPT rapidly converges to the solution, while Adam fails to converge, and AMSGrad exhibits extremely slow convergence. Next, we conduct an experiment using a simple multi-layer perceptron on the MNIST classification task to evaluate the performance of ADOPT in nonconvex optimization. Our findings indicate that ADOPT outperforms existing adaptive gradient methods, including Adam, AMSGrad, and AdaShift. Finally, we evaluate the performance of ADOPT in various practical applications, such as image classification of CIFAR-10 and ImageNet using ResNet [He et al., 2016] and SwinTransformer [Liu et al., 2021], training of deep generative models (NVAE), fine-tuning of language models (LLaMA), and deep reinforcement learning for continuous control. Our empirical results demonstrate that ADOPT achieves superior results over existing algorithms (e.g., Adam) in these practical applications. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the minimization of the objective function $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$ with respect to the parameter $\\pmb\\theta\\in\\mathbb{R}^{D}$ . In this context, we focus on first-order stochastic optimization methods, where only the stochastic gradient $\\textbf{\\textit{g}}$ is accessible. As the objective $f$ can be nonconvex, the goal is to find a stationary point where $\\nabla f\\left(\\pmb{\\theta}\\right)=0$ [Blair, 1985, Vavasis, 1995]. In order to analyze the convergence behavior of stochastic optimization algorithms, the following assumptions are commonly employed in the literature: ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1. The objective function $f(\\pmb\\theta)$ is lower-bounded, i.e., $f(\\pmb\\theta)\\geq f_{\\mathrm{inf}}>-\\infty$ for all $\\pmb{\\theta}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.2. The stochastic gradient $\\scriptstyle g_{t}$ is an unbiased estimator of the objective $f(\\pmb\\theta_{t-1})$ , i.e., $\\mathbb{E}[\\pmb{g}_{t}]=\\nabla f(\\pmb{\\theta}_{t-1})$ for all $t\\geq1$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.3. The objective function is $L$ -smooth on $\\mathbb{R}^{D}$ , i.e., there exists a constant $L>0$ such that $\\|\\nabla f(\\pmb{x})-\\nabla f(\\pmb{y})\\|\\leq L\\|\\pmb{x}-\\pmb{y}\\|$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{D}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.4. Variance of the stochastic gradient is uniformly bounded , i.e., there exists a constant $\\sigma>0$ such that $\\mathbb{E}[\\left\\|\\pmb{\\mathscr{g}}_{t}-\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right\\|^{2}]\\leq\\sigma^{2}$ . ", "page_idx": 2}, {"type": "text", "text": "For the analysis of adaptive gradient methods (e.g., Adam and AdaGrad), many of previous works [D\u00e9- fossez et al., 2022, Li and Orabona, 2019, Ward et al., 2020, Zou et al., 2018] use a little stronger assumption instead of Assumption 2.4 for ease of proofs: ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.5. The stochastic gradient has a finite second moment, i.e., there exists a constant $G>0$ such that $\\mathbb{E}[\\|\\pmb{g}_{t}\\|^{2}]\\leq G^{2}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.5 requires that the true gradient $\\nabla f$ is also uniformly bounded in addition to the variance of the stochastic gradient $\\textbf{\\textit{g}}$ . Moreover, the convergence proof of AMSGrad tends to rely on an even stronger assumption as follows [Chen et al., 2019, Zhou et al., 2018]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.6. The stochastic gradient is uniformly upper-bounded, i.e., there exists a constant $G>0$ such that $\\|g_{t}\\|\\leq G$ . ", "page_idx": 2}, {"type": "text", "text": "In Assumption 2.6, the gradient noise $\\xi_{t}\\,:=\\,g_{t}\\,-\\,\\nabla f$ is assumed to be bounded almost surely in addition to the true graidient $\\nabla f$ . Note that when Assumption 2.6 holds, Assumption 2.5 is automatically satisfied; hence, Assumption 2.6 is a stronger assumption compared to Assumption 2.5. In this paper, we adopt Assumptions 2.1, 2.2, 2.3 and 2.5 for analysis, because one of our motivations is to address the omission of Assumption 2.6. In the analysis, we derive the upper bound of $\\operatorname*{min}_{t}\\{\\mathbb{E}[\\|\\nabla f(\\pmb{\\theta}_{t}))\\|^{4/3}]^{3/2}\\}$ to investigate the convergence rate of the stochastic optimization algorithms, which is commonly performed in the literature [D\u00e9fossez et al., 2022, Zou et al., 2019]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Review of Stochastic Optimization Algorithms for Nonconvex Objectives ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The convergence of the vanilla SGD have been studied extensively in previous works. For smooth nonconvex functions, Gh\u221aadimi and Lan [2013] showed that SGD with a constant learning \u221arate converges with an $\\mathcal{O}(1/\\sqrt{T})$ rate under Assumptions 2.1-2.4 by setting $\\alpha_{t}\\,=\\,\\alpha\\,=\\,\\Theta(1/\\sqrt{T})$ , where $\\alpha_{t}$ is a learning rate at the $t$ -th step, and $T$ is a total number of parameter updates. This convergence rate is known to be minimax optimal up to a constant [Drori a\u221and Shamir, 2020]. For the diminishing learning rate scheme, the convergence bound of ${\\mathcal{O}}(\\log T/{\\sqrt{T}})$ is well-known for $\\alpha_{t}=\\alpha/\\sqrt{t}$ [Ghadimi and Lan, 2013]. Recently, \u221aWang et al. [2021] have proved that SGD with $\\alpha_{t}=\\alpha/\\sqrt{t}$ can also achieve the optimal rate ${\\mathcal{O}}(1/{\\sqrt{T}})$ by additionally assuming that the objective $f$ is upper-bounded. ", "page_idx": 2}, {"type": "text", "text": "While the vanilla SGD is still one of the most popular choices for stochastic optimization, adaptive gradient methods are dominantly used especially for deep learning. In adaptive gradient methods, the parameter $\\pmb{\\theta}$ is updated additionally using the second moment estimate $\\pmb{v}_{t}$ in the following form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t}=\\pmb{\\theta}_{t-1}-\\alpha_{t}\\frac{\\pmb{g}_{t}}{\\sqrt{\\pmb{v}_{t}+\\epsilon^{2}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon$ is a small positive constant. The division between vectors is applied in an element-wise manner, and the addition between a vector $\\textbf{\\em a}$ and a scalar $b$ is defined as $(a+b)_{i}:=a_{i}+b$ . In AdaGrad [Duchi et al., 2011], $\\pmb{v}_{t}$ is defined as $\\pmb{v}_{0}=\\mathbf{0}$ and $\\pmb{v}_{t}=\\pmb{v}_{t-1}+\\pmb{g}_{t}\\odot\\pmb{g}_{t}$ . In RMSprop [Hinton et al., 2012], an exponential moving average is substituted for the simple summation, i.e., $\\pmb{v}_{t}\\,=$ $\\beta_{2}\\pmb{v}_{t-1}+(1-\\beta_{2})\\pmb{g}_{t}\\odot\\pmb{g}_{t}$ , where $0\\leq\\beta_{2}<1$ . Adam [Kingma and Ba, 2014] uses momentum in addition to the second moment estimate to accelerate the convergence as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\pmb{m}}_{t}=\\beta_{1}{\\pmb{m}}_{t-1}+\\left(1-\\beta_{1}\\right){\\pmb{g}}_{t},}\\\\ {{\\pmb{\\theta}}_{t}={\\pmb{\\theta}}_{t-1}-\\alpha_{t}\\frac{{\\pmb{m}}_{t}}{\\sqrt{{\\pmb{v}}_{t}+\\pmb{\\epsilon}^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $m_{0}\\,=\\,{\\bf0}$ . Here, we omit the bias correction technique used in the original paper for clarity. Unfortunately, RMSprop and Adam are not guaranteed to converge even in a simple convex optimization problem as demonstrated by Reddi et al. [2018], whereas AdaGrad with a constant learning rate is known to converge with an ${\\mathcal{O}}(\\log T/{\\sqrt{T}})$ rate under Assupmtions 2.1-2.3 and 2.5 for smooth nonconvex cases [Li and Orabona, 2019, Ward et al., 2020, Zou et al., 2018, Chen et al., 2019, D\u00e9fossez et al., 2022]. Although the convergence of Adam can be assured by choosing $\\beta_{2}$ in a problem-dependent manner [Shi et al., 2020, Zhang et al., 2022, Wang et al., 2022, Li et al., 2023, Wang et al., 2023], it is difficult to know the proper choice of $\\beta_{2}$ for each problem before training. ", "page_idx": 3}, {"type": "text", "text": "To fix the non-convergence of Adam without depending on $\\beta_{2}$ , some researchers have proposed variants of Adam. Reddi et al. [2018] proposed AMSGrad, which substitute $\\hat{\\pmb v}_{t}$ for $\\pmb{v}$ in Eq. (3\u221a), where $\\hat{\\pmb v}_{0}=\\mathbf0$ and $\\hat{\\mathbf{v}}_{t}=\\operatorname*{max}\\left\\{\\hat{{v}}_{t-1},{v}_{t}\\right\\}$ . The idea behind AMSGrad is that the scaling factor $\\sqrt{\\hat{\\pmb{v}}_{t}+\\epsilon^{2}}$ should be non-decreasing to ensure the convergence. After Reddi et al. [2018] originally proved the convergence of AMSGrad for online convex optimization, Chen et al. [2019] showed that AMSGrad with $\\alpha_{t}\\,=\\,\\alpha/\\sqrt{t}$ converges with ${\\mathcal{O}}(\\log T/{\\sqrt{T}})$ for nonconvex settings. Zhou et al. [2018] also analyzed \u221athe convergence of AMSGrad for nonconvex optimiz\u221aation, and derived the convergence rate of $\\mathcal{O}(1/\\sqrt{T})$ for a constant learning rate of $\\alpha_{t}=\\alpha=\\Theta(1/\\sqrt{T})$ . However, their results depend on Assumption 2.6, which is often violated in practice. For example, variational autoencoders [Kingma and Welling, 2014] and diffusion models [Ho et al., 2020, Song et al., 2021] are typical examples in which Assumption 2.6 does not hold because they utilize unbounded Gaussian noise in the gradient estimation. The cause of requirement for Assumption 2.6 is the max operation in the definition of $\\hat{\\pmb v}_{t}$ Since the max operation is convex, $\\mathbb{E}[\\hat{\\pmb{v}}_{t}]\\leq\\operatorname*{max}_{t}\\{\\mathbb{E}[\\pmb{v}_{t}]\\}$ does not hold; hence Assumption 2.6 is required to upper-bound $\\mathbb{E}[\\hat{\\pmb{v}}_{t}]$ in their proofs. ", "page_idx": 3}, {"type": "text", "text": "Zhou et al. [2019] also tried to fix the non-convergent behavior of Adam. Their proposed AdaShift uses $v_{t-n}$ instead of $\\pmb{v}_{t}$ for the second moment estimate, and calculate the momentum using the latest $n$ gradients as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{t}=\\frac{\\sum_{k=0}^{n-1}\\beta_{1}^{k}g_{t-k}}{\\sum_{k=0}^{n-1}\\beta_{1}^{k}},}\\\\ &{\\theta_{t}=\\pmb{\\theta}_{t-1}-\\alpha_{t}\\frac{m_{t}}{\\sqrt{\\pmb{v}_{t-n}+\\epsilon^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the original paper, some additional techniques (e.g., the block-wise adaptive learning rate) are used, but we omit them for clarity here. Though they give theoretical analysis for a single online convex example, any convergence bounds are not provided for nonconvex cases. More detailed discussion on existing analyses is provided in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3 Analysis: Cause of Non-convergence of Adam and How to Fix It ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, to derive an algorithm that can converge with any $\\beta_{2}$ without Assumption 2.6, we analyze the cause of non-convergence of Adam, and discuss how it can be eliminated. To start from a simple case, we first analyze the case without momentum. Subsequently, we extend it to the case with momentum and provide a way to fix the convergence issue of Adam. ", "page_idx": 3}, {"type": "text", "text": "3.1 Case without Momentum ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first analyze the convergence of RMSprop, which corresponds to the no-momentum case of Adam when we omit the bias correction. For RMSprop, we derive the following convergence bound. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Under Assumptions 2.1-2.3 and 2.5, the following holds for the RMSprop with $a$ constant learning rate $\\alpha_{t}=\\alpha$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t=1,\\ldots,T}\\left\\{\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\theta_{t-1}))\\right\\Vert^{4/3}\\right]^{3/2}\\right\\}\\leq C_{1}\\left(\\frac{f_{0}-f_{\\mathrm{inf}}}{\\alpha T}+\\frac{C_{2}}{T}\\log\\left(1+\\frac{G^{2}}{\\epsilon^{2}}\\right)-C_{2}\\log\\beta_{2}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Sketch of proof. By Assumption 2.3, the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(\\pmb{\\theta}_{t}\\right)\\right]\\leq\\mathbb{E}\\left[f\\left(\\pmb{\\theta}_{t-1}\\right)+\\frac{\\alpha^{2}L}{2}\\left\\|\\frac{\\pmb{g}_{t}}{\\sqrt{\\pmb{v}_{t}+\\epsilon^{2}}}\\right\\|^{2}-\\alpha\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)^{\\top}\\left(\\frac{\\pmb{g}_{t}}{\\sqrt{\\pmb{v}_{t}+\\epsilon^{2}}}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Applying Lemmas G.4 and G.6 in the appendix to this, the following inequality is derived: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\theta_{t}\\right)\\right]}\\\\ &{\\leq\\mathbb{E}\\left[f\\left(\\theta_{t-1}\\right)+\\left(\\frac{\\alpha^{2}L}{2}+2\\alpha G\\sqrt{1-\\beta_{2}}\\right)\\left\\Vert\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\right\\Vert^{2}-\\frac{\\alpha}{2}\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\left(\\frac{g_{t}}{\\sqrt{\\tilde{v}_{t}+\\epsilon^{2}}}\\right)\\right]}\\\\ &{\\leq\\mathbb{E}\\left[f\\left(\\theta_{t-1}\\right)+\\left(\\frac{\\alpha^{2}L}{2}+2\\alpha G\\sqrt{1-\\beta_{2}}\\right)\\left\\Vert\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\right\\Vert^{2}\\right]-\\frac{\\alpha}{2}\\frac{\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\right\\Vert^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{T}\\right)G^{2}+\\epsilon^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{\\pmb{v}}_{t}=\\beta_{2}\\pmb{v}_{t-1}+(1-\\beta_{2})\\mathbb{E}[\\pmb{g}_{t}\\odot\\pmb{g}_{t}]$ . Telescoping this for $t=1,\\ldots,T$ and rearranging the terms, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\right\\Vert^{4/3}\\right]^{3/2}\\leq C_{1}\\left(\\frac{f\\left(\\theta_{0}\\right)-f_{\\mathrm{inf}}}{\\alpha}+C_{2}\\log{\\left(\\frac{G^{2}+\\epsilon^{2}}{\\beta_{2}^{T}\\epsilon^{2}}\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the last inequality holds due to Assumption 2.1 and Lemma G.5. Therefore, the bound in Eq.   \n(7) is derived using $\\begin{array}{r}{\\operatorname*{min}_{t=1,\\dots,T}\\{\\mathbb{E}[\\|\\nabla f(\\pmb{\\theta}_{t-1}))\\|^{4/3}]^{3/2}\\}\\le\\sum_{t=1}^{T}\\mathbb{E}[\\|\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\|^{4/3}]^{3/2}/T}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "A deta\u221ailed proof is provided in the appendix. When the learning rate $\\alpha$ is chosen so th\u221aat $\\alpha=$ $\\Theta(1/{\\sqrt{T}})$ , the first and second terms on the right hand side of Eq. (7) converge with $\\mathcal{O}(1/\\sqrt{T})$ and $\\mathcal{O}(1/T)$ rates, respectively. However, the last term includes a constant factor in terms of $T$ , which represents the non-convergent behavior of RMSprop in the smooth nonconvex setting. More precisely, RMSprop is guaranteed to converge only to a bounded region around a stationary point, and the size of the bounded region depends on the hyperparameter $\\beta_{2}$ and the problem-dependent factors $D,G$ , and $L$ . Therefore, we need to choose $\\beta_{2}$ d\u221aependently on each problem to make the bounded region adequately small. Since $\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{\\beta_{2}\\to1}\\log\\beta_{2}/\\sqrt{1-\\beta_{2}}\\stackrel{\\cdot}{=}0}}\\end{array}$ , the size of the bounded region can be made small by setting $\\beta_{2}$ to a value close to 1, which aligns with practical observations. However, how close to 1 it should be relies on the problem-dependent factors, which cannot be observed in advance. This result is consistent with recent results of convergence analyses of Adam and RMSprop [Shi et al., 2020, Zhang et al., 2022]. ", "page_idx": 4}, {"type": "text", "text": "As can be seen from Eqs. (8) and (9), the constant term in Eq. (7) is derived from the last term of Eq. (8). Because $\\scriptstyle g_{t}$ and $\\pmb{v}_{t}$ are not statistically independent, this term is first decomposed as in Eq. (9). After the decomposition, $\\scriptstyle g_{t}$ and $\\tilde{\\pmb{v}}_{t}$ is now conditionally independent given $g_{0},\\ldots,g_{t-1}$ , so Eq. (10) is derived using the following fact: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{g_{t}}{\\sqrt{\\tilde{v}_{t}+\\epsilon^{2}}}\\right]=\\mathbb{E}\\left[\\frac{\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)}{\\sqrt{\\tilde{v}_{t}+\\epsilon^{2}}}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This indicates that, if the second moment estimate $\\pmb{v}_{t}$ is designed to be conditionally independent to $\\scriptstyle g_{t}$ , the constant term in the convergence bound will be removed, because the second term of Eq. (8) ", "page_idx": 4}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/8168f1e461455f9c0a8af1ad633c174baebbf5f5105746bd4f268681b971dcc8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "can be directly lower-bounded without the decomposition. A simple way to achieve the conditional independence is to substitute $v_{t-1}$ for $\\pmb{v}_{t}$ as a second moment estimate, because $v_{t-1}$ does not have information about $\\scriptstyle g_{t}$ . This solution is similar to AdaShift, in which $\\pmb{v}_{t-n}$ is substituted for $\\pmb{v}_{t}$ as described in Eq. (5). In fact, the modified version of RMSprop is identical to AdaShift with $n=1$ and $\\beta_{1}=0$ except for the additional techniques (e.g., the block-wise adaptive learning rate). ", "page_idx": 5}, {"type": "text", "text": "3.2 Case with Momentum ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As we have described, RMSprop can be modified to be convergent by removing the current gradient $\\scriptstyle g_{t}$ from the second moment estimate $\\pmb{v}_{t}$ . However, when we combine adaptive gradient methods with momentum like Adam, the convergence analysis becomes more complicated. Unfortunately, when Adam-style momentum in Eq. (3) is applied, the algorithm does not converge in general even when using $v_{t-1}$ as a second moment estimate instead of $\\pmb{v}_{t}$ . This is because the momentum $\\mathbf{\\nabla}m_{t}$ contains all history of the past gradients $g_{0},\\ldots,g_{t}$ ; hence the second moment estimate always correlates with $\\mathbf{\\nabla}m_{t}$ . AdaShift prevents this problem by calculating the momentum $\\mathbf{\\nabla}m_{t}$ only using the latest $n$ gradients as described in Eq. (5). In that case, the momentum $\\mathbf{\\nabla}m_{t}$ and the second moment estimate $v_{t-n}$ are conditionally independent, so the convergence can be retained. However, this approach has a trade-off in the choice of $n$ . When $n$ is small, $\\mathbf{\\nabla}m_{t}$ has little information about the past gradients; when $n$ is large, $\\pmb{v}_{t-n}$ only has access to the gradient information in the distant past. ", "page_idx": 5}, {"type": "text", "text": "To remove this trade-off, instead of truncating the momentum to the latest $n$ steps, we propose to use momentum of the following form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle m_{t}=\\beta_{1}{\\pmb m}_{t-1}+\\left(1-\\beta_{1}\\right)\\frac{g_{t}}{\\sqrt{{\\pmb v}_{t-1}+\\epsilon^{2}}}},}\\\\ {{\\displaystyle\\pmb\\theta_{t}=\\pmb\\theta_{t-1}-\\alpha_{t}{\\pmb m}_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The main difference to the Adam-style momentum in Eq. (3) is the order of update of $\\mathbf{\\nabla}m_{t}$ and the normalization by $\\sqrt{v_{t-1}+\\epsilon^{2}}$ . In Eq. (3), the normalization is performed after the update of $\\mathbf{\\nabla}m_{t}$ , whereas in Eq. (13), the normalization is first applied to the current gradient $\\scriptstyle g_{t}$ in advance to the update of $\\mathbf{\\nabla}m_{t}$ . In this case, the second moment estimate $v_{t-1}$ is only used to normalize the current gradient $\\scriptstyle g_{t}$ , so the convergence can be guaranteed. A more detailed convergence analysis is provided in Section 4. ", "page_idx": 5}, {"type": "text", "text": "4 Method: Adaptive Gradient Method with the Optimal Convergence Rate ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on the analysis in the previous section, we propose a new adaptive gradient method named ADOPT (ADaptive gradient method with the OPTimal convergence rate). The entire procedure is summarized in Algorithm 4. For a simple discription, we place the update of $\\mathbf{\\nabla}m$ after the p\u221aarameter update in Algorithm 4, but it is equivalent to Eqs. (13) and (14) except that max $\\{\\sqrt{v},\\epsilon\\}$ is substitued for $\\sqrt{\\pmb{v}+\\epsilon^{2}}$ . The substitition is applied because we find that it contributes to slightly better performance in practice. We provide an equivalent expression of Algorithm 4 in Algorithm C in the appendix, which is closer to a practical implementation. By this modification, ADOPT can converge with the optimal rate for smooth nonconvex optimization as follows: ", "page_idx": 5}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/d3d48a8f1d09075d81af1e1258b83816460c089ecc0fa173ba228bd861adf786.jpg", "img_caption": ["Figure 1: Performance comparison between Adam, AMSGrad and ADOPT in a simple univariate convex optimization problem. The plots show transitions of the parameter value, which should converge to the solution $\\theta=-1$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. Under Assumptions 2.1-2.3 and 2.5, the following holds for the ADOPT algorithm with a constant learning rate $\\alpha_{t}=\\alpha=\\Theta\\left(1/\\sqrt{T}\\right)$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t=1,\\ldots,T}\\left\\{\\mathbb{E}\\left[\\left\\|\\nabla f(\\pmb{\\theta}_{t-1}))\\right\\|^{4/3}\\right]^{3/2}\\right\\}\\leq\\mathcal{O}\\left(1/\\sqrt{T}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The detailed proof and related lemmas are provided in the appe\u221andix. We also provide the convergence bound for the case of diminishing learning rate (i.e., $\\alpha_{t}=\\alpha/\\sqrt{t})$ in the appendix, which is closer to practical situations. In that case, ADOPT also converges with the optimal rate. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the experiments, we first validate our ADOPT algorithm using a simple toy example in which Adam is known to fail to converge, and confirm our theoretical findings through numerical simulation. Secondly, we run an experiment of training a simple multi-layer perceptron (MLP) for the MNIST dataset to verify the effectiveness of our ADOPT for nonconvex optimization problems. Finally, we evaluate our ADOPT in a wide range of practical applications, including image classification, natural language processing (NLP) tasks, generative modeling, and deep reinforcement learning. Detailed experimental settings are described in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Toy problem: We consider a convex optimization problem with an objective $f(\\theta)=\\theta$ for $\\theta\\in[-1,1]$ . It is obvious that a solution for the problem is $\\theta=-1$ . Through the optimization, we only have access to the stochastic objective $f_{t}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{t}\\left(\\theta\\right)=\\binom{k^{2}\\theta,}{-k\\theta,}\\quad\\mathrm{~with~probability~}1/k\\quad\\quad\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $k\\geq1$ . Because $\\mathbb{E}[f_{t}(\\theta)]=f(\\theta)$ holds, the stochastic gradient $g_{t}=\\nabla f_{t}(\\theta)$ is an unbiased estimator of the true gradient $\\nabla f$ regardless of the choice of $k$ , satisfying Assumption 2.2. This problem is equivalent, except for scaling, to the stochastic optimization version of Eq. (1) provided by Reddi et al. [2018] as a case where Adam fails to converge. In this setting, the constant $k$ controls the magnitude of gradient noise. When $k=1$ , it corresponds to the noiseless case where $f_{t}=f$ with probability 1. As $k$ gets large, stochastic gradient becomes noisy, making $G$ in Assumptions ", "page_idx": 6}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/43fc8869359ec6f27026023332393d27cc39f8992203e711774b458463bf2da6.jpg", "img_caption": ["Figure 2: Accuracy for training data (left) and test data(right) in MNIST classification. The error bars show the $95\\%$ confidence intervals of three trials. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "2.5 and 2.6 large. Therefore, the optimization will be more difficult when $k$ becomes larger. In the experiment, we set $k=10$ or 50, and compare the robustness of Adam, AMSGrad, and ADOPT for various hyperparameter settings by changing $\\beta_{2}$ from $0.1\\sim0.999$ . We set $\\beta_{1}=0.9$ for all the algorithms, which is a common choice in practice. We set the learning rate to $\\alpha_{t}=0.01/\\sqrt{1+0.01t}$ ", "page_idx": 7}, {"type": "text", "text": "The result is shown in Figure 1. It can be seen that, when $k=10$ , Adam fails to converge except for $\\beta_{2}=0.999$ while AMSGrad and ADOPT rapidly converge to the correct solution, i.e., $\\theta=-1$ , with any $\\beta_{2}$ . In a more extreme case where $k=50$ , Adam fails to converge even with $\\beta_{2}=0.999$ This aligns with Theorem 3.1, since, when the gradient noise is large (i.e., $G$ is large), the bounded region of the convergence bound also gets large, leading to divergence of Adam. Moreover, when $k=50$ , it is observed that the convergence of AMSGrad also becomes much slower than ADOPT. In fact, this phenomenon is also consistent with theory. In this problem setting, the second moment $\\mathbb{E}[g_{t}^{2}]$ is $\\mathcal{O}(\\bar{k}^{3})$ , while the squared norm of the stochastic gradient $g_{t}^{2}$ is ${\\mathcal O}(k^{4})$ . Since the convergence bound of AMSGrad depends on the uniform bound of the stochastic gradient in Assumption 2.6, instead of the second moment in Assumption 2.5, its convergence also deteriorates with the order of $g_{t}^{2}$ . Compared to AMSGrad, ADOPT only depends on the second moment bound for its convergence, so it converges much faster than AMSGrad even in such an extreme setting. ", "page_idx": 7}, {"type": "text", "text": "We also perform ablation study on how the two algorithmic changes from Adam to ADOPT affect the convergence. The differences between Adam and ADOPT are (1) decorrelation between the second moment estimate and the current gradient, and (2) change of order of momentum update and normalization by the second moment estimate. In this experiment, we remove each algorithmic change from ADOPT, and compare the result in the toy example. We set $k=50$ , and $(\\beta_{1},\\beta_{2})=(0.9,0.999)$ , since it is a common hyperparameter choice. The result is shown in Figure 3. It can be observed that ADOPT fails to converge with the exception of either algorithmic change. Therefore, applying both changes is essential to overcome the non-convergence of Adam, which also aligns with theory. These results correspond to the theoretical findings, showing the superiority of ADOPT to Adam and AMSGrad in terms of the convergence speed and its robustness to hyperparameter choices. ", "page_idx": 7}, {"type": "text", "text": "MNIST classification: To investigate the effectiveness of ADOPT on nonconvex optimization, we train nonlinear neural networks for MNIST classification tasks, and compare the performance between ADOPT and existing optimization algorithms, such as Adam, AMSGrad and AdaShift. In this experiment, we use a simple MLP with a\u221a single hidden layer, and the number of hidden units is set to 784. We set the learning rate to $\\alpha_{t}=\\alpha/\\sqrt{t}$ , and $\\alpha$ is tuned in the range of $\\{1,10^{-1},10^{-2},10^{-3}\\}$ . We apply weight decay of $1\\times10^{-4}$ to prevent over-fitting, and run 10K iterations of parameter updates. Figure 2 shows the learning curves of training and test accuracy. We observe our ADOPT performs slightly better than the others in terms of the convergence speed and the final performance. ", "page_idx": 7}, {"type": "text", "text": "Image classification: As a more practical application, we conduct experiments of image classification using real-world image datasets. We first compare ADOPT and Adam in the classification task of the CIFAR-10 dataset using ResNet-18 [He et al., 2016], a widely-used convolutional neural network. We conduct a similar hyperparameter search to the case of MNIST classification. A detailed experimental setting is provided in the appendix. The learning curves of test accuracy are visualized in Figure 4. It can be observed that ADOPT converges a little faster than Adam. ", "page_idx": 7}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/29b798b3239547c1fef7e81b73e475fb906b84e4fec51a15eee079f8610e590d.jpg", "img_caption": ["Figure 3: Ablation study of algorithmic changes between Adam and ADOPT. \"DE\" and CO denote \"decorrelation\" and \"change of order\", respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/3e6709603b6e4dc8c228d9c2d7d6e458463e10f836188705defcba4a5989cda0.jpg", "img_caption": ["Figure 4: Learning curves of test accuracy for CIFAR-10 classification by ResNet-18 trained with Adam and ADOPT. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "rzvVm0LsyK/tmp/c395245e6539063466da2de66ade9febd3871acfdc81da44a45af53731980027.jpg", "table_caption": ["Table 1: Top-1 accuracy $(\\%)$ for ImageNet classification by SwinTransformer. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "rzvVm0LsyK/tmp/84aba2d2eb04587be4ec89b05c6dce054b70e28b0d81e0f12e753b737d9e3b42.jpg", "table_caption": ["Table 2: Negative log-likelihood of NVAEs for MNIST density estimation. Lower is better. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "To confirm that our ADOPT works well for modern neural network architectures based on Transformers [Vaswani et al., 2017], we perform an experiment of ImageNet classification using SwinTransformer [Liu et al., 2021]. We follow the official training recipe of Swin Transformer-tiny provided by Torchvision [Paszke et al., 2019a], and fix the training settings except for the optimizer choice. We use AdamW [Loshchilov and Hutter, 2019] as a baseline because it is set as the default official optimizer. We also compare with AMSGrad as another way to fix the non-convergence issue of Adam. Since AdamW uses decoupled weight decay, we also apply it to the other optimizers for fair comparison. We report the top-1 accuracy at 200 and 300 epochs in Tables 1. We observe that ADOPT outperforms AdamW and AMSGrad throughout the training in terms of the test accuracy, demonstrating the effectiveness of ADOPT for this setting. ", "page_idx": 8}, {"type": "text", "text": "Generative modeling: We train NVAE [Vahdat and Kautz, 2020] for MNIST using our ADOPT. In the official implementation of NVAE, Adamax [Kingma and Ba, 2014], an infinite-norm variant of Adam, is used as an optimizer, so we use Adamax as a baseline method. We use the exactly the same setting of the official implementation except that the learning rate for ADOPT is set to $2\\times\\dot{1}0^{-4}$ since the default value 0.01 is too large for ADOPT. We report the negative log-likelihood for test data on Table 2. It is observed that the model trained with ADOPT shows the better likelihood. ", "page_idx": 8}, {"type": "text", "text": "Pretraining of large language models: We run a pre-training of GPT-2 [Radford et al., 2019] using the nanoGPT [Karpathy, 2022] code base to compare Adam and ADOPT. We use OpenWebText [Gokaslan and Cohen, 2019] as the training data. Experimental setup conforms to the default settings of nanoGPT except for the selection of the optimizer. We also test a case in which the total batch size was changed from 480 to 96, as a setting where the gradient noise becomes larger. The results are summarized in Figure 5. The most notable finding is that in the small batch size case, Adam causes loss spikes in the early stages of training and fails to converge, while ADOPT is always able to train stably. This is consistent with Adam\u2019s theory of non-convergence. As the gradient noise increases, $G$ in Theorem 3.1 also increases, and the constant term in Adam\u2019s convergence bounds becomes non-negligible especially when using a large-scale dataset like OpenWebText. As a result, Adam is more likely to fail to train in such cases. Our ADOPT, on the other hand, does not suffer from this problem because it can always guarantee convergence. We also observed that both Adam and ADOPT work well when the batch size is large, but even in this case, ADOPT performs slightly better. ", "page_idx": 8}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/04c9ad59ee8f540ca043921f1b3dced831718a60b008c5e41f480ad49db4da94.jpg", "img_caption": ["Figure 5: Learning curves of GPT-2 pretraining for training set (left) and validation set (right). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Finetuning of large language models: We finetune the pretrained LLaMA-7B on 52K instructionfollowing data provided by Stanford Alpaca and compare the performance between the default optimizer (Adam) and our ADOPT under the exactly same experimental setting. For evaluation, we use Multi-task Language Understanding (MMLU) Benchmark [Hendrycks et al., 2021], which is widely used to assess the performance of large language models. The MMLU score for LLaMA-7B without finetuning is 35.1. After fine-tuned via instruction-following using the baseline implementation with Adam, the score improves to 41.2. When we substitute ADOPT for Adam, the score even improves to 42.13. The detailed score comparison for each task is summarized in Figure 7 in the appendix. Other experimental results, including deep RL experiments, and detailed experimental settings are also provided in the appendix. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we demystified the fundamental cause of divergence of adaptive gradient methods based on the exponential moving average, such as Adam and RMSprop, in general smooth nonconvex optimization problems, and demonstrate a way to fix the issue, proposing a new optimizer named ADOPT. Not only does ADOPT converge with the optimal rate without depending on a hyperparameter choice in theory, but ADOPT demonstrates better performance in a wide range of pracital applications. ", "page_idx": 9}, {"type": "text", "text": "We expect that this work will serve as a bridge between theory and practice in the research of adaptive gradient methods. Since ADOPT can be safely applied to many machine learning problems without careful tuning of hyperparameters, it can be expected to improve the training stability and the model performance in practice by substituting it for the existing adaptive gradient methods (e.g., Adam). ", "page_idx": 9}, {"type": "text", "text": "One of the limitations of our analysis is that it still relies on the assumption that the second moment of stochastic gradient is uniformly bounded (i.e., Assumption 2.5). Although this assumption is weaker than the bounded stochastic gradient assumption (i.e., Assumption 2.6), it would be more desirable to relax it to the bounded variance assumption (i.e., Assumption 2.4), which is often adopted in the analysis of the vanilla SGD [Ghadimi and Lan, 2013]. For Adam, a recent work by W\u221aang et al. [2023] have derived a problem-dependent convergence bound which achieves the $\\mathcal{O}(1/\\sqrt{T})$ rate without Assumption 2.5. Their proof techniques may help to relax our assumptions in the proof of Theorem 4.1, which we leave as future work. ", "page_idx": 9}, {"type": "text", "text": "From a broader perspective, adaptive gradient methods like Adam have been widely used even for the training of large-scale foundation models (e.g., large language models). Although such models can be useful for people, their negative aspects, such as concerns about copyright infringement, are not negligible. Researchers needs to deeply recognize and understand such social impacts of machine learning algorithms. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. ", "page_idx": 10}, {"type": "text", "text": "Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id $\\equiv$ ryQu7f-RZ.   \nFangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of adam and rmsprop. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11127\u201311135, 2019.   \nXiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1x-x309tm.   \nDongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.   \nSaeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.   \nDimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization, 10(3):627\u2013642, 2000.   \nAhmed Khaled and Peter Richt\u00e1rik. Better theory for SGD in the nonconvex world. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum? id $\\left.=\\right.$ AU4qHN2VkS. Survey Certification.   \nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= PxTIG12RRHS.   \nZhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu. Adashift: Decorrelation and convergence of adaptive learning rate methods. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id $\\mathbf{\\mu=}$ HkgTkhRcKQ.   \nNaichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. Rmsprop converges with proper hyperparameter. In International Conference on Learning Representations, 2020.   \nYushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id $\\equiv$ l5UNyaHqFdO.   \nBohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Zhi-Ming Ma, Tie-Yan Liu, and Wei Chen. Provable adaptivity in adam. arXiv preprint arXiv:2208.09900, 2022.   \nHaochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of adam under relaxed assumptions. arXiv preprint arXiv:2304.13972, 2023.   \nBohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap between the upper bound and lower bound of adam\u2019s iteration complexity. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \nCharles Blair. Problem complexity and method efficiency in optimization (as nemirovsky and db yudin). Siam Review, 27(2):264, 1985.   \nStephen A Vavasis. Complexity issues in global optimization: a survey. Handbook of global optimization, pages 27\u201341, 1995.   \nAlexandre D\u00e9fossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof of adam and adagrad. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id $\\equiv$ ZPQhzTSWA7.   \nXiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In The 22nd international conference on artificial intelligence and statistics, pages 983\u2013992. PMLR, 2019.   \nRachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. The Journal of Machine Learning Research, 21(1):9047\u20139076, 2020.   \nFangyu Zou, Li Shen, Zequn Jie, Ju Sun, and Wei Liu. Weighted adagrad with unified momentum. arXiv preprint arXiv:1808.03408, 2018.   \nYoel Drori and Ohad Shamir. The complexity of finding stationary points with stochastic gradient descent. In International Conference on Machine Learning, pages 2658\u20132667. PMLR, 2020.   \nXiaoyu Wang, Sindri Magn\u00fasson, and Mikael Johansson. On the convergence of step decay step-size for stochastic optimization. Advances in Neural Information Processing Systems, 34:14226\u201314238, 2021.   \nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121\u20132159, 2011. URL http://jmlr.org/papers/v12/duchi11a.html.   \nGeoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6e rmsprop: Divide the gradient by a running average of its recent magnitude, 2012. URL https://www.cs.toronto.edu/ \\~tijmen/csc321/slides/lecture_slides_lec6.pdf.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019a.   \nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id $=$ Bkg6RiCqY7.   \nArash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. Advances in neural information processing systems, 33:19667\u201319679, 2020.   \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\cdot$ d7KBjmI3GmQ.   \nJeff Haochen and Suvrit Sra. Random shuffling beats sgd after finite epochs. In International Conference on Machine Learning, pages 2624\u20132633. PMLR, 2019.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024\u20138035. Curran Associates, Inc., 2019b. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.   \nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1861\u20131870. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html.   \nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012. doi: 10.1109/IROS.2012.6386109.   \nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. The Journal of Machine Learning Research, 22(1):12348\u201312355, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Detailed Relationships to Existing Analyses ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we discuss the relationships between our analysis and existing ones on the convergence of Adam-like optimizers in smooth nonconvex optimization problems. Tables 3 and 4 are a summary of comparisons between them in terms of their problem settings and derived convergence rates. ", "page_idx": 13}, {"type": "text", "text": "Zhang et al. [2022] focus on convergence of Adam in the finite sum problem, where the objective has a following form: ", "page_idx": 13}, {"type": "equation", "text": "$$\nf\\left(\\pmb\\theta\\right)=\\sum_{i=1}^{n}f_{i}\\left(\\pmb\\theta\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "$f_{i}$ is, for example, a loss function for $i$ -th training sample. Although many deep learning problems can be formulated as a finite sum problem, training of the variational autoencoders (VAEs) or diffusion models is out of the finite-sum problem, since their objective is formulated as an infinite sum (i.e., an expectation over continuous variables). Moreover, they assume the stochastic gradient $\\textbf{\\textit{g}}$ is $L$ - Lipschitz, whereas we only assume true gradient $\\nabla f$ is $L$ -Lipschitz. They also assume a growth condition as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert g_{t}\\right\\Vert^{2}\\right]\\leq G_{0}^{2}+G_{1}^{2}\\left\\Vert\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This growth condition is weaker than our Assumption 2.5. Assumption 2.5 is a special case of the growth condition where $G_{1}=0$ . Their derived convergence rate has a constant factor of $\\mathcal{O}(G_{0})$ ; hence the strong growth condition (i.e., $G_{0}=0]$ ) is required to assure convergence. Moreover, to assure convergence, one needs to choose sufficiently large $\\beta_{2}$ , which has to be tuned in a problemdependent manner. ", "page_idx": 13}, {"type": "text", "text": "Wang et al. [2022] also focus on convergence of Adam in the finite sum problem, but they relax the $L$ -Lipschitz condition on $\\textbf{\\textit{g}}$ to the $\\left(L_{0},L_{1}\\right)$ -Lipschitz condition. They also assume the growth condition in Eq. (18), and their convergence rate has the same order with Zhang et al. [2022], so it still requires the strong growth condition (i.e., $G_{0}=0$ ) to assure convergence. The condition of $\\beta_{2}$ is also similar to Zhang et al. [2022]. ", "page_idx": 13}, {"type": "text", "text": "Li et al. [2023] consider Adam\u2019s convergence on general smooth nonconvex problems. Similar to Wang et al. [2022], they use $(L_{0},L_{\\rho})$ -Lipschitz condition on the true gradient $\\nabla f$ . They also assume that the gradient noise is almost surely bounded: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\pmb{\\mathscr{g}}-\\nabla f\\|\\leq\\sigma\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The relationship between this assumption and our Assumption 2.5 is a little complicated. Assumption 2.5 is equivalent to a combination of Assumption 2.4 and the following assumption: ", "page_idx": 13}, {"type": "text", "text": "Assumption A.1. The true gradient is uniformly bounded, i.e., there exist constants $G$ and $\\sigma$ such that $\\left\\|{\\bar{\\nabla}}f\\left(\\pmb{\\theta}\\right)\\right\\|^{2}\\leq G^{2}-\\sigma^{2}$ and $0<\\sigma\\le G$ . ", "page_idx": 13}, {"type": "text", "text": "The bounded noise assumption of Eq. (19) is strictly stronger than Assumption 2.4, but they do not assume the bounded true gradient (i.e., Assumption A.1). The bounded noise assumption is often violated in practice (e.g., training of VAEs), because the gr\u221aadient is often estimated using unbounded noise (i.e., Gaussian noise). Their convergence rate $\\mathcal{O}(1/\\sqrt{T})$ is better than Zhang et al. [2022] and Wang et al. [2022], while it still requires constraints on the hyperparameters, which have to be chosen in a problem-dependent manner. ", "page_idx": 13}, {"type": "text", "text": "D\u00e9fossez et al. [2022] analyzes the c\u221aonvergence of Adam under exactly the same assumptions with ours, and they derive the $\\mathcal{O}(\\log T/\\sqrt{T})$ rate, which is worse than our ADOPT\u2019s convergence rate. Moereover, to assure the convergence, $\\beta_{2}$ has to be chosen dependently on the total number of iterations $T$ . ", "page_idx": 13}, {"type": "text", "text": "Wang et a\u221al. [2023] analyzes the convergence of Adam under Assumptions 2.1-2.4, and they derive the ${\\mathcal{O}}(1/{\\sqrt{T}})$ rate. However, to assure the convergence, $\\beta_{2}$ has to be chosen dependently on the total number of iterations $T$ as in D\u00e9fossez et al. [2022]. ", "page_idx": 13}, {"type": "text", "text": "Chen et al. [2019] and Zhou et al. [2018] analyze the convergence \u221aof AMSGrad fo\u221ar general smooth nonconvex problems, and derive the convergence rate of ${\\mathcal{O}}(\\log T/{\\sqrt{T}})$ and $\\mathcal{O}(1/\\sqrt{T})$ , respectively. However, to guarantee the convergence, the stochastic gradient $\\textbf{\\textit{g}}$ has to be bounded almost surely (Assumption 2.6), which is often \u221aviolated in practice. In addition, the hyperparameter $\\beta_{1}$ and $\\beta_{2}$ should be chosen satisfying $\\beta_{1}<\\sqrt{\\beta_{2}}$ . This constraint is relatively minor compared to the constraint imposed in the analyses of Adam, since it can be satisfied in a problem-independent manner. ", "page_idx": 13}, {"type": "table", "img_path": "rzvVm0LsyK/tmp/050b9f553e8aa87804500cb23092c976d5f513fdd6fd99fd50a341cfc49590fe.jpg", "table_caption": [], "table_footnote": ["Table 3: Comparison of the problem settings between our analysis and other existing works. "], "page_idx": 14}, {"type": "table", "img_path": "rzvVm0LsyK/tmp/9f22bd6658f77012dfc65bbf08b7d1e08e6c012272e416c83d86c8df786f64fa.jpg", "table_caption": ["Table 4: Comparison of the convergence rate and imposed constraints on the hyperparameters between our analysis and other existing works. Please refer to the original papers for the definitions of $\\gamma$ and $c$ . "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B With-Replacement vs. Without-Replacement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the optimization of finite-sum problems, practitioners often use without-replacement sampling, which is also known as random shuffling, to obtain stochastic gradient. In this case, the stochastic gradient has a small bias due to the lack of replacement, so Assumption 2.2 is violated. However, the vanilla SGD is known to converge with the without-replacement strategy [Haochen and Sra, 2019], and some of the analyses of Adam also adopt without-replacement sampling [Zhang et al., 2022, Wang et al., 2022]. ", "page_idx": 14}, {"type": "text", "text": "Unfortunately, we find that our ADOPT has a counter example, in which ADOPT fails to converge when using without-replacement sampling. For example, when we consider minimizing $f(\\theta)\\ =$ $\\textstyle\\sum_{i=1}^{3}f_{i}(\\theta)$ , where $\\theta\\in[-1,1]$ , $f_{1}(\\theta)=1.9\\theta$ and $f_{2}(\\theta)=f_{2}(\\theta)=-\\theta,$ , it can be easily observed that ADOPT with $\\beta_{1}=\\beta_{2}=0$ fails to converge to the correct solution, i.e., $\\theta=1$ . ", "page_idx": 14}, {"type": "text", "text": "This non-convergence can be easily avoided by using the with-replacement strategy. Moreover, the difference between with- and without-replacement sampling becomes negligible when $n$ in the finite-sum $\\textstyle\\sum_{i=1}^{n}f_{i}$ is large enough; hence it does not affect the practical performance very much. In fact, our experiments except for the toy example are performed using without-replacement sampling, but divergent behaviors are not observed. If one applies ADOPT to problems where the difference seems severe (e.g., when training with a small dataset), we recommend to use with-replacement sampling instead of random shuffling for stable training. When one uses PyTorch [Paszke et al., 2019b] for the implementation, for example, with-replacement sampling can be easily applied by specifying replacemnet $=$ True for torch.utils.data.RandomSampler, and feeding it to the sampler argument of torch.utils.data.DataLoader. ", "page_idx": 14}, {"type": "text", "text": "C Another Expression of ADOPT ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Alternative representation of ADOPT algorithm   \nRequire: Learning rate $\\left\\{\\alpha_{t}\\right\\}$ , initial parameter $\\pmb{\\theta}_{0}$   \nRequire: Exponential decay rate $0\\leq\\beta_{1}<1,0\\leq\\beta_{2}\\leq1$ , small constant $\\epsilon>0$ $\\pmb{v}_{0}\\leftarrow\\pmb{g}_{0}\\odot\\pmb{g}_{0}$ for $t=1$ to $T$ do if $t=1$ then $m_{t}\\leftarrow g_{t}/\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}$ else $m_{t}\\leftarrow\\beta_{1}\\cdot m_{t-1}+\\left(1-\\beta_{1}\\right)g_{t}/\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}$ end if \u03b8t \u2190\u03b8t\u22121 \u2212\u03b1tmt $\\pmb{v}_{t}\\leftarrow\\beta_{2}\\cdot\\pmb{v}_{t-1}+\\left(1-\\beta_{2}\\right)\\pmb{g}_{t}\\odot\\pmb{g}_{t}$ end for return $\\{\\pmb\\theta_{t}\\}_{t=1}^{T}$ ", "page_idx": 15}, {"type": "text", "text": "D Recommendation of Hyperparameter Settings for ADOPT ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We experimentally find that our ADOPT works similarly to Adam when the same hyperparameters are used, but $\\epsilon$ should be set to a little larger value (e.g., $1\\times10^{-6},$ ) for ADOPT compared to Adam, in which $\\epsilon$ is set to $1\\times10^{-8}$ by default. Our recommendation of the hyperparameter settings for ADOPT is provided in Table 5. ", "page_idx": 15}, {"type": "table", "img_path": "rzvVm0LsyK/tmp/cad83f48c797d0f280a6d844f8927a4ba2075e8f33bb621cc9a2e0ddd639d801.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 5: Recommended hyperparameters for the ADOPT algorithm ", "page_idx": 15}, {"type": "text", "text": "E Theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem E.1. Under Assumptions 2.1, 2.2, 2.3, and 2.5, if the objective $f$ is \u221aupper-bounded by $f_{\\mathrm{sup}}$ , the following holds for the ADOPT algorithm with a learning rate $\\alpha_{t}=\\alpha/\\sqrt{t}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t=1,\\ldots,T}{\\operatorname*{min}}\\Biggl\\{\\mathbb{E}\\left[\\Vert\\nabla f\\left(\\theta_{t}\\right)\\Vert^{4/3}\\right]^{3/2}\\Biggr\\}}\\\\ &{\\leq\\frac{3\\sqrt{\\operatorname*{max}\\left\\{G^{2},1\\right\\}+\\epsilon^{2}}}{2\\left(\\left(T+1\\right)^{3/2}-1\\right)}\\left(\\frac{f_{\\mathrm{sup}}-f_{\\mathrm{inf}}}{\\alpha}\\left(T+1\\right)+\\left(\\frac{\\sqrt{2}\\alpha\\beta_{1}G^{2}L}{\\epsilon^{2}\\left(1-\\beta_{1}\\right)}+\\frac{\\alpha G^{2}L}{2\\epsilon^{2}}\\right)T\\right)}\\\\ &{\\quad+\\frac{3\\sqrt{\\operatorname*{max}\\left\\{G^{2},1\\right\\}+\\epsilon^{2}}}{2\\left(\\left(T+1\\right)^{3/2}-1\\right)}\\left(\\frac{2\\sqrt{2}\\beta_{1}G^{2}}{\\epsilon\\left(1-\\beta_{1}\\right)}\\left(\\sqrt{T+1}-1\\right)+\\frac{\\alpha\\beta_{1}^{2}G^{2}L}{\\epsilon\\left(1-\\beta_{1}\\right)^{2}}\\frac{T}{T+1}\\right)}\\\\ &{\\quad+\\frac{3\\sqrt{\\operatorname*{max}\\left\\{G^{2},1\\right\\}+\\epsilon^{2}}}{2\\left(\\left(T+1\\right)^{3/2}-1\\right)}\\left(\\frac{2\\alpha\\beta_{1}^{2}G^{2}L}{\\epsilon\\left(1-\\beta_{1}\\right)^{2}}+\\frac{\\alpha^{2}\\beta_{1}G^{2}L}{\\sqrt{2}\\epsilon^{2}\\left(1-\\beta_{1}\\right)}\\right)\\log\\left(T+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "F Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Theorems 4.1 and E.1. We define $\\phi_{t}$ for $t\\geq1$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi_{t}=\\frac{1}{1-\\beta_{1}}\\pmb{\\theta}_{t}-\\frac{\\beta_{1}}{1-\\beta_{1}}\\pmb{\\theta}_{t-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We also define $\\phi_{0}=\\pmb{\\theta}_{0}$ . By Assumption 2.3, the following holds for $t\\geq1$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f\\left(\\phi_{t}\\right)\\leq f\\left(\\phi_{t-1}\\right)+\\nabla f\\left(\\phi_{t-1}\\right)^{\\top}\\left(\\phi_{t}-\\phi_{t-1}\\right)+\\frac{L}{2}\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert^{2}}\\\\ &{\\qquad=f\\left(\\phi_{t-1}\\right)+\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\left(\\phi_{t}-\\phi_{t-1}\\right)}\\\\ &{\\qquad+\\left(\\nabla f\\left(\\phi_{t-1}\\right)-\\nabla f\\left(\\theta_{t-1}\\right)\\right)^{\\top}\\left(\\phi_{t}-\\phi_{t-1}\\right)+\\frac{L}{2}\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert^{2}}\\\\ &{\\leq f\\left(\\phi_{t-1}\\right)+\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\left(\\phi_{t}-\\phi_{t-1}\\right)}\\\\ &{\\qquad+\\left\\Vert\\nabla f\\left(\\phi_{t-1}\\right)-\\nabla f\\left(\\theta_{t-1}\\right)\\right\\Vert\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert+\\frac{L}{2}\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert^{2}}\\\\ &{\\leq f\\left(\\phi_{t-1}\\right)+\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\left(\\phi_{t}-\\phi_{t-1}\\right)}\\\\ &{\\qquad+L\\left\\Vert\\phi_{t-1}-\\theta_{t-1}\\right\\Vert\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert+\\frac{L}{2}\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality holds due to the Cauchy-Schwarz inequality, and the last inequality holds due to Assumption 2.3. ", "page_idx": 16}, {"type": "text", "text": "By taking the expectation, the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[f(\\phi_{t})]\\leq\\mathbb{E}[f(\\phi_{t-1})]+\\mathbb{E}\\left[\\nabla f(\\theta_{t-1})^{\\top}(\\phi_{t}-\\phi_{t-1})\\right]}\\\\ &{\\quad+L\\mathbb{E}\\left[\\|\\phi_{t-1}-\\theta_{t-1}\\|\\right\\|\\phi_{t}-\\phi_{t-1}\\right]+\\frac{L}{2}\\mathbb{E}\\left[\\|\\phi_{t}-\\phi_{t-1}\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}[f(\\phi_{t-1})]+\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}\\left(1-\\beta_{1}^{+-1}\\right)G^{2}}{(1-\\beta_{1})\\epsilon}-\\alpha_{t}\\frac{\\mathbb{E}\\left[\\|\\nabla f(\\theta_{t-1})\\|_{+}^{4/3}\\right]^{3/2}}{\\sqrt{(1-\\beta_{2}^{+2})G^{2}+\\epsilon^{2}}}}\\\\ &{\\quad+\\frac{\\alpha_{t-1}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}^{2}\\left(1-\\beta_{1}^{+-1}\\right)G^{2}L}{\\epsilon^{2}}+\\frac{\\alpha_{t}\\alpha_{t-1}\\beta_{1}\\sqrt{1-\\beta_{1}^{+-1}}G^{2}L}{(1-\\beta_{1})\\epsilon^{2}}}\\\\ &{\\quad+\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}\\left(1-\\beta_{1}^{+-1}\\right)G^{2}L}{2\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}}\\\\ &{\\quad+\\frac{\\alpha_{t}^{2}G^{2}L}{2\\epsilon^{2}}+\\frac{\\alpha_{t}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}\\sqrt{1-\\beta_{1}^{+-1}}G^{2}L}{2(1-\\beta_{1})\\epsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When $\\alpha_{t}=\\alpha$ , the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\phi_{t}\\right)\\right]\\leq\\mathbb{E}\\left[f\\left(\\phi_{t-1}\\right)\\right]-\\alpha\\frac{\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\right\\Vert_{i}^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{T}\\right)G^{2}+\\epsilon^{2}}}+\\frac{\\alpha^{2}\\beta_{1}\\sqrt{1-\\beta_{1}^{t-1}}G^{2}L}{\\left(1-\\beta_{1}\\right)\\epsilon^{2}}+\\frac{\\alpha^{2}G^{2}L}{2\\epsilon^{2}}}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[f\\left(\\phi_{t-1}\\right)\\right]-\\alpha\\frac{\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\right\\Vert^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{T}\\right)G^{2}+\\epsilon^{2}}}+\\frac{\\alpha^{2}\\left(1+\\beta_{1}\\right)G^{2}L}{2\\left(1-\\beta_{1}\\right)\\epsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Telescoping it for $t=1,\\dots,T$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\left[f\\left(\\phi_{T}\\right)\\right]\\leq f\\left(\\pmb{\\theta}_{0}\\right)-\\alpha\\frac{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right\\Vert^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{T}\\right)G^{2}+\\epsilon^{2}}}+\\frac{\\alpha^{2}\\left(1+\\beta_{1}\\right)G^{2}L T}{2\\left(1-\\beta_{1}\\right)\\epsilon^{2}}}\\\\ &{}&{\\leq f\\left(\\pmb{\\theta}_{0}\\right)-\\alpha\\frac{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right\\Vert^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{T}\\right)G^{2}+\\epsilon^{2}}}+\\frac{\\alpha^{2}\\left(1+\\beta_{1}\\right)G^{2}L T}{2\\left(1-\\beta_{1}\\right)\\epsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t=1,\\dots,T}{\\operatorname*{min}}\\left\\lbrace\\mathbb{E}\\left[\\left\\|\\nabla f\\left(\\theta_{t-1}\\right)\\right\\|^{4/3}\\right]^{3/2}\\right\\rbrace}\\\\ &{\\leq\\frac{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(\\theta_{t-1}\\right)\\right\\|^{4/3}\\right]^{3/2}}{T}}\\\\ &{\\leq\\sqrt{\\left(1-\\beta_{2}^{T}\\right)G^{2}+\\epsilon^{2}}\\left(\\frac{f\\left(\\theta_{0}\\right)-f_{\\mathrm{inf}}}{\\alpha T}+\\frac{\\alpha\\left(1+\\beta_{1}\\right)G^{2}L}{2\\left(1-\\beta_{1}\\right)\\epsilon^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $\\alpha_{t}=\\alpha/\\sqrt{t}$ , the following holds for $t\\geq2$ : ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\alpha_{t-1}-\\alpha_{t}=\\alpha\\left(\\frac{1}{\\sqrt{t-1}}-\\frac{1}{\\sqrt{t}}\\right)}}\\\\ &{=\\frac{\\alpha\\left(\\sqrt{t}-\\sqrt{t-1}\\right)}{\\sqrt{t(t-1)}}}\\\\ &{=\\frac{\\alpha}{\\sqrt{t\\left(t-1\\right)}\\left(\\sqrt{t}+\\sqrt{t-1}\\right)}}\\\\ &{\\leq\\frac{\\alpha}{2\\left(t-1\\right)^{3/2}}}\\\\ &{\\leq\\frac{\\sqrt{2}\\alpha}{t^{3/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(38) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left.\\mathcal{E}\\left(\\phi_{1}\\right)\\leq\\mathbb{E}\\left[f\\left(\\phi_{1}\\right)\\right]+\\frac{\\left(\\mu_{1}-\\kappa_{2}\\right)\\phi_{1}\\right)\\left(1-\\beta_{2}^{-1}\\right)\\mathcal{E}^{2}}{\\left(1-\\beta_{1}\\right)\\epsilon}-\\alpha\\frac{\\mathbb{E}\\left[\\gamma\\left(f\\left(\\phi_{1}\\right)\\right)\\right]^{1/2}}{\\sqrt{\\left(1-\\beta_{2}\\right)!}}}&{}\\\\ {+\\frac{\\alpha_{1}\\epsilon\\left(\\mu_{1}-\\kappa_{2}\\right)\\phi_{1}^{2}\\left(1-\\beta_{2}^{-1}\\right)\\mathcal{E}^{2}}{\\left(1-\\beta_{2}\\right)!}+\\alpha\\frac{\\beta_{2}\\epsilon}{\\sqrt{\\left(1-\\beta_{2}\\right)!}}-\\alpha\\frac{\\beta_{1}\\epsilon}{\\sqrt{\\left(1-\\beta_{2}\\right)!}}f\\alpha+\\frac{\\alpha_{2}\\theta_{1}^{-1}}{\\sqrt{\\left(1-\\beta_{2}\\right)!}}}\\\\ {+\\frac{\\alpha_{1}\\epsilon\\left(\\mu_{1}-\\kappa_{2}\\right)\\phi_{1}^{2}\\left(1-\\beta_{3}^{-1}\\right)\\mathcal{E}^{2}}{\\left(1-\\beta_{2}\\right)!}+\\alpha\\frac{\\beta_{2}\\epsilon}{\\sqrt{\\left(1-\\beta_{2}\\right)!}}}\\\\ {+\\frac{\\alpha_{1}\\epsilon\\left(\\mu_{1}-\\kappa_{2}\\right)\\phi_{1}^{2}\\left(1-\\beta_{3}^{-1}\\right)\\mathcal{E}^{2}}{\\left(1-\\beta_{2}\\right)!}}&{+\\frac{\\alpha_{2}\\theta_{1}^{-1}}{\\sqrt{\\left(1-\\beta_{2}\\right)!}}}\\\\ {+\\frac{\\alpha_{1}\\epsilon\\left(\\mu_{1}-\\kappa_{2}\\right)\\phi_{1}^{2}\\left(1-\\beta_{3}^{-1}\\right)\\mathcal{E}^{2}}{\\left(1-\\beta_{2}\\right)!}}\\\\ {+\\mathbb{E}\\left[f\\left(\\phi_{1}\\right)\\right]+\\frac{\\alpha_{1}\\epsilon\\left(\\mu_{1}-\\beta_{2}^{-1}\\right)\\phi_{2}^{2}}{\\sqrt{\\left(1-\\beta_{2}\\right)!}}-\\frac{\\alpha_{2}}{\\sqrt{\\alpha_{1}!}}\\frac{\\mathbb{E}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Multiplying $t$ to the both sides and rearranging the terms, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sqrt{t}\\,\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\right\\Vert^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{T}\\right)G^{2}+\\epsilon^{2}}}}\\\\ &{\\leq\\frac{\\mathbb{E}\\left[f\\left(\\phi_{t-1}\\right)-f\\left(\\phi_{t}\\right)\\right]}{\\alpha}\\cdot t+\\frac{\\alpha\\left(1+\\left(2\\sqrt{2}-1\\right)\\beta_{1}\\right)G^{2}L}{2\\left(1-\\beta_{1}\\right)\\epsilon^{2}}+\\frac{\\sqrt{2}\\beta_{1}G^{2}}{\\left(1-\\beta_{1}\\right)\\epsilon}\\cdot t^{-\\frac{1}{2}}}\\\\ &{\\quad+\\,\\frac{\\alpha\\beta_{1}\\left(1+\\left(2\\sqrt{2}-1\\right)\\beta_{1}\\right)G^{2}L}{\\sqrt{2}\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}t^{-1}+\\frac{\\alpha\\beta_{1}^{2}G^{2}L}{\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}t^{-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{i=1}^{\\mathcal{N}}\\frac{\\sqrt{\\pi}}{\\sqrt{\\pi}}\\left[\\langle\\nabla f(\\theta_{i},\\hat{\\mathbf{r}})|^{4}\\hat{\\mathbf{r}}\\rangle^{1/2}\\right.}\\\\ &{\\Biggr.\\sum_{i=1}^{\\mathcal{N}}\\frac{\\sqrt{\\pi}}{\\sqrt{\\pi}}\\langle\\hat{\\mathbf{r}}_{i}-\\hat{\\mathbf{r}}_{i}^{2}\\rangle\\epsilon^{2}\\epsilon^{2}}\\\\ &{\\leq\\int^{(\\delta_{i})-T}f(\\phi_{i})+\\sum_{i=1}^{\\mathcal{N}-T}f(\\phi_{i})+\\frac{\\alpha(1+(2\\sqrt{2}-1))\\delta_{i}}{2(1-\\delta_{i})\\epsilon^{2}}\\epsilon^{2}\\mathrm{d}T}\\\\ &{\\Biggl.\\sum_{i=1}^{\\mathcal{N}\\delta_{i}/2}\\frac{\\Gamma}{\\sqrt{\\pi}}\\Gamma_{i}^{-1}+\\frac{\\alpha\\beta_{i}(1+(2\\sqrt{2}-1)\\beta_{i})\\epsilon^{2}}{\\sqrt{2(1-\\delta_{i})\\epsilon^{2}}}\\frac{\\sqrt{\\delta_{i}}}{\\Gamma_{i}}\\Gamma_{i}^{-1}+\\frac{\\alpha\\beta_{i}(2\\sqrt{2}-1)}{(1-\\delta_{i})\\epsilon^{2}}\\Gamma_{i}^{-1}}\\\\ &{\\leq\\int^{\\delta_{i}}\\!\\!\\!-\\!\\!\\int_{\\mathbb{Z}}\\!\\!\\!-\\!\\!\\!\\int_{\\mathbb{Z}}\\!\\!\\!-\\!\\!\\!\\int_{\\mathbb{Z}}\\!\\!\\!\\!-\\!\\!\\!\\delta_{i}\\!\\!\\!-\\!\\!\\!\\frac{1}{2}\\delta_{i}\\Gamma\\epsilon^{2}}\\\\ &{\\leq\\frac{\\sqrt{\\alpha_{\\beta}}}{(1-\\delta)\\epsilon^{2}}\\bigg(1+\\int_{0}^{(\\delta_{i})-T}\\partial_{i}\\hat{\\mathbf{r}}\\bigg.+\\!\\!\\frac{(\\alpha\\beta_{i}-1)^{2}}{\\sqrt{2(1-\\delta_{i})\\epsilon^{2}}}\\frac{\\Gamma_{i}}{\\Gamma_{i}}\\bigg.}\\\\ &{\\Biggr.+\\frac{\\alpha\\beta_{i}^{2}\\delta_{i}^{2}}{(1-\\delta_{i})\\epsilon^{2}}\\bigg(1+\\int_{t}^{\\tau^{\\prime}-t}\\!\\!\\!-\\!\\!\\delta_{i}\\! \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the following bound is derived. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t=1,\\ldots,T}{\\operatorname*{min}}\\left\\lbrace\\mathbb{E}\\left[\\|\\nabla f\\left(\\theta_{t-1}\\right)\\|^{4/3}\\right]^{3/2}\\right\\rbrace}\\\\ &{\\leq\\frac{\\sum_{t=1}^{T}\\sqrt{t}\\mathbb{E}\\left[\\|\\nabla f\\left(\\theta_{t-1}\\right)\\|^{4/3}\\right]^{3/2}}{\\sum_{t=1}^{T}\\sqrt{t}}}\\\\ &{\\leq\\frac{\\sum_{t=1}^{T}\\sqrt{t}\\mathbb{E}\\left[\\|\\nabla f\\left(\\theta_{t-1}\\right)\\|^{4/3}\\right]^{3/2}}{\\int_{0}^{T}\\sqrt{t}d t}}\\\\ &{\\leq\\frac{3C_{T}(f_{\\mathrm{sp}}-f_{\\mathrm{fat}})}{2\\alpha}\\frac{1}{\\sqrt{T}}+\\frac{3\\alpha\\left(1+\\left(2\\sqrt{2}-1\\right)\\beta_{1}\\right)C_{T}G^{2}L}{4\\left(1-\\beta_{1}\\right)\\epsilon^{2}\\sqrt{T}}+\\frac{3\\beta_{1}C_{T}G^{2}}{\\sqrt{2}\\left(1-\\beta_{1}\\right)\\epsilon}\\left(\\frac{2}{T}-\\frac{1}{T^{3/2}}\\right)}\\\\ &{\\quad+\\frac{3\\alpha\\beta_{1}\\left(1+\\left(2\\sqrt{2}-1\\right)\\beta_{1}\\right)C_{T}G^{2}L}{2\\sqrt{2}\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}\\left(\\frac{1}{T^{3/2}}+\\frac{\\log T}{T^{3/2}}\\right)+\\frac{3\\alpha\\beta_{1}^{2}C_{T}G^{2}L}{2\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}\\left(\\frac{2}{T^{3/2}}-\\frac{1}{T^{5/2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "G Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma G.1. For all $\\pmb\\theta\\in\\mathbb{R}^{D}$ and $t\\geq1$ , the following holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\|\\leq G.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right\\|=\\sqrt{\\left\\|\\mathbb{E}\\left[\\pmb{g}_{t}\\right]\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\mathbb{E}\\left[\\left\\|\\pmb{g}_{t}\\right\\|^{2}\\right]}}\\\\ &{\\qquad\\qquad\\leq G.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first inequality holds because $\\mathbb{E}[(\\pmb{g}_{t})_{i}]^{2}\\,\\leq\\,\\mathbb{E}[(\\pmb{g}_{t})_{i}^{2}]$ , and the second inequality holds due to Assumption 2.5. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma G.2. For all $\\pmb\\theta\\in\\mathbb{R}^{D}$ and $t\\geq1$ , the following holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\pmb{g}_{t}\\|\\right]\\leq G\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\pmb{{g}}_{t}\\right\\|\\right]\\leq\\mathbb{E}\\left[\\left\\|\\pmb{{g}}_{t}\\right\\|^{2}\\right]^{1/2}}\\\\ &{\\leq G,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality holds due to the H\u00f6lder\u2019s inequality and the second one holds due to Assumption 2.5. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma G.3. For the RMSprop algorithm, the following holds for $t\\geq1$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{i=1}^{D}\\left(\\pmb{v}_{t}\\right)_{i}\\right]\\leq\\left(1-\\beta_{2}^{t}\\right)G^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{i=1}^{D}\\left(v_{t}\\right)_{i}\\right]=\\mathbb{E}\\left[\\left(1-\\beta_{2}\\right)\\displaystyle\\sum_{i=1}^{D}\\sum_{k=1}^{t}\\beta_{2}^{t-k}\\left(g_{k}\\right)_{i}^{2}\\right]}\\\\ {\\displaystyle\\leq\\left(1-\\beta_{2}\\right)G^{2}\\sum_{k=1}^{t}\\beta_{2}^{t-k}}\\\\ {\\displaystyle=\\left(1-\\beta_{2}^{t}\\right)G^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma G.4. For the RMSprop algorithm, the following holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\left(\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\right)\\right]}\\\\ &{\\geq\\frac{1}{2}\\mathbb{E}\\left[\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\left(\\frac{g_{t}}{\\sqrt{\\tilde{v}_{t}+\\epsilon^{2}}}\\right)\\right]-2G\\sqrt{1-\\beta_{2}}\\mathbb{E}\\left[\\bigg\\|\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\bigg\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)^{\\top}\\left(\\frac{\\pmb{g}_{t}}{\\sqrt{\\pmb{v}_{t}+\\epsilon^{2}}}\\right)\\right]=\\sum_{i=1}^{D}\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right)_{i}\\left(\\pmb{g}_{t}\\right)_{i}}{\\sqrt{\\left(\\pmb{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We define $\\tilde{\\pmb{v}}_{t}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{v}}_{t}=\\beta_{2}\\pmb{v}_{t-1}+\\left(1-\\beta_{2}\\right)\\mathbb{E}\\left[\\pmb{g}_{t}\\odot\\pmb{g}_{t}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using this, the following holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}}{\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}}\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}}{\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right]+\\mathbb{E}\\left[\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}\\left(\\frac{1}{\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}}-\\frac{1}{\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}^{2}}{\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right]+\\mathbb{E}\\left[\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}\\left(\\frac{1}{\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}}-\\frac{1}{\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right)\\right]}\\\\ &{\\geq\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}^{2}}{\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right]-\\mathbb{E}\\left[\\left|\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}\\left(\\frac{1}{\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}}-\\frac{1}{\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right)\\right|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality holds due to $A\\geq-|A|$ . For the second term, the following holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}\\left(\\frac{1}{\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}}-\\frac{1}{\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right)\\Bigg|}\\\\ &{=\\left(1-\\beta_{2}\\right)\\left|\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}\\frac{\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]-\\left(g_{t}\\right)_{i}^{2}}{\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}\\left(\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}+\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}\\right)}\\right|}\\\\ &{\\leq\\left(1-\\beta_{2}\\right)\\left(\\frac{\\left|\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}\\right|\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]}{\\sqrt{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}\\left(\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}\\right)}+\\frac{\\left|\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)_{i}\\right|\\left(g_{t}\\right)_{i}^{2}}{\\left(\\left(v_{t}\\right)_{i}+\\epsilon^{2}\\right)\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality holds due to the triangle inequality. For the first term, the following holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\frac{\\vert(\\nabla f\\left(\\theta_{t-1}\\right))_{i}(\\theta_{t})\\vert\\cdot\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]}{\\sqrt{(v_{t})_{i}+\\epsilon^{2}}\\left((\\tilde{w}_{t})_{i}+\\epsilon^{2}\\right)}\\right]}\\\\ &{\\leq\\frac{1}{(1-\\beta_{2})}\\mathbb{E}\\left[\\frac{(\\nabla f\\left(\\theta_{t-1}\\right))_{i}^{2}}{4\\sqrt{(\\tilde{w}_{t})_{i}+\\epsilon^{2}}}\\right]+(1-\\beta_{2})\\mathbb{E}\\left[\\frac{(g_{t})_{i}^{2}\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]^{2}}{\\left((v_{t})_{i}+\\epsilon^{2}\\right)\\left((\\tilde{w}_{t})_{i}+\\epsilon^{2}\\right)^{3/2}}\\right]}\\\\ &{\\leq\\frac{1}{(1-\\beta_{2})}\\mathbb{E}\\left[\\frac{(\\nabla f\\left(\\theta_{t-1}\\right))_{i}^{2}}{4\\sqrt{(\\tilde{w}_{t})_{i}+\\epsilon^{2}}}\\right]+\\mathbb{E}\\left[\\frac{(g_{t})_{i}^{2}\\sqrt{\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]}}{\\sqrt{1-\\beta_{2}}\\left((v_{t})_{i}+\\epsilon^{2}\\right)}\\right]}\\\\ &{\\leq\\frac{1}{(1-\\beta_{2})}\\mathbb{E}\\left[\\frac{(\\nabla f\\left(\\theta_{t-1}\\right))_{i}^{2}}{4\\sqrt{(\\tilde{w}_{t})_{i}+\\epsilon^{2}}}\\right]+\\frac{G}{\\sqrt{1-\\beta_{2}}}\\mathbb{E}\\left[\\frac{(g_{t})_{i}^{2}}{(v_{t})_{i}+\\epsilon^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first inequality is derived using the following fact: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall\\lambda>0,x,y\\in\\mathbb{R},x y\\leq\\frac{\\lambda}{2}x^{2}+\\frac{y^{2}}{2\\lambda}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the second term of Eq. (69), the following holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\frac{\\left|\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}\\left(g_{t}\\right)\\right|\\left(g_{t}\\right)_{i}^{2}}{\\left(\\left(\\sigma_{t}\\right)_{i}+\\epsilon^{2}\\right)\\sqrt{(\\bar{\\theta}_{t})_{i}+\\epsilon^{2}}}\\right]}\\\\ &{\\leq\\frac{1}{(1-\\beta_{2})^{\\mathbb{Z}}}\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}^{2}}{4\\sqrt{(\\bar{\\theta}_{t})_{i}+\\epsilon^{2}}}\\frac{\\left(g_{t}\\right)_{i}^{2}}{\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]}\\right]+(1-\\beta_{2})\\mathbb{E}\\left[\\frac{\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]}{\\sqrt{(\\bar{\\theta}_{t})_{i}+\\epsilon^{2}}}\\frac{\\left(g_{t}\\right)_{i}^{4}}{\\left(\\bar{\\theta}_{t}\\right)_{i}+\\epsilon^{2}}\\right]}\\\\ &{\\leq\\frac{1}{(1-\\beta_{2})^{\\mathbb{Z}}}\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}^{2}}{4\\sqrt{(\\bar{\\theta}_{t})_{i}+\\epsilon^{2}}}\\right]+\\mathbb{E}\\left[\\frac{\\sqrt{\\mathbb{E}\\left[\\left(g_{t}\\right)_{i}^{2}\\right]}\\left(g_{t}\\right)_{i}^{2}}{\\sqrt{1-\\beta_{2}}\\left(\\left(\\bar{\\theta}_{t}\\right)_{i}+\\epsilon^{2}\\right)}\\right]}\\\\ &{\\leq\\frac{1}{(1-\\beta_{2})^{\\mathbb{Z}}}\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\theta_{t-1}\\right)\\right)_{i}^{2}}{4\\sqrt{(\\bar{\\theta}_{t})_{i}+\\epsilon^{2}}}\\right]+\\frac{G}{\\sqrt{1-\\beta_{2}}}\\mathbb{E}\\left[\\frac{\\left(g_{t}\\right)_{i}^{2}}{\\left(\\nu_{t}\\right)_{i}+\\epsilon^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first inequality is derived using Eq. (73). ", "page_idx": 22}, {"type": "text", "text": "Putting these inequalities together, the following is derived: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)^{\\top}\\left(\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\right)\\right]}\\\\ {\\displaystyle\\geq\\sum_{i=1}^{D}\\mathbb{E}\\left[\\frac{\\left(\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right)_{i}^{2}}{2\\sqrt{\\left(\\tilde{v}_{t}\\right)_{i}+\\epsilon^{2}}}\\right]-2G\\sqrt{1-\\beta_{2}}\\mathbb{E}\\left[\\frac{\\left(g_{t}\\right)_{i}^{2}}{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}\\right]}\\\\ {\\displaystyle\\geq\\frac{1}{2}\\mathbb{E}\\left[\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)^{\\top}\\left(\\frac{g_{t}}{\\sqrt{\\tilde{v}_{t}+\\epsilon^{2}}}\\right)\\right]-2G\\sqrt{1-\\beta_{2}}\\mathbb{E}\\left[\\left\\|\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma G.5. For the RMSprop algorithm, the following holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\right\\Vert^{2}\\right]\\leq D\\left(\\log\\left(1+\\frac{\\left(1-\\beta_{2}^{T}\\right)G^{2}}{\\epsilon^{2}}\\right)-T\\log\\beta_{2}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\frac{{\\bf g}_{t}}{\\sqrt{{\\bf v}_{t}+\\epsilon^{2}}}\\right\\|^{2}=\\sum_{i=1}^{D}\\frac{({\\bf g}_{t})_{i}^{2}}{({\\bf v}_{t})_{i}+\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\left(g_{t}\\right)_{i}^{2}}{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}=\\frac{1}{1-\\beta_{2}}\\frac{\\left(1-\\beta_{2}\\right)\\left(g_{t}\\right)_{i}^{2}}{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}}}\\\\ &{}&{\\leq-\\frac{1}{1-\\beta_{2}}\\log\\left(1-\\frac{\\left(1-\\beta_{2}\\right)\\left(g_{t}\\right)_{i}^{2}}{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}\\right)}\\\\ &{}&{=\\frac{1}{1-\\beta_{2}}\\log\\left(\\frac{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}{\\beta_{2}\\left(v_{t-1}\\right)_{i}+\\epsilon^{2}}\\right)}\\\\ &{}&{=\\frac{1}{1-\\beta_{2}}\\left(\\log\\left(\\frac{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}{\\left(v_{t-1}\\right)_{i}+\\epsilon^{2}}\\right)+\\log\\left(\\frac{\\left(v_{t-1}\\right)_{i}+\\epsilon^{2}}{\\beta_{2}\\left(v_{t-1}\\right)_{i}+\\epsilon^{2}}\\right)\\right)}\\\\ &{}&{\\leq\\frac{1}{1-\\beta_{2}}\\left(\\log\\left(\\frac{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}{\\left(v_{t-1}\\right)_{i}+\\epsilon^{2}}\\right)-\\log\\beta_{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\frac{\\left(g_{t}\\right)_{i}^{2}}{\\left(v_{t}\\right)_{i}+\\epsilon^{2}}\\leq\\frac{1}{1-\\beta_{2}}\\left(\\log\\left(\\frac{\\left(v_{T}\\right)_{i}+\\epsilon^{2}}{\\epsilon^{2}}\\right)-T\\log\\beta_{2}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{1-\\beta_{2}}\\left(\\log\\left(1+\\frac{\\left(1-\\beta_{2}^{T}\\right)G^{2}}{\\epsilon^{2}}\\right)-T\\log\\beta_{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\lVert\\frac{g_{t}}{\\sqrt{v_{t}+\\epsilon^{2}}}\\right\\rVert^{2}\\right]\\leq\\displaystyle\\sum_{i=1}^{D}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\frac{\\left(g_{t}\\right)_{i}^{2}}{(v_{t})_{i}+\\epsilon^{2}}\\right]}&{}\\\\ &{\\leq\\displaystyle\\frac{1}{1-\\beta_{2}}\\sum_{i=1}^{D}\\mathbb{E}\\left[\\log\\left(1+\\frac{(v_{T})_{i}}{\\epsilon^{2}}\\right)\\right]-\\frac{D T\\log\\beta_{2}}{1-\\beta_{2}}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{D}\\log\\left(1+\\frac{\\mathbb{E}\\left[(v_{T})_{i}\\right]}{\\epsilon^{2}}\\right)-\\frac{D T\\log\\beta_{2}}{1-\\beta_{2}}}\\\\ &{\\leq\\displaystyle\\frac{D}{1-\\beta_{2}}\\left(\\log\\left(1+\\frac{\\left(1-\\beta_{2}^{T}\\right)G^{2}}{\\epsilon^{2}}\\right)-T\\log\\beta_{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma G.6. For the RMSprop algorithm, the following holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)^{\\top}\\left(\\frac{g_{t}}{\\sqrt{\\beta_{2}\\tilde{v}_{t}+\\epsilon^{2}}}\\right)\\right]\\geq\\frac{\\mathbb{E}\\left[\\Vert\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\Vert^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{t}\\right)G^{2}+\\epsilon^{2}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\nabla f(\\theta_{t-1})^{\\top}\\left(\\frac{\\theta_{t}}{\\sqrt{\\tau_{t}}+\\tau^{2}}\\right)\\right]}\\\\ &{=\\frac{\\gamma_{t}}{\\sqrt{\\tau_{t}}}\\mathbb{E}\\left[\\frac{\\nabla f\\left(\\theta_{t-1}\\right)-\\theta_{t}}{\\sqrt{\\tau_{t}}(\\theta_{t-1})+\\tau^{2}}\\right]}\\\\ &{=\\frac{\\gamma_{t}}{\\sqrt{\\tau_{t}}}\\mathbb{E}\\left[\\frac{\\nabla f\\left(\\theta_{t-1}\\right)+\\theta_{t}}{\\sqrt{\\tau_{t}}(\\theta_{t-1})+\\tau^{2}}\\right]}\\\\ &{\\geq\\frac{\\mathbb{E}\\left[\\frac{\\mathbb{V}\\left[\\theta_{t}\\right]}{\\sqrt{\\tau_{t}}(\\theta_{t-1})+\\tau^{2}}\\right]}{\\sqrt{\\tau_{t}}}\\left(\\frac{\\mathbb{I}_{\\theta}}{\\sqrt{\\tau_{t}}}\\right)}\\\\ &{\\geq\\frac{\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\right\\Vert^{2}\\right]}{\\sqrt{\\tau_{t}}\\left[\\sum_{i=1}^{D}\\left(\\theta_{i}\\right)\\right]+\\epsilon^{2}}}\\\\ &{\\geq\\frac{\\mathbb{E}\\left[\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\Vert^{2}\\right]}{\\sqrt{\\tau_{t}}\\left[\\sum_{i=1}^{D}\\left(\\theta_{i}\\right)\\right]+\\epsilon^{2}}}\\\\ &{\\geq\\frac{\\mathbb{E}\\left[\\Vert\\nabla f\\left(\\theta_{t-1}\\right)\\Vert^{2}\\right]}{\\sqrt{\\tau_{t}}\\left[1-\\beta\\right]}+\\epsilon^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The second equality holds due to Assumption 2.2. The first inequality holds because $\\left(\\tilde{\\boldsymbol{v}}_{t}\\right)_{i}\\geq0$ for all $i=1,\\dots,D$ . The second inequality holds due to the H\u00f6lder\u2019s inequality. The last inequality holds due to Lemma G.3. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma G.7. For the ADOPT algorithm, the following holds for $t\\geq1$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\phi_{t}-\\phi_{t-1}=\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}m_{t-1}-\\alpha_{t}\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we define $\\alpha_{0}=\\alpha$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. For $t=1$ , the following holds by definition: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\phi_{1}-\\phi_{0}=\\frac{1}{1-\\beta_{1}}\\theta_{1}-\\left(\\frac{\\beta_{1}}{1-\\beta_{1}}+1\\right)\\theta_{0}}\\\\ {\\displaystyle=\\frac{1}{1-\\beta_{1}}\\left(\\theta_{1}-\\theta_{0}\\right)}\\\\ {\\displaystyle=-\\frac{\\alpha_{1}g_{1}}{\\operatorname*{max}\\left\\{\\sqrt{v_{0}},\\epsilon\\right\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $t\\geq2$ , the following holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi_{t}-\\phi_{t-1}=\\displaystyle\\frac{1}{1-\\beta_{1}}\\left(\\theta_{t}-\\theta_{t-1}\\right)-\\frac{\\beta_{1}}{1-\\beta_{1}}\\left(\\theta_{t-1}-\\theta_{t-2}\\right)}\\\\ {=\\displaystyle\\frac{1}{1-\\beta_{1}}\\left(\\alpha_{t-1}\\beta_{1}m_{t-1}-\\alpha_{t}m_{t}\\right)}\\\\ {=\\displaystyle\\frac{1}{1-\\beta_{1}}\\left(\\alpha_{t-1}\\beta_{1}m_{t-1}-\\alpha_{t}\\left(\\beta_{1}m_{t-1}+(1-\\beta_{1})\\displaystyle\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}\\right)\\right)}\\\\ {=\\displaystyle\\frac{1}{1-\\beta_{1}}\\left((\\alpha_{t-1}-\\alpha_{t})\\beta_{1}m_{t-1}-\\alpha_{t}\\left(1-\\beta_{1}\\displaystyle\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}\\right)\\right.}\\\\ {\\displaystyle}&{=\\displaystyle\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}m_{t-1}-\\alpha_{t}\\displaystyle\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma G.8. For the ADOPT algorithm, the following holds for $t\\geq1$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\phi_{t-1}-\\pmb{\\theta}_{t-1}=-\\frac{\\alpha_{t-1}\\beta_{1}}{1-\\beta_{1}}\\pmb{m}_{t-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. For $t=1$ , Eq. (104) holds obviously because $\\phi_{0}=\\theta_{0}$ and $\\mathbf{\\nabla}m_{0}={\\bf0}$ . For $t\\geq2$ , the following holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{t-1}-\\pmb{\\theta}_{t-1}=\\left(\\frac{1}{1-\\beta_{1}}-1\\right)\\pmb{\\theta}_{t-1}-\\frac{\\beta_{1}}{1-\\beta_{1}}\\pmb{\\theta}_{t-2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\beta_{1}}{1-\\beta_{1}}\\left(\\pmb{\\theta}_{t-1}-\\pmb{\\theta}_{t-2}\\right)}\\\\ &{\\qquad\\qquad\\qquad=-\\frac{\\alpha_{t-1}\\beta_{1}}{1-\\beta_{1}}\\pmb{m}_{t-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma G.9. For the ADOPT algorithm, the following holds for $t\\geq1$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)^{\\top}\\left(\\pmb{\\phi}_{t}-\\phi_{t-1}\\right)\\right]}\\\\ &{\\leq\\displaystyle\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}\\left(1-\\beta_{1}^{t-1}\\right)G^{2}}{\\left(1-\\beta_{1}\\right)\\sqrt{\\beta_{2}^{t-2}+\\epsilon^{2}}}-\\alpha_{t}\\frac{\\mathbb{E}\\left[\\left\\|\\nabla f\\left(\\pmb{\\theta}_{t-1}\\right)\\right\\|_{i}^{4/3}\\right]^{3/2}}{\\sqrt{\\left(1-\\beta_{2}^{t}\\right)G^{2}+\\epsilon^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\left(\\phi_{t}-\\phi_{t-1}\\right)}\\\\ &{\\quad=\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}\\nabla f\\left(\\theta_{t-1}\\right)\\right)^{\\top}m_{t-1}-\\alpha_{t}\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}}\\\\ &{\\quad\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}\\left\\|\\nabla f\\left(\\theta_{t-1}\\right)\\right\\|\\left\\|m_{t-1}\\right\\|-\\alpha_{t}\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}}\\\\ &{\\qquad\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}G}{1-\\beta_{1}}\\left\\|m_{t-1}\\right\\|-\\alpha_{t}\\nabla f\\left(\\theta_{t-1}\\right)^{\\top}\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By taking the expectation for both sides, the following holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\gamma(\\mu_{0},\\lambda)^{*}\\left(\\Phi_{i}\\!\\!\\!-\\!\\!\\rho_{i},\\!\\!\\frac1\\alpha\\!\\!\\!-\\!\\!\\!\\frac1\\alpha\\!\\!\\!,\\!\\!\\!\\right)\\right]}\\\\ &{\\lesssim\\frac{\\big(\\!\\frac{\\alpha}{\\hbar}\\!\\!\\!-\\!\\!\\frac1\\alpha\\!\\!)\\delta^{2}}{1-\\delta^{3}}[\\frac{\\gamma^{2}}{\\gamma^{3}}\\!\\!\\!-\\!\\!\\frac1\\alpha\\!\\!,\\!\\!\\bigcirc\\!\\!\\!\\frac{\\delta}{\\delta}][\\gamma^{2}(\\mu_{-1})^{\\!-\\!\\frac1\\alpha\\!\\!-\\!\\!\\frac1}{\\delta}]}\\\\ &{\\stackrel{(\\mathrm{S}_{\\hbar})}{\\leq}\\frac{\\big(\\hbar\\!\\!-\\!\\delta\\!-\\!\\!\\big)\\delta^{2}}{1-\\delta^{3}}[\\frac{\\gamma^{2}(\\mu_{-1})^{\\!-\\!\\frac1\\alpha\\!-\\!\\frac1\\alpha\\!\\!-\\!\\frac1\\alpha\\!\\!-\\!\\frac1\\alpha\\!\\!-\\!\\frac1\\alpha\\!\\!-\\!\\frac1\\alpha\\!\\!-\\!\\frac1\\alpha\\!\\!-\\!\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1\\alpha\\!\\!-\\frac1{\\delta^{\\prime}}]}\\\\ &{\\stackrel{(\\mathrm{S}_{\\hbar})}{\\leq}\\frac{(\\hbar\\!\\!-\\!1\\!-\\!\\delta\\! \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma G.10. For the ADOPT algorithm, the following holds for $t\\geq0$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{i=1}^{D}\\left(v_{t}\\right)_{i}\\right]\\leq\\left(1-\\beta_{2}^{t}\\right)G^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{D}\\left(v_{t}\\right)_{i}\\right]=\\mathbb{E}\\left[\\left(1-\\beta_{2}\\right)\\displaystyle\\sum_{i=1}^{D}\\sum_{k=1}^{t}\\beta_{2}^{t-k}\\left(g_{k-1}\\right)_{i}^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(1-\\beta_{2}\\right)G^{2}\\displaystyle\\sum_{k=1}^{t}\\beta_{2}^{t-k}}\\\\ &{\\qquad\\qquad\\qquad=\\left(1-\\beta_{2}^{t}\\right)G^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma G.11. For the ADOPT algorithm, the following holds for $0\\leq t\\leq T$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|m_{t}\\right\\|^{2}\\right]\\leq\\frac{G^{2}}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}\\left[\\left|m_{t}\\right|^{2}\\right]}&{=}&{\\mathbb{E}\\left[\\left|\\begin{array}{l l l}{1}&{0}&{0}\\\\ &{1}&{0}\\\\ &{0}&{0}\\end{array}\\right|^{2}\\right]}\\\\ &{=}&{\\mathbb{E}\\left[\\left|\\begin{array}{l l l}{1}&{m_{t}+1+(1-\\delta_{t})\\frac{\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}\\right|^{2}\\right]}\\\\ &{=}&{\\mathbb{E}\\left[\\left|\\frac{\\theta}{1+\\theta}\\right|m_{t-1}^{2}\\mathbb{E}\\left|\\frac{\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}\\right|^{2}\\right]^{2}+2\\mathbb{E}\\left(1-\\delta_{t}\\right)m_{t}\\frac{\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}}\\\\ &{\\le}&{\\mathbb{E}\\left[\\left|\\frac{\\theta}{1+\\theta}m_{t-1}\\right|^{2}+(1-\\delta_{t})\\left|\\frac{\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}\\right|^{2}\\right]}\\\\ &{\\le}&{\\mathbb{E}\\left[\\left|\\frac{\\theta}{1+\\theta}m_{t-1}\\right|^{2}+\\frac{1-\\delta_{t}}{\\theta}\\frac{\\theta}{1+\\theta}\\right]}\\\\ &{\\le}&{\\mathbb{E}\\left[\\left|\\frac{\\theta}{1+\\theta}m_{t-1}\\right|^{2}+\\frac{1-\\delta_{t}}{\\theta}\\frac{\\theta}{1+\\theta}\\right]}\\\\ &{\\le}&{\\mathbb{E}\\left[\\frac{1-\\delta_{t}}{1+\\theta}\\frac{\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}\\right]}\\\\ &{\\le}&{\\frac{(1-\\delta_{t})\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}\\frac{\\theta}{1+\\theta}\\int_{0}^{1}}\\\\ &{\\le}&{\\frac{(1-\\delta_{t})\\theta}{2}\\frac{\\theta}{1+\\theta}}\\\\ &{\\le}&{\\frac{(2-\\theta)}{\\theta^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "First inequality is derived using the following fact: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\forall\\lambda>0,\\pmb{x},\\pmb{y}\\in\\mathbb{R}^{d},\\pmb{x}^{\\top}\\pmb{y}\\leq\\frac{\\lambda}{2}\\left\\|\\pmb{x}\\right\\|^{2}+\\frac{1}{2\\lambda}\\left\\|\\pmb{y}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By setting $\\lambda=\\left(1-\\beta_{1}\\right)/\\beta_{1},x=\\beta_{1}m_{t-1},y=\\left(1-\\beta_{1}\\right)g_{t}/\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\},$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n2\\beta_{1}\\left(1-\\beta_{1}\\right)m_{t-1}^{\\top}\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}\\leq\\beta_{1}\\left(1-\\beta_{1}\\right)\\left(\\left\\Vert m_{t-1}\\right\\Vert^{2}+\\left\\Vert\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{v_{t-1}},\\epsilon\\right\\}}\\right\\Vert^{2}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Injecting it into Eq. (126), we obtain Eq. (127). ", "page_idx": 26}, {"type": "text", "text": "Lemma G.12. For the ADOPT algorithm, the following holds for $t\\geq0$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|m_{t}\\right\\|\\right]\\leq{\\frac{G}{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\lVert m_{t}\\right\\rVert=\\mathbb{E}\\left[\\left\\lVert(1-\\beta_{1})\\frac{\\sum_{j=0}^{j}\\beta_{j}-k}{k-1}\\frac{g_{K}}{\\operatorname*{max}\\left\\{\\sqrt{0}\\frac{k-1}{\\hbar},c\\right\\}}\\right\\rVert_{1}\\right]}&{}\\\\ {\\le(1-\\beta_{1})\\sum_{j=1}^{j}\\beta_{1}^{\\prime\\prime-1}\\mathbb{E}\\left[\\left\\lVert\\frac{g_{K}}{\\operatorname*{max}\\left\\{\\sqrt{0}\\frac{k}{\\hbar},c\\right\\},c^{j}}\\right\\rVert_{1}\\right]}&{}\\\\ {\\le(1-\\beta_{1})\\sum_{i=1}^{j}\\frac{\\beta_{1}^{\\prime\\prime}-k}{\\epsilon}\\mathbb{E}\\left[\\left\\lVert g_{K}\\right\\rVert^{1}\\right.}\\\\ &{\\left.\\le\\frac{1-\\beta_{1}}{\\epsilon}\\sum_{i=1}^{j}\\mathbb{E}\\left[\\left\\lVert g_{K}\\right\\rVert^{2}\\right]^{1/2}}\\\\ &{\\le\\frac{(1-\\beta_{1})G}{\\epsilon}\\sum_{i=1}^{\\infty}\\mathbb{E}\\left[\\left\\lVert g_{K}\\right\\rVert^{-1}\\right]}\\\\ &{=\\frac{(1-\\beta_{1})G}{\\epsilon}\\mathbb{E}\\frac{\\sum_{j=1}^{j}\\beta_{1}^{\\prime\\prime}-k}{\\epsilon}}\\\\ &{=\\frac{(1-\\beta_{1})G}{\\epsilon}}\\\\ &{\\le\\frac{G}{\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma G.13. For the ADOPT algorithm, the following holds for $t\\geq1$ : ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\phi_{t-1}-\\theta_{t-1}\\right\\|\\,\\left\\|\\phi_{t}-\\phi_{t-1}\\right\\|\\right]}\\\\ &{\\leq\\frac{\\alpha_{t-1}\\,\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\,\\beta_{1}^{2}\\,\\left(1-\\beta_{1}^{t-1}\\right)\\,G^{2}}{\\epsilon^{2}\\,\\left(1-\\beta_{1}\\right)^{2}}+\\frac{\\alpha_{t}\\alpha_{t-1}\\beta_{1}\\sqrt{1-\\beta_{1}^{t-1}}G^{2}}{\\epsilon^{2}\\,\\left(1-\\beta_{1}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\phi_{t-1}-\\theta_{t-1}\\|\\,\\,\\|\\phi_{t}-\\phi_{t-1}\\|}\\\\ &{=\\left\\|-\\frac{\\alpha_{t-1}\\beta_{1}}{1-\\beta_{1}}m_{t-1}\\right\\|\\,\\Bigg\\|\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}m_{t-1}-\\alpha_{t}\\frac{g_{t}}{\\operatorname*{max}\\big\\{\\sqrt{v_{t-1}},\\epsilon\\big\\}}\\Bigg\\|}\\\\ &{\\leq\\frac{\\alpha_{t-1}\\beta_{1}}{1-\\beta_{1}}\\,\\|m_{t-1}\\|\\,\\Bigg(\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}\\,\\|m_{t-1}\\|+\\alpha_{t}\\left\\|\\frac{g_{t}}{\\operatorname*{max}\\big\\{\\sqrt{v_{t-1}},\\epsilon\\big\\}}\\right\\|\\Bigg)}\\\\ &{\\leq\\frac{\\alpha_{t-1}\\,\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}^{2}}{\\left(1-\\beta_{1}\\right)^{2}}\\,\\|m_{t-1}\\|^{2}+\\frac{\\alpha_{t}\\alpha_{t-1}\\beta_{1}}{1-\\beta_{1}}\\,\\|m_{t-1}\\|\\,\\Bigg\\|\\frac{g_{t}}{\\operatorname*{max}\\big\\{\\sqrt{v_{t-1}},\\epsilon\\big\\}}\\Bigg\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Taking the expectation yields: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\phi_{t-1}-\\theta_{t-1}\\right\\Vert\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert\\right]}\\\\ &{\\leq\\frac{\\alpha_{t-1}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}^{2}}{\\left(1-\\beta_{1}\\right)^{2}}\\mathbb{E}\\left[\\left\\Vert m_{t-1}\\right\\Vert^{2}\\right]+\\frac{\\alpha_{t}\\alpha_{t-1}\\beta_{1}}{1-\\beta_{1}}\\mathbb{E}\\left[\\left\\Vert m_{t-1}\\right\\Vert\\left\\Vert\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{\\vartheta_{t-1}},\\epsilon\\right\\}}\\right\\Vert\\right]}\\\\ &{\\leq\\frac{\\alpha_{t-1}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}^{2}}{\\left(1-\\beta_{1}\\right)^{2}}\\mathbb{E}\\left[\\left\\Vert m_{t-1}\\right\\Vert^{2}\\right]+\\frac{\\alpha_{t}\\alpha_{t-1}\\beta_{1}}{\\left(1-\\beta_{1}\\right)\\epsilon}\\mathbb{E}\\left[\\left\\Vert m_{t-1}\\right\\Vert\\left\\Vert g_{t}\\right\\Vert\\right]}\\\\ &{\\leq\\frac{\\alpha_{t-1}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}^{2}\\left(1-\\beta_{1}^{t-1}\\right)G^{2}}{\\epsilon^{2}\\left(1-\\beta_{1}\\right)^{2}}+\\frac{\\alpha_{t}\\alpha_{t-1}\\beta_{1}\\sqrt{1-\\beta_{1}^{t-1}}G^{2}}{\\left(1-\\beta_{1}\\right)\\epsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma G.14. For the ADOPT algorithm, the following holds for $t\\geq1$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}\\left(1-\\beta_{1}^{t-1}\\right)G^{2}}{\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}+\\frac{\\alpha_{t}^{2}G^{2}}{\\epsilon^{2}}+\\frac{\\alpha_{t}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}\\sqrt{1-\\beta_{1}^{t-1}}G^{2}}{\\left(1-\\beta_{1}\\right)\\epsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left||\\phi_{t}-\\phi_{t-1}|\\right|^{2}}\\\\ &{=\\left|\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}m_{t-1}-\\alpha_{t}\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{\\eta_{1}}-\\nu_{t}\\right\\}_{*}}\\right|^{2}}\\\\ &{=\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}}{(1-\\beta_{1})^{2}}||m_{t-1}|^{2}+\\alpha_{t}^{2}\\left|\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{\\eta_{1}}-\\nu_{t}\\right\\}_{*}}\\right|^{2}}\\\\ &{\\quad-\\frac{\\alpha_{t}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}m_{t-1}\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{\\eta_{1}}-\\nu_{t}\\right\\}_{*}}}\\\\ &{\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}}{(1-\\beta_{1})^{2}}||m_{t-1}|^{2}+\\frac{\\alpha_{t}^{2}}{c^{2}}||g_{t}|^{2}}\\\\ &{\\quad+\\frac{\\alpha_{t}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{1-\\beta_{1}}||m_{t-1}|\\left|\\frac{g_{t}}{\\operatorname*{max}\\left\\{\\sqrt{\\eta_{1}}-\\nu_{t}\\right\\}_{*}}\\right|}\\\\ &{\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}}{(1-\\beta_{1})^{2}}||m_{t-1}|^{2}+\\frac{\\alpha_{t}^{2}}{c^{2}}||g_{t}|^{2}+\\frac{\\alpha_{t}}{c^{3}}||m_{t-1}||\\left|g_{t}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking the expectation yields: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\phi_{t}-\\phi_{t-1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}}{\\left(1-\\beta_{1}\\right)^{2}}\\mathbb{E}\\left[\\left\\Vert m_{t-1}\\right\\Vert^{2}\\right]+\\frac{\\alpha_{t}^{2}}{\\epsilon^{2}}\\mathbb{E}\\left[\\left\\Vert g_{t}\\right\\Vert^{2}\\right]+\\frac{\\alpha_{t}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{\\left(1-\\beta_{1}\\right)\\epsilon}\\mathbb{E}\\left[\\left\\Vert m_{t-1}\\right\\Vert\\left\\Vert g_{t}\\right\\Vert\\right]}\\\\ &{\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}\\left(1-\\beta_{1}^{t-1}\\right)G^{2}}{\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}+\\frac{\\alpha_{t}^{2}G^{2}}{\\epsilon^{2}}+\\frac{\\alpha_{t}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}}{\\left(1-\\beta_{1}\\right)\\epsilon}\\mathbb{E}\\left[\\left\\Vert m_{t-1}\\right\\Vert\\left\\Vert g_{t}\\right\\Vert\\right]}\\\\ &{\\leq\\frac{\\left(\\alpha_{t-1}-\\alpha_{t}\\right)^{2}\\beta_{1}^{2}\\left(1-\\beta_{1}^{t-1}\\right)G^{2}}{\\left(1-\\beta_{1}\\right)^{2}\\epsilon^{2}}+\\frac{\\alpha_{t}^{2}G^{2}}{\\epsilon^{2}}+\\frac{\\alpha_{t}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)\\beta_{1}\\sqrt{1-\\beta_{1}^{t-1}}G^{2}}{\\left(1-\\beta_{1}\\right)\\epsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "H Additional Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Deep reinforcement learning: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We train reinforcement learning (RL) agents using the soft actor crtitic algorithm [Haarnoja et al., 2018] with ADOPT for the optimizer. As a benchmark, we use a continuous control tasks of HalfCheetah-v4 on MuJoCo simulator [Todorov et al., 2012]. For comparison to ADOPT, Adam is used as a baseline optimizer. We follow the hyperparameter settings recommended by StableBaselines3 [Raffin et al., 2021], and just change the choice of an optimizer. The result is shown in Figure 6. The error bars indicate $95\\%$ confidence intervals of three trials. We observe slight performance improvement by using ADOPT instead of Adam. ", "page_idx": 28}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/4764c68976508328c6be79e4110cafa2cd6224503b3dea31bbb01568a929e7ae.jpg", "img_caption": ["Figure 6: Performance comparison between Adam and ADOPT in reinforcement learning. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "rzvVm0LsyK/tmp/a218059f16b9fc1b2c0b4720217f298e9a407c3839f4b7a6719081d5db67c2e8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 7: Comparison of MMLU scores for LLaMA-7B finetuned via instruction following using AdamW and ADOPT. ", "page_idx": 29}, {"type": "text", "text": "I Details of Experimental Setups ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "I.1 Code ", "page_idx": 29}, {"type": "text", "text": "Our implementation for the experiment is available at https://github.com/iShohei220/adopt. ", "page_idx": 29}, {"type": "text", "text": "I.2 Total amount of compute ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We run our experiments mainly on cloud GPU instances with $8\\times$ A100. It took approximately 320 hours for our experiments in total. ", "page_idx": 29}, {"type": "text", "text": "I.3 License of Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Datasets: The MNIST database is downloaded from http://yann.lecun.com/exdb/mnist, which is license-free. The terms of access for the ImageNet database is provided at https://www. image-net.org/download. The dataset of Stanford Alpaca is CC BY NC 4.0 (allowing only non-commercial use). ", "page_idx": 29}, {"type": "text", "text": "Pretrained models: The pretrained model of LLaMA is provided under GNU General Public License v3.0. ", "page_idx": 29}, {"type": "text", "text": "Simulator: MuJoCo is provided under Apache License 2.0. ", "page_idx": 29}, {"type": "text", "text": "Code: Our implementation of ImageNet classification is based on the Torchvision\u2019s official training recipe provided at https://github.com/UiPath/torchvision/tree/master/references/ classification. Torchvision is provided under BSD 3-Clause License. We use the official implementation of NVAE provided at https://github.com/NVlabs/NVAE, whose license is described at https://github.com/NVlabs/NVAE/blob/master/LICENSE. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our main contribution is to demystify the cause of non-convergence of Adam, which is clearly written in the abstract and introduction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Limitations are described in the last section. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Assumptions and proofs are provided in Section 2 and the appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Experimental settings are provided in Section 5 and the appendix. We also share the implementation of the experiment. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Data and code are provided in the appendix. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Detailed experimental settings are provided in Section 5 and the appendix. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Error bars are reported in all the figures and tables. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Computational resources used in our experiments are reported in the appendix. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We confirmed that our research conforms with the Code of Ethics. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We discussed them in the last section of the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 34}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: They are provided both in the main paper and the appendix. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: Guid ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]