{"references": [{"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-12-01", "reason": "This paper introduces the Adam optimizer, which is the main subject of the current paper's analysis and improvement."}, {"fullname_first_author": "Sashank J. Reddi", "paper_title": "On the convergence of adam and beyond", "publication_date": "2018-00-00", "reason": "This paper first reveals the theoretical issues of Adam's convergence, which motivates the current research on resolving those issues."}, {"fullname_first_author": "Fangyu Zou", "paper_title": "A sufficient condition for convergences of adam and rmsprop", "publication_date": "2019-00-00", "reason": "This paper provides another theoretical analysis on Adam's convergence, which is compared with the findings of the current paper."}, {"fullname_first_author": "Xiangyi Chen", "paper_title": "On the convergence of a class of adam-type algorithms for non-convex optimization", "publication_date": "2019-00-00", "reason": "This paper analyzes the convergence of Adam-type algorithms under a bounded noise assumption, which is relaxed in the current paper."}, {"fullname_first_author": "Saeed Ghadimi", "paper_title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "publication_date": "2013-00-00", "reason": "This paper provides a convergence analysis of the stochastic gradient descent (SGD) method in non-convex optimization problems, which is used as a comparison in the current paper."}]}