[{"figure_path": "A7wC1CTkYl/figures/figures_0_1.jpg", "caption": "Figure 1: Efficient Lifelong Model Evaluation. Assume an initial pool of n samples and m models evaluated on these samples (left). Our goal is to efficiently evaluate a new model (insertM) at sub-linear cost (right top) and efficiently insert a new sample into the lifelong benchmark (insertD) by determining sample difficulty at sub-linear cost (right bottom). See Section 2 for more details.", "description": "This figure illustrates the core idea of the Sort & Search method.  The left side shows the initial state:  a set of 'n' samples and 'm' models, with their accuracy predictions stored in a matrix. The top-right shows how a new model is efficiently evaluated using a subset of samples ('n''). The bottom-right demonstrates how a new sample is efficiently added to the benchmark by determining its difficulty using a subset of models ('m').", "section": "Efficient Lifelong Model Evaluation"}, {"figure_path": "A7wC1CTkYl/figures/figures_4_1.jpg", "caption": "Figure 1: Efficient Lifelong Model Evaluation. Assume an initial pool of n samples and m models evaluated on these samples (left). Our goal is to efficiently evaluate a new model (insertM) at sub-linear cost (right top) and efficiently insert a new sample into the lifelong benchmark (insertD) by determining sample difficulty at sub-linear cost (right bottom). See Section 2 for more details.", "description": "This figure illustrates the core idea of the Sort & Search method proposed in the paper. The left side shows the initial state with n samples and m models already evaluated.  The right side demonstrates how to efficiently evaluate a new model (top) and add a new sample (bottom) without evaluating all models on all samples.  The new model evaluation leverages a subset of samples selected based on difficulty (determined using existing model evaluations), reducing computation cost. Similarly, the new sample insertion identifies its difficulty using a subset of models. The overall goal is to maintain efficient evaluation of an ever-growing benchmark.", "section": "Efficient Lifelong Model Evaluation"}, {"figure_path": "A7wC1CTkYl/figures/figures_7_1.jpg", "caption": "Figure 3: Main Results. (a,b) We achieve 99% cost-savings for new model evaluation on Lifelong-ImageNet and Lifelong-CIFAR10 showcasing the efficiency (MAE decays exponentially with n') of Sort&Search. (c) S&S is more efficient and accurate compared to the baseline on Lifelong-ImageNet.", "description": "This figure presents the main results of the Sort & Search method.  Subfigures (a) and (b) show the significant reduction in computation cost (99%) achieved by Sort & Search for evaluating new models on the Lifelong-CIFAR10 and Lifelong-ImageNet benchmarks respectively. The mean absolute error (MAE) decreases exponentially as the sampling budget (n') increases, demonstrating the efficiency of the method. Subfigure (c) provides a comparison of Sort & Search against a baseline method, highlighting its superior efficiency and accuracy.", "section": "Main Results"}, {"figure_path": "A7wC1CTkYl/figures/figures_8_1.jpg", "caption": "Figure 2: Full Pipeline of Sort & Search. For efficiently evaluating new models, (Left) we first sort all data samples by difficulty (refer Section 3.1) and (Right) then perform a uniform sampling followed by DP-search and extrapolation for yielding new model predictions (refer Section 3.2). This entire framework can also be transposed to efficiently insert new samples (refer Section 3.3).", "description": "This figure illustrates the overall process of the Sort & Search framework.  The left side shows the sorting of samples based on their difficulty, which is determined by the initial evaluation results. The right side depicts the sampling process and the subsequent prediction of a new model's performance on the remaining samples using dynamic programming. The figure also highlights how the framework can be applied to efficiently insert new samples into the existing benchmark.", "section": "3 Sort & Search: Enabling Efficient Lifelong Model Evaluation"}, {"figure_path": "A7wC1CTkYl/figures/figures_9_1.jpg", "caption": "Figure 5: Error Decomposition Analysis on Lifelong-CIFAR10 (left) and Lifelong-ImageNet (right). We observe that epistemic error (solid line) drops to 0 within only 100 to 1000 samples across both datasets, indicating this error cannot be reduced further by better sampling methods. The total error E is almost entirely irreducible (Aleatoric), induced because new models do not perfectly align with the ranking order P*. This suggests generalizing beyond a single rank ordering, not better sampling strategies, should be the focus of subsequent research efforts.", "description": "The figure shows the error decomposition analysis on two lifelong benchmark datasets: Lifelong-CIFAR10 and Lifelong-ImageNet.  It demonstrates that the epistemic error (the error that can be reduced by increasing the sampling budget) quickly diminishes to near zero, while the aleatoric error (irreducible error due to model misalignment with the sample ranking) remains the dominant source of error. This finding suggests that improving the model's ability to generalize beyond a single ranking, rather than focusing solely on better sampling, may be crucial for enhancing future model evaluation efficiency.", "section": "4.4 Breaking down Sort & Search"}, {"figure_path": "A7wC1CTkYl/figures/figures_20_1.jpg", "caption": "Figure 1: Efficient Lifelong Model Evaluation. Assume an initial pool of n samples and m models evaluated on these samples (left). Our goal is to efficiently evaluate a new model (insertM) at sub-linear cost (right top) and efficiently insert a new sample into the lifelong benchmark (insertD) by determining sample difficulty at sub-linear cost (right bottom). See Section 2 for more details.", "description": "This figure illustrates the core idea of the Sort & Search method for efficient lifelong model evaluation.  The left side shows the initial state with 'n' samples and 'm' models already evaluated. The right side demonstrates how to efficiently add a new model (top) and a new sample (bottom) to the existing benchmark.  The efficiency is achieved by using dynamic programming to selectively evaluate only a subset of samples and models, rather than evaluating everything.", "section": "Efficient Lifelong Model Evaluation"}, {"figure_path": "A7wC1CTkYl/figures/figures_22_1.jpg", "caption": "Figure 1: Efficient Lifelong Model Evaluation. Assume an initial pool of n samples and m models evaluated on these samples (left). Our goal is to efficiently evaluate a new model (insertM) at sub-linear cost (right top) and efficiently insert a new sample into the lifelong benchmark (insertD) by determining sample difficulty at sub-linear cost (right bottom). See Section 2 for more details.", "description": "This figure illustrates the core idea of the Sort & Search method for efficient lifelong model evaluation.  The left side shows the initial state with n samples and m models already evaluated. The right side demonstrates how to efficiently evaluate a new model (top) and add a new sample (bottom) using Sort & Search.  Efficient evaluation is achieved by leveraging dynamic programming to selectively choose subsets of samples and models, thus avoiding the need for complete re-evaluation with each addition.", "section": "Efficient Lifelong Model Evaluation"}, {"figure_path": "A7wC1CTkYl/figures/figures_24_1.jpg", "caption": "Figure 1: Efficient Lifelong Model Evaluation. Assume an initial pool of n samples and m models evaluated on these samples (left). Our goal is to efficiently evaluate a new model (insertM) at sub-linear cost (right top) and efficiently insert a new sample into the lifelong benchmark (insertD) by determining sample difficulty at sub-linear cost (right bottom). See Section 2 for more details.", "description": "This figure illustrates the core concept of efficient lifelong model evaluation.  The left side shows the initial state with 'n' samples and 'm' models already evaluated. The right side demonstrates the two key goals: efficiently evaluating a new model (top) without evaluating it on all samples and efficiently inserting a new sample (bottom) by determining its difficulty.  The figure uses a matrix representation to show the model evaluations and highlights the sub-selection process for both models and samples to achieve efficiency.", "section": "Efficient Lifelong Model Evaluation"}, {"figure_path": "A7wC1CTkYl/figures/figures_24_2.jpg", "caption": "Figure 7: Correlation Analysis between Model Predictions on Lifelong-ImageNet. (a) We note that all correlations between model predictions are positive, signifying the similarities between all models despite their diverse sizes, architectures, and inductive biases. (b) We show the cross-correlation matrix between all model predictions\u2014the x and y axes showcase models, sorted by their accuracies. The floating point numbers on the x and y axes are the model accuracies\u2014the highest accuracy models (70% accuracy) appear at the top and left, while the lowest accuracy models appear at the bottom and right (10% - 30%).", "description": "This figure shows the results of a correlation analysis performed on the predictions of 167 models on the Lifelong-ImageNet dataset.  Panel (a) demonstrates that all Spearman rank correlations between model predictions are positive, highlighting a general similarity in model behavior despite their architectural differences. Panel (b) visualizes a heatmap of the correlation matrix, where models are ordered by their accuracy.  Higher accuracy models show slightly higher correlations with each other compared to lower accuracy models, and very low accuracy models have almost no correlation with other models.", "section": "E Analysis: How Consistently Do Models Follow Global Ranking?"}, {"figure_path": "A7wC1CTkYl/figures/figures_25_1.jpg", "caption": "Figure 8: We change the metric for evaluating the efficacy of Sort & Search from MAE to Spearman correlation-we observe consistently high correlations of 0.5 or greater.", "description": "This figure shows the Spearman correlation between the ground truth model rankings and the model rankings predicted by Sort & Search for different sampling budgets.  High correlations (above 0.5) indicate that Sort & Search maintains the relative ordering of models even with a small sample size, demonstrating its effectiveness in practical applications where model ranking is important.", "section": "F Analysis: Changing the metric from MAE to a Rank Correlation"}, {"figure_path": "A7wC1CTkYl/figures/figures_26_1.jpg", "caption": "Figure 6: Static vs Lifelong Benchmarking. (Top) Static benchmarks incentivise machine learning practitioners to overfit models to specific datasets, weakening their ability to assess generalisation. (Bottom) We conceptualise Lifelong Benchmarks as an alternative paradigm\u2014ever-expanding pools of test samples that resist overfitting while retaining computational tractability.", "description": "This figure compares traditional static benchmarks with the proposed lifelong benchmarks. Static benchmarks focus on fixed datasets, leading to overfitting and limited generalizability. In contrast, lifelong benchmarks use continuously expanding datasets, promoting better generalization and mitigating overfitting.", "section": "2 Lifelong Model Evaluation: Formulation and Challenges"}]