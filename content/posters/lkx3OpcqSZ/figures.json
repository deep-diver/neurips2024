[{"figure_path": "lkx3OpcqSZ/figures/figures_1_1.jpg", "caption": "Figure 1: Decaying spectrum of weight matrices (aka, \"approximate low-rank\")", "description": "This figure shows the decaying singular value profile of the weight matrices in a Llama-2 7B Query model.  The y-axis represents the magnitude of the singular values, and the x-axis represents their index (rank).  The plot demonstrates that a significant portion of the singular values have low magnitude, indicating an inherent low-rank structure in the weight matrices. This low-rank property is exploited by CALDERA for model compression.", "section": "1 Introduction"}, {"figure_path": "lkx3OpcqSZ/figures/figures_1_2.jpg", "caption": "Figure 2: CALDERA decomposes a full-precision weight matrix into a low-rank component (LR), which captures the contribution of the top singular values using BL, BR bits, and Q for the trailing singular values with BQ bits, enabling flexible precision settings for each component. Typically, BQ < BL, BR.", "description": "The figure shows how CALDERA decomposes a full-precision weight matrix (W) into three components: a low-rank component (LR) and a backbone (Q). The low-rank component captures the most significant singular values of the matrix with higher precision (using BL and BR bits), while the backbone captures the remaining, less significant singular values with lower precision (BQ bits). This decomposition allows for flexible precision settings and compression of the model.", "section": "2 Problem Formulation"}, {"figure_path": "lkx3OpcqSZ/figures/figures_27_1.jpg", "caption": "Figure 3: Relative data-aware Frobenius norm error per iteration of CALDERA for selected matrices of LLaMa-2 7B layer 25. For all experiments, the bit precision of Q is 2, and the calibration dataset is the same as used in \u00a75. The first iteration of CALDERA with the Hessian update is omitted, as it has a large error, inhibiting plot readability.", "description": "This figure shows the relative Frobenius norm error per iteration for several variants of the CALDERA algorithm and the QuIP# algorithm.  Different lines represent different configurations of CALDERA (e.g., 4-bit factors, 16-bit factors, with/without Hessian update, with/without randomized Hadamard transform). The results show the convergence behavior of the various methods for different low-rank sizes (64, 128, and 256).", "section": "F Additional Numerical Simulations"}, {"figure_path": "lkx3OpcqSZ/figures/figures_28_1.jpg", "caption": "Figure 4: Relative data-aware Frobenius norm error per iteration of LPLRFACTORIZE, for the decomposition W \u2248 LR, for two matrices in LLaMa-2 7B layer 25.", "description": "This figure shows the convergence of the LPLRFACTORIZE algorithm for two weight matrices in the 25th layer of the LLaMa-2 7B model.  The plots illustrate the relative data-aware Frobenius norm error at each iteration of the algorithm, for different target ranks (64, 128, 256). The randomized Hadamard transform was applied to the weight matrices before factorization, and both low-rank factors were quantized to 4 bits using an E8 lattice quantizer.  The figure demonstrates that the alternating minimization steps in the algorithm effectively reduce the approximation error, although the degree of improvement varies depending on the weight matrix and the target rank.", "section": "F Additional Numerical Simulations"}]