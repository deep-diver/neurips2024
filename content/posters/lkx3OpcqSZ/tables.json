[{"figure_path": "lkx3OpcqSZ/tables/tables_7_1.jpg", "caption": "Table 1: Zero-shot perplexities (denoted by \u2193) and accuracies (\u2191) for LLaMa-2. BQ = 2 bits throughout.", "description": "This table presents the zero-shot performance results for the LLaMa-2 model (7B, 13B, and 70B parameters) after applying the CALDERA compression algorithm.  It compares the performance of CALDERA with different rank (k) values of low-rank factors (L and R) against QuIP# baselines (with and without fine-tuning) and the original unquantized model. The evaluation metrics include perplexity on WikiText2 and C4 datasets, as well as zero-shot accuracy on several tasks (Winogrande, RTE, PiQA, ARC-Easy, and ARC-Challenge).  The \"Avg Bits\" column represents the average number of bits used per parameter in the compressed model.", "section": "5.1 Zero-shot Results"}, {"figure_path": "lkx3OpcqSZ/tables/tables_7_2.jpg", "caption": "Table 2: Zero-shot perplexities (denoted by \u2193) and accuracies (\u2191) for LLaMa-3 8B. BQ = 2 bits throughout.", "description": "This table presents the results of zero-shot evaluations on the LLaMa-3 8B model after applying different compression methods.  It compares CALDERA with different rank values (64, 128, 256) and bit configurations for low-rank factors (4-bit and 16-bit) against QuIP# (without fine-tuning) and the uncompressed baseline.  The evaluation metrics include perplexity on Wiki2 and C4 datasets, and zero-shot accuracy on Winogrande, RTE, PiQA, ArcE, and ArcC tasks. Lower perplexity values and higher accuracy percentages indicate better performance.", "section": "5.1 Zero-shot Results"}, {"figure_path": "lkx3OpcqSZ/tables/tables_8_1.jpg", "caption": "Table 3: Zero-shot perplexities and accuracies for LLaMa-2 7B, with end-to-end fine-tuning of randomized Hadamard transform parameters. BQ = 2 bits throughout. *See Footnote 1.", "description": "The table presents zero-shot perplexity and accuracy results on various tasks for Llama-2 7B model after applying CALDERA with different parameter settings.  It compares the results to QuIP# baselines with and without fine-tuning, and also shows unquantized results. It highlights the impact of varying target rank, bit precision of low rank factors, and whether or not randomized Hadamard transform parameter fine-tuning is employed.  Lower perplexity is better; higher accuracy is better.", "section": "5 Numerical Simulations"}, {"figure_path": "lkx3OpcqSZ/tables/tables_8_2.jpg", "caption": "Table 2: Zero-shot perplexities (denoted by \u2193) and accuracies (\u2191) for LLaMa-3 8B. BQ = 2 bits throughout.", "description": "This table presents the results of zero-shot experiments conducted on the LLaMa-3 8B model.  The experiments involved quantizing the model using different configurations of the CALDERA algorithm, varying the rank of the low-rank factors (L and R) and their bit precision.  The table reports perplexity scores (lower is better) and zero-shot accuracies (higher is better) on several downstream tasks, including language modeling (Wiki2 and C4) and commonsense reasoning (Winogrande, RTE, PiQA, ArcE, ArcC).  Results for QuIP# (without fine-tuning) are included for comparison, demonstrating the effectiveness of CALDERA's low-rank and low-precision approach.", "section": "5 Numerical Simulations"}, {"figure_path": "lkx3OpcqSZ/tables/tables_9_1.jpg", "caption": "Table 1: Zero-shot perplexities (denoted by \u2193) and accuracies (\u2191) for LLaMa-2. BQ = 2 bits throughout.", "description": "The table presents the results of zero-shot experiments on the LLaMa-2 model (7B, 13B, and 70B parameters) after applying the CALDERA algorithm with different compression parameters.  It compares CALDERA's performance against QuIP# (without fine-tuning) as a baseline. The metrics used include perplexity on the Wiki2 and C4 datasets, and zero-shot accuracy on Winogrande, RTE, PiQA, ARC-Easy, and ARC-Challenge datasets.  The table shows perplexity (lower is better) and accuracy (higher is better) for various model sizes and hyperparameter settings (rank and bit budget for the low-rank components).", "section": "5.1 Zero-shot Results"}, {"figure_path": "lkx3OpcqSZ/tables/tables_9_2.jpg", "caption": "Table 6: Throughputs for meta-llama/Llama-2-{7,70}b-hf on an NVIDIA A10G GPU for a batch size and sequence length of 1 (Bq = 2 for all rows)", "description": "This table presents the throughput, measured in tokens per second, for different LLM compression methods using an NVIDIA A10G GPU. The throughput is compared for the uncompressed LLMs (Llama-2 7B and 70B),  CALDERA with different rank and bit-depth configurations, and QuIP#.  The batch size and sequence length are fixed at 1, and the number of bits for the backbone (Q) is consistently 2 for all experiments.", "section": "5.4 Autoregressive Generation Throughput"}, {"figure_path": "lkx3OpcqSZ/tables/tables_14_1.jpg", "caption": "Table 1: Zero-shot perplexities (denoted by \u2193) and accuracies (\u2191) for LLaMa-2. BQ = 2 bits throughout.", "description": "This table presents the zero-shot perplexity and accuracy results for the LLaMa-2 model (7B, 13B, and 70B parameters) compressed using CALDERA.  It shows the impact of different parameters (rank, bit-budget) on the performance of the compressed model compared to QuIP# (with and without fine-tuning).  Lower perplexity indicates better performance. The results are reported for the Wiki2 and C4 datasets, as well as for Winogrande, RTE, PiQA, ARC-Easy, and ARC-Challenge tasks.", "section": "5 Numerical Simulations"}, {"figure_path": "lkx3OpcqSZ/tables/tables_26_1.jpg", "caption": "Table 8: Hyperparameter settings for low-rank adaptation*. Batch size refers to the per-device batch size. All fine-tuning experiments are parallelized across four GPUs.", "description": "This table shows the hyperparameter settings used for low-rank adaptation fine-tuning experiments.  It details the dataset used, block size, batch size (per device), gradient accumulation steps, number of epochs, learning rate, weight decay, learning rate scheduler, and warmup steps for three different tasks: Wikitext2, RTE, and Winogrande. The asterisk indicates that additional details are available elsewhere in the paper.", "section": "5.3 Low Rank Adaptation (LoRA) Fine-tuning Results"}, {"figure_path": "lkx3OpcqSZ/tables/tables_28_1.jpg", "caption": "Table 9: Evaluations of Wikitext2 and C4 perplexities, as well as percent accuracies on some common language modeling benchmarks, on CALDERA-compressed Mistral 7B. All quantizations use calibration datasets released on Huggingface by the authors of QuIP#. BQ = 2 bits throughout, and BL = BR = 4 bits where low-rank factors are present. For fairness of comparison, QuIP# numbers reported do not include RHT finetuning.", "description": "This table shows the performance comparison between CALDERA and QuIP# methods on Mistral 7B model for various tasks.  The performance metrics include perplexity scores on WikiText2 and C4 datasets, and zero-shot accuracies on Winograd Schema Challenge, Physical Interaction Question Answering, ARC-Easy, and ARC-Challenge datasets.  Both methods use calibration datasets from HuggingFace and have 2-bit quantization for the backbone. However, CALDERA further uses 4-bit quantization for low-rank factors. The table demonstrates that CALDERA achieves lower perplexity and generally higher accuracies than QuIP#.", "section": "F.2 Experiments on Mistral-7B"}]