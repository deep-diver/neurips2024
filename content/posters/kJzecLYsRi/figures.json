[{"figure_path": "kJzecLYsRi/figures/figures_3_1.jpg", "caption": "Figure 1: Convergence rates of spectral algorithm with qualification \u03c4 = 2 in Theorem 4.1, Theorem 4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension d. We present four graphs corresponding to four kinds of source conditions: s = 0.01, 1, 3, 5. The x-axis represents asymptotic scaling, \u03b3 : n = d\u03b3; the y-axis represents the convergence rate of excess risk, r: Excess risk = d<sup>r</sup>.", "description": "This figure shows the convergence rates of spectral algorithms with qualification \u03c4=2 for four different source conditions (s = 0.01, 1, 3, 5). Each subplot shows the convergence rate curves for kernel methods against the minimax optimal rate, as a function of the asymptotic scaling \u03b3 (n=d<sup>\u03b3</sup>).  The plots illustrate several key phenomena observed in large-dimensional spectral algorithms including the polynomial approximation barrier, periodic plateau behavior, and the saturation effect.  The x-axis represents the asymptotic scaling, and the y-axis represents the convergence rate of the excess risk.", "section": "4 Exact convergence rate on the excess risk of spectral algorithms"}, {"figure_path": "kJzecLYsRi/figures/figures_13_1.jpg", "caption": "Figure 2: Convergence rates of spectral algorithms with qualification \u03c4 = 1 (KRR), \u03c4 = 2 (iterated ridge regression), \u03c4 = 4 (iterated ridge regression), and \u03c4 = \u221e (kernel gradient flow) in Theorem 4.1, Theorem 4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension d. We present four graphs corresponding to four kinds of source conditions: s = 0.01, 1, 3, 5. The x-axis represents asymptotic scaling, \u03b3 : n = d; the y-axis represents the convergence rate of excess risk, r: Excess risk = d.", "description": "This figure presents a graphical illustration of the convergence rates of several spectral algorithms (KRR, iterated ridge regression, kernel gradient flow) in large dimensions.  It shows how these rates change depending on the smoothness of the regression function (source condition s) and the relationship between the sample size and dimension (asymptotic scaling \u03b3). The figure highlights the polynomial approximation barrier, periodic plateau behavior, and saturation effect observed in large-dimensional spectral algorithms.", "section": "4 Exact convergence rate on the excess risk of spectral algorithms"}, {"figure_path": "kJzecLYsRi/figures/figures_15_1.jpg", "caption": "Figure 3: Results of Experiment 1. We repeated each experiment 50 times and reported the average excess risk for (a) kernel gradient flow (labeled as \"kernel regression\" in our reports) and (b) kernel ridge regression (KRR) on 1000 test samples. We randomly selected y\u2081, y\u2082, y\u2083 and kept them fixed for each repeat. We choose the stopping time t in kernel gradient flow as C\u2081n0.5, where C\u2081 \u2208 {0.001, 0.01, 0.1, 1, 10, 100, 1000}. We use 5-fold cross-validation to select the regularization parameter \u03bb in kernel ridge regression. The alternative values of \u03bb in cross-validation are C\u2082n-C\u00b3, where C\u2082 \u2208 {0.001, 0.005, 0.01, 0.1, 0.5, 1, 2, 5, 10, 40, 100, 300, 1000}, C\u2083 \u2208 {0.1, 0.2, ..., 1.5}.", "description": "This figure shows the results of Experiment 1, comparing kernel gradient flow and kernel ridge regression.  The experiment was repeated 50 times with 1000 test samples.  The stopping time for gradient flow and regularization parameter for KRR were tuned using different methods.  The figure shows the convergence rate of the excess risk for each method with different values of \u03b3 (n = d\u03b3).", "section": "A.2 Numerical experiments"}, {"figure_path": "kJzecLYsRi/figures/figures_16_1.jpg", "caption": "Figure 3: Results of Experiment 1. We repeated each experiment 50 times and reported the average excess risk for (a) kernel gradient flow (labeled as \"kernel regression\" in our reports) and (b) kernel ridge regression (KRR) on 1000 test samples. We randomly selected  and kept them fixed for each repeat. We choose the stopping time t in kernel gradient flow as , where  We use 5-fold cross-validation to select the regularization parameter  in kernel ridge regression. The alternative values of  in cross-validation are , where ", "description": "The figure displays the results of Experiment 1, which compares the performance of kernel gradient flow and kernel ridge regression (KRR) in terms of excess risk.  The experiment was repeated 50 times, and the average excess risk was calculated for both algorithms using 1000 test samples.  The stopping time t for kernel gradient flow was varied, and 5-fold cross-validation was used to select the regularization parameter \u03bb for KRR. Four subfigures (a-d) are presented, each corresponding to a different value of \u03b3 (n=d^\u03b3), showcasing the convergence rates of both algorithms under different scaling parameters. The x-axis represents the logarithmic scale of sample size (n), and the y-axis represents the logarithmic scale of excess risk.", "section": "A.2 Numerical experiments"}, {"figure_path": "kJzecLYsRi/figures/figures_17_1.jpg", "caption": "Figure 5: Results of Experiment 2. It can be seen that the best rate of excess risk for KRR is slower than that of kernel gradient flow.", "description": "This figure shows the results of Experiment 2, which aimed to illustrate the saturation effect of KRR when s > 1.  Two kernels were used: NTK and RBF. The x-axis represents the log10 of the sample size (n), and the y-axis represents the log10 of the excess risk. The figure compares the convergence rates of kernel gradient flow and KRR for a regression function with s = 1.9. The lines represent the fitted convergence rates. The results confirm that when the regression function is sufficiently smooth (s > 1), KRR's convergence rate is slower than that of kernel gradient flow.", "section": "A.2 Numerical experiments"}]