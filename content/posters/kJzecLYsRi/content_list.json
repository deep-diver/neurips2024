[{"type": "text", "text": "On the Saturation Effects of Spectral Algorithms in Large Dimensions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weihao Lu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haobo Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics and Data Science Tsinghua University Beijing, China 100084 luwh19@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics and Data Science Tsinghua University Beijing, China 100084   \nzhang-hb21@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Yicheng Li Department of Statistics and Data Science Tsinghua University Beijing, China 100084 liyc22@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Qian Lin\\* Department of Statistics and Data Science Tsinghua University Beijing, China 100084 qianlin@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The saturation effects, which originally refer to the fact that kernel ridge regression (KRR) fails to achieve the information-theoretical lower bound when the regression function is over-smooth, have been observed for almost 20 years and were rigorously proved recently for kernel ridge regression and some other spectral algorithms over a fixed dimensional domain. The main focus of this paper is to explore the saturation effects for a large class of spectral algorithms (including the KRR, gradient descent, etc.) in large dimensional settings where $n\\asymp d^{\\gamma}$ .More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor). Similar to the results in KRR, we can further determine the exact convergence rates (both upper and lower bounds) of a large class of (optimal tuned) spectral algorithms with different qualification $\\tau$ 's. In particular, we find that these exact rate curves (varying along $\\gamma$ ) exhibit the periodic plateau behavior and the polynomial approximation barrier. Consequently, we can fully depict the saturation effects of the spectral algorithms and reveal a new phenomenon in large dimensional settings (i.e., the saturation effect occurs in large dimensional setting as long as the source condition $s>\\tau$ while it occurs in fixed dimensional setting as long as $s>2\\tau$ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Let's assume we have $n$ i.i.d. samples $\\left({x_{i},y_{i}}\\right)$ from a joint distribution supported on $\\mathbb{R}^{d}\\times\\mathbb{R}$ .The regression problem, one of the most fundamental problems in statistics, aims to find a function $\\hat{f}$ based on these samples such that the excess risk, $\\|\\hat{f}-f_{\\star}\\|_{L^{2}}^{2}=\\mathbb{E}_{x}[(f_{\\star}(x)-\\hat{f}(x))^{2}]$ is small, where $f_{\\star}(x)=\\mathbb{E}[Y|x]$ is the regression function. Many non-parametric regression methods are proposed to solve the regression problem by assuming that $f_{\\star}$ falls into certain function classes, including polynomial splines Stone (1994), local polynomials Cleveland (1979); Stone (1977), the spectral algorithms Caponnetto (2006); Caponnetto and De Vito (2007); Caponnetto and Yao (2010), etc. ", "page_idx": 0}, {"type": "text", "text": "Spectral algorithms, as a classical topic, have been studied since the 1990s. Early works treated certain types of spectral algorithms in their theoretical analysis (Caponnetto (2006); Caponnetto and De Vito (2007); Raskutti et al. (2014); Lin et al. (2020)). These works often consider $d$ as a fixed constant and impose the polynomial eigenvalue decay assumption under a kernel (i.e., there exist constants $0<\\mathfrak{c}\\le\\mathfrak{C}<\\infty$ , such that the eigenvalues of the kernel satisfy $\\mathfrak{c}j^{-\\beta}\\leq\\lambda_{j}\\leq\\mathfrak{C}j^{-\\beta}$ \uff0c $j\\geq1$ for certain $\\beta>1$ depending on the fixed $d$ ). They further assume that $f_{\\star}$ belongs to the reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$ associated with the kernel. Under the above assumptions, they then showed that the minimax rate of the excess risk of regression over the corresponding RKHS is lower bounded by $n^{-\\beta/(\\beta+1)}$ and that some (regularized) spectral algorithms, e.g.,the kernel ridge regression (KRR) and the kernel gradient flow, can produce estimators achieving this minimax optimal rate. ", "page_idx": 1}, {"type": "text", "text": "However, subsequent studies have revealed that when higher regularity (or smoothness) of $f_{\\star}$ is assumed, KRR fails to achieve the information-theoretical lower bound on the excess risk, while kernel gradient flow can do so. Specifically, let's assume that $f_{\\star}$ belongs to the interpolation space $[\\mathcal{H}]^{s}$ of the RKHS $\\mathcal{H}$ with $s>0$ (see, e.g., Steinwart et al. (2009); Dieuleveut et al. (2017); Dicker et al. (2017); Pillaud-Vivien et al. (2018); Lin et al. (2020); Fischer and Steinwart (2020); Celisse and Wahl (2021)). It is then shown that the information-theoretical lower bound on the excess risk is $n^{-s\\beta/(s\\dot{\\beta}+1)}$ When $0\\,<\\,s\\,\\leq\\,2$ ,Caponnetto and De Vito (2007); Yao et al. (2007); Lin et al. (2020); Zhang et al. (2023) have already shown that the upper bound of the excess risks of both KRR and the kernel gradient fow is $n^{-s\\beta/(\\dot{s}\\beta+1)}$ , and hence they are minimax optimal. On the contrary, when $s\\,>\\,2$ , Yao et al. (2007); Lin et al. (2020) showed that the upper bound of the excess risks of kermel gradient flow is $n^{-s\\beta/(s\\beta+1)}$ while the best upper bound of the excess risks of KRR is $n^{-2\\beta/(2\\beta+1)}$ (Caponneto and De Vito (2007). Bauer et al. (2007); Gerfo et al. (2008); Dicker et al. (2017) conjectured that the convergence rate of KRR is bounded below by $n^{-2\\beta/(2\\beta+1)}$ and Li et al. (2022) rigorously proved it. The above phenomenon is often referred to as the saturation effect of KRR: ", "page_idx": 1}, {"type": "text", "text": "KRR is inferior to certain spectral algorithms, such as kernel gradient flow, when $s>2$ ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In recent years, neural network methods have gained tremendous success in many large-dimensional problems, such as computer vision He et al. (2016); Krizhevsky et al. (2017) and natural language processing Devlin (2018). Several groups of researchers tried to explain the superior performance of neural networks on large-dimensional data from the aspects of \"lazy regime\" (Arora et al. (2019); Du et al. (2019, 2018); Li and Liang (2018)). They noticed that, when the width of a neural network is sufficiently large, its parameters/weights stay in a small neighborhood of their initial position during the training process. Later, Jacot et al. (2018); Arora et al. (2019); Hu et al. (2021); Suh et al. (2021); Lai et al. (2023); Li et al. (2024) proved that the time-varying neural network kernel (NNK) converges (uniformly) to a time-invariant neural tangent kernel (NTK) as the width of the neural network goes to infinity, and thus the excess risk of kernel gradient flow with NTK converges (uniformly) to the excess risk of neural networks in the \u201clazy regime'. ", "page_idx": 1}, {"type": "text", "text": "Inspired by the concepts of the \"lazy regime\" and the uniform convergence of excess risk, the machine learning community has experienced a renewed surge of interest in large-dimensional spectral algorithms. The earliest works focused on the consistency of two specific types of spectral algorithms: KRR and kernel interpolation (Liang and Rakhlin (2020); Liang et al. (2020); Ghorbani et al. (2020, 2021); Mei et al. (2021, 2022); Misiakiewicz and Mei (2022); Aerni et al. (2023); Barzilai and Shamir (2023)). In comparison, results on large-dimensional kernel gradient flow were somewhat scarce, and these results largely mirrored those associated with KRR (e.g., Ghosh et al. (2021). Recently, Lu et al. (2023) proved that large-dimensional kernel gradient fow is minimax optimal When $s=1$ . Then, Zhang et al. (2024) provided upper and lower bounds on the convergence rate on the excess risk of KRR for any $s>0$ . Surprisingly, they discovered that for $s>1$ ,theconvergence rate of KRR did not match the lower bound on the minimax rate. Unfortunately, they didn't prove that certain spectral algorithms can reach the lower bound on the minimax rate they provided, and hence they didn't rigorously prove that the saturation effect of KRR occurs in large dimensions. Instead, Zhang et al. (2024) only conjectured that certain spectral algorithms (e.g., kernel gradient fow) can provide minimax optimal estimators after their main results. ", "page_idx": 1}, {"type": "text", "text": "If Zhang et al. (2024)'s conjecture is true, then we can safely conclude that: when the regression function $f_{\\star}$ is smooth enough, KRR is inferior to kernel gradient fow in large dimensions as well. Consequently, previous results on large-dimensional KRR may not be directly extendable to largedimensional neural networks, even if the neural networks are in the \u201clazy regime'. The main focus of this paper is to prove this conjecture by showing that kernel gradient flow is minimax optimal in large dimensions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Saturation effects of fixed-dimensional spectral algorithms. When the dimension $d$ of the data is fixed, the saturation effect of KRR has been conjectured for decades and is rigorously proved in the recent work Li et al. (2022). Suppose $f_{\\star}\\in[\\mathcal{H}]^{s}$ with $s>2$ . It is shown that: (i) the minimax optimal rate is $n^{-s\\beta/(s\\beta+1)}$ (Rastogi and Sampath (2017); Yao et al. (2007); Lin et al. (2020); and (i) the convergence rate on the excess risk of KRR is $n^{-2\\beta/(2\\beta+1)}$ (Li et al. (2022)). More recently, Li et al. (2024) determined the exact generalization error curves of a class of analytic spectral algorithms, which allowed them to further show the saturation effect of spectral algorithms with finite qualification $\\tau$ (see, e.g., Appendix C): suppose $f_{\\star}\\in[\\mathcal{H}]^{s}$ with $s>2\\tau$ , then the convergence rate on the excess risk of the above spectral algorithms is $n^{-\\bar{2}\\tau\\bar{\\beta}/(2\\tau\\beta+1)}$ ", "page_idx": 2}, {"type": "text", "text": "New phenomena in large-dimensional spectral algorithms. In the large-dimensional setting where $n\\,\\asymp\\,d^{\\gamma}$ with $\\gamma\\,>\\,0$ , new phenomena exhibited in spectral algorithms are popular topics in recent machine-learning research. A line of work focused on the polynomial approximation barrier phenomenon (e.g., Ghorbani et al. (2021); Donhauser et al. (2021); Mei et al. (2022); Xia0 et al. (2023); Misiakiewicz (2022); Hu and Lu (2022)). They found that, for the square-integrable regression function, KRR and kernel gradient fow are consistent if and only if the regression function is a polynomial with a low degree. Another line of work considered the benign overfitting of kernel interpolation (i.e., kernel interpolation can generalize) (e.g., Liang and Rakhlin (2020); Liang et al. (2020); Aerni et al. (2023); Barzilai and Shamir (2023); Zhang et al. (2024)). More0ver, two recent work (Lu et al. (2023); Zhang et al. (2024)) discussed two new phenomena exhibited in large-dimensional KRR and kernel gradient fow: the multiple descent behavior and the periodic plateau behavior. The multiple descent behavior refers to the phenomenon that the curve of the convergence rate ( with respect to $n$ ) of the optimal excess risk is non-monotone and has several isolated peaks and valleys; while the periodic plateau behavior refers to the phenomenon that the curve of the convergence rate ( with respect to $d$ ) of the optimal excess risk has constant values when $\\gamma$ is within certain intervals. Finally, Zhang et al. (2024) conjectured that the saturation effect of KRR occurs in large dimensions. The above works imply that these phenomena occur in many spectral algorithms in large dimensions, hence encouraging us to provide a unified explanation of these new phenomena. ", "page_idx": 2}, {"type": "text", "text": "1.2 Our contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we focus on the large-dimensional spectral algorithms with inner product kernels, and we assume that the regression function falls into an interpolation space $[\\mathcal{H}]^{s}$ With $s>0$ Westateour main results as follows: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1 (Restate Theorem 4.1 and 4.2, non-rigorous). Let $\\colon s>0,\\,\\tau\\geq1$ and $\\gamma>0$ befixed real numbers. Denote p as the integer satisfying. $\\gamma\\in[p(s+1),(p+1)(s+1))$ .Then under certain conditions, the excess risk of large-dimensional spectral algorithm with qualification $\\tau$ satisfies ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\left\\Vert\\hat{f}_{\\lambda^{\\star}}-f_{\\star}\\right\\Vert_{L^{2}}^{2}\\;\\middle|\\;X\\right)=\\left\\{\\begin{array}{l l}{\\Theta_{\\mathbb{P}}\\left(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\right)\\cdot p o l y\\left(\\ln(d)\\right),\\quad s\\leq\\tau}\\\\ {\\Theta_{\\mathbb{P}}\\left(d^{-\\operatorname*{min}\\{\\gamma-p,\\frac{\\tau(\\gamma-p+1)+p\\tilde{s}}{\\tau+1},\\tilde{s}(p+1)\\}}\\right)\\cdot p o l y\\left(\\ln(d)\\right),\\quad s>\\tau,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tilde{s}=\\operatorname*{min}\\{s,2\\tau\\}$ ", "page_idx": 2}, {"type": "text", "text": "More specifically, we list the main contributions of this paper as follows: ", "page_idx": 2}, {"type": "text", "text": "(1) In Theorem 3.1, we show that the convergence rate on the excess risk of (optimally-tuned) kernel gradient flow in large dimensions is $\\Theta_{\\mathbb{P}}(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}})$ $\\mathtt{p o l y}(\\ln(d))$ ,which matches the lower bound on the minimax rate given in Theorem 3.3 (up to a logarithmic factor). We find that kernel gradient flow is minimax optimal for any $s>0$ and any $\\gamma>0$ and KRR is not minimax optimal for $s>1$ and for certain ranges of $\\gamma$ (We provide a visual illustration in Figure 2). Consequently, we rigorously prove that the saturation effect of KRR occurs in large dimensions. ", "page_idx": 2}, {"type": "text", "text": "(2) In Theorem 3.3, we enhanced the previous minimax lower bound results given in Lu et al. (2023) and Zhang et al. (2024). Specifically, we show that the minimax lower bound is $\\Omega(d^{-\\operatorname*{min}\\{\\gamma-p,s(p\\dot{+}1)\\}})/\\mathrm{poly}(\\ln(d))$ In comparison, the previous minimax lower bound is $\\Omega(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}})/d^{\\varepsilon}$ for any $\\varepsilon>0$ , and the additional term $d^{\\varepsilon}$ changes the desired convergence rate. ", "page_idx": 3}, {"type": "text", "text": "(3) In Section 4, we determine the convergence rate on the excess risk of large-dimensional spectral algorithms. From our results, we find several new phenomena exhibited in spectral algorithms in large-dimensional settings. We provide a visual illustration of the above phenomena in Figure 1: i) The first phenomenon is the polynomial approximation barrier, and as shown in Figure 1(a), when $s$ is close to zero, the curve of the convergence rate of spectral algorithm drops when $\\gamma\\approx p$ for any integer $p$ and will stay invariant for most of the other $\\gamma$ ; i) The second one is the periodic plateau behavior, and as shown in Figure 1(b) and Figure 1(c), when $0<s<2\\tau$ and $\\gamma\\in[p(\\bar{s}+1)+s+(\\operatorname*{max}\\{s,\\tau\\}-\\tau)/\\tau,(\\bar{p}+1)(s+1))$ for an integer $p\\geq0$ , the convergence rate does not change when $\\gamma$ varies; ii) The final one is the saturation effect, and as shown in Figure 1(c) and Figure 1(d), when $s>\\tau$ , the convergence rate of spectral algorithm can not achieve the minimax lower bound for certain ranges of $\\gamma$ . A detailed discussion about the above three phenomena can be found in Section 4. ", "page_idx": 3}, {"type": "image", "img_path": "kJzecLYsRi/tmp/6ad7711d65656ec1c2bf372b710108030558615ba9f5cdbd34eb03c79973dcd4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Convergence rates of spectral algorithm with qualification $\\tau=2$ in Theorem 4.1, Theorem 4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension $d$ We present four graphs corresponding to four kinds of source conditions: $s=0.01,1,3,5$ .The $\\mathbf{X}$ -axis represents asymptotic scaling, $\\gamma:n\\asymp d^{\\gamma}$ ; the y-axis represents the convergence rate of excess risk, $r:\\mathrm{Excess~risk}\\asymp d^{r}$ ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Suppose that we have observed $n$ i.i.d. samples $(x_{i},y_{i}),i\\in[n]$ from the model: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=f_{\\star}(x)+\\epsilon,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{i}$ 's are sampled from $\\rho_{\\mathcal{X}},\\rho_{\\mathcal{X}}$ is the marginal distribution on $\\mathcal{X}\\subset\\mathbb{R}^{d+1}$ \uff0c $y\\in\\mathcal{V}\\subset\\mathbb{R},f_{\\star}$ is some function defined on a compact set $\\mathcal{X}$ , and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(x,y)\\sim\\rho}\\left[\\epsilon^{2}\\left|\\mathbf{\\Lambda}x\\right|\\right]\\leq\\sigma^{2},\\quad\\rho_{\\mathcal{X}^{-\\mathbf{a}.\\mathbf{e}.}}\\mathbf{\\Lambda}x\\in\\mathcal{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some fixed constant $\\sigma>0$ , where $\\rho$ is the joint distribution of $(x,y)$ on $\\mathcal X\\times\\mathcal X$ . Denote the $n\\times1$ datavectorof $y_{i}$ 's and the $n\\times d$ datamatrixof $x_{i}$ 's by $Y$ and $X$ respectively. ", "page_idx": 3}, {"type": "text", "text": "2.1   Kernel ridge regression and kernel gradient fow ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this subsection, we introduce two specific spectral algorithms, kernel ridge regression and kernel gradient flow, which produce estimators of the regression function $f_{\\star}$ . A further discussion on general spectral algorithms will be provided in Section 4. ", "page_idx": 3}, {"type": "text", "text": "Throughout the paper, we denote $\\mathcal{H}$ as a separable RKHS on $\\mathcal{X}$ with respect to a continuous and positive definite kernel function $K(\\cdot,\\cdot):\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ and there exists a constant $\\kappa$ satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in{\\mathcal{X}}}K(x,x)\\leq\\kappa^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Kernel ridge regression Kernel ridge regresson KRR) constructs an estimator fKRR by solving the penalized least square problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{f}_{\\lambda}^{\\mathrm{KRR}}=\\underset{f\\in\\mathcal{H}}{\\arg\\operatorname*{min}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-f\\left(x_{i}\\right)\\right)^{2}+\\lambda\\|f\\|_{\\mathcal{H}}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda\\,>\\,0$ is referred to as the regularization parameter. The representer theorem (see, e.g., Steinwart and Christmann (2008)) gives an explicit expression of the KRR estimator, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{f}_{\\lambda}^{\\mathsf{K R R}}(x)=K(x,X)(K(X,X)+n\\lambda\\mathbf{I})^{-1}Y.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Kernel gradient fow The gradient flow of the loss function $\\begin{array}{r}{\\mathcal{L}=\\frac{1}{2n}\\sum_{i}(y_{i}-f(x_{i}))^{2}}\\end{array}$ induced a gradient flow in $\\mathcal{H}$ which is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathsf{d}}{\\mathsf{d}t}\\hat{f}_{t}^{\\mathsf{G F}}(x)=-\\frac{1}{n}K(x,X)(\\hat{f}_{t}^{\\mathsf{G F}}(X)-Y).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If we further assume that $\\hat{f}_{0}^{\\mathtt{G F}}(x)=0$ , then we can also give an explicit expression of the kernel gradient flow estimator ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{f}_{t}^{\\tt G F}(x)=K(x,X)K(X,X)^{-1}({\\bf I}-e^{-\\frac{1}{n}K(X,X)t})Y.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2.2  The interpolation space ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Define the integral operator $T_{K}$ as $\\begin{array}{r}{T_{K}(f)(x)=\\int K(x,x^{\\prime})f(x^{\\prime})\\;\\mathsf{d}\\rho_{\\mathcal{X}}(x^{\\prime})}\\end{array}$ . It is well known that $T_{K}$ is a positive, self-adjoint, trace-class, and hence a compact operator (Steinwart and Scovel (2012)). The celebrated Mercer's theorem further assures that ", "page_idx": 4}, {"type": "equation", "text": "$$\nK(x,x^{\\prime})=\\sum_{j}\\lambda_{j}\\phi_{j}(x)\\phi_{j}(x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the eigenvalues $\\{\\lambda_{j},j\\;=\\;1,2,...\\}$ is a non-increasing sequence, and the corresponding eigenfunctions $\\{\\phi_{j}(\\cdot),j=1,2,\\ldots\\}$ are orthonormal in $L^{2}(\\mathcal{X},\\rho_{\\mathcal{X}})$ function space. ", "page_idx": 4}, {"type": "text", "text": "The interpolation space $[\\mathcal{H}]^{s}$ with source condition $s$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n[\\mathcal{H}]^{s}:=\\Big\\{\\sum_{j}a_{j}\\lambda_{j}^{s/2}\\phi_{j}:(a_{j})_{j}\\in\\ell_{2}\\Big\\}\\subseteq L^{2}(\\mathcal{X},\\rho_{\\mathcal{X}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the inner product deduced from ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Big\\|\\sum_{j=1}^{\\infty}a_{j}\\lambda_{j}^{s/2}\\phi_{j}\\Big\\|_{[\\mathcal{H}]^{s}}=\\Big(\\sum_{j=1}^{\\infty}a_{j}^{2}\\Big)^{1/2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is easy to show that $[\\mathcal{H}]^{s}$ is also a separable Hibert space with orthonormal basis $\\{\\lambda_{j}^{s/2}\\phi_{j}\\}_{j}$ Generally speaking, functions in $[\\mathcal{H}]^{s}$ become smoother as $s$ increases (see, e.g., the example of Sobolev RKHS in Edmunds and Triebel (1996); Zhang et al. (2023). ", "page_idx": 4}, {"type": "text", "text": "2.3  Assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we list the assumptions that we need for our main results. ", "page_idx": 4}, {"type": "text", "text": "To avoid potential confusion, we specify the following large-dimensional scenario for kernel regression where we perform our analysis: suppose that there exist three positive constants $c_{1},\\,c_{2}$ and $\\gamma$ suchthat ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{1}d^{\\gamma}\\leq n\\leq c_{2}d^{\\gamma},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and we often assume that $d$ is sufficiently large. ", "page_idx": 4}, {"type": "text", "text": "In this paper, we only consider the inner product kernels defined on the sphere. An inner product kernel is a kernel function $K$ defined on $\\mathbb{S}^{d}$ such that there exists a function $\\Phi\\,:\\,[-1,1]\\,\\rightarrow\\,\\mathbb{R}$ independent of $d$ satisfying that for any $x,x^{\\prime}\\in\\mathbb{S}^{d}$ , we have $K(x,x^{\\prime})=\\Phi(\\langle x,x^{\\prime}\\rangle)$ . If we further ", "page_idx": 4}, {"type": "text", "text": "assume that the marginal distribution $\\rho_{\\mathcal{X}}$ is the uniform distribution on $\\mathcal{X}=\\mathbb{S}^{d}$ , then the Mercer's decomposition for $K$ can be rewritten as ", "page_idx": 5}, {"type": "equation", "text": "$$\nK(x,x^{\\prime})=\\sum_{k=0}^{\\infty}\\mu_{k}\\sum_{j=1}^{N(d,k)}Y_{k,j}(x)Y_{k,j}\\left(x^{\\prime}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Y_{k,j}$ for $j=1,\\cdot\\cdot\\cdot,N(d,k)$ are spherical harmonic polynomials of degree $k$ and $\\mu_{k}$ 's are the eigenvalues of K with multiplicity N(d, 0) = 1; N(d,k) = 2k+d-1 $\\begin{array}{r}{N(d,k)\\,=\\,\\frac{2k+d-1}{k}\\cdot\\frac{(k+d-2)!}{(d-1)!(k-1)!},k\\,=\\,1,2,\\cdot\\cdot\\cdot}\\end{array}$ For more details of the inner product kernels, readers can refer to Gallier (2009). ", "page_idx": 5}, {"type": "text", "text": "Remark 2.1. We consider the inner product kernels on the sphere mainly because the harmonic analysis is clear on the sphere ( e.g., properties of spherical harmonic polynomials are more concise than the orthogonal series on general domains). This makes Mercer's decomposition of the inner product more explicit rather than several abstract assumptions ( e.g., Mei and Montanari (2022). We also notice that very few results are available for Mercer's decomposition of a kernel defined on the general domain, especially when the dimension of the domain is taking into consideration. e.g., even the eigen-decay rate of the neural tangent kernels is only determined for the spheres. Restricted by this technical reason, most works analyzing the spectral algorithm in large-dimensional settings focus on the inner product kernels on spheres (Liang et al., 2020; Ghorbani et al., 2021; Misiakiewicz, 2022; Xiao et al., 2023; Lu et al., 2023, etc.). Though there might be several works that tried to relax the spherical assumption (e.g., Liang et al. (2020); Aerni et al. (2023); Barzilai and Shamir (2023), we can find that most of them (i) adopted a near-spherical assumption; (ii) adopted strong assumptions on the regression function, e.g., $\\bar{f}_{\\star}(x)=x[1]\\bar{x}[2]\\cdot\\cdot\\cdot x[L]$ for an integer $L>0$ ,where $x[i]$ denotes the $i^{\\th}$ -th component of $x$ ; or (i) can not determine the convergence rate on the excess risk of the spectral algorithm. ", "page_idx": 5}, {"type": "text", "text": "To avoid unnecessary notation, let us make the following assumption on the inner product kernel $K$ Assumption 1. $\\Phi(t)\\,\\in\\,\\mathcal{C}^{\\infty}\\left([-1,1]\\right)$ is a fixed function independent of $d$ and there exists a nonnegative sequence of absolute constants $\\{a_{j}\\geq0\\}_{j\\geq0}$ , such that we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Phi(t)=\\sum_{j=0}^{\\infty}a_{j}t^{j},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $a_{j}>0$ for any $j\\leq\\lfloor\\gamma\\rfloor+3$ ", "page_idx": 5}, {"type": "text", "text": "The purpose of Assumption 1 is to keep the main results and proofs clean. Notice that, by Theorem 1.b in Gneiting (2013), the inner product kernel $K$ on the sphere is semi-positive definite for all dimensions if and only if all coefficients $\\{a_{j},j=0,1,2,\\ldots\\}$ are non-negative. One can easily extend our results in this paper when certain coefficients $a_{k}$ 's are zero (e.g., one can consider the two-layer NTK defined as in Section 5 of Lu et al. (2023), with $a_{i}=0$ for any $i=3,5,7,\\cdot\\cdot\\cdot)$ ", "page_idx": 5}, {"type": "text", "text": "In the next assumption, we formally introduce the source condition, which characterizes the relative smoothnessof $f_{\\star}$ withrespecto $\\mathcal{H}$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Source condition). Suppose that $\\begin{array}{r}{f_{\\star}(x)=\\sum_{i=1}^{\\infty}f_{i}\\phi_{i}(x)}\\end{array}$ (a) $f_{\\star}\\in[\\mathcal{H}]^{s}$ for some $s>0$ , and there exists a constant $R_{\\gamma}$ only depending on $\\gamma$ , such that ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|f_{\\star}\\|_{[\\mathcal{H}]^{s}}\\leq R_{\\gamma}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(b) Denote $q$ as the smallest integer such that $q>\\gamma$ and $\\mu_{q}\\neq0$ . Define $\\mathcal{Z}_{d,k}$ as the index set satisfying $\\lambda_{i}\\equiv\\mu_{k},i\\in{\\mathcal{T}}_{d,k}$ . Further suppose that there exists an absolute constant $c_{0}>0$ such that for any $d$ and $k\\in\\{0,1,\\cdots,\\bar{q}\\}$ with $\\mu_{k}\\neq0$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{i\\in\\mathcal{Z}_{d,k}}\\mu_{k}^{-s}f_{i}^{2}\\ge c_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 2 is a common assumption when one is interested in the tight bounds on the excess risk of spectral algorithms (e.g., Caponnetto and De Vito (2007); Fischer and Steinwart (2020), Eq.(8) in Cui et al. (2021), Assumption 3 in Li et al. (2024), and Assumption 5 in Zhang et al. (2024)). Assumption 2 implies that the regression function exactly falls into the interpolation space $[\\mathcal{H}]^{s}$ , that is, $f_{\\star}\\bar{\\in}\\,[\\mathcal{H}]^{s}$ and $f_{\\star}\\notin[\\mathcal{H}]^{t}$ for any $t>s$ . For example, from the proof part I of Lemma D.14, one can check that $f_{\\star}$ with $\\begin{array}{r}{\\sum_{i\\in\\mathbb Z_{d,p}}\\mu_{p}^{-s}f_{i}^{2}=\\sum_{i\\in\\mathbb Z_{d,p+1}}\\mu_{p+1}^{-s}f_{i}^{2}=0}\\end{array}$ can have a faster convergence rate ontheexcessrisk. ", "page_idx": 5}, {"type": "text", "text": "Notations. Let's denote the norm in $L_{2}(\\mathcal{X},\\rho_{\\mathcal{X}})$ as $\\Vert\\cdot\\Vert_{L_{2}}$ . For a vector $x$ ,we use $x[i]$ to denote its $i$ -th component. We use asymptotic notations $O(\\cdot)$ \uff0c $o(\\cdot),\\;\\Omega(\\cdot)$ and $\\Theta(\\cdot)$ . For instance, we say two (deterministic) quantities $\\bar{U}(d),V(d)$ satisfy $U(d)\\,=\\,o(V(d))$ if and only if for any $\\varepsilon\\ >\\ 0$ , there exists a constant $D_{\\varepsilon}$ that only depends on $\\varepsilon$ and the absolute positive constants $\\sigma,\\kappa,s,\\gamma,c_{0},c_{1},c_{2},{\\mathfrak C}_{1},\\cdot\\cdot\\cdot\\,,{\\mathfrak C}_{8}>0$ , such that for any $d>D_{\\varepsilon}$ , we have $U(d)\\,\\bar{<}\\,\\varepsilon V(d)$ . We also write $a_{n}=\\mathrm{poly}(b_{n})$ if there exist a constant $\\theta\\ge0$ , such that $a_{n}=\\Theta(b_{n}^{\\theta})$ . We use the probability versions of the asymptotic notations such as $O_{\\mathbb{P}}(\\cdot),o_{\\mathbb{P}}(\\cdot),\\Omega_{\\mathbb{P}}(\\cdot),\\Theta_{\\mathbb{P}}(\\cdot)$ . For instance, we say the random variables $X_{n},Y_{n}$ satisfying $X_{n}=O_{\\mathbb{P}}(Y_{n})$ if and only if for any $\\varepsilon>0$ , there exist constants $C_{\\varepsilon}$ and $N_{\\varepsilon}$ such that $P\\left(|X_{n}|\\geq C_{\\varepsilon}|Y_{n}|\\right)\\leq\\varepsilon,\\dot{\\forall n}>N_{\\varepsilon}$ ", "page_idx": 6}, {"type": "text", "text": "2.4  Review of the previous results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The following two results are restatements of Theorem 2 and Theorem 5 in Zhang et al. (2024). ", "page_idx": 6}, {"type": "text", "text": "Proposition 2.2. Let $s\\geq1$ and $\\gamma>0$ be fixed real numbers. Denote $p$ as the integer satisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ .Suppose that Assumption $^{\\,I}$ and Assumption 2 hold for s and $\\gamma$ Let $\\hat{f}_{\\lambda}^{\\tt K R R}$ be the function defined in (2). Define $\\tilde{s}=\\operatorname*{min}\\{s,2\\}$ , then there exists $\\lambda^{\\star}>0$ such that we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(\\left\\|\\hat{f}_{\\lambda^{\\star}}^{\\mathtt{K R R}}-f_{\\star}\\right\\|_{L^{2}}^{2}\\;\\Big|\\;X\\right)=\\Theta_{\\mathbb{P}}\\left(d^{-\\operatorname*{min}\\left\\{\\gamma-p,\\frac{\\gamma-p+p\\bar{s}+1}{2},\\tilde{s}(p+1)\\right\\}}\\right)\\cdot p o l y\\left(\\ln(d)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Theta_{\\mathbb{P}}$ only involves constants depending on $s,\\sigma,\\gamma,c_{0},\\kappa,c_{1}$ and $c_{2}$ . In addition, the convergence rates of the generalization error can not be faster than above for any choice of regularization parameter $\\lambda=\\lambda(d,n)\\to0$ ", "page_idx": 6}, {"type": "text", "text": "Proposition 2.3 (Lower bound on the minimax rate). Let $s>0$ and $\\gamma>0$ befixed real numbers. Denote $p$ as the integer satisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ . Let $\\mathcal{P}$ consist of all the distributions $\\rho$ on $\\mathcal X\\times\\mathcal Y$ such that Assumption1 and Assumption 2hold for $s$ and $\\gamma.$ Thenforany $\\varepsilon>0$ wehave: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{f}}\\operatorname*{max}_{\\rho\\in\\mathcal{P}}\\mathbb{E}_{(X,Y)\\sim\\rho^{\\otimes n}}\\left\\|\\hat{f}-f_{\\star}\\right\\|_{L^{2}}^{2}=\\Omega\\left(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\cdot d^{-\\varepsilon}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Omega$ only involves constants depending on $s,\\sigma,\\gamma,c_{0},\\kappa,c_{1},c_{2}$ and $\\varepsilon$ ", "page_idx": 6}, {"type": "text", "text": "From the above two propositions, we can find that when $s>1$ , the convergence rate on the excess risk of KRR does not always match the lower bound on the minimax optimal rate. Zhang et al. (2024) further conjectured that the lower bound on the minimax optimal rate provided in Proposition 2.3 is tight (ignoring the additional term $d^{-\\varepsilon}$ ). Hence, they believed that the saturation effect exists for large-dimensional KRR. ", "page_idx": 6}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we determine the convergence rate on the excess risk of kernel gradient flow as $d^{-\\,\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}$ poly $(\\ln(d))$ whichdifers from the lowerboundon theminimax rate provided in Proposition 2.3 by $d^{\\varepsilon}$ for any $\\varepsilon\\:>\\:0$ .We then tighten the lower bound on the minimax rate $d^{-\\mathnormal{\\hat{\\mathrm{min}}}\\{\\gamma-p,s(p+1)\\}}/\\mathrm{poly}\\,(\\mathrm{ln}(\\dot{d}))$ Based on th aboveresults, we find tha KRis not minimax optimal for $s>1$ and for certain ranges of $\\gamma$ . Therefore, we show that the saturation effect of KRR occurs in large dimensions. ", "page_idx": 6}, {"type": "text", "text": "3.1 Exact convergence rate on the excess risk of kernel gradient flow ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first state our main results in this paper. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 (Kernel gradient flow). Let $s>0$ and $\\gamma>0$ be fixed real numbers. Denote $p$ as the integer satisfying $\\gamma\\in[\\bar{p}(s+1),(p+1)(s+1))$ .Suppose that Assumption $^{\\,l}$ andAssumption2hold for s and $\\gamma$ . Let $\\hat{f}_{t}^{\\tt G F}$ be the function defined in (4). Then there exists $t^{\\star}>0$ .such that we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(\\left\\|\\hat{f}_{t^{\\star}}^{\\mathtt{G F}}-f_{\\star}\\right\\|_{L^{2}}^{2}\\;\\Big|\\;X\\right)=\\Theta_{\\mathbb{P}}\\left(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\right)\\cdot p o l y\\left(\\ln(d)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Theta_{\\mathbb{P}}$ only involves constants depending on $s,\\sigma,\\gamma,c_{0},\\kappa,c_{1}$ and $c_{2}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 is a direct corollary of Theorem 4.1 and Example 2. Combining with the previous results in Proposition 2.3, or our modified minimax rate given in Theorem 3.3, we can conclude that large-dimensional kernel gradient fow is minimax optimal for any $s~>~0$ and any $\\gamma\\,>\\,0$ More importantly, the convergence rate of kernel gradient flow is faster than that of KRR given in Proposition 2.2 when (i) $1<s\\le2$ and $\\gamma\\in(p(s\\bar{+}1)+1,p(s+1)+2s-1)$ forsome $p\\in\\mathbb N$ or (ii) $s>2$ and $\\gamma\\in(p(s+1)+1,(p+1)(s+1))$ for some $p\\in\\mathbb N$ . Therefore, we have proved the saturation effect of KRR in large dimensions. ", "page_idx": 7}, {"type": "text", "text": "Remark 3.2. When $p\\geq1$ the logarithm term poly $(\\ln(d))$ in (12) can be removed. When $p=0$ we have poly $(\\ln(d))=(\\ln(d))^{2}$ in (12). See Appendix D.4 for details. ", "page_idx": 7}, {"type": "text", "text": "3.2  Improved minimax lower bound ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Recall that Proposition 2.3 gave a lower bound on the minimax rate as $d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\cdot d^{-\\varepsilon}$ The following theorem replaces the additional term $d^{-\\varepsilon}$ (which has changed the convergence rate) into a logarithm term poly $^{-1}$ $(\\ln(d))$ (which does not change the desired convergence rate). ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.3 (Improved minimax lower bound). Let $s>0$ and $\\gamma>0$ befixedrealnumbers.Denote $p$ astheintegersatisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ . Let $\\mathcal{P}$ consistofallthedistributions $\\rho$ on $\\mathcal X\\times\\mathcal X$ such that Assumption $^{\\,l}$ andAssumption2holdfor $s$ and $\\gamma$ .Then we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{f}}\\operatorname*{max}_{\\rho\\in\\mathcal{P}}\\mathbb{E}_{(X,Y)\\sim\\rho^{\\otimes n}}\\left\\|\\hat{f}-f_{*}\\right\\|_{L^{2}}^{2}=\\Omega\\left(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\right)\\middle/{p o l y}\\left(\\ln(d)\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Omega$ only involves constants depending on $s,\\sigma,\\gamma,c_{0},\\kappa,c_{1}$ and $c_{2}$ ", "page_idx": 7}, {"type": "text", "text": "4  Exact convergence rate on the excess risk of spectral algorithms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we will give tight bounds on the excess risks of certain types of spectral algorithms, such as kernel ridge regression, iterated ridge regression, kernel gradient flow, and kernel gradient descent. ", "page_idx": 7}, {"type": "text", "text": "Given an analytic filter function $\\varphi_{\\lambda}(\\cdot)$ with qualification $\\tau\\geq1$ (refer to Appendix C for the definitions of analytic filter function and its qualification), we can define a spectral algorithm in the following way (see, e.g., Bauer et al. (2007). For any $y\\in\\mathbb R$ , let $K_{x}:\\mathbb{R}\\rightarrow\\mathcal{H}$ be given by $K_{x}(y)=y\\cdot K(x,\\cdot)$ whose adjoint $K_{x}^{*}:\\mathcal{H}\\rightarrow\\mathbb{R}$ is given by $K_{x}^{*}(f)=\\langle K(x,\\cdot),f\\rangle_{\\mathcal{H}}=f(x)$ Moreover, we denote by $T_{x}=K_{x}K_{x}^{*}$ and $\\begin{array}{r}{T_{X}={\\frac{1}{n}}\\sum_{i=1}^{n}T_{x_{i}}}\\end{array}$ We also define the sample basisfunction ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{g}z=\\frac{1}{n}\\sum_{i=1}^{n}K_{x_{i}}(y_{i})=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\cdot K(x_{i},\\cdot).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Now, the estimator of the spectral algorithm is defined by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{f}_{\\lambda}=\\varphi_{\\lambda}(T_{X})\\hat{g}_{Z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Many commonly used spectral algorithms can be constructed by certain analytic filter functions. We provide two examples (kernel ridge regression and kernel gradient flow) as follows, and put two more examples (iterated ridge regression and kernel gradient descent) in Appendix C. We provide rigorous proof for these examples in Lemma C.3. ", "page_idx": 7}, {"type": "text", "text": "Example 1 (Kernel ridge regression). The filter function of kernel ridge regression (KRR) is wellknowntobe ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varphi_{\\lambda}^{\\tt K R R}(z)=\\frac{1}{z+\\lambda},\\quad\\psi_{\\lambda}^{\\tt K R R}(z)=\\frac{\\lambda}{z+\\lambda},\\quad\\tau=1.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Example 2 (Kernel gradient flow). The filter function is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varphi_{\\lambda}^{\\tt G F}(z)=\\frac{1-e^{-t z}}{z},\\quad\\psi_{\\lambda}^{\\tt G F}(z)=e^{-t z},\\quad t=\\lambda^{-1},\\quad\\tau=\\infty.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For any analytic filter function $\\varphi_{\\lambda}$ with qualification $\\tau\\geq1$ and the corresponding estimator of the spectral algorithm defined in (15), the following two theorems provide exact convergence rates on the excess risk when (i) the regression function is less-smooth, i.e., we have $s\\leq\\tau$ and (ii) $s>\\tau$ ,where $s$ is the source condition coefficient of the regression function given in Assumption 2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. Let $0<s\\le\\tau$ and $\\gamma>0$ be fixed real numbers. Denote p as the integer satisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ .Suppose that Assumption $^{\\,l}$ and Assumption 2 hold for s and $\\gamma$ Let $\\varphi_{\\lambda}(z)$ be an analytic filter function and $\\hat{f}_{\\lambda}$ be the function defined in (15). Suppose one of the following conditions holds: ", "page_idx": 8}, {"type": "equation", "text": "$$\n(i)\\,\\tau=\\infty,\\quad(i i)\\,s>1/(2\\tau),\\quad(i i i)\\,\\gamma>((2\\tau+1)s)/(2\\tau(1+s));\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "thenthereexists $\\lambda^{\\star}>0$ such thatwehave ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(\\left\\|\\hat{f}_{\\lambda^{\\star}}-f_{\\star}\\right\\|_{L^{2}}^{2}\\;\\Big|\\;X\\right)=\\Theta_{\\mathbb{P}}\\left(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\right)\\cdot p o l y\\left(\\ln(d)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\Theta_{\\mathbb{P}}$ only involves constants depending on $s,\\sigma,\\gamma,c_{0},\\kappa,c_{1}$ and $c_{2}$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2. Let $s~>~\\tau$ and $\\gamma\\,>\\,0$ be fixed real numbers. Denote $p$ as the integer satisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ .Suppose that Assumption $^{\\,I}$ and Assumption 2 hold for s and $\\gamma$ .Let $\\varphi_{\\lambda}(z)$ be an analytic filter function and $\\hat{f}_{\\lambda}$ be the function defined in (15). Define $\\tilde{s}=\\operatorname*{min}\\{s,2\\tau\\}$ then there exists $\\lambda^{\\star}>0$ , such that we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(\\left\\|\\hat{f}_{\\lambda^{\\star}}-f_{\\star}\\right\\|_{L^{2}}^{2}\\;\\middle|\\;X\\right)=\\Theta_{\\mathbb{P}}\\left(d^{-\\operatorname*{min}\\left\\{\\gamma-p,\\frac{\\tau(\\gamma-p+1)+p\\tilde{s}}{\\tau+1},\\tilde{s}(p+1)\\right\\}}\\right)\\cdot p o l y\\left(\\ln(d)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\Theta_{\\mathbb{P}}$ only involves constants depending on $s,\\sigma,\\gamma,c_{0},\\kappa,c_{1}$ and $c_{2}$ . In addition, the convergence rates of the generalization error can not be faster than above for any choice of regularization parameter $\\lambda=\\lambda(d,n)\\to0$ ", "page_idx": 8}, {"type": "text", "text": "Remark 4.3. These theorems substantially generalize the results on exact generalization error bounds of analytic spectral algorithms under the fixed-dimensional setting given in Li et al. (2024). Although the \u201canalytic functional argument\" introduced in their proof is still vital for us to deal with the general spectral algorithms, their proof has to rely on the polynomial eigendecay assumption that $\\breve{\\lambda}_{j}\\asymp j^{-\\dot{\\beta}}$ (Assumption 1), which does not hold in large dimensions since the hidden constant factors in the assumption vary with $d11$ (Lu et al. (2023)). Hence, their proof is not easy to generalize to large-dimensional spectral algorithms. ", "page_idx": 8}, {"type": "text", "text": "We provide some graphical illustrations of Theorem 4.1 and Theorem 4.2 in Figure 1 (with $\\tau=2$ and in Appendix A (with $\\tau=1$ \uff0c $\\tau=2$ $\\tau=4$ , and $\\tau=\\infty$ , corresponding to KRR, iterated ridge regression in Example 3 and kernel gradient flow). ", "page_idx": 8}, {"type": "text", "text": "As a direct consequence of Theorem 3.3, Theorem 4.1, and Theorem 4.2, we find that for the spectral algorithm with estimator defined in (15), it is minimax optimal if $s\\leq\\tau$ and the conditions in Theorem 4.1 hold. Moreover, these results show several phenomena for large-dimensional spectral algorithms ", "page_idx": 8}, {"type": "text", "text": "Saturation effect of large-dimensional spectral algorithms with finite qualification.  In the   \nlarge-dimensional setting and for the inner product kernel on the sphere, our results show that the   \nsaturation effect of spectral algorithms occurs when $s>\\tau$ . As shown in Figure 1(c) and Figure 1(d), $s>\\tau$ tm. $\\lambda$ $d^{-\\operatorname*{min}\\{\\gamma-p,\\frac{\\tau(\\gamma-p+1)+p\\tilde{s}}{\\tau+1},\\tilde{s}(p+1)\\}}$   \n$d^{-\\,\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}$ ", "page_idx": 8}, {"type": "text", "text": "Periodic plateau behavior of spectral algorithms when $s\\ \\leq\\ 2\\tau$ .When $0~<~s~\\leq~2\\tau$ and $\\gamma\\in[p(s+1)+s+\\operatorname*{max}\\{s,\\tau\\}/\\tau-1,(p+1)(s+1))$ for an integer $p\\geq0$ , from Theorem 4.1 and Theorem 4.2, the convergence rate on the excess risk of spectral algorithm $d^{-s(p+1)}$ . The above rate does not change when $\\gamma$ varies, which can also be found in Figure 1(b) and Figure 1(c). BIn other words, if we fix a large dimension $d$ and increase $\\gamma$ (or equivalently, increase the sample size $n$ ), the optimal rate of excess risk of a spectral algorithm stays invariant in certain ranges. Therefore, in order to improve the rate of excess risk, one has to increase the sample size above a certain threshold. ", "page_idx": 8}, {"type": "text", "text": "Polynomial approximation barrier of spectral algorithms when $s\\rightarrow0$ .From Theorem 4.1, when $s$ is close to zero, the convergence rate $d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}$ is unchanged in the range $\\gamma\\in$ $[p(s+1)+s,(p+1)(s+1))$ , and increases in the short range $\\gamma\\in[p(s+1),p(s+1)+s)$ . In other words, the excess risk of spectral algorithms will drop when $\\gamma$ exceeds $p(s+1)\\approx p$ for any integer $p$ and will stay invariant for most of the other $\\gamma$ . We term the above phenomenon as the polynomial approximation barrier of spectral algorithms (borrowed from Ghorbani et al. (2021), and it can be illustrated by Figure 1(a) with $s=0.01$ ", "page_idx": 8}, {"type": "text", "text": "Remark 4.4. Ghorbani et al. (2021) discovered the polynomial approximation barrier of KRR. As shown by Figure 5 and Theorem 4 in Ghorbani et al. (2021), if $s=0$ and the true function falls into $L^{2}=[\\dot{H}]^{0}$ , then with high probability we have ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}\\!\\left(\\left\\|\\hat{f}_{\\lambda_{\\star}}^{\\mathrm{KRR}}-f_{\\star}\\right\\|_{L^{2}}^{2}\\right)-\\left\\|\\mathrm{P}_{>p}f_{\\star}\\right\\|_{L^{2}}^{2}\\right|\\leq\\varepsilon\\Big(\\left\\|f_{\\star}\\right\\|_{L^{2}}^{2}+\\sigma^{2}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $p$ is the integer satisfying $\\gamma\\,\\in\\,[p,p+1)$ \uff0c $\\lambda_{\\star}$ is defined as in Theorem 4 in Ghorbani et al. (2021), $\\mathrm{P}_{>\\ell}$ means the projection onto polynomials with degree $>\\ell$ , and $\\varepsilon$ is any positive real number. Notice that (18) implies that the excess risk of KRR will drop when $\\gamma$ exceeds any integer and will stay invariant for other $\\gamma$ , and is consistent with our results for spectral algorithms. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we rigorously prove the saturation effect of KRR in large dimensions. Let $s>0$ and $\\gamma>0$ be fixed real numbers, denote $p$ as the integer satisfying $\\gamma\\in[p(\\bar{s}+1),(p+1)(s+1))$ . Given that the kernel is an inner product kernel defined on the sphere and that $f_{\\star}$ falls into the interpolation space $[\\mathcal{H}]^{s}$ , we first show that the convergence rate on the excess risk of large-dimensional kernel gradient flow is $\\Theta_{\\mathbb{P}}\\left(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\right)$ \u00b7 poly $(\\ln(d))$ (Theorem 3.1), which is faster than that of KRR given in Zhang et al. (2024). We then determine the improved minimax lower bound as $\\Omega\\left(d^{-\\operatorname*{min}\\{\\gamma-p,s(p+1)\\}}\\right)^{-}$ /poly $(\\ln(d))$ (Theorem 3.3). Combining these results, we know that kernel gradient flow is minimax optimal in large dimensions, and KRR is inferior to kernel gradient flow in large dimensions. Our results suggest that previous results on large-dimensional KRR may not be directly extendable to large-dimensional neural networks if the regression function is over-smooth. ", "page_idx": 9}, {"type": "text", "text": "In Section 4, we generalize our results to certain spectral algorithms. We determine the convergence rate on the excess risk of large-dimensional spectral algorithms (Theorem 4.1 and Theorem 4.2). From these results, we find several new phenomena exhibited in large-dimensional spectral algorithms, including the saturation effect, the periodic plateau behavior, and the polynomial approximation barrier. ", "page_idx": 9}, {"type": "text", "text": "In this paper, we only consider the convergence rate on the excess risk of optimal-tuned largedimensional spectral algorithms with uniform input distribution on a hypersphere. We believe that several results in fixed-dimensional setings with input distribution on more general domains (e.g., Haas et al. (2024); Li et al. (2024)) can indeed be extended to large-dimensional settings, although we must carefully consider the constants that depend on $d$ . Furthermore, we believe that by considering the learning curve of large-dimensional spectral algorithms (i.e., the convergence rate on the excess risk of spectral algorithms with any regularization parameter $\\lambda>0$ ) or the convergence rate on the excess risk of large-dimensional kernel interpolation (i.e., KRR with $\\lambda=0$ ), further research can find a wealth of new phenomena compared with the fixed-dimensional setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Lin's research was supported in part by the National Natural Science Foundation of China (Grant 92370122, Grant 11971257). The authors are grateful to the reviewers for their constructive comments that greatly improved the quality and presentation of this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Aerni, M., M. Milanta, K. Donhauser, and F. Yang (2023). Strong inductive biases provably prevent harmless interpolation. arXiv preprint arXiv:2301.07605.   \nArora, S., S. Du, W. Hu, Z. Li, and R. Wang (2019). Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on MachineLearning,pp.322-332.PMLR.   \nArora, S., S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang (2019). On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems 32.   \nBarzilai, D. and O. Shamir (2023). Generalization in kernel regression under realistic assumptions. arXiv preprint arXiv:2312.15995.   \nBauer, FE, S. Pereverzev, and L. Rosasco (2007). On regularization algorithms in learning theory. Journal of Complexity 23(1), 52-72.   \nCaponneto, A. (2006, September). Optimal rates for regularization operators in learning theory. Technical Report CBCL Paper #264/AI Technical Report #062, Massachusets Institute of Technology.   \nCaponnetto, A. and E. De Vito (2007). Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics 7(3), 331-368.   \nCaponnetto, A. and Y. Yao (2010). Cross-validation based adaptation for regularization operator in learning theory. Analysis and Applications 8(02), 161-183.   \nCelisse, A. and M. Wahl (2021). Analyzing the discrepancy principle for kernelized spectral filter learning algorithms. Journal of Machine Learning Research 22(76), 1-59.   \nCleveland, W. S. (1979). Robust locally weighted regresion and smoothing scatterplots. Journal of the American Statistical Association 74(368), 829-836.   \nCui, H., B. Loureiro, F. Krzakala, and L. Zdeborova (2021). Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural Information Processing Systems 34, 10131-10143.   \nDevlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.   \nDicker, L. H., D. P. Foster, and D. Hsu (2017). Kernel ridge vs. principal component regression: Minimax bounds and the qualification of regularization operators. Electronic Journal of Statistics 11(1), 1022 - 1047.   \nDieuleveut, A., N. Flammarion, and F. Bach (2017). Harder, better, faster, stronger convergence rates for least-squares regression. Journal of Machine Learning Research 18(101), 1-51.   \nDonhauser, K., M. Wu, and F. Yang (2021). How rotational invariance of common kernels prevents generalization in high dimensions. In International Conference on Machine Learning, pp. 2804 2814. PMLR.   \nDu, S., J. Lee, H. Li, L. Wang, and X. Zhai (2019). Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-1685. PMLR.   \nDu, S. S., X. Zhai, B. Poczos, and A. Singh (2018). Gradient descent provably optimizes overparameterized neural networks. arXiv preprint arXiv: 1810.02054.   \nEdmunds, D. E. and H. Triebel (1996). Function Spaces, Entropy Numbers, Differential Operators. Cambridge: Cambridge University Press.   \nFischer, S. and I. Steinwart (2020). Sobolev norm learning rates for regularized least-squares algorithms. Journal of Machine Learning Research 21(205), 1-38.   \nGallier, J. (2009). Notes on spherical harmonics and linear representations of lie groups. preprint.   \nGerfo, L. L., L. Rosasco, F. Odone, E. D. Vito, and A. Verri (2008, 07). Spectral Algorithms for Supervised Learning. Neural Computation 20(7), 1873-1897.   \nGhorbani, B., S. Mei, T. Misiakiewicz, and A. Montanari (2020). When do neural networks outperform kernel methods? Advances in Neural Information Processing Systems 3, 14820- 14830.   \nGhorbani, B., S. Mei, T. Misiakiewicz, and A. Montanari (2021). Linearized two-layers neural networks in high dimension. The Annals of Statistics 49(2), 1029 - 1054.   \nGhosh, N., S. Mei, and B. Yu (2021). The three stages of learning dynamics in high-dimensional kernel methods. arXiv preprint arXiv:2111.07167.   \nGneiting, T. (2013). Strictly and non-strictly positive definite functions on spheres. Bernoulli 19(4), 1327 - 1349.   \nHas, M., D. Holzmuiller, U. Luxburg, and I Steinwart (2024). Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. Advances in Neural Information Processing Systems 36.   \nHe, K., X. Zhang, S. Ren, and J. Sun (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778.   \nHu, H. and Y. M. Lu (2022). Sharp asymptotics of kernel ridge regression beyond the linear regime. arXiv preprint arXiv:2205.06798.   \nHu, T, W. Wang, C. Lin, and G. Cheng (2021). Regularization matters: A nonparametric perspective on overparametrized neural network. In International Conference on Artificial Intelligence and Statistics, Pp. 829-837. PMLR.   \nJacot, A., F. Gabriel, and C. Hongler (2018). Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems 31.   \nKrizhevsky, A., I. Sutskever, and G. E. Hinton (2017). Imagenet classification with deep convolutional neural networks. Communications of the ACM 60(6), 84-90.   \nLai, J., M. Xu, R. Chen, and Q. Lin (2023). Generalization ability of wide neural networks on $\\mathbb{R}$ arXiv preprint arXiv:2302.05933.   \nLi, Y., W. Gan, Z. Shi, and Q. Lin (2024). Generalization error curves for analytic spectral algorithms under power-law decay. arXiv preprint arXiv:2401.01599.   \nLi, Y. and Y. Liang (2018). Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in Neural Information Processing Systems 31.   \nLi, Y., Z. Yu, G. Chen, and Q. Lin (2024). On the eigenvalue decay rates of a class of neural-network related kernel functions defined on general domains. Journal of Machine Learning Research 25(82), 1-47.   \nLi, Y, H. Zhang, and Q. Lin (2022). On the saturation effect of kernel ridge regression. In The Eleventh International Conference on Learning Representations.   \nLi, Y., H. Zhang, and Q. Lin (2024). On the asymptotic learning curves of kernel ridge regression under power-law decay. Advances in Neural Information Processing Systems 36.   \nLiang, T. and A. Rakhlin (2020). Just interpolate: Kernel Ridgeless\u201d regression can generalize. The Annals of Statistics 48(3), 1329 - 1347.   \nLiang, T., A. Rakhlin, and X. Zhai (2020). On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels. In Conference on Learning Theory, Ppp. 2683-2711. PMLR.   \nLin, J., A. Rudi, L. Rosasco, and V. Cevher (2020). Optimal rates for spectral algorithms with least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis 48(3), 868-890.   \nLu, W., H. Zhang, Y. Li, M. Xu, and Q. Lin (2023). Optimal rate of kernel regresson in large dimensions. arXiv preprint arXiv:2309.04268.   \nMei, S., T. Misiakiewicz, and A. Montanari (2021). Learning with invariances in random features and kernel models. In Conference on Learning Theory, Pp. 3351-3418. PMLR.   \nMei, S., T. Misiakiewicz, and A. Montanari (2022). Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration. Applied and Computational Harmonic Analysis 59, 3-84.   \nMei, S. and A. Montanari (2022). The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics 75(4), 667-766.   \nMisiakiewicz, T. (2022). Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression. arXiv preprint arXiv:2204.10425.   \nMisiakiewicz, T. and S. Mei (2022). Learning with convolution and pooling operations in kernel methods. Advances in Neural Information Processing Systems 35, 29014-29025.   \nPillaud-Vivien, L., A.Rudi, and F Bach (2018). Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. Advances in Neural Information Processing Systems 31.   \nRaskuti, G., M. J. Wainwright, and B. Yu (2014). Early stopping and non-parametric regression: An optimal data-dependent stopping rule. Journal of Machine Learning Research 15(11), 335-366.   \nRastogi, A. and S. Sampath (2017). Optimal rates for the regularized learning algorithms under general source condition. Frontiers in Applied Mathematics and Statistics 3, 3.   \nSteinwart, I. and A. Christmann (2008). Support vector machines. Springer Science & Business Media.   \nSteinwart I, D. Hush, and C. Scovel (2009). Optimal rates for regularized least squares regresson. In Conference on Learning Theory, pp. 79-93. PMLR.   \nSteinwart, I. and C. Scovel (2012). Mercer's theorem on general domains: On the interaction between measures, kernels, and rkhss. Constructive Approximation 35, 363-417.   \nStone, C. J. (1977). Consistent Nonparametric Regression. The Annals of Statistics 5(4), 595 - 620.   \nStone, C. J. (1994). The Use of Polynomial Splines and Their Tensor Products in Multivariate Function Estimation. The Annals of Statistics 22(1), 118 - 171.   \nSuh, N., H. Ko, and X. Huo (2021). A non-parametric regression viewpoint: Generalization of overparametrized deep relu network under noisy observations. In International Conference on Learning Representations.   \nXiao, L., H. Hu, T. Misiakiewicz, Y. M. Lu, and J. Pennington (2023). Precise learning curves and higher-order scaling limits for dot product kernel regression. Journal of Statistical Mechanics: Theory and Experiment 2023(11), 114005.   \nYang, Y. and A. Barron (1999). Information-theoretic determination of minimax rates of convergence. The Annals of Statistics 27(5), 1564 - 1599.   \nYao, Y., L. Rosasco, and A. Caponnetto (2007). On early stopping in gradient descent learning. Constructive Approximation 26, 289-315.   \nZhang, H., Y. Li, W. Lu, and Q. Lin (2023). On the optimality of misspecified kernel ridge regresson. In International Conference on Machine Learning, Pp. 41331-41353. PMLR.   \nZhang, H., Y. Li, W. Lu, and Q. Lin (2024). Optimal rates of kernel ridge regression under source condition in large dimensions. arXiv preprint arXiv:2401.01270.   \nZhang, H., W. Lu, and Q. Lin (2024). The phase diagram of kernel interpolation in large dimensions. arXiv preprint arXiv:2404.12597. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A  Graphical illustration and numerical experiments of main results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Graphical illustration of Theorem 3.1, Theorem 4.1, and Theorem 4.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that Theorem 3.1, Theorem 4.1, and Theorem 4.2 determined the convergence rate on the excess risk of: (i) large-dimensional kernel gradient flow with $s>0$ ; (ii) large-dimensional spectral algorithm with $\\tau\\geq1$ and $s\\leq\\tau$ ; and (i) large-dimensional spectral algorithm with $\\tau\\geq1$ and $s>\\tau$ ", "page_idx": 13}, {"type": "text", "text": "In Figure 1, we have provided a visual illustration of Theorem 4.1 and Theorem 4.2 when $\\tau=2$ Now, in Figure 2, we provide more visual illustrations of the results of spectral algorithms with $\\tau=1$ $\\tau=2$ $\\tau=4$ ,and $\\tau=\\infty$ , which correspond to kernel ridge regression (KRR), iterated ridge regression in Example 3, and kernel gradient flow. ", "page_idx": 13}, {"type": "image", "img_path": "kJzecLYsRi/tmp/734cf189be147f49248b6033ea55ad774fac2bd3e864fc889e1aec7f06c7bc30.jpg", "img_caption": ["Figure 2: Convergence rates of spectral algorithms with qualification $\\tau=1$ (KRR), $\\tau=2$ (iterated ridge regression), $\\tau=4$ (iterated ridge regression), and $\\tau=\\infty$ (kernel gradient flow) in Theorem 4.1, Theorem 4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension $d$ . We present four graphs corresponding to four kinds of source conditions: $s=0.01,1,3,5$ The $\\mathbf{X}$ -axis represents asymptotic scaling, $\\gamma:n\\asymp d^{\\gamma}$ ; the y-axis represents the convergence rate of excess risk, $r:\\mathrm{Excess~risk}\\asymp d^{r}$ "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2  Numerical experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conducted two experiments using two specific kernels: the RBF kernel and the NTK kernel. Experiment 1 was designed to confirm the optimal rate of kernel gradient flow and KRR when $s=1$ Experiment 2 was designed to illustrate the saturation effect of KRR when $s>1$ ", "page_idx": 14}, {"type": "text", "text": "Experiment 1: We consider the following two inner product kernels: ", "page_idx": 14}, {"type": "text", "text": "(i) RBF kernel with a fixed bandwidth: ", "page_idx": 14}, {"type": "equation", "text": "$$\nK^{\\mathrm{rbf}}(x,x^{\\prime})=\\exp\\Biggl\\{\\Biggl(-\\frac{\\|x-x^{\\prime}\\|_{2}^{2}}{2}\\Biggr)\\Biggr\\},~~x,x^{\\prime}\\in\\mathbb{S}^{d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(i) Neural Tangent Kernel (NTK) of a two-layer ReLU neural network: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{K^{\\mathrm{ntk}}(x,x^{\\prime}):=\\Phi(\\langle x,x^{\\prime}\\rangle),\\;\\;x,x^{\\prime}\\in\\mathbb{S}^{d},}\\\\ &{}&{\\mathrm{P}(t)=\\left[\\sin\\left(\\operatorname{arccos}t\\right)+2(\\pi-\\operatorname{arccos}t)t\\right]/(2\\pi).\\quad\\quad}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The RBF kernel satisfies Assumption 1. For the NTK, the coefficients of $\\Phi(\\cdot),\\,\\{a_{j}\\}_{j=0}^{\\infty}$ , satisfy $a_{j}>0,j\\in\\{0,1\\}\\cup\\{2,4,6,...\\}$ and $a_{j}=0,j\\in\\{3,5,7,...\\}$ (see, e.g., Lu et al. (2023)). As noted after Assumption 1, our results can be extended to inner product kernels with certain zero coefficients $a_{j}$ . Specifically, for any $\\gamma>0$ , as long as $a_{j}>0$ for $j=\\lfloor\\gamma\\rfloor$ \uff0c $\\lfloor\\gamma\\rfloor+1$ , the proof and convergence rate remain the same. Therefore, for $\\gamma<2$ in our experiments, the convergence rates for NTK will be the same as for the RBF kernel. ", "page_idx": 14}, {"type": "text", "text": "We used the following data generation procedure: ", "page_idx": 14}, {"type": "equation", "text": "$$\ny_{i}=f_{*}(x_{i})+\\epsilon_{i},\\;\\;i=1,\\ldots,n,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where each $x_{i}$ is i.i.d. sampled from the uniform distribution on $\\mathbb{S}^{d}$ ,and $\\epsilon_{i}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,1)$ ", "page_idx": 14}, {"type": "text", "text": "We selected the training sample sizes $n$ with corresponding dimensions $d$ such that $n=d^{\\gamma},\\gamma=$ 0.5, 1.0, 1.5, 1.8. For each kernel and dimension $d$ , we consider the following regression function $f_{*}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{*}(x)=K(u_{1},x)+K(u_{2},x)+K(u_{3},x),\\quad{\\mathrm{for~some}}\\quad u_{1},u_{2},u_{3}\\in\\mathbb{S}^{d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This function is in the RKHS $\\mathcal{H}$ , and it is easy to prove that, for any $u_{0}\\in\\mathbb{S}^{d}$ , Assumption 2 (b) holds for $K(u_{0},\\cdot)$ with $s=1$ . Therefore, Assumption 2 holds for $s=1$ . We used logarithmic least squares to fit the excess risk with respect to the sample size, resulting in the convergence rate $r$ . As shown in Figure 3 and Figure 4, the experimental results align well with our theoretical findings. ", "page_idx": 14}, {"type": "text", "text": "Experiment 2: We use most of the settings from Experiment 1, except that the regression function is changed to $f_{*}(x)\\;=\\;\\sqrt{\\mu_{2}^{s}N(d,2)}P_{2}(<\\;\\xi,x\\;>)$ With $s~=~1.9$ $P_{2}(t)\\,:=\\,(d t^{2}\\,-\\,1)/(d\\mathrm{~-~}1)$ the Gegenbauer polynomial, and $\\xi\\:\\in\\:\\mathbb{S}^{d}$ . Notice that the addition formula $P_{2}(<\\,\\xi,x\\,>)\\,=$ N(d.2 y,()Y2,3(z) implie that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|f_{*}\\|_{[\\mathcal{H}]^{s}}^{2}=\\frac{1}{N(d,2)}\\sum_{j=1}^{N(d,2)}Y_{2,j}^{2}(\\xi)=P_{2}(1)=1,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "hence $f_{*}\\in[\\mathcal{H}]^{s}$ and satisfies Assumption 2. ", "page_idx": 14}, {"type": "text", "text": "Our experiment settings are similar to those on page 30 of Li et al. (2022). We choose the regularization parameter for KRR and kernel gradient flow as $\\lambda=0.05\\cdot d^{-\\theta}$ . For KRR, since Corollary D.16 suggests that the optimal regularization parameter is $\\lambda\\asymp d^{-0.7}$ ,weset $\\theta=0.7$ . Similarly, based on Corollary D.16, we set $\\theta=0.5$ for kernel gradient flow. Additionally, we set $\\gamma=1.8$ . The results indicate that the best convergence rate of KRR is slower than that of kernel gradient flow, implying that KRR is inferior to kernel gradient flow when the regression function is sufficiently smooth. ", "page_idx": 14}, {"type": "text", "text": "B Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first restate Theorem 3.3. ", "page_idx": 14}, {"type": "image", "img_path": "kJzecLYsRi/tmp/491ff1afbe0294b40f0602122c52f7453873793c159167e33f8891844047c609.jpg", "img_caption": ["Figure 3: Results of Experiment 1. We repeated each experiment 50 times and reported the average excess risk for (a) kernel gradient fow (labeled as \"kernel regression\" in our reports) and (b) kernel ridge regression (KRR) on 1000 test samples. We randomly selected $u_{1},u_{2},u_{3}$ and kept them fixed for each repeat. We choose the stopping time $t$ in kernel gradient flow as $C_{1}n^{0.5}$ , where $C_{1}\\in\\{0.001,0.01,0.1,1,10,100,1000\\}$ We use 5-fold cross-validation to select the regularization parameter $\\lambda$ in kernel ridge regression. The alternative values of $\\lambda$ in cross-validation are $C_{2}n^{-C_{3}}$ where $C_{2}\\in\\{0.001,0.005,0.01,0.1,0.5,1,2,5,10,40,100,300,1000\\},\\scriptscriptstyle{0}$ $C_{3}\\in\\{0.1,0.2,\\ldots,1.5\\}$ ", "where $\\Omega$ only involves constants depending on $s,\\sigma,\\gamma,c_{0},\\kappa,c_{1}.$ and $c_{2}$ "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Theorem B.1 (Restate Theorem 3.3). Let $s>0$ and $\\gamma>0$ befixedreal numbers.Denotep as the integer satisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ . Let $\\mathcal{P}$ consist of all the distributions $\\rho$ on $\\mathcal X\\times\\mathcal X$ such that Assumption $^{\\,l}$ and Assumption 2 hold for s and $\\gamma$ Then for any $d\\geq\\mathfrak{C},$ a sufficiently large constant only depending on s ${\\mathfrak{z}},\\,{\\mathfrak{\\gamma}},\\,c_{1}$ ,and $c_{2}$ , we have the following claims: ", "page_idx": 15}, {"type": "text", "text": "(i) When $\\gamma\\in(p(s+1),p+p s+s],$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{f}}\\operatorname*{max}_{\\rho\\in\\mathcal{P}}\\mathbb{E}_{(X,Y)\\sim\\rho^{\\otimes n}}\\left\\|\\hat{f}-f_{\\star}\\right\\|_{L^{2}}^{2}\\ge\\frac{\\ln\\ln(d)}{50(\\gamma-p(s+1))(\\ln(d))^{2}}d^{p-\\gamma}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(ii) When $\\gamma\\in(p+p s+s,(p+1)(s+1)]$ ,we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{\\boldsymbol f}}\\operatorname*{max}_{\\rho\\in\\mathcal{P}}\\mathbb{E}_{(X,Y)\\sim\\rho^{\\otimes n}}\\left\\|\\hat{\\boldsymbol f}-\\boldsymbol f_{\\star}\\right\\|_{L^{2}}^{2}=\\Omega\\left(d^{-s(p+1)}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem B.1. The item (i) is a direct corollary of Theorem 5 in Zhang et al. (2024). Now we begin to proof the item (i). We need the following lemma. ", "page_idx": 15}, {"type": "image", "img_path": "kJzecLYsRi/tmp/a84dde09ff5dc834862a091dd67bf51b2499826b56d38a718dcad0fb62bae4e4.jpg", "img_caption": ["Figure 4: A similar plot as Figure 3, but with the RBF kernel. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Lemma B.2 (Restate Lemma 4.1 in Lu et al. (2023). For any $\\delta\\in(0,1)$ and any $0<\\tilde{\\varepsilon}_{1},\\tilde{\\varepsilon}_{2}<\\infty$ only depending on $n$ $d,$ $\\{\\lambda_{j}\\}$ \uff0c $c_{1},\\:c_{2}$ and $\\gamma$ and satisfying ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{V_{K}(\\widetilde{\\varepsilon}_{2},{\\mathcal D})+n\\widetilde{\\varepsilon}_{2}^{2}+\\ln(2)}{V_{2}(\\widetilde{\\varepsilon}_{1},{\\mathcal B})}\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{f}}\\operatorname*{max}_{\\rho\\in\\mathcal{P}}\\mathbb{E}_{(X,Y)\\sim\\rho^{\\otimes n}}\\left\\|\\hat{f}-f_{\\star}\\right\\|_{L^{2}}^{2}\\geq\\frac{1-\\delta}{4}\\tilde{\\varepsilon}_{1}^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\rho_{f_{\\star}}$ is the joint-p.d.f. of $x,y$ given by $(I)$ with $f=f_{\\star}$ $\\mathcal{B}:=\\left\\{f\\in\\mathcal{H},\\;\\|f\\|_{[\\mathcal{H}]^{s}}\\leq R_{\\gamma}\\right\\}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{D}:=\\left\\{\\rho_{f}\\ \\Big|\\ j o i n t\\ d i s t r i b u t i o n\\ \\rho f(y,x)\\ w h e r e\\ x\\sim\\rho x,y=f(x)+\\epsilon,\\epsilon\\sim N(0,\\sigma^{2}),f\\in\\mathcal{B}\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $V_{2},\\,V_{K}$ are the $\\varepsilon$ covering entropies ( as defined in Yang and Barron (1999); Lu et al. (2023)) of $(B,d^{2}=\\|\\cdot\\|_{L^{2}}^{2})$ and $(D,d^{2}=\\,K L$ divergence). ", "page_idx": 16}, {"type": "text", "text": "Suppose $\\gamma\\in(p(s+1),p+p s+s]$ Let $C(p)=\\mathfrak{C}_{12}/10$ be a constant only depending on $\\gamma$ where ${\\mathfrak{C}}_{12}$ are given in Lemma D.13. Then we introduce ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\varepsilon}_{1}^{2}\\triangleq d^{p-\\gamma}/\\ln(d)\\;\\mathrm{and}\\;\\tilde{\\varepsilon}_{2}^{2}\\triangleq C(p)\\frac{d^{p}}{n}\\ln\\ln(d).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "kJzecLYsRi/tmp/f31a8900f6f9f49e15ca140058de880e6b40e67632f0b31122ea5074e10ece82.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 5: Results of Experiment 2. It can be seen that the best rate of excess risk for KRR is slower than that of kernel gradient flow. ", "page_idx": 17}, {"type": "text", "text": "Let us further assume that $d\\geq\\mathfrak{C}$ where ${\\mathfrak C}$ is a sufficiently large constant only depending on $\\gamma,s$ and $c_{1}$ . By Lemma D.11 and Lemma D.13 we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\tilde{\\varepsilon}_{1}^{2}=d^{p-\\gamma}/\\ln(d)<\\frac{\\mathfrak{C}_{9}}{d^{p s}}\\leq\\mu_{p}^{s}}\\\\ {\\displaystyle\\mu_{p+1}^{s}<\\tilde{\\varepsilon}_{2}^{2}=C(p)\\frac{d^{p}}{n}\\ln\\ln(d)\\leq\\frac{C(p)}{c_{1}}d^{p-\\gamma}\\ln\\ln(d)<\\mu_{p}^{s}}\\\\ {\\displaystyle n\\tilde{\\varepsilon}_{2}^{2}\\overset{\\mathrm{Definition~of~}\\mathfrak{C}_{12}}{\\leq}\\frac{1}{10}N(d,p)\\ln\\ln(d).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for any $d\\geq\\mathfrak{C}$ where ${\\mathfrak C}$ is a sufficiently large constant only depending on $s,\\gamma$ and $c_{1}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2}(\\widetilde{\\varepsilon}_{1},\\mathcal{B})^{\\mathrm{\\tiny\\mathrm{~Lemma~A.Sin~Lu~tal.},~(2023)}}\\,K\\,(\\widetilde{\\varepsilon}_{1})\\geq\\frac{1}{2}N(d,p)\\ln\\bigg(\\frac{\\mu_{p}^{s}}{\\widetilde{\\varepsilon}_{1}^{2}}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{\\tiny\\mathrm{~Definition~of~}}\\widetilde{\\varepsilon}_{1}^{2}\\,\\;1}\\\\ &{\\qquad\\qquad\\qquad\\geq\\qquad\\frac{1}{2}N(d,p)\\ln\\Big(\\mathfrak{C}_{9}d^{\\gamma-p(s+1)}\\ln(d)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\frac{1}{2}N(d,p)\\,\\left[(\\gamma-p(s+1))\\ln(d)+\\frac{1}{2}\\ln\\ln(d)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, from Lemma D.11, Lemma D.13, and Lemma D.12, one can check the following claim: ", "page_idx": 17}, {"type": "text", "text": "Claim 1. Suppose $\\gamma\\;\\in\\;(p(s+1),p+p s+s]$ : For any $d\\,\\geq\\,\\mathfrak{C}$ where ${\\mathfrak C}$ is a suffciently large constant only depending on s, $\\gamma$ $c_{1}$ , and $c_{2}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nK\\left(\\sqrt{2}\\sigma\\tilde{\\varepsilon}_{2}/6\\right)\\leq\\frac{1}{2}N(d,p)\\ln\\left(\\frac{18\\mu_{p}^{s}}{\\sigma^{2}\\tilde{\\varepsilon}_{2}^{2}}\\ln\\ln(d)\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for any $d\\geq\\mathfrak{C}$ where ${\\mathfrak C}$ is a sufficiently large constant only depending on $s,\\gamma,c_{1}$ , and $c_{2}$ wehave ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{K}(\\tilde{\\varepsilon}_{2},\\mathcal{D})=V_{2}(\\sqrt{2}\\sigma\\tilde{\\varepsilon}_{2},\\mathcal{B})^{\\mathrm{\\tiny~Lemma~A.5~in~Lu~ta23}}\\,K\\left(\\sqrt{2}\\sigma\\tilde{\\varepsilon}_{2}/6\\right)}\\\\ &{\\qquad\\qquad\\stackrel{\\mathrm{Clain~1}}{\\leq}\\frac{1}{2}N(d,p)\\ln\\left(\\frac{18\\mu_{p}^{s}}{\\sigma^{2}\\tilde{\\varepsilon}_{2}^{2}}\\ln\\ln(d)\\right)}\\\\ &{\\mathrm{\\tiny~Definition~of}\\,\\tilde{\\varepsilon}_{2}^{2}\\,\\frac{1}{2}N(d,p)\\ln\\left(18\\mathfrak{C}_{10}\\sigma^{-2}[C(p)]^{-1}c_{2}d^{\\gamma-p(s+1)}\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{2}N(d,p)\\left[(\\gamma-p(s+1))\\ln(d)+\\frac{1}{5}\\ln\\ln(d)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining (23), (24), and (25), we finally have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{V_{K}(\\tilde{\\varepsilon}_{2},D)+n\\tilde{\\varepsilon}_{2}^{2}+\\ln(2)}{V_{2}(\\tilde{\\varepsilon}_{1},B)}\\leq\\frac{\\left[10(\\gamma-p(s+1))\\ln(d)+4\\ln\\ln(d)\\right]}{\\left[10(\\gamma-p(s+1))\\ln(d)+5\\ln\\ln(d)\\right]}<1,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and from Lemma B.2, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\hat{f}}{\\operatorname*{min}}\\underset{f_{\\star}\\in B}{\\operatorname*{max}}\\mathbb{E}_{(\\mathbf{X},\\mathbf{y})\\sim\\rho_{f_{\\star}}^{\\otimes n}}\\left\\|\\hat{f}-f_{\\star}\\right\\|_{L^{2}}^{2}\\geq\\frac{\\ln\\ln(d)}{4\\ln(d)\\left[10(\\gamma-p(s+1))\\ln(d)+5\\ln\\ln(d)\\right]}d^{p-\\gamma}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\frac{\\ln\\ln(d)}{50(\\gamma-p(s+1))(\\ln(d))^{2}}d^{p-\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "finishing the proof. ", "page_idx": 18}, {"type": "text", "text": "C  Definition of analytic filter functions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first introduce the following definition of analytic filter functions (Bauer et al. (2007); Li et al.   \n(2024). ", "page_idx": 18}, {"type": "text", "text": "Definition C.1 (Analytic filter functions). Let $\\left\\{\\varphi_{\\lambda}:\\left[0,\\kappa^{2}\\right]\\rightarrow\\mathbb{R}_{\\geq0}\\ |\\ \\lambda\\in\\left(0,1\\right)\\right\\}$ be a family of functions indexed with regularization parameter $\\lambda$ and define the remainder function ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\psi_{\\lambda}(z):=1-z\\varphi_{\\lambda}(z).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We say that $\\{\\varphi_{\\lambda}\\mid\\lambda\\in(0,1)\\}$ (or simply $\\varphi_{\\lambda}(z))$ is an analytic filter function if: ", "page_idx": 18}, {"type": "text", "text": "(1) $z\\varphi_{\\lambda}(z)\\in[0,1]$ is non-decreasing with respect to $z$ and non-increasing with respect to $\\lambda$ ", "page_idx": 18}, {"type": "text", "text": "(2) The qualification of this filter function is $\\tau\\,\\in\\,[1,\\infty]$ such that $\\forall\\;0\\leq\\tau^{\\prime}\\leq\\tau$ (and also $\\tau^{\\prime}<\\infty,$ , there exist positive constants $\\mathfrak{C}_{i}$ only depending on $\\tau^{\\prime}$ \uff0c $i={1,2,3,4,5}$ , such that we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varphi_{\\lambda}(z)\\geq\\mathfrak{C}_{1}z^{-1},\\quad\\psi_{\\lambda}(z)\\leq\\mathfrak{C}_{2}(z/\\lambda)^{-\\tau^{\\prime}},\\quad\\forall\\lambda\\in(0,1),z>\\lambda}\\\\ {\\mathfrak{C}_{3}\\leq\\lambda\\varphi_{\\lambda}(z)\\leq\\mathfrak{C}_{4},\\quad\\psi_{\\lambda}(z)\\geq\\mathfrak{C}_{5},\\quad\\forall\\lambda\\in(0,1),z\\leq\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(3) If $\\tau<\\infty$ , then there exists a positive constant ${\\mathfrak{C}}_{6}$ only depending on $\\tau$ and $\\lambda_{1}$ , such that we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\psi_{\\lambda}(\\lambda_{1})\\geq{\\mathfrak{C}}_{6}\\lambda^{\\tau},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\lambda_{1}$ is the largest eigenvalue of $K$ defined in (5); and there exist positive constants $\\mathfrak{C}_{7}$ and $\\mathfrak{C}_{8}$ only depending on $\\tau$ , such that we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{(z/\\lambda)^{2\\tau}\\psi_{\\lambda}^{2}(z)\\geq\\mathfrak{C}_{7},}&{\\forall\\lambda\\in(0,1),z>\\lambda}&\\\\ {(z/\\lambda)^{2\\tau}\\psi_{\\lambda}^{2}(z)\\leq\\mathfrak{C}_{8}z\\varphi_{\\lambda}(z),}&{\\forall\\lambda\\in(0,1),z\\leq\\lambda.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(4) Let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{D_{\\lambda}=\\left\\{z\\in\\mathbb{C}:\\operatorname{Re}z\\in[-\\lambda/2,\\kappa^{2}],\\ |\\mathrm{Im}\\,z|\\leq\\mathrm{Re}\\,z+\\lambda/2\\right\\}}}\\\\ {{\\cup\\left\\{z\\in\\mathbb{C}:\\left|z-\\kappa^{2}\\right|\\leq\\kappa^{2}+\\lambda/2,\\ \\mathrm{Re}\\,z\\geq\\kappa^{2}\\right\\};}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then $\\varphi_{\\lambda}(z)$ can be extended to be an analytic function on some domain containing $D_{\\lambda}$ and the following conditions holds for all $\\lambda\\in(0,1)$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|(z+\\lambda)\\varphi_{\\lambda}(z)|\\le\\tilde{E}\\;\\mathrm{for}\\;\\mathrm{all}\\;z\\in D_{\\lambda};}\\\\ &{|(z+\\lambda)\\psi_{\\lambda}(z)|\\le\\tilde{F}\\lambda\\;\\mathrm{for}\\;\\mathrm{all}\\;z\\in D_{\\lambda};}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Remark C.2. We remark that some of the above properties are not essential for the definition of filter functions in the literature (Bauer et al., 2007; Gerfo et al., 2008), but we introduce them to avoid some unnecessary technicalities in the proof. The requirements of analytic filter functions are first considered in Li et al. (2024) and used for their \u201canalytic functional argument', which will also be vital in our proof. ", "page_idx": 19}, {"type": "text", "text": "The following examples show many commonly used analytic filter functions and their proofs can be found in Lemma C.3, see also Li et al. (2024). ", "page_idx": 19}, {"type": "text", "text": "Example 3 (Iterated ridge regression). Let $q\\geq1$ be fixed. We define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varphi_{\\lambda}^{\\mathrm{IT},q}(z)=\\frac{1}{z}\\left[1-\\frac{\\lambda^{q}}{(z+\\lambda)^{q}}\\right],\\quad\\psi_{\\lambda}^{\\mathrm{IT},q}(z)=\\frac{\\lambda^{q}}{(z+\\lambda)^{q}},\\quad\\tau=q.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Example 4 (Kernel gradient descent). The gradient descent method is the discrete version of gradient flow.Let $\\eta>0$ be a fixed step size. Then, iterating gradient descent with respect to the empirical loss $t$ stepsyieldsthefilterfunction ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\varphi_{\\lambda}^{\\mathrm{GD}}(z)=\\eta\\sum_{k=0}^{t-1}(1-\\eta z)^{k}=\\frac{1-(1-\\eta z)^{t}}{z},\\quad\\lambda=(\\eta t)^{-1},}}\\\\ {{\\displaystyle\\psi_{\\lambda}^{\\mathrm{GD}}(z)=(1-\\eta z)^{t},\\quad\\tau=\\infty.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, when $\\eta$ is small enough, say $\\eta<1/(2\\kappa^{2})$ we have $\\mathrm{Re}(1-\\eta z)>0.$ for $z\\in D_{\\lambda}$ , so we can take the single-valued branch of $(1-\\eta z)^{t}$ even when $t$ is not an integer. Therefore, we can extend the definition of the filter function so that $\\lambda$ can be arbitrary and $t=\\check{(}\\eta\\lambda)^{-1}$ ", "page_idx": 19}, {"type": "text", "text": "Lemma C3. $\\varphi_{\\lambda}^{\\tt K R R},\\;\\varphi_{\\lambda}^{\\tt I T},\\;\\varphi_{\\lambda}^{\\tt G F}$ and $\\varphi_{\\lambda}^{\\mathrm{GD}}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. Notice that (i) $z\\le z+\\lambda\\le2z$ when $z>\\lambda$ ; and that (ii) $\\lambda\\leq z+\\lambda\\leq2\\lambda$ when $z\\le\\lambda$ Hence, the constants ${\\mathfrak{C}}_{1},{\\mathfrak{C}}_{2},{\\mathfrak{C}}_{3},{\\mathfrak{C}}_{4}$ , and ${\\mathfrak{C}}_{6}$ are given in Li et al. (2024). ", "page_idx": 19}, {"type": "text", "text": "For ${\\mathfrak{C}}_{7}$ , when $z>\\lambda$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{(z/\\lambda)^{2\\tau}(\\psi_{\\lambda}^{\\tt K R R}(z))^{2}=\\left(\\displaystyle\\frac{z}{z+\\lambda}\\right)^{2}\\ge1/4}\\\\ {(z/\\lambda)^{2\\tau}(\\psi_{\\lambda}^{\\tt I T}\\mathfrak{I}(z))^{2}=\\left(\\displaystyle\\frac{z}{z+\\lambda}\\right)^{2q}\\ge2^{-2q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For ${\\mathfrak{C}}_{8}$ , when $z\\le\\lambda$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{z^{2\\tau-1}(\\psi_{\\lambda}^{\\mathrm{KRR}}(z))^{2}}{\\lambda^{2\\tau}\\varphi_{\\lambda}^{\\mathrm{KRR}}(z)}=\\cfrac{z}{z+\\lambda}\\leq\\cfrac{1}{2}}\\\\ &{\\frac{z^{2\\tau-1}(\\psi_{\\lambda}^{\\mathrm{IT},q}(z))^{2}}{\\lambda^{2\\tau}\\varphi_{\\lambda}^{\\mathrm{IT},q}(z)}=\\cfrac{z^{2q}}{(z+\\lambda)^{2q}-[\\lambda(z+\\lambda)]^{q}}\\leq\\cfrac{1}{2^{2q}-2^{q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "DProof of Theorem 4.1 and Theorem 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1   Bias-variance decomposition ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first apply a standard bias-variance decomposition on the excess risk of spectral algorithms, and readers can also refer to Zhang et al. (2023, 2024) for more details. ", "page_idx": 19}, {"type": "text", "text": "Recall the definition of $\\hat{g}_{Z}$ and $\\hat{f}_{\\lambda}$ in (14) and (15). Let's define their conditional expectations as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{g}_{Z}:=\\mathbb{E}\\left(\\hat{g}_{Z}|X\\right)=\\frac{1}{n}\\sum_{i=1}^{n}K_{x_{i}}f_{\\star}(x_{i})\\in\\mathcal{H};\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{f}_{\\lambda}:=\\mathbb{E}\\left(\\hat{f}_{\\lambda}|X\\right)=\\varphi_{\\lambda}\\left(T_{X}\\right)\\tilde{g}_{Z}\\in\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let's also define their expectations as ", "page_idx": 20}, {"type": "equation", "text": "$$\ng=\\mathbb{E}\\hat{g}_{Z}=\\int_{\\mathcal{X}}K(x,\\cdot)f_{\\star}(x)\\ \\mathsf{d}\\rho_{\\mathcal{X}}(x)\\in\\mathcal{H},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\nf_{\\lambda}=\\varphi_{\\lambda}\\left(T\\right)g.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have the decomposition ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{f}_{\\lambda}-f_{*}=\\frac{1}{n}\\varphi_{\\lambda}\\left(T_{X}\\right)\\sum_{i=1}^{n}K_{x_{i}}y_{i}-f_{*}}}\\\\ {{\\displaystyle=\\frac{1}{n}\\varphi_{\\lambda}\\left(T_{X}\\right)\\sum_{i=1}^{n}K_{x_{i}}\\left(f_{p}^{*}(x_{i})+\\epsilon_{i}\\right)-f_{*}}}\\\\ {{\\displaystyle=\\varphi_{\\lambda}\\left(T_{X}\\right)\\tilde{g}_{Z}+\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{\\lambda}\\left(T_{X}\\right)K_{x_{i}}\\epsilon_{i}-f_{*}}}\\\\ {{\\displaystyle=\\left(\\tilde{f}_{\\lambda}-f_{*}\\right)+\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{\\lambda}\\left(T_{X}\\right)K_{x_{i}}\\epsilon_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking expectation over the noise $\\epsilon$ conditioned on $X$ and noticing that $\\epsilon|X$ are independent noise with mean O and variance $\\sigma^{2}$ , we obtain the bias-variance decomposition: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\left\\|{\\hat{f}}_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}^{2}\\mid X\\right)=\\mathbf{B}\\mathbf{ias}^{2}(\\lambda)+\\mathbf{V}\\mathbf{ar}(\\lambda),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Bias}^{2}(\\lambda):=\\left\\|\\tilde{f}_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}^{2},\\quad\\mathbf{Var}(\\lambda):=\\frac{\\sigma^{2}}{n^{2}}\\sum_{i=1}^{n}\\left\\|\\varphi_{\\lambda}\\left(T_{X}\\right)K(x_{i},\\cdot)\\right\\|_{L^{2}}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Given the decomposition (40), we next derive the upper and lower bounds of $\\mathbf{Bias}^{2}(\\lambda)$ and $\\mathbf{Var}(\\lambda)$ in the following two subsections. ", "page_idx": 20}, {"type": "text", "text": "Before we close this subsection, let's introduce some quantities and an assumption that will be used frequently in our proof later. Denote the true function as $f_{\\star}=\\sum_{i=1}^{\\infty}f_{i}\\phi_{i}(x)$ , let's define the following quantities: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{N}_{1,\\varphi}(\\lambda)=\\sum_{j=1}^{\\infty}\\left[\\lambda_{j}\\varphi_{\\lambda}(\\lambda_{j})\\right];~\\displaystyle\\mathcal{N}_{2,\\varphi}(\\lambda)=\\sum_{j=1}^{\\infty}\\left[\\lambda_{j}\\varphi_{\\lambda}(\\lambda_{j})\\right]^{2};}}\\\\ {{\\displaystyle\\mathcal{M}_{1,\\varphi}(\\lambda)=\\exp\\left.\\left\\vert\\sum_{x\\in\\mathcal{X}}^{\\infty}\\left(\\psi_{\\lambda}(\\lambda_{j})f_{j}\\phi_{j}(x)\\right)\\right\\vert;~\\displaystyle\\mathcal{M}_{2,\\varphi}(\\lambda)=\\sum_{j=1}^{\\infty}\\left(\\psi_{\\lambda}(\\lambda_{j})f_{j}\\right)^{2};}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "moreover. when $\\varphi_{\\lambda}=\\varphi_{\\lambda}^{\\tt K R R}$ ,we dente $\\mathcal{N}_{k}(\\lambda)=\\mathcal{N}_{k,\\varphi^{\\tt K R R}}(\\lambda)$ and $\\mathcal{M}_{k}(\\lambda)=\\mathcal{M}_{k,\\varphi^{\\tt K R R}}(\\lambda)$ for simplic  \nity, where $k=1,2$   \nAssumption 3. Suppose that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\displaystyle\\operatorname{ess\\,sup}_{x\\in\\mathcal{X}}\\sum_{j=1}^{\\infty}\\left[\\lambda_{j}\\varphi_{\\lambda}(\\lambda_{j})\\right]^{2}\\phi_{j}^{2}(x)\\leq\\mathcal{N}_{2,\\varphi}(\\lambda);\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\displaystyle\\operatorname{ess\\up}_{x\\in\\mathcal{X}}\\sum_{j=1}^{\\infty}\\left[\\lambda_{j}\\varphi_{\\lambda}(\\lambda_{j})\\right]\\phi_{j}^{2}(x)\\leq\\mathcal{N}_{1,\\varphi}(\\lambda);\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\displaystyle{\\mathrm{ess\\:sup}\\sum_{x\\in\\mathcal{X}}^{\\infty}\\left[\\lambda_{j}\\varphi_{\\lambda}^{\\tt K R R}(\\lambda_{j})\\right]\\phi_{j}^{2}(x)}\\leq\\mathcal{N}_{1}(\\lambda).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For simplicity of notations, we denote $h_{x}(\\cdot)=K(x,\\cdot)$ \uff0c $x\\in\\mathscr{X}$ in the rest of the proof. Moreover, we denote $\\stackrel{\\cdot}{T}_{\\lambda}:=(T+\\lambda)^{-1}$ and $T_{X\\lambda}:=(T_{X}+\\lambda)^{-1}$ ", "page_idx": 21}, {"type": "text", "text": "D.2 Variance term ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The following proposition rewrites the variance term using the empirical semi-norm. ", "page_idx": 21}, {"type": "text", "text": "Proposition D.1 (Restate Lemma 9 in Zhang et al. (2024)). The variance term in (41) satisfies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{Var}(\\lambda)=\\frac{\\sigma^{2}}{n}\\int_{\\mathcal{X}}\\|\\varphi_{\\lambda}\\left(T_{X}\\right)h_{x}(\\cdot)\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{X}(x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The operator form (46) allows us to apply concentration inequalities and establish the following two-step approximation. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\left\\lVert\\varphi_{\\lambda}\\left(T_{X}\\right)h_{x}\\right\\rVert_{L^{2},n}^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x)\\stackrel{\\mathrm{A}}{\\approx}\\int_{\\mathcal{X}}\\left\\lVert\\varphi_{\\lambda}\\left(T\\right)h_{x}\\right\\rVert_{L^{2},n}^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x)\\stackrel{\\mathrm{B}}{\\approx}\\int_{\\mathcal{X}}\\left\\lVert\\varphi_{\\lambda}\\left(T\\right)h_{x}\\right\\rVert_{L^{2}}^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Approximation B  The following lemma characterizes the magnitude of Approximation B in high probability. Recall the definitions of $\\mathcal{N}_{1,\\varphi}(\\lambda)$ and $\\mathcal{N}_{2,\\varphi}(\\lambda)$ in (42). ", "page_idx": 21}, {"type": "text", "text": "Lemma D.2 (Approximation B). Suppose that (43) in Assumption 3 holds. Then, for any fixed $\\delta\\in(0,1).$ withprobabilityatleast $1-\\delta$ wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left.\\frac{1}{2}\\int_{\\mathcal{X}}\\|\\varphi_{\\lambda}\\left(T\\right)h_{x}\\|_{L^{2}}^{2}\\,\\mathrm{d}\\rho_{\\mathcal{X}}(x)-R_{2}}\\\\ {\\displaystyle\\leq\\int_{\\mathcal{X}}\\|\\varphi_{\\lambda}\\left(T\\right)h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{\\mathcal{X}}(x)}\\\\ {\\displaystyle\\leq\\frac{3}{2}\\int_{\\mathcal{X}}\\|\\varphi_{\\lambda}\\left(T\\right)h_{x}\\|_{L^{2}}^{2}\\,\\mathrm{d}\\rho_{\\mathcal{X}}(x)+R_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{2}=\\frac{5\\mathcal{N}_{2,\\varphi}(\\lambda)}{3n}\\ln\\frac{2}{\\delta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Define a function ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f(z)=\\int_{\\mathcal{X}}\\left(\\varphi_{\\lambda}\\left(T\\right)h_{x}(z)\\right)^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x)}\\\\ {\\displaystyle\\qquad=\\int_{\\mathcal{X}}\\sum_{j=1}^{\\infty}\\left(\\lambda_{j}\\varphi_{\\lambda}(\\lambda_{j})\\right)^{2}\\phi_{j}^{2}(x)\\phi_{j}^{2}(z)\\mathrm{d}\\rho_{\\mathcal{X}}(x)}\\\\ {\\displaystyle\\qquad=\\sum_{j=1}^{\\infty}\\left(\\lambda_{j}\\varphi_{\\lambda}(\\lambda_{j})\\right)^{2}\\phi_{j}^{2}(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since (43) in Assumption 3 holds, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|f\\|_{L^{\\infty}}\\leq\\,\\mathcal{N}_{2,\\varphi}(\\lambda);\\;\\;\\|f\\|_{L^{1}}=\\mathcal{N}_{2,\\varphi}(\\lambda).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying Proposition 34 in Zhang et al. (2024) for $\\sqrt{f}$ and noticing that $\\|\\sqrt{f}\\|_{L^{\\infty}}=\\sqrt{\\|f\\|_{L^{\\infty}}}=$ $\\mathcal{N}_{2,\\varphi}(\\lambda)^{\\frac{1}{2}}$ ,we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left\\lVert\\sqrt{f}\\right\\rVert_{L^{2}}^{2}-\\frac{5\\mathcal{N}_{2,\\varphi}(\\lambda)}{3n}\\ln\\frac{2}{\\delta}\\leq\\left\\lVert\\sqrt{f}\\right\\rVert_{L^{2},n}^{2}\\leq\\frac{3}{2}\\left\\lVert\\sqrt{f}\\right\\rVert_{L^{2}}^{2}+\\frac{5\\mathcal{N}_{2,\\varphi}(\\lambda)}{3n}\\ln\\frac{2}{\\delta},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least $1-\\delta$ ", "page_idx": 22}, {"type": "text", "text": "On the one hand, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\sqrt{f}\\right\\|_{L^{2},n}^{2}=\\int_{\\mathcal{X}}f(z)\\mathrm{d}P_{n}(z)=\\int_{\\mathcal{X}}\\left[\\int_{\\mathcal{X}}\\left(\\varphi_{\\lambda}\\left(T\\right)h_{x}(z)\\right)^{2}\\mathrm{d}\\rho_{X}(x)\\right]\\mathrm{d}P_{n}(z)}}\\\\ &{}&{=\\int_{\\mathcal{X}}\\left[\\int_{\\mathcal{X}}\\left(\\varphi_{\\lambda}\\left(T\\right)h_{x}(z)\\right)^{2}\\mathrm{d}P_{n}(z)\\right]\\mathrm{d}\\rho_{X}(x)}\\\\ &{}&{=\\int_{\\mathcal{X}}\\left\\|\\varphi_{\\lambda}\\left(T\\right)h_{x}\\right\\|_{L^{2},n}^{2}\\mathrm{d}\\rho_{X}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the other hand, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|\\sqrt{f}\\right\\|_{L^{2}}^{2}=\\displaystyle\\int_{\\mathcal{X}}f(z)\\mathrm{d}\\rho_{\\mathcal{X}}(z)}\\\\ {\\displaystyle=\\int_{\\mathcal{X}}\\left[\\int_{\\mathcal{X}}\\left(\\varphi_{\\lambda}\\left(T\\right)h_{x}(z)\\right)^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x)\\right]\\mathrm{d}\\rho_{\\mathcal{X}}(z)}\\\\ {\\displaystyle=\\int_{\\mathcal{X}}\\left\\|\\varphi_{\\lambda}\\left(T\\right)h_{x}\\right\\|_{L^{2}}^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, (53) implies the desired results. ", "page_idx": 22}, {"type": "text", "text": "Approximation A ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma D.3. Suppose that (43) and (45) in Assumption 3 hold. Suppose that there exists a constant $\\epsilon$ onlydependingonsand $\\gamma_{;}$ such that $\\lambda=\\lambda(n,d)$ satisfies $n^{\\epsilon-1}\\bar{\\mathcal{N}}_{1}(\\lambda)\\to0$ Thenthereexistsan absolute constant $C_{1}$ such that for any fixed $\\delta\\in(0,1)$ when n is sufficiently large, with probability at least $1-\\delta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\displaystyle\\int_{\\mathcal X}\\left\\|\\varphi_{\\lambda}\\left(T_{X}\\right)h_{x}\\right\\|_{L^{2},n}^{2}\\mathrm{d}\\rho_{\\mathcal X}(x)-\\displaystyle\\int_{\\mathcal X}\\left\\|\\varphi_{\\lambda}\\left(T\\right)h_{x}\\right\\|_{L^{2},n}^{2}\\mathrm{d}\\rho_{\\mathcal X}(x)\\right|}\\\\ &{\\leq C_{1}\\left(\\sqrt{\\mathcal N_{2,\\varphi}(\\lambda)}+C_{1}\\sqrt{v\\mathcal N_{1}(\\lambda)}\\ln\\lambda^{-1}\\right)\\cdot\\sqrt{v\\mathcal N_{1}(\\lambda)}\\ln\\lambda^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{v=\\frac{\\sqrt{\\mathsf{V}_{1}(\\lambda)}}{n}\\ln n.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Remark D.4. The proof of Lemma D.3 is mainly based on Lemma 4.18 in Li et al. (2024). Notice that we replace the Assumption 2 in Li et al. (2024) by (45) in Assumption 3 (borrowed from Zhang et al. (2024)), since both of them can deduce same results given by Lemma 4.2 in Li et al. (2024) or Lemma 37 in Zhang et al. (2024). ", "page_idx": 22}, {"type": "text", "text": "Proof. We start with ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\bf D}=\\left|\\|\\varphi_{\\lambda}(T_{X})h_{x}\\|_{L^{2}}-\\|\\varphi_{\\lambda}(T)h_{x}\\|_{L^{2}}\\right|\\leq\\Big\\|T^{\\frac{1}{2}}\\left[\\varphi_{\\lambda}(T)-\\varphi_{\\lambda}(T_{X})\\right]h_{x}\\Big\\|_{\\mu}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using operator calculus, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle T^{\\frac{1}{2}}\\left[\\varphi_{\\lambda}(T)-\\varphi_{\\lambda}(T_{X})\\right]h_{x}}}\\\\ {{\\displaystyle=T^{\\frac12}\\left[\\frac1{2\\pi i}\\oint_{\\Gamma_{\\lambda}}R_{T_{X}}(z)(T-T_{X})R_{T}(z)\\varphi_{\\lambda}(z)\\mathrm{d}z\\right]h_{x}}}\\\\ {{\\displaystyle=\\frac1{2\\pi i}\\oint_{\\Gamma_{\\lambda}}T^{\\frac12}(T_{X}-z)^{-1}(T-T_{X})(T-z)^{-1}h_{x}\\varphi_{\\lambda}(z)\\mathrm{d}z}}\\\\ {{\\displaystyle=\\frac1{2\\pi i}\\oint_{\\Gamma_{\\lambda}}T^{\\frac12}T_{\\lambda}^{-\\frac12}\\cdot T_{\\lambda}^{\\frac12}(T_{X}-z)^{-1}T_{\\lambda}^{\\frac12}\\cdot T_{\\lambda}^{-\\frac12}(T-T_{X})T_{\\lambda}^{-\\frac12}\\cdot T_{\\lambda}^{\\frac12}(T-z)^{-1}T_{\\lambda}^{\\frac12}\\cdot T_{\\lambda}^{-\\frac12}h_{x}\\varphi_{\\lambda}(z)\\mathrm{d}z.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, taking the norms yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}\\leq\\frac{1}{2\\pi}\\Big\\|T^{\\frac{1}{2}}T_{\\lambda}^{-\\frac{1}{2}}\\Big\\|\\cdot\\Big\\|T_{\\lambda}^{\\frac{1}{2}}(T_{X}-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\Big\\|\\cdot\\Big\\|T_{\\lambda}^{-\\frac{1}{2}}(T-T_{X})T_{\\lambda}^{-\\frac{1}{2}}\\Big\\|\\cdot\\Big\\|T_{\\lambda}^{\\frac{1}{2}}(T-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\Big\\|}\\\\ &{\\qquad\\cdot\\left\\|T_{\\lambda}^{-\\frac{1}{2}}h_{x}\\right\\|_{\\mathcal{H}}\\oint_{\\Gamma_{\\lambda}}|\\varphi_{\\lambda}(z)\\mathrm{d}z|}\\\\ &{=\\frac{1}{2\\pi}\\cdot\\mathbf{I}\\cdot\\mathbf{II}\\cdot\\mathbf{III}\\cdot\\mathbf{IV}\\cdot\\mathbf{V}\\cdot\\oint_{\\Gamma_{\\lambda}}|\\varphi_{\\lambda}(z)\\mathrm{d}z|}\\\\ &{\\leq\\frac{1}{2\\pi}\\cdot1\\cdot\\sqrt{6}C\\cdot\\sqrt{\\frac{N_{1}(\\lambda)}{n}\\ln n}\\cdot C\\cdot\\sqrt{\\mathcal{N}_{1}(\\lambda)}\\oint_{\\Gamma_{\\lambda}}|\\varphi_{\\lambda}(z)\\mathrm{d}z|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the second estimation, we use (I) operator calculus, (II and IV) Proposition E.8, (III) Lemma E.7, and $({\\bf V})$ Lemma 37 in Zhang et al. (2024) for each term respectively. Finally, from (63) in Li et al. (2024), we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\oint_{\\Gamma_{\\lambda}}\\left\\vert\\varphi_{\\lambda}(z)\\mathrm{d}z\\right\\vert\\leq C\\ln\\lambda^{-1},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and thus there exists an absolute constant $C_{1}$ , such that we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\bf D}=|\\|\\varphi_{\\lambda}(T_{X})h_{x}\\|_{L^{2}}-\\|\\varphi_{\\lambda}(T)h_{x}\\|_{L^{2}}|\\leq C_{1}\\sqrt{v\\mathcal{N}_{1}(\\lambda)}\\ln{\\lambda^{-1}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, combining (52) and (43) in Assumption 3, we have $\\left\\lVert\\varphi_{\\lambda}(T)h_{x}\\right\\rVert_{L^{2}}^{2}\\leq\\mathcal{N}_{2,\\varphi}(\\lambda)$ and hence ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\varphi_{\\lambda}(T_{X})h_{x}\\right\\|_{L^{2}}+\\left\\|\\varphi_{\\lambda}(T)h_{x}\\right\\|_{L^{2}}\\leq2\\|\\varphi_{\\lambda}(T)h_{x}\\|_{L^{2}}+\\mathbf{D}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\ensuremath{N_{2,\\varphi}}(\\lambda)}+C_{1}\\sqrt{v\\ensuremath{N_{1}}(\\lambda)}\\ln\\lambda^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left\\|\\varphi_{\\lambda}(T_{X})h_{x}\\right\\|_{L^{2}}^{2}-\\left\\|\\varphi_{\\lambda}(T)h_{x}\\right\\|_{L^{2}}^{2}\\right|}\\\\ &{=\\left|\\left\\|\\varphi_{\\lambda}(T_{X})h_{x}\\right\\|_{L^{2}}-\\left\\|\\varphi_{\\lambda}(T)h_{x}\\right\\|_{L^{2}}\\right|\\left(\\left\\|\\varphi_{\\lambda}(T_{X})h_{x}\\right\\|_{L^{2}}+\\left\\|\\varphi_{\\lambda}(T)h_{x}\\right\\|_{L^{2}}\\right)}\\\\ &{\\leq C_{1}\\left(\\sqrt{\\mathscr{N}_{2,\\varphi}(\\lambda)}+C_{1}\\sqrt{v\\mathscr{N}_{1}(\\lambda)}\\ln\\lambda^{-1}\\right)\\cdot\\sqrt{v\\mathscr{N}_{1}(\\lambda)}\\ln\\lambda^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and hence ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert\\int_{\\mathcal{X}}\\left\\|\\varphi_{\\lambda}\\left(T_{X}\\right)h_{x}\\right\\|_{L^{2},n}^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x)-\\displaystyle\\int_{\\mathcal{X}}\\left\\|\\varphi_{\\lambda}\\left(T\\right)h_{x}\\right\\|_{L^{2},n}^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x)\\right\\rceil}\\\\ {\\displaystyle\\leq\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left\\|\\|\\varphi_{\\lambda}(T_{X})h_{x_{i}}\\|_{L^{2}}^{2}-\\|\\varphi_{\\lambda}(T)h_{x_{i}}\\|_{L^{2}}^{2}\\right\\|}\\\\ {\\displaystyle\\leq\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left\\|\\|\\varphi_{\\lambda}(T_{X})h_{x}\\|_{L^{2}}^{2}-\\|\\varphi_{\\lambda}(T)h_{x}\\|_{L^{2}}^{2}\\right\\|}\\\\ {\\displaystyle\\leq C_{1}\\left(\\sqrt{\\mathcal{N}_{2,\\varphi}(\\lambda)}+C_{1}\\sqrt{v\\mathcal{N}_{1}(\\lambda)}\\ln\\lambda^{-1}\\right)\\cdot\\sqrt{v\\mathcal{N}_{1}(\\lambda)}\\ln\\lambda^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Final proof of the variance term Now we are ready to state the theorem about the variance term. Theorem D.5. Suppose that (43) and (45) in Assumption 3 hold. Suppose there exists a constant $\\epsilon>0$ onlydependingonsand $\\gamma$ suchthat $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\mathcal{N}}_{1}(\\lambda)\\cdot n^{\\epsilon-1}\\to0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{{\\mathcal N}_{1}^{2}(\\lambda)}{n\\mathcal{N}_{2,\\varphi}(\\lambda)}}\\cdot\\ln(n)(\\ln\\lambda^{-1})^{2}\\to0;\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\bf V a r}(\\lambda)=\\left[1+o_{\\mathbb{P}}(1)\\right]\\frac{\\sigma^{2}}{n}{\\cal N}_{2,\\varphi}(\\lambda).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{\\mathbf{Var}(\\lambda)\\,=\\,\\frac{\\sigma^{2}}{n}\\int_{\\mathcal{X}}\\left\\|\\varphi_{\\lambda}\\left(T_{X}\\right)h_{x}\\right\\|_{L^{2},n}^{2}\\mathrm{d}\\rho_{X}(x)}\\end{array}$ . Hence, when $n$ is large enough, with probability at least $1-\\delta$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\displaystyle\\int_{x}\\|\\varphi_{\\lambda}(T_{x})\\,h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{x}(x)-\\int\\iint\\!\\varphi_{\\lambda}(T)\\,h_{x}\\|_{L^{2}}^{2}\\,\\mathrm{d}\\rho_{x}(x)\\right|}\\\\ &{\\leq\\left|\\displaystyle\\int_{x}\\|\\varphi_{\\lambda}(T_{x})\\,h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{x}(x)-\\int_{x}\\|\\varphi_{\\lambda}(T)\\,h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{x}(x)\\right|}\\\\ &{\\qquad+\\left|\\displaystyle\\int_{x}\\|\\varphi_{\\lambda}(T)\\,h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{x}(x)-\\int_{x}\\|\\varphi_{\\lambda}(T)\\,h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{x}(x)\\right|}\\\\ {\\overset{\\mathrm{Lem.~}}{\\leq}\\left|\\displaystyle\\int_{x}\\|\\varphi_{\\lambda}(T)\\,h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{x}(x)-\\int_{x}\\|\\varphi_{\\lambda}(T)\\,h_{x}\\|_{L^{2},n}^{2}\\,\\mathrm{d}\\rho_{x}(x)\\right|+\\frac{5\\sqrt{3}\\,\\chi_{x}(\\lambda)}{3n}\\,\\mathrm{d}\\frac{2}{{\\delta}}}\\\\ &{\\overset{\\mathrm{Lem.~}}{\\leq}\\left(\\sqrt{\\Lambda_{2\\varphi}(\\lambda)}\\cdot C_{1}\\sqrt{\\Lambda_{1}(\\lambda)}\\,\\ln^{-1}+C_{1}^{2}\\nu_{1}\\Lambda_{1}(\\lambda)(\\ln\\lambda^{-1})^{2}\\right)+\\frac{5\\sqrt{3}\\,\\chi_{y}(\\lambda)}{3n}\\,\\mathrm{d}\\frac{2}{{\\delta}}}\\\\ &{\\overset{\\mathrm{Mina~ot~}\\nu}{=}\\sqrt{\\frac{\\Lambda_{2\\varphi}(\\lambda)}{n}}\\Lambda_{1}(\\lambda)\\cdot C_{1}\\sqrt{\\ln(n)}\\ln\\lambda^{-1}+\\frac{\\Lambda_{1}^{2}(\\lambda)}{n}\\cdot C_{1}^{2}\\ln(n)(\\ln\\lambda^{-1})^{2}+\\frac{\\Lambda_{2\\varphi}(\\lambda)}{n} \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $n\\geq{\\mathfrak{C}}$ , a suffciently large constant only depending on $\\gamma$ and $C_{1}$ ,wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\bf I}\\cdot C_{1}\\sqrt{\\ln(n)}\\ln\\lambda^{-1}\\leq\\frac{1}{6}\\ensuremath{N_{2,\\varphi}}(\\lambda).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Furthermore, when $\\frac{\\mathcal{N}_{1}^{2}(\\lambda)}{n\\mathcal{N}_{2,\\varphi}(\\lambda)}\\,\\cdot\\,n^{\\epsilon}\\;\\to\\;0$ we have $\\mathbf{I}\\cdot C_{1}\\sqrt{\\ln(n)}\\ln\\lambda^{-1}/\\ensuremath{\\mathcal{N}_{2,\\varphi}}(\\lambda)\\ \\to\\ 0$ and $\\mathbf{II}$   \n$C_{1}^{2}\\ln(n)(\\ln\\lambda^{-1})^{2}/\\ensuremath{N_{2,\\varphi}}(\\lambda)\\to0$   \nFinally, from (52) we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\varphi_{\\lambda}(T)h_{x}\\|_{L^{2}}^{2}=\\sum_{i=1}^{\\infty}\\left(\\lambda_{j}\\varphi_{\\lambda}(\\lambda_{j})\\right)^{2}\\phi_{i}^{2}(z),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and thus the deterministic term writes ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\|\\varphi_{\\lambda}(T)h_{x}\\|_{L^{2}}^{2}\\mathrm{d}\\rho_{\\mathcal{X}}(x)=\\mathcal{N}_{2,\\varphi}(\\lambda).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.3 Bias term ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this subsection, our goal is to determine the upper and lower bounds of bias under some approximation conditions. ", "page_idx": 24}, {"type": "text", "text": "The triangle inequality implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Bias}(\\lambda)=\\left\\|\\tilde{f}_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}\\geq\\left\\|f_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}-\\left\\|\\tilde{f}_{\\lambda}-f_{\\lambda}\\right\\|_{L^{2}}}\\\\ &{\\mathbf{Bias}(\\lambda)\\leq\\left\\|f_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}+\\left\\|\\tilde{f}_{\\lambda}-f_{\\lambda}\\right\\|_{L^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The following lemma characterizes the dominant term of $\\mathbf{Bias}(\\lambda)$ ", "page_idx": 24}, {"type": "text", "text": "Lemma D.6. For any $\\lambda>0$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Vert f_{\\lambda}-f_{\\star}\\Vert_{L^{2}}=\\mathcal{M}_{2,\\varphi}(\\lambda)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|f_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}^{2}=\\left\\|\\displaystyle\\sum_{i=1}^{\\infty}\\lambda_{i}\\varphi_{\\lambda}\\bigl(\\lambda_{i}\\bigr)f_{i}\\phi_{i}(x)-\\displaystyle\\sum_{i=1}^{\\infty}f_{i}\\phi_{i}(x)\\right\\|_{L^{2}}^{2}}\\\\ {\\displaystyle=\\left\\|\\displaystyle\\sum_{i=1}^{\\infty}\\psi_{\\lambda}\\bigl(\\lambda_{i}\\bigr)f_{i}\\phi_{i}(x)\\right\\|_{L^{2}}^{2}}\\\\ {\\displaystyle=\\displaystyle\\sum_{i=1}^{\\infty}\\bigl(\\psi_{\\lambda}\\bigl(\\lambda_{i}\\bigr)f_{i}\\bigr)^{2}}\\\\ {\\displaystyle=\\mathcal{M}_{2,\\varphi}\\bigl(\\lambda\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The following lemma bounds the remainder term of $\\mathbf{Bias}(\\lambda)$ when $s\\geq1$ ", "page_idx": 25}, {"type": "text", "text": "Lemma D.7. Suppose that (45) in Assumption 3 holds. Suppose that there exist constants e and ${\\mathfrak C}$ onlydependingonsand $\\gamma,$ suchthat $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n^{\\epsilon-1}\\mathcal{N}_{1}(\\lambda)\\to0,}}\\\\ {{\\displaystyle\\frac{\\mathcal{N}_{1}(\\lambda)\\mathcal{M}_{1,\\varphi}^{2}(\\lambda)}{n^{2}}=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right),}}\\\\ {{\\displaystyle\\frac{\\mathcal{N}_{1}(\\lambda)}{n}\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\sum_{j=1}^{\\infty}\\frac{\\lambda^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right);}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\tilde{f}_{\\lambda}-f_{\\lambda}\\right\\|_{L^{2}}^{2}=o_{\\mathbb{P}}\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Do the decomposition, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{f}_{\\lambda}-f_{\\lambda}=\\varphi_{\\lambda}(T_{X})\\tilde{g}_{X}-(\\psi_{\\lambda}(T_{X})+\\varphi_{\\lambda}(T_{X})T_{X})f_{\\lambda}}\\\\ &{\\qquad\\quad=\\varphi_{\\lambda}(T_{X})(\\tilde{g}_{X}-T_{X}f_{\\lambda})-\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T)f_{\\star}}\\\\ &{\\qquad\\quad=\\varphi_{\\lambda}(T_{X})(\\tilde{g}_{X}-T_{X}f_{\\lambda})-\\varphi_{\\lambda}(T_{X})\\psi_{\\lambda}(T)g+\\varphi_{\\lambda}(T_{X})\\psi_{\\lambda}(T)g-\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T)f_{\\star}}\\\\ &{\\qquad\\quad=\\varphi_{\\lambda}(T_{X})\\left[\\tilde{g}_{X}-T_{X}f_{\\lambda}-\\psi_{\\lambda}(T)g\\right]+[\\varphi_{\\lambda}(T_{X})\\psi_{\\lambda}(T)T f_{\\star}-\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T)f_{\\star}]}\\\\ &{\\qquad\\quad=\\varphi_{\\lambda}(T_{X})(\\tilde{g}_{X}-T_{X}f_{\\lambda}-g+T f_{\\lambda})+(\\varphi_{\\lambda}(T_{X})T\\psi_{\\lambda}(T)-\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T))f_{\\star}}\\\\ &{\\qquad\\quad=\\mathbf{I}+\\mathbf{II}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(66) ", "page_idx": 25}, {"type": "text", "text": "Bound on I: For the first term in (66), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\bf{I}}\\|_{L^{2}}=\\|\\varphi_{\\lambda}(T_{X})(\\tilde{g}_{X}-T_{X}f_{\\lambda}-g+T f_{\\lambda})\\|_{L^{2}}}\\\\ &{\\qquad=\\left\\|T^{\\frac{1}{2}}\\varphi_{\\lambda}(T_{X})(\\tilde{g}_{X}-T_{X}f_{\\lambda}-g+T f_{\\lambda})\\right\\|_{\\mathbb{M}}}\\\\ &{\\qquad\\le\\left\\|T^{\\frac{1}{2}}T_{\\lambda}^{-\\frac{1}{2}}\\right\\|\\cdot\\left\\|T_{\\lambda}^{\\frac{1}{2}}\\varphi_{\\lambda}(T_{X})T_{\\lambda}^{\\frac{1}{2}}\\right\\|\\cdot\\left\\|T_{\\lambda}^{-\\frac{1}{2}}\\left[(\\tilde{g}_{X}-T_{X}f_{\\lambda})-(g-T f_{\\lambda})\\right]\\right\\|_{\\mathbb{M}}}\\\\ &{\\qquad\\overset{(72)\\operatorname{inf~cupq~etal.}(2024)}{\\le}\\left\\|T_{\\lambda}^{\\frac{1}{2}}\\varphi_{\\lambda}(T_{X})T_{\\lambda}^{\\frac{1}{2}}\\right\\|\\cdot\\left\\|T_{\\lambda}^{-\\frac{1}{2}}\\left[(\\tilde{g}_{X}-T_{X}f_{\\lambda})-(g-T_{\\lambda})\\right]\\right\\|_{\\mathbb{M}}}\\\\ &{\\qquad\\overset{\\mathrm{Peropenisno~}E.1}{\\le}4\\left\\|T_{\\lambda}^{\\frac{1}{2}}T_{X\\lambda}^{-1}T_{\\lambda}^{\\frac{1}{2}}\\right\\|\\cdot\\left\\|T_{\\lambda}^{-\\frac{1}{2}}\\left[(\\tilde{g}_{X}-T_{X}f_{\\lambda})-(g-T f_{\\lambda})\\right]\\right\\|_{\\mathbb{M}}}\\\\ &{\\qquad\\overset{(62)\\operatorname{and}(73)\\operatorname{in}2\\operatorname{cupq~etal.}(2024)}{\\le}12\\big\\|T_{\\lambda}^{-\\frac{1}{2}}\\left[(\\tilde{g}_{X}-T_{X}f_{\\lambda})-(g-T f_{\\lambda})\\right]\\right\\|_{\\mathbb{M}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Denote $\\xi_{i}=\\xi(x_{i})=T_{\\lambda}^{-\\frac12}(K_{x_{i}}f_{\\star}(x_{i})-T_{x_{i}}f_{\\lambda})$ To use Bernstein inequality, we need to bound the $m$ -thmoment of $\\xi(x)$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\xi(x)\\right\\|_{\\mathcal{H}}^{m}=\\mathbb{E}\\left\\|T_{\\lambda}^{-\\frac{1}{2}}K_{x}(f_{\\star}-f_{\\lambda}(x))\\right\\|_{\\mathcal{H}}^{m}}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\Big(\\left\\|T_{\\lambda}^{-\\frac{1}{2}}K(x,\\cdot)\\right\\|_{\\mathcal{H}}^{m}\\mathbb{E}\\big(\\left|(f_{\\star}-f_{\\lambda}(x))\\right|^{m}\\big|\\ x\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that Lemma 37 in Zhang et al. (2024) shows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\left|T_{\\lambda}^{-\\frac{1}{2}}K(x,\\cdot)\\right|\\right|_{\\mathcal{H}}\\leq\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}},\\;\\;\\mu\\mathrm{-a.e.}\\ x\\in\\mathcal{X};\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By definition of $\\mathcal{M}_{1,\\varphi}(\\lambda)$ , we also have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|f_{\\lambda}-f_{\\star}\\right\\|_{L^{\\infty}}=\\left\\|\\sum_{i=1}^{\\infty}\\psi_{\\lambda}(\\lambda_{i})f_{i}\\phi_{i}(x)\\right\\|_{L^{\\infty}}=\\mathcal{M}_{1,\\varphi}(\\lambda).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In addition, we have proved in Lemma D.6 that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}|(f_{\\lambda}(x)-f_{\\star}(x))|^{2}=\\mathcal{M}_{2,\\varphi}(\\lambda).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So we get the upper bound of (67), i.e. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(67)\\leq\\mathcal{N}_{1}(\\lambda)^{\\frac{m}{2}}\\cdot\\|f_{\\lambda}-f_{\\star}\\|_{L^{\\infty}}^{m-2}\\cdot\\mathbb{E}|(f_{\\lambda}(x)-f_{\\star}(x))|^{2}}\\\\ &{\\qquad=\\mathcal{N}_{1}(\\lambda)^{\\frac{m}{2}}\\mathcal{M}_{1,\\varphi}(\\lambda)^{m-2}\\mathcal{M}_{2,\\varphi}(\\lambda)}\\\\ &{\\qquad=\\left(\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\mathcal{M}_{1,\\varphi}(\\lambda)\\right)^{m-2}\\left(\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\mathcal{M}_{2,\\varphi}(\\lambda)^{\\frac{1}{2}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using Lemma 36 in Zhang et al. (2024) with therein notations: $L=\\mathcal{N}_{1}(\\lambda)^{\\frac12}\\mathcal{M}_{1,\\varphi}(\\lambda)$ and $\\sigma=$ $\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\mathcal{M}_{2,\\varphi}(\\lambda)^{\\frac{1}{2}}$ , for any fixed $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mathbf{I}\\|_{L^{2}}\\leq12\\cdot4\\sqrt{2}\\log\\frac{2}{\\delta}\\left(\\frac{\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\mathcal{M}_{1,\\varphi}(\\lambda)}{n}+\\frac{\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\mathcal{M}_{2,\\varphi}(\\lambda)^{\\frac{1}{2}}}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Bound on II: For the second term in (66), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\Pi\\|_{L^{2}}=\\|\\big(\\varphi_{\\lambda}(T_{X})T\\psi_{\\lambda}(T)-\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T)\\big)f_{\\star}\\|_{L^{2}}}&{}\\\\ {\\le\\Big\\|T^{\\frac12}(\\varphi_{\\lambda}(T_{X})T\\psi_{\\lambda}(T)-\\psi_{\\lambda}(T)T\\varphi_{\\lambda}(T))f_{\\star}\\Big\\|_{\\mathcal H}}&{}\\\\ {+\\left\\|T^{\\frac12}(\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T)-\\psi_{\\lambda}(T)T\\varphi_{\\lambda}(T))f_{\\star}\\right\\|_{\\mathcal H}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the first term in (70), we still employ the analytic functional argument: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad T^{\\frac{1}{2}}(\\varphi_{\\lambda}(T_{X})T\\psi_{\\lambda}(T)-\\psi_{\\lambda}(T)T\\varphi_{\\lambda}(T))f_{\\star}}\\\\ &{=T^{\\frac{1}{2}}(\\varphi_{\\lambda}(T_{X})-\\varphi_{\\lambda}(T))T\\psi_{\\lambda}(T)f_{\\star}}\\\\ &{=\\frac{1}{2\\pi i}\\oint_{\\Gamma_{\\lambda}}T^{\\frac{1}{2}}(T_{X}-z)^{-1}(T_{X}-T)(T-z)^{-1}\\varphi_{\\lambda}(z)T\\psi_{\\lambda}(T)f_{\\star}\\mathrm{d}z}\\\\ &{=\\frac{1}{2\\pi i}\\oint_{\\Gamma_{\\lambda}}T^{\\frac{1}{2}}T_{\\lambda}^{-\\frac{1}{2}}\\cdot T_{\\lambda}^{\\frac{1}{2}}(T_{X}-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\cdot T_{\\lambda}^{-\\frac{1}{2}}(T-T_{X})T_{\\lambda}^{-\\frac{1}{2}}}\\\\ &{\\quad\\cdot T_{\\lambda}^{\\frac{1}{2}}(T-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\cdot T_{\\lambda}^{-\\frac{1}{2}}T^{\\frac{1}{2}}\\cdot T^{\\frac{1}{2}}\\psi_{\\lambda}(T)f_{\\star}\\varphi_{\\lambda}(z)\\mathrm{d}z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\pi|T|+|\\gamma_{3}(T_{S})|T\\gamma_{6}(T_{I})-\\phi_{6}(T_{I})T\\gamma_{6}(T)|,f_{6}||\\nu}\\\\ &{\\leq\\int_{0}^{1}\\left|T\\frac{1}{S}\\chi_{6}^{-1}\\right||\\cdot\\left|f_{6}^{1}\\chi_{6}-\\chi_{7}^{-1}\\right|\\cdot\\sum_{l}^{1}\\left|\\cdot\\left|T^{3}\\right|^{2}\\cdot\\left|T^{3}\\right|^{2}\\right|}\\\\ &{\\qquad\\qquad\\cdot\\left|\\gamma_{3}^{-1}(T-s)^{-1}\\chi_{6}^{-1}\\right|\\cdot\\left|\\left|T^{3}\\right|^{-1}f_{l}^{1}\\chi_{6}(T)f_{l}\\right|_{\\mu_{3}}|\\varphi_{6}(s)|;}\\\\ {\\overset{(a)\\mathrm{iname}\\;q\\ u e\\;q\\;\\mathrm{for}}{\\mathop{\\int_{0}^{1}}}\\left|\\int_{0}^{1}\\chi_{6}^{-1}\\left|T^{3}\\right|\\cdot\\left|T^{3}\\right|^{2}\\cdot\\left|T^{3}\\right|\\cdot\\left|T^{3}\\right|_{2}^{-1}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left|\\Gamma_{3}^{1}(T-s)^{-1}\\chi_{6}^{-1}\\right|\\cdot\\left|\\left|T^{3}\\right|\\cdot\\left|T^{3}\\right|_{2}^{-1}\\right|\\cdot\\left|T^{3}\\right|}\\\\ &{\\overset{(b)\\mathrm{inbenume}\\;q\\ u e\\;q\\;\\mathrm{for}}{\\mathop{\\int_{0}^{1}}}\\left|\\int_{0}^{1}\\chi_{6}^{-1}\\left(T-T_{S}\\right)\\chi_{6}^{-1}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left|\\Gamma^{3}\\right|\\cdot\\left|\\phi_{6}(s)\\chi_{6}^{-1}\\right|\\cdot\\left|\\gamma_{6}^{-1}(s)\\chi_{6}^{-1}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left|\\Gamma^{4}\\right|\\cdot\\left|\\phi_{6}(T)\\int_{l}^{1}\\left|\\phi_{6}(s)\\chi_{6}^{-1}\\right|}\\\\ &{\\overset{(c)\\mathrm{inbenume}\\;q\\;\\mathrm{for}}{\\mathop{\\int_{0}^{1}}}\\left|\\int_{0}^{1}\\chi_{ \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{v=\\frac{\\sqrt{\\mathsf{N}_{1}(\\lambda)}}{n}\\ln n}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "For the second term in (70), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad T^{\\frac{1}{2}}(\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T)-\\psi_{\\lambda}(T)T\\varphi_{\\lambda}(T))f_{\\star}}\\\\ &{=T^{\\frac{1}{2}}\\left[\\displaystyle\\frac{1}{2\\pi i}\\oint_{\\Gamma_{\\lambda}}R_{T x}\\left(z\\right)(T-T_{X})R_{T}(z)\\psi_{\\lambda}(z)\\mathrm{d}z\\right]T\\varphi_{\\lambda}(T)f_{\\star}}\\\\ &{=\\displaystyle\\frac{1}{2\\pi i}\\oint_{\\Gamma_{\\lambda}}T^{\\frac{1}{2}}(T x-z)^{-1}(T-T_{X})(T-z)^{-1}\\psi_{\\lambda}(z)T\\varphi_{\\lambda}(T)f_{\\star}\\mathrm{d}z}\\\\ &{=\\displaystyle\\frac{1}{2\\pi i}\\int_{\\Gamma_{\\lambda}}T^{\\frac{1}{2}}T_{\\lambda}^{-\\frac{1}{2}}\\cdot T_{\\lambda}^{\\frac{1}{2}}(T_{X}-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\cdot T_{\\lambda}^{-\\frac{1}{2}}(T-T_{X})T_{\\lambda}^{-\\frac{1}{2}}}\\\\ &{\\quad\\quad\\cdot T_{\\lambda}^{\\frac{1}{2}}(T-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\cdot T_{\\lambda}^{-\\frac{1}{2}}T\\varphi_{\\lambda}(T)f_{\\star}\\psi_{\\lambda}(z)\\mathrm{d}z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, similar to (71), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\pi\\Big\\|T^{\\frac12}\\big(\\psi_{\\lambda}(T_{X})T\\varphi_{\\lambda}(T)-\\psi_{\\lambda}(T)T\\varphi_{\\lambda}(T)\\big)f_{\\star}\\Big\\|_{\\mathcal H}}\\\\ &{\\le\\int_{\\Gamma_{\\lambda}}\\Big\\|T^{\\frac12}T_{\\lambda}^{-\\frac12}\\Big\\|\\cdot\\Big\\|T_{\\lambda}^{\\frac12}(T_{X}-z)^{-1}T_{\\lambda}^{\\frac12}\\Big\\|\\cdot\\Big\\|T_{\\lambda}^{-\\frac12}(T-T_{X})T_{\\lambda}^{-\\frac12}\\Big\\|}\\\\ &{\\quad\\cdot\\left\\|T_{\\lambda}^{\\frac12}(T-z)^{-1}T_{\\lambda}^{\\frac12}\\right\\|\\cdot\\Big\\|T_{\\lambda}^{-\\frac12}T\\varphi_{\\lambda}(T)f_{\\star}\\Big\\|_{\\mathcal H}|\\psi_{\\lambda}(z)\\mathrm d z|}\\\\ &{\\le\\sqrt{6}C^{2}\\sqrt{v}\\Big\\|T_{\\lambda}^{-\\frac12}T\\varphi_{\\lambda}(T)f_{\\star}\\Big\\|_{\\mathcal H}\\int_{\\Gamma_{\\lambda}}|\\psi_{\\lambda}(z)\\mathrm d z|}\\\\ &{\\mathrm{terfunctions~}\\sqrt{6}C^{2}\\sqrt{v}\\Big\\|T_{\\lambda}^{-\\frac12}T\\varphi_{\\lambda}(T)f_{\\star}\\Big\\|_{\\mathcal H}C\\tilde{F}\\lambda\\ln\\lambda^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Combining (66), (69), (70), (71), and (72), there exists a constant ${\\mathfrak{C}}_{1}$ only depending on $\\delta$ and $\\tilde{F}$ such that we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\bar{f}_{\\lambda}-f_{\\lambda}\\right\\|_{L^{2}}}\\\\ &{\\leq\\mathfrak{C}_{1}\\left(\\frac{N_{1}(\\lambda)^{\\frac{1}{2}}M_{1},\\varphi\\left(\\lambda\\right)}{n}+\\frac{\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}M_{2,\\varphi}\\left(\\lambda\\right)^{\\frac{1}{2}}}{\\sqrt{n}}\\right)}\\\\ &{\\qquad+\\mathfrak{C}_{1}\\sqrt{v}M_{2,\\varphi}^{1/2}(\\lambda)\\ln\\lambda^{-1}+\\mathfrak{C}_{1}\\sqrt{v}\\bigg\\|\\bar{T}_{\\lambda}^{-\\frac{1}{2}}T\\varphi_{\\lambda}(T)f_{\\star}\\bigg\\|_{H}\\lambda\\ln\\lambda^{-1}}\\\\ &{\\overset{(\\mathrm{e}^{2})}{\\leq}(n^{-1}N_{1}(\\lambda))^{1/2}\\cdot\\mathfrak{C}_{1}\\mathfrak{C}^{1/2}\\cdot(M_{2,\\varphi}(\\lambda))^{1/2}}\\\\ &{\\qquad+(n^{-1}N_{1}(\\lambda))^{1/2}\\cdot\\mathfrak{C}_{1}\\cdot(M_{2,\\varphi}(\\lambda))^{1/2}}\\\\ &{\\qquad+(n^{-1}N_{1}(\\lambda))^{1/2}\\cdot\\mathfrak{C}_{1}\\cdot(M_{2,\\varphi}(\\lambda))^{1/2}}\\\\ &{\\qquad+o\\left(M_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}N_{2,\\varphi}(\\lambda)\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $s<1$ , we can use the following lemma to bound the remainder term of $\\mathbf{Bias}(\\lambda)$ . This lemma is a modification of Lemma D.7, and its proof is partly based on Lemma 26 in Zhang. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.8. Suppose that (45) in Assumption 3 holds. Suppose that there exist constants E and C onlydependingonsand $\\gamma,$ suchthat $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\nn^{\\epsilon-1}\\mathcal{N}_{1}(\\lambda)\\to0,\n$$$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{n^{\\mathrm{e}-\\mathrm{\\varepsilon}}\\mathcal{N}_{1}(\\lambda)\\to0,}{n}\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\sum_{j=1}^{\\infty}\\frac{\\lambda^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}\\ll\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right);}}\\\\ &{}&{n^{-1}\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\left(\\|f_{\\lambda}\\|_{L^{\\infty}}+n^{\\frac{1-s}{2}+\\epsilon}\\right)=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right)^{1/2};}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "thenwe have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\tilde{f}_{\\lambda}-f_{\\lambda}\\right\\|_{L^{2}}^{2}=o_{\\mathbb{P}}\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Similar to the proof in Lemma D.7, we have the decomposition $\\tilde{f}_{\\lambda}-f_{\\lambda}=\\mathbf{I}+\\mathbf{I}\\mathbf{I}$ with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{I}\\|_{L^{2}}^{2}\\leq12^{2}\\Big\\|T_{\\lambda}^{-\\frac{1}{2}}\\left[\\left(\\tilde{g}_{X}-T_{X}f_{\\lambda}\\right)-\\left(g-T f_{\\lambda}\\right)\\right]\\Big\\|_{\\mathcal{H}}^{2},}\\\\ &{\\|\\mathbf{I}\\|_{L^{2}}^{2}=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Denote $\\xi_{i}\\,=\\,\\xi(x_{i})\\,=\\,T_{\\lambda}^{-{\\frac{1}{2}}}(K_{x_{i}}f_{\\star}(x_{i})-T_{x_{i}}f_{\\lambda})$ . Further consider the subset $\\Omega_{1}\\,=\\,\\{x\\,\\in\\,x\\,:$ $|f_{\\star}(x)|\\;\\leq\\;t\\}$ and $\\Omega_{2}\\,=\\,\\mathcal{X}\\backslash\\Omega_{1}$ ,where $t$ will be chosen appropriately later. Decompose $\\xi_{i}$ as $\\xi_{i}I_{x_{i}\\in\\Omega_{1}}+\\xi_{i}I_{x_{i}\\in\\Omega_{2}}$ and we have the following decomposition: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|{T_{\\lambda}^{-\\frac{1}{2}}\\left[\\left(\\tilde{g}_{X}-T_{X}f_{\\lambda}\\right)-\\left(g-T f_{\\lambda}\\right)\\right]}\\right\\|_{\\mathcal{H}}=\\left\\|{\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}-\\mathbb{E}\\xi_{x}}\\right\\|_{\\mathcal{H}}}\\\\ {\\displaystyle\\leq\\left\\|{\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}I_{x_{i}\\in\\Omega_{1}}-\\mathbb{E}\\xi_{x}I_{x\\in\\Omega_{1}}}\\right\\|_{\\mathcal{H}}+\\|{\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}I_{x_{i}\\in\\Omega_{2}}}\\|_{\\varkappa}+\\|{\\mathbb{E}\\xi_{x}I_{x\\in\\Omega_{2}}}\\|_{\\mathcal{H}}}\\\\ {:=\\mathrm{I}+\\Pi+\\Pi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Next we choose $\\begin{array}{r}{t=n^{\\frac{1-s}{2}+\\epsilon_{t}},q=\\frac{2}{1-s}-\\epsilon_{q}}\\end{array}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{t}<\\epsilon;\\,\\,\\mathrm{~and}\\,\\,\\,\\frac{1-s}{2}+\\epsilon_{t}>1/\\left(\\frac{2}{1-s}-\\epsilon_{q}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then we can bound the three terms in (78) as follows: ", "page_idx": 29}, {"type": "text", "text": "(i) For the first term in (78), denoted as I, notice that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\left(f_{\\lambda}-f_{\\star}\\right)I_{x_{i}\\in\\Omega_{1}}\\|_{L^{\\infty}}\\leq\\|f_{\\lambda}\\|_{L^{\\infty}}+n^{\\frac{1-s}{2}+\\epsilon_{t}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Imitating (67) in the proof of Lemma D.7, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{I}=o_{\\mathbb{P}}\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(i) For the second term in (78), denoted as $\\mathrm{II}$ Since $\\begin{array}{r}{q=\\frac{2}{1-s}-\\epsilon_{q}<\\frac{2}{1-s}}\\end{array}$ , Lemma 42 in Zhang et al. (2024) shows that, ", "page_idx": 29}, {"type": "equation", "text": "$$\n[\\mathcal{H}]^{s}\\hookrightarrow L^{q}(\\mathcal{X},\\mu),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with embedding norm less than a constant $C_{s,\\kappa}$ . Then Assumption 2 (a) implies that there exists $0\\,<\\,C_{q}\\,<\\,\\infty$ only depending on $\\gamma,s$ and $\\kappa$ such that $\\|f_{\\star}\\|_{L^{q}(\\mathcal{X},\\mu)}\\,\\le\\,C_{q}$ .Using the Markov inequality, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nP(x\\in\\Omega_{2})=P\\Big(|f_{\\star}(x)|>t\\Big)\\leq\\frac{\\mathbb{E}|f_{\\star}(x)|^{q}}{t^{q}}\\leq\\frac{(C_{q})^{q}}{t^{q}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Further, since (80) guarantees $t^{q}\\gg n$ ,we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{n}:=P\\left(\\Pi>0\\right)}\\\\ &{\\quad\\le P\\Big(\\;\\exists x_{i}\\;\\mathrm{s.t.}\\;x_{i}\\in\\Omega_{2},\\Big)=1-P\\Big(x_{i}\\notin\\Omega_{2},\\forall x_{i},i=1,2,\\cdots,n\\Big)}\\\\ &{\\quad=1-P\\Big(x\\notin\\Omega_{2}\\Big)^{n}}\\\\ &{\\quad=1-P\\Big(|f_{\\star}(x)|\\le t\\Big)^{n}}\\\\ &{\\quad\\le1-\\Big(1-\\frac{(C_{q})^{q}}{t^{q}}\\Big)^{n}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(ii) For the third term in (78), denoted as II1. Since Lemma 37 in Zhang et al. (2024) implies that $\\|T_{\\lambda}^{-\\frac{1}{2}}k(x,\\cdot)\\|_{\\mathcal{H}}\\leq\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}},\\mu$ -a.e. $x\\in\\mathscr{X}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{III}\\leq\\mathbb{E}\\|\\xi_{x}I_{x\\in\\Omega_{2}}\\|_{\\mathcal{H}}\\leq\\mathbb{E}\\Big[\\|T_{\\lambda}^{-\\frac{1}{2}}k(x,\\cdot)\\|_{\\mathcal{H}}\\cdot\\big|\\big(f_{\\star}-f_{\\lambda}(x)\\big)I_{x\\in\\Omega_{2}}\\big|\\Big]}\\\\ &{\\quad\\leq\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\mathbb{E}\\big|\\big(f_{\\star}-f_{\\lambda}(x)\\big)I_{x\\in\\Omega_{2}}\\big|}\\\\ &{\\quad\\leq\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\,\\|f_{\\star}-f_{\\lambda}\\|_{L^{2}}^{\\frac{1}{2}}\\cdot P\\,\\big(x\\in\\Omega_{2}\\big)^{\\frac{1}{2}}}\\\\ &{\\leq\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\mathcal{M}_{2,\\varphi}(\\lambda)^{\\frac{1}{2}}t^{-\\frac{q}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we use Cauchy-Schwarz inequality for the third inequality and Lemma D.6 for the fourth inequality. Recalling that the choices of $t,q$ satisfy $t^{-q}\\ll n^{-\\bar{1}}$ and we have assumed $n^{\\epsilon-1}\\mathcal{N}_{1}(\\lambda)\\to$ O,wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{III}=o\\left({\\cal M}_{2,\\varphi}(\\lambda)^{\\frac{1}{2}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Plugging (82), (84) and (87) into (78), we finish the proof. ", "page_idx": 29}, {"type": "text", "text": "Final proof of the bias term  Now we are ready to state the theorem about the bias term. ", "page_idx": 29}, {"type": "text", "text": "Theorem D.9 $[s\\geq1]$ .Suppose that (45) in Assumption $^3$ holds. Suppose that there exist constants $\\epsilon$ and ${\\mathfrak C}$ only depending on s and $\\gamma$ such that $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n^{\\epsilon-1}\\mathcal{N}_{1}(\\lambda)\\to0,}}\\\\ {{\\displaystyle\\frac{\\mathcal{N}_{1}(\\lambda)\\mathcal{M}_{1,\\varphi}^{2}(\\lambda)}{n^{2}}\\ll\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right),}}\\\\ {{\\displaystyle\\frac{\\mathcal{N}_{1}(\\lambda)}{n}\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\sum_{j=1}^{\\infty}\\frac{\\lambda^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}\\ll\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right);}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\mathbf{Bias}^{2}(\\lambda)-\\mathcal{M}_{2,\\varphi}(\\lambda)\\right|=o_{\\mathbb{P}}\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Theorem D.10 $\\left.s<1\\right.$ 0. Suppose that (45) in Assumption 3 holds. Suppose that there exist constants Eand ${\\mathfrak C}$ only depending onsand $\\gamma$ suchthat $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{n^{\\epsilon-1}\\mathcal{N}_{1}(\\lambda)\\to0,}}\\\\ &{}&{\\frac{\\mathcal{N}_{1}(\\lambda)}{n}\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\displaystyle\\sum_{j=1}^{\\infty}\\frac{\\lambda^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}\\ll\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right);\\quad}\\\\ &{}&{n^{-1}\\mathcal{N}_{1}(\\lambda)^{\\frac{1}{2}}\\left(\\|f_{\\lambda}\\|_{L^{\\infty}}+n^{\\frac{1-s}{2}+\\epsilon}\\right)=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right)^{1/2};}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left|\\mathbf{Bias}^{2}(\\lambda)-\\mathcal{M}_{2,\\varphi}(\\lambda)\\right|=o_{\\mathbb{P}}\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "D.4 Quantity calculations and conditions verification for the inner product kernels ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In the previous two sections, we have successfully bounded the bias and the variance terms by the quantities $\\mathcal{M}_{2,\\varphi}(\\lambda)$ and $\\mathcal{N}_{2,\\varphi}(\\lambda)$ . In this subsection, we will focus on the inner product kernels on the sphere. We will (i) determine the rates for the above quantities, and (ii) verify all the conditions in Theorem D.5, Theorem D.9 and Theorem D.10. ", "page_idx": 30}, {"type": "text", "text": "Recall that $\\mu_{k}$ and $N(d,k)$ , defined in (9), are the eigenvalues of the inner product kernel $K$ defined on the sphere and the corresponding multiplicity. The following three lemmas (mainly cited from $\\mathrm{Lu}$ et al. (2023)) give concise characterizations of $\\mu_{k}$ and $N(d,k)$ , which is sufficient for the analysis in this paper. ", "page_idx": 30}, {"type": "text", "text": "LemmaD.11.For any fixed integer $p\\geq0,$ there exist constants ${\\mathfrak{C}},{\\mathfrak{C}}_{9}$ and ${\\mathfrak{C}}_{10}$ only depending on $p$ and $\\{a_{j}\\}_{j\\leq p+1}$ , such that for any $d\\geq\\mathfrak{C}$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\mathfrak{C}}_{9}d^{-k}\\leq\\mu_{k}\\leq{\\mathfrak{C}}_{10}d^{-k},\\;\\;k=0,1,\\cdots\\,,p+1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma D.12. For any fixed integer $p~\\geq~0,$ . there exist constants ${\\mathfrak C}$ only depending on $p$ and $\\{a_{j}\\}_{j\\leq p+1}$ ,such that for any $d\\geq\\mathfrak{C},$ wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mu_{k}\\leq\\frac{\\mathfrak{C_{10}}}{\\mathfrak{C_{9}}}d^{-1}\\mu_{p},\\quad k=p+1,p+2,\\cdot\\cdot\\cdot\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ${\\mathfrak{C}}_{9}$ and ${\\mathfrak{C}}_{10}$ are constants given in Lemma $D.I I$ ", "page_idx": 30}, {"type": "text", "text": "Lemma D.13. For any fixed integer $p\\geq0$ there exist constants $\\mathfrak{C}_{11},\\mathfrak{C}_{12}$ and ${\\mathfrak C}$ only depending on $p$ such that for any $d\\geq\\mathfrak{C}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathfrak{C}_{11}d^{k}\\le N(d,k)\\le\\mathfrak{C}_{12}d^{k},\\quad k=0,1,\\cdots,p+1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "With these lemmas, we can begin to bound the quantities $\\mathcal{M}_{2,\\varphi}(\\lambda)$ and $\\mathcal{N}_{2,\\varphi}(\\lambda)$ ", "page_idx": 30}, {"type": "text", "text": "Lemma D.14.Suppose that Assumption $^{\\,l}$ and Assumption 2 hold for s and an integer $p$ Suppose $\\ell\\leq p,\\,t=\\lambda^{-1}\\in\\dot{(d^{\\ell},d^{\\ell+1}]}$ . Then we have the following bound. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle M_{2,\\varphi}(\\lambda)=\\left\\{\\Theta\\left(d^{-s}(\\ell^{+1})\\right)\\phantom{\\bigg(}}}&{{\\tau=\\infty\\qquad\\qquad\\qquad}}\\\\ {{\\displaystyle\\Theta\\left(t^{-2\\tau}d^{\\ell(2\\tau-s)}+d^{-s(\\ell+1)}\\right)}}&{{s\\leq2\\tau<\\infty\\qquad}}\\\\ {{\\displaystyle\\Theta\\left(\\lambda^{2\\tau}\\right)}}&{{s>2\\tau}}\\\\ {{\\displaystyle\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda)}{n}=\\Theta\\left(\\frac{d^{\\ell}}{n}+\\frac{t^{2}}{n d^{\\ell+1}}\\right)}}&{{\\,}}\\\\ {{\\displaystyle\\sum_{k=0}^{\\infty}\\frac{\\lambda^{2}\\mu_{k}\\varphi_{\\lambda}^{2}(\\mu_{k})}{\\lambda+\\mu_{k}}\\,\\sum_{j=1}^{N(d,k)}\\,f_{k,j}^{2}=O\\left(\\lambda^{2}d^{\\operatorname*{max}\\{p(2-s),0\\}}+d^{-s(\\ell+1)}\\right);}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and thus Assumption $^3$ holds. Moreover, when $s\\geq1$ ,We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{1,\\varphi}^{2}(\\lambda)=\\left\\{O\\left(\\lambda^{2\\tau-1}d^{\\ell(2\\tau-s)}+d^{-(\\ell+1)(s-1)}\\right)\\quad s\\leq2\\tau<\\infty}\\\\ {O\\left(\\lambda^{2\\tau-1}d^{\\ell(2\\tau-s)}+d^{-(\\ell+1)(s-1)}\\right)\\quad s\\leq2\\tau<\\infty}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. 1. We begin with $\\mathcal{M}_{2,\\varphi}(\\lambda)$ .If $s\\leq2\\tau$ and $\\tau<\\infty$ , then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{2,\\varphi}(\\lambda)=\\displaystyle\\sum_{k=0}^{\\infty}\\psi_{\\lambda}^{2}(\\mu_{k})\\sum_{j=1}^{N(d,k)}f_{k,j}^{2}}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\sum_{k=0}^{\\infty}\\mathbb{C}_{2}^{2}(\\mu_{k})^{-2\\tau}(\\mu_{k})^{s}\\sum_{j=1}^{N(d,k)}\\left(\\mu_{k}\\right)^{-s}f_{k,j}^{2}+\\displaystyle\\sum_{k=\\ell+1}^{\\infty}\\psi_{\\lambda}^{2}(\\mu_{k})\\sum_{j=1}^{N(d,k)}f_{k,j}^{2}}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\sum_{k=0}^{\\ell}\\mathbb{C}_{2}^{2}(\\mu_{k})^{-2\\tau}(\\mu_{k})^{s}\\sum_{j=1}^{N(d,k)}\\left(\\mu_{k}\\right)^{-s}f_{k,j}^{2}+\\displaystyle\\sum_{k=\\ell+1}^{\\infty}\\left(\\mu_{k}\\right)^{s}\\sum_{j=1}^{N(d,k)}(\\mu_{k})^{-s}f_{k,j}^{2}}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\mathbb{C}_{2}^{2}t^{-2\\tau}(\\mathbb{C}_{0}d^{-\\ell})^{s-2\\tau}\\sum_{k=0}^{\\ell}\\ \\frac{\\iota^{N(d,k)}}{j!}(\\mu_{k})^{-s}f_{k,j}^{2}+(\\mathbb{C}_{10}d^{-\\ell-1})^{s}\\sum_{k=\\ell+1}^{\\infty}\\ \\sum_{j=1}^{N(d,k)}\\left(\\mu_{k}\\right)^{-s}f_{k,j}^{2}}\\\\ &{\\qquad=O\\left(t^{-2\\tau}d^{\\ell(2\\tau-s)}+d^{-s(\\ell+1)}\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and when $\\tau=\\infty$ a similar argument taking $\\tau^{\\prime}<\\tau$ and let $\\tau^{\\prime}\\rightarrow\\infty$ then we have $(t d^{-\\ell})^{-2\\tau^{\\prime}}\\rightarrow0)$ shows that $\\mathcal{M}_{2,\\varphi}(\\lambda)=O(d^{-s(\\ell+1)})$ ", "page_idx": 31}, {"type": "text", "text": "Similarly, if $s\\leq2\\tau$ , then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{2\\sigma}(\\lambda)\\geq\\mathbf{1}\\{\\tau<\\infty\\}\\displaystyle\\sum_{k=1}^{r}\\exp_{0}^{2}(q_{k})^{\\sum_{j=1}^{N}((\\mu_{k}))^{\\alpha_{j}}}\\int_{0}^{\\lambda(\\lambda)}(\\mu_{k})^{-\\nu_{j}}\\hat{L}_{k,j}^{2}}\\\\ &{\\qquad+\\displaystyle\\sum_{k=1}^{r}\\exp_{1}^{2}(\\mu_{k})\\sum_{j=1}^{N(\\lambda)}\\int_{\\lambda}^{\\nu_{j}}\\hat{L}_{j}^{2}}\\\\ &{\\qquad\\geq\\mathbf{1}\\{\\tau<\\infty\\})\\left(\\tau^{-\\nu_{j}}d(^{(2r-1)})\\right)}\\\\ &{\\qquad+\\displaystyle\\sum_{k=t+1}^{r}\\exp_{1}^{2}(\\mu_{k})^{\\nu_{j}}\\sum_{j=1}^{N(\\lambda)}(\\mu_{k})^{-\\nu_{j}}\\hat{L}_{k,j}^{2}}\\\\ &{\\qquad\\geq\\mathbf{1}\\{\\tau<\\infty\\}\\Re\\left(t^{-2}r d^{(2r-1)}\\right)}\\\\ &{\\qquad+\\displaystyle\\sum_{k=1}^{r}(\\tau_{1}\\omega_{1}\\hat{L}^{-\\nu_{j}})^{\\nu_{j}}\\frac{\\hat{L}_{k}(r)^{-\\nu_{j}}}{\\nu_{1}}}\\\\ &{\\qquad=1\\{r<\\infty\\}\\mathbf{1}\\left(t^{-r}d^{(1r-1)}\\right)+\\textrm{P}_{k}^{(1r-1)}\\right)}\\\\ &{\\qquad=1\\{r<\\infty\\}\\mathbf{1}\\left(t^{-r}\\tau_{1}\\omega_{1}\\hat{L}^{-\\nu_{j}}\\right)+\\textrm{O}\\Big(d^{-r((1r-1)}\\Big)\\cdot}\\\\ &{\\qquad=1\\{r<\\infty\\}\\mathbf{1}\\left(t^{-r-1}\\right)\\cdot\\mathbf{1}\\cdot\\mathbf{1}\\left(d^{-r-1}\\right)+\\textrm{O}\\Big(d^{-r((1r-1)}\\Big)\\cdot}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "If $2\\tau<s$ , then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle M_{2,\\varphi}(\\lambda)=\\sum_{k=0}^{\\infty}\\psi_{\\lambda}^{2}(\\mu_{k})\\sum_{j=1}^{N(d,k)}f_{k,j}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\overset{\\mathrm{Lemma~}E.3}{\\leq}\\kappa^{2(s-2\\tau)}\\lambda^{2\\tau}\\sum_{k=0}^{\\infty}\\sum_{j=1}^{N(d,k)}\\mu_{k}^{-s}f_{k,j}^{2}}\\\\ &{\\quad\\quad\\quad=O\\left(\\lambda^{2\\tau}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similarly, if $2\\tau<s$ , then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{2,\\varphi}(\\lambda)\\geq\\psi_{\\lambda}^{2}(\\mu_{0})f_{0,1}^{2}}\\\\ &{\\qquad\\qquad\\geq\\mathfrak{C}_{6}^{2}f_{0,1}^{2}\\cdot\\lambda^{2\\tau}}\\\\ &{\\qquad\\qquad=\\Omega\\left(\\lambda^{2\\tau}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "II. Now let's bound the second term $\\mathcal{N}_{2,\\varphi}(\\lambda)/n$ We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\widetilde{N}_{2,\\varphi}(\\lambda)}{n}=\\frac{1}{n}\\sum_{k=0}^{\\infty}N(d,k)\\left[\\mu_{k}\\varphi_{\\lambda}(\\mu_{k})\\right]^{2}}\\qquad}&{}\\\\ &{\\leq\\frac{1}{n}\\sum_{k=0}^{\\ell}N(d,k)+\\frac{1}{n}\\sum_{k=\\ell+1}^{\\infty}N(d,k)\\left[\\mu_{k}\\varphi_{\\lambda}(\\mu_{k})\\right]^{2}}\\\\ &{\\leq\\frac{1}{n}\\sum_{k=0}^{\\ell}N(d,k)+\\frac{\\mathfrak{C_{4}^{2}}t^{2}}{n}\\sum_{k=\\ell+1}^{\\infty}N(d,k)(\\mu_{k})^{2}}\\\\ &{\\leq\\ell\\frac{N(d,\\ell)}{n}+\\frac{\\mathfrak{C_{4}^{2}}t^{2}}{n}\\mu_{\\ell+1}}\\\\ &{=O\\left(\\frac{d^{2}}{n}+\\frac{t^{2}}{n d^{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda)}{n}\\geq\\frac{\\mathfrak{C_{1}^{2}}}{n}\\sum_{k=0}^{\\ell}N(d,k)+\\frac{\\mathfrak{C_{3}^{2}}t^{2}}{n}\\sum_{k=\\ell+1}^{\\infty}N(d,k)(\\mu_{k})^{2}}\\\\ {\\displaystyle\\geq\\mathfrak{C_{1}^{2}}\\frac{N(d,\\ell)}{n}+\\frac{\\mathfrak{C_{3}^{2}}t^{2}}{n}\\mu_{\\ell+1}}\\\\ {\\displaystyle=\\Omega\\left(\\frac{d^{\\ell}}{n}+\\frac{t^{2}}{n d^{\\ell+1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "II. For the third term, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\displaystyle\\sum_{k=0}^{\\infty}\\frac{\\lambda^{2}\\mu_{k}\\varphi_{\\lambda}^{2}(\\mu_{k})}{\\lambda+\\mu_{k}}\\sum_{j=1}^{N(d,k)}f_{k,j}^{2}\\le\\lambda^{2}R_{\\gamma}^{2}\\left(\\displaystyle\\sum_{k=0}^{p}\\mu_{k}^{s}\\varphi_{\\lambda}^{2}(\\mu_{k})+\\lambda^{-1}\\displaystyle\\sum_{k=p+1}^{\\infty}\\mu_{k}^{s+1}\\mathfrak{C}_{4}^{2}\\lambda^{-2}\\right)}&{}\\\\ {\\displaystyle}&{={\\cal O}\\left(\\lambda^{2}d^{\\operatorname*{max}\\{p(2-s),0\\}}+\\lambda^{-1}d^{-(s+1)(\\ell+1)}\\right)}\\\\ &{={\\cal O}\\left(\\lambda^{2}d^{\\operatorname*{max}\\{p(2-s),0\\}}+d^{-s(\\ell+1)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "IV. Now we show that Assumption 3 holds. Notice that (45) has been verified in Lemma 20 of Zhang et al. (2024). Similarly, one can prove (43) and (44) hold using a similar proof as that for Lemma 20 of Zhang et al. (2024). ", "page_idx": 32}, {"type": "text", "text": "V. For the final term, when $s\\geq1$ ,we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{1,\\varphi}^{2}(\\lambda)=\\displaystyle\\exp_{x\\in\\mathcal{X}}\\left|\\sum_{i=1}^{\\infty}(\\psi_{\\lambda}(\\lambda_{i})f_{i}e_{i}(x))\\right|^{2}}\\\\ &{\\qquad\\qquad\\le\\left(\\displaystyle\\sum_{i=1}^{\\infty}\\frac{\\psi_{\\lambda}(\\lambda_{i})}{\\lambda_{i}\\varphi_{\\lambda}(\\lambda_{i})}f_{i}^{2}\\right)\\cdot\\displaystyle\\exp_{x\\in\\mathcal{X}}\\sum_{i=1}^{\\infty}\\left(\\lambda_{i}\\varphi_{\\lambda}(\\lambda_{i})e_{i}(x)^{2}\\right)}\\\\ &{\\qquad\\qquad\\overset{\\mathrm{Assumpion~3}}{\\le}\\left(\\displaystyle\\sum_{i=1}^{\\infty}\\frac{\\psi_{\\lambda}(\\lambda_{i})}{\\lambda_{i}\\varphi_{\\lambda}(\\lambda_{i})}f_{i}^{2}\\right)\\cdot\\displaystyle\\sum_{i=1}^{\\infty}\\lambda_{i}\\varphi_{\\lambda}(\\lambda_{i})}\\\\ &{:=Q_{1,\\varphi}(\\lambda)\\cdot\\mathcal{N}_{1,\\varphi}(\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For $\\mathcal{Q}_{1,\\varphi}(\\lambda)$ , when $\\tau\\geq s/2$ and $\\tau<\\infty$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{Q}_{1,\\varphi}(\\lambda)=\\displaystyle\\sum_{k=0}^{\\infty}\\frac{\\psi_{\\lambda}^{2}(\\mu_{k})\\mu_{k}^{s-1}}{\\varphi_{\\lambda}(\\mu_{k})}\\sum_{j=1}^{N(d,k)}\\mu_{k}^{-s}f_{k,j}^{2}}\\\\ {\\leq\\displaystyle\\frac{\\mathfrak{C}_{2}^{2}}{\\mathfrak{C}_{1}}\\sum_{k=0}^{\\ell}\\lambda^{2\\tau}\\mu_{k}^{-2\\tau+s}\\,\\sum_{j=1}^{N(d,k)}\\mu_{k}^{-s}f_{k,j}^{2}}\\\\ {\\quad\\,+\\,(\\mathfrak{C}_{3})^{-1}\\lambda\\displaystyle\\sum_{k=\\ell+1}^{\\infty}\\mu_{k}^{s-1}\\displaystyle\\sum_{j=1}^{N(d,k)}\\,\\mu_{k}^{-s}f_{k,j}^{2}}\\\\ {=O\\left(\\lambda^{2\\tau}d^{\\ell(2\\tau-s)}+\\lambda d^{-(\\ell+1)(s-1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Similarly, when $\\tau=\\infty$ , we can show that $\\mathcal{Q}_{1,\\varphi}(\\lambda)=O(\\lambda d^{-(\\ell+1)(s-1)})$ And when $\\tau<s/2$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Q_{1,\\varphi}(\\lambda)=\\displaystyle\\sum_{k=0}^{\\infty}\\frac{\\psi_{k}^{2}(\\mu_{k})\\mu_{k}^{k-1}}{\\psi_{k}(\\mu_{k})}\\sum_{j=1}^{N(d,k)}\\mu_{k}^{-\\nu}\\bar{f}_{k,j}^{2}}&{}\\\\ {\\tan E_{\\mu}:\\underbrace{Q_{\\mu}^{\\nu;\\lambda;\\alpha;\\beta;\\tau}}_{\\mathcal{C}_{1}}}&{\\sum_{k=1}^{P}\\frac{N(d,k)}{\\mu_{k}^{-\\nu}}\\sum_{j=1}^{N(d,k)}\\mu_{k}^{-\\nu}\\bar{f}_{k,j}^{2}}\\\\ &{+\\displaystyle\\sum_{k=r+1}^{\\infty}\\frac{\\psi_{k}^{2}(\\mu_{k})\\mu_{k}^{-\\nu;\\lambda;\\alpha;\\beta;\\tau}}{\\psi_{\\lambda}(\\mu_{k})}\\sum_{j=1}^{N(d,k)}\\mu_{k}^{-\\nu}\\bar{f}_{k,j}^{2}}\\\\ {\\overset{(a)}{\\leq}\\frac{Q_{\\mu}^{2}\\kappa^{(2,k)/2}}{\\epsilon_{1}}\\sum_{j=1}^{N^{2}}\\sum_{\\nu=0}^{P}\\sum_{j=1}^{N^{2}}\\mu_{k}^{-\\nu}\\bar{f}_{k,j}^{2}}&{}\\\\ &{+\\displaystyle\\sum_{k=r+1}^{\\infty}\\epsilon_{k}\\lambda^{2\\nu}\\sum_{j=1}^{N^{2}}\\mu_{k}^{-\\nu}\\bar{f}_{k,j}^{2}}\\\\ &{=O(\\lambda^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $\\mathcal{N}_{1,\\varphi}(\\lambda)$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle{\\mathcal{N}}_{1,\\varphi}(\\lambda)=\\sum_{k=0}^{\\infty}N(d,k)\\left[\\mu_{k}\\varphi_{\\lambda}(\\mu_{k})\\right]}\\\\ &{\\qquad\\le\\sum_{k=0}^{\\ell}N(d,k)+\\sum_{k=\\ell+1}^{\\infty}N(d,k)\\left[\\mu_{k}\\varphi_{\\lambda}(\\mu_{k})\\right]}\\\\ &{\\qquad\\le\\displaystyle\\sum_{k=0}^{\\ell}N(d,k)+\\mathfrak{C_{4}}t\\sum_{k=\\ell+1}^{\\infty}N(d,k)\\mu_{k}}\\\\ &{\\qquad\\le\\ell N(d,\\ell)+\\mathfrak{C_{4}}t}\\\\ &{\\displaystyle=O\\left(d^{\\ell}+\\lambda^{-1}\\right)=O\\left(\\lambda^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, when $s\\geq1$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{1,\\varphi}^{2}(\\lambda)=\\left\\{O\\left(\\lambda^{2\\tau-1}d^{\\ell(2\\tau-s)}+d^{-(\\ell+1)(s-1)}\\right)\\quad\\hfill s\\leq2\\tau<\\infty}\\\\ {O\\left(\\lambda^{2\\tau-1}d^{\\ell(2\\tau-s)}+d^{-(\\ell+1)(s-1)}\\right)\\quad\\hfill s\\leq2\\tau<\\infty}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "From Lemma D.14, we have the following three corollaries. ", "page_idx": 34}, {"type": "text", "text": "Corollary D.15. Let $1~\\le~s~\\le~\\tau$ and $\\gamma\\,>\\,0$ be fixed real numbers. Denote $p$ as the integer satisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ . Suppose one of the following cases holds for $\\lambda^{\\star}=d^{-\\ell}$ or $\\lambda^{\\star}=d^{-\\ell}\\cdot p o l y\\left(\\ln(d)\\right)$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n(l)~p\\ge1,\\,p(s+1)\\le\\gamma<p s+p+s,\\,\\ell=p+1/2\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\np\\geq1,p s+p+s\\leq\\gamma<p s+p+s+1,\\,\\ell=(\\gamma-(p+1)(s-1))/2\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})\\lesssim\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})}{n}=\\Theta\\left(d^{-s(p+1)}+\\frac{d^{p}}{n}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "or ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})\\lesssim\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})}{n}=\\Theta\\left(d^{-s(p+1)}+\\frac{d^{p}}{n}\\right)\\cdot p o l y\\left(\\ln(d)\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Corollary D.16. Let $\\tau<s\\leq2\\tau$ and $\\gamma>0$ befixedrealnumbers.Denote $p$ astheintegersatisfying $\\gamma\\in[p(s+1),(p+1)(s+1))$ . Denote $\\Delta=\\gamma-p(s+1)$ Supposeoneof thefollowingcasesholds for $\\lambda^{\\star}=d^{-\\ell}$ or $\\lambda^{\\star}=d^{-\\ell}\\cdot p o l y\\left(\\ln(d)\\right)$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\geq1,\\,0\\leq\\Delta\\leq\\tau,\\,\\ell=\\ell_{1}:=p+\\Delta/(2\\tau)}\\\\ &{\\gamma\\geq1,\\,\\tau\\leq\\Delta\\leq s+s/\\tau-1,\\,\\ell=\\ell_{2}:=p+(\\Delta+1)/(2\\tau+2)}\\\\ &{\\gamma\\geq1,\\,\\Delta\\geq s+s/\\tau-1,\\,\\ell=\\ell_{3}:=p+(\\Delta+1-s)/2}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})\\asymp\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})}{n}=\\Theta\\left(d^{-\\operatorname*{min}\\left\\{\\gamma-p,\\frac{\\tau(\\gamma-p+1)+p s}{\\tau+1},s(p+1)\\right\\}}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "or ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})\\asymp\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})}{n}=\\Theta\\left(d^{-\\operatorname*{min}\\left\\{\\gamma-p,\\frac{\\tau(\\gamma-p+1)+p s}{\\tau+1},s(p+1)\\right\\}}\\right)\\cdot p o l y\\left(\\ln(d)\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Denote ${\\bf I}=-2\\ell\\tau+2p\\tau-p s$ \uff0c $\\mathbf{II}=-s p-s,\\mathbf{III}=p-\\gamma$ and ${\\bf I V}=2\\ell-\\gamma-p-1$ . From Lemma D.14 we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2,\\varphi}(\\boldsymbol{\\lambda}^{\\star})\\asymp d^{\\mathbf{I}}+d^{\\mathbf{II}},\\quad\\frac{\\mathcal{N}_{2,\\varphi}(\\boldsymbol{\\lambda}^{\\star})}{n}\\asymp d^{\\mathbf{III}}+d^{\\mathbf{IV}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can verify that: ", "page_idx": 34}, {"type": "text", "text": "(1) When $0\\leq\\Delta\\leq\\tau$ and $\\ell=p+\\Delta/(2\\tau)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{II}\\leq\\mathbf{I}=\\mathbf{III}\\geq\\mathbf{IV}{\\mathrm{~and~}}\\operatorname*{min}\\left\\{\\gamma-p,{\\frac{\\tau(\\gamma-p+1)+p s}{\\tau+1}},s(p+1)\\right\\}=\\gamma-p;\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "(2) When $\\tau\\leq\\Delta\\leq s+s/\\tau-1$ and $\\ell=p+(\\Delta+1)/(2\\tau+2)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{II}\\leq\\mathbf{I}=\\mathbf{IV}\\geq\\mathbf{III}{\\mathrm{~and~}}\\operatorname*{min}\\left\\{\\gamma-p,{\\frac{\\tau(\\gamma-p+1)+p s}{\\tau+1}},s(p+1)\\right\\}={\\frac{\\tau(\\gamma-p+1)+p s}{\\tau+1}};\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "(3) When $\\Delta\\geq s+s/\\tau-1$ and $\\ell=p+(\\Delta+1-s)/2$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{I}\\leq\\mathbf{II}=\\mathbf{IV}\\geq\\mathbf{III}{\\mathrm{~and~}}\\operatorname*{min}\\left\\{\\gamma-p,{\\frac{\\tau(\\gamma-p+1)+p s}{\\tau+1}},s(p+1)\\right\\}=s(p+1);\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "(4) When $\\gamma<1$ and $\\ell=\\gamma/2$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\boldsymbol{\\mathbf{III}}}\\geq\\operatorname*{max}\\{{\\boldsymbol{\\mathbf{I}}},{\\boldsymbol{\\mathbf{II}}},{\\boldsymbol{\\mathbf{IV}}}\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Corollary D.17. Let $s<1$ and $\\gamma>0$ be fixed real numbers. Denote $p$ as the integer satisfying $\\gamma\\ \\in\\ [p(s+1),(p+1)(s+1))$ Suppose one of the following cases holds for $\\lambda^{\\star}\\;=\\;d^{-\\ell}$ or $\\lambda^{\\star}=d^{-\\ell}\\cdot p o l y\\left(\\ln(d)\\right)$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tau=\\infty,\\,p\\geq1,\\,p(s+1)\\leq\\gamma<p s+p+s,\\,\\ell=p+s/2\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tau=\\infty,\\,p\\geq1,\\,p s+p+s\\leq\\gamma<p s+p+s+1,\\,\\ell=(\\gamma+p(1-s))/2\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\ns\\le\\gamma<s+1,\\,\\ell=\\operatorname*{min}\\{(\\gamma+(1-s))/2,\\gamma(1+s)-s,\\gamma/2\\}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gg,p(s+1)\\leq\\gamma<p s+p+s,\\,\\ell=(\\gamma+2\\tau p-s p-p)/(2\\tau)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\ns+p+s\\leq\\gamma<p s+p+s+1,\\,\\ell=p+s/(2\\tau)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})+\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})}{n}=\\Theta\\left(d^{-s(p+1)}+\\frac{d^{p}}{n}\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "or ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})+\\frac{\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})}{n}=\\Theta\\left(d^{-s(p+1)}+\\frac{d^{p}}{n}\\right)\\cdot p o l y\\left(\\ln(d)\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "D.4.1  Verification of variance conditions ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Lemma D.18 (Verification of variance conditions for inner-product kernels). Suppose $n\\asymp d^{\\gamma}$ and $s\\geq1,$ for $\\gamma\\in[p(s+1),(p+1)(s+1))$ .For anygiven $\\ell\\geq0$ $i f$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\lambda\\geq\\left\\{\\begin{array}{c c}{{d^{-\\ell}\\left(1+\\ln^{2}(d){\\bf1}\\{\\gamma=2,s=1\\}\\right)}}&{{p\\geq1,\\ 2\\ell\\leq\\operatorname*{max}\\{2p+1,\\gamma-(p+1)(s-1)\\}}}\\\\ {{d^{-\\ell}\\ln^{2}(d)}}&{{p=0,\\gamma\\geq1,\\ 2\\ell\\leq\\operatorname*{max}\\{1,\\gamma-(s-1)\\}}}\\\\ {{d^{-\\ell}}}&{{p=0,\\gamma<1,\\ 2\\ell\\leq\\gamma;}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "then there exists a constant $\\epsilon>0$ only depending on s and $\\gamma_{;}$ such that $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}_{1}(\\lambda)\\cdot n^{\\epsilon-1}\\to0,}\\\\ {\\frac{\\mathcal{N}_{1}^{2}(\\lambda)}{n\\mathcal{N}_{2,\\varphi}(\\lambda)}\\cdot\\ln(n)(\\ln\\lambda^{-1})^{2}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. From Lemma 21 in Zhang et al. (2024), we have $\\mathcal{N}_{1}(\\lambda)\\asymp\\lambda^{-1}$ . When $p=0$ , we have $\\gamma-\\ell>0$ . When $p\\geq1$ , we have $\\gamma-p-1/2\\geq p s-1/2>\\dot{0}$ Therefore, there exists a constant $\\epsilon>0$ only depending on $s$ and $\\gamma$ , such that we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\mathcal{N}}_{1}(\\lambda)\\cdot n^{\\epsilon-1}\\to0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Denote $q:=\\lfloor\\ell\\rfloor$ . From Lemma D.14, we further have $\\mathcal{N}_{2,\\varphi}(\\lambda)=\\Omega\\left(d^{q}+{\\lambda}^{-2}d^{-q-1}\\right)$ . Hence, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{{\\mathcal{N}}_{1}^{2}(\\lambda)}{n{\\mathcal{N}}_{2,\\varphi}(\\lambda)}}\\cdot\\ln(n)(\\ln\\lambda^{-1})^{2}=O\\left({\\frac{(\\ln(d))^{3}}{n(\\lambda^{2}d^{q}+d^{-q-1})}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Denote  := (ln(@d)3 $\\begin{array}{r}{\\Delta^{\\prime}:=\\frac{(\\ln(d))^{3}}{d^{\\gamma-q-1}}}\\end{array}$ , then when $\\Delta=o(1)$ or $\\Delta^{\\prime}=o(1)$ wehave: ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{{\\mathcal N}_{1}^{2}(\\lambda)}{n\\mathcal{N}_{2,\\varphi}(\\lambda)}}\\cdot\\ln(n)(\\ln\\lambda^{-1})^{2}\\to0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now we show that $\\Delta=o(1)$ ", "page_idx": 35}, {"type": "text", "text": "\u00b7 When $p\\geq3$ and $p=2,s>1$ , ince $\\gamma-2\\ell+q\\geq(\\gamma-\\ell-1)+(q+1-\\ell)>0$ we have $\\Delta=o(1)$   \n\u00b7 When $p=2,s=1$ , since $2\\ell-q<\\ell+1<4\\leq\\gamma$ , we have $\\Delta=o(1)$   \n\u00b7 When $p=2,s=1$ , since $2\\ell-q<\\ell+1<4\\leq\\gamma$ we have $\\Delta=o(1)$   \n\u00b7 When $p=1,\\gamma>2s+1$ , since $\\ell<2$ and hence $2\\ell-q<3\\leq\\gamma$ we have $\\Delta=o(1)$   \n\u00b7 When $p=1,s>1,\\gamma\\leq2s+1$ or $p=1,s=1,\\gamma>2$ , since $2\\ell-q\\leq2<\\gamma$ , we have $\\Delta=o(1)$   \n\u00b7 When $p=1$ \uff0c $s=1$ \uff0c $\\gamma=2$ , ince $2\\ell-q\\leq2\\leq\\gamma$ we have $\\Delta=O((\\ln(d))^{-1})$   \n\u00b7 When $p=0$ , ince $\\gamma-2\\ell\\geq0$ , we have $\\Delta=O((\\ln(d))^{-1})$ ", "page_idx": 36}, {"type": "text", "text": "Lemma D.19 (Verification of variance conditions for inner-product kernels: saturation case). Suppose $\\tau<s\\leq2\\tau$ Suppose $n\\asymp d^{\\gamma}$ for $\\gamma\\in[p(s+1)+\\tau,p(s+1)+s+s/\\tau-1]$ Foranygiven $\\ell\\geq0$ $i f$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lambda\\geq d^{-\\ell},\\quad\\ell\\leq p+(\\gamma-p(s+1)+1)/(2\\tau+2);\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "then there exists a constant $\\epsilon>0$ only depending on s and $\\gamma$ such that $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}_{1}(\\lambda)\\cdot n^{\\epsilon-1}\\to0,}\\\\ {\\frac{\\mathcal{N}_{1}^{2}(\\lambda)}{n\\sqrt{2},\\varphi(\\lambda)}\\cdot\\ln(n)(\\ln\\lambda^{-1})^{2}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. From Lemma 21 in Zhang et al. (2024), we have $\\mathcal{N}_{1}(\\lambda)\\asymp\\lambda^{-1}$ . Notice that we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n2(\\tau+1)(\\gamma-p)\\geq\\left\\{2\\tau^{2}+(\\tau-1)\\quad p\\geq1>0;\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, there exists a constant $\\epsilon>0$ only depending on $\\tau,s$ , and $\\gamma$ , such that we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\mathcal{N}}_{1}(\\lambda)\\cdot n^{\\epsilon-1}\\to0.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Denote $q:=\\lfloor\\ell\\rfloor$ . From Lemma D.14, we further have $\\mathcal{N}_{2,\\varphi}(\\lambda)=\\Omega\\left(d^{q}+{\\lambda}^{-2}d^{-q-1}\\right)$ .Hence, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathcal{N}_{1}^{2}(\\lambda)}{n\\mathcal{N}_{2,\\varphi}(\\lambda)}\\cdot\\ln(n)(\\ln\\lambda^{-1})^{2}=O\\left(\\frac{(\\ln(d))^{3}}{n(\\lambda^{2}d^{q}+d^{-q-1})}\\right)}}\\\\ &{}&{=O\\left(\\frac{(\\ln(d))^{3}}{n\\lambda^{2}d^{q}}\\right)+O\\left(\\frac{(\\ln(d))^{3}}{d^{\\gamma-q-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Denote \u25b3 := (ln(d) \uff0c $\\begin{array}{r}{\\Delta^{\\prime}:=\\frac{(\\ln(d))^{3}}{d^{\\gamma-q-1}}}\\end{array}$ . We have: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 When $p\\geq1$ , since ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2(\\tau+1)[\\gamma-2\\ell+q]\\geq2(\\tau+1)[(\\gamma-\\ell-1)+(q+1-\\ell)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq2}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq2(\\tau+1)(\\tau-1)+2[\\tau s+s-1]\\quad p=1}\\\\ &{\\qquad\\qquad\\qquad\\quad>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "we have $\\Delta=o(1)$ ", "page_idx": 36}, {"type": "text", "text": "\u00b7 When $p=0$ , since $\\gamma>1$ , we have $\\Delta^{\\prime}=o(1)$ ", "page_idx": 36}, {"type": "text", "text": "Lemma D.20 (Verification of variance conditions for inner-product kernels: misspecified case). Suppose $n\\asymp d^{\\gamma}$ and $0<s<1,$ for $\\gamma\\in[p(s+1),(p+1)(s\\bar{+}1))$ .Foranygiven $\\ell\\geq0$ $i f$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\lambda\\geq\\left\\{\\begin{array}{c c}{d^{-\\ell}}&{p\\geq1,\\;2\\ell\\leq\\operatorname*{max}\\{2p+s,\\gamma+p(1-s)\\}}\\\\ {d^{-\\ell}}&{p=0,\\gamma>s,\\;2\\ell\\leq\\gamma}\\\\ {d^{-\\ell}\\ln(d)}&{p=0,\\gamma\\leq s,\\;2\\ell\\leq\\gamma;}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then there exists a constant $\\epsilon>0$ only depending on s and $\\gamma_{;}$ suchthat $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}_{1}(\\lambda)\\cdot n^{\\epsilon-1}\\to0,}\\\\ {\\frac{\\mathcal{N}_{1}^{2}(\\lambda)}{n\\sqrt{2},\\varphi(\\lambda)}\\cdot\\ln(n)(\\ln\\lambda^{-1})^{2}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof.When $p~\\geq~1$ , it is a direct result of step 2 (the verification of the second condition in (146) of Zhang et al. (2024)) in the proof of Theorem 3 in Zhang et al. (2024) and the fact that $\\mathcal{N}_{2,\\varphi}(\\lambda)\\asymp\\tilde{\\mathcal{N}_{2}(\\lambda)}$ ", "page_idx": 37}, {"type": "text", "text": "When $p=0$ , a similar argument as the proof for Lemma D.18 give the desired results. ", "page_idx": 37}, {"type": "text", "text": "D.4.2  Verification of bias conditions ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lemma D.21 (Verification of bias conditions). Suppose $1\\le\\,s\\,\\le\\,\\tau$ Suppose $n\\asymp d^{\\gamma}$ for $\\gamma\\in$ $[p(s+1),(p+1)(s+1))$ .For anygiven $\\ell\\geq0$ $i f$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\lambda\\geq\\left\\{\\begin{array}{c c}{{d^{-\\ell}\\left(1+\\ln^{2}(d){\\bf1}\\{\\gamma=2,s=1\\}\\right)}}&{{p\\geq1,\\ 2\\ell\\leq\\operatorname*{max}\\{2p+1,\\gamma-(p+1)(s-1)\\}}}\\\\ {{d^{-\\ell}\\ln^{2}(d)}}&{{\\gamma\\in[1,s+1),\\ 2\\ell\\leq\\operatorname*{max}\\{1,\\gamma-(s-1)\\}}}\\\\ {{d^{-\\ell}}}&{{\\gamma\\in(0,1),\\ 2\\ell\\leq\\gamma;}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then there exists a constant $\\epsilon>0$ only depending on s and $\\gamma_{;}$ such that $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{{\\ensuremath{\\mathcal N}}_{1}(\\lambda){\\ensuremath{\\mathcal M}}_{1,\\varphi}^{2}(\\lambda)}{n^{2}}\\ll\\left({\\ensuremath{\\mathcal M}}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}{\\ensuremath{\\mathcal N}}_{2,\\varphi}(\\lambda)\\right),}\\\\ &{}&{\\displaystyle\\frac{{\\ensuremath{\\mathcal N}}_{1}(\\lambda)}{n}\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\displaystyle\\sum_{j=1}^{\\infty}\\frac{\\lambda^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}\\ll\\left({\\ensuremath{\\mathcal M}}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}{\\ensuremath{\\mathcal N}}_{2,\\varphi}(\\lambda)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. When $1\\leq s\\leq\\tau$ , from Lemma D.14, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\displaystyle\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right)=\\Omega\\left(d^{\\gamma-s(q+1)}+d^{q}\\right)}}\\\\ {{\\displaystyle{\\frac{\\mathcal{N}_{1}(\\lambda)\\mathcal{M}_{1,\\varphi}^{2}(\\lambda)}{n}=O\\left(\\lambda^{2(s-1)}d^{-\\gamma+q s}+\\lambda^{-1}d^{-\\gamma-(q+1)(s-1)}\\right)}}}\\\\ {{\\displaystyle{\\mathrm{V}_{1}(\\lambda)\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\sum_{j=1}^{\\infty}\\frac{(\\lambda)^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}=O\\left((\\ln(d))^{3}\\right)\\cdot O\\left(\\lambda d^{\\operatorname*{max}\\{q(2-s),0\\}}+\\lambda^{-1}d^{-s(q+1)}\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Denote ${\\bf I}=\\lambda^{2(s-1)}d^{-\\gamma+q s}$ \uff0c $\\mathbf{II}=\\lambda^{-1}d^{-\\gamma-(q+1)(s-1)}$ \uff0c $\\mathbf{III}=\\lambda d^{\\operatorname*{max}\\{q(2-s),0\\}}(\\ln(d))^{3}$ , and ${\\bf I V}=$ $\\lambda^{-1}d^{-s(q+1)}(\\ln(d))^{3}$ ", "page_idx": 37}, {"type": "text", "text": "For any $p\\geq0$ and any $s\\geq1$ ", "page_idx": 37}, {"type": "text", "text": "\u00b7 From Lemma D.18, we nave Iv \u300a \u03b1' \\*i +)   \n\u00b7 When $\\gamma\\geq1$ we have $\\gamma\\geq p+1$ , and hence $\\mathbf{II}\\ll\\mathbf{IV}\\ll d^{\\gamma-s(q+1)}$ ; when $\\gamma<1$ , we have $\\mathbf{II}\\ll d^{q}$ with $q=0$   \n\u00b7 When $p~\\geq~1$ or $\\gamma~\\in~(s,s~+~1)$ , since $-\\ell s\\,+\\,q s\\,\\,\\le\\,\\,0$ ,we have ${\\bf I}/d^{\\gamma-s(q+1)}\\;=\\;$ $O(d^{-2(\\gamma-\\ell-s/2)})\\ll1$ ; when $\\gamma\\in(0,s]$ we have ${\\bf I}=O(d^{-2s\\ell+2\\ell-\\gamma})=O(d^{-2s\\ell})\\ll d^{q}$ with $q=0$ ", "page_idx": 37}, {"type": "text", "text": "\u00b7 When $s\\geq2$ wehave $\\mathbf{III}\\ll d^{q}$ ; when $s<2$ and $p=0$ wehave $\\mathbf{III}\\ll d^{q}$ ; when $s<2$ and $p\\geq1$ and $q\\geq1$ , since $\\gamma-\\ell-s>\\operatorname*{min}\\{(s+1)q-\\ell,p s-1/2\\}>0$ we have $\\mathbf{III}/d^{\\gamma-s(q+1)}\\,=\\,d^{-(\\gamma-\\ell-s)-2(\\ell-q)}\\,\\ll\\,1$ or $\\mathbf{III}/d^{q}\\,\\ll\\,1$ ; when $s\\_2$ and $p\\geq1$ and $q=0$ wehave $\\mathbf{III}\\ll d^{q}$ ", "page_idx": 38}, {"type": "text", "text": "Combining all these, we get the desired results. ", "page_idx": 38}, {"type": "text", "text": "Lemma D.22. [Verification of bias conditions: saturation case] Suppose $\\tau\\,<\\,s\\,\\leq\\,2\\tau$ .Suppose $n\\asymp d^{\\gamma}$ for $\\gamma\\in[p(s+1),(p+1)(s+1))$ .For anygiven $\\ell\\geq0$ .if ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda\\geq\\left\\{d^{-\\ell}{\\frac{d^{-\\ell}}{{d^{-\\ell}}\\ln^{2}(d)}}\\quad\\gamma\\in[1,s+1),\\;\\ell\\leq{\\operatorname*{max}\\{\\ell_{1},\\ell_{2},\\ell_{3}\\}}\\right.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\tau,\\Delta,\\ell_{1},\\ell_{2}$ and $\\ell_{3}$ are given in Lemma $D.I6$ : then there exists a constant $\\epsilon>0$ only depending on s and $\\gamma,$ such that $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{{\\ensuremath{\\mathcal N}}_{1}(\\lambda){\\ensuremath{\\mathcal M}}_{1,\\varphi}^{2}(\\lambda)}{n^{2}}\\ll\\left({\\ensuremath{\\mathcal M}}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}{\\ensuremath{\\mathcal N}}_{2,\\varphi}(\\lambda)\\right),}\\\\ &{}&{\\displaystyle\\frac{{\\ensuremath{\\mathcal N}}_{1}(\\lambda)}{n}\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\sum_{j=1}^{\\infty}\\frac{\\lambda^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}\\ll\\left({\\ensuremath{\\mathcal M}}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}{\\ensuremath{\\mathcal N}}_{2,\\varphi}(\\lambda)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. When $\\tau<s\\leq2\\tau$ , from Lemma D.14, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\displaystyle{\\frac{\\sigma^{2}}{n}}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right)=\\Omega\\left(\\lambda^{2\\tau}d^{q(2\\tau-s)}+d^{\\gamma-s(q+1)}+d^{q}\\right)}}\\\\ {{\\displaystyle{\\frac{\\mathcal{N}_{1}(\\lambda)\\mathcal{M}_{1,\\varphi}^{2}(\\lambda)}{n}}=O\\left(\\lambda^{2(\\tau-1)}d^{-\\gamma+q(2\\tau-s)}+\\lambda^{-1}d^{-\\gamma-(q+1)(s-1)}\\right)}}\\\\ {{\\displaystyle{\\mathrm{V}_{1}(\\lambda)\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\sum_{j=1}^{\\infty}\\frac{(\\lambda)^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}=O\\left((\\ln(d))^{3}\\right)\\cdot O\\left(\\lambda d^{\\operatorname*{max}\\{q(2-s),0\\}}+\\lambda^{-1}d^{-s(q+1)}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Denote ${\\bf I}^{\\prime}=\\lambda^{2(\\tau-1)}d^{-\\gamma+q(2\\tau-s)}$ $\\mathbf{II}=\\lambda^{-1}d^{-\\gamma-(q+1)(s-1)}$ $\\mathbf{III}=\\lambda d^{\\operatorname*{max}\\{q(2-s),0\\}}(\\ln(d))^{3}$ ,.and $\\mathbf{IV}=\\lambda^{-1}d^{-s(q+1)}(\\ln(d))^{3}$ ", "page_idx": 38}, {"type": "text", "text": "For any $p\\geq0$ and any $1\\leq\\tau<s\\leq2\\tau$ ", "page_idx": 38}, {"type": "text", "text": "\u00b7 From Lemma D.18 and Lemma D.19, since $\\mathcal{N}_{1}(\\lambda)\\cdot n^{\\epsilon-1}\\to0$ , we have $\\mathbf{IV}\\ll d^{\\gamma-s(q+1)}$   \n\u00b7 When $\\gamma\\geq1$ , we have $\\gamma\\geq p+1$ , and hence $\\mathbf{II}\\ll\\mathbf{IV}\\ll d^{\\gamma-s(q+1)}$ ; when $\\gamma<1$ , we have $\\mathbf{II}\\ll d^{q}$ with $q=0$   \n\u00b7 When $p\\geq1$ , since $-\\ell\\tau+q\\tau\\leq0$ and $\\begin{array}{r l}&{\\quad\\gamma-\\ell-s/2}\\\\ &{\\geq\\operatorname*{max}\\left\\{\\displaystyle\\frac{s(2p-1)}{2},\\displaystyle\\frac{(2\\tau+1)(\\tau+p s)-(\\tau+1)s+p s-1}{2(\\tau+1},p s+\\frac{s(\\tau+1)}{2\\tau}-1\\right\\}}\\\\ &{>0,}\\end{array}$ we have $\\mathbf{I}^{\\prime}/d^{\\gamma-s(q+1)}\\ll1$ when $p=0$ , we have $\\mathbf{I}^{\\prime}=O(d^{-2\\tau\\ell+2\\ell-\\gamma})\\ll d^{q}$ with $q=0$   \n\u00b7 When $\\gamma-p-p s\\in[0,\\tau]\\cup[s+s/\\tau-1,s+1]$ we have $\\ell\\le\\operatorname*{max}\\{\\ell_{1},\\ell_{3}\\}$ . Similar to the proof in Lemma D.21, we can show that ${\\bf I I I}\\ll d^{\\gamma-s(q+1)}+d^{q}$   \n\u00b7 Finally, consider the case $\\gamma-p-p s\\in[\\tau,s+s/\\tau-1]$ . When $s\\geq2$ , we have $\\mathbf{III}\\ll d^{q}$ when $s<2$ since $s>1$ , we have $\\mathbf{III}/d^{q}=\\lambda d^{-q(s-1)}\\ll0$ ", "page_idx": 38}, {"type": "text", "text": "Combining all these, we get the desired results. ", "page_idx": 38}, {"type": "text", "text": "Lemma D.23 (Verification of bias conditions: misspecified case). Suppose $0\\textless s\\textless1$ .Suppose $n\\asymp d^{\\gamma}$ for $\\gamma\\in[p(s+1),(p+1)(s+1))$ . Suppose one of the following holds: ", "page_idx": 39}, {"type": "text", "text": "Suppose one of the following cases holds for $\\lambda=d^{-\\ell}$ or $\\lambda=d^{-\\ell}(\\ln(d))^{2}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau=\\infty,p(s+1)\\leq\\gamma\\leq p s+p+s,}\\\\ &{\\ell\\in[p,p+\\operatorname*{min}\\{1/2,\\gamma s\\}]}\\\\ &{\\tau=\\infty,p s+p+s<\\gamma<p s+p+s+1,}\\\\ &{\\ell\\in[p,\\operatorname*{min}\\{(\\gamma-(p+1)(s-1))/2,\\gamma(1+s)-s(p+1)\\}]}\\\\ &{\\tau<\\infty,p(s+1)\\leq\\gamma\\leq p s+p+s,}\\\\ &{\\ell=(\\gamma+2\\tau p-s p-p)/(2\\tau)}\\\\ &{\\tau<\\infty,p s+p+s<\\gamma<p s+p+s+1,}\\\\ &{\\ell=p+s/(2\\tau).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "then there exists a constant $\\epsilon>0$ only depending on s and $\\gamma$ such that $\\lambda=\\lambda(n,d)$ satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{\\mathcal{N}_{1}(\\lambda)}{n}\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\displaystyle\\sum_{j=1}^{\\infty}\\frac{\\lambda^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}\\ll\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right);}\\\\ &{}&{n^{-2}\\mathcal{N}_{1}(\\lambda)\\left(\\|f_{\\lambda}\\|_{L^{\\infty}}+n^{\\frac{1-s}{2}+\\epsilon}\\right)^{2}=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. When $0<s<1$ , from Lemma D.14, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\displaystyle\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right)=\\Omega\\left(d^{\\gamma-s(p+1)}+d^{p}\\right)}}\\\\ {{n^{-1}\\mathcal{N}_{1}(\\lambda)n^{1-s}=O\\left(\\lambda^{-1}d^{-\\gamma s}\\right)}}\\\\ {{\\mathrm{V}_{1}(\\lambda)\\ln(n)(\\ln\\lambda^{-1})^{2}\\cdot\\displaystyle\\sum_{j=1}^{\\infty}\\displaystyle\\frac{(\\lambda)^{2}\\lambda_{i}\\varphi_{\\lambda}^{2}(\\lambda_{i})}{\\lambda+\\lambda_{i}}f_{i}^{2}=O\\left((\\ln(d))^{3}\\right)\\cdot O\\left(\\lambda d^{\\operatorname*{max}\\{p(2-s),0\\}}+\\lambda^{-1}d^{-s(p+1)}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and the convergence rate of $\\|f_{\\lambda}\\|_{L^{\\infty}}$ can be attained similar to Lemma 25 in Zhang et al. (2024). Since $\\tau\\geq1$ , similar to the proof of Theorem 3 of Zhang et al. (2024), when $1/2<s<1$ wehave ", "page_idx": 39}, {"type": "equation", "text": "$$\nn^{-2}\\mathcal{N}_{1}(\\lambda)\\left(\\|f_{\\lambda}\\|_{L^{\\infty}}+n^{\\frac{1-s}{2}+\\epsilon}\\right)^{2}=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and when $s\\leq1/2$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nn^{-2}\\mathcal{N}_{1}(\\lambda)\\left\\|f_{\\lambda}\\right\\|_{L^{\\infty}}^{2}=o\\left(\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Denote ${\\bf I}=\\lambda^{-1}d^{-\\gamma s}$ $\\mathbf{II}=\\lambda d^{p(2-s)}(\\ln(d))^{3}$ ,and ${\\bf I I I}=\\lambda^{-1}d^{-s(p+1)}(\\ln(d))^{3}.$ ", "page_idx": 39}, {"type": "text", "text": "For any $p\\geq0$ and any $0<s<1$ ", "page_idx": 39}, {"type": "text", "text": "\u00b7 From Lemma D.20, we have ${\\bf I I I}\\ll d^{\\gamma-s(p+1)}$ \uff0c", "page_idx": 39}, {"type": "text", "text": "\u00b7 When $\\gamma\\;\\leq\\;p s\\,+\\,p\\,+\\,s.$ wecanshow ${\\textbf{I}}\\ll\\ d^{p}$ when: (1) $p~\\geq~1$ ,or (2) $p~=~0$ and $s>1/(2\\tau)>0$ , or (3) $\\tau=\\infty$ \uff0c ", "page_idx": 39}, {"type": "text", "text": "\u00b7 When $\\gamma>p s+p+s$ , we can show ${\\mathbf I}\\ll d^{\\gamma-s(p+1)}$ holds if and only if $\\tau=\\infty$ or ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma>\\frac{(2\\tau+1)s+2\\tau(1+s)p}{2\\tau(1+s)},\\quad\\tau=\\tau<\\infty;\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and the above inequality holds when (1) $p\\,>\\,0$ or (2) $p\\,=\\,0,s\\,>\\,1/(2\\tau)\\,>\\,0$ ,or (3) $p=0,\\gamma>((2\\tau+\\mathbf{\\bar{1}})s)/(2\\tau(1+s))$ ", "page_idx": 40}, {"type": "text", "text": "\u00b7 When $\\gamma\\le p s+p+s$ since $\\ell\\geq p>p-p s$ , we have ${\\bf I I}\\ll d^{p}$ \u00b7 When $\\gamma>p s+p+s$ , since $\\ell\\geq p>p-p s$ , we have $\\mathbf{II}\\ll d^{\\gamma-s(p+1)}$ ", "page_idx": 40}, {"type": "text", "text": "Combining all these, we get the desired results. ", "page_idx": 40}, {"type": "text", "text": "D.5 Final proof of Theorem 4.1 and Theorem 4.2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "For each case, the proof can be done in the following steps: ", "page_idx": 40}, {"type": "text", "text": "(i) When $\\lambda\\geq\\lambda^{\\star}$ and $s\\leq2\\tau$ , where the definition of the balanced parameter $\\lambda^{\\star}$ can be found in Corollary D.15 and Corollary D.16, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})=\\Theta_{\\mathbb{P}}\\left(d^{-\\beta^{\\star}}\\right)\\cdot\\mathrm{poly}\\left(\\ln(d)\\right)}\\\\ &{}&{\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)=\\Theta_{\\mathbb{P}}\\left(d^{-\\beta}\\right)\\cdot\\mathrm{poly}\\left(\\ln(d)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Where $d^{-\\beta^{\\star}}$ is the desired convergence rate given in Theorem 4.1 or Theorem 4.2 and $\\beta\\leq\\beta^{*}$ . Similarly, when $s>2\\tau$ , by taking $s=2\\tau$ in Corollary D.16, we also have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{2,\\varphi}(\\lambda^{\\star})+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda^{\\star})=\\Theta_{\\mathbb{P}}\\left(d^{-\\beta^{\\star}}\\right)\\cdot\\mathrm{poly}\\left(\\ln(d)\\right)}\\\\ {\\mathcal{M}_{2,\\varphi}(\\lambda)+\\frac{\\sigma^{2}}{n}\\mathcal{N}_{2,\\varphi}(\\lambda)=\\Theta_{\\mathbb{P}}\\left(d^{-\\beta}\\right)\\cdot\\mathrm{poly}\\left(\\ln(d)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "(ii)When $\\lambda\\geq\\lambda^{\\star}$ , from Lemma D.14, Lemma D.18, Lemma D.19, Lemma D.20, Lemma D.21, Lemma D.22, and Lemma D.23, we know that conditions in Theorem D.5, Theorem D.9, and Theorem D.10 are satisfied. Therefore, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(\\left\\|\\hat{f}_{\\lambda^{\\star}}-f_{\\star}\\right\\|_{L^{2}}^{2}\\;\\Big|\\;X\\right)=\\Theta_{\\mathbb{P}}\\left(d^{-\\beta^{\\star}}\\right)\\cdot\\mathrm{poly}\\left(\\ln(d)\\right)}\\\\ {\\mathbb{E}\\left(\\left\\|\\hat{f}_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}^{2}\\;\\Big|\\;X\\right)=\\Theta_{\\mathbb{P}}\\left(d^{-\\beta}\\right)\\cdot\\mathrm{poly}\\left(\\ln(d)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "(ii) Finally, when $s>\\tau$ , we can further show that: the convergence rates of the generalization error can not be faster than above for any choice of regularization parameter $\\lambda=\\lambda(d,n)\\rightarrow$ 0. Notice that, when $s\\geq1$ , for any $\\lambda<\\lambda^{\\star}$ , from the monotonicity of $\\mathbf{Var}(\\lambda)$ (see, e.g., Li et al. (2024); Zhang et al. (2024)), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\lVert\\hat{f}_{\\lambda}-f_{\\star}\\right\\rVert_{L^{2}}^{2}\\;\\middle|\\;X\\right]\\geq\\mathbf{V}\\mathbf{ar}(\\lambda)\\geq\\mathbf{V}\\mathbf{ar}(\\lambda^{\\star})\\asymp\\mathbb{E}\\left[\\left\\lVert\\hat{f}_{\\lambda^{\\star}}-f_{\\star}\\right\\rVert_{L^{2}}^{2}\\;\\middle|\\;X\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and hence ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\left\\|{\\hat{f}}_{\\lambda}-f_{\\star}\\right\\|_{L^{2}}^{2}\\;{\\Big|}\\;X\\right)=\\Omega_{\\mathbb{P}}\\left(d^{-\\beta^{\\star}}\\right)\\cdot\\mathrm{poly}\\left(\\ln(d)\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "E Auxiliary lemmas ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proposition E.1. For any analytic filter function $\\varphi_{\\lambda}$ we have $(z\\!+\\!\\lambda)\\varphi_{\\lambda}(z)\\leq4$ and $(z\\!+\\!\\lambda)\\psi_{\\lambda}(z)\\leq$ $4\\lambda$ ", "page_idx": 40}, {"type": "text", "text": "Proof. From (28), we have $(z+\\lambda)\\varphi_{\\lambda}(z)\\leq2\\operatorname*{max}\\{z,\\lambda\\}\\varphi_{\\lambda}(z)\\leq2\\operatorname*{max}\\{1,\\mathfrak{C}_{4}\\}\\leq4$ From (27), wehave $(z+\\lambda)\\psi_{\\lambda}(z)\\leq2\\operatorname*{max}\\{z,\\lambda\\}\\psi_{\\lambda}(z)\\leq2\\operatorname*{max}\\{\\mathfrak{C}_{2},1\\}\\lambda\\leq4\\lambda.$ ", "page_idx": 40}, {"type": "text", "text": "Lemma E.2. Let $\\varphi_{\\lambda}$ be an analytic flter function defined in Definition C.1. Then, for any $s\\in[0,1]$ wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z\\in[0,\\kappa^{2}]}\\varphi_{\\lambda}(z)z^{s}\\leq4\\lambda^{s-1}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. For any $z\\,\\in\\,[0,\\kappa^{2}]$ , from Proposition E.1, we have $(z+\\lambda)\\varphi_{\\lambda}(z)\\leq4$ Therefore, from Proposition B.3 in Li et al. (2024), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\varphi_{\\lambda}(z)z^{s}\\leq\\frac{4z^{s}}{z+\\lambda}\\leq4\\lambda^{s-1}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma E.3. Let $\\psi_{\\lambda}$ be defined in Definition C.1. Then, for any $s>2\\tau$ wehave ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z\\in[0,\\kappa^{2}]}z^{s}\\psi_{\\lambda}^{2}(z)\\leq\\mathfrak{C_{2}^{2}}\\kappa^{2(s-2\\tau)}\\lambda^{2\\tau}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. For any $z$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\psi_{\\lambda}(z)\\leq\\mathfrak{C}_{2}(z/\\lambda)^{-\\tau}\\mathbf{1}\\{z>\\lambda\\}+\\mathbf{1}\\{z\\leq\\lambda\\}\\leq\\mathfrak{C}_{2}(z/\\lambda)^{-\\tau},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "hence ", "page_idx": 41}, {"type": "equation", "text": "$$\nz^{s}\\psi_{\\lambda}^{2}(z)\\leq\\mathfrak{C}_{2}^{2}z^{s}z^{-2\\tau}\\lambda^{2\\tau}\\leq\\mathfrak{C}_{2}^{2}\\kappa^{2(s-2\\tau)}\\lambda^{2\\tau}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "E.1 Analytic functional calculus ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "The \u201canalytic functional argument' introduced in Li et al. (2024) is vital in our proof for Theorem 4.1. For readers\u2019 convenience, we collect some of the main ingredients here, see Li et al. (2024) for details. ", "page_idx": 41}, {"type": "text", "text": "Definition E.4. Let $A$ be a linear operator on a Banach space $X$ . The resolvent set $\\rho(A)$ is given by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\rho(A):=\\left\\{\\lambda\\in\\mathbb{C}\\mid A-\\lambda{\\mathrm{~is~invertible}}\\right\\},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and we denote $R_{A}(\\lambda):=(A-\\lambda)^{-1}$ . The spectrum of $A$ is defined by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sigma(A):=\\mathbb{C}\\backslash\\rho(A).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "A simple but key ingredient in the analytic functional calculus is the folowing resolvent identity: ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{A}(\\lambda)-R_{B}(\\lambda)=R_{A}(\\lambda)(B-A)R_{B}(\\lambda)=R_{B}(\\lambda)(B-A)R_{A}(\\lambda).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The resolvent allows us to define the value of $f(A)$ in analog to the form of Cauchy integral formula, where $A$ is an operator and $f$ is an analytic function. The following two propositions are well-known results on operator calculus. ", "page_idx": 41}, {"type": "text", "text": "Proposition E.5 (analytic functional calculus). Let $A$ be an operator on a Hilbert space $H$ and $f$ be an analytic function defined on $D_{f}\\subset\\mathbb{C}$ Let $\\Gamma$ be a contour contained in $D_{f}$ surrounding $\\sigma(A)$ Then, ", "page_idx": 41}, {"type": "equation", "text": "$$\nf(A)=\\frac{1}{2\\pi i}\\oint_{\\Gamma}f(z)(z-A)^{-1}\\mathrm{d}z=-\\frac{1}{2\\pi i}\\oint_{\\Gamma}f(z)R_{A}(z)\\mathrm{d}z,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and it is independent of the choice of $\\Gamma$ ", "page_idx": 41}, {"type": "text", "text": "Now, let $\\Gamma$ be a contour contained in $D_{f}$ surrounding both $\\sigma(A)$ and $\\sigma(B)$ . Using (108), we get ", "page_idx": 41}, {"type": "equation", "text": "$$\nf(A)-f(B)=-\\frac{1}{2\\pi i}\\oint_{\\Gamma}f(z)\\left[R_{A}(z)-R_{B}(z)\\right]\\mathrm{d}z=\\frac{1}{2\\pi i}\\oint_{\\Gamma}R_{B}(z)(A-B)R_{A}(z)f(z)\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proposition E.6 (Spectral mapping theorem). Let $A$ be a bounded self-adjoint operator and $f$ be $a$ continuousfunction on $\\sigma(A)$ .Then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sigma(f(A))=\\left\\{f(\\lambda)~|~\\lambda\\in\\sigma(A)\\right\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Consequently, $\\begin{array}{r}{\\|f(A)\\|=\\operatorname*{sup}_{\\lambda\\in\\sigma(A)}|f(\\lambda)|\\leq\\|f\\|_{\\infty}.}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Let us define the contour $\\Gamma_{\\lambda}$ considered in Li et al. (2024) by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Gamma_{\\lambda}=\\Gamma_{\\lambda,1}\\cup\\Gamma_{\\lambda,2}\\cup\\Gamma_{\\lambda,3}}\\\\ &{\\Gamma_{\\lambda,1}=\\{x\\pm(x+\\eta)i\\in\\mathbb{C}\\mid x\\in[-\\eta,0]\\}}\\\\ &{\\Gamma_{\\lambda,2}=\\{x\\pm(x+\\eta)i\\in\\mathbb{C}\\mid x\\in(0,\\kappa^{2})\\}}\\\\ &{\\Gamma_{\\lambda,3}=\\left\\{z\\in\\mathbb{C}\\mid\\left\\vert z-\\kappa^{2}\\right\\vert=\\kappa^{2}+\\eta,\\;\\mathrm{Re}(z)\\geq\\kappa^{2}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\eta=\\lambda/2$ . Then, since $T$ and $T_{X}$ are positive self-adjoint operators with $\\|T\\|,\\|T_{X}\\|\\leq\\kappa^{2}$ \uff0c we have $\\sigma(T),\\sigma(T_{X})\\,\\subset\\,[0,\\kappa^{2}]$ . Therefore, $\\Gamma_{\\lambda}$ is indeed a contour satisfying the requirement in Proposition E.5. ", "page_idx": 42}, {"type": "text", "text": "Proposition E.7. Suppose that (45\uff09 in Assumption 3 holds. Suppose that $\\lambda\\,=\\,\\lambda(n,d)$ satisfies M In n = o(1). Then for any fxed 8 E (0, 1), when n is suficiently large, with probability at least $1-\\delta$ wehave ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\lambda}^{-\\frac{1}{2}}(T-T_{X})T_{\\lambda}^{-\\frac{1}{2}}\\|\\le\\sqrt{v}.}\\\\ &{\\qquad\\Big\\|T_{\\lambda}^{-\\frac{1}{2}}T_{X\\lambda}^{\\frac{1}{2}}\\Big\\|^{2}\\le2}\\\\ &{\\qquad\\Big\\|T_{\\lambda}^{\\frac{1}{2}}T_{X\\lambda}^{-\\frac{1}{2}}\\Big\\|^{2}\\le3.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. These inequalities are direct results of (56), (58), and (59) in Zhang et al. (2024). ", "page_idx": 42}, {"type": "text", "text": "Proposition E.8 (Restate Proposition 4.13 in Li et al. (2024) with only the constant modified). When (113) holds, there is an absolute constant that for any $z\\in\\Gamma_{\\lambda;}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\lambda}^{\\frac{1}{2}}(T-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\|\\le C}\\\\ &{\\|T_{\\lambda}^{\\frac{1}{2}}(T_{X}-z)^{-1}T_{\\lambda}^{\\frac{1}{2}}\\|\\le\\sqrt{6}C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings in Theorem 3.3 and show that the gradient fow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor) in Theorem 3.1. We further determine the exact convergence rates of a large class of (optimal tuned) spectral algorithms with different qualification $\\tau$ 's,and provide a discussion on new phenomena we find in Section 4. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We explain the reason for considering spherical data in Remark 2.1. We point out in the Conclusion section that our work only considers the optimal-tuned spectral algorithms. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We list all assumptions we need in the statement of our main theorems. We provide a complete (and correct) proof in the Appendix. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPs does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not include experiments ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]