[{"figure_path": "CFez7MFUFd/figures/figures_18_1.jpg", "caption": "Figure 1: Visual results on the dataset of Lai et al. [26].", "description": "This figure displays visual comparisons of different deblurring methods on a subset of images from the Lai et al. dataset [26].  The dataset is known for having severely blurred images, thus making it a challenging benchmark. The first column shows the blurry input images. Subsequent columns demonstrate the deblurred outputs from various traditional and deep learning-based methods.  This provides a qualitative assessment of each method's performance in terms of sharpness, artifact reduction, and overall image quality, compared to the ground truth images in the last column.", "section": "F.1 Visual comparisons on Lai et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_19_1.jpg", "caption": "Figure 2: Visual results on the dataset of Lai et al. [26]", "description": "This figure shows visual comparisons of different deblurring methods on a specific image from the Lai et al. dataset [26]. The results demonstrate that the proposed method produces sharper and more detailed images compared to existing methods, particularly in areas with fine details or complex blurring patterns.", "section": "F.1 Visual comparisons on Lai et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_20_1.jpg", "caption": "Figure 3: Visual results on the dataset of Lai et al. [26]", "description": "This figure shows a visual comparison of the results from different methods on a subset of Lai et al.'s dataset [26]. The results demonstrate the effectiveness of the proposed method in handling severe blurring effects, with sharper details and fewer artifacts compared to existing methods. In contrast, supervised learning methods trained on external datasets yield poor quality results, highlighting the limited generalization performance of supervised approaches when dealing with complex real-world blurring.", "section": "F.1 Visual comparisons on Lai et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_20_2.jpg", "caption": "Figure 10: Visual comparison with supervised methods on challenging cases from RealBlur-J [48].", "description": "This figure compares the deblurring results of the proposed self-supervised method with several state-of-the-art supervised methods on challenging examples from the RealBlur-J dataset.  Despite the proposed method having slightly lower quantitative scores (PSNR and SSIM), the visual results show that it produces sharper images with fewer artifacts compared to the supervised approaches. This highlights the ability of the self-supervised method to generate visually pleasing results even when the quantitative metrics do not show a significant advantage.", "section": "F.4 Visual comparisons on RealBlur dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_21_1.jpg", "caption": "Figure 1: Visual results on the dataset of Lai et al. [26].", "description": "This figure shows a visual comparison of the results from different methods on a selection of images from Lai et al.'s dataset. The top row shows the blurry input image and the results from several traditional methods. The bottom row shows the results from several deep-learning methods (including the proposed method) and ground truth.  It highlights the superior performance of the proposed method in producing sharp, artifact-free results that closely match the ground truth, particularly when compared to existing deep-learning methods. ", "section": "F.1 Visual comparisons on Lai et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_21_2.jpg", "caption": "Figure 10: Visual comparison with supervised methods on challenging cases from RealBlur-J [48].", "description": "This figure compares the visual results of the proposed self-supervised method against several state-of-the-art supervised learning methods on challenging cases from the RealBlur-J dataset.  Despite achieving slightly lower average PSNR and SSIM values overall, the self-supervised method produces images with sharper details and fewer artifacts than the supervised methods in many cases, indicating that the visual quality is better than what the quantitative metrics alone might suggest.", "section": "F.4 Visual comparisons on RealBlur dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_22_1.jpg", "caption": "Figure 1: Visual results on the dataset of Lai et al. [26].", "description": "This figure displays visual comparisons of deblurring results from various methods on a sample image from the Lai et al. dataset [26].  It showcases the blurry input image alongside results from several traditional and deep learning-based deblurring methods including Cho & Lee [12], Xu & Jia [65], Xu et al. [67], Michaeli & Irani [38], Perrone & Favaro [44], Pan-DCP [42], Kaufman & Fattal [20], MPRNet [73], MIMO-UNet [11], Restormer [72], SelfDeblur [47], MCEM [33], VDIP [18], and the authors' proposed method. The ground truth (GT) image is also included for reference. The figure aims to visually demonstrate the superior performance of the authors' proposed approach in producing sharper, artifact-free deblurred images compared to existing methods, especially in handling complex real-world blurring.", "section": "F.1 Visual comparisons on Lai et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_22_2.jpg", "caption": "Figure 7: Visual comparison on the dataset of K\u00f6hler et al. [22]", "description": "This figure compares the deblurring results of different methods on the K\u00f6hler et al. dataset, which is known for its non-uniform blurring. It demonstrates that the proposed method effectively handles this challenging scenario, generating high-quality images with sharp details and fewer artifacts compared to existing methods.  The comparison highlights the limitations of other methods in dealing with complex real-world blurring scenarios.", "section": "F.2 Visual comparisons on K\u00f6hler et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_23_1.jpg", "caption": "Figure 1: Visual results on the dataset of Lai et al. [26].", "description": "This figure shows a visual comparison of different image deblurring methods applied to a sample image from the Lai et al. [26] dataset, which is known for its challenging blurring effects. The top row displays the blurry input image followed by results from several existing methods including Cho et al. [12], Xu & Jia [65], Xu et al. [67], Michaeli & Irani [38], Perrone & Favaro [44], Pan-DCP [42], Kaufman & Fattal [20], MPRNet [73], MIMO-UNet [11], Restormer [72], and SelfDeblur [47]. The bottom row shows results from MCEM [33], VDIP [18], the proposed method, and the ground truth image.", "section": "Visual comparisons on Lai et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_23_2.jpg", "caption": "Figure 10: Visual comparison with supervised methods on challenging cases from RealBlur-J [48].", "description": "This figure shows a visual comparison of the results from state-of-the-art supervised learning methods and the proposed self-supervised method on challenging cases from the RealBlur-J dataset.  The results demonstrate that while the supervised methods achieve slightly higher average PSNR and SSIM values, the self-supervised approach produces results that are often sharper and visually superior, suggesting that the gap between the methods is smaller in terms of visual quality than indicated by quantitative metrics alone.", "section": "F.4 Visual comparisons on RealBlur dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_24_1.jpg", "caption": "Figure 11: Visual comparison on the microscopic deconvolution. All images are originally grayscale but a different colormap is used to better highlight the differences among the various reconstructions.", "description": "This figure shows a visual comparison of the results obtained by various methods on the microscopic deconvolution task. The goal is to highlight the differences in image restoration quality among the compared methods (INIKNet, SelfDeblur, BlindDPS, MCEM, VDIP, and the proposed method).  Each row represents a different microscopic image and its reconstructions. The original blurry images are shown in the first column; followed by restored images from each method, and finally the ground truth (GT) image is displayed in the last column.  A colormap is used to enhance the visual comparison and differences in textures and details.", "section": "F.5 Visual comparisons on Microscopic dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_24_2.jpg", "caption": "Figure 12: Visual comparison on the ablation study.", "description": "This figure in the ablation study section presents a visual comparison of the results obtained using different configurations of the proposed method. It shows the impact of removing the cross-scale consistency loss, using only single-scale training, removing the progressive training scheme, and using different architectures for the image and kernel generators (INR/CNN, MLP/INR, MLP/CNN).  A comparison with the ground truth image is also provided to evaluate the efficacy of the proposed approach.", "section": "Ablation study"}, {"figure_path": "CFez7MFUFd/figures/figures_25_1.jpg", "caption": "Figure 13: Intermediate results of estimated blur kernel and latent image at end of each stage.", "description": "This figure shows intermediate results of the proposed method at different stages of the progressive coarse-to-fine training process. It visualizes how the estimations of both the blur kernel and latent image improve progressively across scales, from coarser resolutions to finer details, eventually generating a high-quality deblurred image. This process demonstrates the effectiveness of the proposed progressive training scheme, illustrating the model's ability to refine its estimates over multiple scales and converge to accurate reconstructions.", "section": "3 Methodology"}, {"figure_path": "CFez7MFUFd/figures/figures_26_1.jpg", "caption": "Figure 1: Visual results on the dataset of Lai et al. [26]", "description": "This figure shows a comparison of the results from different methods on some examples from Lai et al.'s dataset [26]. The dataset is known for its severe blurring effects. The figure demonstrates that the proposed method consistently produces results with sharper details and fewer artifacts compared to existing methods. In contrast, supervised learning methods trained on external datasets yield poor quality results, highlighting the limited generalization performance of supervised approaches when dealing with complex real-world blurring.", "section": "F.1 Visual comparisons on Lai et al.'s dataset"}, {"figure_path": "CFez7MFUFd/figures/figures_27_1.jpg", "caption": "Figure 1: Visual results on the dataset of Lai et al. [26]", "description": "This figure displays the visual comparison of the results from various methods on the dataset of Lai et al. [26]. The dataset is known for its challenging blurring effects and includes images categorized into five groups: manmade, natural, people, saturated, and text. The results showcase the effectiveness of the proposed method in addressing these blurring challenges compared to other methods, both supervised and self-supervised.", "section": "F.1 Visual comparisons on Lai et al.'s dataset"}]