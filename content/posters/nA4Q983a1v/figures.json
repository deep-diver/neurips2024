[{"figure_path": "nA4Q983a1v/figures/figures_1_1.jpg", "caption": "Figure 1: We visualize the Segment-Based Batching approach often used in prior literature. A worker collects a rollout of episodes, denoted by color. Each episode is split and zero-padded to produce a batch of segments, each with a constant, user-specified segment length L. Episodes exceeding the specified length are broken into multiple segments, preventing backpropagation through time from reaching earlier segments. Segments contain zero padding, reducing efficiency, biasing normalization methods, and necessitating padding-aware recurrent loss functions.", "description": "This figure illustrates the Segment-Based Batching (SBB) method commonly used in reinforcement learning.  A worker collects multiple episodes of data, each represented by a different color.  Because recurrent neural networks (RNNs) and Transformers have difficulty handling variable-length sequences, the episodes are split into fixed-length segments (length L).  Shorter episodes are padded with zeros to match the length L. This padding introduces inefficiencies (wasted computation, biased normalization), and theoretical problems (inability to backpropagate through the entire episode due to the padding).", "section": "1 Introduction"}, {"figure_path": "nA4Q983a1v/figures/figures_5_1.jpg", "caption": "Figure 2: A visualization of sampling in TBB, with a batch size of B = 4. Transitions from rollouts are stored in-order in D, with each color denoting a separate episodes. Associated episode begin indices are stored in I. We sample a train batch by randomly selecting from 1. For example, we might sample 4 from I, corresponding to E\u2081 in red. Next, we sample 7 from I, corresponding to E2 in red. We concatenate B = concat(E1, E2) and return the result as a train batch.", "description": "This figure illustrates the Tape-Based Batching (TBB) method for sampling training data.  Transitions from multiple episodes are concatenated into a single tape (D).  Episode boundaries are marked in a separate index array (I).  The method randomly selects B episode segments to form a training batch, concatenating consecutive transitions from the selected episodes. This avoids zero-padding and allows for efficient handling of variable-length episodes.", "section": "4.1 Tape-Based Batching"}, {"figure_path": "nA4Q983a1v/figures/figures_6_1.jpg", "caption": "Figure 1: We visualize the Segment-Based Batching approach often used in prior literature. A worker collects a rollout of episodes, denoted by color. Each episode is split and zero-padded to produce a batch of segments, each with a constant, user-specified segment length L. Episodes exceeding the specified length are broken into multiple segments, preventing backpropagation through time from reaching earlier segments. Segments contain zero padding, reducing efficiency, biasing normalization methods, and necessitating padding-aware recurrent loss functions.", "description": "This figure illustrates the Segment-Based Batching (SBB) method commonly used in recurrent reinforcement learning.  It shows how a worker collects multiple episodes of data, where each episode is a sequence of transitions.  To handle variable episode lengths, the episodes are split into fixed-length segments and padded with zeros. This padding leads to several inefficiencies, such as reduced computational efficiency and biased normalization.", "section": "1 Introduction"}, {"figure_path": "nA4Q983a1v/figures/figures_6_2.jpg", "caption": "Figure 1: We visualize the Segment-Based Batching approach often used in prior literature. A worker collects a rollout of episodes, denoted by color. Each episode is split and zero-padded to produce a batch of segments, each with a constant, user-specified segment length L. Episodes exceeding the specified length are broken into multiple segments, preventing backpropagation through time from reaching earlier segments. Segments contain zero padding, reducing efficiency, biasing normalization methods, and necessitating padding-aware recurrent loss functions.", "description": "This figure illustrates the Segment-Based Batching (SBB) method commonly used in recurrent reinforcement learning.  It shows how a worker collects episodes, which are then split into fixed-length segments and zero-padded.  This process leads to several inefficiencies, including wasted computation on padding, biased normalization due to padding, and a limitation of backpropagation through time.  The figure highlights the problems with using fixed-length segments and motivates the need for the proposed Tape-Based Batching method which removes the need for segmentation.", "section": "1 Introduction"}, {"figure_path": "nA4Q983a1v/figures/figures_6_3.jpg", "caption": "Figure 1: We visualize the Segment-Based Batching approach often used in prior literature. A worker collects a rollout of episodes, denoted by color. Each episode is split and zero-padded to produce a batch of segments, each with a constant, user-specified segment length L. Episodes exceeding the specified length are broken into multiple segments, preventing backpropagation through time from reaching earlier segments. Segments contain zero padding, reducing efficiency, biasing normalization methods, and necessitating padding-aware recurrent loss functions.", "description": "This figure illustrates the Segment-Based Batching (SBB) method commonly used in recurrent reinforcement learning.  It shows how a rollout (a sequence of episodes) is processed.  Each episode is broken into fixed-length segments, and any remaining space in a segment is filled with zeros (zero-padding).  The zero-padding and segmentation introduce issues, such as reduced efficiency, biased normalization, and problems with backpropagation.  The image displays episodes, denoted by color, which are partitioned into segments, highlighting their fixed size, zero-padding, and batching dimension.", "section": "1 Introduction"}, {"figure_path": "nA4Q983a1v/figures/figures_7_1.jpg", "caption": "Figure 3: We demonstrate that SBB can hurt Q learning through truncated BPTT. We examine the Repeat Previous task, with RML = 10, comparing SBB (left) to TBB (right). For SBB, we set L = RML = 10 to capture all necessary information. After training, we plot the cumulative partial derivative with respect to the observations on the y-axis. This partial derivative determines the VML \u2013 how much each prior observation contributes to the Q value. We draw a vertical red line at L = RML = 10. We see that across models, a majority of the Q value is not learnable when using SBB. Even when we set L = \u221e using TBB, we see that the VML still spans far beyond the RML. This surprising finding shows that truncated BPTT degrades recurrent value estimators.", "description": "This figure compares the effects of Segment-Based Batching (SBB) and Tape-Based Batching (TBB) on learning in a recurrent Q-learning setting using the Repeat Previous task.  The y-axis shows the cumulative partial derivative of the Q-value with respect to past observations (Value Memory Length or VML), while the x-axis represents the age of the observation. The red dashed line indicates the Reward Memory Length (RML), which represents the length of the past observations necessary to predict the reward. The left panel shows the results for SBB and the right for TBB.  The figure demonstrates that with SBB, much of the Q-value is not learned due to truncated backpropagation through time, while with TBB, the VML extends beyond the RML, indicating the need for a more complete backpropagation approach.", "section": "5 Experiments and Discussion"}, {"figure_path": "nA4Q983a1v/figures/figures_8_1.jpg", "caption": "Figure 4: We compare TBB (ours) to SBB across POPGym tasks and memory models, reporting the mean and 95% bootstrapped confidence interval of the evaluation return over ten seeds. We find that TBB significantly improves sample efficiency. See Appendix A for more experiments.", "description": "This figure compares the performance of Tape-Based Batching (TBB) and Segment-Based Batching (SBB) across various tasks and memory models in the POPGym benchmark.  The y-axis represents the evaluation return, and the x-axis represents the training epoch.  Error bars (95% bootstrapped confidence intervals) show the variability in performance across multiple runs (10 seeds).  The results demonstrate a significant improvement in sample efficiency using TBB compared to SBB, regardless of the segment length used in SBB.  Appendix A contains further experimental results.", "section": "5 Experiments and Discussion"}, {"figure_path": "nA4Q983a1v/figures/figures_8_2.jpg", "caption": "Figure 5: (Left) We compare how long it takes to compute the discounted return using our memoroid, compared to the standard way of iterating through a batch. Computing the discounted return is orders of magnitude faster when using our memoroid implementation. (Right) we compare the total time to train a policy on Repeat First. For both experiments, we evaluate ten random seeds on a RTX 2080Ti GPU.", "description": "The left panel shows that using memoroids to compute the discounted return is significantly faster than the traditional method. The right panel compares the total training time for TBB and SBB on the Repeat First task, showing that there is no significant difference in training time despite the logarithmic difference in computational complexity.", "section": "Wall-Clock Efficiency"}, {"figure_path": "nA4Q983a1v/figures/figures_13_1.jpg", "caption": "Figure 1: We visualize the Segment-Based Batching approach often used in prior literature. A worker collects a rollout of episodes, denoted by color. Each episode is split and zero-padded to produce a batch of segments, each with a constant, user-specified segment length L. Episodes exceeding the specified length are broken into multiple segments, preventing backpropagation through time from reaching earlier segments. Segments contain zero padding, reducing efficiency, biasing normalization methods, and necessitating padding-aware recurrent loss functions.", "description": "This figure illustrates the common segment-based batching method in recurrent reinforcement learning.  Rollouts (sequences of interactions) are divided into fixed-length segments, zero-padded to ensure consistent length, and processed as batches.  This method introduces inefficiencies due to the zero-padding and prevents full backpropagation through the entire sequence, affecting training efficiency and potentially biasing the results.", "section": "Introduction"}, {"figure_path": "nA4Q983a1v/figures/figures_14_1.jpg", "caption": "Figure 7: We examine three memoroids on Atari environments from the Arcade Learning Environment (ALE) Bellemare et al. (2013), plotting the mean and 95% confidence interval over three random seeds. In all environments, we see that TBB outperforms SBB.", "description": "This figure compares the performance of Tape-Based Batching (TBB) against Segment-Based Batching (SBB) across three different Atari games from the Arcade Learning Environment using three different memoroid models. The results show that TBB consistently outperforms SBB across all games and models tested, demonstrating the efficacy of TBB in improving sample efficiency. The plot displays the mean and 95% confidence intervals of the cumulative return obtained over three runs with different random seeds for both TBB and SBB.", "section": "5 Experiments and Discussion"}, {"figure_path": "nA4Q983a1v/figures/figures_14_2.jpg", "caption": "Figure 3: We demonstrate that SBB can hurt Q learning through truncated BPTT. We examine the Repeat Previous task, with RML = 10, comparing SBB (left) to TBB (right). For SBB, we set L = RML = 10 to capture all necessary information. After training, we plot the cumulative partial derivative with respect to the observations on the y-axis. This partial derivative determines the VML \u2013 how much each prior observation contributes to the Q value. We draw a vertical red line at L = RML = 10. We see that across models, a majority of the Q value is not learnable when using SBB. Even when we set L = \u221e using TBB, we see that the VML still spans far beyond the RML. This surprising finding shows that truncated BPTT degrades recurrent value estimators.", "description": "This figure compares the impact of Segment-Based Batching (SBB) and Tape-Based Batching (TBB) on the ability of recurrent models to learn long-term dependencies in a reinforcement learning task. It plots the cumulative partial derivative of the Q-value with respect to past observations, showing that SBB severely limits the model's ability to learn from observations beyond a certain range (L=10), while TBB allows the model to learn longer-term dependencies. This demonstrates that the truncated backpropagation through time inherent in SBB degrades the accuracy and effectiveness of recurrent value estimation.", "section": "5 Experiments and Discussion"}]