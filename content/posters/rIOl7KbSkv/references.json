{"references": [{"fullname_first_author": "John Kirchenbauer", "paper_title": "A watermark for large language models", "publication_date": "2023-07-23", "reason": "This paper proposes a watermarking scheme for LLMs, which is a central topic of the current paper and heavily influences the methodology and analysis."}, {"fullname_first_author": "Rohith Kuditipudi", "paper_title": "Robust distortion-free watermarks for language models", "publication_date": "2023-07-15", "reason": "This paper introduces a watermarking method with distortion-free properties, directly influencing the study and comparison of watermarking techniques in the current paper."}, {"fullname_first_author": "Zhengmian Hu", "paper_title": "Unbiased watermark for large language models", "publication_date": "2023-10-31", "reason": "This paper addresses the issue of unbiased watermarking, a key property analyzed and contrasted in relation to robustness and other design choices within the current paper."}, {"fullname_first_author": "Nikola Jovanovi\u0107", "paper_title": "Watermark stealing in large language models", "publication_date": "2024-02-14", "reason": "This paper investigates watermark stealing attacks, a crucial aspect addressed and expanded upon in the context of exploring trade-offs in watermark design within the current work."}, {"fullname_first_author": "Ali Naseh", "paper_title": "Stealing the decoding algorithms of language models", "publication_date": "2023-10-30", "reason": "This work explores the vulnerability of language models to decoding algorithm extraction attacks, which is relevant to the current paper's investigation into the security and robustness of watermarking schemes."}]}