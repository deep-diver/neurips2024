[{"figure_path": "HavKlV22xJ/figures/figures_5_1.jpg", "caption": "Figure 1: Consider an MDP with two states and two actions (see Appendix A.1 for details). The 4 black crosses correspond to the value function of the 4 possible policies. When combining policy iteration with a low rank estimation procedure, we just need to control the condition number of the 4 corresponding value matrices. The red dots correspond to the successive estimates V(t) of V* when running value iteration. When applying a value iteration approach, we would need to upper bound the condition number of all the corresponding matrices Q(t) = F(V(t-1)) for t > 1. For a given V, the background color in the figure indicates the value of the condition number of F(V). We see that the dynamics of V(t) under the value iteration algorithm are such that the trajectory (Q(t), t > 1) has to go through regions where the condition number is very high. Hence on this example, a value iteration approach would not work well.", "description": "This figure compares policy iteration and value iteration for a simple MDP with two states and two actions.  Policy iteration only requires bounding the condition number of a small, fixed set of matrices, while value iteration requires bounding the condition number of a much larger, unpredictable set of matrices. The background color shows the condition number, illustrating how value iteration may encounter matrices with very high condition numbers during the process.", "section": "3.3 Policy vs. Value Iteration: the condition number issue"}, {"figure_path": "HavKlV22xJ/figures/figures_13_1.jpg", "caption": "Figure 2: Matrix completion: matrix M* is of size 1000 \u00d7 1000, rank d = 5 and sampled entries have additive Gaussian noise with \u03c3 = 0.01. Number of anchors used was K = 10. All plots are averaged over 30 simulations and a new random matrix M* was generated in every 5 simulations.", "description": "This figure compares four different matrix completion methods: uniform anchors, leveraged anchors, oracle anchors, and SVD. The methods are evaluated based on Frobenius norm and infinity norm. The results show that leveraged anchors provide better performance than uniform anchors and comparable results to oracle anchors, which have prior knowledge of the best anchors.", "section": "A.2 Matrix completion with leveraged anchors"}, {"figure_path": "HavKlV22xJ/figures/figures_14_1.jpg", "caption": "Figure 3: Matrix Q* is obtained from rank d = 5 rewards and transition matrices. Moreover, S = 70, A = 50, \u03b3 = 0.9, and we choose number of anchors K = 15. Observations are noisy with additive Gaussian noise with \u03c3 = 0.01. Plots are averaged over 100 simulations, and new MDPs are generated every 5 simulations, while the number of samples in an iteration t is 10(1.1)t.", "description": "This figure compares the performance of value iteration (VI) and policy iteration (PI) algorithms using two different anchor selection methods: leveraged anchors (based on estimated leverage scores) and uniform anchors.  The plots show the entrywise error between the estimated Q matrix (Q(t)) and the true Q* matrix at each iteration (t).  The results demonstrate that leveraging anchor selection significantly improves the accuracy and reduces the entrywise error compared to uniform anchor selection, highlighting the benefit of adaptive sampling based on leverage scores.", "section": "A.3 Leverage scores for VI and PI"}, {"figure_path": "HavKlV22xJ/figures/figures_14_2.jpg", "caption": "Figure 3: Matrix Q* is obtained from rank d = 5 rewards and transition matrices. Moreover, S = 70, A = 50, y = 0.9, and we choose number of anchors K = 15. Observations are noisy with additive Gaussian noise with \u03c3 = 0.01. Plots are averaged over 100 simulations, and new MDPs are generated every 5 simulations, while the number of samples in an iteration t is 10(1.1)t.", "description": "This figure compares the performance of value iteration (VI) and policy iteration (PI) using different anchor selection methods for low-rank matrix estimation.  It shows how leverage scores (quantifying information) improve the accuracy of the methods compared to uniform random selection of anchors in each iteration.  The experiment uses a low-rank matrix with 70 states and 50 actions, a discount factor of 0.9 and a noise level of 0.01.  The number of samples is increased geometrically in each iteration.", "section": "A.3 Leverage scores for VI and PI"}, {"figure_path": "HavKlV22xJ/figures/figures_14_3.jpg", "caption": "Figure 4: LoRa-VI: Q* generated from low-rank r and P of rank d = 4, S = A = 1000, \u03b3 = 0.1. We used K = 10 anchors, V(0) = 0, rewards are noisy with Gaussian noise \u03c3 = 0.01. All plots are averaged over 5 simulations, each consisting of 50 epochs, and the number of samples in an epoch t is approximately 20(1.05)t(S + A)K.", "description": "This figure compares the performance of three different value iteration methods in estimating a low-rank value matrix Q*.  LoRa-VI with leveraged anchors uses leverage scores to select informative samples, while LoRa-VI with uniform anchors samples uniformly. The full-matrix VI serves as a baseline, using all entries. The results indicate that using leverage scores for anchor selection significantly improves estimation accuracy.", "section": "A.4 Low-rank Value Iteration"}, {"figure_path": "HavKlV22xJ/figures/figures_14_4.jpg", "caption": "Figure 4: LoRa-VI: Q* generated from low-rank r and P of rank d = 4, S = A = 1000, \u03b3 = 0.1. We used K = 10 anchors, V(0) = 0, rewards are noisy with Gaussian noise \u03c3 = 0.01. All plots are averaged over 5 simulations, each consisting of 50 epochs, and the number of samples in an epoch t is approximately 20(1.05)t(S + A)K.", "description": "The figure shows the performance of three different value iteration methods for low-rank MDPs. The x-axis represents the total number of samples used, and the y-axis shows the error in estimating the value matrix Q*. The three methods compared are VI with leveraged anchors, VI with uniform anchors, and full-matrix VI. The results show that VI with leveraged anchors achieves significantly better performance than the other methods, demonstrating the effectiveness of the leveraged sampling approach. The results are averaged over multiple simulations and show error bars to indicate the variability in performance.", "section": "A.4 Low-rank Value Iteration"}, {"figure_path": "HavKlV22xJ/figures/figures_15_1.jpg", "caption": "Figure 5: LoRa-PI: Q* generated from low-rank r and P of rank d = 4, S = A = 1000, \u03b3 = 0.1, T = 5. We used K = 10 anchors, uniformly random initial policy, and noisy rewards with Gaussian noise \u03c3 = 0.01. Plots for PI with anchors are averaged over 3 simulations, while the one for full-matrix PI is simulated once. Each simulation consisted of 20 epochs, and the number of samples in an epoch t is approximately 10(1.15)*(S + A)K.", "description": "This figure shows the performance of three different policy iteration methods: PI with leveraged anchors, PI with uniform anchors, and full-matrix PI.  The plot compares the Frobenius norm and entrywise error of the estimated Q-matrix against the true Q*-matrix, as the total number of trajectories increases.  Leveraged anchors significantly improve the estimation accuracy compared to uniform anchors and the full-matrix method.", "section": "A.5 Low-rank Policy Iteration"}]