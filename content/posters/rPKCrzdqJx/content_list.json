[{"type": "text", "text": "Regret Minimization in Stackelberg Games with Side Information ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Keegan Harris   \nSchool of Computer Science   \nCarnegie Mellon University   \nPittsburgh, PA 15213   \nkeeganh@cs.cmu.edu   \nZhiwei Steven Wu   \nSchool of Computer Science   \nCarnegie Mellon University   \nPittsburgh, PA 15213   \nzhiweiw@cs.cmu.edu   \nMaria-Florina Balcan   \nSchool of Computer Science   \nCarnegie Mellon University   \nPittsburgh, PA 15213   \nninamf@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Algorithms for playing in Stackelberg games have been deployed in real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), which may significantly affect both players\u2019 optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader commits to a (contextdependent) strategy, and the follower best-responds to both the leader\u2019s strategy and the context. We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round. In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve noregret in the full adversarial setting. Motivated by this result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which contexts are stochastic and follower types are adversarial. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A Stackelberg game [30, 31] is a strategic interaction between two utility-maximizing players in which one player (the leader) is able to commit to a (possibly mixed) strategy before the other player (the follower) takes an action. While Stackelberg\u2019s original formulation was used to model economic competition between firms, Stackelberg games have been used to study a wide range of topics in computing ranging from incentives in algorithmic decision-making [15] to radio spectrum utilization [32]. Perhaps the most successful application of Stackelberg games to solve real-world problems is in the field of security, where the analysis of Stackelberg security games has led to new methods in domains such as passenger screening at airports [8], wildlife protection efforts in conservation areas [10], the deployment of Federal Air Marshals on board commercial flights [18], and patrol boat schedules for the United States Coast Guard [1].1 ", "page_idx": 0}, {"type": "text", "text": "However in many real-world settings which are typically modeled as Stackelberg games, the payoffs of the players often depend on additional contextual information which is not captured by the Stackelberg game framework. For example, in airport security the severity of an attack (as well as the \u201cbenefit\u201d of a successful attack to the attacker) depends on factors such as the arrival and departure city of a filght, the whether there are VIP passengers on board, and the amount of valuable cargo on the aircraft. Additionally, there may be information in the time leading up to the attack attempt which may help the security service determine the type of attack which is coming [17]. For instance, in wildlife protection settings factors such as the weather and time of year may make certain species of wildlife easier or harder to defend from poaching, and information such as the location of tire tracks may provide context about which animals are being targeted. As a result, the optimal strategy of both the leader and the follower may change significantly depending on the side information available. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Overview of our results. In order to capture the additional information that the leader and follower may have at their disposal, we formalize such settings as Stackelberg games with side information. Specifically, we consider a setting in which a leader interacts with a sequence of followers in an online fashion. At each time-step, the leader gets to see payoff-relevant information about the current round in the form of a context. After observing the context, the leader commits to a mixed strategy over a finite set of actions, and the follower best-responds to both (1) the leader\u2019s strategy and (2) the context in order to maximize their utility. We allow the follower in each round to be one of $K$ types. Each follower type corresponds to a different mapping from contexts, leader strategies, and follower actions to utilities. While the leader may observe the context before committing to their mixed strategy, they do not get to observe the follower\u2019s type until after the round is over. Under this setting, the goal of the leader is to minimize their regret with respect to the best policy (i.e. the best mapping from contexts to mixed strategies) in hindsight, with respect to the realized sequence of followers and contexts. ", "page_idx": 1}, {"type": "text", "text": "We show that in the fully adversarial setting (i.e. the setting in which both the sequence of contexts and follower types is chosen by an adversary), no-regret learning is not possible, even when the policy class is highly structured. We show this via a reduction from the problem of online linear thresholding, for which it is known that no no-regret learning algorithm exists. Motivated by this impossibility result, we study two natural relaxations: (1) a setting in which the sequence of contexts is chosen by an adversary and the sequence of follower types is chosen stochastically, and (2) a setting in which the sequence of contexts is chosen stochastically and the sequence of follower types is chosen by an adversary. ", "page_idx": 1}, {"type": "text", "text": "In the stochastic follower setting we show that the greedy algorithm (Algorithm 1), which estimates the leader\u2019s expected utility for the given context and plays the mixed strategy which maximizes their estimate, achieves no-regret as long as the total variation distance between their estimate and the true distribution is decreasing with time. We then show how to instantiate the leader\u2019s estimation procedure so that the regret of Algorithm 1 is $O(\\operatorname*{min}\\{K,A_{f}\\}\\sqrt{T\\log(T)})$ , where $T$ is the time horizon, $K$ is the number of follower types, and $A_{f}$ is the number of follower actions. In the stochastic context setting, we show the leader can obtain $O(\\sqrt{K T\\log(T)}+K)$ regret by playing Hedge over a finite set of policies (Algorithm 2). An important intermediate result in both settings is that it is (nearly) without loss of generality to consider leader policies which map to a finite set of mixed strategies ${\\mathcal{E}}_{\\mathbf{z}}$ , given context ${\\bf z}$ .2 ", "page_idx": 1}, {"type": "text", "text": "Next, we extend our algorithms for both types of adversary to the setting in which the leader does not get to observe the follower\u2019s type after each round, but instead only gets to observe their action. We refer to this type of feedback as bandit feedback. Both of our extensions to bandit feedback make use of the notion of a barycentric spanner [4], a special basis under which bounded loss estimators may be obtained for all leader mixed strategies. In the bandit stochastic follower setting, we use the fact that in addition to being bounded, a loss estimator constructed using a barycentric spanner has low variance, in order to show that a natural extension of our greedy algorithm obtains ${\\tilde{O}}(T^{\\bar{2}/3})$ regret.We also make use of barycentric spanners in the (bandit) stochastic context setting, albeit in a different way. Here, our algorithm proceeds by splitting the time horizon into blocks, and using a barycentric spanner to estimate the leader\u2019s utility from playing according to a set of special policies in each block. We then play Hedge over these policies to obtain $\\bar{O}(T^{2/3})$ leader regret.3 See Table 1 for a summary of our results. ", "page_idx": 1}, {"type": "text", "text": "Related work. Letchford et al. [21] consider the problem of learning the leader\u2019s optimal mixed strategy in the repeated Stackelberg game setting against a perfectly rational follower with an unknown payoff matrix. Peng et al. [23] study the same setting as Letchford et al. [21]. They provide improved rates and prove nearly-matching lower bounds. Learning algorithms to recover the leader\u2019s optimal mixed strategy have also been studied in Stackelberg security games [5, 7, 26, 6]. ", "page_idx": 1}, {"type": "text", "text": "Our work builds off of several results established for online learning in (non-contextual) Stackelberg games in Balcan et al. [5]. In particular, our results in Section 4.2 and Appendix C.2 may be viewed as a generalization of their results to the setting in which the payoffs of both players depend on an external context. Roughly speaking, Balcan et al. [5] show that it without loss to play Hedge over a finite set of mixed strategies in order to obtain no-regret against an adversarially-chosen sequence of follower types. In order to handle the additional side information available in our setting, we instead play Hedge over a finite set of policies, each of which map to a finite set of (context-dependent) mixed strategies. However, the discretization argument is more nuanced in our setting. In particular, it is not without loss of generality to consider a finite set of policies. As a result, we need to bound the additional regret incurred by the leader due to the discretization. More recent work on learning in Stackelberg games provides improved regret rates in the full feedback [9] and bandit feedback [6] settings, and considers the effects of non-myopic followers [13] and followers who respond to calibrated predictions [14]. ", "page_idx": 1}, {"type": "table", "img_path": "rPKCrzdqJx/tmp/b8832cbc694ac18ba71a9d1f591120ca18d4f5f6261852b6bd98f9d71ddb17f2.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of our results. Under bandit feedback, we consider a relaxed setting in which only the leader\u2019s utility depends on the side information. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Lauffer et al. [20] study a Stackelberg game setting in which there is an underlying (probabilistic) state space which affects the leader\u2019s rewards, and there is a single (unknown) follower type. In contrast, we study a setting in which the sequence of follower types and/or contexts may be chosen adversarially. Sessa et al. [25] study a repeated game setting in which the players receive additional information (i.e. a context) at each round, much like in our setting. However, their focus is on repeated normal-form games, which require different tools and techniques to analyze compared to the repeated Stackelberg game setting we consider. Other work has also considered repeated normal-form games which change over time in different ways. In particular, Zhang et al. [33], Anagnostides et al. [3] study learning dynamics in time-varying game settings, and Harris et al. [16] study a meta-learning setting in which the game being played changes after a fixed number of rounds. ", "page_idx": 2}, {"type": "text", "text": "Finally, our problem may be viewed is a special case of the contextual bandit setting with adversariallychosen utilities [28, 29, 24], where the learner gets to observe \u201cextra information\u201d in the form of the follower\u2019s type (Section 4) or the follower\u2019s action (Section 5). However, there is much to gain from taking advantage of the additional information and structure that is present in our setting. Besides having generally worse regret rates, another reason not to use off-the-shelf adversarial contextual bandit algorithms in our setting is that they typically require either (1) the learner to know the set of contexts they will face beforehand (the transductive setting; Syrgkanis et al. [28, 29], Rakhlin and Sridharan [24]) or (2) for there to exist a small set of contexts such that any two policies behave differently on at least one context in the set (the small separator setting; Syrgkanis et al. [28]). We require no such assumptions. ", "page_idx": 2}, {"type": "text", "text": "2 Setting and background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We use $[N]\\,:=\\,\\{1,\\dots,N\\}$ to denote the set of natural numbers up to and including $N\\in\\mathbb{N}$ and $\\operatorname{cl}({\\mathcal{P}})$ to denote the closure of the set $\\mathcal{P}$ . $\\mathbf{x}[a]$ denotes the $a$ -th component of vector $\\mathbf{x}$ , and $\\Delta(A)$ denotes the probability simplex over the set $\\boldsymbol{\\mathcal{A}}$ . $\\begin{array}{r}{\\mathrm{TV}(\\mathbf{p},\\mathbf{q})=\\frac{1}{2}\\int|p(x)-q(x)|d x}\\end{array}$ is the total variation distance between distributions $\\mathbf{p}$ and $\\mathbf{q}$ , and $\\mathbb{E}_{t}[x]=\\mathbb{E}[x|\\bar{\\mathcal{F}}_{t}]$ is shorthand for the expected value of the random variable $x$ , conditioned on the filtration up to (but not including) time $t$ . All proofs may be found in the Appendix. Finally, while we present our results for general Stackelberg games with side information, our results are readily applicable to the special case of Stackelberg security games with side information. ", "page_idx": 2}, {"type": "text", "text": "Our setting. We consider a game between a leader and a sequence of followers. In each round $t\\in[T]$ , Nature reveals a context $\\mathbf{z}_{t}\\,\\in\\,\\mathcal{Z}\\,\\subseteq\\,\\mathbb{R}^{d}$ to both players.4 The leader moves first by playing some mixed strategy $\\mathbf{x}_{t}\\in\\mathcal{X}\\subseteq\\mathbb{R}^{A}$ over a set of (finite) leader actions $\\boldsymbol{\\mathcal{A}}$ , i.e., $\\mathbf{x}_{t}\\in\\Delta(A)$ . The size of $\\boldsymbol{\\mathcal{A}}$ is $A:=|A|$ . Having observed the leader\u2019s mixed strategy, the follower best-responds to both $\\mathbf{x}_{t}$ and $\\mathbf{z}_{t}$ by playing some action $a_{f}\\in A_{f}$ , where $\\mathcal{A}_{f}$ is the (finite) set of follower actions and $A_{f}:=|\\mathcal{A}_{f}|$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Follower Best-Response). Follower $f$ \u2019s best-response to context z and mixed strategy x is $\\begin{array}{r}{b_{f}(\\mathbf{z},\\mathbf{x})\\,\\in\\,\\arg\\operatorname*{max}_{a_{f}\\in\\mathcal{A}_{f}}\\,\\sum_{a_{l}\\in\\mathcal{A}}\\mathbf{x}[a_{l}]\\cdot u_{f}(\\mathbf{z},a_{l},a_{f})}\\end{array}$ , where $u_{f}:\\mathcal{Z}\\times\\mathcal{A}\\times\\mathcal{A}_{f}\\,\\to\\,[0,1]$ is follower $f$ \u2019s utility function. In the case of ties, we assume that there is a fixed and known ordering over actions which determines how the follower best-responds, i.e. if $a>a^{\\prime}$ for $a,a^{\\prime}\\in A_{f}$ then the follower will break ties between a and $a^{\\prime}$ in favor of $a$ .5 ", "page_idx": 3}, {"type": "text", "text": "We allow for the follower in round $t$ (denoted by $f_{t.}$ ) to be one of $K~\\ge~1$ follower types $\\{\\alpha^{(1)},\\ldots,\\alpha^{(K)}\\}$ (where $K\\ \\le\\ T)$ . Follower type $\\alpha^{(i)}$ is characterized by utility function $u_{\\alpha^{(i)}}:\\mathcal{Z}\\times\\mathcal{A}\\times\\mathcal{A}_{f}\\,\\to\\,[0,1].$ , i.e. given a context $\\mathbf{z}$ , leader action $a_{l}$ , and follower action $a_{f}$ , a follower of type $\\alpha^{(i)}$ would receive utility $u_{\\alpha^{(i)}}\\big(\\mathbf{z},a_{l},a_{f}\\big)$ . We assume that the set of all possible follower types and their utility functions are known to the leader, but that the follower\u2019s type at round $t$ is not revealed to the leader until after the round is over. We denote the leader\u2019s utility function by $u:$ $\\mathcal{Z}\\times\\mathcal{A}\\times\\mathcal{A}_{f}\\rightarrow[0,1]$ and assume it is known to the leader. We often use the shorthand $u(\\mathbf{z},\\mathbf{x},a_{f})=$ $\\begin{array}{r}{\\sum_{a_{l}\\in\\mathcal{A}}\\mathbf{x}[a_{l}]\\!\\cdot\\!u(\\mathbf{z},a_{l},a_{f})}\\end{array}$ to denote the leader\u2019s expected utility of playing mixed strategy $\\mathbf{x}$ under context ${\\bf z}$ against follower action $a_{f}$ . Follower $f_{t}$ \u2019s expected utility $u_{f_{t}}(\\mathbf{z},\\mathbf{x},a_{f})$ is defined analogously. ", "page_idx": 3}, {"type": "text", "text": "A leader policy $\\pi:\\mathcal{Z}\\rightarrow\\mathcal{X}$ is a (possibly random) mapping from contexts to mixed strategies. If the leader using policy $\\pi_{t}$ in round $t$ and observes context $\\mathbf{z}_{t}$ , their strategy $\\mathbf{x}_{t}$ is given by $\\mathbf{x}_{t}\\sim\\pi_{t}(\\mathbf{z}_{t})$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Optimal Policy). Given a sequence of followers $f_{1},\\ldots,f_{T}$ and contexts $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ , the strategy given by the leader\u2019s optimal-in-hindsight policy for context z is $\\pi^{*}(\\mathbf{z})\\in$ arg $\\begin{array}{r}{\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}u(\\mathbf{z},\\mathbf{x},b_{f_{t}}(\\mathbf{z},\\mathbf{x}))\\cdot\\mathbb{1}\\{\\mathbf{z}_{t}=\\mathbf{z}\\}.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "We measure the leader\u2019s performance against the optimal policy via the notion of contextual Stackelberg regret (regret for short). ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3 (Contextual Stackelberg Regret). Given a sequence of followers $f_{1},\\ldots,f_{T}$ and contexts $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ , the leader\u2019s contextual Stackelberg regret is $\\begin{array}{r l r}{R(T)}&{{}:=}&{}\\end{array}$ $\\begin{array}{r}{\\sum_{t=1}^{T}u(\\mathbf z_{t},\\pi^{*}(\\mathbf z_{t}),b_{f_{t}}(\\mathbf z_{t},\\pi^{*}(\\mathbf z_{t})))-u(\\mathbf z_{t},\\mathbf x_{t},b_{f_{t}}(\\mathbf z_{t},\\mathbf x_{t}))}\\end{array}$ , where $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{T}$ is the sequence of mixed strategies played by the leader. ", "page_idx": 3}, {"type": "text", "text": "If an algorithm achieves regret $R(T)=o(T)$ , we say that it is a no-regret algorithm. We consider three ways in which Nature can select the sequence of contexts/followers: ", "page_idx": 3}, {"type": "text", "text": "1. If the sequence of contexts (resp. follower types) are drawn i.i.d. from some fixed distribution, we say that the sequence of contexts (resp. follower types) are chosen stochastically.   \n2. If Nature chooses the sequence of contexts (resp. follower types) before the first round in order to harm the leader (possibly using knowledge of the leader\u2019s algorithm), we say that the sequence of contexts (resp. follower types) are chosen by a non-adaptive adversary.   \n3. If Nature chooses context $\\mathbf{z}_{t}$ (resp. follower $f_{t}$ ) before round $t$ in order to harm the leader (possibly using knowledge of the leader\u2019s algorithm and the outcomes of the prior $t-1$ rounds), we say that the sequence of contexts (resp. follower types) are chosen by an adaptive adversary. ", "page_idx": 3}, {"type": "text", "text": "Our impossibility results in Section 3 hold when both the sequence of contexts and the sequence of follower types are chosen by either type of adversary. Our positive results in Section 4 hold when either the sequence of contexts or the sequence of follower types are chosen by either type of adversary (and the other sequence is chosen stochastically). Our extension to bandit feedback (Section 5, where the leader only gets to observe the follower\u2019s best-response instead of their type) holds whenever one sequence is chosen by a non-adaptive adversary and the other sequence is chosen stochastically. ", "page_idx": 3}, {"type": "text", "text": "3 On the impossibility of fully adversarial no-regret learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin with a negative result: no-regret learning is not possible in the setting of Section 2 if the sequence of contexts and the sequence of followers is chosen by an adversary. While this is not necessarily surprising given that Definition 2.3 allows for the optimal policy $\\pi^{*}$ to be arbitrarily ", "page_idx": 3}, {"type": "image", "img_path": "rPKCrzdqJx/tmp/a9f39d36238df2da863d3677ae880c9beba4f71bc243e6668c64529c3db5d937.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Summary of our reduction from the online linear thresholding problem. At time $t\\in[T]$ , (1.) the learner observes a point $\\omega_{t}$ , (2.) the learner takes a guess $g_{t}$ , and (3.) the learner observes the true label $y_{t}$ . Given a regret minimizer for our setting, we show how to use it in a black-box way (by constructing functions $h_{1},h_{2},h_{3})$ to achieve no-regret in the online linear thresholding problem. ", "page_idx": 4}, {"type": "text", "text": "complex, we show that this result holds even when the policy class to which $\\pi^{*}$ belongs is highly structured. We show this via a reduction to the online linear thresholding problem, for which it is known that no-regret learning is impossible. ", "page_idx": 4}, {"type": "text", "text": "Online linear thresholding. The online linear thresholding problem is a repeated two-player game between a learner and an adversary. Before the first round, an adversary chooses a cutoff $s\\in[0,1]$ which is unknown to the learner. In each round, the adversary chooses a point $\\omega_{t}\\in[0,1]$ and reveals it to the learner. $\\omega_{t}$ is assigned label $y_{t}\\,=\\,1$ if $\\omega_{t}~>~s$ and label $y_{t}\\,=\\,-1$ otherwise. Given $\\omega_{t}$ , the learner makes a guess $g_{t}\\,\\in\\,[0,1]$ (the probability they place on $y_{t}\\,=\\,1)$ , and receives utility $u_{\\mathrm{OLT}}(\\omega_{t},g_{t})=g_{t}\\cdot\\mathbb{1}\\{y_{t}=1\\}+(1-g_{t})\\cdot\\mathbb{1}\\{y_{t}=-1\\}$ . The learner gets to observe $y_{t}$ after round $t$ is over. The learner\u2019s policy $\\pi_{t}:[0,1]\\to[0,1]$ is a mapping from points in $[0,1]$ to guesses in $[0,1]$ . The optimal policy $\\pi^{*}$ makes guess $\\pi^{*}(\\omega_{t})=1$ if $\\omega_{t}~>~s$ and $\\pi^{*}(\\omega_{t})=0$ otherwise. The learner\u2019s regret after $T$ rounds is given by $\\begin{array}{r}{R_{\\mathrm{OLT}}(T)=T-\\sum_{t=1}^{T}{u_{\\mathrm{OLT}}(\\omega_{t},g_{t})}}\\end{array}$ , since the optimal policy achieves utility 1 in every round. In order to prove a lower bound on contextual Stackelberg regret in our setting, we make use of the following well-known lower bound on regret in the online linear thresholding setting (see e.g. [12]). ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. Any algorithm suffers regret $R_{\\mathrm{OLT}}(T)\\,=\\,\\Omega(T)$ in the online linear thresholding problem when the sequence of points $\\omega_{1},\\ldots,\\omega_{T}$ is chosen by an adversary. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. If an adversary can choose both the sequence of contexts $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ and the sequence of followers $f_{1},\\ldots,f_{T}$ , no algorithm can achieve better than $\\Omega(T)$ contextual Stackelberg regret in expectation over the internal randomness of the algorithm, even when $\\pi^{*}$ is restricted to come from the set of linear thresholding functions. ", "page_idx": 4}, {"type": "text", "text": "The reduction from online linear thresholding proceeds by creating an instance of our setting such that the sequence of contexts $z_{1},\\dots,z_{T}$ correspond to the sequence of points $\\omega_{1},\\ldots,\\omega_{T}$ encountered by the learner, and the sequence of follower types $f_{1},\\ldots,f_{T}$ correspond to the sequence of labels $y_{1},\\dots,y_{T}$ . We then show that a no-regret algorithm in the online thresholding problem can be obtained by using an algorithm which minimizes contextual Stackelberg regret on the constructed game instance as a black box. However this is a contradiction, since by Lemma 3.1 the online thresholding problem is not online learnable by any algorithm. See Figure 1 for a visualization of our reduction. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, this reduction works because the adversary can \u201chide\u201d information about the follower\u2019s type $f_{t}$ in the context $z_{t}$ . However, there exists a family of problem instances in which learning this relationship between contexts and follower types as hard as learning the threshold in the online linear thresholding problem, for which no no-regret learning algorithm exists by Lemma 3.1. ", "page_idx": 4}, {"type": "text", "text": "4 Limiting the power of the adversary ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Motivated by the impossibility result of Section 3, we study two natural relaxations of the fully adversarial setting: one in which the sequence of followers is chosen stochastically but the contexts are chosen adversarially (Section 4.1) and one in which the sequence of contexts is chosen stochastically but followers are chosen adversarially (Section 4.2). In both settings we allow the adversary to be adaptive. ", "page_idx": 4}, {"type": "text", "text": "An important structural results for both Section 4.1 and Section 4.2 is that for any context $\\mathbf{z}\\in{\\mathcal{Z}}$ , the leader incurs only negligible regret by restricting themselves to policies which map to mixed strategies in some finite (and computable) set ${\\mathcal{E}}_{\\mathbf{z}}$ . In order to state this result formally, we need to introduce the notion of a contextual best-response region, which is a generalization of the notion of a best-response region in (non-contextual) Stackelberg games (e.g. [21, 5]). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 4.1 (Contextual Follower Best-Response Region). For follower type $\\alpha^{(i)}$ , follower action $a_{f}\\in A_{f}$ , and context $\\mathbf{z}\\in{\\mathcal{Z}}$ , let $\\mathcal{X}_{\\mathbf{z}}(\\alpha^{(i)},a_{f})\\subseteq\\mathcal{X}$ denote the set of all leader mixed strategies such that a follower of type $\\alpha^{(i)}$ best-responds to all $\\mathbf{x}\\in\\mathcal{X}_{\\mathbf{z}}(\\alpha^{(i)},a_{f})$ by playing action $a_{f}$ under context z, i.e., $\\mathcal{X}_{\\mathbf{z}}(\\alpha^{(i)},a_{f})=\\{\\mathbf{x}\\in\\mathcal{X}:b_{\\alpha^{(i)}}(\\mathbf{z},\\mathbf{x})=a_{f}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 4.2 (Contextual Best-Response Region). For a given function $\\sigma:\\{\\alpha^{(1)},\\ldots,\\alpha^{(K)}\\}\\rightarrow{\\cal A}_{f}$ , let $\\mathcal{X}_{\\bf z}(\\sigma)$ denote the set of all leader mixed strategies such that under context ${\\bf z}$ , a follower of type $\\alpha^{(i)}$ plays action $\\sigma(\\alpha^{(i)})$ for all $i\\in[K]$ , i.e. $\\mathcal{X}_{\\mathbf{z}}(\\sigma)=\\cap_{i\\in[K]}\\mathcal{X}_{\\mathbf{z}}(\\alpha^{(i)},\\sigma(\\alpha^{(i)}))$ . ", "page_idx": 5}, {"type": "text", "text": "For a fixed contextual best-response region $\\mathcal{X}_{\\bf z}(\\sigma)$ , we refer to the corresponding $\\sigma$ as the bestresponse function for region ${\\mathcal{X}}_{\\mathbf{z}}(\\sigma)$ , as it maps each follower type to its best-response for every leader strategy $\\mathbf{x}\\in\\mathcal{X}_{\\mathbf{z}}(\\sigma)$ . We sometimes use ${\\boldsymbol{\\sigma}}^{(\\mathbf{z},\\mathbf{x})}$ to refer to the best-response function associated with mixed strategy $\\mathbf{x}$ under context $\\mathbf{z}$ , and we use $\\Sigma_{\\mathbf{z}}$ to refer to the set of all best-response functions under context $\\mathbf{z}$ . Note that $|\\Sigma_{\\mathbf{z}}|\\leq A_{f}^{K}$ for any context $\\mathbf{z}\\in{\\mathcal{Z}}$ . This gives us an upper-bound on the number of best-response regions for a given context. ", "page_idx": 5}, {"type": "text", "text": "One useful property of all contextual best-response regions is that they are convex and bounded polytopes. To see this, observe that every contextual follower best-response region (and therefore every contextual best-response region) is (1) a subset of $\\Delta^{d}$ and (2) the intersection of finitelymany half-spaces. While every $\\mathcal{X}_{\\bf z}(\\sigma)$ is convex and bounded, it is not necessarily closed. If every contextual best-response region were closed, it would be without loss of generality for the leader to restrict themselves to the set of policies which map every context to an extreme point of some contextual best-response region. In what follows, we show that the leader does not \u201close too much\u201d (as measured by regret) by restricting themselves to policies which map to some approximate extreme point of a contextual best-response region. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.3 $\\boldsymbol{\\ddot{\\beta}}$ -approximate extreme points). Fix a context $\\mathbf{z}\\in{\\mathcal{Z}}$ and consider the set of all nonempty contextual best-response regions. For $\\delta>0$ , $\\mathcal{E}_{\\mathbf{z}}(\\delta)$ is the set of leader mixed strategies such that for all best-response functions $\\sigma$ and any $\\mathbf{x}\\in\\Delta(\\mathcal{A}_{l})$ that is an extreme point of $c l(\\mathcal{X}_{\\bf z}(\\sigma))$ , $\\mathbf{x}\\in\\mathcal{E}_{\\mathbf{z}}(\\delta)$ $i f\\mathbf{x}\\in\\mathcal{X}_{\\mathbf{z}}(\\sigma)$ . Otherwise there is some $\\mathbf{x}^{\\prime}\\in\\mathcal{E}_{\\mathbf{z}}(\\delta)$ such that $\\mathbf{x}^{\\prime}\\in\\mathcal{X}_{\\mathbf{z}}(\\sigma)$ and $\\|\\mathbf{x}^{\\prime}-\\mathbf{x}\\|_{1}\\leq\\delta$ . ", "page_idx": 5}, {"type": "text", "text": "Note that Definition 4.3 is constructive. We set $\\begin{array}{r}{\\delta=\\mathcal{O}(\\frac{1}{T})}\\end{array}$ so that the additional regret from only considering policies which map to points in $\\cup_{\\mathbf{z}\\in\\mathcal{Z}}\\mathcal{E}_{\\mathbf{z}}(\\delta)$ is negligible. As a result, we use the shorthand $\\mathcal{E}_{\\mathbf{z}}:=\\mathcal{E}_{\\mathbf{z}}(\\delta)$ throughout the sequel. The following lemma is a generalization of Lemma 4.3 in Balcan et al. [5] to our setting, and its proof uses similar techniques from convex analysis. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.4. For any sequence of followers $f_{1},\\ldots f_{T}$ and any leader policy $\\pi$ , there exists a policy $\\pi^{(\\mathcal{E})}\\,:\\,\\mathcal{Z}\\,\\rightarrow\\,\\cup_{{\\bf z}\\in\\mathcal{Z}}\\dot{\\mathcal{E}}_{{\\bf z}}$ that, when given context z, plays a mixed strategy in ${\\mathcal{E}}_{\\mathbf{z}}$ and guarantees that $\\begin{array}{r}{\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))\\leq1.}\\end{array}$ . Moreover, the same resu lt holds in expectation over any distribution over follower types $\\mathcal{F}$ . ", "page_idx": 5}, {"type": "text", "text": "Since we do not restrict the context space to be finite, the leader cannot pre-compute ${\\mathcal{E}}_{\\mathbf{z}}$ for every $\\mathbf{z}\\in{\\mathcal{Z}}$ before the game begins. Instead, they can compute $\\mathcal{E}_{\\mathbf{z}_{t}}$ in round $t$ before they commit to their mixed strategy. While $\\mathcal{E}_{\\mathbf{z}_{t}}$ is computatable, it may be exponentially large in $A_{f}$ and $K$ . However this is to be expected as Li et al. [22] show that in its general form, solving the non-contextual version of the online Stackelberg game problem is NP-Hard. ", "page_idx": 5}, {"type": "text", "text": "4.1 Stochastic follower types and adversarial contexts ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this setting we allow the sequence of contexts to be chosen by an adversary, but we restrict the sequence of followers to be sampled i.i.d. from some (unknown) distribution over follower types $\\mathcal{F}$ . When picking context $\\mathbf{z}_{t}$ , we allow the adversary to have knowledge of $\\mathcal{F}$ and $f_{1},\\ldots,f_{t-1}$ , but not $f_{t}$ . Under this relaxation, our measure of algorithm performance is expected contextual Stackelberg regret, where the expectation is taken over the randomness in the distribution over follower types. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.5 (Expected Contextual Stackelberg Regret). Given a distribution over followers $\\mathcal{F}$ and a sequence of contexts $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ , the leader\u2019s expected contextual Stackeleberg regret is $\\begin{array}{r}{\\mathbb{E}[R(T)]:=\\mathbb{E}_{f_{1},\\ldots,f_{T}\\sim\\mathcal{F}}[\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\mathbf{x}_{t},b_{f_{t}}(\\mathbf{z}_{t},\\mathbf{x}_{t}))]}\\end{array}$ , where $\\pi^{*}$ is $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ $\\mathcal{F}$ ", "page_idx": 5}, {"type": "text", "text": "Input: $\\widehat{\\bf p}_{1}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "for $t=1,\\dots,T$ do ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Observe $\\mathbf{z}_{t}$ , commit to $\\begin{array}{r}{\\mathbf{x}_{t}=\\pi_{t}(\\mathbf{z}_{t})=\\arg\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{E}_{\\mathbf{z}_{t}}}\\widehat{\\mathbb{E}}_{t}[u(\\mathbf{z}_{t},\\mathbf{x},b_{f_{t}}(\\mathbf{z}_{t},\\mathbf{x})]}\\end{array}$ (Equation (1)). Receive utility $u\\big(\\mathbf{z}_{t},a_{l},b_{f_{t}}\\big(\\mathbf{z}_{t},\\mathbf{x}_{t}\\big)\\big)$ where $a_{l}\\sim\\mathbf{x}_{t}$ , a nd observe follower type $f_{t}$ . Update $\\widehat{\\mathbf{p}}_{t}\\to\\widehat{\\mathbf{p}}_{t+1}$ ", "page_idx": 6}, {"type": "text", "text": "Under this setting, the utility for policy $\\pi$ may be written as $\\begin{array}{r}{\\mathfrak{T}_{f_{1},\\ldots,f_{T}\\sim\\mathcal{F}}\\left[\\sum_{t=1}^{T}u(\\overline{{\\mathbf{z}_{t}}},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\overline{{\\mathbf{z}}}_{t})))\\right]=\\sum_{t=1}^{\\dot{T}}\\dot{\\mathbb{E}}_{f_{1},\\ldots,f_{t-1}\\sim\\mathcal{F}}[\\widetilde{\\mathbb{E}}_{t}[u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))].}\\end{array}$ ))]]. Our algorithm (Algorithm 1) proceeds by estimating the inner expectation $\\mathbb{E}_{t}[u\\big({\\mathbf{z}}_{t},\\bar{\\pi}\\big({\\mathbf{z}}_{t}\\big),b_{f_{t}}\\big({\\mathbf{z}}_{t},\\bar{\\pi}\\big({\\mathbf{z}}_{t}\\big)\\big)\\big)]$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}_{t}[u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))]:=\\int u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),a_{f})d\\widehat{p}_{t}(b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}))=a_{f})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and acting greedily with respect to our estimate. Here $\\widehat{p}_{t}(b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}))=a_{f})$ is the (estimated) probability that the follower\u2019s best-response is $a_{f}$ , given  c ontext $\\mathbf{z}_{t}$ and leader mixed strategy $\\pi(\\mathbf{z}_{t})$ . As we will see, different instantiations of $\\widehat{p_{t}}$ will lead to different regret rates for Algorithm 1. However, before instantiating Algorithm 1 w it h a specific estimation procedure, we provide a general result which bounds the regret of Algorithm 1 in terms of the total variation distance between the sequence $\\{\\widehat{p}_{t}\\}_{t\\in[T]}$ and the true distribution $p$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Let $\\mathbf{p}(\\mathbf{z},\\mathbf{x}):=[p(b_{f_{t}}(\\mathbf{z},\\mathbf{x})=a_{f})]_{a_{f}\\in\\mathcal{A}_{f}}$ and $\\widehat{\\mathbf{p}}_{t}(\\mathbf{z},\\mathbf{x}):=[\\widehat{p}_{t}(b_{f_{t}}(\\mathbf{z},\\mathbf{x})=a_{f})]_{a_{f}\\in\\mathcal{A}_{f}}.$ . The expected contextual Stackelberg regret (Definition 4.5) o f  Algorithm 1  satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Im[R(T)]\\leq1+2\\sum_{t=1}^{T}\\mathbb{E}_{f_{1},\\ldots,f_{t-1}}[\\mathrm{TV}(\\mathbf{p}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))+\\mathrm{TV}(\\mathbf{p}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})))].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6 shows that the regret of Algorithm 1 is proportional to how well it estimates $\\mathbf{p}(\\mathbf{z},\\mathbf{x})$ over time (as measured by total variation distance), on (1) the sequence of contexts chosen by the adversary and (2) the sequence of mixed strategies played by Algorithm 1 and the (near-)optimal policy $\\pi^{(\\mathcal{E})}$ . While we instantiate Algorithm 1 in the setting where there are finitely-many follower types and follower actions, Theorem 4.6 opens the door to provide meaningful regret guarantees in settings in which there are infinitely-many follower types and/or follower actions.6 We now instantiate the estimation procedure in Algorit\u221ahm 1 in two different ways to get end-to-end regret guarantees. First, the leader can get regret $\\bar{O(K\\sqrt{T\\log T})}$ by estimating the distribution of follower types directly. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.7. If $\\widehat{\\mathbf{p}}_{t}\\,=\\,\\{\\widehat{p}_{t}(f_{t}\\,=\\,\\alpha^{(i)})\\}_{i\\in[K]},$ , $\\begin{array}{r}{\\widehat{p}_{t+1}(f\\,=\\,\\alpha^{(i)})\\,=\\,\\frac{1}{t}\\sum_{\\tau=1}^{t}\\mathbb{1}\\{f_{\\tau}\\,=\\,\\alpha^{(i)}\\},}\\end{array}$ , and $\\begin{array}{r}{\\widehat{p}_{1}(f=\\alpha^{(i)})=\\frac{1}{K}}\\end{array}$ for $i\\in[K]$ , then the regret of Algorithm $^{\\,l}$ satisfies $\\mathbb{E}[R(T)]=O(K{\\sqrt{T\\log(T)}})$ . ", "page_idx": 6}, {"type": "text", "text": "The leader can obtain a complementary regret bound of $O(A_{f}{\\sqrt{T\\log T}})$ if they instead estimate the probability that the follower best-responds with action $a_{f}\\in A_{f}$ , given a particular context $\\mathbf{z}$ and leader mixed strategy $\\mathbf{x}$ .7 In what follows, we use $\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a_{f})}\\in\\{0,1\\}^{K}$ to refer to the indicator vector whose $i$ -th component is $\\mathbb{1}\\{\\sigma^{(\\mathbf{z},\\mathbf{x})}(\\alpha^{(i)})=a_{f}\\}$ , i.e. the indicator that a follower of type $\\alpha^{(i)}$ best-responds to context $\\mathbf{z}$ and mixed strategy $\\mathbf{x}$ by playing action $a_{f}$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.8. If $\\begin{array}{r l r}{\\widehat{\\bf p}_{t}({\\bf z},{\\bf x})}&{{}}&{=}&{\\{\\widehat{p}_{t}\\big(\\mathbb{1}_{(\\sigma^{({\\bf z},{\\bf x})}=a_{f})}\\big)\\big\\}_{a_{f}\\in\\mathcal{A}_{f}},}\\end{array}$ , $\\widehat{p}_{t+1}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a)}\\big)$ = $\\begin{array}{r}{\\frac{1}{t}\\sum_{\\tau=1}^{t}\\mathbb{1}\\{b_{f_{\\tau}}(\\mathbf{z},\\mathbf{x})\\ =\\ a\\}}\\end{array}$ , and $\\begin{array}{r}{\\widehat{p}_{1}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a)}\\big)\\ =\\ \\frac{1}{A_{f}}}\\end{array}$ for $a_{f}~\\in~{\\mathcal{A}}_{f}$ , then the regret of $A l.$ - gorithm $^{\\,l}$ satisfies $\\mathbb{E}[R(T)]=O(A_{f}{\\sqrt{T\\log(T)}})$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Stochastic contexts and adversarial follower types ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now consider the setting in which the sequence of contexts are drawn i.i.d. from some unknown distribution $\\mathcal{P}$ and the follower $f_{t}$ is chosen by an adversary with knowledge of $\\mathcal{P}$ and $\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{t-1}$ , ", "page_idx": 6}, {"type": "text", "text": "Input: Set of weights $\\Omega$ ", "page_idx": 7}, {"type": "text", "text": "Let $\\mathbf{q}_{1}[\\pi^{(\\omega)}]:=1$ , $\\begin{array}{r}{\\mathbf{p}_{1}[\\pi^{(\\omega)}]:=\\frac{1}{|\\Pi|}}\\end{array}$ for all $\\pi^{(\\omega)}\\in\\Pi:=\\{\\pi^{(\\omega)}\\}_{\\omega\\in\\Omega}$ ", "page_idx": 7}, {"type": "text", "text": "do Sample $\\pi_{t}\\sim\\mathbf{p}_{t}$ , $a_{l,t}\\sim\\pi_{t}(\\mathbf{z}_{t})$ , receive utility $\\boldsymbol{u}(\\mathbf{z}_{t},a_{l,t},b_{f_{t}}(\\pi_{t}(\\mathbf{z}_{t})))$ , observe type $f_{t}$ . For each policy $\\pi^{(\\omega)}\\in\\Pi$ , compute $\\ell_{t}[\\pi^{(\\omega)}]:=-u({\\mathbf z}_{t},\\pi^{(\\omega)}({\\mathbf z}_{t}),b_{f_{t}}({\\mathbf z}_{t},\\pi^{(\\omega)}({\\mathbf z}_{t})))$ ) and set $\\begin{array}{r}{\\mathbf{q}_{t+1}[\\pi^{(\\omega)}]=\\exp(-\\eta\\sum_{s=1}^{t}\\ell_{s}[\\pi^{(\\omega)}])}\\end{array}$ , $\\begin{array}{r}{\\mathbf{p}_{t+1}\\big[\\boldsymbol{\\pi}^{(\\omega)}\\big]=\\mathbf{q}_{t+1}\\big[\\boldsymbol{\\pi}^{(\\omega)}\\big]/\\sum_{\\pi^{(\\omega^{\\prime})}\\in\\Pi}\\mathbf{q}_{t+1}\\big[\\boldsymbol{\\pi}^{(\\omega^{\\prime})}\\big]}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "but not $\\mathbf{z}_{t}$ . As was the case in Section 4.1, we consider a relaxed notion of regret which compares the performance of the leader to the best policy in expectation, although now the expectation is taken with respect to the distribution over contexts $\\mathcal{P}$ . ", "page_idx": 7}, {"type": "text", "text": "Definition 4.9 (Expected Contextual Stackelberg Regret, II). Given a distribution over contexts $\\mathcal{P}$ and a sequence of followers $f_{1},\\ldots,f_{T}$ , the leader\u2019s expected contextual Stackeleberg regret is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(T)]:=\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}\\sim\\mathcal{P}}\\left[\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\mathbf{x}_{t},b_{f_{t}}(\\mathbf{z}_{t},\\mathbf{x}_{t}))\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pi^{*}$ is the optimal policy given knowledge of $f_{1},\\ldots,f_{T}$ and $\\mathcal{P}$ . ", "page_idx": 7}, {"type": "text", "text": "Our key insight is that when the sequence of contexts is generated stochastically, to obtain no-regret it suffices to (1) play a standard, off-the-shelf online learning algorithm (e.g. Hedge) over a finite (albeit exponentially-large) set of policies in order to find one which is approximately optimal and then (2) bound the resulting discretization error. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.10. When the sequence of contexts is determined stochastically, the expected utility of any fixed policy $\\pi$ may be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{\\tilde{z}}_{\\mathbf{z}_{1},\\hdots,\\mathbf{z}_{T}}\\left[\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))\\right]=\\sum_{i=1}^{K}\\mathbb{E}_{\\mathbf{z}}[u(\\mathbf{z},\\pi(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi(\\mathbf{z})))]\\left(\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t-1}}[\\mathbb{I}\\{\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})\\}]\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using Lemma 4.10, we now show that it suffices to play Hedge over a finite set of policies $\\Pi$ in order for the leader to obtain no-regret (Algorithm 2). The key step in our analysis is to show that the discretization error is small for our chosen policy class \u03a0.8 For a given weight vector $\\omega\\in\\mathbb{R}^{K}$ , let $\\begin{array}{r}{\\pi^{(\\omega)}(\\mathbf{z}):=\\arg\\operatorname*{max}_{\\mathbf{z}\\in\\mathcal{E}_{\\mathbf{z}}}\\sum_{i=1}^{K}u(\\mathbf{z},\\mathbf{x},b_{\\alpha^{(i)}}(\\mathbf{z},\\mathbf{x}))\\cdot\\omega[i]}\\end{array}$ . For a given set of weight vectors $\\Omega$ , we set $\\Pi$ to be the induced policy class, i.e. $\\Pi:=\\{\\pi^{(\\omega)}\\}_{\\omega\\in\\Omega}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.11. If \u2126= {\u03c9 : \u03c9 \u2208\u2206K, T \u00b7 \u03c9[i] \u2208N, \u2200i \u2208[K]} and \u03b7 = $\\eta=\\sqrt{\\frac{\\log{\\Pi}}{T}}$ , then Algorithm 2 obtains expected contextual Stackelberg regret (Definition 4.9) $\\mathbb{E}[R(T)]=O\\left({\\sqrt{K T\\log T}}+K\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "We conclude by briefly comparing our results with those of the non-contextual Stackelberg game setting of Balcan et al. [5]. In particular, the setting of this subsection may be viewed as a generalization of the setting of Balcan et al. [5] in which the leader and follower utilities at time $t$ also depend on a stochastically-generated context $\\mathbf{z}_{t}$ . When $|\\mathcal{Z}|=1$ , we recover their setting exactly. Under their non-contextual setting, there is only one set of approximate extreme points ${\\mathcal{E}}_{\\mathbf{z}}$ , and so we write the $\\mathcal{E}=\\mathcal{E}_{\\mathbf{z}}$ . Here it suffices to consider the set of constant \u201cpolicies\u201d which always map to one of the (approximate) extreme points in $\\mathcal{E}$ . Plugging this choice of $\\Pi$ into Algorithm 2, we recover their algorithm (and therefore also their regret rates) exactly. ", "page_idx": 7}, {"type": "text", "text": "However, it is also worth noting that more care must be taken to obtain regret guarantees against an adaptive adversary in our setting compared to the non-contextual setting of Balcan et al. [5].9 In particular, we need to bound the discretization error due to considering a finite set of policies, but it is without loss of generality to consider a finite set of mixed strategies in the non-contextual setting. ", "page_idx": 7}, {"type": "image", "img_path": "rPKCrzdqJx/tmp/c53441d3c51c983794fe6eeb086929bc192621c5a42ccd33cb92f9f35122b816.jpg", "img_caption": ["(a) Non-stochastic contexts, stochas- (b) Stochastic contexts, non- (c) Stochastic contexts, stochastic \u2019tic follower types. stochastic follower types. follower types. ", "Figure 2: Cumulative average reward of Algorithm 1, Algorithm 2, and the algorithm of Balcan et al. [5] (which does not take side information into consideration) over five runs in a synthetic data setup. Shaded regions represent one standard deviation. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Simulations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We empirically evaluate the performance of Algorithm 1 and Algorithm 2 on synthetically-generated data. We consider a setup in which $K=5$ , $A=A_{f}=3$ , and the context dimension $d=3$ . Utility functions are linear in both the context and player actions, and are sampled u.a.r. from $[-1,1]^{3\\times3\\times3}$ . ", "page_idx": 8}, {"type": "text", "text": "We compare the cumulative reward of our algorithms to each other and the algorithm of Balcan et al. [5] (which does not leverage side information) as a baseline. We simulate non-stochastic context arrivals in Figure 2a by displaying the same context for $T/4$ time-steps in a row. Follower types are chosen u.a.r. from each of the five follower types. In Figure 2b, contexts are generated stochastically by sampling each component u.a.r. from $[-1,1]$ . Followers are chosen non-stochastically by deterministically cycling over the five types. In Figure $_{2\\mathrm{c}}$ , both contexts and follower types are chosen stochastically. Specifically, contexts are generated as in Figure 2b and follower types are generated as in Figure 2a. ", "page_idx": 8}, {"type": "text", "text": "We find that Algorithm 1 and Algorithm 2 perform similarly across instances, and both significantly out-perform the baseline of Balcan et al. [5]. It would be interesting to find instances for which Algorithm 1 (resp. Algorithm 2) performs poorly whenever followers (resp. contexts) are chosen non-stochastically. ", "page_idx": 8}, {"type": "text", "text": "5 Extension to bandit feedback ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have so far assumed that the leader gets to observe the follower\u2019s type after each round. However this assumption may not always hold in real-world Stackelberg game settings. For example, in cyber security domains it may be hard to deduce the organization responsible for a failed cyber attack. In wildlife protection, a very successful poacher may never be seen by the park rangers. Instead, the leader may only be able to observe the action the follower takes at each round. Following previous work on learning in non-contextual Stackelberg games, we refer to this type of feedback as bandit feedback. What can we say about the leader\u2019s ability to learn under bandit feedback when there is side information? ", "page_idx": 8}, {"type": "text", "text": "While our impossibility result of Section 3 immediately applies to this more challenging setting, our algorithms from Section 4 do not. This is because we can no longer compute quantities such as $\\mathbb{1}\\{f_{t}=$ $\\alpha^{(i)}\\}$ or $b_{f_{t}}(\\mathbf{z}_{t},\\mathbf{x})$ for an arbitrary mixed strategy $\\mathbf{x}$ from just follower $f_{t}$ \u2019s action alone. We still assume that the follower is one of $K$ different types, although $f_{t}$ is now never revealed to the leader. ", "page_idx": 8}, {"type": "text", "text": "We allow ourselves two relaxations when designing learning algorithms which operate under bandit feedback. First, while the leader\u2019s utility function may still depend on the context $\\mathbf{z}_{t}$ , we assume that the follower\u2019s utility is a function of the leader\u2019s mixed strategy $\\mathbf{x}_{t}$ alone, i.e. $u_{f}(\\mathbf{z},\\mathbf{x},a_{f})=$ $u_{f}(\\mathbf{x},a_{f})$ for all $\\textbf{z}\\in\\mathcal{Z}$ . This allows us to drop the dependence on $\\mathbf{z}_{t}$ from both the follower\u2019s best response and the set of approximate extreme points, i.e. $b_{f}(\\mathbf{z},\\mathbf{x})$ becomes $b_{f}(\\mathbf{x})$ and ${\\mathcal{E}}_{\\mathbf{z}}$ becomes $\\mathcal{E}$ . Furthermore, our definitions of contextual follower best-response region (Definition 4.1) and contextual best-response region (Definition 4.2) collapse to their non-contextual counterparts. Depending on the application domain, the assumption that only the leader\u2019s utility depends on the side information may be reasonable. For instance, while an institution would prefer that a server with less traffic is hacked compared to one with more, a hacker might only care about the information hosted on the server (which may not be related to network traffic patterns). Second, we design algorithms with regret guarantees which only hold against a non-adaptive adversary.10 Despite these relaxations, the problem of learning under bandit feedback still remains challenging because of the exponentially large size of $\\mathcal{E}$ . While a natural first step is to estimate $p(b_{f_{t}}(\\mathbf{x})\\bar{\\mathbf{\\xi}}=\\bar{\\mathbf{\\xi}}a_{f})$ ) (i.e. the probability that follower at round $t$ best-responds with action $a_{f}$ when the leader plays mixed strategy $\\mathbf{x}$ ) for all $\\mathbf{x}\\in\\mathcal{E}$ and $a_{f}\\in A_{f}$ , doing so naively would take exponentially-many rounds, due to the size of $\\mathcal{E}$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Building off of results in the non-contextual setting of Balcan et al. [5], we leverage the fact that the leader\u2019s utility for different mixed strategies is not independent. Instead, they are linearly related through the frequency of follower types which take a particular action, given a particular leader mixed strategy. Therefore, it suffices to estimate this linear function (which can be done using as few as $K$ samples) to get an unbiased estimate of $p(b_{f_{t}}(\\mathbf{x})\\,=\\,a_{f})$ for any $\\mathbf{x}\\in{\\mathcal{E}}$ and $a_{f}\\in A_{f}$ . Borrowing from the literature on linear bandits, we use a barycentric spanner [4] to estimate $\\{\\{p(b_{f_{t}}(\\bar{\\mathbf{x}})=a_{f})\\}_{a_{f}\\in A_{f}}\\}_{\\mathbf{x}\\in\\mathcal{E}}$ in both partial adversarial settings we consider. A barycentric spanner for compact vector space $\\mathcal{W}$ is a special basis such that any vector in $\\mathcal{W}$ may be expressed as a linear combination of elements in the basis, with each linear coefficient being in the range $[-1,1]$ . ", "page_idx": 9}, {"type": "text", "text": "In Appendix C.1, we use the property that estimators constructed using barycentric spanners have low variance to show that an explore-then-exploit algorithm achieves $O(K^{2/3}A_{f}^{2/3}T^{2/3}\\log^{1/3}T)$ expected contextual Stackelberg regret in the setting with stochastic follower types and adversarial contexts. Specifically, our algorithm (Algorithm 3) plays a special set of $K$ mixed strategies $N$ times each, then uses barycentric spanners to estimate $\\{p_{t}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a_{f})}\\big)\\}_{a_{f}\\in\\mathcal{A}_{f}}$ for all $\\mathbf{x}\\in\\mathcal{X}$ and $\\mathbf{z}\\in{\\mathcal{Z}}$ after which Algorithm 3 plays greedily like in Section 4.1. ", "page_idx": 9}, {"type": "text", "text": "In Appendix C.2, we use the property that estimators constructed using barycentric spanners are bounded to design a reduction to our algorithm in Section 4.2 which achieves $O(K A_{f}^{1/3}T^{2/3}\\log^{1/3}T)$ expected contextual Stackelberg regret whenever the sequence of contexts is chosen stochastically and the\u221a sequence of follower types is chosen by an adversary. Finally, while it may be possible to obtain $O(\\sqrt{T})^{-}$ regret without using barycentric spanners, this would come at the cost of a linear dependence on $|\\mathcal{E}|$ (and therefore an exponential dependence on $K$ and $A_{f}$ ) in regret. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We initiate the study of Stackelberg games with side information, which despite the presence of side information in many Stackelberg game settings, has not received attention from the community. We focus on the online setting in which the leader faces a sequence of contexts and follower types. We show that when both sequences are chosen adversarially, no-regret learning is not possible even for highly str\u221auctured policy classes. When either sequence is chosen stochastically, we obtain algorithms with $\\tilde{O}(\\sqrt{T})$ regret. We also explore an extension to bandit feedback, in which we obtain $\\tilde{O}(T^{2/3})$ regret in both settings. There are several exciting avenues for future research; we highlight two below. ", "page_idx": 9}, {"type": "text", "text": "1. Intermediate forms of adversary. The two relaxations of the fully adversarial setting that we consider, while natural, rule out the leader learning about the follower\u2019s type from the context. Although we prove that learning is impossible in the fully adversarial setting, our lower bound does not rule out, e.g. settings where the mapping from contexts to follower types has finite Littlestone dimension. It would be interesting to further explore this direction to pin down when no-regret learning is possible. ", "page_idx": 9}, {"type": "text", "text": "2. $\\tilde{O}(T^{1/2})$ regret under bandit feedback. Bernasconi et al. [6] obtain ${\\cal O}(T^{1/2})$ regret when learning in non-contextual Stackelberg games under bandit feedback against an adversarially-chosen sequence of follower types via a reduction to adversarial linear bandits. However, applying similar steps to Bernasconi et al. in our setting results in a reduction to a generalization of the (adversarial) contextual bandit problem for which we are not aware of any regret minimizing algorithm. Nevertheless, we view exploring whether $\\tilde{O}(T^{1/2})$ contextual Stackelberg regret is possible under bandit feedback as a natural and exciting future direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by NSF Grants CCF-1910321, #1763786, and by an NDSEG fellowship. The authors would like to thank the anonymous reviewers for valuable feedback, and Martino Bernasconi, Matteo Castiglioni, and Andrea Celli for helpful discussions surrounding related work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Bo An, Fernando Ord\u00f3\u00f1ez, Milind Tambe, Eric Shieh, Rong Yang, Craig Baldwin, Joseph DiRenzo III, Kathryn Moretti, Ben Maule, and Garrett Meyer. A deployed quantal responsebased patrol planning system for the us coast guard. Interfaces, 43(5):400\u2013420, 2013.   \n[2] Bo An, Milind Tambe, and Arunesh Sinha. Stackelberg security games (ssg) basics and application overview. Improving Homeland Security Decisions, page 485, 2017.   \n[3] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On the convergence of no-regret learning dynamics in time-varying games. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 34f1c2e7ab91b6fa481ad0286a08ad02-Abstract-Conference.html.   \n[4] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. Comput. Syst. Sci., 74(1):97\u2013114, 2008. doi: 10.1016/J.JCSS.2007.04.016. URL https: //doi.org/10.1016/j.jcss.2007.04.016.   \n[5] Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D. Procaccia. Commitment without regrets: Online learning in stackelberg security games. In Tim Roughgarden, Michal Feldman, and Michael Schwarz, editors, Proceedings of the Sixteenth ACM Conference on Economics and Computation, EC \u201915, Portland, OR, USA, June 15-19, 2015, pages 61\u201378. ACM, 2015. doi: 10.1145/2764468.2764478. URL https://doi.org/10.1145/2764468. 2764478.   \n[6] Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Francesco Trov\u00f2, and Nicola Gatti. Optimal rates and efficient algorithms for online bayesian persuasion. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23- 29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2164\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v202/ bernasconi23a.html.   \n[7] Avrim Blum, Nika Haghtalab, and Ariel D. Procaccia. Learning optimal commitment to overcome insecurity. pages 1826\u20131834, 2014. URL https://proceedings.neurips.cc/ paper/2014/hash/cc1aa436277138f61cda703991069eaf-Abstract.html.   \n[8] Matthew Brown, Arunesh Sinha, Aaron Schlenker, and Milind Tambe. One size does not fit all: A game-theoretic approach for dynamically and effectively screening for threats. In Dale Schuurmans and Michael P. Wellman, editors, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pages 425\u2013431. AAAI Press, 2016. doi: 10.1609/AAAI.V30I1.10023. URL https://doi.org/10.1609/aaai. v30i1.10023.   \n[9] Constantinos Daskalakis and Vasilis Syrgkanis. Learning in auctions: Regret is hard, envy is easy. Games Econ. Behav., 134:308\u2013343, 2022. doi: 10.1016/J.GEB.2022.03.001. URL https://doi.org/10.1016/j.geb.2022.03.001.   \n[10] Fei Fang, Peter Stone, and Milind Tambe. When security games go green: Designing defender strategies to prevent poaching and illegal fishing. In Qiang Yang and Michael J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 2589\u20132595. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/367.   \n[11] Anupam Gupta. 14: Online learning: Experts and bandits. 15850: Advanced Algorithms course notes, 2023.   \n[12] Nika Haghtalab. Lecture 12: Introduction to online learning 2. CS6781: Theoretical Foundations of Machine Learning course notes, 2020.   \n[13] Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, and Alexander Wei. Learning in stackelberg games with non-myopic agents. In David M. Pennock, Ilya Segal, and Sven Seuken, editors, EC \u201922: The 23rd ACM Conference on Economics and Computation, Boulder, CO, USA, July 11 - 15, 2022, pages 917\u2013918. ACM, 2022. doi: 10.1145/3490486.3538308. URL https://doi.org/10.1145/3490486.3538308.   \n[14] Nika Haghtalab, Chara Podimata, and Kunhe Yang. Calibrated stackelberg games: Learning optimal commitments against calibrated agents. 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ c23ccf9eedf87e4380e92b75b24955bb-Abstract-Conference.html.   \n[15] Moritz Hardt, Nimrod Megiddo, Christos H. Papadimitriou, and Mary Wootters. Strategic classification. In Madhu Sudan, editor, Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, Cambridge, MA, USA, January 14-16, 2016, pages 111\u2013122. ACM, 2016. doi: 10.1145/2840728.2840730. URL https://doi.org/10.1145/2840728. 2840730.   \n[16] Keegan Harris, Ioannis Anagnostides, Gabriele Farina, Mikhail Khodak, Steven Wu, and Tuomas Sandholm. Meta-learning in games. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id $\\equiv$ uHaWaNhCvZD.   \n[17] Indiana Intelligence Fusion Center Iifc. 8 signs of terrorism, Jul 2022. URL https://www.in. gov/iifc/8-signs-of-terrorism/.   \n[18] Manish Jain, Jason Tsai, James Pita, Christopher Kiekintveld, Shyamsunder Rathi, Milind Tambe, and Fernando Ord\u00f3\u00f1ez. Software assistants for randomized patrol planning for the LAX airport police and the federal air marshal service. Interfaces, 40(4):267\u2013290, 2010. doi: 10.1287/INTE.1100.0505. URL https://doi.org/10.1287/inte.1100.0505.   \n[19] Debarun Kar, Thanh H Nguyen, Fei Fang, Matthew Brown, Arunesh Sinha, Milind Tambe, and Albert Xin Jiang. Trends and applications in stackelberg security games. Handbook of dynamic game theory, pages 1\u201347, 2017.   \n[20] Niklas T. Lauffer, Mahsa Ghasemi, Abolfazl Hashemi, Yagiz Savas, and Ufuk Topcu. No-regret learning in dynamic stackelberg games. IEEE Trans. Autom. Control., 69(3):1418\u20131431, 2024. doi: 10.1109/TAC.2023.3330797. URL https://doi.org/10.1109/TAC.2023.3330797.   \n[21] Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the optimal strategy to commit to. In Marios Mavronicolas and Vicky G. Papadopoulou, editors, Algorithmic Game Theory, Second International Symposium, SAGT 2009, Paphos, Cyprus, October 18-20, 2009. Proceedings, volume 5814 of Lecture Notes in Computer Science, pages 250\u2013262. Springer, 2009. doi: 10.1007/978-3-642-04645-2\\_23. URL https://doi.org/10. 1007/978-3-642-04645-2_23.   \n[22] Yuqian Li, Vincent Conitzer, and Dmytro Korzhyk. Catcher-evader games. pages 329\u2013337, 2016. URL http://www.ijcai.org/Abstract/16/054.   \n[23] Binghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. Learning optimal strategies to commit to. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 2149\u20132156. AAAI Press, 2019. doi: 10.1609/AAAI.V33I01.33012149. URL https://doi.org/10.1609/aaai.v33i01. 33012149.   \n[24] Alexander Rakhlin and Karthik Sridharan. BISTRO: an efficient relaxation-based method for contextual bandits. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 1977\u2013 1985. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/rakhlin16.html.   \n[25] Pier Giuseppe Sessa, Ilija Bogunovic, Andreas Krause, and Maryam Kamgarpour. Contextual games: Multi-agent learning with side information. 2020. URL https://proceedings. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "neurips.cc/paper/2020/hash/f9afa97535cf7c8789a1c50a2cd83787-Abstract. ", "page_idx": 12}, {"type": "text", "text": "html.   \n[26] Arunesh Sinha, Debarun Kar, and Milind Tambe. Learning adversary behavior in security games: A PAC model perspective. pages 214\u2013222, 2016. URL http://dl.acm.org/citation.cfm? id=2936958.   \n[27] Arunesh Sinha, Fei Fang, Bo An, Christopher Kiekintveld, and Milind Tambe. Stackelberg security games: Looking beyond a decade of success. In J\u00e9r\u00f4me Lang, editor, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 5494\u20135501. ijcai.org, 2018. doi: 10.24963/IJCAI.2018/775. URL https://doi.org/10.24963/ijcai.2018/775.   \n[28] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E. Schapire. Efficient algorithms for adversarial contextual learning. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 2159\u20132168. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/ syrgkanis16.html.   \n[29] Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E. Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. pages 3135\u20133143, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ dfa92d8f817e5b08fcaafb50d03763cf-Abstract.html.   \n[30] Heinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media, 1934.   \n[31] Bernhard von Stengel and Shmuel Zamir. Leadership games with convex strategy sets. Games Econ. Behav., 69(2):446\u2013457, 2010. doi: 10.1016/J.GEB.2009.11.008. URL https://doi. org/10.1016/j.geb.2009.11.008.   \n[32] Jin Zhang and Qian Zhang. Stackelberg game for utility-based cooperative cognitiveradio networks. In Edward W. Knightly, Carla-Fabiana Chiasserini, and Xiaojun Lin, editors, Proceedings of the 10th ACM Interational Symposium on Mobile Ad Hoc Networking and Computing, MobiHoc 2009, New Orleans, LA, USA, May 18-21, 2009, pages 23\u201332. ACM, 2009. doi: 10.1145/1530748.1530753. URL https://doi.org/10.1145/1530748.1530753.   \n[33] Mengxiao Zhang, Peng Zhao, Haipeng Luo, and Zhi-Hua Zhou. No-regret learning in timevarying zero-sum games. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 26772\u201326808. PMLR, 2022. URL https://proceedings.mlr. press/v162/zhang22an.html. ", "page_idx": 12}, {"type": "text", "text": "A Appendix for Section 3: On the impossibility of fully adversarial no-regret learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 3.2. If an adversary can choose both the sequence of contexts $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ and the sequence of followers $f_{1},\\ldots,f_{T}$ , no algorithm can achieve better than $\\Omega(T)$ contextual Stackelberg regret in expectation over the internal randomness of the algorithm, even when $\\pi^{*}$ is restricted to come from the set of linear thresholding functions. ", "page_idx": 13}, {"type": "text", "text": "Proof. We proceed via proof by contradiction. Assume that there exists an algorithm ALG which achieves $o(T)$ contextual Stackelberg regret against an adversarially-chosen sequence of contexts and follower types. Note that at every time-step, ALG takes as input a context $\\mathbf{z}_{t}$ and produces a mixed strategy $\\mathbf{x}_{t}$ . ", "page_idx": 13}, {"type": "text", "text": "We now describe the family of contextual Stackelberg game instances we reduce to. Consider the setting in which there are two follower types $\\mathbf{\\chi}_{\\alpha}^{(1)}$ and $\\alpha^{(2)}$ ) and two leader/follower actions $(\\mathcal{A}=\\mathcal{A}_{f}=\\{a_{1},a_{2}\\})$ ). Suppose that the context space is of the form $\\mathcal{Z}=[0,1]$ , and that regardless of the realized context or leader mixed strategy, the best-response of follower type $\\alpha^{(1)}$ is to play action $a_{1}$ $(b_{\\alpha^{(1)}}({\\bf z},{\\bf x})=a_{1}$ , $\\forall\\mathbf{z}\\in\\mathcal{Z},\\mathbf{x}\\in\\mathcal{X})$ and the best-response of follower type $\\alpha^{(2)}$ is to play action $a_{2}$ $(b_{\\alpha^{(2)}}(\\mathbf{z},\\mathbf{x})=a_{2}$ , $\\forall\\mathbf{z}\\in\\mathcal{Z},\\mathbf{x}\\in\\mathcal{X})$ . Since the follower\u2019s best-response does not depend on the leader\u2019s mixed strategy or the context, we use the shorthand $b_{f_{t}}:=b_{f_{t}}(\\mathbf{z}_{t},\\mathbf{x}_{t})$ . Finally, suppose that the leader\u2019s utility function is given by $u(\\mathbf{z},a_{l},a_{f})=\\mathbb{1}\\{a_{l}\\,\\overset{\\cdot\\,.\\,}{=}\\,a_{f}\\}$ . Note that this is a special case of our general setting (described in Section 2). ", "page_idx": 13}, {"type": "text", "text": "The reduction from online linear thresholding proceeds as follows. In each round $t\\in[T]$ , ", "page_idx": 13}, {"type": "text", "text": "1. Given a point $\\omega_{t}\\in[0,1]$ , we give the context $z_{t}=\\omega_{t}$ as input to ALG. ", "page_idx": 13}, {"type": "text", "text": "2. In return, we receive mixed strategy $\\mathbf{x}_{t}\\in\\Delta(\\{a_{1},a_{2}\\})$ from ALG. We set $g_{t}=\\mathbf{x}_{t}[1]$ .1 ", "page_idx": 13}, {"type": "text", "text": "3. Play according to $g_{t}$ , and receive label $y_{t}$ and utility $u_{\\mathrm{OLT}}\\big(\\omega_{t},g_{t}\\big)$ from Nature. Give follower type ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{t}={\\binom{\\alpha^{(1)}\\ \\mathrm{if}\\ y_{t}=1}{\\alpha^{(2)}\\ \\mathrm{if}\\ y_{t}=-1}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Observe that under this reduction, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pi^{*}(\\mathbf{z})=\\left\\{{\\big[}1~0{\\big]}^{\\top}~{\\mathrm{~if~}}~z>s~{\\mathrm{~and}}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "since if $z>s$ , $f_{t}=\\alpha^{(1)}$ and otherwise $f_{t}=\\alpha^{(0)}$ . By playing according to $\\pi^{*}$ , we can ensure that $u(z_{t},\\pi^{*}(z_{t}),b_{f_{t}})\\,=\\,1$ for all $t\\,\\in\\,[T]$ . $\\pi^{*}$ must then be optimal, because 1 is the largest possible per-round utility that the leader can receive. ", "page_idx": 13}, {"type": "text", "text": "Since ALG achieves no-contextual-Stackelberg-regret, we know by Definition 2.3 that ", "page_idx": 13}, {"type": "equation", "text": "$$\nR(T)=\\sum_{t=1}^{T}u({\\bf z}_{t},\\pi^{*}({\\bf z}_{t}),b_{f_{t}})-u({\\bf z}_{t},{\\bf x}_{t},b_{f_{t}})=o(T).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To conclude, it suffices to show that $R_{\\mathrm{OLT}}(T)=o(T)$ using Equation (2) and Equation (3). Applying Equation (2), we see that ", "page_idx": 13}, {"type": "equation", "text": "$$\nR(T)=T-\\sum_{t=1}^{T}(\\mathbf{x}_{t}[1]\\cdot\\mathbb{1}\\{b_{f_{t}}=a_{1}\\}+\\mathbf{x}_{t}[2]\\cdot\\mathbb{1}\\{b_{f_{t}}=a_{2}\\}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By construction, $\\mathbb{1}\\{b_{f_{t}}\\,=\\,a_{1}\\}\\,=\\,\\mathbb{1}\\{y_{t}\\,=\\,1\\}$ , $\\mathbb{1}\\{b_{f_{t}}\\,=\\,a_{2}\\}\\,=\\,\\mathbb{1}\\{y_{t}\\,=\\,-1\\}$ , $\\mathbf{x}_{t}[1]\\;=\\;g_{t}$ , and $\\mathbf{x}_{t}[2]=1-g_{t}$ . Substituting this into Equation (4), we see that ", "page_idx": 13}, {"type": "equation", "text": "$$\nR(T)=T-\\sum_{t=1}^{T}(g_{t}\\cdot\\mathbb{1}\\{y_{t}=1\\}+(1-g_{t})\\cdot\\mathbb{1}\\{y_{t}=-1\\})=:R_{\\mathrm{OLT}}(T).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "11Observe that xt[2] = 1 \u2212gt. ", "page_idx": 13}, {"type": "text", "text": "By Equation (3) and Equation (5), we can conclude that $R_{\\mathrm{OLT}}(T)\\,=\\,o(T)$ . However, this is a contradiction since no no-regret learning algorithm exists for the online linear thresholding problem by Lemma 3.1. Therefore it must not be possible to achieve no-contextual-Stackelberg-regret whenever the sequence of contexts and follower types is chosen by an adversary. ", "page_idx": 14}, {"type": "text", "text": "B Appendix for Section 4: Limiting the power of the adversary ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 4.4. For any sequence of followers $f_{1},\\ldots f_{T}$ and any leader policy $\\pi$ , there exists a policy $\\pi^{(\\mathcal{E})}\\,:\\,\\mathcal{Z}\\,\\rightarrow\\,\\cup_{{\\bf z}\\in\\mathcal{Z}}\\dot{\\mathcal{E}}_{{\\bf z}}$ that, when given context $\\mathbf{z}$ , plays a mixed strategy in ${\\mathcal{E}}_{\\mathbf{z}}$ and guarantees that $\\begin{array}{r}{\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))\\leq1.}\\end{array}$ . Moreover, the same resu lt holds in expectation over any distribution over follower types $\\mathcal{F}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Observe that for any $\\mathbf{z}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi^{*}(\\mathbf{z}):=\\arg\\displaystyle\\operatorname*{max}_{\\mathbf{x}\\in\\Delta(\\mathcal{A}_{l})}\\sum_{t=1}^{T}\\mathbb{1}\\{\\mathbf{z}_{t}=\\mathbf{z}\\}\\sum_{a_{l}\\in\\mathcal{A}_{l}}\\mathbf{x}[a_{l}]\\cdot u(\\mathbf{z},a_{l},b_{f_{t}}(\\mathbf{z},\\mathbf{x}))}\\\\ &{\\quad=\\arg\\displaystyle\\operatorname*{max}_{\\mathbf{x}\\in\\Delta(\\mathcal{A}_{l})}\\sum_{i=1}^{K}\\sum_{a_{l}\\in\\mathcal{A}_{l}}\\mathbf{x}[a_{l}]\\cdot u(\\mathbf{z},a_{l},b_{\\alpha^{(i)}}(\\mathbf{z},\\mathbf{x}))\\sum_{t=1}^{T}\\mathbb{1}\\{\\mathbf{z}_{t}=\\mathbf{z},f_{t}=\\alpha^{(i)}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The solution to the above optimization may be obtained by first solving ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{x}_{a_{1:K}}(\\mathbf{z})=\\arg\\operatorname*{max}_{\\mathbf{x}\\in\\Delta(A_{l})}\\sum_{i=1}^{K}\\sum_{a_{l}\\in A_{l}}\\mathbf{x}[a_{l}]\\cdot u(\\mathbf{z},a_{l},a^{(i)})\\cdot\\sum_{t=1}^{T}\\mathbb{1}\\{\\mathbf{z}_{t}=\\mathbf{z},f_{t}=\\alpha^{(i)}\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for every possible setting of $a^{(1)},\\ldots,a^{(K)}$ , and then taking the maximum of all feasible solutions. Since Equation (6) is an optimization over contextual best-response region $\\mathcal{X}_{\\mathbf{z}}(a^{(1)},\\ldots,a^{(K)})$ and all contextual best-response regions are convex polytopes, $\\pi^{*}\\bar{(\\mathbf{z})}$ will be an extreme point of some contextual best-response region, although it may not be attained. Overloading notation, let $\\mathcal{X}_{\\mathbf{z}}(\\pi^{*}(\\mathbf{z}))$ denote the contextual best-response region corresponding to $\\pi^{*}(\\mathbf{z})$ , i.e., $\\pi^{\\ast}(\\mathbf{z})\\in\\mathcal{X}_{\\mathbf{z}}(\\pi^{\\ast}(\\mathbf{z}))$ . Since for a fixed context $\\textbf{z}\\in\\mathcal{Z}$ the leader\u2019s utility is a linear function of $\\mathbf{x}$ over the convex polytope $\\mathcal{X}_{\\mathbf{z}}(\\pi^{*}(\\mathbf{z}))$ , there exists a point $\\mathbf{x}(\\mathbf{z})\\in\\mathrm{cl}(\\mathcal{X}_{\\mathbf{z}}(\\pi^{*}(\\mathbf{z})))$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}u(\\mathbf{z},\\mathbf{x}(\\mathbf{z}),b_{f_{t}}(\\mathbf{z},\\pi^{*}(\\mathbf{z})))\\cdot\\mathbb{1}\\left\\{\\mathbf{z}_{t}=\\mathbf{z}\\right\\}\\geq\\sum_{t=1}^{T}u(\\mathbf{z},\\pi^{*}(\\mathbf{z}),b_{f_{t}}(\\mathbf{z},\\pi^{*}(\\mathbf{z})))\\cdot\\mathbb{1}\\left\\{\\mathbf{z}_{t}=\\mathbf{z}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\mathbf{x}^{\\prime}(\\mathbf{z})$ denote the corresponding point in ${\\mathcal{E}}_{\\mathbf{z}}$ such that $\\|\\mathbf{x}^{\\prime}(\\mathbf{z})-\\mathbf{x}(\\mathbf{z})\\|_{1}\\leq\\delta$ . (Such a point will always exist by Definition 4.3.) Since $u\\in[0,1]$ and is linear in $\\mathbf{x}$ for a fixed context and follower best-response, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\sum_{t=1}^{T}u(\\mathbf z,\\mathbf x^{\\prime}(\\mathbf z),b_{f_{t}}(\\mathbf z,\\mathbf x^{\\prime}(\\mathbf z)))\\cdot\\mathbb{1}\\{\\mathbf z_{t}=\\mathbf z\\}=\\displaystyle\\sum_{t=1}^{T}u(\\mathbf z,\\mathbf x^{\\prime}(\\mathbf z),b_{f_{t}}(\\mathbf z,\\pi^{*}(\\mathbf z)))\\cdot\\mathbb{1}\\{\\mathbf z_{t}=\\mathbf z\\}}&{}&\\\\ {\\displaystyle\\ge\\displaystyle\\sum_{t=1}^{T}(u(\\mathbf z,\\mathbf x(\\mathbf z),b_{f_{t}}(\\mathbf z,\\pi^{*}(\\mathbf z)))-\\delta)\\cdot\\mathbb{1}\\{\\mathbf z_{t}=\\mathbf z\\}}&{}&\\\\ {\\displaystyle}&{\\ge\\displaystyle\\sum_{t=1}^{T}(u(\\mathbf z,\\pi^{*}(\\mathbf z),b_{f_{t}}(\\mathbf z,\\pi^{*}(\\mathbf z)))-\\delta)\\cdot\\mathbb{1}\\{\\mathbf z_{t}=\\mathbf z\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing over all unique $\\mathbf{z}$ encountered by the algorithm over $T$ time-steps, we obtain the desired result for the policy $\\pi^{(\\bar{\\mathcal{E}})}$ which plays mixed strategy $\\boldsymbol{\\pi}^{(\\mathcal{E})}(\\mathbf{z})=\\mathbf{x}^{\\prime}(\\mathbf{z})$ when given context $\\mathbf{z}$ . Finally, observe that the same line of reasoning holds whenever we are interested in the optimal policy $i n$ expectation with respect to some distribution $\\mathcal{F}$ over followers, as is the case in, e.g. Section 4.1 (with $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{1}\\{\\mathbf{z}_{t}=\\mathbf{z},f_{t}=\\alpha^{(i)}\\}}\\end{array}$ replaced with $\\mathbb{P}(f=\\alpha^{(i)})^{\\cdot}$ ). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.1 Section 4.1: Stochastic follower types and adversarial contexts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 4.6. Let $\\mathbf{p}(\\mathbf{z},\\mathbf{x}):=[p(b_{f_{t}}(\\mathbf{z},\\mathbf{x})=a_{f})]_{a_{f}\\in\\mathcal{A}_{f}}$ and $\\widehat{\\mathbf{p}}_{t}(\\mathbf{z},\\mathbf{x}):=[\\widehat{p}_{t}(b_{f_{t}}(\\mathbf{z},\\mathbf{x})=a_{f})]_{a_{f}\\in\\mathcal{A}_{f}}$ . The expected contextual Stackelberg regret (Definition 4.5) o f  Algorithm 1  satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Im[R(T)]\\leq1+2\\sum_{t=1}^{T}\\mathbb{E}_{f_{1},\\ldots,f_{t-1}}[\\mathrm{TV}(\\mathbf{p}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))+\\mathrm{TV}(\\mathbf{p}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})))].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 4.6. For any $\\mathbf{z}\\in{\\mathcal{Z}}$ and $t\\in[T]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{E}}_{t}[u(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z})))]\\leq\\mathbb{E}_{t}[u(\\mathbf{z},\\pi_{t}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi_{t}(\\mathbf{z})))]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\widehat{\\mathbb{E}}_{t}[u(\\mathbf{z},\\pi_{t}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi_{t}(\\mathbf{z})))]-\\mathbb{E}_{t}[u(\\mathbf{z},\\pi_{t}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi_{t}(\\mathbf{z})))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since utilities are bounded in $[0,1]$ and the expectations $\\mathbb{E}_{t}$ and ${\\widehat{\\mathbb{E}}}_{t}$ are taken with respect to $\\mathbf{p}$ and $\\widehat{\\mathbf{p}}_{t}$ respectively, we can upper-bound $\\widehat{\\mathbb{E}}_{t}[u(\\mathbf{z},\\pi_{t}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi_{t}(\\mathbf{z})))]-\\mathbb{E}_{t}[u(\\mathbf{z},\\pi_{t}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi_{t}(\\mathbf{z})))]$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int|d\\widehat{p}_{t}(\\mathbf{z},\\pi_{t}(\\mathbf{z}))-d p_{t}(\\mathbf{z},\\pi_{t}(\\mathbf{z}))|=2\\mathrm{TV}(\\mathbf{p}(\\mathbf{z},\\pi_{t}(\\mathbf{z})),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z},\\pi_{t}(\\mathbf{z}))).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Putting everything together, we get that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\mathrm{\\i}}_{t}[u(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z})))]\\le\\mathbb{E}_{t}[u(\\mathbf{z},\\pi_{t}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi_{t}(\\mathbf{z})))]+2\\mathrm{TV}(\\mathbf{p}(\\mathbf{z},\\pi_{t}(\\mathbf{z})),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z},\\pi_{t}(\\mathbf{z}))).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We now use this fact to bound the expected regret. By Lemma 4.4, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R(T)]\\leq1+\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{f_{1},\\dots,f_{t}}[u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})))]}\\\\ &{\\qquad\\quad\\leq1+\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{f_{1},\\dots,f_{t-1}}[\\mathbb{E}_{t}[u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))]}\\\\ &{\\qquad\\quad-\\displaystyle\\widehat{\\mathbb{E}}_{t}[u(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{f_{t}}(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z})))]+2\\mathrm{TV}(\\mathbf{p}(\\mathbf{z},\\pi_{t}(\\mathbf{z})),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z},\\pi_{t}(\\mathbf{z})))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By repeating the same steps as above, we can upper-bound ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t}[u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))]-\\widehat{\\mathbb{E}}_{t}[u(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{f}(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z})))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by $2\\mathrm{TV}\\big(\\mathbf{p}\\big(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}\\big),\\widehat{\\mathbf{p}}_{t}\\big(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}\\big)\\big)$ . This gets us the desired result. ", "page_idx": 15}, {"type": "text", "text": "Corollary 4.7. If $\\widehat{\\mathbf{p}}_{t}\\,=\\,\\{\\widehat{p}_{t}(f_{t}\\,=\\,\\alpha^{(i)})\\}_{i\\in[K]},$ , $\\begin{array}{r}{\\widehat{p}_{t+1}(f\\,=\\,\\alpha^{(i)})\\,=\\,\\frac{1}{t}\\sum_{\\tau=1}^{t}\\mathbb{1}\\{f_{\\tau}\\,=\\,\\alpha^{(i)}\\},}\\end{array}$ , and $\\widehat{p}_{1}(f=\\alpha^{(i)})=\\textstyle\\frac{1}{K}$ for $i\\in[K]$ , then the regret of Algorithm $^{\\,l}$ satisfies $\\mathbb{E}[R(T)]=O(K{\\sqrt{T\\log(T)}})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. For $t\\geq2$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{TV}({\\bf p}({\\bf z},{\\bf x}),\\widehat{{\\bf p}}_{t}({\\bf z},{\\bf x}))=\\frac{1}{2}\\sum_{i=1}^{K}|p_{t}(f_{t}=\\alpha^{(i)})-\\widehat{p}_{t}(f_{t}=\\alpha^{(i)})|\\;\\;}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~=\\frac{1}{2}\\sum_{i=1}^{K}\\frac{1}{t-1}\\left|\\sum_{\\tau=1}^{t-1}1\\{f_{\\tau}=\\alpha^{(i)}\\}-\\mathbb{E}_{f_{\\tau}}[1\\{f_{\\tau}=\\alpha^{(i)}\\}]\\right|}\\;~}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any $\\mathbf{z}\\in{\\mathcal{Z}}$ and $\\mathbf{x}\\in\\mathcal{X}$ . By Hoeffding\u2019s inequality, we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{t-1}\\left|\\sum_{\\tau=1}^{t-1}\\mathbb{1}\\{f_{\\tau=\\alpha^{(i)}}\\}-\\mathbb{E}_{f_{\\tau}}[\\mathbb{1}\\{f_{\\tau=\\alpha^{(i)}}\\}]\\right|\\le2\\sqrt{\\frac{\\log(2T)}{t-1}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "simultaneously for all $t\\in[T]$ and $i\\in[K]$ , with probability at least $\\textstyle1-{\\frac{1}{T^{2}}}$ . Dropping the dependence of $\\mathbf{p}$ , $\\widehat{\\mathbf{p}}_{t}$ on $\\mathbf{z}$ and $\\mathbf{x}$ , we can conclude that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{f_{1},\\ldots,f_{t-1}}[\\mathrm{TV}(\\mathbf{p},\\widehat{\\mathbf{p}}_{t})]\\leq K\\sqrt{\\frac{\\log(2T)}{t-1}}+\\frac{1}{2T}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(since $K\\le T)$ ), and so ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(T)]\\leq1+4\\sum_{t=1}^{T}K\\left(\\sqrt{\\frac{\\log(2T)}{t-1}}+\\frac{1}{2T}\\right)=O\\left(K\\sqrt{T\\log(T)}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Corollary 4.8. If $\\begin{array}{r l r}{\\widehat{\\bf p}_{t}({\\bf z},{\\bf x})}&{{}}&{=}&{\\{\\widehat{p}_{t}\\big(\\mathbb{1}_{(\\sigma^{({\\bf z},{\\bf x})}=a_{f})}\\big)\\big\\}_{a_{f}\\in{\\cal A}_{f}},\\ \\ \\ \\ \\widehat{p}_{t+1}\\big(\\mathbb{1}_{(\\sigma^{({\\bf z},{\\bf x})}=a)}\\big)}\\end{array}$ $=$ $\\begin{array}{r}{\\frac{1}{t}\\sum_{\\tau=1}^{t}\\mathbb{1}\\big\\{b_{f_{\\tau}}(\\mathbf{z},\\mathbf{x})\\big.\\ =\\ a\\big\\},}\\end{array}$ , and $\\begin{array}{r}{\\widehat{p}_{1}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a)}\\big)\\ =\\ \\frac{1}{A_{f}}}\\end{array}$ for $a_{f}~\\in~{\\mathcal{A}}_{f}$ , then the regret of $A l.$ - gorithm $^{\\,l}$ satisfies $\\mathbb{E}[R(T)]=O(A_{f}{\\sqrt{T\\log(T)}})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. For $t\\geq2$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\mathbf{p}(\\mathbf{z},\\mathbf{x}),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z},\\mathbf{x}))=\\displaystyle\\frac{1}{2}\\sum_{a\\in A_{f}}|p_{t}(\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a)})-\\widehat{p}_{t}(\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a)})|}\\\\ &{\\phantom{\\quad\\quad}=\\displaystyle\\frac{1}{2}\\sum_{a\\in A_{f}}|p(\\mathbb{1}_{(\\sigma^{(\\mathbf{z},\\mathbf{x})}=a)})-\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbb{1}\\{b_{f_{\\tau}}(\\mathbf{z},\\mathbf{x})=a\\}|}\\\\ &{\\phantom{\\quad\\quad}=\\displaystyle\\frac{1}{2}\\sum_{a\\in A_{f}}\\frac{1}{t-1}\\left|\\sum_{\\tau=1}^{t-1}\\mathbb{1}\\{b_{f_{\\tau}}(\\mathbf{z},\\mathbf{x})=a\\}-\\mathbb{E}_{f_{\\tau}}[\\mathbb{1}\\{b_{f_{\\tau}}(\\mathbf{z},\\mathbf{x})=a\\}]\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any $\\mathbf{z}\\in\\mathcal{Z},\\mathbf{x}\\in\\mathcal{X}$ . By Hoeffding\u2019s inequality, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{t-1}\\left|\\sum_{\\tau=1}^{t-1}\\mathbb{1}\\big\\{b_{f_{\\tau}}(\\mathbf{z},\\mathbf{x})=a\\big\\}-\\mathbb{E}_{f_{\\tau}}\\big[\\mathbb{1}\\big\\{b_{f_{\\tau}}(\\mathbf{z},\\mathbf{x})=a\\big\\}\\big]\\right|\\leq2\\sqrt{\\frac{\\log(2T)}{t-1}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "simultaneously for all $t\\in[T]$ and $i\\in[K]$ , with probability at least $\\textstyle1-{\\frac{1}{T^{2}}}$ . Using this fact, we can conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{f_{1},\\ldots,f_{t-1}}[\\mathrm{TV}(\\mathbf{p}(\\mathbf{z},\\mathbf{x}),\\widehat{\\mathbf{p}}_{t}(\\mathbf{z},\\mathbf{x}))]\\leq A_{f}\\sqrt{\\frac{\\log(2T)}{t-1}}+\\frac{1}{2T}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(since $K\\le T)$ ) and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R(T)]\\leq1+4\\displaystyle\\sum_{t=1}^{T}\\left(A_{f}\\sqrt{\\frac{\\log(2T)}{t-1}}+\\frac{1}{2T}\\right)}\\\\ &{\\qquad\\qquad\\leq3+4A_{f}\\sqrt{\\log(2T)}\\displaystyle\\int_{t=0}^{T}\\frac{1}{\\sqrt{t}}d t}\\\\ &{\\qquad=O\\left(A_{f}\\sqrt{T\\log(T)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Section 4.2: Stochastic contexts and adversarial follower types ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following regret guarantee for Hedge is a well-known result. (See, e.g. Gupta [11].) ", "page_idx": 16}, {"type": "text", "text": "Lemma B.1. Hedge enjoys expected regret rate $O({\\sqrt{T\\log n}})$ when there are n actions, the learning rate is chosen to be $\\begin{array}{r}{\\eta\\,=\\,\\sqrt{\\frac{\\log n}{T}}}\\end{array}$ , and the sequence of utilities for each arm are chosen by an adversary. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4.10. When the sequence of contexts is determined stochastically, the expected utility of any fixed policy $\\pi$ may be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{1}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}}\\left[\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))\\right]=\\sum_{i=1}^{K}\\mathbb{E}_{\\mathbf{z}}[u(\\mathbf{z},\\pi(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi(\\mathbf{z})))]\\left(\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t-1}}[\\mathbb{I}\\{\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})\\}]\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. For any fixed policy $\\pi$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))\\Bigg]=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t}}\\left[\\displaystyle\\sum_{i=1}^{K}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{\\alpha^{(i)}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))\\mathbb{1}\\{f_{t}=\\alpha^{(i)}\\}\\right]}\\\\ {=\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\mathbb{E}_{\\mathbf{z}_{t}}[u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{\\alpha^{(i)}}(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t})))]\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t-1}}[\\mathbb{1}\\{f_{t}=\\alpha^{(i)}\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second line uses the fact that $f_{t}$ cannot depend on $\\mathbf{z}_{t}$ , and the result follows from the fact that $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ are i.i.d. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Theorem 4.11. If \u2126= {\u03c9 : \u03c9 \u2208\u2206K, T \u00b7 \u03c9[i] \u2208N, \u2200i \u2208[K]} and \u03b7 = $\\eta=\\sqrt{\\frac{\\log{\\Pi}}{T}}$ logT \u03a0, then Algorithm 2 obtains expected contextual Stackelberg regret (Definition 4.9) $\\mathbb{E}[R(T)]=O\\left({\\sqrt{K T\\log T}}+K\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. By Lemma 4.4, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R(T)]=\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})))\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})))\\right]+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\pi^{(\\omega^{*})}$ denote the optimal-in-hindsight policy in $\\Pi$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}[R(T)]\\leq\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})))\\right]}\\\\ &{\\qquad\\qquad+\\left.\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t})))\\right]+\\mathfrak{I}(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t})))\\right]+}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To conclude, it suffices to bound the discretization error, as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{T}}\\left[\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\omega^{*})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t})))\\right]\\le O\\left(\\sqrt{T\\log|\\Pi(t,\\pi_{t}(\\textbf{z}_{t})|)}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which follows from applying the standard regret guarantee of Hedge (Lemma B.1 in the Appendix). Applying Lemma 4.10, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi_{\\Xi_{1},\\dots,\\Xi_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E}^{*})}(\\mathbf{z}_{t}),b_{f_{t}}(\\mathbf{z}_{t},\\pi^{(\\mathcal{E}^{*})}(\\mathbf{z}_{t})))\\right]}\\\\ &{=\\displaystyle\\sum_{i=1}^{K}(\\mathbb{E}_{\\mathbf{z}}[u(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z})))-u(\\mathbf{z},\\pi^{(\\mathcal{E}^{*})}(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi^{(\\mathcal{E}^{*})}(\\mathbf{z})))])\\left(\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t-1}}[\\mathbb{I}\\{\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z})\\}]\\right)}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}_{\\mathbf{z}}[u(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z})))]\\cdot\\left(\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t-1}}[\\mathbb{I}\\{f_{t}=\\alpha^{(i)}\\}-T\\cdot\\omega^{*}[i]\\right)}\\\\ &{+\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}_{\\mathbf{z}}[u(\\mathbf{z},\\pi^{(\\mathcal{E}^{*})}(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi^{(\\mathcal{E}^{*})}(\\mathbf{z})))]\\cdot\\left(T\\cdot\\omega^{*}[i]-\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t-1}}[\\mathbb{I \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the inequality follows from adding and subtracting $\\begin{array}{r}{\\sum_{i=1}^{K}\\mathbb{E}_{\\mathbf{z}}\\big[u\\big(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}))\\big)\\big]\\,.}\\end{array}$ $T\\cdot\\omega^{*}[i]$ and $\\begin{array}{r}{\\sum_{i=1}^{K}\\mathbb{E}_{\\mathbf{z}}\\big[u(\\mathbf{z},\\pi^{(\\omega^{*})}(\\mathbf{z}),b_{\\alpha^{(i)}}(\\mathbf{z},\\pi^{(\\omega^{*})}(\\mathbf{z})))\\big]\\cdot\\bar{T^{*}}\\cdot\\omega^{*}[i]}\\end{array}$ . Finally, we can upper-bound the discretiza tion error by ", "page_idx": 17}, {"type": "equation", "text": "$$\n2\\sum_{i=1}^{K}\\left|T\\cdot\\omega^{*}[i]-\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{t-1}}[\\mathbb{1}\\{f_{t}=\\alpha^{(i)}\\}]\\right|\\le2K\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by using the fact that the sender\u2019s utility is bounded in $[0,1]$ . Piecing everything together and observing that $|\\Pi|\\leq T^{K}$ gives us the desired regret guarantee. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C Appendix for Section 5: Extension to bandit feedback ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Stochastic follower types and adversarial contexts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall from Section 4.1 that $\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\in\\{0,1\\}^{K}$ is the indicator vector whose $i$ -th component is $\\mathbb{1}\\{\\sigma^{(\\mathbf{x})}(\\alpha^{(i)})=a_{f}\\}$ , i.e. the indicator that a follower of type $\\alpha^{(i)}$ best-responds to mixed strategy $\\mathbf{x}$ by playing action $a_{f}$ . For any fixed policy $\\pi$ , we can write the sender\u2019s expected utility in round $t$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{f_{t}}\\left[u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),b_{f_{t}}(\\pi(\\mathbf{z}_{t})))\\right]=\\displaystyle\\sum_{a_{f}\\in\\mathcal{A}_{f}}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),a_{f})\\cdot p(b_{f}(\\pi(\\mathbf{z}_{t}))=a_{f})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{a_{f}\\in\\mathcal{A}_{f}}u(\\mathbf{z}_{t},\\pi(\\mathbf{z}_{t}),a_{f})\\cdot p(\\mathbb{1}_{(\\sigma^{(\\pi(\\mathbf{z}_{t}))}=a_{f})})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $p(b_{f}(\\pi(\\mathbf{z}))=a_{f}):=\\mathbb{E}_{f\\sim\\mathcal{F}}[\\mathbb{1}\\{b_{f}(\\mathbf{z})=a_{f}\\}]$ , the first line follows from the assumption of a non-adaptive adversary, the second line follows from the fact that $f_{1},\\ldots,f_{T}$ are drawn i.i.d., and ", "page_idx": 18}, {"type": "equation", "text": "$$\np(b_{f}(\\pi(\\mathbf{z}))=a_{f})=p(\\mathbb{1}_{(\\sigma^{(\\pi(\\mathbf{z}))}=a_{f})}):=\\sum_{i=1}^{K}\\mathbb{1}\\{b_{\\alpha^{(i)}}(\\pi(\\mathbf{z}))=a_{f}\\}\\cdot\\mathbb{P}(f=\\alpha^{(i)}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbb{P}(f=\\alpha^{(i)})$ is the probability that follower $f$ is of type $\\alpha^{(i)}$ . Note that $p\\big(\\mathbb{1}_{(\\sigma^{(\\pi(\\mathbf{z}_{t}))}=a_{f})}\\big)$ (and therefore Eft[u(zt, \u03c0(zt), bft(\u03c0(zt)))]) is linear in 1(\u03c3(\u03c0(zt))=af ). ", "page_idx": 18}, {"type": "text", "text": "Given this reformulation, a natural approach is to estimate $p\\big(\\mathbb{1}_{(\\sigma^{(\\pi(\\mathbf{z}_{t}))}=a_{f})}\\big)$ as $\\widehat{p}\\big(\\mathbb{1}_{(\\sigma^{(\\pi(\\mathbf{z}_{t}))}=a_{f})}\\big)$ and act greedily with respect to our estimate, like we did in Section 4.1. To do so, we define the set $\\mathcal{W}:=\\{\\mathbb{1}_{(\\sigma=a_{f})}\\ |\\ \\forall\\bar{a}_{f}\\in\\mathcal{A}_{f},\\ \\sigma\\in\\Sigma\\}$ and estimate $p(\\mathbf{b})$ for every element $\\mathbf{b}$ in the barycentric spanner $B:=\\{\\mathbf{b}^{(1)},\\dots,\\mathbf{b}^{(K)}\\}$ of $\\mathcal{W}$ .12 ", "page_idx": 18}, {"type": "text", "text": "We estimate $p(\\mathbf{b})$ as follows: For every $\\mathbf{b}\\in\\mathcal{B}$ , there must be a mixed strategy $\\mathbf{x}^{(\\mathbf{b})}$ and follower action $a^{(\\mathbf{b})}$ such that $\\mathbf{b}=\\mathbb{1}_{(\\sigma^{\\mathbf{x}^{(b)}}=a^{(\\mathbf{b})})}.$ . Therefore, if the leader plays mixed strategy $\\mathbf{x}^{(\\mathbf{b})}\\ N$ times, we set $\\begin{array}{r}{\\widehat{p}(\\mathbf{b})\\;=\\;\\frac{1}{N}\\sum_{t\\in[N]}\\mathbb{1}\\big\\{b_{f_{t}}(\\mathbf{x}^{(\\mathbf{b})})\\;=\\;a^{(\\mathbf{b})}\\big\\}}\\end{array}$ . Given estimates $\\{\\widehat{\\mathbf{p}}(\\mathbf{b})\\}_{\\mathbf{b}\\in B}$ , we can estimate $p\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\big)$ for any $\\mathbf{x}\\in\\mathcal{E}$ and $a_{f}\\in A_{f}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{p}(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}):=\\sum_{i=1}^{K}\\lambda_{i}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\big)\\cdot\\widehat{p}(\\mathbf{b}^{(i)}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\lambda_{i}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\big)\\in[-1,1]$ for $i\\in[K]$ are the coefficients from the barycentric spanner.13 Note that this is an unbiased estimator, due to the fact that $p\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\big)$ is a linear function. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 3 plays each mixed strategy $\\mathbf{x}^{(\\mathbf{b})}$ for $\\mathbf{b}\\in B\\,N>0$ times in order to obtain an estimate of each $\\mathbf{p}(\\mathbf{b})$ .14 It then uses these estimates to construct estimates for all $\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}$ (and therefore also $\\mathbb{E}_{f}\\left[u(\\mathbf{z},\\mathbf{x},b_{f}(\\mathbf{x}))\\right]$ for all $\\mathbf{x}\\in\\mathcal{E}$ and $\\mathbf{z}\\in{\\mathcal{Z}}$ ). Finally, in the remaining rounds Algorithm 3 acts greedily with respect to its estimate, much like in Algorithm 1. ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1. If $\\begin{array}{r}{N\\,=\\,O\\left(\\frac{A_{f}^{2/3}T^{2/3}\\log^{1/3}(T)}{K^{1/3}}\\right)}\\end{array}$ then the expected contextual Stackelberg regret of Algorithm $^3$ (Definition 4.5) satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(T)]=O\\left(K^{2/3}A_{f}^{2/3}T^{2/3}\\log^{1/3}(T)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof Sketch. The key step in our analysis is to show that for any best-response function $\\sigma\\in\\Sigma$ and follower action $a_{f}\\in A_{f}$ , $\\begin{array}{r}{\\mathrm{Var}(\\widehat{p}(\\mathbb{1}_{(\\sigma=a_{f})}))\\leq\\frac{K}{N}}\\end{array}$ (Lemma C.3). Using this fact, we can bound the cumulative total variation distance between $\\widehat{p}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}_{t})}=a_{f}}\\big)$ and $p\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}_{t})}=a_{f}}\\big)\\big)$ for any sequence of mixed strategies and follower actions in the \u201cexploit\u201d phase (Lemma C.5). The rest of the analysis follows similarly to the proof of Corollary 4.7. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "We now turn our attention to learning under bandit feedback when the sequence of contexts is chosen stochastically and the choice of follower type is adversarial. While we still use barycentric spanners to estimate $\\big\\{\\{\\widehat{p}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\big)\\}_{a_{f}\\in\\mathcal{A}_{f}}\\big\\}\\mathbf{x}\\!\\in\\!\\mathcal{E}$ , we can no longer do all of our exploration \u201cup front\u201d like in Appendix C.1 because the follower types are now adversarially chosen. Instead, we follow the technique of Balcan et al. [5] and split the time horizon into $Z$ consecutive, evenly-sized blocks. In block $B_{\\tau}$ , we pick a random time-step to estimate $p_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}^{(\\mathbf{b})})}=a^{(\\mathbf{b})})}\\big)$ , i.e. the probability that a follower in block $B_{\\tau}$ best-responds to mixed strategy $\\mathbf{x}^{(\\mathbf{b})}$ by playing action $a^{(\\mathbf{b})}$ , for every element in our barycentric spanner $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . If whenever the leader plays $\\mathbf{x}^{(\\mathbf{b})}$ the follower best-responds with action $a^{(\\mathbf{b})}$ , we set $\\widehat{p}_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}^{(\\mathbf{b})})}=a^{(\\mathbf{b})})}\\big)=1$ . Otherwise we set $\\widehat{p}_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}^{(\\mathbf{b})})}=a^{(\\mathbf{b})})}\\big)=0$ . Since the time-step in which we play $\\mathbf{x}^{(\\mathbf{b})}$ is chosen uniformly from all time-steps in $B_{\\tau}$ , $\\widehat{p}_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})})=a^{(\\mathbf{b})})}\\big)$ is an unbiased estimate of $p_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}^{(\\mathbf{b})})}=a^{(\\mathbf{b})})}\\big)$ . While $\\widehat{p}_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})})=a^{(\\mathbf{b})})}\\big)$ no longer has low variance since it must be recomputed separately for every block $B_{\\tau}$ , it is still bounded. Therefore, we can use our estimates $\\{\\widehat{p_{\\tau}}\\big(\\mathbb{1}_{\\mathfrak{\\sigma}^{(\\mathbf{x}^{(\\mathbf{b})})}=a^{(\\mathbf{b})})}\\big)\\}_{\\mathbf{b}\\in B}$ , along with the corresponding linear coefficients from the barycentric spanner, to get a bounded (and unbiased) estimate for every $p\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\big)$ . ", "page_idx": 19}, {"type": "text", "text": "Once we have estimates for $\\{\\{p_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\times)}=a_{f})}\\big)\\}_{a_{f}\\in\\mathcal{A}_{f}}\\}_{\\mathbf{x}\\in\\mathcal{E}}$ , we proceed via a reduction to Algorithm 2. In particular, in every block $B_{\\tau}$ we use our estimates $\\{\\{\\widehat{p}_{\\tau}(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})})\\}_{a_{f}\\in\\mathcal{A}_{f}}\\}\\mathbf{x}\\{\\varepsilon\\}$ to construct an (unbiased and bounded) estimate of the average utility for all policies in our finite policy class $\\Pi$ during block $B_{\\tau}$ . At the end of each block, we feed this estimate into the Hedge update step, which updates the weights of all policies for the next block. Finally, when we are not exploring (i.e. estimating $p_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}^{(\\mathbf{b})})}=a^{(\\mathbf{b})})}\\big)$ for some $\\mathbf{b}\\in\\mathcal{B}$ ), we sample the leader\u2019s policy according to the distribution over policies given by Hedge from the previous block. This process is summarized in Algorithm 4. ", "page_idx": 19}, {"type": "text", "text": "Theorem C.2. If $N=O(T^{2/3}A_{f}^{1/3}\\log^{1/3}T)$ , then Algorithm 4 obtains expected contextual Stackelberg regret (Definition 4.9) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(T)]\\le{\\cal O}\\left(K A_{f}^{1/3}T^{2/3}\\log^{1/3}(T)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof Sketch. The analysis proceeds similarly to the analysis of Theorem 6.1 in Balcan et al. [5]. We highlight the key differences here. The first key difference is that while Balcan et al. [5] play Hedge over a finite set of leader strategies, we play Hedge over a finite set of leader policies, each of which map to one of finitely-many leader strategies for a given context. Second, unlike in Balcan et al. [5] it is not sufficient to only estimate $\\{\\{p_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}\\big)\\}_{a_{f}\\in\\mathcal{A}_{f}}\\}_{\\mathbf{x}\\in\\mathcal{E}}$ to obtain an unbiased estimate of the utility of each policy in $\\Pi$ in each time block\u2014we must also specify a context (or set of contexts) to use in our estimator. We show that it suffices to select a context uniformly at random from the contexts $\\{\\mathbf{z}_{t}\\}_{t\\in B_{\\tau}}$ encountered in the block. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C.3 Proofs for Appendix C.1: Stochastic follower types and adversarial contexts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma C.3. For any $\\sigma_{f}\\in\\Sigma$ and $a_{f}\\in A_{f}$ , $\\begin{array}{r}{\\mathrm{Var}\\big(\\widehat{p}\\big(\\mathbb{1}_{(\\sigma=a_{f})}\\big)\\big)\\leq\\frac{K}{N}.}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Let $\\mathcal{B}=\\{\\mathbf{b}^{(1)},\\dots,\\mathbf{b}^{(K)}\\}$ be the Barycentric spanner of $\\mathcal{W}$   \nfor $i=1,\\ldots,K$ do for $\\tau=1,\\ldots,N$ do Play mixed strategy $\\mathbf{x}^{(\\mathbf{b}^{(i)})}$ , observe best-response $a_{f(i-1)\\cdot N+\\tau}$ end Compute $\\begin{array}{r}{\\widehat{p}(\\mathbf{b}^{(i)})=\\frac{1}{N}\\sum_{\\tau=1}^{N}\\mathbb{1}\\{b_{f_{\\tau}}(\\mathbf{x}^{(\\mathbf{b}^{(i)})})=a^{(\\mathbf{b}^{(i)})}\\}}\\end{array}$   \nend   \nCompute $\\begin{array}{r}{\\widehat{p}(\\mathbb{1}_{(\\sigma=a_{f})})=\\sum_{i=1}^{K}\\lambda_{i}\\big(\\mathbb{1}_{(\\sigma=a_{f})}\\big)\\cdot\\widehat{p}(\\mathbf{b}^{(i)})}\\end{array}$ for all $\\sigma\\in\\Sigma$ , $a_{f}\\in A_{f}$   \nfor $t=K\\cdot N+1,\\ldots,T$ do Observe context $\\mathbf{z}_{t}$ , commit to mixed strategy $\\begin{array}{r}{\\mathbf{x}_{t}=\\widehat{\\pi}(\\mathbf{z}_{t})=\\arg\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{E}}\\sum_{a_{f}\\in A_{f}}\\widehat{p}(\\overset{\\cdot\\cdot}{\\mathbb{1}_{(\\sigma^{(\\mathbf{x})}=a_{f})}})\\cdot u(\\mathbf{z}_{t},\\mathbf{x},a_{f}).}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Var}(\\widetilde{y}(1,\\{\\mathrm{eno}_{j}\\})):=\\mathrm{E}[(\\widetilde{y}(1,\\{\\mathrm{eno}_{j}\\}))^{2}]-\\mathrm{E}[\\widetilde{y}(1,\\{\\mathrm{eno}_{j}\\})]^{2}}\\\\ &{=\\mathrm{E}[\\widetilde{y}(1,\\{\\mathrm{eno}_{j}\\})^{2}]-\\mathrm{P}^{2}(1,\\{\\mathrm{eno}_{j}\\})}\\\\ &{=\\mathrm{E}\\left[\\left(\\displaystyle\\sum_{j=1,j}^{K}\\lambda_{j}(1,\\ {\\mathrm{eno}_{j}\\})\\widetilde{y}(8,0)\\right)^{2}\\right]-p^{2}(1,\\{\\mathrm{eno}_{j}\\})}\\\\ &{=\\mathrm{E}\\left[\\displaystyle\\sum_{j=1,j}^{K}\\lambda_{j}(1,\\ {\\mathrm{eno}_{j}\\})\\widetilde{y}^{2}(8,0)\\right]}\\\\ &{+\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{j=1,j\\neq j}^{K}\\lambda_{i}(1,\\{\\mathrm{eno}_{j}\\})\\lambda_{j}(1,\\{\\mathrm{eno}_{j}\\})\\widetilde{y}(8^{(k)})\\widetilde{y}(8^{(k)})\\right]-p^{2}(1,\\{\\mathrm{eno}_{j}\\})}\\\\ &{=\\displaystyle\\sum_{j=1}^{K}\\lambda_{j}^{2}(1,\\ {\\mathrm{eno}_{j}\\})\\widetilde{y}(1,\\{\\mathrm{eno}_{j}\\})}\\\\ &{+\\displaystyle\\sum_{j=1,j=1,0}^{K}\\lambda_{j}(1,\\{\\mathrm{eno}_{j}\\})\\lambda_{j}(1,\\{\\mathrm{eno}_{j}\\})\\widetilde{y}(8^{(k)})\\widetilde{y}(8^{(j)})-p^{2}(1,\\{\\mathrm{eno}_{j}\\})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Observe that since (1) the follower in each round is drawn independently from $\\mathcal{F}$ and (2) the rounds used to compute $\\widehat{p}(\\mathbf{b}^{(i)})$ do not overlap with the rounds used to compute $\\widehat{p}(\\mathbf{b}^{(j)})$ for $j\\neq i$ , $\\widehat{p}(\\mathbf{b}^{(i)})$ and $\\widehat{p}(\\mathbf{b}^{(j)})$ are independent random variables for $j\\neq i$ . Therefore ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{Var}(\\widehat{p}(\\mathbb{I}_{(\\sigma=a_{f})}))=\\sum_{j=1}^{K}\\lambda_{j}^{2}(\\mathbb{1}_{(\\sigma=a_{f})})\\mathbb{E}[\\widehat{p}^{2}(\\mathbf{b}^{(j)})]}}\\\\ &{}&{\\qquad+\\sum_{i=1}^{K}\\sum_{j=1,j\\neq i}^{K}\\lambda_{i}(\\mathbb{1}_{(\\sigma=a_{f})})\\lambda_{j}(\\mathbb{1}_{(\\sigma=a_{f})})p(\\mathbf{b}^{(i)})p(\\mathbf{b}^{(j)})-p^{2}(\\mathbb{1}_{(\\sigma=a_{f})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now turn our focus to $\\mathbb{E}[\\widehat{p}^{2}(\\mathbf{b}^{(j)})]$ . Observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbb{E}[\\hat{p}^{2}(\\mathbf{b}^{(j)})]=\\mathbb{E}\\left[\\left(\\frac{1}{N}\\sum_{\\tau=1}^{N}\\mathbb{1}\\{b_{f_{\\tau}}(\\mathbf{x}^{(\\mathbf{b}^{(j)})}=a_{f}^{(\\mathbf{b}^{(j)})}))\\}\\right)^{2}\\right]}\\\\ {\\displaystyle=\\frac{1}{N^{2}}\\mathbb{E}\\left[\\sum_{\\tau=1}^{N}\\sum_{\\tau^{\\prime}=1}^{N}\\mathbb{1}\\{b_{f_{\\tau}}(\\mathbf{x}^{(\\mathbf{b}^{(j)})}=a_{f}^{(\\mathbf{b}^{(j)})})\\}\\cdot\\mathbb{1}\\{b_{f_{\\tau^{\\prime}}}(\\mathbf{x}^{(\\mathbf{b}^{(j)})}=a_{f}^{(\\mathbf{b}^{(j)})})\\}\\right]}\\\\ {\\displaystyle=\\frac{1}{N^{2}}(N\\cdot p(\\mathbf{b}^{(j)})+N(N-1)p^{2}(\\mathbf{b}^{(j)}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plugging this into our expression for $\\mathrm{Var}(\\widehat{p}(\\mathbb{1}_{(\\sigma=a_{f})}))$ , we see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathrm{Vor}\\{\\hat{\\sigma}(\\hat{\\tau}(\\tau_{-})_{i},\\hat{\\tau}_{-})\\}=}&{\\displaystyle\\frac{1}{N}\\sum_{t=1}^{N}\\hat{x}_{t}^{2}(1_{(t-s_{n-})})\\hat{\\gamma}_{0}(\\hat{\\theta}^{t})+(N-1)\\hat{\\sigma}^{2}(\\hat{\\theta}^{t})}\\\\ &{+\\displaystyle\\sum_{t\\le1}\\sum_{i=1}^{N}\\hat{x}_{t}^{2}\\big(1_{(t-s_{n-})_{i}}\\big)\\hat{\\gamma}_{1}(1_{(s-)_{i}})\\hat{\\eta}(\\hat{\\theta}^{t})-\\hat{\\gamma}^{2}(1_{(t-s_{n-})_{i}})}\\\\ &{=\\displaystyle\\frac{1}{N}\\sum_{t=1}^{N}\\hat{x}_{t}^{2}(1_{(t-s_{n-})_{i}})\\big(y(\\hat{\\theta}^{t})-y^{2}(\\hat{\\theta}^{t})\\big)}\\\\ &{+\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\hat{x}_{t}^{2}(1_{(t-s_{n-})_{i}})\\big(y(\\hat{\\theta}^{t})\\big)y(\\hat{\\theta}^{t})-\\hat{\\gamma}^{2}(1_{(t-s_{n-})_{i}})}\\\\ &{=\\displaystyle\\frac{1}{N}\\sum_{t=1}^{N}\\hat{x}_{t}^{3}(1_{(t-s_{n-})_{i}})\\big(y(\\hat{\\theta}^{t})\\cdot\\lambda_{t}(1_{(s-)_{i}})(1-p(\\hat{\\theta}^{t}))\\big)}\\\\ &{+\\displaystyle\\left(\\sum_{t=1}^{N}\\hat{x}_{t}^{3}(1_{(s-)_{i}})y(\\hat{\\theta}^{t})\\right)^{2}-p^{2}(1_{(t-s_{n-})_{i}})}\\\\ &{=\\displaystyle\\frac{1}{N}\\sum_{t=1}^{N}\\hat{x}_{t}^{3}(1_{(t-s_{n-})_{i}})\\big(y(\\hat{\\theta}^{t})\\big)\\cdot\\hat{\\gamma}^{2}(1_{(t-s_{n-})_{i}})}\\\\ &{\\le\\displaystyle\\frac{N}{N}\\sum_{t=\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last line follows from the fact that $\\lambda_{j}\\big(\\mathbb{1}_{(\\sigma=a_{f})}\\big)\\in[-1,1]$ and $p(\\mathbf{b}^{(j)})\\in[0,1]$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma C.4. For any $\\mathbf{z}\\in{\\mathcal{Z}}$ , ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a_{f}\\in A_{f}}p\\big(\\mathbb{I}_{(\\sigma^{(\\hat{\\pi}(\\mathbf{z}))}=a_{f})}\\big)\\cdot u(\\mathbf{z},\\widehat{\\pi}(\\mathbf{z}),a_{f})\\geq\\displaystyle\\sum_{a_{f}\\in A_{f}}\\widehat{p}\\big(\\mathbb{1}_{(\\sigma^{(\\pi^{(\\hat{\\pi})}(\\mathbf{z}))}=a_{f})}\\big)\\cdot u(\\mathbf{z},\\pi^{(\\mathcal{E})}(\\mathbf{z}),a_{f})}\\\\ &{\\displaystyle-\\sum_{a_{f}\\in A_{f}}|\\widehat{p}\\big(\\mathbb{1}_{(\\sigma^{(\\hat{\\pi}(\\mathbf{z}))}=a_{f})}\\big)-p\\big(\\mathbb{1}_{(\\sigma^{(\\hat{\\pi}(\\mathbf{z}))}=a_{f})}\\big)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By the definition of $\\widehat{\\pi}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{r\\in A_{f}}\\widehat{p}(\\mathbf{1}_{(\\sigma^{(\\pi(\\varepsilon)}\\mathfrak{c})\\circ a_{f})})\\cdot u(\\mathbf{z},\\pi^{(\\varepsilon)}(\\mathbf{z}),a_{f})\\leq\\displaystyle\\sum_{a_{f}\\in A_{f}}\\widehat{p}(\\mathbf{1}_{(\\sigma^{(\\pi(\\mathbf{z})}\\mathfrak{c}=a_{f})})\\cdot u(\\mathbf{z},\\widehat{\\pi}(\\mathbf{z}),a_{f})}\\\\ {=\\displaystyle\\sum_{a_{f}\\in A_{f}}p(\\mathbf{1}_{(\\sigma^{(\\pi(\\mathbf{z})}\\mathfrak{c}=a_{f})})\\cdot u(\\mathbf{z},\\widehat{\\pi}(\\mathbf{z}),a_{f})}\\\\ {+\\displaystyle\\sum_{a_{f}\\in A_{f}}(\\widehat{p}(\\mathbf{1}_{(\\sigma^{(\\pi(\\mathbf{z})}\\mathfrak{c}=a_{f})})-p(\\mathbf{1}_{(\\sigma^{(\\pi(\\mathbf{z})}\\mathfrak{c}=a_{f})}))\\cdot u(\\mathbf{z},\\widehat{\\pi}(\\mathbf{z})}\\\\ {\\leq\\displaystyle\\sum_{a_{f}\\in A_{f}}p(\\mathbf{1}_{(\\sigma^{(\\pi(\\mathbf{z})}\\mathfrak{c}=a_{f})})\\cdot u(\\mathbf{z},\\widehat{\\pi}(\\mathbf{z}),a_{f})}\\\\ {+\\displaystyle\\sum_{a_{f}\\in A_{f}}|\\widehat{p}(\\mathbf{1}_{(\\sigma^{(\\pi(\\mathbf{z})}\\mathfrak{c}=a_{f})})-p(\\mathbf{1}_{(\\sigma^{(\\pi(\\mathbf{z})}\\mathfrak{c}=a_{f})})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "the desired result may be obtained by rearranging terms. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5. For any sequence of mixed strategies $\\mathbf{x}_{N K+1},\\ldots,\\mathbf{x}_{T}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\substack{t=N K+1\\,a_{f}\\in\\mathcal{A}_{f}}}^{T}|\\widehat{p}(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}_{t})}=a_{f})})-p(\\mathbb{1}_{(\\sigma^{(\\mathbf{x}_{t})}=a_{f})})|\\leq2A_{f}T\\sqrt{\\frac{K\\log(T)}{N}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least $\\textstyle1-{\\frac{1}{T}}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. By Lemma C.3 and a Hoeffding bound, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\widehat{p}(\\mathbb{1}_{(\\sigma=a_{f})})-p(\\mathbb{1}_{(\\sigma=a_{f})})|\\leq\\sqrt{\\frac{2K\\log(1/\\delta)}{N}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least $1\\,-\\,\\delta$ , for any particular $(\\sigma,a_{f})$ pair. Taking a union bound over the randomness in estimating $p(\\mathbf{b}^{(1)}),\\ldots,p(\\mathbf{b}^{(K)})$ , we see that ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\widehat{p}(\\mathbb{1}_{(\\sigma=a_{f})})-p(\\mathbb{1}_{(\\sigma=a_{f})})|\\leq\\sqrt{\\frac{2K\\log(K/\\delta)}{N}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least $1-\\delta$ , simultaneously for all $(\\sigma,a_{f})$ . The desired result follows by summing over $T$ and $A_{f}$ , and setting $\\begin{array}{r}{\\delta=\\frac{1}{T}}\\end{array}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Theorem C.1. If $\\begin{array}{r}{N\\,=\\,O\\left(\\frac{A_{f}^{2/3}T^{2/3}\\log^{1/3}(T)}{K^{1/3}}\\right)}\\end{array}$ then the expected contextual Stackelberg regret of Algorithm 3 (Definition 4.5) satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(T)]=O\\left(K^{2/3}A_{f}^{2/3}T^{2/3}\\log^{1/3}(T)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbb{H}(T)|=\\mathbb{E}_{f_{1}\\sim f_{2}\\sim\\pi}\\Bigg[\\sum_{t=1}^{T}\\mathbf{\\Phi}_{(\\pi,\\pi^{*})}(\\pi_{t},\\pi^{*}(\\alpha_{t}),b_{f_{1}}(\\pi^{*}(\\alpha)))-u(\\pi,\\pi_{*}(\\pi_{t}),b_{f_{1}}(\\pi_{t}(\\pi_{t})))\\Bigg]}\\\\ &{\\phantom{\\quad}\\le1+\\mathbb{E}_{f_{1}\\sim f_{2}\\sim\\pi}\\Bigg[\\sum_{t=1}^{T}u(\\pi_{t},\\pi^{*}(\\alpha),b_{f_{1}}(\\pi^{*}(\\alpha),\\pi_{t}))-u(\\pi,\\pi_{*}(\\pi_{t}),b_{f_{1}}(\\pi_{t}(\\pi_{t})))\\Bigg]}\\\\ &{\\phantom{\\quad}\\le1+K N+\\mathbb{E}_{f_{1}\\sim\\pi_{*}(-1-f_{2})}\\times\\Bigg[\\sum_{t=K+1}^{T}\\mathbf{\\Phi}_{(\\pi_{t},\\pi^{*}(\\pi^{*}(\\pi),b_{f_{1}}(\\pi^{*}(\\pi),\\pi_{t})))-u(\\pi_{t},\\pi_{*}(\\pi_{t}),b_{f_{1}}(\\pi))}}\\\\ &{\\phantom{\\quad\\quad=1+}-\\mathbb{K}N+\\mathbb{E}_{f_{2}\\sim P}\\Bigg[\\sum_{t=K+1}^{T}u(\\pi_{t},\\pi^{*}(\\alpha),b_{f_{1}}(\\pi^{*}(\\alpha),1))-u(\\pi_{t},\\tilde{\\pi}(\\pi_{t}),b_{f}(\\pi(\\pi_{t})))\\Bigg]}\\\\ &{\\phantom{\\quad\\quad=1+\\mathbb{K}N+\\mathbb{E}_{f_{1}\\sim\\pi}}\\Bigg[\\sum_{t=K+1}^{T}\\sum_{t=K+1}^{M}(\\pi_{t},\\pi^{*}(\\alpha),a_{f})-1(b_{f}\\pi_{t}(\\pi^{*}(\\pi),a_{f_{1}}))-u_{f})}\\\\ &{\\phantom{\\quad\\quad=1+\\mathbb{K}N+\\mathbb{E}_{f_{1}\\sim\\pi}}\\Bigg[\\sum_{t=K+1}^{T}\\sum_{t=K+1}^{M}(\\pi_{t},\\pi^{*}(\\alpha),a_{f \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Lemma C.4, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon[R(T)]\\leq1+K N+\\displaystyle\\sum_{t=N K+1}^{T}\\displaystyle\\sum_{\\substack{u=(\\pi^{(s)}(\\pi^{(s)}(\\pi^{(s)}(\\pi))=a_{f})})}\\mu(\\mathbf{1}_{(\\sigma^{(u)}(\\pi^{(s)}(\\pi_{u}))=a_{f})})-\\hat{p}(\\mathbf{1}_{(\\sigma^{(c)}(\\pi))=a_{f})})}\\\\ &{\\quad\\quad\\quad\\sum_{t=N K+1}^{T}\\displaystyle\\sum_{\\substack{u=(\\pi^{(s)}(\\pi^{(s)}(\\pi^{(s)}(\\pi))=a_{f})})}-p(\\mathbf{1}_{(\\sigma^{(u)}(\\pi^{(s)}(\\pi))=a_{f})})\\Big\\}}\\\\ &{\\quad\\quad\\quad\\leq1+K N+\\displaystyle\\sum_{t=N K+1}^{T}\\displaystyle\\sum_{u=(\\pi^{(s)}(\\pi^{(s)}(\\pi^{(s)}(\\pi))=a_{f}))}^{\\left[\\hat{p}(\\mathbf{1}_{(\\sigma^{(c)}(\\pi_{u}))=a_{f}})\\right]}-p(\\mathbf{1}_{(\\sigma^{(c)}(\\pi_{u}))=a_{f}})\\Big\\}}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\sum_{t=N K+1}^{T}\\displaystyle\\sum_{\\substack{u=(\\pi^{(s)}(\\pi^{(s)}(\\pi))=a_{f})}}^{\\left[\\hat{p}(\\mathbf{1}_{(\\sigma^{(u)}(\\pi))=a_{f}})\\right]-p(\\mathbf{1}_{(\\sigma^{(u)}(\\pi))=a_{f}}))\\Big\\}}}\\\\ &{\\quad\\quad\\quad\\leq3+K N+4I_{f}T\\sqrt{\\frac{\\mathrm{Kiog}(T)}{N}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last line follows from Lemma C.5. The desired result follows by the setting of $N$ . ", "page_idx": 23}, {"type": "text", "text": "C.4 Proofs for Appendix C.2: Stochastic contexts and adversarial follower types ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Definition C.6. Let $\\begin{array}{r l r}{u_{\\tau}(\\pi)}&{:=}&{\\sum_{a_{f}\\in\\mathcal{A}_{f}}u(\\mathbf{z}_{\\tau},\\pi(\\mathbf{z}_{\\tau}),a_{f})\\,\\cdot\\,p_{\\tau}\\big(\\mathbb{1}_{(\\sigma^{(\\pi(\\mathbf{z}_{\\tau}))}=a_{f})}\\big)}\\end{array}$ and $\\begin{array}{r l r l}{\\widehat{u}_{\\tau}(\\pi)}&{{}:=}&{}\\end{array}$ $\\begin{array}{r l}&{\\sum_{a_{f}\\in\\mathcal{A}_{f}}u\\big(\\mathbf{z}_{\\tau},\\pi\\big(\\mathbf{z}_{\\tau}\\big),a_{f}\\big)\\cdot\\sum_{j=1}^{K}\\lambda_{j}\\big(\\mathbb{1}_{\\left(\\sigma^{(\\pi(\\mathbf{z}_{\\tau}))}=a_{f}\\right)}\\big)\\cdot\\widehat{p}_{\\tau}\\big(\\mathbf{b}^{(j)}\\big),}\\end{array}$ , where $\\mathbf{z}_{\\tau}\\,\\sim\\,\\mathrm{Unif}\\{\\mathbf{z}_{t}\\,:\\,t\\,\\in\\,B_{\\tau}\\}$ , $\\mathcal{B}=\\{\\mathbf{b}^{(1)},\\dots,\\mathbf{b}^{(K)}\\}$ is the Barycentric spanner for $\\mathcal{W}$ , and $\\widehat{p}(\\mathbf{b})=1\\,i f\\,b_{f_{t(\\mathbf{b})}}(\\mathbf{x}^{(\\mathbf{b})})=a_{f}^{(\\mathbf{b})}\\,a n d$ $\\widehat{p}(\\mathbf{b})=0$ otherwise. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.7. For any fixed policy $\\pi$ $\\begin{array}{r l r l r}{\\mathrm{~\\boldmath~\\mathbb~{~E~}~}_{\\{\\mathbf{z}_{\\tau}\\}_{t\\in B_{\\tau}}}\\mathbb{E}[\\widehat{u}_{\\tau}(\\pi)]}&{{}=}&{\\mathbb{E}_{\\mathbf{z}_{\\tau}\\sim\\mathcal{P}}[u_{\\tau}(\\pi)]}&{{}=}&{}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{z}_{\\tau}\\sim\\mathcal{P}}[\\sum_{a_{f}\\in A_{f}}u(\\mathbf{z}_{\\tau},\\pi(\\mathbf{z}_{\\tau}),a_{f})\\cdot p_{\\tau}(\\mathbb{1}_{(\\sigma^{(\\pi(\\mathbf{z}_{\\tau}))}=a_{f})})],}\\end{array}$ , where the second expectation is taken over the randomness in selecting the explore time-steps and in drawing $\\mathbf{z}_{\\tau}\\ \\sim\\ \\mathrm{Unif}\\!\\left\\{\\mathbf{z}_{t}\\ :\\ t\\ \\in\\ B_{\\tau}\\right\\}$ . Moreover, $\\widehat{u}_{\\tau}(\\pi)\\in[-K A_{f},K A_{f}]$ . ", "page_idx": 23}, {"type": "text", "text": "Algorithm 4: Learning with stochastic contexts: bandit feedback ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Consider $\\Pi:=\\{\\pi^{(\\omega)}\\}_{\\omega\\in\\Omega}$ Let $\\begin{array}{r}{\\mathbf{q}_{1}[\\pi^{(\\omega)}]:=1,\\mathbf{p}_{1}[\\pi^{(\\omega)}]:=\\frac{1}{|\\Pi|}}\\end{array}$ for all $\\pi^{(\\omega)}\\in\\Pi$ Let $\\mathcal{B}=\\{\\mathbf{b}^{(1)},\\dots,\\mathbf{b}^{(K)}\\}$ be the Barycentric spanner of $\\mathcal{W}$ for $\\tau=1,\\dots,Z$ do Choose random perturbation over $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and explore time-steps in $B_{\\tau}$ uniformly at random Choose a time-step in $B_{\\tau}$ uniformly at random whose context will be used as $\\mathbf{z}_{\\tau}$ ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "for $t\\in B_{\\tau}$ do If $t$ is an explore time-step, play the corresponding mixed strategy $\\mathbf{x}^{(\\mathbf{b}_{t})}$ in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . If $a_{f_{t}}=a^{(\\mathbf{b}_{t}^{-})}$ , set $\\widehat{p}_{\\tau}(\\mathbf{b}_{t})=1$ . Otherwise, set $\\widehat{p}_{\\tau}(\\mathbf{b}_{t})=0$ . Otherwise ( $t$ is an   exploit round), sample \u03c0t  \u223c pt, $\\begin{array}{r}{a_{l,t}\\sim\\pi_{t}(\\mathbf{z}_{t})}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "For each policy $\\pi^{(\\omega)}\\in\\Pi$ , compute $\\begin{array}{r l}&{\\widehat{\\ell}_{\\tau}[\\pi^{(\\omega)}]:=\\stackrel{-}{-}\\sum_{a_{f}\\in A_{f}}\\sum_{i=1}^{K}\\lambda_{i}\\big(\\mathbb{1}_{\\{\\sigma^{(\\pi(\\pi_{\\tau}))}=a_{f})\\}}\\cdot\\widehat{p}(\\mathbf{b}^{(i)})\\cdot u(\\mathbf{z}_{\\tau},\\pi^{(\\omega)}(\\mathbf{z}_{\\tau}),a_{f}).}\\\\ &{{\\mathrm{set}}\\ \\mathbf{q}_{\\tau+1}[\\pi^{(\\omega)}]=\\exp\\left(-\\eta\\sum_{s=1}^{\\tau}\\widehat{\\ell}_{s}[\\pi^{(\\omega)}]\\right)\\mathrm{~and~}}\\\\ &{\\mathbf{p}_{t+1}[\\pi^{(\\omega)}]=\\mathbf{q}_{t+1}[\\pi^{(\\omega)}]/\\sum_{\\pi^{(\\omega^{\\prime})}\\in\\Pi}\\mathbf{q}_{t+1}[\\pi^{(\\omega^{\\prime})}].}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\mathrm{s}_{t}\\big(\\Delta_{t}\\big(\\pi(\\pi)\\big)=\\mathbb{E}_{\\{\\pi\\}_{t}\\}\\exp\\bigg[\\sum_{\\sigma\\in A_{t}}u\\big(\\pi_{r},\\pi(\\pi_{r}),a_{\\sigma}\\big)\\cdot\\sum_{j=1}^{K}\\big(1_{\\sigma^{(r)}(\\pi^{(r)})=a_{\\sigma^{(j)}}}\\big)\\cdot\\hat{p}_{r}\\big(\\mathbf{b}^{(\\theta)}\\big)\\bigg]}\\\\ &{=\\mathbb{E}_{\\{\\pi\\}_{t}\\}_{c_{0}}\\mathbb{E}_{\\pi_{r}\\sim\\mathrm{vinf}(\\pi_{r})\\in B_{r}}\\bigg[\\sum_{\\sigma^{\\prime}\\in A_{t}}u\\big(\\pi_{r},\\pi(\\pi_{r}),a_{\\sigma^{\\prime}}\\big)\\cdot\\sum_{j=1}^{K}\\big(1_{\\sigma^{(r)}(\\pi^{(r)})=a_{j}}\\big)\\cdot\\mathbb{E}_{\\sigma}}\\\\ &{=\\mathbb{E}_{\\{\\pi\\}_{t}\\in B_{r}}\\mathbb{E}_{\\pi_{r}\\sim\\mathrm{vinf}(\\pi_{r})\\in B_{r}}\\bigg[\\sum_{\\sigma\\in A_{t}}u\\big(\\pi_{r},\\pi(\\pi_{r}),a_{\\sigma^{\\prime}}\\big)\\cdot\\sum_{j=1}^{K}\\big(1_{\\sigma^{(r)}(\\pi^{(r)})=a_{j}}\\big)\\cdot p_{r}}\\\\ &{=\\mathbb{E}_{\\{\\pi\\}_{t}\\sim\\mathrm{vinf}(\\pi_{r})\\in B_{r}}\\bigg[\\sum_{\\sigma\\in A_{t}}u\\big(\\pi_{r},\\pi(\\pi_{r}),a_{\\sigma^{\\prime}}\\big)\\cdot p_{\\pi^{\\prime}}\\big(1_{\\sigma^{(r)}(\\pi^{(r)})=a_{\\sigma^{\\prime}}}\\big)\\bigg]}\\\\ &{=\\mathbb{E}_{\\pi_{r}\\sim\\mathcal{P}}\\left[\\sum_{\\sigma\\in A_{t}}u\\big(\\pi_{r},\\pi(\\pi_{r}),a_{\\sigma}\\big)\\cdot p_{\\pi}\\big(1_{\\sigma^{(r)}(\\pi^{(r)})=a_{\\sigma^{\\prime}}}\\big)\\right]=\\mathbb{E}_{\\pi_{r}\\sim\\mathcal{P}}[u_{\\tau}(\\pi)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The following lemma is analogous to Equation (1) in Balcan et al. [5]. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.8. ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{N}\\sim\\mathcal{P}}\\left[\\sum_{\\tau=1}^{N}u_{\\tau}\\big(\\pi^{(\\mathcal{E})}\\big)-\\mathbb{E}u_{\\tau}\\big(\\pi_{\\tau}\\big)\\right]\\le\\sqrt{N\\kappa\\log|\\Pi|}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $R_{N,\\kappa}$ is an upper-bound on the regret of (full-information) Hedge which takes as input a sequence of $N$ losses/utilities which are bounded in $[-\\kappa,\\kappa]$ and are parameterized by $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{N}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{N}\\sim\\mathcal{P}}\\left[\\displaystyle\\sum_{r=1}^{N}\\sum_{\\tau\\in\\mathbb{N}}\\left\\mathbf{p}_{r}[\\boldsymbol{\\pi}]\\cdot\\boldsymbol{u}_{r}(\\pi)\\right]=\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{N}\\sim\\mathcal{P}}\\left[\\displaystyle\\sum_{r=1}^{N}\\sum_{\\tau\\in\\mathbb{N}}\\mathbf{p}_{r}[\\boldsymbol{\\pi}]\\cdot\\mathbb{E}[\\hat{u}_{r}(\\pi)]\\right]}&{}\\\\ {=\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{N}\\sim\\mathcal{P}}\\left[\\displaystyle\\sum_{r=1}^{N}\\sum_{\\tau\\in\\mathbb{N}}\\mathbf{p}_{r}[\\boldsymbol{\\pi}]\\cdot\\hat{\\boldsymbol{u}}_{r}(\\pi)\\right]}&{}\\\\ {\\ }&{\\ge\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{N}\\sim\\mathcal{P}}\\mathbb{E}\\left[\\displaystyle\\operatorname*{max}_{\\tau\\in\\mathbb{N}}\\sum_{\\Tilde{\\pi}}\\hat{u}_{r}(\\pi)-R_{N,\\kappa}\\right]}&{}\\\\ {\\ }&{\\ge\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{N}\\sim\\mathcal{P}}\\left[\\displaystyle\\operatorname*{max}_{\\tau\\in\\mathbb{N}}\\sum_{\\tau=1}^{N}\\hat{u}_{r}(\\pi)\\right]-R_{N,\\kappa}\\sim}\\\\ {\\ }&{=\\mathbb{E}_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{N}\\sim\\mathcal{P}}\\left[\\displaystyle\\operatorname*{max}_{\\tau\\in\\mathbb{N}}\\sum_{i=1}^{N}u_{r}(\\pi)-R_{N,\\kappa}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first line uses Lemma C.7 and the fact that $\\mathbf{z}_{1},\\..\\ .\\ ,\\mathbf{z}_{T}\\sim\\mathcal{P}$ are i.i.d., and $R_{N,\\kappa}$ is the regret of Hedge after $N$ time-steps when losses are bounded in $[-\\kappa,\\kappa]$ . Rearranging terms and using the fact that the expected regret of Hedge after $N$ time-steps is at most $\\sqrt{N\\kappa\\log|\\Pi|}$ gets us the desired result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Theorem C.2. If $N=O(T^{2/3}A_{f}^{1/3}\\log^{1/3}T)$ , then Algorithm 4 obtains expected contextual Stackelberg regret (Definition 4.9) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[R(T)]\\le{\\cal O}\\left(K A_{f}^{1/3}T^{2/3}\\log^{1/3}(T)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ s[R(T)]:=\\mathbb E\\mathbb E_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{T}\\sim\\mathcal P}\\left[\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t}),b_{f_{t}}(\\pi^{*}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\pi_{t}(\\mathbf{z}_{t})))\\right]}\\\\ &{\\qquad\\quad\\le1+\\mathbb E\\mathbb E_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{T}\\sim\\mathcal P}\\left[\\displaystyle\\sum_{t=1}^{T}u(\\mathbf{z}_{t},\\pi^{*}(\\mathbf{z}_{t})(\\mathbf{z}_{t}),b_{f_{t}}(\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\pi_{t}(\\mathbf{z}_{t})))\\right]}\\\\ &{\\qquad\\quad=1+\\mathbb E\\mathbb E_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{T}\\sim\\mathcal P}\\left[\\displaystyle\\sum_{\\tau=1}^{N}\\sum_{t\\in B_{T}}u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{t}(\\mathbf{z}_{t}),b_{f_{t}}(\\pi_{t}(\\mathbf{z}_{t})))\\right]}\\\\ &{\\qquad\\quad\\le1+K N+\\mathbb E\\mathbb E_{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{T}\\sim\\mathcal P}\\left[\\displaystyle\\sum_{\\tau=1}^{N}\\sum_{t\\in B_{T}}u(\\mathbf{z}_{t},\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t}),b_{f_{t}}(\\pi^{(\\mathcal{E})}(\\mathbf{z}_{t})))-u(\\mathbf{z}_{t},\\pi_{\\tau}(\\mathbf{z}_{t}),b_{f_{t}}(\\pi_{t}(\\mathbf{z}_{t})))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=1+K N+\\mathbb{E}\\mathbb{E}_{t_{1}\\sim\\pi_{t}\\sim\\pi^{(\\delta)}}\\Bigg[\\displaystyle\\sum_{t=1}^{N}\\displaystyle\\sum_{\\substack{u=1,u=1}}\\displaystyle\\sum_{u=1}^{t}\\Big(u_{t_{1}}\\Big-u^{\\delta}(x_{1})\\Big(x_{1}\\Big)\\cdot1\\Big(b_{t_{1}}\\Big(u^{\\delta}(x_{1})\\Big)-a_{j}\\Big)}\\\\ &{-\\upsilon\\Big(a_{1},x_{1}\\Big(x_{1}\\Big),a_{j}\\Big)\\cdot1\\Big(b_{t_{1}}\\Big(x_{1}\\Big(x_{1})\\Big)-a_{j}\\Big)\\Big]}\\\\ &{=1+K N+\\mathbb{E}\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\sum_{u=1}^{s}\\Big(b_{u,v}\\Big(b_{u,v}\\Big(b_{u,v}\\Big(b_{u,v}\\Big(b_{u,v}\\Big)-b_{u,v}\\Big(b_{u,v}\\Big)\\Big)\\Big(x_{1}\\Big)\\sum_{u=1}^{s}\\Big(c_{u,v}^{\\delta}\\Big)}\\\\ &{\\Big(u_{1}\\Big(x_{1}\\Big)^{U}\\Big(b_{u,v}\\Big(a_{1}\\Big),a_{j}\\Big)\\cdot1\\Big(b_{u,v}\\Big(b^{\\delta}(x_{1})\\Big)-a_{j}\\Big)-\\Big(u_{1}\\Big(x_{1}\\Big(x_{1}\\Big(\\cdot(v_{1})\\Big(a_{1}\\Big)\\cdot a_{j}\\Big)+a_{j}\\Big)\\Big)\\Big)\\Big)}\\\\ &{=1+K N+\\mathbb{E}\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\sum_{u=i-s_{t}\\sim w_{t}\\sim(\\delta)(x_{1}-u_{i})\\sim\\pi^{(\\delta)}}\\displaystyle\\sum_{u=i-s_{t}\\sim\\pi_{t}\\sim\\pi_{t}\\sim\\delta_{t}(-s_{t})}\\left(\\displaystyle\\sum_{u=i-s_{t}\\in\\delta_{t}}\\sum_{u=i-s_{t}\\in\\delta_{t}}\\right.}\\\\ &{\\left.\\Big(u_{1}\\Big(x_{1},u^{\\delta}\\Big(b_{u,v}\\Big),a_{j}\\Big)-1\\Big(b_{u,v}\\Big(a^{\\delta}(x_{1})\\Big)-a_{j}\\Big)-a_{j}\\Big(u_{1}\\Big(x_{1}\\Big(x\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second line follows from Lemma 4.4, the third from splitting the time horizon into blocks, the fourth from loss due to exploration, the fifth due to reformulating the reward as a function of different follower actions, the sixth due to linearity of expectation, and the seventh line follows from the fact that (1) $\\pi_{\\tau}$ is independent of $\\mathbf{z}_{t}$ for all $t\\in B_{\\tau}$ and (2) $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ are independent. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\;\\leq1+K N+B\\sum_{\\sigma=1}^{K}\\frac{\\gamma_{\\sigma,\\sigma}}{\\gamma_{\\sigma,\\sigma}}\\sum_{\\sigma_{t},\\ldots,\\sigma_{t+1},\\ldots,\\sigma_{t+1},\\ldots,\\sigma_{t}\\;=\\frac{\\gamma_{\\sigma,\\sigma}}{\\gamma_{\\sigma,\\sigma}}}p\\overline{{g_{t,\\sigma}}}_{t,\\sigma_{t+1},\\ldots,\\sigma_{t+1},\\ldots,\\sigma_{t+1},\\ldots,\\sigma_{t}}\\;\\right|\\;\\sum_{\\sigma\\in\\mathcal{L}_{\\sigma}}\\bigg|(\\pi_{\\sigma,\\sigma},\\pi^{(\\sigma)}(\\sigma_{t}),\\sigma_{t})}\\\\ &{\\quad\\cdot\\left(\\frac{\\gamma_{\\sigma,\\sigma}}{\\gamma_{\\sigma,\\sigma}}\\sum_{i,\\sigma_{t}}^{1}\\big|\\gamma_{\\sigma_{i}}(v^{(i)}(v_{i})_{i}=\\sigma_{t})\\big|\\right)-\\eta\\big(\\pi_{\\sigma},\\pi_{t};\\sigma_{t}\\big),\\bigg\\rangle\\;\\left(\\frac{\\gamma_{\\sigma,\\sigma}}{\\gamma_{\\sigma,\\sigma}}\\sum_{i,\\sigma_{t}}^{1}\\big|\\gamma_{\\sigma_{i}}(v_{i},\\sigma_{t}),\\sigma_{t}\\big)\\right)}\\\\ &{\\quad=1+K N^{-1}\\delta\\gamma_{\\sigma,t}}\\\\ &{\\qquad+B\\sum_{\\sigma=1}^{K}\\frac{\\gamma_{\\sigma,\\sigma}}{\\gamma_{\\sigma,\\sigma}}\\sum_{i,\\sigma_{t},\\ldots,\\sigma_{t},\\sigma_{t},\\ldots,\\sigma_{t},\\sigma_{t+1},\\ldots,\\sigma_{t+1},\\sigma_{t}\\;=\\frac{\\gamma_{\\sigma,\\sigma}}{\\gamma_{\\sigma,\\sigma}}}\\Bigg|\\;\\sum_{\\sigma\\in\\mathcal{L}_{\\sigma}}\\bigg|(\\pi_{\\sigma,\\sigma},\\pi^{(\\sigma)}(v_{i})_{\\sigma})_{\\sigma_{t}}\\;\\gamma_{\\sigma}\\big(\\mathrm{I}_{\\sigma,\\sigma},\\sigma_{t}\\big)}\\\\ &{\\quad-\\eta\\big(\\pi_{\\sigma},\\pi_{t};\\sigma_{t}\\big),\\gamma_{\\sigma}\\big(\\mathrm{I}_{\\sigma+\\sigma_{t+1},\\sigma_{t+1},\\sigma_{t}}\\big)\\Bigg|^{2}\\Bigg\\rangle}\\\\ &{\\leq1+K N+B\\sum_{\\sigma=1}^{K}\\frac{\\gamma_{\\sigma,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first line comes from multiplying and dividing by $|B_{\\tau}|$ , the second line comes from the definition of $p_{\\tau}$ , the third from the definition of $u_{\\tau}$ , the fourth follows from linearity of expectation and the fact that $\\mathbf{z}_{1},\\dots,\\mathbf{z}_{T}$ are i.i.d., the fifth follows from applying Lemma C.8, and the sixth line follows from the definition of $B$ and the fact that $|\\Pi|\\leq N^{K}$ . Setting $N$ gets us the final result. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: As our work is theoretical, there are no positive or negative societal impacts. However, we hope that our work will one day lead to the design and implementation of better algorithms for solving Stackelberg security games in practice, which would have a positive societal impact in domains such as airport security and the protection of wildlife against poaching. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]