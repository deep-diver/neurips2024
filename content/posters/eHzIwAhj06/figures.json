[{"figure_path": "eHzIwAhj06/figures/figures_4_1.jpg", "caption": "Figure 1: Class-balanced upsampling and upweighting experience catastrophic collapse. We compare subsetting, wherein data is removed to set every class to the same size as the smallest class, upsampling, wherein the sampling probabilities of each class are adjusted so that the mini-batches are class-balanced in expectation, and upweighting, wherein the loss for the smaller classes is scaled by the class-imbalance ratio. We observe a catastrophic collapse over the course of training of upsampling and upweighting on CelebA and CivilComments, the more class-imbalanced datasets. Subsetting reduces WGA on Waterbirds because it removes data from the small minority group within the majority class. MultiNLI is class-balanced a priori, so we do not include it here.", "description": "This figure compares three class-balancing techniques (subsetting, upsampling, and upweighting) against a no-class-balancing baseline across three datasets (Waterbirds, CelebA, and CivilComments). It shows that upsampling and upweighting lead to a catastrophic collapse in worst-group accuracy (WGA) over training epochs, especially on the more imbalanced datasets (CelebA and CivilComments). In contrast, subsetting reduces WGA on Waterbirds due to its removal of data from a small minority group, while MultiNLI, already class-balanced, shows no change.", "section": "3 Nuanced effects of class-balancing on group robustness"}, {"figure_path": "eHzIwAhj06/figures/figures_5_1.jpg", "caption": "Figure 1: Class-balanced upsampling and upweighting experience catastrophic collapse. We compare subsetting, wherein data is removed to set every class to the same size as the smallest class, upsampling, wherein the sampling probabilities of each class are adjusted so that the mini-batches are class-balanced in expectation, and upweighting, wherein the loss for the smaller classes is scaled by the class-imbalance ratio. We observe a catastrophic collapse over the course of training of upsampling and upweighting on CelebA and CivilComments, the more class-imbalanced datasets. Subsetting reduces WGA on Waterbirds because it removes data from the small minority group within the majority class. MultiNLI is class-balanced a priori, so we do not include it here.", "description": "The figure displays the worst-group accuracy (WGA) over training epochs for three class-balancing techniques (subsetting, upsampling, upweighting) and a no-class-balancing baseline on three datasets (Waterbirds, CelebA, CivilComments). Upsampling and upweighting show a catastrophic collapse in WGA, especially on the more imbalanced datasets. Subsetting improves WGA on Waterbirds but not CelebA or CivilComments.", "section": "3 Nuanced effects of class-balancing on group robustness"}, {"figure_path": "eHzIwAhj06/figures/figures_7_1.jpg", "caption": "Figure 3: Scaling class-balanced pretrained models can improve worst-group accuracy. We finetune each model size starting from pretrained checkpoints and plot the test worst-group accuracy (WGA) as well as the interpolation threshold, where model reaches 100% training accuracy. We find model scaling is generally beneficial for WGA only in conjunction with appropriate class-balancing, and scaling on imbalanced datasets or with the wrong method can harm robustness. Note MultiNLI is class-balanced a priori and is not interpolated. See Appendix C for training accuracy plots.", "description": "This figure shows the impact of model scaling on the worst-group accuracy (WGA) when finetuning pretrained models with different class-balancing techniques. It demonstrates that scaling improves WGA with appropriate class balancing but can be harmful with inappropriate techniques or imbalanced data.  The results are shown for four different datasets.", "section": "Model scaling improves WGA of class-balanced finetuning"}, {"figure_path": "eHzIwAhj06/figures/figures_7_2.jpg", "caption": "Figure 3: Scaling class-balanced pretrained models can improve worst-group accuracy. We finetune each model size starting from pretrained checkpoints and plot the test worst-group accuracy (WGA) as well as the interpolation threshold, where model reaches 100% training accuracy. We find model scaling is generally beneficial for WGA only in conjunction with appropriate class-balancing, and scaling on imbalanced datasets or with the wrong method can harm robustness. Note MultiNLI is class-balanced a priori and is not interpolated. See Appendix C for training accuracy plots.", "description": "This figure shows the impact of model scaling on worst-group accuracy (WGA) when finetuning pretrained models with different class-balancing techniques.  Across four datasets, it demonstrates that scaling improves WGA only when combined with the appropriate class-balancing method.  Scaling without proper balancing, or on imbalanced datasets, can negatively affect WGA.  The MultiNLI dataset, being pre-balanced, shows a different trend.", "section": "Model scaling improves WGA of class-balanced finetuning"}, {"figure_path": "eHzIwAhj06/figures/figures_8_1.jpg", "caption": "Figure 5: Group disparities are visible in the top eigenvalues of the group covariance matrices. We visualize the mean, across 3 experimental trials, of the top 10 eigenvalues of the group covariance matrices for a ConvNeXt-V2 Nano finetuned on Waterbirds and CelebA and a BERT Small finetuned on CivilComments and MultiNLI. The standard deviations are omitted for clarity. The models are finetuned using the best class-balancing method from Section 3 for each dataset. The group numbers are detailed in Table 2 and the minority groups within each class are denoted with an asterisk. The largest \u03bb\u2081(g) in each case belongs to a minority group, though it may not be the worst group, and minority group eigenvalues are overall larger than majority group eigenvalues within the same class.", "description": "This figure visualizes the top 10 eigenvalues of group covariance matrices for four datasets (Waterbirds, CelebA, CivilComments, MultiNLI) after finetuning with the best class balancing method.  It highlights that the largest eigenvalue for each dataset belongs to a minority group, and that minority groups generally have larger eigenvalues than majority groups within the same class. This suggests a spectral imbalance that may contribute to group disparities in model performance.", "section": "5 Spectral imbalance may exacerbate group disparities"}, {"figure_path": "eHzIwAhj06/figures/figures_9_1.jpg", "caption": "Figure 6: Group-wise spectral imbalance is apparent once conditioned on the classes. We plot the mean and standard deviation, across 3 experimental trials, of the intra-class spectral norm ratio p(y), or the ratio of the top eigenvalues of the minority and majority group covariance matrices, for each class y \u2208 Y. We compute this metric using a finetuned ConvNeXt-V2 Nano on Waterbirds and CelebA and a finetuned BERT Small on CivilComments and MultiNLI, each using the best class-balancing method from Section 3 for each dataset. The key observation is that p(y) is at least one for all classes y \u2208 Y (except a single seed for class 0 on CelebA), illustrating a group disparity captured by the eigenspectrum once we condition on the classes.", "description": "This figure shows that even when classes are balanced, there is still a spectral imbalance between minority and majority groups within each class. The intra-class spectral norm ratio (p(y)) is calculated as the ratio of the top eigenvalue of the minority group's covariance matrix to that of the majority group's covariance matrix, within each class. The results show that p(y) is greater than or equal to 1 for almost all classes across all datasets, indicating that minority groups tend to have larger spectral norms than majority groups within the same class.", "section": "5 Spectral imbalance may exacerbate group disparities"}, {"figure_path": "eHzIwAhj06/figures/figures_17_1.jpg", "caption": "Figure 7: Mixture balancing ablation studies. We perform two ablation studies on our mixture balancing method. First, we vary the class-imbalance ratio across the x axis. On the left-hand side, using a class-imbalance ratio of 1:1 reduces to the subsetting technique; on the right-hand side, using the original class-imbalance ratio in the dataset reduces to upsampling. Second, we perform an ablation of whether subsetting is essential in mixture balancing. We plot our proposed method (which takes a subset of data based on the class-imbalance ratio, then performs upsampling) against the same method without subsetting, instead adjusting the class probabilities on the entire dataset as specified by the class-imbalance ratio. MultiNLI is class-balanced a priori, so we do not include it here.", "description": "This figure shows ablation studies on the mixture balancing method. The first study varies the class-imbalance ratio, showing the impact of using a balanced subset (subsetting) vs. not using a subset (upsampling). The second study compares the full mixture balancing method to a version without the initial subsetting step. The results demonstrate that the mixture method outperforms other techniques, particularly on class-imbalanced datasets.", "section": "3 Nuanced effects of class-balancing on group robustness"}, {"figure_path": "eHzIwAhj06/figures/figures_17_2.jpg", "caption": "Figure 1: Class-balanced upsampling and upweighting experience catastrophic collapse. We compare subsetting, wherein data is removed to set every class to the same size as the smallest class, upsampling, wherein the sampling probabilities of each class are adjusted so that the mini-batches are class-balanced in expectation, and upweighting, wherein the loss for the smaller classes is scaled by the class-imbalance ratio. We observe a catastrophic collapse over the course of training of upsampling and upweighting on CelebA and CivilComments, the more class-imbalanced datasets. Subsetting reduces WGA on Waterbirds because it removes data from the small minority group within the majority class. MultiNLI is class-balanced a priori, so we do not include it here.", "description": "The figure shows the test worst-group accuracy (WGA) over training epochs for three different class-balancing techniques: subsetting, upsampling, and upweighting, compared to the no class-balancing baseline.  The results are shown for three datasets: Waterbirds, CelebA, and CivilComments.  Upsampling and upweighting show a catastrophic collapse in WGA on CelebA and CivilComments, while subsetting reduces WGA on Waterbirds due to the dataset's specific group structure.  MultiNLI, being already class-balanced, is not included.", "section": "3 Nuanced effects of class-balancing on group robustness"}, {"figure_path": "eHzIwAhj06/figures/figures_18_1.jpg", "caption": "Figure 1: Class-balanced upsampling and upweighting experience catastrophic collapse. We compare subsetting, wherein data is removed to set every class to the same size as the smallest class, upsampling, wherein the sampling probabilities of each class are adjusted so that the mini-batches are class-balanced in expectation, and upweighting, wherein the loss for the smaller classes is scaled by the class-imbalance ratio. We observe a catastrophic collapse over the course of training of upsampling and upweighting on CelebA and CivilComments, the more class-imbalanced datasets. Subsetting reduces WGA on Waterbirds because it removes data from the small minority group within the majority class. MultiNLI is class-balanced a priori, so we do not include it here.", "description": "The figure shows the test worst-group accuracy (WGA) over training epochs for three different class-balancing techniques: subsetting, upsampling, and upweighting, compared to the baseline without class balancing.  The results are shown for three datasets: Waterbirds, CelebA, and CivilComments.  The figure demonstrates that upsampling and upweighting can lead to a significant decrease in WGA as training progresses, a phenomenon called \"catastrophic collapse\". Subsetting has varied effects depending on the dataset's group structure. MultiNLI is excluded because it's already class-balanced.", "section": "3 Nuanced effects of class-balancing on group robustness"}, {"figure_path": "eHzIwAhj06/figures/figures_19_1.jpg", "caption": "Figure 3: Scaling class-balanced pretrained models can improve worst-group accuracy. We finetune each model size starting from pretrained checkpoints and plot the test worst-group accuracy (WGA) as well as the interpolation threshold, where model reaches 100% training accuracy. We find that model scaling is generally beneficial for WGA only in conjunction with appropriate class-balancing, and scaling on imbalanced datasets or with the wrong method can harm robustness. Note MultiNLI is class-balanced a priori and is not interpolated. See Appendix C for training accuracy plots.", "description": "This figure shows the effect of model scaling on worst-group accuracy (WGA) for four datasets (Waterbirds, CelebA, CivilComments, MultiNLI) using different class-balancing techniques.  It demonstrates that scaling pretrained models improves WGA but only when combined with appropriate class balancing.  Scaling without proper class balancing or on already imbalanced datasets can negatively impact WGA.  The interpolation threshold is also shown, indicating when models reach 100% training accuracy.", "section": "Model scaling improves WGA of class-balanced finetuning"}, {"figure_path": "eHzIwAhj06/figures/figures_19_2.jpg", "caption": "Figure 3: Scaling class-balanced pretrained models can improve worst-group accuracy. We finetune each model size starting from pretrained checkpoints and plot the test worst-group accuracy (WGA) as well as the interpolation threshold, where model reaches 100% training accuracy. We find that model scaling is generally beneficial for WGA only in conjunction with appropriate class-balancing, and scaling on imbalanced datasets or with the wrong method can harm robustness. Note MultiNLI is class-balanced a priori and is not interpolated. See Appendix C for training accuracy plots.", "description": "This figure shows the impact of model scaling on worst-group accuracy (WGA) for four different datasets (Waterbirds, CelebA, CivilComments, and MultiNLI) with different class-balancing techniques.  It demonstrates that scaling pretrained models improves WGA but only when combined with appropriate class balancing.  Scaling with inappropriate techniques or imbalanced data can harm robustness.  MultiNLI is unique because it's already class-balanced.", "section": "Model scaling improves WGA of class-balanced finetuning"}, {"figure_path": "eHzIwAhj06/figures/figures_20_1.jpg", "caption": "Figure 3: Scaling class-balanced pretrained models can improve worst-group accuracy. We finetune each model size starting from pretrained checkpoints and plot the test worst-group accuracy (WGA) as well as the interpolation threshold, where model reaches 100% training accuracy. We find model scaling is generally beneficial for WGA only in conjunction with appropriate class-balancing, and scaling on imbalanced datasets or with the wrong method can harm robustness. Note MultiNLI is class-balanced a priori and is not interpolated. See Appendix C for training accuracy plots.", "description": "This figure displays the results of an experiment on the impact of model scaling on worst-group accuracy (WGA) when finetuning pretrained models. The experiments were performed on four datasets with varying class-balancing techniques. The results show that model scaling improves WGA only when used with appropriate class balancing, and that scaling on imbalanced datasets can even be harmful to WGA.", "section": "Model scaling improves WGA of class-balanced finetuning"}, {"figure_path": "eHzIwAhj06/figures/figures_21_1.jpg", "caption": "Figure 5: Group disparities are visible in the top eigenvalues of the group covariance matrices. We visualize the mean, across 3 experimental trials, of the top 10 eigenvalues of the group covariance matrices for a ConvNeXt-V2 Nano finetuned on Waterbirds and CelebA and a BERT Small finetuned on CivilComments and MultiNLI. The standard deviations are omitted for clarity. The models are finetuned using the best class-balancing method from Section 3 for each dataset. The group numbers are detailed in Table 2 and the minority groups within each class are denoted with an asterisk. The largest \u03bb\u2081 in each case belongs to a minority group, though it may not be the worst group, and minority group eigenvalues are overall larger than majority group eigenvalues within the same class.", "description": "The figure visualizes the top 10 eigenvalues of group covariance matrices for four datasets (Waterbirds, CelebA, CivilComments, MultiNLI) after finetuning with the best class-balancing method.  It highlights that the largest eigenvalue often belongs to a minority group, and minority groups generally have larger eigenvalues than majority groups within the same class, suggesting a spectral imbalance that might contribute to group disparities.", "section": "Spectral imbalance may exacerbate group disparities"}, {"figure_path": "eHzIwAhj06/figures/figures_21_2.jpg", "caption": "Figure 5: Group disparities are visible in the top eigenvalues of the group covariance matrices. We visualize the mean, across 3 experimental trials, of the top 10 eigenvalues of the group covariance matrices for a ConvNeXt-V2 Nano finetuned on Waterbirds and CelebA and a BERT Small finetuned on CivilComments and MultiNLI. The standard deviations are omitted for clarity. The models are finetuned using the best class-balancing method from Section 3 for each dataset. The group numbers are detailed in Table 2 and the minority groups within each class are denoted with an asterisk. The largest \u03bb\u2081 in each case belongs to a minority group, though it may not be the worst group, and minority group eigenvalues are overall larger than majority group eigenvalues within the same class.", "description": "This figure visualizes the top 10 eigenvalues of group covariance matrices for four datasets (Waterbirds, CelebA, CivilComments, MultiNLI) after fine-tuning with the best class balancing methods.  It highlights that minority groups tend to have larger eigenvalues than majority groups within the same class, suggesting a spectral imbalance that could contribute to group disparities.  Note that the largest eigenvalue isn't always from the worst-performing group.", "section": "5 Spectral imbalance may exacerbate group disparities"}, {"figure_path": "eHzIwAhj06/figures/figures_21_3.jpg", "caption": "Figure 5: Group disparities are visible in the top eigenvalues of the group covariance matrices. We visualize the mean, across 3 experimental trials, of the top 10 eigenvalues of the group covariance matrices for a ConvNeXt-V2 Nano finetuned on Waterbirds and CelebA and a BERT Small finetuned on CivilComments and MultiNLI. The standard deviations are omitted for clarity. The models are finetuned using the best class-balancing method from Section 3 for each dataset. The group numbers are detailed in Table 2 and the minority groups within each class are denoted with an asterisk. The largest \u03bb\u2081 in each case belongs to a minority group, though it may not be the worst group, and minority group eigenvalues are overall larger than majority group eigenvalues within the same class.", "description": "This figure visualizes the top 10 eigenvalues of group covariance matrices for four datasets (Waterbirds, CelebA, CivilComments, MultiNLI) after finetuning with the best class-balancing method.  It shows that minority groups tend to have larger top eigenvalues than majority groups within the same class, indicating a potential spectral imbalance that might contribute to group disparities in model performance.", "section": "5 Spectral imbalance may exacerbate group disparities"}, {"figure_path": "eHzIwAhj06/figures/figures_22_1.jpg", "caption": "Figure 5: Group disparities are visible in the top eigenvalues of the group covariance matrices. We visualize the mean, across 3 experimental trials, of the top 10 eigenvalues of the group covariance matrices for a ConvNeXt-V2 Nano finetuned on Waterbirds and CelebA and a BERT Small finetuned on CivilComments and MultiNLI. The standard deviations are omitted for clarity. The models are finetuned using the best class-balancing method from Section 3 for each dataset. The group numbers are detailed in Table 2 and the minority groups within each class are denoted with an asterisk. The largest \u03bb\u2081 in each case belongs to a minority group, though it may not be the worst group, and minority group eigenvalues are overall larger than majority group eigenvalues within the same class.", "description": "This figure visualizes the top 10 eigenvalues of group covariance matrices for four datasets (Waterbirds, CelebA, CivilComments, MultiNLI) after finetuning with different class-balancing methods.  It shows that minority groups tend to have larger top eigenvalues compared to majority groups within the same class, even after class balancing, suggesting a potential source of group disparities.", "section": "Spectral imbalance may exacerbate group disparities"}, {"figure_path": "eHzIwAhj06/figures/figures_22_2.jpg", "caption": "Figure 1: Class-balanced upsampling and upweighting experience catastrophic collapse. We compare subsetting, wherein data is removed to set every class to the same size as the smallest class, upsampling, wherein the sampling probabilities of each class are adjusted so that the mini-batches are class-balanced in expectation, and upweighting, wherein the loss for the smaller classes is scaled by the class-imbalance ratio. We observe a catastrophic collapse over the course of training of upsampling and upweighting on CelebA and CivilComments, the more class-imbalanced datasets. Subsetting reduces WGA on Waterbirds because it removes data from the small minority group within the majority class. MultiNLI is class-balanced a priori, so we do not include it here.", "description": "The figure displays the test worst-group accuracy (WGA) over training epochs for three class-balancing techniques (subsetting, upsampling, upweighting) and a no class-balancing baseline on three datasets (Waterbirds, CelebA, CivilComments). Upsampling and upweighting show a catastrophic collapse in WGA on CelebA and CivilComments, whereas subsetting reduces WGA on Waterbirds due to its removal of data from a minority group in the majority class.  MultiNLI is excluded as it's already class balanced.", "section": "3 Nuanced effects of class-balancing on group robustness"}, {"figure_path": "eHzIwAhj06/figures/figures_23_1.jpg", "caption": "Figure 6: Group-wise spectral imbalance is apparent once conditioned on the classes. We plot the mean and standard deviation, across 3 experimental trials, of the intra-class spectral norm ratio p(y), or the ratio of the top eigenvalues of the minority and majority group covariance matrices, for each class y \u2208 Y. We compute this metric using a finetuned ConvNeXt-V2 Nano on Waterbirds and CelebA and a finetuned BERT Small on CivilComments and MultiNLI, each using the best class-balancing method from Section 3 for each dataset. The key observation is that p(y) is at least one for all classes y \u2208 Y (except a single seed for class 0 on CelebA), illustrating a group disparity captured by the eigenspectrum once we condition on the classes.", "description": "This figure shows the intra-class spectral norm ratio (p(y)) for each class in four different datasets.  The p(y) metric is the ratio of the largest eigenvalue of the minority group's covariance matrix to that of the majority group's covariance matrix, within each class. The results demonstrate that, even when classes are balanced, a spectral imbalance exists, where minority groups consistently exhibit larger spectral norms than majority groups within the same class. This suggests a potential link between spectral imbalance and group disparities.", "section": "Spectral imbalance may exacerbate group disparities"}]