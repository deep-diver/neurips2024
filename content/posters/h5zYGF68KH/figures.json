[{"figure_path": "h5zYGF68KH/figures/figures_0_1.jpg", "caption": "Figure 1: Pipeline overview. PaGoDA deterministically encodes with downsampling followed by DDIM inversion, and constructs its decoder in a progressively growing manner.", "description": "This figure shows the pipeline of PaGoDA, which consists of three stages. Stage 1 is diffusion pretraining on downsampled data. Stage 2 is diffusion distillation (using DDIM inversion) to a one-step generator. Stage 3 is super-resolution, progressively upscaling the generator. The figure visually demonstrates how PaGoDA efficiently encodes high-resolution images by downsampling, then progressively grows its decoder to achieve high-resolution generation.", "section": "PaGoDA's Proposed Training Pipeline"}, {"figure_path": "h5zYGF68KH/figures/figures_1_1.jpg", "caption": "Figure 1: Pipeline overview. PaGoDA deterministically encodes with downsampling followed by DDIM inversion, and constructs its decoder in a progressively growing manner.", "description": "This figure illustrates the three stages of the PaGoDA pipeline.  First, a high-resolution image is downsampled. Then, DDIM inversion is applied to the downsampled image, which creates a latent representation. This latent representation is then used by a one-step generator to reconstruct the image at the lower resolution. Finally, a super-resolution stage progressively upscales the generated low-resolution image to the original high resolution.", "section": "PaGoDA's Proposed Training Pipeline"}, {"figure_path": "h5zYGF68KH/figures/figures_1_2.jpg", "caption": "Figure 2: (Top) At Stage 2, PaGoDA learns the one-step generator at a base resolution. (Down) At Stage 3, PaGoDA progressively learns for super-resolution by adding additional network blocks.", "description": "This figure illustrates the architecture of PaGoDA's decoder during the progressive growing stage. The top half shows the network architecture at Stage 2 (distillation), which takes a latent vector as input and generates an image at the base resolution. The bottom half shows the architecture at Stage 3 (super-resolution), where additional blocks are added to progressively increase the resolution of the generated image. The blocks marked with an asterisk (*) are frozen during training.", "section": "PaGoDA's Proposed Training Pipeline"}, {"figure_path": "h5zYGF68KH/figures/figures_3_1.jpg", "caption": "Figure 3: Effect of the reconstruction loss in Stage 3. Without the reconstruction loss, the object moves at each resolution jump.", "description": "This figure shows the effect of using the reconstruction loss in Stage 3 of the PaGoDA model.  Two sets of images are shown: one where the reconstruction loss was used (a) and one where it was not used (b). Each set shows an image upscaled progressively from 64x64 to 512x512 resolution. The images in (a) show consistent object positioning across all resolutions, highlighting the stabilizing effect of the reconstruction loss.  In contrast, the images in (b) demonstrate that the object moves slightly as the resolution increases when the reconstruction loss is omitted.  This clearly shows the value of the reconstruction loss in ensuring consistent object placement during upscaling. ", "section": "3.3 Stage 3: Progressively Growing Decoder for Super-Resolution"}, {"figure_path": "h5zYGF68KH/figures/figures_4_1.jpg", "caption": "Figure 4: The adversarial loss makes PaGoDA competitive with GAN-based super-resolution models in Stage 3.", "description": "The figure shows a plot comparing the Fr\u00e9chet Inception Distance (FID) scores achieved by PaGoDA and StyleGAN-XL across different image resolutions (64x64, 128x128, 256x256, and 512x512).  The results demonstrate that PaGoDA, with the help of the adversarial loss, achieves FID scores comparable to those of StyleGAN-XL, a state-of-the-art GAN-based super-resolution model, particularly at higher resolutions. This supports PaGoDA's effectiveness in Stage 3 (Super-Resolution) of its progressive growing pipeline.", "section": "5.1.1 Quantitative Results"}, {"figure_path": "h5zYGF68KH/figures/figures_4_2.jpg", "caption": "Figure 5: Comparison of  Ldstl and Lrec, both combined with Ladv, using identical hyperparameters. Lrec shows the robust performance, also supported by Theorem 3.1.", "description": "This figure compares the Fr\u00e9chet Inception Distance (FID) scores achieved using two different loss functions: Ldstl (noise-to-data distillation loss) and Lrec (reconstruction loss), each combined with an adversarial loss (Ladv).  The results demonstrate that the reconstruction loss (Lrec) shows significantly more robust performance than the noise-to-data distillation loss (Ldstl), even when using a weaker teacher model (higher Teacher FID score). This robustness is supported by Theorem 3.1 within the paper.", "section": "3 Progressive Growing of Diffusion Autoencoder"}, {"figure_path": "h5zYGF68KH/figures/figures_6_1.jpg", "caption": "Figure 6: Uncurated samples generated by PaGoDA at resolution 512 \u00d7 512 without CFG. Left: class 31 (tree frog); Right: class 33 (loggerhead turtle).", "description": "This figure shows examples of images generated by the PaGoDA model at a resolution of 512x512 pixels without using Classifier-Free Guidance (CFG). The left side displays images of class 31 (tree frog), while the right side shows images of class 33 (loggerhead turtle).  The images are presented as an uncurated sample of the model's output, meaning they are not hand-picked or specially selected to showcase the model's best results but rather provide a representation of the model's typical output.", "section": "5.1 PaGoDA Tested on ImageNet without CFG"}, {"figure_path": "h5zYGF68KH/figures/figures_8_1.jpg", "caption": "Figure 8: Controllable generation of PaGoDA with various tasks.", "description": "This figure demonstrates the versatility of the PaGoDA model in handling various image manipulation tasks.  Panel (a) showcases inpainting, where missing parts of an image are filled in realistically. Panel (b) shows super-resolution, where a low-resolution image is upscaled to a higher resolution with improved detail. Panel (c) presents class transfer, where the features of one object are transferred to another, changing the object's appearance while maintaining its overall structure.  Lastly, panel (d) illustrates latent interpolation, where smooth transitions are created between different images by blending their latent representations.", "section": "5.2 Discussion on Controllability"}, {"figure_path": "h5zYGF68KH/figures/figures_8_2.jpg", "caption": "Figure 7: Comparison between PaGoDA and CDM.", "description": "This figure compares the Fr\u00e9chet Inception Distance (FID) scores of PaGoDA and CDM across different resolutions (32, 64, 128, 256).  It demonstrates PaGoDA's superior performance at higher resolutions, highlighting its robustness and ability to maintain image quality even at increased complexity.  In contrast, CDM shows a significant increase in FID scores as the resolution increases, indicating a decline in image generation quality.", "section": "5.1.2 Discussion on Base Resolution"}, {"figure_path": "h5zYGF68KH/figures/figures_9_1.jpg", "caption": "Figure 9: PaGoDA offers faster inference than the one-step LCM.", "description": "This figure compares the sampling speed of PaGoDA and LCM (Latent Consistency Model).  It shows a stacked bar chart with two bars representing LCM and PaGoDA. The LCM bar is divided into two sections, \"64x64 Latent (z)\" and \"Decoder (z \u2192 x)\", representing the time taken for latent space processing and upscaling to image. The PaGoDA bar is similarly divided into \"64x64 Pixel\" and \"Prog. Growing\", representing processing time on the downsampled image and the progressive growing super-resolution stage. A red arrow points from the top of the LCM Decoder section to the PaGoDA Prog. Growing section, labeled with \"2x Faster\", indicating that PaGoDA's inference is significantly faster than LCM. This is due to PaGoDA's use of a single-step generator with direct generation of high-resolution images compared to the multi-step approach of LCM.", "section": "5 Experiments"}, {"figure_path": "h5zYGF68KH/figures/figures_23_1.jpg", "caption": "Figure 6: Uncurated samples generated by PaGoDA at resolution 512 \u00d7 512 without CFG. Left: class 31 (tree frog); Right: class 33 (loggerhead turtle).", "description": "This figure shows example images generated by the PaGoDA model at a resolution of 512x512 pixels, without using classifier-free guidance.  The two example images shown represent different classes from the ImageNet dataset: a tree frog (class 31) and a loggerhead turtle (class 33). The images demonstrate the model's ability to generate diverse and realistic-looking samples at high resolution.", "section": "5.1 PaGoDA Tested on ImageNet without CFG"}, {"figure_path": "h5zYGF68KH/figures/figures_25_1.jpg", "caption": "Figure 12: Example of recaptioned image-text pair.", "description": "This figure shows an example of how the LLaVA model re-captioned an image-text pair.  The original caption, \"Watch your head as you enter the <PERSON> house\", is contrasted with the LLaVA-generated caption, which provides a more detailed and descriptive rendering of the scene depicted in the image. This illustrates the use of LLaVA to improve the quality and accuracy of text-image pairings for the training data.", "section": "4.1 Classifier-Free Guided Adversarial Loss"}, {"figure_path": "h5zYGF68KH/figures/figures_25_2.jpg", "caption": "Figure 13: Caption vs. Recaption. From left to right, CFG scale increases. The caption and its corresponding recaption are given by the exemplary case in Figure 12.", "description": "This figure shows the comparison between caption generation and recaption generation using DeepFloyd-IF and PaGoDA models. The left two images display the original captions generated by both models, and the right two images show the recaptioned results. Each row represents a different CFG scale, demonstrating how the quality of generation changes with the scale.  The recaptioned samples generally outperform the original captions, especially when CFG scale is small. The recaptioning method is used to generate more relevant and accurate text-image pairs.", "section": "5.3 Text-to-Image Generation"}, {"figure_path": "h5zYGF68KH/figures/figures_25_3.jpg", "caption": "Figure 13: Caption vs. Recaption. From left to right, CFG scale increases. The caption and its corresponding recaption are given by the exemplary case in Figure 12.", "description": "This figure compares the caption generation results of the DeepFloyd-IF model and the PaGoDA model with different CFG scales. It shows that the recaptioned samples generally outperform the original caption samples, particularly when the CFG scale is small. The recaptioning process involves using a language model with vision assistance to generate more relevant and accurate descriptions of the images based on the text prompts.", "section": "5.3 Text-to-Image Generation"}, {"figure_path": "h5zYGF68KH/figures/figures_26_1.jpg", "caption": "Figure 15: Discriminator architecture.", "description": "This figure shows the architecture of the discriminator used in PaGoDA.  The discriminator takes as input DINO features, text CLIP embeddings, and the CFG (classifier-free guidance) scale (\u03c9).  It consists of multiple transformer blocks processing the text embedding and multiple convolutional layers processing the DINO features.  The CFG scale is incorporated into the discriminator through an additional embedding that's combined with the feature and text embeddings before being passed to the discriminator head. The per-token hinge loss is used for training the discriminator.", "section": "4.1 Classifier-Free Guided Adversarial Loss"}, {"figure_path": "h5zYGF68KH/figures/figures_26_2.jpg", "caption": "Figure 16: Effect of CLIP regularization.", "description": "The figure shows the FID and CLIP scores for three different models: DeepFloyd-IF, PaGoDA with CLIP regularization, and PaGoDA without CLIP regularization. The x-axis represents the CLIP score, and the y-axis represents the FID score. The figure demonstrates that CLIP regularization improves both FID and CLIP scores, indicating improved sample quality and alignment between text and images.", "section": "4 PaGoDA with Classifier-Free Guidance"}, {"figure_path": "h5zYGF68KH/figures/figures_27_1.jpg", "caption": "Figure 17: Human evaluation result on T2I with CFG set to be 7 across models.", "description": "This figure shows the results of a human evaluation comparing the image quality and prompt alignment of different text-to-image models.  The models compared include PaGoDA (with and without several components such as CFG, reconstruction loss, and CLIP), DeepFloyd, SD1.5, and LCM (SD1.5).  The evaluation uses a preference rate, with higher scores indicating better performance.  PaGoDA generally performs well against the other models, and removing key components negatively impacts its performance.", "section": "5 Experiments"}]