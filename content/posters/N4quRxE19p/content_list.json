[{"type": "text", "text": "AVATAR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shirley $\\mathbf{W}\\mathbf{u}^{\\S}$ , Shiyu Zhao\u00a7, Qian Huang\u00a7, Kexin Huang\u00a7, Michihiro Yasunaga\u00a7, Kaidi Cao\u00a7 Vassilis N. Ioannidis\u2020, Karthik Subbian\u2020, Jure Leskovec\u2217\u00a7, James $\\mathbf{Zou^{*\\S}}$ ", "page_idx": 0}, {"type": "text", "text": "\u2217Equal senior authorship. \u00a7Department of Computer Science, Stanford University \u2020Amazon ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AVATAR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate AVATAR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AVATAR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of $14\\%$ on the $\\operatorname{Hit}\\@1$ metric for the retrieval datasets and $13\\%$ for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Autonomous agents powered by large language models (LLMs) offer substantial promise for complex problem-solving [6, 39, 41, 55, 65]. These agents demonstrate remarkable capabilities in reasoning [46, 47, 54, 55] and planning [8, 13, 14, 62]. Additionally, their functionality is extended through the use of external tools that provide access to external or private data and specialized operations, such as APIs for interacting with knowledge bases and search engines. These tools enable agents to perform complex tasks like multi-step problem-solving and retrieving diverse information, which is essential for complex retrieval and question-answering (QA) [13, 21, 26, 33, 38, 40, 48]. ", "page_idx": 0}, {"type": "text", "text": "Despite the promising capabilities of LLM agents, it remains challenging to engineer effective prompts that guide these agents through a multi-stage process for real-world problem-solving. This process involves (1) decomposing a complex question into an actionable plan with simpler steps, (2) strategically using provided tools to gather relevant information, and, finally, (3) synthesizing intermediate results to produce a coherent and accurate response. Each step requires extensive manual effort and numerous iterations of trial and error to refine the prompts. ", "page_idx": 0}, {"type": "text", "text": "Current approaches have primarily focused on directly deploying agents using complex humandesigned \u201cmega-prompts\u201d [18, 24, 55], which require lots of manual trial and error. Nevertheless, such hand-engineered mega-prompts may also result in brittle implementations with suboptimal accuracy (see Figure 2 (a)), where the ReAct agent [55] easily produces trivial and misleading answers to customers\u2019 queries about specific products. Furthermore, existing research [4, 5, 45, 50, 56, 60, 64] on employing LLMs as optimizers often fails to adequately refine the complex strategies for enhancing tool integration and usage. This lack of strategic optimization can lead to less effective, nongeneralizable agent applications in complex real-world scenarios. ", "page_idx": 0}, {"type": "image", "img_path": "N4quRxE19p/tmp/25dedf9752f2f4bbcf84d28f1bb73972a186f26de8f61c1818d6f3a622b527a3.jpg", "img_caption": ["Figure 1: Overview of AVATAR. AVATAR consists of a actor LLM and a comparator LLM. (a) During optimization, the actor generates actions to answer queries by leveraging the provided tools. Then, the comparator contrasts a set of well-performing (positive) and poorly-performing (negative) queries, automatically generating holistic prompts to teach the actor more effective retrieval strategies and tool usage (cf. Section 4). (b) At deployment, the actor with optimized prompts or actions can be effectively used to answer new queries. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Present work: AVATAR. To address these challenges, we introduce AVATAR, an automated framework that optimizes agents for effective tool utilization and excellent task performance. Specifically, we leverage key insights from contrastive reasoning and build a comparator module (\u201ctrainer\u201d) to generate holistic instructions and prompts (i.e., , computing a robust \u201cgradient\u201d) to optimize an actor LLM. We demonstrate our framework on challenging tasks of knowledge base retrieval, which involve complex multi-stage procedures and extensive tool usage, and general QA tasks. Specifically, AVATAR includes two phases: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Optimization phase. The core of our optimization framework (Figure 1) is a comparator LLM that automatically generates holistic prompts to teach a actor LLM to differentiate between effective and ineffective tool usage. The comparator takes positive and negative data samples, where the current agent performs well and poorly, respectively, to identify overall gaps and systematic errors exhibited by the agent. Unlike per-sample instructions, which can easily lead to overfitting on individual data points, by constructing multiple samples as a \u201cbatch,\u201d the comparator can extract a more robust \u201cgradient\u201d to \u201cbackpropagate\u201d to the actor. In other words, the comparator can provide more effective and adaptive prompts through batch-wise contrastive reasoning, helping the agent identify flaws in solving challenging multi-stage problems. Following previous methods [30, 41, 56, 63], we also maintain a memory bank with selected past instructions to prevent the actor LLM from repeating previous mistakes. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Deployment phase. After the optimization phase, the actor with best-performing prompts can be selected for the testing instances. Moreover, in complex retrieval tasks, the iterative optimization through our AVATAR framework updates the actor for more effective and generalizable action sequences, enabling direct generalization to novel user inquiries at deployment. In Figure 2 (b), the optimized actor creates three novel strategies: 1) precise decomposition of problems by extracting multifaceted attributes, 2) effective tool usage through a sophisticated and robust scoring system, and 3) the strategic combination of different scores, determined by learned coefficients, ensuring accurate and comprehensive retrieval. ", "page_idx": 1}, {"type": "text", "text": "Experimental evaluation. We conduct extensive experiments on four retrieval datasets and three QA datasets. The retrieval tasks are highly complex, involving multimodal data, including textual, visual, and relational information. AVATAR consistently outperforms state-of-the-art methods, showing a substantial $14\\%$ improvement in the Hit $@1$ metric. Impressively, with only 25 iterations, AVATAR boosts the $\\operatorname{Hit}\\!\\circledcirc1$ metric from an initial $5.1\\%$ to $28.6\\%$ on FLICKR30K-ENTITIES [35] and the Recall $@20$ metric from $30.3\\%$ to $39.3\\%$ on STARK-PRIME [49]. For general QA datasets, AVATAR outperforms state-of-the-art methods by $13\\%$ on average. These improvements, achieved through iterative updates to the prompts, underscore AVATAR\u2019s ability to optimize agents for complex tasks and effective tool usage. Our key contributions are: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce AVATAR, a novel framework that optimizes an actor for effective tool utilization through a comparator module that automatically generates holistic prompts. \u2022 We demonstrate AVATAR on four complex retrieval tasks and three QA tasks, where it significantly outperforms existing agent methods in terms of task performance and generalization ability. \u2022 We provide a comprehensive analysis of the actor\u2019s evolution during optimization, highlighting how comparator automatically provides targeted instructions that improve and generalize the actor. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LLM Agents. Recent research has leveraged the remarkable language understanding and reasoning abilities of LLMs [1, 41, 47, 54, 55] to complete downstream tasks. For complex tasks that require enhanced capabilities, previous works have positioned LLMs as agents that can interact with environments [4, 6, 13, 18, 21, 26, 27, 40, 48, 55], leverage external tools [6, 28, 31, 33, 36, 38, 39, 66, 68], and gather experiences [7, 61]. For example, ReAct [55] conducts reasoning and action in an interleaved way, retrieving information from Wikipedia to support reasoning. ", "page_idx": 2}, {"type": "text", "text": "LLM Agents for Retrieval. Previous research has applied LLM agents to Information Retrieval (IR) systems through pretraining [2, 9, 16, 57], reranking [12, 42], and prompting techniques [11, 18]. In IR systems, the retriever module directly influences the performance of downstream tasks, such as retrieval-augmented generation [20, 29, 30] and knowledge-intensive question answering [34, 52]. For example, EHRAgent [40] is designed for EHR question-answering, capable of retrieving relevant clinical knowledge through a structured tool-use planning process and an interactive coding mechanism. However, these LLM agents usually employ heuristic (zero-shot) prompts or rely on few-shot examples [18, 25, 40, 55] for downstream tasks, which lack more informed guidance on generating effective retrieval strategies and tool-assisted actions. ", "page_idx": 2}, {"type": "text", "text": "Agent Optimization. In the field of optimizing LLM agents, previous works have modified the parameters of LLM backbones through fine-tuning or instruction tuning to enhance agent capability [3, 15, 19, 23, 32, 33, 37, 43, 51, 58, 59] or generated better prompts through iterative prompt tuning [11, 18, 45, 50, 56]. Recently, Zhang et al. [60] conducted agent training by iteratively updating the agents\u2019 functions according to the execution history. However, these methods do not explicitly consider targeted optimization for tool usage or the impact on complex multi-stage tasks. Additionally, enhancing agents\u2019 generalization abilities [10, 31, 44], essential for real-world applications, has received less attention. In our work, we focus on automatically generating holistic instructions via a novel contrastive reasoning mechanism, targeting effective tool usage and agents\u2019 generalization ability. Compared to fine-tuning approaches, AvaTaR offers advantages by requiring only a small subset of training data and tool descriptions, making it more adaptable and less computationally intensive. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 1: Tools. We define tools or APIs as a set of implemented functions with specified input and output variables. We denote the abstract tool space as ${\\mathcal{T}}=\\{f_{k}:{\\mathcal{T}}_{f_{k}}\\to{\\mathcal{O}}_{f_{k}}\\mid{\\bar{k}}=1,2,.\\;.\\;.\\}$ , where $f_{k}$ maps the input $\\mathcal{T}_{f_{k}}$ to the output $O_{f_{k}}$ . For example, the tools can be APIs used for accessing external knowledge via a search index, an encoder model that generates vector representations from text or image data, or a task-specific classifier that outputs probabilities over a list of classes. ", "page_idx": 2}, {"type": "text", "text": "Definition 2: Agents. An LLM agent, defined as $A:\\mathcal{P}\\rightarrow\\alpha$ , is controlled by verbal prompts to generate a flow of actions needed to complete a task. Here $\\alpha$ denotes the action sequence $[\\alpha_{1},\\ldots,\\alpha_{L}]$ , where each action is defined by a tuple $(f\\,\\in\\,\\mathcal{T},i\\,\\in\\,\\mathcal{Z}_{f},o\\,\\in\\,\\mathcal{O}_{f})$ , consisting of a tool function, specified input(s), and a designated variable that receives the output(s). Each action in the sequence can leverage the outputs generated by previous actions, with the final action $\\alpha_{L}$ rendering the results for the task. ", "page_idx": 2}, {"type": "image", "img_path": "N4quRxE19p/tmp/17650feb0cccaf2a8b6fdb7558cebc7d506c18c72038482ac472c0aea722c18e.jpg", "img_caption": ["Figure 2: Comparison between AVATAR and ReAct. (a) The ReAct agent exhibits incomplete task decomposition and employs suboptimal tool combinations, such as lengthy string matching, leading to poor task performance. (b) AVATAR decomposes the task into multiple steps, such as type flitering and flexible token matching. Moreover, it implements robust tool usage and precise synthesis with learned parameters from the optimization phase to achieve excellent performance on new queries. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Multi-step problem-solving. Real-world problems are inherently complex and cannot be effectively addressed through straightforward solutions or simple tool usage alone. Solving real-world problems with LLM agents can be structured into a multi-stage procedure: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Decomposition of the problem: The procedure begins by breaking down a complex question into an actionable plan characterized by simpler steps. This decomposition is crucial for setting clear objectives and facilitating focused problem-solving. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Tool-assisted subproblem solving: In the subsequent phase, agents strategically utilize tools from the established tool space $\\tau$ to gather solutions for each step. This stage is essential for acquiring the necessary information required to effectively address each subproblem of the decomposed problem. ", "page_idx": 3}, {"type": "table", "img_path": "N4quRxE19p/tmp/e1985b11fa3d5eedf0bc6cbe839b183f11c0b7c46716b720f45482266a2b233e.jpg", "table_caption": ["Table 1: Key differences between AVATAR and prevailing agent methods. AVATAR demonstrates the ability to: 1) self-improve on specific tasks, 2) retain memory throughout the optimization process, 3) enhance the agent\u2019s generalization capability, and 4) autonomously generate holistic, high-quality prompts for better tool usage. Please refer to Section 4 for details. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "\u2022 Synthesis and response formulation: The final stage involves synthesizing the intermediate results to construct a precise response. This synthesis not only combines the data but may also refine the response through trials and adjustments, ensuring the solution\u2019s accuracy and relevance. ", "page_idx": 4}, {"type": "text", "text": "For example, retrieval tasks are inherently complex and demanding. Given a user query $q$ , retrieval tasks aim to identify or generate a ranked list of relevant entities $E$ from the entity space of a knowledge base. Each query is associated with a set of ground truth answers, denoted as $Y$ , which are used to compute the quality of the prediction. Specifically, the LLM agent is required to 1) comprehend a user\u2019s request, 2) utilize the provided tools to identify and analyze relevant information in the large knowledge space, which may contain multimodal data sources, and finally, 3) integrate all gathered information to reason and generate an accurate response. ", "page_idx": 4}, {"type": "text", "text": "4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Each step in the multi-stage problem-solving process (described in Section 3) requires effective prompts to identify key flaws and improve task performance. However, refining the agents\u2019 prompts demands extensive manual effort and numerous iterations of trial and error. ", "page_idx": 4}, {"type": "text", "text": "To address this, we introduce an automated and novel optimization framework, AVATAR, which generates prompts to improve agents\u2019 tool usage and task performance. In Table 1, we highlight four critical aspects of our approach compared with prevailing agent frameworks [27, 41, 55]. Here, we introduce the two main LLM components in AVATAR: a actor LLM (Section 4.1) and a comparator LLM (Section 4.2). ", "page_idx": 4}, {"type": "text", "text": "4.1 Actor Construction and Challenges ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Actor. The actor agent, as defined in Section 3, is responsible for generating initial actions based on the initial instructions/prompts and adjusting actions according to updated instructions. Specifically, the initial instructions provide details about the task and available tools, where tools can be introduced in programming languages such as Python. During optimization, the prompts further incorporate the previous action sequence and updated instructions to adjust these actions. The actor then generates revised actions, which could include a combination of tool usage through programming language (code generation) along with natural language explanations of how the tools are employed. ", "page_idx": 4}, {"type": "text", "text": "Challenges in multi-step complex tasks. A common approach to updating instructions utilizes execution results or performance data from a specific instance, often through techniques like selfexplanation [4, 27] or self-reflection [41, 56]. However, this approach may not be suitable for complex tasks involving tool usage. Complex multi-step tasks include multiple interacting factors that influence overall performance, such as problem decomposition and tool selection. Consequently, instructions generated for a failed/negative query instance tend to be narrow in scope and may fail to identify flaws across all components of a complex solution. Additionally, while certain tool combinations may be effective for one type of input, their effectiveness can vary across different scenarios, potentially leading to decreased performance when applied to varied cases. ", "page_idx": 4}, {"type": "image", "img_path": "N4quRxE19p/tmp/ad069bc7c643fb6cf571feb10e49e9d9ce76a55655b4ff5f664342704e8ae2b9.jpg", "img_caption": ["Figure 3: Demonstration example during optimization. Best viewed in color. The task of the comparator is to automatically generate instructions based on sampled positive and negative queries. Then comparator provides holistic instructions that guide the actor to improve query decomposition, utilize better tools, and incorporate more comprehensive information. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Automate Holistic Instruction Generation with Comparator ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To address these challenges, we construct a comparator LLM to update the instructions for the actor. Instead of optimizing on a sampled instance, comparator aims to identify systematic flaws throughout the structured actions/solutions. ", "page_idx": 5}, {"type": "text", "text": "Step 1: Constructing positive and negative queries. To achieve this goal, as shown in Figure 1, the comparator samples a set of data (question-answer pairs), evaluates the current action sequence on the queries, and categorizes them into well-performing (positive) and poorly-performing (negative) groups based on their performance. Specifically, we define two thresholds, $\\ell$ and $h$ (where $0<h\\leq\\ell<1)$ ), which serve as the upper and lower bounds for constructing positive and negative queries, respectively. Queries with an evaluation metric (e.g., Recall) value above $\\ell$ are classified as positive, while those below $h$ are classified as negative. Based on the training dynamics, one could consider adapting the lower bound to ensure a sufficient number of negative samples for selection. After classification, we use random sampling to create a mini-batch of $b$ queries, with an equal split of positive and negative queries $(b/2$ each) for contrastive reasoning. ", "page_idx": 5}, {"type": "text", "text": "Step 2: Generating instructions through contrastive reasoning. After this, the comparator is tasked with contrasting the two groups of queries based on their key characteristics, attributing the performance gap to specific tool usage within the complex solution, and finally suggesting general modifications that can improve overall task performance. The instructions generated by the comparator are then appended to the initial prompts to update the actor. ", "page_idx": 5}, {"type": "text", "text": "Insights/Justification for the comparator. To illustrate the insights, we draw an analogy from deep neural network training, where extremely small batch sizes can introduce significant noise in gradient estimates and high variance in model updates. By adopting a batched training strategy and sampling positive and negative queries as two \u201cmini-batches,\u201d comparator can extract a robust \u201cgradient\u201d to update the actor. This approach encourages comparator to generate more general and comprehensive instructions on the complex action sequence, including problem decomposition, solutions to subproblems, and the final synthesis. Moreover, as contrastive reasoning directly targets disentangling the performance gap related to input patterns and how they are handled differently by the tools, it is particularly effective in helping comparator differentiate and select tools for use. Finally, by identifying systemic flaws across a wide array of negative queries, comparator generates modifications that are not only tailored to individual samples but also to diverse data samples, enhancing generalization to novel cases. ", "page_idx": 5}, {"type": "text", "text": "Demonstration example. Figure 3 illustrates an example where comparator contrasts the patterns of positive and negative queries, identifying discrepancies in tool usage within the action sequence. It reveals that, compared to positive queries, negative queries feature more complex product descriptions, more subtle brand mentions, and additional relevant product mentions. These observations suggest: 1) an incomplete problem decomposition involving query attributes like detailed product features, 2) a potentially imprecise brand match using embedding similarity, and 3) a lack of consideration for related products in the results. Informed by these insights, actor updates its action sequence to address these subproblems and use the tools more effectively for the task, such as replacing the embedding tool with an LLM verification tool. ", "page_idx": 6}, {"type": "text", "text": "4.3 Logistic Instructions and Memory Construction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Logistic instructions. While instructions from the comparator are designed to improve task performance, we incorporate two types of orthogonal instructions to ensure the actions are valid and can be executed efficiently. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Validity check: This instruction is triggered internally during the execution of each action. It ensures the validity of the actor\u2019s actions, such as verifying the correct use of function calls. \u2022 Timeout error: To prevent inefficient action sequences that may stall the actor, we implement a timeout mechanism that triggers an error if processing exceeds a specified threshold. This error prompts the actor to adopt more efficient strategies, such as eliminating redundant operations. ", "page_idx": 6}, {"type": "text", "text": "Memory Bank. During optimization, we utilize a memory bank inspired by human decision-making processes, following Shinn et al. [41], where humans typically address current problems by analyzing the current situation and referencing past experiences. The memory bank stores tuples of action sequences, instructions from comparator, and the performance of these action sequences on a small training set (sampled from positive and negative queries). To manage the context size input to actor, we retain only the top-5 action sequences with the best performance. This memory bank enables actor to learn from both immediate instructions and historical results. ", "page_idx": 6}, {"type": "text", "text": "Deployment. At deployment, we can apply the optimized instructions or, as shown in Figure 1, the optimized actor /action sequence, which includes effective tool utilization and problem-solving strategies, to answer queries or retrieve entities. In the experiments, we demonstrate AVATAR\u2019s flexibility by applying different deployment strategies. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tasks and Evaluation. We conduct experiments on the following datasets: ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "\u2022 Four challenging retrieval datasets from STARK [49] and FLICKR30K-ENTITIES [35] to demonstrate AVATAR in handling complex real-world tasks (cf. details in Appendix A). For each query in the retrieval datasets, the task is to retrieve relevant entities, such as nodes in a knowledge graph or images in knowledge bases. During deployment, we directly apply the optimized action sequence to the test queries. We assess task performance by comparing the consistency of the results with the ground truth answers in the datasets, using Hit $@1$ , Hit $@5$ , Recall $@20$ , and Mean Reciprocal Rank (MRR) as the metrics. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Three question-answering (QA) benchmarks: HotpotQA [53], ArxivQA [22], ToolQA [67], where the task is to provide natural language answers to the questions. We sample 100, 100, and 40 training queries, and 100, 100, and 60 testing queries for the three benchmarks, respectively. During deployment, the actor LLM uses optimized instructions to generate the action sequence for obtaining the answer. We use exact match (EM) score on HotpotQA, following previous methods. For ArxivQA and ToolQA, we use the LLM judge score for more reliable evaluation. ", "page_idx": 6}, {"type": "text", "text": "Baselines. For the knowledge retrieval tasks, we employ several embedding-based retriever models for our evaluation, following Wu et al. [49]: Dense Passage Retriever (DPR) Karpukhin et al. [17]; Vector Similarity Search methods ada-002 and multi-ada-002 using text-embedding-ada-002 from OpenAI; and a relation-aware model, QAGNN [57], for the STARK benchmark. Additionally, we include four prevailing agent frameworks to further enrich our evaluation: ", "page_idx": 6}, {"type": "text", "text": "\u2022 ReAct [55] conducts reasoning and action in an in-context and interleaved manner to enable LLMs to interactively analyze observed information and perform actions. ", "page_idx": 6}, {"type": "table", "img_path": "N4quRxE19p/tmp/b71a87d740ca8b1cce2ac41a53fb729956a1f202b06e83d9efc1f50b0c06baee.jpg", "table_caption": ["Table 2: Retrieval performance $(\\%)$ on STARK benchmark. Last row shows the relative improvements over the best metric value in each column. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 Reflexion [41] uses self-reflection on the current task completion and stores these reflections in an episodic memory buffer to enhance decision-making in subsequent trials. ", "page_idx": 7}, {"type": "text", "text": "\u2022 ExpeL [61] extracts insights from successful and failed action sequences, retrieving and including them in the context during inference. We apply ExpeL on the QA datasets and, due to its high cost on large-scale retrieval tasks, compare it with AVATAR on a sampled STARK-MAG test set. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Retroformer [56] reinforces LLM agents and automatically tunes their prompts by learning a retrospective model through policy gradient. We compare the performance of AVATAR with the reported result by Retroformer on HotpotQA due to the additional training involved. ", "page_idx": 7}, {"type": "text", "text": "We include an ablation model, AVATAR-C, which removes the comparator from our optimization pipeline. This comparison aims to validate the effectiveness of the comparator. The LLM version information is provided in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Function library. For the knowledge retrieval tasks, our function library consists of twenty-eight functions that facilitate access to, operation on, and reasoning over the knowledge information by LLM agents. For the QA tasks, we provide web search tools such as Google and Arxiv search APIs. See Appendix E for details. We used the same function library across all agent methods. ", "page_idx": 7}, {"type": "text", "text": "General pipeline. For AVATAR, we optimize the agent for a fixed number of epochs and select the action sequence or instruction with the highest performance. We use the same initial prompt structure, the metric Recall $@20$ or Accuracy for constructing positive and negative queries, and hyperparameters ${\\ell=h=0.5}$ , $b=20$ ) for all datasets. ", "page_idx": 7}, {"type": "text", "text": "5.1 Textual and Relational Retrieval Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We employ the AMAZON, MAG, and PRIME datasets from the STARK benchmark [49], a large-scale semi-structured retrieval benchmark that integrates textual and relational knowledge (cf. detailed description in Appendix A). Here, the entities to be retrieved are defined as nodes in a graph structure, with knowledge associated with each entity including both textual descriptions and relational data. We use the official splits from the STARK benchmark. ", "page_idx": 7}, {"type": "text", "text": "Takeaway 1: AVATAR outperforms state-of-the-art models. Table 3 shows that AVATAR substantially outperforms leading models such as Reflexion across all metrics on the STARK benchmark. Notably, the average improvement of AVATAR is $15.6\\%$ on $\\operatorname{Hit}\\!\\left(\\omega\\right)1$ and $9.5\\%$ on MRR. ReAct agents, however, cannot optimize based on instructions for improved tool usage and tend to select tools based on the LLM\u2019s prior knowledge, which may not be optimal for the given task. We observe that ReAct agents apply similar tools across various queries and struggle to explore alternative tool usage even with extensive in-context reasoning. Results for agent methods using GPT-4 Turbo are provided in Appendix B, showing similar conclusions. For comparison with ExpeL, the results in Table 6 show that it performs similarly to ReAct, underperforming AVATARby a large margin. ", "page_idx": 7}, {"type": "text", "text": "Takeaway 2: Comparator greatly impacts the actor\u2019s performance. The comparison of AVATAR with its ablation variant, AVATAR-C, highlights the significant advantages of the comparator module. Although AVATAR-C conducts validity and timeout checks, integrating Comparator into AVATAR adds a comprehensive instruction mechanism crucial for identifying clear directions to improve the agents, underlining comparator\u2019s key role in optimizing actor. ", "page_idx": 7}, {"type": "image", "img_path": "N4quRxE19p/tmp/bdceed34cc34a4e6b618c13c9bd53b1d528f02db1c19486b9bf1d408f9c3cca5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "N4quRxE19p/tmp/6f94a6fde74f6377f961ed8ee84d7fd58ee10dcc7c6de7c01697a17742c48be6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Takeaway 3: AVATAR effectively improves agents during optimization. Figure 4 illustrates the agents\u2019 performance on the validation set during optimization. Impressively, AVATAR agents show significant performance improvements, e.g., from $35\\%$ to $75\\%$ on AMAZON and from $20\\%$ to $78\\%$ on MAG. This evidence strongly supports the effectiveness of the instructions generated by our comparator. Additionally, our memory bank, which stores past best-performing actions, encourages AVATAR agents to gradually converge by the end of the optimization process. ", "page_idx": 8}, {"type": "text", "text": "Takeaway 4: AVATAR can generalize to real-world tasks. Comparator generates instructions tailored to groups of retrieval queries, promoting generalizable modifications for novel queries. We validate this capability by applying optimized actions to human-generated leave-out queries from the STARK benchmark, which differ notably from the training data used to optimize our agents. Results in Table 5 (Appendix B) show that AVATAR significantly outperforms other models, achieving an average improvement of $20.9\\%$ on $\\operatorname{Hit}\\!\\circledcirc1$ . Further, in another study of Appendix B, we assess AVATAR\u2019s robustness to hyperparameters $h$ and $\\ell$ , showing that it maintains stable performance and generalization across different parameter values. ", "page_idx": 8}, {"type": "text", "text": "5.2 Image Retrieval Task ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further experiment on FLICKR30K ENTITIES [35], an image retrieval dataset of $30\\mathrm{k}$ images with annotated bounding boxes and descriptive phrases (Appendix A). In Table 2, AVATAR again shows significant improvements. In contrast, Reflexion agents struggle with \u201coverfitting,\u201d where they are easily misled by specific image data, leading to inappropriate actions (e.g., trying to \u201cextract the color of a hat\u201d from images without hats). AVATAR effectively avoids such pitfalls through batch-wise contrastive reasoning, which provides a broader perspective. ", "page_idx": 8}, {"type": "text", "text": "Takeaway 5: AVATAR generates impressive and generalizable actions. The final actions of the AVATAR agent, shown in Figure ?? (left) and detailed in Figure 8 (Appendix B), achieve advanced performance. Notably, AVATAR skillfully manages input queries and leverages Inverse Document Frequency (IDF) scores to refine phrase matching, ultimately synthesizing accurate answers. Beyond using existing tools, AVATAR agents can develop high-level tools, such as IDF-based reweighting, suggesting a promising direction for dynamic tool libraries and enhanced tool generation. ", "page_idx": 8}, {"type": "text", "text": "Takeaway 6: Emerging Behaviors during Optimization. In Figure 6, we present concrete cases illustrating key interactions between actor and comparator. In each instance, comparator identifies critical flaws, including information omission, ineffective tool usage, and suboptimal synthesis of varying scores. The instructions subsequently prompt actor to enhance retrieval strategies, tool selection, and precise score combinations. Furthermore, frequent references to tool usage underscore comparator\u2019s focused examination of tool utilization during optimization. ", "page_idx": 8}, {"type": "image", "img_path": "N4quRxE19p/tmp/d09c866c225dca8ecbc411e7563825107fc1f6e6b2f74534274f0a0408d9506a.jpg", "img_caption": ["Figure 6: Representative instruction types from the comparator. We provide three cases where the comparator guides the actor towards (1) better divide-and-conquer strategies for multi-step problemsolving, (2) more sensible differentiation between good and bad tool usage/combinations, and (3) adjustments in the weights to generate the final answers. We record the number of occurrences $X$ under each instruction type over 25 iterations on FLICKR30K-ENTITIES, indicated by $(X/25)$ . "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "N4quRxE19p/tmp/4d1d49fad0646f550174ad06294b761b748d2754fd4f2ac83644071ccdf0e040.jpg", "table_caption": ["Table 3: Performance $(\\%)$ on three QA benchmarks. Last row shows the relative improvements over the best metric value in each column. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Question Answering Tasks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finally, we applied AVATAR to three widely used QA benchmarks. For ToolQA, we tested AVATAR and the baselines on two different domains: SciREX, which focuses on extracting information from full-length machine learning papers, and Agenda, which involves personal agenda-related questions. Both datasets have easy and hard versions. ", "page_idx": 9}, {"type": "text", "text": "Takeaway 7: AVATAR outperforms on QA tasks by offering better context understanding. Table 3 shows that AVATAR consistently outperforms state-of-the-art methods across all three QA datasets, with especially strong results on TOOLQA. In SCIREX-HARD, which focuses on extracting complex information from long scientific papers, AVATAR shows a $33.1\\%$ improvement, while in AGENDA-HARD, it achieves a $25.0\\%$ relative gain. These improvements are attributed to AVATAR\u2019s ability to generate optimized prompts that help the agent better understand the broader patterns and contexts of the questions, leading to more accurate answers and improved generalization across question types, from simple to complex. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we introduce AVATAR, a novel framework that automates the optimization of LLM agents for enhanced tool utilization in multi-step problems, focusing on complex retrieval and QA tasks. AVATAR demonstrates remarkable improvements across seven diverse datasets. This success can largely be attributed to the comparator module, which effectively refines agent performance through the iterative generation of holistic and strategic prompts. A key innovation of comparator is its use of contrastive reasoning with batch-wise sampling, enabling it to identify systemic flaws and extract robust \u201cgradients\u201d for comprehensive agent improvement across diverse scenarios. While we observe substantial progress from AVATAR, we discuss its limitations in Appendix D regarding its scalability etc.Future work can explore extending this methodology to other challenging agent tasks, visual reasoning tasks, and more dynamic environments, or designing better memory banks for dynamically storing knowledge and experience from past training. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank lab members in Zou and Leskovec\u2019s labs for discussions and for providing feedback on our manuscript. We also gratefully acknowledge the support of DARPA under Nos. N660011924033 (MCS); NSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM); Stanford Data Applications Initiative, Wu Tsai Neurosciences Institute, Stanford Institute for Human-Centered AI, Chan Zuckerberg Initiative, Amazon, Genentech, GSK, Hitachi, SAP, and UCB. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities. ", "page_idx": 10}, {"type": "text", "text": "REFERENCES ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. [n. d.]. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. ([n. d.]). arXiv:2308.09687 ", "page_idx": 10}, {"type": "text", "text": "[2] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In ICML.   \n[3] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023. FireAct: Toward Language Agent Fine-tuning. arXiv:2310.05915   \n[4] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. 2023. Teaching Large Language Models to Self-Debug. (2023).   \n[5] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning. In EMNLP. ACL.   \n[6] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, and Jianfeng Gao. [n. d.]. Agent AI: Surveying the Horizons of Multimodal Interaction. ([n. d.]). arXiv:2401.03568   \n[7] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. 2024. AutoGuide: Automated Generation and Selection of StateAware Guidelines for Large Language Model Agents. CoRR abs/2403.08978 (2024). arXiv:2403.08978   \n[8] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. [n. d.]. MindAgent: Emergent Gaming Interaction. ([n. d.]). arXiv:2309.09971   \n[9] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In ICML. PMLR.   \n[10] Simon Jerome Han, Keith J. Ransom, Andrew Perfors, and Charles Kemp. 2024. Inductive reasoning in humans and large language models. Cogn. Syst. Res. (2024).   \n[11] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering. (2024). arXiv:2402.07630   \n[12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv:2305.08845 (2023).   \n[13] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. In ICML.   \n[14] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2022. Inner Monologue: Embodied Reasoning through Planning with Language Models. In CoRL.   \n[15] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. 2024. MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use. ICLR (2024).   \n[16] Vassilis N Ioannidis, Xiang Song, Da Zheng, Houyu Zhang, Jun Ma, Yi Xu, Belinda Zeng, Trishul Chilimbi, and George Karypis. 2022. Efficient and effective training of language and graph neural network models. arXiv preprint arXiv:2206.10781 (2022).   \n[17] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In EMNLP.   \n[18] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2023. Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. arXiv:2212.14024 [cs.CL]   \n[19] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. 2022. CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. In NeurIPS.   \n[20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs.CL]   \n[21] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society. abs/2303.17760 (2023). arXiv:2303.17760   \n[22] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large VisionLanguage Models. In ACL.   \n[23] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs. In EMNLP. Association for Computational Linguistics.   \n[24] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. AgentBench: Evaluating LLMs as Agents. (2023). arXiv:2308.03688   \n[25] Robert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, and Shuyan Zhou. 2023. Hierarchical Prompting Assists Large Language Model on Web Navigation. In Findings of the Association for Computational Linguistics: EMNLP 2023.   \n[26] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. In NeurIPS.   \n[27] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. In NeurIPS.   \n[28] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted question-answering with human feedback. ArXiv (2021).   \n[29] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. [n. d.]. WebGPT: Browser-assisted question-answering with human feedback. ([n. d.]). arXiv:2112.09332   \n[30] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. 2023. MemGPT: Towards LLMs as Operating Systems. 2310.08560 (2023).   \n[31] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. ART: Automatic multi-step reasoning and tool-use for large language models. arXiv:2303.09014 (2023).   \n[32] Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. TALM: Tool Augmented Language Models. arXiv:2205.12255   \n[33] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large Language Model Connected with Massive APIs. CoRR (2023). arXiv:2305.15334   \n[34] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. arxiv 2302.12813 (2023).   \n[35] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2017. Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. Int. J. Comput. Vis. (2017).   \n[36] Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023. WebCPM: Interactive Web Search for Chinese Long-form Question Answering. In Proceedings of ACL 2023. Association for Computational Linguistics.   \n[37] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ToolLLM: Facilitating Large Language Models to Master $16000+$ Real-world APIs. arxiv 2307.16789 (2023).   \n[38] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. In NeurIPS.   \n[39] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. In NeurIPS.   \n[40] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May D Wang. 2024. EHRAgent: Code Empowers Large Language Models for Complex Tabular Reasoning on Electronic Health Records. arXiv:2401.07128 (2024).   \n[41] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In NeurIPS.   \n[42] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. [n. d.]. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. In EMNLP, year $=$ 2023.   \n[43] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023. ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. (2023). arXiv:2306.05301   \n[44] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman. 2024. Hypothesis Search: Inductive Reasoning with Language Models. ICLR (2024).   \n[45] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, and Zhiting Hu. 2023. PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. (2023). arXiv:2310.16427   \n[46] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In ICLR.   \n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In NeurIPS.   \n[48] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (2023). arXiv:2308.08155   \n[49] Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec. 2024. STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases. (2024). arXiv:2404.13207   \n[50] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2024. Large Language Models as Optimizers. (2024).   \n[51] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023. GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction. In NeurIPS, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.).   \n[52] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. EMNLP (2018).   \n[53] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In EMNLP.   \n[54] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In NeurIPS.   \n[55] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In ICLR.   \n[56] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. 2024. Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (2024).   \n[57] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering.   \n[58] Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024. ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. arXiv:2401.00741 (2024).   \n[59] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. [n. d.]. AgentTuning: Enabling Generalized Agent Abilities for LLMs. ([n. d.]). arXiv:2310.12823   \n[60] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. 2024. Training Language Model Agents without Modifying Language Models. (2024). arXiv:2402.11359   \n[61] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. ExpeL: LLM Agents Are Experiential Learners. In AAAI.   \n[62] Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023. Outline, then details: Syntactically guided coarse-to-fine code generation. ICML (2023).   \n[63] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. MemoryBank: Enhancing Large Language Models with Long-Term Memory. In AAAI.   \n[64] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large Language Models are Human-Level Prompt Engineers. In ICLR.   \n[65] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. arXiv:2308.07107 (2023).   \n[66] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. ToolQA: A Dataset for LLM Question Answering with External Tools. In NeurIPS.   \n[67] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. ToolQA: A Dataset for LLM Question Answering with External Tools. arXiv:2306.13304   \n[68] Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, and Yueting Zhuang. [n. d.]. Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering. ([n. d.]). arXiv:2402.14320 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Retrieval Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "STARK. On the STARK benchmark, we are given a relation-text knowledge base, based on a knowledge graph $G=(V,E)$ and a collection of free-text documents $D$ . We represent the relationtext knowledge base of size $n$ as $\\mathcal{E}\\,=\\,\\{(v_{i},d_{i},g_{i})\\}_{i=1}^{n}$ , where $v_{i}\\in V$ represents a node on the knowledge graph, $d_{i}\\in D$ is the text document related to the node, and $g_{i}$ is the connected component of $G$ containing $v_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "The query set $Q$ in STARK is derived from both $G$ and $D$ , where each $q_{i}\\in Q$ contains requirements based on $d_{i}$ and $g_{i}$ . The answer set $A_{i}$ , which includes $v_{i}$ , is a set of nodes satisfying both relational and textual requirements. The task on STARK is defined as follows: Given the knowledge base $\\mathcal{E}$ consisting of relational and textual information, and a text query $q_{i}$ , the output is a set of nodes $A_{i}$ such that $\\forall a_{i}\\in A_{i}$ , $a_{i}$ satisfies the relational requirements in the knowledge graph and textual requirements in its text documents. ", "page_idx": 15}, {"type": "text", "text": "FLICKR30K ENTITIES. On the FLICKR30K ENTITIES dataset, we are given an image-text knowledge base. We denote an image-text knowledge base of size $n$ as $\\bar{\\mathcal{E}}\\ =\\ \\{(v_{i},q_{i},\\bar{T_{i}})\\}_{i=1}^{n}$ . Sample $i$ consists of an image $v_{i}$ , its descriptive caption $q_{i}$ , and entity bounding box information $T_{i}$ . Specifically, $T_{i}=\\{(c_{i j},p_{i j})\\}_{j=1}^{b_{i}}$ , where $b_{i}$ represents the number of bounding boxes annotated in image $i$ , $c_{i j}$ is the coordinate of the $j$ -th bounding box, and $p_{i j}$ describes the entity in the corresponding bounding box. ", "page_idx": 15}, {"type": "text", "text": "In our task, the image captions serve as the text query; therefore, all $q_{i}$ in the dataset are not accessible to the agent to prevent information leakage. However, the agent can access $v_{i}$ and $T_{i}$ to fully utilize the vision and language information. The task on FLICKR30K ENTITIES is defined as follows: Given the knowledge base $\\mathcal{E}$ with images and bounding box information, and a text query $q_{i}$ , the output is an image $v_{i}$ that satisfies the visual requirements in the image and textual requirements in the corresponding bounding boxes $T_{i}$ . ", "page_idx": 15}, {"type": "image", "img_path": "N4quRxE19p/tmp/e001038f8aa46a35e31cf6571ecc520d86d22950f40958bb622266afce3c613b.jpg", "img_caption": ["Figure 7: Example data on FLICKR30K ENTITIES. Each entity is an image along with its image patches and associated phrases with the image patches. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Experiment Details and Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Experiment Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "LLM versions for agent methods. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 For the knowledge retrieval tasks, we use claude-3-opus as the backbone LLM in the main paper by default, and report results using gpt-4-turbo in Appendix B due to space limitations. \u2022 For the QA tasks, we use gpt-4 for HotpotQA for fair comparison with previous methods and $\\mathtt{g p t-40}$ for the other two QA datasets. ", "page_idx": 15}, {"type": "table", "img_path": "N4quRxE19p/tmp/44f30854f6ca28e794867ce91fe316bbc2c0d80cfafb508e0569e650f2711032.jpg", "table_caption": ["Table 4: Retrieval performance $(\\%)$ on STARK benchmark. Last row shows the relative improvements over the best metric value among the baselines. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Additional Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "(1) AVATAR results on STARK using GPT-4 Turbo (0125) as LLM backbone. In Table 4, we provide the results on STARK using GPT-4 Turbo (0125) as the backbone LLM. ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "N4quRxE19p/tmp/412067314349a18aa44df4ffe95025a5477512fce75d592f92f916c0ffd0cb41.jpg", "table_caption": ["Table 5: Retrieval performance $(\\%)$ on the leave-out sets of human-generated queries in STARK. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "N4quRxE19p/tmp/2b2a9f93e13f60a29ba33935a08c8c9f524157437193cc29359b1a3a2508b041.jpg", "table_caption": ["(2) AVATAR results on STARK\u2019s human-generated splits. In Table 5, we demonstrate AVATAR\u2019s ability to generalize to test queries with distributions different from the question-answering pairs used to optimize the actor agents. ", "Table 6: Performance metrics for different models on the subset of the STARK-MAG dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "(3) AVATAR results and comparison with ExpeL on STARK-MAG subset. In Table 6, AVATAR demonstrates consistently higher performance than ExpeL across most metrics, notably achieving the highest $\\operatorname{Hit}\\@1$ and MRR scores. While ExpeL performs well in Recall $@20$ , AVATAR \u2019s overall improvements highlight its superior capability in precise retrieval tasks and tool-assisted knowledge retrieval. ", "page_idx": 16}, {"type": "text", "text": "(4) Final action sequence by AVATAR on FLICKR30K-ENTITIES. In Figure 8, we present the final actions optimized by AVATAR on FLICKR30K-ENTITIES. ", "page_idx": 16}, {"type": "image", "img_path": "N4quRxE19p/tmp/097dbb31991ea50c5b48b8558bcbb6e15eb5c52b3a9aa2c57983763f9cf1adc9.jpg", "img_caption": ["Figure 8: Optimized Action Sequence by AVATAR on FLICKR30K-ENTITIES.. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(5) Sensitivity of AVATAR to upper and lower bounds. We evaluated various combinations of $\\ell$ and $h$ , focusing on the STARK-AMAZON dataset due to computational constraints. Table 7 presents the $\\operatorname{Hit}\\!\\circledcirc1$ results for different $\\ell$ and $h$ values. ", "page_idx": 17}, {"type": "text", "text": "Table 7: Hit $@1$ results for different combinations of $\\ell$ and $h$ values on the STARK-AMAZON dataset. ", "page_idx": 17}, {"type": "table", "img_path": "N4quRxE19p/tmp/31a1691c28320383dffa8e7af61348e24119a42474c2d9f03f6f2f427e301aaf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Key Observations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The framework exhibits robustness to variations in $\\ell$ and $h$ , with $\\operatorname{Hit}\\!\\circled{\\omega}\\,1$ fluctuations limited to a range of $2.7\\%$ .   \n\u2022 A performance decline is observed when the gap between $\\ell$ and $h$ becomes too large, potentially due to the exclusion of certain training queries that fall within the $(h,\\ell)$ interval.   \n\u2022 A moderate gap between $\\ell$ and $h$ leads to slight performance improvements, suggesting that a balanced separation between positive and negative queries can enhance pattern differentiation without compromising the number of training queries. ", "page_idx": 17}, {"type": "text", "text": "The results indicate that setting $\\ell=0.6$ and $h=0.5$ yields an improved $\\operatorname{Hit}\\!\\circledcirc1$ score compared to the baseline reported in the original paper. Overall, this analysis underscores the robustness of the framework, which relies on a minimal set of hyperparameters, including $\\ell,h$ , batch size $b$ , and training epochs. ", "page_idx": 18}, {"type": "text", "text": "C Prompts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We keep only two prompt templates for our framework on all tasks: (1) The prompt template given to actor as initially instructions, and (2) the prompt template given to the comparator to conduct contrastive reasoning and generate the instructions for the actor. Below are the complete templates: ", "page_idx": 18}, {"type": "text", "text": "This is the prompt given to actor as initially instructions: ", "page_idx": 18}, {"type": "table", "img_path": "N4quRxE19p/tmp/afc6a73f117a52b092cf2da38c73888b370cc00486ae70ccd53ee2a54dd04fcf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "- Observe the example queries carefully and consider the key attributes to extract.   \n- Use \u2018\u2018\u2018python and \u2018\u2018\u2018 to wrap the complete code, and do not use any other $\\hookrightarrow$ delimiters.   \n- You can use any of the pre-implemented APIs but should avoid modifying them.   \n- You can include other functions besides \u2018get_node_score_dict\u2018, but ensure they are $\\hookrightarrow$ fully implemented.   \n- The code should be complete without placeholders and dummy functions.   \n- Optimize the integrity of the code, e.g., corner cases.   \n- Minimize computational expenses by early elimination of candidate nodes that don\u2019t $\\hookrightarrow$ meet relational requirement (if any).   \n- Avoid conducting unnecessary and redundant computations, especially when using \u2192loops.   \n- Make use of \u2018parameter_dict\u2018 to avoid hard-coding parameters and weights.   \n- Use the functions that end with \u2018by_llm\u2018 wisely for more accurate searches.   \n- Use \u2018debug_print\u2018 smartly to print out any informative intermediate results for $\\hookrightarrow$ debugging.   \n- Exclude or comment out any example uses of \u2018get_node_score_dict\u2018 in the output $\\hookrightarrow$ code.   \nYour output: ", "page_idx": 19}, {"type": "text", "text": "This is the prompt given to comparator to generate the instructions for the actor: ", "page_idx": 19}, {"type": "text", "text": "<initial_prompt>   \n<previous_actions>   \nAfter executing the above actions on user queries, some queries have yielded good $\\hookrightarrow$ results, while others have not. Below are the queries along with their $\\hookrightarrow$ corresponding evaluation metrics:   \nWell-performing queries:   \n<positive_queries_and_metric>   \nPoorly-performing queries:   \n<negative_queries_and_metric>   \nTask:   \n(1) Firstly, identify and contrast the patterns of queries that have achieved good $\\hookrightarrow$ results with those that have not.   \n(2) Then, review the computational logic for any inconsistencies in the previous $\\hookrightarrow$ actions.   \n(3) Lastly, specify the modification that can lead to improved performance on the $\\hookrightarrow$ negative queries. You should focus on capturing the high-level pattern of $\\hookrightarrow$ the queries relevant to the knowledge base schema. ", "page_idx": 19}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We identify several potential limitations of our work: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Scalability: AvaTaR is designed to scale with large language models (LLMs) that support extended context lengths (up to $128\\mathbf{k}$ tokens), enabling it to handle numerous tools and complex tasks. However, increased latency and other practical limitations may hinder performance in scenarios requiring hundreds of tools or high complexity. Future research could focus on incorporating specialized, tool-augmented LLMs as auxiliary agents to facilitate smoother scaling. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Computation Requirements: Managing longer contexts and multiple tool interactions within AvaTaR increases computational demands, which can significantly raise operational costs. These requirements necessitate substantial resources to maintain efficient performance, particularly when scaling to larger datasets or more intricate tasks. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Potential Failure Modes: Although AvaTaR performs well on known queries, its performance may diminish when faced with queries that require new or unfamiliar combinations of tools. This limitation could be mitigated by integrating adaptive learning techniques and continuous monitoring, which would allow AvaTaR to better handle novel tool requirements. ", "page_idx": 19}, {"type": "text", "text": "E Function library ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Complex Retrieval Tasks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Please refer to Table 8 and Table 9 for the detailed functions. ", "page_idx": 20}, {"type": "text", "text": "E.2 General QA Tasks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For general QA tasks, we use the following tools: ", "page_idx": 20}, {"type": "text", "text": "\u2022 WEB_SEARCH: A general-purpose tool that performs web searches to answer questions. Useful for retrieving up-to-date information from the internet when other sources are unavailable.   \n\u2022 ARXIV_SEARCH: This tool retrieves information about academic papers from Arxiv using a paper\u2019s unique ID. This function call can provide metadata and other details for academic references.   \n\u2022 Wiki_SEARCH: If you have a question or name to lookup, this tool uses a Wikipedia search to retrieve relevant information.   \n\u2022 RETRIEVE_FROM_DB: This tool is used to retrieve relevant information from a database. This is only available on ToolQA. ", "page_idx": 20}, {"type": "table", "img_path": "N4quRxE19p/tmp/62eca5c3913b34bc1c020f782de19997fb4bd316bab688778df80f17adbd0471.jpg", "table_caption": ["Table 8: Function library on STARK "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "N4quRxE19p/tmp/35acb73146602e795b8c38d4aacbc70652bfda57808a6965256f2a300fd4130f.jpg", "table_caption": [], "table_footnote": ["Table 9: Function library on Flickr30K Entities "], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We design a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and make comprehensive analysis on the evolution of our key modules. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We did extensive survey on related work in the area of LLM agents, agent optimization, LLM agent for retrieval, and further discuss their limitations ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We elaborate the experiment details in the Experiment section including datasets, baselines, function libraries etc. We also release all the prompts we are using in the experiments for reproducibility. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our code and data are accessible at https://anonymous.4open.science/ r/AvaTaR-FBC4/. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include dataset information and training details in the Experiment part and Appendix. We also clearly describe the knowledge base and formally introduce the task settings. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We run our experiments on a single NVIDIA A100-SXM4-80GB GPU and 32-core CPUs. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not induce any potential research harm mentioned in NeurIPS Code of Ethics in our paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discussed the impact in the introduction section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our method provides a framework to better use LM but not releasing a LM. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Creators or original owners of assets mentioned in the paper are properly cited and the license and terms of use are respected. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}]