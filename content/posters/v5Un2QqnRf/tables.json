[{"figure_path": "v5Un2QqnRf/tables/tables_7_1.jpg", "caption": "Table 1: Results on fundamental vision-centric tasks and vision-language tasks. We use \"-\" and gray cell to indicate the result is not reported and the corresponding task is not supported by the method, respectively.", "description": "This table compares the performance of different methods on various vision-centric tasks (object detection, instance segmentation, pose estimation) and vision-language tasks (visual grounding, referring segmentation).  The methods are categorized into three groups: Task-specific Specialists (models specialized for a single task), Vision Generalists (models capable of handling multiple vision tasks), and LMM Generalists (large multimodal models).  The table shows that the proposed Lumen method performs competitively with or surpasses existing methods, demonstrating versatility across different tasks.", "section": "4.2 Results on Versatile Tasks"}, {"figure_path": "v5Un2QqnRf/tables/tables_7_2.jpg", "caption": "Table 2: Results on prevalent VQA benchmarks. Here, we employ English MMBench dev, SEEDBench image, MME test, MMMU val and MathVista mini sets for evaluation.", "description": "This table shows the results of the Lumen model on several popular Visual Question Answering (VQA) benchmarks.  It compares the performance of Lumen against other state-of-the-art Large Multimodal Models (LMMs) on various datasets, including MMBench (a comprehensive benchmark for multimodal LMMs), SEEDBench (focused on image understanding), MME (a multi-modal reasoning benchmark), MMMU (a massive multi-discipline multimodal benchmark), and MathVista (a benchmark for mathematical reasoning). The table highlights the model's performance across different datasets and demonstrates its capabilities in understanding and reasoning with both visual and textual information.", "section": "4.2 Results on Versatile Tasks"}, {"figure_path": "v5Un2QqnRf/tables/tables_8_1.jpg", "caption": "Table 3: Ablation studies on model designs. (a) Effect of different \"V-L dense aligner\" design choices. (b) Effect of the pretrained mask decoder. cIoUs are reported. (c) Effect of LMM baseline.", "description": "This table presents ablation study results to analyze the impact of different design choices on the Lumen model's performance. It investigates three aspects: the architecture of the V-L dense aligner (comparing convolutional and transformer-based designs), the choice of pretrained mask decoder (comparing a custom-trained decoder with the SAM model), and the impact of different Large Multimodal Model (LMM) baselines (comparing LLaVA v1.0, LLaVA v1.0* and LLaVA v1.5).  The results help understand which design choices contribute most to the model's overall effectiveness.", "section": "4.3 Ablation Studies"}, {"figure_path": "v5Un2QqnRf/tables/tables_8_2.jpg", "caption": "Table 4: Effect of multi-task training. AP50 on RefCOCOg val set and COCO val set are reported.", "description": "This table presents the results of an ablation study investigating the impact of multi-task training on the model's performance.  It shows the Average Precision at 50% Intersection over Union (AP50) for object detection on the COCO validation set and visual grounding on the RefCOCOg validation set.  Different rows represent experiments where different combinations of tasks (object detection, visual grounding, pose estimation, and visual question answering) were included during training.  The table demonstrates the effect of including or excluding specific tasks on the model's performance on the two target benchmarks.", "section": "4.3 Ablation Studies"}, {"figure_path": "v5Un2QqnRf/tables/tables_8_3.jpg", "caption": "Table 5: Effect of the training recipe on VQA performances. MMBench is used for evaluation\u00b2.", "description": "This table presents the ablation study on the effect of different training recipes on the visual question answering (VQA) performance. The MMBench benchmark is used for evaluation.  Three training recipes are compared: using only Phase 1 data, using Phase 1 and Phase 2 data (with VQA data), and only using Phase 2 data (VQA data).  The results are broken down into different metrics: Answer Rate (AR), Correct Percentage (CP), False Positives-Comprehensive (FP-C), False Positives-Simple (FP-S), Logical Reasoning (LR), Reading and Reasoning (RR), and Overall.  The table highlights how different training phases and data sources impact the VQA performance.", "section": "4.3 Ablation Studies"}, {"figure_path": "v5Un2QqnRf/tables/tables_9_1.jpg", "caption": "Table 1: Results on fundamental vision-centric tasks and vision-language tasks. We use \"-\" and gray cell to indicate the result is not reported and the corresponding task is not supported by the method, respectively.", "description": "This table presents a comparison of the proposed Lumen model's performance against other state-of-the-art models on several vision-centric and vision-language tasks.  It shows results for object detection, instance segmentation, pose estimation, visual grounding, and referring segmentation. The table highlights Lumen's performance relative to task-specific specialists (models designed for specific tasks), vision generalists (models that handle multiple visual tasks), and LMM generalists (large multimodal models). The table is structured to allow easy comparison across different model categories and task types.", "section": "4.2 Results on Versatile Tasks"}, {"figure_path": "v5Un2QqnRf/tables/tables_9_2.jpg", "caption": "Table 1: Results on fundamental vision-centric tasks and vision-language tasks. We use \"-\" and gray cell to indicate the result is not reported and the corresponding task is not supported by the method, respectively.", "description": "This table presents a comparison of the proposed Lumen model's performance against other state-of-the-art models on a variety of vision-centric and vision-language tasks.  The vision-centric tasks include object detection, instance segmentation, and pose estimation, while the vision-language tasks include visual grounding and referring segmentation.  The table shows the performance metrics (AP, AP50, AP75, etc.) for each task and model, highlighting Lumen's strengths in both categories.", "section": "4.2 Results on Versatile Tasks"}, {"figure_path": "v5Un2QqnRf/tables/tables_16_1.jpg", "caption": "Table 7: Results on fundamental vision-centric tasks and vision-language tasks. We use \"-\" and gray cell to indicate the result is not reported and the corresponding task is not supported by the method, respectively.", "description": "This table presents a comparison of various methods on object detection, instance segmentation, pose estimation, visual grounding, and referring segmentation tasks.  It shows the performance of different models, categorized into task-specific specialists, vision generalists, and LMM generalists, across several metrics (AP, AP50, AP75, and cIoU).  The table highlights the relative strengths and weaknesses of different approaches to address these vision-centric tasks.", "section": "4.2 Results on Versatile Tasks"}, {"figure_path": "v5Un2QqnRf/tables/tables_17_1.jpg", "caption": "Table 8: Performance comparison with state-of-the-art specialists and generalists on the referring image segmentation task.", "description": "This table presents a comparison of the performance of Lumen against other state-of-the-art models on three visual grounding benchmarks: RefCOCO, RefCOCO+, and RefCOCOg.  The table shows the performance of various methods (both specialists focusing on a single task and generalists tackling multiple visual tasks) using the 'val' and 'test' splits of each benchmark.  It highlights Lumen's competitive performance, particularly in comparison to generalist models, in spite of not utilizing pixel-level supervision during training.", "section": "4.2 Results on Versatile Tasks"}, {"figure_path": "v5Un2QqnRf/tables/tables_18_1.jpg", "caption": "Table 3: Ablation studies on model designs. (a) Effect of different \"V-L dense aligner\" design choices. (b) Effect of the pretrained mask decoder. cIoUs are reported. (c) Effect of LMM baseline.", "description": "This table presents the results of ablation studies conducted to analyze the impact of different design choices on the model's performance.  Specifically, it examines the effect of various V-L dense aligner designs, the use of a pretrained versus a trained mask decoder, and different LMM baselines. The results are evaluated using Average Precision (AP) and Intersection over Union (IoU).", "section": "4.3 Ablation Studies"}, {"figure_path": "v5Un2QqnRf/tables/tables_18_2.jpg", "caption": "Table 3: Ablation studies on model designs. (a) Effect of different \"V-L dense aligner\" design choices. (b) Effect of the pretrained mask decoder. cIoUs are reported. (c) Effect of LMM baseline.", "description": "This table presents the ablation study results of the Lumen model. It shows the impact of different design choices on the model's performance. Specifically, it investigates the effect of different V-L dense aligner designs, the impact of using a pretrained vs. a newly trained mask decoder, and the influence of different Large Multimodal Model (LMM) baselines on the model's performance, measured by Average Precision (AP) and Intersection over Union (IoU).", "section": "4.3 Ablation Studies"}, {"figure_path": "v5Un2QqnRf/tables/tables_18_3.jpg", "caption": "Table 3: Ablation studies on model designs. (a) Effect of different \"V-L dense aligner\" design choices. (b) Effect of the pretrained mask decoder. cIoUs are reported. (c) Effect of LMM baseline.", "description": "This table presents the ablation study results of the Lumen model. It explores the impact of different components on the model's performance. Specifically, it investigates the effect of the Vision-Language dense aligner's design (convolutional vs. transformer-based), the effect of using a pre-trained mask decoder (SAM vs. a newly trained one), and the effect of using different Large Language Models (LLaVA-v1.0, LLaVA-v1.0*, LLaVA-v1.5) as the backbone. The results are evaluated in terms of Average Precision (AP), AP at 50% IoU (AP50), and AP at 75% IoU (AP75).", "section": "4.3 Ablation Studies"}, {"figure_path": "v5Un2QqnRf/tables/tables_18_4.jpg", "caption": "Table 9: Ablation studies on model designs. (a) Effect of different input sizes for the dense alignment. (b) Effect of different pre-trained vision encoders. (c) Effect of different K value on dense prediction task. (d) Effect of different numbers of training iterations.", "description": "This table presents the ablation study results on Lumen's model design. It shows the impact of different input sizes, vision encoders, K values for dense prediction, and the number of training iterations on the model's performance, specifically measured by AP, AP50, and AP75.", "section": "4.3 Ablation Studies"}]