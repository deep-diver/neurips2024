[{"figure_path": "v5Un2QqnRf/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of our proposed Lumen with previous methods. Previous methods (e.g., Griffon [19]) serialize bounding box coordinates into discrete token sequences to conform to the language-oriented outputs format of LMMs, disregarding the unordered nature inherent in bounding boxes. Our Lumen first predicts unified heatmaps for various tasks. These heatmaps are further used for guiding simple decoding tools with the parsed task-type indicators to support versatile visual tasks. We omit the user instruction of referring to all persons in the image for conciseness.", "description": "This figure compares the proposed Lumen model with previous approaches for handling vision-centric tasks within large multimodal models. Previous methods convert bounding box coordinates into sequential token sequences, which is convenient but ignores the inherent orderlessness of the boxes. Lumen, instead, initially predicts unified heatmaps for various tasks. These heatmaps are then used, with parsed task indicators, to guide simple decoding tools and thus handle diverse vision tasks effectively.", "section": "1 Introduction"}, {"figure_path": "v5Un2QqnRf/figures/figures_3_1.jpg", "caption": "Figure 2: Overall framework of the proposed Lumen. Our Lumen consists of two stages. In the first stage, the input image and the instruction with designed special tokens are embedded and fed into a large language model to interact and comprehend visual and textual contents. Then, the [LOC] token output and high-resolution image features are further aligned to produce a heatmap denoting cross-modal matching probabilities. In the second stage, the heatmap can serve as a strong indication for various vision tasks, and the task outputs can be obtained with lightweight task-specific decoders. The routing of the decoding pathway is determined by the task token (e.g., [DET] in image) output generated in the first stage.", "description": "This figure illustrates the two-stage architecture of the Lumen model.  Stage 1 focuses on task-agnostic matching using a large language model to align image and instruction features, resulting in a heatmap. Stage 2 performs task-specific decoding based on this heatmap, using lightweight decoders for various vision tasks (object detection, instance segmentation, pose estimation, visual grounding, referring segmentation). The task-specific decoder is selected based on the task token generated in stage 1.", "section": "3 Method"}, {"figure_path": "v5Un2QqnRf/figures/figures_4_1.jpg", "caption": "Figure 2: Overall framework of the proposed Lumen. Our Lumen consists of two stages. In the first stage, the input image and the instruction with designed special tokens are embedded and fed into a large language model to interact and comprehend visual and textual contents. Then, the [LOC] token output and high-resolution image features are further aligned to produce a heatmap denoting cross-modal matching probabilities. In the second stage, the heatmap can serve as a strong indication for various vision tasks, and the task outputs can be obtained with lightweight task-specific decoders. The routing of the decoding pathway is determined by the task token (e.g., [DET] in image) output generated in the first stage.", "description": "This figure illustrates the two-stage framework of the Lumen model.  Stage 1 focuses on task-agnostic matching, where the image and instruction are processed by a large language model to generate a heatmap representing the alignment between visual concepts and the instruction.  Stage 2 uses this heatmap for task-specific decoding, routing the information to lightweight decoders based on the task type.  This decoupled approach allows for versatile vision-centric capabilities.", "section": "3 Method"}, {"figure_path": "v5Un2QqnRf/figures/figures_16_1.jpg", "caption": "Figure 5: Detailed designs of \"V-L Dense Aligner\". [LOC] is the special token output from the LMM. [M] is the special token used for predicting the matching probabilities (i.e., heatmaps) in the first class-agnostic matching stage in the main paper. [H] and [W] are special tokens used as a simple box decoder for predicting additional height and width in the second class-specific decoding stage in the main paper. We use dotted lines to illustrate that these tokens are added in the second stage.", "description": "The figure shows the detailed architecture of the V-L Dense Aligner, a key component of the Lumen model.  It uses a transformer-based architecture with multiple cross-attention and self-attention layers. The input includes image embeddings and special tokens ([LOC], [M], [H], [W]) from the large language model (LLM).  The [LOC] token represents the location information from the LLM's output, [M] predicts the heatmap, and [H] and [W] are used for box decoder to predict height and width. The output is a heatmap showing the alignment between the image and instruction, crucial for downstream vision tasks. The design uses a two-stage process:  The first stage performs task-agnostic matching, and the second stage performs task-specific decoding, using lightweight decoders guided by the heatmap.", "section": "3.1.2 Model Architecture"}, {"figure_path": "v5Un2QqnRf/figures/figures_19_1.jpg", "caption": "Figure 2: Overall framework of the proposed Lumen. Our Lumen consists of two stages. In the first stage, the input image and the instruction with designed special tokens are embedded and fed into a large language model to interact and comprehend visual and textual contents. Then, the [LOC] token output and high-resolution image features are further aligned to produce a heatmap denoting cross-modal matching probabilities. In the second stage, the heatmap can serve as a strong indication for various vision tasks, and the task outputs can be obtained with lightweight task-specific decoders. The routing of the decoding pathway is determined by the task token (e.g., [DET] in image) output generated in the first stage.", "description": "This figure illustrates the two-stage architecture of the Lumen model.  The first stage involves a task-agnostic matching process where the input image and instruction are processed by a large language model to create a shared representation (heatmap) indicating the relationship between visual concepts and the instruction.  The second stage uses this heatmap as guidance for task-specific decoding, where lightweight decoders generate outputs tailored to specific tasks (object detection, instance segmentation, pose estimation). The type of output is determined by special tokens in the instruction.", "section": "3 Method"}]