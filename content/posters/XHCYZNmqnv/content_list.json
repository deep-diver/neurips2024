[{"type": "text", "text": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jonas Ngnaw\u00e9 Sabyasachi Sahoo IID-Universit\u00e9 Laval and Mila IID-Universit\u00e9 Laval and Mila jonas.ngnawe.1@ulaval.ca sabyasachi.sahoo.1@ulaval.ca ", "page_idx": 0}, {"type": "text", "text": "Yann Pequignot IID-Universit\u00e9 Laval yann.pequignot@iid.ulaval.ca ", "page_idx": 0}, {"type": "text", "text": "Fr\u00e9d\u00e9ric Precioso Universit\u00e9 C\u00f4te d\u2019Azur, CNRS, INRIA, I3S, Maasai frederic.precioso@univ-cotedazur.fr ", "page_idx": 0}, {"type": "text", "text": "Christian Gagn\u00e9 IID-Universit\u00e9 Laval, Mila and Canada CIFAR AI Chair christian.gagne@gel.ulaval.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model\u2019s vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency \u2013 a property that links the input space margins and the logit margins in robust models \u2013 for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model\u2019s logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively and confidently use the logit margin to detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to assess adversarial vulnerability in deployment scenarios efficiently. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks are known to be vulnerable to adversarial perturbations, visually insignificant changes in the input resulting in the so-called adversarial examples that alter the model\u2019s prediction (Biggio et al., 2013; Goodfellow et al., 2015). They constitute actual threats in real-world scenarios (Evtimov et al., 2017; Gnanasambandam et al., 2021), jeopardizing their deployment in sensitive and safety-critical systems such as autonomous driving, aeronautics, and health care. Research in the field has been intense and produced various adversarial training strategies to defend against the vulnerability to adversarial perturbations with bounded $\\ell_{p}$ norm (e.g., $p=2$ , $p=\\infty$ ) through augmentation, regularization, and detection (Xu et al., 2017; Madry et al., 2018; Zhang et al., 2019; Carmon et al., 2019; Wang et al., 2020; Wu et al., 2020; Rice et al., 2020), to cite a few. The empirical robustness (adversarial accuracy) of these adversarially trained models is still far behind their high performance in terms of accuracy. It is typically estimated by assessing the vulnerability of samples of a given test set using adversarial attacks (Carlini & Wagner, 2016; Madry et al., 2018) or an ensemble of attacks such as the standard AutoAttack (Croce & Hein, 2020b). The objective of that evaluation is to determine if, for a given normal sample, an adversarial instance exists within a given $\\epsilon$ -ball around it. Yet, this robustness evaluation over a specific test set provides a global property of the model but not a local property specific to a single instance (Seshia et al., 2018; Dreossi et al., 2019). Beyond that specific test set, obtaining this information for each new sample would typically involve rerunning adversarial attacks or performing a formal robustness verification, which in certain contexts may be computationally prohibitive in terms of resources and time. Indeed, the computational cost makes it prohibitive to estimate robust accuracy at scale on large test sets and/or large models, for example, when using AutoAttack in standard mode. Moreover, in high-stakes deployment scenarios, knowing the vulnerability of single instances in real-time (i.e., their susceptibility to adversarial attacks) would be valuable, for example, to reduce risk, prioritize resources, or monitor operations. Therefore, there is a need for efficient and scalable ways to determine the vulnerability of a model\u2019s decision on a given sample. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The input space margin (i.e., the distance of the sample to the model\u2019s decision boundary in the input space), or input margin in short, can be used as a score to determine whether the sample is non-robust and, as such, likely to be vulnerable to adversarial attacks. Computing the exact input margin is intractable for deep neural networks (Katz et al., 2017; Elsayed et al., 2018; Jordan & Dimakis, 2020). These input margins may not be meaningful for fragile models with zero adversarial accuracies as all samples are vulnerable (close to the decision boundary). However, for robustly trained models, where only certain instances are vulnerable, the input margin is very useful for identifying the critical samples. Previous research studies have explored input margins of deep neural networks during training, focusing on their temporal evolution (Mickisch et al., 2020; Xu et al., 2023), and their exploitation in improving adversarial robustness through instance-reweighting with approximations (Zhang et al., 2020; Liu et al., 2021) and margin maximization (Elsayed et al., 2018; Ding et al., 2020; Xu et al., 2023). However, to the best of our knowledge, no previous research studies the relationship between the input space margin and the logit margin of robustly trained deep classifiers in the context of vulnerability detection. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we investigate how the deep representation of robust models can provide information about the vulnerability of any single sample to adversarial attacks. We specifically address whether the logit margin as an approximation of the distance to the decision boundary in the feature space of the deep neural network (penultimate layer) can reliably serve as a proxy of the input margin for vulnerability detection. When this holds, we will refer to the model as being margin-consistent. The margin consistency property implies that the model can directly identify instances where its robustness may be compromised simply from a simple forward pass using the logit margin. Fig. 1 illustrates this idea of margin consistency. The following contributions are presented in the paper: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce the notion of margin consistency1, a property to characterize robust models that allow the use of their logit margin as a proxy estimation for the input space margin in the context of non-robust sample detection. We prove that margin consistency is a necessary and sufficient condition to reliably use the logit margin for detecting non-robust samples.   \n\u2022 Through an extensive empirical investigation of pre-trained models on CIFAR10 and CIFAR100 with various adversarial training strategies, mainly taken from RobustBench (Croce et al., 2021), we provide evidence that almost all the investigated models display high margin consistency, i.e., there is a strong correlation between the input margin and the logit margin.   \n\u2022 We confirm experimentally that models with high margin consistency perform well in detecting samples vulnerable to adversarial attacks based on their logit margin. In contrast, models with weaker margin consistency exhibit poorer performance.   \n\u2022 For models where margin consistency does not hold, exhibiting a weak correlation between the input margins and the logit margins, we simulate margin consistency by learning to map the model\u2019s feature representation to a pseudo-margin with a better correlation through a simple learning scheme. ", "page_idx": 1}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/8d7d7b7b7f0e03bc7d035329203c3fd3b0205d512e4d2388b2e1dabf9dac2a8b.jpg", "img_caption": ["Figure 1: Illustration of the input space margin, margin in the feature space and margin consistency. The model preserves the relative position of samples to the decision boundary in the input space to the feature space. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Notation and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation We consider $f_{\\theta}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{K}$ a deep neural network classifier with weights $\\theta$ trained on a dataset of samples drawn iid from a distribution $\\mathcal{D}$ on a product space $\\mathcal X\\times\\mathcal X$ . Each sample $\\mathbf{x}$ in the input space $\\mathcal{X}\\subset\\mathbb{R}^{n}$ has a unique corresponding label $\\bar{\\boldsymbol{y}}\\in\\mathcal{Y}=\\bar{\\{1,2,\\ldots,K\\}}$ . The prediction of $\\mathbf{x}$ is given by $\\begin{array}{r}{\\hat{y}(\\mathbf{x})=\\arg\\operatorname*{max}_{k\\in\\mathcal{Y}}{f_{\\theta}^{k}(\\mathbf{x})}}\\end{array}$ , where $f_{\\theta}^{k}(\\mathbf{x})$ is the $k$ -th component of $f_{\\theta}(\\mathbf{x})$ . We consider that a deep neural network is composed of a feature extractor $h_{\\psi}:\\mathcal{X}\\rightarrow\\mathbb{R}^{m}$ and a linear head with $K$ linear classifiers $\\{\\mathbf{w_{k}},b_{k}\\}$ such that $f_{\\theta}^{k}(\\mathbf{x})=\\mathbf{w_{k}}^{\\top}h_{\\psi}(\\mathbf{x})+b_{k}$ . The predictive distribution $p_{\\theta}(y|\\mathbf x)$ is obtained by taking the softmax of the output $f_{\\theta}({\\bf x})$ . A perturbed sample $\\mathbf{x}^{\\prime}$ can be obtained by adding a perturbation $\\delta$ to $\\mathbf{x}$ within an $\\epsilon$ -ball $B_{p}(\\mathbf{x},\\epsilon)$ , an $\\ell_{p}$ -norm ball of radius $\\epsilon>0$ centered at $\\mathbf{x}$ , $\\left\\{\\mathbf{x}^{\\prime}:\\|\\mathbf{x}^{\\prime}-\\mathbf{x}\\|_{p}=\\|\\delta\\|_{p}<\\epsilon\\right\\}$ . The distance $\\Vert\\mathbf{x}^{\\prime}-\\mathbf{x}\\Vert_{p}=\\Vert\\delta\\Vert_{p}$ represents the perturbation size defined as $(\\sum_{i=1}^{n}|\\delta_{i}|^{p})^{\\frac{1}{p}}$ . In this paper, we will focus on $\\ell_{\\infty}$ norm $(\\|\\mathbf x\\|_{\\infty}=\\operatorname*{max}_{i=1,\\dots,n}|x_{i}|)$ , which is the most commonly used norm in the literature. ", "page_idx": 2}, {"type": "text", "text": "Local robustness Different notions of local robustness exist in the literature (Gourdeau et al., 2021; Zhong et al., 2021; Han et al., 2023). In this paper, we equate local robustness to $\\ell_{p}$ -robustness, a standard notion corresponding to the invariance of the decision within the $\\ell_{p}$ $\\epsilon$ -ball around the sample (Bastani et al., 2016; Fawzi et al., 2018) and formalized in terms of $\\epsilon_{}$ -robustness. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. A model $f$ is \u03f5-robust at point $\\mathbf{x}$ if for any $\\mathbf{x}^{\\prime}\\in B_{p}(\\mathbf{x},\\epsilon)$ $\\mathbf{\\acute{x}}^{\\prime}$ in the \u03f5-ball around x), we have $\\hat{y}(\\mathbf{x}^{\\prime})=\\hat{y}(\\mathbf{x})$ . ", "page_idx": 2}, {"type": "text", "text": "For a given robustness threshold $\\epsilon$ , a data instance is said to be non-robust for the model if this model is not $\\epsilon$ -robust on it. This means it is possible to construct an adversarial sample from that instance in its vicinity (i.e., within an $\\epsilon$ -ball distance from the original instance). A vulnerable sample to adversarial attacks is necessarily non-robust. This notion of local robustness can be quantified in the worst-case or, on average, inside the $\\epsilon$ -ball. We focus here on the worst-case measurement given by the input margin, also referred to as the minimum distortion or the robust radius (Szegedy et al., 2014; Carlini & Wagner, 2016; Weng, 2019) ", "page_idx": 2}, {"type": "text", "text": "The input space margin is the distance to the decision boundary of $f$ in the input space. It is the norm of a minimal perturbation required to change the model\u2019s decision at a test point $\\mathbf{x}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nd_{i n}(\\mathbf{x})=\\operatorname*{inf}\\{\\|\\delta\\|_{p}:\\delta\\in\\mathbb{R}^{n}{\\mathrm{~s.t.~}}{\\hat{y}}(\\mathbf{x})\\neq{\\hat{y}}(\\mathbf{x}+\\delta)\\}=\\operatorname*{sup}\\{\\epsilon:f{\\mathrm{~is~}}\\epsilon{\\mathrm{-robust~at~}}\\mathbf{x}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "An instance $\\mathbf{x}$ is non-robust for a robustness threshold $\\epsilon$ if $d_{i n}(\\mathbf{x})\\leq\\epsilon$ . Evaluating Eq. 1 for deep networks is known to be intractable in the general case. An upper bound approximation can be obtained using a point $\\ensuremath{\\mathbf{x}}_{0}^{\\prime}$ , the closest adversarial counterpart of $\\mathbf{x}$ in $\\ell_{p}$ norm by $\\bar{\\hat{d}}_{i n}(\\mathbf{x})=\\|\\mathbf{x}-\\mathbf{x}_{0}^{\\prime}\\|_{p}$ (see Fig. 1). ", "page_idx": 2}, {"type": "text", "text": "The logit margin is the difference between the two largest logits. For a sample $\\mathbf{x}$ classified as $i=\\hat{y}(\\mathbf{x})=\\arg\\operatorname*{max}_{j\\in\\mathcal{Y}}{f_{\\theta}^{j}(\\mathbf{x})}$ the logit margin is defined as $\\left(f_{\\theta}^{i}(\\mathbf{x})-\\operatorname*{max}_{j,j\\neq i}f_{\\theta}^{j}(\\mathbf{x})\\right)>0$ . It is an approximation of the distance to the decision boundary of $f_{\\theta}$ in the feature space. The decision boundary in the feature space around ${\\bf z}=h_{\\psi}({\\bf x})$ , the feature representation of $\\mathbf{x}$ , is composed of $(K-1)$ linear decision boundaries (hyperplanes) ${\\bf D}{\\bf B}_{i j}=\\{{\\bf z}^{\\prime}\\in\\mathbb{R}^{m}:{\\bf w}_{i}^{\\top}{\\bf z}^{\\prime}+b_{i}={\\bf w}_{j}^{\\top}{\\bf z}^{\\prime}+b_{j}\\}\\,(j\\neq$ $i,$ ). The margin in the feature space is therefore the distance to the closest hyperplane $\\operatorname*{min}_{j,j\\neq i}d(\\mathbf{z},\\mathrm{DB}_{i j})$ , where the distance $d(\\mathbf{z},\\mathrm{DB}_{i j})$ from ${\\bf z}$ to a hyperplane $\\mathrm{DB}_{i j}$ has a closed-form expression: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d(\\mathbf{z},\\mathbf{DB}_{i j})=\\operatorname*{inf}\\{\\|\\eta\\|_{p}:\\eta\\in\\mathbb{R}^{m}\\,\\mathrm{~s.t.~}\\,\\mathbf{z}+\\eta\\in\\mathbf{DB}_{i j}\\}=\\frac{f_{\\theta}^{i}(\\mathbf{x})-f_{\\theta}^{j}(\\mathbf{x})}{\\|\\mathbf{w}_{i}-\\mathbf{w}_{j}\\|_{q}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\|\\cdot\\|_{q}}$ is the dual norm of $p$ , $\\begin{array}{r}{q=\\frac{p}{p-1}}\\end{array}$ for $p>1$ (Moosavi-Dezfooli et al., 2016; Elsayed et al., 2018). ", "page_idx": 3}, {"type": "text", "text": "When the classifiers ${\\bf w}_{j}$ are equidistant, i.e. $\\|\\mathbf{w}_{i}-\\mathbf{w}_{j}\\|_{q}=C$ whenever $i\\neq j$ , the margin becomes: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j,j\\neq i}\\frac{f_{\\theta}^{i}(\\mathbf{x})-f_{\\theta}^{j}(\\mathbf{x})}{C}=\\frac{1}{C}\\operatorname*{min}_{j,j\\neq i}\\left(f_{\\theta}^{i}(\\mathbf{x})-f_{\\theta}^{j}(\\mathbf{x})\\right)=\\frac{1}{C}\\Bigg(\\underbrace{f_{\\theta}^{i}(\\mathbf{x})-\\operatorname*{max}_{j,j\\neq i}f_{\\theta}^{j}(\\mathbf{x})}_{\\mathrm{logit}\\mathrm{margin}}\\Bigg).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Under the equidistance assumption, the logit margin is proportional (equal up to a scaling factor) to the margin in the feature space. We will denote the logit margin of $\\mathbf{x}$ by $d_{o u t}(\\mathbf{x})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{o u t}(\\mathbf{x})=f_{\\boldsymbol{\\theta}}^{i}(\\mathbf{x})-\\operatorname*{max}_{j,j\\neq i}f_{\\boldsymbol{\\theta}}^{j}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.2 Margin Consistency ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Definition 2. A model is margin-consistent if there is a monotonic relationship between the input space margin and the logit margin, i.e., $d_{i n}(\\mathbf{\\bar{x_{1}}})\\leq d_{i n}(\\mathbf{x_{2}})\\Leftrightarrow d_{o u t}(\\mathbf{x_{1}})\\leq d_{o u t}(\\mathbf{\\bar{x}_{2}})$ , $\\forall\\mathbf{x}_{1},\\mathbf{x}_{2}\\in\\mathcal{X}$ . ", "page_idx": 3}, {"type": "text", "text": "A margin-consistent model preserves the relative position of samples to the decision boundary from the input space to the feature space. A sample further from (closer to) the decision boundary in the input space remains further from (closer to) the decision boundary in the feature space with respect to other samples, as illustrated in Fig. 1. ", "page_idx": 3}, {"type": "text", "text": "We can evaluate margin consistency by computing the Kendall rank correlation $(\\tau\\in[-1,1])$ between the logit margins and the input margins over a test set. The Kendall rank correlation tests the existence and strength of a monotonic relationship between two variables. It makes no assumption on the distribution of the variables and is robust to outliers (Chattamvelli, 2024). While a positive value of $\\tau$ indicates samples are ranked similarly (or identically for $\\tau=1$ ) according to logit margins and input margins, a negative value of $\\tau$ indicates that one margin\u2019s ranking is roughly reversed. Perfect margin consistency corresponds to the situation $\\tau=1$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Non-robust Samples Detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Non-robust detection can be defined as a scored-based binary classification task where non-robust samples constitute the positive class, and the input margin $d_{i n}$ induces a perfect discriminative function $g$ for that: ", "page_idx": 3}, {"type": "equation", "text": "$$\ng(\\mathbf{x};f_{\\theta})=\\mathbb{1}_{[d_{i n}(\\mathbf{x})\\leq\\epsilon]}={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}\\mathbf{x}{\\mathrm{~is~non}}\\mathrm{{-robust}}}\\\\ {0}&{{\\mathrm{if~}}\\mathbf{x}{\\mathrm{~is~robust}}}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If a model is margin-consistent, its logit margin can also be a discriminative score to detect nonrobust samples. The following theorem establishes that this is a necessary and sufficient condition. Therefore, the degree to which a model is margin-consistent should determine the discriminative power of the logit margin. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. If a model is margin-consistent, then for any robustness threshold \u03f5, there exists a threshold \u03bb for the logit margin $d_{o u t}$ that separates perfectly non-robust samples and robust samples. Conversely, if for any robustness threshold \u03f5, $d_{o u t}$ admits a threshold $\\lambda$ that separates perfectly non-robust samples from robust samples, then the model is margin-consistent. ", "page_idx": 3}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/14c29faf333f8377adbe3438f9d1a8d6d78b0f680c3340f50de4d08c8eace19a.jpg", "img_caption": ["(a) Margin consistency implies $d_{o u t}$ can perfectly separate non robust samples in $A_{\\epsilon}$ from robust samples. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/e1f4b962e2e6c02a1b054bdbe79b1b0f628d8ac123b310c504011b7d67ebc130.jpg", "img_caption": ["(b) Without margin consistency, $d_{o u t}$ cannot be a good discriminator for robust and non-robust samples. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Illustration of Theorem 1\u2019s proof. ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. Fig. 2 presents intuition behind the proof of Theorem 1. For the first part of the theorem (see Fig. 2a), if there is a monotonic relationship between $d_{i n}$ and $d_{o u t}$ (margin consistency), any point $\\mathbf{x}$ with $d_{i n}$ less than the threshold $\\epsilon$ (non-robust) will also have $d_{o u t}$ less than $\\lambda_{0}=d_{o u t}(\\dot{\\bf x_{0}})$ (with $d_{i n}(\\mathbf{x_{0}})=\\epsilon)$ . For the second part (see Fig. 2b), if there are two points $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ with nonconcordant $d_{i n}$ and $d_{o u t}$ (no margin consistency), then for a threshold $\\epsilon_{0}$ between $d_{i n}(x_{1})$ and $d_{o u t}(x_{2})$ , they will both have different classes but no threshold of $d_{o u t}$ can classify them both correctly. The complete proof of Theorem 1 is deferred to Appendix A. Common metrics for detection include (Hendrycks & Gimpel, 2017; Corbi\u00e8re et al., 2019; Zhu et al., 2023): the Area Under the Receiver Operating Curve (AUROC), which ensures the ability of a model to distinguish between the positive and negative classes across all possible thresholds; the Area Under the PrecisionRecall Curve (AUPR), which evaluates the trade-off between precision and recall and is less sensitive to imbalance between positive and negative classes; and the False Positive Rate (FPR) at a $95\\%$ True Positive Rate (TPR) $(\\mathbf{FPR}@95)$ , that is crucial in systems where missing positive cases can have serious consequences, such as minimizing the number of vulnerable samples missed. The AUROC and AUPR of a perfect classifier is 1, while 0.5 for a random classifier. ", "page_idx": 4}, {"type": "text", "text": "3 Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Datasets and models We investigate various pre-trained models on CIFAR10 and CIFAR100 datasets (Krizhevsky, 2009). The majority of models were loaded from the RobustBench model $200^{2}$ (Croce et al., 2021), with a few more models that are ResNet-18 (He et al., 2016) models we trained on CIFAR10 with Standard Adversarial Training (Madry et al., 2018), TRADES (Zhang et al., 2019), Logit Pairing (ALP and CLP, Kannan et al. (2018)), and MART (Wang et al., 2020), using the experimental setup of Wang et al. (2020). ", "page_idx": 4}, {"type": "text", "text": "Input margin estimation This is done using FAB attack (Croce & Hein, 2020a), which is an attack that minimally perturbs the initial instance. Xu et al. (2023) used it in their adversarial training strategy as a reliable way to compute the closest boundary point given enough iterations. We perform the untargeted FAB attack without restricting the distortion to find the boundary for all the samples in the test set instead of constraining the perturbation inside a given $\\epsilon$ -ball when evaluating robustness. As a sanity check for the measured distances, we compare the ratio of correct samples $\\mathbf{x}$ with estimated input margins greater than $\\epsilon=8/255$ and the robust accuracy in $\\ell_{\\infty}$ norm measured with AutoAttack (Croce & Hein, 2020b) at $\\epsilon=8/255$ . Both quantities estimate the same thing, with a mean absolute difference over the models of 1.3 and 0.48 for CIFAR10 and CIFAR100, respectively, which are reasonable. ", "page_idx": 4}, {"type": "text", "text": "The estimation of the input margins over the 10, 000 test samples allows us to create for a given threshold $\\epsilon$ a pool of vulnerable samples that can be successfully attacked at threshold $\\epsilon$ and nonvulnerable samples that were not able to be attacked. Training and distance estimations were run on an NVIDIA Titan Xp GPU (1x). ", "page_idx": 4}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/fcf4c27a6813f7b237664feebbf536e4e93899b23e0e13dd5f3e5df256a410df.jpg", "img_caption": ["Figure 3: Margin consistency of various models: there is a strong correlation between input space margin and logit margin for most $\\ell_{\\infty}$ robust models tested, the exceptions being DI0 and XU80 on CIFAR10. See Table 1 for the references on the models. The correlations are given with standard error for the y-axis values in each interval. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.2 Results and Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Correlation analysis The results presented in Fig. 3 show that the logit margin has a strong correlation (up to 0.86) with the input margin, which means that they have a level of margin consistency for those models. The plots are given with standard error for the y-axis values in each interval. However, we also observe that two models (i.e., DI0 (Ding et al., 2020) and XU80 (Xu et al., 2023) WideResNets) have a weaker correlation. We show in Sec. 3.3 that we can learn to map the feature representation of these models to a pseudo-margin that reflects the distance to the decision boundary in the input space. Additional results on ImageNet with $\\ell_{\\infty}$ norm and on $\\ell_{2}$ -robust models on CIFAR10 are given in Table 5 of appendix E. ", "page_idx": 5}, {"type": "text", "text": "Vulnerable samples detection We present the results for the robustness threshold $\\epsilon=8/255$ in Table 1. As expected with the strong correlations, the performance over the non-robust detection task is excellent. We can note that the metrics are lower for the two models with low correlations and particularly very high $\\mathrm{FPR}@95$ . The performance remains quite good with different values of $\\epsilon$ (cf. appendix B). Moreover, we show in appendix F.1 that the empirical robust accuracy of margin-consistent models can be accurately estimated by attacking only a small subset of the test set. ", "page_idx": 5}, {"type": "text", "text": "Margin Consistency and Lipschitz Smoothness A neural network $f$ is said to be $L$ -Lipschitz if $\\|f(\\mathbf{x}_{1})-f(\\mathbf{x}_{2})\\|\\,\\leq\\,L\\|\\mathbf{x}_{1}-\\mathbf{x}_{2}\\|$ , $\\forall\\mathbf{x}_{1},\\mathbf{x}_{2}$ . Lipschitz smoothness is important for adversarial robustness because a small Lipschitz constant $L$ guarantees the network\u2019s output cannot change more than a factor $L$ of the change in the input. There are strategies to directly constraint the Lipschitz constant to achieve 1-Lipschitz networks (Cisse et al., 2017; Li et al., 2019; Serrurier et al., 2021; Araujo et al., 2023). Empirical adversarial training strategies only indirectly encourage Lipschitz\u2019s smoothness of the model. However, we note that Lipschitz\u2019s continuity of the feature extractor $h_{\\psi}$ does not imply margin consistency of the model. Considering two points $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ with $0<d_{i n}(\\mathbf{x}_{1})<d_{i n}(\\mathbf{x}_{2})$ , the $L$ -Lipschitz condition implies that $d_{o u t}(\\mathbf{x}_{i})\\leq L d_{i n}(\\mathbf{x}_{i})$ for $i=1,2$ . However, as long as $\\dot{d}_{o u t}(\\mathbf{x}_{1})>0$ , it is possible $a$ priori to have $d_{o u t}(\\mathbf{x}_{2})<d_{o u t}(\\mathbf{x}_{1})$ , thus violating the margin consistency condition, while still satisfying the previous relations. We also note that the strength of the correlation, i.e. the level of margin consistency, does not depend on the robust accuracy (see Fig. 4a and 4b). ", "page_idx": 5}, {"type": "text", "text": "Insight into when margin consistency may hold? We hypothesize that when the feature extractor $h_{\\psi}$ behaves locally as an isometry (preserving distances, up to a scaling factor $\\kappa$ , at least for directions normal to the decision boundary), i.e., $\\|\\mathbf{x}-\\mathbf{\\bar{x}}^{\\prime}\\|_{p}=\\kappa\\|\\bar{h_{\\psi}}(\\mathbf{x})-h_{\\psi}(\\mathbf{\\bar{x}}^{\\prime})\\|_{p}$ , margin consistency will occur. Given an input sample $\\mathbf{x}$ , by definition $d_{o u t}({\\bf x})=||{\\bf z}-{\\bf z}^{\\prime}||$ where ${\\bf z}=h_{\\psi}({\\bf x})$ and $\\mathbf{z}^{\\prime}$ the closest point to $\\mathbf{z}$ on the feature space decision boundary. The bijectivity of a local isometry implies that we have $h_{\\psi}(\\mathbf{x}^{\\prime})=\\mathbf{z}^{\\prime}$ , i.e. the representation of the closest point to $\\mathbf{x}$ in input space matches the closest point to the representation of $\\mathbf{x}$ in the feature space. In that case we will have $\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|=\\kappa\\|\\mathbf{z}-\\mathbf{z}^{\\prime}\\|$ which implies margin consistency. Experimentally, what we observe is that on the one hand, the input margin and the distance between the feature representations of $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$ (feature distance) correlate and on the other hand, the feature distance and the logit margin also correlate (Fig. 5a and Fig. 5b respectively). ", "page_idx": 5}, {"type": "table", "img_path": "XHCYZNmqnv/tmp/18935b5cb1597bbc7bcf14b45e3e4b097ec367399fde294077695648552da24f.jpg", "table_caption": [], "table_footnote": ["Table 1: Correlations and vulnerable points detection performance at $\\overline{{\\epsilon\\;=\\;8/255}}$ on different adversarially trained models. "], "page_idx": 6}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/d9988e306510fca3e665583dc224cc9f18dad3805394aa7e44466e654173e4b9.jpg", "img_caption": ["Figure 4: Distribution of the correlation between input margins and logit margins in $\\ell_{\\infty}$ with robust accuracy. The strength of the correlation, which indicates the level of margin consistency, does not depend on the robust accuracy. References on models are given in Table 1. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.3 Learning a Pseudo-Margin ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For the two models that are weakly margin-consistent, we are proposing to directly learn a mapping that maps the feature representation of a sample to a pseudo-margin that reflects the relative position of the samples to the decision boundary in the input space. We use a learning scheme similar to the one of Corbi\u00e8re et al. (2019), with a small ad hoc neural network for learning the confidence of the instances. Given some samples with estimations of their input margins, the objective is to learn to map their feature representation to a pseudo-margin that correlates with the input margins. This learning task can be seen as a learning-to-rank problem. We use a simple learning-to-rank algorithm for that purpose, which is a pointwise regression approach (He et al., 2008) relying on the mean squared error as a surrogate loss. ", "page_idx": 6}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/bb9c11c3519c9bd733499ba36d141fd0e16e173734448f308ea588d1ee9af455.jpg", "img_caption": ["(a) Input space margins vs feature distances. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/af74a2eeae2ac7a0dbdb9d6028b3ca805e42acffa7dcb9615439690f681d1b30.jpg", "img_caption": ["Figure 5: The correlations between the input margin, the distance between the feature representations of samples and their closest adversaries (feature distance \u2013 $\\|h_{\\psi}(x)-h_{\\psi}(x^{\\prime})\\|)$ , and the logit margin may be due to the local isometry of the feature extractor. See Table 1 for the specific references on the model ID. The correlations are given with standard error for the $\\mathrm{y}.$ -axis values in each interval. ", "(b) Feature distances vs logit margins. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For the experiment, we used a similar architecture and training protocol as (Corbi\u00e8re et al., 2019) with a fully connected network with five dense layers of 512 neurons, with ReLU activations for the hidden layers and a sigmoid activation at the output layer. We learn using 5000 examples sampled randomly from the training set, with $20\\%$ (1000 examples) held as a validation. Fig. 6 and Table 2 show the improved correlation on the learned score compared to the logit margin for both models. The correlations are given with standard error for the y-axis values in each interval. The network has learned to recover the relative positions of the samples from the feature representation. ", "page_idx": 7}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Detection tasks in machine learning are found to be of three main types: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Adversarial Detection The goal of adversarial detection (Xu et al., 2017; Carlini & Wagner, 2017) is to discriminate adversarial samples from clean and noisy samples. An adversarial example is a malicious example found by adversarially attacking a sample; it has a different class while being close to the original sample. A vulnerable (non-robust) sample is a normal sample that admits an adversarial example close to it. The two detection tasks are very distinct. Adversarial detection is a defence mechanism like adversarial training; Tramer (2022) has established that both tasks are equivalent problems with the same difficulty.   \n\u2022 Out-of-Distribution (OOD) detection In OOD detection (Hendrycks & Gimpel, 2017; Peng et al., 2024; Yang et al., 2021), the objective is to detect instances far from the distribution of the training data. These are often instances with different labels from the training labels or instances with the same label as training labels but with a covariate shift. For example, for a model trained on the CIFAR10 dataset, samples from the SVHN dataset (Netzer et al., 2011) or the corrupted version of CIFAR10-C (Hendrycks & Dietterich, 2019) are OOD samples for such a model.   \n\u2022 Misclassification Detection (MisD) It consists in detecting whether the classifier\u2019s prediction is incorrect. This is also referred to as Failure Detection or Trustworthiness Detection (Corbi\u00e8re et al., 2019; Jiang et al., 2018; Luo et al., 2021; Granese et al., 2021; Zhu et al., 2023). MisD is often used for selective classification (classification with a reject option) (Geifman & El-Yaniv, 2017) to abstain from predicting samples on which the model is likely to be wrong. A score for non-robust detection cannot tell if the sample is incorrect, as a vulnerable sample could be from any side of the decision boundary. ", "page_idx": 7}, {"type": "text", "text": "Formal robustness verification aims at certifying whether a given sample is $\\epsilon$ -robust or if it is not an adversarial counter-example can be provided (Brix et al., 2023b). Some complete exact methods based on solving Satisfiability Modulo Theory problems (Katz et al., 2017; Carlini et al., 2017; Huang et al., 2017) or Mixed-Integer Linear Programming (Cheng et al., 2017; Lomuscio & Maganti, 2017; Fischetti & Jo, 2017) provide formal certification given enough time. However, in practice, they are tractable only up to 100,000 activations (Tjeng et al., 2019). Incomplete but effective methods based on linear and convex relaxation methods and Branch-and-Bound methods (Zhang et al., 2018; Salman et al., 2019; Xu et al., 2020, 2021; Zhang et al., 2022; Shi et al., 2023) are faster but conservative, without guaranteed certifications even if given enough time. Scaling them to bigger architectures such as WideResNets and large Transformers is still challenging even with GPU accelartion(Brix et al., 2023a; K\u00f6nig et al., 2024). Weng et al. (2018) converts the problem of finding the robust radius (input margin) as a local Lipschitz constant estimation problem. Computing the Lipschitz constant of Deep Nets is NP-hard (Virmaux & Scaman, 2018) and Jordan & Dimakis (2020) proved that there is no efficient algorithm to compute the local Lipschitz constant. The estimation provided by Weng et al. (2018) requires random sampling and remains computationally expensive to obtain a good approximation. Vulnerability detection with margin-consistent models does not provide certificates but an empirical estimation of the robustness of a sample as evaluated by adversarial attacks. At scale, it can help filter the samples to undergo formal verification and a more thorough adversarial attack for resource prioritization. ", "page_idx": 7}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/308cf76c49bc5101b88d5beb0f9c361c6996621fb939541f6f32b190e240cac1.jpg", "img_caption": ["Figure 6: Correlation improvement of the learned pseudo-margin over the logit margin for DI0 (Ding et al., 2020) and XU80 ( $\\mathrm{\\DeltaXu}$ et al., 2023). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "XHCYZNmqnv/tmp/cc2546c6f5d06fb607785a93136b05a27ff7f74946d61deb81ac67b82ede63fd.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison of the correlation and detection performance between the actual logit margin and the pseudo-margin learned. The models are initially weakly margin-consistent, but the pseudo-margin learned from feature representations simulates the margin consistency with higher correlation and better discriminative power. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Limitations and Perspectives ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Vulnerability detection scope The scope of this work is $\\ell_{p}$ robustness measured by the input space margin; the minimum distortion that changes the model\u2019s decision while this does not give a full view of the $\\ell_{p}$ robustness. Samples may be at the same distance to the decision boundary and have unequal unsafe neighbourhoods given by an average estimation over the $\\epsilon$ -neighbourhood considered. The average estimation of local robustness for a given $\\epsilon$ -neighborhood remains an open problem, so whether it is possible to extract other notions of robustness from the feature representation efficiently could be a potential avenue for further exploration. ", "page_idx": 8}, {"type": "text", "text": "Attack-based verification The margin consistency property does not rely on attacks; however, its verification and the learning of a pseudo-margin with an attack-based estimation may not be possible if the model cannot be attacked on a sufficient number of samples. The assumption is that we can always successfully provide the closest point to the decision with a sufficient budget. This is a reasonable assumption since the studied models are not perfectly robust, and the empirical evidence so far with adaptive attacks is that no defence is foolproof, which justifies the need to detect the non-robust samples. It might occur that we need to combine with an attack such as CW-attack (Carlini & Wagner, 2016) to find the closest adversarial sample. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Influence of terminal phase of training The work of Papyan et al. (2020) shows that when deep neural network classifiers are trained beyond zero training error and beyond zero cross-entropy loss (aka terminal phase of training), they fall into a state known as neural collapse. Neural collapse is a state where the within-class variability of the feature representations collapses to their class means, the class means, and the classifiers become self-dual and converge to a specific geometric structure, an equiangular tight frame (ETF) simplex, and the network classifier converges to nearest train class center. This implies that we may lose the margin consistency property. While neural collapse predicts that all representations collapse on their class mean, in practice, perfect collapse is not quite achieved, and it is precisely the divergence of a representation from its class mean (or equivalently its $\\bf{w_{i}}$ ) which encodes the information we seek about the distance to the decision boundary in the input space. Exploring the impact of the neural collapse on margin consistency as models tend toward a collapsed state could provide valuable insights into generalization and adversarial robustness. ", "page_idx": 9}, {"type": "text", "text": "Adaptaptive attacks and adversarial examples In this paper, we study the margin consistency of models on their training distribution by reporting the Kendall rank correlation between the logit margin and the input margin on the test set. The study of this property on inputs from a different distribution or specifically crafted examples is left for future research. However, we observe that the adversarial examples used for the input margin estimation have significantly smaller logit margins than the detection thresholds (see Table 4 in appendix D). This indicates that these specific adversarial examples are indeed identified as non-robust instances, together with clean non-robust samples. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work addresses the question of efficiently estimating local robustness in the $\\ell_{p}$ sense at a per-instance level in robust deep neural classifiers in deployment scenarios. We introduce margin consistency as a necessary and sufficient condition to use the logit margin of a deep classifier as a reliable proxy estimation of the input margin for detecting non-robust samples. Our investigation of various robustly trained models shows that they have high margin consistency, which leads to a high performance of the logit margins in detecting vulnerable samples to adversarial attacks. We also find that margin consistency does not always hold, with some models having a weak correlation between the input margin and the logit margin. In such cases, we show that it is possible to learn to map the feature representation to a better-correlated pseudo-margin that simulates the margin consistency and performs better on vulnerability detection. Finally, we present some limitations of this work, mainly the scope of robustness, the attack-based verification, the impact of neural collapse in terminal phases of training, and vulnerability to adaptive attacks. Beyond its highly practical importance, we see this as a motivation to extend the analysis of robust models and the properties of their feature representations in the context of vulnerability detection. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Addepalli, S., Jain, S., Sriramanan, G., Khare, S., and Radhakrishnan, V. B. Towards achieving adversarial robustness beyond perceptual limits. In ICML 2021 Workshop on Adversarial Machine Learning, 2021. URL https://openreview.net/forum?id $\\equiv$ SHB_znlW5G7.   \nAddepalli, S., Jain, S., Sriramanan, G., and Venkatesh Babu, R. Scaling adversarial training to large perturbation bounds. In European Conference on Computer Vision, pp. 301\u2013316. Springer, 2022.   \nAraujo, A., Havens, A. J., Delattre, B., Allauzen, A., and Hu, B. A unified algebraic perspective on lipschitz neural networks. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=k71IGLC8cfc.   \nBastani, O., Ioannou, Y., Lampropoulos, L., Vytiniotis, D., Nori, A., and Criminisi, A. Measuring neural net robustness with constraints. Advances in neural information processing systems, 29, 2016.   \nBiggio, B., Corona, I., Maiorca, D., Nelson, B., \u0160rndi\u00b4c, N., Laskov, P., Giacinto, G., and Roli, F. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pp. 387\u2013402. Springer, 2013.   \nBrix, C., Bak, S., Liu, C., and Johnson, T. T. The fourth international verification of neural networks competition (vnn-comp 2023): Summary and results. arXiv preprint arXiv:2312.16760, 2023a.   \nBrix, C., M\u00fcller, M. N., Bak, S., Johnson, T. T., and Liu, C. First three years of the international verification of neural networks competition (vnn-comp). International Journal on Software Tools for Technology Transfer, 25(3):329\u2013339, 2023b.   \nCarlini, N. and Wagner, D. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM workshop on artificial intelligence and security, pp. 3\u201314, 2017.   \nCarlini, N. and Wagner, D. A. Towards evaluating the robustness of neural networks. 2017 IEEE Symposium on Security and Privacy (SP), pp. 39\u201357, 2016.   \nCarlini, N., Katz, G., Barrett, C., and Dill, D. L. Provably minimally-distorted adversarial examples. arXiv preprint arXiv:1709.10207, 2017.   \nCarmon, Y., Raghunathan, A., Schmidt, L., Duchi, J. C., and Liang, P. S. Unlabeled data improves adversarial robustness. Advances in neural information processing systems, 32, 2019.   \nChattamvelli, R. Correlation in Engineering and the Applied Sciences: Applications in R. Springer Nature, 2024.   \nCheng, C.-H., N\u00fchrenberg, G., and Ruess, H. Maximum resilience of artificial neural networks. In Automated Technology for Verification and Analysis: 15th International Symposium, ATVA 2017, Pune, India, October 3\u20136, 2017, Proceedings 15, pp. 251\u2013268. Springer, 2017.   \nCisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and Usunier, N. Parseval networks: Improving robustness to adversarial examples. In International conference on machine learning, pp. 854\u2013863. PMLR, 2017.   \nCorbi\u00e8re, C., Thome, N., Bar-Hen, A., Cord, M., and P\u00e9rez, P. Addressing failure prediction by learning model confidence. Advances in Neural Information Processing Systems, 32, 2019.   \nCroce, F. and Hein, M. Minimally distorted adversarial examples with a fast adaptive boundary attack. In International Conference on Machine Learning, pp. 2196\u20132205. PMLR, 2020a.   \nCroce, F. and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206\u20132216. PMLR, 2020b.   \nCroce, F., Andriushchenko, M., Sehwag, V., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P., and Hein, M. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id $\\cdot$ SSKZPJCt7B.   \nCui, J., Tian, Z., Zhong, Z., Qi, X., Yu, B., and Zhang, H. Decoupled kullback-leibler divergence loss. arXiv preprint arXiv:2305.13948, 2023.   \nDebenedetti, E., Sehwag, V., and Mittal, P. A light recipe to train robust vision transformers. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pp. 225\u2013253. IEEE, 2023.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.   \nDing, G. W., Sharma, Y., Lui, K. Y. C., and Huang, R. Mma training: Direct input space margin maximization through adversarial training. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HkeryxBtPB.   \nDreossi, T., Ghosh, S., Sangiovanni-Vincentelli, A., and Seshia, S. A. A formalization of robustness for deep neural networks. arXiv preprint arXiv:1903.10033, 2019.   \nElsayed, G., Krishnan, D., Mobahi, H., Regan, K., and Bengio, S. Large margin deep networks for classification. Advances in neural information processing systems, 31, 2018.   \nEngstrom, L., Ilyas, A., Salman, H., Santurkar, S., and Tsipras, D. Robustness (python library), 2019. URL https://github.com/MadryLab/robustness.   \nEvtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A., Rahmati, A., and Song, D. Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2 (3):4, 2017.   \nFawzi, A., Fawzi, H., and Fawzi, O. Adversarial vulnerability for any classifier. Advances in neural information processing systems, 31, 2018.   \nFischetti, M. and Jo, J. Deep neural networks as 0-1 mixed integer linear programs: A feasibility study. arXiv preprint arXiv:1712.06174, 2017.   \nGeifman, Y. and El-Yaniv, R. Selective classification for deep neural networks. Advances in neural information processing systems, 30, 2017.   \nGnanasambandam, A., Sherman, A. M., and Chan, S. H. Optical adversarial attack. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 92\u2013101, 2021.   \nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6572.   \nGourdeau, P., Kanade, V., Kwiatkowska, M., and Worrell, J. On the hardness of robust classification. The Journal of Machine Learning Research, 22(1):12521\u201312549, 2021.   \nGranese, F., Romanelli, M., Gorla, D., Palamidessi, C., and Piantanida, P. Doctor: A simple method for detecting misclassification errors. Advances in Neural Information Processing Systems, 34: 5669\u20135681, 2021.   \nHan, T., Srinivas, S., and Lakkaraju, H. Efficient estimation of local robustness of machine learning models. In ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH), 2023. URL https://openreview.net/forum?id $=$ ZGSfAElJmp.   \nHe, C., Wang, C., Zhong, Y.-X., and Li, R.-F. A survey on learning to rank. In 2008 International Conference on Machine Learning and Cybernetics, volume 3, pp. 1734\u20131739. Ieee, 2008.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.   \nHendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=HJz6tiCqYm.   \nHendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $\\equiv$ Hkg4TI9xl.   \nHendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. In International conference on machine learning, pp. 2712\u20132721. PMLR, 2019.   \nHuang, X., Kwiatkowska, M., Wang, S., and Wu, M. Safety verification of deep neural networks. In International conference on computer aided verification, pp. 3\u201329. Springer, 2017.   \nJiang, H., Kim, B., Guan, M., and Gupta, M. To trust or not to trust a classifier. Advances in neural information processing systems, 31, 2018.   \nJordan, M. and Dimakis, A. G. Exactly computing the local lipschitz constant of relu networks. Advances in Neural Information Processing Systems, 33:7344\u20137353, 2020.   \nKannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018.   \nKatz, G., Barrett, C., Dill, D. L., Julian, K., and Kochenderfer, M. J. Reluplex: An efficient smt solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pp. 97\u2013117. Springer, 2017.   \nK\u00f6nig, M., Bosman, A. W., Hoos, H. H., and van Rijn, J. N. Critically assessing the state of the art in neural network verification. Journal of Machine Learning Research, 25(12):1\u201353, 2024.   \nKrizhevsky, A. Learning multiple layers of features from tiny images. Technical Report TR-2009, University of Toronto, 2009.   \nLi, Q., Haque, S., Anil, C., Lucas, J., Grosse, R. B., and Jacobsen, J.-H. Preventing gradient attenuation in lipschitz constrained convolutional networks. Advances in neural information processing systems, 32, 2019.   \nLiu, F., Han, B., Liu, T., Gong, C., Niu, G., Zhou, M., Sugiyama, M., et al. Probabilistic margins for instance reweighting in adversarial training. Advances in Neural Information Processing Systems, 34:23258\u201323269, 2021.   \nLomuscio, A. and Maganti, L. An approach to reachability analysis for feed-forward relu neural networks. arXiv preprint arXiv:1706.07351, 2017.   \nLuo, Y., Wong, Y., Kankanhalli, M. S., and Zhao, Q. Learning to predict trustworthiness with steep slope loss. Advances in Neural Information Processing Systems, 34:21533\u201321544, 2021.   \nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id $=$ rJzIBfZAb.   \nMickisch, D., Assion, F., Gre\u00dfner, F., G\u00fcnther, W., and Motta, M. Understanding the decision boundary of deep neural networks: An empirical study. arXiv preprint arXiv:2002.01810, 2020.   \nMoosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2574\u20132582, 2016.   \nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A. Y., et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 4. Granada, 2011.   \nPang, T., Lin, M., Yang, X., Zhu, J., and Yan, S. Robustness and accuracy could be reconcilable by (proper) definition. In International Conference on Machine Learning, pp. 17258\u201317277. PMLR, 2022.   \nPapyan, V., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652\u201324663, 2020.   \nPeng, B., Luo, Y., Zhang, Y., Li, Y., and Fang, Z. Conjnorm: Tractable density estimation for out-ofdistribution detection. In Proceedings of the International Conference on Learning Representations, 2024.   \nRade, R. and Moosavi-Dezfooli, S.-M. Helper-based adversarial training: Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In ICML 2021 Workshop on Adversarial Machine Learning, 2021. URL https://openreview.net/forum?id $\\equiv$ BuD2LmNaU3a.   \nRebuff,i S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., and Mann, T. Fixing data augmentation to improve adversarial robustness. arXiv preprint arXiv:2103.01946, 2021.   \nRice, L., Wong, E., and Kolter, Z. Overftiting in adversarially robust deep learning. In International Conference on Machine Learning, pp. 8093\u20138104. PMLR, 2020.   \nSalman, H., Yang, G., Zhang, H., Hsieh, C.-J., and Zhang, P. A convex relaxation barrier to tight robustness verification of neural networks. Advances in Neural Information Processing Systems, 32:9835\u20139846, 2019.   \nSehwag, V., Mahloujifar, S., Handina, T., Dai, S., Xiang, C., Chiang, M., and Mittal, P. Robust learning meets generative models: Can proxy distributions improve adversarial robustness? arXiv preprint arXiv:2104.09425, 2021.   \nSerrurier, M., Mamalet, F., Gonz\u00e1lez-Sanz, A., Boissin, T., Loubes, J.-M., and Del Barrio, E. Achieving robustness in classification using optimal transport with hinge regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 505\u2013514, 2021.   \nSeshia, S. A., Desai, A., Dreossi, T., Fremont, D. J., Ghosh, S., Kim, E., Shivakumar, S., VazquezChanlatte, M., and Yue, X. Formal specification for deep neural networks. In International Symposium on Automated Technology for Verification and Analysis, pp. 20\u201334. Springer, 2018.   \nShi, Z., Jin, Q., Kolter, J. Z., Jana, S., Hsieh, C.-J., and Zhang, H. Formal verification for neural networks with general nonlinearities via branch-and-bound. 2nd Workshop on Formal Verification of Machine Learning (WFVML 2023), 2023.   \nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R. Intriguing properties of neural networks. In Bengio, Y. and LeCun, Y. (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6199.   \nTjeng, V., Xiao, K. Y., and Tedrake, R. Evaluating robustness of neural networks with mixed integer programming. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id $\\equiv$ HyGIdiRqtm.   \nTramer, F. Detecting adversarial examples is (nearly) as hard as classifying them. In International Conference on Machine Learning, pp. 21692\u201321702. PMLR, 2022.   \nVirmaux, A. and Scaman, K. Lipschitz regularity of deep neural networks: analysis and efficient estimation. Advances in Neural Information Processing Systems, 31, 2018.   \nWang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q. Improving adversarial robustness requires revisiting misclassified examples. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id $\\cdot$ rklOg6EFwS.   \nWang, Z., Pang, T., Du, C., Lin, M., Liu, W., and Yan, S. Better diffusion models further improve adversarial training. In International Conference on Machine Learning (ICML), 2023.   \nWeng, T.-W. Proven: Verifying robustness of neural networks with a probabilistic approach - powerpoint presentation. https://icml.cc/media/Slides/icml/2019/grandball(11-11-00) -11-12-15-4739-proven_verifyi.pdf, 2019. (Accessed on 05/23/2023).   \nWeng, T.-W., Zhang, H., Chen, P.-Y., Yi, J., Su, D., Gao, Y., Hsieh, C.-J., and Daniel, L. Evaluating the robustness of neural networks: An extreme value theory approach. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id $\\cdot$ BkUHlMZ0b.   \nWu, D., Xia, S.-T., and Wang, Y. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33:2958\u20132969, 2020.   \nXu, K., Shi, Z., Zhang, H., Wang, Y., Chang, K.-W., Huang, M., Kailkhura, B., Lin, X., and Hsieh, C.-J. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020.   \nXu, K., Zhang, H., Wang, S., Wang, Y., Jana, S., Lin, X., and Hsieh, C.-J. Fast and Complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=nVZtXBI6LNn.   \nXu, W., Evans, D., and Qi, Y. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017.   \nXu, Y., Sun, Y., Goldblum, M., Goldstein, T., and Huang, F. Exploring and exploiting decision boundary dynamics for adversarial robustness. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\cdot$ aRTKuscKByJ.   \nYang, J., Zhou, K., Li, Y., and Liu, Z. Generalized out-of-distribution detection: A survey. arxiv. arXiv preprint arXiv:2110.11334, 2021.   \nZhang, H., Weng, T.-W., Chen, P.-Y., Hsieh, C.-J., and Daniel, L. Efficient neural network robustness certification with general activation functions. Advances in Neural Information Processing Systems, 31:4939\u20134948, 2018. URL https://arxiv.org/pdf/1811.00866.pdf.   \nZhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., and Jordan, M. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pp. 7472\u20137482. PMLR, 2019.   \nZhang, H., Wang, S., Xu, K., Wang, Y., Jana, S., Hsieh, C.-J., and Kolter, Z. A branch and bound framework for stronger adversarial attacks of ReLU networks. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 26591\u201326604, 2022.   \nZhang, J., Zhu, J., Niu, G., Han, B., Sugiyama, M., and Kankanhalli, M. Geometry-aware instancereweighted adversarial training. arXiv preprint arXiv:2010.01736, 2020.   \nZhong, Z., Tian, Y., and Ray, B. Understanding local robustness of deep neural networks under natural variations. In International Conference on Fundamental Approaches to Software Engineering, pp. 313\u2013337. Springer, Cham, 2021.   \nZhu, F., Cheng, Z., Zhang, X.-Y., and Liu, C.-L. Openmix: Exploring outlier samples for misclassification detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12074\u201312083, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 1. If a model is margin-consistent, then for any robustness threshold \u03f5, there exists a threshold \u03bb for the logit margin $d_{o u t}$ that perfectly separates non-robust samples and robust samples. Conversely, if for any robustness threshold \u03f5, $d_{o u t}$ admits a threshold $\\lambda$ that perfectly separates non-robust samples from robust samples, then the model is margin-consistent. ", "page_idx": 15}, {"type": "text", "text": "Proof. Formally, for a finite sample $S$ and nonegative values $\\epsilon\\geq0,\\lambda\\geq0$ , we define: ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{\\epsilon}^{S}:=\\{x\\in S:d_{i n}(\\mathbf{x})\\leq\\epsilon\\}\\quad\\mathrm{and}\\quad B_{\\lambda}^{S}:=\\{x\\in S:d_{o u t}(\\mathbf{x})\\leq\\lambda\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We say that $d_{o u t}$ perfectly separates non-robust samples from robust samples if for any finite sample $S\\subseteq\\mathcal{X}$ and every $\\epsilon\\geq0$ there exists $\\lambda\\geq0$ such that $A_{\\epsilon}^{S}=B_{\\lambda}^{S}$ . ", "page_idx": 15}, {"type": "text", "text": "Necessity: We proceed by contraposition and assume that the model is not margin-consistent, i.e., there exist two samples $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ such that $d_{o u t}(\\mathbf{x}_{1})\\leq d_{o u t}(\\mathbf{x}_{2})$ and $d_{i n}(\\mathbf{x}_{1})>d_{i n}(\\mathbf{x}_{2})$ . By taking $S=\\{\\mathbf{x}_{1},\\mathbf{x}_{2}\\}$ and $\\epsilon=d_{i n}(\\mathbf{x}_{2})$ we have that $A_{\\epsilon}^{S}=\\{\\mathbf{x}_{2}\\}$ . However for any $\\lambda\\geq0$ , if $\\mathbf{\\dot{x}}_{2}\\in\\dot{B}_{\\lambda}^{S}$ , then $d_{o u t}(\\mathbf{x}_{1})\\leq d_{o u t}(\\mathbf{x}_{2})\\leq\\lambda$ and so $\\mathbf{x}_{1}\\in B_{\\lambda}^{S}$ . Therefore $d_{o u t}$ does not perfectly separates non-robust samples from robust samples. ", "page_idx": 15}, {"type": "text", "text": "Sufficiency: Let\u2019s assume the model is margin-consistent. Let $S$ be a finite sample and consider a threshold $\\epsilon$ . Let $\\mathbf{x}_{\\mathrm{0}}$ be an element of the finite set $A_{\\epsilon}^{S}$ such that $d_{i n}(\\mathbf{x}_{0})=\\operatorname*{max}\\{d_{i n}(x):x\\in A_{\\epsilon}^{S}\\}$ and let $\\lambda=d_{o u t}(\\mathbf{x}_{0})$ . Since the model is margin-consistent, then for every $\\mathbf{x}\\in S$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nx\\in A_{\\epsilon}^{S}\\Leftrightarrow d_{i n}(\\mathbf{x})\\leq\\epsilon\\Leftrightarrow d_{i n}(\\mathbf{x})\\leq d_{i n}(\\mathbf{x}_{0})\\Leftrightarrow d_{o u t}(\\mathbf{x})\\leq d_{o u t}(\\mathbf{x}_{0})\\Leftrightarrow d_{o u t}(\\mathbf{x})\\leq\\lambda\\Leftrightarrow x\\in B_{\\lambda}^{S}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This means we have $A_{\\epsilon}^{S}=B_{\\lambda_{0}}^{S}$ , which shows that $d_{o u t}$ perfectly separates non-robust samples from robust samples. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Our formulation of perfect separation using finite samples is not fundamentally necessary, however it avoids dealing with the intricacy of the continuum. ", "page_idx": 15}, {"type": "text", "text": "B Detection Performance with Different Values of $\\epsilon$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present in Fig. 7 the performance of the detection for various values of the robustness threshold. We can see that the high margin consistency allows the logit margin to be a good proxy for detection at various thresholds. Note that below $\\epsilon=2/255$ and beyond $\\epsilon={16}/{255}$ , the ratio of vulnerable points to non-vulnerable points becomes too imbalanced, with little to no positive instances beyond $\\bar{\\epsilon}=32/255$ . ", "page_idx": 15}, {"type": "text", "text": "C Equidistance Assumption of the Linear Classifiers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Eq. 3 in Sec. 2.1, we show that we can approximate the exact feature margin by the logit margin when the classifiers $\\mathbf{w}_{k}$ are equidistant, i.e. $\\|\\mathbf{w}_{i}-\\mathbf{w}_{j}\\|=C$ whenever $i\\neq j$ . The results in Table 3 show that using the logit margin instead of the exact minimum feature margin has a negligible effect on the results. However, computing the exact feature margin requires computing the minimum over $K-1$ pairs of scaled logit differences. The approximation provided by the logit margin thereby circumvents the computational overhead of the minimum search, which can take a second instead of just microseconds for inference. This difference can add up to hours at scale, offering scalability when dealing with a large number of classes. We additionally provide boxplots for the $K(K-1)/2$ distances between pairs of classifiers for each model (45 for CIFAR10 and 4950 for CIFAR100) in Fig. 8 and 9. ", "page_idx": 15}, {"type": "text", "text": "D Logit Margins of Adversarial Examples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Adversarial examples are perturbed samples that are close to the decision boundary. Therefore, we would expect these samples to have a very small logit margin for strongly margin-consistent models. ", "page_idx": 15}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/132c40798717afb37e06f1bf29b85e7c01f0e37c996fa5fc3581bb466f160d77.jpg", "img_caption": ["(b) RA11 (Rade & Moosavi-Dezfooli, 2021) on CIFAR100 "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "", "img_caption": ["Figure 7: Variation of AUROC score for different threshold values $\\epsilon$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Here, we study the adversarial examples we used to estimate the input margin. In Table 4, we present the $99^{\\mathrm{th}}$ percentile of logit margins of adversarial examples and the detection threshold selected to obtain $95\\%$ True Positive Rate. We can observe that the values of the adversarial logit margins are significantly smaller than the detection threshold, so they would be detected as non-robust \u2013 just like clean samples that lie close to the decision boundary. ", "page_idx": 16}, {"type": "text", "text": "E Results on ImageNet and $\\ell_{2}$ -robust models on CIFAR10 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 5 shows the results with $\\ell_{\\infty}$ -robust models on ImageNet (Deng et al., 2009), a larger dataset, and results on $\\ell_{2}$ -robust models (only available on CIFAR10 in Robustbench). The results extend well in both situations. ", "page_idx": 16}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/41d61882f4d88844445a2ce39d0d9892d18514428f1b3d758e6bb296a4f8bbcb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 8: Equidistance of classifiers for CIFAR10 models. The boxplot reports the distances\u2019 minimum value, lower quartile (Q1), median, upper quartile (Q3), and maximum value. ", "page_idx": 17}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/e2742f694241276f72b8b549b38be30192cff41183afd04be209e168c7659267.jpg", "img_caption": ["Figure 9: Equidistance of classifiers for CIFAR100 models. The boxplot reports the distances\u2019 minimum value, lower quartile (Q1), median, upper quartile (Q3), and maximum value. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "XHCYZNmqnv/tmp/28022391bb794c5aca9f67d3a2c262f9b42ff5dbf147294eb521012c90340e5b.jpg", "table_caption": ["Algorithm 1 Sample Efficient Robustness Estimation with Margin Consistency "], "table_footnote": ["Table 3: Correlations between the input margin and the logit margin (Lm) or the exact Feature margin $(\\mathrm{Fm})$ and detection scores on CIFAR10 ( $\\ell_{\\infty}$ , $\\epsilon=8/255)$ . "], "page_idx": 18}, {"type": "text", "text": "F Additional Applications ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Sample Efficient Robust Accuracy Estimation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Margin consistency enables empirical robustness evaluation over an arbitrarily large test set by only attacking a small subset of test samples. For a robustness evaluation at threshold $\\epsilon$ (e.g., $\\epsilon=8/255$ in $\\ell_{\\infty}$ norm on CIFAR10 and CIFAR100), we randomly sample a small subset of the test set and determine the threshold $\\lambda$ for the logit margin that best corresponds to $\\epsilon$ . We propose to use this threshold $\\lambda$ to estimate the standard robust accuracy by evaluating the proportion of test samples which are correctly classified and whose logit margin is above $\\lambda$ (see Algorithm 1). A naive way to set ", "page_idx": 18}, {"type": "text", "text": "1: Input: Test Dataset $(X,Y)\\in(\\mathcal{X}\\times\\mathcal{Y})^{N}$ , Robustness threshold $\\epsilon>0$ , Subset size $n\\ll N$ .   \n2: Output: Robust Accuracy Estimation $A_{r}$   \n3: - Select uniformly at random a subset $X_{n}$ of $n$ samples from $X$ .   \n4: - Compute the estimations of the input margins on $X_{n}$ , $D_{n}=\\{\\hat{d}_{i n}(\\mathbf{x}):\\mathbf{x}\\in X_{s}\\}$   \n5: - Create ground truth labels for vulnerability at threshold $\\epsilon$ i.e. $\\mathbb{1}_{[\\hat{d}_{i n}(\\mathbf{x})\\leq\\epsilon]}(\\mathbf{x})$ , for $\\mathbf{x}\\in X_{s}$ .   \n6: - Determine the threshold $\\lambda$ of $d_{o u t}$ that gives best approximation of robust accuracy on $X_{s}$ .   \n7: $-\\,{\\mathcal{A}}_{r}=|\\{\\mathbf{x}\\in X:\\,d_{o u t}(\\mathbf{x})>\\lambda$ and $\\hat{y}(\\mathbf x)=y\\}|/N$ ", "page_idx": 18}, {"type": "text", "text": "the threshold $\\lambda$ at line 6 of Algorithm 1 would be to set it to the detection threshold at $\\alpha=95\\%$ TPR or $\\alpha=90\\%$ TPR, but the logit margin threshold could vary from one model to another; therefore a better way is to select it by tuning over values $\\alpha\\,\\geq\\,0.80$ that gives the best approximation of the robust accuracy in terms of the absolute error on the small subset $X_{s}$ . The same idea allows estimating a model\u2019s vulnerability over a large dataset without the labels. ", "page_idx": 18}, {"type": "text", "text": "We show that this leads to an accurate estimation of the robust accuracy of the investigated models evaluated with over 10, 000 by attacking only a random subset of size 500. Fig. 10 shows the absolute error of the estimation obtained using 500 samples. As expected, the estimation over the two weakly margin consistent models is not accurate while having a relatively small absolute difference on the strongly margin consistent models. ", "page_idx": 18}, {"type": "text", "text": "F.2 Robustness Bias Analysis using the Logit Margin ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Robust models often display robustness bias, namely a disparity of robustness across classes. Interestingly, in a strongly margin-consistent model (see Fig. 11, top row), we show that these discrepancies across classes with respect to input margin are reflected in the logit margin. Additionally, the margin ", "page_idx": 18}, {"type": "table", "img_path": "XHCYZNmqnv/tmp/b78fbfd9bdf27df73ae36c29ba95c68d8edb6e0f8bb40b67aa78ff8b75708a11.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "XHCYZNmqnv/tmp/d128c3c7efb2294e63630d39ad688501c6f969d762aad084e2562d95bab3e403.jpg", "table_caption": ["Table 4: Logits of the adversarial examples are very small compared to the logit margin for the logit margin threshold for the detection at epsilon $\\scriptstyle1=8/255$ . See Table 1 for the specific references on the model ID. "], "table_footnote": ["Table 5: Correlations between the input margin and the logit margin and detection scores on CIFAR10 $\\ell_{2}$ $?_{2},\\epsilon=0.5)$ ) and ImageNet $\\mathcal{l}_{\\infty}$ , $\\epsilon=4/255$ , 1000 samples). See Table 1 for the specific references on the model ID. "], "page_idx": 19}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/b565b76e5edeac35d5482cd0fc176b24a0b5a822c43a4a4686e383dd8ccab4f9.jpg", "img_caption": ["Figure 10: Estimations of the robust accuracy reported by Robustbench using logit margins with only 500 samples are quite accurate both on CIFAR10 and CIFAR100 for strongly margin-consistent models. The numbers indicate the absolute difference between the two values, averaged over ten subsets. See Table 1 for the specific references on the model ID. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "XHCYZNmqnv/tmp/c928474a0d9b493995f63d3071b2858342a253df7662a87a4fc92bedcdc52a73.jpg", "img_caption": ["Figure 11: Robustness bias analysis with the input margin (first column) vs the logit margin (second column) and correlation scores per class (third column). We have a margin-consistent model on the top row, and on the bottom row, we have a weakly margin-consistent model. On the first column (resp. second column), each boxplot represents the distribution of the input margin (resp. logit margin) for the corresponding CIFAR10 class. The dotted blue line indicates the threshold on the first two columns and is the correlation score computed over all classes. The threshold $\\lambda$ in the second column is the logit margin threshold at $95\\%$ TPR for detection at $\\epsilon\\,=\\,8/255$ . The robustness bias of the strongly margin consistent model can be detected using the logit margin, unlike the weakly margin consistent model. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "consistency remains strong for each class. However, for a weakly margin-consistent model (Fig. 11, bottom row), significant disparities exist between the correlations across classes, making using logit margin as a proxy for input margin problematic. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have presented clearly the main claims in the abstract and the introduction; in particular, the end of the introduction details the list of the contributions and the main results. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have created a specific section \"Limitations\". ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper\u2019s theorem, formulas, and proof are numbered and cross-referenced. We have provided the proof in the supplemental material, and we have provided a short proof sketch to provide intuition in the main paper. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All our experiments involve CIFAR10 and CIFAR100 datasets, and the majority of models were loaded from the RobustBench model zoo (Croce et al., 2021) and a few models are ResNet-18 (He et al., 2016) models that we trained on CIFAR10 with Standard Adversarial Training (Madry et al., 2018), TRADES (Zhang et al., 2019), Logit Pairing (ALP and CLP, Kannan et al. (2018)), and MART (Wang et al., 2019). The code is available at: https://github.com/ngnawejonas/margin-consistency. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All the experiments are based on standard datasets CIFAR10, CIFAR100 and models mainly provided on RobustBench model zoo (Croce et al., 2021), https:// github.com/RobustBench/robustbench. The code is available at: https://github. com/ngnawejonas/margin-consistency. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: For the first part of the experiments, the parameters for estimating the input margins are explained. In the second part, on learning a pseudo-margin, we specified the architecture, dataset size, and training and validation splits and gave the reference for the choices in the training protocol. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The article details the computing resources used for the experiments. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss potential positive and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This work aims at detecting vulnerable unseen samples for robust trained models. There is no obivous negative societal impact per se in this task, but as for any predictive model, inexact predictions may impact downstream decisions and thus, model quality must be taken into account to control such situations. ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The original papers that produced code package and the datasets used for evaluation are carefully referenced (https://github.com/RobustBench/robustbench). ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The code produced to evaluate our approach is documented and the information about training and limitations are discussed in the paper. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}]