{"importance": "This paper is crucial for researchers working with LLMs in high-stakes applications like healthcare.  It **highlights a critical limitation of current LLMs\u2014their inability to effectively seek information in interactive settings**\u2014and proposes a novel benchmark (MEDIQ) to evaluate this. This opens up new avenues for improving LLM reliability and developing more robust, interactive AI systems.", "summary": "MEDIQ benchmark revolutionizes LLM evaluation by shifting from static to interactive clinical reasoning, revealing LLMs' struggles with proactive information-seeking and highlighting the importance of question-asking for reliable AI.", "takeaways": ["Current LLMs struggle with proactive information-seeking in interactive settings, impacting reliability.", "MEDIQ benchmark simulates realistic clinical interactions to evaluate question-asking abilities.", "Strategies like abstention and rationale generation improve LLM performance in interactive scenarios, but a gap with perfect performance remains."], "tldr": "Most LLM benchmarks use a static, single-turn format, ignoring real-world interactive scenarios. This is problematic for high-stakes applications, where incomplete information is common and follow-up questions are crucial for reliable decision-making.  The unreliability stems from LLMs' training to answer any question, even without sufficient knowledge.\nTo address this, the paper introduces MEDIQ, an interactive benchmark that simulates clinical interactions. It features a 'Patient System' providing partial information and an 'Expert System' (LLM) that asks clarifying questions before making a diagnosis.  Results show that directly prompting LLMs to ask questions decreases performance, highlighting the difficulty of adapting LLMs to interactive settings. However, using abstention strategies to manage uncertainty improves accuracy, demonstrating the importance of developing LLMs that can proactively seek more information.", "affiliation": "University of Washington", "categories": {"main_category": "Natural Language Processing", "sub_category": "Question Answering"}, "podcast_path": "W4pIBQ7bAI/podcast.wav"}