[{"figure_path": "W4pIBQ7bAI/figures/figures_1_1.jpg", "caption": "Figure 1: Information Seeking Task. In standard medical QA tasks (left), all necessary information is given to the assistant model at the same time. When given partial information, current LLMs only provides general responses (middle). In a more realistic scenario (right), the presentation of patient information relies on proactive elicitation from the doctor; our proposed MEDIQ framework operationalizes this scenario.", "description": "This figure compares three scenarios of medical question answering. The first is a standard setup where all necessary information is given at once. The second scenario shows a realistic scenario where LLMs are given partial information at the start and only provide general responses. Finally, the third scenario illustrates the desired information-seeking behavior where the model proactively asks questions to gather more information before providing a response. This last scenario is the one that MEDIQ is trying to achieve.", "section": "1 Introduction"}, {"figure_path": "W4pIBQ7bAI/figures/figures_2_1.jpg", "caption": "Figure 2: The MEDIQ Benchmark. MEDIQ operationalizes a more realistic dynamic clinical interaction between a Patient system and an Expert system to evaluate info-seeking and question-asking.", "description": "This figure illustrates the MEDIQ benchmark's framework, which simulates a realistic clinical interaction.  The Patient System, possessing the complete patient record, responds to questions from the Expert System.  The Expert System, starting with incomplete information, determines if it has sufficient data to answer the medical question; if not, it asks follow-up questions to the Patient System. This process continues iteratively until the Expert System has enough information to answer the question confidently. The figure visually represents the information flow and decision-making process within the MEDIQ framework.", "section": "MEDIQ: Dynamic Medical Consultation Framework Overview"}, {"figure_path": "W4pIBQ7bAI/figures/figures_3_1.jpg", "caption": "Figure 3: Expert system information flow breakdown.", "description": "This figure shows the five steps involved in the Expert system: initial assessment, abstention, question generation, information integration, and decision making.  Each step is modular, making the system easily modifiable. The Abstention module is key, determining whether the system should answer the question or ask a follow-up question.  This decision is based on confidence level. The system proceeds to Question Generation if more information is needed and to Decision Making if confident.", "section": "2.2.1 Expert System Breakdown"}, {"figure_path": "W4pIBQ7bAI/figures/figures_6_1.jpg", "caption": "Figure 1: Information Seeking Task. In standard medical QA tasks (left), all necessary information is given to the assistant model at the same time. When given partial information, current LLMs only provides general responses (middle). In a more realistic scenario (right), the presentation of patient information relies on proactive elicitation from the doctor; our proposed MEDIQ framework operationalizes this scenario.", "description": "This figure illustrates the difference between standard and realistic medical question-answering (QA) tasks.  Standard QA provides all necessary information upfront, while realistic QA mirrors real-world clinical scenarios where information is incomplete initially.  The figure emphasizes that effective clinical reasoning often involves a doctor proactively seeking additional details from the patient through follow-up questions, a process that current LLMs struggle to emulate. The MEDIQ framework aims to address this challenge.", "section": "1 Introduction"}, {"figure_path": "W4pIBQ7bAI/figures/figures_7_1.jpg", "caption": "Figure 10: Performance of abstain strategies on iMEDQA. Each line is an abstain strategy with increasing confidence thresholds. Darker colors are results with rationale generation (RG); dashed lines are with self-consistency (SC). The BEST system (Scale+RG+SC,) significantly outperforms the BASIC baseline (\u25b2).", "description": "This figure shows the performance of different abstention strategies on the iMEDQA dataset.  The x-axis represents the average conversation length (number of questions asked), and the y-axis represents the accuracy.  Different lines represent different strategies (BASIC, Average, Numerical, Binary, Scale) with and without self-consistency (SC) and rationale generation (RG). The figure demonstrates that incorporating rationale generation and self-consistency leads to improved accuracy.  The best performing strategy (Scale+RG+SC) significantly outperforms the baseline (BASIC).", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/figures/figures_8_1.jpg", "caption": "Figure 8: Effect of the abstention module on Expert system performance (efficiency & accuracy). (a) Accuracy over the number questions when the abstention response is not provided to the question generator context (NC) to isolate the effect of confidence thresholds on model performance. Each line is a different abstain method with increasing confidence thresholds at each point. Accuracy increases with # Qs as threshold increases. (b) Expert confidence estimation throughout the interaction with and without rationale generation (RG), averaged over self-consistency levels 1, 3, and 5. The expected calibration error (ECE) is shown in the legend. RG leads to more conservative and accurate confidence estimates. (c) Accuracy of Expert system variants with abstention response available (solid marker) vs. not available (hollow marker) to the question generator module. Including abstention response in question generator context improves accuracy, and is amplified with rationale generation (RG) and self-consistency (SC).", "description": "This figure analyzes how the abstention module in the MEDIQ Expert system affects its performance.  Panel (a) shows the relationship between accuracy and the number of questions asked, varying the confidence threshold and whether the abstention response was included in the question generation process.  Panel (b) illustrates how rationale generation impacts the accuracy of confidence estimates over the course of the interaction. Panel (c) demonstrates how including the abstention response in the question generation process improves the model's accuracy, especially when combined with rationale generation and self-consistency.", "section": "5.2 How does abstention improve Expert system performance?"}, {"figure_path": "W4pIBQ7bAI/figures/figures_24_1.jpg", "caption": "Figure 1: Information Seeking Task. In standard medical QA tasks (left), all necessary information is given to the assistant model at the same time. When given partial information, current LLMs only provides general responses (middle). In a more realistic scenario (right), the presentation of patient information relies on proactive elicitation from the doctor; our proposed MEDIQ framework operationalizes this scenario.", "description": "This figure compares three scenarios of medical question answering.  The first shows a standard setup where all information is provided at once. The second shows a realistic scenario where only partial information is initially available, and current LLMs fail to adequately seek additional information. The third, representing the proposed MEDIQ framework, depicts the ideal scenario where the LLM proactively seeks further information through a conversational interaction.", "section": "1 Introduction"}, {"figure_path": "W4pIBQ7bAI/figures/figures_26_1.jpg", "caption": "Figure 10: Performance of abstain strategies on iMEDQA. Each line is an abstain strategy with increasing confidence thresholds. Darker colors are results with rationale generation (RG); dashed lines are with self-consistency (SC). The BEST system (Scale+RG+SC,) significantly outperforms the BASIC baseline (\u25b2).", "description": "This figure shows the performance of different abstention strategies in the MEDIQ benchmark on the iMEDQA dataset.  The x-axis represents the average conversation length (number of questions asked), and the y-axis represents the accuracy of the model. Different lines represent different abstention strategies (e.g., using a numerical confidence score, a Likert scale, or a binary decision), with and without rationale generation and self-consistency. The results demonstrate that incorporating rationale generation and self-consistency significantly improves accuracy, especially when using a Likert scale for confidence assessment.  The best-performing strategy (Scale+RG+SC) substantially surpasses the basic approach.", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/figures/figures_28_1.jpg", "caption": "Figure 11: Impact of interactive system on diagnostic accuracy across medical specialties and demographics. Specialties benefiting the most (left) and least (middle) from interaction; improvement by difficulty (right).", "description": "This figure shows the impact of interactive information-seeking on diagnostic accuracy across various medical specialties and difficulty levels of questions.  It highlights that interactive systems improve accuracy in some specialties, like ophthalmology,  but not all. The improvement is also more significant for more complex, clinically focused questions.", "section": "4 Results"}, {"figure_path": "W4pIBQ7bAI/figures/figures_28_2.jpg", "caption": "Figure 1: Information Seeking Task. In standard medical QA tasks (left), all necessary information is given to the assistant model at the same time. When given partial information, current LLMs only provides general responses (middle). In a more realistic scenario (right), the presentation of patient information relies on proactive elicitation from the doctor; our proposed MEDIQ framework operationalizes this scenario.", "description": "This figure compares three different scenarios of medical question answering. The standard setup (left) provides all necessary information at once, while the realistic setup (middle and right) only provides partial information initially.  Current LLMs struggle in the realistic scenario, giving only general responses (middle), while the desired behavior (right) would involve a doctor actively eliciting the necessary information through follow-up questions, which is the behavior that the MEDIQ benchmark aims to evaluate.", "section": "1 Introduction"}]