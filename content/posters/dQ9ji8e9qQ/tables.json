[{"figure_path": "dQ9ji8e9qQ/tables/tables_7_1.jpg", "caption": "Table 1: Evaluation of the effectiveness of ARE editing for attacking large language models via the refusal rates of various LLMs under diverse attack methods. The best attack results are shown in bold. A lower refusal rate is indicative of enhanced attack effectiveness, given that the sum of the attack success rate and the refusal rate equals 1.", "description": "This table presents the results of an experiment evaluating the effectiveness of Adversarial Representation Engineering (ARE) in attacking large language models (LLMs).  It compares the refusal rates (percentage of times the LLM refused to generate a response) of several different attack methods across three different LLMs: Llama-2-7B-Chat, Vicuna-7B, and Guanaco-7B.  The methods are categorized as template-based, optimization-based, and editing-based.  A lower refusal rate indicates a more effective attack because it implies a higher success rate in eliciting harmful responses. The ARE method is shown to have significantly lower refusal rates than other methods, demonstrating its effectiveness as an attack method.", "section": "5.1 Alignment: To Generate (harmful responses) or not to generate"}, {"figure_path": "dQ9ji8e9qQ/tables/tables_7_2.jpg", "caption": "Table 2: Evaluating the effectiveness of ARE defense method through comparative analysis on refusal rate of different jailbreak attack methods on aligned large models fine-tuned with our ARE defense approach. A higher refusal rate indicates better defense effectiveness.", "description": "This table presents a comparison of the effectiveness of different defense methods against various jailbreak attacks on two large language models: Llama-2-7B-Chat and Vicuna-7B.  The defense methods include: No Defense (baseline), Self-Reminder [52], In-Context Defense (ICD) [50], and the proposed Adversarial Representation Engineering (ARE) method. The attacks used for evaluation are AutoDAN and GCG. The refusal rate, representing the percentage of times the model successfully resists the attack, is reported for each combination of model, defense method, and attack. A higher refusal rate indicates stronger defense capabilities.", "section": "5.1 Alignment: To Generate (harmful responses) or not to generate"}, {"figure_path": "dQ9ji8e9qQ/tables/tables_8_1.jpg", "caption": "Table 3: Evaluation of the effectiveness of ARE editing for encouraging and discouraging hallucination. The % Right Answer highlighted in red denotes the highest hallucination rate, and in blue denotes the lowest hallucination rate.", "description": "This table presents the results of experiments evaluating the effectiveness of the Adversarial Representation Engineering (ARE) framework in controlling the hallucination rate of Large Language Models (LLMs).  It compares ARE's performance against baseline methods (Self-Reminder, ITI, and a control with no perturbation) for both increasing and decreasing the rate of hallucinations.  The results are shown for two different LLMs (Llama-2-7B and Mistral-7B) and highlight the success of ARE in both increasing and decreasing hallucination rates.", "section": "5.2 Hallucination: To Hallucinate or Not to Hallucinate"}, {"figure_path": "dQ9ji8e9qQ/tables/tables_9_1.jpg", "caption": "Table 1: Evaluation of the effectiveness of ARE editing for attacking large language models via the refusal rates of various LLMs under diverse attack methods. The best attack results are shown in bold. A lower refusal rate is indicative of enhanced attack effectiveness, given that the sum of the attack success rate and the refusal rate equals 1.", "description": "This table presents the results of different attack methods on three large language models (LLMs): Llama-2-7B-Chat, Vicuna-7B, and Guanaco-7B.  The attacks are categorized into template-based, optimization-based, and editing-based approaches. The table shows the refusal rate for each method, indicating the percentage of times the model refused to generate a response deemed harmful.  A lower refusal rate indicates a more successful attack.  The ARE method's results are highlighted in bold, demonstrating its superior performance in comparison to other editing-based attacks.", "section": "5.1 Alignment: To Generate (harmful responses) or not to generate"}, {"figure_path": "dQ9ji8e9qQ/tables/tables_9_2.jpg", "caption": "Table 5: Comparing the quality and diversity of text generated by different editing approaches on Llama2-7B.", "description": "This table compares the quality and diversity of text generated by different editing approaches on the Llama2-7B model.  It assesses the naturalness and usefulness of the generated texts by evaluating several metrics, including Self-BLEU (measuring text uniqueness), Repetition-4 (measuring 4-gram repetition), and Repetition-Sen (measuring sentence-level repetition). Lower scores generally indicate higher quality and diversity.  The results show that the ARE method achieves a better balance of quality and diversity compared to other approaches, demonstrating its effectiveness in preserving or even improving text generation quality during the editing process.", "section": "5.3 Text Generation Quality Issues"}, {"figure_path": "dQ9ji8e9qQ/tables/tables_14_1.jpg", "caption": "Table 6: Evaluation of the effectiveness of ARE editing for attacking large-scale models via the refusal rates (%) of various LLMs under diverse attack methods. The best defense results are shown in bold.", "description": "This table presents the results of evaluating the effectiveness of the proposed Adversarial Representation Engineering (ARE) method in attacking large language models (LLMs) by measuring refusal rates.  Three different LLMs (Llama-2-13B-Chat, Llama-2-70B-Chat, and Vicuna-13B) were tested using the ARE method and compared against a baseline method (Contrast Vector). Lower refusal rates indicate more successful attacks. The table demonstrates that ARE achieves significantly lower refusal rates than the baseline, indicating a more effective attack strategy.", "section": "5.1 Alignment: To Generate (harmful responses) or not to generate"}]