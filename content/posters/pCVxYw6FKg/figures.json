[{"figure_path": "pCVxYw6FKg/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Standard MLP. The hidden nodes (grey hatches) can be freely permuted, which induces permutation parameter symmetries. Black edges denote trainable parameters. (Middle) Our W-Asymmetric MLP, which fixes certain weights to be constant and untrainable (colored dashed lines) to break parameter symmetries. (Right) Our \u03c3-Asymmetric MLP, which uses our FiGLU nonlinearity involving a fixed matrix F (colored dashed lines) to break parameter symmetries.", "description": "This figure illustrates the three types of MLPs used in the paper: standard MLPs, W-Asymmetric MLPs, and \u03c3-Asymmetric MLPs.  Standard MLPs have hidden nodes that are fully interchangeable, leading to permutation symmetries.  W-Asymmetric MLPs break these symmetries by fixing some weights to be constant and non-trainable. \u03c3-Asymmetric MLPs use a different nonlinearity called FiGLU, which also helps to remove symmetries. The figure uses color-coding to distinguish trainable parameters (black) from fixed parameters (colored dashed lines).", "section": "Asymmetric Networks"}, {"figure_path": "pCVxYw6FKg/figures/figures_3_1.jpg", "caption": "Figure 2: Depiction of our W-Asymmetric approach to removing parameter symmetries. Entries with a black outline are untrained. Note that the W-Asym linear map has 2 nonzeros per row, the W-Asym convolution with fixed entries has 8 fixed entries for its single output channel, and the W-Asym convolution with fixed filters has a single input filter fixed. We often use a constant number of fixed entries per row or output channel in our experiments.", "description": "This figure illustrates the W-Asymmetric approach to reduce parameter space symmetries in neural networks. It shows how this method modifies standard and convolutional layers by fixing some of the weights (black outlined entries). The linear layer has 2 non-zero entries per row, the fixed-entry convolutional layer has 8 fixed entries, and the fixed-filter convolutional layer has one fixed input filter.  The number of fixed entries is a hyperparameter that can be adjusted during experiments.", "section": "4.1 Computation Graph Approach (W-Asymmetric Networks)"}, {"figure_path": "pCVxYw6FKg/figures/figures_5_1.jpg", "caption": "Figure 3: Linear mode connectivity: test loss curves along linear interpolations between trained networks. (Left) MLP on MNIST. (Middle) ResNet with 8\u00d7 width on CIFAR-10. (Right) GNN on ogbn-arXiv. W-Asymmetric networks interpolate the best, followed by networks aligned with Git-Rebasin, then \u03c3-Asymmetric networks, and finally standard networks.", "description": "This figure displays test loss curves for linear interpolations between two trained neural networks.  The interpolation is done by linearly combining the weights of the two networks. Three different network architectures are shown: MLP (Multilayer Perceptron) on the MNIST dataset, ResNet (Residual Network) with 8x width on CIFAR-10, and GNN (Graph Neural Network) on the ogbn-arXiv dataset.  The results demonstrate that W-Asymmetric networks show the best interpolation performance (lowest loss during interpolation), followed by networks whose weights have been aligned using the Git-Rebasin method.  \u03c3-Asymmetric networks perform better than standard networks, but worse than the others.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/figures/figures_7_1.jpg", "caption": "Figure 4: Bayesian neural network training loss over time for depth 8 MLPs on MNIST (left), ResNet110 on CIFAR-10 (middle), and ResNet20 with BatchNorm on CIFAR-100 (right). W-Asymmetric networks train more quickly, and achieve lower training loss.", "description": "This figure shows training loss curves for standard and W-Asymmetric Bayesian neural networks across three different datasets and architectures: MNIST with depth 8 MLPs, CIFAR-10 with ResNet110, and CIFAR-100 with ResNet20 with Batch Normalization.  The results demonstrate that W-Asymmetric networks consistently achieve lower training loss and faster convergence than their standard counterparts.", "section": "5.2 Bayesian Neural Networks"}, {"figure_path": "pCVxYw6FKg/figures/figures_8_1.jpg", "caption": "Figure 3: Linear mode connectivity: test loss curves along linear interpolations between trained networks. (Left) MLP on MNIST. (Middle) ResNet with 8\u00d7 width on CIFAR-10. (Right) GNN on ogbn-arXiv. W-Asymmetric networks interpolate the best, followed by networks aligned with Git-Rebasin, then \u03c3-Asymmetric networks, and finally standard networks.", "description": "This figure shows the test loss curves for linear interpolation between two independently trained networks for three different network architectures (MLP, ResNet, and GNN) and datasets (MNIST, CIFAR-10, and ogbn-arXiv).  The x-axis represents the interpolation coefficient, ranging from 0 (first network) to 1 (second network). The y-axis shows the test loss.  The figure demonstrates that W-Asymmetric networks exhibit the best linear mode connectivity (smooth interpolation with low loss), followed by networks aligned using the Git-Rebasin method, then \u03c3-Asymmetric networks, and lastly standard networks. This highlights the improved loss landscape properties of Asymmetric networks.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/figures/figures_15_1.jpg", "caption": "Figure 6: Epochs until reaching 70% training accuracy on CIFAR-10 when varying the hyperparameters of W-Asymmetric ResNets; we vary number of fixed entries nfix and standard deviation \u03ba of the fixed entries F. Entries further to the bottom and right are more asymmetric, while the entries further to the top and left are more like standard networks (the leftmost column are all standard networks). We see that more-asymmetric networks need more time to train.", "description": "This figure shows the effect of varying hyperparameters on training time for W-Asymmetric ResNets on CIFAR-10. The hyperparameters are the number of fixed entries (nfix) and the standard deviation (\u03ba) of the fixed entries.  As the number of fixed entries increases and the standard deviation increases, the networks become more asymmetric and require significantly more epochs to reach 70% training accuracy. The heatmap visually represents this relationship.", "section": "A Additional Observations on Asymmetric Networks"}]