[{"type": "text", "text": "Contextual Decision-Making with Knapsacks Beyond the Worst Case ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhaohua Chen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rui Ai ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Computer Science Peking University Haidian, Beijing, China chenzhaohua@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "IDSS&LIDS Massachusetts Institute of Technology Cambridge, MA 02139, USA ruiai@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Mingwei Yang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuqi Pan ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dept. of Management Science and Engineering Stanford University Stanford, CA 94305, USA mwyang@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138, USA yuqipan@g.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Chang Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaotie Deng ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dept. of Computer Science Northwestern University Evanston, IL 60208, USA wc@u.northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science   \nInstitute for Artificial Intelligence   \nPeking University   \nHaidian, Beijing, China   \nxiaotie@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the framework of a dynamic decision-making scenario with resource constraints. In this framework, an agent, whose target is to maximize the total reward under the initial inventory, selects an action in each round upon observing a random request, leading to a reward and resource consumptions that are further associated with an unknown random external factor. While previous research has already established an $\\widetilde{O}(\\sqrt{T})$ worst-case regret for this problem, this work offers two results that go beyond the worst-case perspective: one for the worst-case gap between benchmarks and another for logarithmic regret rates. We first show that an $\\Omega({\\sqrt{T}})$ distance between the commonly used fluid benchmark and the online optimum is unavoidable when the former has a degenerate optimal solution. On the algorithmic side, we merge the re-solving heuristic with distribution estimation skills and propose an algorithm that achieves an $\\widetilde O(1)$ regret as long as the fuid LP has a unique and non-degenerate solution. Furthermore, we prove that our algorithm maintains a near-optimal $\\widetilde{O}(\\sqrt{T})$ regret even in the worst cases and extend these results to the setting where the request and external factor are continuous. Regarding information structure, our regret results are obtained under two feedback models, respectively, where the algorithm accesses the external factor at the end of each round and at the end of a round only when a non-null action is executed. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In online contextual decision-making problems with knapsack constraints (CDMK for short), an agent is required to make sequential decisions over a finite time horizon to maximize the accumulated reward under initial resource constraints [13, 14]. To be more specific, in each round $t=1,\\dots,T$ a request $\\theta_{t}$ and an external factor $\\gamma_{t}$ are independently generated from two distributions, and only $\\theta_{t}$ is revealed to the agent. Based on the request, the agent should irrevocably choose an action $a_{t}$ , which results in a reward $r(\\theta_{t},a_{t},\\gamma_{t})$ and a consumption vector $\\pmb{c}(\\theta_{t},a_{t},\\gamma_{t})$ of resources. The agent's target is to optimize the sum of rewards $\\textstyle\\sum_{t=1}^{T}r(\\theta_{t},a_{t},\\gamma_{t})$ before theresources are depleted. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The contextual decision-making with knapsacks problem presents two key challenges when compared to closely related problems (e.g., the network revenue management problem): (1) choices are made without observing the external factor, and (2) distributions of requests and external factors are unknown. However, the complexity of CDMK makes it a suitable mathematical abstraction for many real-life scenarios, and there are extensive application scenarios with this kind of information structure. We use the following examples as illustrations and motivation. ", "page_idx": 1}, {"type": "text", "text": "Example 1.1 (Supply chain management). In supply chain management, a factory needs to allocate among $T$ repositories consecutively and is constrained by the total inventory. Given the request $\\theta_{t}$ for each repository, the factory chooses the number of goods transported to it as action $a_{t}$ .However,the factory has randomized transportation costs for different locations and traffic conditions denoted by an external factor $\\gamma_{t}$ for the $t$ -th repository, which finally influences rewards. The factory needs to form an optimal scheme to allocate its goods under uncertainty to all these repositories. ", "page_idx": 1}, {"type": "text", "text": "Example 1.2 (Dynamic bidding in repeated auctions with budgets [8, 9]). In this circumstance, an advertiser acquires the value of the ad slot $\\theta_{t}$ at the start of each auction and chooses a bid $a_{t}$ accordingly. The agent's gain in this auction, as a consequence, is collaboratively determined by the value, the bid, and the highest competing bid $\\gamma_{t}$ , and has a form of $\\theta_{t}\\mathbb{1}(a_{t}>\\gamma_{t})$ . Additionally, the payment is $a_{t}\\mathbb{1}(a_{t}>\\gamma_{t})$ for the first-price auction and $\\gamma_{t}\\mathbb{1}(a_{t}>\\gamma_{t})$ for the second-price auction, respectively. It is to be noted that the highest competing bid is inaccessible to the agent before committing to the bid, as all advertisers bid simultaneously. Meanwhile, its distribution is decided by other advertisers, which is also unknown to the agent before the auctions. ", "page_idx": 1}, {"type": "text", "text": "The CDMK model can also capture other well-studied problems, including multi-secretary, online linear programming, online matching, etc., as discussed in Balseiro et al. [8]. Previous studies of the CDMK problem have shown that the worst-case regret is $\\widetilde{O}(\\sqrt{T})$ when the initial resources are linearly proportional to the horizon length $T$ [35, 23]. However, it is still unclear whether we can achieve a better regret guarantee for the CDMK problem beyond worst-case scenarios. In particular, can we design algorithms to obtain an $o(\\sqrt{T})$ regret only under mild assumptions that hold for almost all possible CDMK instances? Meanwhile, can these algorithms still obtain good regret guarantees even in the worst cases? At last, previous works would adopt specific benchmarks to measure the regret of algorithms, but how are these benchmarks close to the rewards that the optimal online algorithm can achieve? This work widely addresses these questions. ", "page_idx": 1}, {"type": "text", "text": "1.1  Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This work makes three main contributions, summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "The fluid optimum can be $\\Omega({\\sqrt{T}})$ away from the online optimum. Since the online optimum is hard to characterize, previous works always use an alternative benchmark to measure the performance of any online algorithm, and the fuid optimum (also known as the deterministic LP) is a common choice [23, 34]. However, we demonstrate that when the fuid benchmark has a unique and degenerate solution, then an $\\Omega({\\sqrt{T}})$ gap is unavoidable between these two optima (cf. Theorem 2.1). While Han et al. [23] has also provided a similar lower bound result for the related contextual bandits with knapsacks (CBwK) problem, their condition depends on the inseparability of the possible expected reward/consumption function set. In other words, their condition may not perform well when this feasible set is small. Furthermore, their condition is rather complicated to verify. In contrast, our condition only depends on the underlying problem instance and is concise and easy to check. The proof of our result extends the approach of Vera and Banerjee [37] to the CDMK problem. ", "page_idx": 1}, {"type": "text", "text": "An $\\widetilde O(1)$ regret via re-solving under mild assumptions with full/partial information feedback. Since an $\\widetilde{O}(\\sqrt{T})$ worst-case regret is already known [35], we investigate how well an online algorithm can perform beyond worst cases by applying the re-solving heuristic in conjunction with distribution estimation techniques, as given in Algorithm 1. This method has been considered in the problems of network revenue management (NRM) and bandits with knapsacks (BwK). (See Section 1.2 for a literature review.) However, to our knowledge, we are the first to extend this method to the CDMK problem, which poses new challenges as decisions should be made according to the request. To avoid worst cases, we explicitly suppose that the fuid problem has a unique and non-degenerate solution (cf. Assumption 3.1). This assumption is mild in three aspects: (1) it captures almost all CDMK problem instances, as slightly perturbing any LP can help it satisfy the unique optimality and non-degeneracy conditions; (2) it is less restrictive than the assumptions given in Sankararaman and Slivkins [31], which require that there are at most two resources; and (3) it is almost necessary for an $o(\\sqrt{T})$ regret bound to be established by Theorem 2.1, when using the fuid optimum as the benchmark. Under the assumption, our main results show that the re-solving heuristic reaches an $O(1)$ regret with full information (cf. Theorem 3.1) and an ${\\cal O}(\\log T)$ regret with partial information (cf. Theorem 4.1). To our knowledge, these are the first $\\widetilde O(1)$ regret results in the CDMK problem beyond the worst case with only mild assumptions. Importantly, unlike previous results, these regret bounds are also independent of the number of actions. ", "page_idx": 1}, {"type": "table", "img_path": "Dgt6sh2ruQ/tmp/3af373385d86dd3a0781d70dd607f8a1e7f41bb2320e451016dfd2bd8ac449a5.jpg", "table_caption": ["Table 1: A summary of our algorithmic results on Algorithm 1. Constants are omitted. "], "table_footnote": ["$u,v$ : the mass/density function of the context and the external factor. $\\alpha_{p\\in\\{u,v\\}}\\colon(\\beta+d)/(2\\beta+d)$ $p$ is a $d\\!.$ dimension distribution and $p\\in\\Sigma(\\beta,L)$ . (See Appendix A.) "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Within our results, the full information model assumes that the agent sees the external factor at the end of each round. In contrast, in the partial information model, the agent acquires the external factor only when a non-null action is adopted. In Example 1.1, if the factory can learn the road condition via map services, it then observes the external factor no matter its chosen action, reflecting the full information feedback. However, it can sometimes only observe transportation costs when it transports goods, resembling a non-null action. This is a case of partial information feedback. In the auction market illustrated in Example 1.2, agents might also face these two kinds of information models. In some situations, bidders can always view others' bids after the auction, while in other cases, only those who bid non-zero values can observe others\u2019 bids. Non-zero bidding here reflects a non-null action. Therefore, these two information models hold strong practical significance. ", "page_idx": 2}, {"type": "text", "text": "Other state-of-the-art results consider bandit information feedback, in which the agent only sees the reward and the consumption rather than the external factor. However, they explicitly assume a specific (e.g., linear) relationship between the conditional expected reward-consumption pair and the request [3, 31, 23, 35], whereas our results do not impose any underlying distribution structures, bypassing realizability issues [23]. On this side, our information model is comparable to those in existingwork. ", "page_idx": 2}, {"type": "text", "text": "A near-optimal regret even in worst cases with full/partial information feedback and an extension to continuous randomness. We further explore how well our Algorithm 1 performs even in worst-case scenarios. With full information feedback, we show that an $O({\\sqrt{T\\log T}})$ regret is achieved (cf. Theorem 5.1). This bound is asymptotically equal to the state-of-the-art with this information model [23, 35]. Even with partial information, we can still guarantee a universal $O({\\sqrt{T}}\\log T)$ regret (cf. Theorem 5.2), which is optimal up to a logarithmic factor. These results demonstrate the applicability and robustness of the re-solving heuristic in CDMK problems, regardless of some specific instances. For completeness, we extend our algorithm and analysis to the situation in which the distributions of request and external factor are continuous and derive corresponding regret results (cf. Theorems A.1 and A.2). ", "page_idx": 2}, {"type": "text", "text": "We summarize our algorithmic results on Algorithm 1 in Table 1. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Contextual decision-making/bandits with knapsacks. The issue most closely related to the CDMK problem is the problem of contextual bandits with knapsacks (CBwK), introduced by Agrawal and Devanur [3]. The main difference between the CDMK problem and the CBwK problem is that in the latter, an explicit model of the external factor is missed, and the bandit information feedback is considered. That is, only the consumption and the reward are revealed to the agent at the end of a round rather than the external factor. Along this research line, two primary methodologies have been proposed to solve the problem. The first approach aims to select the best probabilistic strategy within the policy set [7], and Agrawal et al. [4] adopts this approach to achieve an $\\widetilde{O}(\\sqrt{T})$ regret. This heuristic originates from the subject of contextual bandits [16, 2], and requires a cost-sensitive classification oracle to achieve computation efficiency. ", "page_idx": 3}, {"type": "text", "text": "On the other hand, another approach views the problem from the perspective of the Lagrangian dual space. It uses a dual update method that reduces the CBwK problem to the online convex optimization (OCO) problem. In particular, some work [3, 31, 34, 29] assumes a linear relationship between the conditional expectation of the reward-consumption pair and the request-action pair. This line adopts techniques for estimating linear function classes [1, 6, 33, 17] and combines them with OCO methods to achieve sub-linear regret. ", "page_idx": 3}, {"type": "text", "text": "Apart from the above studies, some results [23, 35, 36] are not restricted to linear expectation functions. To deal with more general problems with bandit feedback, they plug model-reliable online regression methods [21, 20] into the dual update framework. As a result, their algorithms\u2019 regret is the sum of the regret on online regression and online convex optimization, respectively. Nevertheless, the online regression technique still limits the conditionally expected reward-consumption functions. ", "page_idx": 3}, {"type": "text", "text": "In the CDMK literature, Liu and Grigas [29] have considered full information feedback, where the agent sees the external factor at the end of each round. Motivated by practice, our work further considers a partial feedback model, in which the agent observes the external factor when a non-null action is chosen (cf. Section 2). ", "page_idx": 3}, {"type": "text", "text": "The re-solving heuristic and related problems. Unlike the above approaches, our work adopts the re-solving method, also known as the \"certainty equivalence\" (CE) heuristic. Under this approach, the agent (in)frequently solves the fuid optimization problem with the remaining resources to obtain a probability control in each round. This method comes from the literature on the network revenue management (NRM) problem, which can be seen as a simplification of the CDMK problem without the existence of external factors or the external factor not getting involved in the resource consumption [41]. Some researches in this setting also assumes known request distributions [25, 5, 24, 18, 12, 27, 15, 10, 38, 11, 26]. These works show that the re-solving-based method can obtain a constant regret under certain non-degeneracy assumptions and can generally obtain a square-root regret [15]. Recently, the re-solving method is also extended to the general dynamic resourceconstrained reward collection problem in Balseiro et al. [8], which assumes the knowledge of request and external factor distributions and achieves $O(1)$ to ${\\cal O}(\\log T)$ regret for different action space cardinalities. ", "page_idx": 3}, {"type": "text", "text": "We should mention that the re-solving technique, together with other methods, has also been adopted for the bandits with knapsacks (BwK) problem [22, 19, 41, 38, 28, 31] to achieve an ${\\cal O}(\\log T)$ regret under different assumptions. For example, an essential result by Sankararaman and Slivkins [31] achieves ${\\cal O}(\\log T)$ regret in BwK under the best-arm assumption and two resources. However, CDMK is a more challenging problem than BwK in that the decision has to be based on the received request. Thus, no optimal static action mode is irrelevant to the round, which adds a layer of complexity to the re-solving method. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider an agent interacting with the environment for $T$ rounds. There are $n$ kinds of resources, with an average amount of $\\rho^{i}$ for resource $i$ in each round, resulting in a total of $\\rho^{i}T$ amount of resource $i$ We suppose that $\\mathbf{0}<\\pmb{\\rho}=\\pmb{\\rho}_{1}=(\\pmb{\\rho}^{i})_{i\\in[n]}\\le\\mathbf{1}$ is independent of $T$ with a maximum entry of $\\rho^{\\mathrm{max}}\\leq1$ and a minimum entry of $\\rho^{\\mathrm{min}}>0$ \uff1a ", "page_idx": 3}, {"type": "text", "text": "At the beginning of each round $t\\geq1$ , the agent observes a request $\\theta_{t}\\,\\in\\,\\Theta$ drawn i.i.d. from a distribution $\\boldsymbol{\\mathcal{U}}$ and should choose an action $a_{t}$ from a set of actions $A$ . Given the request $\\theta_{t}$ and the action $a_{t}$ , the agent receives a random reward $r_{t}\\,\\in\\,[0,1]$ and a consumption vector of resources $c_{t}\\in[0,1]^{n}$ , both of which are related to an external factor $\\gamma_{t}\\in\\Gamma$ drawn i.i.d. from a distribution $\\nu$ . In other words, there is a reward function $r:\\Theta\\times A\\times\\Gamma\\to[0,1]$ and a consumption vector function $c:\\Theta\\times A\\times\\Gamma\\to[0,1]^{n}$ , such that $r_{t}=r(\\theta_{t},a_{t},\\gamma_{t})$ and ${\\pmb{c}}_{t}\\,\\dot{=}\\,{\\pmb{c}}(\\theta_{t},a_{t},\\gamma_{t})$ . We suppose these two functions are pre-known to the agent. We further define $R(\\theta,a):=\\mathbb{E}_{\\gamma}[r(\\theta,a,\\gamma)]$ , and $C(\\theta,a):=\\mathbb{E}_{\\gamma}[c(\\theta,a,\\gamma)]$ ", "page_idx": 4}, {"type": "text", "text": "We impose minimum restrictions on the distributions $\\boldsymbol{\\mathcal{U}}$ and $\\nu$ . In the main body of this work, we only suppose that the support sets of both distributions are finite. Specifically, we let $k=|\\Theta|$ be the size of the request set. We denote the mass function of $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{V}$ by $u(\\boldsymbol{\\theta})$ and $v(\\gamma)$ , respectively. We will extend to the situation that these two distributions can be continuous in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "The agent's objective is to maximize the cumulative rewards over the period under initial resource constraints, which is a sequential decision-making problem. To ensure feasibility, we assume the existence of a null action (denoted by O) in the action set $A$ . Under the null action, the reward and the consumption of any resource are zero, regardless of the request and the external factor. In other words, we have $r(\\theta_{t},0,\\gamma_{t})=0$ and $c(\\theta_{t},0,\\gamma_{t}\\bar{)}=\\mathbf{0}$ for any $(\\bar{\\theta_{t}},\\gamma_{t})\\in\\Theta\\times\\Gamma$ We use $A^{+}:=A\\setminus\\{0\\}$ to denote the set of non-null actions and let $m:=|A^{+}|$ be its size. ", "page_idx": 4}, {"type": "text", "text": "We would like to discuss here the necessity of the null action, which is widely used in related works [7, 3, 4, 35]. An alternative common choice for the null action is the so-called \u201cearly stop when resource exhausted\" [31] in the BwK problem with no contexts. In reality, when the agent faces some \u201cbad\u201d contexts, a better choice is not \u201centering the market' to avoid, for example, small rewards but large consumption. As a comparison, struggling to come up with a non-null action here could occupy the space for serving those \u201cgood\"\u2019 contexts, and stopping before these contexts arrive mayinevitablycause an $\\Omega(T)$ regret. This illustrates that introducing a null action is necessary in the CDMK problem. In fact, in this problem, contexts play the role of revealing information and deterring unreasonable deals. ", "page_idx": 4}, {"type": "text", "text": "We consider the set of non-anticipating strategies $\\Pi$ . In particular, let $\\mathcal{H}_{t}$ be the history the agent could access at the start of round $t$ . Then, for any non-anticipating strategy $\\pi\\in\\Pi$ $a_{t}$ should depend only on $(\\boldsymbol{\\theta}_{t},\\boldsymbol{\\mathcal{H}}_{t})$ , that is, $a_{t}=a_{t}^{\\pi}(\\theta_{t},\\mathcal{H}_{t})$ . For abbreviation, we write $a_{t}^{\\pi}=a_{t}^{\\pi}(\\theta_{t},\\mathcal{H}_{t})$ when there is no confusion. ", "page_idx": 4}, {"type": "text", "text": "Therefore, we can define the agent's optimization problem as below: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{\\mathrm{ON}}:=\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{\\theta\\sim\\mathcal{U}^{T},\\gamma\\sim\\nu^{T}}\\left[\\sum_{t=1}^{T}r(\\theta_{t},a_{t}^{\\pi},\\gamma_{t})\\right],\\quad\\mathrm{~s.t.~}\\sum_{t=1}^{T}c(\\theta_{t},a_{t}^{\\pi},\\gamma_{t})\\leq\\rho T,\\,\\forall\\theta\\in\\Theta^{T},\\gamma\\in\\Gamma^{T}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The fuid benchmark. In practice, however, computing the expected reward of the optimal online strategy would require solving a high-dimension (probably infinite) dynamic programming, which is intractable. Hence, we turn to consider the fuid benchmark to measure the performance of a strategy, which is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle V^{\\mathrm{FL}}:=T\\cdot\\operatorname*{max}_{\\phi\\in\\mathbb{N}+4\\to\\mathbb{R}}\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\displaystyle\\sum_{a\\in A^{+}}R(\\theta,a)\\phi(\\theta,a)\\right],}\\\\ {\\mathrm{s.t.~}\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\displaystyle\\sum_{a\\in A^{+}}C(\\theta,a)\\phi(\\theta,a)\\right]\\leq\\rho;\\displaystyle\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\forall\\theta\\in\\Theta;\\phi(\\theta,a)\\geq0,\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For a better understanding, $V^{\\mathrm{FL}}$ reflects the maximum expected total rewards an agent can win when a static strategy is adopted and the resource constraints are only to be satisfied in expectation. Therefore, this optimization problem is a linear program in which the decision variable $\\bar{\\phi(\\theta,a)}$ represents the probability that the agent chooses action $a$ upon seeing request $\\theta$ . It is a well-known result that $V^{\\mathrm{FL}}$ gives an upper bound on VON. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.1 (Balseiro et al. [8]). $V^{\\mathrm{FL}}\\geq V^{\\mathrm{ON}}$ ", "page_idx": 4}, {"type": "text", "text": "Thus, we evaluate the performance of a non-anticipating strategy $\\pi$ by comparing its expected accumulated reward $R e w^{\\pi}$ with the fluid benchmark $\\dot{V}^{\\mathrm{FL}}$ , which is a common choice in literature [34, 23]. However, we prove that such a benchmark choice may lead to a $\\Omega({\\sqrt{T}})$ gap as long as $V^{\\mathrm{FL}}$ is degenerate. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 2.1 (Worst-casegap).When $V^{\\mathrm{FL}}$ hasa uniqueanddegenerateoptimalsolution, $V^{\\mathrm{FL}}-$ $V^{\\mathrm{ON}}=\\Omega({\\sqrt{T}})$ ", "page_idx": 5}, {"type": "text", "text": "Despite the worst-case lower bound, we prove in this work that for any CDMK instance in which $V^{\\mathrm{FL}}$ has a unique non-degenerate optimal solution cf. Assumption 3.1), we can obtain an $\\widetilde O(1)$ gap compared to the fluid benchmark. Thus, it is still a good choice in most cases. ", "page_idx": 5}, {"type": "text", "text": "Information feedback model. In this work, we consider two types of information feedback models, with increasing difficulty obtaining a sample of the external factor $\\gamma$ ", "page_idx": 5}, {"type": "text", "text": "\u00b7 [Full information feedback.] The agent is able to observe $\\gamma_{t}$ at the end of each round $t$ \u00b7 [Partial information fedback.] The agent can observe $\\gamma_{t}$ at the end of round $t$ onlyif $a_{t}\\neq0$ ", "page_idx": 5}, {"type": "text", "text": "In general, with full information feedback, the agent can observe an i.i.d. sample from $\\nu$ each round, which is the optimal scenario for learning the distribution. Nevertheless, such an assumption could be strong since the reward and consumption vector are irrelevant to the external factor when the agent chooses the null action $a=0$ . Thereby, a more realistic information model is the partial feedback one, where the external factor is only accessible when $a\\ne0$ . This limitation also increases the difficulty of learning the distribution $\\mathcal{V}$ since the agent observes fewer samples under this model than under full information feedback. It is important to note that the partial information model represents a transition from full to bandit information feedback, under which only the reward and consumption vector are accessible in each round, rather than the external factor. Real-life instances of partial information feedback include Examples 1.1 and 1.2, as we have discussed in the introduction. ", "page_idx": 5}, {"type": "text", "text": "3   The Re-Solving Heuristic ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this work, we introduce the re-solving heuristic to the CDMK problem. The resulting algorithm is presented in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "To briefly describe the algorithm, we start by defining an optimization problem that captures the optimal fluid control for each round, assuming complete knowledge of $\\boldsymbol{\\mathcal{U}}$ and $\\nu$ .For any $\\pmb{\\kappa}\\in[0,1]^{n}$ wedefine $J(\\kappa)$ as the following optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{J(\\kappa):=\\operatorname*{max}_{\\phi\\cdot\\Theta\\times A^{+}\\to\\mathbb{R}}\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{\\alpha\\in A^{+}}R(\\theta,a)\\phi(\\theta,a)\\right],}}\\\\ {\\displaystyle{\\mathrm{s.t.~}\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}C(\\theta,a)\\phi(\\theta,a)\\right]\\leq\\kappa;\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\forall\\theta\\in\\Theta;\\phi(\\theta,a)\\geq0,\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Evidently, we have $V^{\\mathrm{FL}}=T\\cdot J(\\pmb\\rho)=T\\cdot J(\\pmb\\rho_{1})$ by definition. Intuitively, in each round $t$ , the best fuid choice of the agent is given by the optimal solution $\\phi_{t}^{*}$ of LP $J(\\rho_{t})$ where $\\rho_{t}$ is the average budget of the remaining rounds, including round $t$ . Nevertheless, since full knowledge of the exact distributions $\\boldsymbol{\\mathcal{U}}$ and $\\nu$ is lacking, the agent can only solve an estimated programming $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ as outlined in Algorithm 1, with the following realization: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\displaystyle\\widehat{J}\\left(\\rho_{t},\\mathcal{H}_{t}\\right):=\\operatorname*{max}_{\\phi:\\Theta\\times A^{+}\\to\\mathbb{R}}\\displaystyle\\mathbb{E}_{\\sim\\hat{\\mathcal{U}}_{t}}\\left[\\sum_{a\\in A^{+}}\\mathbb{E}_{\\gamma\\sim\\widehat\\nu_{t}}\\left[r(\\theta,a,\\gamma)\\right]\\phi(\\theta,a)\\right],}\\\\ {\\displaystyle\\colon\\mathbb{E}_{\\hat{\\mathcal{U}}_{t}}\\left[\\sum_{a\\in A^{+}}\\mathbb{E}_{\\widehat\\nu_{t}}\\left[c(\\theta,a,\\gamma)\\right]\\phi(\\theta,a)\\right]\\leq\\rho_{t};\\displaystyle\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\forall\\theta\\in\\Theta;\\,\\phi(\\theta,a)\\geq0,\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\widehat{\\mathcal{U}}_{t}$ and $\\widehat{\\nu}_{t}$ represent the empirical distribution of $\\theta$ and $\\gamma$ , respectively, according to the sample history given by $\\mathcal{H}_{t}$ . Specifically, let $\\mathcal{T}_{t}$ be the set of rounds that the agent accesses the external factor. The mass functions of these two estimated distributions are standard as follows: $\\widehat{u}_{t}(\\theta):=\\#[\\theta$ appears in pevious $t\\!-\\!1$ roundsl/ $\\stackrel{\\prime}{t}-1$ .\uff0c $\\widehat{v}_{t}(\\gamma):=\\#[\\gamma$ appearsinroundsin $\\left.\\mathcal{T}_{t}\\right]\\big/|\\mathcal{T}_{t}|$ ", "page_idx": 5}, {"type": "text", "text": "Input: $\\rho,T$   \nInitialization: $\\mathcal{T}_{1}\\gets\\emptyset$ $B_{1}\\leftarrow\\rho T$   \nfor $t=1$ to $T$ do Observe $\\theta_{t}$ $\\rho_{t}\\gets B_{t}/(T-t+1)$ $\\widehat{\\phi}_{t}^{*}\\gets$ the solution to $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ Choose $a_{t}\\in A$ randomly such that for $a\\in A^{+}$ \uff0c $\\operatorname*{Pr}[a_{t}=a]=\\widehat{\\phi}_{t}^{*}(\\theta_{t},a)$ , and $\\operatorname*{Pr}[a_{t}=0]=$ $\\begin{array}{r}{1-\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta_{t},a)}\\end{array}$ . if (FULL-INFO) V ${}^{/}P\\!A R T I A L{}-I N F O\\land a_{t}\\neq0;$ then Observe $\\gamma_{t}$ .\uff0c $\\mathcal{T}_{t+1}\\leftarrow\\mathcal{T}_{t}\\cup\\{t\\}$ else $\\mathcal{T}_{t+1}\\leftarrow\\mathcal{T}_{t}$ .\uff0c end if $B_{t+1}\\leftarrow B_{t}-c_{t}$ .\uff0c $B_{t+1}^{i}<1$ for some $i\\in[n]$ then break; end if   \nend for ", "page_idx": 6}, {"type": "text", "text": "It is worth noting that the estimated distribution of $\\theta,\\widehat{\\mathcal{U}}_{t}$ , is always based on $t-1$ samples since the agent received an independent sample from $\\boldsymbol{\\mathcal{U}}$ at the beginning of each round. On the other hand, the empirical distribution of the external factor $\\gamma$ $\\widehat{\\nu}_{t}$ , is estimated from $\\left|{\\mathcal{Z}}_{t}\\right|$ independent samples. With full information feedback, $|Z_{t}|=t-1$ ; whereas with partial information feedback, $|\\mathcal{T}_{t}|\\leq\\bar{t}-1$ equals the number of times the agent chooses an action $a\\ne0$ before round $t$ . For brevity, for the estimated programming, we write $\\widehat{C}_{t}(\\theta,a):=\\mathbb{E}_{\\gamma\\sim\\widehat{\\nu}_{t}}[c(\\theta,a,\\gamma)]$ and $\\widehat{R}_{t}(\\theta,a):=\\mathbb{E}_{\\gamma\\sim\\widehat{\\nu}_{t}}[r(\\theta,a,\\gamma)]$ ", "page_idx": 6}, {"type": "text", "text": "As per Algorithm 1, the agent's decision mode in round $t$ is given by the optimal solution $\\widehat{\\phi}_{t}^{*}$ of programming $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ : The algorithm stops when the resources are near depletion, that is, $B^{i^{-}}\\!<\\bar{1}$ for some resource $i\\in[n]$ and we use $T_{0}$ to denote the stopping time of Aigorithm 1, i.e., $T_{0}:=\\operatorname*{min}\\{T,\\operatorname*{min}\\{t:\\exists i\\in[n],\\dot{B}_{t+1}^{i}<1\\}\\}$ ", "page_idx": 6}, {"type": "text", "text": "For an analysis beyond the worst-case scenario, a crucial assumption we will make is that the fuid problem possesses good regularity properties, i.e., it is an LP with a unique and non-degenerate solution. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1. The optimal solution to $J(\\rho_{1})$ is unique and non-degenerate. ", "page_idx": 6}, {"type": "text", "text": "As pointed out by Bumpensanti and Wang [12], uniqueness and non-degeneracy are a critical factor for an $o(\\sqrt{T})$ regret bound to hold in the CDMK problem, at least for the frequent re-solving technique we use in this work [25]. Intuitively, if $J(\\rho_{1})$ is degenerate, then with any minor error on the estimation, the optimal solution to $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ can have a major different landscape with the optimal solution to $J(\\rho_{1}\\bar{)}$ in the sense of basic variables and binding constraints, and this will lead to an $O(\\sqrt{T})$ accumulated regret. For completeness, we formally define the above concepts. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.1. A context-action pair $(\\theta,a)$ is a basic variable for $J(\\rho_{1})$ if $\\phi_{1}^{*}(\\theta,a)>0$ , or else, it is a non-basic variable. Similarly, define basic/non-basic variables for $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ ", "page_idx": 6}, {"type": "text", "text": "Definition 3.2. $i\\in[n]$ is a binding constraint for $J(\\rho_{1})$ $\\begin{array}{r}{\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)C^{i}(\\theta,a)\\phi_{1}^{*}(\\theta,a)=\\rho_{1}^{i}}\\end{array}$ or else it is a non-binding constraint. We let $S:=\\{i$ is a binding constraint for $J(\\rho_{1})\\}$ \uff0c $\\textstyle{\\mathcal{T}}=[n]\\setminus{\\mathcal{S}}$ and we use $\\kappa|_{\\mathcal{S}}$ or $\\kappa|\\tau$ to define the sub-vector of $\\kappa$ confined on $\\boldsymbol{S}$ or $\\tau$ , respectively. Further, $\\theta\\in\\Theta$ is a binding constraint for $J(\\rho_{1})$ $\\begin{array}{r}{\\sum_{a\\in A^{+}}\\phi_{1}^{*}(\\theta,a)=1}\\end{array}$ , or else it is a non-binding constraint. Similarly, define binding/non-binding constraints for $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ ", "page_idx": 6}, {"type": "text", "text": "As stated above, we want to guarantee that when the \u201cdistance'\u2019 between $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ and $J(\\rho_{1})$ is sufficiently small, the optimal solution to these two programmings have the same landscapes. In this sense, we consider a stability factor $D$ to measure such a threshold, as presented by Mangasarian and Shiau [30]. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.1 (Stability). Under Assumption 3.1, there is a maximum $D>0$ suchthatwhenthe followingholds: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\left\\{\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty},\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\right\\}\\leq D,}\\\\ &{\\quad\\quad\\operatorname*{max}\\left\\{\\|\\rho_{1}|_{\\mathcal{S}}-\\rho_{t}|_{\\mathcal{S}}\\|_{\\infty}\\,,\\operatorname*{max}\\left\\{\\rho_{1}|_{\\mathcal{T}}-\\rho_{t}|_{\\mathcal{T}}\\right\\}\\right\\}\\leq D,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$J(\\rho_{1})$ and $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ share the same sets of basic/non-basic variables and binding/non-binding constraints. ", "page_idx": 7}, {"type": "text", "text": "With the assumption, below we present the main result of this work, which is proved in Appendix C.1. Theorem 3.1. Under Assumption 3.1, with full information feedback, the expected accumulated rewardRewbroughtby Algorithm $^{\\,l}$ when $T\\rightarrow\\infty$ satisfies: ", "page_idx": 7}, {"type": "equation", "text": "$$\nV^{\\mathrm{FL}}-R e w=O\\left(\\frac{n^{2}+k}{D^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is independent of $T$ ", "page_idx": 7}, {"type": "text", "text": "The intuition behind Theorem 3.1 is to conduct a regret decomposition in a Lagrangian manner, motivated by Chen et al. [15]. This leads to three remaining terms (cf. Appendix C). For the first two Lagrangian product terms, thanks to Proposition 3.1, they equal 0 as long as the estimates of the distributions are sufficiently accurate with an error of $O(D)$ , which will happen with high probability after a constant number of rounds. The last term refects how the stopping time of Algorithm 1 is close to the total time-span $T$ . On this front, we are left to demonstrate that the resources are spent smoothly. Intuitively, this property is guaranted by combining two observations: (1) In each round, the action mode ensures that resources are spent evenly in expectation in the estimation world due to the re-solving step, and (2) the distance between the estimation world and the real world diminishes to zero, with the accumulation of samples. The complete reasoning is much more detailed. ", "page_idx": 7}, {"type": "text", "text": "We now compare Theorem 3.1 with results in prior work. We first mention that our benchmark $V^{\\mathrm{FL}}$ is larger than the benchmark used in Slivkins and Foster [35] and Slivkins et al. [36], as proved in Appendix B. Thus, our result provides a stronger regret upper bound. As for the constants in the regret bound, first, our regret does not involve $m$ explicitly. This is superior to existing results, which report an $\\widetilde{O}(\\sqrt{m})$ reliance [7, 4, 35, 23]. As an intuitive reason, the number of actions does not explicitly appear in our Algorithm 1, but only contributes to the dimension of the linear program. Second, although $k$ does not always appear in previous works, this is inevitable in our bound, brought by the estimation error of the context distribution. Third, for the BwK problem, the well-known ${\\cal O}(\\log T)$ result given by Sankararaman and Slivkins [31] supposes that $n\\leq2$ , and it is still unclear whether their analysis can be extended to an arbitrary number of resources. Our result does not suffer from such a limit. Finally, we remark that in the absence of resource constraints, $D$ is precisely half the gap between the mean rewards of the best and second-best arms. Thus, $D$ resembles the reward-gap-like parameter in the multi-armed bandit literature. The dependence on $D$ of our result is similar to the first result in Sankararaman and Slivkins [31] and is the same with Chen et al. [15], and it is still unclear whether the dependence can be improved. We should also note that we omit the dependence of our regret bound on the unknown size of the external factor set in all our results, which could be improved via parameterized estimation techniques. ", "page_idx": 7}, {"type": "text", "text": "One key implication of Theorem 3.1 is that the re-solving heuristic's regret is independent of the number of rounds beyond the worst-case with full information. This result significantly improved over previous state-of-the-art results under mild assumptions, surpassing the solutions proposed by Han et al. [23] and Slivkins and Foster [35]. In particular, their solutions come from the BwK literature and rely on dual update and upper confidence bound (UCB) heuristics, which only provide aworst-caseregretof $O({\\sqrt{T\\log T}})$ ", "page_idx": 7}, {"type": "text", "text": "4 Partial Information Feedback ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now shift to consider the re-solving method's performance with partial information feedback, under which the agent only sees the external factor $\\gamma_{t}$ when her choice is non-null in round $t$ ,i.e., $a_{t}~\\neq~0$ . Apparently, with less information, the learning speed of the distribution $\\nu$ decreases, hindering the re-solving procedure's quick convergence to an optimal solution. Nevertheless, we demonstrate that the performance of the re-solving method only faces an ${\\cal O}(\\log T)$ multiplicative degradation under partial information feedback. Our primary theorem in this section is as follows: ", "page_idx": 7}, {"type": "image", "img_path": "Dgt6sh2ruQ/tmp/9a3d3567b437aedac3faf783b9c99c5373a046533b3f7193d6f2512bff00d045.jpg", "img_caption": ["Figure 1: An illustration of Lemma 4.1. Before $\\Theta(T)$ rounds, the probability of accessing frequency at any time step is at least $\\Omega(1/\\log T)$ , and we have an overall $\\Omega(1)$ frequency with high probability. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 4.1. Under Assumption 3.1, with partial information feedback, the expected accumulated reward Rew broughtby Algorithm $^{\\,l}$ when $T\\to\\infty$ satisfies: ", "page_idx": 8}, {"type": "equation", "text": "$$\nV^{\\mathrm{FL}}-R e w=O\\left(\\frac{n^{2}+k+\\log T}{D^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Before we come to the technical parts, we first place Theorem 4.1 within the literature. As previously mentioned, $\\Omega({\\sqrt{T}})$ is a worst-case lower bound on the regret even with full information fedback and thus also extends as a lower bound with partial information feedback. However, Theorem 4.1 steps beyond the worst case by providing an ${\\cal O}(\\log T)$ upper bound for regular problem instances. This result outperforms the universal $O({\\bar{\\sqrt{T\\log T}}})$ regret by Han et al. [23] and Slivkins and Foster [35]. While the result is asymptotically equivalent to that of Sankararaman and Slivkins [31], it imposes fewer restrictions on the problem structure, as previously discussed. ", "page_idx": 8}, {"type": "text", "text": "We now provide an intuitive understanding of the proof of Theorem 4.1. The crux lies in analyzing the frequency with which Algorithm 1 can access an independent sample of the external factor. To this end, we use $Y_{t}=|{\\mathcal{T}}_{t}|\\leq t-1$ to denote the number of times a non-null action is chosen before time $t$ , or equivalently, the number of i.i.d. samples from $\\nu$ observed by the agent before time $t$ under partial information feedback. The following crucial technical lemma provides a lower bound on $Y_{t}$ ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.1. There is a constant $0<C_{b}<1/2$ such that with probability $1\\!-\\!O(1/T)$ the following hold for Algorithm $^{\\,l}$ ", "page_idx": 8}, {"type": "text", "text": "The proof of Lemma 4.1 is deferred to Appendix D.2, and an illustration is presented in Figure 1. In simple terms, during the initial $\\Theta(\\log T)$ rounds (the shaded segment), the re-solving method cannot guarantee the accessing frequency since the learning of the request distribution $\\boldsymbol{\\mathcal{U}}$ has yet to converge sufficiently. However, after $\\Theta(\\log T)$ rounds, Algorithm 1 ensures a constant probability of obtaining a new example in each round, provided that the remaining resources are sufficient. As a consequence, before $\\Theta(T)$ rounds, we can guarantee an $\\Omega(1/\\log T)$ accessing frequency at any time step and an overall $\\Omega(1)$ frequency with high probability, as established by a concentration inequality. The remaining proof of Theorem 4.1 is provided in Appendix D.1. ", "page_idx": 8}, {"type": "text", "text": "5  Relaxing the Regularity Assumption - A Worst-Case Guarantee ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Sections 3 and 4, we have proved that Algorithm 1 can achieve an $\\widetilde O(1)$ regretfor CDMK problems under full and partial information feedbacks, assuming certain regular conditions (cf. Assumption 3.1). Put differently, the re-solving heuristic nicely deals with regular scenarios. In this section, we complement the above by showing that this method can attain nearly optimal regret in the worst cases. Furthermore, in Appendix A, we extend our analysis to cases where the context and external factor distributions can be continuous. ", "page_idx": 8}, {"type": "text", "text": "Our main results are given below, and their proofs are provided in Appendices E.1 and E.2, respectively. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1. With full information feedback, the expected accumulated reward Rew brought by AlgorithmI satisies: $V^{\\mathrm{FL}^{*}}-R e w=O(k\\sqrt{T\\log T}+n)$ as $T\\to\\infty$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.2. With partial information feedback, the expected accumulated reward Rew brought by Algorithm1 satisfies: $V^{\\mathrm{FL}}-R e w=O(k\\sqrt{T}\\log T+n)$ as $T\\to\\infty$ ", "page_idx": 8}, {"type": "image", "img_path": "Dgt6sh2ruQ/tmp/bd24df4c43eee7034e61547d3fd59dc48a77449d7c2cc3ce2f2b839c15fb252b.jpg", "img_caption": ["Figure 2: Regret of Algorithm 1 under different number of total rounds $T=2000\\cdot2^{k}$ for integer $0\\leq k\\leq5$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "As given by Theorem 2.1, the worst-case regret of any online CDMK algorithm is $\\Omega({\\sqrt{T}})$ ,while Theorems 5.1 and 5.2 indicate that the re-solving heuristic reaches near-optimality in such cases. Further, state-of-the-art algorithms [23, 35] can at most obtain an $\\widetilde{O}(\\sqrt{T})$ regret even with full/partial information feedback. Our algorithm also achieves this near-optimal regret bound in worst cases. ", "page_idx": 9}, {"type": "text", "text": "6    Numerical Validations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we use numerical experiments to verify our analysis. We perform our simulation experiments with either full or partial information feedback under two cases. The first is with a degenerate optimal solution, and the second is with a unique and non-degenerate optimal solution. We delay more details, including the choice of the problem instances, to Appendix F. ", "page_idx": 9}, {"type": "text", "text": "Figure 2 describes the relationship between the regret and the number of total rounds $T$ under all four settings. We set the horizon $T$ to be $2000\\cdot2^{k}$ for integer $0\\leq k\\leq5$ . The figure displays both the sample mean (the line) and the $99\\%$ -confidence interval (the light color zone) calculated by the results of 50 estimations for the regret, where each estimation comprises 400 independent trials. Observe that when the LP $J(\\boldsymbol{\\rho})$ is degenerate, the regret grows on the order of $\\tilde{O}(\\sqrt{T})$ under both full information and partial information settings. Further, when the underlying LP $J(\\rho)$ has a unique and non-degenerate optimal solution, the regret does not scale with $T$ under the full information setting. In the partial information setting, the regret slowly grows with $T$ , which matches our $\\widetilde O(1)$ theoretical guarantee. ", "page_idx": 9}, {"type": "text", "text": "7 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work establishes the effectiveness of the re-solving heuristic in the contextual decision-making problem with knapsack constraints. We first prove that the gap between the fuid optimum and online optimum is $\\Omega({\\sqrt{T}})$ when the fluid LP has a unique and degenerate optimal solution. Further, we show that the re-solving method reaches an $O(1)$ regret with full information and an ${\\cal O}(\\log T)$ regret with partial information when the fuid LP has a unique and non-degenerate optimal solution, even compared to the fuid benchmark. Considering the sufficient condition for the $\\Omega({\\sqrt{T}})$ lower bound, our non-degeneracy assumption is mild, especially when comparing with the two-resource condition required in Sankararaman and Slivkins [31]. ", "page_idx": 9}, {"type": "text", "text": "Further, we show that even in worst cases, the re-solving method achieves an $O({\\sqrt{T\\log T}})$ regret with full information feedback and an $O({\\sqrt{T}}\\log T)$ regret with partial information feedback. These results are comparable to start-of-the-art results [23, 35]. We also extend our analysis to the continuous randomness case for completeness. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China (Grant No. 62172012), by Alibaba Group through Alibaba Innovative Research Program, and by Peking University-Alimama Joint Laboratory of AI Innovation. Part of this work was done when Rui Ai, Mingwei Yang, Yuqi Pan, and Chang Wang were undergraduates in Peking University. The authors thank Yinyu Ye for his kind suggestions and all anonymous reviewers for their helpful feedback. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. Advances in Neural Information Processing Systems, 24, 2011. [2]  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pages 1638-1646. PMLR, 2014.   \n[3] Shipra Agrawal and Nikhil Devanur. Linear contextual bandits with knapsacks. Advances in Neural Information Processing Systems, 29, 2016. [4]  Shipra Agrawal, Nikhil R Devanur, and Lihong Li. An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives. In Conference on Learning Theory, pages 4-18. PMLR, 2016.   \n[5]  Alessandro Arlotto and Itai Gurvich. Uniformly bounded regret in the multisecretary problem. Stochastic Systems, 9(3):231-260, 2019. [6]  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397-422, 2002.   \n[7]  Ashwinkumar Badanidiyuru, John Langford, and Aleksandrs Slivkins. Resourceful contextual bandits. In Conference on Learning Theory, pages 1109-1134. PMLR, 2014. [8]  Santiago Balseiro, Omar Besbes, and Dana Pizarro. Survey of dynamic resource constrained reward collection problems: Unified model and analysis. Available at SSRN 3963265, 2021. [9]  Santiago R Balseiro and Yonatan Gur. Learning in repeated auctions with budgets: Regret minimization and equilibrium. Management Science, 65(9):3952-3968, 2019.   \n[10] Omar Besbes, Yash Kanoria, and Akshit Kumar. The multisecretary problem with many types. arXiv preprint arXiv:2205.09078, 2022.   \n[11]  Robert L Bray. Logarithmic regret in multisecretary and online linear programs with continuous valuations. Operations Research, 2024.   \n[12]  Pornpawee Bumpensanti and He Wang. A re-solving heuristic with uniformly bounded loss for network revenue management. Management Science, 66(7):2993-3009, 2020.   \n[13] Matteo Castiglioni, Andrea Celli, and Christian Kroer. Online learning with knapsacks: the best ofboth worlds. In International Conference on Machine Learning, pages 2767-2783. PMLR, 2022.   \n[14]  Andrea Celli, Matteo Castiglioni, and Christian Kroer. Best of many worlds guarantees for online learning with knapsacks. arXiv preprint arXiv:2202.13710, 2022.   \n[15]  Guanting Chen, Xiaocheng Li, and Yinyu Ye. An improved analysis of 1p-based control for revenue management. Operations Research, 2022.   \n[16]  Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 169-178, 2011.   \n[17] Adam N Elmachtoub and Paul Grigas. Smart \u201cpredict, then optimize\". Management Science, 68(1):9-26, 2022.   \n[18] Kris Johnson Ferreira, David Simchi-Levi, and He Wang. Online network revenue management using thompson sampling. Operations research, 66(6):1586-1602, 2018.   \n[19]  Arthur Flajolet and Patrick Jaillet. Logarithmic regret bounds for bandits with knapsacks. arXiv preprint arXiv:1510.01800, 2015.   \n[20] Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In International Conference on Machine Learning, pages 3199-3210. PMLR, 2020.   \n[21] Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire. Practical contextualbandits with regression oracles. In International Conference onMachine Learning, pages 1539-1548. PMLR, 2018.   \n[22]  Andras Gyorgy, Levente Kocsis, Ivett Szab6, and Csaba Szepesvari. Continuous time associative bandit problems. In IJCAl, pages 830-835, 2007.   \n[23]  Yuxuan Han, Jialin Zeng, Yang Wang, Yang Xiang, and Jiheng Zhang. Optimal contextual bandits with knapsacks under realizibility via regression oracles. arXiv preprint arXiv:2210.11834, 2022.   \n[24]  Stefanus Jasin. Performance of an Ip-based control for revenue management with unknown demand parameters. Operations Research, 63(4):909-915, 2015.   \n[25]  Stefanus Jasin and Sunil Kumar. A re-solving heuristic with bounded revenue loss for network revenue management with customer choice. Mathematics of Operations Research, 37(2): 313-345, 2012.   \n[26] Jiashuo Jiang, Will Ma, and Jiawei Zhang. Degeneracy is ok: Logarithmic regret for network revenue management with indiscrete distributions. arXiv preprint arXiv:2210.07996, 2022.   \n[27]  Xiaocheng Li and Yinyu Ye. Online linear programming: Dual convergence, new algorithms, and regret bounds. Operations Research, 2021.   \n[28] Xiaocheng Li, Chunlin Sun, and Yinyu Ye. The symmetry between arms and knapsacks: A primal-dual approach for bandits with knapsacks. In International Conference on Machine Learning, pages 6483-6492. PMLR, 2021.   \n[29]  Heyuan Liu and Paul Grigas. Online contextual decision-making with a smart predict-thenoptimize method. arXiv preprint arXiv:2206.07316, 2022.   \n[30] Olvi L Mangasarian and T-H Shiau. Lipschitz continuity of solutions of linear inequalities, programs and complementarity problems. SIAM Journal on Control and Optimization, 25(3): 583-595, 1987.   \n[31]  Karthik Abinav Sankararaman and Aleksandrs Slivkins. Bandits with knapsacks beyond the worst case. Advances in Neural Information Processing Systems, 34:23191-23204, 2021.   \n[32]  Gerard Sierksma. Linear and integer programming: theory and practice. CRC Press, 2001.   \n[33] Vidyashankar Sivakumar, Steven Wu, and Arindam Banerjee. Structured linear contextual bandits: A sharp and geometric smoothed analysis. In International Conference on Machine Learning, pages 9026-9035. PMLR, 2020.   \n[34]  Vidyashankar Sivakumar, Shiliang Zuo, and Arindam Banerjee. Smoothed adversarial linear contextual bandits with knapsacks. In International Conference on Machine Learning, pages 20253-20277. PMLR, 2022.   \n[35]  Aleksandrs Slivkins and Dylan Foster. Efficient contextual bandits with knapsacks via regression. arXiv preprint arXiv:2211.07484, 2022.   \n[36]  Aleksandrs Slivkins, Karthik Abinav Sankararaman, and Dylan J Foster. Contextual bandits with packing and covering constraints: A modular lagrangian approach via regression. In The Thirty Sixth Annual Conference on Learning Theory, pages 4633-4656. PMLR, 2023.   \n[37]  Alberto Vera and Sidhartha Banerjee. The bayesian prophet: A low-regret framework for online decision making. Management Science, 67(3): 1368-1391, 2021.   \n[38]  Alberto Vera, Siddhartha Banerjee, and Itai Gurvich. Online allocation and pricing: Constant regret via bellman inequalities. Operations Research, 69(3):821-840, 2021.   \n[39] Larry Wasserman. Density estimation - lecture note for 36-708 statistical methods for machine learning, Carnegie Mellon University, 2019.   \n[40]  Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities for the 11 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep, 2003.   \n[41] Huasen Wu, Rayadurgam Srikant, Xin Liu, and Chong Jiang. Algorithms with logarithmic or sublinear regret for constrained contextual bandits. Advances in Neural Information Processing Systems,28,2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix of \u201cContextual Decision-Making with Knapsacks Beyond the Worst Case' ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We begin by outlining the structure of the whole appendix. In Appendix A, we state our regret results with continuous randomness. In Appendix B, we prove the regret worst-case result (cf. Theorem 2.1). Appendix C.1 devotes to proving the main result (cf. Theorem 3.1), where Appendix C.1.1 presents the Lagrangian regret decomposition, and Appendices C.1.2 and C.1.3 analyze different terms in the decomposition. Appendices C.2 to C.6 complement missing proofs of lemmas arising in Appendix C.1. Appendix D focuses on proving the regret results in the partial information setting, where Appendix D.1 derives Theorem 4.1, and Appendix D.2 proves the crucial Lemma 4.1 for the partial feedback model. Appendices D.3 and D.4 prove lemmas arising in previous parts of Appendix D. Appendix E deals with Theorems 5.1 and 5.2, which show that the re-solving method is near-optimal even in worst cases. At last, Appendix G complements the missing details of Appendix A with continuous randomness. ", "page_idx": 13}, {"type": "text", "text": "A From Discrete Randomness to Continuous Randomness ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the main body of this work, we explicitly assume that both the context set and the external factor set are discrete. Such an assumption can suitably capture most real-life situations. For example, in an agent's online bidding problem with budget constraints, if we presume that the context is the agent's actual value and the external factor is the highest competing bid, it is natural to suppose that all these three values are discrete. Nevertheless, for theoretical completeness, we expand our results in this section to circumstances where these two sets are infinite, i.e., the two underlying randomnesses are continuous. It is imperative to note that the scenario where one randomness is discrete and the other is continuous would be analogous in analysis by incorporating the techniques presented in Section 5. ", "page_idx": 13}, {"type": "text", "text": "Conceptually, the re-solving heuristic still works: we solve the optimization problem in each round concerning the remaining resources based on previous estimates. However, technically, since the distributions of context and external factors are continuous, we should further elaborate on the setting. In this section, we suppose that the context set $\\Theta=[0,1]^{d_{u}}$ and the external factor set $\\Gamma=[0,1]^{d_{v}}$ We denote $u(\\boldsymbol{\\theta})$ and $v(\\gamma)$ as the density function of $\\boldsymbol{\\mathcal{U}}$ and $\\nu$ , respectively. We assume that $p\\in\\{u,v\\}$ belongs to the $\\beta_{p}$ -order $L_{p}$ -Holder smooth class $\\Sigma(\\beta_{p},L_{p})$ . Here, for the foundation, given a vector $\\boldsymbol{s}=(s_{1},...,s_{d})$ , define ", "page_idx": 13}, {"type": "equation", "text": "$$\n|s|=s_{1}+\\cdot\\cdot\\cdot+s_{d},\\quad D^{s}=\\frac{\\partial^{s_{1}+\\cdot\\cdot\\cdot+s_{d}}}{\\partial x_{1}^{s_{1}}\\cdot\\cdot\\cdot\\partial x_{d}^{s_{d}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Subsequently, for a positive integer $\\beta$ ,the $\\beta$ -order $L$ -Holder smooth class is defined as ", "page_idx": 13}, {"type": "equation", "text": "$\\Sigma(\\beta,L):=\\{g:|D^{s}g(x)-D^{s}g(y)|\\leq L\\|x-y\\|_{2}$ ", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now,suppose $X_{1},\\cdot\\cdot\\cdot,X_{k}$ are $k$ i.i.d. samples from a distribution with density function $p\\in\\Sigma(\\beta,L)$ According to Wasserman [39], we have the following result, which implies that we can calculate an estimator from these samples that converges to the density function. ", "page_idx": 13}, {"type": "text", "text": "Proposition A.1 (Wasserman [39]). Suppose $X_{1},\\cdot\\cdot\\cdot,X_{k}$ are drawn i.i.d. from a $d$ -dimension distribution $\\mathcal{P}$ withdensity $p\\in\\Sigma(\\beta,L)$ for some $L>0$ ,and $k$ is sufficiently large.Then there exists an estimator ${\\widehat{p}}_{k}$ such that for any $\\epsilon>0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{x}|p(x)-\\widehat{p}_{k}(x)|>\\frac{C\\sqrt{\\log(k/\\epsilon)}}{k^{\\beta/(2\\beta+d)}}\\right]\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $C$ a constant. ", "page_idx": 13}, {"type": "text", "text": "The details of constructing such a density estimator are postponed to Appendix G.1. We now return to the re-solving heuristic and Algorithm 1. In the algorithm, with continuous randomness, the constrained optimization problem to be solved in each round $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ for $t=1,2,\\cdot\\cdot\\cdot$ becomes: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\widehat{J}(\\rho_{t},\\mathcal{H}_{t}):=\\operatorname*{max}_{\\phi:\\Theta\\times A^{+}\\to\\mathbb{R}}\\,\\int_{\\theta}\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\int_{\\gamma}r(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\widehat{u}_{t}(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{s.t.~}}&{\\displaystyle\\int_{\\theta}\\sum_{a\\in A^{+}\\atop a\\in A^{+}}\\phi(\\theta,a)\\int_{\\gamma}c(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\widehat{u}_{t}(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta\\leq\\rho_{t},}\\\\ &{\\qquad\\quad\\displaystyle\\sum_{a\\in A^{+}\\atop a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\quad\\forall\\theta\\in\\Theta,}\\\\ &{\\phi(\\theta,a)\\geq0,\\quad\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Correspondingly, the reference optimization problem $J(\\rho_{t})$ is given below: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\rho_{t}):=\\underset{\\phi:\\Theta\\times A^{+}\\rightarrow\\mathbb R}{\\operatorname*{max}}\\,\\int_{\\theta}\\underset{a\\in A^{+}}{\\sum}\\,\\phi(\\theta,a)\\int_{\\gamma}r(\\theta,a,\\gamma)v(\\gamma)u(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta,}\\\\ {\\mathrm{s.t.}\\quad}&{\\displaystyle\\int_{\\theta}\\underset{a\\in A^{+}}{\\sum}\\,\\phi(\\theta,a)\\int_{\\gamma}c(\\theta,a,\\gamma)v(\\gamma)u(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta\\leq\\rho_{t},}\\\\ &{\\displaystyle\\qquad\\qquad\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\quad\\forall\\theta\\in\\Theta,}\\\\ &{\\displaystyle\\qquad\\qquad\\phi(\\theta,a)\\geq0,\\quad\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "At this point, it is worth mentioning that solving $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ in each round could be hard as it could be a continuous yet non-convex constrained optimization problem. Nevertheless, we assume the existence of an oracle that aids us in solving this optimization, and we focus on the regret of the re-solving method. Let $\\alpha_{u}:=(\\beta_{u}+d_{u})/(2\\beta_{u}+d_{u})$ and $\\alpha_{v}:=(\\beta_{v}+d_{v})/(2\\beta_{v}+d_{v})$ ,andwehave the following two results, respectively, under full and partial information feedback. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1. Under continuous randomness, with full information feedback, the expected accumulatedrewardRewbroughtbyAlgorithm $^{\\,I}$ satisfies: ", "page_idx": 14}, {"type": "equation", "text": "$$\nV^{\\mathrm{FL}}-R e w=O\\big((T^{\\alpha_{u}}+T^{\\alpha_{v}}+T^{1/2})\\sqrt{\\log T}+n),\\quad T\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem A.2. Under continuous randomness, with partial information feedback, the expected accumulatedrewardRewbroughtbyAlgorithm $^{\\,I}$ satisfies: ", "page_idx": 14}, {"type": "equation", "text": "$$\nV^{\\mathrm{FL}}-R e w=O\\big((T^{\\alpha_{u}}+T^{1/2})\\sqrt{\\log T}+T^{\\alpha_{v}}\\log^{3/2-\\alpha_{v}}T+n\\big),\\quad T\\to\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The proofs of the above theorems are presented in Appendices G.2 and G.3, respectively, which almost follow the threads of Theorems 5.1 and 5.2. ", "page_idx": 14}, {"type": "text", "text": "B  Specifying the Worst-Case Gap - Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To prove the lemma, we first introduce an intermediate value, which we denote as $V^{\\mathrm{Hyb}}$ , to upper bound $V^{\\mathrm{ON}}$ , and show that the gap between $V^{\\mathrm{Hyb}}$ and $V^{\\mathrm{FL}}$ .is $O({\\sqrt{T}})$ under the given condition. Specifically, we have the following definition: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\mathrm{Hyb}}:=\\mathbb{E}_{\\theta_{1},\\cdots,\\theta_{T}}\\left[\\underset{\\phi_{1},\\cdots,\\phi_{T}:A^{+}\\rightarrow\\mathbb{R}}{\\operatorname*{max}}\\sum_{t=1}^{T}\\sum_{a\\in A^{+}}R(\\theta_{t},a)\\phi_{t}(a)\\right],}\\\\ &{\\qquad\\qquad\\mathrm{s.t.}\\quad\\displaystyle\\sum_{t=1}^{T}\\sum_{a\\in A^{+}}C(\\theta_{t},a)\\phi_{t}(a)\\leq\\rho T,}\\\\ &{\\qquad\\qquad\\displaystyle\\sum_{a\\in A^{+}}\\phi_{t}(a)\\leq1,\\quad\\forall t\\in[T],}\\\\ &{\\qquad\\qquad\\phi_{t}(a)\\geq0,\\quad\\forall(t,a)\\in[T]\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To seethat $V^{\\mathrm{Hyb}}$ gives an upper bound on $V^{\\mathrm{ON}}$ , we fix arequest raectory $\\theta_{1},\\cdot\\cdot\\cdot,\\theta_{T}$ Now for any non-anticipating strategy $\\pi$ , we let ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{t}^{\\pi}(a)=\\mathrm{Pr}[a_{t}^{\\pi}=a\\;|\\;\\theta_{1},\\cdots\\;,\\theta_{t}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "be the total probability that $a_{t}^{\\pi}=a$ conditioning on the pre-determined request sequence, with respect to $\\gamma_{1},\\cdots,\\gamma_{t-1}$ and the randomness of strategy $\\pi$ . We show that $\\{p_{t}^{\\pi}\\}_{t=1,\\cdots,T}$ is a feasible solution 0 $V^{\\mathrm{Hyb}}$ under $\\theta_{1},\\cdot\\cdot\\cdot,\\theta_{T}$ . Here, a key observation is that for any $t\\in[T]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[c(\\theta_{t},a_{t}^{\\pi},\\gamma_{t})\\ |\\ \\theta_{1},\\cdots\\ ,\\theta_{t}\\right]=\\mathbb{E}_{\\gamma_{t}}\\left[\\displaystyle\\sum_{a\\in A^{+}}c(\\theta_{t},a_{t}^{\\pi},\\gamma_{t})\\cdot\\operatorname*{Pr}[a_{t}^{\\pi}=a\\ |\\ \\theta_{1},\\cdots\\ ,\\theta_{t}]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{a\\in A^{+}}C(\\theta_{t},a)p_{t}^{\\pi}(a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the above, the first expectation is taken on $\\gamma_{1},\\cdot\\cdot\\cdot\\;,\\gamma_{t}$ and the random choice of strategy $\\pi$ .Since $\\begin{array}{r}{\\sum_{t=1}^{T}c(\\theta_{t},a_{t}^{\\pi},\\gamma_{t})\\le\\rho_{T}}\\end{array}$ always holds, we derive that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{a\\in A^{+}}C(\\theta_{t},a)p_{t}^{\\pi}(a)=\\mathbb{E}\\left[\\sum_{t=1}^{T}c(\\theta_{t},a_{t}^{\\pi},\\gamma_{t})\\mid\\theta_{1},\\cdot\\cdot\\cdot\\mid\\theta_{T}\\right]\\leq\\rho T,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which indicates that $\\{p_{t}^{\\pi}\\}_{t=1,\\cdots,T}$ is feasible to $V^{\\mathrm{Hyb}}$ under $\\theta_{1},\\cdot\\cdot\\cdot,\\theta_{T}$ . To the same reason, we also have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{a\\in A^{+}}R(\\theta_{t},a)p_{t}^{\\pi}(a)=\\mathbb{E}\\left[\\sum_{t=1}^{T}r(\\theta_{t},a_{t}^{\\pi},\\gamma_{t})\\ |\\ \\theta_{1},\\cdot\\cdot\\cdot\\ ,\\theta_{T}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "equals the conditional expected reward of strategy $\\pi$ Thus, since $V^{\\mathrm{Hyb}}$ is a maximization problem for any request trajectory, we conclude that $V^{\\mathrm{Hyb}^{\\bullet}}\\geq V^{\\mathrm{ON}}$ ", "page_idx": 15}, {"type": "text", "text": "It remains to show that when $V^{\\mathrm{FL}}$ .or $J(\\boldsymbol{\\rho})$ has a unique and degenerate solution, $V^{\\mathrm{FL}}-V^{\\mathrm{Hyb}}=$ $\\Omega({\\sqrt{T}})$ We first present a transformation of $V^{\\mathrm{Hyb}}$ We let ", "page_idx": 15}, {"type": "equation", "text": "$$\nx(\\theta):={\\frac{\\#[\\mathrm{appearance~of~}\\theta]}{T}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "be the random variable indicating the frequency of $\\theta$ when $\\theta$ is drawn $T$ times i.i.d. from $\\boldsymbol{\\mathcal{U}}$ . Obviously, the mean of $\\textbf{\\em x}$ is $\\textbf{\\em u}$ .We now demonstrate that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\mathrm{Hyb}}=T\\cdot\\mathbb{E}_{\\boldsymbol{x}}\\left[\\underset{\\boldsymbol{\\phi}:\\boldsymbol{\\Theta}\\times\\boldsymbol{A}^{+}\\rightarrow\\mathbb{R}}{\\operatorname*{max}}\\sum_{\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta}}x(\\boldsymbol{\\theta})\\underset{\\boldsymbol{a}\\in\\boldsymbol{A}^{+}}{\\sum}R(\\boldsymbol{\\theta},\\boldsymbol{a})\\boldsymbol{\\phi}(\\boldsymbol{\\theta},\\boldsymbol{a})\\right]\\,,}\\\\ &{\\quad\\quad\\quad\\mathrm{s.t.}\\quad\\displaystyle\\sum_{\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta}}x(\\boldsymbol{\\theta})\\sum_{\\boldsymbol{a}\\in\\boldsymbol{A}^{+}}C(\\boldsymbol{\\theta},\\boldsymbol{a})\\boldsymbol{\\phi}(\\boldsymbol{\\theta},\\boldsymbol{a})\\leq\\rho,}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\displaystyle\\sum_{\\boldsymbol{a}\\in\\boldsymbol{A}^{+}}\\boldsymbol{\\phi}(\\boldsymbol{\\theta},\\boldsymbol{a})\\leq1,\\quad\\forall\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta},}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\boldsymbol{\\phi}(\\boldsymbol{\\theta},\\boldsymbol{a})\\geq0,\\quad\\forall(\\boldsymbol{\\theta},\\boldsymbol{a})\\in\\boldsymbol{\\Theta}\\times\\boldsymbol{A}^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To see this, in form (2), it is not hard to see that conditioning on $\\theta_{1},\\cdot\\cdot\\cdot,\\theta_{T}$ , the value of the optimization is only related to the number of times that any $\\theta\\in\\Theta$ appears in the sequence, and irrelevant with their arriving order. Therefore, by taking an average, it is without loss of generality to suppose that $\\phi_{t_{1}}^{*}=\\phi_{t_{2}}^{*}$ as long as $\\theta_{t_{1}}=\\theta_{t_{2}}$ . Under such an observation, it is natural that (2) is equivalent to (3). ", "page_idx": 15}, {"type": "text", "text": "For convenience, we now recall the definition of $V^{\\mathrm{FL}}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\mathrm{FL}}=T\\cdot\\underset{\\phi:\\Theta\\times A^{+}\\to\\mathbb{R}}{\\operatorname*{max}}\\sum_{\\theta\\in\\Theta}u(\\theta)\\underset{a\\in A^{+}}{\\sum}R(\\theta,a)\\phi(\\theta,a),}\\\\ &{\\mathrm{s.t.}\\quad\\displaystyle\\sum_{\\theta\\in\\Theta}u(\\theta)\\sum_{a\\in A^{+}}C(\\theta,a)\\phi(\\theta,a)\\leq\\rho,}\\\\ &{\\qquad\\quad\\displaystyle\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\quad\\forall\\theta\\in\\Theta,}\\\\ &{\\qquad\\quad\\phi(\\theta,a)\\geq0,\\quad\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Sierksma [32], we know that when $J(\\boldsymbol{\\rho})$ has a unique and degenerate solution, then its dual form has multiple solutions. We then adopt the framework of Vera and Banerjee [37]. In particular, we let $\\mathbf{\\nabla}\\lambda\\geq\\mathbf{0}$ be the dual variable vector for the resource constraints, and ${\\pmb{\\mu}}\\geq{\\bf0}$ be the dual variable vector for the probability feasibility constraints. If we take $\\omega(\\theta)=\\mu(\\theta)/u(\\theta)$ , then the dual programming of $V^{\\mathrm{FL}^{\\star}}/T$ is the following as a function of $\\textbf{\\em u}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\mathcal{D}[Z(\\boldsymbol{u})]=\\underset{\\lambda,\\boldsymbol{\\omega}}{\\mathrm{min}}\\,\\rho^{\\top}\\lambda+\\boldsymbol{u}^{\\top}\\boldsymbol{\\omega},}\\\\ {\\mathrm{s.t.}\\quad\\lambda^{\\top}C(\\boldsymbol{\\theta},\\boldsymbol{a})+\\boldsymbol{\\omega}(\\boldsymbol{\\theta})\\geq R(\\boldsymbol{\\theta},\\boldsymbol{a}),\\quad\\forall(\\boldsymbol{\\theta},\\boldsymbol{a})\\in\\boldsymbol{\\Theta}\\times\\boldsymbol{A}^{+},}\\\\ {\\lambda\\geq\\mathbf{0},\\quad\\boldsymbol{\\omega}\\geq\\mathbf{0}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, suppose $(\\lambda^{1},\\omega^{1})$ and $(\\lambda^{2},\\omega^{2})$ are two different optimal solutions to $\\mathcal{D}[Z(u)]$ , which directly leads to $\\mathbf{\\hat{\\lambda}}^{1}\\neq\\mathbf{\\dot{\\lambda}}^{2}$ by the programming formation. We let $\\lambda^{\\prime}=\\lambda^{1}-\\lambda^{2}$ and $\\dot{\\omega}^{\\prime}=\\bar{\\omega}^{1}-\\omega^{2}$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\rho^{\\top}\\lambda^{1}+u^{\\top}\\omega^{1}=\\rho^{\\top}\\lambda^{2}+u^{\\top}\\omega^{2}\\implies\\rho^{\\top}\\lambda^{\\prime}+u^{\\top}\\omega^{\\prime}=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Further, notice that $(\\lambda^{1},\\omega^{1})$ and $(\\lambda^{2},\\omega^{2})$ are both feasible for $\\mathcal{D}[Z(x)]$ for any $\\textbf{\\em x}$ .Since $\\mathcal{D}[Z(x)]$ is a minimization problem, by a convex combination, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}[Z(x)]\\leq(\\rho^{\\top}\\lambda^{1}+x^{\\top}\\omega^{1})\\mathbf{1}[\\rho^{\\top}\\lambda^{\\prime}+x^{\\top}\\omega^{\\prime}\\leq0]+(\\rho^{\\top}\\lambda^{2}+x^{\\top}\\omega^{2})\\mathbf{1}[\\rho^{\\top}\\lambda^{\\prime}+x^{\\top}\\omega^{\\prime}>0].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Further, by optimality, we know that for any $\\textbf{\\em x}$ \uff0c ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{D}[Z(u)]=(\\rho^{\\top}\\lambda^{1}+u^{\\top}\\omega^{1})\\mathbf{1}[\\rho^{\\top}\\lambda^{\\prime}+x^{\\top}\\omega^{\\prime}\\leq0]+(\\rho^{\\top}\\lambda^{2}+u^{\\top}\\omega^{2})\\mathbf{1}[\\rho^{\\top}\\lambda^{\\prime}+x^{\\top}\\omega^{\\prime}>0].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, by weak duality, since $V^{\\mathrm{Hyb}}/T$ for any given $\\textbf{\\em x}$ is a maximization problem, we know from the above two equations that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(V^{\\mathrm{FL}}-V^{\\mathrm{Hyb}}\\right)\\!/\\varGamma}\\\\ &{\\geq\\mathcal{D}[Z(u)]-\\mathbb{E}_{x}\\left[\\mathcal{D}[Z(x)]\\right]}\\\\ &{\\geq\\mathbb{E}_{x}\\left[((u-x)^{\\top}\\omega^{1})\\mathbf{1}[\\rho^{\\top}\\lambda^{\\prime}+x^{\\top}\\omega^{\\prime}\\leq0]+((u-x)^{\\top}\\omega^{2})\\mathbf{1}[\\rho^{\\top}\\lambda^{\\prime}+x^{\\top}\\omega^{\\prime}>0]\\right]}\\\\ &{\\stackrel{\\mathrm{(a)}}{=}\\mathbb{E}_{x}\\left[((u-x)^{\\top}\\omega^{1})\\mathbf{1}[(u-x)^{\\top}\\omega^{\\prime}\\geq0]+((u-x)^{\\top}\\omega^{2})(1-\\mathbf{1}[(u-x)^{\\top}\\omega^{\\prime}\\geq0])\\right]}\\\\ &{\\stackrel{\\mathrm{(b)}}{=}\\mathbb{E}_{x}\\left[((u-x)^{\\top}\\omega^{\\prime})\\mathbf{1}[(u-x)^{\\top}\\omega^{\\prime}\\geq0]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, (a) is due to (4), and (b) is since the mean of $\\textbf{\\em x}$ is $\\textbf{\\em u}$ . Now, we let $\\xi=\\sqrt{T}({\\pmb u}-{\\pmb x})^{\\top}{\\pmb\\omega}^{\\prime}$ be the normalized scaled variable. By the Central Limit Theorem, $\\xi\\mathbf{1}[\\xi\\ge0]$ converges to a half-normal distribution, which has a constant expectation. Thus, we arrive at $V^{\\mathrm{FL}}-V^{\\mathrm{Hyb}}=\\Omega({\\sqrt{T}})$ , which finishes the proof. ", "page_idx": 16}, {"type": "text", "text": "C Missing Proofs in Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now give a proof of Theorem 3.1. The proof draws inspiration from that of Chen et al. [15], but significantly diverges in terms of the problem setting. ", "page_idx": 16}, {"type": "text", "text": "C.1.1 Regret Decomposition ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We start by presenting a regret decomposition approach, which stands on the dual viewpoint. We first recall the optimization problem $V^{\\mathrm{FL}}\\ '=T\\cdot J(\\hat{\\rho_{1}})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\rho_{1}):=\\underset{\\phi:\\Theta\\times A^{+}\\to\\mathbb R}{\\operatorname*{max}}~\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}R(\\theta,a)\\phi(\\theta,a)\\right],}\\\\ &{~~~~\\mathrm{s.t.}~~\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\displaystyle\\sum_{a\\in A^{+}}C(\\theta,a)\\phi(\\theta,a)\\right]\\leq\\rho_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\quad\\forall\\theta\\in\\Theta,}\\\\ &{\\phi(\\theta,a)\\geq0,\\quad\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall that $u(\\boldsymbol{\\theta})$ denotes the mass function of $\\boldsymbol{\\mathcal{U}}$ , then the above linear program can be expanded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\rho_{1}):=\\displaystyle\\operatorname*{max}_{\\phi:\\Theta\\times A^{+}\\to\\mathbb R}\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)R(\\theta,a)\\phi(\\theta,a),}\\\\ &{\\quad\\mathrm{~s.t.~}\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)C(\\theta,a)\\phi(\\theta,a)\\leq\\rho_{1},}\\\\ &{\\quad\\quad\\displaystyle\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\quad\\forall\\theta\\in\\Theta,}\\\\ &{\\quad\\quad\\displaystyle\\phi(\\theta,a)\\geq0,\\quad\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Nowlet $\\mathbf{\\nabla}\\lambda\\geq\\mathbf{0}$ be the dual vector for the consumption constraint and $\\{\\mu^{*}(\\theta)\\}_{\\theta\\in\\Theta}\\geq\\mathbf{0}$ be the dual variables for the action distribution constraint. By the strong duality of linear program, there is an optimal dual variable tuple $(\\lambda^{*},\\{\\mu^{*}(\\theta)\\}_{\\theta\\in\\Theta})\\geq\\dot{\\mathbf{0}}$ such that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\rho_{1})=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\phi_{1}^{*}(\\theta,a)+(\\lambda^{*})^{\\top}\\rho_{1}+\\displaystyle\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)}\\\\ &{\\qquad=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)\\phi_{1}^{*}(\\theta,a)+(\\lambda^{*})^{\\top}\\rho_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here $\\phi_{1}^{*}$ is the optimal solution to $J(\\rho_{1})$ . With (5), we have the following lemma for regret decomposition. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. For any stopping time $T_{e}\\leq T_{0}$ adapted to the process $\\{B_{t}\\}\\,^{\\prime}s,$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\mathrm{FL}}-R e w}\\\\ &{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{e}}\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\left(\\phi_{1}^{*}(\\theta,a)-\\widehat{\\phi}_{t}^{*}(\\theta,a)\\right)\\right]}\\\\ &{\\ +\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{e}}\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)\\left(1-\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)\\right)\\right]}\\\\ &{\\ +\\left(\\lambda^{*}\\right)^{\\top}\\mathbb{E}\\left[B_{T_{e}+1}\\right]+\\displaystyle\\operatorname*{max}_{\\theta\\in\\Theta,a\\in A^{+}}\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)\\cdot\\mathbb{E}\\left[T-T_{e}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The proof of Lemma C.1 is deferred to Appendix C.2. We now give a brief explanation on this result. The first two terms in (6) depicts the gap between the choice of Algorithm 1 and the optimal decision. This is apparent for the first term. For the second term, we should notice that by complementary slackness,for each $\\theta\\in\\Theta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu^{*}(\\theta)\\cdot\\left(1-\\sum_{a\\in A^{+}}\\phi_{1}^{*}(\\theta,a)\\right)=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, the second term in (6) is bounded if $\\widehat{\\phi}_{t}^{*}$ is close to $\\phi_{1}^{*}$ ", "page_idx": 17}, {"type": "text", "text": "On the other hand, the last two terms are closely related to the choice of stopping time $T_{e}$ and the consumption behavior of Algorithm 1. Intuitively, if $T_{e}$ is sufficiently close to $T$ , then $\\mathbb{E}[T-T_{e}]$ should be appropriately bounded. Nevertheless, if the algorithm spends the resources too fast, then such a sufficiently large $T_{e}$ would be impossible. Conversely, if the resources are consumed substantially slower than the optimal, then the term $\\mathbb{E}[B_{T_{e}+1}]$ , the remaining resources at the stopping time, would be unbounded. ", "page_idx": 17}, {"type": "text", "text": "In the following, we will deal with these two parts correspondingly. A crux to the analysis is to pick a satisfying stopping time $T_{e}$ , which we will first cover. ", "page_idx": 17}, {"type": "text", "text": "C.1.2  The Gap to Optimal Decision ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first give a realization of the stopping time $T_{e}$ . With Proposition 3.1 in hand, we can derive that when condition (1) is met, it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\left(\\phi_{1}^{*}(\\theta,a)-\\widehat\\phi_{t}^{*}(\\theta,a)\\right)=0,}}\\\\ {{\\displaystyle\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)\\left(1-\\displaystyle\\sum_{a\\in A^{+}}\\widehat\\phi_{t}^{*}(\\theta,a)\\right)=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To see these, notice that by the dual feasibility of $J(\\rho_{1})$ , we have $u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-$ $\\mu^{*}(\\theta)\\leq0$ . When $u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)<0,$ by primal optimality, $\\phi_{1}^{*}(\\theta,a)=0$ and thus $(\\theta,a)$ is non-basic for $J(\\rho_{1})$ . By Proposition 3.1, $(\\theta,a)$ is also non-basic for $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ and $\\widehat{\\phi}_{t}^{*}(\\theta,a)=0$ holds as well. This finishes the deduction of (7). A similar reasoning on binding constraints would help us achieve (8), which we omit here. ", "page_idx": 18}, {"type": "text", "text": "As the above goes, it is then natural for us to define $T_{e}$ the stopping time in our analysis as follows: ", "page_idx": 18}, {"type": "text", "text": "$T_{e}:=\\operatorname*{min}\\{T_{0},\\operatorname*{min}\\{t:\\operatorname*{max}\\{\\|\\rho_{1}|_{\\mathcal{S}}-\\rho_{t}|_{\\mathcal{S}}\\|_{\\infty},\\operatorname*{max}\\{\\rho_{1}|_{\\mathcal{T}}-\\rho_{t}|_{\\mathcal{T}}\\}\\}>D\\}-1\\},$ (9) where $T_{0}$ is the stopping time of Algorithm 1. With the definition, we always have $\\operatorname*{max}\\{\\|\\rho_{1}\\|_{S}-$ $\\rho_{t}|_{S}|\\big|_{\\infty},\\operatorname*{max}\\{\\rho_{1}|_{\\mathcal{T}}-\\rho_{t}|_{\\mathcal{T}}\\}\\}\\leq D$ when $t\\leq T_{e}$ . What we are left is to bound the situation when $\\mathrm{max}\\{\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty},\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\}>D$ for $1\\leq t\\leq T_{e}$ . In total, we arrive at the following result for this part, with the proof given in Appendix C.4: ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. Under Assumption 3.1, with full information feedback, we have when $T\\rightarrow\\infty$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T_{c}}\\sum_{\\substack{\\theta\\in\\Theta,a\\in A^{+}}}\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\left(\\phi_{1}^{*}(\\theta,a)-\\widehat{\\phi}_{t}^{*}(\\theta,a)\\right)\\right]}\\\\ {\\displaystyle=O\\left(\\frac{k}{D^{2}}\\right),\\quad}\\\\ {\\displaystyle}&{\\mathbb{E}\\left[\\sum_{t=1}^{T_{c}}\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)\\left(1-\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)\\right)\\right]=O\\left(\\frac{k}{D^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We are now only left to bound the last two terms in (6)) ", "page_idx": 18}, {"type": "text", "text": "C.1.3  The Gap to Optimal Consumption ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As presented in (6), we now bound the remaining two terms, respectively $\\mathbb{E}[B_{T_{e}+1}]$ and $\\mathbb{E}[T-T_{e}]$ for $T_{e}$ defined in (9). It turns out that these two terms are closely related. Due to this observation, we would first bound $(\\lambda^{*})^{\\top}\\cdot\\mathbb{E}[B_{T_{e}+1}]$ by $\\mathbb{E}[T-T_{e}]$ , and then bound $\\mathbb{E}[T-T_{e}]$ ", "page_idx": 18}, {"type": "text", "text": "Now by the strong duality of $J(\\rho_{1})$ , we know that complementary slackness holds, that is $\\lambda^{*}\\lvert\\boldsymbol{\\tau}=\\mathbf{0}$ We therefore have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\lambda^{*})^{\\top}{\\mathbb{E}}\\left[B_{T_{\\epsilon}+1}\\right]\\leq(\\lambda^{*})^{\\top}{\\mathbb{E}}\\left[B_{T_{\\epsilon}}\\right]=(\\lambda^{*}|_{\\mathcal{S}})^{\\top}{\\mathbb{E}}\\left[B_{T_{\\epsilon}}|_{\\mathcal{S}}\\right]=(\\lambda^{*}|_{\\mathcal{S}})^{\\top}{\\mathbb{E}}\\left[(T-T_{e}+1)\\rho_{T_{\\epsilon}}|_{\\mathcal{S}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\stackrel{{\\mathrm{(a)}}}{\\leq}n(\\rho^{\\operatorname*{max}}+D)\\|\\lambda^{*}\\|_{\\infty}\\cdot{\\mathbb{E}}\\left[T-T_{e}+1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the above, recall that $\\rho^{\\mathrm{max}}$ denotes the maximum coordinate of $\\rho_{1}$ , and $D$ is specified in Proposition 3.1. Consequently, (a) is due to the definition of $T_{e}$ and that $\\|\\pmb{\\rho}_{1}+D\\mathbf{1}\\|_{\\infty}\\leq\\rho^{\\operatorname*{max}}+D$ ", "page_idx": 18}, {"type": "text", "text": "We are left to bound $\\mathbb{E}[T-T_{e}]$ . Nevertheless, this part would be rather technical and involved.   \nTherefore we defer the analysis to Appendix C.5, and only give the final bounds. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.3. Under Assumption 3.1, with full information feedback, we have when $T\\rightarrow\\infty$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\lambda^{*})^{\\top}{\\mathbb{E}}\\left[B_{T_{e}+1}\\right]+\\operatorname*{max}_{\\theta\\in\\Theta,a\\in A^{+}}\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}\\cdot C(\\theta,a)\\right){\\mathbb{E}}\\left[T-T_{e}\\right]=O\\left(\\frac{n^{2}}{D^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining Lemmas C.1 to C.3, we arrive at Theorem 3.1. ", "page_idx": 18}, {"type": "text", "text": "C.2  Proof of Lemma C.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proof is obtained by the following set of (in)equalities ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\tau_{-}\\int_{(\\tau_{-}^{h})}-\\tau_{-}^{h}\\Big[\\frac{1}{h_{w}^{3}}\\Big(\\eta_{0}h_{w}\\Big_{*}\\Big_{0}\\Big)\\Big]}\\\\ &{\\stackrel{(a)}{=}\\mathbb{E}\\tau_{-}\\int_{(\\tau_{-}^{h})}-\\sum_{s=0}^{h}\\Big[\\frac{1}{h_{w}^{3}}\\sum_{i=1,i=1}^{h}\\alpha_{i}\\Big(h_{w}^{3}\\Big_{*}\\Big(h_{w}^{3}\\Big)\\Big(h_{w}^{4}\\Big)\\Big)}\\\\ &{\\stackrel{(b)}{=}\\Big(\\sum_{s=0}^{h}\\Big(\\eta_{0}h_{w}^{3}\\Big_{*}\\alpha_{1}-(h_{w}^{3}\\Big)^{s}\\Big(\\tau_{-}^{h}\\Big_{1}\\Big)-h_{w}^{s}\\Big)\\Big)\\zeta(s)+\\Big(X_{s}^{*}\\Big)r_{1}+\\sum_{s=0}^{h}\\alpha_{1}^{*}\\Big(h_{w}^{3}\\Big)}\\\\ &{-\\sum_{s=1}^{h}\\sum_{i=1,i=1}^{h}\\alpha_{i}\\Big(h_{w}^{3}\\Big_{*}\\alpha_{1}^{*}\\Big(h_{w}^{4}\\Big_{*}\\Big)\\Big(h_{w}^{4}\\Big)\\Big)}\\\\ &{\\stackrel{(b)}{=}\\mathbb{E}\\Bigg[\\frac{\\sum_{s=0}^{h}\\sum_{i=1}^{h}\\alpha_{i}\\Big(h_{w}^{3}\\Big(h_{w}^{3}\\Big(h_{w}^{3}\\Big)-(X_{s}^{*}\\Big)^{s}\\Big(\\tau_{-}^{h}\\Big)+\\Big)-h_{w}^{s}\\Big(h_{w}^{3}\\Big)\\Big(h_{w}^{4}\\Big(h_{w}^{3}\\Big)-\\tilde{h}_{w}^{4}\\Big(h_{w}^{3}\\Big)\\Big)\\Bigg]}\\\\ &{+\\sum_{s=1}^{h}\\sum_{i=1}^{h}\\alpha_{i}\\Big(h_{w}^{3}\\Big(\\tau_{-}^{h}\\Big(h_{w}^{2}\\Big(h_{w}^{3}\\Big)\\Big)\\Big)+\\Bigg(\\sum_{s=0}^{h}\\Big(\\eta_{0}h_{\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the above set of derivations, (a) holds since $T_{0}\\geq T_{e}$ , (b) is due to Optional Stopping Theorem since $T_{e}$ is a stopping time, (c) is by the strong duality of $J(\\rho_{1})$ as given by (5), (d) establishes by rearranging terms. At last, for (e), the diminishing term is by strong duality, the transformation from the fourth term in (d) to the third term in (e) is derived by another application of Optional Stopping Theorem on the accumulated consumption vector, and for the last term, the upper bound is achieved since $\\begin{array}{r}{\\sum_{a\\in A^{+}}\\phi_{1}^{*}(\\theta,a)\\leq1}\\end{array}$ for any $\\theta\\in\\Theta$ and $\\textstyle\\sum_{\\theta\\in\\Theta}u(\\theta)=1$ ", "page_idx": 19}, {"type": "text", "text": "C.3 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We will apply the stability result in Chen et al. [15] as an intermediate to prove our version. As given, weknowthat $J(\\rho_{1})$ and $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ has the same set of basic/non-basic variables and binding/non", "page_idx": 19}, {"type": "text", "text": "binding constraints as long as the following conditions hold for some constant ${\\cal D}_{0}>0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\left(u(\\theta)\\displaystyle\\sum_{\\gamma}v(\\gamma)r(\\theta,a,\\gamma)-\\widehat{u}_{t}(\\theta)\\displaystyle\\sum_{\\gamma}\\widehat{v}_{t}(\\gamma)r(\\theta,a,\\gamma)\\right)_{(\\theta,a)\\in\\Theta\\times A^{+}}\\right\\rVert_{\\infty}\\le D_{0},}\\\\ {\\left\\lVert\\left(u(\\theta)\\displaystyle\\sum_{\\gamma}v(\\gamma)c^{i}(\\theta,a,\\gamma)-\\widehat{u}_{t}(\\theta)\\displaystyle\\sum_{\\gamma}\\widehat{v}_{t}(\\gamma)c^{i}(\\theta,a,\\gamma)\\right)_{(\\theta,a)\\in\\Theta\\times A^{+}}\\right\\rVert_{\\infty}\\le D_{0},\\quad\\forall i\\in[n],}\\\\ {\\lVert\\rho_{1}\\vert_{\\infty}-\\rho_{t}\\vert_{S}\\vert_{\\infty}\\le D_{0},\\quad\\operatorname*{max}\\left\\{\\rho_{1}\\vert_{T}-\\rho_{t}\\vert_{T}\\right\\}\\le D_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, by a standard insertion technique, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad u(\\theta)\\displaystyle\\sum_{\\gamma}v(\\gamma)r(\\theta,a,\\gamma)-\\widehat{u}_{t}(\\theta)\\sum_{\\gamma}\\widehat{v}_{t}(\\gamma)r(\\theta,a,\\gamma)}\\\\ &{=(u(\\theta)-\\widehat{u}_{t}(\\theta))\\displaystyle\\sum_{\\gamma}v(\\gamma)r(\\theta,a,\\gamma)+\\widehat{u}_{t}(\\theta)\\displaystyle\\sum_{\\gamma}(v(\\gamma)-\\widehat{v}_{t}(\\gamma))r(\\theta,a,\\gamma)}\\\\ &{\\overset{\\mathrm{(a)}}{\\leq}\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For (a), the first term is bounded since $r(\\theta,a,\\gamma)\\leq1$ and $\\textstyle\\sum_{\\gamma}v(\\gamma)=1$ . The second term is similarly bounded as $\\widehat{u}_{t}(\\theta)\\leq1$ . Therefore, we let $D=D_{0}/2$ , then when we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}\\le D,\\quad\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\le D,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "the first condition in (11) is met. An almost identical reasoning also holds for the second condition in (11). Consequently we finish the proof of the lemma. ", "page_idx": 20}, {"type": "text", "text": "C.4  Proof of Lemma C.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall that we are going to prove that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T_{c}}\\sum_{\\substack{\\theta\\in\\Theta,a\\in A^{+}}}\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\left(\\phi_{1}^{*}(\\theta,a)-\\widehat\\phi_{t}^{*}(\\theta,a)\\right)\\right]}\\\\ {\\displaystyle=O\\left(\\frac{k}{D^{2}}\\right),\\quad}\\\\ {\\displaystyle}&{\\mathbb{E}\\left[\\sum_{t=1}^{T_{c}}\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)\\left(1-\\sum_{a\\in A^{+}}\\widehat\\phi_{t}^{*}(\\theta,a)\\right)\\right]=O\\left(\\frac{k}{D^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "when $T\\to\\infty$ under Assumption 3.1. For simplicity, we give the following abbreviations: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{t}:=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in a^{+}}\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\left(\\phi_{1}^{*}(\\theta,a)-\\widehat\\phi_{t}^{*}(\\theta,a)\\right),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle Q_{t}:=\\displaystyle\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)\\left(1-\\displaystyle\\sum_{a\\in A^{+}}\\widehat\\phi_{t}^{*}(\\theta,a)\\right),}\\\\ &{\\displaystyle\\mathcal E_{u,t}:=[\\|(u(\\theta)-\\widehat u_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}\\leq D],\\quad\\mathcal E_{v,t}:=[\\|(v(\\gamma)-\\widehat v_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\leq D].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On this end, we first utilize Proposition 3.1 to show that when condition (1) holds, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nP_{t}=Q_{t}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Specifically, for $P_{t}$ , by the dual feasibility of $J(\\rho_{1})$ , we have $u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)\\,-$ $\\mu^{*}(\\theta)\\leq0$ .When $u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)<0$ , by primal optimality, $\\phi_{1}^{*}(\\theta,a)=0$ and thus $(\\theta,a)$ is non-basic for $J(\\rho_{1})$ . By Proposition 3.1, $(\\theta,a)$ is also non-basic for $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ and $\\widehat{\\phi}_{t}^{*}(\\theta,a)=0$ holds as well. In conjunction with the case that $u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-$ $\\mu^{*}(\\theta)=0$ , we obtain that $P_{t}=0$ ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "For $Q_{t}$ , notice that we have $\\mu^{*}(\\theta)\\geq0$ for any $\\theta\\in\\Theta$ . The case that $\\mu^{*}(\\theta)=0$ , again, does not contribute to the total sum. When $\\mu^{*}(\\theta)>0$ , by complementary slackness, $\\begin{array}{r}{\\sum_{a\\in A^{+}}\\phi_{1}^{*}(\\theta,a)=1}\\end{array}$ i.e., $\\theta$ is a binding constraint for $J(\\rho_{1})$ . This, by Proposition 3.1, implies that $\\theta$ is also binding for $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ , which shows that the second term is also zero. ", "page_idx": 21}, {"type": "text", "text": "With the above, it remains to consider the situation that condition (1) does not hold when $t\\leq T_{e}$ ,or in other words, $\\mathcal{E}_{u,t}\\wedge\\mathcal{E}_{v,t}$ does not hold. Note that $P_{t}\\leq1$ and $Q_{t}\\leq1$ always hold. Thus, we only need to bound the probability that $\\neg(\\mathcal{E}_{u,t}\\wedge\\mathcal{E}_{v,t})$ . By a union bound, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\neg(\\mathcal{E}_{u,t}\\land\\mathcal{E}_{v,t})]=\\mathrm{Pr}[\\neg\\mathcal{E}_{u,t}\\lor\\neg\\mathcal{E}_{v,t}]\\leq\\mathrm{Pr}[\\neg\\mathcal{E}_{u,t}]+\\mathrm{Pr}[\\neg\\mathcal{E}_{v,t}].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the first term above, we apply the Hoeffding's inequality and a union bound to derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}[\\neg\\mathcal{E}_{u,t}]=\\operatorname*{Pr}[\\Vert(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\Vert_{\\infty}>D]\\le2k\\exp\\left(-2D^{2}(t-1)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Whereas for the second term, we use the concentration result in Weissman et al. [40] to derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}[\\neg{\\mathcal E}_{v,t}]=\\operatorname*{Pr}[\\Vert(v(\\gamma)-\\widehat v_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\Vert_{1}>D]\\le\\left(2^{|\\Gamma|}-2\\right)\\exp\\left(-D^{2}(t-1)/2\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Synthesizing the above all, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[P_{t}]=\\mathbb{E}[P_{t}\\ |\\ \\mathcal{E}_{u,t}\\wedge\\mathcal{E}_{v,t}]\\cdot\\operatorname*{Pr}[\\mathcal{E}_{u,t}\\wedge\\mathcal{E}_{v,t}]+\\mathbb{E}[P_{t}\\ |\\ \\neg(\\mathcal{E}_{u,t}\\wedge\\mathcal{E}_{v,t})]\\cdot\\operatorname*{Pr}[\\neg(\\mathcal{E}_{u,t}\\wedge\\mathcal{E}_{v,t})]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq0+1\\cdot\\operatorname*{Pr}[\\neg(\\mathcal{E}_{u,t}\\wedge\\mathcal{E}_{v,t})]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\leq2k\\exp\\left(-2D^{2}(t-1)\\right)+\\left(2^{|\\Gamma|}-2\\right)\\exp\\left(-D^{2}(t-1)/2\\right),}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\mathbb{E}[Q_{t}]\\leq2k\\exp\\left(-2D^{2}(t-1)\\right)+\\left(2^{|\\Gamma|}-2\\right)\\exp\\left(-D^{2}(t-1)/2\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summing (13) and (14) from 1 to $T_{e}$ , we achieve that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T_{e}}P_{t}\\right],\\mathbb{E}\\left[\\sum_{t=1}^{T_{e}}Q_{t}\\right]\\right\\}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\left(2k\\exp\\left(-2D^{2}(t-1)\\right)+\\left(2^{|\\Gamma|}-2\\right)\\exp\\left(-D^{2}(t-1)/2\\right)\\right)}\\\\ &{\\leq\\displaystyle\\frac{2k}{1-\\exp\\left(-2D^{2}\\right)}+\\frac{2^{|\\Gamma|}-2}{1-\\exp\\left(-D^{2}/2\\right)}=O\\left(\\frac{k}{D^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which conclude the proof of the lemma. ", "page_idx": 21}, {"type": "text", "text": "C.5 Proof of Lemma C.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As implied by (10), the proof of this lemma reduces to bound $\\mathbb{E}[T-T_{e}]$ , i.e., showing that $T_{e}$ is sufficiently close to $T$ . On this side, we first recall the definition of $T_{e}$ in (9): ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{e}:=\\operatorname*{min}\\{T_{0},\\operatorname*{min}\\{t:\\operatorname*{max}\\{\\|\\rho_{1}|_{\\mathcal{S}}-\\rho_{t}|_{\\mathcal{S}}\\|_{\\infty},\\operatorname*{max}\\{\\rho_{1}|_{\\mathcal{T}}-\\rho_{t}|_{\\mathcal{T}}\\}\\}>D\\}-1\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $T_{0}$ is the stopping time of Algorithm 1, and $\\boldsymbol{S}$ and $\\tau$ correspondingly represent the set of binding/non-binding resource constraints in LP $J(\\rho_{1})$ . For simplicity, we define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}(\\rho_{1},D,S):=\\{\\kappa:\\operatorname*{max}\\{||\\rho_{1}|_{S}-\\kappa|_{S}\\|_{\\infty},\\operatorname*{max}\\{\\rho_{1}|_{\\mathcal{T}}-\\kappa|_{\\mathcal{T}}\\}\\}\\leq D\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is without loss of generality to suppose that $D<\\rho^{\\mathrm{min}}$ .We let ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{D}:=\\operatorname*{min}\\{t:\\rho_{t}\\notin\\mathcal{N}(\\rho_{1},D,S)\\}-1,\\quad T_{-}=\\lfloor T+1-1/(\\rho^{\\operatorname*{min}}-D)\\rfloor.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We show that if $t\\leq T_{-}$ and $t\\leq T_{D}$ , then $t\\leq T_{e}$ . In fact, under the condition, we derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{t}\\geq(T-t+1)(\\rho_{1}-D\\mathbf{1})\\geq\\frac{1}{\\rho^{\\operatorname*{min}}-D}(\\rho_{1}-D\\mathbf{1})\\geq\\mathbf{1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that $t\\leq T_{0}$ , and therefore $t\\leq T_{e}$ . As a result, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[T_{e}\\right]=\\sum_{t=1}^{T}\\operatorname*{Pr}\\left[T_{e}\\ge t\\right]\\ge\\sum_{t=1}^{T_{-}}\\operatorname*{Pr}\\left[T_{e}\\ge t\\right]\\ge\\sum_{t=1}^{T_{-}}\\operatorname*{Pr}\\left[T_{D}\\ge t\\right]=T_{-}-\\sum_{t=1}^{T_{-}}\\operatorname*{Pr}\\left[t>T_{D}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Before we continue to bound (15), we first give an observation on the dynamics of $\\rho_{t}$ .By the update process of the budget, we have for any $t\\geq1$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{t+1}=B_{t}-c_{t}\\implies\\rho_{t+1}(T-t)=\\rho_{t}(T-t+1)-c_{t}}\\\\ &{\\qquad\\qquad\\qquad\\implies\\rho_{t+1}=\\rho_{t}+\\displaystyle\\frac{\\rho_{t}-c_{t}}{T-t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now let ", "page_idx": 22}, {"type": "equation", "text": "$$\nM_{t}^{C}:=\\frac{\\rho_{t}-\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)C(\\theta,a)\\right]}{T-t},\\quad N_{t}^{C}:=\\frac{\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)C(\\theta,a)\\right]-c_{t}}{T-t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We then have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{\\rho}_{t+1}-\\pmb{\\rho}_{t}=\\frac{\\pmb{\\rho}_{t}-\\pmb{c}_{t}}{T-t}=M_{t}^{C}+\\pmb{N}_{t}^{C}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now define an auxiliary proces that benefits the analysis. Specifically, for $t\\in[T]$ let ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{\\rho}_{t}:=\\left\\{\\begin{array}{l l}{\\rho_{t},}&{t\\leq T_{D};}\\\\ {\\rho_{T_{D}},}&{t>T_{D}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\pmb{\\rho}}_{t+1}-\\tilde{\\pmb{\\rho}}_{t}=\\left\\{\\begin{array}{l l}{\\pmb{M}_{t}^{C}+\\pmb{N}_{t}^{C},}&{t\\leq T_{D};}\\\\ {0,}&{t>T_{D}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We further define the following two auxiliary variables for $t\\in[T]$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{M}_{t}^{C}:=\\left\\{\\!\\!\\begin{array}{l l}{M_{t}^{C},}&{t\\leq T_{D};}\\\\ {0,}&{t>T_{D}.}\\end{array}\\!\\!\\right.,\\quad\\widetilde{N}_{t}^{C}:=\\left\\{\\!\\!\\begin{array}{l l}{N_{t}^{C},}&{t\\leq T_{D};}\\\\ {0,}&{t>T_{D}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As a result, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{\\rho}}_{t+1}-\\widetilde{\\pmb{\\rho}}_{t}=\\widetilde{\\pmb{M}}_{t}^{C}+\\widetilde{\\pmb{N}}_{t}^{C}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we come back to (15). Notice that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}\\left[t>T_{D}\\right]}\\\\ &{=\\operatorname*{Pr}\\left[\\rho_{s}\\notin\\!\\mathcal{N}(\\rho_{1},D,S)\\mathrm{~for~some~}s\\le t\\right]=\\operatorname*{Pr}\\left[\\tilde{\\rho}_{t}\\notin\\!\\mathcal{N}(\\rho_{1},D,S)\\right]}\\\\ &{\\le\\operatorname*{Pr}\\left[\\left\\Vert\\displaystyle\\sum_{\\tau=1}^{t-1}\\left(\\widetilde{M}_{\\tau}^{C}+\\widetilde{N}_{\\tau}^{C}\\right)|_{S}\\right\\Vert_{\\infty}>D\\mathrm{~or~min~}\\displaystyle\\sum_{\\tau=1}^{t-1}\\left(\\widetilde{M}_{\\tau}^{C}+\\widetilde{N}_{\\tau}^{C}\\right)|_{\\tau}<-D\\right]}\\\\ &{\\le\\operatorname*{Pr}\\left[\\left\\Vert\\displaystyle\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}|_{S}\\right\\Vert_{\\infty}>D/2\\mathrm{~or~min~}\\displaystyle\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}|_{\\tau}<-D/2\\right]+\\operatorname*{Pr}\\left[\\left\\Vert\\displaystyle\\sum_{\\tau=1}^{t-1}\\widetilde{N}_{\\tau}^{C}\\right\\Vert_{\\infty}\\ge D/2\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the second term in (18), we observe that each entry of $\\{\\sum_{\\tau<t}\\widetilde{N}_{\\tau}^{C}\\}_{t}$ is a martingale with the absolute value of the $\\tau$ -th increment bounded by $1/(T-\\tau)$ Since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t-1}\\frac{1}{(T-\\tau)^{2}}\\leq\\frac{1}{T-t},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "by applying the Azuma-Hoeffding inequality and a union bound, we achieve that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\left\\lVert\\sum_{\\tau=1}^{t-1}\\widetilde{N}_{\\tau}^{C}\\right\\rVert_{\\infty}\\geq D/2\\right]\\leq2n\\exp\\left(-\\frac{(T-t)D^{2}}{8}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now come back to the first term in (18),for any $\\{D_{1},\\cdot\\cdot\\cdot,D_{t-1}\\}$ such that $\\begin{array}{r l}{\\sum_{\\tau=1}^{t-1}D_{\\tau}/(T-\\tau)\\leq}&{{}}\\end{array}$ $D/2$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\{\\left\\|\\displaystyle\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\vert_{s}\\right\\|_{\\infty}>D/2\\mathrm{~or~}\\operatorname*{min}_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\vert_{\\tau}<-D/2\\right\\}}\\\\ &{\\Longrightarrow\\left\\{\\left\\|\\widetilde{M}_{\\tau}^{C}\\vert_{s}\\right\\|_{\\infty}>\\displaystyle\\frac{D_{\\tau}}{T-\\tau}\\mathrm{~or~}\\operatorname*{min}\\widetilde{M}_{\\tau}^{C}\\vert_{\\tau}<-\\displaystyle\\frac{D_{\\tau}}{T-\\tau}\\right\\}\\mathrm{~for~some~}\\tau\\in[T-1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\dot{\\cdot}_{\\tau}(D_{\\tau}):=\\left(\\left\\|M_{\\tau}^{C}\\right\\|_{\\mathcal{S}}\\right\\|_{\\infty}\\leq\\frac{D_{\\tau}}{T-\\tau}\\right)\\wedge\\left(\\operatorname*{min}{M_{\\tau}^{C}}\\big|_{\\mathcal{T}}\\geq-\\frac{D_{\\tau}}{T-\\tau}\\right)\\mathrm{~holds~for~}\\forall\\rho_{\\tau}\\in\\mathcal{N}(\\rho_{1},D,\\mathcal{S}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\widetilde{M}_{\\tau}^{C}\\neq0$ only when $t\\leq T_{D}$ , i.e., $\\rho_{t}\\in\\mathcal{N}(\\rho_{1},D,S)$ , by the defnition of $\\mathcal{E}_{\\tau}(D_{\\tau})$ , we have the following claim: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\left\\lVert\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\right\\rVert_{S}\\right\\rVert_{\\infty}>D/2\\;\\mathrm{or~}\\operatorname*{min}\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\big|_{\\tau}<-D/2\\right\\rangle\\subseteq\\bigcup_{\\tau=1}^{t-1}\\neg\\mathcal{E}_{\\tau}(D_{\\tau}),\\quad\\forall\\sum_{\\tau=1}^{t-1}\\frac{D_{\\tau}}{T-\\tau}\\le D/2.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, we forward to bound $\\operatorname*{Pr}[\\neg\\mathcal{E}_{\\tau}(D_{\\tau})]$ for a suitable choice of $\\{D_{\\tau}\\}_{1\\leq\\tau\\leq T}$ . Recall that we have defined events $\\mathcal{E}_{u,\\tau}$ and $\\mathcal{E}_{v,\\tau}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{E}_{u,\\tau}:=[\\Vert(u(\\theta)-\\widehat{u}_{\\tau}(\\theta))_{\\theta\\in\\Theta}\\Vert_{\\infty}\\leq D],\\quad\\mathcal{E}_{v,\\tau}:=[\\Vert(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\Vert_{1}\\leq D].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We have the following lemma, which we are going to prove in Appendix C.6: ", "page_idx": 23}, {"type": "text", "text": "Lemma C.4. When $\\pmb{\\rho}_{\\tau}\\in\\mathcal{N}(\\pmb{\\rho}_{1},D,S)$ and $\\mathcal{E}_{u,\\tau}\\wedge\\mathcal{E}_{v,\\tau}$ hold, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(T-\\tau)\\left\\lVert M_{\\tau}^{C}\\right\\rVert_{S}\\right\\rVert_{\\infty}\\leq\\left\\lVert(u(\\theta)-\\widehat{u}_{\\tau}(\\theta))_{\\theta\\in\\Theta}\\right\\rVert_{1}+\\left\\lVert(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\right\\rVert_{1},}\\\\ &{(T-\\tau)\\operatorname*{min}M_{\\tau}^{C}\\big|_{\\tau}\\geq-\\lVert(u(\\theta)-\\widehat{u}_{\\tau}(\\theta))_{\\theta\\in\\Theta}\\rVert_{1}-\\lVert(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\rVert_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Further, t is clear that $\\left(T-\\tau\\right)\\left\\|M_{\\tau}^{C}\\right\\|_{S}\\left\\|_{\\infty}\\leq1$ and $(T-\\tau)M_{\\tau}^{C}\\vert_{\\tau}\\geq-1$ holds. Inspired by the above observations, we let the series of $D_{1},\\cdot\\cdot\\cdot,D_{T-1}$ be the following form: ", "page_idx": 23}, {"type": "equation", "text": "$$\nD_{\\tau}=\\left\\{\\begin{array}{l l}{1,}&{\\tau\\leq\\eta T;}\\\\ {(\\tau-1)^{-1/4},}&{\\tau>\\eta T,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\eta\\in(0,1)$ is a constant to be specified. We need to satisfy the following constraints: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T-1}\\frac{D_{t}}{T-t}\\le D/2,\\quad(\\eta T)^{-1/4}<D.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, the first constraint is instructed by (19), and the second is to guarantee that when $\\parallel(u(\\theta)-$ $\\widehat{u}_{\\tau}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}<(\\tau-1)^{-1/4}$ for $\\tau>\\eta T,\\mathcal{E}_{u,\\tau}\\wedge\\mathcal{E}_{v,\\tau}$ naturally holds, and therefore we can apply Lemma C.4. For the first one, we notice that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{T-1}\\frac{D_{\\tau}}{T-\\tau}=\\sum_{\\tau=1}^{\\eta T}\\frac{1}{T-\\tau}+\\sum_{\\tau=\\eta T+1}^{T-1}\\frac{1}{(T-\\tau)(\\tau-1)^{1/4}}\\le\\log\\frac{T-1}{(1-\\eta)T-1}+\\frac{\\log T}{(\\eta T)^{1/4}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, for some $\\eta$ such that $\\log(1-\\eta)\\,\\ge\\,-D/4,\\,\\sum_{t=1}^{T}D_{t}/(T-t)\\le\\,D/2$ establishes for sufficiently large $T\\gg1$ , and the second constraint is also satisfied. ", "page_idx": 23}, {"type": "text", "text": "We are now prepared to bound $\\operatorname*{Pr}[\\neg\\mathcal{E}_{\\tau}(D_{\\tau})]$ for the $\\{D_{\\tau}\\}$ we just proposed. To start with, when $\\tau\\leq\\eta T,\\mathcal{E}_{\\tau}(D_{\\tau})$ always holds, thus $\\operatorname*{Pr}[\\lnot{\\mathcal{E}}_{\\tau}(D_{\\tau})]=0$ .When $\\tau>\\eta T$ ,since $\\tau^{-1/4}/2<D$ by Hoeffding's inequality and union bound, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\neg\\mathcal{E}_{\\tau}(D_{\\tau})]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\mathrm{(a)}}{\\leq}\\operatorname*{Pr}\\left[\\|(u(\\theta)-\\widehat{u}_{\\tau}(\\theta))\\theta\\epsilon\\Theta\\|_{1}\\leq(\\tau-1)^{-1/4}/2\\right]+\\operatorname*{Pr}\\left[\\|(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\leq(\\tau-1)^{-1/4}/2\\right]}\\\\ &{\\leq2k\\exp\\left(-\\frac{(\\tau-1)^{1/2}}{8k^{2}}\\right)+2|\\Gamma|\\exp\\left(-\\frac{(\\tau-1)^{1/2}}{8|\\Gamma|^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, (a) is by Lemma C.4 and a union bound. Therefore, according to (19), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\left\\lVert\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\big\\rvert_{\\boldsymbol{S}}\\right\\rVert_{\\infty}>D/2\\ \\mathrm{or}\\ \\operatorname*{min}\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\big\\rvert_{\\mathcal{T}}<-D/2\\right]\\leq\\sum_{\\tau=1}^{t-1}\\operatorname*{Pr}[\\neg\\mathcal{E}_{\\tau}(D_{\\tau})],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{v}_{\\mathrm{T}}\\left[\\left\\lVert\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\right\\rVert_{S}\\right]_{\\infty}>D/2\\;\\mathrm{or}\\;\\operatorname*{min}_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\big|_{\\tau}<-D/2\\right]\\leq\\left\\{\\sum_{\\tau=\\eta\\mathrm{T}+1}^{0,}\\exp\\left\\{-\\tau^{1/2}\\right\\},\\ \\ t>\\eta T+1\\right\\}\\times\\mathrm{var}\\;\\frac{1}{2}>0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Plugging the into (18) and (15), we obtain that when $T\\to\\infty$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[T-T_{e}\\right]}\\\\ &{\\le T-T_{-}}\\\\ &{\\quad+\\displaystyle\\sum_{t=1}^{T_{-}}\\left(\\operatorname*{Pr}\\left[\\left\\|\\displaystyle\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\right\\|_{S}\\right\\|_{\\infty}>D/2\\;\\mathrm{or~min}\\displaystyle\\sum_{\\tau=1}^{t-1}\\widetilde{M}_{\\tau}^{C}\\big|_{\\mathcal{T}}<-D/2\\right]+2n\\exp\\left(-\\frac{(T-t)D^{2}}{8}\\right)\\right)}\\\\ &{\\le\\displaystyle\\frac{1}{\\rho^{\\mathrm{min}}-D}+2n(1-\\exp(-D^{2}/8))^{-1}+O(T^{2})\\exp\\left(-T^{1/2}\\right)=O\\left(\\frac{n}{D^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "At last, combining with (10), we finally finish the proof of Lemma C.3. ", "page_idx": 24}, {"type": "text", "text": "C.6 Proof of Lemma C.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To start with, we notice that ", "page_idx": 24}, {"type": "equation", "text": "$$\n(T-\\tau)M_{\\tau}^{C}=\\pmb{\\rho}_{\\tau}-\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)C(\\theta,a)\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, notice that $\\pmb{\\rho}_{\\tau}\\in\\mathcal{N}(\\pmb{\\rho}_{1},D,S)$ and $\\mathcal{E}_{u,\\tau}\\wedge\\mathcal{E}_{v,\\tau}$ are the condition of Proposition 3.1, therefore, the set of resource binding constraints of $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ are identical to that of $J(\\rho_{1})$ , i.e., $\\boldsymbol{S}$ . Hence, for any $i\\in[n]$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\rho_{\\tau}^{i}|s-\\mathbb{E}_{\\theta\\sim\\mathcal{M}}\\left[\\displaystyle\\sum_{\\alpha\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)C^{i}(\\theta,a)|s\\right]}\\\\ &{=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\widehat{u}_{\\tau}(\\theta)\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat{v}_{\\tau}(\\gamma)c^{i}(\\theta,a,\\gamma)|s-\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}v(\\gamma)c^{i}(\\theta,a,\\gamma)|s}\\\\ &{=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}(u(\\theta)-\\widehat{u}_{\\tau}(\\theta))\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}v(\\gamma)c^{i}(\\theta,a,\\gamma)|s}\\\\ &{\\quad+\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\widehat{u}_{\\tau}(\\theta)\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}(\\widehat{v}_{\\tau}(\\gamma)-v(\\gamma))c^{i}(\\theta,a,\\gamma)|s}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$\\begin{array}{r l}{\\lefteqn{\\mathrm{\\Pi}_{\\leq}^{\\mathrm{(a)}}\\,\\|(u(\\theta)-\\widehat{u}_{\\tau}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}.}}\\end{array}$ ", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, the bound on the first term in (a) establishes because for any $\\theta\\in\\Theta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}v(\\gamma)\\pmb{c}^{i}(\\theta,a,\\gamma)|s\\leq1\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since $\\begin{array}{r}{\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\leq1}\\end{array}$ The bound on the second term is similar. Thus, we achieve the result for binding constraints. The proof for non-binding constraints resembles the above by noticing that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\rho_{\\tau}|T\\geq\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\widehat{u}_{\\tau}(\\theta)\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat{v}_{\\tau}(\\gamma)c(\\theta,a,\\gamma)|\\tau.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D Missing Proofs in Section 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "With Lemma 4.1 in hand, we now show how to derive Theorem 4.1. Specifically, the regret decomposition technique in Lemma C.1 still works fine. We only need to re-derive corresponding results for Lemmas C.2 and C.3. We have the following results on this side, which are proved respectively in Appendices D.3 and D.4. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.1. Under Assumption 3.1, with partial information feedback, we have when $T\\to\\infty$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T_{c}}\\sum_{\\substack{\\theta\\in\\Theta,a\\in A^{+}}}\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\left(\\phi_{1}^{*}(\\theta,a)-\\widehat\\phi_{t}^{*}(\\theta,a)\\right)\\right]}\\\\ {\\displaystyle=O\\left(\\frac{k}{D^{2}}\\log T\\right),}\\\\ {\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T_{c}}\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)\\left(1-\\sum_{a\\in A^{+}}\\widehat\\phi_{t}^{*}(\\theta,a)\\right)\\right]=O\\left(\\frac{k+\\log T}{D^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma D.2. Under Assumption 3.1, with partial information feedback, we have when $T\\to\\infty$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n(\\lambda^{*})^{\\top}{\\mathbb{E}}\\left[B_{T_{c}+1}\\right]+\\operatorname*{max}_{\\theta\\in\\Theta,a\\in A^{+}}\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}\\cdot C(\\theta,a)\\right){\\mathbb{E}}\\left[T-T_{e}\\right]=O\\left({\\frac{n}{D^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemmas C.1, D.1 and D.2 in together leads to Theorem 4.1. ", "page_idx": 25}, {"type": "text", "text": "D.2Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Some preparations are required before we come to prove the lemma. To start with, we notice that $Y_{\\tau}=\\bar{\\operatorname*{Pr}}[\\bar{a}_{1}\\neq0]+\\cdot\\cdot\\cdot+\\operatorname*{Pr}[a_{t-1}\\neq0]$ . By the control rule of Algorithm 1, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[a_{\\tau}\\neq0]=\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\mid\\mathcal{H}_{\\tau}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We first give a lower bound on $\\begin{array}{r}{\\mathbb{E}_{\\theta\\sim\\mathcal{U}}[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)}\\end{array}$ \u2014 $\\mathcal{H}_{\\tau}]$ with $\\rho_{\\tau}$ \uff0c taking $\\begin{array}{r}{\\mathbb{E}_{\\theta\\sim\\hat{\\mathcal{U}}_{\\tau}}[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\mid\\mathcal{H}_{\\tau}]}\\end{array}$ as an intermediate. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.3. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\widehat{\\mathcal{U}}_{\\tau}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\mid\\mathcal{H}_{\\tau}\\right]\\ge\\operatorname*{min}\\left\\{1,\\operatorname*{min}\\rho_{\\tau}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma $D.3$ .To start with, when $\\rho_{\\tau}~\\geq~1$ , then clearly, all the resource constraints in $\\widehat{J}(\\rho_{\\tau},\\dot{\\mathcal{H}}_{\\tau})$ are satisfied even when $\\begin{array}{r}{\\sum_{a\\in A^{+}}\\phi(\\theta,a)=1}\\end{array}$ holds for any $\\theta\\in\\Theta$ . Therefore, an optimal solution should have this form. ", "page_idx": 25}, {"type": "text", "text": "We now consider the case that $\\operatorname*{min}\\rho_{\\tau}~<~1$ . In this case, if there is a feasible solution that $\\begin{array}{r}{\\sum_{a,\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\,=\\,1}\\end{array}$ holds for any $\\theta\\:\\in\\:\\Theta$ then the proof is also fnished. Otherwise, there is at least a binding resource constraint in $\\widehat{J}(\\rho_{\\tau},\\mathcal{H}_{\\tau})$ , which we denote by $i^{*}$ . Consequently, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\hat{\\mathcal{U}}_{\\tau}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\right]\\geq\\mathbb{E}_{\\theta\\sim\\hat{\\mathcal{U}}_{\\tau}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\widehat{C}_{\\tau}^{i^{*}}(\\theta,a)\\right]=\\rho_{\\tau}^{i^{*}}\\geq\\operatorname*{min}\\rho_{\\tau}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This finishes the proof of the lemma. ", "page_idx": 25}, {"type": "text", "text": "Thus, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{Pr}[a_{\\tau}\\neq0]=\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\mid\\mathcal{H}_{\\tau}\\right]}&{\\leq\\mathbb{E}_{\\theta\\sim\\hat{\\mathcal{U}}_{\\tau}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\mid\\mathcal{H}_{\\tau}\\right]-\\|u(\\theta)-\\widehat{u}_{\\tau}(\\theta)\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\operatorname*{min}\\left\\{1,\\operatorname*{min}{\\rho_{\\tau}}\\right\\}-\\|u(\\theta)-\\widehat{u}_{\\tau}(\\theta)\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Further, we have the follwing result bounding $\\operatorname*{min}\\rho_{\\tau}$ when $t$ is no larger than a fraction of $T$ ", "page_idx": 26}, {"type": "text", "text": "Lemma D.4.When $t\\leq(\\rho^{\\mathrm{min}}/2)\\cdot T,\\,\\operatorname*{min}\\rho_{\\tau}\\geq\\rho^{\\mathrm{min}}/2.$ ", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma D.4. In fact, for $t\\leq(\\rho^{\\mathrm{min}}/2)\\cdot T$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pmb{\\rho}_{\\tau}=\\frac{T\\cdot\\pmb{\\rho}_{1}-\\sum_{\\tau=1}^{t-1}\\pmb{c}_{\\tau}}{T-t+1}\\geq\\frac{T\\cdot\\pmb{\\rho}_{1}-t\\cdot\\mathbf{1}}{T}\\geq\\frac{\\pmb{\\rho}_{1}}{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "Now, by Weissman et al. [40], with probability $1-O(1/T)$ ,we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|u(\\theta)-\\widehat{u}_{\\tau}(\\theta)\\|_{1}\\leq\\frac{\\rho^{\\operatorname*{min}}}{4},\\quad\\forall\\tau\\geq\\Theta(\\log T).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking into (20), we derive that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[a_{\\tau}\\neq0]\\geq\\frac{\\rho^{\\mathrm{min}}}{4},\\quad\\forall\\Theta(\\log T)\\leq t\\leq\\frac{\\rho^{\\mathrm{min}}}{2}\\cdot T.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consequently, within the period, the probability that there are $\\Omega(\\log T)$ consecutive rounds in which the agent chooses to quit in all these rounds is ${\\cal O}(1/T)$ . This proves the first part. Meanwhile, at time $t=\\bar{\\lceil}(\\rho^{\\mathrm{min}}/2)\\cdot T\\rceil+\\bar{1}$ , by Azuma-Hoeffding inequality, we derive that with probability $1-O(1/T)$ $\\begin{array}{r}{Y_{t}=\\sum_{\\tau=1}^{t-1}\\operatorname*{Pr}[a_{\\tau}\\neq0]\\ge\\Omega(T)}\\end{array}$ which proves the second part. ", "page_idx": 26}, {"type": "text", "text": "D.3 Proof of Lemma D.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We concentrate on adapting the proof of Lemma C.2 into the partial information feedback setting. To start with, we suppose that the conditions given in Lemma 4.1 hold. In fact, since the failure probability is only ${\\cal O}(1/T)$ , and the sum is upper bounded by $O(T)$ , therefore the failure case only contributes $O(1)$ to the total expectation. ", "page_idx": 26}, {"type": "text", "text": "Now, recall the following definitions: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{t}:=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in a^{+}}\\left(u(\\theta)\\left(R(\\theta,a)-(\\lambda^{*})^{\\top}C(\\theta,a)\\right)-\\mu^{*}(\\theta)\\right)\\left(\\phi_{1}^{*}(\\theta,a)-\\widehat\\phi_{t}^{*}(\\theta,a)\\right),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad Q_{t}:=\\displaystyle\\sum_{\\theta\\in\\Theta}\\mu^{*}(\\theta)\\left(1-\\displaystyle\\sum_{a\\in A^{+}}\\widehat\\phi_{t}^{*}(\\theta,a)\\right),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\varepsilon_{u,t}:=[\\|(u(\\theta)-\\widehat u_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}\\leq D],\\quad\\mathcal E_{v,t}:=[\\|(v(\\gamma)-\\widehat v_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\leq D],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and by (13) and (14), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[P_{t}]\\leq\\operatorname*{Pr}[\\neg\\mathcal{E}_{u,t}]+\\operatorname*{Pr}[\\neg\\mathcal{E}_{v,t}],\\quad\\mathbb{E}[Q_{t}]\\leq\\operatorname*{Pr}[\\neg\\mathcal{E}_{u,t}]+\\operatorname*{Pr}[\\neg\\mathcal{E}_{v,t}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, the bound on $\\operatorname{Pr}[\\neg\\xi_{u,t}]$ inherits the analysis in the proof of Lemma C.2, as partial information feedback does not affect the learning of the request distribution. That is, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}[\\neg\\mathcal{E}_{u,t}]=\\operatorname*{Pr}[\\Vert(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\Vert_{\\infty}>D]\\le2k\\exp\\left(-2D^{2}(t-1)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For $\\operatorname{Pr}[\\neg\\xi_{v,t}]$ when $t\\leq\\Theta(\\log T)$ , it is obviously bounded by 1. By Lemma 4.1, when $\\Theta(\\log T)\\leq$ $t\\leq C_{b}\\cdot T$ , by Weissman et al. [40], we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\neg\\mathcal{E}_{v,t}]=\\operatorname*{Pr}[\\Vert(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\Vert_{1}>D]\\le\\left(2^{|\\Gamma|}-2\\right)\\exp\\left(-\\frac{D^{2}C_{f}(t-1)}{2\\log T}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Further, when $t>C_{b}\\cdot T$ , we correspondingly derive ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\neg\\mathcal{E}_{v,t}]\\leq\\Big(2^{|\\Gamma|}-2\\Big)\\exp\\left(-\\frac{D^{2}C_{r}(t-1)}{2}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Putting the above together, we achieve that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\mathbb E\\left[\\displaystyle\\sum_{t=1}^{T_{k}}P_{t}\\right],\\mathbb E\\left[\\displaystyle\\sum_{t=1}^{T_{k}}Q_{t}\\right]\\right\\}}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{T}2k\\exp\\left(-2D^{2}(t-1)\\right)+\\Theta(\\log T)}\\\\ &{\\quad+\\left(2^{\\Gamma\\Gamma}-2\\right)\\left(\\displaystyle\\sum_{t=\\Theta(\\log T)}^{c_{\\mathrm{v}},T}\\exp\\left(-\\frac{D^{2}C_{f}(t-1)}{2\\log T}\\right)+\\displaystyle\\sum_{t=c_{b}\\cdot T+1}^{T}\\exp\\left(-\\frac{D^{2}C_{r}(t-1)}{2}\\right)\\right)}\\\\ &{\\le\\Theta\\left(\\displaystyle\\frac{k}{D^{2}}\\right)+\\Theta(\\log T)+\\left(2^{\\Gamma\\Gamma}-2\\right)\\left(\\displaystyle\\frac{\\Theta(1)}{1-\\exp\\left(-\\Theta(D^{2}/\\log T)\\right)}+\\exp(-\\Theta(T))\\right)}\\\\ &{\\le O\\left(\\displaystyle\\frac{k+\\log T}{D^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 27}, {"type": "text", "text": "D.4 Proof of Lemma D.2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As in Appendix D.3 when we prove Lemma C.2, we only consider the case when the conditions in Lemma 4.1 establish, as the contribution of the failure cases on the expectation-sum is $O(1)$ Wenow bound $\\mathbb{E}[T-T_{e}]$ in the good case when the sample accessing frequency under partial information feedback is guaranteed. Specifically, as predefined in the proof of Lemma C.2, we only need to re-calculate the following, as the other terms remain unchanged with partial information: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=\\eta T+2}^{T_{-}}\\sum_{\\tau=\\eta T+1}^{t-1}\\operatorname*{Pr}\\left[\\Vert(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\Vert_{1}\\leq(\\tau-1)^{-1/4}/2\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, $\\eta$ is specified in the definition of $D_{\\tau}$ . It is hard for us to directly compare $\\eta$ and $C_{b}$ in Lemma 4.1. Nevertheless, in any case, we know that when $T$ is sufciently large, $Y_{\\tau}/(\\tau-1)=\\Omega(1/\\log T)$ for $\\tau\\geq\\eta T$ . Therefore, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\|(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\leq(\\tau-1)^{-1/4}/2\\right]\\leq2|\\Gamma|\\exp\\left(-\\frac{(\\tau-1)^{1/2}}{|\\Gamma|^{2}O(\\log T)}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{T_{-}}_{t=\\eta T+2}\\underbrace{t^{-1}}_{\\tau=\\eta T+1}\\mathrm{Pr}\\left[\\|(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\leq(\\tau-1)^{-1/4}/2\\right]}\\\\ &{\\leq2|\\Gamma|\\underbrace{T_{-}}_{t=\\eta T+2}\\underbrace{t^{-1}}_{\\tau=\\eta T+1}\\exp\\left(-\\frac{(\\tau-1)^{1/2}}{|\\Gamma|^{2}O(\\log T)}\\right)}\\\\ &{=O(T^{2})\\exp\\left(-\\Omega\\left(\\frac{T^{1/2}}{\\log T}\\right)\\right)=O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining with the other parts, Lemma D.2 is proved. ", "page_idx": 27}, {"type": "text", "text": "E  Missing Proofs in Section 5 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E.1 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We will prove Theorem 5.1 in the follwing, and we are inspired by the analysis in Chen et al. [15]. ", "page_idx": 27}, {"type": "text", "text": "E.1.1 Another Regret Decomposition ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Different from our analysis for the regular cases, in general circumstances, we introduce another regret decomposition method. The reason for involving such an alternative is that without the regularity assumptions, we no longer have any local stability guarantee even when the estimates are close. Therefore, the decision given by Algorithm 1 does not coincides with the optimal decision even when the distribution learning process converges well, and the corresponding analysis in Section 3 does not work out anymore. ", "page_idx": 28}, {"type": "text", "text": "We now present a more general regret decomposition as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\mathrm{FL}}-R e w=T\\cdot J(\\rho_{1})-\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{0}}r(\\theta_{t},a_{t},\\gamma_{t})\\right]}\\\\ &{\\qquad\\qquad\\stackrel{\\mathrm{(a)}}{=}T\\cdot J(\\rho_{1})-\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{0}}\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right]}\\\\ &{\\qquad\\stackrel{\\mathrm{(b)}}{=}J(\\rho_{1})\\cdot\\mathbb{E}\\left[T-T_{0}\\right]+\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{0}}\\left(J(\\rho_{1})-\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, (a) holds due to the Optimal Stopping Theorem, since $T_{0}$ is a stopping time. Meanwhile, by the decision process, we have for any $\\theta_{t}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{a_{t},\\gamma_{t}}[r(\\theta_{t},a_{t},\\gamma_{t})\\mid\\theta_{t}]=\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta_{t},a)R(\\theta_{t},a).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Further, (b) is by a re-arrangement. To give a bound for (21), we respectively analyze $\\mathbb{E}[T-T_{0}]$ the stopping time, and difference between the optimal accumulated rewards and the real ones. ", "page_idx": 28}, {"type": "text", "text": "E.1.2 Bounding the Stopping Time ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To settle the stopping time, we first reduce it to $\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)$ for $t\\leq T_{0}$ , and then deals with these values. We notice that $t\\leq T_{0}$ as long as that $\\mathbf{}B_{t}\\geq\\mathbf{1}$ ,or $\\rho_{t}\\geq{1}/({T}-t+1)$ . Now, since for any $i\\in[n]$ \uff0c ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{t}^{i}=\\rho_{1}^{i}-(\\rho_{1}-\\rho_{t})^{i}\\ge\\rho^{\\operatorname*{min}}-\\operatorname*{max}\\left(\\rho_{1}-\\rho_{t},0\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we have $\\operatorname*{min}\\pmb{\\rho}_{t}\\geq\\rho^{\\operatorname*{min}}-\\operatorname*{max}(\\pmb{\\rho}_{1}-\\pmb{\\rho}_{t},0)$ . Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{t\\leq T_{0}\\,\\Longleftarrow\\,\\rho^{\\mathrm{min}}-\\operatorname*{max}\\,(\\rho_{1}-\\rho_{t},0)\\geq\\frac{1}{T-t+1}}\\\\ {\\Longleftrightarrow\\,\\operatorname*{max}\\,(\\rho_{1}-\\rho_{t},0)\\leq\\rho^{\\mathrm{min}}-\\frac{1}{T-t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\mathbb{E}[T_{0}]\\geq\\operatorname*{Pr}[T_{0}\\geq t]\\cdot t$ for any $t\\in[T]$ , we only need to bound the following term for some certain $t$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{max}\\left(\\rho_{1}-\\rho_{t},0\\right)\\leq\\rho^{\\mathrm{min}}-\\frac{1}{T-t+1}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We will further prove the following lemma in Appendix E.3: ", "page_idx": 28}, {"type": "text", "text": "Lemma E.1. It holds for any $t<T$ that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{max}\\left(\\rho_{1}-\\rho_{t},0\\right)\\geq\\Theta\\left(\\frac{1}{T-1}+k\\sum_{\\tau=2}^{t-1}\\sqrt{\\frac{\\log T}{(T-\\tau)^{2}(\\tau-1)}}+\\sqrt{\\frac{\\log T}{T-t}}\\right)\\right]\\leq O\\left(\\frac{k+n}{T}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "With the light of Lemma E.1, it is natural for us to compute ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{\\tau=2}^{t-1}\\sqrt{\\frac{\\log T}{(T-\\tau)^{2}(\\tau-1)}}\\leq\\left\\{\\begin{array}{l l}{\\sqrt{\\log T}\\cdot\\frac{4\\sqrt{t-2}}{T-1},}&{2\\leq t\\leq(T+1)/2;}\\\\ {\\sqrt{\\log T}\\cdot\\frac{2}{\\sqrt{T-t}},}&{t>(T+1)/2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In fact, to derive the above, we notice that when $2\\leq t\\leq(T+1)/2$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t-1}{\\frac{1}{(T-\\tau)(\\tau-1)^{1/2}}}\\leq{\\frac{2}{T-1}}\\sum_{\\tau=2}^{t-1}{\\frac{1}{(\\tau-1)^{1/2}}}\\leq{\\frac{4{\\sqrt{t-1}}}{T-1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Meanwhile, when $t>(T+1)/2$ , we have $T-t<t-1$ , which leads to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{\\tau=2}^{t-1}\\frac{1}{(T-\\tau)(\\tau-1)^{1/2}}\\leq\\sqrt{\\frac{8}{T-1}}+\\sum_{\\tau=(T+1)/2}^{t-1}\\frac{1}{(T-\\tau)^{3/2}}\\leq\\frac{2}{\\sqrt{T-t}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "With these calculations, we come back to the bound on $\\mathbb{E}[T_{0}]$ , we notice that when $T$ is sufficiently large and $t=T-O(\\log T)$ , it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{1}{T-1}+k\\sum_{\\tau=2}^{t-1}\\sqrt{\\frac{\\log T}{(T-\\tau)^{2}(\\tau-1)}}+\\sqrt{\\frac{\\log T}{T-t}}\\right)+\\frac{1}{T-t+1}=O(1)\\leq\\rho^{\\mathrm{min}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[T-T_{0}\\right]=T-\\mathbb{E}[T_{0}]\\overset{\\mathrm{(a)}}{\\leq}T-\\operatorname*{Pr}[T_{0}\\geq T-O(\\log T)]\\cdot(T-O(\\log T))}\\\\ &{\\overset{\\mathrm{(b)}}{\\leq}T-\\left(1-O\\left(\\frac{1}{T}\\right)\\right)\\cdot(T-O(\\log T))=O(\\log T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the above, (a) is because $\\mathbb{E}[T_{0}]\\,\\geq\\,\\mathrm{Pr}[T_{0}\\,\\geq\\,t]\\cdot t$ for any fixed $t$ , and (b) is due to Lemma E.1.   \nConsequently, we finish the analysis of the stopping time in (21). ", "page_idx": 29}, {"type": "text", "text": "E.1.3 The Gap to the Optimal Reward ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The rest part of (21) that we are left to consider is the following: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J(\\rho_{1})-\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]}\\\\ &{=\\Big(J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\Big)+\\left(\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that the second difference term in (24) refects the estimation error on distributions of the context and the external factor, which leads to the following result as to be proved in Appendix E.4: ", "page_idx": 29}, {"type": "text", "text": "Lemma E.2. We have for $t\\geq2$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right]\\leq O\\left(k\\sqrt{\\frac{\\log T}{t-1}}+\\frac{k}{T}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma E.2 induces an $O({\\sqrt{T\\log T}})$ accumulated regret considering (24) when summing from $t=2$ $T_{0}\\leq T$ . While for the first term in (24), our main thread here is to bound $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ with $J(\\rho_{t})$ To fix the idea, we compare these two optimization problems: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\rho_{t}):=\\displaystyle\\operatorname*{max}_{\\phi:\\Theta\\times A+\\to\\mathbb{R}_{+}}\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\phi(\\theta,a)\\sum_{\\gamma}r(\\theta,a,\\gamma)v(\\gamma),}\\\\ &{\\quad\\mathrm{s.t.~}\\quad\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\phi(\\theta,a)\\sum_{\\gamma}c(\\theta,a,\\gamma)v(\\gamma)\\leq\\rho_{t},}\\\\ &{\\quad\\quad\\quad\\quad\\displaystyle\\sum_{a\\in A^{+}}\\phi(\\theta,a)\\leq1,\\quad\\forall\\theta\\in\\Theta,}\\\\ &{\\quad\\quad\\quad\\quad\\phi(\\theta,a)\\geq0,\\quad\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{J}(\\rho_{t},\\mathcal{H}_{t}):=\\underset{\\phi:\\Theta\\times A^{+}\\rightarrow\\mathbb{R}_{+}}{\\operatorname*{max}}\\underset{\\theta\\in\\Theta,a\\in A^{+}}{\\sum}\\ \\widehat{u}_{t}(\\theta)\\phi(\\theta,a)\\sum_{\\gamma}r(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma),}\\\\ &{\\mathrm{s.t.}\\quad\\underset{\\theta\\in\\Theta,a\\in A^{+}\\atop\\sum a\\in A^{+}}{\\sum}\\ \\widehat{u}_{t}(\\theta)\\phi(\\theta,a)\\sum_{\\gamma}c(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\leq\\rho_{t},}\\\\ &{\\qquad\\quad\\quad\\underset{a\\in A^{+}}{\\sum}\\phi(\\theta,a)\\leq1,\\quad\\forall\\theta\\in\\Theta,}\\\\ &{\\qquad\\quad\\phi(\\theta,a)\\geq0,\\quad\\forall(\\theta,a)\\in\\Theta\\times A^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, conceptually, if there is a $0<\\eta_{t}\\leq1$ such that for any $(\\theta,a)\\in\\Theta\\times A^{+}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\nu(\\theta)\\sum_{\\gamma}c(\\theta,a,\\gamma)v(\\gamma)\\geq\\eta_{t}\\widehat{u}_{t}(\\theta)\\sum_{\\gamma}c(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "then for an optimal solution $\\phi_{t}^{*}$ of $J(\\rho_{t})$ , we see that $\\eta_{t}\\phi_{t}^{*}$ is a feasible solution of the programming $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ . Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\geq\\eta_{t}\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\widehat{u}_{t}(\\theta)\\phi_{t}^{*}(\\theta,a)\\sum_{\\gamma}r(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)}\\\\ &{\\overset{\\mathrm{(a)}}{\\geq}\\eta_{t}\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\phi_{t}^{*}(\\theta,a)\\sum_{\\gamma}r(\\theta,a,\\gamma)v(\\gamma)}\\\\ &{\\quad-\\eta_{t}(\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1})}\\\\ &{=\\eta_{t}J(\\rho_{t})-(\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, since $r(\\theta,a,\\gamma)\\leq1$ and $\\begin{array}{r}{\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)\\leq1}\\end{array}$ for any $\\theta$ , (a) is expanded as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\widehat{u}_{t}(\\theta)\\phi_{t}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat{v}_{t}(\\gamma)r(\\theta,a,\\gamma)-\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\phi_{t}^{*}(\\theta,a)\\sum_{\\gamma}v(\\gamma)r(\\theta,a,\\gamma)}\\\\ &{=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}(\\widehat{u}_{t}(\\theta)-u(\\theta))\\phi_{t}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat{v}_{t}(\\gamma)r(\\theta,a,\\gamma)}\\\\ &{\\displaystyle+\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\phi_{t}^{*}(\\theta,a)\\sum_{\\gamma}(\\widehat{v}_{t}(\\gamma)-v(\\gamma))r(\\theta,a,\\gamma)}\\\\ &{\\displaystyle\\leq\\lVert(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\rVert_{1}+\\lVert(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\rVert_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Consequently, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})}\\\\ &{\\leq(1-\\eta_{t})J(\\rho_{1})+\\eta_{t}\\big(J(\\rho_{1})-J(\\rho_{t})\\big)+\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "On top of this, a key observation is that ", "page_idx": 30}, {"type": "equation", "text": "$$\nJ(\\pmb{\\rho}_{1})-J(\\pmb{\\rho}_{t})\\leq\\frac{\\operatorname*{max}{(\\pmb{\\rho}_{1}-\\pmb{\\rho}_{t},0)}}{\\rho^{\\operatorname*{min}}}\\cdot J(\\pmb{\\rho}_{1}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In fact, when $\\rho_{1}\\leq\\rho_{t}$ , (26) is natural as $J(\\pmb{\\rho}_{1})\\leq J(\\pmb{\\rho}_{t})$ . Otherwise, let $\\phi_{1}^{*}$ be the optimal solution to the programming $J(\\rho_{1})$ . Let $i^{*}$ be the index that minimizes $\\rho_{t}^{i^{*}}/\\rho_{1}^{i^{*}}$ . We have $\\rho_{1}^{i^{*}}>\\rho_{t}^{i^{*}}$ . Evidently, we know that $\\phi_{1}^{*}\\cdot\\rho_{t}^{i^{*}}/\\rho_{1}^{i^{*}}$ is a feasible solution to the programming of $J(\\rho_{t})$ . By the optimality of $J(\\rho_{t})$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nJ(\\pmb{\\rho}_{t})\\geq\\frac{\\pmb{\\rho}_{t}^{i^{*}}}{\\pmb{\\rho}_{1}^{i^{*}}}\\cdot J(\\pmb{\\rho}_{1}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which leads to ", "page_idx": 30}, {"type": "equation", "text": "$$\nJ(\\rho_{1})-J(\\rho_{t})\\leq\\left(1-\\frac{\\rho_{t}^{i^{*}}}{\\rho_{1}^{i^{*}}}\\right)\\cdot J(\\rho_{1})=\\frac{\\rho_{1}^{i^{*}}-\\rho_{t}^{i^{*}}}{\\rho_{1}^{i^{*}}}\\cdot J(\\rho_{1})\\leq\\frac{\\operatorname*{max}{(\\rho_{1}-\\rho_{t})}}{\\rho^{\\operatorname*{min}}}\\cdot J(\\rho_{1}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Synthesizing the above two parts, (26) is proved. ", "page_idx": 31}, {"type": "text", "text": "Asfor $\\mathbb{E}[\\operatorname*{max}(\\pmb{\\rho}_{1}-\\pmb{\\rho}_{t},0)]$ , we note that for any non-negative random variable $X$ with upper bound $\\bar{X}$ and any positive $\\xi$ wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[X]\\leq\\xi\\operatorname*{Pr}[X\\leq\\xi]+\\bar{X}\\left(1-\\operatorname*{Pr}[X\\leq\\xi]\\right)\\leq\\xi+\\bar{X}\\left(1-\\operatorname*{Pr}[X\\leq\\xi]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Notice that $\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)$ is certainly upper bounded by 1. Therefore, as a corollary of Lemma E.1 wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}\\left(\\rho_{1}-\\rho_{t},0\\right)\\right]\\leq\\left\\{\\begin{array}{l l}{\\displaystyle O\\left(k\\frac{\\sqrt{(t-2)\\log T}}{T}+\\sqrt{\\frac{\\log T}{T-t}}+\\frac{k+n}{T}\\right),}&{2\\leq t\\leq(T+1)/2;}\\\\ {\\displaystyle O\\left(k\\sqrt{\\frac{\\log T}{T-t}}+\\frac{k+n}{T}\\right),}&{t>(T+1)/2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We almost finish the bound now except for determining $\\eta_{t}$ in (25), which we hope is as close to 1 as possible. Nevertheless, we leave the technical parts to Appendix E.5 which derives the following lemma on the total bound: ", "page_idx": 31}, {"type": "text", "text": "Lemma E.3. ", "text_level": 1, "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\Big(J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\Big)\\right]=O(k\\sqrt{T\\log T}+n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, we sum the result in Lemma E.2 from $t=2$ to $T_{0}$ , and plus the constant term for $t=1$ to obtain that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\left(\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right)\\right]=O(k\\sqrt{T\\log T}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Synthesizing Lemma E.3, (24), (23), and (21), we derive Theorem 5.1. ", "page_idx": 31}, {"type": "text", "text": "E.2Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The proof of this theorem follows the line of Theorem 5.1, and the only difference is to adopt Lemma 4.1 when considering the concentration of estimates. On this side, we can disregard the cases when $t\\leq\\Theta(\\log T)$ , as the accumulated regret in this phase is bounded by ${\\cal O}(\\log T)$ . On the other hand, the time range that $t\\geq\\Theta(T)$ is asymptotically identical to the full information setting since the accessing frequency is a constant. We only need to consider the case that $\\Theta(\\log T)\\leq t\\leq\\Theta(T)$ whenwe have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\Vert(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\Vert_{1}\\leq-\\Theta\\left(k\\sqrt{\\frac{\\log T}{t-1}}\\right)\\right]\\leq O\\left(\\frac{k}{T^{2}}\\right),}\\\\ &{\\operatorname*{Pr}\\left[\\Vert(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\Vert_{1}\\leq-\\Theta\\left(\\vert\\Gamma\\vert\\frac{\\log T}{\\sqrt{t-1}}\\right)\\right]\\leq O\\left(\\frac{\\vert\\Gamma\\vert}{T^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Taking into the proof of Lemma E.1 and then into the main body, we should find a sufficient large $t$ suchthat ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta\\left(\\frac{\\log T}{T-1}+k\\displaystyle\\sum_{\\tau=\\Theta(\\log T)}^{\\Theta(T)}\\frac{\\log T}{\\sqrt{(T-\\tau)^{2}(\\tau-1)}}+k\\displaystyle\\sum_{\\tau=\\Theta(T)}^{t-1}\\sqrt{\\frac{\\log T}{(T-\\tau)^{2}(\\tau-1)}}+\\sqrt{\\frac{\\log T}{T-t}}\\right)}\\\\ &{\\leq\\rho^{\\mathrm{min}}-\\frac{1}{T-t+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and $t=T-O(\\log T)$ still suffces. Therefore, $\\mathbb{E}[T-T_{0}]\\,=\\,O(\\log T)$ also holds under partial information feedback. ", "page_idx": 31}, {"type": "text", "text": "Nevertheless, for the counterpart of Lemma E.2, by (28), when we sum from $t=1$ to $T_{0}\\leq T$ ,we derive that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\left(\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right)\\right]\\leq O(k\\sqrt{T}\\log T).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "At last, for $J(\\pmb{\\rho}_{1})-\\widehat{J}(\\pmb{\\rho}_{t},\\mathcal{H}_{t})$ , we face the same degradation on the estimation accuracy, which leads to ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\Big(J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\Big)\\right]=O(k\\sqrt{T}\\log T+n).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, Theorem 5.2 is achieved. ", "page_idx": 32}, {"type": "text", "text": "E.3Proof of Lemma E.1 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Now that we are going to bound $\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)$ . Recall the definitions below which we give in Appendix C.5 when we prove Lemma C.3: ", "page_idx": 32}, {"type": "equation", "text": "$$\nM_{t}^{C}:=\\frac{\\rho_{t}-\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)C(\\theta,a)\\right]}{T-t},\\quad N_{t}^{C}:=\\frac{\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)C(\\theta,a)\\right]-c_{t}}{T-t}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By (16), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pmb{\\rho}_{t+1}-\\pmb{\\rho}_{t}=\\frac{\\pmb{\\rho}_{t}-\\pmb{c}_{t}}{T-t}=M_{t}^{C}+\\pmb{N}_{t}^{C}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Consequently, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}(\\rho_{1}-\\rho_{t})=\\operatorname*{max}\\left(-\\left(\\sum_{\\tau=1}^{t-1}M_{\\tau}^{C}+\\sum_{\\tau=1}^{t-1}N_{\\tau}^{C}\\right)\\right)\\leq-\\operatorname*{min}\\sum_{\\tau=1}^{t-1}M_{\\tau}^{C}-\\operatorname*{min}\\sum_{\\tau=1}^{t-1}N_{\\tau}^{C}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For the second term, we notice that each entry of $\\{\\Sigma_{\\tau<t}\\,N_{\\tau}^{C}\\}_{t}$ is a martingale with the absolute value of the $\\tau$ -th increment bounded by $1/(T-\\tau)$ .Since ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t-1}\\frac{1}{(T-\\tau)^{2}}\\leq\\frac{1}{T-t},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by applying the Azuma-Hoeffding inequality and a union bound, we achieve that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[-\\operatorname*{min}\\sum_{\\tau=1}^{t-1}N_{\\tau}^{C}\\ge\\sqrt{\\frac{2\\log T}{T-t}}\\right]\\le\\frac{n}{T}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "On the other hand, for the first term, when $\\tau=1$ , it is apparent that $-\\operatorname*{min}M_{1}^{C}\\leq1/(T-1)$ .When $\\tau\\geq2$ ,wehavefor any $i\\in[n]$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(T-\\tau)\\left(M_{\\tau}^{\\mathcal{C}}\\right)^{i}}\\\\ &{=\\rho_{\\tau}^{i}-\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{\\ell\\in\\mathcal{A}^{+}}\\widehat\\phi_{\\tau}^{*}(\\theta,a)C^{i}(\\theta,a)\\right]}\\\\ &{\\stackrel{(a)}{\\geq}\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in\\mathcal{A}^{+}}\\widehat{u}_{\\tau}(\\theta)\\widehat\\phi_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat\\nu_{\\tau}(\\gamma)c^{i}(\\theta,a,\\gamma)-\\sum_{\\theta\\in\\Theta,a\\in\\mathcal{A}^{+}}u(\\theta)\\widehat\\phi_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}v(\\gamma)c^{i}(\\theta,a,\\gamma)}\\\\ &{=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in\\mathcal{A}^{+}}\\left(\\widehat{u}_{\\tau}(\\theta)-u(\\theta)\\right)\\widehat\\phi_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat\\nu_{\\tau}(\\gamma)c^{i}(\\theta,a,\\gamma)}\\\\ &{\\quad+\\sum_{\\theta\\in\\Theta,a\\in\\mathcal{A}^{+}}u(\\theta)\\widehat\\phi_{\\tau}^{*}(\\theta,a)\\sum_{\\gamma}(\\widehat\\nu_{\\tau}(\\gamma)-v(\\gamma))c^{i}(\\theta,a,\\gamma)}\\\\ &{\\quad\\theta\\in\\Theta,a\\in\\mathcal{A}^{+}}\\\\ &{\\geq-\\|(u(\\theta)-\\widehat\\kappa_{\\tau}(\\theta))\\)_{\\theta\\in\\Theta}\\|_{1}-\\|(v(\\gamma)-\\widehat\\nu_{\\tau}(\\gamma))\\_{\\tau}\\mathrm{erf}\\ \\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In the above, (a) is because $\\widehat{\\phi}_{\\tau}^{*}$ is feasible for $\\widehat{J}(\\rho_{\\tau},\\mathcal{H}_{\\tau})$ . By Hoeffding's inequality and a union bound, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\lVert(u(\\theta)-\\widehat{u}_{\\tau}(\\theta))_{\\theta\\in\\Theta}\\rVert_{1}\\leq-k\\sqrt{\\frac{\\log T}{\\tau-1}}\\right]\\leq\\frac{k}{T^{2}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\lVert(v(\\gamma)-\\widehat{v}_{\\tau}(\\gamma))_{\\gamma\\in\\Gamma}\\rVert_{1}\\leq-|\\Gamma|\\sqrt{\\frac{\\log T}{\\tau-1}}\\right]\\leq\\frac{|\\Gamma|}{T^{2}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, suppose the above events hold for all $\\tau\\le T$ with failure probability only ${\\cal O}(1/T)$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[-\\operatorname*{min}\\sum_{\\tau=1}^{t-1}M_{\\tau}^{C}\\geq\\Theta\\left(\\frac{1}{T-1}+k\\sum_{\\tau=2}^{t-1}\\sqrt{\\frac{\\log T}{(T-\\tau)^{2}(\\tau-1)}}\\right)\\right]\\leq O\\left(\\frac{k}{T}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining (29) and (30), we derive the lemma. ", "page_idx": 33}, {"type": "text", "text": "E.4 Proof of Lemma E.2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We notice that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})=\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\widehat{u}_{t}(\\theta)\\widehat{\\phi}_{t}^{*}(\\theta,a)\\sum_{\\gamma}r(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}\\widehat{u}_{t}(\\theta)\\widehat{\\phi}_{t}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat{v}_{t}(\\gamma)r(\\theta,a,\\gamma)-\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\widehat{\\phi}_{t}^{*}(\\theta,a)\\sum_{\\gamma}v(\\gamma)r(\\theta,a,\\gamma)}\\\\ &{=\\displaystyle\\sum_{\\theta\\in\\Theta,a\\in A^{+}}(\\widehat{u}_{t}(\\theta)-u(\\theta))\\widehat{\\phi}_{t}^{*}(\\theta,a)\\sum_{\\gamma}\\widehat{v}_{t}(\\gamma)r(\\theta,a,\\gamma)}\\\\ &{\\displaystyle+\\sum_{\\theta\\in\\Theta,a\\in A^{+}}u(\\theta)\\widehat{\\phi}_{t}^{*}(\\theta,a)\\sum_{\\gamma}(\\widehat{v}_{t}(\\gamma)-v(\\gamma))r(\\theta,a,\\gamma)}\\\\ &{\\displaystyle\\leq\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta\\sim\\mathcal{U}}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\leq\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By Hoeffding's inequality and a union bound, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{Pr}\\left[\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}\\geq k\\sqrt{\\frac{\\log T}{2(t-1)}}\\right]\\leq\\frac{k}{T},}\\\\ {\\displaystyle\\operatorname*{Pr}\\left[\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\geq|\\Gamma|\\sqrt{\\frac{\\log T}{2(t-1)}}\\right]\\leq\\frac{|\\Gamma|}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Further, the difference we hope to analyze is certainly upper bounded by 1. As a result, with (27), we finish the proof. ", "page_idx": 33}, {"type": "text", "text": "E.5 Proof of Lemma E.3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We come to consider $J(\\pmb{\\rho}_{1})-\\widehat{J}(\\pmb{\\rho}_{t},\\mathcal{H}_{t})$ . As per the thread in the main body, we let ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\delta_{t}:=\\frac{\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}}{\\operatorname*{min}_{\\theta\\in\\Theta,a\\in A^{+}}\\{\\operatorname*{min}\\{u(\\theta)C(\\theta,a)>0\\}\\}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We now claim that for any $(\\theta,a,i)\\in\\Theta\\times A^{+}\\times[n]$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widehat{u}_{t}(\\theta)\\sum_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\leq(1+\\delta_{t})u(\\theta)\\sum_{\\gamma}c^{i}(\\theta,a,\\gamma)v(\\gamma).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The above is obvious if $C^{i}(\\theta,a)=\\mathbf{0}$ ,or $\\pmb{c}^{i}(\\theta,a,\\gamma)=0$ holds for any $\\gamma$ When $C(\\theta,a)\\neq\\mathbf{0}$ , then for any $i\\in[n]$ \uff0c ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widehat{u}_{t}(\\theta)\\sum_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)-u(\\theta)\\sum_{\\gamma}c^{i}(\\theta,a,\\gamma)v(\\gamma)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(\\widehat{u}_{t}(\\theta)-u(\\theta))\\displaystyle\\sum_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)+u(\\theta)\\displaystyle\\sum_{\\gamma}c^{i}(\\theta,a,\\gamma)(\\widehat{v}_{t}(\\gamma)-v(\\gamma))}\\\\ &{\\leq\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}}\\\\ &{\\leq\\delta_{t}u(\\theta)\\displaystyle\\sum_{\\gamma}c^{i}(\\theta,a,\\gamma)v(\\gamma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This finish the explanation of the claim. Upon that, if we let $\\eta_{t}:=1-\\delta_{t}\\leq1/(1+\\delta_{t})$ , we derive that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{u(\\theta)\\sum_{\\gamma}c(\\theta,a,\\gamma)v(\\gamma)\\le\\frac{1}{1+\\delta_{t}}\\widehat{u}_{t}(\\theta)\\sum_{\\gamma}c(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)}}\\\\ &{}&{\\le\\eta_{t}\\widehat{u}_{t}(\\theta)\\sum_{\\gamma}c(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "With respect to (25) and (26), we obtain that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathscr{H}_{t})}\\\\ &{\\leq J(\\rho_{1})\\cdot\\left(1-\\eta_{t}+\\frac{\\operatorname*{max}\\big(\\rho_{1}-\\rho_{t},0\\big)}{\\rho^{\\operatorname*{min}}}\\right)+\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}}\\\\ &{=J(\\rho_{1})\\cdot\\left(\\delta_{t}+\\frac{\\operatorname*{max}\\big(\\rho_{1}-\\rho_{t},0\\big)}{\\rho^{\\operatorname*{min}}}\\right)+\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}+\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As we have already shown in the main part that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}\\left(\\rho_{1}-\\rho_{t},0\\right)\\right]\\leq\\left\\{\\begin{array}{l l}{\\displaystyle O\\left(k\\frac{\\sqrt{(t-2)\\log T}}{T}+\\sqrt{\\frac{\\log T}{T-t}}+\\frac{k+n}{T}\\right),}&{2\\leq t\\leq(T+1)/2;}\\\\ {\\displaystyle O\\left(k\\sqrt{\\frac{\\log T}{T-t}}+\\frac{k+n}{T}\\right),}&{t>(T+1)/2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "it suffices for us to bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}],\\mathbb{E}[\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}],\\mathbb{E}[(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "On this side, as we have shown that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{Pr}\\left[\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}\\geq k\\sqrt{\\frac{\\log T}{2(t-1)}}\\right]\\leq\\frac{k}{T},}\\\\ {\\displaystyle\\operatorname*{Pr}\\left[\\|(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}\\geq|\\Gamma|\\sqrt{\\frac{\\log T}{2(t-1)}}\\right]\\leq\\frac{|\\Gamma|}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "it is natural that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\{\\mathbb{E}[\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{\\infty}],\\mathbb{E}[\\|(u(\\theta)-\\widehat{u}_{t}(\\theta))_{\\theta\\in\\Theta}\\|_{1}],\\mathbb{E}[(v(\\gamma)-\\widehat{v}_{t}(\\gamma))_{\\gamma\\in\\Gamma}\\|_{1}]\\}}\\\\ &{\\le O\\left(k\\sqrt{\\frac{\\log T}{t-1}}+\\displaystyle\\frac{k}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, putting all the above into (31) and summing from $t=1$ to $T_{0}\\leq T$ wehave ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\Big(J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\Big)\\right]=O(k\\sqrt{T\\log T}+n).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 34}, {"type": "text", "text": "F  Details of Numerical Validations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Specifically, we consider the following instance: There are two types of resources, three types of contexts, and two types of external factors. The unknown mass function of context and external factor are $(u(\\theta_{1}),u\\bar{(}\\bar{\\theta}_{2}),u(\\theta_{3}))=(0.3,0.3,0.4)$ and $(v(\\gamma_{1}),v(\\gamma_{2}))=(0.5,0.5)$ ,respectively.The resource consumption is represented by ", "page_idx": 35}, {"type": "equation", "text": "$$\nC^{(1)}=\\left[\\!\\!\\begin{array}{c c c}{{0.9}}&{{1.1}}\\\\ {{1.8}}&{{2.2}}\\\\ {{1.2}}&{{0.8}}\\end{array}\\!\\!\\right],\\;\\;\\;C^{(2)}=\\left[\\!\\!\\begin{array}{c c c}{{2.1}}&{{1.9}}\\\\ {{0.8}}&{{1.2}}\\\\ {{0.9}}&{{1.1}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\pmb{c}(\\theta_{i},1,\\gamma_{j})=(C_{i,j}^{(1)},C_{i,j}^{(2)})^{\\top}$ for all $(i,j)$ . The reward function is represented by ", "page_idx": 35}, {"type": "equation", "text": "$$\nR=\\left[\\!\\!{\\begin{array}{l l}{1.2}&{0.8}\\\\ {1.3}&{1.1}\\\\ {0.7}&{0.9}\\end{array}}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $r(\\theta_{i},1,\\gamma_{j})=R_{i,j}$ for all $i,j$ . Thus the underlying LP is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J(\\pmb\\rho)=\\operatorname*{max}0.3\\cdot x_{1}+0.36\\cdot x_{2}+0.32\\cdot x_{3},~~~~}\\\\ {\\mathrm{s.t.}~~~0.3\\cdot x_{1}+0.6\\cdot x_{2}+0.4\\cdot x_{3}\\leq\\rho^{1},}\\\\ {0.6\\cdot x_{1}+0.3\\cdot x_{2}+0.4\\cdot x_{3}\\leq\\rho^{2},}\\\\ {0\\leq x_{i}\\leq1,~i=1,2,3,~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $x_{i}\\,=\\,\\phi(\\theta_{i})$ for all $i\\,=\\,1,2,3$ . For a degenerate instance, we set $\\pmb{\\rho}=(1,1.15)^{\\top}$ with the optimal solution $\\pmb{x}^{*}\\,=\\,(1,0.5,1)^{\\top}$ . For a non-degenerate problem instance, we set the average resources as $\\pmb{\\rho}=(1,1)^{\\top}$ and the unique optimal solution is $\\mathbf{x}^{*}=(2/3,2/3,1)^{\\top}$ ;Herewerelaxthe restriction that the consumption and reward are upper bounded by 1, as this condition can be met easily by scaling, with the regret accordingly scaling. Such a relaxation is to make the LP form more visually appealing. ", "page_idx": 35}, {"type": "text", "text": "G Missing Details in Appendix A ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "G.1  The Density Estimator ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We now present details on the kernel density estimator which we apply in Appendix A for approximating continuous distributions, which comes from Wasserman [39]. We consider a one-dimensional kernelfunction $K$ suchthat ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\int K(x)\\,\\mathrm{d}x=1;}\\\\ &{\\bullet\\,\\int x^{s}K(x)\\,\\mathrm{d}x=0,\\quad\\forall1\\leq s\\leq\\beta;}\\\\ &{\\bullet\\,\\int|x|^{\\beta}|K(x)|\\,\\mathrm{d}x<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, given $\\ell$ independent samples $X_{1},\\cdot\\cdot\\cdot,X_{\\ell}$ from $P$ and a positive number $h$ called the bandwidth, the kernel density estimator is defined as ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\widehat{p}}_{\\ell}(x)={\\frac{1}{\\ell}}\\sum_{i=1}^{\\ell}{\\frac{1}{h^{d}}}K\\left({\\frac{\\|x-X_{i}\\|_{2}}{h}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Furthemre, tsatisfy Propsition , we should e $h\\asymp k^{1/(2\\beta+d)}\\log k$ When $p\\in\\Sigma(\\beta,L)$ the density of $\\mathcal{P}$ on $\\mathbb{R}^{d}$ ", "page_idx": 35}, {"type": "text", "text": "G.2 Proof of Theorem A.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "By (21), we know that ", "page_idx": 35}, {"type": "equation", "text": "$$\nV^{\\mathrm{FL}}-R e w=J(\\rho_{1})\\cdot\\mathbb{E}\\left[T-T_{0}\\right]+\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\left(J(\\rho_{1})-\\mathbb{E}_{\\theta}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right)\\right],\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and we bound these terms in order. For the expected stopping time $\\mathbb{E}[T_{0}]$ , by the analysis in Section 5, our goal turns into bounding $\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)$ , which further by (16) and (29), reduces to bound $M_{\\tau}^{\\tilde{C}}$ . With continuous randomness, we have for any $i\\in[n]$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(T-\\tau)\\left(M_{f}^{\\mathcal{T}}\\right)^{i}}\\\\ &{=\\rho_{\\tau}^{i}-\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{c\\in\\mathcal{A}}\\hat{\\phi}_{\\tau}^{*}(\\theta,\\alpha)C^{*}(\\theta,\\alpha)\\right]}\\\\ &{\\overset{(a)}{\\geq}\\int_{\\theta_{\\alpha}\\in\\mathcal{A}^{+}}\\int_{(0,t)}^{\\theta}\\sum_{c^{\\prime}}(\\theta,\\alpha,\\gamma)\\tilde{w}_{\\tau}(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta}\\\\ &{\\quad-\\int_{\\theta_{\\alpha}\\in A^{+}}\\int_{(0,t)}^{\\theta}\\sum_{c^{\\prime}}(\\theta,\\alpha,\\gamma)v^{\\prime}(\\theta)\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta}\\\\ &{=\\int_{\\theta_{\\alpha}\\in A^{+}}\\hat{\\phi}_{\\tau}^{*}(\\theta,\\alpha)\\int_{\\gamma}^{\\epsilon}(\\theta,\\alpha,\\gamma)\\tilde{w}_{\\tau}(\\theta)\\,\\mathrm{d}\\hat{w}_{\\tau}(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta}\\\\ &{\\quad+\\int_{\\theta_{\\alpha}\\in A^{+}}\\hat{w}_{\\tau}(\\theta,\\alpha)\\int_{\\gamma}^{\\epsilon}(\\theta,\\alpha,\\gamma)(\\hat{w}_{\\tau}(\\hat{r})-v(\\theta))\\,\\mathrm{d}\\hat{w}_{\\tau}\\,\\mathrm{d}\\theta}\\\\ &{\\quad+\\int_{\\theta_{\\alpha}\\in A^{+}}\\hat{w}_{\\tau}(\\theta,\\alpha)\\int_{\\gamma}^{\\epsilon}\\hat{c}(\\theta,\\alpha,\\gamma)(\\hat{w}_{\\tau}(\\hat{r})-v(\\gamma))n(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta}\\\\ &{\\overset{(b)}{\\geq}-\\operatorname*{sup}_{i\\in\\mathcal{B}^{+}}(\\theta)-\\hat{w}_{\\tau}(\\theta)-\\operatorname*{sup}_{{\\tau}}|(v(\\gamma)-\\hat{v}_{\\tau}(\\gamma))|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the above, (a)is by the constraint feasibility of $\\widehat{\\phi}_{\\tau}^{*}$ , and (b) is because $\\begin{array}{r}{\\sum_{a\\in A^{+}}\\widehat{\\phi}_{\\tau}^{*}(\\theta,a)\\leq1}\\end{array}$ holds for any $\\theta\\in\\Theta$ . Further, by Proposition A.1, we have for $\\tau=\\Omega(1)$ \uff0c ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\underset{\\theta}{\\operatorname*{sup}}\\,|u(\\theta)-\\widehat{u}_{\\tau}(\\theta)|\\leq-\\Theta\\left(\\sqrt{\\log T}(\\tau-1)^{\\alpha_{u}-1}\\right)\\right]\\leq\\frac{1}{T^{2}},}\\\\ &{\\operatorname*{Pr}\\left[\\underset{\\gamma}{\\operatorname*{sup}}\\,|v(\\theta)-\\widehat{v}_{\\tau}(\\theta)|\\leq-\\Theta\\left(\\sqrt{\\log T}(\\tau-1)^{\\alpha_{v}-1}\\right)\\right]\\leq\\frac{1}{T^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, when $t=\\Omega(1)$ , we derive that with failure probability ${\\cal O}(n/T)$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tan\\left(\\rho_{1}-\\rho_{t},0\\right)\\le\\Theta\\left(\\frac{1}{T-1}+\\sqrt{\\log T}\\sum_{\\tau=\\Theta(1)}^{t-1}\\left(\\frac{(\\tau-1)^{\\alpha_{u}-1}}{T-\\tau}+\\frac{(\\tau-1)^{\\alpha_{v}-1}}{T-\\tau}\\right)+\\sqrt{\\frac{\\log T}{T-t}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Further, for $p\\in\\{u,v\\}$ , when $t\\le(T+1)/2$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{\\tau=\\Theta(1)}^{t-1}\\frac{(\\tau-1)^{\\alpha_{p}-1}}{T-\\tau}\\leq\\frac{2}{T-1}\\sum_{\\tau=2}^{t-1}(\\tau-1)^{\\alpha_{p}-1}\\leq\\frac{2(t-2)^{\\alpha_{p}}}{\\alpha_{p}(T-1)};\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and when $t>(T+1)/2$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{\\tau=\\Theta(1)}^{t-1}\\frac{(\\tau-1)^{\\alpha_{p}-1}}{T-\\tau}\\leq\\frac{1}{\\alpha_{p}}\\left(\\frac{2}{T-1}\\right)^{1-\\alpha_{p}}+\\sum_{\\tau=(T+1)/2}^{t-1}(T-\\tau)^{\\alpha_{p}-2}\\leq\\frac{(T-t)^{\\alpha_{p}-1}}{1-\\alpha_{p}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, when $t=T-\\Theta(\\log^{(2(1-\\operatorname*{max}\\{1/2,\\alpha_{u},\\alpha_{v}\\}))^{-1}}T)$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta\\left(\\displaystyle\\frac{1}{T-1}+\\sqrt{\\log T}\\sum_{\\tau=\\Theta(1)}^{t-1}\\left(\\frac{(\\tau-1)^{\\alpha_{u}-1}}{T-\\tau}+\\frac{(\\tau-1)^{\\alpha_{v}-1}}{T-\\tau}\\right)+\\sqrt{\\frac{\\log T}{T-t}}\\right)}\\\\ &{\\leq\\rho^{\\mathrm{min}}-\\displaystyle\\frac{1}{T-t+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which leads to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[T-T_{0}\\right]=O\\left(\\log^{(2(1-\\operatorname*{max}\\{1/2,\\alpha_{u},\\alpha_{v}\\}))^{-1}}T\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This concludes the analysis of the stopping time. ", "page_idx": 37}, {"type": "text", "text": "For the second part, By (24), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J(\\rho_{1})-\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]}\\\\ &{=\\Big(J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\Big)+\\left(\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "On the second difference term, similar to the proof of Lemma E.2, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\widehat{\\boldsymbol{J}}}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}\\left[\\displaystyle\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]}\\\\ &{=\\displaystyle\\int_{\\theta}\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)\\int_{\\gamma}r(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\widehat{u}_{t}(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta-\\int_{\\theta}\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)\\int_{\\gamma}r(\\theta,a,\\gamma)v(\\gamma)u(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta}\\\\ &{\\leq\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|+\\operatorname*{sup}_{\\gamma}|(v(\\gamma)-\\widehat{v}_{t}(\\gamma)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, when $t=\\Omega(1)$ , by taking $\\epsilon=1/T$ in Proposition A.1 and (27), we arrive that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right]=O\\left(\\sqrt{\\log T}\\left((t-1)^{\\alpha_{u}-1}+(t-1)^{\\alpha_{v}-1}\\right)+\\frac{1}{T}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We now focus on $J(\\pmb{\\rho}_{1})-\\widehat{J}(\\pmb{\\rho}_{t},\\mathcal{H}_{t})$ . We let ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\delta_{t}:=\\frac{\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|+\\operatorname*{sup}_{\\gamma}|(v(\\gamma)-\\widehat{v}_{t}(\\gamma)|}{\\operatorname*{min}_{\\theta\\in\\Theta,a\\in A^{+}}\\{\\operatorname*{min}\\{u(\\theta)C(\\theta,a)>0\\}\\}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We prove that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\widehat{u}_{t}(\\theta)\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\,\\mathrm{d}\\gamma\\leq(1+\\delta_{t})u(\\theta)\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)v(\\gamma)\\,\\mathrm{d}\\gamma\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "holds for any $(\\theta,a,i)$ tuple, which is obvious if $c^{i}(\\theta,a,\\gamma)$ is almost surely zero with respect to $\\gamma$ Otherwise, we observe that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widehat{u}_{t}(\\theta)\\displaystyle\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\,\\mathrm{d}\\gamma-u(\\theta)\\displaystyle\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)v(\\gamma)\\,\\mathrm{d}\\gamma}\\\\ &{=\\left(\\widehat{u}_{t}(\\theta)-u(\\theta)\\right)\\displaystyle\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\,\\mathrm{d}\\gamma+u(\\theta)\\displaystyle\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)(\\widehat{v}_{t}(\\gamma)-v(\\gamma))\\,\\mathrm{d}\\gamma}\\\\ &{\\leq\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|+\\operatorname*{sup}_{\\gamma}|(v(\\gamma)-\\widehat{v}_{t}(\\gamma)|}\\\\ &{\\leq\\delta_{t}u(\\theta)\\displaystyle\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)v(\\gamma)\\,\\mathrm{d}\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and thus, with $\\eta_{t}:=1-\\delta_{t}\\leq1/(1+\\delta_{t})$ , we derive that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{u(\\theta)\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)v(\\gamma)\\,\\mathrm{d}\\gamma\\leq\\frac{1}{1+\\delta_{t}}\\widehat{u}_{t}(\\theta)\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\,\\mathrm{d}\\gamma}}\\\\ &{}&{\\leq\\eta_{t}\\widehat{u}_{t}(\\theta)\\int_{\\gamma}c^{i}(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\,\\mathrm{d}\\gamma.~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This proves the above inequality. Thus, for an optimal solution $\\phi_{t}^{*}$ of $J(\\rho_{t})$ ,we see that $\\eta_{t}\\phi_{t}^{*}$ is a feasible solution of the programming $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ . Thus, we notice that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\geq\\eta_{t}\\int_{\\theta}\\sum_{a\\in A^{+}}\\phi_{t}^{*}(\\theta,a)\\int_{\\gamma}r(\\theta,a,\\gamma)\\widehat{v}_{t}(\\gamma)\\widehat{u}_{t}(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\eta_{t}\\displaystyle\\int_{\\theta}\\sum_{a\\in A^{+}}\\phi_{t}^{*}(\\theta,a)\\displaystyle\\int_{\\gamma}r(\\theta,a,\\gamma)v(\\gamma)u(\\theta)\\,\\mathrm{d}\\gamma\\,\\mathrm{d}\\theta}\\\\ &{\\ -\\ \\eta_{t}(\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|+\\operatorname*{sup}_{\\gamma}|(v(\\gamma)-\\widehat{v}_{t}(\\gamma)|)}\\\\ &{=\\eta_{t}J(\\rho_{t})-(\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|+\\operatorname*{sup}_{\\gamma}|(v(\\gamma)-\\widehat{v}_{t}(\\gamma)|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "With respect to (26), we obtain that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})}\\\\ &{\\leq J(\\rho_{1})\\cdot\\left(1-\\eta_{t}+\\frac{\\operatorname*{max}{\\left(\\rho_{1}-\\rho_{t},0\\right)}}{\\rho^{\\operatorname*{min}}}\\right)+\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|+\\operatorname*{sup}_{\\gamma}|(v(\\gamma)-\\widehat{v}_{t}(\\gamma)|}\\\\ &{=J(\\rho_{1})\\cdot\\left(\\delta_{t}+\\frac{\\operatorname*{max}{\\left(\\rho_{1}-\\rho_{t},0\\right)}}{\\rho^{\\operatorname*{min}}}\\right)+\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|+\\operatorname*{sup}_{\\gamma}|(v(\\gamma)-\\widehat{v}_{t}(\\gamma)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now, when $t=\\Theta(1)$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{\\theta}{\\operatorname*{sup}}\\,|u(\\theta)-\\widehat{u}_{t}(\\theta)|\\right]=O\\left(\\sqrt{\\log T}(t-1)^{\\alpha_{u}-1}+\\frac{1}{T}\\right),}\\\\ &{\\mathbb{E}\\left[\\underset{\\gamma}{\\operatorname*{sup}}\\,|v(\\gamma)-\\widehat{v}_{t}(\\gamma)|\\right]=O\\left(\\sqrt{\\log T}(t-1)^{\\alpha_{v}-1}+\\frac{1}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By the previous reasoning on $\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)$ , we obtain that when $t=\\Omega(1)$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)\\right]}\\\\ &{\\leq\\Theta\\left(\\displaystyle\\frac{1}{T-1}+\\sqrt{\\log T}\\displaystyle\\sum_{\\tau=\\Theta(1)}^{t-1}\\left(\\frac{(\\tau-1)^{\\alpha_{u}-1}}{T-\\tau}+\\frac{(\\tau-1)^{\\alpha_{v}-1}}{T-\\tau}\\right)+\\sqrt{\\frac{\\log T}{T-t}}+\\frac{n}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, summing from $t=1$ to $T_{0}\\leq T$ , we achieve that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\left(J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\right)\\right]=O\\left((T^{1/2}+T^{\\alpha_{u}}+T^{\\alpha_{v}})\\sqrt{\\log T}+n\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining with previous bounds on $\\mathbb{E}[T-T_{0}]$ and the estimation errors, we derive the theorem. ", "page_idx": 38}, {"type": "text", "text": "G.3Proof of Theorem A.2 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Similar to the proof of Theorem 5.2, we concentrate on re-bounding the three terms under partial information fedback, respectively $\\begin{array}{r}{\\mathbb{E}[T\\!-\\!T_{0}],\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\!-\\!\\mathbb{E}_{\\theta}[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)]}\\end{array}$ and $J(\\rho_{1})-$ $\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})$ .Asfor $\\mathbb{E}[T-T_{0}]$ , with Lemma 4.1, we argue here that the main term in bounding $\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)$ when $t=\\Theta(T)$ becomes ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Theta\\left(\\sqrt{\\log T}\\left(\\sum_{\\tau=\\Theta(1)}^{t-1}\\frac{(\\tau-1)^{\\alpha_{u}-1}}{T-\\tau}+\\sum_{\\tau=\\Theta(1)}^{\\Theta(T)}\\frac{((\\tau-1)/\\log T)^{\\alpha_{v}-1}}{T-\\tau}+\\sum_{\\tau=\\Theta(T)}^{t-1}\\frac{(\\tau-1)^{\\alpha_{v}-1}}{T-\\tau}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Consequently, when $t$ is close to $T$ , we have with failure probability ${\\cal O}(1/T)$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\operatorname*{max}(\\rho_{1}-\\rho_{t},0)}\\\\ &{\\le\\Theta\\left(\\displaystyle\\frac{1}{T-1}+\\sqrt{\\log T}\\left((T-t)^{\\alpha_{u}-1}+(T-t)^{-1/2}\\right)+(T-t)^{\\alpha_{v}-1}\\log^{3/2-\\alpha_{v}}T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This leads to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[T-T_{0}]=O\\left(\\log^{\\operatorname*{max}(1,1/(2-2\\alpha_{u}),(3-2\\alpha_{v})/(2-2\\alpha_{v}))}T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the estimation error term $\\begin{array}{r}{\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)]}\\end{array}$ , when $\\Omega(1)\\le t\\le\\Theta(T)$ the bound now becomes ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})-\\mathbb{E}_{\\theta}\\left[\\sum_{a\\in A^{+}}\\widehat{\\phi}_{t}^{*}(\\theta,a)R(\\theta,a)\\right]\\right]}\\\\ &{=O\\left(\\sqrt{\\log T}(t-1)^{\\alpha_{u}-1}+\\log^{3/2-\\alpha_{v}}T\\cdot(t-1)^{\\alpha_{v}-1}+\\frac{1}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "At last, for $J(\\pmb{\\rho}_{1})-\\widehat{J}(\\pmb{\\rho}_{t},\\mathcal{H}_{t})$ , we derive that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{0}}\\operatorname*{max}(\\rho_{1}-\\rho_{t},0)\\right]=O\\left(\\sqrt{\\log T}\\left(T^{\\alpha_{u}}+T^{1/2}\\right)+\\log^{3/2-\\alpha_{v}}T\\cdot T^{\\alpha_{v}}+n\\right),}\\\\ &{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{0}}\\operatorname*{sup}_{\\theta}|u(\\theta)-\\widehat{u}_{t}(\\theta)|\\right]=O\\left(\\sqrt{\\log T}\\cdot T^{\\alpha_{u}}\\right),}\\\\ &{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T_{0}}\\operatorname*{sup}_{\\gamma}|v(\\gamma)-\\widehat{v}_{t}(\\gamma)|\\right]=O\\left(\\log^{3/2-\\alpha_{v}}T\\cdot T^{\\alpha_{v}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Putting together, we obtain that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T_{0}}\\left(J(\\rho_{1})-\\widehat{J}(\\rho_{t},\\mathcal{H}_{t})\\right)\\right]=O\\left(\\sqrt{\\log T}\\left(T^{\\alpha_{u}}+T^{1/2}\\right)+\\log^{3/2-\\alpha_{v}}T\\cdot T^{\\alpha_{v}}+n\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Synthesizing all the above, we finish the proof of the theorem. ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work performed by the authors. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions and a complete (and correct) proof for each theoretical result. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper provides open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u25cf At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u25cf Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper specifies all the training and test details necessary to understand the results. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u25cf The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper reports error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u25cf The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources needed to reproduce the experiments. All experiments can be conducted on a personal computer. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u25cf The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u25cf If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u25cf The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper discusses both potential positive societal impacts and negative societal impacts of the work performed. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u25cf The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u25cf If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u25cf The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not use existing assets. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u25cf For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u25cf At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u25cf The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u25cf The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}]