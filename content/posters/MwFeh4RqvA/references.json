{"references": [{"fullname_first_author": "Alex Nichol", "paper_title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models", "publication_date": "2021-12-10", "reason": "This paper is foundational for text-to-image generation using diffusion models, a core concept used in the current research."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper significantly improved image quality in text-to-image generation by using latent diffusion models, which the current research builds upon."}, {"fullname_first_author": "Chitwan Saharia", "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding", "publication_date": "2022-12-01", "reason": "This paper demonstrates the capabilities of combining diffusion models and large language models for high-quality and more faithful text-to-image generation."}, {"fullname_first_author": "Omer Bar-Tal", "paper_title": "Multidiffusion: Fusing diffusion paths for controlled image generation", "publication_date": "2023-07-01", "reason": "This paper introduces a multi-layer approach to image generation, which directly addresses the limitations of single-stage diffusion models."}, {"fullname_first_author": "Yuheng Li", "paper_title": "Gligen: Open-set grounded text-to-image generation", "publication_date": "2023-06-01", "reason": "This paper presents a method for layout-based image generation, addressing the need for more control and structure in generating images from text prompts, a problem tackled by the current paper."}]}