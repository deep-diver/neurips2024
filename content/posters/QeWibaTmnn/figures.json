[{"figure_path": "QeWibaTmnn/figures/figures_1_1.jpg", "caption": "Figure 1: Our Language-guided Task vs. Traditional Dexterous Grasp Tasks. Traditional methods focus either solely on grasp quality or on fixed and limited functionalities. Our approach enables the generation of dexterous grasps based on human language, enhancing natural human-robot interactions.", "description": "This figure compares the proposed language-guided dexterous grasp generation task with traditional approaches. Traditional methods focus either on grasp quality or fixed functionalities, lacking the flexibility of language-based control. The proposed method allows robots to generate dexterous grasps based on human language instructions, enabling more natural human-robot interaction.", "section": "1 Introduction"}, {"figure_path": "QeWibaTmnn/figures/figures_2_1.jpg", "caption": "Figure 2: Visualization of the impact of penetration loss (Pen. in the figure) on grasp performance: intention alignment, quality, and diversity. (a) illustrates penetration loss causes intention misalignment and its absence results in severe object penetration. (b) shows three sampling results under the same conditions, and demonstrates that penetration loss leads to reduced diversity.", "description": "This figure shows the effects of using a penetration loss term during training of a dexterous grasp generation model.  The left panel (a) compares models trained with and without penetration loss.  It shows that including penetration loss causes a misalignment between the intended grasp and the actual grasp, and that excluding it results in the hand penetrating the object. The right panel (b) compares grasp diversity, showing that models trained without penetration loss produce more diverse grasps, but the grasps may be infeasible due to penetration. The figure highlights the tradeoff between generating feasible grasps and achieving diverse, intent-aligned grasps, leading to the adoption of a two-stage training approach in the proposed DexGYSGrasp framework.", "section": "2 Related work"}, {"figure_path": "QeWibaTmnn/figures/figures_3_1.jpg", "caption": "Figure 3: The construction process of the DexGYSNet dataset. (a) The HOIR strategy retargets the human hand to the dexterous hand by three steps, maintaining hand-object interaction consistency and avoiding physical infeasibility (shown in black circle). (b) The annotation system automatically annotates language guidance for hand-object pairs with the help of LLM.", "description": "This figure illustrates the two-stage process of creating the DexGYSNet dataset.  The first stage, (a) Hand-Object Interaction Retargeting (HOIR), shows how human hand poses are transferred to robotic dexterous hands while maintaining hand-object contact. This involves three steps: initialization, fingertip alignment, and interaction refinement. The second stage, (b) LLM-assisted Language Guidance Annotation, uses a Large Language Model (LLM) to automatically generate natural language instructions based on the hand-object interaction data.", "section": "3 DexGYSNet Dataset"}, {"figure_path": "QeWibaTmnn/figures/figures_4_1.jpg", "caption": "Figure 4: Quantitative experimental results with different object penetration loss weights Apen. Intention is quantified by the Chamfer distance (CD) between predictions and targets. Diversity is assessed by the standard deviation of hand translation dt. Object penetration is evaluated by the penetration depth (Pen.) from the object point cloud to the hand mesh. Our method uniquely achieves high performance in terms of intention consistency, diversity, and penetration avoidance.", "description": "This figure shows the quantitative experimental results obtained with different object penetration loss weights (\u03bbpen).  It compares the performance of the proposed method against baselines across three key metrics: Intention Consistency (measured by Chamfer Distance, CD), Diversity (measured by standard deviation of hand translation, \u03b4t), and Object Penetration (Penetration depth).  The results demonstrate that the proposed method effectively balances intention alignment, grasp diversity, and penetration avoidance, outperforming the baselines.", "section": "4 DexGYSGrasp framework"}, {"figure_path": "QeWibaTmnn/figures/figures_5_1.jpg", "caption": "Figure 5: Overview of our framework. (a) With only the regression loss, intention and diversity grasp component is trained to reconstruct the original hand pose from the noise poses, based on language and object condition. (b) With both regression and penetration losses, Quality Grasp Component is trained to refine the coarse pose improve the grasp quality while maintain intension consistency.", "description": "This figure shows the architecture of the DexGYSGrasp framework, which consists of two components: the Intention and Diversity Grasp Component and the Quality Grasp Component. The first component focuses on generating diverse and intention-aligned grasps using only regression loss, while the second component refines the grasps to improve quality by incorporating both regression and penetration losses.  A progressive training strategy is employed, with the first component's output used as input to the second. The figure illustrates the flow of information through both components and highlights the role of different loss functions in the training process.", "section": "4.2 Progressive Grasp Components"}, {"figure_path": "QeWibaTmnn/figures/figures_6_1.jpg", "caption": "Figure 6: Visualization of generated dexterous grasp. The top visualizes one sample for each object and guidance pair. The bottom visualizes four samples, the bottom left shows that the generated grasp are consistent with clear and specific guidance, while the bottom right shows that the diversity achieved under relatively ambiguous instructions.", "description": "This figure visualizes examples of dexterous grasps generated by the proposed DexGYSGrasp framework.  The top row shows one example grasp for each object and corresponding language instruction demonstrating the system's ability to generate accurate grasps based on clear instructions. The bottom row showcases four examples of grasps generated for the same object but with more ambiguous instructions, highlighting the framework's capacity to generate diverse grasps that still align with the intent of the instruction.", "section": "5 Experiments"}, {"figure_path": "QeWibaTmnn/figures/figures_8_1.jpg", "caption": "Figure 7: Visualization of real world experiments.", "description": "This figure shows the results of real-world experiments using the DexGYSGrasp framework with an Allegro hand, a Flexiv Rizon 4 arm, and an Intel Realsense D415 camera.  The experiments involved several objects, each with multiple grasp instructions given as natural language commands.  The images illustrate the successful execution of the grasps, demonstrating the system's ability to perform dexterous manipulation according to human language instruction. The results show varying degrees of success (3/10 to 9/10), highlighting the current limitations and the need for improvement in real-world applications.", "section": "5.7 Experiments in Real World"}, {"figure_path": "QeWibaTmnn/figures/figures_14_1.jpg", "caption": "Figure 5: Overview of our framework. (a) With only the regression loss, intention and diversity grasp component is trained to reconstruct the original hand pose from the noise poses, based on language and object condition. (b) With both regression and penetration losses, Quality Grasp Component is trained to refine the coarse pose improve the grasp quality while maintain intension consistency.", "description": "This figure shows the overall architecture of the DexGYSGrasp framework, which consists of two main components: the Intention and Diversity Grasp Component and the Quality Grasp Component.  The first component, using only regression loss, reconstructs the original hand pose from noisy data, conditioned on language and object information. The second component refines the initially generated grasp by incorporating both regression and penetration losses to improve grasp quality while maintaining intention consistency.", "section": "4.2 Progressive Grasp Components"}, {"figure_path": "QeWibaTmnn/figures/figures_15_1.jpg", "caption": "Figure 9: The Extension of DexGYSNet to more dexterous hands.", "description": "This figure shows the generalization of the DexGYSNet dataset to various dexterous hands.  It displays the grasps generated for several common objects (bottle, camera, frying pan, game controller, mug, and pincer) across four different hand models: a human hand, a Shadow Hand, an Allegro Hand, and a Leap Hand. Each row presents the grasp poses generated for the same language guidance instruction, highlighting the consistent generation across the different hand types and demonstrating the flexibility and adaptability of the DexGYSNet dataset.", "section": "3 DexGYSNet Dataset"}, {"figure_path": "QeWibaTmnn/figures/figures_16_1.jpg", "caption": "Figure 10: (a) Evaluation of intention consistency using the Fr\u00e9chet Inception Distance between the <generation hand and object> and the ground truth. (b) When the ground truth is not available (e.g., evaluation on a 3D object dataset), we employ GPT4-o for evaluation.", "description": "This figure shows two ways to evaluate the intention consistency of the generated grasps. (a) shows how the Fr\u00e9chet Inception Distance (FID) is used to compare the generated grasp with the ground truth. (b) explains that when the ground truth is unavailable (like in 3D object datasets), the LLM GPT-4 is used to assess the consistency between the generated grasp and a text description of the intended action.", "section": "A.3.2 Metrics Details"}, {"figure_path": "QeWibaTmnn/figures/figures_17_1.jpg", "caption": "Figure 2: Visualization of the impact of penetration loss (Pen. in the figure) on grasp performance: intention alignment, quality, and diversity. (a) illustrates penetration loss causes intention misalignment and its absence results in severe object penetration. (b) shows three sampling results under the same conditions, and demonstrates that penetration loss leads to reduced diversity.", "description": "This figure shows the effect of using a penetration loss term in training a dexterous grasp model.  The top row (a) demonstrates that using penetration loss causes misaligned grasps (inconsistent with the intended action), whereas not using it causes the hand to penetrate the object. The bottom row (b) shows that using penetration loss also reduces the diversity of generated grasps. This highlights the challenge of simultaneously optimizing for intention alignment, grasp quality, and diversity, and motivates the use of a progressive training approach.", "section": "2 Related work"}, {"figure_path": "QeWibaTmnn/figures/figures_17_2.jpg", "caption": "Figure 12: Visualization of our DexGYSGrasp framework with task-oriented simple input.", "description": "This figure demonstrates the flexibility of the DexGYSGrasp framework in handling task-oriented instructions.  Instead of detailed language descriptions, simple commands like \"use\" and \"hold\" are used as input.  The figure shows examples of generated grasps for various objects (mug, tablet, camera) under these simplified instructions, highlighting the system's ability to adapt to different task types.", "section": "4.2 Progressive Grasp Components"}, {"figure_path": "QeWibaTmnn/figures/figures_18_1.jpg", "caption": "Figure 13: The illustration of our real world experiments settings.", "description": "The image shows the physical setup for the real-world experiments.  A Flexiv Rizon 4 arm, an Allegro Hand, and an Intel Realsense D415 camera are shown.  Various 3D printed objects are also visible, indicating the test objects used. The setup involves a robotic arm equipped with a dexterous hand, positioned to interact with the objects in front of a dark background. A depth camera is used to capture the 3D information for the scene.", "section": "5.7 Experiments in Real World"}, {"figure_path": "QeWibaTmnn/figures/figures_18_2.jpg", "caption": "Figure 14: Real world experiments pipeline.", "description": "This figure shows the pipeline used to obtain the full point cloud from real-world RGB and depth images.  First, Grounding DINO detects the object's bounding box in the RGB image. Then, SAM is used to generate a segmentation mask. This mask is used to crop the relevant part of the depth image, generating a partial point cloud. Finally, a point cloud completion network takes the partial point cloud to generate a complete full point cloud which is then used in the DexGYSGrasp framework. ", "section": "5.7 Experiments in Real World"}, {"figure_path": "QeWibaTmnn/figures/figures_19_1.jpg", "caption": "Figure 7: Visualization of real world experiments.", "description": "The figure visualizes real-world experiments showing the robot performing dexterous grasps based on various language commands.  It demonstrates the robot's ability to successfully manipulate objects like a power drill, trigger sprayer, game controller, pincer, frying pan, and wine glass, by adapting its grasp according to the instructions.", "section": "5 Experiments"}]