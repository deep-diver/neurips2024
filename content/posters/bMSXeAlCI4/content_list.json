[{"type": "text", "text": "Entropy testing and its application to testing Bayesian networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cle\u00b4ment L. Canonne University of Sydney clement.canonne@sydney.edu.au ", "page_idx": 0}, {"type": "text", "text": "Joy Qiping Yang University of Sydney qyan6238@uni.sydney.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper studies the problem of entropy identity testing: given sample access to a distribution $p$ and a fully described distribution $q$ (both are discrete distributions over the support of size $k$ ), and the promise that either $p=q$ or $|H(p){-}H(q)|\\geqslant\\varepsilon$ , where $H(\\cdot)$ denotes the Shannon entropy, a tester needs to distinguish between the two cases with hi\u221agh probability. We establish a near-optimal sample complexity bound of $\\tilde{\\Theta}(\\sqrt{k}/\\varepsilon+1/\\varepsilon^{2})$ for this problem, and show how to apply it to the problem of identity testing for in-degree- $\\textit{d n}$ -dimensional Bayesian networks, obtaining an upper bound of ${\\tilde{O}}\\!\\left(2^{d/2}{\\bar{n_{/}}}^{*}+n^{2}/\\varepsilon^{4}\\right)$ . This improves on the sample complexity bound of ${\\tilde{O}}(2^{d/2}n^{\\mathrm{{j}}}/\\varepsilon^{4})$ from [CDKS20], which required an additional assumption on the structure of the (unknown) Bayesian network. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Entropy is a fundamental information theory notion, which quantifies the amount of \u201cuncertainty\u201d a given random variable carries. Since its introduction by Shannon, this notion has found myriads of applications, and is central \u2013 among others \u2013 to compression and coding, probability, electrical engineering, and learning theory. ", "page_idx": 0}, {"type": "text", "text": "As a result, the task of estimating the Shannon entropy of a discrete random variable (or, equivalently, its probability distribution) from samples has naturally emerged, starting (in Computer Science) with the work of [BDKR02] which considered multiplicative approximations. Additive approximation of the entropy (within $\\pm\\varepsilon$ ) was then considered in a series of papers [VV11a, VV11b, VV13, HJW15a, ADOS17], culminating with the work of [WY16], which establishes the optimal sample complexity, $\\begin{array}{r}{\\Theta\\!\\Big(\\frac{k}{\\varepsilon\\log k}+\\frac{\\log^{2}k}{\\varepsilon^{2}}\\Big)}\\end{array}$ ,1 where $k\\gg1$ is the domain size. ", "page_idx": 0}, {"type": "text", "text": "While the resulting sample complexity is sublinear in the domain size $k$ , it is only so by a mere logarithmic factor. In some settings, paying this near-linear dependence in the amount of data necessary is impractical, typically in the large-domain regime (e.g., for high-dimensional data, where $k$ is exponential in the dimension); moreover, it may even be unnecessary. Specifically, one may not be concerned so much about the (approximate) value of the entropy of a distribution, but rather about whether it is above a threshold, or differs from that of a given purported model. ", "page_idx": 0}, {"type": "text", "text": "It is this latter task we introduce and consider in our work, which can be seen as a variant of the standard identity testing question from distribution testing: given a reference known hypothesis distribution $q$ over a domain of size $k$ , and i.i.d. samples from an unknown distribution $p$ , what is the sample complexity of testing whether $p$ is equal to $q$ , or their entropies differ significantly? And, crucially, is this testing task more sample-efficient than that of estimating $H(p)$ ? ", "page_idx": 0}, {"type": "text", "text": "Note that in the case where $q$ is the uniform distribution over the domain, this task is equivalent to distinguishing between $H(p)=\\log k$ and $H(p)<\\log k-\\varepsilon$ . ", "page_idx": 1}, {"type": "table", "img_path": "bMSXeAlCI4/tmp/75a5b77ff027fe1f8426e6c4172c2ebd4d12bb120f87ed24c58f6746bc23ecde.jpg", "table_caption": [], "table_footnote": ["samples are necessary in the worst case. "], "page_idx": 1}, {"type": "text", "text": "Our main contribution is to show that the testing question can indeed be performed much more efficiently than the estimation one, at least for most parameter regimes. Specifically, we establish the following theorem: ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1. The sample complexity of entropy identity testing is $O\\Big(\\sqrt{k\\log(k/\\varepsilon)}/\\varepsilon+\\log^{2}(k)/\\varepsilon^{2}\\Big)$ . Moreover, this is nearly tight: $\\Omega\\left(\\sqrt{k}/\\varepsilon+\\log^{2}k/\\varepsilon^{2}\\right)$ ", "page_idx": 1}, {"type": "text", "text": "Interestingly, this differs both from the estimation task (which, as discussed before, has a near-linear dependence on the domain\u221a size $k$ ) but also from identity testing in total variation distance, which has sample complexity $\\Theta({\\sqrt{k}}/\\varepsilon^{2})$ (see Section 1.1). ", "page_idx": 1}, {"type": "text", "text": "Application: Identity testing for Bayesian networks. As an application of Theorem 1.1, we derive an efficient algorithm for identity testing (in total variation distance) for maximum in-degree $d$ Bayesian networks (shorten as degree- $d$ Bayes net in the remaining of the paper):2 ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.2 (Informal; see Theorem 3.1). There is an algorithm which, given sample access to a degree-d Bayes net p and the full description of a reference degree- $d$ Bayes net $q$ (both over $\\{0,1\\}^{n})$ , takes O\u02dc 2d/22n $\\begin{array}{r}{\\tilde{O}\\Big(\\frac{2^{d/2}n}{\\varepsilon^{2}}+\\frac{n^{2}}{\\varepsilon^{4}}\\Big)}\\end{array}$ samples from $p$ , and distinguishes between $p=q$ and $d_{\\mathrm{TV}}(p,q)\\geq\\varepsilon$ . ", "page_idx": 1}, {"type": "text", "text": "Prior to this, the best known sample complexity upper bound for this task [CDKS20] was quadratically worse in both $n$ and $\\varepsilon$ , and further required an assumption on the underlying graph structure of both $p$ and $q$ . We emphasize that (1) our result improves on the sample complexity of the learning baseline for $d\\gg\\log(n/\\varepsilon)$ , and on its computational efficiency; and (2) compared to the previous testing results, removes strong structural assumptions which considerably limited their applicability. We elaborate on this in the next section. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "As previously discussed, entropy estimation has received a considerable amount of interest from computer scientists, information theorists and statisticians [BDKR02, Pan04, HJW15a, WY16]. Entropy is also a key example of symmetric property (invariant to relabeling of the domain) [VV11a, VV11b, VV13, ADOS17], and has been considered in other settings as well, e.g., the quantum case [GHS21, AISW20] and the memory-limited setting [ABIS19, AMNW22]. Estimation of some generalizations of Shannon entropy, such as the family of Re\u00b4nyi entropies, also have been studied [AOST17]. ", "page_idx": 1}, {"type": "text", "text": "Over the years, sample complexity of identity testing for discrete distribution has been intensively studied and essentially settled [Pan08, $\\mathrm{BFF^{+}01}$ , VV17]. In high dimensions, however, the square root dependence of the sample complexity on the domain size means that most identity testing tasks of interest require sample complexity exponential in the dimension. Moreover, this curse of dimensionality extends to a large range of distribution testing problems [BCY22, Theorem B.1]. As such, many turn to the study of testing distributions under additional natural structural assumptions, such as graphical models: [BGKV21] look at identity testing for product distributions (degree-0 Bayes nets) and give the optimal bound of $\\Theta({\\sqrt{n|\\Sigma|/\\varepsilon^{2}}})$ , where $|\\Sigma|$ is the alphabet size of each variable (rather than binary alphabet studied in our paper). [DDK19, KDDC23] study testing Ising models, obtaining sample complexity bounds that are $\\mathrm{poly}(n/\\varepsilon)$ ; [DP16], [CDKS20] give tight results to identity testing and closeness testing for a variety of constant in-degree Bayes nets, which also gives polynomial sample complexity bounds. ", "page_idx": 1}, {"type": "text", "text": "However, the testing algorithms provided in [CDKS20] and [DP16] are not fully satisfactory, as they require some strong assumptions on Bayes nets. Specifically, [CDKS20, Theorem 21] assumes that the topological ordering of the two Bayes nets are the same, and shows that under this assumption $O(2^{d/2}n^{2}/\\varepsilon^{4})$ samples are sufficient.3 [CDKS20, Theorem 17] makes the further stringent restriction that the reference Bayes net has to be balanced, i.e., that the conditional probabilities are all bounded away from 0 and 1; moreover, it also requires every parental configuration to be bounded from 0, and that the structure of the unknown Bayes net be a subset of that of the reference one. The result of [DP16, Theorem 4.2] combined with the Hellinger tester from [DKW18, Theorem 1] implies that, under the assumption that $p$ and $q$ share the same factorization structure (i.e., their associated DAGs are the same or one is a subgraph of the other), then this problem is solvable in $\\tilde{O}\\left(2^{d/2}n/\\varepsilon^{2}\\right)$ samples. While this latter sample complexity is near-optimal (in some regime4), in view of the $\\Omega\\left(2^{d/2}n/\\varepsilon^{2}\\right)$ lower bound obtained in [BCY22, Theorem 4.1], the factorization structure requirement considerably limits the applicability of the algorithm. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "One can also compare our result to the learning results on Bayesian networks, as any learning algorithms enables testing as well (the \u201ctesting-by-learning\u201d baseline). It is known [CDKS20]) that learning degree- $\\cdot d$ Bayes nets can be done with $\\bar{\\tilde{O}}(2^{d}n/\\varepsilon^{\\bar{2}})$ samples, without any structural assumptions. Our testing result improves on this sample complexity as long as $n^{2}/\\varepsilon^{4}\\,\\ll\\,2^{d}n/\\varepsilon^{2}$ , i.e., for $d\\gg\\log(n/\\bar{\\varepsilon})$ ; moreover, it is worth noting that the known learning algorithms are computationally inefficient (running in time $n^{O(d n)}$ via an enumeration of all possible underlying graph structures [CDKS20, BGMV20]), and this is believed to be inherent [CHM04]. In contrast, our algorithm runs in time poly $(n^{d},1/\\varepsilon)$ . ", "page_idx": 2}, {"type": "text", "text": "1.2 Techniques overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Testing in entropy. A first idea is to use the conversion between total variation (TV) distance and entropy difference to reduce this problem to identity testing in TV: When $d_{\\mathrm{TV}}(p,q)\\ \\leqslant$ $1/2$ , then $\\begin{array}{r}{|H(p)\\,-\\,H(q)|~\\leqslant~d_{\\mathrm{TV}}(p,q)\\log\\frac{k}{d_{\\mathrm{TV}}(p,q)}}\\end{array}$ [CK11, Lemma 2.7]. This gives an upper bound of $O\\big(\\frac{\\sqrt{k}\\log^{2}(k/\\varepsilon)}{\\varepsilon^{2}}\\big)$ , which is already better than the sample complexity of estimation: $\\begin{array}{r}{O\\left(\\frac{k}{\\varepsilon\\log k}+\\frac{\\log^{2}k}{\\varepsilon^{2}}\\right)}\\end{array}$ for the parameter $k$ . However, it is not clear whether the quadratic dependence on $\\varepsilon$ is necessary: indeed, the \u201chard instances\u201d for TV testing (the Paninski construction [Pan08]), small perturbations around the uniform distribut\u221aion which have TV distance $\\varepsilon$ from uniform, actually only have entropy $\\log k-\\Theta(\\varepsilon^{2})$ . \u221aThe $\\Omega(\\sqrt{k}/\\varepsilon^{2})$ uniformity testing lower bound from these hard instances thus only implies an $\\Omega(\\sqrt{k}/\\varepsilon)$ entropy identity testing lower bound! ", "page_idx": 2}, {"type": "text", "text": "A next natural idea is to strengthen the lower bound. However, it then becomes clear that the Paninski [Pan08] construction cannot be improved: as just mentioned, when its TV distance \u221ato the uniform distribution is around $\\Theta({\\sqrt{\\varepsilon}})$ its entropy difference to it is only $\\Theta(\\varepsilon)$ (giving an $\\Omega(\\sqrt{k}/\\varepsilon)$ lower bound). Moreover, this is not a coincidence: when the reference distribution $q$ is uniform, we are able to get a matching upper bound using [DKW18, Algorithm 1], upon noticing that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(p)=\\log k-d_{\\mathrm{KL}}(p\\|u_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which implies $d_{\\mathrm{KL}}(p\\|u_{k})\\,=\\,\\log k\\,-\\,H(p)\\,\\geqslant\\,\\varepsilon$ , where $u_{k}$ is the uniform distribution on $[k]$ and $d_{\\mathrm{KL}}$ denotes the Kullback\u2013Leibler divergence. Interestingly, a completely different hard instance, against a very much non-uniform reference distribution, does yield the second term of our lower bound, $\\Omega(\\log^{2}k/\\varepsilon^{2})$ . ", "page_idx": 2}, {"type": "text", "text": "Inspired by these two different lower bounds, we can generalize (1) by defining $\\boldsymbol{\\mathcal{A}}$ as the set of \u201cnot too small probability elements under $q^{*}$ , and then observing (looking ahead, using the inequality (12)) that ", "page_idx": 2}, {"type": "equation", "text": "$$\n|H(p_{A})-H(q_{A})|\\leqslant|d_{\\mathrm{KL}}(p_{A}\\|q_{A})|+\\left|\\sum_{i\\in A}(p_{i}-q_{i})\\log\\frac{1}{q_{i}}\\right|\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $H(p_{A})$ is the \u201centropy\u201d of the sub-distribution restricted to the set $\\boldsymbol{\\mathcal{A}}$ . In particular, this hints that one could solve the general problem by testing if either of the two terms on the right-hand-side is large. The name of the game now is to (i) choose the threshold for $\\boldsymbol{\\mathcal{A}}$ (i.e., what does it mean for an element to have \u201cnot too small probability under $q^{*}.$ ), and (ii) have algorithms to test whether these two quantities are noticeably large. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Let us focus on how to test the first term of (2). If $\\begin{array}{r}{\\operatorname*{min}_{i}q_{i}\\;\\geqslant\\;\\Omega\\;\\!\\left(\\frac{\\varepsilon}{k}\\right)}\\end{array}$ , we can adapt and use an algorithm of [DKW18] to efficiently test $d_{\\mathrm{KL}}(p\\|q)\\;\\geq\\;\\varepsilon$ vs. $p~=~q$ . In addition, if $\\log(1/q_{i})$ is bounded, then in fact, estimating the second term to $O(\\varepsilon)$ is possible as well. Thus it is natural to wonder if we can afford to neglect the region where $q_{i}\\,\\leqslant\\,{\\frac{\\varepsilon}{k}}$ . Indeed, the impact on entropy is at most ${\\cal O}(\\tau\\log(k/\\tau))$ if we are to remove regions with at most $O(\\tau)$ as mass. Thus, by adjusting the appropriate threshold, we can still detect difference in entropy even if we only test on elements with greater than \u03c4/k masses, where \u03c4 =log(\u03b5k/\u03b5). ", "page_idx": 3}, {"type": "text", "text": "The problem then becomes to check if $p$ puts more than $100\\cdot\\tau$ mass in $\\bar{A}=\\{i\\in[k]:q_{i}<\\tau/k\\}$ , which costs $O(1/\\tau)=O(\\log(k/\\varepsilon)/\\varepsilon)$ samples. If it does, then it cannot be the case that $p=q$ ; we can reject. After this stage, both $p(\\bar{\\mathcal{A}}),q(\\bar{\\mathcal{A}})\\leqslant O(\\tau)$ . To move forward, we need to check the influence on entropy: $H(p)$ and $H(q)$ . By Jensen\u2019s inequality and monotonicity of $\\begin{array}{r}{f(x)=x\\log\\frac{1}{x}}\\end{array}$ when $\\textstyle x<{\\frac{1}{e}}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\bar{A}}p_{i}\\log\\frac{1}{p_{i}}\\leqslant p(\\bar{A})\\log\\frac{k}{p(\\bar{A})}\\leqslant\\tau\\log\\frac{k}{\\tau}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, the impact on entropy will be at most $O\\left(\\tau\\log{\\frac{k}{\\tau}}\\right)$ . Setting $\\begin{array}{r}{\\tau=\\frac{\\varepsilon}{\\log(k/\\varepsilon)}}\\end{array}$ , this becomes $O(\\varepsilon)$ , which gives us the room to check if $|H(p_{A})-H(p_{A})|\\geqslant100\\varepsilon$ or $p_{\\mathcal{A}}=q_{\\mathcal{A}}$ . ", "page_idx": 3}, {"type": "text", "text": "Testing Bayesian networks. Similar to [DP16, Theorem 4.2],5 the identity testing algorithm is straight-forward: check every possible subset of size $d+1$ if $p_{L}=q_{L}$ or one of them is far apart, where $p_{L}$ denotes the marginalization of $p$ over subset $L$ .6 If $p=q$ , then obviously, all such tests will accept with high probability; and by subadditivity of squared Hellinger distance, this will guarantee that for any DAG, and a projection $p_{G}$ (resp. $q_{G}$ ) of $p$ (resp. $q$ ) unto $G$ (see, [DP16, Corollary 2.4]): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\mathrm{H}}(p_{G},q_{G})\\leqslant\\varepsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While it seems unintuitive that this method does not work without the common-structure assumption, directly extending this argument does not work. Indeed, if they $\\mathit{\\Delta}_{p}$ and $q$ ) do not share a common structure, then it is clear that $p\\neq q$ ; but the tricky part is the farness case, when $\\mathrm{d}_{\\mathrm{H}}(p,q)\\geqslant\\varepsilon$ \u221aand $p,q$ does not share a common structure; the tester could still accept if every $\\mathrm{d_{H}}(p_{L},q_{L})\\leqslant\\varepsilon/\\sqrt{n}$ , which only guarantees that $\\mathrm{d}_{\\mathrm{H}}(p_{G},q_{G})\\leqslant\\varepsilon$ for every $G$ . Our fix to the problem is to additionally check if there exists a common graph $\\tilde{G}$ where $\\mathrm{d}_{\\mathrm{H}}(p,p_{\\tilde{G}})$ and $\\mathrm{d}_{\\mathrm{H}}(q,q_{\\tilde{G}})$ are close. If it does, via triangular inequality, we can in fact show that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\mathrm{H}}(p,q)\\leqslant\\mathrm{d}_{\\mathrm{H}}(q,q_{\\tilde{G}})+\\mathrm{d}_{\\mathrm{H}}(q_{\\tilde{G}},p_{\\tilde{G}})+\\mathrm{d}_{\\mathrm{H}}(p,p_{\\tilde{G}})\\leqslant O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This handles the last possible issue. To this end, we will conduct local entropy identity test between $p$ and $q$ for subset of size $d+1$ and $d$ . The idea is that if all tests pass, then we can conclude that $p$ and $q$ are close in local entropy and thereafter, we can utilize entropy of $q$ to learn the graphical structure $[\\mathrm{KCG}^{+}23]$ of $p$ (which uses no additional samples). ", "page_idx": 3}, {"type": "text", "text": "Preliminaries and notation. The (Shannon) entropy $H$ of a discrete distribution $p$ supported on $[k]$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nH(p)=-\\sum_{i\\in[k]}p_{i}\\log p_{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The conditional entropy $H(p_{X}\\ \\mid\\ p_{Y})$ for $X$ supported on $\\mathcal{X}$ , and $Y$ on $\\boldsymbol{\\wp}$ , defined by the joint distribution $p_{X,Y}$ , can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\nH(p_{X}\\mid p_{Y})=-\\sum_{x\\in{\\mathcal{X}},y\\in{\\mathcal{Y}}}p(x,y)\\log{\\frac{p(x,y)}{p(y)}}=H(p_{X,Y})-H(p_{Y}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We adopt the entropy notation for a sub-probability vector $\\begin{array}{r}{H(q_{A})=\\sum_{i\\in\\mathcal{A}}q_{i}\\log\\frac{1}{q_{i}}}\\end{array}$ . Throughout this paper, we will use $e$ as base of the log and of the entropy. We will use $\\leftarrow$ for variable assignment. ", "page_idx": 3}, {"type": "text", "text": "We adopt the standard $O(\\cdot),\\Omega(\\cdot)$ and $\\Theta(\\cdot)$ asymptotic notation and use $\\widetilde{.}$ to hide any polylogarithmic factors in the argument. We will use various metrics or divergences on probability distributions: Kullback\u2013Leibler $(d_{\\mathrm{KL}})$ , Hellinger $(d_{\\mathrm{H}})$ , chi-squared $(d_{\\chi^{2}})$ , and total variation $(d_{\\mathrm{TV}})$ . We denote $p_{\\cal A}$ as restricting $p$ onto the elements in $\\boldsymbol{\\mathcal{A}}$ , and we denote distributional distances restricting on $\\boldsymbol{\\mathcal{A}}$ as follows: $\\begin{array}{r}{d_{\\mathrm{KL}}(p_{A},q_{A})=\\sum_{i\\in\\mathcal{A}}p_{i}\\log\\frac{p_{i}}{q_{i}}}\\end{array}$ . $\\begin{array}{r}{\\mathrm{d}_{\\mathrm{H}}(p_{\\mathcal{A}},q_{\\mathcal{A}})=\\frac{1}{\\sqrt{2}}\\sqrt{\\sum_{i\\in\\mathcal{A}}\\left(\\sqrt{p_{i}}-\\sqrt{q_{i}}\\right)^{2}}}\\end{array}$ . For a set $\\boldsymbol{\\mathcal{A}}$ , we write $\\textstyle p(A)=\\sum_{i\\in A}p_{i}$ . We also have the following inequality [DKW18, Proposition 1]: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{\\mathrm{TV}}(p_{A},q_{A})\\leqslant{\\sqrt{2}}\\,\\mathrm{d}_{\\mathrm{H}}(p_{A},q_{A})\\leqslant{\\sqrt{\\sum_{i\\in A}(q_{i}-p_{i})+d_{\\mathrm{KL}}(p_{A},q_{A})}}\\leqslant{\\sqrt{d_{\\chi^{2}}(p_{A},q_{A})}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A distribution $p$ supported over the hypercube $\\{0,1\\}^{n}$ is a Bayesian network if its probability mass function satisfies the factorization associated with $G$ , a directed acyclic graph (DAG): ", "page_idx": 4}, {"type": "equation", "text": "$$\np(x_{1},\\cdot\\cdot\\cdot\\,,x_{n})=\\prod_{i=1}^{n}p(x_{i}|\\Pi_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $\\Pi_{i}$ is the set of parents of $X_{i}$ in $G$ ; and we say that $p$ is Markov with respect to DAG $G$ . In section 3, slightly abusing notation, we use $p_{G}$ to denote a projection of a Bayes net $p$ to a DAG $G$ (which it may or may not be Markov with respect to; see Definition 3.2). We work in the Poissonized setting (see, e.g., [Can22, Appendix C]) \u2013 instead of drawing $N$ samples directly from $p$ , we draw $Y\\sim\\operatorname{Poi}(N)$ samples from $p$ , where $\\mathrm{Poi}(N)$ denotes the random variable distributed as the Poisson distribution with parameter $N$ . The Poissonized and usual sampling settings are equivalent for constant probability of failure, up to a (small) multiplicative factor in the sample complexity. ", "page_idx": 4}, {"type": "text", "text": "2 Near-optimal entropy testing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We prove Theorem 1.1, establishing the sample complexity upper and lower bounds separately. ", "page_idx": 4}, {"type": "text", "text": "We will prove the following theorem: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1. There is an algorithm (Algorithm $^{\\,I}$ ) which, given n samples from a discrete distribution $p$ , the full description of a reference distribution $q_{\\cdot}$ , both over $[k]$ , and parameter $\\varepsilon\\ >\\ 0$ , distinguishes between $p=q$ and $|H(p)-H(q)|\\geqslant\\varepsilon$ with probability at least $2/3$ , as long as ", "page_idx": 4}, {"type": "equation", "text": "$$\nn\\geq c_{1}\\left(\\frac{\\sqrt{k\\log(k/\\varepsilon)}}{\\varepsilon}+\\frac{\\log^{2}(k)}{\\varepsilon^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $c_{2}\\varepsilon\\leqslant k$ , for some absolute constants $c_{1},c_{2}>0$ . Moreover, the algorithm runs in time linear in the number of samples n and the domain size $k$ . ", "page_idx": 4}, {"type": "text", "text": "The proof will rely on the two following claims and Lemma 2.4, which is a straightforward extension of [DKW18, Lemma 2]. Their proofs are deferred to Appendix B. Throughout, we let \u03c4 :=16 log\u03b5(k/\u03b5), and A := i \u2208[k] | qi \u2a7e\u03c4k , as in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Claim 2.2. Let $\\boldsymbol{\\mathcal{A}}$ be any set such that $p(\\bar{A})<\\varepsilon/2$ . Then, $i f|H(p_{A})-H(q_{A})|\\geqslant\\varepsilon$ , we must have $\\begin{array}{r}{(i)\\,d_{\\mathrm{KL}}(p_{A}\\|q_{A})\\geqslant\\frac{\\varepsilon}{2}\\,o r\\,(i i)\\,|\\sum_{i\\in A}(p_{i}-q_{i})\\log\\bigl(\\frac{1}{q_{i}}\\bigr)|\\geqslant\\frac{\\varepsilon}{2}.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Claim 2.3. Let $\\hat{p}$ be the empirical estimator for an unknown discrete distribution $p$ supported on $[k]$ , based on $\\mathrm{Poi}(m)$ samples, where $\\begin{array}{r}{m\\,=\\,\\Theta\\Big(\\frac{\\log^{2}(k)}{\\varepsilon^{2}}\\Big)}\\end{array}$ ; assume that $d_{\\chi^{2}}(p_{A},q_{A})\\;\\leqslant\\;\\varepsilon/8$ and $\\begin{array}{r}{p(\\bar{\\mathcal{A}})+q(\\bar{\\mathcal{A}})\\leqslant4\\tau=\\frac{1}{4}\\frac{\\varepsilon}{\\log(k/\\varepsilon)},}\\end{array}$ ,7 then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\left|\\sum_{i\\in A}(p_{i}-{\\hat{p}}_{i})\\log{\\frac{1}{q_{i}}}\\right|\\geqslant{\\frac{1}{8}}\\varepsilon\\right]\\leqslant{\\frac{1}{100}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "7One can remove the assumption that $p({\\bar{A}})+q({\\bar{A}})\\leqslant4\\tau$ , at the cost of a slightly worse constant. ", "page_idx": 4}, {"type": "text", "text": "Require: Sample access to $p$ and full description of $q$ , both over $[k]$ ; accuracy parameter $\\varepsilon$ .   \n1: Set $\\begin{array}{r}{\\tau:=\\frac{\\varepsilon}{16\\log(k/\\varepsilon)}}\\end{array}$ , and $\\begin{array}{r}{\\mathcal{A}:=\\left\\{i\\in\\left[k\\right]\\mid q_{i}\\geqslant\\frac{\\tau}{k}\\right\\}}\\end{array}$ .   \n2: Take $m_{1}=48/\\tau$ samples from $p$ and compute the empirical $\\hat{p}^{\\prime}$ .   \n3: Compute $Z_{1}=\\hat{p}^{\\prime}(\\bar{A})$ . ", "page_idx": 5}, {"type": "text", "text": "4: if $Z_{1}\\geqslant2\\tau$ then return reject ", "page_idx": 5}, {"type": "text", "text": "$\\mathsf{P}\\,N_{i}$ : the empirical count among samples of the $i$ -th element. 5: Let $\\begin{array}{r}{m_{2}=65536\\left(\\frac{\\sqrt{k\\cdot\\log(k/\\varepsilon)}}{\\varepsilon}\\right)}\\end{array}$ Draw $\\mathrm{Poi}(m_{2})$ samples from $p$ and compute ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ_{2}=\\sum_{i\\in\\mathcal{A}}\\frac{(N_{i}-N q_{i})^{2}-N_{i}}{N q_{i}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "6: if $Z_{2}\\geqslant\\frac{1}{16}m_{2}\\varepsilon$ then return reject ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "7: Let $\\begin{array}{r}{m_{3}=140800\\left(\\frac{\\log^{2}(k)}{\\varepsilon^{2}}\\right)}\\end{array}$   \n8: Draw $\\mathrm{Poi}(m_{3})$ samples from $p$ , compute the empirical $\\hat{p}$ ; let $\\begin{array}{r}{Z_{3}\\gets\\left|\\sum_{i}(\\hat{p}_{i}-q_{i})\\log\\left(\\frac{1}{q_{i}}\\right)\\right|}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "9: if $Z_{3}\\geqslant\\frac{1}{8}\\varepsilon$ then return reject ", "page_idx": 5}, {"type": "text", "text": "10: return accept ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.4. Let $\\mathcal{A}:=\\{i\\in[k]\\ |\\ q_{i}\\geqslant\\alpha\\}$ . Let $\\begin{array}{r}{m_{2}\\,\\geqslant\\,16384\\,\\mathrm{max}\\left\\{\\sqrt{\\frac{1}{\\alpha\\varepsilon}},\\frac{\\sqrt{k}}{\\varepsilon}\\right\\}}\\end{array}$ be the number of samples used to compute $Z_{2}$ . Then $\\mathbb{E}[Z_{2}]=m_{2}d_{\\chi^{2}}(p_{A},q_{A})$ . Moreover, if $d_{\\chi^{2}}(p_{A},q_{A})\\leqslant\\frac{\\varepsilon}{2}$ , then $\\begin{array}{r}{\\mathrm{Var}[Z_{2}]\\leqslant(\\frac{1}{32}m_{2}\\varepsilon)^{2}}\\end{array}$ . If $d_{\\chi^{2}}(p_{A},q_{A})\\geqslant\\varepsilon$ , then $\\mathrm{Var}[Z_{2}]\\leqslant O(\\mathbb{E}[Z_{2}]^{2})$ . ", "page_idx": 5}, {"type": "text", "text": "Proof of Theorem 2.1. We prove the statement by analyzing Algorithm 1. First, note that excluding the set of $\\bar{\\mathcal A}$ (elements with small mass), can change the value of $H(q)$ by at most $\\varepsilon/8$ : indeed, by Jensen\u2019s inequality $(f(x)\\,=\\,\\log x$ is concave) and $x\\log{\\frac{1}{x}}$ being monotonically increasing in $(\\bar{0},1/e)$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nH(q_{\\bar{A}})=\\sum_{i\\in\\bar{A}}q_{i}\\log\\frac{1}{q_{i}}\\leqslant q(\\bar{A})\\log\\frac{|\\bar{A}|}{q(\\bar{A})}\\leqslant\\tau\\log\\frac{k}{\\tau}=\\frac{\\varepsilon}{16\\log(k/\\varepsilon)}\\log\\left(\\frac{16k}{\\varepsilon/\\log(k/\\varepsilon)}\\right)\\leqslant\\frac{1}{8}\\varepsilon,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "when $\\tau\\leqslant1/e$ . Similarly, if $p(\\bar{A})\\leqslant3\\tau$ , we have that $\\begin{array}{r}{H(p_{\\bar{A}})\\leqslant\\frac{3}{8}\\varepsilon}\\end{array}$ . Therefore, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\varepsilon\\leqslant|H(p)-H(q)|}&{\\leqslant}&{|H(p_{A})-H(q_{A})|+|H(p_{\\bar{A}})-H(q_{\\bar{A}})|}\\\\ &{\\leqslant}&{|H(p_{A})-H(q_{A})|+|H(p_{\\bar{A}})|+|H(q_{\\bar{A}})|}\\\\ &{\\leqslant}&{|H(p_{A})-H(q_{A})|+\\frac{1}{2}\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For Line 4, we prove the following: with probability at least $99/100$ , if $Z_{1}\\geqslant2\\tau$ , then $p(\\bar{A})\\geqslant\\tau$ ; and if $Z_{1}<2\\tau$ , then $p(\\bar{A})<3\\tau$ (this is a standard technique; see e.g., [Can22, Fact 2.2].) For the sake of completeness we include the full derivation in the Appendix A. ", "page_idx": 5}, {"type": "text", "text": "After Line 4 of Algorithm 1. We conclude from the above that ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "i. $\\boldsymbol{\\mathcal{A}}$ still has sufficient entropy gap to test on: $\\begin{array}{r}{|H(p_{A})-H(q_{A})|\\geqslant\\frac{1}{2}\\varepsilon.}\\end{array}$ . ii. With probability at least $99/100$ , when $p\\,=\\,q$ , it will not be rejected in Algorithm 4 of Line 4; and once it is pass through this stage, we have $p(\\bar{A})\\leqslant3\\bar{\\tau}$ . ", "page_idx": 5}, {"type": "text", "text": "Completeness: when $p=q$ . ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "\u2022 We have that $d_{\\chi^{2}}(p_{A},q_{A})\\,=\\,0$ , and via Lemma 2.4, we know that $\\mathbb{E}[Z_{2}]\\,=\\,0$ and $\\mathrm{Var}[Z_{2}]\\leqslant$ $\\textstyle{\\frac{1}{32^{2}}}m_{2}^{2}\\varepsilon^{2}$ . By Chebyshev\u2019s inequality, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[|Z_{2}-\\mathbb{E}[Z_{2}]|\\geqslant2\\sqrt{\\operatorname{Var}[Z_{2}]}\\right]\\leqslant\\frac{1}{4},\\quad\\mathrm{~and~so~}\\quad\\operatorname*{Pr}[Z_{2}\\geqslant2\\cdot\\frac{1}{32}m_{2}\\varepsilon+\\mathbb{E}[Z_{2}]]\\leqslant\\frac{1}{4};\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and we have $\\begin{array}{r}{\\operatorname*{Pr}[Z_{2}\\geqslant\\frac{1}{16}m_{2}\\varepsilon]\\leqslant\\frac{1}{4}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 On the other hand, by Claim 2.3, setting $\\begin{array}{r}{m_{3}\\,=\\,\\frac{140800\\log^{2}(k)}{\\varepsilon^{2}}}\\end{array}$ , we have that with probability at least 99/100, ", "page_idx": 6}, {"type": "equation", "text": "$$\nZ_{3}=\\left|\\sum_{i\\in{\\cal A}}(\\hat{p}_{i}-q_{i})\\log\\frac{1}{q_{i}}\\right|=\\left|\\sum_{i\\in\\cal A}(\\hat{p}_{i}-p_{i})\\log\\frac{1}{q_{i}}\\right|\\leqslant\\frac{1}{8}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Therefore, with probability at least $\\textstyle1-{\\frac{1}{4}}-{\\frac{2}{100}}={\\frac{73}{100}}>{\\frac{2}{3}}$ , the tester will accept. ", "page_idx": 6}, {"type": "text", "text": "Soundness: when $|H(p)-H(q)|\\,\\geqslant\\,\\varepsilon.$ . If $p(\\bar{A})\\,\\geqslant\\,3\\tau$ then $\\hat{p}(\\bar{A})\\geqslant2\\tau$ with probability $99/100$ , and the algorithm will output Reject. We proceed assuming $p(\\bar{\\cal A})\\,\\leqslant\\,3\\tau$ and recall Item ii. from before, we have $|H(p_{A})\\stackrel{\\cdot}{-}H(\\dot{q_{A}})|\\;\\geqslant\\;\\textstyle{\\frac{1}{2}}\\bar{\\varepsilon}$ . By Claim 2.2, we have that either $d_{\\mathrm{KL}}(p_{A},q_{A})\\;\\geqslant$ $\\textstyle{\\frac{1}{4}}\\varepsilon$ or $\\begin{array}{r}{\\left|\\sum_{i\\in\\mathcal{A}}(p_{i}-q_{i})\\log\\left(1/q_{i}\\right)\\right|\\;\\geqslant\\;\\frac{1}{4}\\varepsilon}\\end{array}$ . We apply Lemma 2.4, setting $\\alpha\\,=\\,\\tau/k$ and $m_{2}~\\geqslant$ $65536\\sqrt{k\\log(k/\\varepsilon)}/\\varepsilon$ . ", "page_idx": 6}, {"type": "text", "text": "\u2022 If $\\begin{array}{r l}{\\quad}&{{}d_{\\mathrm{KL}}(p_{A}\\Vert q_{A})\\geqslant\\frac{1}{4}\\varepsilon}\\end{array}$ , with (4) and $\\exp(3/2)\\leqslant k/\\varepsilon$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{8}\\varepsilon\\leq-3\\tau+d_{\\mathrm{KL}}(p_{A},q_{A})\\leqslant\\sum_{i\\in A}(q_{i}-p_{i})+d_{\\mathrm{KL}}(p_{A},q_{A})\\leqslant d_{\\chi^{2}}(p_{A},q_{A}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which by Lemma 2.4, and our setting of $m_{2}$ and $\\alpha$ , implies $\\begin{array}{r}{\\mathrm{Var}[Z_{2}]\\,\\leqslant\\,(\\frac{1}{4}\\mathbb{E}[Z_{2}])^{2}}\\end{array}$ and $\\mathbb{E}[Z_{2}]=$ $m_{2}\\cdot d_{\\chi^{2}}(p_{A},q_{A})\\geqslant\\textstyle{\\frac{1}{8}}m_{2}\\varepsilon$ . By Chebyshev, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[|Z_{2}-\\mathbb{E}[Z_{2}]|\\geqslant2\\sqrt{\\mathrm{Var}[Z_{2}]}\\right]\\leqslant\\frac{1}{4}\\;\\mathrm{and~so~}\\operatorname*{Pr}[Z_{2}\\leqslant\\frac{1}{16}m_{2}\\varepsilon]\\leqslant\\frac{1}{4}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u2022 On the other hand, if it is the case that $\\begin{array}{r}{\\left|\\sum_{i\\in\\mathcal{A}}(p_{i}-q_{i})\\log\\left(1/q_{i}\\right)\\right|\\;\\geqslant\\;\\frac{1}{4}\\varepsilon}\\end{array}$ , by Claim 2.3, setting $m_{3}=140800\\log^{2}(k)/\\varepsilon^{2}$ , with probability at least $99/100$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{4}\\varepsilon}&{\\leqslant}&{\\displaystyle\\left\\vert\\sum_{i}p_{i}\\log\\displaystyle\\frac{1}{q_{i}}-q_{i}\\log\\displaystyle\\frac{1}{q_{i}}\\right\\vert}\\\\ &{\\leqslant}&{\\displaystyle\\left\\vert\\sum_{i}(p_{i}-\\hat{p}_{i})\\log\\displaystyle\\frac{1}{q_{i}}\\right\\vert+\\left\\vert\\sum_{i}(\\hat{p}_{i}-q_{i})\\log\\displaystyle\\frac{1}{q_{i}}\\right\\vert}\\\\ &{\\leqslant}&{\\displaystyle\\frac{1}{8}\\varepsilon+\\left\\vert\\sum_{i}(\\hat{p}_{i}-q_{i})\\log\\displaystyle\\frac{1}{q_{i}}\\right\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We have that $\\begin{array}{r}{Z_{3}=\\left|\\sum_{i}(\\hat{p}_{i}-q_{i})\\log\\frac{1}{q_{i}}\\right|\\geqslant\\frac{1}{8}\\varepsilon}\\end{array}$ and thus with probability at least $\\begin{array}{r}{1-{\\frac{1}{4}}-{\\frac{2}{100}}={\\frac{73}{100}}}\\end{array}$ , the following will happen, the tester will reject: either $p(\\bar{A})\\,\\geqslant\\,3\\tau$ , and it is rejected at Line 4 of Algorithm 4, or it passes and $p(\\bar{A})\\leqslant3\\tau$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\nZ_{2}\\geqslant\\textstyle{\\frac{1}{8}}m_{2}\\varepsilon~\\mathrm{or}~Z_{3}\\geqslant\\textstyle{\\frac{1}{8}}\\varepsilon,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and will be rejected. This concludes the proof. ", "page_idx": 6}, {"type": "text", "text": "Remark 2.5. We note that we can slightly improve the sample complexity of Theorem 1.1 (specifically, improving on the $\\sqrt{k\\log(k/\\varepsilon)}$ term), at the price of a more complicated algorithm, by adding thresholds $\\begin{array}{r}{\\tau^{\\prime}=\\frac{\\varepsilon}{\\log\\log\\left(k/\\varepsilon\\right)}}\\end{array}$ , $\\begin{array}{r}{\\tau^{\\prime\\prime}=\\frac{\\varepsilon}{\\log\\log\\log(k/\\varepsilon)}}\\end{array}$ , and considering separately the elements in $A^{\\prime}=\\{i:q_{i}\\in(\\tau/k,\\tau^{\\overline{{\\prime}}}/\\bar{k}]\\}$ , $\\mathcal{A}^{\\prime\\prime}=\\{i:q_{i}\\,\\in\\,(\\tau^{\\prime}/k,\\tau^{\\prime\\prime}/k]\\},$ ; specifically, by grouping them in groups, and \u201cmerging\u201d each group to get a \u201cnew\u201d element with larger probability. For the sake of clarity, we defer this improvement to Appendix $E$ . ", "page_idx": 6}, {"type": "text", "text": "2.2 An $\\Omega(\\sqrt{k}/\\varepsilon+\\log^{2}k/\\varepsilon^{2})$ lower bound ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The $\\Omega(\\sqrt{k}/\\varepsilon+\\log^{2}k/\\varepsilon^{2})$ lower bound comes from the combination of Lemma 2.6 and Lemma 2.7. We obtain Lemma 2.6 through the classical hard instance used for uniformity testing [Pan08] and a simple conversion between TV distance and entropy difference gives the result. We note that distributions close to uniform distribution actually have smaller entropy difference (uniform distribution is quite special: having the highest entropy of $\\log k$ ). Indeed, replacing the uniform distribution with a slightly biased distribution, we obtain another hard instance for Lemma 2.7, using the classical Le Cam\u2019s two-point method. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Identity testing for bounded degree Bayes nets ", "page_idx": 7}, {"type": "text", "text": "Require: Sample access to Bayes net $p$ , full description of Bayes net $q$ , accuracy parameter $\\varepsilon$ ,   \nin-degree $d$ and dimension $n$ .   \n1: $\\begin{array}{r}{S_{1}\\gets O\\left(\\left(\\frac{2^{d/2}n\\sqrt{d\\log(n/\\varepsilon)}}{\\varepsilon^{2}}+\\frac{d^{2}n^{2}}{\\varepsilon^{4}}\\right)d\\log n\\right)}\\end{array}$ samples from $p$ ;   \n2: 2d\u03b5/22n log(1/\u03b5) \u00b7 log n samples from p;   \n3: for all $L\\in\\mathcal{N}_{d+1}\\cup\\mathcal{N}_{d}$ do $\\mathsf{\\Delta}\\triangleright\\mathcal{N}_{\\ell}$ is all subsets of $\\{0,1\\}^{n}$ with size $\\ell$   \n4: Call Algorithm 1 with $p_{L},q_{L}$ and $S_{1}$ ; $\\triangleright$ Entropy test on $p_{L}$ and $q_{L}$ with accuracy $\\varepsilon^{2}/n$ .   \n5: if Entropy test rejects then return reject   \n6: $\\begin{array}{r}{S_{3}\\gets O\\left(\\frac{d\\log(n)\\cdot\\log\\left(1/\\varepsilon\\right)}{\\varepsilon^{2}}\\right)}\\end{array}$ samples from $p$ and compute its empirical distribution $\\hat{p}$ ;   \n7: for all $i\\in[n]$ do   \n8: if $\\hat{p}_{X_{i},\\Pi_{i}^{G}}(\\bar{A}_{i}^{\\prime})\\geqslant\\Omega(\\varepsilon^{2}/\\log(1/\\varepsilon))$ then return reject   \n9: Check $\\begin{array}{r}{d_{\\mathrm{KL}}\\big(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}\\big|\\big|q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}\\big)\\geqslant\\frac{\\varepsilon^{2}}{n}}\\end{array}$ or $d_{\\mathrm{KL}}(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}||q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}})=0.$   \n10: if $i$ -th KL test says far then return reject ", "page_idx": 7}, {"type": "text", "text": "11: return accept $\\triangleright$ Accept if all tests pass. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Lemma 2.6. With fewer than $c_{3}\\cdot\\sqrt{k}/\\varepsilon$ samples from $p$ , no tester can distinguish between $p=q$ and $\\begin{array}{r}{|H(p)-H(q)|\\geqslant\\varepsilon}\\end{array}$ with probability higher than $2/3$ , where $c_{3}>0$ is an absolute constant. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2.7. With fewer than $c_{4}\\cdot\\log^{2}k/\\varepsilon^{2}$ samples from $p_{i}$ , no tester can distinguish between $p=q$ and $\\begin{array}{r}{|H(p)-H(q)|\\geqslant\\varepsilon}\\end{array}$ with probability higher than $2/3$ , where $c_{4}>0$ is an absolute constant. ", "page_idx": 7}, {"type": "text", "text": "3 Application to identity testing for Bayes nets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now provide an application of our main entropy identity testing theorem, to obtain an improved \u201cstandard\u201d identity testing algorithm for Bayesian networks: ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.1. Given sample access to an in-degree \u221ad Bayes net p and full description of in-degree $d$ Bayes net q, Algorithm 2 takes $\\begin{array}{r}{C\\cdot\\left(\\frac{2^{d/2}n d^{3/2}\\log n\\cdot\\sqrt{\\log(n/\\varepsilon)}}{\\varepsilon^{2}}+\\frac{d^{3}n^{2}\\cdot\\log n}{\\varepsilon^{4}}\\right)}\\end{array}$ samples to test between $p=q$ or $\\mathrm{d}_{\\mathrm{H}}(p,q)\\geqslant\\Omega(\\varepsilon)$ , where $C>0$ is an absolute constant. Moreover, the algorithm runs in time polynomial in $n^{d}$ and $1/\\varepsilon$ . ", "page_idx": 7}, {"type": "text", "text": "Before proceeding to the analysis of our algorithm, we require the following definitions. ", "page_idx": 7}, {"type": "text", "text": "Definition 3.2. $A$ projection of a Bayes net $p$ on $\\{0,1\\}^{n}$ unto a DAG $G$ is denoted $p_{G}$ , and is defined by its probability mass function (PMF) as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\np_{G}(X_{1},...\\,,X_{n})=\\prod_{i=1}^{n}p(X_{i}\\mid\\Pi_{i}^{G}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Pi_{i}^{G}$ is the set of parents of $X_{i}$ in $G$ . ", "page_idx": 7}, {"type": "text", "text": "Denote $\\textstyle\\mathcal{U}:=\\bigcup_{i=1}^{n}\\mathcal{A}_{i}$ , where $\\begin{array}{r}{\\mathcal{A}_{i}\\,:=\\,\\Bigl\\{x\\in\\{0,1\\}^{n}:q_{X_{i},\\Pi_{i}^{G}}(x_{i}(x),\\pi_{i}^{G}(x))\\geqslant\\Omega\\left(\\frac{\\varepsilon^{2}/\\log(1/\\varepsilon)}{2^{d+1}n}\\right)\\Bigr\\}.}\\end{array}$ This gives us the property that marginalization over $X_{i}=x_{i},\\Pi_{i}^{G}=\\pi_{i}^{G}$ works nicely as we include elements only based on its local property (as long as $q_{X_{i},\\Pi_{i}}$ is large enough). And $q$ is Markov w.r.t. $G$ . We use $(x_{i},\\pi_{i})\\in\\mathcal{A}_{i}^{\\prime}$ , where $\\begin{array}{r}{\\mathcal{A}_{i}^{\\prime}=\\left\\{x^{\\prime}\\in\\{0,1\\}^{|\\Pi_{i}^{G}|+1}:q_{X_{i},\\Pi_{i}^{G}}(x^{\\prime})\\geqslant\\Omega\\left(\\frac{\\varepsilon^{2}/\\log(1/\\varepsilon)}{2^{d+1}n}\\right)\\right\\}}\\end{array}$ $(a,B)\\in\\mathcal{A}_{i}^{\\prime}$ , we have that as long as $(x_{i}(x),\\pi_{i}(x))=(a,B)$ , then $x\\in A_{i}$ and vice versa, which means that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{U}=\\bigcup_{i=1}^{n}\\mathcal{A}_{i}=\\bigcup_{i=1}^{n}\\{x\\in\\{0,1\\}^{n}:(x_{i}(x),\\pi_{i}^{G}(x))\\in\\mathcal{A}_{i}^{\\prime}\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We will check if $p_{X_{i},\\Pi_{i}^{G}}(\\bar{\\mathcal{A}}_{i}^{\\prime})\\quad\\geqslant\\quad\\Omega(\\varepsilon^{2}/\\log(1/\\varepsilon))$ and reject early if true; this takes $\\begin{array}{r}{O\\left(\\frac{d\\log(n)\\cdot\\log(1/\\varepsilon)}{\\varepsilon^{2}}\\right)}\\end{array}$ samples for all tests to be correct via a union bound. After passing this test, we ", "page_idx": 7}, {"type": "text", "text": "can conclude that ", "page_idx": 8}, {"type": "equation", "text": "$$\np(\\bar{\\mathcal{U}})=\\sum_{x\\in\\bigcap_{i=1}^{n}\\bar{A}_{i}}p(x)\\leqslant\\sum_{x\\in\\bar{\\mathcal{A}}_{1}}p(x)\\leqslant\\sum_{x^{\\prime}\\in\\bar{\\mathcal{A}}_{1}^{\\prime}}p_{X_{1},\\Pi_{1}^{G}}(x^{\\prime})=p_{X_{1},\\Pi_{1}^{G}}(\\bar{\\mathcal{A}}_{1}^{\\prime})\\leqslant O(\\varepsilon^{2}/\\log(1/\\varepsilon)).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Similarly, we can upper bound $q(\\bar{\\mathcal{U}})\\leqslant q_{X_{i},\\Pi_{i}^{G}}(\\bar{\\mathcal{A}}_{i}^{\\prime})\\leqslant O(\\varepsilon^{2}/\\log(1/\\varepsilon))$ . Denote $p_{G;\\mathcal{U}}$ as projecting $p$ onto $G$ , which gives $p_{G}$ and then restricting the distribution $p_{G}$ to take elements in $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 8}, {"type": "text", "text": "Claim 3.3. Suppose that $p(\\bar{\\mathcal{U}})\\leqslant O(\\varepsilon^{2}/\\log(1/\\varepsilon))$ , then we have ", "page_idx": 8}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p_{\\bar{\\mathcal{U}}}\\|p_{G;\\bar{\\mathcal{U}}})\\geqslant-p(\\bar{\\mathcal{U}})\\cdot\\log\\left(\\frac{1}{p(\\bar{\\mathcal{U}})}\\right)\\geqslant-O(\\varepsilon^{2}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We will also need the following lemma whose proof is deferred to Appendix $\\mathbf{C}$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.4. Suppose $d_{H}^{2}(p,q)\\geqslant\\Omega(\\varepsilon^{2})$ and $d_{\\mathrm{KL}}(p\\|p_{G})\\leqslant O(\\varepsilon^{2}),$ , then we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}d_{\\mathrm{KL}}\\big(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}\\big||q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}\\big)\\geqslant\\Omega\\big(\\varepsilon^{2}\\big).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Therefore testing $\\begin{array}{r}{d_{\\mathrm{KL}}\\big(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}\\big|\\big|q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}\\big)\\geqslant\\frac{\\varepsilon^{2}}{n}}\\end{array}$ over all $i$ suffices to detect this case. ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 3.1. We show the result by analyzing Algorithm 2. By Theorem 1.1, the sample complexity for entropy testing on any subset $L$ of size (dimension) $d$ or $d\\,+\\,1$ , is $O\\Big(2^{d/2}n\\sqrt{d\\log(n/\\varepsilon)}/\\varepsilon^{2}+d^{2}n^{2}/\\varepsilon^{4}\\Big)$ . To guarantee the success of every tests employed in the algorithm, we increase the sample complexity of each test by an extra $O(\\log(n^{d+1}))=O(d\\log n)$ factor to boost their success probability to $\\begin{array}{r}{\\dot{1}-\\frac{1}{100n^{d+1}}}\\end{array}$ (via a standard majority vote technique), which will allow us to use a union bound over all tests as there are at most $n^{d+1}$ subsets with size $d+1$ . For this step, the sample complexity will be ", "page_idx": 8}, {"type": "equation", "text": "$$\nO\\left(\\left(\\frac{2^{d/2}n\\sqrt{d\\log(n/\\varepsilon)}}{\\varepsilon^{2}}+\\frac{d^{2}n^{2}}{\\varepsilon^{4}}\\right)d\\log n\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "With this in hand, we will proceed with the analysis under the event that every entropy test performed is correct (which by the above happens with high probability). If distribution $p$ manages to pass all the entropy tests, it must satisfy the following: ", "page_idx": 8}, {"type": "equation", "text": "$$\n|H(p_{L})-H(q_{L})|\\leqslant\\frac{\\varepsilon^{2}}{n},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for every subset $L$ of size $d+1$ for the latter, and every subset $L$ of size $d$ or $d+1$ for the former. From here, in principle, we can perform structural learning of $p$ through $H(q_{L})$ , which then gives us an approximated DAG $\\hat{G}$ of $p$ and we can check $\\mathrm{d}_{\\mathrm{H}}(p_{\\hat{G}},q_{\\hat{G}})$ . Unfortunately, structural learning of Bayes nets is known to be computationally hard in many settings [Ho\u00a8f93, CHM04], and so this would lead to a computationally inefficient algorithm. ", "page_idx": 8}, {"type": "text", "text": "Instead, we argue that this (learning) step can be bypassed entirely: the intuition of the argument is to view structure learning for Bayes net as an optimization problem; and any assignment $x$ to the two optimization problems (structure learning of $p$ and $q$ ) satisfy $f_{1}(x)=f_{2}(\\dot{x}){\\pm}O\\!\\left(\\varepsilon^{2}\\right)$ due to their local entropy being close8 \u2013 this means that an optima $x_{1}$ for $f_{1}$ satisfies $\\mathrm{min}_{x}\\,f_{1}(x)=f_{1}(x_{1})\\geqslant$ $f_{2}(x_{1})-\\overdot{\\varepsilon^{2}}\\geqslant\\operatorname*{min}_{x}\\,f_{2}(x)-\\varepsilon^{2}$ and vice versa (optima $x_{2}$ for $f_{2}$ ). ", "page_idx": 8}, {"type": "text", "text": "More formally, by the celebrated works of Chow and Liu [CL68] and its generalization to Bayes net, one can write the KL divergence between Bayes net and its projection to any graph $G$ as difference between sum of $n$ local conditional entropies (we provide a derivation for the sake of completeness in F): ", "page_idx": 8}, {"type": "equation", "text": "$$\n0\\leqslant d_{\\mathrm{KL}}(p\\|p_{G})=-\\sum_{i=1}^{n}H(p_{X_{i},X_{\\Pi_{i}}}|p_{X_{\\Pi_{i}}})+\\sum_{i=1}^{n}H(p_{X_{i},X_{\\Pi_{i}^{G}}}|p_{X_{\\Pi_{i}^{G}}}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(q\\|q_{G^{\\prime}})=-\\sum_{i=1}^{n}H(q_{X_{i},X_{\\Pi_{i}^{G}}}|q_{X_{\\Pi_{i}^{G}}})+\\sum_{i=1}^{n}H(q_{X_{i},X_{\\Pi_{i}}}|q_{X_{\\Pi_{i}}}),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $X_{\\Pi_{i}}$ denotes the parents of $X_{i}$ in Bayes net $p$ and $X_{\\Pi_{i}^{G}}$ denotes the parents of $X_{i}$ in DAG $G$ . Here we assume that $q$ is Markov with respect to $G$ . Since the local entropies between $p$ and $q$ are close by $O(\\varepsilon^{2}/n)$ (see (6)) and its relation to conditional entropy via (3), we can conclude the following: ", "page_idx": 9}, {"type": "equation", "text": "$$\nH(q_{X_{i},X_{\\Pi_{i}}}|q_{X_{\\Pi_{i}}})-O(\\varepsilon^{2}/n)\\leqslant H(p_{X_{i},X_{\\Pi_{i}}}|p_{X_{\\Pi_{i}}})\\leqslant H(q_{X_{i},X_{\\Pi_{i}}}|q_{X_{\\Pi_{i}}})+O(\\varepsilon^{2}/n).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "equation", "text": "$$\nH(q_{X_{i},X_{\\Pi_{i}^{G}}}|q_{X_{\\Pi_{i}^{G}}})-O(\\varepsilon^{2}/n)\\leqslant H(p_{X_{i},X_{\\Pi_{i}^{G}}}|p_{X_{\\Pi_{i}^{G}}})\\leqslant H(q_{X_{i},X_{\\Pi_{i}^{G}}}|q_{X_{\\Pi_{i}^{G}}})+O(\\varepsilon^{2}/n).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "By our assumption, since $q$ is Markov to $G$ , we can combine (7), (9) and (10), which then give: ", "page_idx": 9}, {"type": "equation", "text": "$$\n1\\leqslant d_{\\mathrm{KL}}(p\\|p_{G})\\leqslant-\\sum_{i=1}^{n}H(q_{X_{i},X_{\\Pi_{i}}}|q_{X_{\\Pi_{i}}})+\\sum_{i=1}^{n}H(q_{X_{i},X_{\\Pi_{i}^{G}}}|q_{X_{\\Pi_{i}^{G}}})+O(\\varepsilon^{2})=-d_{\\mathrm{KL}}(q\\|q_{G^{\\prime}})+O(\\varepsilon^{2}).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $p$ is Markov to $G^{\\prime}$ , as $\\Pi_{i}$ is the set of parents of $X_{i}$ for $p$ . Rearranging terms and we have ", "page_idx": 9}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p\\|p_{G})\\leqslant O(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "With this at hand, we continue onto the $\\mathrm{KL}$ testing part. The algorithm will check if $p_{X_{i},\\Pi_{i}^{G}}(\\bar{A}_{i}^{\\prime})\\geqslant$ $\\Omega(\\varepsilon^{2}/\\log(1/\\varepsilon))$ and reject early if it is true (this costs $\\begin{array}{r}{O\\left(\\frac{d\\log\\left(n\\right)\\cdot\\log\\left(1/\\varepsilon\\right)}{\\varepsilon^{2}}\\right))}\\end{array}$ and then check for every $i\\in[n]$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}||q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}})\\geqslant\\frac{\\varepsilon^{2}}{n}\\;\\;\\mathrm{or}\\;\\;d_{\\mathrm{KL}}(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}||q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}})=0.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Using Lemma 2.4 (after converting from $\\mathrm{KL}$ to $\\chi^{2}$ noting the mass of $\\mathcal{A^{\\prime}}$ on both distributions) and a union bound over $n$ tests, the sample complexity is ", "page_idx": 9}, {"type": "equation", "text": "$$\nO\\left(\\frac{2^{d/2}n}{\\varepsilon^{2}}\\sqrt{\\log(1/\\varepsilon)}\\cdot\\log n\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Following this, we look at the two cases: ", "page_idx": 9}, {"type": "text", "text": "\u2022 If $p=q$ , then with high probability, $p$ will pass all entropy tests, all KL local tests and the tester will accept.   \n\u2022 If $\\mathrm{d}_{\\mathrm{H}}(p,q)\\geqslant\\varepsilon$ , either it fails one of the entropy test. If it does pass the entropy test, then we must have that $d_{\\mathrm{KL}}(p\\|p_{G})\\leqslant O(\\varepsilon^{2})$ by (11). Then following Lemma 3.4 and Lemma 2.4, the tester will reject. ", "page_idx": 9}, {"type": "text", "text": "In total, the sample complexity is (dominated by entropy testing): ", "page_idx": 9}, {"type": "equation", "text": "$$\nO\\left(\\left(\\frac{2^{d/2}n\\sqrt{d\\log(n/\\varepsilon)}}{\\varepsilon^{2}}+\\frac{d^{2}n^{2}}{\\varepsilon^{4}}\\right)d\\log n\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "This concludes the proof of the theorem. ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion and open problems ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study a variant of distribution testing problem in terms of entropy difference; we give nearly tight upper and lower sample complexity bounds for the problem. We subsequently apply our entropy testing algorithm to identity testing of Bayes nets, which unlike prior works, makes merely the necessary assumptions (the bound on the in-degree of the Bayes nets). ", "page_idx": 9}, {"type": "text", "text": "Future direction. We believe the closeness (two-sample) testing variant of the problem (testing if two unknown distribution $p$ and $q$ are the same or far in terms of entropy difference) could also be interesting; and, notably, has connections to other distribution testing problems: first, it should lead to a natural solution to closeness testing of Bayes nets via ideas in this paper. Second, solving the closeness entropy testing problem give another path to testing independence in terms of mutual information (studied in $[{\\mathbf{B}}{\\mathbf{G}}{\\mathbf{P}}^{+}23]$ and also covered in [CDKS18]), a notion closely related to entropy. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank the reviewers for their suggestions and efforts which help improve this paper.   \nYang would like to acknowledge the support of the JD Technology Scholarship. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[ABIS19] Jayadev Acharya, Sourbh Bhadane, Piotr Indyk, and Ziteng Sun. Estimating entropy of distributions in constant space. In NeurIPS, pages 5163\u20135174, 2019.   \n[ADOS17] Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A unified maximum likelihood approach for estimating symmetric properties of discrete distributions. In ICML, volume 70 of Proceedings of Machine Learning Research, pages 11\u201321. PMLR, 2017.   \n[AISW20] Jayadev Acharya, Ibrahim Issa, Nirmal V. Shende, and Aaron B. Wagner. Estimating quantum entropy. IEEE J. Sel. Areas Inf. Theory, 1(2):454\u2013468, 2020.   \n[AMNW22] Maryam Aliakbarpour, Andrew McGregor, Jelani Nelson, and Erik Waingarten. Estimation of entropy in constant space with improved sample complexity. In NeurIPS, 2022.   \n[AOST17] Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, and Himanshu Tyagi. Estimating renyi entropy of discrete distributions. IEEE Trans. Inf. Theory, 63(1):38\u201356, 2017. [BCY22] Arnab Bhattacharyya, Cle\u00b4ment L. Canonne, and Joy Qiping Yang. Independence testing for bounded degree bayesian network. CoRR, abs/2204.08690, 2022.   \n[BDKR02] Tu\u02d8gkan Batu, Sanjoy Dasgupta, Ravi Kumar, and Ronitt Rubinfeld. The complexity of approximating entropy. In STOC, pages 678\u2013687. ACM, 2002.   \n$[\\mathbf{B}\\mathbf{F}\\mathbf{F}^{+}01]$ Tugkan Batu, Lance Fortnow, Eldar Fischer, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. Testing random variables for independence and identity. In FOCS, pages 442\u2013451. IEEE Computer Society, 2001.   \n[BGKV21] Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, and NV Vinodchandran. Testing product distributions: A closer look. In Algorithmic Learning Theory, pages 367\u2013396. PMLR, 2021.   \n[BGMV20] Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, and N. V. Vinodchandran. Efficient distance approximation for structured high-dimensional distributions via learning. In NeurIPS, 2020.   \n$[\\mathbf{BGP}^{+}23]$ Arnab Bhattacharyya, Sutanu Gayen, Eric Price, Vincent Y. F. Tan, and N. V. Vinodchandran. Near-optimal learning of tree-structured distributions by chow and liu. SIAM J. Comput., 52(3):761\u2013793, 2023. [BY02] Ziv Bar-Yossef. The Complexity of Massive Data Set Computations. PhD thesis, UC Berkeley, 2002. Adviser: Christos Papadimitriou. Available at http://webee. technion.ac.il/people/zivby/index_files/Page1489.html. [Can22] Cle\u00b4ment L. Canonne. Topics and techniques in distribution testing: A biased but representative sample. Found. Trends Commun. Inf. Theory, 19(6):1032\u20131198, 2022.   \n[CDKS18] Cle\u00b4ment L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Testing conditional independence of discrete distributions. In STOC, pages 735\u2013748. ACM, 2018.   \n[CDKS20] Cle\u00b4ment L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Testing bayesian networks. IEEE Trans. Inf. Theory, 66(5):3132\u20133170, 2020. Preprint available at arXiv:1612.03156.   \n[CHM04] David Maxwell Chickering, David Heckerman, and Christopher Meek. Large-sample learning of bayesian networks is np-hard. J. Mach. Learn. Res., 5:1287\u20131330, 2004. [CK11] Imre Csisza\u00b4r and Ja\u00b4nos K\u00a8orner. Information Theory - Coding Theorems for Discrete Memoryless Systems, Second Edition. Cambridge University Press, 2011. [CL68] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Trans. Inf. Theory, 14(3):462\u2013467, 1968.   \n[DDK19] Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing ising models. IEEE Trans. Inf. Theory, 65(11):6829\u20136852, 2019.   \n[DKW18] Constantinos Daskalakis, Gautam Kamath, and John Wright. Which distribution distances are sublinearly testable? In Proceedings of the Twenty-Ninth Annual ACMSIAM Symposium on Discrete Algorithms, pages 2747\u20132764. SIAM, 2018. [DP16] Constantinos Daskalakis and Qinxuan Pan. Square hellinger subadditivity for bayesian networks and its applications to identity testing. arXiv preprint arXiv:1612.03164, 2016. Full version of [DP17]. [DP17] Constantinos Daskalakis and Qinxuan Pan. Square hellinger subadditivity for bayesian networks and its applications to identity testing. In COLT, volume 65 of Proceedings of Machine Learning Research, pages 697\u2013703. PMLR, 2017.   \n[GHS21] Tom Gur, Min-Hsiu Hsieh, and Sathyawageeswar Subramanian. Sublinear quantum algorithms for estimating von neumann entropy. CoRR, abs/2111.11139, 2021.   \n[HJW15a] Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Adaptive estimation of shannon entropy. In ISIT, pages 1372\u20131376. IEEE, 2015.   \n[HJW15b] Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Adaptive estimation of shannon entropy. CoRR, abs/1502.00326, 2015. [Ho\u00a8f93] Klaus-U Ho\u00a8ffgen. Learning and robust learning of product distributions. In Proceedings of the sixth annual conference on Computational learning theory, pages 77\u201383, 1993.   \n$[\\mathrm{KCG}^{+}23]$ Neville Kenneth Kitson, Anthony C. Constantinou, Zhigao Guo, Yang Liu, and Kiattikun Chobtham. A survey of bayesian network structure learning. Artif. Intell. Rev., 56(8):8721\u20138814, 2023.   \n[KDDC23] Anthimos Vardis Kandiros, Constantinos Daskalakis, Yuval Dagan, and Davin Choo. Learning and testing latent-tree ising models efficiently. In COLT, volume 195 of Proceedings of Machine Learning Research, pages 1666\u20131729. PMLR, 2023. [Pan04] Liam Paninski. Estimating entropy on $m$ bins given fewer than $m$ samples. IEEE Trans. Inform. Theory, 50(9):2200\u20132203, 2004. [Pan08] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data. IEEE Transactions on Information Theory, 54(10):4750\u20134755, 2008. [VV11a] Gregory Valiant and Paul Valiant. Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts. In STOC, pages 685\u2013694. ACM, 2011.   \n[VV11b] Gregory Valiant and Paul Valiant. The power of linear estimators. In FOCS, pages 403\u2013412. IEEE Computer Society, 2011. [VV13] Paul Valiant and Gregory Valiant. Estimating the unseen: Improved estimators for entropy and other properties. In NIPS, pages 2157\u20132165, 2013. [VV17] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity testing. SIAM J. Comput., 46(1):429\u2013455, 2017. [WY16] Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best polynomial approximation. IEEE Trans. Inform. Theory, 62(6):3702\u20133720, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Derivation for Line 4 in Algorithm 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We need to show the following: with probability at least $99/100$ , if $Z_{1}\\geqslant2\\tau$ , then $p(\\bar{\\cal A})\\geqslant\\tau$ ; and if $Z_{1}<2\\tau$ , then $p(\\bar{A})<3\\tau$ . For the first one, we prove by contrapositive: with high probability $1-{\\frac{1}{200}}$ , $p(\\bar{\\mathcal{A}})<\\tau\\Rightarrow Z_{1}<2\\tau$ . Suppose $T=\\mathrm{Binomial}(m_{1},\\tau)$ and setting $\\begin{array}{r}{m_{1}=\\frac{48}{\\tau}\\geqslant\\frac{3\\log200}{\\tau}}\\end{array}$ , and using a Chernoff bound, we have the following ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[T\\geqslant2\\tau]\\leqslant\\exp(-\\tau\\cdot m_{1}/3)\\leqslant{\\frac{1}{200}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since any $\\mathrm{Binomial}(m_{1},p(\\bar{A}))$ will be first-order stochastic dominated by $\\mathrm{Binomial}(m_{1},\\tau)$ if $p(\\bar{A})<\\dot{\\tau}$ , we can conclude the following: if $p(\\bar{A})<\\tau$ , then $\\begin{array}{r}{\\mathrm{Pr}[Z_{1}\\geqslant2\\tau]\\leqslant\\mathrm{Pr}[T\\geqslant2\\tau]\\leqslant\\frac{1}{200}}\\end{array}$ . For the latter, we prove via its contrapositive: with probability $\\begin{array}{r}{1-\\frac{1}{200},p(\\bar{\\mathcal{A}})\\geqslant3\\tau\\Rightarrow Z_{1}\\geqslant2\\tau}\\end{array}$ . As p( A\u00af) \u2a7e3\u03c4, take m1 = 4\u03c48 $\\begin{array}{r}{m_{1}=\\frac{48}{\\tau}\\geqslant\\frac{9\\log200}{\\tau}}\\end{array}$ , by a Chernoff bound, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{r}[\\hat{p}^{\\prime}(\\bar{A})\\leqslant2\\tau]\\leqslant\\mathrm{Pr}[\\hat{p}^{\\prime}(\\bar{A})\\leqslant(1{-}1/3){\\cdot}p(\\bar{A})]\\leqslant\\exp\\left(-\\frac{m_{1}\\cdot p(\\bar{A})}{18}\\right)\\leqslant\\exp\\left(-\\frac{m_{1}\\cdot\\tau}{9}\\right)\\leqslant\\frac{1}{200}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining the two with a union bound concludes the proof. ", "page_idx": 12}, {"type": "text", "text": "B Deferred proofs from Section 2.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Claim 2.2. Let $\\boldsymbol{\\mathcal{A}}$ be any set such that $p(\\bar{A})<\\varepsilon/2$ . Then, $i f|H(p_{A})-H(q_{A})|\\geqslant\\varepsilon$ , we must have $\\begin{array}{r}{(i)\\,d_{\\mathrm{KL}}(p_{A}\\|q_{A})\\geqslant\\frac{\\varepsilon}{2}\\,o r\\,(i i)\\,\\big|\\sum_{i\\in A}(p_{i}-q_{i})\\log\\bigl(\\frac{1}{q_{i}}\\bigr)\\big|\\geqslant\\frac{\\varepsilon}{2}.}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Proof of Claim 2.2. We can bound $|H(p_{A})-H(q_{A})|$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\varepsilon\\le|H(p_{A})-H(q_{A})|}&{=}&{\\displaystyle\\left|\\sum_{i\\in A}\\left(p_{i}\\log\\frac{1}{p_{i}}-q_{i}\\log\\frac{1}{q_{i}}\\right)\\right|}\\\\ &{=}&{\\displaystyle\\left|\\sum_{i\\in A}\\left(p_{i}\\log\\frac{q_{i}}{p_{i}}+p_{i}\\log\\frac{1}{q_{i}}-q_{i}\\log\\frac{1}{q_{i}}\\right)\\right|}\\\\ &{\\leqslant}&{\\displaystyle\\left|\\sum_{i\\in A}p_{i}\\log\\frac{q_{i}}{p_{i}}\\right|+\\left|\\sum_{i\\in A}\\left(p_{i}\\log\\frac{1}{q_{i}}-q_{i}\\log\\frac{1}{q_{i}}\\right)\\right|}\\\\ &{=}&{|d_{\\mathrm{KL}}(p_{A}||q_{A})|+\\displaystyle\\left|\\sum_{i\\in A}(p_{i}-q_{i})\\log\\frac{1}{q_{i}}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which implies that at least one of the two terms is at least $\\varepsilon/2$ . If it is the second, we are done; otherwise, we know that either ", "page_idx": 12}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p_{A}\\|q_{A})\\geqslant\\frac{1}{2}\\varepsilon\\,\\mathrm{~or~}\\,d_{\\mathrm{KL}}(p_{A}\\|q_{A})\\leqslant-\\frac{1}{2}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We will rule out the second case, using that $\\log{\\frac{1}{x}}\\geqslant1-x$ for $x>0$ ,9 ", "page_idx": 12}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p_{A}\\|q_{A})=\\sum_{i\\in A}p_{i}\\log\\frac{p_{i}}{q_{i}}\\geqslant\\sum_{i\\in A}p_{i}\\left(1-\\frac{q_{i}}{p_{i}}\\right)=q(\\bar{A})-p(\\bar{A})>-\\frac{1}{2}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, we cannot have $\\begin{array}{r}{d_{\\mathrm{KL}}(p_{A}\\vert\\vert q_{A})\\leqslant-\\frac{1}{2}\\varepsilon}\\end{array}$ and so $\\begin{array}{r}{d_{\\mathrm{KL}}(p_{A}\\Vert q_{A})\\geqslant\\frac{1}{2}\\varepsilon}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Claim 2.3. Let $\\hat{p}$ be the empirical estimator for an unknown discrete distribution $p$ supported on $[k]$ , based on $\\mathrm{Poi}(m)$ samples, where $\\begin{array}{r}{m\\,=\\,\\Theta\\Big(\\frac{\\log^{2}(k)}{\\varepsilon^{2}}\\Big)}\\end{array}$ ; assume that $d_{\\chi^{2}}(p_{A},q_{A})\\;\\leqslant\\;\\varepsilon/8$ and $\\begin{array}{r}{p(\\bar{\\mathcal{A}})+q(\\bar{\\mathcal{A}})\\leqslant4\\tau=\\frac{1}{4}\\frac{\\varepsilon}{\\log(k/\\varepsilon)}}\\end{array}$ ,10 then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\left|\\sum_{i\\in{\\mathcal{A}}}(p_{i}-\\hat{p}_{i})\\log\\frac{1}{q_{i}}\\right|\\geqslant\\frac{1}{8}\\varepsilon\\right]\\leqslant\\frac{1}{100}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "9In the case of $p_{i}=0$ , we still have $\\begin{array}{r}{p_{i}\\log\\left(\\frac{p_{i}}{q_{i}}\\right)\\geqslant p_{i}-q_{i}}\\end{array}$ .   \n10One can remove the assumption that $p(\\bar{\\mathcal{A}})+\\dot{q}(\\bar{\\mathcal{A}})\\leqslant4\\tau$ , at the cost of a slightly worse constant. ", "page_idx": 12}, {"type": "text", "text": "Proof of Claim 2.3. We follow the same analysis as in [WY16]. Letting $\\begin{array}{r}{Y_{i}:=(p_{i}-\\hat{p}_{i})\\log\\frac{1}{q_{i}}}\\end{array}$ for , we have and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{Var}[Y_{i}]=\\mathbb{E}[(Y_{i}-\\mathbb{E}[Y_{i}])^{2}]=\\mathbb{E}[Y_{i}^{2}]=\\mathbb{E}[(p_{i}-\\hat{p}_{i})^{2}]\\log^{2}\\frac{1}{q_{i}}=\\frac{1}{m^{2}}(m p_{i})\\log^{2}\\frac{1}{q_{i}}=\\frac{p_{i}}{m}\\log^{2}\\frac{1}{q_{i}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\begin{array}{r}{Y:=\\sum_{i\\in\\mathcal{A}}(p_{i}-\\hat{p}_{i})\\log\\frac{1}{q_{i}}}\\end{array}$ . We will use our assumption that $d_{\\chi^{2}}(p_{A},q_{A})\\leqslant\\varepsilon/8$ and $p(\\bar{\\mathcal{A}})+$ $\\begin{array}{r}{q(\\bar{A})\\leqslant\\frac{1}{4}\\frac{\\varepsilon}{\\log(k/\\varepsilon)}}\\end{array}$ below. Note that, our analysis is in the Poissonized setting: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log(1)}&{=\\quad\\operatorname{sur}\\left[\\sum_{(i)}\\bar{\\rho}_{i}-\\bar{\\rho}_{i}\\right]\\log\\frac{1}{q}}\\\\ &{=\\quad\\frac{\\sum_{i=1}^{B{\\nu}_{i}}\\bar{\\rho}_{i}}{\\sum_{a=1}^{B{\\nu}_{i}}\\mu_{a}^{2}}\\Bigg(\\frac{1}{q}\\Bigg)}\\\\ &{=\\quad\\frac{\\sum_{i=1}^{B{\\nu}_{i}}\\left(\\kappa_{i}\\right)\\left(\\kappa_{i}\\right)\\left(1+\\kappa_{i}^{\\mathrm{op}}\\left(\\frac{\\kappa_{i}}{q}\\right)\\right)^{2}}{\\sum_{a=1}^{B{\\nu}_{i}}\\left(\\kappa_{i}\\right)\\left(1+\\kappa_{i}^{\\mathrm{op}}\\left(\\frac{\\kappa_{i}}{q}\\right)\\right)^{3}}}\\\\ &{\\leqslant\\quad\\frac{\\sum_{i=1}^{B{\\nu}_{i}}\\bar{\\rho}_{i}}{\\sum_{a=1}^{B{\\nu}_{i}}\\mu_{a}^{2}}\\left(\\lambda\\left(\\kappa_{i}\\left(\\frac{1}{q}\\right)\\right)\\right)^{2}+2\\left(\\kappa_{i}\\left(\\frac{\\kappa_{i}}{q}\\right)\\right)^{3}\\right)}\\\\ &{=\\quad\\frac{\\sum_{i=1}^{B{\\nu}_{i}}\\bar{\\rho}_{i}}{\\sum_{a=1}^{B{\\nu}_{i}}\\mu_{a}^{2}}\\left(\\frac{1}{\\bar{\\rho}_{i}}\\right)+\\frac{\\sum_{i=1}^{B{\\nu}_{i}}\\bar{\\rho}_{i}}{\\sum_{a=1}^{B{\\nu}_{i}}\\mu_{a}^{2}}\\left(\\frac{\\bar{\\rho}_{i}}{\\bar{\\rho}_{i}}\\right)}\\\\ &{\\leqslant\\quad\\frac{\\sum_{i=1}^{B{\\nu}_{i}}\\bar{\\rho}_{i}}{\\sum_{a=1}^{B{\\nu}_{i}}\\mu_{a}^{2}}\\left(\\frac{1}{\\bar{\\rho}_{i}}\\right)+\\frac{\\sum_{i=1}^{B{\\nu}_{i}}\\bar{\\rho}_{i}}{\\sum_{a=1}^{B{\\nu}_{i}}\\mu_{a}^{2}}\\left(\\frac{\\bar{\\rho}_{i}}{\\bar{\\rho}_{i}}-\\frac{1}{\\bar{\\rho}_{i}}\\right)+ \n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For (13), we analyze by two cases: if $\\frac{p_{i}}{q_{i}}\\;\\;\\;\\geqslant\\;\\;1$ , we have that $\\begin{array}{r l r}{p_{i}\\log^{2}\\left(\\frac{p_{i}}{q_{i}}\\right)}&{{}\\leqslant}&{p_{i}\\left(\\frac{p_{i}}{q_{i}}-1\\right)}\\end{array}$ ; otherwise, $\\begin{array}{r l r}{p_{i}\\log^{2}\\left(\\frac{p_{i}}{q_{i}}\\right)}&{=}&{p_{i}\\log^{2}\\left(\\frac{q_{i}}{p_{i}}\\right)~<~p_{i}\\left(\\frac{q_{i}}{p_{i}}-1\\right)}\\end{array}$ . And we use [HJW15b, Lemma3], $\\begin{array}{r}{\\sum_{i\\in\\mathcal{A}}p_{i}\\log^{2}\\left(\\frac{1}{p_{i}}\\right)\\leqslant2\\log^{2}k+3}\\end{array}$ in (14). We use the premise and (4) in (15) and we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\nd_{\\mathrm{TV}}(p,q)=d_{\\mathrm{TV}}(p_{A},q_{A})+d_{\\mathrm{TV}}(p_{\\bar{A}},q_{\\bar{A}})\\leqslant{\\sqrt{d_{\\chi^{2}}(p_{A},q_{A})}}+p({\\bar{A}})+q({\\bar{A}})\\leqslant{\\sqrt{\\frac{\\varepsilon}{8}}}+4\\tau;\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the last step is obtained by noticing that $\\log(k)\\geqslant{\\frac{2}{3}}$ for $k\\geqslant2$ . By Chebyshev\u2019s inequality, we then have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[|Y|\\geqslant10{\\sqrt{\\frac{38\\log^{2}k}{m}}}\\right]\\leqslant\\operatorname*{Pr}\\left[|Y-\\mathbb{E}[Y]|\\geqslant10{\\sqrt{\\operatorname{Var}[Y]}}\\right]\\leqslant{\\frac{1}{100}}\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and this last inequality yields the statement as long as 22\u00d7100\u00d782 log2(k) = 140800 log2(k). ", "page_idx": 13}, {"type": "text", "text": "Lemma 2.4. Let $\\mathcal{A}:=\\{i\\in[k]\\ |\\ q_{i}\\geqslant\\alpha\\}$ . Let $\\begin{array}{r}{m_{2}\\,\\geqslant\\,16384\\,\\mathrm{max}\\left\\{\\sqrt{\\frac{1}{\\alpha\\varepsilon}},\\frac{\\sqrt{k}}{\\varepsilon}\\right\\}}\\end{array}$ be the number of samples used to compute $Z_{2}$ . Then $\\mathbb{E}[Z_{2}]=m_{2}d_{\\chi^{2}}(p_{A},q_{A})$ . Moreover, if $d_{\\chi^{2}}(p_{A},q_{A})\\leqslant\\frac{\\varepsilon}{2}$ , then $\\begin{array}{r}{\\mathrm{Var}[Z_{2}]\\leqslant(\\frac{1}{32}m_{2}\\varepsilon)^{2}}\\end{array}$ . If $d_{\\chi^{2}}(p_{A},q_{A})\\geqslant\\varepsilon$ , then $\\mathrm{Var}[Z_{2}]\\leqslant O(\\mathbb{E}[Z_{2}]^{2})$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 2.4. The proof is a relatively straightforward modification of the argument of [DKW18, Lemma 2]. We have the expectation and variance of $Z_{2}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[Z_{2}]=m_{2}d_{\\chi^{2}}(p_{A},q_{A})\\mathrm{~and~}\\operatorname{Var}[Z_{2}]=\\sum_{i\\in A}\\left[2\\frac{p_{i}^{2}}{q_{i}^{2}}+4m_{2}\\frac{p_{i}(p_{i}-q_{i})^{2}}{q_{i}^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It boils down to bounding the following, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{2\\displaystyle\\sum_{i\\in A}\\frac{p_{i}^{2}}{q_{i}^{2}}}}&{{\\leqslant}}&{{4k+4\\displaystyle\\sum_{i\\in A}\\frac{(p_{i}-q_{i})^{2}}{q_{i}^{2}}}}\\\\ {{}}&{{\\leqslant}}&{{4k+\\displaystyle\\frac{4}{\\alpha}\\displaystyle\\sum_{i\\in A}\\frac{(p_{i}-q_{i})^{2}}{q_{i}}}}\\\\ {{}}&{{\\leqslant}}&{{4k+\\displaystyle\\frac{4}{\\alpha m_{2}}\\mathbb{E}[Z_{2}].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Derivation of the inequalities follow from [DKW18, proof of Lemma 2]. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle4m_{2}\\sum_{i\\in A}\\frac{p_{i}(p_{i}-q_{i})^{2}}{q_{i}^{2}}}&{\\leqslant}&{\\displaystyle4m_{2}\\left(\\sum_{i\\in A}\\frac{p_{i}^{2}}{q_{i}^{2}}\\right)^{1/2}\\left(\\sum_{i\\in A}\\frac{(p_{i}-q_{i})^{4}}{q_{i}^{2}}\\right)^{1/2}}\\\\ &{\\leqslant}&{\\displaystyle4m_{2}\\left(4k+\\frac{4}{\\alpha m_{2}}\\mathbb{E}[Z_{2}]\\right)^{1/2}\\left(\\sum_{i\\in A}\\frac{(p_{i}-q_{i})^{2}}{q_{i}}\\right)}\\\\ &{=}&{\\displaystyle4\\left(2\\sqrt{k}+2\\sqrt{\\frac{1}{\\alpha m_{2}}\\mathbb{E}[Z_{2}]}\\right)\\mathbb{E}[Z_{2}]}\\\\ &{=}&{\\displaystyle8\\sqrt{k}\\mathbb{E}[Z_{2}]+8(\\alpha m_{2})^{-1/2}(\\mathbb{E}[Z_{2}])^{3/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combing both, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Var}[Z_{2}]\\leqslant4k+\\left(\\frac{4}{\\alpha m_{2}}+8\\sqrt{k}\\right)\\mathbb{E}[Z_{2}]+8(\\alpha m_{2})^{-1/2}(\\mathbb{E}[Z_{2}])^{3/2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $d_{\\chi^{2}}(p_{A},q_{A})\\leqslant\\varepsilon/2$ , then $\\begin{array}{r}{\\mathbb{E}[Z_{2}]\\leqslant{\\frac{m_{2}\\varepsilon}{2}}}\\end{array}$ ; and we solve $\\begin{array}{r}{\\mathrm{Var}[Z_{2}]\\leqslant(\\frac{1}{32}m_{2}\\varepsilon)^{2}}\\end{array}$ , which gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n4k+\\left({\\frac{4}{\\alpha m_{2}}}+8\\sqrt{k}\\right){\\frac{m_{2}\\varepsilon}{2}}+8(\\alpha m_{2})^{-1/2}\\bigl({\\frac{m_{2}\\varepsilon}{2}}\\bigr)^{3/2}\\leqslant({\\frac{1}{32}}m_{2}\\varepsilon)^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We solve for the relaxation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{LHS}\\leqslant4\\cdot\\operatorname*{max}\\left\\{4k,\\frac{2\\varepsilon}{\\alpha},4\\sqrt{k}m_{2}\\varepsilon,8\\frac{m_{2}}{\\sqrt{\\alpha}2^{3/2}}\\varepsilon^{3/2}\\right\\}\\leqslant(\\frac{1}{32}m_{2}\\varepsilon)^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the end, we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{128\\cdot\\frac{\\sqrt{k}}{\\varepsilon},64\\sqrt{\\frac{2}{\\alpha\\varepsilon}},32^{2}\\cdot16\\sqrt{k},32^{2}\\cdot16\\frac{1}{\\sqrt{\\alpha\\varepsilon}\\sqrt{2}}\\right\\}\\leqslant32^{2}\\cdot16\\cdot\\operatorname*{max}\\left\\{\\frac{\\sqrt{k}}{\\varepsilon},\\sqrt{\\frac{1}{\\alpha\\varepsilon}}\\right\\}\\leqslant m_{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $d_{\\chi^{2}}(p_{A},q_{A})\\geqslant\\varepsilon$ , then $\\mathbb{E}[Z_{2}]\\geqslant m_{2}\\varepsilon$ ; and we solve $\\begin{array}{r}{\\mathrm{Var}[Z_{2}]\\leqslant(\\frac{1}{4}\\mathbb{E}[Z_{2}])^{2}}\\end{array}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n4k+\\left({\\frac{4}{\\alpha m_{2}}}+8{\\sqrt{k}}\\right)\\mathbb{E}[Z_{2}]+8(\\alpha m_{2})^{-1/2}(\\mathbb{E}[Z_{2}])^{3/2}\\leqslant({\\frac{1}{4}}\\mathbb{E}[Z_{2}])^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is equivalent to the following ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{4k}{(\\mathbb{E}[Z_{2}])^{3/2}}+\\left(\\frac{4}{\\alpha m_{2}}+8\\sqrt{k}\\right)\\frac{1}{(\\mathbb{E}[Z_{2}])^{1/2}}+8(\\alpha m_{2})^{-1/2}\\leqslant\\frac{1}{16}(\\mathbb{E}[Z_{2}])^{1/2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Further relaxing the solution, it is enough to have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{4k}{(\\mathbb{E}[Z_{2}])^{3/2}}+\\left(\\frac{4}{\\alpha m_{2}}+8\\sqrt{k}\\right)\\displaystyle\\frac1{(\\mathbb{E}[Z_{2}])^{1/2}}+8(\\alpha m_{2})^{-1/2}}\\\\ {\\leqslant}&{\\displaystyle\\frac{4k}{(m_{2}\\varepsilon)^{3/2}}+\\left(\\frac{4}{\\alpha m_{2}}+8\\sqrt{k}\\right)\\displaystyle\\frac1{(m_{2}\\varepsilon)^{1/2}}+8\\frac1{\\sqrt{\\alpha m_{2}}}}\\\\ {\\leqslant}&{\\displaystyle\\frac1{16}(m_{2}\\varepsilon)^{1/2}\\leqslant\\displaystyle\\frac1{16}(\\mathbb{E}[Z_{2}])^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as long as the following holds, ", "page_idx": 15}, {"type": "equation", "text": "$$\nm_{2}\\geqslant64\\operatorname*{max}\\left\\{\\frac{2\\sqrt{k}}{\\varepsilon},2\\sqrt{\\frac{1}{\\alpha\\varepsilon}},8\\frac{\\sqrt{k}}{\\varepsilon},8\\sqrt{\\frac{\\alpha}{\\varepsilon}}\\right\\}=\\operatorname*{max}\\left\\{128\\sqrt{\\frac{1}{\\alpha\\varepsilon}},512\\frac{\\sqrt{k}}{\\varepsilon}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Letting $\\begin{array}{r}{m_{2}\\geqslant512\\operatorname*{max}\\left\\{\\sqrt{\\frac{1}{\\alpha\\varepsilon}},\\frac{\\sqrt{k}}{\\varepsilon}\\right\\}}\\end{array}$ , we have that both statements. ", "page_idx": 15}, {"type": "text", "text": "C Proofs of testing Bayesian networks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Claim 3.3. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d_{\\mathrm{KL}}(p_{G}||p_{G:\\mathcal{B}})}&{=\\phantom{-}\\sum p(x)\\log\\frac{p(x)}{p_{G}(x)}}\\\\ &{=\\phantom{-}-\\sum p(x)\\log\\frac{p_{G}(x)}{p_{F}(x)}}\\\\ &{\\geqslant\\phantom{-}-\\left(\\sum p_{G}|\\alpha\\rangle\\right)\\cdot\\log\\left(\\frac{\\sum_{x\\in\\mathcal{B}}p(x)\\cdot\\frac{p_{G}(x)}{p_{G}(x)}}{\\sum_{x\\in\\mathcal{B}}p(x)}\\right)}\\\\ &{=\\phantom{-}-p(\\overline{{u}})\\cdot\\log\\left(\\frac{p_{G}(\\overline{{u}})}{p(\\overline{{u}})}\\right)}\\\\ &{\\geqslant\\phantom{-}-O(\\varepsilon^{2}/\\log(1/\\varepsilon))\\cdot\\log\\left(\\frac{1}{\\varepsilon^{2}/\\log(1/\\varepsilon)}\\right)}\\\\ &{\\geqslant\\phantom{-}-O(\\varepsilon^{2}/\\log(1/\\varepsilon))\\cdot\\log\\left(\\frac{1}{\\varepsilon^{2}/\\log(1/\\varepsilon)}\\right)}\\\\ &{\\geqslant\\phantom{-}-O(\\varepsilon^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we use monotonicity of $-x\\log{\\frac{1}{x}}$ and the fact that $p_{G}(\\bar{\\mathcal{U}})\\leqslant1$ in the second last inequality. ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 3.4. By Claim 3.3 and the assumption that $d_{\\mathrm{KL}}(p\\|p_{G})\\leqslant O(\\varepsilon^{2})$ , we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p_{\\mathcal{U}}\\|p_{G;\\mathcal{U}})=d_{\\mathrm{KL}}(p\\|p_{G})-d_{\\mathrm{KL}}(p_{\\bar{\\mathcal{U}}}\\|p_{G;\\bar{\\mathcal{U}}})\\leqslant O(\\varepsilon^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n7(\\varepsilon^{2})\\leqslant d_{H}^{2}(p,q)=d_{H}^{2}(p u,q u)+d_{H}^{2}(p_{\\bar{U}},q_{\\bar{U}})\\leqslant d_{H}^{2}(p u,q u)+{\\frac{1}{2}}(p(\\bar{U})+q(\\bar{U}))\\leqslant d_{H}^{2}(p u,q u)+O(\\varepsilon^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Omega(\\varepsilon^{2})\\leqslant d_{H}^{2}(p u,q u)\\leqslant d_{\\mathrm{KL}}(p u\\|q u)+q(\\mathcal{U})-p(\\mathcal{U})\\Rightarrow d_{\\mathrm{KL}}(p u\\|q u)\\geqslant\\Omega(\\varepsilon^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By (18), we write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\eta(\\hat{\\sigma}^{(2)}-d_{\\mathrm{in}}(\\mathrm{fiel}))\\|\\mathcal{E}_{\\alpha,i})}&{\\leqslant d_{\\mathrm{in}}(|\\mathcal{N}_{i}||\\mathcal{H}_{\\alpha})}\\\\ &{=}&{\\displaystyle\\sum_{s\\in\\{a,b\\}}|\\hat{\\sigma}_{i}|\\hat{\\sigma}_{i}|\\hat{\\sigma}_{i}|}\\\\ &{=}&{\\displaystyle\\sum_{s\\in\\{a,b\\}}|\\hat{\\sigma}_{i}|\\hat{\\sigma}_{i}|\\hat{\\sigma}_{i}|}\\\\ &{=}&{\\displaystyle\\sum_{s\\in\\{a,b\\}}|\\hat{\\sigma}_{i}|\\hat{\\sigma}_{i}|\\hat{\\sigma}_{i}|\\hat{\\sigma}_{i}^{(2)}}\\\\ &{\\leqslant}&{\\displaystyle\\sum_{s\\in\\{a,b\\}}\\rho_{1}\\left(\\sum_{s=1}^{b}\\sum_{\\ell_{1}(\\hat{\\sigma}_{i},\\hat{\\sigma}_{i}^{(2)})}\\frac{\\hat{\\sigma}_{i}}{\\sqrt{a,b}}\\log\\frac{\\hat{\\sigma}_{i}^{(2)}}{\\mu(\\hat{\\sigma}_{i}^{(2)})}\\right)}\\\\ &{=}&{\\displaystyle\\sum_{s\\in\\{a,b\\}}\\rho_{1}()\\left(\\sum_{s=1}^{b}\\log\\frac{\\hat{\\sigma}_{i}(\\hat{\\sigma}_{i},\\hat{\\sigma}_{i}^{(2)})}{\\sqrt{a,b}}-\\sum_{\\ell_{1}(\\hat{\\sigma}_{i},\\hat{\\sigma}_{i}^{(2)})}\\log\\frac{\\hat{\\sigma}_{i}^{(2)}}{\\mu(\\hat{\\sigma}_{i}^{(2)})}\\right)}\\\\ &{=}&{\\displaystyle\\sum_{s\\in\\{a,b\\}}\\sum_{i=1}^{b}\\rho_{i}()\\left(\\log\\frac{\\hat{\\sigma}_{i}(\\hat{\\sigma}_{i},\\hat{\\sigma}_{i}^{(2)})}{\\sqrt{a,b}}-\\sum_{\\ell_{1}(\\hat{\\sigma}_{i},\\hat{\\sigma}_{i}^{(2)})}\\log\\frac{\\hat{\\sigma}_{i}^{(2)}}{\\mu(\\hat{\\sigma}_{i}^{(2)})}\\right)}\\\\ &{=}&{\\displaystyle\\sum_{s\\in\\{a,b\\}}\\sum_{\\ell_{1}(\\hat{\\sigma}_{i},\\hat{\\sigma}_{\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}d_{\\mathrm{KL}}(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}||q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}})\\geqslant\\Omega(\\varepsilon^{2})+\\sum_{i=1}^{n}d_{\\mathrm{KL}}(p_{\\Pi_{i}^{G};A_{i}^{\\prime}}||q_{\\Pi_{i}^{G};A_{i}^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p_{\\Pi_{i}^{G};\\bar{A}_{i}^{\\prime}}||q_{\\Pi_{i}^{G};\\bar{A}_{i}^{\\prime}})\\geqslant-p_{\\Pi_{i}^{G}}(\\bar{\\mathcal{A}}_{i}^{\\prime})\\cdot\\log\\left(\\frac{q_{\\Pi_{i}^{G}}(\\bar{\\mathcal{A}}_{i}^{\\prime})}{p_{\\Pi_{i}^{G}}(\\bar{\\mathcal{A}}_{i}^{\\prime})}\\right)\\geqslant O(\\varepsilon^{2}/n).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And we can conclude by rearranging: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}d_{\\mathrm{KL}}(p_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}}\\Vert q_{X_{i},\\Pi_{i}^{G};A_{i}^{\\prime}})\\geqslant\\Omega(\\varepsilon^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Proofs of entropy testing lower bounds ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 2.6. With fewer than $c_{3}\\cdot\\sqrt{k}/\\varepsilon$ samples from $p$ , no tester can distinguish between $p=q$ and $\\begin{array}{r}{|H(p)-H(q)|\\geqslant\\varepsilon}\\end{array}$ with probability higher than $2/3$ , where $c_{3}>0$ is an absolute constant. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 2.6. This follo\u221aws from the standard uniformity testing lower bound [Pan08], which provides a lower bound of $\\Omega(\\sqrt{k}/\\eta^{2})$ : there exists a family of distributions that are hard to distinguish from uniform $u_{k}$ , using fewer than $c_{1}\\cdot\\sqrt{k}/\\eta$ samples. Let $k$ be an even number; the construction is by taking $\\theta=\\{-1,1\\}^{k/2}$ uniformly at random, and letting, for every $i\\in[k/2]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\theta}^{\\mathrm{no}}(2i)=\\frac{1+\\theta_{i}\\cdot\\eta}{k},\\qquad p_{\\theta}^{\\mathrm{no}}(2i+1)=\\frac{1-\\theta_{i}\\cdot\\eta}{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can verify that for any $\\theta$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n|H(p_{\\theta}^{\\mathrm{no}})-H(u_{k})|=\\log k-\\frac{k}{2}\\left(\\frac{1+\\eta}{k}\\log\\left(\\frac{1+\\eta}{k}\\right)+\\frac{1-\\eta}{k}\\log\\left(\\frac{1-\\eta}{k}\\right)\\right)=\\Theta(\\eta^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Setting $\\eta=\\varepsilon^{2}$ yields the lower bound of $\\begin{array}{r}{\\Omega\\left(\\frac{\\sqrt{k}}{\\varepsilon}\\right)}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Lemma 2.7. With fewer than $c_{4}\\cdot\\log^{2}k/\\varepsilon^{2}$ samples from $p_{i}$ , no tester can distinguish between $p=q$ and $|H(p)-H(q)|\\geqslant\\varepsilon$ with probability higher than $2/3$ , where $c_{4}>0$ is an absolute constant. ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 2.7. Following [WY16, B.2 Proof of Proposition 2], we look at the same construction but with different parameters \u03b5\u2032 = log(2(\u03b5k\u22121)): ", "page_idx": 17}, {"type": "equation", "text": "$$\np=\\left({\\frac{1}{3(k-1)}},\\ldots,{\\frac{1}{3(k-1)}},{\\frac{2}{3}}\\right),\\quad q=\\left({\\frac{1+\\varepsilon^{\\prime}}{3(k-1)}},\\ldots,{\\frac{1+\\varepsilon^{\\prime}}{3(k-1)}},{\\frac{2-\\varepsilon^{\\prime}}{3}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "One can check that ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(q)-H(p)\\geqslant{\\frac{1}{3}}\\log(2(k-1))\\varepsilon^{\\prime}-\\varepsilon^{\\prime^{2}}=\\Omega(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, direct calculation of the (squared) Hellinger distance shows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\mathrm{H}}(p,q)^{2}=\\Theta(\\varepsilon^{\\prime2})=\\Theta\\left(\\frac{\\varepsilon^{2}}{\\log^{2}k}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that $p$ and $q$ cannot be distinguished with fewer than $c_{4}{\\frac{\\log^{2}k}{\\varepsilon^{2}}}$ samples [BY02, Theorem 4.7]. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E Sketch proof of $\\begin{array}{r}{O\\left(\\frac{\\sqrt{k\\log\\log\\log(k/\\varepsilon)}}{\\varepsilon}+\\frac{\\log^{2}(k)}{\\varepsilon^{2}}\\right)}\\end{array}$ upper bound. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We will rely the following inequality for compression, both via Jensen\u2019s inequality, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\sum_{i\\in\\Delta}p_{i}\\right)\\log\\left(\\frac{1}{\\sum_{i\\in\\Delta}p_{i}}\\right)\\leqslant\\sum_{i\\in\\Delta}p_{i}\\log\\frac{1}{p_{i}}\\leqslant\\left(\\sum_{i\\in\\Delta}p_{i}\\right)\\log\\left(\\frac{|\\Delta|}{\\sum_{i\\in\\Delta}p_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as $\\log(x)$ is concave and $\\log\\left({\\frac{1}{x}}\\right)$ is convex. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\Delta}p_{i}\\log\\frac{1}{p_{i}}\\geqslant\\left(\\sum_{i\\in\\Delta}p_{i}\\right)\\log\\left(\\frac{\\sum_{i\\in\\Delta}p_{i}}{\\sum_{i\\in\\Delta}p_{i}^{2}}\\right)\\geqslant\\left(\\sum_{i\\in\\Delta}p_{i}\\right)\\log\\left(\\frac{1}{\\sum_{i\\in\\Delta}p_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "suggesting that if we merge elements of $\\Delta$ into one, then we will lose a $\\mathrm{log}(|\\Delta|)$ factor of the entropy. By merging enough elements, we can then reduce this problem into the first case, where elements have large enough mass in each location. ", "page_idx": 17}, {"type": "text", "text": "Claim E.1. Let $S\\subseteq[k],\\,i f p(S)-q(S)>-\\eta,$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n|d_{\\mathrm{KL}}(p_{S}\\|q_{S})|\\geqslant\\eta\\Rightarrow d_{\\mathrm{KL}}(p_{S}\\|q_{S})\\geqslant\\eta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. If $\\lvert d_{\\mathrm{KL}}(p_{S}\\lvert\\lvert q_{S})\\rvert\\geqslant\\eta$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p s\\|q s)\\geqslant\\eta\\ \\mathrm{or}\\ d_{\\mathrm{KL}}(p s\\|q s)\\leqslant-\\eta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will rule out the second case, using that $\\log{\\frac{1}{x}}\\geqslant1-x$ for $x>0$ , 11 ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{\\mathrm{KL}}(p_{S}\\|q_{S})=\\sum_{i\\in S}p_{i}\\log\\frac{p_{i}}{q_{i}}\\geqslant\\sum_{i\\in S}p_{i}\\left(1-\\frac{q_{i}}{p_{i}}\\right)=p(S)-q(S)>-\\eta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we cannot have $d_{\\mathrm{KL}}(p_{S}\\|q_{S})\\leqslant-\\eta$ and so $d_{\\mathrm{KL}}(p_{S}\\|q_{S})\\geqslant\\eta$ . ", "page_idx": 17}, {"type": "text", "text": "We use the same idea as our first upper bound but choose a series of thresholds. Let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle{S_{3}:=\\left\\{i\\in[k]|q_{i}\\geqslant\\Omega\\left(\\frac{\\varepsilon}{k\\log\\log\\log(k/\\varepsilon)}\\right)\\right\\};}}&{\\,}&\\\\ {\\displaystyle{\\left\\{i\\in[k]|\\Omega\\left(\\frac{\\varepsilon}{k\\log\\log(k/\\varepsilon)}\\right)\\leqslant q_{i}\\leqslant O\\left(\\frac{\\varepsilon}{k\\log\\log\\log(k/\\varepsilon)}\\right)\\right\\};}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "11In the case of $p_{i}=0$ , we still have $\\begin{array}{r}{p_{i}\\log\\left(\\frac{p_{i}}{q_{i}}\\right)\\geqslant p_{i}-q_{i}}\\end{array}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\nS_{1}=\\left\\{i\\in[k]|\\Omega\\left(\\frac{\\varepsilon}{k\\log(k/\\varepsilon)}\\right)\\leqslant q_{i}\\leqslant O\\left(\\frac{\\varepsilon}{k\\log\\log(k/\\varepsilon)}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The following calculation ensues ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\Omega(\\varepsilon)}&{\\leqslant}&{|H(p_{A})-H(q_{A})|}\\\\ &{\\leqslant}&{\\displaystyle\\left|\\sum_{i=1}^{3}d_{\\mathrm{KL}}(p_{\\mathcal{S}_{i}},q_{\\mathcal{S}_{i}})\\right|+\\left|\\sum_{i\\in A}(p_{i}-q_{i})\\log\\frac{1}{q_{i}}\\right|}\\\\ &{\\leqslant}&{\\displaystyle\\sum_{i=1}^{3}|d_{\\mathrm{KL}}(p_{\\mathcal{S}_{i}},q_{\\mathcal{S}_{i}})|+\\left|\\sum_{i\\in A}(p_{i}-q_{i})\\log\\frac{1}{q_{i}}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have that one of the four terms will be at least $\\Omega(\\varepsilon/4)$ . If it is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\sum_{i\\in\\mathcal{A}}(p_{i}-q_{i})\\log\\frac{1}{q_{i}}\\right|\\geqslant\\Omega(\\varepsilon),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is testable with $O\\left(\\frac{\\log^{2}(k)}{\\varepsilon^{2}}\\right)$ samples using arguments from proof of Theorem 2.1. If it is $|d_{\\mathrm{KL}}(p_{S_{i}},q_{S_{i}})|\\geqslant\\Omega(\\varepsilon)$ , for $i={1,2,3}$ . We have the following: ", "page_idx": 18}, {"type": "text", "text": "Case $S_{3}$ . Suppose $|d_{\\mathrm{KL}}(p_{S_{3}},q_{S_{3}})|\\geqslant\\Omega(\\varepsilon)$ . We will check whether $\\begin{array}{r}{p(S_{3})\\geqslant\\Omega\\left(\\frac{\\varepsilon}{\\log\\log\\log(k/\\varepsilon)}\\right)}\\end{array}$ , if not, we can reject. We proceed assuming the inequality holds. Note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n|p(S_{3})-q(S_{3})|=|p(\\overline{{S_{3}}})-q(\\overline{{S_{3}}})|\\leqslant O\\left(\\frac{\\varepsilon}{\\log\\log\\log(k/\\varepsilon)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, $\\begin{array}{r}{p(\\overline{{S_{3}}})\\,-\\,q(\\overline{{S_{3}}})\\,>\\,-O\\left(\\frac{\\varepsilon}{\\log\\log\\log(k/\\varepsilon)}\\right)}\\end{array}$ , and by Claim E.1, we have that $d_{\\mathrm{KL}}(p_{S_{3}},q_{S_{3}})\\gtrless$ $\\Omega(\\varepsilon)$ . Using (4), we then have that $d_{\\chi^{2}}(p_{S_{3}},q_{S_{3}})\\ \\ \\geqslant\\ \\ \\Omega(\\varepsilon)$ . Using Lemma 2.4 (setting k log log \u03b5log(k/\u03b5) ), and similar argument from the proof of Theorem 2.1, we have that $\\underset{q_{S_{3}}.}{O\\left(\\frac{\\sqrt{k\\log\\log\\log(k/\\varepsilon)}}{\\varepsilon}\\right)}$ suffices to check between the case that $d_{\\chi^{2}}(p_{S_{3}},q_{S_{3}})\\,\\geqslant\\,\\Omega(\\varepsilon)$ and $p_{S_{3}}~=$ ", "page_idx": 18}, {"type": "text", "text": "Case $S_{2}$ . Suppose $|d_{\\mathrm{KL}}(p_{S_{2}},q_{S_{2}})|\\geqslant\\Omega(\\varepsilon)$ . We will check whether ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Omega\\left(\\frac{\\varepsilon}{\\log\\log(k/\\varepsilon)}\\right)\\leqslant p(S_{2})\\leqslant O\\left(\\frac{\\varepsilon}{\\log\\log\\log(k/\\varepsilon)}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "if not, we will reject. We proceed assuming the inequality holds. Now, recall that the main bottleneck of the $\\chi^{2}$ tester analyzed in Lemma 2.4 is due to the minimum probability $\\alpha=\\operatorname*{min}_{i\\in S_{2}}q_{i}$ (increasing this would decrease the sample complexity). Our main idea here is to increase $\\alpha$ by merging a suitable number $(\\log\\log(k/\\varepsilon)$ in this case) of elements into one single bin to form a new distribution to test. Denote $\\Delta_{j}$ where log l|oSg2(|k/\u03b5) and  j \u2206j = S2. We will subsequently treat every elements in $\\Delta_{j}$ as 1 bin in the new distribution, calling it $p_{\\Delta},q_{\\Delta}$ and denote $p(\\Delta_{j}),q(\\Delta_{j})$ as mass on $\\Delta_{j}$ , where $\\begin{array}{r}{\\check{p}(\\Delta_{j})=\\sum_{i\\in\\Delta_{j}}p_{i}}\\end{array}$ . This gives us the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{.~}q(\\Delta_{j})\\quad\\geqslant\\quad\\Omega\\left(\\frac{\\varepsilon}{k\\log\\log(k/\\varepsilon)}\\right)\\,\\mathrm{\\;:\\;}|\\Delta_{j}|\\quad\\geqslant\\quad\\Omega\\left(\\frac{\\varepsilon}{k}\\right)\\,\\mathrm{;\\;~the\\;~domain\\;\\;size\\;\\;is\\;\\,}\\frac{|S_{2}|}{\\operatorname*{min}_{j}|\\Delta_{j}|}\\quad\\leqslant}\\\\ &{\\quad O\\left(\\frac{k}{\\log\\log(k/\\varepsilon)}\\right)\\cdot}\\\\ &{\\cdot\\sum_{j}p(\\Delta_{j})=p(S_{2})\\leqslant O\\left(\\frac{\\varepsilon}{\\log\\log\\log(k/\\varepsilon)}\\right)\\mathrm{and}\\sum_{j}q(\\Delta_{j})=q(S_{2})\\leqslant O\\left(\\frac{\\varepsilon}{\\log\\log\\log(k/\\varepsilon)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "iii. Their entropy difference is preserved, which we will prove next: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\sum_{j}p(\\Delta_{j})\\log{\\frac{1}{p(\\Delta_{j})}}-\\sum_{j}q(\\Delta_{j})\\log{\\frac{1}{q(\\Delta_{j})}}\\right|\\geqslant\\Omega(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that these are better conditions compared to i. and ii. in the proof of Theorem 2.1 (in this analysis, using ii., it is sufficient to prove that $d_{\\mathrm{KL}}(p_{\\Delta},q_{\\Delta})\\geqslant\\Omega(\\varepsilon)$ in view of Claim E.1). The gain comes from the fact that we can apply Lemma 2.4 with better $\\begin{array}{r}{\\alpha=\\operatorname*{min}_{j}q(\\Delta_{j})\\geqslant\\Omega\\left(\\frac{\\varepsilon}{k}\\right)}\\end{array}$ and thus ", "page_idx": 19}, {"type": "equation", "text": "$$\nO\\left({\\sqrt{\\frac{1}{\\alpha\\varepsilon}}}+{\\sqrt{\\frac{k^{\\prime}}{\\varepsilon}}}\\right)=O\\left({\\sqrt{\\frac{k}{\\varepsilon^{2}}}}\\right)=O\\left({\\frac{\\sqrt{k}}{\\varepsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, the gain only affect Claim 2.3 by constant factors. The soundness and completeness then follows similarly to the proof of Theorem 2.1. We prove (iii.) next: ", "page_idx": 19}, {"type": "text", "text": "Suppose $H(p_{S_{2}})-H(q_{S_{2}})\\geqslant\\varepsilon$ , then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Omega(\\varepsilon)}&{\\leqslant\\;\\sum_{l\\in\\mathcal{S}_{2}}p_{l}\\log\\frac{1}{p_{l}}-\\sum_{l\\in\\mathcal{S}_{2}}q_{l}\\log\\frac{1}{q_{l}}}\\\\ &{=\\;\\sum_{j}\\sum_{i\\in\\mathcal{A}_{3}}p_{i,j}\\log\\frac{1}{p_{i,j}}-\\sum_{j}\\sum_{i\\in\\mathcal{A}_{2}}q_{i,j}\\log\\frac{1}{q_{i,j}}}\\\\ &{\\leqslant\\;\\sum_{j}p(\\Delta_{j})\\log\\frac{|\\Delta_{j}|}{p(\\Delta_{j})}-\\sum_{j}q(\\Delta_{j})\\log\\frac{1}{q(\\Delta_{j})}}\\\\ &{=\\;\\sum_{j}p(\\Delta_{j})\\log|\\Delta_{j}|+\\sum_{j}p(\\Delta_{j})\\log\\frac{1}{p(\\Delta_{j})}-\\sum_{j}q(\\Delta_{j})\\log\\frac{1}{q(\\Delta_{j})}.}\\\\ &{\\leqslant\\;\\;O\\left(\\frac{\\varepsilon}{\\log\\log(\\log(k/\\varepsilon))}\\right)\\operatorname*{max}\\log|\\Delta_{j}|+\\sum_{j}p(\\Delta_{j})\\log\\frac{1}{p(\\Delta_{j})}-\\sum_{j}q(\\Delta_{j})\\log\\frac{1}{q(\\Delta_{j})}\\frac{(\\varepsilon2)}{q(\\Delta_{j})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the (21) is due to (19) and for (22), recall that j p(\u2206j) = p(S2) \u2a7dO log log l\u03b5og(k/\u03b5) . Suppose $H(q_{S_{2}})-H(p_{S_{2}})\\geq\\Omega(\\varepsilon)$ , the same goes below: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\Omega(\\varepsilon)}&{\\leqslant}&{\\displaystyle\\sum_{l}q_{l}\\log\\frac{1}{q_{l}}-\\sum_{l}p_{l}\\log\\frac{1}{p_{l}}}\\\\ &{=}&{\\displaystyle\\sum_{j}\\sum_{i\\in\\Delta_{j}}q_{i,j}\\log\\frac{1}{q_{i,j}}-\\sum_{j}\\sum_{i\\in\\Delta_{j}}p_{i,j}\\log\\frac{1}{p_{i,j}}}\\\\ &{\\leqslant}&{\\displaystyle\\sum_{j}q(\\Delta_{j})\\log\\frac{|\\Delta_{j}|}{q(\\Delta_{j})}-\\sum_{j}p(\\Delta_{j})\\log\\frac{1}{p(\\Delta_{j})}}\\\\ &{\\leqslant}&{O(\\varepsilon)+\\displaystyle\\sum_{j}q(\\Delta_{j})\\log\\frac{1}{q(\\Delta_{j})}-\\sum_{j}p(\\Delta_{j})\\log\\frac{1}{p(\\Delta_{j})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we have proved (iii.). ", "page_idx": 19}, {"type": "text", "text": "Case $S_{1}$ .The proof follow similar to Case $S_{2}$ , but by merging $\\log(k/\\varepsilon)$ elements. ", "page_idx": 19}, {"type": "text", "text": "F Derivation for KL decomposition ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\alpha_{\\mathrm{LG},1}(y,p_{G})}\\\\ &{=\\frac{p_{G}}{\\sqrt{\\alpha_{2}}\\omega_{2}}\\left(\\alpha\\right)\\log{\\frac{p_{G}(x)}{p_{G}(x)}}}\\\\ &{=\\underbrace{\\sum_{\\ell\\in\\{1,1\\}^{n}}p_{\\ell}(x)\\log{\\frac{\\prod_{i=1}^{n}p_{\\ell}(x_{i}|x_{i}|)}{\\prod_{i=1}^{n}p_{\\ell}(x_{i}|x_{i}|^{2})}}}_{\\ell\\in\\{1\\}^{n}\\omega_{1}}}\\\\ &{=\\underbrace{\\sum_{\\ell\\in\\{1,1\\}^{n}}p_{\\ell}(x)\\log{\\left(\\prod_{i=1}^{n}(x_{i}|x_{i}|)\\right)}}_{\\ell\\in\\{1,1\\}^{n}}-p_{\\ell}(x)\\log{\\left(\\prod_{i=1}^{n}(x_{i}|x_{i}|^{\\alpha})\\right)}}\\\\ &{=\\underbrace{\\sum_{\\ell\\in\\{1,1\\}^{n}}p_{\\ell}(x)\\log(p_{G}(x)|x_{i}|)}_{\\ell\\in\\{1,1\\}^{n}}-p_{\\ell}(x)\\log[\\alpha_{1}(x_{i}|^{\\alpha})}\\\\ &{=\\underbrace{\\sum_{\\ell\\in\\{1,1\\}^{n}}p_{\\ell}(x)\\log(p_{G}(x)|x_{i}|)-p_{\\ell}(x)\\log(p_{G}(x)|x_{i}|^{2})}_{\\ell\\in\\{1,1\\}^{n}}}\\\\ &{=\\underbrace{\\sum_{\\ell\\in\\ \\{1,1\\}^{n}}\\sum_{\\ell\\in\\{1,1\\}^{n}}p_{\\ell}(x)\\log(p_{G}(x)|x_{i}|^{\\alpha})}_{\\ell\\in\\ \\{1,1\\}^{n}}-p_{\\ell}(x)\\log(p_{G}(x_{i}|x_{i}|^{\\alpha}))}\\\\ &{=\\underbrace{\\sum_{\\ell\\in\\ \\{1,1\\}^{n}}\\left(\\alpha_{1}\\sum_{\\ell=1}^{n}p_{\\ell}(x_{i},\\cdot)\\log(p_{G}(x)|x_{i}|)\\right)}_{\\ell\\in\\ \\{1,1,2\\}^{n}}-\\underbrace{\\sum_{\\ell\\in\\ \\{1,1\\}^{n}}p_{\\ell}(x_{i},\\\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\pi_{i},\\Pi_{i}$ denote the parents of $x_{i},X_{i}$ in Bayes net $p$ (a set of random variables or their domain); and $\\pi_{i}^{G},\\Pi_{i}^{G}$ as the parents defined by $G$ . $p_{G}$ is the projection of $p$ unto $G$ as defined by Definition 3.2. It is not hard to see that the derivation extends beyond the case of hypercube, $\\{0,1\\}^{n}$ . ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The proofs are provided either in the main paper or the appendix, both part of the submission. Assumptions are fully stated in the theorem and lemma statements. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not contain experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 23}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: this work is theoretical in nature; it is hard to predict its societal impact. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the ", "page_idx": 24}, {"type": "text", "text": "technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]