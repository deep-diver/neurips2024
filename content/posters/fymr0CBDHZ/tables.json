[{"figure_path": "fymr0CBDHZ/tables/tables_2_1.jpg", "caption": "Table 1: Mean and standard deviation of Pearson correlation coefficients (r) calculated between style and linguistics embeddings for real and TTS/VC samples across 5 unseen speakers. Significant difference (as per Welch\u2019s t-test) is seen between real speech and all types of generated speech.", "description": "This table presents the Pearson correlation coefficients (r) and standard deviations calculated between style and linguistic embeddings for real and synthetic speech samples.  The data includes results for five unseen speakers and various TTS/VC systems.  The significance of the difference between real and generated speech is evaluated using Welch's t-test, demonstrating a statistically significant difference.", "section": "3.1 Motivation"}, {"figure_path": "fymr0CBDHZ/tables/tables_6_1.jpg", "caption": "Table 2: Detection performance on different deepfake datasets. Experiments were repeated three times with different random seeds, and average metric values are reported. #Param refers to the number of trainable parameters (in millions). For SLIM, we sum up parameters trained at both stages. A few models do not make their code open-source, we therefore include the metrics reported in their papers and skip parameter calculation (N/A). Lowest EERs are bolded per category.", "description": "This table compares the performance of SLIM against other state-of-the-art audio deepfake detection models across four different datasets: ASVspoof2019, ASVspoof2021, In-the-wild, and MLAAD-EN.  The metrics used for comparison are Equal Error Rate (EER) and F1-score. The table also indicates whether the model's frontend (feature extraction) was frozen or fine-tuned during training, and the number of trainable parameters for each model.  The results highlight SLIM's superior generalization capabilities, especially to unseen attacks.", "section": "4 Experiments"}, {"figure_path": "fymr0CBDHZ/tables/tables_17_1.jpg", "caption": "Table 3: Summary of datasets used for Stage 1 and Stage 2 training and evaluation.", "description": "This table details the datasets used in both stages of the SLIM model training and evaluation.  Stage 1 uses data for self-supervised contrastive learning (only real data) while stage 2 uses labeled data (real and fake) for supervised training.  The table lists the dataset name, the split (train, valid, or test), the number of samples, the number of real and fake samples, the number of attacks (types of deepfakes), the type of speech (scripted or spontaneous), and the recording environment (studio or in-the-wild).", "section": "4 Experiments"}, {"figure_path": "fymr0CBDHZ/tables/tables_19_1.jpg", "caption": "Table 2: Detection performance on different deepfake datasets. Experiments were repeated three times with different random seeds, and average metric values are reported. #Param refers to the number of trainable parameters (in millions). For SLIM, we sum up parameters trained at both stages. A few models do not make their code open-source, we therefore include the metrics reported in their papers and skip parameter calculation (N/A). Lowest EERs are bolded per category.", "description": "This table compares the performance of SLIM and several other state-of-the-art (SOTA) models on four different audio deepfake detection datasets: ASVspoof2019, ASVspoof2021, In-the-wild, and MLAAD-EN.  The table shows the Equal Error Rate (EER) and F1 score for each model on each dataset, and also indicates the number of trainable parameters (in millions) for each model.  The models are categorized based on whether they fine-tune the feature extraction frontend or keep it frozen during training.  The table highlights that SLIM outperforms other models with frozen frontends on out-of-domain datasets, while maintaining competitive performance on in-domain datasets.  The results demonstrate SLIM's superior generalizability to unseen attacks.", "section": "4.2 Experiment results"}, {"figure_path": "fymr0CBDHZ/tables/tables_20_1.jpg", "caption": "Table 2: Detection performance on different deepfake datasets. Experiments were repeated three times with different random seeds, and average metric values are reported. #Param refers to the number of trainable parameters (in millions). For SLIM, we sum up parameters trained at both stages. A few models do not make their code open-source, we therefore include the metrics reported in their papers and skip parameter calculation (N/A). Lowest EERs are bolded per category.", "description": "This table presents a comparison of the performance of various deepfake detection models on four different datasets: ASVspoof2019, ASVspoof2021, In-the-wild, and MLAAD-EN.  The metrics used for comparison are Equal Error Rate (EER) and F1-score.  The table also indicates whether the model's frontend (feature extraction) was frozen or finetuned during training, and the number of trainable parameters in millions for each model.  This allows for analysis of model performance across datasets, the impact of frontend finetuning, and model complexity.", "section": "4 Experiments"}]