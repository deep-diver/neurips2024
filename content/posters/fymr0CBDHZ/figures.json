[{"figure_path": "fymr0CBDHZ/figures/figures_3_1.jpg", "caption": "Figure 1: SLIM \u2013 A two-stage training framework for ADD. Stage 1 extracts style and linguistics representations from frozen SSL encoders, compresses them, and aims to minimize the distance between the compressed representations (Lcross), as well as the intra-subspace redundancy (Lstyle and Llinguistics). The Stage 1 features and the original subspace representations (pretrained SSL embeddings) are combined in Stage 2 to learn a classifier via supervised training.", "description": "This figure illustrates the SLIM (Style-Linguistics Mismatch) model's two-stage training process. Stage 1 uses self-supervised learning on real speech data to learn style-linguistic dependencies, compressing these features to minimize redundancy and distance between representations. Stage 2 leverages these compressed features, along with original features, for supervised classification of real and fake speech samples.", "section": "3 Method"}, {"figure_path": "fymr0CBDHZ/figures/figures_7_1.jpg", "caption": "Figure 2: Cosine distance (log scale) calculated between the style and linguistics dependency features for ASVspoof 2021 DF eval, In-the-wild, and MLAAD-EN. Whiskers from top to bottom represent the 75% quartile, median, and 25% quartile of the distribution.", "description": "This violin plot shows the distribution of cosine distances between style and linguistic dependency features for real and fake speech samples across three datasets: ASVspoof2021 DF eval, In-the-wild, and MLAAD-EN.  The y-axis represents the cosine distance (log scale), indicating the similarity between the two feature sets. A smaller distance suggests a stronger correlation, while a larger distance indicates a greater mismatch.  The plot visually compares the distributions for bonafide (real) and deepfake audio samples within each dataset, highlighting the differences in style-linguistic dependency between real and fake speech. Whiskers represent the 75th, median, and 25th percentiles of the distributions.", "section": "Style-linguistics mismatch of deepfakes"}, {"figure_path": "fymr0CBDHZ/figures/figures_7_2.jpg", "caption": "Figure 3: Projected embeddings using t-SNE for style-linguistic representations: (a) subspace embeddings - real class, (b) subspace embeddings - fake class, (c) dependency features - real class, (d) dependency features - fake class. Data distributions are visualized on the upper and right side of the embedding plots. Red: ASVspoof2021; Green: In-the-wild; Blue: MLAAD-EN.", "description": "This figure visualizes the style and linguistic features learned by SLIM using t-SNE for dimensionality reduction.  It shows how well the model separates real and fake speech samples from different datasets (ASVspoof2021, In-the-wild, MLAAD-EN).  The top row shows the embeddings from the original subspaces (style and linguistic), while the bottom row displays the dependency features learned in Stage 1 of the SLIM model, which aim to capture the style-linguistics mismatch in deepfakes. The visualization helps to understand the effectiveness of the learned features in discriminating between real and fake speech, particularly across different datasets.", "section": "Analysis of style-linguistics dependency features"}, {"figure_path": "fymr0CBDHZ/figures/figures_8_1.jpg", "caption": "Figure 4: Mel-spectrograms of select samples from In-the-wild. SLIM classifies all four correctly, and when reporting fakes, provides guidance on abnormalities in style and/or linguistics. Also, the dependency and subspace features in SLIM are complementary to each other. Left: samples missed by dependency features but correctly identified by the style and linguistic features; right: vice versa.", "description": "This figure shows four mel-spectrograms from the In-the-wild dataset, illustrating different characteristics of both real and fake speech samples.  The top two examples highlight common issues with fake audios: high-frequency artifacts and unnatural pauses. The bottom two showcase examples of real speech: one with atypical style (elongated words) and another with a noisy recording.  The caption highlights SLIM's ability to correctly identify all four samples, and indicates that the model uses features from different subspaces (style and linguistics) in a complementary way. The different subspaces capture diverse artifacts and anomalies, therefore improving the overall detection performance.", "section": "4.2 Experiment results"}, {"figure_path": "fymr0CBDHZ/figures/figures_16_1.jpg", "caption": "Figure 5: Spearman correlation coefficients calculated across all layers from two pretrained Wav2vec-XLSR backbones. Blue highlights layers 0-10 from Wav2vec-SER to represent style information. Red highlights layers 14-21 from Wav2vec-ASR to represent linguistics information. The correlation values between the selected layers can be read from the overlapping region.", "description": "This figure shows a heatmap representing the Spearman correlation coefficients between different layers of two pretrained Wav2vec-XLSR models: one fine-tuned for speech emotion recognition (Wav2vec-SER) and another for speech recognition (Wav2vec-ASR).  The x and y axes represent layers from Wav2vec-SER and Wav2vec-ASR respectively. The color intensity represents the correlation strength, with warmer colors indicating higher correlation.  The blue and red rectangles highlight the chosen layers (0-10 and 14-21) from Wav2vec-SER and Wav2vec-ASR respectively, indicating the style and linguistic features used in the SLIM model. The near-zero correlation between these selected layers suggests a good disentanglement between style and linguistic information.", "section": "A.2 Layer-wise analysis of pretrained SSL models"}, {"figure_path": "fymr0CBDHZ/figures/figures_17_1.jpg", "caption": "Figure 6: Projected WavLM embeddings for real and fake classes from the four employed datasets. Left: real class embeddings. Right: fake class embeddings.", "description": "This figure uses t-SNE to visualize the WavLM embeddings of real and fake audio samples from four datasets: ASVspoof2019, ASVspoof2021, In-the-wild, and MLAAD-EN.  The visualization shows how well the embeddings separate the real and fake audio samples from each dataset.  The left panel shows real samples, while the right panel shows fake samples. The different colors represent the different datasets.  The plot helps to illustrate the model's ability to distinguish between real and fake speech and how this ability varies across datasets.", "section": "A.3 Dataset details"}, {"figure_path": "fymr0CBDHZ/figures/figures_18_1.jpg", "caption": "Figure 1: SLIM \u2013 A two-stage training framework for ADD. Stage 1 extracts style and linguistics representations from frozen SSL encoders, compresses them, and aims to minimize the distance between the compressed representations (Lcross), as well as the intra-subspace redundancy (Lstyle and Llinguistics). The Stage 1 features and the original subspace representations (pretrained SSL embeddings) are combined in Stage 2 to learn a classifier via supervised training.", "description": "This figure illustrates the SLIM (Style-Linguistics Mismatch) model's two-stage training process. Stage 1 focuses on self-supervised learning using only real speech samples to extract style and linguistic features and their dependencies.  It involves compressing these features to minimize redundancy and distance between the compressed style and linguistic representations. In Stage 2, these compressed features are combined with the original features and used to train a supervised classifier for audio deepfake detection, using both real and fake speech samples. The frozen SSL encoders from Stage 1 highlight that the improvement in generalization doesn't come from finetuning, but the novel features learned in Stage 1.", "section": "3 Method"}]