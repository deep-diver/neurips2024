[{"figure_path": "aAR0ejrYw1/figures/figures_0_1.jpg", "caption": "Figure 1: Images that sound. We use diffusion models to generate visual spectrograms (second row) that look like natural images, which we call images that sound. These spectrograms can be converted into natural sounds (third row) using a pretrained vocoder, or colorized to obtain more visually pleasing results (first row). Please refer to our website to listen to the sounds.", "description": "This figure shows the results of generating images that sound like the audio prompt provided. The top row shows colorized versions of the generated spectrograms. The middle row shows the grayscale spectrograms, which are generated by diffusion models and look like natural images. The bottom row shows the waveforms obtained by converting the spectrograms into audio using a pretrained vocoder. The figure provides two examples of different images and their corresponding sounds.", "section": "Abstract"}, {"figure_path": "aAR0ejrYw1/figures/figures_1_1.jpg", "caption": "Figure 2: Images vs. spectrograms. We show grayscale images generated from Stable Diffusion [96] on the left, followed by log-mel spectrograms generated from Auffusion [118] in the middle, and our generated images that sound results on the right.", "description": "This figure compares three types of images:  grayscale images generated by Stable Diffusion (a text-to-image model), log-mel spectrograms generated by Auffusion (a text-to-spectrogram model), and the authors' generated \"images that sound\".  The comparison aims to highlight how the authors' method produces spectrograms that visually resemble natural images while also sounding like natural audio when played as spectrograms. Each row shows an example image and its corresponding spectrogram from the two different models and the proposed method. The goal is to illustrate that the authors' approach is successful in bridging the gap between the visual appearance of typical images and the acoustic properties of natural sounds encoded in spectrograms.", "section": "2 Related Work"}, {"figure_path": "aAR0ejrYw1/figures/figures_3_1.jpg", "caption": "Figure 3: Composing audio and visual diffusion models. We generate the visual spectrogram that can be visualized as an image or played as a sound. Given a noisy latent zt, we apply visual and audio diffusion models, each guided by a text prompt, to compute noise estimates e(t)v and e(t)a respectively. We obtain the multimodal noise estimate e(t) by a weighted average, then use it as part of the iterative denoising process. Finally, we decode the clean latent z0 to a spectrogram and convert it into a waveform using a pretrained vocoder (or by Griffin-Lim [47]).", "description": "This figure illustrates the process of composing audio and visual diffusion models to generate a visual spectrogram that can be both seen as an image and heard as a sound.  It shows how noisy latent data (zt) is denoised iteratively using information from both an image diffusion model (guided by a visual text prompt) and an audio diffusion model (guided by an audio text prompt). The noise estimates from each model are combined through weighted averaging to create a multimodal noise estimate. This estimate is then used to iteratively refine the latent representation until a clean latent (z0) is obtained. Finally, this clean latent is decoded into a spectrogram, which can be converted into a waveform using a vocoder, resulting in the final output: a visual spectrogram that sounds like natural audio and looks like a natural image.", "section": "3 Method"}, {"figure_path": "aAR0ejrYw1/figures/figures_6_1.jpg", "caption": "Figure 3: Composing audio and visual diffusion models. We generate the visual spectrogram that can be visualized as an image or played as a sound. Given a noisy latent zt, we apply visual and audio diffusion models, each guided by a text prompt, to compute noise estimates et and eat) respectively. We obtain the multimodal noise estimate (t) by a weighted average, then use it as part of the iterative denoising process. Finally, we decode the clean latent zo to a spectrogram and convert it into a waveform using a pretrained vocoder (or by Griffin-Lim [47]).", "description": "This figure illustrates the process of generating a visual spectrogram using a combined approach of image and audio diffusion models.  A noisy latent representation (zt) is denoised iteratively using information from both models, guided by separate text prompts for the desired visual and audio aspects.  The weighted average of the noise estimates from each model is used to create a multimodal noise estimate, which then facilitates the denoising process in a shared latent space. The final clean latent representation (z0) is decoded into a spectrogram, which can then be converted into an audio waveform using a pre-trained vocoder (or alternative methods). The diagram clearly shows the model's workflow and how the visual and audio information are combined during the generation process.", "section": "3 Method"}, {"figure_path": "aAR0ejrYw1/figures/figures_7_1.jpg", "caption": "Figure 1: Images that sound. We use diffusion models to generate visual spectrograms (second row) that look like natural images, which we call images that sound. These spectrograms can be converted into natural sounds (third row) using a pretrained vocoder, or colorized to obtain more visually pleasing results (first row). Please refer to our website to listen to the sounds.", "description": "This figure showcases the core concept of the paper: generating images that also function as spectrograms (visual representations of sound). The top row displays colorized versions of the generated spectrograms, enhancing their visual appeal. The middle row presents the grayscale spectrograms themselves, which resemble natural images. The bottom row shows the waveforms (actual sound representation) derived from these spectrograms using a pretrained vocoder.  The examples illustrate the successful synthesis of images that both look visually appealing and sound natural when played as audio.", "section": "1 Introduction"}, {"figure_path": "aAR0ejrYw1/figures/figures_8_1.jpg", "caption": "Figure 3: Composing audio and visual diffusion models. We generate the visual spectrogram that can be visualized as an image or played as a sound. Given a noisy latent zt, we apply visual and audio diffusion models, each guided by a text prompt, to compute noise estimates et and eat) respectively. We obtain the multimodal noise estimate (t) by a weighted average, then use it as part of the iterative denoising process. Finally, we decode the clean latent zo to a spectrogram and convert it into a waveform using a pretrained vocoder (or by Griffin-Lim [47]).", "description": "This figure illustrates the process of generating a visual spectrogram using two pre-trained diffusion models: one for images and one for audio.  Both models operate in a shared latent space.  The process starts with a noisy latent representation (zt).  Separate noise estimates are generated using both the visual and audio models, guided by their respective text prompts. These noise estimates are combined using a weighted average to create a multimodal noise estimate. This combined estimate is then used in an iterative denoising process to refine the latent representation, ultimately resulting in a clean latent (z0). This latent is then decoded into a spectrogram, which can be converted to a waveform using a vocoder or colorized for better visual appeal.", "section": "3 Method"}, {"figure_path": "aAR0ejrYw1/figures/figures_17_1.jpg", "caption": "Figure 3: Composing audio and visual diffusion models. We generate the visual spectrogram that can be visualized as an image or played as a sound. Given a noisy latent zt, we apply visual and audio diffusion models, each guided by a text prompt, to compute noise estimates et and eat) respectively. We obtain the multimodal noise estimate (t) by a weighted average, then use it as part of the iterative denoising process. Finally, we decode the clean latent zo to a spectrogram and convert it into a waveform using a pretrained vocoder (or by Griffin-Lim [47]).", "description": "This figure illustrates the method used to generate images that sound.  It shows how the visual and audio diffusion models are combined to generate a visual spectrogram by denoising a noisy latent. The process uses text prompts to guide both diffusion models. The resulting latent is then decoded into a spectrogram, which is converted to a waveform using a vocoder. The figure clearly depicts the multimodal nature of the approach, combining visual and audio information.", "section": "3 Method"}, {"figure_path": "aAR0ejrYw1/figures/figures_18_1.jpg", "caption": "Figure 2: Images vs. spectrograms. We show grayscale images generated from Stable Diffusion [96] on the left, followed by log-mel spectrograms generated from Auffusion [118] in the middle, and our generated images that sound results on the right.", "description": "This figure compares three types of image generation results side-by-side.  The left column shows grayscale images generated by Stable Diffusion, a standard text-to-image model. The middle column displays log-mel spectrograms (visual representations of sound) produced by Auffusion, a text-to-spectrogram model. The right column shows the authors' novel approach, combining image and spectrogram generation to create images that also function as natural-sounding spectrograms.  Each row represents a different image/sound pair, demonstrating how the model generates visually appealing spectrograms that also correspond to natural audio.", "section": "2 Related Work"}, {"figure_path": "aAR0ejrYw1/figures/figures_19_1.jpg", "caption": "Figure 4: Qualitative comparison. We show our qualitative results along with the imprint and SDS baselines given visual (first) and audio (second) prompts. Please zoom in for better viewing.", "description": "This figure presents a qualitative comparison of the proposed method for generating images that sound with two baselines: imprint and SDS.  For each of three pairs of image and sound prompts, four generated spectrograms are shown. The top row shows the results from the proposed method. The middle row shows the results from the imprint baseline. The bottom row shows the results from the SDS baseline.  Each spectrogram is shown alongside its corresponding waveform. Visual inspection reveals the differences in quality and alignment between the generated images and their corresponding audio.  The results demonstrate that the proposed method produces higher quality samples that better match both image and audio prompts compared to the baselines.", "section": "4 Experiments"}, {"figure_path": "aAR0ejrYw1/figures/figures_20_1.jpg", "caption": "Figure 9: Random results. We show random results from our approach, using random audio and visual text prompt pairs. We provide failure cases in the last two rows. Please zoom in for better viewing.", "description": "This figure shows examples generated by the proposed method using various combinations of random image and audio prompts.  The results illustrate the model's ability to generate spectrograms that attempt to fulfill both the visual and audio descriptions, but also highlights scenarios where the model struggles to balance both successfully (failure cases). The figure's purpose is to demonstrate both successes and failures of the model.", "section": "4.3 Human Studies"}, {"figure_path": "aAR0ejrYw1/figures/figures_21_1.jpg", "caption": "Figure 3: Composing audio and visual diffusion models. We generate the visual spectrogram that can be visualized as an image or played as a sound. Given a noisy latent zt, we apply visual and audio diffusion models, each guided by a text prompt, to compute noise estimates et and eat) respectively. We obtain the multimodal noise estimate (t) by a weighted average, then use it as part of the iterative denoising process. Finally, we decode the clean latent zo to a spectrogram and convert it into a waveform using a pretrained vocoder (or by Griffin-Lim [47]).", "description": "This figure illustrates the process of generating images that sound using a combination of image and audio diffusion models.  A noisy latent representation is iteratively denoised using estimates from both models, guided by separate text prompts for the visual and audio aspects. The final denoised latent is decoded into a spectrogram, which can then be converted to an audio waveform using a vocoder.", "section": "3 Method"}]