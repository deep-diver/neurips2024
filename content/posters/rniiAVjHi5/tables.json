[{"figure_path": "rniiAVjHi5/tables/tables_2_1.jpg", "caption": "Table 1: Summary of main results for solving problem (1) with our methods. \"Convergence rate\" is expressed in terms of the expected function residual at iteration k (or t, depending on the method). \"SO complexity\" denotes the cumulative stochastic-oracle complexity of the method since its start and up to iteration k (or t), which is defined as the number of queries to the stochastic oracle \u011d; for SVRG methods, we assume that querying the (inexact) full-gradient oracle \u011f is n times more expensive than \u011d, and define the SO complexity as N\u1ef9 + nN\u1ef9, where Ng and N\u0101 are the number of queries to \u011d and \u011d, respectively. The second and third columns should be understood in terms of the BigO-notation which we omit for brevity.", "description": "This table summarizes the main results of the paper, presenting the convergence rate and stochastic oracle complexity for four different algorithms: UniSgd, UniFastSgd, UniSvrg, and UniFastSvrg.  These algorithms are variations of stochastic gradient descent methods that use AdaGrad stepsizes. The table shows how the performance of these algorithms varies depending on different assumptions about the problem and the stochastic oracle. The assumptions relate to the smoothness of the objective function and the variance of the stochastic gradient. The convergence rates are expressed in terms of the expected function residual, while the stochastic oracle complexity measures the total number of queries to the stochastic gradient oracle.", "section": "3 Main Algorithms and Stepsize Update Rules"}, {"figure_path": "rniiAVjHi5/tables/tables_6_1.jpg", "caption": "Table 1: Summary of main results for solving problem (1) with our methods. \"Convergence rate\" is expressed in terms of the expected function residual at iteration k (or t, depending on the method). \"SO complexity\" denotes the cumulative stochastic-oracle complexity of the method since its start and up to iteration k (or t), which is defined as the number of queries to the stochastic oracle \u011d; for SVRG methods, we assume that querying the (inexact) full-gradient oracle \u011f is n times more expensive than \u011d, and define the SO complexity as N\u1ef9 + nN\u1ef9, where Ng and N\u0101 are the number of queries to \u011d and \u011d, respectively. The second and third columns should be understood in terms of the BigO-notation which we omit for brevity.", "description": "This table summarizes the convergence rates and stochastic oracle complexities for four algorithms: UniSgd, UniFastSgd, UniSvrg, and UniFastSvrg.  The convergence rate is expressed in terms of the expected function residual, and the stochastic oracle complexity represents the number of queries to the stochastic gradient oracle.  Assumptions made for each algorithm are also listed. The algorithms are variations on stochastic gradient descent, employing AdaGrad stepsizes and incorporating variance reduction techniques where appropriate.", "section": "Main Algorithms and Stepsize Update Rules"}, {"figure_path": "rniiAVjHi5/tables/tables_7_1.jpg", "caption": "Table 1: Summary of main results for solving problem (1) with our methods. \"Convergence rate\" is expressed in terms of the expected function residual at iteration k (or t, depending on the method). \"SO complexity\" denotes the cumulative stochastic-oracle complexity of the method since its start and up to iteration k (or t), which is defined as the number of queries to the stochastic oracle \u011d; for SVRG methods, we assume that querying the (inexact) full-gradient oracle \u011f is n times more expensive than \u011d, and define the SO complexity as N\u1ef9 + nN\u1ef9, where Ng and N\u0101 are the number of queries to \u011d and \u011d, respectively. The second and third columns should be understood in terms of the BigO-notation which we omit for brevity.", "description": "This table summarizes the convergence rates and stochastic oracle complexities of four algorithms: UniSgd, UniFastSgd, UniSvrg, and UniFastSvrg.  These algorithms are all variants of stochastic gradient descent methods, each with different properties (basic vs accelerated, and with or without variance reduction). The table shows how their convergence rates and computational costs scale with the number of iterations (k or t), problem parameters, and variance of stochastic gradients. The \"SO complexity\" column indicates the number of stochastic gradient evaluations required by each algorithm.", "section": "3 Main Algorithms and Stepsize Update Rules"}, {"figure_path": "rniiAVjHi5/tables/tables_8_1.jpg", "caption": "Table 1: Summary of main results for solving problem (1) with our methods. \"Convergence rate\" is expressed in terms of the expected function residual at iteration k (or t, depending on the method). \"SO complexity\" denotes the cumulative stochastic-oracle complexity of the method since its start and up to iteration k (or t), which is defined as the number of queries to the stochastic oracle \u011d; for SVRG methods, we assume that querying the (inexact) full-gradient oracle \u011f is n times more expensive than \u011d, and define the SO complexity as N\u1ef9 + nN\u1ef9, where Ng and N\u0101 are the number of queries to \u011d and \u011d, respectively. The second and third columns should be understood in terms of the BigO-notation which we omit for brevity.", "description": "This table summarizes the convergence rates and stochastic oracle complexities of four algorithms proposed in the paper for solving a composite optimization problem. The algorithms are UniSgd, UniFastSgd (accelerated version of UniSgd), UniSvrg (incorporating SVRG variance reduction), and UniFastSvrg (accelerated version of UniSvrg).  The table shows the convergence rate (in terms of expected function residual), the stochastic oracle complexity (number of queries to the stochastic oracle), and the assumptions under which each algorithm achieves its reported complexity.", "section": "Main Algorithms and Stepsize Update Rules"}]