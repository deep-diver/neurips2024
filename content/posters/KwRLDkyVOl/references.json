{"references": [{"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023", "reason": "This paper introduces Direct Preference Optimization (DPO), a core method that the current paper builds upon and extends."}, {"fullname_first_author": "Michael U Gutmann", "paper_title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "publication_date": "2012", "reason": "This paper introduces Noise Contrastive Estimation (NCE), a foundational technique that is central to the proposed NCA and InfoNCA methods."}, {"fullname_first_author": "Aaron van den Oord", "paper_title": "Representation learning with contrastive predictive coding", "publication_date": "2018", "reason": "This paper introduces InfoNCE, a variant of NCE that is closely related to the InfoNCA method proposed in the current paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021", "reason": "This paper introduces CLIP, a model used for evaluating the performance of language models, which is relevant to the experimental setup of the current paper."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022", "reason": "This paper discusses aligning language models with human feedback, a topic directly relevant to the current paper's focus on aligning language models with explicit rewards."}]}