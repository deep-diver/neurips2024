[{"figure_path": "KwRLDkyVOl/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of NCA and InfoNCA algorithm for aligning language models. Both reward loss and pairwise preference loss are given. We provide pseudocode in Appendix B.", "description": "This table compares the InfoNCA and NCA algorithms, focusing on their model definitions, target, loss functions for both reward and preference datasets, and the resulting optimal reward model.  It highlights the differences in how they approach the likelihood ratios (relative vs. absolute) and the theoretical guarantees each offers.  The table also provides a reference to the appendix for further details on the algorithms' pseudocode.", "section": "4 NCA: Fixing Decreased Response Likelihood Issue for InfoNCA"}, {"figure_path": "KwRLDkyVOl/tables/tables_6_1.jpg", "caption": "Table 2: Comparison between reward-based methods (InfoNCA, NCA) and preference-based methods (DPO, IPO, etc.) in LLM alignment. We focus on the general instruction-following abilities of each method measured by GPT-4 evaluations and human preference. The highest number in each benchmark is bolded and the second highest is underlined.", "description": "This table compares the performance of reward-based language model alignment methods (InfoNCA and NCA) against several preference-based methods (DPO, IPO, etc.). The comparison is based on two metrics: MT-bench and AlpacaEval, which assess general instruction-following ability.  The \"Win vs. DPO\" column shows the percentage of wins for each method against DPO.  The highest score for each metric is bolded, and the second-highest is underlined.  The results show that the reward-based methods generally outperform the preference-based ones.", "section": "5.1 Aligning Language Models with Explicit Rewards"}, {"figure_path": "KwRLDkyVOl/tables/tables_7_1.jpg", "caption": "Table 3: Alignment results for UltraInteract. We mark numbers that have decreased (\u2193) after training.", "description": "This table presents the performance comparison of different language model alignment methods (DPO and NCA) on the UltraInteract benchmark. It shows the scores achieved by the Mistral-7B-SFT and Mistral-8x7B-SFT models before and after applying DPO and NCA algorithms. The benchmark includes various reasoning tasks (BBH (CoT)), coding tasks (LeetCode, HumanEval), and mathematical tasks (GSMPLUS, MATH, TheoremQA, SVAMP, ASDiv). The downward arrow (\u2193) indicates that the score has decreased after applying a specific algorithm.  The table highlights the impact of NCA on reasoning performance.", "section": "5.2 NCA vs. DPO in Aligning Language Models with Pairwise Preference"}, {"figure_path": "KwRLDkyVOl/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of NCA and InfoNCA algorithm for aligning language models. Both reward loss and pairwise preference loss are given. We provide pseudocode in Appendix B.", "description": "This table compares the InfoNCA and NCA algorithms for aligning language models.  It details the model definition, reward and preference datasets used, the loss functions (for both K>1 and the special case of K=2 which reduces to DPO), the loss type, the target of optimization, and the optimal solution for r\u03b8*. The pseudocode for both algorithms is available in Appendix B.", "section": "4 NCA: Fixing Decreased Response Likelihood Issue for InfoNCA"}, {"figure_path": "KwRLDkyVOl/tables/tables_19_1.jpg", "caption": "Table 2: Comparison between reward-based methods (InfoNCA, NCA) and preference-based methods (DPO, IPO, etc.) in LLM alignment. We focus on the general instruction-following abilities of each method measured by GPT-4 evaluations and human preference. The highest number in each benchmark is bolded and the second highest is underlined.", "description": "This table compares the performance of reward-based language model alignment methods (InfoNCA, NCA) against preference-based methods (DPO, IPO, etc.) on general instruction-following tasks.  The evaluation metrics used are GPT-4 scores and human preference ratings. The highest score in each benchmark is highlighted in bold, with the second-highest underlined.  This allows for a clear comparison of the relative effectiveness of each method in aligning language models with human intent.", "section": "5.1 Aligning Language Models with Explicit Rewards"}, {"figure_path": "KwRLDkyVOl/tables/tables_19_2.jpg", "caption": "Table 2: Comparison between reward-based methods (InfoNCA, NCA) and preference-based methods (DPO, IPO, etc.) in LLM alignment. We focus on the general instruction-following abilities of each method measured by GPT-4 evaluations and human preference. The highest number in each benchmark is bolded and the second highest is underlined.", "description": "This table compares different language model alignment methods using the MT-bench and AlpacaEval benchmarks.  It shows the performance of reward-based methods (InfoNCA and NCA) against preference-based methods (DPO, IPO, etc.) when aligning language models with explicit rewards and preference data.  The best and second-best performing methods are highlighted for each benchmark.", "section": "5.1 Aligning Language Models with Explicit Rewards"}, {"figure_path": "KwRLDkyVOl/tables/tables_21_1.jpg", "caption": "Table 1: Comparison of NCA and InfoNCA algorithm for aligning language models. Both reward loss and pairwise preference loss are given. We provide pseudocode in Appendix B.", "description": "This table compares the InfoNCA and NCA algorithms for aligning language models. It shows how both algorithms handle reward datasets (x \u2192 {Yi, ri}1:K) and preference datasets (x \u2192 {Yw > y\u0131}).  The table details the model definitions, loss functions (for both reward and preference data), loss types, optimization targets, and optimal solutions for both algorithms.  It highlights the key differences between the two approaches and how they relate to the DPO algorithm. Pseudocode for both algorithms is provided in Appendix B.", "section": "4 NCA: Fixing Decreased Response Likelihood Issue for InfoNCA"}, {"figure_path": "KwRLDkyVOl/tables/tables_21_2.jpg", "caption": "Table 1: Comparison of NCA and InfoNCA algorithm for aligning language models. Both reward loss and pairwise preference loss are given. We provide pseudocode in Appendix B.", "description": "This table compares two algorithms, InfoNCA and NCA, used for aligning language models.  It shows how they differ in their model definitions, the type of datasets they handle (reward and preference), the loss functions they employ, and their optimization targets. It highlights that InfoNCA subsumes DPO as a special case and notes that pseudocode for both algorithms is available in Appendix B.", "section": "4 NCA: Fixing Decreased Response Likelihood Issue for InfoNCA"}]