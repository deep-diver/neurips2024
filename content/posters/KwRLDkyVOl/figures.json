[{"figure_path": "KwRLDkyVOl/figures/figures_1_1.jpg", "caption": "Figure 1: InfoNCA/NCA allows direct LM optimization for both reward and preference data.", "description": "This figure illustrates the InfoNCA/NCA framework's ability to directly optimize language models (LMs) using both reward and preference data.  The top section shows the traditional approach of using preference data (pairwise comparisons) to train a reward model, which is then used with PPO or DPO to extract the LM policy. The middle section shows how reward datasets, where each response is explicitly annotated with a scalar reward, can be used with InfoNCA/NCA to achieve the same goal. The bottom section highlights that the DPO loss used in preference-based methods is a special case of the proposed InfoNCA objective. This allows InfoNCA/NCA to seamlessly handle both types of data.", "section": "3 InfoNCA: Extending DPO from Preference to Explicit Rewards"}, {"figure_path": "KwRLDkyVOl/figures/figures_1_2.jpg", "caption": "Figure 2: Pairwise NCA prevents chosen likelihood from decreasing while DPO cannot.", "description": "This figure compares the behavior of Direct Preference Optimization (DPO) and Noise Contrastive Alignment (NCA) in a pairwise setting.  The left panel shows the relative data likelihood of winning and losing responses over training epochs. DPO exhibits a decreasing trend in the likelihood of the chosen response, while NCA maintains a relatively stable likelihood. The right panel highlights the difference in likelihood margins between the two methods.  This demonstrates that NCA effectively prevents the likelihood of chosen responses from decreasing, unlike DPO.", "section": "4 NCA: Fixing Decreased Response Likelihood Issue for InfoNCA"}, {"figure_path": "KwRLDkyVOl/figures/figures_3_1.jpg", "caption": "Figure 3: DPO, InfoNCA, and NCA all optimize LLM through classification tasks. DPO compares two responses and tells which one is preferred. InfoNCA compares multiple responses and identifies the one sampled from \u03c0* (Sec. 3.1). NCA predicts the model source of a single response (Sec. 4.1).", "description": "This figure illustrates the core difference between DPO, InfoNCA, and NCA in terms of their approach to optimizing language models. DPO uses a pairwise comparison, InfoNCA uses a multi-class classification approach to identify the optimal response from multiple candidates given their rewards, and NCA uses a binary classification to predict the model source of a single response given its reward.", "section": "3 InfoNCA: Extending DPO from Preference to Explicit Rewards"}, {"figure_path": "KwRLDkyVOl/figures/figures_6_1.jpg", "caption": "Figure 4: More suboptimal responses can also increase LLMs' instruction-following ability. We fix the highest-reward response in the UltraFeedback dataset and ablate the number of suboptimal responses per prompt, resulting in different contrastive response numbers K during training. Left: Evaluation results under the same set of hyperparameters. Right: Performance-KL trade-off under various \u03b1 and \u03b2. Each dot represents an independent experiment trained for 1 epoch.", "description": "This figure shows the impact of the number of suboptimal responses (K) on the performance of InfoNCA and NCA models on two different benchmarks: MT-bench and Alpaca. The left panel shows that increasing K leads to improved performance in both benchmarks. The right panel visualizes the trade-off between performance (MT-bench score) and KL divergence (KL(\u03c0\u03b8||\u03bc)) for various values of \u03b1 and \u03b2, further confirming the positive impact of including more suboptimal responses during training.", "section": "5.1 Aligning Language Models with Explicit Rewards"}, {"figure_path": "KwRLDkyVOl/figures/figures_7_1.jpg", "caption": "Figure 5: Comparision of data likelihood between InfoNCA/DPO and NCA.", "description": "This figure compares the changes in data likelihood during training for InfoNCA/DPO and NCA.  The plots show the model reward for chosen responses (those preferred by human evaluators or given high rewards) and rejected responses over the training epochs (or steps).  The key observation is that InfoNCA/DPO often shows a decrease in the likelihood of the chosen responses, whereas NCA maintains or increases this likelihood.  This demonstrates NCA's ability to prevent the likelihood of correct responses from decreasing during training, a problem observed with InfoNCA/DPO.", "section": "4 NCA: Fixing Decreased Response Likelihood Issue for InfoNCA"}, {"figure_path": "KwRLDkyVOl/figures/figures_8_1.jpg", "caption": "Figure 6: NCA is more robust to hyperparameter changes and KL derivations. Left: Ablation results of \u03b1 and \u03b2 for UltraFeedback-binarized. Right: Results for UltraFeedback-reward (K = 4).", "description": "This figure shows the robustness of NCA compared to DPO and InfoNCA methods regarding hyperparameter sensitivity.  The left panel displays the results of ablating \u03b1 and \u03b2 for a binarized version of the UltraFeedback dataset. The right panel shows the results for the full UltraFeedback reward dataset with K=4. The plots demonstrate that NCA's performance is less affected by changes in \u03b1 and \u03b2 compared to DPO and InfoNCA.  The x-axis represents the KL divergence between the learned policy (\u03c0\u03b8) and the pretrained language model (\u03bc), and the y-axis shows the MT-bench score.", "section": "4 NCA: Fixing Decreased Response Likelihood Issue for InfoNCA"}, {"figure_path": "KwRLDkyVOl/figures/figures_17_1.jpg", "caption": "Figure 1: InfoNCA/NCA allows direct LM optimization for both reward and preference data.", "description": "The figure illustrates how InfoNCA and NCA, two novel algorithms introduced in the paper, enable direct language model (LM) optimization using both reward and preference data.  It contrasts these methods with existing approaches like DPO and PPO, which primarily handle preference data.  The diagram highlights that InfoNCA/NCA directly optimize the LM policy from the available rewards or pairwise preferences, unlike other methods which involve indirect approaches or limitations in handling reward data.", "section": "3 InfoNCA: Extending DPO from Preference to Explicit Rewards"}, {"figure_path": "KwRLDkyVOl/figures/figures_18_1.jpg", "caption": "Figure 3: DPO, InfoNCA, and NCA all optimize LLM through classification tasks. DPO compares two responses and tells which one is preferred. InfoNCA compares multiple responses and identifies the one sampled from \u03c0* (Sec. 3.1). NCA predicts the model source of a single response (Sec. 4.1).", "description": "This figure illustrates the differences in how three different language model alignment methods (DPO, InfoNCA, and NCA) approach the problem.  DPO uses a binary classification approach, comparing two responses to determine which is preferred. InfoNCA, on the other hand, handles multiple responses by attempting to identify the one response that comes from the optimal distribution. Finally, NCA uses a binary classification approach to determine whether a single response comes from the optimal or pretrained language model.", "section": "3 InfoNCA: Extending DPO from Preference to Explicit Rewards"}]