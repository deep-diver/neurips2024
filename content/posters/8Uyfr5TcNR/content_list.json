[{"type": "text", "text": "Robust Reinforcement Learning with General Utility ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziyi Chen, Yan Wen, Zhengmian Hu, Heng Huang Department of Computer Science, Institute of Health Computing, University of Maryland College Park College Park, MA 20742, USA {zc286,ywen1,zhu123,heng}@umd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning (RL) problem with general utility is a powerful decision making framework that covers standard RL with cumulative cost, exploration problems, and demonstration learning. Existing works on RL with general utility do not consider the robustness under environmental perturbation, which is important to adapt RL system in the real-world environment that differs from the training environment. To train a robust policy, we propose a robust RL framework with general utility, which subsumes many existing RL frameworks including RL, robust RL, RL with general utility, constrained RL, robust constrained RL, pure exploration, robust entropy regularized RL, etc. Then we focus on popular convex utility functions, with which our proposed learning framework is a challenging nonconvex-nonconcave minimax optimization problem, and design a two-phase stochastic policy gradient type algorithm and obtain its sample complexity result for gradient convergence. Furthermore, for convex utility on a widely used polyhedral ambiguity set, we design an algorithm and obtain its convergence rate to a global optimalsolution. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) is an important decision-making framework [41] aiming to find the optimal policy that minimizes accumulative cost, which is also a linear utility function of occupancy measure. Recent works have extended standard RL to more general utility functions to account for a variety of practical needs, including risk-sensitive applications [22, 8, 52], exploration maximization [18, 54, 51, 13, 6], and safety constraints [54, 51, 13]. There are provably convergent algorithms to solve RL with general utility [54, 55, 6]. However, these works study RL with general utility in a fixed environment, which may fail in many applications where the policy is trained in a simulation environment but implemented in a different real-world environment [37, 56]. ", "page_idx": 0}, {"type": "text", "text": "To make the policy robust to such environmental change, robust RL has been proposed to find the optimal robust policy under the worst possible environment [36, 20, 48, 45, 14]. However, all the existing robust RL works restrict to linear utility function to our knowledge. Therefore, we aim to answer the followingresearch question: ", "page_idx": 0}, {"type": "text", "text": "Q: Can we train a robust policy for RL with general utility and obtain convergence results? ", "page_idx": 0}, {"type": "text", "text": "1.1  Our Contributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We affirmatively answer this question by proposing robust RL with general utility, the first learning framework that obtains a robust policy for general utility in the worst possible environment. It is formulated as a minimax optimization problem $\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ where $f$ is the utility function and $\\lambda_{\\theta,\\xi}$ is the occupancy measure under the policy parameter $\\theta\\in\\Theta$ and the environmental transition kernel parameter $\\xi\\in\\Xi$ . Our robust RL with general utility is a combination of its two important special cases, namely, RL with general utility [54] (formulated as $\\operatorname*{min}_{\\theta\\in\\Theta}f(\\lambda_{\\theta,\\xi})$ where the environmental parameter $\\xi$ is fixed) and robust RL [36] where $f$ is restricted to linear utility function. This new learning framework also covers many other existing RL frameworks including constrained RL [2] and robust constrained RL [43] with safety critical applications such as healthcare and unmanned drones, entropy regularized RL [10] and its robust extension [32] which help agents learn from human demonstration, pure exploration [18] with application to explore an environment with sparse reward signals and its robust extension, etc. These examples use convex utility functions $f$ , which is the focus of this paper. See Section 2.1 for details of these examples. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Then, we focus on designing provably convergent algorithms for our new proposed learning framework with the widely used convex utility function $f$ . In this case, our objective $\\operatorname*{min}_{\\theta}\\operatorname*{max}_{\\xi}f(\\lambda_{\\theta,\\xi})$ is still a highly challenging nonconvex-nonconcave minimax optimization problem. Hence, we have to utilize the structure and properties of $\\lambda_{\\theta,\\xi}$ to design algorithms and obtain convergence results. To elaborate, we design a projected stochastic gradient descent ascent algorithm with two phases. Interestingly, the first phase targeted at the objective function $f$ obtains a stationary point of a different envelope function. Hence, we add a second phase targeted at a corrected objective $\\widetilde{f}$ to converge to a near-stationary solution of the original objective $f$ . The convergence analysis is non-trivial with two novel techniques. First, we have proved a projected gradient dominance property (Proposition 4) that is much stronger than the existing one on convex utility, with less assumptions, no bias term and applicability to more general parameterized policy. Second, in the convergence analysis of the second phase, we obtain convergence to a global Nash equilibrium (thus a stationary point) of $\\widetilde{f}$ by Proposition 4, which is close to a stationary point of $f$ by proving that $\\nabla_{\\xi}\\tilde{f}(\\lambda_{\\theta,\\xi})\\!\\approx\\!\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})$ ", "page_idx": 1}, {"type": "text", "text": "Furthermore, with convex utility function $f$ and the widely used $s$ -rectangular polyhedral ambiguity set $\\Xi$ (including the popular $L^{\\dagger}$ and $L^{\\infty}$ ambiguity sets), we design an alternative algorithm which converges to a global optimal solution of this nonconvex-nonconcave optimization problem at a sublinear rate ${\\mathcal O}(1/\\bar{K})$ (Theorem 3). This is much more challenging than global convergence for convex RL (that is, RL with convex utility function and fixed $\\xi$ ) [54, 51, 6] and for robust RL with linear utility satisfying Bellman equation [36, 20, 45, 15, 25], so we need novel algorithm design and novel techniques. First, we prove that arg $\\mathrm{max}_{\\xi}f(\\lambda_{\\theta,\\xi})$ can be obtained in the finite set of vertices $V(\\Xi)$ (Proposition 6). This is intuitive if $f(\\bar{\\lambda}_{\\theta,\\cdot})$ is a convex function but in many applications, only $\\overset{\\cdot}{f}(\\lambda)$ is convex. To solve this challenge, we prove a novel local invertibility property of $\\lambda_{\\theta,}$ . (Proposition 5) by checking Bellman equation of $\\lambda_{\\theta,\\xi}$ state by state in two cases. Then we prove Proposition 6 using a novel state-by-state extension from an optimal non-vertex $\\xi$ to an optimal vertex $\\xi$ . Second, the major difficulty to design our algorithm is to find a descent direction of $\\Gamma(\\theta):=\\operatorname*{max}_{\\xi}f(\\lambda_{\\theta,\\xi})$ . We select the near-optimal vertices $\\xi\\in\\Xi_{k}\\subset V(\\Xi)$ that may affect the optimization progress $\\hat{\\Gamma}(\\theta_{k+1})-\\Gamma(\\theta_{k})$ , and find the descent direction with provably large descent for all the corresponding functions $\\{f(\\lambda_{\\theta_{k},\\xi})\\}_{\\xi\\in\\Xi_{k}}$ (Proposition 7) via convex optimization. Third, by Proposition 6, the global convergence measure $\\tilde{\\Delta_{k}}:=\\bar{\\Gamma}(\\theta_{k})-\\mathrm{min}_{\\theta}\\,\\Gamma(\\theta)$ at each iteration $k$ either is ${\\mathcal{O}}(1/k)$ -close to optimal $(\\Gamma(\\theta_{k})\\leq{\\mathcal{O}}(1/k))$ or enjoys large descent (Eq. (26)), so we prove the convergence in 3 cases: ${\\mathcal{O}}(1/K)$ -optimal final $\\theta_{K}$ , iterate Eq. (26) from $\\theta_{0}$ or from a $\\bar{\\mathcal{O}}(1/k)$ -optimal intermediate $\\theta_{k}$ ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "RL with General Utility. Standard RL aims to optimize over the accumulated reward/cost [21, 41]. Some early operation research works focus on other non-linear objectives such as variance-penalized MDPs [12], risk-sensitive objectives [22, 8, 52], entropy exploration [18], constrained RL [2, 1, 35] and learning from demonstration [39, 3]. ", "page_idx": 1}, {"type": "text", "text": "[54] proposes RL with general utilities to cover the above applications and applies variational policy gradient method that provably converges to the global optimal solution for convex utility. [55] proposes a variance reduced policy gradient algorithm which requires $\\widetilde{\\mathcal{O}}(\\epsilon^{-3})$ samples to achieve an $\\epsilon_{}$ -stationary policy for general utility and $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$ samples to achieve an $\\epsilon$ global optimal policy for convex utility and overparameterized policy. [51] provides a meta-algorithm to solve the convex MDP problem as a min-max game between a policy player and a cost player who produces rewards that the policy player must maximize. They further show that any method-solving problems under the standard RL settings can be used to solve the more general convex MDP problem. [27] obtains policy gradient theorem for RL with general utilities. [6] proposes a simpler single-loop parameter-free normalized policy gradient algorithm with recursive momentum variance reduction. This algorithm requires $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ samples to achieve $\\epsilon$ -stationary policy in general and $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$ samples to achieve $\\epsilon$ -global optimal policy for convex utility. For large finite state action spaces, it requires $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$ samples to achieve $\\epsilon$ -stationary policy via linear function approximation of the occupancy measure. [53] proposes decentralized multi-agent RL with general utilities. [13] shows that convex RL is a subclass of multi-agent mean-field games. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Robust RL. Robust RL is designed to learn a policy that is robust to perturbation of environmental factors. Usually robust RL is NP-hard [45], but becomes tractable for ambiguity sets that is $(s,a)$ rectangular [36, 20, 45, 44, 29, 56] or $s$ -rectangular [45, 42, 23, 26]. Methods to solve robust RL include value iteration [36, 20, 45, 15, 25], policy iteration [20, 4, 24] and policy gradient [29, 44, 56, 42, 26, 17, 28]. ", "page_idx": 2}, {"type": "text", "text": "2  Robust Reinforcement Learning with General Utility ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. The space of probability distribution on a space $\\mathcal{X}$ is denoted as $\\Delta^{\\mathcal{X}}$ . If $\\mathcal{X}$ is finite, we denote its cardinality as $|{\\mathcal{X}}|$ $\\|\\cdot\\|_{p}$ denotes $p$ norm of vectors and $\\|\\cdot\\|=\\|\\cdot\\|_{2}$ by default. ", "page_idx": 2}, {"type": "text", "text": "Reinforcement Learning with General Utility. Reinforcement Learning (RL) with general utility is an emerging learning framework [54, 55, 6], specified by a tuple $\\langle S,{\\mathcal{A}},p_{\\xi},f,\\rho,\\gamma\\rangle$ , with finite state space $\\boldsymbol{S}$ , finite action space $\\boldsymbol{\\mathcal{A}}$ , transition kernel $p_{\\xi}\\in(\\Delta^{S})^{S\\times A}$ parameterized by $\\boldsymbol{\\xi}\\in\\Xi(\\Xi\\subset\\mathbb{R}^{d_{\\Xi}}$ is convex and compact), discount factor $\\gamma\\in(0,1)$ , general utility function $f:\\Delta^{S\\times A}\\rightarrow\\mathbb{R}$ and the distribution $\\rho\\in\\Delta^{\\hat{S}}$ of the initial state $s_{0}$ . At time $t$ given the environmental state $s_{t}$ , the agent takes action $a_{t}\\sim\\pi_{\\theta}(\\cdot|s_{t})$ based on a policy $\\pi_{\\theta}\\in(\\Delta^{A})^{S}$ parameterized by $\\theta\\in\\Theta$ $\\Theta\\subset\\mathbb{R}^{d_{\\Theta}}$ is convex). Then the environment transitions to state $s_{t+1}\\sim p_{\\xi}(\\cdot|s_{t},a_{t})$ . The occupancy measure $\\lambda_{\\theta,\\xi}\\in\\Delta^{S\\times\\bar{A}}$ at $(s,a)\\!\\in\\!S\\!\\times\\!A$ is defined below. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\lambda_{\\theta,\\xi}(s,a)\\ {\\stackrel{\\mathrm{def}}{=}}\\ (1-\\gamma)\\sum_{t=0}^{+\\infty}\\gamma^{t}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a|s_{0}\\sim\\rho),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}$ denotes the probability measure of the Markov chain $\\{s_{t},a_{t}\\}_{t\\ge0}$ induced by policy $\\pi_{\\theta}$ \uff0c transition kernel $p_{\\xi}$ and the initial distribution $\\rho$ . The aim of the agent is to find the optimal policy $\\pi_{\\theta}$ that solves $\\operatorname*{min}_{\\theta\\in\\Theta}f(\\lambda_{\\theta,\\xi})$ given fixed transition kernel $p_{\\xi}$ . Here, $f(\\lambda_{\\theta,\\xi})$ can be seen as the overall cost of selecting policy $\\pi_{\\theta}$ in the environment $p_{\\xi}$ , and there are many examples of the utility function $f$ covering a variety of applications (See Section 2.1). However, existing works on RL with general utility assume a fixed environmental transition kernel $p_{\\xi}$ , which may fail in many applications where the policy is deployed in a real-world environment different from the simulation environment for training. To obtain a policy that is robust to such environmental change, we propose a new learning framework called robust RL with General Utility as follows. ", "page_idx": 2}, {"type": "text", "text": "Our Proposed Robust RL with General Utility. The goal of our proposed robust RL with general utility is to find an optimal robust policy under the worst possible environmental parameter $\\xi$ from an ambiguityset $\\Xi$ , as formulated by the following minimax optimization problem with general utility function $f$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In practice, the distance between the real-world environment (for deployment) and simulation environment (for training) is assumed to be bounded. Therefore, $\\Xi$ is usually set as a neighborhood around the nominal kernel $\\widehat{\\xi}$ estimated from the simulation environment, i.e. $\\Xi\\,=\\,\\{\\xi\\,\\in\\,\\mathbb{R}^{d_{\\Theta}}$ $d(\\xi,\\widehat{\\xi})\\leq d_{0}\\}$ with distance measure $d$ and the distance upper bound $d_{0}\\geq0$ ", "page_idx": 2}, {"type": "text", "text": "2.1 Examples of Our Robust RL with General Utility ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Example 1: RL with General Utility. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "When $\\bar{\\Xi}=\\{\\widehat{\\xi}\\}$ for a fixed environmental parameter $\\widehat{\\xi}$ our proposed robust RL with general utility (2) reduces to (non-robust) $R L$ with general utility $\\operatorname*{min}_{\\theta\\in\\Theta}f(\\lambda_{\\theta,\\widehat{\\xi}})$ , as introduced above. ", "page_idx": 2}, {"type": "text", "text": "Example 2: Robust Constrained RL and Its Special Cases ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Robust constrained RL [38, 43, 40] is an emerging learning framework where an agent should obey safety conditions in all possible real-world environments, which is important in safety critical applications such as healthcare and unmanned aerial vehicle [43]. For math formulation, denote $\\hat{c^{(0)}},c^{(1)},\\ldots,c^{(K)}$ ascost functions $S\\times A\\to\\mathbb{R}$ . At time $t$ , the agent receives performance-related cost $c^{(0)}(s_{t},a_{t})$ and safety-related costs $\\{c^{(k)}(s_{t},a_{t})\\}_{k=1}^{K}$ .Define value functions $V_{\\theta,\\xi}^{(k)}$ and robust value functions $V_{\\theta}^{(k)}$ as fllows. ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{\\theta,\\xi}^{(k)}\\ \\stackrel{\\mathrm{def}}{=}\\ \\langle c^{(k)},\\lambda_{\\theta,\\xi}\\rangle=\\sum_{s,a}c^{(k)}(s,a)\\lambda_{\\theta,\\xi}(s,a),\\quad V_{\\theta}^{(k)}\\ \\stackrel{\\mathrm{def}}{=}\\ \\operatorname*{max}_{\\xi\\in\\Xi}V_{\\theta,\\xi}^{(k)},\\ k=0,1,\\ldots,K.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then robust constrained RL is formulated as the following constrained policy optimization problem. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}V_{\\theta}^{(0)},\\;\\mathrm{s.t.}\\;V_{\\theta}^{(k)}\\leq\\tau_{k}\\;\\mathrm{for\\;all}\\;k=1,\\ldots,K,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where Tk E R is the safety threshold, and V(k) means that the safety constraints $V_{\\theta,\\xi}^{(k)}\\le\\tau_{k}$ holds for any environmental parameter $\\xi\\in\\Xi$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. The robust constrained RL problem (4) is a special case of our proposed robust RL with general utility (2) using the following convex utility function $f$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\lambda)=\\left\\{\\!\\!\\begin{array}{l l}{\\langle c^{(0)},\\lambda\\rangle,\\qquad\\qquad}&{\\mathrm{if}\\,\\,\\langle c^{(k)},\\lambda\\rangle\\leq\\tau_{k}\\,\\,\\mathrm{for\\,all}\\,\\,k=1,\\dots,K}\\\\ {+\\infty,\\qquad}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After removing the safety constraints, robust constrained $R L$ reduces to an important special case called robust $R L$ (formulated as $\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}\\langle c^{(0)},\\lambda_{\\theta,\\xi}\\rangle$ with linear utility function $f(\\lambda)\\;=$ $\\langle c^{(0)},\\lambda\\rangle)$ [36]. Furthermore, when $\\Xi=\\{\\widehat{\\xi}\\}$ for fixed $\\widehat{\\xi}$ robust constrained $R L$ and robust $R L$ reduce toconstrained $R L$ [2] and $R L$ [41] respectively. All these examples are important special cases of our proposed robust RL with general utility based on Proposition 1. ", "page_idx": 3}, {"type": "text", "text": "Example 3: Robust Entropy Regularized RL and Its Special Cases. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Robust entropy regularized RL is also an important RL framework with application to imitation learning and inverse reinforcement learning which help agents learn from human experts\u2019 demonstration [32, 33], and is formulated as the following minimax optimization problem. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}\\sum_{s,a}\\big[\\lambda_{\\theta,\\xi}(s,a)c(s,a)\\big]-\\mu\\sum_{s}\\big[\\lambda_{\\theta,\\xi}(s)\\mathcal{H}[\\pi_{\\theta}(\\cdot|s)]\\big],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $c$ is a cost function, $\\begin{array}{r}{\\lambda_{\\theta,\\xi}(s)=\\sum_{a}\\lambda_{\\theta,\\xi}(s,a)}\\end{array}$ is the state occupancy measure, and $\\mathcal{H}[\\pi_{\\theta}(\\cdot|s)]=$ $\\begin{array}{r}{-\\sum_{a}\\pi_{\\theta}(a|s)\\log\\pi_{\\theta}(a|s)}\\end{array}$ is the entropy regularizer (with coefficient $\\mu\\geq0$ ) which encourages the agent to explore more states and actions and helps to prevent early convergence to sub-optimal policies. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. The robust entropy regularized RL problem (6) is a special case of our proposed robust $R L$ with general utility (2) using the following convex utility function $f$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\lambda)=\\sum_{s,a}\\lambda(s,a)\\Big[c(s,a)+\\mu\\log\\frac{\\lambda(s,a)}{\\sum_{a^{\\prime}}\\lambda(s,a^{\\prime})}\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $\\mu=0$ , robust entropy regularized $R L$ (6) reduces to robust $R L$ [36]. When $c\\equiv0$ but $\\mu>0$ robust entropy regularized $R L$ reduces to robust pure exploration. Furthermore, when $\\Xi=\\{\\widehat{\\xi}\\}$ robust entropyregularized $R L$ robust $R L$ and robust pure exploration reduce to entropy regularized RL [10], $R L$ [41] and pure exploration [18] respectively. All these examples are important special cases of our proposed robust RL with general utility based on Proposition 2. ", "page_idx": 3}, {"type": "text", "text": "2.2  Gradients for Our Robust RL with General Utility ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 1. The gradients of the objective function (2) for our proposed robust RL with general utilitycanbecomputedasfollows. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})=\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left[\\sum_{t=0}^{+\\infty}\\gamma^{t}\\frac{\\partial f(\\lambda_{\\theta,\\xi})}{\\partial\\lambda_{\\theta,\\xi}(s_{t},a_{t})}\\sum_{h=0}^{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{h}|s_{h})\\Bigg|s_{0}\\sim\\rho\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})=\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left[\\sum_{t=0}^{+\\infty}\\gamma^{t}\\frac{\\partial f(\\lambda_{\\theta,\\xi})}{\\partial\\lambda_{\\theta,\\xi}(s_{t},a_{t})}\\sum_{h=0}^{t}\\nabla_{\\xi}\\log p_{\\xi}(s_{h+1}|s_{h},a_{h})\\right|s_{0}\\sim\\rho\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We make the following standard assumptions which are also used in RL with general utility [55, 6]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. There exist constants $l_{\\pi_{\\theta}},L_{\\pi_{\\theta}},l_{p_{\\xi}},L_{p_{\\xi}}\\,>\\,0$ such that for all $s,s^{\\prime}\\,\\in\\,{\\mathcal{S}}_{}$ $a\\in{\\mathcal{A}}$ $\\theta,\\theta^{\\prime}\\in\\Theta$ and $\\xi,\\xi^{\\prime}\\in\\Xi$ we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\|\\le\\ell_{\\pi_{\\theta}},}&{\\|\\nabla_{\\theta}\\log\\pi_{\\theta^{\\prime}}(a|s)-\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\|\\le L_{\\pi_{\\theta}}\\|\\theta^{\\prime}-\\theta\\|,}\\\\ {\\|\\nabla_{\\xi}\\log p_{\\xi}(s^{\\prime}|s,a)\\|\\le\\ell_{p_{\\xi}},}&{\\|\\nabla_{\\xi}\\log p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-\\nabla_{\\xi}\\log p_{\\xi}(s^{\\prime}|s,a)\\|\\le L_{p_{\\xi}}\\|\\xi^{\\prime}-\\xi\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption2.Therexistconstants $l_{\\lambda},L_{\\lambda}>0$ such that forall $\\lambda,\\lambda^{\\prime}\\in\\Delta^{S\\times A}$ $\\|\\nabla_{\\lambda}f(\\lambda)\\|\\leq l_{\\lambda}$ and $\\|\\bar{\\nabla_{\\lambda}}f(\\lambda^{\\prime})-\\nabla_{\\lambda}f(\\lambda)\\|\\le L_{\\lambda}\\|\\lambda^{\\prime}-\\lambda\\|$ ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. Under Assumptions $^{\\,l}$ and 2, the gradients (8) and (9) satisfy the following bounds forany $\\theta,\\theta^{\\prime}\\in\\Theta$ and $\\xi,\\xi^{\\prime}\\in\\Xi$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\|\\le\\ell_{\\theta}:=\\frac{\\ell_{\\pi_{\\theta}}}{(1-\\gamma)^{2}},\\quad\\|\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})\\|\\le\\ell_{\\xi}:=\\frac{\\ell_{p_{\\xi}}}{(1-\\gamma)^{2}},}\\\\ &{\\quad\\quad\\quad\\|\\nabla_{\\theta}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})-\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\|\\le L_{\\theta,\\theta}\\|\\theta^{\\prime}-\\theta\\|+L_{\\theta,\\xi}\\|\\xi^{\\prime}-\\xi\\|,}\\\\ &{\\quad\\quad\\quad\\|\\nabla_{\\xi}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})-\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})\\|\\le L_{\\xi,\\theta}\\|\\theta^{\\prime}-\\theta\\|+L_{\\xi,\\xi}\\|\\xi^{\\prime}-\\xi\\|,}&{(12)}\\\\ &{\\quad\\quad\\cdot\\theta_{\\theta}:=\\frac{\\ell_{\\pi_{\\theta}}^{2}\\sqrt{|\\mathcal{A}|}(L_{\\lambda}+\\ell_{\\lambda}\\sqrt{|\\mathcal{S}||\\mathcal{A}|})}{(1-\\gamma)^{3}}+\\frac{L_{\\pi_{\\theta}}\\ell_{\\lambda}}{(1-\\gamma)^{2}},L_{\\theta,\\xi}:=\\frac{\\gamma\\ell_{\\pi_{\\theta}}\\ell_{p_{\\xi}}\\sqrt{|\\mathcal{S}|}}{(1-\\gamma)^{3}}(L_{\\lambda}+2\\ell_{\\lambda}\\sqrt{|\\mathcal{S}||\\mathcal{A}|}),L_{\\xi,\\theta}:=}\\\\ &{\\frac{\\sqrt{|\\mathcal{A}|}(L_{\\lambda}+\\ell_{\\lambda}\\sqrt{|\\mathcal{S}||\\mathcal{A}|})}{(1-\\gamma)^{3}},\\ L_{\\xi,\\xi}:=\\frac{\\gamma\\ell_{p_{\\xi}}\\sqrt{|\\mathcal{S}|}(L_{\\lambda}+2\\ell_{\\lambda}\\sqrt{|\\mathcal{S}||\\mathcal{A}|})}{(1-\\gamma)^{3}}+\\frac{\\ell_{\\lambda}(L_{p_{\\xi}}+\\ell_{p_{\\xi}}^{2}|\\mathcal{S}|)}{(1-\\gamma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practice, the exact gradients (8) and (9) are unavailable and can only be estimated via stochastic samples. We refer the details to Appendix $\\mathbf{C}$ as those largely follow [6]. ", "page_idx": 4}, {"type": "text", "text": "Define the following projected gradients with stepsizes $b,a>0$ , which have been used to measure convergence of algorithms to stationary points of optimization [30, 5, 47] and RL problems [49, 46, 34]. ", "page_idx": 4}, {"type": "equation", "text": "$$\nG_{b}^{(\\theta)}(\\theta,\\xi):=\\frac{1}{b}\\left[\\theta-\\mathrm{proj}_{\\Theta}\\big(\\theta-b\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\big)\\right],\\ \\ G_{a}^{(\\xi)}(\\theta,\\xi):=\\frac{1}{a}\\left[\\mathrm{proj}_{\\Xi}\\big(\\xi+a\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})\\big)-\\xi\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3  Gradient Convergence for Convex Utility ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Assumption 3. The utility function $f(\\lambda)$ is convex. ", "page_idx": 4}, {"type": "text", "text": "Robust RL with convex utility functions $f$ subsumes many important special cases, including robust constrained RL, robust entropy regularized RL, constrained RL, robust RL, RL, pure exploration, etc., as shown in Examples 2 and 3 in Section 2.1. ", "page_idx": 4}, {"type": "text", "text": "Partially inspired by the gradient descent ascent (GDA) algorithm [31] for nonconvex-concave minimax optimization, we design the projected stochastic GDA algorithm (Algorithm 1) with two phases to solve robust RL with convex utility. The first phase (called original phase) can be seen as projected stochastic GDA algorithm on the original objective function $f$ . Specifically, in the $k$ -th the outer loop with fixed $\\xi_{k}$ , the inner loop applies $T$ projected stochastic gradient descent steps (14) t0 obtain $\\theta_{k}$ which converges to the global solution of $\\begin{array}{r}{\\Phi(\\xi_{k}):=\\operatorname*{min}_{\\theta\\in\\Theta}\\bar{f}(\\lambda_{\\theta,\\xi_{k}})}\\end{array}$ as $f$ is convex. Then, we update $\\xi_{k}$ using the projected stochastic gradient ascent step (15). However, the output $\\tilde{\\xi}$ of the first phase only converges to a stationary point of the following the envelope function $\\widetilde{\\Phi}^{\\mathrm{~l~}}$ \uff1a ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\Phi}(\\xi):=\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}\\left[\\Phi(\\xi^{\\prime})-L_{\\xi,\\xi}\\|\\xi^{\\prime}-\\xi\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To converge to a stationary point of $f$ we add the second phase (called corrected phase) which applies projected stochastic GDA to the following corrected objective. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}\\widetilde{f}(\\theta,\\xi):=f(\\lambda_{\\theta,\\xi})-L_{\\xi,\\xi}\\|\\xi-\\widetilde{\\xi}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "1: Hyperparameters: $K,T,K_{\\_}^{\\prime}T^{\\prime},T^{\\prime},\\alpha,\\beta,a,b,L_{\\xi,\\xi},\\{m_{\\lambda}^{(k)},H_{\\lambda}^{(k)},m_{\\theta}^{(k)},H_{\\theta}^{(k)}\\}_{k=1}^{4}.$ ${\\theta_{0},\\theta_{K}\\in\\Theta}$ # Begin original phase to solve the original optimization problem (2)   \n3: for Iterations $k=0,1,\\ldots,K-1$ do   \n4: Initialize $\\theta_{k,0}\\gets\\theta_{0}$   \n5: for Inner steps $t=0,1,\\ldots,T-1$ do   \n6: Obtain gk,t? $g_{k,t}^{(\\theta)}\\!\\approx\\!\\nabla_{\\theta}f(\\lambda_{\\theta_{k,t},\\xi_{k}})$ by Algrithm 3 withyprprameters $m_{\\lambda}^{(1)},H_{\\lambda}^{(1)},m_{\\theta}^{(1)},H_{\\theta}^{(1)}$   \n7: Apply the following projected stochastic policy gradient descent step. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{k,t+1}=\\mathrm{proj}_{\\Theta}\\big(\\theta_{k,t}-\\alpha g_{k,t}^{(\\theta)}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "8: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "9: Assign $\\theta_{k}\\gets\\theta_{k,T}$ ", "page_idx": 5}, {"type": "text", "text": "10: Obtain $g_{k}^{(\\xi)}\\!\\approx\\!\\nabla_{\\xi}f(\\lambda_{\\theta_{k},\\xi_{k}})$ $m_{\\lambda}^{(2)},H_{\\lambda}^{(2)},m_{\\xi}^{(2)},H_{\\xi}^{(2)}$   \n11: Apply the following projected stochastic gradient descent step. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\xi_{k+1}=\\mathrm{proj}_{\\Xi}\\big(\\xi_{k}+\\beta g_{k}^{(\\xi)}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "12: end for. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$\\widetilde{\\xi}$ $\\{\\xi_{k}\\}_{k=0}^{K-1}$   \n14:forIterations $k=K,K+1,\\ldots,K+K^{\\prime}-1$ do   \n15: Initialize $\\xi_{k,0}\\gets\\hat{\\xi}$   \n16: forInner steps $t=0,1,\\ldots,T^{\\prime}-1$ do   \n17: Obtain $g_{k,t}^{(\\xi)}{\\approx}\\nabla_{\\xi}f(\\lambda_{\\theta_{k},\\xi_{k,t}})$ by Algorithm 3withyprparameters $m_{\\lambda}^{(3)},H_{\\lambda}^{(3)},m_{\\xi}^{(3)},H_{\\xi}^{(3)}$   \n18: Apply the following projected stochastic gradient ascent step. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\xi_{k,t+1}=\\mathrm{proj}_{\\Xi}\\big[\\xi_{k,t}+a\\big(g_{k,t}^{(\\xi)}-2L_{\\xi,\\xi}\\big(\\xi_{k,t}-\\widetilde{\\xi}\\big)\\big)\\big].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "19: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "20: Assign $\\xi_{k}\\gets\\xi_{k,T^{\\prime}}$   \n21: Obtain gk $g_{k}^{(\\theta)}\\approx\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi_{k}})$ by Algorithm3 with hyperprameters $m_{\\lambda}^{(4)},H_{\\lambda}^{(4)},m_{\\theta}^{(4)},H_{\\theta}^{(4)}$   \n22: Apply the following projected stochastic gradient descent step. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{k+1}=\\mathrm{proj}_{\\Theta}\\big(\\theta_{k}-b g_{k}^{(\\theta)}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "23: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "24: Output: $(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})$ where $\\widetilde{k}$ is obtained from $\\{K,K+1,\\ldots,K+K^{\\prime}-1\\}$ uniformly at random. ", "page_idx": 5}, {"type": "text", "text": "The convergence analysis of Algorithm 1 is challenging largely because $f(\\lambda_{\\theta,\\xi})$ is only a convex function of $\\lambda_{\\theta,\\xi}$ not of $\\theta$ . To tackle this challenge for non-robust convex RL with fixed $\\xi$ ,[54] assumed that a global Lipschitz continuous inverse mapping from $\\lambda_{\\theta,\\xi}$ to $\\theta$ exists. [55, 6] relaxed this assumption to the following assumption of local inverse mapping, which covers the popular direct policy parameterization $\\bar{\\pi}_{\\boldsymbol{\\theta}}(a|s)\\bar{=\\theta}_{s,a}$ [6] and softmax policy parameterization $\\pi_{\\theta}(a|s)=$ aeod(oe,a) (see Propostio 8 for the proof). ", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Local Invertibility of $\\lambda.,\\!\\xi)$ .There exists constants $\\ell_{\\lambda^{-1}}>0$ and ${\\overline{{\\delta}}}\\in(0,1)$ such that for any fixed $\\theta\\in\\Theta$ and $\\xi\\in\\Xi$ the occupancy measure (1) satisfies: ", "page_idx": 5}, {"type": "text", "text": "$^{\\,l}$ . There exists sets $\\mathcal{U}_{\\theta,\\xi}\\,\\subset\\,\\Theta$ and $\\bar{\\nu}_{\\lambda_{\\theta,\\xi}}\\subset\\bar{\\Delta}^{s\\times A}$ that contain $\\theta$ and $\\lambda_{\\theta,\\xi}$ respectively, such that $\\lambda_{\\theta,\\xi}:\\mathcal{U}_{\\theta,\\xi}\\to\\mathcal{V}_{\\lambda_{\\theta,\\xi}}$ is a bijection. Its inverse denoted as $\\lambda_{\\theta,\\xi}^{-1}\\;i s\\;\\ell_{\\lambda^{-1}}$ -Lipscthiz. 2. There exists at least one optimal policy $\\begin{array}{r}{\\theta^{*}(\\xi)\\in\\arg\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}f(\\lambda_{\\theta^{\\prime},\\xi})}\\end{array}$ such that for any $\\delta\\in[0,\\overline{{\\delta}}]$ (1 - 8)\u51650,g + o\u5165e\\*(E),g E V>0,c ", "page_idx": 5}, {"type": "text", "text": "Proposition 4 (Projected Gradient Dominance for Convex Utility). Under Assumptions 1-4, the utilityfunction $f$ satisfies thefollowinggradient dominancepropertyforany $\\theta\\in\\Theta$ and $\\xi\\in\\Xi$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(\\lambda_{\\theta,\\xi})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}f(\\lambda_{\\theta^{\\prime},\\xi})\\leq\\bigl[\\sqrt{2}\\ell_{\\lambda^{-1}}\\bigl(\\beta L_{\\theta,\\theta}+1\\bigr)+\\beta\\ell_{\\theta}\\bigr]\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark: Proposition 4 indicates that the function $f(\\lambda.,\\xi)$ is projected gradient dominant for convex utility function $f$ , which is important in the convergence analysis of Algorithm 1. Our Proposition 4 is stronger than Lemma F.7 of [6], a similar gradient dominance property for convex RL which requires assumption of positive definite Fisher information matrix, involves bias in the error term and focuses on unconstrained optimization with softmax parameterized policy (a special of our general parameterized policy with constrained variable $\\theta\\in\\Theta$ ", "page_idx": 6}, {"type": "text", "text": "Technical Novelty. In our proof, to tackle the constraint $\\theta\\in\\Theta$ which is more challenging than the unconstrainedcase $\\Theta=\\mathbb{R}^{|S||A|}$ in 55, 6], we aply Asumption 4 to $\\theta^{\\prime}:=\\theta-\\beta G_{\\beta}^{(\\theta)}(\\bar{\\theta_{,}}\\bar{\\xi})$ not to the obviouschoice $\\theta$ , which yields $\\theta_{\\delta}\\in\\Theta$ for any $\\delta\\in[0,\\overline{{\\delta}}]$ such that $\\lambda_{\\theta_{\\delta},\\xi}=(1-\\delta)\\lambda_{\\theta^{\\prime},\\xi}+\\delta\\lambda_{\\theta^{*}(\\xi),\\xi}\\in$ $\\mathcal{V}_{\\lambda_{\\theta,\\xi}}$ . Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}f(\\lambda_{\\theta^{\\prime},\\xi})^{\\top}(\\theta_{\\delta}-\\theta^{\\prime})\\overset{(i)}{\\geq}\\left[\\nabla_{\\theta}f(\\lambda_{\\theta^{\\prime},\\xi})\\!-\\!\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\!+\\!G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\right]^{\\top}\\!(\\theta_{\\delta}\\!-\\!\\theta^{\\prime})\\overset{(i i)}{\\geq}-\\mathcal{O}[\\delta]|G_{\\beta}^{(\\theta)}(\\theta,\\xi)||,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where (i) uses the projection property $(\\theta_{\\delta}\\!-\\!\\theta^{\\prime})^{\\top}[G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\!-\\!\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})]\\leq0$ and (i uses $\\|\\theta_{\\delta}-\\theta^{\\prime}\\|\\leq$ $O(\\delta)$ . The above bound implies Eq. (20) since $f$ is convex and $\\ell_{\\theta}$ -Lipscthiz. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5. \u4e09 is convex and compact with diameter $D_{\\Xi}:=\\operatorname*{max}_{\\xi,\\xi^{\\prime}\\in\\Xi}\\left\\|\\xi^{\\prime}-\\xi\\right\\|>0.$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 holds for the commonly used direct kernel parameterization $p_{\\xi}(s^{\\prime}|s,a)=\\xi(s,a,s^{\\prime})$ (for all $s,s^{\\prime}\\in\\mathcal{S}$ and $a\\in{\\mathcal{A}})[42,28,26,17]$ and $\\Xi$ defined a compact neighborhood around a nominal transition kernel parameter $\\widehat{\\xi}$ ", "page_idx": 6}, {"type": "text", "text": "We show the gradient convergence result of Algorithm 1 by the following theorem and demonstrate the gradient convergence by the experiments in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Gradient Convergence for Convex Utility). Suppose Assumptions 1-5 hold. For any precision \u2264 48Le [2lx-1 (Lo.,0 + 4L) + l , we can always find proper hyperparameter values of Algorithm $^{\\,l}$ (see Eqs. (127)-(150) in Appendix N.6 for these hyperparamter values) such that the algorithm output $(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})$ is an $\\epsilon$ -close toa stationary point, that is, $\\mathbb{E}\\big[\\|G_{b}^{(\\theta)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}\\big]\\le\\epsilon^{2}$ and $\\mathbb{E}[\\|G_{a}^{(\\xi)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]\\le\\epsilon^{2}$ with proietedgradiens $G_{b}^{(\\theta)}$ and $G_{a}^{(\\xi)}$ defned in Eg, (13) The mumber ofrequire stochaste omplesis $\\begin{array}{r}{\\mathcal{O}\\Big[\\frac{\\log^{2}[(1-\\gamma)^{-1}\\epsilon^{-1}]}{(1-\\gamma)^{25}\\epsilon^{10}}\\Big]}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch of Theorem 2 and Technical Novelty. Inspired by Appendix $\\mathrm{D}$ of [31], $\\widetilde{\\xi}$ from the first phase satisfies $\\mathbb{E}\\|\\nabla\\widetilde{\\Phi}(\\widetilde{\\xi})\\|^{2}\\,\\rightarrow\\,0$ (see Appendix N.2). Then, $\\xi_{k}\\,:=\\,\\xi_{k,T^{\\prime}}$ from the inner update (16) of the second phase converges to the unique maximizer (denoted as $\\xi_{k}^{*}$ ) of the $L_{\\xi,\\xi^{-}}$ concave function $\\ensuremath{\\widetilde{f}}(\\theta_{k},\\cdot)$ as $T^{\\prime}\\to+\\infty$ (see Appendix N.3). This means the update step (17) is approximately projected gradient descent for $\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{\\Psi}(\\theta)$ , which yields the convergence rate of $\\mathbb{E}\\big[\\|G_{b}^{(\\theta)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}\\big]$ (see Appendix N.4). ", "page_idx": 6}, {"type": "text", "text": "However, the bigest challenge s to obtain the convergence rate of $\\mathbb{E}\\big[\\|G_{a}^{(\\xi)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}\\big]$ (see Appendix N.5), which corresponds to $\\nabla_{\\xi}f$ while the second corrected phase aims at the corrected objective $\\widetilde{f}$ To show that $\\nabla_{\\xi}\\widetilde{f}(\\theta_{k},\\xi_{k})\\approx\\nabla_{\\xi}f(\\theta_{k},\\xi_{k}).$ note that $\\nabla_{\\xi}\\widetilde{f}(\\theta_{k},\\xi_{k})-\\nabla_{\\xi}f(\\theta_{k},\\xi_{k})=-2L_{\\xi,\\xi}(\\xi_{k}-\\widetilde{\\xi})$ and that $\\nabla\\widetilde\\Phi(\\widetilde\\xi)\\,=\\,2L_{\\xi,\\xi}[\\xi^{*}(\\widetilde\\xi)-\\widetilde\\xi]\\,\\approx\\,0$ (already proved) where $\\xi^{*}(\\widetilde{\\xi})$ is the unique maximizer of $\\Phi(\\xi^{\\prime})\\,-\\,L_{\\xi,\\xi}\\|\\xi^{\\prime}\\,-\\,\\widetilde\\xi\\|^{2}$ , a strongly concave function of $\\xi^{\\prime}$ in Eq. (18). Hence, it suffices to show $\\xi_{k}\\approx\\xi^{*}(\\xi)$ . Note that $(\\theta_{k},\\xi_{k})$ is an approximate Nash equilibrium of $\\widetilde{f}$ i.e., $\\xi_{k}\\approx\\xi_{k}^{\\ast}:=$ arg $\\mathrm{max}_{\\xi\\in\\Xi}\\widetilde{f}(\\theta_{k},\\xi)$ (proved above) and $\\theta_{k}\\approx\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi_{k})$ (derived below). ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\widetilde{f}(\\theta_{k},\\xi_{k})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\widetilde{f}(\\theta^{\\prime},\\xi_{k})]=\\mathbb{E}[f(\\lambda_{\\theta_{k},\\xi_{k}})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}f(\\lambda_{\\theta^{\\prime},\\xi_{k}})]\\overset{(i)}{\\leq}\\mathcal{O}(\\mathbb{E}\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|)\\leq\\mathcal{O}(\\epsilon),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where (i) uses Proposition 4. Hence, based on the property of Nash equilibrium, we have $\\xi_{k}\\approx$ arg $\\mathrm{max}_{\\xi}\\psi(\\xi)=\\xi^{*}(\\widetilde{\\xi})$ where $\\begin{array}{r}{\\psi(\\xi):=\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi)=\\Phi(\\xi)-L_{\\xi,\\xi}\\|\\xi-\\widetilde{\\xi}\\|^{2}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "4  Global Convergence on Polyhedral Ambiguity Set ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section aims to obtain a global optimal policy $\\theta^{*}$ that minimizes the robust utility $\\Gamma(\\theta)\\ {\\stackrel{\\mathrm{def}}{=}}$ $\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ . This maximization is challenging for convex utility $f$ . In contrast, global convergence results have been obtained without such challenge in some important special cases, including convex RL with fixed $\\xi$ [54, 51, 6] and robust RL where linear utility $f$ is amenable to both $\\operatorname*{min}_{\\theta\\in\\Theta}$ and $\\operatorname*{max}_{\\xi\\in\\Xi}$ [36, 45, 42, 23, 26]. Fortunately, we will show that by using the popular $s$ -rectangular polyhedral ambiguityset $\\Xi$ , arg $\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ always exists among the finitely many vertices of $\\Xi$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.1  S-rectangular Polyhedral Ambiguity Set ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this subsection, we will introduce the popular $s$ -rectangular polyhedral ambiguity set, and derive its important propositions for designing globally converged algorithm. ", "page_idx": 7}, {"type": "text", "text": "Fhe global convergence is generally NP-hard, even for the important special case called robust RL with linear utility, [45]. A common practice to make the problem tractable is to use direct kernel parameterization $p_{\\xi}(s^{\\prime}|s,a)=\\xi(s,a,s^{\\prime})$ [42, 28, 26, 17] and assume the ambiguity set $\\Xi$ tosatisfy some certain rectangularity conditions, such as $s$ -rectangularity defined below [45, 42, 23, 26]. ", "page_idx": 7}, {"type": "text", "text": "Assumption 6. We use direct kernel parameterization and assume that $\\Xi$ is $s$ -rectangular,i.e., $\\Xi=\\check{\\times}_{s\\in\\mathcal{S}}\\Xi_{s}:=\\{\\xi\\in(\\Delta^{S})^{S\\times A}:\\xi(s,\\dot{\\cdot},\\cdot)\\in\\Xi_{s},\\forall s\\in\\mathcal{S}\\}$ aCartesianproduct of $\\Xi_{s}\\subset\\bar{(\\Delta^{s})^{A}}$ ", "page_idx": 7}, {"type": "text", "text": "Proposition 5 (Local Invertibility of $\\lambda_{\\theta,}.$ ).Suppose Assumption $^{\\sc6}$ holds and $\\Xi$ is a convex set. For any $\\theta\\in\\Theta$ $\\xi_{0},\\xi_{1}\\in\\Xi$ and $\\delta\\in[0,1]$ define the following kernel parameters $\\xi_{\\delta}\\in(\\Delta^{S})^{S\\times A}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\xi_{\\delta}(s,a,s^{\\prime})=\\left\\{\\begin{array}{l l}{\\mathrm{arbitrary~as~long~as~}\\xi_{\\delta}(s,a,\\cdot)\\in\\Delta^{S},\\qquad\\qquad\\mathrm{if~}\\lambda_{\\theta,\\xi_{0}}(s)\\!=\\!\\lambda_{\\theta,\\xi_{1}}(s)\\!=\\!0}\\\\ {\\!\\!\\frac{\\delta\\lambda_{\\theta,\\xi_{1}}(s)\\xi_{1}(s,a,s^{\\prime})+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s)\\xi_{0}(s,a,s^{\\prime})}{\\delta\\lambda_{\\theta,\\xi_{1}}(s)+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s)},\\mathrm{otherwise~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\textstyle\\lambda_{\\theta,\\xi}(s)\\;:=\\;\\sum_{a\\in{\\cal A}}\\lambda_{\\theta,\\xi}(s,a)$ for any $s\\;\\in\\;{\\mathcal{S}},\\;\\theta\\;\\in\\;\\Theta$ and $\\xi\\ \\in\\ \\Xi$ Then $\\xi_{\\delta}~\\in~\\Xi$ and its corresponding occupancymeasurs $\\lambda_{\\theta,\\xi_{\\delta}}=\\delta\\lambda_{\\theta,\\xi_{1}}+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}$ ", "page_idx": 7}, {"type": "text", "text": "Remark: Proposition 5 indicates that the mapping from $\\xi$ to $\\lambda_{\\theta,\\xi}$ is locally invertible for $s$ rectangular set $\\Xi$ , which is important to solve the aforementioned challenge that convex utility is not amenable for $\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ . This role is similar to that played by the local invertibility assumption (Assumption 4) for policy $\\theta$ . To our knowledge, Proposition 5 has never been obtained in the existing literature. ", "page_idx": 7}, {"type": "text", "text": "Assumption 7. Under Assumption $6$ for every $s\\in S$ $\\Xi_{s}$ is a polyhedron spanned by a finite set of verices $V(\\Xi_{s})\\!:=\\!\\{\\xi_{m}^{(s)}\\}_{m=1}^{M_{s}}\\!\\subset\\!\\Xi_{s}$ i.e, $\\begin{array}{r}{\\Xi_{s}=\\Big\\{\\sum_{m=1}^{M_{s}}{\\nu_{m}\\xi_{m}^{(s)}:\\nu_{m}}\\geq0,\\sum_{m=1}^{M_{s}}{\\nu_{m}}=1\\Big\\}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Polyhedral ambiguity set defined by Assumption 7 includes the widely used $s$ -rectangular $L_{1}$ and $L_{\\infty}$ ambiguity sets, defined as $\\Xi=\\{\\xi\\in(\\Delta^{S})^{S\\times A}:\\|\\xi(s,:,:)-\\widehat\\xi(s,:,:)\\|_{p}\\,\\le\\,\\alpha_{s},\\forall s\\,\\in\\,\\mathcal{S}\\}$ for $p\\in\\{1,\\infty\\}$ respectively [7, 19, 16], where $\\widehat{\\xi}\\in\\Xi$ is the nominal transition kernel usually obtained via empirical estimation. On polyhedral ambiguity set, the optimal kernels arg $\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ can always be obtained at the vertices of E, as shown below. ", "page_idx": 7}, {"type": "text", "text": "Proposition 6. Under Assumptions 3, 6 and 7, for any $\\theta\\ \\in\\ \\Theta$ wehave $\\begin{array}{r l}{\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})\\;=\\;}&{{}}\\end{array}$ $\\mathrm{max}_{\\xi\\in V(\\Xi)}\\,f\\big(\\lambda_{\\theta,\\xi}\\big)$ where $V(\\Xi)=\\times_{s\\in S}V(\\Xi_{s})$ is the vertex set. ", "page_idx": 7}, {"type": "text", "text": "Technical Novelty. Suppose a non-vertex kernel $\\xi^{*}\\in\\arg\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})/V(\\Xi)$ is optimal. Since $f$ is convex, if $\\lambda_{\\theta,\\xi^{*}}$ is a convex combination of $\\lambda_{\\theta,\\xi_{1}^{*}}$ and $\\lambda_{\\theta,\\xi^{(\\epsilon)}}$ for some $\\xi_{1},\\xi_{0}\\in\\Xi$ (corresponding to $\\xi_{1}^{*}$ \uff0c $\\xi^{(\\epsilon)}$ respectively in the proof in Appendix I), then $\\xi_{1},\\xi_{0}$ are also optimal. Ideally, if $\\xi_{1}\\in V(\\Xi)$ or $\\bar{\\xi}_{0}\\;\\in\\;V(\\Xi)$ , the proof is done. However, this is not guaranteed since in Proposition 5 and Assumption 6, the convex combination coefficients differ among the states $s\\,\\in\\,S$ . To solve this challenge, it suffices to find such optimal $\\xi_{1}$ that differs from $\\xi^{*}$ at only one state $s$ such that the non-vertex $\\xi^{*}(s)\\,\\notin\\,V(\\Xi_{s})$ is replaced with vertex $\\xi_{1}(s)\\,\\in\\,V(\\Xi_{s})$ . Then we can conduct such change from non-vertex to vertex for only one state $s$ at a time until the kernel becomes vertex at every state, while keeping the optimality all the way. To find such $\\xi_{1}(s)$ , note that on polyhedral set $\\Xi_{s}$ , there always exist $\\xi_{1}(s)\\in V(\\Xi_{s})$ and $\\xi_{0}(s)\\in\\Xi_{s}$ such that the non-vertex point $\\xi^{*}(s)$ is a convex combination of $\\xi_{1}(s)$ and $\\xi_{0}(s)$ , while $\\xi^{*}(s^{\\prime})=\\xi_{1}(s^{\\prime})=\\xi_{0}(s^{\\prime})$ for any $s^{\\prime}\\neq\\bar{s}$ . Hence, there exists $\\delta\\in[0,1]$ such that $\\xi^{\\ast}=\\xi_{\\delta}$ defined by Proposition 5, which implies that $\\lambda_{\\theta,\\xi^{*}}$ is a convex combination of $\\lambda_{\\theta,\\xi_{1}^{*}}$ and $\\lambda_{\\theta,\\xi^{(\\epsilon)}}$ ", "page_idx": 7}, {"type": "text", "text": "The original objective (2) is equivalent to the minimization problem $\\operatorname*{min}_{\\theta\\in\\Theta}\\Gamma(\\theta)$ ,where $\\Gamma(\\theta)\\;:=\\;\\,\\mathrm{max}_{\\xi\\in V(\\Xi)}\\:f\\!\\left(\\lambda_{\\theta,\\xi}\\right)$ with finite vertex set $V(\\Xi)$ based on Proposition 6. A natural choice to solve this minimization problem is the following policy update rule (for simplicity we consider the unconstrained policy space $\\Theta=$ $\\mathbb{R}^{d_{\\Theta}}$ as in [55, 6]). ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\theta_{k+1}=\\theta_{k}-\\beta_{k}d_{k},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\beta_{k}\\,>\\,0$ is the stepsize and $d_{k}$ is a unit descent direction of $\\Gamma(\\boldsymbol{\\theta}_{k})$ . Subgradient descent method seems an obvi", "page_idx": 8}, {"type": "text", "text": "Algorithm 2 Globally Converged Algorithm for Convex Utility on Polyhedral Ambiguity Set ", "page_idx": 8}, {"type": "text", "text": "Hyerper $K$ $\\{\\sigma_{k},\\epsilon_{k},\\beta_{k}\\}_{k=0}^{K-1}$ $\\theta_{0}\\in\\Theta$   \n3: for Iterations $k=0,1,\\ldots,K-1$ do   \n4: Calculate $\\lambda_{\\theta_{k},\\xi}$ \uff0c $f(\\lambda_{\\theta_{k},\\xi})$ and $\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi})$ for all $\\xi\\in V(\\Xi)$   \n5: Select near-optimal vertices $\\Xi_{k}\\!:=\\!\\{\\xi\\in V(\\Xi):f(\\lambda_{\\theta_{k},\\xi})\\geq$ maxg'\u2208V(=) $f(\\lambda_{\\theta_{k},\\xi^{\\prime}})-\\sigma_{k}\\}$   \n6: Find $d_{k}^{\\prime}\\in B_{1}:=\\{d\\in\\mathbb{R}^{d_{\\Theta}}:\\|d\\|\\leq1\\}$ such that $\\begin{array}{r}{A_{k}(d_{k}^{\\prime})\\le\\operatorname*{min}_{d\\in B_{1}}A_{k}(d)+\\epsilon_{k}}\\end{array}$ $\\dot{A}_{k}$ is defined in $E q$ (23). One way to solve $\\operatorname*{min}_{d\\in B_{1}}A_{k}(d)$ is to apply projected subgradient method (28) and obtain $d_{k}^{\\prime}\\in\\arg\\operatorname*{max}_{d\\in\\{d_{k,t}:0\\leq t\\leq T\\}}A_{k}(d)._{}$ \uff09   \n7: Let $d_{k}:=d_{k}^{\\prime}/\\lVert d_{k}^{\\prime}\\rVert$ and obtain $\\theta_{k+1}$ by Eq. (22).   \n8: end for ", "page_idx": 8}, {"type": "text", "text": "", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "9:Output: $(\\theta_{K},\\xi_{K})$ Where $\\xi_{K}\\in\\arg\\operatorname*{max}_{\\xi\\in V(\\Xi)}f\\big(\\lambda_{\\theta_{K},\\xi}\\big).$ ", "page_idx": 8}, {"type": "text", "text": "ous choice for $d_{k}$ which aligns with the direction of a subgradient $\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi_{k}})$ where $\\xi_{k}~\\in$ arg $\\mathrm{max}_{\\xi\\in V(\\Xi)}f(\\lambda_{\\theta_{k},\\xi})$ . However, the convergence analysis of subgradient descent method [11] requires the convexity of $f(\\lambda._{,\\xi_{k}})$ which does not hold in our setting, and the function value is not monotonically decreasing. To solve these challenges, we design Algorithm 2 which selects near-optimal vertices $\\Xi_{k}:=\\bar{\\{\\xi\\in V(\\Xi):f(\\lambda_{\\theta_{k},\\xi})\\geq\\operatorname*{max}_{\\xi^{\\prime}\\in V(\\Xi)}\\bar{f}(\\lambda_{\\theta_{k},\\bar{\\xi^{\\prime}}})-\\sigma_{k}\\}}$ with a certain threshold $\\sigma_{k}>0$ and obtains $d_{k}$ by solving the convex optimization problem $\\mathrm{min}_{d\\in B_{1}}\\,A_{k}(d)$ up to precision $\\epsilon_{k}>0$ , where $A_{k}(d)$ below denotes effective descent of $\\Gamma(\\boldsymbol{\\theta}_{k})$ along the direction $d$ ", "page_idx": 8}, {"type": "equation", "text": "$$\nA_{k}(d):=\\operatorname*{max}_{\\xi\\in\\Xi_{k}}\\big[\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi})^{\\top}d\\big],d\\in B_{1}:=\\{d^{\\prime}\\in\\mathbb{R}^{d_{\\Theta}}:\\|d^{\\prime}\\|\\leq1\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here we only care about the near-optimal vertices in $\\Xi_{k}\\subset V(\\Xi)$ because for any worse vertices $\\begin{array}{r}{\\xi\\in V(\\Xi)/\\Xi_{k},f(\\theta_{k},\\xi)<\\operatorname*{max}_{\\xi^{\\prime}\\in V(\\Xi)}f(\\lambda_{\\theta_{k},\\xi^{\\prime}})\\!-\\!\\sigma_{k}}\\end{array}$ implies $f(\\theta_{k+1},\\xi)<\\operatorname*{max}_{\\xi^{\\prime}\\in V(\\Xi)}f(\\lambda_{\\theta_{k+1},\\xi^{\\prime}})$ for appropriate $\\sigma_{k}>0$ . This means such worse $\\xi$ can not affect the optimization progress $\\Gamma(\\theta_{k})-$ $\\Gamma(\\theta_{k+1})$ . Hence, by solving $\\operatorname*{min}_{d\\in B_{1}}A_{k}(d)$ , we can obtain a direction $d_{k}$ in which all the potentially effective function values $\\{f(\\lambda_{\\theta_{k},\\xi})\\}_{\\xi\\in\\Xi_{k}}$ have uniformly large amount of descent $-\\nabla f(\\lambda_{\\theta_{k},\\xi})^{\\top}d_{k}$ To analyze the global convergence of Algorithm 2, we want to guarantee sufficient descent $\\Gamma(\\theta_{k})-$ $\\Gamma(\\theta_{k+1})$ whenever $\\theta_{k}$ is not close to optimal. It suffices to slightly alter Assumption 4 as follows. ", "page_idx": 8}, {"type": "text", "text": "Assumption 8. A variant of Assumption 4 holds which replaces the non-robust optimal policy $\\theta^{*}(\\xi)$ with a robust optimal policy $\\theta^{*}\\in\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\Gamma(\\theta)$ and shrinks the range from $\\xi\\in\\Xi$ $\\xi\\in V(\\Xi)$ ", "page_idx": 8}, {"type": "text", "text": "Remark: Assumption 8 is no stronger than Assumption 4 and also covers the popular direct policy parameterization. Also, Assumption 8 guarantees that from any policy $\\theta\\in\\Theta$ , there exists a partial curve $\\{\\theta_{\\delta}:\\delta\\in[0,\\overline{{\\delta}}]\\}$ towards a robust optimal policy $\\theta^{*}$ such that $\\lambda_{\\theta_{\\delta},\\xi}=(1-\\delta)\\lambda_{\\theta,\\xi}+\\delta\\lambda_{\\theta^{*},\\xi}$ we can utilize convexity of $f$ and obtain the following important suficient descent property. ", "page_idx": 8}, {"type": "text", "text": "Proposition 7 (Sufficient Descent on Polyhedral Ambiguity Set). Under Assumptions 1-3 and $\\boldsymbol{\\vartheta}$ at any $\\bar{\\theta}\\in\\Theta:=\\dot{\\mathbb{R}}^{d_{\\Theta}}$ ,there exists a unit descent direction $d$ $l\\left(\\|d\\|=1\\right)$ )suchthat ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f\\big(\\lambda_{\\theta,\\xi}\\big)-f\\big(\\lambda_{\\theta^{*},\\xi}\\big)\\leq\\big[-\\sqrt{2}\\ell_{\\lambda^{-1}}\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})^{\\top}d\\big]_{+},\\forall\\xi\\in\\Xi}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\theta^{*}\\in\\arg\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta)$ is given by Assumption 8 and $x_{+}:=\\operatorname*{max}(x,0)$ for any $x\\in\\mathbb R$ ", "page_idx": 8}, {"type": "text", "text": "Remark: $d$ in Proposition 7 is a good descent direction since whenever the function value gap $f(\\lambda_{\\theta,\\xi})-f(\\lambda_{\\theta^{*},\\xi})>0.$ it is dominated by the gradient descent amount $-\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})^{\\top}d>0$ Unlike existing gradient dominance properties for robust RL [42, 26, 17], $f(\\lambda_{\\theta,\\xi})-f(\\lambda_{\\theta^{*},\\bar{\\xi}})\\leq0$ is possible sowe use $[\\cdot]_{+}$ to cover all cases. This brings challenge and thus novel techniques to obtain the first global convergence result of our robust RL with general convex utility as follows. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 (Global Convergence for Convex Utility on Polyhedral Ambiguity Set). Implement Algorithm2with $\\begin{array}{r}{\\beta_{k}=\\frac{2\\sqrt{2}\\ell_{\\lambda}}{k+2}}\\end{array}$ K+2\uff0c0k= $\\begin{array}{r}{\\sigma_{k}=\\frac{4\\sqrt{2}\\ell_{\\theta}\\ell_{\\lambda}}{k+2}}\\end{array}$ 4/2le> and any ek > 0. Then under Assumptions 1-3, 6-8, the algorithm output $\\theta_{K}$ has the following global convergence rate. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\n\\Gamma(\\theta_{K})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\leq\\sqrt{2}\\ell_{\\lambda^{-1}}\\operatorname*{max}_{1\\leq k\\leq K}\\epsilon_{k}+\\frac{4\\ell_{\\lambda^{-1}}}{K+1}(\\ell_{\\lambda^{-1}}L_{\\theta,\\theta}+2\\sqrt{2}\\ell_{\\theta}).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Remark: The convergence rate ${\\mathcal{O}}(1/K)$ matches the state-of-the-art of policy gradient type methods for robust RL [26], while the error term $\\epsilon_{k}$ results from solving the convex optimization problem $\\operatorname*{min}_{d\\in B_{1}}A_{k}(d)$ in line 6 of Algorithm 2. ", "page_idx": 9}, {"type": "text", "text": "Technical Novelty. Applying Proposition 7 to Algorithm 2 with $\\sigma_{k}=2\\beta_{k}\\ell_{\\theta}$ ,wehave ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\Delta_{k}:=\\Gamma(\\theta_{k})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\leq\\left[\\sqrt{2}\\ell_{\\lambda^{-1}}[\\epsilon_{k}-A_{k}(d_{k}^{\\prime})]\\right]_{+}+2\\beta_{k}\\ell_{\\theta}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "To overcome the main difficulty caused by $[\\cdot]_{+}$ above, we analyze each $k$ -th iteration in 2 cases $A_{k}(d_{k}^{\\prime})\\,\\geq\\,0$ and $A_{k}(d_{k}^{\\prime})\\,<\\,0$ If $A_{k}(d_{k}^{\\prime})\\,\\geq\\,0$ , then $\\Delta_{k}\\le\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{k}+2\\beta_{k}\\ell_{\\theta}$ and thus $\\Delta_{k+1}\\leq$ $\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{k}+3\\beta_{k}\\ell_{\\theta}$ ; If $A_{k}(d_{k}^{\\prime})<0$ , then in Eq. (26) we replace $A_{k}(d_{k}^{\\prime})$ with $A_{k}(d_{k})\\leq A_{k}(d_{k}^{\\prime})<0$ and remove $[\\cdot]_{+}$ . This along with $\\begin{array}{r}{\\Gamma(\\theta_{k+1})-\\Gamma(\\theta_{k})\\le\\beta_{k}A_{k}(d_{k})+\\frac{L_{\\theta,\\theta}}{2}\\beta_{k}^{2}}\\end{array}$ (by smoothness) implies ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\Delta_{k+1}\\leq\\frac{k}{k+2}\\Delta_{k}+\\mathcal{O}\\Big[\\frac{\\epsilon_{k}}{k+2}+\\frac{1}{(k+2)^{2}}\\Big].\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Then we obtain the rate (25) in 3 cases: If $A_{k}(d_{k}^{\\prime})<0$ for all $k=0,1,\\ldots,K-1$ , iterate Eq. (27) from $\\Delta_{0}$ ; If $A_{K-1}(d_{K-1})\\geq0,\\Delta_{K}\\leq\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{K}+\\sigma_{k}$ ;If $A_{K^{\\prime}-1}(d_{K^{\\prime}-1})\\geq0$ while $A_{k}(d_{k})<0$ for all $k=K^{\\prime},\\ldots,K-1$ , iterate Eq. (27) from $\\Delta_{K^{\\prime}}\\le\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{K^{\\prime}-1}+3\\beta_{K^{\\prime}-1}\\ell_{\\theta}$ ", "page_idx": 9}, {"type": "text", "text": "Algorithm 2 involves convex optimization problems $\\mathrm{min}_{d\\in B_{1}}\\,A_{k}(d)$ , which can be solved via the following projected subgradient method for $t=0,1,\\ldots,T-1$ ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{k,t+1}\\leftarrow\\mathrm{proj}_{B_{1}}[d_{k,t}-\\alpha\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi_{k,t}})],\\mathrm{where~}\\xi_{k,t}\\in\\arg\\operatorname*{max}_{\\xi\\in\\Xi_{k}}\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi})^{\\top}d_{k,t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The best direction $d_{k}^{\\prime}\\in\\arg\\operatorname*{max}_{d\\in\\{d_{k,t}:0\\leq t\\leq T\\}}A_{k}(d)$ from the above subgradient method achieves $\\epsilon_{k}$ accuracy within $T=\\mathcal{O}(\\epsilon_{k}^{-2})$ steps [11], which yields the following complexity result. ", "page_idx": 9}, {"type": "text", "text": "Corollary 1. Under the conditions of Theorem $3$ for any $\\epsilon>0$ implement Algorithm 2 with $K=$ $8\\ell_{\\lambda^{-1}}\\epsilon^{-1}(\\ell_{\\lambda^{-1}}L_{\\theta,\\theta}+2\\sqrt{2}\\ell_{\\theta})$ iterations and $T=36\\ell_{\\lambda^{-1}}^{2}\\ell_{\\theta}^{2}\\epsilon^{-2}$ subgradient descent updates (28) with stepsize \u03b1 = 3lx-1ee to obtain $d_{k}^{\\prime}$ . Then the output $\\overline{{\\theta}}_{K}$ achieves $\\Gamma(\\theta_{K})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\le\\epsilon$ ", "page_idx": 9}, {"type": "text", "text": "Finally, we can prove that all these Assumptions 1-8 required by our convergence results (Theorems 2 and 3) can be satisfied by the following examples. ", "page_idx": 9}, {"type": "text", "text": "Proposition 8. Assumptions 1-8 are all satisfied if we use the following choices: ", "page_idx": 9}, {"type": "text", "text": "\u00b7 Sofmax policy parameterization me(als) =p where $\\theta\\,\\in\\,\\Theta\\,=\\,[-R,R]^{|S|\\times|A|}$ for some constant $R>0$ to prevent $\\pi_{\\theta}(a|s)$ from approaching $\\boldsymbol{O}$ ", "page_idx": 9}, {"type": "text", "text": "\u00b7 Direct kernel parameterization $p_{\\xi}(s^{\\prime}|s,a)\\;=\\;\\xi_{s,a,s^{\\prime}}$ with $s$ -rectangular $L_{1}$ or $L_{\\infty}$ ambiguity sets defined as $\\Xi\\,=\\,\\{\\xi\\,\\in\\,(\\Delta^{S})^{S\\times A}\\,:\\,\\|\\xi(s,:,:)-\\widehat{\\xi}(s,:,:)\\|_{p}\\,\\le\\,\\alpha_{s},\\forall s\\,\\in\\,\\mathcal{S}\\}$ for $p\\,\\in\\,\\{1,\\infty\\}$ respectively, where the fixed nominal kernel $\\widehat{\\xi}$ satisfies $\\widehat{\\xi}(s,a,s^{\\prime})>\\alpha_{s},\\forall s,a,s^{\\prime}$ to prevent $p_{\\xi}(s^{\\prime}|s,a)$ from approaching $\\boldsymbol{O}$ ", "page_idx": 9}, {"type": "text", "text": "\u00b7The utilityfunction $f(\\lambda)$ defined in Eq. (7) for robust entropy regularized RL and its special cases, withintherange $\\lambda\\in\\Lambda=\\{\\lambda_{\\theta,\\xi}:\\theta\\in\\Theta,\\xi\\in\\Xi\\}$ forthedomains $\\Theta$ and $\\Xi$ selectedabove. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose robust RL with general utility, the first learning framework that obtains a robust policy for RL with general utility. We propose a stochastic policy gradient type algorithm for convex utilities and obtains its sample complexity result for gradient convergence. Furthermore, for convex utility on polyhedral ambiguity set, we propose an alternative policy gradient type algorithm and obtain its global convergence rate. Note that this globally converged algorithm requires enumeration among many vertices, and thus it is an important future direction to reduce enumeration by utilizing structural properties. In addition, to extend the results to large or continuous state-action space is also an interesting direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially supported by NSF IIS 2347592, 2347604, 2348159, 2348169, DBI 2405416, CCF 2348306, CNS 2347617. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22-31, 2017.   \n[2] Eitan Altman. Constrained Markov decision processes. Routledge, 2021.   \n[3] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469-483, 2009. [4]  Kishan Panaganti Badrinath and Dileep Kalathil. Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In International Conference on Machine Learning, pages 511-520, 2021.   \n[5]  Maxim Viktorovich Balashov. The gradient projection algorithm for a proximally smooth set and a function with lipschitz continuous gradient. Sbornik: Mathematics, 211(4):481, 2020.   \n[6]  Anas Barakat, Ilyas Fatkhullin, and Niao He. Reinforcement learning with general utilities: Simpler variance reduction and large state-action space. In International Conference on Machine Learning, 2023.   \n[7] Bahram Behzadian, Marek Petrik, and Chin Pang Ho. Fast algorithms for $l_{\\infty}$ -constrained srectangular robust mdps. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), 2021.   \n[8]  Vivek S Borkar and Sean P Meyn. Risk-sensitive optimal control for markov decision processes with monotone cost. Mathematics of Operations Research, 27(1):192-209, 2002.   \n[9]  Winfried Bruns and Joseph Gubeladze. Polytopes, rings, and K-theory. Springer Science & Business Media, 2009.   \n[10] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. Operations Research, 70(4):2563- 2578,2022.   \n[11]  Yuxin Chen. Subgradient methods. https: / /yuxinchen2020. github.io/ele522 _optimization/lectures/subgradient_methods.pdf, 2020.   \n[12]  Jerzy A Filar, Lodewijk CM Kallenberg, and Huey-Min Lee.  Variance-penalized markov decision processes. Mathematics of Operations Research, 14(1):147-161, 1989.   \n[13] Matthieu Geist, Julien Perolat, Mathieu Lauriere, Romuald Elie, Sarah Perrin, Oliver Bachem, Remi Munos, and Olivier Pietquin. Concave utility reinforcement learning: The mean-field game viewpoint. In Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems, pages 489-497, 2022.   \n[14] Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow. Safe policy improvement by minimizing robust baseline regret. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), volume 29, 2016.   \n[15]  Julien Grand-Cl\u00e9ment and Christian Kroer. Scalable first-order methods for robust mdps. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12086-12094, 2021.   \n[16]  Julien Grand-Cl\u00e9ment and Marek Petrik. Reducing blackwell and average optimality to discounted mdps via the blackwell discount factor. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), 2023.   \n[17] Etash Kumar Guha and Jason D Lee.  Solving robust mdps through no-regret dynamics. ArXiv:2305.19035, 2023.   \n[18] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy exploration. In International Conference on Machine Learning, pages 2681-2691, 2019.   \n[19]  Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for ll-robust markov decision processes. Journal of Machine Learning Research, 22(275):1-46, 2021.   \n[20] Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):257-280, 2005.   \n[21] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial inteligence research, 4:237-285, 1996.   \n[22]  Lodewijk CM Kallenberg. Survey of linear programming for standard and nonstandard markovian control problems. part i: Theory. Zeitschrift fir Operations Research, 40:1-42, 1994.   \n[23] Navdeep Kumar, Esther Derman, Matthieu Geist, Kfr Levy, and Shie Mannor. Policy gradient for rectangular robust markov decision processes. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), 2023.   \n[24] Navdeep Kumar, Kfr Levy, Kaixin Wang, and Shie Mannor. Efficient policy iteration for robust markov decision processes via regularization. ArXiv:2205.14327, 2022.   \n[25] Navdeep Kumar, Kfir Levy, Kaixin Wang, and Shie Mannor. An efficient solution to srectangular robust markov decision processes. ArXiv:2301.13642, 2023.   \n[26] Navdeep Kumar, Inura Usmanova, Kfir Yehuda Levy, and Shie Mannor. Towards faster global convergence of robust policy gradient methods. In Sixteenth European Workshop on Reinforcement Learning, 2023.   \n[27] Navdeep Kumar, Kaixin Wang, Kfr Levy, and Shie Mannor. Policy gradient for reinforcement learning with general utilities. ArXiv:2210.00991, 2023.   \n[28]  Mengmeng Li, Tobias Sutter, and Daniel Kuhn. Policy gradient algorithms for robust mdps with non-rectangular uncertainty sets. ArXiv:2305.19004, 2023.   \n[29]  Yan Li, Guanghui Lan, and Tuo Zhao. First-order policy optimization for robust markov decision pr0cess. ArXiv:2209.10579, 2023.   \n[30]  Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimization. In Proceedingsof the International Conference on Neural Information Processing Systems (Neurips), pages 5569-5579, 2018.   \n[31]  Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, pages 6083-6093, 2020.   \n[32] Tien Mai and Patrick Jaillet. Robust entropy-regularized markov decision processes. ArXiv:2112.15364, 2021.   \n[33] Daniel J Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy Mann, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.   \n[34]  Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms for decentralized multi-agent reinforcement learning. In International Conference on Machine Learning, pages 15007-15049, 2022.   \n[35] Sobhan Miryoosefi, Kiante Brantley, Hal Daume, Miroslav Dudik, and Robert E Schapire. Reinforcement learning with convex constraints. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), pages 14093-14102, 2019.   \n[36]  Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition matrices. Operations Research, 53(5):780-798, 2005.   \n[37] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 3803-3810. IEEE, 2018.   \n[38]  Reazul Hasan Russel, Mouhacine Benosman, and Jeroen Van Baar. Robust constrained-mdps: Soft-constrained robust policy optimization under model uncertainty. ArXiv:2010.04870, 2020.   \n[39]  Stefan Schaal. Learning from demonstration. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), volume 9, 1996.   \n[40] Zhongchang Sun, Sihong He, Fei Miao,and Shaofeng Zou. Constrained reinforcment leaning under model mismatch. ArXiv:2405.01327, 2024.   \n[41] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[42]  Qiuhao Wang, Chin Pang Ho, and Marek Petrik. Policy gradient in robust mdps with global convergence guarantee. In Proceedings of the International Conference on Machine Learning (ICML), volume 202, pages 35763-35797, 23-29 Jul 2023.   \n[43] Yue Wang, Fei Miao, and Shaofeng Zou. Robust constrained reinforcement learning. ArXiv:2209.06866, 2022.   \n[44]  Yue Wang and Shaofeng Zou. Policy gradient method for robust reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML),pages 23484-23526, 2022.   \n[45] Wolfram Wiesemann, Daniel Kuhn, and Berg Rustem. Robust markov decision processes. Mathematics of Operations Research, 38(1):153-183, 2013.   \n[46] Lin Xiao. On the convergence rates of policy gradient methods. Journal of Machine Learning Research, 23(282):1-36, 2022.   \n[47]  Tesi Xiao, Krishna Balasubramanian, and Saeed Ghadimi. A projection-free algorithm for constrained stochastic multi-level composition optimization. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), 2022.   \n[48] Huan Xu and Shie Mannor. Parametric regret in uncertain markov decision processes. In Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference, pages 3606-3613. IEEE, 2009.   \n[49] Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive variance reduction. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.   \n[50] Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In International Conference on Artificial Intelligence and Statistics, pages 3332-3380, 2022.   \n[51]  Tom Zahavy, Brendan O'Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex mdps. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), volume 34, pages 25746-25759, 2021.   \n[52] Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning via distributional risk in the dual domain. ArXiv:2002.12475, 2020.   \n[53] Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Multi-agent reinforcement learning with general utilities via decentralized shadow reward actor-critic. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 9031-9039, 2022.   \n[54] Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), pages 4572-4583,2020.   \n[55] Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang. On the convergence and sample effciency of variance-reduced policy gradient method. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), 2021.   \n[56]  Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, Panganamala Kumar, and Chao Tian. Natural actor-critic for robust reinforcement learning with function approximation. In Proceedings of the International Conference on Neural Information Processing Systems (Neurips), 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A  Experiments 15   \nB Supporting Lemmas 16   \nC Stochastic Gradients 19   \nD  Proof of Proposition 1 20   \nProof of Proposition 2 21   \nF Proof of Proposition 3 21   \nG  Proof of Proposition 4 26   \nH Proof of Proposition 5 26   \n1  Proof of Proposition 6 27   \nProof of Proposition 7 28   \nK  Proof of Proposition 8 29   \nK.1 Proof of Assumptions 1, 2 and 3 . . . 29   \nK.2 Proof of Assumptions 5, 6 and 7 about ambiguity set \u4e09 . : 30   \nK.3 Proof of Assumptions 4 and 8 . . 30 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "L Proof of Proposition 9 33 ", "page_idx": 14}, {"type": "text", "text": "M Proof of Theorem 1 35 ", "page_idx": 14}, {"type": "text", "text": "rroo1 o1 1neorem 30   \nN.1  Convergence Rate of Inner Update Step (14) of the First Original Phase . . 36   \nN.2 Convergence Rate of $\\mathbb{E}[\\|\\nabla\\widetilde{\\Phi}(\\widetilde{\\xi})\\|^{2}]$ from the First Original Phase. . . 37   \nN.3  Convergence of the Inner Update Step (16) of the Second Corrected Phase . . 39   \nN.4  Convergence Rate of $\\mathbb{E}[\\|G_{b}^{(\\theta)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]$ 40   \nN.5 Convergence Rate of $\\mathbb{E}[\\|G_{a}^{(\\xi)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]$ . 42   \nN.6  Substituting Hyperparameters . . . : 43   \nO Proof of Theorem 3 46   \nO.1Analyze the $k$ -th Iteration. 47   \nO.2Obtain the Convergence Rate (25) . . 47 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "P Proof of Corollary 1 48 ", "page_idx": 14}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present simulation results of Algorithm 1 for convex utility. ", "page_idx": 14}, {"type": "text", "text": "Simulation Setting. We choose ${\\cal S}=\\{1,2,\\cdots,{\\cal S}\\}$ with $S\\,=\\,10$ states and ${\\mathcal{A}}=\\left\\{1,2,\\cdot\\cdot\\cdot,A\\right\\}$ with $A\\,=\\,5$ actions. The discount factor is $\\gamma\\,=\\,0.95$ and we select uniform distribution as the initial state distribution $\\rho$ . To optimize the objective function (2), we apply direct parameterization to policy parameter $\\theta_{s,a}\\;=\\;\\bar{\\pi}(a|s)\\;\\in\\;\\Theta\\;=\\;(\\Delta^{A})^{S}$ and transition kernel parameter $\\xi_{s,a,s^{\\prime}}\\,=$ $p(s^{\\prime}|s,a)\\,\\in\\,(\\Delta^{S})^{\\mathcal{S}\\times\\mathcal{A}}$ . In order to preserve $\\overline{{\\xi}}(:,:,s^{\\prime})\\,\\in\\,\\Delta^{s}$ , We select nominal kernel $\\overline{{\\xi}}(\\cdot,\\cdot,s^{\\prime})$ as Cs $\\frac{|10\\!+\\!\\varepsilon_{s^{\\prime}}|}{\\sum_{s^{\\prime}}|10\\!+\\!\\varepsilon_{s^{\\prime}}|}$ where $\\varepsilon_{s^{\\prime}}\\overset{\\mathrm{i.i.d}}{\\sim}\\mathcal{N}(0,1)$ for ach $s^{\\prime}\\in\\mathcal{S}$ Thenwesltslra $\\begin{array}{r}{r=0.01<\\operatorname*{min}_{s,a,s^{\\prime}}\\overline{{\\xi}}_{s,a,s^{\\prime}}}\\end{array}$ and use the $L^{2}$ ambiguity set $\\Xi:=\\{\\xi:\\|\\xi(s,:,:)-\\overline{{\\xi}}(s,:,:)\\|\\leq r\\}$ (for transition kernel) such that all $\\xi\\in\\Xi$ have all positive entries. As for the general utility function $f$ we use the following convex entropy function with application to exploration (Example 2.2 of [54]). ", "page_idx": 14}, {"type": "image", "img_path": "8Uyfr5TcNR/tmp/84c0ae3115257a988ae46fc30408629198706392b12abce11fcec5b38a4cdd6b.jpg", "img_caption": ["Figure 1: Numerical Experimental Result (the green vertical line denotes the transition from Phase I toPhase $\\mathrm{II}$ of Algorithm 1). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi}):=-\\sum_{s}\\lambda_{\\theta,\\xi}(s)\\log\\lambda_{\\theta,\\xi}(s)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\lambda_{\\theta,\\xi}(s):=\\sum_{a\\in\\mathcal{A}}\\lambda_{\\theta,\\xi}(s,a)}\\end{array}$ denotes the state visitation measure for any $s\\in{\\mathcal{S}},\\theta\\in\\Theta$ and $\\xi\\in\\Xi$ ", "page_idx": 15}, {"type": "text", "text": "Hyperparameters. For Algorithm 1, we use the following hyperparameters obtained from finetuning but not from Theorem 2: $K=200$ \uff0c $T=25$ \uff0c $K^{\\prime}=300$ \uff0c $T^{\\prime}=25$ $\\alpha=0.002$ $\\beta=0.001$ \uff0c \u03b1 = 0.002, b = 0.002, Lg,\u2264 = 20, m\u03b1) m(1) = 15, H H(1) = 100, m\u03b1) $m_{\\theta}^{(1)}=15$ = 15, H $H_{\\theta}^{(1)}=100$ = 100, m) $m_{\\lambda}^{(2)}=15$ $H_{\\lambda}^{(2)}=100,m_{\\xi}^{(2)}=15,H_{\\xi}^{(2)}=100,m_{\\lambda}^{(3)}=10,H_{\\lambda}^{(3)}=100,m_{\\xi}^{(3)}=1$ H(3) 100,m> $m_{\\lambda}^{(4)}=10$ $H_{\\lambda}^{(4)}=100$ =100,mg $m_{\\theta}^{(4)}=10$ \uff0c $H_{\\theta}^{(4)}=100$ ", "page_idx": 15}, {"type": "text", "text": "Environment. The experiment is implemented on Python 3.8 on AMD EPYC-7313 CPU with 3.00GHz, which costs about 1.5 hours in total. ", "page_idx": 15}, {"type": "text", "text": "Results. The numerical result of Algorithm 1 is shown in Figure 1. Here the y-axis is the norm of the true projected gradient $\\sqrt{\\|G_{b}^{(\\theta)}(\\theta_{k},\\xi_{k})\\|^{2}+\\|G_{a}^{(\\xi)}(\\theta_{k},\\xi_{k})\\|^{2}}$ at each outer iteration $k$ of both phases of Algorithm 1 (separated by the green vertical dashed line), and the ${\\bf X}$ -axis is the sample complexity (i.e., the total number of generated samples up to iteration $k$ ). Figure 1 shows that the projected gradient decays and converges to a small value, which matches Theorem 2. ", "page_idx": 15}, {"type": "text", "text": "B Supporting Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 1. Under Assumption $^{\\,I}$ for any $s,s^{\\prime}\\in\\mathcal{S},\\,a\\in\\mathcal{A},\\,\\theta,\\theta^{\\prime}\\in\\Theta,\\,\\xi,\\xi^{\\prime}\\in\\Xi,$ wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\pi_{\\theta^{\\prime}}(a|s)-\\pi_{\\theta}(a|s)|\\le\\ell_{\\pi_{\\theta}}\\|\\theta^{\\prime}-\\theta\\|,\\quad|p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-p_{\\xi}(s^{\\prime}|s,a)|\\le\\ell_{p_{\\xi}}\\|\\xi^{\\prime}-\\xi\\|,}\\\\ &{\\left\\|\\nabla_{\\xi}p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-\\nabla_{\\xi}p_{\\xi}(s^{\\prime}|s,a)\\right\\|\\le\\left[L_{p_{\\xi}}p_{\\xi^{\\prime}}(s^{\\prime}|s,a)+\\ell_{p_{\\xi}}^{2}\\right]\\|\\xi^{\\prime}-\\xi\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Based on Assumption 1, the following inequalities holds for all $s,s^{\\prime}\\in\\mathcal{S}$ $a\\in{\\mathcal{A}}$ $\\theta\\:\\in\\:\\Theta$ $\\xi\\in\\Xi$ , which by Lagrange mean value theorem directly proves Eq. (30) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\theta}\\pi_{\\theta}(a|s)\\|\\le\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\|\\le\\ell_{\\pi_{\\theta}},\\ \\ \\|\\nabla_{\\xi}p_{\\xi}(s^{\\prime}|s,a)\\|\\le\\|\\nabla_{\\xi}\\log p_{\\xi}(s^{\\prime}|s,a)\\|\\le\\ell_{p_{\\xi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we prove Eq. (31) as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{\\xi}p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-\\nabla_{\\xi}p_{\\xi}(s^{\\prime}|s,a)\\right\\|}\\\\ &{=\\!\\!\\left\\|p_{\\xi^{\\prime}}(s^{\\prime}|s,a)\\nabla_{\\xi}\\log p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-p_{\\xi}(s^{\\prime}|s,a)\\nabla_{\\xi}\\log p_{\\xi}(s^{\\prime}|s,a)\\right\\|}\\\\ &{\\le\\!p_{\\xi^{\\prime}}(s^{\\prime}|s,a)\\!\\left\\|\\nabla_{\\xi}\\log p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-\\nabla_{\\xi}\\log p_{\\xi}(s^{\\prime}|s,a)\\right\\|}\\\\ &{\\quad+\\left|p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-p_{\\xi}(s^{\\prime}|s,a)\\right|\\!\\left\\|\\nabla_{\\xi}\\log p_{\\xi}(s^{\\prime}|s,a)\\right\\|}\\\\ &{\\overset{(i)}{\\le}\\!p_{\\xi^{\\prime}}(s^{\\prime}|s,a){\\cal L}_{p_{\\xi}}\\|\\xi^{\\prime}-\\xi\\|+\\ell_{p_{\\xi}}\\cdot\\ell_{p_{\\xi}}\\|\\xi^{\\prime}-\\xi\\|}\\\\ &{\\le\\!\\left[{\\cal L}_{p_{\\xi}}p_{\\xi^{\\prime}}(s^{\\prime}|s,a)+{\\ell}_{p_{\\xi}}^{2}\\right]\\|\\xi^{\\prime}-\\xi\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (i) uses Eq. (30) and Assumption 1. ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. For any $\\theta\\in\\Theta$ and $\\xi\\in\\Xi,$ the occupancy measure $\\lambda_{\\theta,\\xi}$ defined by Eq. (1) is the unique solution to the following Bellman equation of $\\lambda\\in\\mathbb{R}^{|S|\\times|A|}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda(s^{\\prime},a^{\\prime})=\\Bigl[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda(s,a)p_{\\xi}(s^{\\prime}|s,a)\\Bigr]\\pi_{\\theta}(a^{\\prime}|s^{\\prime}),\\quad s^{\\prime}\\in S,a^{\\prime}\\in\\cal A.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, th state ccupanymes $\\begin{array}{r}{\\lambda_{\\theta,\\xi}(s):=\\sum_{a\\in\\mathcal{A}}\\lambda_{\\theta,\\xi}(s,a)}\\end{array}$ satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{\\theta,\\xi}(s,a)=\\lambda_{\\theta,\\xi}(s)\\pi_{\\theta}(a|s).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. First, we can prove that $\\lambda_{\\theta,\\xi}$ satisfies Eq. (32) as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda_{\\theta,\\epsilon}(\\theta^{'},\\alpha^{'})=(1-\\gamma)\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{[0}\\nu_{\\theta,\\epsilon})(s-s^{\\prime},a_{\\alpha}=a^{\\prime}|s_{0}-\\rho)}\\\\ {=\\pi_{\\theta}(a^{\\prime}|s^{\\prime}|)(1-\\gamma)\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{[0}\\nu_{\\theta,\\epsilon}(s-s^{\\prime}|s_{0}-\\rho)}\\\\ {=\\pi_{\\theta}(a^{\\prime}|s^{\\prime}|)(1-\\gamma)\\displaystyle\\sum_{t=s}^{\\infty}\\gamma^{[0}\\nu_{\\theta,\\epsilon}(s)=\\gamma^{[1}\\omega^{-1}\\gamma^{-1}\\mathbb{P}_{s\\geq r_{0}}(s_{1}+a)=s^{\\prime}|s_{0}-\\rho)]}\\\\ {=\\pi_{\\theta}(a^{\\prime}|s^{\\prime}|)(1-\\gamma)\\displaystyle\\sum_{t=s}^{\\infty}\\gamma\\displaystyle\\sum_{s=0}^{s+1}\\gamma\\sum_{u=s}^{\\infty}\\gamma^{[1}\\nu_{u,\\epsilon}(s)=(s^{\\prime}|s_{0}-\\rho)]}\\\\ {=\\pi_{\\theta}(a^{\\prime}|s^{\\prime}|)(1-\\gamma)\\displaystyle\\left(\\gamma^{(1)}+\\gamma\\displaystyle\\sum_{s=0}^{s+1}\\gamma\\displaystyle\\sum_{u=s}^{\\infty}\\gamma\\right.}\\\\ {=\\pi_{\\theta}(a^{\\prime}|s^{\\prime}|)(1-\\gamma)\\displaystyle\\sum_{t=s}^{\\infty}\\gamma\\displaystyle\\sum_{u=s}^{s}\\gamma(s^{\\prime}|s_{u})\\right.}\\\\ {\\ }&{\\quad\\left.\\left((1-\\gamma)\\displaystyle\\sum_{t=s}^{s}\\gamma^{[0}\\nu_{\\theta,\\epsilon}(s-s,a_{\\alpha}=a^{\\prime}|s_{0}-\\rho)\\right)\\right)\\right.}\\\\ {=\\left[(1-\\gamma)\\displaystyle\\sum_{t=s}^{s}\\gamma^{[0}\\nu_{\\theta,\\epsilon}(s-s,a_{\\alpha}=a^{\\prime}|s_{0})\\right]\\nu_{\\theta}(a^{\\prime}|s^{\\prime},a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we prove the uniqueness. Suppose $\\lambda_{1},\\lambda_{2}\\in\\mathbb{R}^{|S|\\times|A|}$ satisfies Eq. (32). Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s^{\\prime},a^{\\prime}}|\\lambda_{2}(s^{\\prime},a^{\\prime})-\\lambda_{1}(s^{\\prime},a^{\\prime})|=\\displaystyle\\sum_{s^{\\prime},a^{\\prime}}\\gamma\\pi_{\\theta}(a^{\\prime}|s^{\\prime})\\Big|\\sum_{s,a}[\\lambda_{2}(s,a)-\\lambda_{1}(s,a)]p_{\\xi}(s^{\\prime}|s,a)\\Big|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\gamma\\displaystyle\\sum_{s^{\\prime}}\\sum_{s,a}|\\lambda_{2}(s,a)-\\lambda_{1}(s,a)|p_{\\xi}(s^{\\prime}|s,a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\gamma\\displaystyle\\sum_{s,a}|\\lambda_{2}(s,a)-\\lambda_{1}(s,a)|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Which implies that $\\begin{array}{r}{(1-\\gamma)\\sum_{s,a}|\\lambda_{2}(s,a)-\\lambda_{1}(s,a)|\\leq0}\\end{array}$ and thus $\\lambda_{2}=\\lambda_{1}$ , .e, the solution to Eq.   \n(32) is unique. ", "page_idx": 17}, {"type": "text", "text": "Finally, we will prove Eq. (33). Note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda_{\\theta,\\xi}(s)=\\sum_{a\\in\\cal A}\\lambda_{\\theta,\\xi}(s,a)\\overset{(i)}{=}(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda(s,a)p_{\\xi}(s^{\\prime}|s,a),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (i) uses Eq. (32). Then Eq. (33) can be proved by substituting the above equality into Eq. (32). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. Under Assumption $^{\\,I}$ the occupancy measure (1) satisfies the following Lipschitz propertiesforany $\\theta,\\theta^{\\prime}\\in\\Theta$ and $\\xi,\\xi^{\\prime}\\in\\Xi$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\lambda_{\\theta^{\\prime},\\xi^{\\prime}}-\\lambda_{\\theta,\\xi}\\|\\leq\\frac{\\gamma\\|p_{\\xi^{\\prime}}-p_{\\xi}\\|+\\|\\pi_{\\theta^{\\prime}}-\\pi_{\\theta}\\|}{1-\\gamma}\\leq\\frac{\\gamma\\ell_{p_{\\xi}}\\sqrt{|\\mathcal{S}|}\\|\\xi^{\\prime}-\\xi\\|+\\ell_{\\pi_{\\theta}}\\sqrt{|\\mathcal{A}|}\\|\\theta^{\\prime}-\\theta\\|}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. For any $\\theta,\\theta^{\\prime}\\in\\Theta$ and $\\xi,\\xi^{\\prime}\\in\\Xi$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{v}_{i}\\approx-\\frac{1}{w_{0}(\\tau)}}\\\\ &{=\\sqrt{\\frac{1}{w_{0}(\\tau)}\\left[w_{0}(\\tau)-w_{i}(\\tau)w_{i}(\\tau)\\right]^{2}}}\\\\ &{\\lesssim[(1-\\gamma)w_{0}(\\tau)]^{1/2}\\sum_{k=1}^{\\infty}\\mathrm{t}_{2}(s_{k}(s_{k})[w_{k}(\\tau),u_{i}])}\\\\ &{~~~~+\\left((1-\\gamma)w_{0}(\\tau)+\\sum_{k=1}^{\\infty}\\mathrm{t}_{2}(s_{k})[w_{k}(\\tau),u_{i}]\\right)[w_{k}(\\tau)-w_{i}(\\tau)]^{1/2}}\\\\ &{~~~~\\times\\sqrt{\\frac{1}{w_{0}(\\tau)}\\left[w_{0}(\\tau)\\right]^{1/2}}\\sum_{k=1}^{\\infty}\\mathrm{t}_{2}(s_{k})[w_{k}(\\tau),u_{i}])[w_{k}(\\tau)}\\\\ &{~~~~+\\sqrt{\\frac{1}{w_{0}(\\tau)}\\left[w_{0}(\\tau)\\right]^{1/2}}\\sum_{k=1}^{\\infty}\\mathrm{t}_{2}(s_{k})[w_{k}(\\tau),u_{i}])^{1/2}}\\\\ &{~~~~~\\times\\sqrt{\\frac{1}{w_{0}(\\tau)}\\left[w_{0}(\\tau)\\right]^{1/2}}\\sum_{k=1}^{\\infty}\\mathrm{t}_{2}(s_{k})[w_{k}(\\tau),u_{i}])[w_{k}(\\tau)-w_{i}(\\tau)]^{1/2}}\\\\ &{~~~~\\times\\sqrt{\\frac{1}{w_{0}(\\tau)}\\left[(1-\\gamma)w_{0}(\\tau)+\\sum_{k=1}^{\\infty}\\mathrm{t}_{2}(s_{k})[w_{k}(\\tau),u_{i}])[w_{k}(\\tau)-w_{i}(\\tau)]^{1/2}}}\\\\ &{~~~~\\times\\sqrt{\\frac{1}{w_{0}(\\tau)}\\left[w_{0}(\\tau)-w_{i}(\\tau)\\right]^{1/2}}}\\\\ &{~~~~~+\\sqrt{\\frac{1}{w_{0}(\\tau)}\\left[w_{0}(\\tau)\\right]^{1/2}}\\sum_{k=1}^\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i v)}{\\le}\\gamma\\ell_{p_{\\xi}}\\sqrt{|{\\cal S}|}\\|\\xi^{\\prime}-\\xi\\|+\\gamma\\sqrt{\\displaystyle\\sum_{s^{\\prime}}\\sum_{s,a}p_{\\xi}(s^{\\prime}|s,a)|\\lambda_{\\theta^{\\prime},\\xi^{\\prime}}(s,a)-\\lambda_{\\theta,\\xi}(s,a)|^{2}}+\\ell_{\\pi_{\\theta}}\\sqrt{|{\\cal A}|}\\|\\theta^{\\prime}-\\theta\\|}\\\\ &{\\le\\gamma\\ell_{p_{\\xi}}\\sqrt{|{\\cal S}|}\\|\\xi^{\\prime}-\\xi\\|+\\gamma\\|\\lambda_{\\theta^{\\prime},\\xi^{\\prime}}-\\lambda_{\\theta,\\xi}\\|+\\ell_{\\pi_{\\theta}}\\sqrt{|{\\cal A}|}\\|\\theta^{\\prime}-\\theta\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (i) uses Eq. (32), (i) uses triangular inequality, (i) uses Lemma 1, $\\begin{array}{r}{\\sum_{a^{\\prime}}\\pi_{\\theta^{\\prime}}(a^{\\prime}|s^{\\prime})^{2}\\leq1}\\end{array}$ and $\\begin{array}{r}{\\sum_{s^{\\prime}}[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda_{\\theta,\\xi}(s,a\\bar{)}p_{\\xi}(s^{\\prime}|s,\\bar{a})]=1}\\end{array}$ and (iv) uses Jensen's inequality. Then Eq. (34) can be proved by rearranging the above inequality. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. The distance between any pair of probability vectors $x,y\\in\\Delta^{x}$ on finite space $\\mathcal{X}$ has the upper bound that $\\|x-y\\|\\leq{\\sqrt{2}}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Denote $d=|\\mathcal{X}|$ and $x_{j},y_{j}$ as the $j$ -th entry of $x,y$ respectively. Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|x-y\\|^{2}\\leq\\sum_{j=1}^{d}x_{j}^{2}+y_{j}^{2}-2x_{j}y_{j}\\leq\\sum_{j=1}^{d}x_{j}+y_{j}=2.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 5. Under Assumptions $^{\\,l}$ -2, the projected gradients in (13) have the following properties. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|\\leq\\|\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\|\\leq\\ell_{\\theta},\\quad\\|G_{\\beta}^{(\\xi)}(\\theta,\\xi)\\|\\leq\\|\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})\\|\\leq\\ell_{\\xi},}\\\\ &{\\|G_{\\beta}^{(\\theta)}(\\theta^{\\prime},\\xi^{\\prime})-G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|\\leq\\Big(\\frac{1}{\\beta}+L_{\\theta,\\theta}\\Big)\\|\\theta^{\\prime}-\\theta\\|+L_{\\theta,\\xi}\\|\\xi^{\\prime}-\\xi\\|,}\\\\ &{\\|G_{\\alpha}^{(\\xi)}(\\theta^{\\prime},\\xi^{\\prime})-G_{\\alpha}^{(\\xi)}(\\theta,\\xi)\\|\\leq L_{\\xi,\\theta}\\|\\theta^{\\prime}-\\theta\\|+\\Big(\\frac{1}{\\alpha}+L_{\\xi,\\xi}\\Big)\\|\\xi^{\\prime}-\\xi\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The proof for $\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|$ in Eq. (35) simply follows from the contraction property of projection as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|:=\\frac{1}{\\beta}\\|\\theta-\\mathrm{proj}_{\\Theta}\\big(\\theta-\\beta\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\big|\\big|\\le\\frac{1}{\\beta}\\big\\|\\theta-\\big(\\theta-\\beta\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\big)\\big\\|=\\|\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, $\\|\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\|\\leq\\ell_{\\theta}$ by Proposition 3. The proof logic for $\\|G_{\\beta}^{(\\xi)}(\\theta,\\xi)\\|$ is the same. Next, we prove Eq. (36) as follows and the proof of Eq. (37) follows the same logic ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|G_{\\beta}^{(\\theta)}(\\theta^{\\prime},\\xi^{\\prime})-G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|=\\displaystyle\\frac{1}{\\beta}\\|\\mathrm{proj}_{\\Theta}[\\theta^{\\prime}-\\beta\\nabla_{\\theta}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})]-\\mathrm{proj}_{\\Theta}[\\theta-\\beta\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})]\\|}&{}\\\\ {\\le\\displaystyle\\frac{1}{\\beta}\\|\\theta^{\\prime}-\\theta\\|+\\|\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})-\\nabla_{\\theta}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})\\|}&{}\\\\ {\\overset{(i)}{\\le}\\displaystyle\\Big(\\frac{1}{\\beta}+L_{\\theta,\\theta}\\Big)\\|\\theta^{\\prime}-\\theta\\|+L_{\\theta,\\xi}\\|\\xi^{\\prime}-\\xi\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) uses Proposition 3. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6. Suppose $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is a closed convex set. For any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $x^{\\prime}\\in\\mathcal{X}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n[x^{\\prime}-\\mathrm{proj}_{X}(x)]^{\\top}[x-\\mathrm{proj}_{X}(x)]\\leq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For any $\\delta~\\in~(0,1]$ $x_{\\delta}\\;:=\\;\\delta x^{\\prime}+(1\\,-\\,\\delta)\\mathrm{proj}_{\\mathcal{X}}(x)$ belongs to the convex set $\\mathcal{X}$ since $x^{\\prime},\\operatorname{proj}_{\\mathcal{X}}(x)\\in\\mathcal{X}$ . Then based on the definition of projection we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\|x_{\\delta}-x\\|^{2}-\\|\\mathrm{proj}_{\\mathcal{X}}(x)-x\\|^{2}}\\\\ &{\\quad\\leq\\|x_{\\delta}-\\mathrm{proj}_{\\mathcal{X}}(x)\\|^{2}-2[x_{\\delta}-\\mathrm{proj}_{\\mathcal{X}}(x)]^{\\top}[x-\\mathrm{proj}_{\\mathcal{X}}(x)]}\\\\ &{\\quad=\\delta^{2}\\|x^{\\prime}-\\mathrm{proj}_{\\mathcal{X}}(x)\\|^{2}-2\\delta[x^{\\prime}-\\mathrm{proj}_{\\mathcal{X}}(x)]^{\\top}[x-\\mathrm{proj}_{\\mathcal{X}}(x)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above inequality can be rearranged as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n[x^{\\prime}-\\mathrm{proj}_{\\chi}(x)]^{\\top}[x-\\mathrm{proj}_{\\chi}(x)]\\leq{\\frac{\\delta}{2}}\\|x^{\\prime}-\\mathrm{proj}_{\\chi}(x)\\|^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which proves Eq. (39) as $\\delta\\rightarrow+0$ ", "page_idx": 18}, {"type": "text", "text": "C Stochastic Gradients ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To get the stochastic estimation of the gradients (8) and (9), we first estimate the occupancy measure (1) as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\lambda}(\\tau^{(\\lambda)};s,a):=\\!\\frac{1-\\gamma}{m_{\\lambda}}\\sum_{i=1}^{m_{\\lambda}}\\sum_{h=0}^{H_{\\lambda}-1}\\gamma^{h}\\mathbb{1}\\{s_{i,h}^{(\\lambda)}=s,a_{i,h}^{(\\lambda)}=a\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbb{1}\\{\\cdot\\}$ is an indicator function and $\\tau^{(\\lambda)}:=\\{\\tau_{i}^{(\\lambda)}\\}_{i=1}^{m_{\\lambda}}$ contains $m_{\\lambda}$ independent trajectories $\\tau_{i}^{(\\lambda)}:=\\{s_{i,h}^{(\\lambda)},a_{i,h}^{(\\lambda)}\\}_{h=0}^{H_{\\lambda}-1}\\,(i=1,\\ldots,m_{\\lambda})$ of lenght $H_{\\lambda}$ $\\pi_{\\theta}$ kernel $p_{\\xi}$ . Then the estimated cost function is $\\widehat{c}:=\\nabla_{\\lambda}f[\\widehat{\\lambda}(\\tau^{(\\lambda)})]$ ", "page_idx": 18}, {"type": "text", "text": "1: Input: $z:=(\\theta,\\xi)\\in\\mathcal{Z}:=\\Theta\\times\\mathcal{Z}$ $m_{\\lambda},H_{\\lambda},m_{\\theta},H_{\\theta},m_{\\xi},H_{\\xi}.$ $\\tau_{i}^{(\\lambda)}:=\\{s_{i,h}^{(\\bar{\\lambda})},a_{i,h}^{(\\lambda)}\\}_{h=0}^{H_{\\lambda}-1}$ $(i=1,\\dotsc,m_{\\lambda})$ $\\pi_{\\boldsymbol{\\theta},\\,p_{\\xi}}$   \n4: Obtain $\\widehat{\\lambda}(\\tau^{(\\lambda)};s,a)$ for every $s,a\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ by Eq. (40) with $\\tau^{(\\lambda)}:=\\{\\tau_{i}^{(\\lambda)}\\}_{i=1}^{m_{\\lambda}}$   \n5: Obtain $\\widehat{c}:=\\nabla_{\\lambda}f[\\widehat{\\lambda}(\\tau^{(\\lambda)})]$   \n6: Generate independent rajectories T) := (s,h, a, h- sh, alh)He-1 (i = , .,ma) from To, e$\\boldsymbol{g}^{(\\theta)}(\\tau^{(\\theta)},\\theta,\\xi,\\widehat{c})$ $\\tau^{(\\theta)}:=\\{\\tau_{i}^{(\\theta)}\\}_{i=1}^{m_{\\theta}}$ $\\tau_{i}^{(\\xi)}:=\\{s_{i,h}^{(\\xi)},a_{i,h}^{(\\xi)}\\}_{h=0}^{H_{\\xi}-1}\\,(i=1,\\ldots,m_{\\xi})\\,\\mathrm{from}\\,\\tau_{\\theta},j=1,\\ldots,m_{\\theta}\\,.$   \n9: Obtain $g^{(\\xi)}(\\tau^{(\\xi)},\\theta,\\xi,\\widehat{c})$ by Eqg (42) with $\\tau^{(\\xi)}:=\\{\\tau_{i}^{(\\xi)}\\}_{i=1}^{m_{\\xi}}$   \n10: Output: $g^{(\\theta)}(\\tau^{(\\theta)},\\theta,\\xi,\\widehat{c})\\approx\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})$ \uff0c $g^{(\\xi)}(\\tau^{(\\xi)},\\theta,\\xi,\\widehat{c})\\approx\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})$ ", "page_idx": 19}, {"type": "text", "text": "The stochastic gradients (8) and (9) can be approximated respectively by the following stochastic sample averaged values known as GPOMDP [50]. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{(\\theta)}(\\tau^{(\\theta)},\\theta,\\xi,\\widehat{c})=\\displaystyle\\frac{1}{m_{\\theta}}\\sum_{i=1}^{m_{\\theta}}\\left[\\sum_{t=0}^{H_{\\theta}-1}\\gamma^{t}\\widehat{c}(s_{i,t}^{(\\theta)},a_{i,t}^{(\\theta)})\\sum_{h=0}^{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{i,h}^{(\\theta)}\\mid s_{i,h}^{(\\theta)})\\right],}\\\\ &{g^{(\\xi)}(\\tau^{(\\xi)},\\theta,\\xi,\\widehat{c})=\\displaystyle\\frac{1}{m_{\\lambda}}\\sum_{i=1}^{m_{\\lambda}}\\left[\\sum_{t=0}^{H_{\\lambda}-1}\\gamma^{t}\\widehat{c}(s_{i,t}^{(\\xi)},a_{i,t}^{(\\xi)})\\sum_{h=0}^{t}\\nabla_{\\xi}\\log p_{\\xi}(s_{i,h+1}^{(\\xi)}\\mid s_{i,h}^{(\\xi)},a_{i,h}^{(\\xi)})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where $\\tau^{(\\theta)}~:=~\\{\\tau_{i}^{(\\theta)}\\}_{i=1}^{m_{\\theta}}$ and $\\tau^{(\\xi)}~:=~\\{\\tau_{i}^{(\\xi)}\\}_{i=1}^{m_{\\xi}}$ contain $m_{\\theta}$ independent rjectories $\\tau_{i}^{\\left(\\theta\\right)}\\;:=\\;$ (=,,m\uff09andm inependent trajectories $\\tau_{i}^{(\\xi)}\\ :=\\ \\{s_{i,h}^{(\\xi)},a_{i,h}^{(\\xi)}\\}_{h=0}^{H_{\\xi}-1}\\ \\cup$ $\\{s_{i,H_{\\xi}}^{(\\xi)}\\}\\,(i=1,\\dots,m_{\\xi})$ respectively, both generated from the policy $\\pi_{\\theta}$ and transition kernel $p_{\\xi}$ ", "page_idx": 19}, {"type": "text", "text": "We summarize the procedure of obtaining the stochastic gradients (8) and (9) in Algorithm 3. These stochastic gradients approximate the true gradients with the following error bounds. ", "page_idx": 19}, {"type": "text", "text": "Proposition 9. Under Assumptions $^{\\,l}$ and 2, the stochastic gradients (41) and (42) have the following errorbounds. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\|g^{(\\theta)}(\\tau^{(\\theta)},\\theta,\\xi,\\widehat{c})-\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\|^{2}}\\\\ &{\\le\\displaystyle\\frac{3\\ell_{\\pi_{\\theta}}^{2}}{(1-\\gamma)^{4}}\\Big[L_{\\lambda}^{2}|{\\cal S}||{\\cal A}|\\Big(\\frac{1}{m_{\\lambda}}+\\gamma^{2H_{\\lambda}}\\Big)+\\frac{\\ell_{\\lambda}^{2}}{m_{\\theta}}+\\ell_{\\lambda}^{2}[1+H_{\\theta}(1-\\gamma)]^{2}\\gamma^{2H_{\\theta}}\\Big],}\\\\ &{\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\|g^{(\\xi)}(\\tau^{(\\xi)},\\theta,\\xi,\\widehat{c})-\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})\\|^{2}}\\\\ &{\\le\\displaystyle\\frac{3\\ell_{p_{\\xi}}^{2}}{(1-\\gamma)^{4}}\\Big[L_{\\lambda}^{2}|{\\cal S}||{\\cal A}|\\Big(\\frac{1}{m_{\\lambda}}+\\gamma^{2H_{\\lambda}}\\Big)+\\frac{\\ell_{\\lambda}^{2}}{m_{\\xi}}+\\ell_{\\lambda}^{2}[1+H_{\\xi}(1-\\gamma)]^{2}\\gamma^{2H_{\\xi}}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "DProof of Proposition 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As follows, we slightlyrewrite the utility function $f$ defined in Eq. (5), by replacing $\\lambda$ with $\\lambda_{\\theta,\\xi}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(\\lambda_{\\theta,\\xi})=\\left\\{\\begin{array}{l l}{\\langle c^{(0)},\\lambda_{\\theta,\\xi}\\rangle,}\\\\ {+\\infty,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, for any $\\theta\\in\\Theta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})=\\left\\{\\begin{array}{l l}{\\operatorname*{max}_{\\xi\\in\\Xi}\\langle c^{(0)},\\lambda_{\\theta,\\xi}\\rangle,}\\\\ {+\\infty,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle c^{(k)},\\lambda_{\\theta,\\xi}\\rangle\\leq\\tau_{k}\\;{\\mathrm{for~all}}\\;\\xi\\in\\Xi\\;{\\mathrm{and}}\\;k=1,\\ldots,K\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recaling thedefnitionof therbust value function (3), i.e. $V_{\\theta}^{(k)}\\ \\ \\stackrel{\\mathrm{def}}{=}\\ \\operatorname*{max}_{\\xi\\in\\Xi}\\langle c^{(k)},\\lambda_{\\theta,\\xi}\\rangle$ the equation above can be rewritten as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})=\\left\\{V_{\\theta}^{(0)},\\atop+\\infty,\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, $\\theta\\:\\in\\:\\Theta$ minimizes $\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ if and only if $\\theta$ solves the constrained robust RL problem (4), as repeated below. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}V_{\\theta}^{(0)},\\;\\mathrm{s.t.}\\;V_{\\theta}^{(k)}\\leq\\tau_{k}\\;\\mathrm{for}\\;\\mathrm{all}\\;k=1,\\ldots,K.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we will prove that $f(\\lambda)$ defined in Eq. (5) is a convex function. Note that $A_{k}=\\{\\lambda\\in\\Delta^{s\\times A}$ $\\langle c^{(k)},\\lambda\\rangle\\leq\\tau_{k}\\}$ is a convex set, so $A=\\cap_{k=1}^{K}A_{k}$ is also a convex set. Then for any $\\lambda_{1},\\lambda_{0}\\in\\Delta^{S\\times A}$ and $\\alpha\\in[0,1]$ , we aim to prove that ", "page_idx": 20}, {"type": "equation", "text": "$$\nf[\\alpha\\lambda_{1}+(1-\\alpha)\\lambda_{0}]\\leq\\alpha f(\\lambda_{1})+(1-\\alpha)f(\\lambda_{0}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If either $\\lambda_{1}\\notin A$ or $\\lambda_{0}\\notin A$ , then Eq. (45) obviously holds as the right side equals $+\\infty$ Otherwise, if $\\lambda_{1},\\lambda_{0}\\in A$ , then $\\delta\\lambda_{1}+(1-\\delta)\\lambda_{0}\\in A$ as $A$ is a convex set, and thus Eq. (45) holds with equality as proved below. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f[\\delta\\lambda_{1}+(1-\\delta)\\lambda_{0}]=\\langle c^{(0)},\\delta\\lambda_{1}+(1-\\delta)\\lambda_{0}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\delta\\langle c^{(0)},\\lambda_{1}\\rangle+(1-\\delta)\\langle c^{(0)},\\lambda_{0}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\delta f(\\lambda_{1})+(1-\\delta)f(\\lambda_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "E Proof of Proposition 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The utility function $f$ in Eq. (7) satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{f(\\lambda_{\\theta,\\xi})=\\sum_{s,a}\\lambda_{\\theta,\\xi}(s,a)\\Big[c(s,a)+\\mu\\log\\frac{\\lambda_{\\theta,\\xi}(s,a)}{\\sum_{a^{\\prime}}\\lambda_{\\theta,\\xi}(s,a^{\\prime})}\\Big]}}\\\\ &{}&{\\stackrel{(i)}{=}\\sum_{s,a}\\big[\\lambda_{\\theta,\\xi}(s,a)c(s,a)\\big]+\\mu\\sum_{s,a}\\lambda_{\\theta,\\xi}(s)\\pi_{\\theta}(a|s)\\log\\pi_{\\theta}(a|s)}\\\\ &{}&{\\stackrel{(i i)}{=}\\sum_{s,a}\\big[\\lambda_{\\theta,\\xi}(s,a)c(s,a)\\big]-\\mu\\sum_{s}\\big[\\lambda_{\\theta,\\xi}(s)\\mathcal{H}[\\pi_{\\theta}(\\cdot|s)]\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (i) uses $\\begin{array}{r}{\\lambda_{\\theta,\\xi}(s)\\,=\\,\\sum_{a}\\lambda_{\\theta,\\xi}(s,a)}\\end{array}$ and Eq.(33) that $\\lambda_{\\theta,\\xi}(s,a)\\,=\\,\\lambda_{\\theta,\\xi}(s)\\pi_{\\theta}(a|s)$ ,and (i) denotes the entropy function that $\\begin{array}{r}{\\mathcal{\\dot{H}}[\\pi_{\\theta}(\\cdot|s)]\\,=\\,-\\sum_{a}\\pi_{\\theta}(a|s)\\log\\pi_{\\theta}(a|s)}\\end{array}$ . The above function is exactly the minimax objective function (6) of the robust entropy regularized RL. ", "page_idx": 20}, {"type": "text", "text": "Finally, we will prove that $f(\\lambda)$ defined in Eq. (7) is a convex function. For any $\\lambda_{0},\\lambda_{1}\\in\\Delta^{S\\times A}$ and $\\delta\\in[0,1]$ , denote $\\lambda_{\\delta}=\\delta\\lambda_{1}+(1-\\delta)\\lambda_{0}$ \uff0c $\\begin{array}{r}{\\lambda_{\\delta}(s)=\\sum_{a}\\lambda_{\\delta}(s,a)}\\end{array}$ and policy $\\begin{array}{r}{\\pi_{\\delta}(a|s)=\\frac{\\lambda_{\\delta}(s,a)}{\\lambda_{\\delta}(s)}}\\end{array}$ \u5165s(s.a) Then, the convexity of $f$ can be proved asfollows. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\delta f(\\lambda_{1})+(1-\\delta)f(\\lambda_{0})-f(\\lambda_{\\delta})}\\\\ &{=\\mu\\displaystyle\\sum_{s,a}\\Big[\\delta\\lambda_{1}(s,a)\\log\\frac{\\lambda_{1}(s,a)}{\\lambda_{1}(s)}+(1-\\delta)\\lambda_{0}(s,a)\\log\\frac{\\lambda_{0}(s,a)}{\\lambda_{0}(s)}-\\lambda_{\\delta}(s,a)\\log\\frac{\\lambda_{\\delta}(s,a)}{\\lambda_{\\delta}(s)}\\Big]}\\\\ &{=\\mu\\displaystyle\\sum_{s,a}\\Big[\\delta\\lambda_{1}(s,a)\\log\\pi_{1}(a|s)+(1-\\delta)\\lambda_{0}(s,a)\\log\\pi_{0}(a|s)}\\\\ &{\\quad-\\left[\\delta\\lambda_{1}(s,a)+(1-\\delta)\\lambda_{0}(s,a)\\right]\\log\\pi_{\\delta}(a|s)\\Big]}\\\\ &{=\\mu\\displaystyle\\sum_{s,a}\\Big[\\delta\\lambda_{1}(s)\\pi_{1}(a|s)\\log\\frac{\\pi_{1}(a|s)}{\\pi_{\\delta}(a|s)}+(1-\\delta)\\lambda_{0}(s)\\pi_{0}(a|s)\\log\\frac{\\pi_{0}(a|s)}{\\pi_{\\delta}(a|s)}\\Big]}\\\\ &{=\\mu\\displaystyle\\sum_{s}\\Big[\\delta\\lambda_{1}(s)\\mathrm{KL}[\\pi_{1}(\\cdot|s)]\\pi_{\\delta}(\\cdot|s)\\Big]+(1-\\delta)\\lambda_{0}(s)\\mathrm{KL}[\\pi_{0}(\\cdot|s)]\\pi_{\\delta}(\\cdot|s)\\Big]\\Big[\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "F Proof of Proposition 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The first formula of Eq. (10) can be proved as follows and the second formula can be proved in the sameway. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\|\\overset{(i)}{\\le}\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left[\\sum_{t=0}^{+\\infty}\\gamma^{t}|c(s_{t},a_{t})|\\sum_{h=0}^{t}\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{h}|s_{h})\\|\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\stackrel{(i i)}{\\leq}\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left[\\overset{+\\infty}{\\underset{t=0}{\\sum}}\\gamma^{t}\\overset{t}{\\underset{h=0}{\\sum}}\\ell_{\\pi_{\\theta}}\\right]}\\\\ &{=\\ell_{\\pi_{\\theta}}\\overset{+\\infty}{\\underset{t=0}{\\sum}}\\gamma^{t}(t+1)=\\frac{\\ell_{\\pi_{\\theta}}}{(1-\\gamma)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (i) uses Eq. (41) and (i) uses $c(s_{t},a_{t})\\in[0,1]$ and Assumption 1.   \nDefine the following $\\mathrm{v}$ function. ", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{\\theta,\\xi}(c):=\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}c(s_{t},a_{t})\\Big|s_{0}\\sim\\rho\\Big].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For any fixed cost function $c:S\\times A\\rightarrow\\mathbb{R}$ , the gradient $\\nabla_{\\theta}V_{\\theta,\\xi}(c)$ can be rewritten as follows. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{\\theta}K_{\\theta,\\theta}(e)(\\frac{1}{\\theta})\\bar{\\theta}\\alpha_{R,\\theta}\\left[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma_{\\epsilon}(s_{i},a_{i})\\displaystyle\\sum_{l=0}^{\\infty}\\gamma_{\\theta}\\log_{\\theta}(a_{i}|s_{l})\\right]}&{}\\\\ {=}&{\\mathbb{E}_{\\theta,\\theta}\\left[\\displaystyle\\sum_{i=0}^{\\infty}\\gamma_{\\theta}\\log_{\\theta}(a_{i}|s_{l})\\right]\\displaystyle\\sum_{t=1}^{\\infty}\\gamma_{\\theta}\\gamma^{-t}\\kappa_{\\theta}(s_{i},a_{i})\\right]}\\\\ &{=}&{\\displaystyle\\sum_{k=0}^{\\infty}\\gamma_{\\theta}\\sum_{\\ell=1}^{\\infty}\\nabla_{\\theta}\\pi_{\\theta}(s_{i}-s_{i},a_{0}-a_{i})\\Theta_{\\theta}\\left[\\nabla_{\\theta}\\log_{\\theta}(a_{i}|s)\\right]}\\\\ &{\\quad\\displaystyle\\sum_{k=0}^{\\infty}\\gamma_{\\theta}\\left\\{\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{-k}(s_{i},a_{i})\\displaystyle\\sum_{j=1}^{\\infty}\\alpha_{S_{j}}a_{j}(a_{i}|s_{l})\\right.}\\\\ &{\\quad\\displaystyle=\\sum_{k=0}^{\\infty}\\displaystyle\\sum_{i=0}^{\\infty}\\gamma_{\\theta}\\exp_{\\theta}(b_{i}-s_{i},a_{i}-a_{i})\\Theta_{\\theta}\\left[\\exp_{\\theta}\\log_{\\theta}(a_{i}|s)\\right.}\\\\ &{\\quad\\displaystyle\\left.\\mathbb{E}_{\\theta,\\theta}\\left(\\displaystyle\\sum_{i=0}^{\\infty}\\gamma^{\\theta}(s_{i},a_{i})\\right)\\right]=0,}\\\\ {\\bar{\\theta}\\alpha_{1}-\\displaystyle\\sum_{j=1}^{\\infty}\\sum_{1=0}^{\\infty}\\lambda_{2}(s_{i},a_{j})\\Theta_{\\theta}\\left[\\exp_{\\theta}(a_{i}|s)\\right]\\Theta_{\\theta}\\left[\\exp_{\\theta}(a_{i}|s_{l})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (i) uses Eq. (5) of [6] and (i) uses the occupancy measure (1) and defines the following $\\mathrm{\\DeltaQ}$ function. ", "page_idx": 21}, {"type": "equation", "text": "$$\nQ_{\\theta,\\xi}(s,a;c):=\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left(\\sum_{t=0}^{+\\infty}\\gamma^{t}c(s_{t},a_{t})\\bigg\\vert s_{0}=s,a_{0}=a\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The above $\\mathrm{\\DeltaQ}$ function has the following upper bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n|Q_{\\theta,\\xi}(s,a;c)|\\leq\\frac{c_{\\operatorname*{max}}}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $c_{\\operatorname*{max}}:=\\operatorname*{max}_{s,a}|c(s,a)|$ and also satisfies the following Bellman equation. ", "page_idx": 21}, {"type": "equation", "text": "$$\nQ_{\\theta,\\xi}(s,a;c)=c(s,a)+\\gamma\\sum_{s^{\\prime},a^{\\prime}}p_{\\xi}(s^{\\prime}|s,a)\\pi_{\\theta}(a^{\\prime}|s^{\\prime})Q_{\\theta,\\xi}(s^{\\prime},a^{\\prime};c).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, for any $\\theta,\\theta^{\\prime}\\in\\Theta,\\xi,\\xi^{\\prime}\\in\\Xi$ and fixed cost function $c$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{s,a}|Q_{\\theta^{\\prime},\\xi^{\\prime}}(s,a;c)-Q_{\\theta,\\xi}(s,a;c)|}\\\\ &{\\le\\gamma\\displaystyle\\operatorname*{max}_{s,a}\\sum_{s^{\\prime},a^{\\prime}}|p_{\\xi^{\\prime}}(s^{\\prime}|s,a)-p_{\\xi}(s^{\\prime}|s,a)|\\pi_{\\theta^{\\prime}}(a^{\\prime}|s^{\\prime})|Q_{\\theta^{\\prime},\\xi^{\\prime}}(s^{\\prime},a^{\\prime};c)|}\\\\ &{\\quad+\\displaystyle\\gamma\\operatorname*{max}_{s,a}\\sum_{s^{\\prime},a^{\\prime}}p_{\\xi}(s^{\\prime}|s,a)|\\pi_{\\theta^{\\prime}}(a^{\\prime}|s^{\\prime})-\\pi_{\\theta}(a^{\\prime}|s^{\\prime})||Q_{\\theta^{\\prime},\\xi^{\\prime}}(s^{\\prime},a^{\\prime};c)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n+\\gamma\\operatorname*{max}_{s,a}\\sum_{s^{\\prime},a^{\\prime}}p_{\\xi}(s^{\\prime}|s,a)\\pi_{\\theta}(a^{\\prime}|s^{\\prime})|Q_{\\theta^{\\prime},\\xi^{\\prime}}(s^{\\prime},a^{\\prime};c)-Q_{\\theta,\\xi}(s^{\\prime},a^{\\prime};c)|\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i)}{\\le}\\frac{\\gamma c_{\\operatorname*{max}}}{1-\\gamma}\\big(\\displaystyle\\operatorname*{max}_{s,a}\\|p_{\\xi^{\\prime}}(\\cdot|s,a)-p_{\\xi}(\\cdot|s,a)\\|_{1}+\\displaystyle\\operatorname*{max}_{s}\\|\\pi_{\\theta^{\\prime}}(\\cdot|s)-\\pi_{\\theta}(\\cdot|s)\\|_{1}\\big)}\\\\ &{\\quad+\\gamma\\displaystyle\\operatorname*{max}_{s,a}|Q_{\\theta^{\\prime},\\xi^{\\prime}}(s,a;c)-Q_{\\theta,\\xi}(s,a;c)|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) uses Eq. (49). Rearranging the above inequality yields that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{s,a}{\\operatorname*{max}}\\,|Q_{\\theta^{\\prime},\\xi^{\\prime}}(s,a;c)-Q_{\\theta,\\xi}(s,a;c)|}\\\\ &{\\le\\!\\frac{\\gamma c_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\big(\\underset{s,a}{\\operatorname*{max}}\\,\\|p_{\\xi^{\\prime}}(\\cdot|s,a)-p_{\\xi}(\\cdot|s,a)\\|_{1}+\\underset{s}{\\operatorname*{max}}\\,\\|\\pi_{\\theta^{\\prime}}(\\cdot|s)-\\pi_{\\theta}(\\cdot|s)\\|_{1}\\big)}\\\\ &{\\overset{(i)}{\\le}\\!\\frac{\\gamma c_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\big(\\ell_{p_{\\xi}}|S|\\|\\xi^{\\prime}-\\xi\\|+\\ell_{\\pi_{\\theta}}|A|\\|\\theta^{\\prime}-\\theta\\|\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) uses Lemma 1. For any $\\theta\\in\\Theta,\\xi\\in\\Xi$ and fixed cost functions $c,c^{\\prime}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s,a}{\\operatorname*{max}}\\left|Q_{\\theta,\\xi}(s,a;c)-Q_{\\theta,\\xi}(s,a;c^{\\prime})\\right|\\overset{(i)}{\\le}\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left(\\left.\\underset{t=0}{\\overset{+\\infty}{\\sum}}\\gamma^{t}|c^{\\prime}(s_{t},a_{t})-c(s_{t},a_{t})|\\right|s_{0}=s,a_{0}=a\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\sum_{t=0}^{+\\infty}\\gamma^{t}\\Vert c^{\\prime}-c\\Vert_{\\infty}=\\frac{\\Vert c^{\\prime}-c\\Vert_{\\infty}}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) uses Eq. (48). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{v}:=\\mathbf{R}:=\\mathbf{0},\\quad\\forall\\in\\mathcal{V}_{r}[\\phi_{1},\\phi_{2}(\\mathbf{x}_{t})]}\\\\ &{:=\\mathbf{1}\\cdot\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}}\\\\ &{:=\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}}\\\\ &{\\qquad\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}}\\\\ &{\\qquad+\\frac{1}{1-\\mathbf{1}\\times\\mathbf{1}}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}\\mathbf{1}\\times\\mathbf{1}}\\\\ &{\\qquad+\\frac{1}{1-\\mathbf{1}\\times\\mathbf{1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\leq\\frac{\\gamma\\ell_{\\pi_{\\theta}}\\ell_{p_{\\xi}}\\sqrt{|\\mathcal{S}|}}{(1-\\gamma)^{3}}(L_{\\lambda}+2\\ell_{\\lambda}\\sqrt{|\\mathcal{S}||\\mathcal{A}|})\\|\\xi^{\\prime}-\\xi\\|+\\Big(\\frac{\\ell_{\\pi_{\\theta}}^{2}\\sqrt{|\\mathcal{A}|}(L_{\\lambda}+\\ell_{\\lambda}\\sqrt{|\\mathcal{S}||\\mathcal{A}|})}{(1-\\gamma)^{3}}+\\frac{L_{\\pi_{\\theta}}\\ell_{\\lambda}}{(1-\\gamma)^{2}}\\Big)\\|\\theta^{\\prime}-\\theta\\|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (i) uses the gradient (47), (ii) uses Assumptions 1-2 and Eqs. (49), (51) and (52) with $c_{\\operatorname*{max}}=\\|\\nabla_{\\lambda}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})\\|_{\\infty}$ replaced by its upper bound $\\ell_{\\lambda}$ , (ii) uses Assumption 2, (iv) uses Lemma 3 and (v) uses Assumption 1. ", "page_idx": 23}, {"type": "text", "text": "The proof of Eq. (12) follows the same logic. To elaborate, $\\nabla_{\\xi}V_{\\theta,\\xi}(c)$ can be derived as follows in a similar way to the derivation of Eq. (47) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{I}:=\\mathbb{E}_{\\pi,\\neq}\\left[\\displaystyle\\sum_{i=0}^{k-\\infty}\\gamma_{\\ell}^{i-1}e^{j_{\\ell}}(x_{i},a_{i})\\displaystyle\\sum_{h=0}^{k}\\nabla_{\\ell}\\log p_{\\ell}(s_{h+1}|s_{h},a_{h})\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{k=0}^{+\\infty}\\gamma_{k-\\ell}^{i}\\sum_{a_{i},a_{j}\\in\\mathcal{S}_{h}}(s_{h}-a_{i},s_{h+1}=s^{\\prime}|s_{0}-\\rho)\\nabla_{\\ell}\\log p_{\\ell}(s^{\\prime}|s,a)}\\\\ &{\\quad=\\displaystyle\\sum_{k=0}^{k}\\gamma_{\\ell-h}^{i-1}e^{j_{\\ell}-h}c_{(k},a_{i)}\\bigg\\lvert s_{h}=s,a_{h}=a,s_{h+1}=s^{\\prime}\\bigg)}\\\\ &{\\quad=\\displaystyle\\sum_{k=0}^{k}\\gamma_{\\ell-h}^{i-1}\\sum_{a_{i},a_{j}\\in\\mathcal{S}_{h}}(s_{h}-a_{i},s_{h}-a)p_{\\ell}(s^{\\prime}|s,a)\\nabla_{\\ell}\\log p_{\\ell}(s^{\\prime}|s,a)}\\\\ &{\\quad=\\displaystyle\\sum_{h=0}^{k}\\gamma_{\\ell-h}^{i-1}\\sum_{a_{i},a_{j}\\in\\mathcal{S}_{h}}(s_{h}-a,a_{h}-a)s_{\\ell}e^{j_{\\ell}}(s,a)\\nabla_{\\ell}\\log p_{\\ell}(s^{\\prime}|s,a)}\\\\ &{\\quad\\displaystyle\\mathbb{E}_{\\pi,a_{h}}\\left(c(s,a)+\\displaystyle\\sum_{i=1}^{k}\\gamma_{\\ell}(s_{i},a_{i})\\bigg\\lvert s_{0}-s,a_{0}=a,s_{1}=s^{\\prime}\\right)}\\\\ &{\\quad\\displaystyle\\bigoplus_{i=1}^{k}\\sum_{j\\in\\mathcal{S}_{h}}s_{\\ell}(s,a_{i},s_{j})\\nabla_{\\ell}\\xi_{\\ell}(s^{\\prime}|s,a)\\big\\lvert c(s,a)+\\gamma_{\\ell}\\xi_{\\ell}(s^{\\prime};e)\\big\\rvert,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (i) uses the occupancy measure (1) and defines the following $\\mathrm{v}$ function. ", "page_idx": 23}, {"type": "equation", "text": "$$\nV_{\\theta,\\xi}(s^{\\prime};c):=\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}c(s_{t},a_{t})\\Big|s_{0}=s^{\\prime}\\Big].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The above $\\mathrm{v}$ function has the following upper bound ", "page_idx": 23}, {"type": "equation", "text": "$$\n|V_{\\theta,\\xi}(s,a;c)|\\leq\\frac{c_{\\operatorname*{max}}}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $c_{\\operatorname*{max}}:=\\operatorname*{max}_{s,a}|c(s,a)|$ and also satisfies the following Bellman equation. ", "page_idx": 23}, {"type": "equation", "text": "$$\nV_{\\theta,\\xi}(s;c)=\\sum_{a}\\pi_{\\theta}(a|s)\\Big[c(s,a)+\\gamma\\sum_{s^{\\prime}}p_{\\xi}(s^{\\prime}|s,a)V_{\\theta,\\xi}(s^{\\prime};c)\\Big].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As a result, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{s}{\\mathrm{max}}\\left\\vert V_{\\theta^{\\prime},\\xi^{\\prime}}(s;c)-V_{\\theta,\\xi}(s;c)\\right\\vert}\\\\ &{\\leq\\underset{s}{\\mathrm{max}}\\sum_{a}\\left\\vert\\pi_{\\theta^{\\prime}}(a\\vert s)-\\pi_{\\theta}(a\\vert s)\\right\\vert\\Big\\vert\\Big\\vert c(s,a)\\Big\\vert+\\gamma\\sum_{s^{\\prime}}p_{\\xi^{\\prime}}(s^{\\prime}\\vert s,a)\\big\\vert V_{\\theta^{\\prime},\\xi^{\\prime}}(s^{\\prime};c)\\Big\\vert\\Big\\vert}\\\\ &{\\quad+\\gamma\\underset{s}{\\mathrm{max}}\\sum_{a}\\pi_{\\theta}(a\\vert s)\\sum_{s^{\\prime}}\\left\\vert p_{\\xi^{\\prime}}(s^{\\prime}\\vert s,a)-p_{\\xi}(s^{\\prime}\\vert s,a)\\right\\vert V_{\\theta^{\\prime},\\xi^{\\prime}}(s^{\\prime};c)\\Big\\vert}\\\\ &{\\quad+\\gamma\\underset{s}{\\mathrm{max}}\\sum_{a}\\pi_{\\theta}(a\\vert s)\\sum_{s^{\\prime}}p_{\\xi}(s^{\\prime}\\vert s,a)\\big\\vert V_{\\theta^{\\prime},\\xi^{\\prime}}(s^{\\prime};c)-V_{\\theta,\\xi}(s^{\\prime};c)\\big\\vert\\Big\\vert}\\\\ &{\\overset{(i)}{\\leq}\\underset{1-\\gamma}{\\mathrm{max}}\\Big(\\varepsilon_{\\pi_{\\theta}}\\vert A\\vert\\vert\\theta^{\\prime}-\\theta\\vert\\vert+\\gamma\\ell_{\\theta_{\\sf E}}\\vert S\\vert\\vert\\xi^{\\prime}-\\xi\\vert\\vert\\Big)+\\gamma\\underset{s}{\\mathrm{max}}\\vert V_{\\theta^{\\prime},\\xi^{\\prime}}(s;c)-V_{\\theta,\\xi}(s;c)\\vert,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (i) uses Eq. (55) and Lemma 1. Rearranging the above inequality yields that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s}|V_{\\theta^{\\prime},\\xi^{\\prime}}(s;c)-V_{\\theta,\\xi}(s;c)|\\leq\\frac{c_{\\operatorname*{max}}}{(1-\\gamma)^{2}}\\big(\\ell_{\\pi_{\\theta}}|A|\\|\\theta^{\\prime}-\\theta\\|+\\gamma\\ell_{p_{\\xi}}|S|\\|\\xi^{\\prime}-\\xi\\|\\big).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similar to Eq. (52), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s}|V_{\\theta,\\xi}(s;c^{\\prime})-V_{\\theta,\\xi}(s;c)|\\leq\\frac{\\|c^{\\prime}-c\\|_{\\infty}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we can prove Eq. (12) as follows. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\nabla_{\\xi}W_{\\xi}(\\tau)|\\leq\\xi_{\\eta}W_{\\xi}\\mathcal{E}_{\\xi}(t,\\tau)\\mathcal{E}_{\\xi}(t,\\tau)\\left|\\mathcal{E}_{\\xi}(t,\\tau)\\right|}\\\\ &{\\frac{1}{\\eta_{0}^{2}}\\sum_{i=1}^{J_{\\eta}}\\left\\{K_{\\xi}(\\epsilon,\\epsilon)-\\xi_{\\eta}(\\epsilon,\\epsilon)\\right\\}\\Big|\\nabla_{\\xi}W_{\\xi}(\\tau)\\xi_{\\eta}(\\epsilon,\\epsilon)\\right|}\\\\ &{\\quad|\\nabla_{\\xi}\\big|\\mathcal{H}(\\tau)\\big|\\leq\\frac{\\epsilon}{\\eta_{0}^{2}}\\big|\\mathcal{H}(\\tau)\\big|\\Big|\\mathcal{H}(\\tau)\\big|}\\\\ &{\\quad+\\frac{1}{\\eta_{0}^{4}}\\sum_{i=1}^{J_{\\eta}}\\big\\{\\frac{1}{|\\xi_{\\eta}|}\\big(\\epsilon,\\epsilon)-\\xi_{\\eta}(\\epsilon,\\epsilon)\\big|\\big(\\frac{\\tau}{\\eta_{0}}\\big)\\xi_{\\eta}(\\epsilon,\\epsilon)\\big|\\big\\}}\\\\ &{\\quad\\forall\\xi\\in\\mathbb{R}_{\\cal F}^{3},[-\\xi_{\\eta}(\\epsilon,\\epsilon)]\\Big|\\mathcal{H}(\\tau)\\big|\\Big|\\mathcal{H}(\\tau)\\Big|\\Big|+\\frac{1}{\\eta_{0}^{4}}\\int_{[0]}\\big|\\mathrm{d}\\tau\\big|\\tilde{\\xi}(\\tau)\\big|\\Big|}\\\\ &{\\quad+\\frac{1}{\\eta_{0}^{2}}\\int_{[0]}\\big\\langle\\frac{1}{|\\xi_{\\eta}|}\\big(\\epsilon,\\epsilon)\\big|\\nabla_{\\xi}W_{\\xi}(\\tau)\\big|\\xi_{\\eta}\\big|\\big|\\nabla_{\\xi}\\big|\\mathcal{H}(\\tau)\\big|\\Big|-\\frac{\\epsilon}{\\eta_{0}}\\big|\\mathrm{d}\\tau\\big|\\nabla_{\\xi}W_{\\xi}(\\tau)\\big|\\Big|}\\\\ &{\\quad+\\frac{1}{\\eta_{0}^{4}}\\sum_{i=1}^{J_{\\eta}}\\big\\{\\frac{1}{|\\xi_{\\eta}|}\\big(\\epsilon,\\epsilon)\\big|\\big(\\frac{\\tau}{\\eta_{0}}\\big)\\big|\\big(\\frac{\\tau}{\\eta_{0}}\\big)\\big|\\big(\\frac{\\tau}{\\eta_{0}}\\big)\\big|\\big(\\frac{\\tau}{\\eta_{0}}\\big)\\\n$$$$\n\\begin{array}{r l}&{\\quad_{1}-_{7}\\sum_{k\\in\\{a,b\\}}^{\\infty}\\Bigg(\\mathrm{loss}(\\omega^{*}),\\omega^{*})\\Bigg|}\\\\ &{\\quad+\\frac{1}{1-\\gamma}\\sum_{k\\in\\{a,b\\}}^{\\infty}\\Bigg[\\mathrm{loss}(\\omega^{*})\\Bigg(\\omega^{*})\\Bigg]\\Bigg|}\\\\ &{\\quad_{\\{a,b\\}}\\Bigg[\\mathrm{loss}(\\omega^{*})\\Bigg[\\mathrm{loss}(\\omega^{*})\\mathrm{Re}_{a,b}[\\omega^{*}])\\Bigg]\\Bigg|+\\mathrm{Pe}_{a,b}[\\omega_{a,b}]\\Bigg|\\mathrm{loss}(\\omega^{*})\\mathrm{-\\mathbb{P}}_{a,b}[(\\lambda_{a,b}[\\omega_{a}])]}\\\\ &{\\quad+\\frac{1}{\\gamma}\\sum_{k\\in\\{a,b\\}}^{\\infty}\\Bigg[\\mathrm{loss}(\\omega^{*})\\Bigg]\\Bigg|_{\\{b,a\\}}\\Bigg[\\mathrm{loss}(\\omega^{*})\\mathrm{-\\mathbb{P}}_{a,b}[\\omega_{a,b}]\\Bigg]}\\\\ &{\\quad+\\frac{1}{\\gamma-_{1}}\\int_{\\{a,b\\}}^{\\infty}\\Bigg(\\mathrm{loss}(\\omega^{*})\\frac{\\mathrm{Pe}_{a,b\\}[\\omega^{*}]}{\\mathrm{Pe}_{a,b}[\\omega^{*}]}\\Bigg)\\mathrm{d}\\omega^{*}\\mathrm{d}\\lambda_{1}\\Bigg|\\mathrm{loss}(\\omega^{*})\\mathrm{-\\mathbb{Q}}_{a,b}[\\omega^{*}]}\\\\ &{\\quad+\\frac{\\gamma_{1}}{\\gamma-_{2}}\\sum_{k\\in\\{a,b\\}}^{\\infty}\\Bigg[\\mathrm{loss}(\\omega^{*})\\Bigg(\\mathrm{loss}(\\omega^{*})\\mathrm{-\\mathbb{P}}_{a,b}[\\omega_{a,b}[\\omega^{*}])\\Bigg|\\mathrm{loss}(\\omega^{*})\\mathrm{-\\mathbb{P}}_{a,b}[\\omega_{b,b}[\\omega^{*}])}\\\\ &{\\quad+\\frac{\\gamma_{1}}{\\gamma-_{3}}\\sum_{k\\in\\{a,b\\}}^{\\infty}\\mathrm{loss}(\\omega^{*})\\Bigg[\\mathrm{loss}(\\omega^{*})\\mathrm{Re}_{a,b}[\\omega^{*}])\\Bigg]\\Bigg|}\\\\ &{\\quad+\\frac{\\gamma_{2}}{\\gamma-_{4}}\\sum_{k\\in\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (i uses the gradient 53and denotes f()(sa)) as the $(s,a)$ -th element of $\\nabla_{\\lambda}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})$ , (ii) uses $\\nabla_{\\xi}p_{\\xi^{\\prime}}(s^{\\prime}|s,a)=p_{\\xi^{\\prime}}(s^{\\prime}|s,a)\\nabla_{\\xi}\\log p_{\\xi^{\\prime}}(s^{\\prime}|s,a),$ Assumptions 1-2 and Eqs. (31), (55), (57) and (58) with $c_{\\operatorname*{max}}\\,=\\,\\|\\nabla_{\\lambda}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})\\|_{\\infty}$ replaced by its upper bound $\\ell_{\\lambda}$ , (ii) uses Assumptions 2 and (iv) uses Eq. (34). ", "page_idx": 24}, {"type": "text", "text": "G Proof of Proposition 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Implement one projected gradient step from $\\theta$ and obtain $\\theta^{\\prime}\\,=\\,\\mathrm{proj}_{\\Theta}\\big(\\theta\\,-\\,\\beta\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})\\big)\\,=\\,\\theta\\,-\\,$ $\\beta G_{\\beta}^{(\\theta)}(\\theta,\\xi)$ where he prjtedgradient $G_{\\beta}^{(\\theta)}(\\theta,\\xi)$ is defined by Eqg (13). Based on Assumpion 4, for any $\\delta\\in[0,\\overline{{\\delta}}]$ , there exists $\\theta_{\\delta}\\in\\Theta$ such that $\\lambda_{\\theta_{\\delta},\\xi}=(1-\\delta)\\lambda_{\\theta^{\\prime},\\xi}+\\delta\\lambda_{\\theta^{*}(\\xi),\\xi}\\in\\mathcal{V}_{\\lambda_{\\theta,\\xi}}.$ Based on Assumption 4, for any $\\delta\\in[0,\\overline{{\\delta}}]$ , there exists $\\theta_{\\delta}\\in\\Theta$ such that $\\lambda_{\\theta_{\\delta},\\xi}=(1-\\delta)\\lambda_{\\theta^{\\prime},\\xi}+\\delta\\lambda_{\\theta^{*}(\\xi),\\xi}\\in$ $\\mathcal{V}_{\\lambda_{\\theta^{\\prime},\\xi}}$ . Then we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta_{\\delta}-\\theta^{\\prime}\\|\\overset{(i)}{\\le}\\ell_{\\lambda^{-1}}\\|\\lambda_{\\theta_{\\delta},\\xi}-\\lambda_{\\theta,\\xi}\\|=\\delta\\ell_{\\lambda^{-1}}\\big\\|\\lambda_{\\theta^{*}(\\xi),\\xi}-\\lambda_{\\theta,\\xi}\\big\\|\\overset{(i i)}{\\le}\\sqrt{2}\\delta\\ell_{\\lambda^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (i) uses the $L_{\\theta,\\theta}$ -smoothness of $f(\\lambda.,\\xi)$ based on Proposition 3, (i) uses Lemma 4. ", "page_idx": 25}, {"type": "text", "text": "By applying Lemma 6 to $\\mathcal{X}\\,=\\,\\Theta$ $x\\,=\\,\\theta\\mathrm{~-~}\\beta\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})$ $x^{\\prime}\\,=\\,\\theta_{\\delta}\\,\\in\\,\\Theta$ (SC $)\\ \\mathrm{proj}_{\\mathcal{X}}(x)\\,=\\,\\theta^{\\prime}\\,=$ $\\theta-\\beta G_{\\beta}^{(\\theta)}(\\theta,\\xi))$ ,weobtainthat ", "page_idx": 25}, {"type": "equation", "text": "$$\n(\\theta_{\\delta}-\\theta^{\\prime})^{\\top}[G_{\\beta}^{(\\theta)}(\\theta,\\xi)-\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})]\\leq0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then on one hand, $f(\\lambda_{\\theta_{\\delta},\\xi})$ has the following lower bound. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\lambda_{\\theta_{s},\\xi})\\overset{(i)}{\\underset{>}{\\sum}}f(\\lambda_{\\theta^{\\prime},\\xi})+\\nabla_{\\theta}f(\\lambda_{\\theta^{\\prime},\\xi})^{\\top}(\\theta_{\\delta}-\\theta^{\\prime})-\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{\\delta}-\\theta^{\\prime}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\overset{(i i)}{\\geq}f(\\lambda_{\\theta^{\\prime},\\xi})+\\big[\\nabla_{\\theta}f(\\lambda_{\\theta^{\\prime},\\xi})-\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})+G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\big]^{\\top}(\\theta_{\\delta}-\\theta^{\\prime})-\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{\\delta}-\\theta^{\\prime}\\|^{2}}\\\\ &{\\quad\\quad\\overset{(i i i)}{\\geq}f(\\lambda_{\\theta^{\\prime},\\xi})-\\big(L_{\\theta,\\theta}\\big\\|\\theta^{\\prime}-\\theta\\big\\|+\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|\\big)\\|\\theta_{\\delta}-\\theta^{\\prime}\\|-\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{\\delta}-\\theta^{\\prime}\\|^{2}}\\\\ &{\\quad\\quad\\overset{(i v)}{\\geq}f(\\lambda_{\\theta^{\\prime},\\xi})-\\sqrt{2}\\delta\\ell_{\\lambda^{-1}}\\big(\\beta L_{\\theta,\\theta}+1\\big)\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|-L_{\\theta,\\theta}\\delta^{2}\\ell_{\\lambda^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (i) and (ii) use $L_{\\theta,\\theta}$ -smoothness of $f(\\lambda.,\\xi)$ based on Proposition 3, (i) uses Eq. (60), and (iv) uses Lemma 5, $\\|\\theta_{\\delta}-\\theta^{\\prime}\\|\\,\\leq\\,\\sqrt{2}\\delta\\ell_{\\lambda^{-1}}$ (obtained in the same way as Eq. (59)) and $\\theta^{\\prime}-\\theta=$ $-\\beta G_{\\beta}^{(\\theta)}(\\theta,\\xi)$ . On the other hand, $f(\\lambda_{\\theta_{\\delta},\\xi})$ has the follwing upper bound since $f$ is convex. ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\lambda_{\\theta_{\\delta},\\xi})\\leq(1-\\delta)f(\\lambda_{\\theta^{\\prime},\\xi})+\\delta f(\\lambda_{\\theta^{*}(\\xi),\\xi})=(1-\\delta)f(\\lambda_{\\theta^{\\prime},\\xi})+\\delta\\operatorname*{min}_{\\theta^{\\prime\\prime}\\in\\Theta}f(\\lambda_{\\theta^{\\prime\\prime},\\xi}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The above two inequalities (61) and (62) imply that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\lambda_{\\theta^{\\prime},\\xi})-\\underset{\\theta^{\\prime\\prime}\\in\\Theta}{\\operatorname*{min}}\\;f(\\lambda_{\\theta^{\\prime\\prime},\\xi})\\leq\\underset{\\delta\\to+0}{\\operatorname*{lim}}\\underset{\\delta}{\\operatorname*{sup}}\\,\\frac{1}{\\delta}\\big[f(\\lambda_{\\theta^{\\prime},\\xi})-f(\\lambda_{\\theta_{\\delta},\\xi})\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{2}\\ell_{\\lambda^{-1}}\\big(\\beta L_{\\theta,\\theta}+1\\big)\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, we prove Eq. (20) as follows. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\lambda_{\\theta,\\xi})\\overset{(i)}{\\leq}f(\\lambda_{\\theta^{\\prime},\\xi})+\\ell_{\\theta}\\|\\theta^{\\prime}-\\theta\\|}\\\\ &{\\qquad\\overset{(i i)}{\\leq}\\underset{\\theta^{\\prime\\prime}\\in\\Theta}{\\operatorname*{min}}\\;f(\\lambda_{\\theta^{\\prime\\prime},\\xi})+\\big[\\sqrt{2}\\ell_{\\lambda^{-1}}\\big(\\beta L_{\\theta,\\theta}+1\\big)+\\beta\\ell_{\\theta}\\big]\\|G_{\\beta}^{(\\theta)}(\\theta,\\xi)\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (i) uses Eq. (10) which implies that $f_{\\lambda.,\\xi}$ .s $\\ell_{\\theta}$ -Lipschitz,(i) uses $\\theta^{\\prime}-\\theta=-\\beta G_{\\beta}^{(\\theta)}(\\theta,\\xi)$ and Eq. (63). ", "page_idx": 25}, {"type": "text", "text": "H Proof of Proposition 5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Wewill first prove that $\\xi_{\\delta}\\in\\Xi$ . Note that the $s$ -rectangular ambiguity set $\\Xi$ canbe expressed as a Cartesian product of $\\Xi_{s}$ for all $s\\in S$ . Hence, as $\\Xi$ is convex, $\\Xi_{s}$ is convex for all $s\\in S$ . Therefore, for any $s\\in S$ $\\xi_{\\delta}(s,\\cdot,\\cdot)\\in\\Xi_{s}$ since it is a convex combination of $\\xi_{0}(s,\\cdot,\\cdot)\\in\\Xi_{s}$ and $\\xi_{1}(s,\\cdot,\\cdot)\\in\\Xi_{s}$ defined by Eq. (21), so $\\xi_{\\delta}\\in\\Xi$ ", "page_idx": 25}, {"type": "text", "text": "Next, we will prove $\\lambda_{\\theta,\\xi_{\\delta}}=\\delta\\lambda_{\\theta,\\xi_{1}}+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}$ Denote $\\lambda_{\\delta}:=\\delta\\lambda_{\\theta,\\xi_{1}}+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}$ , so the aim becomes to prove $\\lambda_{\\theta,\\xi_{\\delta}}=\\lambda_{\\delta}$ . Based on Lemma 2, it suffices to prove the following equation. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{\\delta}(s^{\\prime},a^{\\prime})=\\left[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda_{\\delta}(s,a)\\xi_{\\delta}(s^{\\prime}|s,a)\\right]\\pi_{\\theta}(a^{\\prime}|s^{\\prime}),\\quad s^{\\prime}\\in S,a^{\\prime}\\in\\cal A.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For each $s\\in S$ , consider the following two cases. ", "page_idx": 26}, {"type": "text", "text": "(Case 1): $\\lambda_{\\theta,\\xi_{1}}(s)>0$ or $\\lambda_{\\theta,\\xi_{0}}(s)>0$ Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{\\delta}(s,a)\\xi_{\\delta}(s,a,s^{\\prime})}\\\\ &{\\overset{(i)}{=}\\big[\\delta\\lambda_{\\theta,\\xi_{1}}(s,a)+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s,a)\\big]\\frac{\\delta\\lambda_{\\theta,\\xi_{1}}(s)\\xi_{1}(s,a,s^{\\prime})+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s)\\xi_{0}(s,a,s^{\\prime})}{\\delta\\lambda_{\\theta,\\xi_{1}}(s)+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s)}}\\\\ &{\\overset{(i i)}{=}\\pi_{\\theta}(a|s)\\big[\\delta\\lambda_{\\theta,\\xi_{1}}(s)+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s)\\big]\\frac{\\delta\\lambda_{\\theta,\\xi_{1}}(s)\\xi_{1}(s,a,s^{\\prime})+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s)\\xi_{0}(s,a,s^{\\prime})}{\\delta\\lambda_{\\theta,\\xi_{1}}(s)+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s)}}\\\\ &{\\overset{(i i i)}{=}\\delta\\lambda_{\\theta,\\xi_{1}}(s,a)\\xi_{1}(s,a,s^{\\prime})+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s,a)\\xi_{0}(s,a,s^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (i) uses Eq. (21) and $\\lambda_{\\delta}:=\\delta\\lambda_{\\theta,\\xi_{1}}+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}$ , (ii) and (ii) use Eq. (33). ", "page_idx": 26}, {"type": "text", "text": "(Case 2): $\\lambda_{\\theta,\\xi_{0}}(s)=\\lambda_{\\theta,\\xi_{1}}(s)=0$   \nIn this case, $\\lambda_{\\delta}(s)=\\delta\\lambda_{\\theta,\\bar{\\xi_{1}}}(s)\\!+\\!(1\\!-\\!\\delta)\\lambda_{\\theta,\\xi_{0}}(s)=0$ and thus $\\lambda_{\\delta}(s,a)=\\lambda_{\\theta,\\xi_{1}}(s,a)=\\lambda_{\\theta,\\xi_{0}}(s,a)=$   \nO for any $a\\in A$ , so Eq. (65) also holds for any choice of $\\xi_{\\delta}(s,\\cdot,\\cdot)$ ", "page_idx": 26}, {"type": "text", "text": "Therefore, we can prove Eq. (64) as follows. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\displaystyle\\sum_{s,a}\\lambda_{\\delta}(s,a)\\xi_{\\delta}(s^{\\prime}|s,a)\\Big]\\pi_{\\theta}(a^{\\prime}|s^{\\prime})}\\\\ &{\\quad\\overset{(i)}{=}\\Big[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\displaystyle\\sum_{s,a}[\\delta\\lambda_{\\theta,\\xi_{1}}(s,a)\\xi_{1}(s,a,s^{\\prime})+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}(s,a)\\xi_{0}(s,a,s^{\\prime})]\\Big]\\pi_{\\theta}(a^{\\prime}|s^{\\prime})}\\\\ &{\\quad\\overset{(i)}{=}-\\delta\\Big[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\displaystyle\\sum_{s,a}\\lambda_{\\theta,\\xi_{1}}(s,a)\\xi_{1}(s,a,s^{\\prime})\\Big]\\pi_{\\theta}(a^{\\prime}|s^{\\prime})}\\\\ &{\\quad\\,+\\,(1-\\delta)\\Big[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\displaystyle\\sum_{s,a}\\lambda_{\\theta,\\xi_{0}}(s,a)\\xi_{0}(s,a,s^{\\prime})\\Big]\\pi_{\\theta}(a^{\\prime}|s^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "(\u5165e,(s,a)+(1-8)>e,s(s,a) =\u5165s(s,a), ", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (i) uses Eq. (65) and $\\lambda_{\\delta}:=\\delta\\lambda_{\\theta,\\xi_{1}}+(1-\\delta)\\lambda_{\\theta,\\xi_{0}}$ and (i applies Lemma 2 to $\\lambda_{\\theta,\\xi_{1}}$ and $\\lambda_{\\theta,\\xi_{0}}$ ", "page_idx": 26}, {"type": "text", "text": "1 Proof of Proposition 6 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Fix any $\\theta\\ \\in\\ \\Theta$ , and there exists at least one $\\xi^{*}\\ \\in\\ \\arg\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f(\\lambda_{\\theta,\\xi^{\\prime}})$ . If $\\xi^{*}\\;\\in\\;V(\\Xi)\\;:=$ $\\times_{s\\in S}V(\\Xi_{s})$ , then this proposition directly holds. Hence, we focus on the case where $\\xi^{*}\\notin V(\\Xi)$ which means $\\xi^{*}(s_{0})\\not\\in V(\\Xi_{s_{0}})$ for at least one $s_{0}\\in\\mathcal{S}$ ", "page_idx": 26}, {"type": "text", "text": "Based on Assumption 6-7, there exists a probability vector $\\nu:=[\\nu_{1},\\dots,\\nu_{M_{s_{0}}}]$ such that $\\xi^{*}(s_{0})=$ $\\sum_{m=1}^{M_{s_{0}}}\\nu_{m}\\xi_{m}^{(s_{0})}$ , where we denote $\\xi^{\\prime}(s):=\\xi^{\\prime}(s,\\cdot,\\cdot)\\in\\Xi_{s}$ for all $\\xi^{\\prime}\\in\\Xi$ Without loss of generality, we assume $\\nu_{1}=\\operatorname*{max}_{1\\leq m\\leq M_{s}}\\nu_{m}$ (Otherwise we can make this assumption hold by permutating the elements in each $\\Xi_{s}$ ). ", "page_idx": 26}, {"type": "text", "text": "For any $\\epsilon>0$ , define $\\xi_{1}^{*},\\xi^{(\\epsilon)}\\in(\\Delta^{S})^{S\\times A}$ such that $\\xi_{1}^{*}(s)=\\xi^{(\\epsilon)}(s)=\\xi^{*}(s)$ for any $s\\neq s_{0}$ , while at $s=s_{0}$ we define $\\xi_{1}^{*}(s_{0})=\\xi_{1}^{(s_{0})}\\in V(\\Xi_{s_{0}})$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\xi^{(\\epsilon)}(s_{0})=\\xi^{*}(s_{0})+\\epsilon[\\xi^{*}(s_{0})-\\xi_{1}^{*}(s_{0})]=[(1+\\epsilon)\\nu_{1}-\\epsilon]\\xi_{1}^{(s_{0})}+\\sum_{m=2}^{M_{s_{0}}}(1+\\epsilon)\\nu_{m}\\xi_{m}^{(s_{0})},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\xi^{*}=\\frac{\\epsilon\\xi^{(\\epsilon)}+\\xi_{1}^{*}}{1+\\epsilon}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It is easily seen that $\\xi_{1}^{*}\\in\\ \\Xi$ by its definition. _ Since $\\begin{array}{r l}{\\operatorname*{lim}_{\\epsilon\\to+0}[(1+\\epsilon)\\nu_{1}(s)\\,-\\,\\epsilon]\\ =\\ \\nu_{1}(s)\\ =}&{{}}\\end{array}$ $\\operatorname*{max}_{1\\leq m\\leq M_{s}}\\nu_{m}(s)>\\bar{0}$ , there exists a sufficiently small $\\epsilon>0$ such that $[(1+\\epsilon)\\nu_{1}(s)-\\epsilon,(1+$ $\\epsilon)\\nu_{2}(s),\\ldots,(1+\\epsilon)\\nu_{M_{s}}(s)]\\in[0,1]^{|\\Xi_{s}|}$ is a probability vector and thus $\\xi^{(\\epsilon)}\\in\\Xi$ . Furthermore, select arbitrary $\\delta\\in[0,1]$ $\\lambda_{\\theta,\\xi^{(\\epsilon)}}(s_{0}\\bar{)}=\\bar{\\lambda}_{\\theta,\\xi_{1}^{*}}(s_{0})=0$ and the following $\\delta$ otherwise. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\delta=\\frac{\\lambda_{\\theta,\\xi^{(\\epsilon)}}(s_{0})}{\\epsilon\\lambda_{\\theta,\\xi_{1}^{*}}(s_{0})+\\lambda_{\\theta,\\xi^{(\\epsilon)}}(s_{0})},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Where $\\begin{array}{r}{\\lambda_{\\theta,\\xi}(s):=\\sum_{a\\in\\mathcal{A}}\\lambda_{\\theta,\\xi}(s,a)}\\end{array}$ is defined as the state occupancymeasure for any $s\\in{\\mathcal{S}},\\theta\\in\\Theta$ and $\\xi\\in\\Xi$ . Then it can be directly verified that $\\xi^{*}$ satisfies the following equality. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\xi^{*}(s,a,s^{\\prime})=\\left\\{\\begin{array}{l l}{\\mathrm{arbitrary~as~long~as~}\\xi^{*}(s,a,\\cdot)\\in\\Delta^{S},\\qquad\\qquad\\mathrm{~if~}\\lambda_{\\theta,\\xi^{(\\epsilon)}}(s)\\!=\\!\\lambda_{\\theta,\\xi_{1}^{*}}(s)\\!=\\!0}\\\\ {\\displaystyle\\frac{\\delta\\lambda_{\\theta,\\xi_{1}^{*}}(s)\\xi_{1}^{*}(s,a,s^{\\prime})\\!+\\!(1\\!-\\!\\delta)\\lambda_{\\theta,\\xi^{(\\epsilon)}}(s)\\xi^{(\\epsilon)}(s,a,s^{\\prime})}{\\delta\\lambda_{\\theta,\\xi_{1}^{*}}(s)\\!+\\!(1\\!-\\!\\delta)\\lambda_{\\theta,\\xi^{(\\epsilon)}}(s)},\\mathrm{otherwise~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, based on Proposition 5, $\\xi^{*}$ satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda_{\\theta,\\xi^{*}}=\\delta\\lambda_{\\theta,\\xi_{1}^{*}}+(1-\\delta)\\lambda_{\\theta,\\xi^{(\\epsilon)}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On one hand, $f(\\lambda_{\\theta,\\xi_{1}^{*}})\\leq f(\\lambda_{\\theta,\\xi^{*}})$ and $f\\big(\\lambda_{\\theta,\\xi^{(\\epsilon)}}\\big)\\leq f(\\lambda_{\\theta,\\xi^{*}})$ since $\\xi^{*}\\;\\xi^{*}\\in\\arg\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f(\\lambda_{\\theta,\\xi^{\\prime}})$ On the other hand, the above Eq. (68) along with convexity of $f$ implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\lambda_{\\theta,\\xi^{*}})\\leq\\delta f(\\lambda_{\\theta,\\xi_{1}^{*}})+(1-\\delta)f(\\lambda_{\\theta,\\xi^{(\\epsilon)}}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{f\\big(\\lambda_{\\theta,\\xi_{1}^{*}}\\big)=f\\big(\\lambda_{\\theta,\\xi^{(\\epsilon)}}\\big)=f\\big(\\lambda_{\\theta,\\xi^{*}}\\big)=\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f\\big(\\lambda_{\\theta,\\xi^{\\prime}}\\big).}\\end{array}$ If $\\xi_{1}^{*}\\in V(\\Xi)$ ,thentheproof is done. Otherwise, note that $\\xi_{0}^{*}(s_{0})\\notin V(\\Xi_{s_{0}})$ while $\\xi_{1}^{*}(s_{0})\\,\\in\\,V(\\Xi_{s_{0}})$ , and $\\xi_{1}^{*}(s)\\,=\\,\\xi_{0}^{*}(s)$ for any $s\\neq s_{0}$ . Hence, in the same way, we can obtain the sequence $\\xi_{2}^{*},\\xi_{3}^{*},\\dots,\\xi_{N}^{*}$ that satisfies the following conditions by changing non-vertex into vertex at one state each time until no non-vertex remains (i.e., until the condition 2 below holds): ", "page_idx": 27}, {"type": "text", "text": "As a result, we find the optimal vertex $\\xi_{N}^{*}\\in V(\\Xi)\\cap$ arg $\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f(\\lambda_{\\theta,\\xi^{\\prime}})$ , which concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "JProof of Proposition 7 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Based on Assumption 8, for any $\\theta\\:\\in\\:\\Theta$ and $\\delta\\;\\in\\;[0,\\overline{{\\delta}}]$ , there exists $\\theta_{\\delta}\\ \\in\\ \\Theta$ such that $\\lambda_{\\theta_{\\delta},\\xi}\\;=\\;$ (1 - 8)\u5165e,g + 0\u5165o\\*,\u00b7 ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta_{\\delta}-\\theta\\|\\overset{(i)}{\\le}\\ell_{\\lambda^{-1}}\\|\\lambda_{\\theta_{\\delta},\\xi}-\\lambda_{\\theta,\\xi}\\|=\\delta\\ell_{\\lambda^{-1}}\\big\\|\\lambda_{\\theta^{*},\\xi}-\\lambda_{\\theta,\\xi}\\big\\|\\overset{(i i)}{\\le}\\sqrt{2}\\delta\\ell_{\\lambda^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (i) uses the $L_{\\theta,\\theta}$ -smoothness of $f(\\lambda.,\\xi)$ based on Proposition 3, (ii) uses Lemma 4. Hence, on one hand, using $L_{\\theta,\\theta}$ -smoothness of $f(\\lambda.,\\xi)$ based on Proposition 3, $f(\\lambda_{\\theta_{\\delta},\\xi})$ has the following lower bound. ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\lambda_{\\theta_{\\delta},\\xi})\\geq f(\\lambda_{\\theta,\\xi})+\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})^{\\top}(\\theta_{\\delta}-\\theta)-\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{\\delta}-\\theta\\|^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand, $f(\\lambda_{\\theta_{\\delta},\\xi})$ has the following upper bound since $f$ is convex. ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\lambda_{\\theta_{\\delta},\\xi})\\leq(1-\\delta)f(\\lambda_{\\theta,\\xi})+\\delta f(\\lambda_{\\theta^{*},\\xi})=(1-\\delta)f(\\lambda_{\\theta,\\xi})+\\delta\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the above two inequalities, we obtain that ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\nabla_{\\theta}f(\\lambda_{\\theta,\\xi})^{\\top}\\frac{\\theta_{\\delta}-\\theta}{\\|\\theta_{\\delta}-\\theta\\|}\\geq\\frac{f(\\lambda_{\\theta,\\xi})-f(\\lambda_{\\theta_{\\delta},\\xi})}{\\|\\theta_{\\delta}-\\theta\\|}-\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{\\delta}-\\theta\\|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i)}{\\geq}\\frac{\\delta[f(\\lambda_{\\theta,\\xi})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})]}{\\|\\theta_{\\delta}-\\theta\\|}-\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{\\delta}-\\theta\\|}\\\\ &{\\overset{(i i)}{\\geq}\\frac{f\\left(\\lambda_{\\theta,\\xi}\\right)-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f\\left(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}}\\right)}{\\sqrt{2}\\ell_{\\lambda^{-1}}}-\\frac{\\sqrt{2}\\delta\\ell_{\\lambda^{-1}}L_{\\theta,\\theta}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (i) uses Eq. (71), and (i) uses Eq. (69) and assumes $\\begin{array}{r}{f(\\lambda_{\\theta,\\xi})\\geq\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})}\\end{array}$ without loss of generality (otherwise, Eq. (24) trivially holds). ", "page_idx": 28}, {"type": "text", "text": "Based on the Bolzano- Weierstrass theorem, there exists sequence $\\delta_{n}\\to+0$ such that $\\frac{\\theta_{\\delta_{n}}\\!-\\!\\theta}{\\|\\theta_{\\delta_{n}}\\!-\\!\\theta\\|}\\,\\rightarrow$ $d\\,\\in\\,\\mathbb{R}^{d_{\\Theta}}$ as $n\\to+\\infty$ . Hence, $\\|d\\|\\,=1$ and we can conclude the proof by letting $\\delta\\,=\\,\\delta_{n}$ and $n\\to+\\infty$ in the above inequality. ", "page_idx": 28}, {"type": "text", "text": "K Proof of Proposition 8 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "K.1  Proof of Assumptions 1, 2 and 3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proposition 2 indicates that the utility function $f$ defined by Eq. (7) is convex, which proves Assumption 3. ", "page_idx": 28}, {"type": "text", "text": "To prove Assumptions 1 and 2, it suffices to prove that the following functions have bounded first-order and second-order derivatives for any $(s,a,s^{\\prime})\\in\\mathcal S\\times\\mathcal A\\times\\mathcal S$ ", "page_idx": 28}, {"type": "equation", "text": "$\\{\\lambda_{\\theta,\\xi}:\\theta\\in\\Theta,\\xi\\in\\Xi\\}$ ", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any $\\theta\\in\\Theta=[-R,R]^{|S|\\times|A|}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pi_{\\theta}(a|s)=\\frac{\\exp(\\theta_{s,a})}{\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}})}\\geq\\pi_{\\operatorname*{min}}\\overset{\\mathrm{def}}{=}\\frac{\\exp(-R)}{\\exp(-R)+(|A|-1)\\exp(R)}>0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $p=1$ or $p=\\infty$ , any $\\xi\\in\\Xi$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\xi}(s^{\\prime}|s,a)=\\xi(s,a,s^{\\prime})\\geq\\widehat{\\xi}(s,a,s^{\\prime})-\\|\\xi(s,:,:)-\\widehat{\\xi}(s,:,:)\\|_{p}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\widehat{\\xi}(s,a,s^{\\prime})-\\alpha_{s}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\xi_{\\operatorname*{min}}\\overset{\\mathrm{def}}{=}\\underset{s,a,s^{\\prime}}{\\operatorname*{min}}[\\widehat{\\xi}(s,a,s^{\\prime})-\\alpha_{s}]>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\xi_{\\operatorname*{min}}>0$ since it is minimum over finitely many positive numbers $\\widehat{\\xi}(s,a,s^{\\prime})-\\alpha_{s}$ .Then for any $\\theta\\in\\Theta$ and $\\xi\\in\\Xi$ ,wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda_{\\theta,\\xi}(s,a)\\overset{\\mathrm{def}}{=}(1-\\gamma)\\displaystyle\\sum_{t=0}^{+\\infty}\\gamma^{t}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a|s_{0}\\sim\\rho)}\\\\ {\\ge\\gamma(1-\\gamma)\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{1}=s,a_{1}=a|s_{0}\\sim\\rho)}\\\\ {=\\gamma(1-\\gamma)\\displaystyle\\sum_{s^{\\prime},a^{\\prime}}\\rho(s^{\\prime})\\pi_{\\theta}(a^{\\prime}|s^{\\prime})p_{\\xi}(s|s^{\\prime},a^{\\prime})\\pi_{\\theta}(a|s)}\\\\ {\\ge\\gamma(1-\\gamma)\\displaystyle\\sum_{s^{\\prime},a^{\\prime}}\\rho(s^{\\prime})\\pi_{\\theta}(a^{\\prime}|s^{\\prime})\\xi_{\\mathrm{min}}\\pi_{\\mathrm{min}}}\\\\ {=\\lambda_{\\mathrm{min}}\\overset{\\mathrm{def}}{=}\\xi_{\\mathrm{min}}\\pi_{\\mathrm{min}}\\gamma(1-\\gamma)>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, for any $(s,a,s^{\\prime}),(s_{1},a_{1},s_{1}^{\\prime}),(s_{2},a_{2},s_{2}^{\\prime})\\in S\\times A\\times S$ , we obtain al the derivative bounds as follows, where $\\mathbb{I}\\{\\cdot\\}$ is an indicator function. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial\\log\\pi_{\\theta}(a|s)}{\\partial\\theta(s_{1},a_{1})}=\\mathbb{1}\\{s_{1}=s\\}\\big[\\mathbb{1}\\{a_{1}=a\\}-\\pi_{\\theta}(a_{1}|s)\\big]\\in[-1,1].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\log\\pi_{\\theta}(a|s)}{\\partial\\theta(s_{1},a_{1})\\partial\\theta(s_{2},a_{2})}=-\\mathbb{1}\\big\\{s_{1}=s\\}\\pi_{\\theta}(a_{1}|s)\\frac{\\partial\\log\\pi_{\\theta}(a|s)}{\\partial\\theta(s_{2},a_{2})}\\in[-1,1].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial\\log p_{\\xi}(s^{\\prime}|s,a)}{\\partial\\xi(s_{1},a_{1},s_{1}^{\\prime})}=\\frac{1}{\\xi(s,a,s^{\\prime})}\\mathbb{1}\\{(s_{1},a_{1},s_{1}^{\\prime})=(s,a,s^{\\prime})\\}\\in[0,\\xi_{\\operatorname*{min}}^{-1}].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\log p_{\\xi}(s^{\\prime}|s,a)}{\\partial\\xi(s_{1},a_{1},s_{1}^{\\prime})\\partial\\xi(s_{2},a_{2},s_{2}^{\\prime})}=-\\xi^{-2}(s,a,s^{\\prime})\\mathbb{1}\\{(s_{1},a_{1},s_{1}^{\\prime})\\!=\\!(s_{2},a_{2},s_{2}^{\\prime})\\!=\\!(s,a,s^{\\prime})\\}\\in[-\\xi_{\\operatorname*{min}}^{-2},0].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial f(\\lambda)}{\\partial\\lambda(s_{1},a_{1})}\\!=\\!c(s_{1},a_{1})\\!+\\!\\log\\frac{\\lambda(s_{1},a_{1})}{\\sum_{a^{\\prime}}\\lambda(s_{1},a^{\\prime})}\\!+\\!1\\!-\\!\\frac{\\lambda(s_{1},a_{1})}{\\sum_{a^{\\prime}}\\lambda(s_{1},a^{\\prime})}\\!\\in\\!\\Big[c_{\\mathrm{min}}\\!-\\!\\log(|{\\cal A}|\\lambda_{\\mathrm{min}}),c_{\\mathrm{max}}\\!+\\!1\\Big],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $c_{\\operatorname*{min}}=\\operatorname*{min}_{s,a}c(s,a)$ and $c_{\\operatorname*{max}}=\\operatorname*{max}_{s,a}c(s,a)$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial^{2}f(\\lambda)}{\\partial\\lambda(s_{1},a_{1})\\partial\\lambda(s_{2},a_{2})}\\!=\\!\\!1\\{s_{1}=s_{2}\\}\\Big[\\frac{\\mathbb{I}\\{a_{1}=a_{2}\\}}{\\lambda(s_{1},a_{1})}\\!-\\!\\frac{1}{\\sum_{a^{\\prime}}\\lambda(s_{1},a^{\\prime})}\\!-\\!\\frac{\\mathbb{I}\\{a_{1}=a_{2}\\}-\\lambda(s_{1},a_{1})}{[\\sum_{a^{\\prime}}\\lambda(s_{1},a^{\\prime})]^{2}}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\in\\Big[-\\frac{1}{|\\mathcal{A}|\\lambda_{\\operatorname*{min}}}-\\frac{1}{|\\mathcal{A}|^{2}\\lambda_{\\operatorname*{min}}^{2}},\\frac{1}{\\lambda_{\\operatorname*{min}}}+\\frac{1}{|\\mathcal{A}|\\lambda_{\\operatorname*{min}}}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "K.2 Proof of Assumptions 5, 6 and 7 about ambiguity set $\\Xi$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "It is straighforward to verify that the ambiguity set $\\Xi=\\{\\xi\\in(\\Delta^{S})^{\\mathcal{S}\\times A}:\\|\\xi(s,:,:)-\\widehat\\xi(s,:,:)\\|_{p}\\leq$ $\\alpha_{s},\\forall s\\in\\dot{S}\\}\\,(p\\in\\{1,\\infty\\})$ is convex and compact, which proves Assumption 5. ", "page_idx": 29}, {"type": "text", "text": "Assumption 6 can be proved easily by letting $\\Xi_{s}\\;=\\;\\{\\xi_{s}\\;\\in\\;(\\Delta^{S})^{\\mathcal{A}}\\;:\\;\\|\\xi_{s}\\,-\\,\\widehat{\\xi}(s,:,:)\\|_{p}\\;\\le\\;\\alpha_{s}\\}$ $(p\\in\\{\\bar{1},\\infty\\})$ ", "page_idx": 29}, {"type": "text", "text": "Then we will prove Assumption 7, that is, $\\Xi_{s}=\\{\\xi_{s}\\in(\\Delta^{S})^{\\mathcal{A}}:\\|\\xi_{s}-{\\widehat{\\xi}}(s,:,:)\\|_{p}\\leq\\alpha_{s}\\}\\left(p\\in\\{1,\\infty\\}\\right)$ is a polyhedron. Based on Definition 1.1 and Theorem 1.26 of [9], it is equivalent to prove that $\\Xi_{s}$ is bounded (already proved above) and is an intersection of finitely many closed half-planes (obvious based on the definitions of $\\|\\cdot\\|_{1}$ and $\\|\\cdot\\|_{\\infty})$ ", "page_idx": 29}, {"type": "text", "text": "K.3Proof of Assumptions 4 and 8 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We will only prove Assumption 8, since Assumption 4 can be proved in the same way. ", "page_idx": 29}, {"type": "text", "text": "Fix any $\\xi\\in\\Xi$ and $\\theta\\in\\Theta=[-R,R]^{|S|\\times|A|}$ Then we sect any $\\begin{array}{r}{\\theta^{*}=\\theta^{*}(\\theta)\\in\\arg\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta_{\\mathrm{min}}}\\|\\theta^{\\prime}-}\\end{array}$ $\\theta\\|_{\\infty}$ where $\\Theta_{\\mathrm{min}}:=\\mathrm{arg}\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})$ is a compact set since $\\Gamma$ is a continuous function. ", "page_idx": 29}, {"type": "text", "text": "Define the following notations. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 $\\lambda_{\\theta,\\xi}^{(\\delta)}=(1-\\delta)\\lambda_{\\theta,\\xi}+\\delta\\lambda_{\\theta^{*},\\xi}$ $\\delta\\in[0,1]$ (we selet $\\overline{{\\delta}}=1$   \n\u00b7 Policy \u03c0 $\\begin{array}{r}{\\pi_{\\theta,\\xi}^{(\\delta)}(a|s)=\\frac{\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a)}{\\lambda_{\\theta,\\xi}^{(\\delta)}(s)}}\\end{array}$ where $\\begin{array}{r}{\\lambda_{\\theta,\\xi}^{(\\delta)}(s)=\\sum_{a^{\\prime}}\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a^{\\prime})}\\end{array}$   \n(Note that $\\lambda_{\\theta,\\xi}^{(\\delta)}(s)\\geq\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a)\\geq\\lambda_{\\operatorname*{min}}>0$ 0 $\\lambda_{\\theta,\\xi}^{(\\delta)}(s)$ can be the denomiatorand $\\pi_{\\theta,\\xi}^{(\\delta)}(a|s)>0)$ $\\theta_{\\theta,\\xi}^{(\\delta)}\\in\\mathbb{R}^{|S|\\times A}$ with each entry defined as follows. $(\\theta_{\\theta,\\xi}^{(\\delta)})_{s,a}=\\log\\left[\\frac{(1-\\delta)\\lambda_{\\theta,\\xi}(s,a)+\\delta\\lambda_{\\theta^{*},\\xi}(s,a)}{(1-\\delta)\\lambda_{\\theta,\\xi}(s,a)\\exp(-\\theta_{s,a})+\\delta\\lambda_{\\theta^{*},\\xi}(s,a)\\exp(-\\theta_{s,a}^{*})}\\right],$ (76) ", "page_idx": 29}, {"type": "text", "text": "which is alid sine $\\pi_{\\theta,\\xi}^{(\\delta)}(a|s)>0$ and $\\pi_{\\theta}(a|s)\\geq\\pi_{\\mathrm{min}}>0$ ", "page_idx": 29}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathcal{V}_{\\theta,\\xi}=\\{\\lambda_{\\theta,\\xi}^{(\\delta)}:\\delta\\in[0,1]\\}\\subset\\Delta^{s\\times A}.}\\end{array}$ ", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, it remains to prove the following two statements. ", "page_idx": 29}, {"type": "text", "text": "CPI: $\\theta_{\\theta,\\xi}^{(0)}=\\theta$ $\\bar{\\theta}_{\\theta,\\xi}^{(1)}=\\theta^{*}$ (P2): Ue,\u03b5 C \u03b8 = [-R, R]IS|x|Al. ", "page_idx": 29}, {"type": "text", "text": "(P3): $\\lambda_{\\theta,\\xi}^{(\\delta)}=\\lambda_{\\theta_{\\theta,\\xi}^{(\\delta)},\\xi}.$ ", "page_idx": 30}, {"type": "text", "text": "(P4): The maping $\\theta_{\\theta,\\xi}^{(\\delta)}\\rightarrow\\lambda_{\\theta,\\xi}^{(\\delta)}$ from $\\mathcal{U}_{\\theta,\\xi}$ 10 $\\lambda_{\\theta,\\xi}$ isabj ", "page_idx": 30}, {"type": "text", "text": "(P1) obviously follows from Eq. (76). ", "page_idx": 30}, {"type": "text", "text": "Note that $(\\theta_{\\theta,\\xi}^{(\\delta)})_{s,a}$ defnedbyq snefo f $\\delta\\in[0,1]$ and PI) imples hat $(\\theta_{\\theta,\\xi}^{(0)})_{s,a}\\,=\\,\\theta_{s,a}\\,\\in\\,[-R,R]$ and $\\theta_{\\theta,\\xi}^{(1)}\\,=\\,\\theta_{s,a}^{*}\\,\\in\\,[-R,R]$ Thefore, $(\\theta_{\\theta,\\xi}^{(\\delta)})_{s,a}\\;\\in\\;[-R,R]$ Which proves (P2). ", "page_idx": 30}, {"type": "text", "text": "To prove (P3), rewrite Eq. (76) as follows. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\theta_{\\theta,\\xi}^{(\\delta)})_{s,a}}\\\\ &{=\\log[\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a)]-\\log\\left[(1-\\delta)\\lambda_{\\theta,\\xi}(s)\\pi_{\\theta}(a|s)\\exp(-\\theta_{s,a})+\\delta\\lambda_{\\theta^{*},\\xi}(s)\\pi_{\\theta^{*}}(a|s)\\exp(-\\theta_{s,a}^{*})\\right]}\\\\ &{=\\log[\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a)]-\\log\\left[\\frac{(1-\\delta)\\lambda_{\\theta,\\xi}(s)}{\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}})}+\\frac{\\delta\\lambda_{\\theta^{*},\\xi}(s)}{\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}}^{*})}\\right]}\\\\ &{=\\log\\left[\\lambda_{\\theta,\\xi}^{(\\delta)}(s)\\pi_{\\theta,\\xi}^{(\\delta)}(a|s)\\right]-\\log\\left[\\frac{(1-\\delta)\\lambda_{\\theta,\\xi}(s)}{\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}})}+\\frac{\\delta\\lambda_{\\theta^{*},\\xi}(s)}{\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}}^{*})}\\right]}\\\\ &{=\\log\\left[\\pi_{\\theta,\\xi}^{(\\delta)}(a|s)\\right]+c_{\\delta}(s),\\quad(\\mathrm{~c~o~t~h~})\\pi_{\\theta,\\xi}(s)\\pi_{\\theta^{*}}(s)\\ ~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "$\\begin{array}{r}{c_{\\delta}(s):=\\log\\left[\\lambda_{\\theta,\\xi}^{(\\delta)}(s)\\right]-\\log\\left[\\frac{(1-\\delta)\\lambda_{\\theta,\\xi}(s)}{\\sum_{a^{\\prime}}\\exp({\\theta_{s,a^{\\prime}}(s)}}+\\frac{\\delta\\lambda_{\\theta^{*},\\xi}(s)}{\\sum_{a^{\\prime}}\\exp({\\theta_{s,a^{\\prime}}^{*}})}\\right]}\\end{array}$ Thefore ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{\\theta,\\xi}^{(\\delta)}}(a|s)=\\!\\frac{\\exp\\big[(\\theta_{\\theta,\\xi}^{(\\delta)})_{s,a}\\big]}{\\sum_{a^{\\prime}}\\exp\\big[(\\theta_{\\theta,\\xi}^{(\\delta)})_{s,a^{\\prime}}\\big]}=\\pi_{\\theta,\\xi}^{(\\delta)}(a|s)=\\frac{\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a)}{\\lambda_{\\theta,\\xi}^{(\\delta)}(s)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that for any $\\theta^{\\prime}\\in\\mathbb{R}^{|S|\\times|A|}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\theta^{\\prime},\\xi}(s^{\\prime})=\\displaystyle\\sum_{a^{\\prime}}\\lambda_{\\theta^{\\prime},\\xi}(s^{\\prime},a^{\\prime})}\\\\ &{\\qquad\\qquad\\overset{(i)}{=}\\displaystyle\\sum_{a^{\\prime}}\\Big[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\displaystyle\\sum_{s,a}\\lambda_{\\theta^{\\prime},\\xi}(s,a)p_{\\xi}(s^{\\prime}|s,a)\\Big]\\pi_{\\theta^{\\prime}}(a^{\\prime}|s^{\\prime})}\\\\ &{\\qquad=(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\displaystyle\\sum_{s,a}\\lambda_{\\theta^{\\prime},\\xi}(s,a)p_{\\xi}(s^{\\prime}|s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where (i) uses Lemma 2. Then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\biggl[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a)p_{\\xi}(s^{\\prime}|s,a)\\biggr]\\pi_{\\theta_{\\theta,\\xi}^{(\\delta)}}(a^{\\prime}|s^{\\prime})-\\lambda_{\\theta,\\xi}^{(\\delta)}(s^{\\prime},a^{\\prime})}}\\\\ {{\\displaystyle\\frac{(i)}{=}\\pi_{\\theta_{\\theta,\\xi}^{(\\delta)}}(a^{\\prime}|s^{\\prime})\\biggl[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda_{\\theta,\\xi}^{(\\delta)}(s,a)p_{\\xi}(s^{\\prime}|s,a)-\\lambda_{\\theta,\\xi}^{(\\delta)}(s^{\\prime})\\biggr]}}\\\\ {{\\displaystyle\\frac{(i i)}{=}\\delta\\pi_{\\theta_{\\theta,\\xi}^{(\\delta)}}(a^{\\prime}|s^{\\prime})\\Bigl[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda_{\\theta,\\xi}(s,a)p_{\\xi}(s^{\\prime}|s,a)-\\lambda_{\\theta,\\xi}(s^{\\prime})\\Bigr]}}\\\\ {{\\displaystyle\\qquad+\\,(1-\\delta)\\pi_{\\theta_{\\theta,\\xi}^{(\\delta)}}(a^{\\prime}|s^{\\prime})\\Bigl[(1-\\gamma)\\rho(s^{\\prime})+\\gamma\\sum_{s,a}\\lambda_{\\theta^{*},\\xi}(s,a)p_{\\xi}(s^{\\prime}|s,a)-\\lambda_{\\theta^{*},\\xi}(s^{\\prime})\\Bigr]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where (i) uses Eq. (78), (i) uses \u5165 $\\lambda_{\\theta,\\xi}^{(\\delta)}\\,=\\,(1\\,-\\,\\delta)\\lambda_{\\theta,\\xi}\\,+\\,\\delta\\lambda_{\\theta^{\\ast},\\xi}$ and (ii) uses the Eq. (79) for $\\theta^{\\prime}\\in\\{\\theta^{\\ast},\\theta\\}$ . Based on Lemma 2, the equality above implies (P3). ", "page_idx": 30}, {"type": "text", "text": "Next, we prove (P4). Note that the maping from $\\theta$ $\\lambda_{\\theta,\\xi}^{(\\delta)}=\\lambda_{\\theta_{\\theta,\\xi}^{(\\delta)},\\xi}$ is Lipschitz contiuous based on Lemma 3. Hence, we only need to consider its reverse mapping. ", "page_idx": 30}, {"type": "text", "text": "If $\\lambda_{\\theta^{*},\\xi}~=~\\lambda_{\\theta,\\xi}$ then $\\pi_{\\theta^{*}}~=~\\pi_{\\theta}$ . Hence, $\\theta\\;\\in\\;\\Theta_{\\mathrm{min}}\\;:=\\;\\arg\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})$ because $\\Gamma(\\theta^{\\prime})\\;=\\;$ $\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}f(\\lambda_{\\theta^{\\prime},\\xi^{\\prime}})$ can be seen as a function of $\\pi_{\\theta^{\\prime}}$ . Therefore, $\\theta^{*}=\\theta$ whichmeans $\\mathcal{U}_{\\theta,\\xi}=\\{\\theta\\}$ and $\\mathcal{V}_{\\theta,\\xi}=\\{\\lambda_{\\theta,\\xi}\\}$ are singletons. In this case, (P4) trivially holds. ", "page_idx": 31}, {"type": "text", "text": "Therefore, we focus on the case where $\\lambda_{\\theta^{\\ast},\\xi}\\neq\\lambda_{\\theta,\\xi}$ . Before proving (P4), we will prove the following statement. ", "page_idx": 31}, {"type": "text", "text": "(P5)There exists a constant $L^{\\prime}>0$ such that $\\|\\theta^{*}-\\theta\\|_{\\infty}\\leq L^{\\prime}\\|\\lambda_{\\theta^{*},\\xi}-\\lambda_{\\theta,\\xi}\\|_{\\infty}$ forany $\\theta\\in\\Theta=$ $[-R,R]^{|S|\\times|A|}$ ", "page_idx": 31}, {"type": "text", "text": "Define $\\theta^{\\prime*}\\,\\in\\,\\theta\\,\\in\\,\\mathbb{R}^{|S|\\times|A|}$ such that $\\begin{array}{r}{\\theta_{s,a}^{\\prime*}=\\theta_{s,a}^{*}+\\frac{1}{|A|}\\sum_{a^{\\prime}}(\\theta_{s,a^{\\prime}}-\\theta_{s,a^{\\prime}}^{*})}\\end{array}$ Then it can be eaily verified that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{a^{\\prime}}\\pi_{\\theta^{\\prime*},\\xi}^{\\prime*}=\\pi_{\\theta^{*},\\xi},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\pi_{\\theta}(a|s)=\\frac{\\exp(\\theta_{s,a})}{\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}})}}\\end{array}$ and $\\begin{array}{r}{\\pi_{\\theta^{\\prime}}(a|s)=\\frac{\\exp(\\theta_{s,a}^{\\prime})}{\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}}^{\\prime})}}\\end{array}$ $\\theta_{s,a}^{\\prime}-\\theta_{s,a}=\\log\\pi_{\\theta^{\\prime}}(a|s)-$ $\\log\\pi_{\\theta}(a|s)+u(s)$ where $\\begin{array}{r}{u(s):=\\log\\big[\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}}^{\\prime})\\big]-\\log\\big[\\sum_{a^{\\prime}}\\exp(\\theta_{s,a^{\\prime}})\\big]}\\end{array}$ . Then combining with Eq. (81), we obtain that $\\begin{array}{r}{u(s)=\\frac{1}{|A|}\\sum_{a^{\\prime}}[\\log\\pi_{\\theta}(a^{\\prime}|s)-\\log\\pi_{\\theta^{\\prime}}(a^{\\prime}|s)]}\\end{array}$ Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\log\\pi_{\\delta}(a)>-\\log\\pi_{\\delta}(a)|s\\}+\\frac{1}{\\int_{\\mathbb{Z}}\\pi_{\\delta}}\\log\\pi_{\\delta}(a|s)-\\log\\pi_{\\delta}(a^{\\prime}|s)\\}}\\\\ &{\\overset{(i)}{\\le}\\frac{\\pi_{\\delta}}{\\sum_{m=1}^{1}\\log\\pi_{\\delta}}(a)s-\\pi_{\\delta}(a)s\\Big|+\\frac{\\pi_{\\delta}^{-1}}{\\int_{\\mathbb{Z}}|a|_{s}^{\\prime}}\\sum_{\\alpha^{\\prime}}|\\pi_{\\delta}(a^{\\prime}|s)-\\pi_{\\delta^{\\prime}}(a^{\\prime}|s)|}\\\\ &{\\le2\\pi_{\\delta}^{-1}\\operatorname*{max}\\left[\\pi_{\\delta}(a^{\\prime}|s)-\\pi_{\\delta}(a^{\\prime}|s)\\right]}\\\\ &{\\le2\\pi_{\\delta}^{-1}\\operatorname*{max}\\left[\\frac{1}{3}\\pi_{\\delta}(a^{\\prime}|s)-\\frac{1}{3}\\pi_{\\delta}(a^{\\prime}|s)\\right]}\\\\ &{\\le2\\pi_{\\delta}^{-1}\\operatorname*{max}\\left[\\frac{1}{3}\\pi_{\\delta}(a^{\\prime}|s)-\\frac{1}{3}\\pi_{\\delta}(a^{\\prime}|s)\\right]}\\\\ &{=2\\pi_{\\delta}^{-1}\\operatorname*{max}\\left[\\left[\\frac{3+\\pi_{\\delta}}{3}\\sum_{k=\\delta}^{\\infty}\\left(s\\right)-\\frac{3+\\pi_{\\delta}(a^{\\prime}|s)}{3}+\\frac{3+\\pi_{\\delta}(a^{\\prime}|s)}{\\lambda\\alpha_{\\delta}+(s)\\delta}\\right]\\left[3\\alpha_{\\delta,\\xi}(a)-\\lambda\\pi_{\\delta^{\\prime}}(a^{\\prime}|s)\\right]}\\\\ &{\\overset{(i i)}{\\le}\\frac{2}{\\left(1+\\lambda\\alpha_{\\delta}^{2}\\right)\\operatorname*{max}\\left[3\\alpha_{\\delta}s\\right]}|\\pi_{\\delta}(a^{\\prime}|s)-\\lambda\\pi_{\\delta}(a^{\\prime}|s)-\\lambda\\pi_{\\delta}(a^{\\prime}|s)|}\\\\ &{\\ \\ \\ \\ \\ \\ +\\left\\{\\frac{2}{\\lambda+1/3}\\frac{2}{3}\\alpha_{\\delta}^{2}\\operatorname*{max}\\left[\\frac{3}{3}\\pi_{\\delta}(a,s^{\\prime}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where (i) uses Eqs. (73) and (80) which imply that $\\pi_{\\theta}(a|s),\\pi_{\\theta^{*}}(a|s)\\,=\\,\\pi_{\\theta^{\\prime}}(a|s)\\,\\in\\,[\\pi_{\\operatorname*{min}},1]$ for $\\theta,\\theta^{*}\\in\\Theta$ ,(i) uses $\\lambda_{\\theta^{*},\\xi}(s),\\lambda_{\\theta,\\xi}(s)\\geq|A|\\lambda_{\\mathrm{min}}$ for $\\theta,\\theta^{*}\\in\\Theta$ as a result of Eq. (75). ", "page_idx": 31}, {"type": "text", "text": "Based on the definition of $\\theta^{\\prime*}$ , we have $\\begin{array}{r}{\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}-\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}=\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}-\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}\\leq}\\end{array}$ $2R$ .Therefore, for each $s\\ \\in\\ S$ , there are three cases: $\\begin{array}{r}{-R\\,\\leq\\,\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}\\,\\leq\\,\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}\\,\\leq\\,R,}\\end{array}$ $\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}>R$ and mina/ $\\theta_{s,a^{\\prime}}^{\\prime*}<-R$ , and we can define $\\theta^{\\prime\\prime}\\in\\mathbb{R}^{|S|\\times|A|}$ as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\theta_{s,a}^{\\prime\\prime}=\\left\\{\\!\\!\\begin{array}{l l}{\\theta_{s,a}^{\\prime*},\\quad}&{-R\\leq\\displaystyle\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}\\leq\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}\\leq R}\\\\ {\\theta_{s,a}^{\\prime*}-\\displaystyle\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}+R,\\quad}&{\\displaystyle\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}>R}\\\\ {\\theta_{s,a}^{\\prime*}-\\displaystyle\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}-R,\\quad}&{\\displaystyle\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}<-R}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It can be easily verified that the $\\theta^{\\prime\\prime}$ defined above satisfies $\\theta^{\\prime\\prime}\\;\\in\\;\\Theta\\;=\\;[-R,R]^{|{\\cal S}|\\times|{\\cal A}|}$ (since $\\mathrm{max}_{a^{\\prime}}\\,\\theta_{s,a^{\\prime}}^{\\prime*}\\,-\\,\\mathrm{min}_{a^{\\prime}}\\,\\theta_{s,a^{\\prime}}^{\\prime*}\\,\\le\\,2R)$ and $\\pi_{\\theta^{\\prime\\prime}}\\,=\\,\\pi_{\\theta^{\\prime*}}\\,=\\,\\pi_{\\theta^{*}}$ (the second $=$ comes from Eq. (80)). Therefore, $\\theta^{\\prime\\prime}\\in\\Theta_{\\mathrm{min}}$ and thus ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta^{*}-\\theta\\|_{\\infty}\\overset{(i)}{\\leq}\\|\\theta^{\\prime\\prime}-\\theta\\|_{\\infty}\\leq\\|\\theta^{\\prime\\prime}-\\theta^{\\prime*}\\|_{\\infty}+\\|\\theta^{\\prime*}-\\theta\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Where iluses $\\theta^{\\prime\\prime}\\in\\Theta_{\\mathrm{min}}$ alnd $\\begin{array}{r}{\\theta^{*}\\in\\arg\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta_{\\mathrm{min}}}\\|\\theta^{\\prime}-\\theta\\|_{\\infty}}\\end{array}$ To further obtain the upper bound of $\\|\\theta^{\\prime\\prime}-\\theta^{\\prime*}\\|_{\\infty}$ in the above inequality, we discuss the three aforementioned cases. ", "page_idx": 32}, {"type": "text", "text": "(Case I: When $\\begin{array}{r}{-R\\leq\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}\\leq\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}\\leq R,}\\end{array}$ we have $\\theta_{s,a}^{\\prime\\prime}-\\theta_{s,a}^{\\prime*}=0$ ", "page_idx": 32}, {"type": "text", "text": "(Cas I): When $\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}>R$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n0<\\theta_{s,a}^{\\prime*}-\\theta_{s,a}^{\\prime\\prime}=\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}-R\\overset{(i)}{\\leq}\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}-\\operatorname*{max}_{a^{\\prime}}\\theta_{s,a^{\\prime}}\\leq\\|\\theta^{\\prime*}-\\theta\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where (i) uses $\\theta\\in\\Theta=[-R,R]^{|S|\\times|A|}$ ", "page_idx": 32}, {"type": "text", "text": "(Case II): When $\\operatorname*{min}_{a^{\\prime}}$ $\\theta_{s,a^{\\prime}}^{\\prime*}<-R$ wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0<\\theta_{s,a}^{\\prime\\prime}-\\theta_{s,a}^{\\prime*}=-\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}-R\\overset{(i)}{\\leq}\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}-\\operatorname*{min}_{a^{\\prime}}\\theta_{s,a^{\\prime}}^{\\prime*}\\leq\\|\\theta^{\\prime*}-\\theta\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where (i) uses $\\theta\\in\\Theta=[-R,R]^{|S|\\times|A|}$ ", "page_idx": 32}, {"type": "text", "text": "Summarizing the above three cases, we obtain that $\\|\\theta^{\\prime\\prime}-\\theta^{\\prime*}\\|_{\\infty}\\leq\\|\\theta^{\\prime*}-\\theta\\|_{\\infty}$ and therefore Eq. (83) implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\theta^{*}-\\theta\\|_{\\infty}\\leq\\|\\theta^{\\prime\\prime}-\\theta^{\\prime*}\\|_{\\infty}+\\|\\theta^{\\prime*}-\\theta\\|_{\\infty}\\leq2\\|\\theta^{\\prime*}-\\theta\\|_{\\infty}\\overset{(i)}{\\leq}\\frac{8\\|\\lambda_{\\theta^{*},\\xi}-\\lambda_{\\theta,\\xi}\\|_{\\infty}}{|\\mathcal{A}|\\lambda_{\\operatorname*{min}}^{2}\\pi_{\\operatorname*{min}}}\\overset{\\mathrm{def}}{=}L^{\\prime},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where (i) uses Eq. (82). This proves (P5). ", "page_idx": 32}, {"type": "text", "text": "We consider Eq. (76) as a function of $\\delta$ and take its derivative as follows. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{\\partial\\left(\\theta_{\\theta_{\\theta}^{*}}\\right)_{s,\\theta}}{\\partial\\delta}\\right|}\\\\ &{=\\left|\\frac{\\lambda_{\\theta^{*}}}{(1-\\delta)\\lambda_{\\theta}\\xi(s,\\theta)}-\\lambda_{\\theta,\\xi}(s,a)+\\frac{\\lambda_{\\theta^{*}}}{(1-\\delta)\\lambda_{\\theta}\\xi(s,a)}\\exp(-\\theta_{s,a}^{*})-\\lambda_{\\theta,\\xi}(s,a)\\exp(-\\theta_{s,a})\\right|}\\\\ &{\\overset{(i)}{\\le}\\frac{\\left|\\lambda_{\\theta^{*}}\\xi-\\lambda_{\\theta,\\xi}\\right|\\left|\\lambda_{\\theta^{*}}}{\\lambda_{\\theta\\mathrm{in}}}+\\frac{\\left|\\lambda_{\\theta^{*}}\\cdot\\xi(s,a)\\right|\\exp(-\\theta_{s,a}^{*})-\\exp(-\\theta_{s,a})\\right|+\\exp(-\\theta_{s,a})\\left|\\lambda_{\\theta^{*}}\\cdot\\xi(s,a)-\\lambda_{\\theta,\\xi}(s,a)}{\\lambda_{\\theta\\mathrm{in}}\\exp(-\\theta_{s,a}^{*})}}\\\\ &{\\overset{(i i)}{\\le}\\frac{\\left|\\lambda_{\\theta^{*}}\\cdot\\xi-\\lambda_{\\theta,\\xi}\\right|\\left|\\lambda_{\\theta^{*}}}{\\lambda_{\\theta\\mathrm{in}}}+\\frac{\\exp(R)\\left|\\theta_{s,a}^{*}-\\theta_{s,a}\\right|+\\exp(R)\\left|\\lambda_{\\theta^{*}}\\cdot\\xi(s,a)-\\lambda_{\\theta,\\xi}(s,a)\\right|}{\\lambda_{\\operatorname*{min}}\\exp(-R)}}\\\\ &{\\overset{(i i i)}{\\le}\\frac{\\left|\\lambda_{\\theta^{*}}\\cdot\\xi-\\lambda_{\\theta,\\xi}\\right|\\left|\\lambda_{\\theta^{*}}}{\\lambda_{\\theta\\mathrm{in}}}+\\frac{\\exp(R)\\left|\\theta_{s,a}^{*}-\\theta_{s,a}\\right|+\\exp(R)\\left|\\lambda_{\\theta^{*}}\\cdot\\xi(s,a)-\\lambda_{\\theta,\\xi}(s,a)\\right|}{\\lambda_{\\operatorname*{min}}\\exp(-R)}}\\\\ &{\\overset{(i i i)}{\\le}\\frac{\\left|\\lambda_{\\theta^{*}}\\cdot\\xi-\\lambda_{\\theta,\\xi}\\right|\\left|\\infty}{\\lambda_{\\theta^{*}}}+\\exp(2R)\\cdot\\frac{L^{\\left|\\mathbb{I}\\right|\\lambda_{\\theta^{\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where (i) uses $\\theta,\\theta^{*}\\in\\Theta=[-R,R]^{|\\cal S|\\times|\\cal A|}$ and Eq. (75), (i) uses $\\theta,\\theta^{*}\\in\\Theta=[-R,R]^{|\\cal S|\\times|\\cal A|}$ (ii) uses Eq. (84). ", "page_idx": 32}, {"type": "text", "text": "Therefore, for any $\\delta,\\delta^{\\prime}\\in[0,1]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{\\theta,\\xi}^{(\\delta^{\\prime})}-\\theta_{\\theta,\\xi}^{(\\delta)}\\|_{\\infty}\\leq\\!\\frac{2(L^{\\prime}+1)\\exp(2R)}{\\lambda_{\\operatorname*{min}}}\\|\\lambda_{\\theta^{*},\\xi}-\\lambda_{\\theta,\\xi}\\|_{\\infty}|\\delta^{\\prime}-\\delta|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\!\\frac{2(L^{\\prime}+1)\\exp(2R)}{\\lambda_{\\operatorname*{min}}}\\|\\lambda_{\\theta,\\xi}^{(\\delta^{\\prime})}-\\lambda_{\\theta,\\xi}^{(\\delta)}\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which proves the statement (P4) and thus proves Assumption 8. ", "page_idx": 32}, {"type": "text", "text": "Assumption 4 can be proved in the same way simply by replacing $\\theta^{*}$ with any $\\theta^{*}(\\xi)\\ \\ \\in$ arg $\\mathrm{min}_{\\theta^{\\prime}\\in\\Theta}\\,f(\\lambda_{\\theta^{\\prime},\\xi})$ ", "page_idx": 32}, {"type": "text", "text": "LProof of Proposition 9 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The estimated occupancy measure (40) is an unbiased estimator of the following truncated occupancy measure with truncation level $H_{\\lambda}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\lambda_{\\theta,\\xi}^{(H_{\\lambda})}(s,a)\\ {\\stackrel{\\mathrm{def}}{=}}\\ (1-\\gamma)\\sum_{t=0}^{H_{\\lambda}-1}\\gamma^{t}{\\mathbb P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a|s_{0}\\sim\\rho).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Denote $\\widehat{\\lambda}(\\tau^{(\\lambda)}):=\\left.\\left[\\widehat{\\lambda}(\\tau^{(\\lambda)};s,a)\\right]_{s,a\\in{\\cal S}\\times{\\cal A}}\\in\\mathbb{R}^{|{\\cal S}||{\\cal A}|}$ $\\lambda_{\\theta,\\xi}^{(H_{\\lambda})}\\,:=\\,\\big[\\lambda_{\\theta,\\xi}^{(H_{\\lambda})}(s,a)\\big]_{s,a\\in\\ensuremath{\\mathcal{S}}\\times A}\\,\\in\\,\\mathbb{R}^{|\\ensuremath{\\mathcal{S}}||A|},$ $\\lambda_{\\theta,\\xi}:=\\,\\left[\\lambda_{\\theta,\\xi}(s,a)\\right]_{s,a\\in\\mathcal{S}\\times\\mathcal{A}}\\in\\mathbb{R}^{|\\dot{S}||\\mathcal{A}|}$ , Then the estimation errorof occupancy measure has the following upper bound. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\Big\\lVert\\widehat{\\lambda}(\\tau^{(\\lambda)})-\\lambda_{\\theta,\\xi}\\Big\\rVert^{2}}\\\\ &{\\stackrel{(i)}{=}\\mathrm{Var}_{\\pi_{\\theta},p_{\\xi}}\\Big[\\widehat{\\lambda}(\\tau^{(\\lambda)})\\Big]+\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\Big\\lVert\\lambda_{\\theta,\\xi}^{(H_{\\lambda})}-\\lambda_{\\theta,\\xi}\\Big\\rVert^{2}}\\\\ &{\\stackrel{(i i)}{=}\\frac{1}{m_{\\lambda}}\\mathrm{Var}\\big[\\widehat{\\lambda}_{1}(\\tau_{1}^{(\\lambda)})\\big]+\\displaystyle\\sum_{s,a}\\Big|(1-\\gamma)\\sum_{t=H_{\\lambda}}^{+\\infty}\\gamma^{t}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a|s_{0}\\sim\\rho)\\Big|^{2}}\\\\ &{\\stackrel{(i i i)}{\\le}\\frac{1}{m_{\\lambda}}\\mathbb{E}\\big\\lVert\\widehat{\\lambda}_{1}(\\tau_{1}^{(\\lambda)})\\big\\rVert^{2}+\\Big[(1-\\gamma)\\displaystyle\\sum_{t=H_{\\lambda}}^{+\\infty}\\gamma^{t}\\Big]\\sum_{s,a}\\Big[(1-\\gamma)\\displaystyle\\sum_{t=H_{\\lambda}}^{+\\infty}\\gamma^{t}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a|s_{0}\\sim\\rho)\\Big]}\\\\ &{\\stackrel{(i v)}{\\le}\\frac{1}{m_{\\lambda}}+\\gamma^{2H_{\\lambda}},}&{(87)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where (i) uses $\\mathbb{E}\\|X\\|^{2}=\\operatorname{Var}X+\\|\\mathbb{E}X\\|^{2}$ for random vector $X:=\\widehat{\\lambda}(\\tau^{(\\lambda)})-\\lambda_{\\theta,\\xi}$ , (ii) uses Eqs. (1) and (86) and uses the fact that $\\widehat{\\lambda}$ defined by Eq. (40) is average among the $m_{\\lambda}$ i.i.d. individual estimators $\\begin{array}{r}{\\widehat{\\lambda}_{i}(\\tau_{i}^{(\\lambda)};s,a):=(1-\\gamma)\\sum_{h=0}^{H_{\\lambda}-1}\\dot{\\gamma^{h}}\\mathbb{1}\\bar{\\{{s_{i,h}^{(\\lambda)}=s,a_{i,h}^{(\\lambda)}=a\\}}}}\\end{array}$ $i=1,\\dots,m_{\\lambda}$ $\\mathrm{Var}X\\leq\\mathbb{E}\\|X\\|^{2}$ for random vector $X:=\\widehat{\\lambda}_{1}(\\tau^{(\\lambda)})$ and $\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a|s_{0}\\sim\\rho)\\in[0,1]$ and (iv) uses $\\begin{array}{r}{0\\!\\leq\\!\\|\\widehat{\\lambda}_{1}(\\tau_{i}^{(\\lambda)}\\|^{2}\\!\\leq\\!\\sum_{s,a}\\widehat{\\lambda}_{1}(\\tau_{i}^{(\\lambda)};s,a)=1}\\end{array}$ and $\\begin{array}{r}{\\sum_{s,a}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a|s_{0}\\sim\\rho)=1}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Define the cost function as $c:=\\nabla_{\\lambda}f(\\lambda_{\\theta,\\xi})$ . The error of estimating $c$ by $\\widehat{c}:=\\nabla_{\\lambda}f[\\widehat{\\lambda}(\\tau^{(\\lambda)})]$ has the following upper bounds. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\|\\widehat{c}-c\\|_{\\infty}^{2}=\\!\\!\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\|\\nabla_{\\lambda}f\\big[\\widehat{\\lambda}(\\tau^{(\\lambda)})-\\nabla_{\\lambda}f(\\lambda_{\\theta,\\xi})\\big]\\|_{\\infty}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\overset{(i)}{\\le}\\!\\!L_{\\lambda}^{2}\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\|\\widehat{\\lambda}(\\tau^{(\\lambda)})-\\lambda_{\\theta,\\xi}\\|^{2}}\\\\ &{\\qquad\\qquad\\quad\\overset{(i i)}{\\le}\\!\\!L_{\\lambda}^{2}\\Big(\\frac{1}{m_{\\lambda}}+\\gamma^{2H_{\\lambda}}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\Vert\\widehat{c}-c\\Vert^{2}\\leq|S||A|\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\Vert\\widehat{c}-c\\Vert_{\\infty}^{2}\\leq L_{\\lambda}^{2}|S||A|\\Big(\\frac{1}{m_{\\lambda}}+\\gamma^{2H_{\\lambda}}\\Big)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where (i) uses Assumption 2 and (ii) uses Eq. (87). Also, $c$ and $\\widehat{c}$ have the following norm bound based on Assumption 2. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\|c\\|_{\\infty},\\|\\widehat{c}\\|_{\\infty}\\right)\\leq\\ell_{\\lambda}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that $\\boldsymbol{g}^{(\\theta)}(\\tau^{(\\theta)},\\theta,\\xi,c)$ defined by Eq.(41) (replace $\\widehat{c}$ Wwith $c$ ) isthe average of the following $m_{\\theta}$ i.i.d. individual stochastic gradients. ", "page_idx": 33}, {"type": "equation", "text": "$$\ng_{i}^{(\\theta)}(\\tau_{i}^{(\\theta)},\\theta,\\xi,c)=\\sum_{t=0}^{H_{\\theta}-1}\\gamma^{t}c(s_{i,t}^{(\\theta)},a_{i,t}^{(\\theta)})\\sum_{h=0}^{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{i,h}^{(\\theta)}\\mid s_{i,h}^{(\\theta)}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then it can be easily sen that $g_{i}^{(\\theta)}(\\tau_{i}^{(\\theta)},\\theta,\\xi,c)$ defned above is an unbiased estimator of the following truncated policy gradient. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}f^{(H_{\\theta})}(\\lambda_{\\theta,\\xi})=\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left[\\sum_{t=0}^{H_{\\theta}-1}\\gamma^{t}c(s_{t},a_{t})\\sum_{h=0}^{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{h}|s_{h})\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Also, $g_{i}^{(\\theta)}(\\tau_{i}^{(\\theta)},\\theta,\\xi,c)$ can be bouded as follows byusing Eq. (90) and Assumption 1. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|g_{i}^{(\\theta)}(\\tau_{i}^{(\\theta)},\\theta,\\xi,c)\\|\\leq\\sum_{t=0}^{H_{\\theta}-1}\\gamma^{t}|c(s_{i,t}^{(\\theta)},a_{i,t}^{(\\theta)})|\\sum_{h=0}^{t}\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{i,h}^{(\\theta)}\\mid s_{i,h}^{(\\theta)})\\|\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\leq\\sum_{t=0}^{H_{\\theta}-1}(t+1)\\gamma^{t}\\ell_{\\lambda}\\ell_{\\pi_{\\theta}}\\leq\\frac{\\ell_{\\lambda}\\ell_{\\pi_{\\theta}}}{(1-\\gamma)^{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we can prove Eq. (43) as follows, and Eq. (44) can be proved using the same logic. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\pi\\pi_{1}\\sim\\pi_{1}}[\\Big|\\Phi^{\\pi}(r^{(s)},\\theta,\\xi)-\\nabla e^{j(\\lambda_{\\delta})}e^{j(\\lambda_{\\delta})}]\\Big|^{2}}\\\\ &{\\le\\!\\!\\!\\Delta\\pi_{\\pi_{1}\\sim\\pi_{1}}[\\Big|\\Phi^{\\pi}(r^{(s)},\\theta,\\xi)-\\varphi^{\\pi}(\\pi^{(s)},\\theta,\\xi)\\Big|^{2}}\\\\ &{\\quad+3\\pi_{\\pi_{1}\\sim\\pi_{1}}[\\Big|\\Phi^{\\pi}(r^{(s)},\\theta,\\xi)-\\nabla e^{j(\\lambda_{\\delta})}(\\cdot\\Delta_{\\pi_{1}\\sim\\pi_{1}}^{(s)})]^{2}+3\\pi_{1}\\nabla_{\\pi}\\cdot\\!\\!e^{j(\\lambda_{\\delta})}(\\cdot\\Delta_{\\pi_{\\delta}})-\\nabla e^{j(\\lambda_{\\delta})}]\\Big|^{2}}\\\\ &{\\overset{(a)}{\\le}\\!\\!\\!32\\pi_{1}\\!+\\!\\!\\frac{1}{\\ln{\\pi_{1}}}\\!\\displaystyle\\frac{1}{m_{\\pi}^{2}}\\sum_{i=1}^{\\pi}\\gamma_{i}\\big[\\langle\\Phi_{i\\pi}^{(s)},\\theta,\\hat{e}_{i\\pi}^{(s)}-e^{j(\\lambda_{\\pi_{1}}^{(s)},\\theta,\\xi)}\\rangle-\\langle\\Phi_{i\\pi}^{(s)},\\theta,\\Phi_{\\pi}(\\theta,\\phi_{\\pi}^{(s)},\\theta_{\\pi}^{(s)})\\big|^{2}\\big]}\\\\ &{\\quad+3\\pi_{\\pi_{1}\\sim\\pi_{1}}[\\Big|\\bar{\\rho}^{(s)}(\\pi^{(s)},\\theta,\\xi)\\rangle+3\\Big|\\nabla_{\\pi_{1}\\sim\\pi_{1}}\\frac{1}{m_{\\pi}^{2}}\\gamma_{i}\\big|\\gamma_{i}(\\cdot\\Delta_{\\pi_{1}},\\theta)\\displaystyle\\sum_{j=1}^{\\pi}\\gamma_{\\nabla}\\big|\\bar{\\rho}_{i\\pi}\\log\\pi_{\\pi}(\\bar{u}_{i\\pi})\\big|^{2}}\\\\ &{\\quad+3\\pi_{\\pi_{1}\\sim\\pi_{1}}[\\Big|\\bar{\\rho}^{(s)}(\\pi^{(s)},\\theta,\\xi)\\Big|+3\\Big|\\mathbf \n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where (i) uses Eqs. (8),(41),(92) and $\\nabla_{\\theta}f^{(H_{\\theta})}(\\lambda_{\\theta,\\xi})=\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}g^{(\\theta)}(\\tau^{(\\theta)},\\theta,\\xi,c)$ (i) uses Eq. (90), Assumnption and $\\begin{array}{r}{g^{(\\theta)}(\\tau^{(\\theta)},\\theta,\\xi,c)=\\frac{1}{m_{\\theta}}\\sum_{i=1}^{m_{\\theta}}g_{i}^{(\\theta)}(\\tau_{i}^{(\\theta)},\\theta,\\xi,c)}\\end{array}$ where $\\{g_{i}^{(\\theta)}(\\tau_{i}^{(\\theta)},\\theta,\\xi,c)\\}_{i=1}^{m_{\\theta}}$ are independent, (ii) uses Eqs. (89) and (93). ", "page_idx": 34}, {"type": "text", "text": "M  Proof of Theorem 1 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The policy gradient (8) is proved by Eqs. (5) and (6) of [6]. We will next prove the transition gradient (9). ", "page_idx": 34}, {"type": "text", "text": "Under the transition kernel $p_{\\xi}$ and policy $\\pi_{\\theta}$ , the probability of obtaining a certain trajectory $\\tau_{t}=$ $\\left\\{s_{h},a_{h}\\right\\}_{h=0}^{t}\\cup\\left\\{s_{t+1}\\right\\}$ with initial state distribution $\\rho$ can be expressed as follows. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})=\\rho(s_{0})\\prod_{h=0}^{t}\\left[\\pi_{\\theta}(a_{h}|s_{h})p_{\\xi}(s_{h+1}|s_{h},a_{h})\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence, the gradient of the log of the above probability can be computed as follows. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla_{\\xi}\\log\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})=\\sum_{h=0}^{t}\\nabla_{\\xi}\\log p_{\\xi}(s_{h+1}\\vert s_{h},a_{h}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Denote $c:=\\nabla_{\\lambda}f(\\lambda_{\\theta,\\xi})$ . Then the transition gradient (9) can be obtained as follows. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi})=\\nabla_{\\lambda}f(\\lambda_{\\theta,\\xi})^{\\top}\\nabla_{\\xi}\\lambda_{\\theta,\\xi}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{s,a}c(s,a)\\nabla_{\\xi}\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(s_{t}=s,a_{t}=a)}\\\\ &{\\qquad\\qquad=\\nabla_{\\xi}\\displaystyle\\sum_{t=0}^{\\infty}\\int\\gamma^{t}c(s_{t},a_{t})\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})\\mathrm{d}\\tau_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\sum_{t=0}^{\\infty}\\int\\gamma^{t}c(s_{t},a_{t})\\nabla_{\\xi}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})\\mathrm{d}\\tau_{t}}\\\\ {\\displaystyle=\\sum_{t=0}^{\\infty}\\int\\gamma^{t}c(s_{t},a_{t})\\left[\\frac{\\nabla_{\\xi}\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})}{\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})}\\right]\\cdot\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})\\mathrm{d}\\tau_{t}}\\\\ {\\displaystyle=\\sum_{t=0}^{\\infty}\\mathbb{E}_{\\tau_{t}\\sim\\mathbb{P}_{\\tau_{\\theta},p_{\\xi}}}\\left[\\gamma^{t}c(s_{t},a_{t})\\nabla_{\\xi}\\log\\mathbb{P}_{\\pi_{\\theta},p_{\\xi}}(\\tau_{t})\\right]s_{0}\\sim\\rho\\right]}\\\\ {\\displaystyle\\overset{(i)}{=}\\mathbb{E}_{\\pi_{\\theta},p_{\\xi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}c(s_{t},a_{t})\\sum_{h=0}^{t}\\nabla_{\\xi}\\log p_{\\xi}(s_{h+1}|s_{h},a_{h})\\right]s_{0}\\sim\\rho^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where (i) uses Eq. (94). ", "page_idx": 35}, {"type": "text", "text": "NProof of Theorem 2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Based on Proposition 9, we define the following error terms $E_{j}^{(\\xi)}$ and $E_{j}^{(\\theta)}\\;(i,j\\in\\{1,2,3,4\\})$ bound the estimation errors of the stochastic gradients in lines 6, 10, 17 and 21 of Algorithm 1 respectively. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{j}^{(\\theta)}:=\\!\\frac{3\\ell_{\\pi_{\\theta}}^{2}}{(1-\\gamma)^{4}}\\Big[L_{\\lambda}^{2}|S||A|\\Big(\\frac{1}{m_{\\lambda}^{(j)}}+\\gamma^{2H_{\\lambda}^{(j)}}\\Big)+\\frac{\\ell_{\\lambda}^{2}}{m_{\\theta}^{(j)}}+\\ell_{\\lambda}^{2}[1+H_{\\theta}^{(j)}(1-\\gamma)]^{2}\\gamma^{2H_{\\theta}^{(j)}}\\Big],}\\\\ &{E_{j}^{(\\xi)}:=\\!\\frac{3\\ell_{p_{\\xi}}^{2}}{(1-\\gamma)^{4}}\\Big[L_{\\lambda}^{2}|S||A|\\Big(\\frac{1}{m_{\\lambda}^{(j)}}+\\gamma^{2H_{\\lambda}^{(j)}}\\Big)+\\frac{\\ell_{\\lambda}^{2}}{m_{\\xi}^{(j)}}+\\ell_{\\lambda}^{2}[1+H_{\\xi}^{(j)}(1-\\gamma)]^{2}\\gamma^{2H_{\\xi}^{(j)}}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then we prove Theorem 2 in the following procedures. ", "page_idx": 35}, {"type": "text", "text": "N.1  Convergence Rate of Inner Update Step (14) of the First Original Phase ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Lemma7.SupposeAssumptions $^{1-4}$ hold. Apply the projected stochastic gradient descent step (14) inAlgorithm 1 to the policy optimization problem $\\operatorname*{min}_{\\theta\\in\\Theta}$ $f(\\lambda_{\\theta,\\xi_{k}})$ with fixed $\\xi_{k}\\in\\Xi$ Select stepsize \u03b1 = 2L0.,e and initialization $\\theta_{k,0}=\\theta_{0}$ . Then the output $\\theta_{k}:=\\theta_{k,T}$ globallyconvergesat thefollowingrateforany $\\delta\\in[0,\\overline{{\\delta}}]$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\big[f(\\lambda_{\\theta_{k},\\xi_{k}})-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\,f(\\lambda_{\\theta,\\xi_{k}})\\big|\\xi_{k}\\big]}\\\\ &{\\leq\\!(1-\\delta)^{T}\\mathbb{E}\\big[f(\\lambda_{\\theta_{0},\\xi_{k}})-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\,f(\\lambda_{\\theta,\\xi_{k}})\\big|\\xi_{k}\\big]+4L_{\\theta,\\theta}\\ell_{\\lambda^{-1}}^{2}\\delta+\\frac{E_{1}^{(\\theta)}}{\\delta L_{\\theta,\\theta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $E_{1}^{(\\theta)}$ is defined in Eq. (95). ", "page_idx": 35}, {"type": "text", "text": "Proof. For any $\\theta_{k,t}\\,\\in\\,\\Theta$ in the update rule (14), there exists at least one optimal policy $\\theta_{k,t}^{*}\\in$ arg $\\mathrm{min}_{\\theta^{\\prime}\\in\\Theta}f(\\lambda_{\\theta^{\\prime},\\xi_{k}})$ such that for any $\\delta\\,\\in\\,[0,\\overline{{\\delta}}]$ , there exists $\\theta_{k,t}^{(\\delta)}\\,\\in\\,\\Theta$ that aisfe \u5165k $(1-\\delta)\\lambda_{\\theta_{k,t},\\xi_{k}}+\\delta\\lambda_{\\theta_{k,t}^{*},\\xi_{k}}.$ Since $f$ is convex, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nf(\\lambda_{\\theta_{k,t}^{(\\delta)},\\xi_{k}})\\leq(1-\\delta)f(\\lambda_{\\theta_{k,t},\\xi_{k}})+\\delta f(\\lambda_{\\theta_{k,t}^{*},\\xi_{k}})=(1-\\delta)f(\\lambda_{\\theta_{k,t},\\xi_{k}})+\\delta\\operatorname*{min}_{\\theta\\in\\Theta}f(\\lambda_{\\theta,\\xi_{k}}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, we analyze the optimization progress of the stochastic gradient descent step (14) as follows. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\;f(\\lambda_{\\theta_{k,t+1},\\xi_{k}})}\\\\ &{\\stackrel{(i)}{\\le}f(\\lambda_{\\theta_{k,t},\\xi_{k}})+\\nabla_{\\theta}f(\\lambda_{\\theta_{k,t},\\xi_{k}})^{\\top}(\\theta_{k,t+1}-\\theta_{k,t})+\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{k,t+1}-\\theta_{k,t}\\|^{2}}\\\\ &{=f(\\lambda_{\\theta_{k,t},\\xi_{k}})+\\big[\\nabla_{\\theta}f(\\lambda_{\\theta_{k,t},\\xi_{k}})-g_{k,t}^{(\\theta)}\\big]^{\\top}(\\theta_{k,t+1}-\\theta_{k,t})}\\\\ &{\\quad+\\,(g_{k,t}^{(\\theta)})^{\\top}(\\theta_{k,t+1}-\\theta_{k,t})+\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{k,t+1}-\\theta_{k,t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{P(u_{k-1},d_{m})+\\frac{1}{2}\\|u_{k-1}^{\\prime}\\|^{2}u_{k-1}^{\\prime}\\}=\\frac{\\alpha}{\\alpha_{k-1}}\\alpha_{k}\\alpha_{k-1}-\\beta_{k}}\\\\ &{\\quad+\\alpha_{k}^{2}\\alpha_{k}^{2}\\Bigg\\{V(u_{k-1},d_{m})+\\frac{1}{2}\\mathbb{E}\\{P(u_{k-1},d_{m})+P(u_{k-1},d_{m})^{2}}\\\\ &{\\le2\\alpha_{k}\\alpha_{k}\\alpha_{k}+\\alpha_{k}^{2}\\alpha_{k}^{2}\\alpha_{k}-\\alpha_{k}^{2}\\alpha_{k}^{2}\\alpha_{k}^{2}}\\\\ &{\\quad+\\frac{4\\alpha_{k}^{2}\\alpha_{k}\\alpha_{k}}{\\alpha_{k}\\alpha_{k}}+\\alpha_{k}^{2}\\alpha_{k}^{2}\\alpha_{k}^{2}\\alpha_{k}-\\beta_{k}\\alpha_{k}^{2}\\beta_{k}}\\\\ &{\\quad+\\frac{4\\alpha_{k}^{2}\\alpha_{k}\\alpha_{k}}{\\alpha_{k}\\alpha_{k}}\\alpha_{k}+\\alpha_{k}^{2}\\alpha_{k}^{2}\\alpha_{k}^{2}\\alpha_{k}^{3}+\\beta_{k}\\alpha_{k}^{4}\\alpha_{k}^{4}-\\alpha_{k}\\alpha_{k}^{2}\\alpha_{k}^{2}}\\\\ &{\\stackrel{(a)}{\\le}\\frac{4\\alpha_{k}\\alpha_{k}\\alpha_{k}\\alpha_{k}}{1-\\alpha_{k}^{2}\\alpha_{k}^{3}\\alpha_{k}^{4}}\\Bigg\\}}\\\\ &{\\quad+\\frac{4\\alpha_{k}^{2}\\alpha_{k}\\alpha_{k}\\alpha_{k}}{1-\\alpha_{k}^{3}\\alpha_{k}^{2}\\alpha_{k}\\alpha_{k}\\alpha_{k}^{3}\\alpha_{k}^{4}}}\\\\ &{\\le2\\alpha_{k}^{2}\\alpha_{k}\\alpha_{k}+\\alpha_{k}^{2}\\alpha_{k}^{2}\\alpha_{k}^{4}-\\alpha_{k}^{3}\\alpha_{k}^{4}\\alpha_{k}^{5}-\\alpha_{k}^{6}\\alpha_{k}^{6}\\alpha_{k}^{2}}\\\\ &{\\quad+\\alpha_{k}^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (i) and (ii) use $L_{\\theta,\\theta}$ -smoothness of $f(\\lambda._{,\\xi_{k}})$ based on Proposition 3, (i) uses the update rule (14) with stepsize $\\begin{array}{r}{\\alpha=\\frac{1}{2L_{\\theta,\\theta}}}\\end{array}$ which implies that $\\begin{array}{r}{\\dot{\\theta}_{k,t+1}\\in\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\left[(g_{k,t}^{(\\theta)})^{\\top}(\\theta-\\theta_{k,t})+L_{\\theta,\\theta}\\right\\|\\theta-}\\end{array}$ $\\theta_{k,t}\\|^{2}]$ , (iv) uses Eq. (98) and the following inequality. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\big\\|\\theta_{k,t}^{(\\delta)}-\\theta_{k,t}\\big\\|\\overset{(i)}{\\le}\\ell_{\\lambda^{-1}}\\big\\|\\lambda_{\\theta_{k,t}^{(\\delta)},\\xi_{k}}-\\lambda_{\\theta_{k,t},\\xi_{k}}\\big\\|=\\ell_{\\lambda^{-1}}\\delta\\big\\|\\lambda_{\\theta_{k,t}^{*},\\xi_{k}}-\\lambda_{\\theta_{k,t},\\xi_{k}}\\big\\|\\overset{(i i)}{\\le}\\sqrt{2}\\ell_{\\lambda^{-1}}\\delta,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (i) uses Assumption 4 and (i) uses Lemma 4. Rearranging Eq. (99) and taking conditional expectation,weobtain that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\big[f\\big(\\lambda_{\\theta_{k,t+1},\\xi_{k}}\\big)-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}f(\\lambda_{\\theta,\\xi_{k}})\\big|\\xi_{k}\\big]}\\\\ &{\\leq\\!(1-\\delta)\\mathbb{E}\\big[f\\big(\\lambda_{\\theta_{k,t},\\xi_{k}}\\big)-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}f(\\lambda_{\\theta,\\xi_{k}})\\big|\\xi_{k}\\big]\\!+\\!\\frac{1}{L_{\\theta,\\theta}}\\mathbb{E}\\big[\\|\\nabla_{\\theta}f\\big(\\lambda_{\\theta_{k,t},\\xi_{k}}\\big)-g_{k,t}^{(\\theta)}\\|^{2}\\big|\\xi_{k}\\big]\\!+\\!4L_{\\theta,\\theta}\\ell_{\\lambda-1}^{2}\\delta^{2}}\\\\ &{\\stackrel{(i)}{\\le}\\!(1-\\delta)\\mathbb{E}\\big[f\\big(\\lambda_{\\theta_{k,t},\\xi_{k}}\\big)-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}f(\\lambda_{\\theta,\\xi_{k}})\\big|\\xi_{k}\\big]+4L_{\\theta,\\theta}\\ell_{\\lambda-1}^{2}\\delta^{2}+\\frac{E_{1}^{(\\theta)}}{L_{\\theta,\\theta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where (i) uses Eq. (43) in Proposition 9 and $E_{1}^{(\\theta)}$ defined in Eq. (95) with $j\\;=\\;1$ .Then the convergence rate (97) can be proved by iterating the above inequality as follows. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "N.2  Convergence Rate of $\\mathbb{E}[\\|\\nabla\\widetilde{\\Phi}(\\widetilde{\\xi})\\|^{2}]$ from the First Original Phase ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma8. Imlement the frst original phase of Algoritm 1 with stepsize\u03b1=2 and $\\beta=$ $\\frac{1}{2L_{\\xi,\\xi}\\sqrt{K}}$   \n$\\epsilon_{0}>0$ as follows. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(\\lambda_{\\theta_{k},\\xi_{k}})-\\operatorname*{min}_{\\theta\\in\\Theta}f(\\lambda_{\\theta,\\xi_{k}})\\big|\\xi_{k}\\big]\\leq\\epsilon_{0}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then,theoutput $\\tilde{\\xi}$ of the frst original phase has the following convergence rate. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\big\\|\\nabla\\widetilde{\\Phi}(\\widetilde{\\xi})\\big\\|^{2}\\big]\\leq\\frac{8f^{*}-8\\mathbb{E}\\big[\\Phi(\\xi_{0})\\big]}{K\\beta}+10L_{\\xi,\\xi}\\beta\\ell_{\\xi}^{2}+20L_{\\xi,\\xi}\\epsilon_{0}+20E_{2}^{(\\xi)},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $E_{2}^{(\\xi)}$ is defined in Eq. (96) with $j=2$ ", "page_idx": 37}, {"type": "text", "text": "Proof. For any fixed $\\xi\\in\\Xi$ , define the optimal policy parameter $\\theta^{*}(\\xi)$ and the optimal utility value $\\Phi(\\xi)$ as follows. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta^{*}(\\xi):\\=\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\,f(\\lambda_{\\theta,\\xi})}\\\\ &{\\Phi(\\xi):=\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\,f(\\lambda_{\\theta,\\xi})=f(\\lambda_{\\theta^{*}(\\xi),\\xi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $f(\\lambda_{\\theta,\\cdot})$ is $L_{\\xi,\\xi}$ -smooth for any $\\theta\\;\\in\\;\\Theta$ based on Proposition 3, for any $(\\theta,\\xi)\\;\\in\\;\\Theta\\,\\times\\,\\Xi$ $\\begin{array}{r}{f(\\lambda_{\\theta,\\xi^{\\prime}})\\,-\\,\\frac{L_{\\xi,\\xi}}{2}\\|\\xi^{\\prime}-\\xi\\|^{2}}\\end{array}$ is a concave function of $\\xi^{\\prime}$ . As a result, $\\Phi(\\xi^{\\prime})\\,-\\,L_{\\xi,\\xi}\\,\\|\\xi^{\\prime}\\,-\\,\\xi\\|^{2}$ is a $L_{\\xi,\\xi}$ -strongly concave function of $\\xi^{\\prime}$ and thus it has the following unique maximizer. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\xi^{*}(\\xi):=\\underset{\\xi^{\\prime}\\in\\Xi}{\\arg\\operatorname*{max}}\\left[\\Phi(\\xi^{\\prime})-L_{\\xi,\\xi}\\|\\xi^{\\prime}-\\xi\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Accordingly, we define the following Moreau envelope function (repeat Eq. (18)). ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\widetilde{\\Phi}(\\xi):=\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi}\\big[\\Phi(\\xi^{\\prime})-L_{\\xi,\\xi}\\|\\xi^{\\prime}-\\xi\\|^{2}\\big]=\\Phi[\\xi^{*}(\\xi)]-L_{\\xi,\\xi}\\|\\xi^{*}(\\xi)-\\xi\\|^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Based on Lemma 3.6 of [31], $\\widetilde{\\Phi}$ is $L_{\\xi,\\xi}$ -smooth with ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla\\widetilde{\\Phi}(\\xi)=2L_{\\xi,\\xi}[\\xi^{*}(\\xi)-\\xi]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Similar to Lemma D.3 of [31], we obtain the following ascent property of the above envelope function $\\widetilde{\\Phi}$ for any $k=0,1,\\ldots,K-1$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\left\\|\\bar{\\mathcal{X}}^{*}(\\Delta)\\right\\|_{\\mathcal{X}}^{2}\\left(\\Delta\\xi\\right)\\}=\\frac{\\mathcal{G}_{p,q}\\left(\\Delta\\xi\\right)}{2}\\mathrm{i}\\mathcal{,f}\\left(\\Delta\\xi\\right)}\\\\ &{\\quad\\mathbb{E}\\left(\\left\\{\\phi_{q}\\left(\\Delta\\xi\\right)\\right\\}\\right|_{\\mathcal{X}}^{2}\\left(\\Delta\\xi\\right)\\mathrm{e}^{\\mathrm{i}\\phi_{1}}\\!\\!\\right)\\!\\Bigg\\{\\!\\mathrm{f}(\\Delta\\xi)\\!\\Bigg\\}\\!=\\!\\!\\!\\frac{\\mathcal{G}_{p,q}\\left(\\Delta\\phi_{q}\\right)}{2}[\\Delta\\phi_{q}\\!\\Bigg]\\!\\Bigg|_{\\mathcal{X}}^{2}}\\\\ &{\\quad\\times\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (i) uses Eqs. (15) and (105), (i) holds for any $\\tau_{k}>0$ whose value is to be determined later, (ii) uses contraction property of projection and $\\xi^{*}(\\xi_{k})\\in\\Xi$ , (iv) uses Propositions 3-9 and the error term $E_{2}^{(\\xi)}$ eedinEq96,E0013,ius10wh that $\\widetilde{\\Phi}(\\xi_{k})=\\Phi[\\xi^{*}(\\xi_{k})]-L_{\\xi,\\xi}\\|\\xi^{*}(\\xi_{k})-\\xi_{k}\\|^{2}\\geq\\Phi(\\xi_{k})$ , (vi uses Eq. (106). Taking unconditional $k=0,1,\\ldots,K-1$ Wwith $\\begin{array}{r}{\\beta=\\frac{1}{2L_{\\xi,\\xi}\\sqrt{K}}\\in\\left[0,\\frac{1}{2L_{\\xi,\\xi}}\\right]}\\end{array}$ \uff0c Tk\u4e09 BLe#\u2264, we obtain that ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\beta}{8}\\sum_{k=0}^{K-1}\\mathbb{E}\\big[\\big\\|\\nabla\\widetilde{\\Phi}(\\xi_{k})\\big\\|^{2}\\big]}\\\\ &{\\le\\!\\mathbb{E}\\big[\\widetilde{\\Phi}(\\xi_{K})-\\widetilde{\\Phi}(\\xi_{0})\\big]+\\displaystyle\\frac{5K}{4}L_{\\xi,\\xi}\\beta^{2}\\ell_{\\xi}^{2}+\\displaystyle\\frac{5K}{2}L_{\\xi,\\xi}\\beta\\epsilon_{0}+K L_{\\xi,\\xi}\\beta^{2}\\Big(1+\\frac{2}{\\beta L_{\\xi,\\xi}}\\Big)E_{2}^{(\\xi)}}\\\\ &{\\displaystyle\\overset{(i)}{\\le}f^{*}-\\mathbb{E}\\big[\\Phi(\\xi_{0})\\big]+\\frac{5K}{4}L_{\\xi,\\xi}\\beta^{2}\\ell_{\\xi}^{2}+\\frac{5K}{2}L_{\\xi,\\xi}\\beta\\epsilon_{0}+\\frac{5K\\beta E_{2}^{(\\xi)}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where (i) uses the following range of $\\widetilde{\\Phi}(\\xi)$ (defined in Eq. (105)) that holds for any $\\xi\\in\\Xi$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\Phi}(\\xi)\\leq\\underset{\\xi^{\\prime}\\in\\Xi}{\\operatorname*{max}}\\,\\Phi(\\xi^{\\prime})=\\underset{\\xi^{\\prime}\\in\\Xi}{\\operatorname*{max}}\\,\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\,f(\\lambda_{\\theta,\\xi^{\\prime}})\\leq\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\,\\underset{\\xi^{\\prime}\\in\\Xi}{\\operatorname*{max}}\\,f(\\lambda_{\\theta,\\xi^{\\prime}})=f^{*}}\\\\ &{\\widetilde{\\Phi}(\\xi)\\geq\\Phi(\\xi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As a result, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\big[\\|\\nabla\\widetilde{\\Phi}(\\widetilde{\\xi})\\|^{2}\\big]=\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\big[\\big\\|\\nabla\\widetilde{\\Phi}(\\xi_{k})\\big\\|^{2}\\big]}\\\\ {\\displaystyle\\leq\\frac{8f^{*}-8\\mathbb{E}\\big[\\Phi(\\xi_{0})\\big]}{K\\beta}+10L_{\\xi,\\xi}\\beta\\ell_{\\xi}^{2}+20L_{\\xi,\\xi}\\epsilon_{0}+20E_{2}^{(\\xi)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "N.3 Convergence of the Inner Update Step (16) of the Second Corrected Phase ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Next we focus on the second corrected phase which aims to solve the following minimax optimization problem (repeat Eq. (19)). ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}\\widetilde{f}(\\theta,\\xi):=f(\\lambda_{\\theta,\\xi})-L_{\\xi,\\xi}\\|\\xi-\\widetilde{\\xi}\\|^{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Where $\\widetilde{\\xi}$ is obtained from $\\{\\xi_{k}\\}_{k=0}^{K-1}$ uniformly at random in the first riginal phase. Based on Proposition 3, it can be easily verified that $\\widetilde{f}$ has the following smoothness properties and ${\\widetilde{f}}(\\theta,\\cdot)$ is $L_{\\xi,\\xi}$ -strongly concave. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}\\widetilde{f}(\\theta^{\\prime},\\xi^{\\prime})-\\nabla_{\\theta}\\widetilde{f}(\\theta,\\xi)\\|\\le L_{\\theta,\\theta}\\|\\theta^{\\prime}-\\theta\\|+L_{\\theta,\\xi}\\|\\xi^{\\prime}-\\xi\\|,}\\\\ &{\\|\\nabla_{\\xi}\\widetilde{f}(\\theta^{\\prime},\\xi^{\\prime})-\\nabla_{\\xi}\\widetilde{f}(\\theta,\\xi)\\|\\le L_{\\xi,\\theta}\\|\\theta^{\\prime}-\\theta\\|+3L_{\\xi,\\xi}\\|\\xi^{\\prime}-\\xi\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Next, we will see the convergence rate of the projected stochastic gradient ascent steps (16) to the following optimal variable, which is unique due to strong concavity of $\\widetilde{f}(\\theta_{k},\\cdot)$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\xi_{k}^{*}:=\\arg\\operatorname*{max}_{\\xi\\in\\Xi}\\widetilde{f}(\\theta_{k},\\xi).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The optimization progress of each step of Eq. (16) for $k=K,K+1,\\ldots,K+K^{\\prime}-1$ can be bounded as follows. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\big[\\|\\xi_{k,t+1}-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]}\\\\ &{\\overset{(i)}{\\le}\\mathbb{E}\\big[\\|\\xi_{k,t}+a\\big(g_{k,t}^{(\\xi)}-2L_{\\xi,\\xi}(\\xi_{k,t}-\\widetilde{\\xi})\\big)-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]}\\\\ &{\\overset{(i i)}{\\le}(1+c_{k})\\mathbb{E}\\big[\\|\\xi_{k,t}+a\\nabla_{\\xi}\\widetilde{f}(\\theta_{k},\\xi_{k,t})-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]}\\\\ &{\\quad\\quad+\\big(1+c_{k}^{-1}\\big)\\mathbb{E}\\big[\\|a\\big(g_{k,t}^{(\\xi)}-2L_{\\xi,\\xi}(\\xi_{k,t}-\\widetilde{\\xi})-\\nabla_{\\xi}\\widetilde{f}(\\theta_{k},\\xi_{k,t})\\big)\\|^{2}\\big|\\theta_{k}\\big]}\\\\ &{\\overset{(i i i)}{=}(1+c_{k})\\mathbb{E}\\big[\\|\\xi_{k,t}-\\xi_{k}^{*}\\|^{2}+2a\\langle\\nabla_{\\xi}\\widetilde{f}\\big(\\theta_{k},\\xi_{k,t}\\big)-\\nabla_{\\xi}\\widetilde{f}\\big(\\theta_{k},\\xi_{k}^{*}\\big),\\xi_{k,t}-\\xi_{k}^{*}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,a^{2}\\|\\nabla_{\\xi}\\widetilde{f}(\\theta_{k},\\xi_{k,t})-\\nabla_{\\xi}\\widetilde{f}(\\theta_{k},\\xi_{k}^{*})\\|^{2}\\big|\\theta_{k}\\big]}\\\\ &{\\quad+\\,a^{2}(1+c_{k}^{-1})\\mathbb{E}\\big[\\|g_{k,t}^{(\\xi)}-\\nabla_{\\xi}f(\\lambda_{\\theta_{k},\\xi_{k,t}})\\|^{2}\\big|\\theta_{k}\\big]}\\\\ &{\\stackrel{(i v)}{\\le}(1+c_{k})\\big(1-2L_{\\xi,\\xi}a+9L_{\\xi,\\xi}^{2}a^{2}\\big)\\mathbb{E}\\big[\\|\\xi_{k,t}-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]+a^{2}(1+c_{k}^{-1})E_{3}^{(\\xi)}}\\\\ &{\\stackrel{(v)}{=}\\frac{16}{17}\\mathbb{E}\\big[\\|\\xi_{k,t}-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]+\\frac{2E_{3}^{(\\xi)}}{9L_{\\xi,\\xi}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where (i) uses Eq. (16), $\\xi_{k}^{\\ast}\\in\\Xi$ and contraction property of projection, (i) holds for any $c_{k}>0$ whose value will be assigned later, (ii) uses $\\nabla_{\\xi}\\tilde{f}(\\theta_{k},\\xi_{k}^{*})=0$ and the definition of $\\widetilde{f}$ in Eq. (110), (iv) uses Proposition 9, the error term E() defined in Eq. (96) as well as the 3Lg,s-smoothness and $L_{\\xi,\\xi}$ strongly concaviyof $\\widetilde{f}(\\theta_{k},\\cdot)$ (se Ega 112), and () uses $\\begin{array}{r}{a=\\frac{1}{9L_{\\xi,\\xi}}}\\end{array}$ and $\\begin{array}{r}{c_{k}=\\frac{1}{17}}\\end{array}$ Iterating the unconditional expectation of Eq. (114) over $t=0,1,\\ldots,T^{\\prime}-1$ , we obtain that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\big[\\|\\xi_{k}-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]=\\mathbb{E}\\big[\\|\\xi_{k,T^{\\prime}}-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]}\\\\ {\\displaystyle\\leq\\biggr(\\frac{16}{17}\\big)^{T^{\\prime}}\\mathbb{E}\\big[\\|\\xi_{k,0}-\\xi_{k}^{*}\\|^{2}\\big|\\theta_{k}\\big]+\\frac{17(2E_{2}^{(\\xi)})}{9L_{\\xi,\\xi}^{2}}\\leq\\Big(\\frac{16}{17}\\Big)^{T^{\\prime}}D_{\\Xi}^{2}+\\frac{34E_{3}^{(\\xi)}}{9L_{\\xi,\\xi}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the second $\\leq$ denotes $D_{\\Xi}:=\\operatorname*{sup}_{\\xi,\\xi^{\\prime}\\in\\Xi}\\left\\|\\xi^{\\prime}-\\xi\\right\\|$ as the diameter of the compact set $\\Xi$ ", "page_idx": 39}, {"type": "text", "text": "N.4  Convergence Rate of $\\mathbb{E}[\\|G_{b}^{(\\theta)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]$ ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Since ${\\widetilde{f}}(\\theta,\\cdot)$ is strongly concave,it has unique maximizer $\\widetilde{\\xi}^{\\ast}(\\theta)$ and the corresponding function value $\\widetilde\\Psi(\\boldsymbol{\\theta})$ defined as follows. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\xi}^{*}(\\theta):=\\underset{\\xi\\in\\Xi}{\\arg\\operatorname*{max}}\\,\\widetilde{f}(\\theta,\\xi)}\\\\ &{\\widetilde{\\Psi}(\\theta):=\\underset{\\xi\\in\\Xi}{\\operatorname*{max}}\\,\\widetilde{f}(\\theta,\\xi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Furthermore, since $\\widetilde f(\\theta,\\cdot)$ .s $L_{\\xi,\\xi}$ -strongly concave and $\\widetilde{f}$ has the smoothness properties (111) and (112), we can easily obtain that $\\widetilde{\\xi}^{\\ast}(\\theta)$ .s $(L_{\\xi,\\theta}/L_{\\xi,\\xi})$ -Lipschitz and $\\widetilde{\\Psi}$ .s $\\widetilde{L}:=L_{\\theta,\\theta}+L_{\\theta,\\xi}L_{\\xi,\\theta}/L_{\\xi,\\xi^{-}}$ smooth with the following gradient, following the proof of Lemma 4.3 in [31]. 2 ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\nabla\\widetilde\\Psi(\\theta)=\\nabla_{1}\\widetilde f[\\theta,\\widetilde\\xi^{*}(\\theta)]=\\nabla_{1}f(\\lambda_{\\theta,\\widetilde\\xi^{*}(\\theta)}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that for any $k=K,\\dots,K+K^{\\prime}-1$ , the projected stochastic gradient ascent step (17) satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|g_{k}^{(\\theta)}\\|=\\!\\frac{1}{b}\\big\\|\\theta_{k}-\\big(\\theta_{k}-b g_{k}^{(\\theta)}\\big)\\big\\|}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\geq\\frac{(i)}{b}\\big\\|\\mathrm{proj}_{\\Theta}\\big(\\theta_{k}-b g_{k}^{(\\theta)}\\big)-\\big(\\theta_{k}-b g_{k}^{(\\theta)}\\big)\\big\\|}\\\\ {\\displaystyle\\frac{(i i)}{=}\\frac{1}{b}\\|\\theta_{k+1}-\\theta_{k}+b g_{k}^{(\\theta)}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where (i) uses $\\theta_{k}\\in\\Theta$ and the definition of projection and (i) uses the stochastic gradient descent step (17). The above inequality implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\n(g_{k}^{(\\theta)})^{\\top}(\\theta_{k+1}-\\theta_{k})\\leq-\\frac{1}{2b}\\|\\theta_{k+1}-\\theta_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, we analyze the optimization progress of the potential function (117) along the projected stochastic gradient descent step (17) as follows. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\widetilde{\\Psi}(\\theta_{k+1})}\\\\ &{\\stackrel{(i)}{\\le}\\mathbb{E}\\Big[\\widetilde{\\Psi}(\\theta_{k})+\\nabla\\widetilde{\\Psi}(\\theta_{k})^{\\top}(\\theta_{k+1}-\\theta_{k})+\\displaystyle\\frac{\\widetilde{L}}{2}\\|\\theta_{k+1}-\\theta_{k}\\|^{2}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\langle\\overline{{u}}\\rangle}{\\xi}\\bigg[\\overline{{\\Psi}}(\\phi_{1})+\\langle\\nabla_{1}f(\\mathcal{A}_{n,\\widehat{\\xi}_{n}^{\\prime}(\\cdot)})-g_{\\mu}^{(\\mu)},\\theta_{\\hbar+1}-\\theta_{k}\\rangle+(g_{\\mu}^{(\\mu)})^{\\top}(\\theta_{\\hbar+1}-\\theta_{k})}\\\\ &{\\qquad+\\frac{\\lambda}{2}\\|\\theta\\|_{\\mathcal{H}_{1}}+}\\\\ &{\\overset{(a)}{\\le}\\mathbb{E}\\bigg[\\overline{{\\Psi}}(\\phi_{1})+\\frac{1}{2}\\overline{{L}}\\bigg]\\bigg]\\bigg(\\theta_{\\hbar}^{0}-\\nabla_{1}f(\\mathcal{A}_{n,\\widehat{\\xi}_{n}^{\\prime}(\\cdot)})\\bigg)^{2}+\\frac{\\lambda}{2}\\|\\theta_{\\hbar+1}-\\theta_{k}\\|^{2}-\\frac{1}{2\\widehat{h}}\\|\\theta_{\\hbar+1}-\\theta_{k}\\|^{2}}\\\\ &{\\qquad+\\frac{\\lambda}{2}\\|\\theta_{\\hbar+1}-\\theta_{k}\\|^{2}}\\\\ &{\\le\\mathbb{E}\\bigg[\\overline{{\\Psi}}(\\phi_{1})+\\frac{1}{2}\\bigg\\|\\theta_{\\hbar}^{0}-\\nabla_{\\theta}f(\\mathcal{A}_{n,\\widehat{\\xi}_{n}})\\bigg\\|^{2}+\\frac{1}{2}\\bigg\\|\\nabla_{\\theta}f(\\mathcal{A}_{n,\\widehat{\\xi}_{n}})-\\nabla_{1}f(\\mathcal{A}_{n,\\widehat{\\xi}_{n}^{\\prime}(\\cdot)})\\bigg\\|^{2}}\\\\ &{\\qquad-\\bigg(\\frac{1}{2\\hbar}-\\widetilde{L}\\bigg)\\|\\theta_{\\hbar+1}-\\theta_{k}\\|^{2}\\bigg]}\\\\ &{\\overset{(c)}{\\le}\\mathbb{E}\\bigg[\\overline{{\\Psi}}(\\theta_{\\hbar})+\\frac{\\dot{L}\\overline{{\\xi}}_{n}^{(0)}}{2}\\frac{\\lambda}{\\overline{{L}}}\\bigg(\\frac{1}{\\overline{{\\Psi}}}-\\overline{{\\Psi}}(\\phi_{1})\\bigg)\\bigg]-\\frac{1}{4}\\mathbb{E}\\|\\theta_{\\hbar+1}-\\theta_{k}\\|^{2}}\\\\ &{\\overset{(c)}{\\le}\\mathbb{E}\\bigg[\\overline{{\\Psi}}(\\phi_{1})+\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where (i) uses the $\\widetilde{L}:=L_{\\theta,\\theta}+L_{\\theta,\\xi}L_{\\xi,\\theta}/L_{\\xi,\\xi}$ smoothnessof $\\widetilde{\\Psi}$ ,(i) uses Eq. (118),(ii) uses Cauchy$\\begin{array}{r}{b=\\frac{1}{4\\tilde{L}}}\\end{array}$ , Propositions 3-9 and the error tem  dened $\\begin{array}{r}{G_{b}^{(\\theta)}(\\theta_{k},\\xi_{k})\\,=\\,\\frac{1}{b}\\big[\\theta_{k}\\,-\\mathrm{proj}_{\\Theta}[\\theta_{k}\\,-\\,b\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi_{k}})]\\big]}\\end{array}$   \n$k=K,\\dots,K+K^{\\prime}-1$ . Its norm can be bounded as follows. ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[\\vert G_{t}^{(s)}(\\theta_{t+1},G_{t})\\vert^{2}}\\\\ &{=\\frac{1}{\\beta}\\vert\\mathbb{E}\\{\\vert G_{t}\\vert^{3}-\\theta_{t}+K G_{t}^{(s)}(\\theta_{t},\\xi_{t})-(\\theta_{t+1}-\\theta_{t})\\}\\vert^{2}}\\\\ &{\\le\\frac{2}{\\beta}\\mathbb{E}\\{\\vert\\theta_{t+1}-\\theta_{t}+K G_{t}^{(s)}(\\theta_{t},\\xi_{t})\\vert^{2}+\\frac{2}{\\beta}\\vert^{2}\\}\\vert-(\\theta_{t+1}-\\theta_{t})\\vert\\}^{2}}\\\\ &{\\overset{(a)}{\\le}\\frac{2}{\\beta}\\mathbb{E}\\{\\vert\\nabla\\phi_{0}\\vert\\theta_{t}-b_{0}^{(s)}\\vert-\\nabla\\phi(\\theta_{t})\\vert\\}+B\\nabla_{\\theta}^{2}\\vert(\\lambda\\phi_{t+1})\\vert^{2}}\\\\ &{\\quad+\\frac{8}{\\beta}\\mathbb{E}\\Big[\\!\\mathbb{E}\\{\\tilde{\\psi}(\\theta_{t})-\\tilde{\\Psi}(\\theta_{t+1})\\}\\!\\Big]+\\frac{\\hat{L}_{t}^{(s)}}{L}+\\frac{D_{t}^{2}L_{2}^{2}}{L}\\!\\Big(\\frac{16}{\\Gamma}\\Big)^{T}+\\frac{3(L_{2}^{2}\\xi_{t}^{2}\\zeta_{t}^{2})}{9\\hat{L}\\hat{L}_{\\xi}^{2}}\\Bigg]}\\\\ &{\\overset{(a)}{\\le}2\\vert\\mathbb{E}\\vert_{\\theta}^{(s)}-\\nabla\\phi(\\lambda\\phi_{s},\\xi_{t})\\vert^{2}}\\\\ &{\\quad+\\frac{8}{\\beta}\\vert\\mathbb{E}\\{\\tilde{\\psi}(\\theta_{t})-\\tilde{\\Psi}(\\theta_{t+1})\\}+\\frac{L_{t}^{(s)}}{L}+\\frac{D_{t}^{2}L_{2}^{2}}{\\Delta}\\Big(\\frac{16}{\\Gamma}\\Big)^{T}+\\frac{3(L_{2}^{2}\\xi_{t}^{2}\\zeta_{t}^{2})}{9\\hat{L}L_{2}^{2}}}\\\\ &{\\overset{(a)}{\\le}2\\vert\\mathbb{E}\\vert_{\\theta}^{(s)}+\\frac{8}{\\beta}\\mathbb{E} \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where (i) uses Eqs. (17) and (120) as well as $\\begin{array}{r}{G_{b}^{(\\theta)}(\\theta_{k},\\xi_{k})=\\frac{1}{b}\\big[\\theta_{k}-\\mathrm{proj}_{\\Theta}[\\theta_{k}-b\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi_{k}})]\\big]}\\end{array}$ (i ses Eq 11), and (i uses Propositon and the errorterm $E_{4}^{(\\theta)}$ defined by Eq. (95). By rearranging the above inequality and averaging it over $k=K,K+1,\\ldots,K+K^{\\prime}-1$ , we obtain the convergence rate of $\\mathbb{E}[\\|\\bar{G_{b}^{(\\theta)}}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]$ as follows. ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\|G_{b}^{(\\theta)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]=\\displaystyle\\frac{1}{K^{\\prime}}\\sum_{k=K}^{K+K^{\\prime}-1}\\mathbb{E}\\|G_{b}^{(\\theta)}(\\theta_{k},\\xi_{k})\\|^{2}}\\\\ {\\displaystyle\\le2E_{4}^{(\\theta)}+\\frac{8E_{4}^{(\\theta)}}{b\\widetilde{L}}+\\frac{8}{b K^{\\prime}}\\mathbb{E}[\\widetilde{\\Psi}(\\theta_{K})-\\widetilde{\\Psi}(\\theta_{K+K^{\\prime}})]+\\frac{8D\\_{\\Xi}^{2}L_{\\theta,\\xi}^{2}}{b\\widetilde{L}}\\Big(\\frac{16}{17}\\Big)^{T^{\\prime}}+\\frac{272L_{\\theta,\\xi}^{2}E_{3}^{(\\xi)}}{9b\\widetilde{L}L_{\\xi,\\xi}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\overset{(i)}{\\leq}34E_{4}^{(\\theta)}+\\frac{32\\widetilde{L}}{K^{\\prime}}\\mathbb{E}\\big[\\widetilde{f}(\\theta_{K},\\widetilde{\\xi}^{*}(\\theta_{K}))-\\widetilde{f}(\\theta_{K+K^{\\prime}},\\widetilde{\\xi}^{*}(\\theta_{K}))\\big]+32D_{\\Xi}^{2}L_{\\theta,\\xi}^{2}\\Big(\\frac{16}{17}\\Big)^{T^{\\prime}}+\\frac{1088L_{\\theta,\\xi}^{2}E_{3}^{(\\xi)}}{9L_{\\xi,\\xi}^{2}}}}\\\\ {{\\overset{(i i)}{=}34E_{4}^{(\\theta)}+\\frac{32\\widetilde{L}}{K^{\\prime}}\\mathbb{E}\\big[f(\\lambda_{\\theta_{K},\\widetilde{\\xi}^{*}(\\theta_{K})})-f(\\lambda_{\\theta_{K+K^{\\prime}},\\widetilde{\\xi}^{*}(\\theta_{K})})\\big]+32D_{\\Xi}^{2}L_{\\theta,\\xi}^{2}\\Big(\\frac{16}{17}\\Big)^{T^{\\prime}}+\\frac{1088L_{\\theta,\\xi}^{2}E_{3}^{(\\xi)}}{9L_{\\xi,\\xi}^{2}}}}\\\\ {{\\overset{(i i i)}{\\leq}34E_{4}^{(\\theta)}+\\frac{32\\widetilde{L}}{K^{\\prime}}\\big[\\Gamma(\\theta_{K})-f^{*}\\big]+32D_{\\Xi}^{2}L_{\\theta,\\xi}^{2}\\Big(\\frac{16}{17}\\Big)^{T^{\\prime}}+\\frac{1088L_{\\theta,\\xi}^{2}E_{3}^{(\\xi)}}{9L_{\\xi,\\xi}^{2}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where (i) uses Eqs. (116)-(117) and selects the stepsize $\\begin{array}{r}{b=\\frac{1}{4\\tilde{L}}}\\end{array}$ , (ii) uses Eq. (110), and (ii) uses $f^{*}:=\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ and $\\Gamma(\\theta):=\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})$ ", "page_idx": 41}, {"type": "text", "text": "N.5  Convergence Rate of $\\mathbb{E}[\\|G_{a}^{(\\xi)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]$ ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\psi(\\xi):=\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi)=\\Phi(\\xi)-L_{\\xi,\\xi}\\|\\xi-\\widetilde{\\xi}\\|^{2}}\\end{array}$ . Then, on one hand, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi\\big[\\xi^{*}(\\widetilde{\\xi})\\big]-\\psi(\\xi_{k})\\overset{(i)}{=}\\displaystyle\\operatorname*{max}_{\\xi\\in\\Xi}\\psi(\\xi)-\\psi(\\xi_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\operatorname*{max}_{\\xi\\in\\Xi}\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi)-\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi_{k})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\operatorname*{max}_{\\xi\\in\\Xi}\\widetilde{f}(\\theta_{k},\\xi)-\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi_{k})}\\\\ &{\\qquad\\qquad\\qquad\\overset{(i i)}{=}\\displaystyle\\frac{\\langle i\\rangle}{\\widetilde{f}(\\theta_{k},\\xi_{k}^{*})}-\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(\\romannumeral4)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\widetilde{f}(\\theta_{k},\\xi_{k})+\\frac{3{\\cal L}_{\\xi,\\xi}}{2}\\lVert\\xi_{k}-\\xi_{k}^{*}\\rVert^{2}-\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where (i) uses Eq. (104),(ii) uses Eq. (113),(ii) uses $\\nabla_{\\xi}\\tilde{f}(\\theta_{k},\\xi_{k}^{*})=0$ at the optimal variable $\\xi_{k}^{*}$ defined by Eq. (113) and $3L_{\\xi,\\xi}$ -smoothness of ${\\widetilde{f}}(\\theta,\\cdot)$ implied by Eq. (112). On the other hand, since ${\\widetilde{f}}(\\theta,\\cdot)$ is $L_{\\xi,\\xi}$ -strongly concave, $\\psi$ is $L_{\\xi,\\xi}$ -stronglyconcave.Hence, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(\\xi_{k})\\leq\\psi\\big[\\xi^{*}(\\widetilde{\\xi})\\big]+\\nabla\\psi\\big[\\xi^{*}(\\widetilde{\\xi})\\big]^{\\top}\\big[\\xi^{*}(\\widetilde{\\xi})-\\xi_{k}\\big]-\\displaystyle\\frac{L_{\\xi,\\xi}}{2}\\big\\|\\xi^{*}(\\widetilde{\\xi})-\\xi_{k}\\big\\|^{2}}\\\\ &{\\quad\\quad\\quad\\overset{(i)}{=}\\psi\\big[\\xi^{*}(\\widetilde{\\xi})\\big]-\\displaystyle\\frac{L_{\\xi,\\xi}}{2}\\big\\|\\xi^{*}(\\widetilde{\\xi})-\\xi_{k}\\big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where (i) uses $\\nabla\\psi\\left[\\xi^{*}(\\widetilde{\\xi})\\right]=0$ at the unique optimizer $\\xi^{*}(\\widetilde{\\xi})=\\arg\\operatorname*{max}_{\\xi\\in\\Xi}\\psi(\\xi)$ (see Eq. (104)). Then, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|G_{\\alpha}^{(1)}(\\theta_{t},\\xi_{t})\\|^{2}}\\\\ &{=\\frac{1}{\\alpha}\\Big\\|\\operatorname{lnon}_{\\theta}\\Big(\\xi_{t}+\\alpha\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi_{t}})\\Big)-\\xi_{t}\\Big\\|^{2}}\\\\ &{\\overset{(a)}{\\le}\\frac{2}{\\alpha}\\Big\\|\\operatorname{lnon}_{\\theta}\\Big(\\xi_{t}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t})\\Big)-\\xi_{t}\\Big\\|-\\left[\\operatorname{prop}_{\\theta}\\big(\\xi_{t}^{\\star}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t}^{\\star})\\big)-\\xi_{t}\\right]\\Big\\|^{2}}\\\\ &{\\quad+\\frac{2}{\\alpha}\\Big\\|\\operatorname{lnon}_{\\theta}\\Big\\rangle\\Big(\\xi_{t}+\\alpha\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi_{t}})\\Big)-\\operatorname{prop}_{\\theta}\\big(\\xi_{t}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t})\\big)\\Big\\|^{2}}\\\\ &{\\le\\frac{4}{\\alpha^{2}}\\Big\\|\\operatorname{lnon}_{\\theta}\\Big\\rangle\\Big(\\xi_{t}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t})\\Big)-\\operatorname{prop}_{\\theta}\\big(\\xi_{t}^{\\star}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t}^{\\star})\\big)\\Big\\|^{2}+\\frac{4}{\\alpha^{2}}\\|\\xi_{t}^{\\star}-\\xi_{t}\\|^{2}}\\\\ &{\\quad+\\frac{2}{\\alpha^{2}}\\|\\operatorname{prop}_{\\theta}\\big(\\xi_{t}+\\alpha\\nabla_{\\xi}f(\\lambda_{\\theta,\\xi_{t}})\\big)-\\operatorname{prop}_{\\theta}\\big(\\xi_{t}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t})\\big)\\big\\|^{2}}\\\\ &{\\le\\frac{4}{\\alpha^{2}}\\|(\\xi_{t}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t}))-(\\xi_{t}^{\\star}+\\alpha\\nabla_{\\xi}\\widetilde{f}(\\theta_{t},\\xi_{t}))\\big\\|^{2}+\\frac{4}{\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\overset{(i i i)}{\\leq}\\Big(72L_{\\xi,\\xi}^{2}+\\displaystyle\\frac{12}{a^{2}}\\Big)\\|\\xi_{k}-\\xi_{k}^{*}\\|^{2}+16L_{\\xi,\\xi}^{2}\\big[\\|\\xi_{k}-\\xi^{*}(\\widetilde{\\xi})\\|^{2}+\\|\\xi^{*}(\\widetilde{\\xi})-\\widetilde{\\xi}\\|^{2}\\big]}&\\\\ &{\\overset{(i v)}{\\leq}\\Big(72L_{\\xi,\\xi}^{2}+\\displaystyle\\frac{12}{a^{2}}\\Big)\\|\\xi_{k}-\\xi_{k}^{*}\\|^{2}+32L_{\\xi,\\xi}\\Big(\\widetilde{f}\\big(\\theta_{k},\\xi_{k}\\big)+\\displaystyle\\frac{3L_{\\xi,\\xi}}{2}\\|\\xi_{k}-\\xi_{k}^{*}\\|^{2}-\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta}\\widetilde{f}(\\theta,\\xi_{k})\\Big)+4\\|\\nabla\\widetilde{\\Phi}(\\widetilde{\\xi})\\|}&\\\\ &{\\overset{(v)}{=}\\Big(120L_{\\xi,\\xi}^{2}+\\displaystyle\\frac{12}{a^{2}}\\Big)\\|\\xi_{k}-\\xi_{k}^{*}\\|^{2}+32L_{\\xi,\\xi}\\big[f\\big(\\lambda_{\\theta_{k},\\xi_{k}}\\big)-\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta}f\\big(\\lambda_{\\theta,\\xi_{k}}\\big)\\big]+4\\|\\nabla\\widetilde{\\Phi}(\\widetilde{\\xi})\\|^{2},}&{\\quad{\\scriptstyle(125\\pi)\\scriptscriptstyle(\\sqrt{\\lambda_{\\theta_{k},\\xi_{k}}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where (i) uses prc $\\l)_{\\mathbf{j}}\\!\\left(\\xi_{k}^{*}+a\\nabla_{\\xi}\\widetilde{f}(\\theta_{k},\\xi_{k}^{*})\\right)-\\xi_{k}^{*}=0$ based on Eq. (113), (i uses the definition of $\\widetilde{f}$ given by Eq. (110), (ii) uses $3L_{\\xi,\\xi}$ -smoothness of $\\ensuremath{\\widetilde{f}}(\\theta_{k},\\cdot)$ based on Eq. (112), (iv) uses Eqs. (106), (123) and (124), (v) uses Eq. (110). Taking expectation of the above Eq. (125) and averaging it over $k=K,\\dots,K+K^{\\prime}-1$ we obtain the convergence rate of $\\mathbb{E}[\\|G_{a}^{(\\xi)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}]$ as follows. ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbb E}[|(Z_{t}^{n}(\\ell)_{t}\\xi_{t}|)]}\\\\ &{\\le\\overline{{{\\mathcal E}}}\\left(10L_{0}^{2}\\xi_{t}+\\frac{\\ell}{2}\\right)^{1/2}\\sum_{k=1}^{K-1}\\mathbb{E}\\left[|A_{k}-\\xi|\\right]^{t}+{\\mathbb Q}[\\Gamma_{0}^{k}(\\xi)]^{2}}\\\\ &{\\quad+\\frac{32L_{0}^{2}K}{K}{4}{\\mathbb E}[|A_{k}-\\xi^{n}-\\xi^{n}|]}\\\\ &{\\quad+\\frac{32L_{0}^{2}K}{12}{\\mathbb E}[|A_{k}|]^{\\frac{3}{2}}{\\mathbb E}[|(\\overline{{\\eta}}_{k}\\xi_{t})-\\frac{32L_{0}^{2}K}{4}{\\mathbb Q}^{2}(\\lambda_{k}\\xi_{t})]}\\\\ &{\\overset{(a)}{\\le}{\\mathbb E}[10L_{0}^{2}\\xi_{t}+\\frac{12L_{0}^{2}}{4}]\\left[\\left(\\frac{\\|A_{0}\\|^{2}\\Gamma_{2}^{\\prime}\\eta_{2}}{|\\eta_{2}|}+\\frac{32L_{0}^{2}\\ell}{8L_{0}^{2}\\xi_{t}}\\right)\\right.}\\\\ &{\\quad+\\left.4\\left(\\frac{\\|\\mathcal{F}^{\\prime}-\\mathcal{K}^{\\prime}(\\overline{{\\xi}}_{t})\\|}{2}+10L_{0}L_{0}^{3}\\xi_{t}^{2}+20L_{\\xi}\\kappa_{0}+20L_{0}^{2}\\right)\\right.}\\\\ &{\\quad\\left.+32L_{0}L_{0}^{2}\\sqrt{2}\\xi_{t}-(16L_{0}-1)+b\\eta_{0}^{2}\\left[\\left(\\|A_{0}\\|^{2}\\eta_{0}\\left(\\xi_{t}\\right),\\xi_{t}\\right)\\right]\\right.}\\\\ &{\\overset{(b)}{\\le}{\\mathbb E}\\left(10L_{0}^{2}\\xi_{t}+\\frac{12L_{0}^{2}}{4}\\right)\\left[\\left(\\frac{\\|A_{0}\\|^{2}\\Gamma_{2}^{\\prime}\\eta_{2}}{|\\eta_{2}|}+\\frac{32L_{0}^{2}\\zeta}{8L_{0}^{2} \n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where (i) uses Eqs. (20), (101) and (115), (i) uses Eq. (122), (i) uses $\\ell_{\\theta},\\ell_{\\xi}\\,=\\,\\mathcal{O}[(1-\\gamma)^{-2}]$ \uff0c $L_{\\theta,\\xi},L_{\\xi,\\xi},\\widetilde{L}=\\mathcal{O}[(1-\\gamma)^{-3}]$ based on Proposition 3 and selects stepsizes $\\begin{array}{r}{a=\\frac{1}{9L_{\\xi,\\xi}}=\\mathcal{O}[(1-\\gamma)^{3}]}\\end{array}$ $\\begin{array}{r}{b=\\frac{1}{4\\tilde{L}}=\\mathcal{O}[(1-\\gamma)^{3}],\\beta=\\frac{1}{2L_{\\xi,\\xi}\\sqrt{K}}=\\mathcal{O}[K^{-1/2}(1-\\gamma)^{3}].}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "N.6  Substituting Hyperparameters ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Denote $\\delta=\\mathrm{min}\\left[\\overline{{\\delta}},\\frac{\\epsilon^{2}}{5760L_{\\xi,\\xi}L_{\\theta,\\theta}\\ell_{\\lambda^{-1}}^{2}},\\frac{1}{2}\\right]=\\mathcal{O}[(1-\\gamma)^{6}\\epsilon^{2}]$ Thenwe selt lwin h rameter values. ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K=36\\epsilon^{-4}\\big\\{64L_{\\xi,\\xi}\\big[f^{*}-\\mathbb{E}[\\Phi(\\xi_{0})]\\big]+20L_{\\xi}^{2}\\big\\}^{2}=\\mathcal{O}[(1-\\gamma)^{-8}\\epsilon^{-4}]}\\\\ &{~{\\cal T}=\\frac{1}{3\\delta}\\log\\left\\{140L_{\\xi,\\xi}\\epsilon^{-2}\\mathbb{E}\\Big[\\Gamma(\\theta_{0})-\\textstyle\\operatorname*{min}_{\\theta\\in\\Theta,\\xi\\in\\Xi}f(\\lambda_{\\theta,\\xi})\\Big]\\right\\}=\\mathcal{O}\\Big[\\frac{\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]}{(1-\\gamma)^{6}\\epsilon^{2}}\\Big]}\\\\ &{K^{\\prime}=\\frac{294912\\hat{L}L_{\\xi,\\xi}^{2}}{\\hat{L}^{2}\\epsilon^{4}}\\big[\\Gamma(\\theta_{K})-f^{*}\\big]\\big[\\sqrt{2}\\ell_{\\lambda^{-1}}\\big(L_{\\theta,\\theta}+4\\widetilde{L}\\big)+\\ell_{\\theta}\\big]^{2}=\\mathcal{O}[(1-\\gamma)^{-9}\\epsilon^{-4}]}\\\\ &{{\\cal T}^{\\prime}=33\\log\\big\\{54L_{\\xi,\\xi}D_{\\Xi}L_{\\theta,\\xi}\\tilde{L}^{-1}\\epsilon^{-2}\\big[\\sqrt{2}\\ell_{\\lambda^{-1}}\\big(L_{\\theta,\\theta}+4\\widetilde{L}\\big)+\\ell_{\\theta}\\big]\\big\\}=\\mathcal{O}\\big(\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]\\big)}\\\\ &{~\\alpha=\\frac{1}{2L_{\\theta,\\theta}}}\\\\ &{\\beta=\\frac{1}{2L_{\\xi,\\xi}\\sqrt{K}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1a gLg,s\u201d (133)  \n$\\begin{array}{r l}&{\\quad\\ b=\\frac{1}{L},}\\\\ &{\\quad=\\displaystyle\\frac{1}{L^{2}R^{1/2}}\\operatorname{trin}\\mathcal{L}_{\\neq}^{()},}\\\\ &{\\quad=\\displaystyle\\frac{1}{L^{2/2}R^{1/2}}\\operatorname{trin}\\left(-\\frac{2L^{2}}{L^{2}(1-\\gamma)^{\\frac{3}{2}}}-\\sigma(1-\\gamma)\\cdots\\right),}\\\\ &{\\quad=\\displaystyle\\frac{1}{L^{2/2}R^{1/2}}\\operatorname{trin}\\left(-\\frac{2L^{2}}{L^{2}(1-\\gamma)^{\\frac{3}{2}}}\\right)\\left[4-(\\sigma(1-\\gamma)\\cdots)^{m_{1}-1},\\right.}\\\\ &{\\quad\\left.=\\displaystyle\\frac{1}{L^{2/2}R^{1/2}}\\operatorname{trin}\\left(-\\frac{2L^{2}}{L^{2}(1-\\gamma)^{\\frac{3}{2}}}+\\sigma(1-\\gamma)\\right)+\\right.}\\\\ &{\\quad\\left.=\\displaystyle\\frac{1}{L^{2/2}}\\operatorname{trin}\\left(-\\frac{2L^{2}}{L^{2}(1-\\gamma)}\\right)+\\frac{1}{L^{2/2}(1-\\gamma)^{\\frac{3}{2}}}\\right]\\left[4-(\\sigma(1-\\gamma)^{\\frac{3}{2}-1}\\gamma^{-1}\\right]}\\\\ &{\\quad=\\displaystyle\\frac{1}{L^{2/2}}\\operatorname{trin}\\left(-\\frac{2L^{2}}{L^{2}(1-\\gamma)}+\\frac{1}{L^{2}(1-\\gamma)^{\\frac{3}{2}}}\\right)\\left[\\frac{8\\sigma U_{0}^{2}(\\sigma(1-\\gamma)^{\\frac{3}{2}})}{L^{2}(1-\\gamma)^{\\frac{3}{2}}}\\right]-\\sigma\\left[\\frac{16L^{2}\\gamma^{-1}}{L^{2}(1-\\gamma)^{\\frac{3}{2}}}-\\sigma(1-\\gamma)\\right]}\\\\ &{\\quad=\\displaystyle\\frac{1}{L^{2/2}}\\operatorname{trin}\\left(-\\frac{2L^{2}}{L^{2}(1-\\gamma)}-\\sigma(1-\\gamma)\\right)+\\sigma\\left[4-\\frac{2L^{2}}{L^{2}(1-\\gamma)^{\\frac{3}{2}}}\\right],}\\\\ &{\\quad=\\displaystyle\\frac{16852L^{2}\\gamma^{-1}}$ (134)(135)(136)(137)(138)(139)  \n$\\begin{array}{r l}{a_{1^{2}}^{n}=\\frac{(n-1)(n-2)(n-3)(n-5)}{(n-2)(n-3)(n-4)}-\\mathcal{O}(1-n),\\qquad}&{\\mathrm{ot~and~}}\\\\ {a_{2^{2}}^{n}=\\frac{1}{2n-1}\\left[\\frac{{\\{180,n\\}}^{2}(2^{n},\\frac{n}{2})^{\\top}}{\\Gamma\\left({n-2}\\right)}\\psi_{0}^{2}\\right]=\\mathcal{O}\\left[\\left({n-1}\\right)\\cdots\\gamma\\right],\\qquad}&{\\mathrm{and~}}\\\\ {a_{4^{2}}^{n}=\\frac{1}{2n-1}\\left[\\frac{{\\{180,n\\}}^{2}(2^{n},\\frac{n}{2})^{\\top}}{\\Gamma\\left({n-2}\\right)}\\psi_{0}^{2}\\right]=\\mathcal{O}\\left[\\left({n-3}\\right)\\cdots\\gamma\\right],\\qquad}&{\\mathrm{and~}}\\\\ {a_{4^{2}}^{n}=\\frac{1}{2n-1}\\left[\\frac{\\{180,n\\}^{2}(2^{n},\\frac{n}{2})^{\\top}}{\\Gamma\\left({n-2}\\right)}+\\frac{\\{180,n\\}^{2}}{\\Gamma\\left({n-2}\\right)}\\left[\\frac{\\{290,n}\\left(2^{n},\\frac{n}{2}\\right)^{\\top}}{\\Gamma\\left({n-2}\\right)}-\\sigma\\left({n-1}\\right)\\right]^{n-1}\\right]}&{\\mathrm{at~and~}}\\\\ {a_{4^{2}}^{n}=\\frac{1208044_{4^{2}}\\gamma\\beta}{\\Gamma\\left({n-2}\\right)}\\psi_{0}^{2}\\bigg[\\Gamma\\left({n-4}\\pm\\frac{\\gamma}{2}\\right)+\\frac{\\{180,n\\}^{2}}{\\Gamma\\left({n-2}\\right)}-\\sigma\\left({1-\\gamma}\\right)\\gamma\\cdots\\bigg]}&{\\mathrm{and~}}\\\\ {a_{4^{2}}^{n}=\\frac{1208044_{4^{2}}\\gamma\\beta}{\\Gamma\\left({n-2}\\right)}\\frac{\\sqrt{\\gamma}}{\\Gamma\\left({n-2}\\right)}\\chi\\left({n-1}\\right)\\sqrt{\\Gamma\\left({n-4}\\pm\\frac{\\gamma}{2}\\right)}-\\sigma\\left({1-\\gamma}\\right)^{n-1}\\sigma^{n}-\\gamma\\right],\\qquad} $ \uff09\uff09\uff09  \n$\\begin{array}{r l}&{H_{\\lambda_{1}}^{(n)}=\\cfrac{1}{2\\ln(n+1)}\\,\\bigg[\\frac{1399842\\ln(1\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\ln(1\\!\\sqrt{2}\\pi)\\!+\\!(\\!\\sqrt{2}\\!-\\!1\\!-\\!\\lambda_{1}\\!+\\!\\lambda_{1}^{\\prime}\\!)^{2}}{1\\!-\\!\\rho_{\\lambda_{1}}(n\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\ln(1\\!-\\!\\lambda_{1}^{\\prime}\\!)^{2}}\\bigg]}\\\\ &{\\qquad=\\cfrac{(\\ln(1\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\ln(1\\!-\\!\\lambda_{1}^{\\prime}\\!-\\!\\lambda_{1}^{\\prime}\\!))}{1\\!-\\!\\rho_{\\lambda_{1}}(n\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\ln(1\\!-\\!\\lambda_{1}^{\\prime}\\!)^{2}}\\,\\bigg[\\frac{\\rho\\!-\\!(1\\!-\\!\\rho_{\\lambda_{1}}\\!\\ln(1\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\!-\\!(\\!\\sqrt{2}\\!-\\!1\\!-\\!\\lambda_{1}\\!-\\!\\lambda_{1}\\!)\\!)}{\\sqrt{\\!\\rho}\\!\\exp(\\!-\\!1\\!\\sqrt{2}\\!\\cdot\\!\\!\\frac{\\rho_{\\lambda_{1}}^{\\prime}}{n\\!-\\!\\rho_{\\lambda_{1}}(n\\!-\\!\\lambda_{1}^{\\prime}\\!-\\!\\lambda_{1}^{\\prime}\\!)^{2}})}\\bigg]}\\\\ &{H_{\\lambda_{1}}^{(n)}=\\cfrac{4}{\\sigma}\\bigg[\\cfrac{1}{1\\Biggl(\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\ln(1\\!-\\!\\lambda_{1}^{\\prime}\\!-\\!\\lambda_{1}^{\\prime}\\!)}\\bigg]}\\\\ &{\\qquad=\\cfrac{4}{\\sigma}\\bigg[\\cfrac{1}{1\\Biggl(\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\ln(1\\!-\\!\\rho_{\\lambda_{1}}^{\\prime})\\ln(1\\!-\\!(\\!\\frac{1}{\\lambda_{1}^{\\prime}\\!-\\!\\lambda_{1}^{\\prime}\\!-\\!\\lambda_{1}\\!-\\!\\lambda_{1}^{\\prime}\\!)})}\\bigg]\\,\\bigg[$", "page_idx": 43}, {"type": "equation", "text": "$$\n=\\mathcal{O}\\Big[\\frac{\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]}{1-\\gamma}\\Big]\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Substituting the above hyperparameter choices into Eqs. (95) and (96), we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E_{1}^{(a)}:=\\frac{3{\\mathcal U}_{s-1}^{2}}{(1-\\gamma)^{4}}\\left[L_{1}^{2}|\\mathcal{S}||\\mathcal{A}|\\displaystyle\\alpha\\Big(\\frac{1}{m_{0}^{(1)}}+\\gamma^{2\\pi/1}\\Big)+\\frac{C_{1}^{2}}{m_{0}^{(1)}}+\\mathcal{E}_{1}^{(1)}({\\boldsymbol H}_{s}^{(1)}(1-\\gamma)|^{2})^{2}|\\mathcal{F}|^{3}\\right]}\\\\ &{\\le\\frac{3{\\mathcal U}_{s-1}^{2}}{(1+\\gamma)^{4}}\\left[\\frac{1}{\\beta_{0}^{2}}|\\mathcal{F}||\\mathcal{A}|\\displaystyle\\alpha\\Big(\\frac{1}{m_{0}^{(1)}}+\\gamma^{2\\pi/1}\\Big)+\\frac{C_{2}^{2}}{m_{0}^{(1)}}+\\mathcal{E}_{1}^{(2)}({\\boldsymbol H}_{s}^{(1)}(1-\\gamma)|^{2})^{2}|\\mathcal{F}|^{3}\\right]}\\\\ {E_{2}^{(a)}:=\\frac{3{\\mathcal U}_{s-1}^{2}}{(1-\\gamma)^{4}}\\left[L_{1}^{2}|\\mathcal{S}||\\mathcal{A}|\\displaystyle\\alpha\\Big(\\frac{1}{m_{0}^{(1)}}+\\gamma^{2\\pi/1}\\Big)+\\frac{C_{2}^{2}}{m_{0}^{(1)}}+\\mathcal{E}_{1}^{(1)}({\\boldsymbol H}_{s}^{(2)}(1-\\gamma)|^{2})^{2}|\\mathcal{F}|^{3}\\right]}\\\\ &{\\le\\frac{3{\\mathcal U}_{s-1}^{2}}{(1-\\gamma)^{4}}\\left(\\frac{1}{\\beta_{0}^{2}}|\\mathcal{F}||\\mathcal{A}|\\displaystyle\\alpha\\Big(\\frac{1}{m_{0}^{(1)}}+\\gamma^{2\\pi/1}\\Big)+\\frac{C_{2}^{2}}{m_{0}^{(1)}}+\\mathcal{E}_{1}^{(1)}({\\boldsymbol H}_{s}^{(1)}(1-\\gamma)|^{2})^{2}|\\mathcal{F}|^{3}\\right]}\\\\ {E_{3}^{(0)}:=\\frac{3{\\mathcal U}_{s-1}^\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Where we used $\\big[1+H_{\\theta}^{(k)}(1-\\gamma)\\big]^{2}\\gamma^{H_{\\theta}^{(k)}}\\leq2$ $\\begin{array}{r}{H_{\\theta}^{(k)}\\geq\\frac{4}{1-\\gamma}\\log\\left(\\frac{2}{1-\\gamma}\\right)(k=1,2,3,4).}\\end{array}$ $\\xi\\in\\Xi,\\theta_{T^{\\prime}}$ ", "page_idx": 44}, {"type": "text", "text": "Lemma 7 implies that for any fixed obtained from the update rule (14) satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\big[f(\\lambda_{\\theta_{T},\\xi})-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}f(\\lambda_{\\theta,\\xi})\\big]}\\\\ &{\\stackrel{(i)}{\\le}(1-\\delta)^{T}\\mathbb{E}\\big[f(\\lambda_{\\theta_{0},\\xi_{k}})-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}f(\\lambda_{\\theta,\\xi_{k}})\\big]+4L_{\\theta,\\theta}\\ell_{\\lambda^{-1}}^{2}\\delta+\\frac{E_{1}^{(\\theta)}}{\\delta L_{\\theta,\\theta}}}\\\\ &{\\stackrel{(i i)}{\\le}\\mathbb{E}\\bigg[\\Gamma(\\theta_{0})-\\underset{\\theta\\in\\Theta,\\xi\\in\\Xi}{\\operatorname*{min}}f(\\lambda_{\\theta,\\xi})\\bigg]\\exp\\bigg\\{\\log(1-\\delta)\\cdot\\frac{1}{\\delta}\\log\\bigg[1440L_{\\xi,\\xi}\\epsilon^{-2}\\mathbb{E}\\Big[\\Gamma(\\theta_{0})-\\underset{\\theta\\in\\Theta,\\xi\\in\\Xi}{\\operatorname*{min}}f(\\lambda_{\\theta,\\xi})\\Big]\\bigg\\}}\\\\ &{\\quad+\\frac{\\epsilon^{2}}{1440L_{\\xi,\\xi}}+\\frac{\\epsilon^{2}}{1440L_{\\xi,\\xi}}}\\\\ &{\\stackrel{(i i i)}{\\le}\\frac{\\epsilon^{2}}{480L_{\\xi,\\xi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where (i) uses Lemma 7, (i) uses Eqs. (128), (151) and s760Le,cLo,o- ,(i) uses log(1 - 8) \u2264 $-\\delta$ for $\\delta\\;\\in\\;[0,1/2]$ . Hence, the above inequality implies that $\\epsilon_{0}$ defined by Lemma 8 satisfies $\\begin{array}{r}{\\epsilon_{0}\\leq\\frac{\\epsilon^{2}}{480L_{\\xi,\\xi}}}\\end{array}$ ", "page_idx": 44}, {"type": "text", "text": "As a result, we can prove that $\\mathbb{E}\\big[\\|G_{a}^{(\\xi)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}\\big]\\le\\epsilon^{2}$ and $\\mathbb{E}\\big[\\|G_{b}^{(\\theta)}(\\theta_{\\widetilde{k}},\\xi_{\\widetilde{k}})\\|^{2}\\big]\\le\\epsilon^{2}$ by substituting $\\begin{array}{r}{\\epsilon_{0}\\ \\leq\\ \\frac{\\epsilon^{2}}{480L_{\\xi,\\xi}}}\\end{array}$ and Eqs. (127)-(154) into the convergence rates (122) and (126). The number of samples required by Algorithm 1 is ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{K T(m_{\\lambda}^{(1)}H_{\\lambda}^{(1)}+m_{\\theta}^{(1)}H_{\\theta}^{(1)})+K(m_{\\lambda}^{(2)}H_{\\lambda}^{(2)}+m_{\\xi}^{(2)}H_{\\xi}^{(2)})}}\\\\ &{{\\ \\ +\\ K^{\\prime}T^{\\prime}(m_{\\lambda}^{(3)}H_{\\lambda}^{(3)}+m_{\\xi}^{(3)}H_{\\xi}^{(3)})+K^{\\prime}(m_{\\lambda}^{(4)}H_{\\lambda}^{(4)}+m_{\\theta}^{(4)}H_{\\theta}^{(4)})}}\\\\ &{{=\\!\\!\\mathcal{O}[(1-\\gamma)^{-8}\\epsilon^{-4}]\\mathcal{O}\\Big[\\frac{\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]}{(1-\\gamma)^{6}\\epsilon^{2}}\\Big]\\mathcal{O}[(1-\\gamma)^{-10}\\epsilon^{-4}]\\mathcal{O}\\Big[\\frac{\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]}{1-\\gamma}\\Big]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ +\\ \\mathcal{O}[(1-\\gamma)^{-8}\\epsilon^{-4}]\\mathcal{O}[(1-\\gamma)^{-4}\\epsilon^{-2}]\\mathcal{O}\\Big[\\frac{\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]}{1-\\gamma}\\Big]}\\\\ &{\\ +\\ \\mathcal{O}[(1-\\gamma)^{-9}\\epsilon^{-4}]\\mathcal{O}\\big(\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]\\big)\\mathcal{O}[(1-\\gamma)^{-10}\\epsilon^{-4}]\\mathcal{O}\\Big[\\frac{\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]}{1-\\gamma}\\Big]}\\\\ &{\\ +\\ \\mathcal{O}[(1-\\gamma)^{-9}\\epsilon^{-4}]\\mathcal{O}[(1-\\gamma)^{-10}\\epsilon^{-4}]\\mathcal{O}\\Big[\\frac{\\log[(1-\\gamma)^{-1}\\epsilon^{-1}]}{1-\\gamma}\\Big]}\\\\ &{\\ =\\mathcal{O}\\Big[\\frac{\\log^{2}[(1-\\gamma)^{-1}\\epsilon^{-1}]}{(1-\\gamma)^{25}\\epsilon^{10}}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "0 Proof of Theorem 3 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Note that $\\begin{array}{r l r}{\\sigma_{k}}&{{}=}&{2\\beta_{k}\\ell_{\\theta}}\\end{array}$ : So by the definition of $\\begin{array}{r l r l r l r}{\\Xi_{k}}&{:=}&{\\{\\xi}&{\\in}&{V(\\Xi)}&{:}&{f(\\lambda_{\\theta_{k},\\xi})}&{\\geq}\\end{array}$ $\\mathrm{max}_{\\xi^{\\prime}\\in V(\\Xi)}\\,f\\bigl(\\lambda_{\\theta_{k},\\xi^{\\prime}}\\bigr)-2\\beta_{k}\\ell_{\\theta}\\bigr\\}$ wehave ", "page_idx": 45}, {"type": "equation", "text": "$$\nf\\big(\\lambda_{\\theta_{k},\\xi}\\big)<\\operatorname*{max}_{\\xi^{\\prime}\\in V(\\Xi)}f\\big(\\lambda_{\\theta_{k},\\xi^{\\prime}}\\big)-2\\beta_{k}\\ell_{\\theta},\\quad\\forall\\xi\\in V(\\Xi)/\\Xi_{k}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Hence, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\xi^{\\prime}\\in\\Xi}{\\operatorname*{max}}\\,f(\\lambda_{\\theta_{k},\\xi^{\\prime}})-f(\\lambda_{\\theta_{k+1},\\xi})\\overset{(i)}{\\geq}\\underset{\\xi^{\\prime}\\in\\Xi}{\\operatorname*{max}}\\,f(\\lambda_{\\theta_{k},\\xi^{\\prime}})-f(\\lambda_{\\theta_{k},\\xi})-2\\ell_{\\theta}\\|\\theta_{k+1}-\\theta_{k}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\overset{(i i)}{>}2\\beta_{k}\\ell_{\\theta}-2\\ell_{\\theta}\\beta_{k}=0,\\quad\\forall\\xi\\in V(\\Xi)/\\Xi_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where (i) uses Eq. (10) in Proposition 3 which implies that $f(\\lambda.,\\xi)$ is $\\ell_{\\theta}$ -Lipschitz continuous, (i) uses Eq. (155) and the update rule (22) with $\\|d_{k}\\|=1$ . Eqs. (155) and (156) respectively imply the following two equations. ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Gamma(\\theta_{k})=\\underset{\\xi^{\\prime}\\in\\Xi_{k}}{\\mathrm{max}}\\;f(\\lambda_{\\theta_{k},\\xi^{\\prime}})\\geq f(\\lambda_{\\theta_{k},\\xi_{k+1}^{*}}),}\\\\ &{\\Gamma(\\theta_{k+1})=\\underset{\\xi^{\\prime}\\in\\Xi_{k}}{\\mathrm{max}}\\;f(\\lambda_{\\theta_{k+1},\\xi^{\\prime}})=f(\\lambda_{\\theta_{k+1},\\xi_{k+1}^{*}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\xi_{k+1}^{*}\\in\\arg\\operatorname*{max}_{\\xi^{\\prime}\\in\\Xi_{k}}f(\\lambda_{\\theta_{k+1},\\xi^{\\prime}})$ ", "page_idx": 45}, {"type": "text", "text": "Based on Proposition 7, there exists a unit descent direction $\\widetilde{d}_{k}$ $_k\\:(\\|\\widetilde{d}_{k}\\|=1)$ )suchthat ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0<f\\big(\\lambda_{\\theta_{k},\\xi}\\big)\\!-\\!\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\!\\le\\!f\\big(\\lambda_{\\theta_{k},\\xi}\\big)\\!-\\!f\\big(\\lambda_{\\theta^{*},\\xi}\\big)\\!\\le\\!\\big[-\\sqrt{2}\\ell_{\\lambda^{-1}}\\nabla_{\\theta}f\\big(\\lambda_{\\theta_{k},\\xi}\\big)^{\\top}\\widetilde d_{k}\\big]_{+},\\forall\\xi\\in\\Xi_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Gamma(\\theta_{k})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\operatorname*{min}}\\,\\Gamma(\\theta^{\\prime})\\overset{(i)}{\\underset{\\xi^{\\prime}\\in V(\\Xi)}{\\operatorname*{max}}}f(\\lambda_{\\theta_{k},\\xi^{\\prime}})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\operatorname*{min}}\\,\\Gamma(\\theta^{\\prime})}&{}\\\\ {\\overset{(i i)}{\\leq}\\underset{\\xi\\in\\Xi_{k}}{\\operatorname*{min}}\\,f(\\lambda_{\\theta_{k},\\xi})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\operatorname*{min}}\\,\\Gamma(\\theta^{\\prime})+2\\beta_{k}\\ell_{\\theta}}&{}\\\\ {\\overset{(i i i)}{\\leq}\\underset{\\xi\\in\\Xi_{k}}{\\operatorname*{min}}\\,\\big[-\\sqrt{2}\\ell_{\\lambda^{-1}}\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi})^{\\top}\\widetilde{d}_{k}\\big]_{+}+2\\beta_{k}\\ell_{\\theta}}&{}\\\\ {=\\big[-\\sqrt{2}\\ell_{\\lambda^{-1}}A_{k}(\\widetilde{d}_{k})\\big]_{+}+2\\beta_{k}\\ell_{\\theta}}&{}\\\\ {\\overset{(i v)}{\\leq}\\big[\\sqrt{2}\\ell_{\\lambda^{-1}}[\\epsilon_{k}-A_{k}(d_{k}^{\\prime})]\\big]_{+}+2\\beta_{k}\\ell_{\\theta},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where (i) uses Proposition 6, (i) uses $\\Xi_{k}:=\\{\\xi\\in V(\\Xi):f(\\lambda_{\\theta_{k},\\xi})\\,\\geq\\,\\mathrm{max}_{\\xi^{\\prime}\\in V(\\Xi)}\\,f(\\lambda_{\\theta_{k},\\xi^{\\prime}})\\,-\\,\\mathrm{max}_{\\xi^{\\prime}\\in V(\\Xi)}\\,f(\\lambda_{\\theta_{k},\\xi^{\\prime}})\\}\\nonumber\\,$ $2\\beta_{k}\\ell_{\\theta}\\}$ (ii uses Eq. (159), (iv) uses $\\begin{array}{r}{A_{k}(\\widetilde d_{k})\\geq\\operatorname*{min}_{d\\in B_{1}}A_{k}(d)\\geq A_{k}(d_{k}^{\\prime})-\\epsilon_{k}}\\end{array}$ based online 6 of Algorithm 2. ", "page_idx": 45}, {"type": "text", "text": "0.1  Analyze the $k$ th Iteration ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "(Case 1): If $A_{k}(d_{k}^{\\prime})\\ge0$ , then Eq. (160) implies that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\Gamma(\\theta_{k})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\leq\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{k}+2\\beta_{k}\\ell_{\\theta}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence, by $\\ell_{\\theta}$ -Lipschitz continuity of $\\Gamma(\\cdot):=\\operatorname*{max}_{\\xi\\in\\Xi}f(\\lambda._{,\\xi})$ (based on Proposition 3), we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\Gamma(\\theta_{k+1})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\leq\\Gamma(\\theta_{k})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})+\\ell_{\\theta}\\|\\theta_{k+1}-\\theta_{k}\\|\\overset{(i)}{\\leq}\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{k}+3\\beta_{k}\\ell_{\\theta},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where (i) uses Eq. (161) and the update rule (155) with $\\|d_{k}\\|=1$ ", "page_idx": 46}, {"type": "text", "text": "(Case 2): If $A_{k}(d_{k}^{\\prime})<0$ , then since $\\|d_{k}^{\\prime}\\|\\leq1$ \uff0c $d_{k}\\,=\\,d_{k}^{\\prime}/\\lVert d_{k}^{\\prime}\\rVert$ satisfies $A_{k}(d_{k})\\,\\leq\\,A_{k}(d_{k}^{\\prime})\\,<\\,0$ Hence, Eq. (160) implies that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\Gamma(\\theta_{k})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\leq\\sqrt{2}\\ell_{\\lambda^{-1}}[\\epsilon_{k}-A_{k}(d_{k})]+2\\beta_{k}\\ell_{\\theta}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "As a result, we bound the one-step optimization progress of $\\Gamma(\\boldsymbol{\\theta}_{k})$ as follows. ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Gamma(\\theta_{k+1})-\\Gamma(\\theta_{k})\\overset{(i)}{\\leq}f(\\lambda_{\\theta_{k+1},\\xi_{k+1}^{*}})-f(\\lambda_{\\theta_{k},\\xi_{k+1}^{*}})}&{}\\\\ {\\overset{(i i)}{\\leq}\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi_{k+1}^{*}})^{\\top}(\\theta_{k+1}-\\theta_{k})+\\frac{L_{\\theta,\\theta}}{2}\\|\\theta_{k+1}-\\theta_{k}\\|^{2}}&{}\\\\ {\\overset{(i i i)}{=}\\beta_{k}\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi_{k+1}^{*}})^{\\top}d_{k}+\\frac{L_{\\theta,\\theta}}{2}\\beta_{k}^{2}}&{}\\\\ {\\overset{(i v)}{\\leq}\\beta_{k}A_{k}(d_{k})+\\frac{L_{\\theta,\\theta}}{2}\\beta_{k}^{2}}&{}\\\\ {\\overset{(v)}{\\leq}\\beta_{k}\\Big(\\epsilon_{k}+\\frac{L_{\\theta,\\theta}\\beta_{k}}{2}+\\frac{\\sqrt{2}\\beta_{k}\\ell_{\\theta}}{\\ell_{\\lambda}-1}\\Big)-\\frac{\\beta_{k}}{\\sqrt{2}\\ell_{\\lambda-1}}\\big[\\Gamma(\\theta_{k})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\operatorname*{min}}\\,\\Gamma(\\theta^{\\prime})\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where (i) uses Eqs. (157) and (158), (ii) uses $L_{\\theta,\\theta}$ -smoothness of $f(\\lambda.,\\xi)$ based on Proposition 3, (ii) uses the update rule (22) with $\\|d_{k}\\|=1$ , (iv) uses $\\xi_{k+1}^{*}\\in\\Xi_{k}$ and the definition of $A_{k}$ in Eq. (23), (v) uses Eq. (163). Rearranging the above inequality yields that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Gamma(\\theta_{k+1})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\mathrm{min}}\\,\\Gamma(\\theta^{\\prime})}\\\\ &{\\leq\\biggr(1-\\frac{\\beta_{k}}{\\sqrt{2}\\ell_{\\lambda}-1}\\biggr)\\bigl[\\Gamma(\\theta_{k})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\mathrm{min}}\\,\\Gamma(\\theta^{\\prime})\\bigr]+\\beta_{k}\\biggl(\\epsilon_{k}+\\frac{L_{\\theta,\\theta}\\beta_{k}}{2}+\\frac{\\sqrt{2}\\beta_{k}\\ell_{\\theta}}{\\ell_{\\lambda}-1}\\biggr)}\\\\ &{\\overset{(i)}{=}\\frac{k}{k+2}\\bigl[\\Gamma(\\theta_{k})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\mathrm{min}}\\,\\Gamma(\\theta^{\\prime})\\bigr]+\\frac{2\\sqrt{2}\\ell_{\\lambda}-1\\epsilon_{k}}{k+2}+\\frac{8\\ell_{\\lambda}^{2}-1}{(k+2)^{2}}\\Bigl(\\frac{L_{\\theta,\\theta}}{2}+\\frac{\\sqrt{2}\\ell_{\\theta}}{\\ell_{\\lambda}-1}\\Bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where (i) uses $\\begin{array}{r}{\\beta_{k}=\\frac{2\\sqrt{2}\\ell_{\\lambda^{-1}}}{k+2}}\\end{array}$ ", "page_idx": 46}, {"type": "text", "text": "0.2 Obtain the Convergence Rate (25) ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "(Case 1):  If $A_{k}(d_{k}^{\\prime})\\;<\\;0$ for all $k\\,=\\,0,1,\\ldots,K\\,-\\,1$ , then we iterate Eq. (164) over $k\\,=$ $0,1,\\dots,K-1$ as follows. ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma(\\theta_{K})-\\underset{\\theta^{\\prime}\\in\\Theta}{\\mathrm{min}}\\,\\Gamma(\\theta^{\\prime})}\\\\ &{\\le\\displaystyle\\sum_{k=0}^{K}\\frac{k(k+1)}{K(K+1)}\\Big[\\frac{2\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{k}}{k+1}+\\frac{8\\ell_{\\lambda^{-1}}^{2}}{(k+1)^{2}}\\Big(\\frac{L_{\\theta,\\theta}}{2}+\\frac{\\sqrt{2}\\ell_{\\theta}}{\\ell_{\\lambda^{-1}}}\\Big)\\Big]}\\\\ &{\\le\\displaystyle\\frac{2\\sqrt{2}\\ell_{\\lambda^{-1}}}{K(K+1)}\\sum_{k=1}^{K}(k\\epsilon_{k})+\\frac{4\\ell_{\\lambda^{-1}}}{K+1}(\\ell_{\\lambda^{-1}}L_{\\theta,\\theta}+2\\sqrt{2}\\ell_{\\theta})}\\\\ &{\\le\\!\\sqrt{2}\\ell_{\\lambda^{-1}}\\underset{1\\le k\\le K}{\\mathrm{max}}\\,\\epsilon_{k}+\\frac{4\\ell_{\\lambda^{-1}}}{K+1}(\\ell_{\\lambda^{-1}}L_{\\theta,\\theta}+2\\sqrt{2}\\ell_{\\theta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "(Case 2): If $A_{K-1}(d_{K-1})\\geq0$ , then Eq. (162) holds for $k=K-1$ ,i.e., ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Gamma(\\theta_{K})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\leq\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{K-1}+3\\beta_{K-1}\\ell_{\\theta}\\leq\\sqrt{2}\\ell_{\\lambda^{-1}}\\epsilon_{K-1}+\\frac{6\\sqrt{2}\\ell_{\\theta}\\ell_{\\lambda^{-1}}}{K+1}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "(Case 3): If $A_{K^{\\prime}-1}(d_{K^{\\prime}-1})\\;\\geq\\;0$ for some $K^{\\prime}\\;\\in\\;\\{1,\\ldots,K-1\\}$ while $A_{k}(d_{k})\\;<\\;0$ for all $k=K^{\\prime},\\ldots,K-1$ , then we iterate Eq. (164) over $k=K^{\\prime},\\ldots,K-1$ as follows. ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma^{(\\theta_{k})}-\\frac{\\operatorname*{lim}\\{V^{\\theta}\\}}{n}}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{K}\\frac{k(k+1)}{K(k+1)}\\left[\\frac{2\\sqrt{\\mathcal{T}_{k}}\\times\\varepsilon_{k}}{k+1}+\\frac{8\\sqrt{2}_{s-1}^{2}}{(k+1)^{2}}\\left(\\frac{L_{\\theta}s}{2}+\\frac{\\sqrt{2}_{s-1}^{2}\\varepsilon_{k}}{\\varepsilon_{k-1}}\\right)\\right]}\\\\ &{+\\frac{K^{2}(K^{2}+1)}{K(k+1)}\\Gamma^{(\\theta_{k})}-\\frac{\\operatorname*{lim}\\{P^{\\theta}\\}}{n}}\\\\ &{\\overset{(a)\\le\\mathcal{B}(\\mathcal{A}(x))}{\\le}\\displaystyle\\sum_{k=1}^{K}\\frac{\\operatorname*{lim}\\{P^{\\theta}\\}}{n}+\\frac{4\\sqrt{s-1}(K^{-}K^{\\theta})}{K(k+1)}(\\varepsilon_{k-1}L_{\\theta}+2\\sqrt{\\mathcal{T}_{\\theta}})}\\\\ &{\\overset{(b)\\le}\\displaystyle\\frac{K^{2}(K^{2}+1)}{K(k+1)}\\sum_{k=1}^{K}(\\varepsilon_{k})+\\frac{4\\sqrt{s-1}(K^{2}+1)}{K(k+1)}(\\varepsilon_{k-1}L_{\\theta}+2\\sqrt{\\mathcal{T}_{\\theta}})}\\\\ &{\\overset{(b)\\le}\\displaystyle\\frac{K^{2}(K^{2}+1)}{K(k+1)}\\left(\\sqrt{2}\\varepsilon_{k}\\wedge\\varepsilon_{k}+2\\sqrt{\\mathcal{B}_{k}}\\right)}\\\\ &{\\overset{(b)\\le}\\displaystyle\\frac{2\\sqrt{\\mathcal{T}_{k}}\\times\\varepsilon_{k}}{K(k+1)}\\sum_{k=1}^{K}(\\varepsilon_{k})+\\frac{4\\sqrt{s-1}(K^{2}+K^{2})}{K(k+1)}(\\varepsilon_{k-1}L_{\\theta}+2\\sqrt{\\mathcal{T}_{\\theta}})}\\\\ &{\\quad+\\frac{K^{2}(K^{2}+1)}{K(k+1)}\\Big(\\sqrt{2}\\varepsilon_{k}\\wedge\\varepsilon_{k}+\\frac{4\\sqrt{s}}{K}\\Big)\\Big(\\varepsilon_{k}+2\\sqrt{\\mathcal{T}_{k}}\\Big)}\\\\ &{\\le\\sqrt{2}\\varepsilon_{k}-\\operatorname*{limsy}_{k \n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where (i) applies Eq. (162) to $k=K^{\\prime}$ and (ii) uses $\\begin{array}{r}{\\beta_{k}=\\frac{2\\sqrt{2}\\ell_{\\lambda^{-1}}}{k+2}}\\end{array}$ ", "page_idx": 47}, {"type": "text", "text": "In sum, the convergence rate (25) holds in all the above three cases. ", "page_idx": 47}, {"type": "text", "text": "P Proof of Corollary 1 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Note that $\\begin{array}{r}{A_{k}(d):=\\operatorname*{max}_{\\xi\\in\\Xi_{k}}\\left[\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi})^{\\top}d\\right]}\\end{array}$ defined by Eq. (23) is convex and $\\ell_{\\theta}$ -Lipschitz continuous. Hence, the best diretion $d_{k}\\in\\arg\\operatorname*{max}_{d\\in\\{d_{k,t}:0\\leq t\\leq T_{k}\\}}A_{k}(d)$ obtained from the subgradient method (28) converge at the following rate [11]. ", "page_idx": 47}, {"type": "equation", "text": "$$\nA_{k}(d_{k})-\\operatorname*{min}_{d\\in B_{1}}A_{k}(d)\\stackrel{(i)}{\\leq}\\frac{\\|d_{k,0}-d_{k}^{*}\\|^{2}+\\ell_{\\theta}^{2}T\\alpha^{2}\\,\\,(i i)}{2T\\alpha}\\,\\stackrel{(i i)}{\\leq}\\,\\frac{4+4}{24\\ell_{\\lambda^{-1}}\\epsilon^{-1}}=\\frac{\\epsilon}{3\\ell_{\\lambda^{-1}}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "$d_{k_{\\cdot}}^{*}\\in\\arg\\operatorname*{max}_{d\\in B_{1}}A_{k}(d)$ $\\begin{array}{r}{T\\,=\\,\\frac{36\\ell_{\\lambda^{-1}}^{2}\\ell_{\\theta}^{2}}{\\epsilon^{2}}\\,=\\,\\mathcal{O}[(1\\,-\\,\\gamma)^{-4}\\epsilon^{-2}]}\\end{array}$ $\\alpha=$ $\\frac{\\epsilon}{3\\ell_{\\lambda}-1\\,\\ell_{\\theta}^{2}}$ $\\|d_{k,0}\\|,\\|d_{k}^{*}\\|\\,\\leq\\,1$ $\\epsilon_{k}\\ =\\ \\frac{\\bar{\\epsilon}}{3\\ell_{\\lambda^{-1}}}$ Substituting $\\begin{array}{r}{\\epsilon_{k}=\\frac{\\epsilon}{3\\ell_{\\lambda}-1}}\\end{array}$ \uff0c $\\begin{array}{r}{K=\\frac{8\\ell_{\\lambda}-1}{\\epsilon}(\\ell_{\\lambda}-1L_{\\theta,\\theta}+2\\sqrt{2}\\ell_{\\theta})=\\mathcal{O}[(1-\\gamma)^{-3}\\epsilon^{-1}]}\\end{array}$ into the covergence rate (25), we obtain that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Gamma(\\theta_{K})-\\operatorname*{min}_{\\theta^{\\prime}\\in\\Theta}\\Gamma(\\theta^{\\prime})\\leq\\!\\sqrt{2}\\ell_{\\lambda^{-1}}\\operatorname*{max}_{1\\leq k\\leq K}\\epsilon_{k}+\\frac{4\\ell_{\\lambda^{-1}}}{K+1}(\\ell_{\\lambda^{-1}}L_{\\theta,\\theta}+2\\sqrt{2}\\ell_{\\theta})\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The above $\\epsilon$ accuracy above requires $K|V(\\Xi)|\\,=\\,\\mathcal{O}[|V(\\Xi)|(1-\\gamma)^{-3}\\epsilon^{-1}]$ evaluations to $\\lambda_{\\theta_{k},\\xi}$ $f(\\lambda_{\\theta_{k},\\xi})$ and $\\nabla_{\\theta}f(\\lambda_{\\theta_{k},\\xi})$ $\\mathbf{\\Lambda}_{k,\\xi}),\\,K T=\\mathcal{O}[(1-\\gamma)^{-7}\\epsilon^{-3}]$ subgradient updates (28), and $K=\\mathcal{O}[(1-$ $\\gamma)^{-3}\\epsilon^{-1}]$ gradient descent updates to the policy gradient descent updates (22). ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: In the abstract and introduction, we have accurately reflect our contributions, namely, our proposed new learning framework (robust RL with general utility), and our proposed algorithms as well as their convergence results for convex utilities, concave utilities and other utilities that satisfy weak Minty variational inequality. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: We mention the limitations in the conclusion section. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: All our lemmas, propositions and theorems contain assumptions (if there are) and the corresponding proofs are shown in the appendix. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: In the experiment section, we have provided all details needed to reproduce our experiments, including the problem setup (e.g. utilization function, transition kernel parameterization, policy parameterization, ambiguity set, discount factor), the algorithms we implemented (as shown in our algorithm boxes) and the hyperparameter choices. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We have uploaded our code which generates the simulation data for our experiments. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . CC / public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We have shown the experimental settings in Appendix A ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [No] ", "page_idx": 50}, {"type": "text", "text": "Justification: We report one time implementation of each algorithm. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We have shown the hardware and computing resource in Appendix A. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code ofEthics https: / /neurips.Cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We do not see any ethics violation of our theoretical study. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: We do not see any societal impact of our foundational theoretical study since it is not tied to particular applications, let alone deployments. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 51}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This paper does not release data. The code is simply implementation of the algorithms in this work, which does not pose any risks. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We have uploaded an anonymized zip file containing our code and a readme file on how to use the code. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 54}]