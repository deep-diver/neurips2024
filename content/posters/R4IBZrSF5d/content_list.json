[{"type": "text", "text": "Virtual Scanning: Unsupervised Non-line-of-sight Imaging from Irregularly Undersampled Transients ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xingyu Cui1 Huanjing Yue1 Song $\\mathbf{Li^{2,3}}$ Xiangjun $\\mathbf{Yin^{1}}$ Yusen Hou1 Yun Meng2,3 Kai $\\mathbf{Z0u^{2,3}}$ Xiaolong $\\mathbf{H}\\mathbf{u}^{2,\\bar{3}}$ Jingyu $\\mathbf{Yang^{1}}\\,$ ", "page_idx": 0}, {"type": "text", "text": "1School of Electrical and Information Engineering, Tianjin University, China 2School of Precision Instrument and Optoelectronic Engineering, Tianjin University, China 3Key Lab. of Optoelectronic Information Science and Technology, Ministry of Education, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Non-line-of-sight (NLOS) imaging allows for seeing hidden scenes around corners through active sensing. Most previous algorithms for NLOS reconstruction require dense transients acquired through regular scans over a large relay surface, which limits their applicability in realistic scenarios with irregular relay surfaces. In this paper, we propose an unsupervised learning-based framework for NLOS imaging from irregularly undersampled transients (IUT). Our method learns implicit priors from noisy irregularly undersampled transients without requiring paired data, which is difficult and expensive to acquire and align. To overcome the ambiguity of the measurement consistency constraint in inferring the albedo volume, we design a virtual scanning process that enables the network to learn within both range space and null space for high-quality reconstruction. We devise a physics-guided SUREbased denoiser to enhance robustness to ubiquitous noise in low-photon imaging conditions. Extensive experiments on both simulated and real-world data validate the performance and generalization of our method. Compared with the state-of-theart (SOTA) method, our method achieves higher fidelity, greater robustness, and remarkably faster inference times by orders of magnitude. The code and model are available at https://github.com/XingyuCuii/Virtual-Scanning-NLOS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Non-line-of-sight (NLOS) imaging aims to reconstruct hidden scenes beyond the direct line of sight of the detector, garnering interest across various fields such as robot vision, autonomous driving, rescue operations, remote sensing, and medical imaging [1\u20135]. In a typical active confocal NLOS imaging system, as depicted in Fig. 1, a laser source and a detector are both focused on the same point on a relay surface. Pulses emitted by the laser reflect off the surface to illuminate the hidden scene. The detector captures photons bouncing back from the scene toward the relay surface, referred to as transients, from which the hidden scene can be recovered using elaborately designed algorithms. ", "page_idx": 0}, {"type": "text", "text": "While existing works have achieved remarkable breakthroughs, they also face significant limitations that hinder their practical applicability. These methods assume dense and regular scanning of a large relay surface, which may not be feasible in realistic scenarios with irregular relay surfaces such as latticed windows or fences. Transients obtained through irregular undersampling can result in severe ill-posedness, leading to artifacts or reconstruction failure. This raises the challenging task of NLOS imaging from irregularly undersampled transients (IUT). To address this, Liu et al. [6] proposed introducing manually designed strong regularization terms under a functional optimization framework. However, this approach is hindered by the need for lengthy numerical iterative computations. Recent learning-based NLOS imaging methods [7\u201310] enable quick inference, but they require supervision from a large amount of paired data, including well-aligned ground-truth albedos, which are difficult and expensive to acquire. Therefore, exploring NLOS imaging under unsupervised learning is worthwhile to eliminate the heavy reliance on paired data. ", "page_idx": 0}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/73de54d666430214fb225d1db1dffd63599263e37eb68865fdc2e93c582fcce4.jpg", "img_caption": ["Figure 1: Illustration of the active confocal NLOS imaging with an irregular relay surface. Yellow points indicate scannable points, while blue points indicate non-scannable points. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel unsupervised framework capable of learning implicit priors from noisy IUT. This framework comprises two components: a virtual scanning reconstruction network (VSRnet) that learns high-quality measurement-to-albedo mapping beyond the range space induced by the measurement consistency, and a SURE-based denoiser that enhances our method\u2019s robustness to measurement noise by incorporating the physics model of the low-photon time-resolved detector. We conduct extensive experiments on simulated data, publicly available real data, and data acquired from our self-built NLOS system. Our method outperforms existing algorithms, particularly in providing robustness for real data and diverse irregular relay surfaces. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an unsupervised NLOS imaging framework capable of learning implicit priors from noisy IUT, effectively overcoming the dependency on paired data that is difficult to acquire and align. \u2022 We introduce a virtual scanning process that enables the network to learn within both range and null spaces for high-quality reconstruction from IUT, extending NLOS imaging to realistic scenarios with irregular relay surfaces. \u2022 We propose a SURE-based denoiser, an unsupervised physics-guided module that incorporates the low-photon time-resolved detector\u2019s physics model, to enhance our method\u2019s robustness to noise. \u2022 We evaluate our method on simulated, publicly available, and self-captured real data, demonstrating its superior reconstruction quality and significantly faster inference compared to the SOTA method. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Model-based NLOS reconstruction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Model-based NLOS algorithms have achieved significant advances in recent years. Direct reconstruction algorithms [11\u201315] offer rapid implementations for NLOS imaging, while iterative algorithms [16\u201318, 5] leverage more accurate physical models for higher reconstruction quality. Recent efforts aim to extend NLOS imaging to challenging real-world scenarios. For instance, Manna et al. [19] and Gu et al. [20] addressed NLOS imaging with dynamic and non-planar relay surfaces, respectively. However, these methods still rely on large relay surfaces and dense measurements. Another set of algorithms [21\u201325] aims to achieve high spatial resolution reconstruction using sparse sampling measurements to significantly reduce acquisition time. Yet, they are constrained to specific scanning patterns, such as regular or Hadamard patterns. To address NLOS imaging from irregularly undersampled transients, Liu et al. [6] introduced a reconstruction model using Confocal Complemented Signal-Object Collaborative Regularization (CC-SOCR). Despite its effectiveness, CC-SOCR suffers from long inference times due to its iterative nature. ", "page_idx": 1}, {"type": "text", "text": "2.2 Learning-based NLOS reconstruction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recently, deep learning has gained attention for NLOS imaging due to its learning capabilities and fast inference. Chopite et al. [7] first employed deep learning for NLOS reconstruction, but their method fell short compared to model-based approaches due to the lack of physical guidance. Physics-guided methods [8, 26, 9, 10] have since emerged to enhance reconstruction quality, particularly for realworld data. Recent works focus on NLOS imaging from regularly undersampled transients [27, 28], but they require large datasets of paired data and fail to reconstruct from IUT. Furthermore, these supervised algorithms still have significant room for improvement in robustness on real data due to the gap between simulated datasets used for training and real-world data. ", "page_idx": 2}, {"type": "text", "text": "3 Problem formulation and motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 NLOS Imaging from IUT ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fig. 1 illustrates confocal NLOS imaging with time-resolved systems. By scanning a set of points $P=\\{(x_{i},y_{i},0)\\mid i=1,2,\\ldots,s,\\ x_{i},y_{i}\\in\\mathbb{R}\\}$ on the relay surface, the forward model of NLOS imaging can be modeled as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tau(p,t)=\\int_{Q}\\frac{\\kappa(q)}{\\|p-q\\|^{4}}\\cdot\\delta(2\\|p-q\\|-t c)d q\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau$ is the spatial-temporal measurement, $p=(x,y,0)$ denotes the scanning point, $\\kappa(q)$ denotes the albedo value at point $q$ in the 3D hidden scene $Q$ , $c$ is the speed of light. The distance $\\|p-q\\|$ is related to the time of flight $t$ through the Dirac delta function $\\delta(\\cdot)$ . For compact presentation and analysis, we employed a discretized version of the above forward model. Let $u\\in\\mathbb{R}^{s t}$ and $\\boldsymbol{\\rho}\\in\\mathbb{R}^{l^{2}z}$ represent the vectorized measurements and albedos of the hidden object, respectively, where $l$ denotes the size of the vertical and horizontal dimensions, and $z$ denotes the depth dimension. We denote the forward operator, also known as the light transport matrix, by $H\\in\\mathbb{R}^{s t\\times l^{2}z}$ . The forward processing can be described by the following linear model: ", "page_idx": 2}, {"type": "equation", "text": "$$\nu=H\\rho.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notably, the forward operator $H$ is related not only to the optical-electronic characteristics of the NLOS system but also to the scannable region on the relay surface. ", "page_idx": 2}, {"type": "text", "text": "3.2 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Deep learning-based algorithms have demonstrated significant potential to enhance NLOS imaging performance compared to model-based approaches. Most prior work has adopted a supervised paradigm, which requires substantial amounts of high-quality paired data. However, it is prohibitively expensive or even infeasible to acquire ground-truth 3D albedo volumes precisely aligned with the spatio-temporal measurements. This limitation motivates us to develop an unsupervised NLOS reconstruction framework that avoids this dependency and further enhances the generalization of learning-based methods to real-world data. ", "page_idx": 2}, {"type": "text", "text": "The standard approach in unsupervised learning is to train the reconstruction mapping $f_{\\theta}$ by minimizing the measurement consistency (MC) loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{u}\\|H f_{\\theta}(u)-u\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Nevertheless, solely enforcing the MC loss without ground truth supervision does not guarantee highquality reconstruction. This can be analyzed from the perspective of the range-null decomposition [29]. Let $\\mathcal{N}_{H}=\\{v\\in\\mathbb{R}^{l^{2}z}\\ |\\ H v=0\\}$ be the null space of the operator $H$ . Its complementary space is the range space of $H^{\\top}$ , denoted by $\\mathcal{R}_{H}=\\{H^{\\top}u,u\\in\\mathbb{R}^{s t}\\}$ , such that $\\mathbb{R}^{l^{2}z}=\\mathcal{R}_{H}\\oplus\\mathcal{N}_{H}$ . ", "page_idx": 2}, {"type": "text", "text": "Any albedo volume $\\rho$ can be decomposed into a range-space component and a null-space component. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\rho=}&{{}}&{\\underbrace{H^{\\dagger}H\\rho}_{}}&{}&{{}+\\underbrace{(I-H^{\\dagger}H)\\rho}_{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/0904d9f35345b8165e328932e2b7eb8519224d859b3e1def3247731b89ce03f7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The pipeline of our unsupervised framework: (a) The SURE-based denoiser, which consists of an encoder-decoder network designed for IUT and is trained by minimizing SURE loss in the first stage; (b) The virtual scanning reconstruction network (VSRnet), which consists of a Unet-like network and is trained by MC loss and VS loss in the second stage; (c) Relay surfaces used for training (yellow indicates scanning areas, and their corresponding 3D binary masks are used in implementation); (d) The virtual scanning process, which involves virtually observing $\\rho^{(1)}$ with a relay surface $M_{k}$ that is distinct from $M_{g}$ and enforcing consistency between $\\rho^{(1)}$ and $\\rho^{(2)}$ . ", "page_idx": 3}, {"type": "text", "text": "where $H^{\\dag}\\,\\in\\,\\mathbb{R}^{l^{2}z\\times s t}$ is the pseudo-inverse of $H$ satisfying $H H^{\\dagger}H\\,=\\,H$ . The operator $H^{\\dagger}H$ projects the sample $\\rho$ into the range space $\\mathcal{R}_{H}\\colon\\mathcal{D}_{r}(\\rho)=\\dot{H}^{\\dagger}\\check{H_{\\rho}}$ , whereas its complementary operator $(I-H^{\\dagger}H)$ projects $\\rho$ into the null-space $\\mathcal{N}_{H}\\colon\\mathcal{D}_{n}(\\rho)=(I-H^{\\dagger}H)\\rho$ . As long as the trained network $f_{\\theta}$ reconstructs from input $u$ as $f_{\\theta}\\bar{(}u)=D_{r}(\\rho)\\ddot{+}v_{n}$ for $\\forall\\ \\ v_{n}\\in\\mathcal{N}_{H}$ , the reconstructed volume $f_{\\theta}(u)$ would fully meet the MC requirement: $H(\\mathcal{D}_{r}(\\rho)+v_{n})=H H^{\\dagger}H\\rho+H v_{n}=u$ since we have $H H^{\\dagger}H\\rho=u$ and $H v_{n}=0$ . This suggests that the MC constraint only locates the albedo volume in a broad subspace surrounding the range-space projection $\\mathcal{D}_{r}(\\rho)$ , and the inference of the component $\\mathcal{N}_{H}$ is ad-hoc without guidance. ", "page_idx": 3}, {"type": "text", "text": "This necessitates an unsupervised framework capable of learning beyond the range space. We note that model-based algorithms [18, 6, 25, 23] manually design regularization terms to learn beyond range space, but suffer from long inference times. In other computational imaging tasks, supervised methods [30, 31, 29, 32] and unsupervised methods [33\u201337] have been proposed to recover nullspace components of the reconstructions. Along this avenue, we propose an effective unsupervised framework capable of learning in both range-null spaces for NLOS imaging from IUT. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Unsupervised framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Fig. 2 shows the proposed unsupervised framework via virtual scanning for NLOS imaging from IUT. The framework consists of two components: 1) a virtual scanning reconstruction network (VSRnet) to recover the 3D albedo volume from both range and null space, and 2) a SURE-based denoiser to enhance the robustness to ubiquitous noise in transients. Given a set of $G$ noisy irregularly undersampled transients $\\mathcal{U}=\\left\\{\\tilde{u}_{g}\\in\\mathbb{R}^{s t}|\\;g=1,\\ldots,G\\right\\}$ and the set of their corresponding forward operators $\\mathcal{H}=\\{H_{g}\\in\\mathbb{R}^{s t\\times l^{2}z}|\\;g=1,\\dots,G\\}$ , our goal is to train a deep neural mapping without labeled supervision to reconstruct the 3D albedo volume $\\rho$ from the noisy IUT $\\tilde{u}$ . As discussed in ", "page_idx": 3}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/1e4e0139f866008e2f59618950efdea09aa5c2be39d5a4b922ec7a49461c4129.jpg", "img_caption": ["Figure 3: Toy visualization of NLOS reconstruction from the perspective of range-null space decomposition (RNSD). (a) illustrates the RNSD of $\\rho$ observed by $H_{1}$ . (b) illustrates that $\\rho$ cannot be accurately recovered with only the measurement consistency loss. (c) shows that the proposed virtual scanning promotes the acquisition of null-space components. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Sec. 3.1, due to the one-to-one correspondence between the forward operator and the relay surface, we will loosely use $H_{g}$ to refer to different relay surfaces without ambiguity in the following sections. ", "page_idx": 4}, {"type": "text", "text": "We first briefly sketch the workflow of the inference stage, as shown in Fig. 2(a) and $2({\\mathfrak{b}})$ . The noisy IUT $\\tilde{u}_{g}$ , observed through an irregular relay surface $H_{g}$ , is first zero-padded to $\\tilde{u}_{g r}$ on a full scanning grid. This is then passed through the SURE-based denoiser $F_{\\phi}$ to remove noise. Subsequently, the albedo volume $\\rho$ is reconstructed using VSRnet $f_{\\theta}$ , which incorporates the physical prior of NLOS imaging (LCT [11]) and a learned reconstruction mapping $F_{\\theta}$ . During the training stage, the denoiser is regularized by the SURE loss, while the reconstruction network is regularized by the MC loss. Additionally, we introduce the virtual scanning process (Fig. 2(d)) to capture the measurement details within the null space of observation operators. In this process, the reconstructed $\\rho^{(1)}$ is projected into the measurement space as $u_{k r}$ by virtually scanning with a different relay surface $H_{k}\\in\\mathcal{H}$ , which is distinct from $H_{g}$ . The virtual scanned measurement $u_{k r}$ is then projected back into the reconstruction space as $\\rho^{(2)}$ by the reconstruction network. We impose a virtual scanning (VS) loss between the two reconstructed volumes, $\\rho^{(1)}$ and $\\rho^{(2)}$ , to promote null-space learning (Sec. 4.2). The modules of our framework and the loss functions utilized for training are detailed in the following subsections. The structures of $F_{\\phi}$ and $F_{\\theta}$ are detailed in the Supplementary Material (SM). ", "page_idx": 4}, {"type": "text", "text": "4.2 Virtual Scanning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Strategy The training strategy of VSRnet is depicted in Fig. 2(b) and 2(d). Specifically, at the reconstruction module (Fig. 2(b)), the denoised sample $\\hat{u}_{g r}$ is initially transformed to the albedo domain using the inverse operator of LCT $H_{r}^{\\dagger}$ . The resulting $H_{r}^{\\dag}\\hat{u}_{g r}$ is then mapped to the albedo volume $\\rho^{(1)}$ by the reconstruction network $F_{\\theta}$ with a global residual connection. At the virtual scanning module, the reconstructed albedo volume $\\rho^{(1)}$ is projected into a virtual undersampled measurement $u_{k r}$ using another forward operator $H_{k}\\ \\in\\ \\mathcal{H}$ $(H_{k}~\\neq~H_{g})$ . To achieve efficient implementation of forward operators, we decouple $H_{k}$ into $M_{k}\\odot H_{r}$ , which can be efficiently computed using Hadamard product and the fast Fourier transform. In practice, $M_{k}\\in\\mathbb{R}^{l\\times l\\times z}$ is the 3D binary mask associated with the relay surface, constructed by repeating the 2D sampling pattern $t$ times along the time dimension. $H_{r}$ represents the forward operator of LCT. Following the same reconstruction pipeline, an albedo volume, denoted by $\\rho^{(2)}$ , is obtained from the virtual measurement $u_{k r}$ . The processes of obtaining $\\rho^{(1)}$ and $\\rho^{(2)}$ can be formalized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\rho}^{(1)}=f_{\\theta}(F_{\\phi}(\\mathrm{Pad}(\\tilde{u}_{g}),M_{g})\\odot M_{g}),}\\\\ &{\\boldsymbol{\\rho}^{(2)}=f_{\\theta}(H_{r}\\boldsymbol{\\rho}^{(1)}\\odot M_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The two reconstructed albedos, $\\rho^{(1)}$ and $\\rho^{(2)}$ , should be identical if the learned mapping $F_{\\theta}$ enables perfect reconstruction. This motivates us to impose a proximity constraint between $\\rho^{(1)}$ and $\\rho^{(2)}$ in the albedo domain, named the virtual scanning loss. The virtual scanning process in the training pipeline facilitates the learning of the null-space component, thereby providing a promising prior that complements the range space, as analyzed below. ", "page_idx": 4}, {"type": "text", "text": "Analysis Fig. 3 provides a more intuitive understanding of how the proposed virtual scanning facilitates the network in learning the null-space component. Let $H_{1}$ and $H_{2}$ be two observation operators associated with two different relay surfaces. We observe $\\rho$ using the forward operator $H_{1}$ , obtaining the measurement $u_{1}$ . As illustrated in Fig. 3(a), $\\rho$ is projected into the range-space $\\mathcal{R}_{H_{1}}$ of $H_{1}$ . When we only impose the MC constraint, the resulting output $\\rho^{(1)}$ will belong to the following set $\\chi_{H_{1}}$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{X}_{H_{1}}=\\{v\\ |\\ H_{1}^{\\dagger}H_{1}v=\\mathcal{D}_{r}(\\rho),\\mathcal{D}_{r}(\\rho)\\in\\mathcal{R}_{H_{1}}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As depicted by the blue dashed line in Fig. 3(b), there exist multiple outputs that satisfy the measurement consistency. If, for instance, we obtain inaccurate estimated results denoted as $\\rho^{(1)}$ through $\\rho^{(1)}=\\mathcal{D}_{r}(\\rho)+\\dot{F}_{\\theta}(\\mathcal{D}_{r}(\\rho))$ , we then utilize $H_{2}$ to virtually scan $\\rho^{(1)}$ and project it into the range space $\\mathcal{R}_{H_{2}}$ . Subject to the constraint $\\rho^{(1)}=\\rho^{(2)}$ , $F_{\\theta}(\\mathcal{D}_{r}(\\rho^{(1)}))$ converges to $\\mathcal{D}_{n}(\\rho^{(1)})$ due to the relationship $\\rho^{(1)}=\\mathcal{D}_{r}(\\rho^{(1)})+\\mathcal{D}_{n}(\\rho^{(1)})$ . Following this iteration, the network can learn within $\\mathcal{R}_{H_{1}}$ and $\\mathcal{N}_{H_{2}}$ . The entire process is illustrated in Fig. 3(c). Similarly, by altering the order of operators $H_{1}$ and ${\\bar{H}}_{2},F_{\\theta}$ will also observe within $\\mathcal{R}_{H_{2}}$ and $\\mathcal{N}_{H_{1}}$ . In practice, a set of operators $\\mathcal{H}$ is provided to enhance the network\u2019s generalization across various operators with different relay surfaces. ", "page_idx": 5}, {"type": "text", "text": "4.3 SURE-based denoiser ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "NLOS imaging inherently operates under photon-limited conditions, and noise in transient measurements can lead to severe background artifacts in the albedo space. This hinders the network\u2019s ability to learn implicit priors from noisy IUT, especially in unsupervised learning. Inspired by previous studies [38\u201342], we introduce a physics-guided unsupervised denoiser to suppress measurement noise. Specifically, we leverage Stein\u2019s Unbiased Risk Estimator (SURE) [43] to derive an unsupervised learning loss function that considers the physical model of the time-resolved detector in NLOS imaging systems, which can be modeled as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{u}\\sim\\mathrm{Poisson}(u+b),}\\\\ &{u=H\\rho,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $b$ represents the dark counts of the detector along with the background photons. The parameterized denoiser $F_{\\phi}$ learns to map the noisy measurement $\\tilde{u}$ to its clean version $u$ via minimization of the SURE loss function given in Eq. (8). ", "page_idx": 5}, {"type": "text", "text": "4.4 Loss function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given a measurement set $\\{\\Tilde{u}_{i,g},i=1,2,\\dots,I,g=1,2,\\dots,G\\}$ collected by observing $I$ hidden scenes, each with $G$ relay surfaces, the training of our framework involves three loss functions: the SURE loss $\\mathcal{L}_{\\mathrm{SURE}}$ , the MC loss ${\\mathcal{L}}_{\\mathrm{MC}}$ , and the VS loss $\\mathcal{L}_{\\mathrm{VS}}$ . The SURE loss is an unbiased estimation of the mean squared error (MSE) under the Poisson noise model taking dark count consideration: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{SURE}}=\\mathbb{E}_{\\{i,g\\}}\\big\\{\\cfrac{1}{s t}\\|\\tilde{u}_{i,g}-F_{\\phi}(\\tilde{u}_{i,g})\\|_{2}^{2}-\\cfrac{1}{s t}(\\mathbf{1}+b)^{\\top}\\tilde{u}_{i,g}}\\\\ &{\\mathrm{~+~}\\cfrac{2}{s t}b^{\\top}F_{\\phi}(\\tilde{u}_{i,g})+\\cfrac{2}{s t\\varepsilon}(e_{i,g}\\odot\\tilde{u}_{i,g})^{\\top}\\big(F_{\\phi}(\\tilde{u}_{i,g}+\\varepsilon e_{i,g})-F_{\\phi}(\\tilde{u}_{i,g})\\big)\\big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\varepsilon$ is a positive number, $e_{i,g}\\in\\{-1,1\\}^{s t}$ is a binary vector, whose elements follow a Bernoulli distribution with equal probability [40], and $\\odot$ denotes element-wise multiplication. Detailed derivation of the SURE loss is given in supp. material. ", "page_idx": 5}, {"type": "text", "text": "We adopted the mean squared error for ${\\mathcal{L}}_{\\mathrm{MC}}$ and ${\\mathcal{L}}_{\\mathrm{VS}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\operatorname{MC}}=\\mathbb{E}_{\\{i,g\\}}\\|\\hat{u}_{i,g}-H_{g}(f_{\\theta}(\\hat{u}_{i,g}))\\|_{2}^{2},}\\\\ &{\\mathcal{L}_{\\operatorname{VS}}=\\mathbb{E}_{\\{i,g\\}}\\|f_{\\theta}(\\hat{u}_{i,g})-f_{\\theta}(H_{k}(f_{\\theta}(\\hat{u}_{i,g}))\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We first trained the denoiser $F_{\\phi}$ using the SURE loss. Next, we froze the SURE-based denoiser and trained the network $F_{\\theta}$ with a combined loss function, $\\mathcal{L}(\\theta)=\\mathcal{L}_{M C}(\\theta)+\\beta\\mathcal{L}_{V S}(\\theta)$ , where $\\beta$ is a trade-off parameter (see SM for training details). ", "page_idx": 5}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experiment setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset We generated 8,000 transients using the transient rasterizer from [8] with default parameters. The transients have a spatial-temporal resolution of $128\\times128\\times512$ with a bin width of $33\\,\\mathrm{ps}$ . The dataset contains 111 objects from the alphanumerics dataset [9], covering lowercase and uppercase letters from the English and Greek alphabets, and numerals from 0 to 9. We also rendered a sample \u201cbunny\u201d for quantitative comparison. To assess our method\u2019s generalization, we tested it on real-world data acquired by three different systems [10\u201312] and a self-built system (see SM for system details). The hidden scenes feature various reflective materials, depth ranges, and geometric shapes, and were acquired under different conditions including scanned areas, spatial resolutions, bin widths, and integration times. For a fair comparison with CC-SOCR [6] within a manageable time frame, we resized the full-sampled transients [10, 12] to $128\\times128\\times256$ and the full-sampled transients [11] to $64\\times64\\times256$ . We used these resized transients for all methods. ", "page_idx": 5}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/d5c3bdce1ee2e7b94e63d3ad4d2b0229af4795420f598f79c5973bb32c1644e6.jpg", "img_caption": ["Figure 4: Reconstruction results of the \u201cbunny\u201d with different relay surfaces. The intensity images are normalized to the range of 0 to 1 through maximum value normalization. PSNR (dB) for each intensity image is displayed in the top right corner. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Relay surface To simulate the irregularly undersampled process, we extracted signals from the full-sampled transients according to various irregular relay surfaces. For training, we sampled five horizontal shutter patterns with intervals of [4, 8, 12, 16, 20] and 40 uniform rotations from 0 to 180 degrees, resulting in 200 sampling patterns (see Fig. 2(c)). For testing, we included more realistic irregular relay surfaces to evaluate our method\u2019s generalization capability in real-world scenarios. ", "page_idx": 6}, {"type": "text", "text": "Compared methods We compare our method with three traditional direct reconstruction algorithms (LCT [11], FK [12], RSD [13]), two learning-based algorithms (Unsupervised NeTF [26], Supervised USM [28]), and one iterative algorithm (CC-SOCR [6]). Since LCT, FK, RSD, and USM are designed for raster scanning, we use zero-padded versions of the IUT as input, similar to our method. NeTF and CC-SOCR accept irregularly undersampled transients. For a fair comparison, we also applied our SURE-based denoiser to pre-denoise the transients for all compared methods. However, we did not apply the pre-denoising step for CC-SOCR, as the results reconstructed by CC-SOCR did not show significant improvements. This is because the strong CC-SOCR regularization inherently performs some level of denoising. We compute the peak signal-to-noise ratio (PSNR) to quantitatively evaluate the reconstruction results on the simulated dataset for all methods. ", "page_idx": 6}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Simulated data Fig. 4 shows comparison results on the \u201cbunny\u201d. LCT, FK, NeTF, CC-SOCR, and our method can recover the main body. However, FK and CC-SOCR lose structures around the ear, while LCT and RSD exhibit aliasing artifacts due to irregular undersampling. Among the learning-based methods, NeTF produces a blurry object with diffused artifacts and loss of structures around the ear. USM struggles to adapt to irregularly undersampled transients and fails to reconstruct the hidden object. In contrast, our method successfully recovers most geometric structures of the bunny without aliasing artifacts, achieving the best quantitative results. ", "page_idx": 6}, {"type": "text", "text": "Real-world data We first tested our method on publicly available real datasets [10\u201312] (Fig.5), and then on self-captured real-world data (Fig.6). Without proper regularization, the direct reconstruction methods, i. e., LCT and RSD, exhibit severe aliasing artifacts due to undersampling. FK is depthsensitive and fails to recover far-end structures in the hidden scene. NeTF tends to produce blurry shapes due to its struggle with utilizing limited information in IUT. USM produces results that are nearly overwhelmed by aliasing artifacts in irregularly undersampled cases, but can achieve acceptable results in regularly undersampled cases (Fig.6). CC-SOCR can recover main objects for all the test cases, but loses fine structures at distant depth layers (\u201cMan Deer\u201d, \u201cLetter\u201d, \u201cDeer\u201d and \u201cTeaser\u201d) and underperforms on glossy (\u201cDragon\u201d) or retroreflective (\u201cSU\u201d) objects. Our method achieves the best quality for various objects and sampling patterns. Our method generates stable results on real-world data with diverse attributes and relay surfaces despite being trained only on a simple alphanumeric dataset and shutter-like relay surfaces. It notably outperforms the range-space solver LCT, successfully removing aliasing artifacts while preserving structure. The promising generalization highlights the effectiveness of our method in learning beyond the range space. ", "page_idx": 6}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/bd271ad38a85aefc1ea21e549c1675d96b29f00288ce572ed394ca11cc563f10.jpg", "img_caption": ["Figure 5: Reconstruction results of publicly available real-world dataset [10\u201312] with different relay surfaces. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/0a3c3d1d0eba9b2155a7fb72f3be5aeb01410cf3fe9e1e823d5eea517caf8815.jpg", "img_caption": ["Figure 6: Reconstruction results of self-captured real-world dataset with different relay surfaces. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Inference time ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 1: Inference time of various methods. The values in the table represent the average inference time across 16 IUT with size of $128\\times128\\times256$ . ", "page_idx": 8}, {"type": "table", "img_path": "R4IBZrSF5d/tmp/457655092335b57290347fc5495a599653370b0d7b819265e577e3fa725d83e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We compare the inference time of various methods on an Intel(R) Xeon(R) Platinum 8369B 2.90GHz CPU with 32 cores and an NVIDIA 3090 GPU, respectively. The inference times of NeTF and CC-SOCR vary with different IUT. Therefore, we average the inference times across 16 IUT shown in Fig. 5 and Fig. 6 each with a size of $128\\times128\\times256.$ . As shown in Tab. 1, the direct reconstruction methods, LCT, FK, and RSD, are faster than the other methods. Unlike these one-step approaches, CC-SOCR iteratively solves the functional model by a series of alternative sub-problems, requiring nearly eight hours to reconstruct an albedo. NeTF stands on the per-scene rendering framework and thus requires significantly longer inference times than the other two learning-based methods, USM, and our method. Note that both our method and CC-SOCR achieve more accurate reconstructions than the other methods. However, our method is $12{,}000\\times$ faster than CC-SOCR in CPU mode. ", "page_idx": 8}, {"type": "text", "text": "5.4 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We validate the effectiveness of the two core components of our method: the virtual scanning process (VS) and the SURE-based denoiser. For quantitative evaluation, we simulated a dataset of 1,000 transients by rendering objects with complex geometries, such as chairs, clocks, guitars, sofas, and motorcycles. Our method and its two variants were tested on the simulated transients sampling with 15 different relay surfaces. For qualitative evaluation, we assess the two components using realworld datasets, \u201cTeaser\u201d and \u201cDragon\u201d. \u201cTeaser\u201d features complex structures, which helps evaluate VS\u2019s ability to recover details. \u201cDragon\u201d, with its glossy material, exhibits a lower signal-to-noise ratio in its acquired transient, which helps validate the SURE-based denoiser\u2019s effectiveness. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Ablation study for the virtual scanning process and the SURE-based denoiser. ", "page_idx": 9}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/11fd5d68d1e4ad505a96e0e830a55145cc9e73e6df21f3f0cf81c7da55c2ec4a.jpg", "img_caption": ["Figure 7: Ablation studies on the virtual scanning process (VS) (a) and the SURE-based denoiser (b). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Virtual Scanning As shown in Tab. 2, the virtual scanning process resulted in an average improvement of 0.89 dB. Fig. 7 demonstrates that without the VS component, our method can only recover a limited portion of the scene structure, which is impaired by aliasing artifacts. In contrast, our full model reconstructs more detailed and cleaner structures, confirming the effectiveness of VS component in recovering null-space components. ", "page_idx": 9}, {"type": "text", "text": "SURE-based denoiser As evident from the quantitative results, the SURE-denoiser achieved an average performance improvement of $1.83\\;\\mathrm{dB}$ , which is more significant than that of VS. This result aligns with expectations because background noise affects the entire reconstruction volume, while aliasing artifacts caused by irregular undersampling disrupt the main structure. For both LCT and our method, as illustrated in Fig. 7(b), the denoiser effectively suppresses noise artifacts. In Sec. 5.2, we apply the denoiser to competing methods to enhance their robustness to noise for a fair comparison. This demonstrates the versatility of the denoiser as a plug-and-play module in other NLOS algorithms. ", "page_idx": 9}, {"type": "text", "text": "Additional ablation studies on the hyperparameters within the loss functions and the relay surfaces used for training are provided in the supplementary materials. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion In this paper, we propose an unsupervised learning-based framework for NLOS imaging from irregularly undersampled transients (IUT). By introducing a virtual scanning process and a SURE-based denoiser, our framework achieves high-quality and fast NLOS reconstructions from IUT. Furthermore, it can be trained solely from noisy IUT, enabling future work on direct learning from real-world datasets to bridge the gap between simulated and real datasets. Our method outperforms the state-of-the-art method on both simulated and real-world data with various relay surfaces. In future work, we will extend our method to non-confocal imaging systems for more practical applications. ", "page_idx": 9}, {"type": "text", "text": "Limitations Our method faces two main limitations. Firstly, it is constrained to the confocal system due to the lack of a high-speed, low-memory non-confocal forward operator for deep learning training. However, the framework is theoretically extendable to general forward operators, indicating future research on new forward operators in NLOS imaging may not require special consideration for irregular undersampling. Secondly, while our method is entirely unsupervised, we only present the results of our method which is trained using simulated transients due to the time-consuming nature of collecting real transient datasets. Nonetheless, this approach partially mitigates the data gap introduced by simulated ground truth, and the results demonstrate excellent performance. Transitioning to deep learning that exclusively uses real transients is one of our future goals. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China under Grants 62231018, 62071322, 62072331. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] D. Faccio, A. Velten, and G. Wetzstein, \u201cNon-line-of-sight imaging,\u201d Nature Reviews Physics, vol. 2, no. 6, pp. 318\u2013327, 2020.   \n[2] S. Chan, R. E. Warburton, G. Gariepy, J. Leach, and D. Faccio, \u201cNon-line-of-sight tracking of people at long range,\u201d Optics express, vol. 25, no. 9, pp. 10 109\u201310 117, 2017.   \n[3] M. Isogawa, Y. Yuan, M. O\u2019Toole, and K. M. Kitani, \u201cOptical non-line-of-sight physics-based 3d human pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 7013\u20137022.   \n[4] N. Scheiner, F. Kraus, F. Wei, B. Phan, F. Mannan, N. Appenrodt, W. Ritter, J. Dickmann, K. Dietmayer, B. Sick et al., \u201cSeeing around street corners: Non-line-of-sight detection and tracking in-the-wild using doppler radar,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2068\u20132077.   \n[5] C. Wu, J. Liu, X. Huang, Z.-P. Li, C. Yu, J.-T. Ye, J. Zhang, Q. Zhang, X. Dou, V. K. Goyal et al., \u201cNon-line-of-sight imaging over 1.43 km,\u201d Proceedings of the National Academy of Sciences, vol. 118, no. 10, p. e2024468118, 2021.   \n[6] X. Liu, J. Wang, L. Xiao, Z. Shi, X. Fu, and L. Qiu, \u201cNon-line-of-sight imaging with arbitrary illumination and detection pattern,\u201d Nature Communications, vol. 14, no. 1, p. 3230, 2023.   \n[7] J. Grau Chopite, M. B. Hullin, M. Wand, and J. Iseringhausen, \u201cDeep non-line-of-sight reconstruction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 960\u2013969.   \n[8] W. Chen, F. Wei, K. N. Kutulakos, S. Rusinkiewicz, and F. Heide, \u201cLearned feature embeddings for non-line-of-sight imaging and recognition,\u201d ACM Transactions on Graphics (ToG), vol. 39, no. 6, pp. 1\u201318, 2020.   \n[9] F. Mu, S. Mo, J. Peng, X. Liu, J. H. Nam, S. Raghavan, A. Velten, and Y. Li, \u201cPhysics to the rescue: Deep non-line-of-sight reconstruction for high-speed imaging,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[10] Y. Li, J. Peng, J. Ye, Y. Zhang, F. Xu, and Z. Xiong, \u201cNlost: Non-line-of-sight imaging with transformer,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 313\u201313 322.   \n[11] M. O\u2019Toole, D. B. Lindell, and G. Wetzstein, \u201cConfocal non-line-of-sight imaging based on the light-cone transform,\u201d Nature, vol. 555, no. 7696, pp. 338\u2013341, 2018.   \n[12] D. B. Lindell, G. Wetzstein, and M. O\u2019Toole, \u201cWave-based non-line-of-sight imaging using fast fk migration,\u201d ACM Transactions on Graphics (ToG), vol. 38, no. 4, pp. 1\u201313, 2019.   \n[13] X. Liu, I. Guill\u00e9n, M. La Manna, J. H. Nam, S. A. Reza, T. Huu Le, A. Jarabo, D. Gutierrez, and A. Velten, \u201cNon-line-of-sight imaging using phasor-field virtual wave optics,\u201d Nature, vol. 572, no. 7771, pp. 620\u2013623, 2019.   \n[14] S. Xin, S. Nousias, K. N. Kutulakos, A. C. Sankaranarayanan, S. G. Narasimhan, and I. Gkioulekas, \u201cA theory of fermat paths for non-line-of-sight shape reconstruction,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 6800\u20136809.   \n[15] S. I. Young, D. B. Lindell, B. Girod, D. Taubman, and G. Wetzstein, \u201cNon-line-of-sight surface reconstruction using the directional light-cone transform,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 1407\u20131416.   \n[16] M. La Manna, F. Kine, E. Breitbach, J. Jackson, T. Sultan, and A. Velten, \u201cError backprojection algorithms for non-line-of-sight imaging,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 41, no. 7, pp. 1615\u20131626, 2018.   \n[17] B. Ahn, A. Dave, A. Veeraraghavan, I. Gkioulekas, and A. C. Sankaranarayanan, \u201cConvolutional approximations to the general non-line-of-sight imaging operator,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 7889\u20137899.   \n[18] X. Liu, J. Wang, Z. Li, Z. Shi, X. Fu, and L. Qiu, \u201cNon-line-of-sight reconstruction with signal\u2013object collaborative regularization,\u201d Light: Science & Applications, vol. 10, no. 1, p. 198, 2021.   \n[19] M. La Manna, J.-H. Nam, S. A. Reza, and A. Velten, \u201cNon-line-of-sight-imaging using dynamic relay surfaces,\u201d Optics express, vol. 28, no. 4, pp. 5331\u20135339, 2020.   \n[20] C. Gu, T. Sultan, K. Masumnia-Bisheh, L. Waller, and A. Velten, \u201cFast non-line-of-sight imaging with non-planar relay surfaces,\u201d in 2023 IEEE International Conference on Computational Photography (ICCP). IEEE, 2023, pp. 1\u201312.   \n[21] C. A. Metzler, D. B. Lindell, and G. Wetzstein, \u201cKeyhole imaging: non-line-of-sight imaging and tracking of moving objects along a single optical path,\u201d IEEE Transactions on Computational Imaging, vol. 7, pp. 1\u201312, 2020.   \n[22] M. Isogawa, D. Chan, Y. Yuan, K. Kitani, and M. O\u2019Toole, \u201cEfficient non-line-of-sight imaging from transient sinograms,\u201d in Computer Vision \u2013 ECCV 2020, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds. Springer International Publishing, 2020, pp. 193\u2013208.   \n[23] J.-T. Ye, X. Huang, Z.-P. Li, and F. Xu, \u201cCompressed sensing for active non-line-of-sight imaging,\u201d Optics Express, vol. 29, no. 2, pp. 1749\u20131763, 2021.   \n[24] W. Yang, C. Zhang, W. Jiang, Z. Zhang, and B. Sun, \u201cNone-line-of-sight imaging enhanced with spatial multiplexing,\u201d Optics Express, vol. 30, no. 4, pp. 5855\u20135867, 2022.   \n[25] X. Liu, J. Wang, L. Xiao, X. Fu, L. Qiu, and Z. Shi, \u201cFew-shot non-line-of-sight imaging with signalsurface collaborative regularization,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 303\u201313 312.   \n[26] S. Shen, Z. Wang, P. Liu, Z. Pan, R. Li, T. Gao, S. Li, and J. Yu, \u201cNon-line-of-sight imaging via neural transient fields,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 7, pp. 2257\u20132268, 2021.   \n[27] J. Wang, X. Liu, L. Xiao, Z. Shi, L. Qiu, and X. Fu, \u201cNon-line-of-sight imaging with signal superresolution network,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 17 420\u201317 429.   \n[28] Y. Li, Y. Zhang, J. Ye, F. Xu, and Z. Xiong, \u201cDeep non-line-of-sight imaging from under-scanning measurements,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[29] J. Schwab, S. Antholzer, and M. Haltmeier, \u201cDeep null space learning for inverse problems: convergence analysis and rates,\u201d Inverse Problems, vol. 35, no. 2, p. 025008, 2019.   \n[30] C. K. S\u00f8nderby, J. Caballero, L. Theis, W. Shi, and F. Husz\u00e1r, \u201cAmortised map inference for image super-resolution,\u201d arXiv preprint arXiv:1610.04490, 2016.   \n[31] M. Mardani, E. Gong, J. Y. Cheng, S. Vasanawala, G. Zaharchuk, M. Alley, N. Thakur, S. Han, W. Dally, J. M. Pauly et al., \u201cDeep generative adversarial networks for compressed sensing automates mri,\u201d arXiv preprint arXiv:1706.00051, 2017.   \n[32] D. Chen and M. E. Davies, \u201cDeep decomposition learning for inverse imaging problems,\u201d in Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVIII 16. Springer, 2020, pp. 510\u2013526.   \n[33] A. Pajot, E. De B\u00e9zenac, and P. Gallinari, \u201cUnsupervised adversarial image reconstruction,\u201d in International conference on learning representations, 2018.   \n[34] J. Liu, Y. Sun, C. Eldeniz, W. Gan, H. An, and U. S. Kamilov, \u201cRare: Image reconstruction using deep priors learned without groundtruth,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 6, pp. 1088\u20131099, 2020.   \n[35] B. Yaman, S. A. H. Hosseini, S. Moeller, J. Ellermann, K. U\u02d8gurbil, and M. Ak\u00e7akaya, \u201cSelf-supervised learning of physics-guided reconstruction neural networks without fully sampled reference data,\u201d Magnetic resonance in medicine, vol. 84, no. 6, pp. 3172\u20133191, 2020.   \n[36] J. Tachella, D. Chen, and M. Davies, \u201cUnsupervised learning from incomplete measurements for inverse problems,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 4983\u20134995, 2022.   \n[37] A. Bora, E. Price, and A. G. Dimakis, \u201cAmbientgan: Generative models from lossy measurements,\u201d in International conference on learning representations, 2018.   \n[38] Y. C. Eldar, \u201cGeneralized sure for exponential families: Applications to regularization,\u201d IEEE Transactions on Signal Processing, vol. 57, no. 2, pp. 471\u2013481, 2008.   \n[39] F. Luisier, T. Blu, and M. Unser, \u201cImage denoising in mixed poisson\u2013gaussian noise,\u201d IEEE Transactions on Image Processing, p. 696\u2013708, Mar 2011. [Online]. Available: http://dx.doi.org/10.1109/tip.2010.2073477   \n[40] Y. Le Montagner, E. D. Angelini, and J.-C. Olivo-Marin, \u201cAn unbiased risk estimator for image denoising in the presence of mixed poisson\u2013gaussian noise,\u201d IEEE Transactions on Image processing, vol. 23, no. 3, pp. 1255\u20131268, 2014.   \n[41] D. Chen, J. Tachella, and M. E. Davies, \u201cRobust equivariant imaging: a fully unsupervised framework for learning to image from noisy and partial measurements,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5647\u20135656.   \n[42] S. Soltanayev and S. Y. Chun, \u201cTraining deep learning based denoisers without ground truth data,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[43] C. M. Stein, \u201cEstimation of the mean of a multivariate normal distribution,\u201d The annals of Statistics, pp. 1135\u20131151, 1981.   \n[44] G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catanzaro, \u201cImage inpainting for irregular holes using partial convolutions,\u201d in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 85\u2013100.   \n[45] D. Ulyanov, A. Vedaldi, and V. Lempitsky, \u201cInstance normalization: The missing ingredient for fast stylization,\u201d arXiv preprint arXiv:1607.08022, 2016.   \n[46] J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker, and D. Rueckert, \u201cAttention gated networks: Learning to leverage salient regions in medical images,\u201d Medical image analysis, vol. 53, pp. 197\u2013207, 2019.   \n[47] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[48] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.   \n[49] S. Ramani, T. Blu, and M. Unser, \u201cMonte-carlo sure: A black-box optimization of regularization parameters for general denoising algorithms,\u201d IEEE Transactions on image processing, vol. 17, no. 9, pp. 1540\u20131554, 2008.   \n[50] C. A. Metzler, A. Mousavi, R. Heckel, and R. G. Baraniuk, \u201cUnsupervised learning with stein\u2019s unbiased risk estimator,\u201d arXiv preprint arXiv:1805.10531, 2018.   \n[51] V. Antun, F. Renna, C. Poon, B. Adcock, and A. C. Hansen, \u201cOn instabilities of deep learning in image reconstruction and the potential costs of ai,\u201d Proceedings of the National Academy of Sciences, vol. 117, no. 48, pp. 30 088\u201330 095, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/c86a8526e0a1d300f6fd3df4499b5f8b1d01f2d98b43c21ae5a677e102455b02.jpg", "img_caption": ["Figure 8: Overview of our P-Unet with 3D partial convolution for denoising irregularly undersampled transients. The spatial resolution of transients features in each layer is depicted below the blocks, and their respective channel counts are presented above the blocks. The resolutions and channels of the sampling matrices are identical to the transients features in the corresponding layer. The numbers on the blue arrows indicate the kernel size of the 3D convolution. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "We begin by presenting the architecture of the network $F_{\\phi}$ . Since the vanilla convolution treats all voxels of the zero-padded IUT as valid, it leads to the distortion of information in undersampled transients during feature propagation. As illustrated in Fig. 8, we present an encoder-decoder network (P-Unet) based on 3D partial convolution [44]. Partial convolution utilizes a sampling matrix to differentiate between valid and invalid voxels, effectively capturing spatial information from the transients. Additionally, we incorporate the instance normalization (IN) layer [45], which is insensitive to input distribution. This layer enhances the network\u2019s generalization capability to diverse zero-padded IUT with varying distributions from different sampling rates. ", "page_idx": 13}, {"type": "text", "text": "Given that the performance of $F_{\\theta}$ in VSRnet mainly depends on acquiring null space information from the training forward operators, we adopt a 3D residual network featuring an attention-gated network [46]. While attention mechanisms might not significantly improve reconstruction quality, they help speed up the network\u2019s training process. ", "page_idx": 13}, {"type": "text", "text": "B Training details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our method is implemented using PyTorch [47], and we employ the Adam optimizer [48] with a weight decay of $1\\dot{0}^{-8}$ . In the first stage, the SURE-based denoiser model $F_{\\phi}$ is trained with a batch size of 4 for 40 epochs. We set the initial learning rate to $1\\times10^{-3}$ and reduce it by a factor of 0.1 at epoch 30. Subsequently, in the second stage, the VSRnet model $F_{\\theta}$ is trained with a batch size of 2 for 20 epochs, utilizing an initial learning rate of $5\\times10^{-4}$ and a reduction by a factor of 0.1 at epoch 10. In each epoch, we randomly select 40 complete simulated transients for each relay surface and extract signals from them to generate irregularly undersampled transients for training. All models were trained on 2 NVIDIA 3090 GPUs, taking nearly 40 hours in total. Regarding the loss function, the hyperparameters $\\varepsilon$ , $\\beta$ and $b$ are set to 0.1, 0.001 and 4, respectively. ", "page_idx": 13}, {"type": "text", "text": "C Details of proposed SURE-based denoiser ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Results of compared methods using our denoiser ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We applied the proposed SURE-based denoiser to the compared methods and selected relatively better results for comparative experiments. As depicted in Fig. 9, the results of LCT, FK, and RSD exhibit reduced diffuse background noise when the denoiser is applied. For learning-based algorithms, our denoiser effectively removes cluster artifacts caused by measurement noise, thereby enhancing the clarity of primary structures. However, in the case of CC-SOCR, which already incorporates strong regularization, the improvement of our denoiser is limited. This is likely due to the algorithm\u2019s sub-problem already including a denoising effect. ", "page_idx": 13}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/9efed1e05b52fbbb476bb68cba5248bd0d4ce222eef33356e46dbcd8947c2d3b.jpg", "img_caption": ["Figure 9: Qualitative results of compared methods with and without the SURE-based denoiser. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "C.2 Derivation of the SURE loss ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Stein\u2019s Unbiased Risk Estimation (SURE) framework has been developed and used in prior signal denoising works [38\u201342, 49]. These works did not consider the dark current of sensor outputs that might be removed in some image signal pipelines. Therefore, we extend the SURE denoising model to NLOS imaging by incorporating more practical noise characteristics of the detector. In this section, we provide the derivation of the SURE loss mentioned in the main text, following the notations in [40, 41]. ", "page_idx": 14}, {"type": "text", "text": "For compact presentation, we simply denote the training set of transients by $\\{\\tilde{u}_{j}\\;\\in\\;\\mathbb{R}^{s t},j\\;=\\;$ $1,2,\\ldots,I\\times G\\}$ instead of the two-subscript version $\\{\\Tilde{u}_{i,g},i=1,2,\\dots,I,g=1,2,\\dot{\\dots},G\\}$ . $I$ is the number of observed hidden scenes, and $G$ is the number of forward operators associated with relay surfaces. $s$ and $t$ are the number of scanning points and the number of time bins in each histogram of the transient, respectively. ", "page_idx": 14}, {"type": "text", "text": "In the supervised learning, given a paired dataset of noisy transients $\\{\\Tilde{u}_{j}\\}$ and its clean version $\\{u_{j}\\}$ , the model $F_{\\phi}$ can be trained by minimizing the mean squared error (MSE) loss function: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\{\\tilde{u},u\\}}\\Big\\{\\sum_{j=1}^{J}\\frac{1}{s t}\\|u_{j}-F_{\\phi}(\\tilde{u}_{j})\\|^{2}\\Big\\}=\\mathbb{E}_{u}\\Big\\{\\sum_{j=1}^{J}\\frac{1}{s t}\\mathbb{E}_{\\tilde{u}|u}\\|u_{j}-F_{\\phi}(\\tilde{u}_{j})\\|^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "However, the clean transient set $\\{u_{j}\\}$ is not available. To achieve unsupervised learning only with noisy transients, our goal is to obtain an unbiased estimator of the mean squared error (MSE) standing on the SURE framework. To this end, we can further decompose the inner expectation in Eq. 10 as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\tilde{u}|u}\\|u_{j}-F_{\\phi}(\\tilde{u}_{j})\\|^{2}=\\mathbb{E}_{\\tilde{u}|u}\\big\\{\\|u_{j}\\|^{2}\\big\\}+\\mathbb{E}_{\\tilde{u}|u}\\big\\{\\|F_{\\phi}(\\tilde{u}_{j})\\|^{2}\\big\\}-2\\mathbb{E}_{\\tilde{u}|u}\\big\\{u_{j}^{\\top}F_{\\phi}(\\tilde{u}_{j})\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The unbiased estimator of the second term in Eq. 11 is $\\Vert{F_{\\phi}}(\\tilde{u}_{j})\\Vert^{2}$ , which can be obtained without the clean transient $u_{j}$ . The first term in Eq. 11 can be rewritten as $\\mathbb{E}_{\\tilde{u}|u}\\{u_{j}^{\\top}\\tilde{u}_{j}\\}$ because $\\mathbb{E}_{\\tilde{u}|u}\\{\\tilde{u}_{j}\\}=u_{j}$ . The first and third terms depend on $u$ . Thus, we need to obtain unbiased estimators for them by considering the characteristics of the noise model . ", "page_idx": 14}, {"type": "text", "text": "In the case of NLOS imaging, the noisy transients can be modeled as $\\tilde{u}\\sim\\mathrm{Poisson}(u+b)$ , where $b$ denotes the dark counts. The unsupervised expressions of $\\mathbb{E}_{\\tilde{u}|u}\\{u_{j}^{\\top}\\tilde{u}_{j}\\}$ and $\\mathbb{E}_{\\tilde{u}|u}\\{u_{j}^{\\top}F_{\\phi}(\\tilde{u}_{j})\\}$ can be obtained using the following lemma. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 (Lemma 1.2 in [40]) Let $v\\in\\mathbb{R}^{s t}$ such that $v\\sim P o i s s o n(u)$ be an independent random variables and let $\\Phi:\\mathbb{R}^{s t}\\rightarrow\\mathbb{R}^{s t}$ be a function such that $\\mathbb{E}_{v|u}\\{|\\Phi_{m}(v)|\\}<+\\infty$ for all $m$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{v\\mid u}\\{u^{\\top}\\Phi(v)\\}=\\mathbb{E}_{v\\mid u}\\{v^{\\top}\\Phi^{[-1]}(v)\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\Phi(v)=F_{\\phi}(\\tilde{u})$ , then $\\mathbb{E}_{\\tilde{u}|u}\\{u_{j}^{\\top}F_{\\phi}(\\tilde{u}_{j})\\}$ can be expressed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\Tilde{u}\\mid u}\\{u_{j}^{\\top}F_{\\phi}(\\Tilde{u}_{j})\\}}\\\\ &{=\\mathbb{E}_{v\\mid u}\\big\\{u_{j}^{\\top}\\Phi(v_{j})\\big\\}}\\\\ &{=\\mathbb{E}_{v\\mid u}\\big\\{(u_{j}+b)^{\\top}\\Phi(v_{j})\\big\\}-\\mathbb{E}_{v\\mid u}\\big\\{b^{\\top}\\Phi(v_{j})\\big\\}}\\\\ &{=\\mathbb{E}_{v\\mid u}\\big\\{v_{j}^{\\top}\\Phi^{[-1]}(v_{j})\\big\\}-\\mathbb{E}_{v\\mid u}\\big\\{b^{\\top}\\Phi(v_{j})\\big\\}}\\\\ &{=\\mathbb{E}_{\\Tilde{u}\\mid u}\\big\\{\\Tilde{u}_{j}^{\\top}F_{\\phi}^{[-1]}(\\Tilde{u}_{j})\\big\\}-\\mathbb{E}_{\\Tilde{u}\\mid u}\\big\\{b^{\\top}F_{\\phi}(\\Tilde{u}_{j})\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, we use the first-order Taylor approximation $F_{\\phi}^{[-1]}(\\tilde{u}_{j})\\approx F_{\\phi}(\\tilde{u}_{j})-\\partial F_{\\phi}(\\tilde{u}_{j})$ to simplify Eq. 13 into: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{u}|u}\\big\\{\\tilde{u}_{j}^{\\top}F_{\\phi}(\\tilde{u}_{j})\\big\\}-\\mathbb{E}_{\\tilde{u}|u}\\big\\{\\tilde{u}_{j}^{\\top}\\partial F_{\\phi}(\\tilde{u}_{j})\\big\\}-\\mathbb{E}_{\\tilde{u}|u}\\big\\{b^{\\top}F_{\\phi}(\\tilde{u}_{j})\\big\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, the unbiased estimator of Eq. 14 is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{u}_{j}^{\\top}F_{\\phi}(\\tilde{u}_{j})-\\tilde{u}_{j}^{\\top}\\partial F_{\\phi}(\\tilde{u}_{j})-b^{\\top}F_{\\phi}(\\tilde{u}_{j}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that it is difficult to obtain an analytic form of $\\partial F_{\\phi}(\\tilde{u}_{j})$ when $F_{\\phi}(\\cdot)$ is a neural network. We adopt the Monte-Carlo approach [49] to obtain an estimate of the divergence of $F_{\\phi}(\\tilde{u})$ with the following approximation: ", "page_idx": 15}, {"type": "equation", "text": "$$\nd i v_{\\tilde{u}}\\{F_{\\phi}(\\tilde{u})\\}\\approx\\frac{e^{\\top}}{\\varepsilon}(F_{\\phi}(\\tilde{u}+\\varepsilon e)-F_{\\phi}(\\tilde{u})),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\varepsilon$ is a small positive number, and $e\\in\\{-1,1\\}^{s t}$ is a binary vector whose entities follow a Bernoulli distribution with equal probability [40]. ", "page_idx": 15}, {"type": "text", "text": "Similarly, let $\\Phi(v)$ be an identity function such that $\\Phi(v)=v$ , and $v\\,=\\,\\tilde{u}$ . Using the first-order Taylor approximation, $\\mathbb{E}_{\\tilde{u}|u}\\{u_{j}^{\\top}\\tilde{u}_{j}\\}$ can be expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\tilde{u}|u}\\big\\{u_{j}^{\\top}\\tilde{u}_{j}\\big\\}}\\\\ &{=\\mathbb{E}_{v|u}\\big\\{v_{j}^{\\top}\\Phi^{[-1]}(v_{j})\\big\\}-\\mathbb{E}_{v|u}\\big\\{b^{\\top}\\Phi(v_{j})\\big\\}}\\\\ &{\\approx\\mathbb{E}_{v|u}\\big\\{v_{j}^{\\top}(v_{j}-\\partial v_{j})\\big\\}-\\mathbb{E}_{v|u}\\big\\{b^{\\top}\\Phi(v_{j})\\big\\}}\\\\ &{=\\mathbb{E}_{\\tilde{u}|u}\\big\\{\\tilde{u}_{j}^{\\top}(\\tilde{u}_{j}-\\partial\\tilde{u}_{j})\\big\\}-\\mathbb{E}_{\\tilde{u}|u}\\big\\{b^{\\top}\\tilde{u}_{j}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We obtain the unbiased estimator of Eq. 17: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{u}_{j}^{\\top}(\\tilde{u}_{j}-{\\mathbf1})-b^{\\top}\\tilde{u}_{j},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where 1 is a vector consisting of $^{s t}$ ones. With the above derivation, the total unbiased estimator of the MSE is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{J}\\frac{1}{s t}\\{\\|\\tilde{u}_{j}\\|^{2}-2\\tilde{u}_{j}^{\\top}F_{\\phi}(\\tilde{u}_{j})+\\|F_{\\phi}(\\tilde{u}_{j})\\|^{2}-\\mathbf{1}^{\\top}\\tilde{u}_{j}-b^{\\top}\\tilde{u}_{j}}\\\\ &{\\displaystyle+2b^{\\top}F_{\\phi}(\\tilde{u}_{j})+\\frac{2}{\\varepsilon}(e_{j}\\odot\\tilde{u}_{j})^{\\top}(F_{\\phi}(\\tilde{u}_{j}+\\varepsilon e_{j})-F_{\\phi}(\\tilde{u}_{j}))\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "After rearrangement, Eq. 19 can be expressed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{J}\\frac{1}{s t}\\{\\|\\tilde{u}_{j}-F_{\\phi}(\\tilde{u}_{j})\\|^{2}-(1+b)^{\\top}\\tilde{u}_{j}}\\\\ &{\\displaystyle+2b^{\\top}F_{\\phi}(\\tilde{u}_{j})+\\frac{2}{\\varepsilon}(e_{j}\\odot\\tilde{u}_{j})^{\\top}(F_{\\phi}(\\tilde{u}_{j}+\\varepsilon e_{j})-F_{\\phi}(\\tilde{u}_{j}))\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we get the SURE loss function as described in the main text: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{SURE}}=\\mathbb{E}_{\\{i,g\\}}\\big\\{\\displaystyle\\frac{1}{s t}\\|\\tilde{u}_{i,g}-F_{\\phi}(\\tilde{u}_{i,g})\\|_{2}^{2}-\\displaystyle\\frac{1}{s t}(\\mathbf{1}+b)^{\\top}\\tilde{u}_{i,g}}\\\\ &{~+\\displaystyle\\frac{2}{s t}b^{\\top}F_{\\phi}(\\tilde{u}_{i,g})+\\displaystyle\\frac{2}{s t\\varepsilon}(e_{i,g}\\odot\\tilde{u}_{i,g})^{\\top}\\big(F_{\\phi}(\\tilde{u}_{i,g}+\\varepsilon e_{i,g})-F_{\\phi}(\\tilde{u}_{i,g})\\big)\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "During the training process, we use zero-padded irregularly undersampled transients as input, but only consider valid values in the calculation of the SURE loss. ", "page_idx": 15}, {"type": "text", "text": "D Details of our NLOS system ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We used a femtosecond fiber laser as a light source with a central wavelength of $1560~\\mathrm{nm}$ and a pulse repetition rate of $82\\,\\mathrm{MHz}$ . The light was split by a $99:1$ fiber coupler. The channel with $1\\%$ of the light was attenuated and sent into a fast photodetector, functioning as the start signal for the time-to-amplitude convertor (TAC). The other channel, with $99\\%$ of the light, was connected to a collimator by a piece of single mode fiber (SMF) and then travelled through a hole in the mirror. The beam was diffusively reflected by the relay surface, and the raster scanning was controlled by the beam-steering mirror. After diffusive reflections by the object and again by the relay surface, the echo light travelled back along the same optical path and was reflected to a second collimator. Finally, the echo light was detected by a fractal superconducting nanowire single-photon detector (SNSPD) coupled with SMF. The output voltage pulses from the fractal SNSPD were amplified by the RF amplifiers and input to the TAC as a stop signal for the coincidence counting. The transients were captured over a $0.{\\dot{8}}\\times0.8\\:m^{2}$ scanning area, with a size of $128\\times128\\times512$ and a bin width of 8 ps. ", "page_idx": 16}, {"type": "text", "text": "E Discussion ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/515e79fba32893e929297e4118a02e7a95cc84465e60b145e023404c2b9df5f8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 10: (a) The top view of the NLOS imaging system. The green oval region on the object indicates the area from which the scanning point can capture information. Light with different optical distances is represented by three colors. (b) The first row shows the maximum intensity projections of the transient along the time dimension, with the corresponding sampling rates indicated at the top. ", "page_idx": 16}, {"type": "text", "text": "In this section, we discuss how different relay surfaces affect reconstruction quality. Generally, a relay surface with a large missing area leads to result in poor reconstruction quality due to information loss. To verify this, we removed signals from the fully-sampled transient according to the relay surfaces and reconstructed them using our method. It is clear that parts of the hidden scene facing large missing areas are nearly impossible to reconstruct, as illustrated in the second, third, and fourth columns of Fig 10 (b). However, even with the same sampling rate, a more uniform distribution of scanning points produces better results (e.g., comparing the fifth column to the second). ", "page_idx": 16}, {"type": "text", "text": "Therefore, in the challenging task of NLOS imaging from IUT, the sampling rate is not the sole determinant of reconstruction quality. When the detector focuses a point on the relay surface, only the parts of the hidden scene that are close to that point can be observed. As shown in Fig 10 (a), the light attenuation term $1/r^{4}$ causes that a histogram $(1\\times1\\times T)$ captured from scanning point $p$ primarily contains information from hidden areas close to that point, as indicated by the green oval area. This results in poor reconstruction when large areas of the relay surface are invalid for scanning. However, determining the acceptable proportion of the invalid area relative to the entire surface for successful reconstruction is theoretically challenging, as it depends on various factors, including the relative depth between the hidden scene and the relay surface, the scene\u2019s reflectivity and normals, detector efficiency, and laser power. We consider this an important area for future work. ", "page_idx": 16}, {"type": "text", "text": "F Additional ablation study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we extend our ablation study further to explore the optimal values and sensitivity of the hyperparameters within the loss functions. Additionally, we examine the effect of relay surfaces used for training. To accommodate variations in testing data, we evaluated our method on different IUT of the \u201cbunny\u201d, sampled according to 15 distinct relay surfaces mentioned in the main text. ", "page_idx": 16}, {"type": "text", "text": "F.1 Ablation study on hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 3: Effect of the hyperparameters $\\varepsilon$ on the denoising performance in terms of PSNR (mean value and variance). ", "page_idx": 17}, {"type": "table", "img_path": "R4IBZrSF5d/tmp/d0069a65a2b9391eb95bde68651836c8ec75be8991400e0558eb4e182bd2628b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Effect of the positive number $\\varepsilon$ In our experiments, the transient is normalized to the range $[0,100]$ . Following the recommendation in [50], where the value of $\\varepsilon$ is suggested to be set around $\\dot{\\mathrm{max}}(u)/1000$ , we set $\\varepsilon$ to 0.1. As shown in Tab. 3, our SURE-based denoiser exhibits optimal performance when $\\varepsilon=0.1$ . ", "page_idx": 17}, {"type": "table", "img_path": "R4IBZrSF5d/tmp/5259b04e12c8a6992b3ffd06a248aa7556806472f4d789c64f6de3757ebeb8c8.jpg", "table_caption": ["Table 4: Effect of the hyperparameters $\\beta$ on the reconstruction performance in terms of PSNR (mean value and variance). "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Effect of the hyperparameter $\\beta$ The hyperparameter $\\beta$ acts as a weight to balance the MC loss and the VS loss. As shown in Tab. 4, our method achieves optimal performance when $\\beta=0.001$ . The effectiveness of our method decreases notably as $\\beta$ decreases. When the VS loss is completely disabled $(\\beta=0)$ ), the model fails to learn beyond the range space, highlighting the significance of virtual scanning in our approach. ", "page_idx": 17}, {"type": "text", "text": "F.2 Ablation study on relay surfaces for training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "It\u2019s widely recognized that the robustness of neural networks depends heavily on the diversity of the training dataset. As discussed in [51], some neural networks struggle to adapt to changes in the sampling rate. They perform best when the sampling rate of input matches that of training dataset. Any deviation, whether a decrease or increase in the testing data\u2019s sampling rate, can lead to a decline in performance. To address this challenge, a common strategy is to augment the diversity of the training data\u2019s sampling rate. ", "page_idx": 17}, {"type": "text", "text": "Table 5: Effect of the intervals of relay surfaces for training on the reconstruction performance in terms of PSNR (mean value and variance). ", "page_idx": 17}, {"type": "table", "img_path": "R4IBZrSF5d/tmp/1e07f4a6074d72569271a0d89e9fa69cbefd8e1ce718cb9dd2660bba9a0d8d56.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Interval In the context of NLOS imaging from irregularly undersampled transients, we can modify the shape of the relay surface to adjust the sampling rate of transients. In theory, light reflecting from the hidden scene spreads across the entire relay surface. However, due to the radiometric fall-off $\\|p-q\\|^{4}$ , only a limited portion of the reflected light, concentrated in a small area of the relay surface, can be effectively captured by the time-resolved detector. As a result, we manipulate the interval of the shutter-like surfaces to modify the local sampling rate of undersampled transients. ", "page_idx": 17}, {"type": "text", "text": "As shown in Tab. 5, the optimal combination of interval values for relay surfaces is [4, 8, 12, 16, 20]. Our method\u2019s performance shows a decline with a reduction in the variety of interval types. This suggests that using undersampled transients with a more diverse range of sampling rates for training can enhance the generalization capability of our method. ", "page_idx": 17}, {"type": "text", "text": "Table 6: Effect of the rotations of relay surfaces for training on the reconstruction performance in terms of PSNR (mean value and variance). ", "page_idx": 18}, {"type": "table", "img_path": "R4IBZrSF5d/tmp/6a10fd9dde0d61d34d6815b5db905c01655f24415ed85dc2d7867a9acccd9fe5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Rotation Similarly, we can modulate the diversity of relay surfaces by varying the number of rotations. As shown in Tab. 6, the optimal number of rotations is 40 when the count of transients using for training is 8,000 in total. When the number of rotations falls below 40, the diversity of relay surfaces becomes inadequate. Conversely, when it exceeds 40, the allocated transients for each relay surfaces becomes too limited. ", "page_idx": 18}, {"type": "text", "text": "G Additional results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We present additional results on publicly available real data [10, 12] with more irregular relay surfaces, as shown in Fig 11 and $\\mathrm{Fig}\\ 12$ . The experiment setup for data processing and the compared methods remains consistent with the description in the main text. ", "page_idx": 18}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/08dfb7f9a51ee17a7f6b1fab3aa14b044dc9dcb8309103f8f053ef7d55db1875.jpg", "img_caption": ["Figure 11: Reconstruction results of publicly available real-world dataset [10] with different relay surfaces. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "R4IBZrSF5d/tmp/d5ae455ffcca0fd6b36b223945933c8826aacad07ad24e072a1f63dcfa0ea455.jpg", "img_caption": ["Figure 12: Reconstruction results of publicly available real-world dataset [12] with different relay surfaces. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We make the main claims in introduction in Sec. 1. The claims matches theoretical results in Sec. 3.2 and experimental results in Sec. 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper discusses the limitations in Sec. 6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide the analysis of the motivation in Sec. 3.2 and the virtual scanning strategy in Sec. 4.2. And we provide the derivation of the proposed SURE-based loss in Sec. C.2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We describe proposed framework in Sec. 4. We present proposed network architecture in Sec. A and provide training details in Sec. B. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The data and code will be released after publication. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We specify all the training and testing details in Sec. 5, Sec. B and Sec. F. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: The computational resources require for error bars are too high. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide information on the computer resources in Sec. 5.3 and Sec. B. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We reviewed the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work on non-line-of-sight imaging does not have any apparent societal impacts. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cited the original paper that provided code package and dataset in references. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: The new assets, including the dataset and code, will be released with documentation after publication. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]