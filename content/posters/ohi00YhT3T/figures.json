[{"figure_path": "ohi00YhT3T/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dual-stream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor p, and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.", "description": "This figure illustrates the overall architecture of the proposed multimodal framework for fMRI-based visual reconstruction and language interaction. It shows three main components: a dual-stream fMRI feature extractor that aligns fMRI features with visual embeddings from VAE and CLIP, a 3D fMRI preprocessor for efficient data handling, and a multimodal LLM that integrates fMRI features for natural language interactions, enabling tasks such as visual reconstruction, question-answering, and complex reasoning. The dual-stream feature extractor enables alignment with CLIP and VAE embeddings efficiently, simplifying integration with LLMs.  The fMRI preprocessor employs a trilinear interpolation and patching strategy to preserve the 3D structural information, and the multimodal LLM facilitates interactions via natural language instructions and generation of text or image responses. ", "section": "Methodology"}, {"figure_path": "ohi00YhT3T/figures/figures_3_2.jpg", "caption": "Figure 2: Description of fMRI data preprocessing. First align the data of different subjects, then patch them, and finally remove activity-irrelevant patches.", "description": "This figure illustrates the preprocessing steps applied to fMRI data before feeding it into the model.  It begins with a set of fMRI scans from multiple subjects (S1, S2...Sk).  First, trilinear interpolation is used to resize the data to a consistent, uniform size (R x X x Y x Z).  Next, the data is divided into small cubic patches. Finally, patches that are considered irrelevant (inactive patches) are filtered out, leaving only the active patches that contain important information for the analysis. This process maintains the spatial structure of the data, crucial for accurate analysis and consistency across subjects.", "section": "3.1 fMRI Feature Preprocessor"}, {"figure_path": "ohi00YhT3T/figures/figures_5_1.jpg", "caption": "Figure 3: Demonstration of the model's capabilities for engaging in multi-round dialogue, complex reasoning, visual reconstruction, and concept location tasks using fMRI data.", "description": "This figure showcases the model's performance across four key tasks using fMRI data: (1) Multi-round Dialogue: The model demonstrates its ability to engage in extended conversations related to the visual stimuli. (2) Complex Reasoning:  The model is tested on its capacity to answer complex questions related to the visual scene requiring inference and reasoning. (3) Visual Reconstruction: The model reconstructs the visual stimuli from the fMRI brain scans, demonstrating its ability to translate brain activity into visual representations. (4) Concept Locating:  The model identifies and locates specific concepts from the visual stimuli within the brain signals. Each task includes a question, answer, and an image for visual reference, demonstrating the different capabilities of the model.", "section": "Experiments"}, {"figure_path": "ohi00YhT3T/figures/figures_7_1.jpg", "caption": "Figure 4: Visual reconstruction results showcasing the comparison between (a) using the average signal from all trials and (b) using the first visual stimulus.", "description": "This figure compares visual reconstruction results from the proposed method and three other methods (MindEye, Brain-Diffuser, Takagi et al., DREAM) across different visual stimuli.  The left-hand side (a) shows results when averaging the fMRI signal across multiple trials for each stimulus. The right-hand side (b) shows results obtained using only the first trial's fMRI response, representing a more challenging and realistic single-trial scenario. The figure demonstrates that the proposed method produces visually accurate and consistent reconstruction results under both conditions, highlighting its effectiveness in real-world scenarios.", "section": "4.3 Visual Reconstruction"}, {"figure_path": "ohi00YhT3T/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation analysis of the hyperparameter \u03b2 on visual reconstruction performance.", "description": "This figure presents an ablation study on the impact of the hyperparameter \u03b2 on visual reconstruction performance. It shows four subfigures: (a) Pixel Correlation (PixCorr) vs. \u03b2, (b) CLIP score vs. \u03b2, (c) AlexNet(5) score vs. \u03b2, and (d) SSIM vs. CLIP score. Each subfigure shows the performance for four different subjects (subj01, subj02, subj05, and subj07) with and without LLMs. The results indicate that an optimal balance between noise and prior information is crucial for visual reconstruction, and that using LLMs improves performance especially for high-level features.", "section": "4.3 Visual Reconstruction"}, {"figure_path": "ohi00YhT3T/figures/figures_8_2.jpg", "caption": "Figure 6: Visualization of the impact of \u03b2 on visual reconstruction.", "description": "This figure shows the impact of the hyperparameter \u03b2 on the visual reconstruction results. The top row displays reconstructions without LLMs, while the bottom row uses LLMs. Each column represents a different value of \u03b2 (0.6, 0.75, 0.9, 0.98, 1.0).  The visual stimuli are shown to the right. The figure demonstrates how changes in \u03b2 affect image quality and the ability of the model to reconstruct fine details of the images.", "section": "4.3 Visual Reconstruction"}, {"figure_path": "ohi00YhT3T/figures/figures_8_3.jpg", "caption": "Figure 7: Differential heatmaps of neural activity representing various semantic information for the same visual stimulus.", "description": "This figure visualizes the brain regions activated by specific concepts derived from visual stimuli.  It uses heatmaps to show the spatial distribution of neural responses corresponding to different semantic concepts within the brain. Each row represents a different image from the dataset, and each column shows the activation heatmap for a specific caption related to that image. The heatmaps show that different concepts activate different regions of the brain, suggesting that our brain processes visual information in a distributed manner, assigning different brain areas to different concepts within a scene.", "section": "Concept Localization"}, {"figure_path": "ohi00YhT3T/figures/figures_13_1.jpg", "caption": "Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dual-stream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor p, and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.", "description": "This figure shows the overall architecture of the proposed multimodal framework. It consists of three main components: a dual-stream fMRI feature extractor that aligns fMRI data with visual embeddings from VAE and CLIP, a 3D fMRI preprocessor for efficient data handling, and a multimodal LLM that integrates with the fMRI features for interactive communication and visual reconstruction. The framework allows for natural language interaction, enabling tasks such as brain captioning, complex reasoning, and visual reconstruction.", "section": "3 Methodology"}, {"figure_path": "ohi00YhT3T/figures/figures_14_1.jpg", "caption": "Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dual-stream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor p, and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.", "description": "This figure shows the overall architecture of the proposed multimodal framework that combines fMRI feature extraction with Large Language Models (LLMs) for visual reconstruction and interactive communication.  It highlights three key components: 1) A dual-stream pathway that aligns fMRI features with VAE and CLIP embeddings for efficient feature matching; 2) A 3D fMRI preprocessor and a feature extractor that preserves the spatial structure of fMRI data; and 3) A multimodal LLM that integrates with the extracted fMRI features to process natural language instructions and generate responses (text or images). This integrated approach enhances the capabilities of non-invasive brain decoding for a wide range of tasks.", "section": "3 Methodology"}, {"figure_path": "ohi00YhT3T/figures/figures_15_1.jpg", "caption": "Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dual-stream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor p, and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.", "description": "This figure illustrates the proposed multimodal framework which combines fMRI feature extraction with large language models (LLMs) for interactive communication and visual reconstruction.  It shows three key components: (a) a dual-stream pathway aligning fMRI features with visual embeddings from a Variational Autoencoder (VAE) and CLIP; (b) a 3D fMRI preprocessor and feature extractor; and (c) the integration of a multimodal LLM with the fMRI data for processing natural language instructions and generating either text-based responses or visual reconstructions. The framework emphasizes the efficient alignment of fMRI data with visual and linguistic information through a unified architecture.", "section": "3 Methodology"}, {"figure_path": "ohi00YhT3T/figures/figures_15_2.jpg", "caption": "Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dual-stream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor p, and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.", "description": "This figure illustrates the overall architecture of the proposed multimodal framework.  It shows three main components:  (a) A dual-stream feature alignment module using Variational Autoencoders (VAEs) and CLIP embeddings to align fMRI features with visual information. (b) A 3D fMRI preprocessor and feature extractor that preserves spatial information from the fMRI data using a Vision Transformer 3D model. (c) An LLM that integrates with the fMRI features to perform language-based tasks, such as generating responses to questions, visual reconstruction, and concept localization.  The framework utilizes a combined approach of fMRI data and LLMs for advanced brain-computer interaction and cognitive modeling.", "section": "3 Methodology"}, {"figure_path": "ohi00YhT3T/figures/figures_17_1.jpg", "caption": "Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dual-stream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor p, and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.", "description": "This figure shows a block diagram of the proposed multimodal framework for brain decoding. It combines fMRI feature extraction with LLMs to enable interactive communication and visual reconstruction. The framework has three main components: 1) a dual-stream pathway for aligning fMRI features with visual embeddings from VAE and CLIP; 2) a 3D fMRI preprocessor and feature extractor that preserves spatial information in fMRI data; 3) a multimodal LLM that integrates with fMRI features to process natural language instructions and generate responses (text or image).", "section": "3 Methodology"}, {"figure_path": "ohi00YhT3T/figures/figures_18_1.jpg", "caption": "Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dual-stream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor p, and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.", "description": "This figure illustrates the overall architecture of the proposed model, showing the integration of three main components: a dual-stream fMRI feature extractor that aligns fMRI data with visual embeddings from VAE and CLIP; a 3D fMRI preprocessor for efficient data handling; and a multimodal LLM for interactive communication and visual reconstruction. The dual-stream feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models. The preprocessor maintains the 3D structural integrity of the fMRI data. The LLM integrates the extracted features, enabling natural language interactions and various tasks, such as visual reconstruction and concept localization.", "section": "3 Methodology"}, {"figure_path": "ohi00YhT3T/figures/figures_19_1.jpg", "caption": "Figure 12: Heatmaps illustrating brain regions activated by specific captions derived from visual stimuli, demonstrating the spatial distribution of neural responses.", "description": "This figure visualizes the results of concept localization within brain signals.  Heatmaps are shown for various visual stimuli, each overlaid with a heatmap representing the brain activation patterns corresponding to specific captions describing those stimuli. The color intensity in the heatmaps indicates the strength of brain activation, allowing for the visualization of which brain regions are most strongly associated with specific concepts within the images. The figure demonstrates the spatial distribution of neural responses related to different visual semantic concepts. ", "section": "D.3 Concept Localization"}]