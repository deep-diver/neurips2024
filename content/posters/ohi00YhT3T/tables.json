[{"figure_path": "ohi00YhT3T/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative analysis of brain captioning, detailed descriptions, and complex reasoning tasks. Some results are derived from UMBRAE [24]. Results are compared to those from other studies, with best, second, and third highlighted. Underline indicates the best result under identical conditions, while * denotes results obtained using BLIP2-generated captions as ground truth.", "description": "This table presents a quantitative comparison of the proposed method's performance on three tasks: brain captioning, detailed description generation, and complex reasoning, using various evaluation metrics (BLEU, METEOR, ROUGE, CIDEr, SPICE, CLIP-S).  It compares the performance against several state-of-the-art methods.  The results are broken down to show the impact of using the Vision Transformer 3D (ViT3D) component of the model and show results using BLIP2-generated captions as a ground truth for more accurate comparisons.", "section": "4 Experiments"}, {"figure_path": "ohi00YhT3T/tables/tables_7_1.jpg", "caption": "Table 2: Quantitative evaluation on visual reconstruction. Performance metrics are reported across different levels of features, with the best, second and third scores highlighted. The underline indicates the best result under the same conditions. Our method achieves state-of-the-art results using a single model trained across subjects (# Models = 1).", "description": "This table presents a quantitative comparison of the proposed visual reconstruction method against several other state-of-the-art methods.  It evaluates performance across various low-level (Pixelwise Correlation, SSIM, AlexNet features) and high-level (Inception, CLIP, EfficientNet-B, SwAV) metrics.  Results are shown for both multi-trial and single-trial conditions, highlighting the method's ability to generalize and achieve high performance even with limited data.", "section": "4.3 Visual Reconstruction"}, {"figure_path": "ohi00YhT3T/tables/tables_16_1.jpg", "caption": "Table 7: Performance comparison on visual reconstruction tasks when using different LLMs and different instructions. The underline indicates the best result under the same conditions.", "description": "This table presents a quantitative analysis of visual reconstruction performance using different large language models (LLMs) and instruction types.  It compares the performance of a model without LLMs against models using Vicuna-13B and Llama-3-8B, with both \"brief descriptions\" and \"instruction reconstruction\" instructions.  The metrics used assess both low-level (pixel-level fidelity) and high-level (semantic accuracy) aspects of the image reconstruction, allowing for a comprehensive evaluation of the models' effectiveness.", "section": "4.3 Visual Reconstruction"}]