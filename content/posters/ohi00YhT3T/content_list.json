[{"type": "text", "text": "Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guobin Shen1,2,4, Dongcheng Zhao1,2, Xiang $\\mathbf{H}\\mathbf{e}^{1,5}$ , Linghao Feng1,3, Yiting Dong1,2,4, Jihang Wang1,5, Qian Zhang 1,2,5 and Yi Zeng1,2,3,4,5\u2020 ", "page_idx": 0}, {"type": "text", "text": "1 Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences 2 Center for Long-term Artificial Intelligence ", "page_idx": 0}, {"type": "text", "text": "3 Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, CAS 4 School of Future Technology, University of Chinese Academy of Sciences 5 School of Artificial Intelligence, University of Chinese Academy of Sciences ", "page_idx": 0}, {"type": "text", "text": "{shenguobin2021, zhaodongcheng2016, hexiang2021, fenglinghao2022, dongyiting2020, wangjihang2021, q.zhang, yi.zeng}@ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Decoding non-invasive brain recordings is pivotal for advancing our understanding of human cognition but faces challenges due to individual differences and complex neural signal representations. Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks. Our framework integrates 3D brain structures with visual semantics using a Vision Transformer $3D$ . This unified feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models and allowing extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with diverse fMRI-image-related textual data to support multimodal large model development. Integrating with LLMs enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes. These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The decoding of non-invasive brain recordings, such as those obtained from fMRI [1], is a cornerstone of cognitive neuroscience [2; 3; 4]. This process offers unparalleled insights into the neural underpinnings of human cognition, contributing not only to fundamental scientific knowledge but also to advancements in clinical and technological applications. Despite its potential, the field faces significant challenges primarily due to the high variability of brain activity across individuals [5] and the complexity inherent in the neural representations of cognitive processes [6]. ", "page_idx": 0}, {"type": "text", "text": "Brain decoding techniques have traditionally relied on customized, subject-specific models [7; 6; 8]. These models necessitate intricate and costly experimental setups, depending on multiple trials to achieve reliable results. Such approaches, while helpful, are inherently limited in scalability and flexibility, hindering broader application and generalization across different populations and conditions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Visual reconstruction [9] aims to recreate perceived visual stimuli from brain signals and is considered one of the benchmarks of brain decoding. However, this approach often struggles to accurately reproduce the visual experiences of individuals, generally lacking semantic precision and interpretability [8]. This inability to effectively decode and reconstruct signals restricts our understanding of how sensory information is processed. Recognizing these limitations, our study introduces language modalities as a critical enhancement designed to assess decoding performance more effectively and enrich brain-computer interfaces\u2019 interaction capabilities. ", "page_idx": 1}, {"type": "text", "text": "Addressing these multifaceted challenges, our research introduces the Vision Transformer $3D$ (ViT3D) [10] specifically tailored to the domain of visual reconstruction. Unlike traditional approaches that often reduce complex brain regions to one-dimensional vectors [11; 6; 12; 13], losing critical spatial structure information, our implementation of ViT3D preserves the three-dimensional structural integrity of the brain data. This adaptation enables an unprecedented enhancement in the extraction of visual semantic information, ensuring a deeper fidelity and richness in the decoded visual representations. ", "page_idx": 1}, {"type": "text", "text": "Our fMRI feature extractor includes a unified network backbone and two alignment heads for feature matching. This setup enables efficient, high-quality visual reconstructions across subjects from one experimental trial. By simply aligning the extractor\u2019s output with CLIP embeddings [14] and features of Variational Autoencoder (VAE) [15], our method eliminates the need for multiple, subjectspecific models, substantially simplifying the decoding process. This straightforward and effective configuration reduces the resources required for brain decoding and showcases the potential for easy integration with Large Language Models (LLMs), enhancing its usability across various applications. ", "page_idx": 1}, {"type": "text", "text": "Moreover, our research delves into the integration of brain recordings with visual and linguistic data within a comprehensive multimodal framework using LLMs. This integration significantly improves visual reconstruction performance and introduces the groundbreaking capability for direct interaction through natural language. Our model facilitates diverse communication with brain data using natural language and precisely localizes linguistic concepts within brain signals. These advancements help deepen our understanding of the interactions between language, perception, and neural activity. Additionally, to bolster the development of these multimodal models, we have augmented the brainrecording visual dataset with natural language enhancements. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Our fMRI feature extractor, based on Vision Transformer $3D$ , aligns fMRI features with visual embeddings at multiple levels, integrating 3D brain structures with visual semantics. This eliminates the need for subject-specific models and enables efficient data extraction from single trials, significantly reducing training costs and enhancing practical usability in real-world scenarios.   \n\u2022 We expanded the language dimension of our fMRI-visual dataset to build a multimodal large model capable of decoding fMRI data. This enhancement boosts brain decoding performance and broadens the application scope to include tasks like visual reconstruction, question-answering, and complex reasoning while also allowing precise localization and manipulation of language-based concepts within brain signals.   \n\u2022 Experimental results on the Natural Scenes Dataset (NSD) [16] for visual reconstruction and language interaction tasks demonstrate that our method surpasses existing models, effectively achieving concept localization and elimination. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Non-invasive techniques such as functional magnetic resonance imaging (fMRI) are pivotal in providing direct insights into neural activities, significantly deepening our understanding of complex cognitive processes from neural network structures [17] to advanced image and language processing tasks [18; 19]. This section reviews key developments in fMRI-based brain decoding, particularly emphasizing the shift from simple subject-specific analyses to more complex, multimodal interpretations. ", "page_idx": 1}, {"type": "text", "text": "Visual Reconstruction from Brain Activity Visual reconstruction from brain activity involves translating brain recordings into the visual stimuli perceived by subjects. Early methods, like those developed by Horikawa et al. [18], relied on sparse linear regression to predict features extracted by convolutional neural networks from fMRI data. Recent advancements in generative artificial intelligence, particularly diffusion models [20], have propelled efforts to reconstruct visual stimuli directly from fMRI. For instance, Lin et al. [21] aligned fMRI data with image features and corresponding CLIP embeddings to facilitate image generation using fine-tuned StyleGAN [22]. Similarly, Takagi et al. [9] improved the quality of visual reconstructions by aligning fMRI with CLIP text embeddings and the latent spaces of diffusion models. Xia et al. [11] aligned fMRI data from dimensions of image CLIP features, depth, and color using T2I Adapters [23] for fine-grained conditional control. Despite these advancements, the complexity of such methods, involving multiple independent modules, complicates their integration with technologies like LLMs and restricts their generalizability across different subjects. We observed that some contemporary works also attempt cross-subject alignment; however, these methods either require subject-specific parameters [24] or face performance issues compared to subject-specific models [25]. ", "page_idx": 2}, {"type": "text", "text": "fMRI Data Processing Efficiently processing fMRI data to extract visually relevant activities typically involves simplifying the data into one-dimensional vectors and selecting voxels most responsive to visual stimuli. Traditional methods utilize simple linear regression or fully connected networks to predict visual stimulus features [18; 21]. However, these methods often lose essential spatial structural information, which is critical given the individual differences in brain anatomy. To address these challenges, innovations such as Vision Transformer 3D (ViT3D) have been developed for managing data with intricate spatial structures [26; 27]. ViT3D segments 3D data into patches, preserving local spatial information within each patch and maintaining overall structural integrity through self-attention mechanism [28], thereby enhancing the performance of brain activity extraction [29]. ", "page_idx": 2}, {"type": "text", "text": "Multimodal Integration with Brain Signals The utilization of language as a medium for representation allows for the expression of complex concepts with precision and abstraction. The advent of LLMs has showcased their potential to act as bridges across different modalities, enhancing interactions with visual and audio data through natural language [30; 31]. For example, approaches like those by D\u00e9fossez et al. [32], which align brain recordings with spoken language to decode speech non-invasively, have demonstrated the effectiveness of combining brain recordings with LLMs. However, these approaches are often limited by the specificity of the fMRI feature extractors used, which can restrict the scalability and the size of the models employed. By integrating our specially designed cross-subject fMRI feature extractor, we enhance visual reconstruction quality and enable complex reasoning and direct interaction with model outputs using natural language, achieving precise localization of open natural language concepts within the brain. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our approach is designed to tackle the key challenges encountered in the visual reconstruction of brain activity and the integration of LLMs with multimodal data. Traditional brain decoding methods, especially those involving fMRI, often struggle with the complexity of accurately reconstructing visual stimuli and generalizing these models across different subjects. Furthermore, while LLMs hold significant potential for enhancing interactions across various cognitive models, their integration with neuroimaging data has been hindered by the need for non-scalable or efficient customized, subject-specific models. ", "page_idx": 2}, {"type": "text", "text": "In the following sections, we detail our methodology\u2019s components, as seen in Fig. 1. We describe the architecture of our feature preprocessor that maintains the spatial structure of fMRI data, our unified fMRI feature extractor, and the integration strategies for the network with LLMs. We also elaborate on the multimodal interaction techniques that enable direct and meaningful communication between the computational model and the neural representations and the implementation details for visual reconstruction. ", "page_idx": 2}, {"type": "text", "text": "3.1 fMRI Feature Preprocessor ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "fMRI quantifies changes in the blood-oxygen-level-dependent (BOLD) signals to characterize neural activity. The BOLD signal for a given subject can be represented as a 3D matrix $b_{\\mathrm{origin}}\\in\\mathbb{R}^{X_{s}\\times Y_{s}\\times Z_{s}}$ , where $s$ indexes the subject, accounting for inter-individual variability. Traditional preprocessing methods typically involve masking voxels sensitive to the specific task, followed by flattening the remaining data into a 1D vector. For subject s, the processed fMRI signal can thus be represented as $b_{s}\\in\\mathbb{R}^{1\\times N_{s}}$ , where $N_{s}$ denotes the number of voxels selected after masking. ", "page_idx": 2}, {"type": "image", "img_path": "ohi00YhT3T/tmp/cd33a2b495bb841867a1daceb9c7a10f38f3ea757887b287eaf0873ff235eea7.jpg", "img_caption": ["Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: (a) a dualstream pathway for feature alignment with VAE and CLIP embeddings. (b) A 3D fMRI preprocessor $p$ , and an fMRI feature extractor. (c) A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "However, this approach results in a loss of spatial structural information, complicating alignment across different subjects. We propose a feature extraction method that preserves spatial structure, as shown in Fig. 2. Starting with the original BOLD signal $b_{\\mathrm{origin}}$ , we first use trilinear interpolation to resize the data to a uniform dimension, ensuring maximal spatial consistency across subjects\u2019 brains while not introducing subject-specific parameters. After resizing, the normalized data un", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "ohi00YhT3T/tmp/1ab985e2a9b1bcca80879af6c1dcce95a48654fc8e5d3b817714d97fbdda7641.jpg", "img_caption": ["Figure 2: Description of fMRI data preprocessing. First align the data of different subjects, then patch them, and finally remove activity-irrelevant patches. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "dergoes a patching process where it is divided into smaller cubic segments, each of dimension $C=r^{3}$ . This step retains the local spatial features within each segment, preserving the 3D structure crucial for accurate analysis. Finally, patches containing non-task-relevant information are flitered out to reduce computational load. This results in patched data with dimensions $\\mathbb{R}^{N\\times C}$ , where $N$ is the number of patches retained that contain meaningful information. The entire preprocessing operation can be summarized as a mapping: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{p:b_{\\mathrm{origin}}^{s}\\mapsto b\\ |\\ b_{\\mathrm{origin}}^{s}\\in\\mathbb{R}^{X_{s}\\times Y_{s}\\times Z_{s}},b\\in\\mathbb{R}^{N\\times C}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Eq. 1, $p$ is the preprocessing function applies resizing, patching, and masking to transform the original fMRI data into a structured format. This function provides a uniform representation of the BOLD signals across different subjects, ensuring that the spatial structure is preserved and capable of integration with Transformer architectures. ", "page_idx": 3}, {"type": "text", "text": "3.2 Dual-Stream fMRI Feature Extractor ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Visual reconstruction tasks typically begin by mapping the processed BOLD signal, $b$ , to various estimated visual feature spaces, represented as $\\{\\hat{z_{1}},\\hat{z_{2}},\\cdot\\cdot\\cdot\\}$ , where $z$ indicates different levels of visual features and $\\hat{z}$ denotes the visual features estimated from the BOLD signals. Subsequently, these features are used to reconstruct the image, represented as $\\hat{x}$ , from the visual features. To enhance the quality of the reconstructed images, complex feature extractor designs are required, which increases the preprocessing and computational overhead. However, thanks to the design of our fMRI feature extractor, we can achieve efficient visual reconstruction using a single network backbone, as shown in Fig. 1(b). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Specifically, the patched features obtained from Eq. 1 are directly processed through a Transformer Encoder $\\mathcal{E}_{b}$ to extract features, obtaining the hidden states from the last layer, $h^{N_{b}^{-}}=\\mathcal{E}_{b}(b)$ , where $N_{b}$ represents the number of layers of the encoder. These outputs are then aligned with the visual stimulus\u2019s CLIP embeddings $z_{c}=\\mathcal{E}_{c}(x)$ and VAE features $z_{v}=\\mathcal{E}_{v}(x)$ , where $x$ represents the visual stimulus. The loss function used to train the fMRI feature extractor can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{align}}=\\mathbb{E}_{(b,x)\\sim P(B,X)}\\left[\\left|\\left|f_{w_{c}}(h_{0}^{N_{b}})-\\mathcal{E}_{c}(x)\\right|\\right|_{2}^{2}+\\alpha\\left|\\left|f_{w_{v}}(h_{0}^{N_{b}})-\\mathcal{E}_{v}(x)\\right|\\right|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Eq. 2, the expectation $(b,x)\\sim P(B,X)$ averages the alignment loss across samples from the $\\mathbf{B}$ (fMRI signals) and $\\mathbf{X}$ (corresponding visual stimuli). Here, $h_{0}^{N_{b}}$ represents the output from the first token of the last hidden state layer of the encoder $\\mathcal{E}_{b}$ . The functions $f_{w_{c}}$ and $f_{w_{v}}$ are two-layer perceptrons designed to align the extracted fMRI features with these embeddings. The hyperparameter $\\alpha$ balances the losses between the alignments of CLIP and VAE features. Through this dual-stream configuration, we create a backbone network that incorporates different levels of visual features. Using only a simple MSE loss function, we achieve high-quality visual reconstruction. ", "page_idx": 4}, {"type": "text", "text": "3.3 Multimodal Interaction with fMRI ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our feature extractor architecture, equipped with a single backbone network, is adept at encapsulating various feature levels, making it highly suitable for integration with LLMs. Inspired by advancements such as those in LLaVA [33], we utilize the penultimate hidden states of our network, $h^{N_{b}-1}$ , as multimodal tokens of fMRI data. A two-layer perceptron $f_{t}$ projects this state to the same dimension as the text embeddings, resulting in the fMRI embeddings $\\bar{t}=f_{t}(h^{N_{b}-1})$ . Considering a sequence of question-answer pairs related to the fMRI data $[q_{0},a_{0},q_{1},a_{1},\\dots,q_{L},a_{L}]$ , the training objective is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}{p_{\\theta}(\\mathbf{A}|\\mathbf{Q},t)}=\\operatorname*{max}\\prod_{j=0}^{L}p_{\\theta}\\big(a_{j}|q_{j},a_{j-1},\\dots,q_{0},t\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Eq. 3 describes the probability of generating a sequence of answers $\\mathbf{A}$ given a sequence of questions $\\mathbf{Q}$ and the derived fMRI embeddings $t$ . Each answer $a_{j}$ is conditionally dependent on all preceding questions and answers, as well as the context embeddings derived from fMRI data, where $\\theta$ represents the trainable parameters of LLMs. ", "page_idx": 4}, {"type": "text", "text": "To effectively couple fMRI data with language models, annotated data is essential. Although the NSD [16] uses labeled visual stimuli from the COCO dataset [34], semantic mismatches often occur due to modifications such as image cropping performed when displaying images to subjects (Appendix A.1). Moreover, the original captions in NSD are not detailed enough to capture nuanced semantic information. Recognizing the importance of comprehensive linguistic annotations, we have constructed a diverse instructional dataset that includes various textual data: brief descriptions, detailed descriptions, continuous dialogues, complex reasoning tasks, instruction reconstruction, and concept localization. ", "page_idx": 4}, {"type": "text", "text": "3.4 Interaction and Reconstruction via LLMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our fine-tuned model can understand information embedded within fMRI data and adhere to human instructions. Interactions and explanations of visual stimuli content occur through natural language. A typical dialogue format is structured as follows: <human>:[image] [instruction] <bot>:[answer]. Here, [instruction] denotes a natural language instruction, which during inference is tokenized and embedded, while [image] acts as a placeholder, replaced by the fMRI data embedding $t$ . The model then responds based on the directive and the embedded fMRI data. [answer] represents the response generated by the LLMs. ", "page_idx": 4}, {"type": "text", "text": "After instruction-based fine-tuning, the model communicates directly via natural language and supports visual reconstruction and location identification of concepts expressed in natural language, as shown in Fig. 3. These are facilitated respectively through Stable UnCLIP [20] for visual reconstruction and GradCAM [35] for concept localization. ", "page_idx": 4}, {"type": "image", "img_path": "ohi00YhT3T/tmp/3cb4facf500539ffa46e37863f19d90ab2fb1ac62debdcf17fd3671c1dbe3471.jpg", "img_caption": ["Figure 3: Demonstration of the model\u2019s capabilities for engaging in multi-round dialogue, complex reasoning, visual reconstruction, and concept location tasks using fMRI data. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "For visual reconstruction, the LLM initially generates a reconstruction prompt $a_{r}$ , which, combined with the latent representations $\\hat{z}_{v}$ and $\\hat{z}_{c}$ from the fMRI feature extractor, results in the generation of an image. This process can be formalized as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{x}=\\mathcal{D}((1-\\beta)\\hat{z}_{v}+\\beta\\sigma\\mid\\hat{z}_{c},a_{r}),\\quad\\sigma\\in\\mathcal{N}(0,1).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Eq. 4, $\\mathcal{D}$ represents the frozen UnCLIP, used for visual reconstruction, where $\\hat{z}_{c}$ and $a_{r}$ serve as conditional information during the denoising process, and $\\hat{z}_{v}$ acts as the initial latent representation of the image. The hyperparameter $\\beta$ is used to introduce noise into the latent space prior, balancing lowlevel features brought by the prior and high-level information controlled by the diffusion conditions during the denoising process. For concept localization in natural language, LLMs activate the feature extractor using keywords from the instructions, enabling precise location identification of the discussed concepts. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset and Preprocessing: We utilized the Natural Scenes Dataset (NSD) [16], containing highresolution 7Tesla fMRI scans and corresponding visual stimuli from COCO [34]. The dataset involved eight subjects, but analyses focused on the four (sub $\\dot{7}\\,0\\,1$ , sub $\\Dot{\\b{\\b{\\b{\\b}}}}\\,0\\,2$ , sub $\\dot{7}\\,0\\,5$ and $\\operatorname{sub}\\dot{]}07;$ ) who completed all sessions. Modifications like cropping necessitated re-annotation of images using BLIP2 [36] for captions and DETR [37] for bounding boxes to maintain consistency. fMRI data was standardized to dimensions $83\\times104\\times81$ using trilinear interpolation and segmented into $14\\times14\\times14$ patches. ", "page_idx": 5}, {"type": "text", "text": "Architecture and Training: Our architecture integrates CLIP ViT-L/14 [14] and an AutoencoderKL [15] for image feature extraction, aligned with fMRI data processed through a 16-layer Transformer Encoder [28]. This setup employed two perceptrons $(f_{w_{c}},f_{w_{v}})$ to align features with CLIP and VAE, respectively. Training involved a multi-stage approach where the LLM was initially frozen, followed by a fine-tuning stage for both the LLM and the Transformer Encoder. For visual reconstructions, the model utilized UnCLIP-2 [20] with $\\beta$ set to 0.93, and concept localization was achieved using GradCAM [35]. For more details on the dataset and experimental setup, please refer to Appendices A, B, and C. Additional experimental results can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "4.2 Captioning and Question Answering ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tab. 1 shows the performance of our method on multimodal language tasks. With the introduction of LLMs, we have expanded the task forms to include brain captions, detailed descriptions, and complex reasoning, as illustrated in Fig. 3. Our approach has demonstrated superior performance on the majority of metrics for the brain captioning task. Notably, our model can generalize across subjects without the need to train separate models for each subject or introduce subject-specific parameters. ", "page_idx": 5}, {"type": "text", "text": "Given the semantic mismatch between captions and images in the original NSD (Section 3.3), we reran the experiment using BLIP2 [36]-generated captions as ground truth. The results, shown in Tab. 1, show significant improvements when evaluated against BLIP2-generated captions, confirming the effectiveness of our model in the brain captioning task and the reasonableness of the task setting. ", "page_idx": 5}, {"type": "text", "text": "Beyond brain captioning, we have incorporated tasks for detailed description and complex reasoning. Our model also achieved the best performance on these two tasks, suggesting that it can generate not only simple captions but also detailed descriptions and perform complex reasoning. The model\u2019s performance increases on complex reasoning tasks, possibly due to the richer semantic information in the questions, which our model captures more effectively. An ablation study was also conducted, revealing a noticeable performance drop in multimodal language tasks when the structural-preserving features of fMRI data were not extracted using ViT3D. Instead, the fMRI data were flattened and patched, similar to methods used in other literature, while maintaining the same fMRI feature extractor structure. This underlines the effectiveness of ViT3D and the capability of our model in multimodal tasks. ", "page_idx": 5}, {"type": "table", "img_path": "ohi00YhT3T/tmp/86aefe556d6ca4a6abea5d7b77b9747c7c3fa01cec9cec9f6c71a3b29cff8daa.jpg", "table_caption": ["Table 1: Quantitative analysis of brain captioning, detailed descriptions, and complex reasoning tasks. Some results are derived from UMBRAE [24]. Results are compared to those from other studies, with best , second , and third highlighted. Underline indicates the best result under identical conditions, while \u2217denotes results obtained using BLIP2-generated captions as ground truth. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.3 Visual Reconstruction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While our primary objective extends beyond mere visual decoding from fMRI data, visual reconstruction offers a tangible demonstration of a model\u2019s comprehension of fMRI data. Therefore, we conducted visual reconstruction experiments and compared our results with those from other studies. The quantitative evaluation highlights our method\u2019s proficiency. ", "page_idx": 6}, {"type": "text", "text": "Tab. 2 showcases that our model competes with or surpasses traditional subject-specific frameworks on several metrics. Notably, it excels in high-level feature matching, demonstrating the model\u2019s ability to effectively leverage LLMs for interpreting complex visual data. The robust performance across various visual stimuli confirms our model\u2019s comprehensive understanding of fMRI data. Experiments without key components like LLM and VAE features highlight the significance of each element in our method, which is crucial for achieving state-of-the-art results. Moreover, we have conducted single-trial experiments, opting to use only the first visual stimulus, similar to the approach of MindEye [6], rather than averaging signals from three identical stimuli, which typically escalates data collection costs. Even under these more stringent conditions, our model shows only a slight decrease in performance, enhancing its feasibility for practical applications. Visual reconstruction examples are provided in Fig. 4, illustrating the effectiveness of our approach. ", "page_idx": 6}, {"type": "text", "text": "The balance between noise introduction and feature preservation in fMRI data visual reconstruction is governed by the hyperparameter $\\beta$ . Fig. 5 presents a detailed ablation study on how different $\\beta$ values impact various metrics. Fig. 5(a) shows that Pixel Correlation (PixCorr) peaks at intermediate $\\beta$ values, indicating the optimal balance between injected noise and retained prior. The integration of the LLMs does not significantly influence low-level feature capture. In Fig. 5(b), increasing $\\beta$ enhances CLIP accuracy, with LLM integration having a substantial effect on capturing high-level features. Fig. 5(c) indicates the features of the 5 th layer of AlexNet, similar to CLIP features, effectively represent the similarity between reconstructed images and visual stimuli, capturing high-level features accurately. Additionally, Fig. 5(d) illustrates that both the Structural Similarity Index (SSIM) and ", "page_idx": 6}, {"type": "table", "img_path": "ohi00YhT3T/tmp/e08bb2772472bc155e44424736e03f3bd2fec4e8fb7be67c12f5e98cfcdd248c.jpg", "table_caption": ["Table 2: Quantitative evaluation on visual reconstruction. Performance metrics are reported across different levels of features, with the best , second and third scores highlighted. The underline indicates the best result under the same conditions. Our method achieves state-of-the-art results using a single model trained across subjects $(\\#\\operatorname{Models}=1)$ ). "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ohi00YhT3T/tmp/146e2db7ba6fa7ea0cf365dac6dbc00b8d1841e93d3cc42b02a23f41ab7f69df.jpg", "img_caption": ["Figure 4: Visual reconstruction results showcasing the comparison between (a) using the average signal from all trials and (b) using the first visual stimulus. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "CLIP scores benefti from optimally chosen $\\beta$ values, with LLM integration enhancing overall image quality and semantic accuracy. Appropriately adjusting $\\beta$ helps balance the representation of different feature levels in the reconstructed images. Fig. 6 provides examples of visual reconstructions at various $\\beta$ values, demonstrating the model\u2019s enhanced capabilities. ", "page_idx": 7}, {"type": "text", "text": "4.4 Concept Localization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further our understanding of semantic concept localization within brain signals, we capitalized on the alignment between our fMRI encoder and CLIP features, which were developed during the training phase. Building on this, we devised a method to localize concepts within brain signals. Specifically, we first fine-tuned Language Models (LLMs) to extract the target concepts from natural language. These concepts, once encoded through the CLIP text encoder, served as targets for GradCAM, which facilitated the localization of the concept within brain signals. To enhance the precision of our localization, we trained three models with varying patch sizes (14, 12, 10) and utilized the penultimate layers of all models to extract semantic features. Fig. 7 illustrates the brain signal localization results for different semantic information, indicating our method\u2019s capacity to discriminate the positions of various semantics within brain signals for the same visual stimulus. ", "page_idx": 7}, {"type": "image", "img_path": "ohi00YhT3T/tmp/11c9bd5ce86e7c522090811c7e1e91db93e2ff40dd896643e83f368f98ef8b35.jpg", "img_caption": ["Figure 5: Ablation analysis of the hyperparameter $\\beta$ on visual reconstruction performance. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ohi00YhT3T/tmp/9d3d7768e316c7486f6350d07385bf2af601ed58ce2a971747908cbf7cf74750.jpg", "img_caption": ["Figure 6: Visualization of the impact of $\\beta$ on visual reconstruction. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To validate the efficacy of our method, we conducted an ablation study on the semantic concepts. After localizing the concepts in the original brain signals, we zeroed out the signals in the identified voxels and performed feature extraction and visual reconstruction using the modified brain signals. As depicted in Fig. 8, the removal of neural activity in specific brain regions associated with certain semantic concepts resulted in the omission of the corresponding semantics in the visual reconstruction. This substantiates the validity of our concept localization method within brain signals and demonstrates our approach\u2019s capacity for extracting and modifying semantic information in brain activity, which is pivotal for comprehending semantic information processing in the brain. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our study has successfully developed and validated a novel brain decoding framework that leverages the capabilities of Vision Transformer 3D in conjunction with fMRI data, enhanced by the integration of LLMs. This approach has demonstrated a notable improvement in the reconstruction of visual stimuli from brain signals, offering a more precise and interpretable understanding of the underlying neural mechanisms. The experimental results confirmed the robustness of our model in performing various cognitive tasks, including captioning, question-answering, and visual reconstruction, all from single-trial fMRI data. By enabling accurate localization of linguistic concepts within the brain, our work has potential applications in developing brain-computer interfaces and advanced cognitive modeling. Conclusively, this research contributes to the broader endeavor of decoding and interpreting brain activity, with significant implications for neuroscience and technology interface development. The fusion of advanced AI models with neuroimaging opens new avenues for exploring the intricacies of human cognition and the seamless integration of technology with neural processes. ", "page_idx": 8}, {"type": "image", "img_path": "ohi00YhT3T/tmp/5bbc9896fbce796353f67f5ec6fdc829a49f3f0f70ba6e99982254e2bacad7d0.jpg", "img_caption": ["Figure 7: Differential heatmaps of neural activity representing various semantic information for the same visual stimulus. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 8: Validation of concept localization by semantic signal nullification and its effect on visual reconstruction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was financially supported by a funding from Institute of Automation, Chinese Academy of Sciences (Grant No. E411230101). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Nikos K Logothetis. What we can do and what we cannot do with fmri. Nature, 453(7197):869\u2013 878, 2008.   \n[2] Yiheng Hu and Qing Yu. Spatiotemporal dynamics of self-generated imagery reveal a reverse cortical hierarchy from cue-induced imagery. Cell Reports, 42(10), 2023.   \n[3] Russell A Poldrack. The future of fmri in cognitive neuroscience. Neuroimage, 62(2):1216\u2013 1220, 2012.   \n[4] Russell A Poldrack. The role of fmri in cognitive neuroscience: where do we stand? Current opinion in neurobiology, 18(2):223\u2013227, 2008.   \n[5] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22710\u201322720, 2023.   \n[6] Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, et al. Reconstructing the mind\u2019s eye: fmri-to-image with contrastive learning and diffusion priors. Advances in Neural Information Processing Systems, 36, 2024.   \n[7] Andrew Luo, Margaret Marie Henderson, Michael J Tarr, and Leila Wehbe. Brainscuba: Finegrained natural language captions of visual cortex selectivity. In The Twelfth International Conference on Learning Representations, 2023.   \n[8] Jiaxuan Chen, Yu Qi, Yueming Wang, and Gang Pan. Mindgpt: Interpreting what you see with non-invasive brain recordings. arXiv preprint arXiv:2309.15729, 2023.   \n[9] Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14453\u201314463, 2023.   \n[10] Jean Lahoud, Jiale Cao, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, and Ming-Hsuan Yang. 3d vision with transformers: A survey. arXiv preprint arXiv:2208.04309, 2022.   \n[11] Weihao Xia, Raoul de Charette, Cengiz Oztireli, and Jing-Hao Xue. Dream: Visual decoding from reversing human visual system. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 8226\u20138235, 2024.   \n[12] Jingyuan Sun, Mingxiao Li, Zijiao Chen, Yunhao Zhang, Shaonan Wang, and Marie-Francine Moens. Contrast, attend and diffuse to decode high-resolution images from brain activities. Advances in Neural Information Processing Systems, 36, 2024.   \n[13] Andrew Luo, Maggie Henderson, Leila Wehbe, and Michael Tarr. Brain diffusion for visual exploration: Cortical discovery using large scale generative models. Advances in Neural Information Processing Systems, 36, 2024.   \n[14] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[15] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[16] Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116\u2013126, 2022.   \n[17] Stephen M Smith, Karla L Miller, Gholamreza Salimi-Khorshidi, Matthew Webster, Christian F Beckmann, Thomas E Nichols, Joseph D Ramsey, and Mark W Woolrich. Network modelling methods for fmri. Neuroimage, 54(2):875\u2013891, 2011.   \n[18] Tomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierarchical visual features. Nature communications, 8(1):15037, 2017.   \n[19] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. Semantic reconstruction of continuous language from non-invasive brain recordings. Nature Neuroscience, 26(5):858\u2013866, 2023.   \n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[21] Sikun Lin, Thomas Sprague, and Ambuj K Singh. Mind reader: Reconstructing complex images from brain activities. Advances in Neural Information Processing Systems, 35:29624\u201329636, 2022.   \n[22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020.   \n[23] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4296\u20134304, 2024.   \n[24] Weihao Xia, Raoul de Charette, Cengiz \u00d6ztireli, and Jing-Hao Xue. Umbrae: Unified multimodal brain decoding. In European Conference on Computer Vision (ECCV), 2024.   \n[25] Shizun Wang, Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Mindbridge: A cross-subject brain decoding framework. arXiv preprint arXiv:2404.07850, 2024.   \n[26] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574\u2013584, 2022.   \n[27] Wang Wenxuan, Chen Chen, Ding Meng, Yu Hong, Zha Sen, and Li Jiangyun. Transbts: Multimodal brain tumor segmentation using transformer. In International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, pages 109\u2013119, 2021.   \n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[29] Yucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20730\u201320740, 2022.   \n[30] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022.   \n[31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[32] Alexandre D\u00e9fossez, Charlotte Caucheteux, J\u00e9r\u00e9my Rapin, Ori Kabeli, and Jean-R\u00e9mi King. Decoding speech perception from non-invasive brain recordings. Nature Machine Intelligence, 5(10):1097\u20131107, 2023.   \n[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[35] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n[36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[37] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.   \n[38] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. arXiv preprint arXiv:2312.03700, 2023.   \n[39] Weijian Mai and Zhijun Zhang. Unibrain: Unify image reconstruction and captioning all in one diffusion model from human brain activity. arXiv preprint arXiv:2308.07428, 2023.   \n[40] Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, and Nicola Toschi. Brain captioning: Decoding human brain activity into images and text. arXiv preprint arXiv:2305.11560, 2023.   \n[41] Zijin Gu, Keith Jamison, Amy Kuceyeski, and Mert R Sabuncu. Decoding natural image stimuli from fmri data with a surface-based convolutional network. In Medical Imaging with Deep Learning, 2023.   \n[42] Furkan Ozcelik and Rufin VanRullen. Natural scene reconstruction from fmri signals using generative latent diffusion. Scientific Reports, 13(1):15666, 2023.   \n[43] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018.   \n[44] Meta. Meta llama 3. https://github.com/meta-llama/llama3, 2024.   \n[45] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013 612, 2004.   \n[46] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   \n[47] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.   \n[48] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[49] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912\u20139924, 2020.   \n[50] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Natural Scenes Dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We conducted our experiments on the Natural Scenes Dataset (NSD) [16], which consists of highresolution 7Tesla fMRI scans collected from eight healthy adult participants. Our analysis focused on the four subjects (sub $\\dot{7}\\,0\\,1$ , sub $\\Dot{\\b{\\b{\\b{\\b}}}}\\,0\\,2$ , sub $\\dot{7}\\,0\\,5$ , and $_{\\mathrm{{Sub}\\,\\dot{]}\\,0}\\,7}$ ) who completed all data collection sessions. Participants were exposed to thousands of natural scene images from the COCO dataset [34] during the sessions. However, the NSD required preprocessing to correct temporal resampling for slice timing differences and spatial interpolation to adjust for head motion and spatial distortion. We processed the scans in a $1.8{\\-}\\mathrm{mm}$ native volume space, particularly targeting the \"nsdgeneral\" region known for its high sensitivity to visual stimuli. This area, predominantly covering the posterior cortex, is crucial for targeted analysis of visual processing. ", "page_idx": 13}, {"type": "image", "img_path": "ohi00YhT3T/tmp/ac85f7f5446b47ccf40df675c62937c1b977b48470247e87b79a906c0460e294.jpg", "img_caption": ["Figure 9: Examples of some images and corresponding captions from the NSD dataset. Due to some image operations such as cropping, there is a mismatch between the original captions and the instance bounding boxes. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Our testing protocols included using the average response from three trials associated with each image to enhance reliability, a common practice in recent studies, as well as assessing each response separately to provide a more challenging and practically relevant test scenario. This approach allowed us to rigorously evaluate our method under realistic and diverse conditions, ensuring thorough validation against established benchmarks. Modifications such as cropping when presenting images to participants led to mismatches between the original captions and instance bounding boxes, as illustrated in Fig. 9. To ensure data consistency, we re-annotated the cropped images, generating eight captions for each image using BLIP2 [36]. ", "page_idx": 13}, {"type": "text", "text": "A.2 Language Extension ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To ensure compatibility between fMRI data and Large Language Models (LLMs) and to enable instruction-following and diversified interactions, we extended the Natural Scenes Dataset (NSD) with natural language annotations. This extension includes seven types of dialogues: brief descriptions, detailed descriptions, continuous dialogues, complex reasoning tasks, instruction reconstruction, and concept localization. ", "page_idx": 13}, {"type": "text", "text": "We first generated concise descriptions of the visual stimuli using BLIP2 and integrated these with the original COCO dataset captions to create brief descriptions of the images. Subsequently, DETR [37] was employed to generate bounding boxes for these images. We then combined the image captions and bounding box information as inputs and utilized GPT-3.5-turbo-0125 to generate various forms of dialogues. These dialogues were manually adjusted for format and content to ensure consistency and relevancy. For specifics on the prompts used during generation, please refer to the supplementary document. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Furthermore, we constructed a multimodal fine-tuning dataset based on the generated dialogues. For each piece of fMRI data, we created corresponding language extensions. Commands for brief and detailed descriptions are illustrated in Tab. 3 and 4, respectively. We randomly selected questions and corresponding answers from these to construct Q&A pairs, enhancing the model\u2019s ability to engage in meaningful dialogue based on the visual content. For continuous dialogues and complex reasoning, we used dialogues generated by $\\mathtt{G P T-3.5-t u r b o-0125}$ . For instruction reconstruction, the commands are shown in Tab. 5, which aim to have the model generate detailed descriptions of visual stimuli using concise expressions. For concept localization, the commands are shown in Tab. 6, used to extract concepts mentioned in the prompts and visualize the model\u2019s attention using grad-CAM [35]. ", "page_idx": 14}, {"type": "image", "img_path": "ohi00YhT3T/tmp/a5fddaf8fbaa35178f8e1fd9e93b7a39a609067fee4578b93f08a37e097f3613.jpg", "img_caption": ["Table 3: The list of instructions for brief description. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B fMRI Data Preprocessing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We initially preprocess the fMRI data to ensure consistency. Specifically, we resize the data to uniform dimensions using trilinear interpolation. The BOLD signal dimensions for sub $\\dot{7}\\,0\\,1$ are used as the standard, set at $83\\times104\\times81$ . After applying zero-padding to the edges, we divide the data into $14\\times14\\times14$ patches to preserve local information. We then employ a mask created from the union of the \u201cnsdgeneral\u201d regions across all subjects in the NSD dataset, which helps in eliminating information unrelated to the visual stimuli. This process results in data formatted as $N\\times C$ , where $N$ is the number of retained patches, and $C$ represents the size of each patch. ", "page_idx": 14}, {"type": "text", "text": "During the training of our feature extractors, we enhance data variability by applying MixUp [43] to different fMRI responses from the same subject for the same visual stimuli. MixUp coefficients are generated using a uniform distribution, with the mixing ratio $\\lambda\\sim U(0,1)$ , to blend features from different trials. This technique aids in developing a robust model by exposing it to interpolated data points, fostering generalization across varied neural responses. ", "page_idx": 14}, {"type": "text", "text": "C Architecture and Training Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "CLIP ViT-L/14 [14] and Autoencoder KL [15] were used as feature extractors for images, aligning with fMRI data. For the fMRI data, we employed a 16-layer Transformer Encoder [28] with a hidden size of 768 to extract features, using the class token from the last layer as the output. Two two-layer perceptrons with a hidden dimension of 1024, $f_{w_{c}}$ and $f_{w_{v}}$ , were used to align with CLIP and VAE features, respectively, with the hyperparameter $\\alpha$ set to $1/64$ . The learning rate was set to $5\\times10^{-4}$ , training for 30 epochs. For multimodal interaction, the aforementioned Transformer Encoder was frozen, using its second-to-last layer\u2019s hidden states as the fMRI token, processed through a two-layer perceptron $f_{w_{t}}$ , to interact with Llama-3-8B [44]. Training was divided into two stages: initially, the LLM was frozen, tuning only $f_{w_{t}}$ , and in the second stage, both the LLM and $f_{w_{t}}$ were fine-tuned simultaneously. The learning rate was set to $2\\times10^{-5}$ , training for one epoch. In the visual reconstruction phase, the LLM called upon UnCLIP-2 [20] for visual reconstruction, with the hyperparameter $\\beta$ set to 0.93. For concept location, the LLM first extracted keywords and then localized them using GradCAM [35], visualizing the results. ", "page_idx": 14}, {"type": "image", "img_path": "ohi00YhT3T/tmp/287e700c5ed0b59f9ce1af806a892292d10afd8e31bf13041b41532de1a8c996.jpg", "img_caption": ["Table 4: The list of instructions for detailed description. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "ohi00YhT3T/tmp/52e7a1b252a3df25b8233846e79628b74750236d6df69b7fceea3de656e64a7d.jpg", "img_caption": ["Table 6: The list of instructions for concept localization. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "All experiments were conducted on a server equipped with 8 NVIDIA A100 GPUs, each with 80 GB of memory. Training of the feature extractor was performed on a single GPU with a batch size set to 32, and the training duration across subjects was approximately 8 hours. For fine-tuning with LLMs, all 8 GPUs were utilized. During the phase where only $f_{w_{t}}$ was fine-tuned, the batch size was set to 32, and the training time was about 4 hours. In the phase where LLMs were unfrozen for joint fine-tuning, the batch size was adjusted to 24, extending the training time to approximately 36 hours. ", "page_idx": 15}, {"type": "text", "text": "C.1 Metrics for Visual Reconstruction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For evaluating visual decoding performance, we adhere to the established suite of eight metrics, commonly utilized in the field [41; 6; 11; 9]. The metrics are divided into two categories: low-level and high-level. Low-level metrics include Pixelwise Correlation (PixCorr) and Structural Similarity Index Metric (SSIM) [45], as well as AlexNet(2) and AlexNet(5) [46], which measure the fidelity of reconstructed images against ground truth. High-level metrics comprise Inception [47], CLIP [14], EfficientNet-B (EffNet-B) [48], and SwAV-ResNet50 (SwAV) [49], which evaluate the semantic accuracy of the reconstructions. ", "page_idx": 16}, {"type": "text", "text": "Following protocols from previous research [11], we downsampled the generated images from a resolution of $512\\times512$ to $425\\times425$ , which matches the resolution of ground truth images in the NSD Dataset. This resolution adjustment was specifically for PixCorr and SSIM evaluations. For other metrics, the generated images were processed according to the input requirements of each respective model. ", "page_idx": 16}, {"type": "text", "text": "Two-way identification tests were conducted following the methodology of Ozcelik and VanRullen [42]. For each model, we calculated the Pearson correlation between embeddings of the ground truth image and its reconstruction, as well as between the ground truth image and another random reconstruction from the test set. A test was marked as correct if the correlation with the ground truth was higher than with the unrelated reconstruction. Performance for each test sample was averaged over all possible pairwise comparisons with the other 981 reconstructions to eliminate any bias from random selection. This resulted in 982 averaged percentage correct outputs, which were then averaged to derive the final metrics presented in Tab 2. ", "page_idx": 16}, {"type": "text", "text": "In addition to the established metrics, we introduced a testing protocol by utilizing only the first fMRI record for each visual stimulus to construct a single-trial test [6]. This approach presents a more stringent and practical challenge, reflecting a scenario closer to real-world applications where each neural response is unique and not averaged over multiple instances. ", "page_idx": 16}, {"type": "text", "text": "D More Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Visual Reconstruction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Fig. 10 shows some randomly selected visual reconstruction results of different subjects under the single-trial condition. Consistent with the results in the main article, our method produces consistent visual reconstruction results across different subjects. This shows that our method has good generalization performance across different subjects. But there are also some erroneous reconstruction results. ", "page_idx": 16}, {"type": "table", "img_path": "ohi00YhT3T/tmp/78c36fc43a9205fb5bc1da1762db166bc2d9a4353cbf889507b0e3db97b0e112.jpg", "table_caption": ["Table 7: Performance comparison on visual reconstruction tasks when using different LLMs and different instructions. The underline indicates the best result under the same conditions. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Tab. 7 shows the performance comparison of visual reconstruction tasks when using different large language models (LLMs) and different generation instructions. We compared the two models Vicuna13B [50] and Llama-3-8B [44]. The results indicate that our method is compatible with various LLMs and performs better in visual reconstruction tasks when using more powerful LLMs. When using Llama-3-8B, the best performance was achieved across the majority of metrics. Additionally, we explored the impact of different generation instructions on the visual reconstruction task. The results show that using instructions specifically designed for visual reconstruction can improve the performance of the visual reconstruction task. We believe this is because these instructions enable the LLMs to generate more detailed descriptions of visual stimuli, thereby enhancing the performance of the visual reconstruction task. ", "page_idx": 16}, {"type": "image", "img_path": "ohi00YhT3T/tmp/4fb82e90a0d4a3674b46ffef7ffbd1dd5b40d177eeb296371a179b330525f9cc.jpg", "img_caption": ["Figure 10: More visual reconstruction results for different subjects under single-trial conditions. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.2 Language Interaction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We show some examples of multimodal interactions from fMRI data, as shown in Fig. 11. The model conducted multimodal interactions based on fMRI signals and generated different forms of dialogue, including brief descriptions, detailed descriptions, and complex reasoning. ", "page_idx": 17}, {"type": "image", "img_path": "ohi00YhT3T/tmp/6d10c161733cf9597d4e1b4410e4f5fc24e38a08a289a6d97dd9914a86b83a2f.jpg", "img_caption": ["Figure 11: Some examples of multimodal interactions from fMRI data. The images are for reference only. The model performs multi-modal interaction based on fMRI signals and generates different forms of dialogue, including brief descriptions, detailed descriptions, and complex reasoning. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.3 Concept Localization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the section, we explored the neural correlates of visual stimuli by mapping the captions derived from the visual content directly onto brain signals. This process involved using GradCAM [35] to generate heatmaps that visually represent the regions of the brain activated in response to specific elements of the visual stimuli. These heatmaps provide a compelling visualization of how different concepts associated with the images are processed across various areas of the brain. ", "page_idx": 18}, {"type": "text", "text": "Fig. 12 displays heatmaps that localize the brain activity corresponding to the captions of visual stimuli. These images are crucial for understanding the distribution and intensity of neural responses as they relate to the cognitive processing of visual information. By analyzing these heatmaps, we can infer which areas of the brain are most involved in the interpretation and semantic processing of the stimuli, providing insights into the underlying mechanisms of visual perception and cognitive response. ", "page_idx": 18}, {"type": "text", "text": "The localization of these concepts within the brain not only aids in validating theoretical models of brain function but also enhances our understanding of the cognitive processes involved in visual perception. Such detailed mappings are instrumental in advancing our knowledge of the brain\u2019s architecture and its functional connectivity in response to complex stimuli. ", "page_idx": 18}, {"type": "image", "img_path": "ohi00YhT3T/tmp/eb71c3e47846a66ed521271e396db4fee24987599eac28b9639db0ca0bb9c48c.jpg", "img_caption": ["Figure 12: Heatmaps illustrating brain regions activated by specific captions derived from visual stimuli, demonstrating the spatial distribution of neural responses. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "While our study introduces several innovative approaches to the decoding of non-invasive brain recordings and extends the capabilities of visual reconstruction using advanced computational models, there are several limitations that should be acknowledged: ", "page_idx": 19}, {"type": "text", "text": "Generalization across Diverse Populations: Our method was validated primarily using the Natural Scenes Dataset (NSD), which consists of data from a limited number of subjects. Although we demonstrate robustness across these subjects, the generalizability of our findings to broader populations remains an area for further investigation. Differences in neural anatomy and functional organization across individuals that are not represented in the NSD could affect the efficacy and accuracy of our model in wider applications. ", "page_idx": 19}, {"type": "text", "text": "Computational Complexity and Resource Requirements: The implementation of our framework, particularly the integration with Large Language Models (LLMs) and advanced image processing techniques like ViT3D, requires substantial computational resources. This might restrict the utility of our approach in environments with limited computational capacity or in real-time applications where rapid processing is crucial. ", "page_idx": 19}, {"type": "text", "text": "Challenges in Real-World Application:While the single-trial test setting introduced in our study adds a layer of practical relevance by evaluating model performance in a more realistic scenario, it also presents challenges. The variability in single-trial fMRI responses, which can be influenced by numerous uncontrollable factors such as minor head movements or physiological fluctuations, may lead to inconsistencies in decoding accuracy. This variability emphasizes the need for further refinement of noise reduction and signal processing techniques. ", "page_idx": 20}, {"type": "text", "text": "Ethical Considerations: The development and application of brain decoding technologies raise ethical questions, particularly concerning privacy and consent. As our methods advance and potentially become capable of decoding more detailed and sensitive information from brain data, ensuring ethical deployment and the protection of participant data becomes paramount. ", "page_idx": 20}, {"type": "text", "text": "These limitations highlight the need for continuous improvement and careful consideration in the deployment of brain decoding technologies. Addressing these challenges through further research and development will be crucial for realizing the full potential of non-invasive brain decoding in both scientific research and clinical applications. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Limitations are discussed in Appendix E. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The manuscript does not contain theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Appendices A, B and C we describe in detail the settings required to reproduce the experiment, and for the datasets and key code that will be released with this manuscript, we have provided it in the supplementary material. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Code and language extensions for the NSD dataset are provided in the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Detailed training details and evaluation metrics are provided in Appendices B and C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: The results of the manuscript do not include error bars, mainly for the following reasons: 1. In order to make a fair comparison with other methods, other literature in this field does not report error bars. 2. Multiple experiments involving LLMs are computationally expensive. 3. The results of this manuscript have been verified on different subjects, and the results are consistent. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Detailed hardware facilities and computational overhead are provided in Appendix B. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: This manuscript does not involve direct human subjects; all data used are from publicly available datasets that comply with the ethical standards of NIPS. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Detailed social impact is discussed in the Conclusion and Appendix E. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There is no relevant risk. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do cite all the existing assets in our paper as well as in our codebase. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]