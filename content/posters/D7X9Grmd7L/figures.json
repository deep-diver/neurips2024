[{"figure_path": "D7X9Grmd7L/figures/figures_1_1.jpg", "caption": "Figure 1: Zero-Shot Change Detection with AnyChange on a wide range of application scenarios in geoscience. Each subfigure presents the pre-event image, the post-event image, and their change instance masks in order. The boundary of each change instance mask is rendered by cyan, and meanwhile, these change masks are also drawn on pre/post-event images to show more clearly where the change occurred. The color of each change mask is used to distinguish between different instances.", "description": "This figure showcases the AnyChange model's zero-shot change detection capabilities across various real-world applications.  Each row displays a different application (e.g., urbanization, disaster assessment), with three columns showing the before image, the after image, and a mask highlighting the changes detected. The cyan outlines on the images visually delineate the change areas, while the colored masks within the change areas differentiate individual changed objects.", "section": "1 Introduction"}, {"figure_path": "D7X9Grmd7L/figures/figures_2_1.jpg", "caption": "Figure 2: Segment Any Change Models, AnyChange. SAM forward: given grid points {p} as prompts and input images, SAM produces object masks {mt,i} and image embedding zt on the image at time t. Bitemporal Latent Matching does a bidirectional matching to compute the change confidence score for each change proposal, and then top-k sorting or thresholding is applied for zero-shot change proposal and detection. Point Query allows users to click some points (the case of two points in this subfigure) with the same category to filter class-agnostic change masks via semantic similarity for object-centric change detection.", "description": "This figure illustrates the AnyChange model's architecture. It shows how the Segment Anything Model (SAM) is used as a base, with bitemporal latent matching for zero-shot change detection and a point query mechanism for object-centric change detection.  The process begins with SAM processing the images to produce object masks and image embeddings.  Then, a bidirectional matching step (bitemporal latent matching) compares the embeddings to assess the change confidence score for each proposed change. This is followed by a top-k selection or thresholding for change detection. Finally, a point query mechanism allows users to select specific areas of interest, which helps refine the change detection results by leveraging semantic similarities to filter class-agnostic masks and pinpoint object-centric changes.", "section": "3 Segment Any Change Models"}, {"figure_path": "D7X9Grmd7L/figures/figures_4_1.jpg", "caption": "Figure 3: Empirical evidence on semantic similarity on (a) the same satellite image and (b) the satellite images at different times. (a) The visualization of the first three PCA components indicates the objects with the same category are well matched with each other in SAM's latent space; Three red point queries confirm the existence of semantic similarity on the same satellite image. (b) The object proposals (indicated by red points) from the satellite image at t1 as queries are used to match all proposals from the satellite image at t2. (best viewed with zoom, especially for the point query)", "description": "This figure provides empirical evidence supporting the existence of semantic similarities within and between satellite images at different times.  Part (a) shows intra-image similarity using PCA visualization and point queries on a single image to demonstrate that objects of the same category have similar embeddings in SAM's latent space. Part (b) shows inter-image similarity by using object proposals from one image as queries to match proposals in another image from the same location but at a different time, showing consistent results and semantic similarity persists even with temporal differences.", "section": "3.2 Exploring the Latent Space of SAM"}, {"figure_path": "D7X9Grmd7L/figures/figures_4_2.jpg", "caption": "Figure 3: Empirical evidence on semantic similarity on (a) the same satellite image and (b) the satellite images at different times. (a) The visualization of the first three PCA components indicates the objects with the same category are well matched with each other in SAM's latent space; Three red point queries confirm the existence of semantic similarity on the same satellite image. (b) The object proposals (indicated by red points) from the satellite image at t\u2081 as queries are used to match all proposals from the satellite image at t2. (best viewed with zoom, especially for the point query)", "description": "This figure provides empirical evidence supporting the existence of semantic similarities within and between satellite images at different times using the Segment Anything Model (SAM).  The left panel (a) shows intra-image similarity, demonstrating how objects of the same category in a single image have similar representations in SAM's latent space (visualized via PCA and a probing experiment). The right panel (b) shows inter-image similarity, demonstrating that object proposals from an image at one time point (t1) can be successfully matched to similar objects in an image from the same location at a different time point (t2). This similarity is crucial for the AnyChange model's ability to perform zero-shot change detection. ", "section": "3.2 Exploring the Latent Space of SAM"}, {"figure_path": "D7X9Grmd7L/figures/figures_8_1.jpg", "caption": "Figure 6: Examples of Point Query Mechanism. The effects of w/o point query, one-point query, and three-point queries are shown in sequence from left to right. (best viewed digitally with zoom, especially for the red points)", "description": "This figure demonstrates the effectiveness of the point query mechanism in AnyChange. The leftmost column shows the images at time t and t+1, respectively. The remaining columns demonstrate how the change masks generated by AnyChange change with different numbers of point queries. When no point query is used (i.e., the second column), AnyChange generates class-agnostic change masks, which represent all types of change. Using point queries allows the model to focus on specific semantic objects of interest. As the number of point queries increases, the change masks become more accurate and focused on the objects that are specified using the point queries.", "section": "4.2 Zero-shot Object-centric Change Detection"}, {"figure_path": "D7X9Grmd7L/figures/figures_14_1.jpg", "caption": "Figure 1: Zero-Shot Change Detection with AnyChange on a wide range of application scenarios in geoscience. Each subfigure presents the pre-event image, the post-event image, and their change instance masks in order. The boundary of each change instance mask is rendered by cyan, and meanwhile, these change masks are also drawn on pre/post-event images to show more clearly where the change occurred. The color of each change mask is used to distinguish between different instances.", "description": "This figure showcases AnyChange's zero-shot change detection capabilities across various geoscience applications.  Each row displays a before image, an after image, and a corresponding change mask.  The change mask highlights areas where changes have occurred, with different colors indicating distinct changes. The cyan outlines show the boundaries of these change areas clearly on the before and after images.", "section": "1 Introduction"}, {"figure_path": "D7X9Grmd7L/figures/figures_14_2.jpg", "caption": "Figure 6: Examples of Point Query Mechanism. The effects of w/o point query, one-point query, and three-point queries are shown in sequence from left to right. (best viewed digitally with zoom, especially for the red points)", "description": "This figure demonstrates the impact of using point queries in the AnyChange model for object-centric change detection. It shows how the model's output changes as the number of point queries increases, progressing from class-agnostic change masks (no point query) to more focused and accurate change detection as more point queries are used.", "section": "4.2 Zero-shot Object-centric Change Detection"}, {"figure_path": "D7X9Grmd7L/figures/figures_14_3.jpg", "caption": "Figure 4: Examples of Point Query Mechanism. The effects of w/o point query, one-point query, and three-point queries are shown in sequence from left to right. (best viewed digitally with zoom, especially for the red points)", "description": "This figure demonstrates the point query mechanism of the AnyChange model.  It shows how class-agnostic change masks (without a point query) can be refined to focus on object-centric changes by specifying points of interest.  The left column displays pre-event imagery, the middle column displays post-event imagery, and the right column shows the corresponding change masks. The rows illustrate different query scenarios: no query, a single point query, and multiple point queries, demonstrating how the precision of change detection improves with additional semantic information from user-provided points.", "section": "4 Experiments"}]