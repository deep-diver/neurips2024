[{"figure_path": "wH36UKML4x/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the proposed method. Given an ERM-trained model (similar to DFR [1]), the following steps are performed: (a) we randomly split the held-out dataset into train and validation splits. (b) An environment inference method is utilized to infer diverse environments from the validation split. (c) We evaluate train split samples on the initial ERM classifier and sort high-loss and low-loss samples of each class for loss-based sampling. (d) Finally, we perform last-layer retraining on the loss-based selected samples. Each retraining setting (e.g., different k for loss-based sampling) is validated based on the worst accuracy of the inferred environments. Note that majority and minority groups are shown with dark and light colors for better visualization, but are not known in our setting.", "description": "This figure illustrates the EVaLS method's four steps: 1) splitting the held-out dataset into train and validation sets, 2) utilizing environment inference to create various environments from the validation set, 3) employing loss-based sampling to generate a balanced dataset of high and low loss samples for retraining, and 4) retraining the last layer based on the worst accuracy of the generated environments. This process helps improve the model's robustness against group shifts without group annotations.", "section": "3 Environment-based Validation and Loss-based Sampling"}, {"figure_path": "wH36UKML4x/figures/figures_5_1.jpg", "caption": "Figure 2: The proportion of minority(majority) samples across different classes within various percentages of DLL samples with highest (lowest) loss for the Waterbirds (a) and CelebA (b) datasets. Minority group samples are more prevalent among high-loss samples, while majority group samples dominate the low-loss areas. The error bars are calculated across three ERM models.", "description": "This figure shows the proportion of minority and majority samples at various loss percentiles in the Waterbirds and CelebA datasets.  It demonstrates that high-loss samples are enriched in minority group samples, while low-loss samples are dominated by majority group samples. This observation supports the method's use of high- and low-loss samples as representatives of minority and majority groups.", "section": "3.1 Loss-Based Instance Sampling"}, {"figure_path": "wH36UKML4x/figures/figures_6_1.jpg", "caption": "Figure 3: Two spurious correlations in a dataset. (a) If both spurious attributes are known, they can be utilized to fit a classifier that captures the essential attributes. (b) In the absence of knowledge about both spurious attributes, the model would depend on them for classification, leading to incorrect classification of minority samples. (c) If one spurious attribute is unknown (Spurious 2), the model becomes robust only to the known spurious correlation (Spurious 1), but it still underperforms on minority samples. (d) The Dominoes-CMF dataset, which contains two spurious attributes.", "description": "This figure illustrates the concept of spurious correlation and how it can affect model performance. In (a), a model correctly identifies the core attributes when both spurious attributes are known. (b) shows that if both spurious attributes are unknown, the model relies on them, leading to poor performance on minority samples.  (c) demonstrates that even if only one spurious attribute is unknown, model robustness is limited, still underperforming on minority groups. Finally, (d) introduces the Dominoes CMF dataset used in the paper, which is specifically designed to have two independent spurious attributes.", "section": "2.2 Robustness of a Trained Model to an Unknown Shortcut"}, {"figure_path": "wH36UKML4x/figures/figures_15_1.jpg", "caption": "Figure 1: Overview of the proposed method. Given an ERM-trained model (similar to DFR [1]), the following steps are performed: (a) we randomly split the held-out dataset into train and validation splits. (b) An environment inference method is utilized to infer diverse environments from the validation split. (c) We evaluate train split samples on the initial ERM classifier and sort high-loss and low-loss samples of each class for loss-based sampling. (d) Finally, we perform last-layer retraining on the loss-based selected samples. Each retraining setting (e.g. different k for loss-based sampling) is validated based on the worst accuracy of the inferred environments. Note that majority and minority groups are shown with dark and light colors for better visualization, but are not known in our setting.", "description": "This figure illustrates the proposed EVaLS method, showing the steps involved in creating a robust model. Starting with an ERM-trained model, the held-out dataset is randomly split.  Diverse environments are inferred from the validation set using an inference method. The training set is then evaluated, sorting samples by loss into high and low-loss groups. Finally, last-layer retraining is performed on these loss-based samples, using worst-environment accuracy to guide hyperparameter tuning and model selection. Note that the method does not use group annotations.", "section": "3 Environment-based Validation and Loss-based Sampling"}, {"figure_path": "wH36UKML4x/figures/figures_18_1.jpg", "caption": "Figure 1: Overview of the proposed method. Given an ERM-trained model (similar to DFR [1]), the following steps are performed: (a) we randomly split the held-out dataset into train and validation splits. (b) An environment inference method is utilized to infer diverse environments from the validation split. (c) We evaluate train split samples on the initial ERM classifier and sort high-loss and low-loss samples of each class for loss-based sampling. (d) Finally, we perform last-layer retraining on the loss-based selected samples. Each retraining setting (e.g. different k for loss-based sampling) is validated based on the worst accuracy of the inferred environments. Note that majority and minority groups are shown with dark and light colors for better visualization, but are not known in our setting.", "description": "This figure shows the workflow of the proposed EVaLS method. It starts with an ERM-trained model and then performs four steps: 1) splitting the held-out data, 2) inferring diverse environments from the validation split, 3) performing loss-based sampling to create a balanced dataset, and 4) retraining the last layer of the model based on the worst accuracy of the inferred environments. The figure illustrates the process using different colors for majority and minority groups, but emphasizes that these groups are not known a priori in the proposed method.", "section": "Environment-based Validation and Loss-based Sampling"}, {"figure_path": "wH36UKML4x/figures/figures_18_2.jpg", "caption": "Figure 1: Overview of the proposed method. Given an ERM-trained model (similar to DFR [1]), the following steps are performed: (a) we randomly split the held-out dataset into train and validation splits. (b) An environment inference method is utilized to infer diverse environments from the validation split. (c) We evaluate train split samples on the initial ERM classifier and sort high-loss and low-loss samples of each class for loss-based sampling. (d) Finally, we perform last-layer retraining on the loss-based selected samples. Each retraining setting (e.g. different k for loss-based sampling) is validated based on the worst accuracy of the inferred environments. Note that majority and minority groups are shown with dark and light colors for better visualization, but are not known in our setting.", "description": "This figure shows the overall workflow of the proposed Environment-based Validation and Loss-based Sampling (EVaLS) method. It starts by splitting the held-out dataset into training and validation sets. Then, diverse environments are inferred from the validation set using an environment inference method.  The training set is then evaluated using an ERM classifier, and high-loss and low-loss samples are identified and sorted for loss-based sampling. Finally, last-layer retraining is performed on the selected samples, and model selection is based on the worst-case accuracy across the inferred environments.", "section": "Environment-based Validation and Loss-based Sampling"}, {"figure_path": "wH36UKML4x/figures/figures_19_1.jpg", "caption": "Figure 1: Overview of the proposed method. Given an ERM-trained model (similar to DFR [1]), the following steps are performed: (a) we randomly split the held-out dataset into train and validation splits. (b) An environment inference method is utilized to infer diverse environments from the validation split. (c) We evaluate train split samples on the initial ERM classifier and sort high-loss and low-loss samples of each class for loss-based sampling. (d) Finally, we perform last-layer retraining on the loss-based selected samples. Each retraining setting (e.g. different k for loss-based sampling) is validated based on the worst accuracy of the inferred environments. Note that majority and minority groups are shown with dark and light colors for better visualization, but are not known in our setting.", "description": "This figure illustrates the workflow of the proposed Environment-based Validation and Loss-based Sampling (EVaLS) method.  It shows how a pre-trained model is used, held-out data is split, diverse environments are inferred, loss-based sampling creates a balanced dataset, and finally, last-layer retraining is performed with model selection based on worst-case environment accuracy. The key is that group annotations are not needed for any of these steps.", "section": "3 Environment-based Validation and Loss-based Sampling"}, {"figure_path": "wH36UKML4x/figures/figures_22_1.jpg", "caption": "Figure 1: Overview of the proposed method. Given an ERM-trained model (similar to DFR [1]), the following steps are performed: (a) we randomly split the held-out dataset into train and validation splits. (b) An environment inference method is utilized to infer diverse environments from the validation split. (c) We evaluate train split samples on the initial ERM classifier and sort high-loss and low-loss samples of each class for loss-based sampling. (d) Finally, we perform last-layer retraining on the loss-based selected samples. Each retraining setting (e.g. different k for loss-based sampling) is validated based on the worst accuracy of the inferred environments. Note that majority and minority groups are shown with dark and light colors for better visualization, but are not known in our setting.", "description": "This figure illustrates the Environment-based Validation and Loss-based Sampling (EVaLS) method proposed in the paper.  It shows a four-step process: 1) Randomly splitting a held-out dataset into training and validation sets. 2) Using environment inference methods on the validation set to identify diverse environments representing different group shifts. 3) Sorting the training samples based on their loss values from the ERM-trained model and selecting a balanced set of high-loss and low-loss samples. 4) Performing last-layer retraining with the balanced dataset and selecting the model based on the worst-case accuracy across the diverse environments. The visual representation uses dark and light colors to represent majority and minority groups, highlighting that group labels are not needed in EVaLS.", "section": "3 Environment-based Validation and Loss-based Sampling"}]