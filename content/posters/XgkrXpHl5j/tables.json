[{"figure_path": "XgkrXpHl5j/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of multimodal method proposed in the fusion phase.", "description": "This table compares various multimodal fusion methods, highlighting their differences in terms of task type (Native Multimodal Tasks or Extended Multimodal Tasks), feature alignment, gradient flow impact on feature extractors, generalization capabilities across tasks, ability to handle missing modalities, and computational complexity.  It provides a summary of existing methods to contextualize the proposed GMF approach.", "section": "2 Related Works"}, {"figure_path": "XgkrXpHl5j/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of EMTs and GMTs methods on VGGSound.", "description": "This table compares the performance of various multimodal fusion methods (both EMT and GMT) on the VGGSound dataset for audio-visual event classification tasks.  It shows the classification accuracy (ACC), real-time processing speed (CPU time in seconds), number of parameters, and FLOPS for each method, broken down by whether the feature extractors were trained or frozen.  The table helps to evaluate the effectiveness of different fusion techniques, particularly considering the effect of trainable versus frozen feature extractors.", "section": "5.3 Evaluation"}, {"figure_path": "XgkrXpHl5j/tables/tables_8_1.jpg", "caption": "Table 3: Comparison of NMTs and GMTs methods on ActivityNet.", "description": "This table presents a comparison of various multimodal methods, namely CLIP, METER, Perceiver, MAP-IVR, and GMF, on the ActivityNet dataset.  The performance metrics used are mean average precision (mAP) at different recall levels (mAP@10, mAP@20, mAP@50, mAP@100).  The table also includes model parameters and FLOPs (floating-point operations) to demonstrate the computational complexity of each approach.", "section": "5.1 Datasets and experimental tasks"}, {"figure_path": "XgkrXpHl5j/tables/tables_8_2.jpg", "caption": "Table 4: Comparison of fusion methods based on different feature extractors on FakeAVCeleb.", "description": "This table presents a comparison of different multimodal fusion methods on the FakeAVCeleb dataset.  The methods are evaluated based on their performance using different feature extractors (Baseline, MISA, UAVM, DrFuse, Perceiver, and the proposed GMF), focusing on the classification accuracy (ACC) and Area Under the Curve (AUC) metrics. The table highlights the performance variations among the methods under different feature extraction conditions. The results provide insights into the effectiveness and robustness of the proposed GMF compared to existing state-of-the-art multimodal fusion methods.", "section": "5.1 Datasets and experimental tasks"}, {"figure_path": "XgkrXpHl5j/tables/tables_24_1.jpg", "caption": "Table 2: Comparison of EMTs and GMTs methods on VGGSound.", "description": "This table compares the performance of several multimodal fusion methods (including the proposed GMF) on the VGGSound dataset for audio-visual event classification tasks. It shows the accuracy, real-time processing speed, number of parameters, and FLOPS for each method, with different settings of using trainable or frozen feature extractors and different modalities (audio only, video only, audio-visual).  The results highlight the performance and efficiency of the proposed GMF.", "section": "5.3 Evaluation"}, {"figure_path": "XgkrXpHl5j/tables/tables_24_2.jpg", "caption": "Table 2: Comparison of EMTs and GMTs methods on VGGSound.", "description": "This table compares the performance of several multimodal fusion methods (EMTs and GMTs) on the VGGSound dataset for audio-visual event classification.  It shows the accuracy (ACC) and area under the curve (AUC) for each model, along with details on the feature extractors used (whether they were trained or frozen), and the computational resources required (parameters and FLOPS). The table highlights the effectiveness of the proposed GMF method in comparison to other state-of-the-art models.", "section": "5.3 Evaluation"}, {"figure_path": "XgkrXpHl5j/tables/tables_25_1.jpg", "caption": "Table 2: Comparison of EMTs and GMTs methods on VGGSound.", "description": "This table compares the performance of various multimodal fusion methods, including the proposed GMF, on the VGGSound dataset for emotion recognition tasks.  It shows the accuracy (ACC) and area under the curve (AUC) for audio (A), video (V), and audio-visual (AV) inputs.  The table also indicates whether the feature extractors were trainable or frozen and includes the number of parameters and computational cost (FLOPs and CPU time) for each method. This allows for a comprehensive comparison of the different approaches in terms of accuracy, efficiency, and generalizability.", "section": "5.3 Evaluation"}, {"figure_path": "XgkrXpHl5j/tables/tables_27_1.jpg", "caption": "Table 1: Comparison of multimodal method proposed in the fusion phase.", "description": "This table compares various multimodal fusion methods, highlighting their approaches to feature alignment, gradient flow, task generalizability, handling of missing modalities, and computational complexity.  It categorizes methods by their suitability for native or extended multimodal tasks, offering a comparative overview of their strengths and limitations.", "section": "Related Works"}]