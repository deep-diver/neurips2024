[{"figure_path": "XgkrXpHl5j/figures/figures_2_1.jpg", "caption": "Figure 1: Stages of information entropy change. Where Z\u2081 might be a set of vectors ({ ZA, . . ., ZM }) or a vector, depending on the fusion method F(\u00b7), and C(\u00b7) stands for classifier.", "description": "This figure illustrates the three main stages of multimodal learning: feature extraction, fusion, and classification.  In the extraction stage, unimodal feature extractors (f(j)) process inputs from each modality (X(j)) to produce modality-specific features. These features are then fused using a fusion method (F) to create a joint representation (Z). Finally, the joint representation is passed to a classifier (C) to generate the output (Y).  The figure highlights the changes in information entropy across each stage, suggesting a reduction in entropy from the raw inputs to the final output.", "section": "3 Theory"}, {"figure_path": "XgkrXpHl5j/figures/figures_6_1.jpg", "caption": "Figure 2: Structure of GMF. The input is taken from f(Xi, 0) and the output is taken as Zi. This is done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be directly used for classification or can be connected to other fusion modules. See Appendix J", "description": "This figure illustrates the Generalized Multimodal Fusion (GMF) method's three-stage process: dissociation, concentration, and reconstruction.  The input features are from multiple modalities and are first dissociated (separated into modality-specific and modality-invariant components).  Next, the features are concentrated (grouped into relevant subsets). Finally, the features are reconstructed into the sorted output.  The GMF can function as a stand-alone front-end, easily integrated with various backend modules such as concatenation or more advanced fusion methods.", "section": "4 Methodology"}, {"figure_path": "XgkrXpHl5j/figures/figures_12_1.jpg", "caption": "Figure 1: Stages of information entropy change. Where Z\u2081 might be a set of vectors ({ ZA, . . ., ZM }) or a vector, depending on the fusion method F(\u00b7), and C(\u00b7) stands for classifier.", "description": "This figure illustrates the three main stages of multimodal learning: feature extraction, fusion, and classification.  In the extraction stage, unimodal feature extractors, f(j)(.,\u03b8(j)), process individual modality inputs X(j) to produce modality-specific feature vectors f(j)(X(j), \u03b8(j)). These features are then combined in the fusion stage using a fusion method F(., \u03b8F) to generate a set of fused features Z\u1d62. Finally, in the classification stage, a classifier C(., \u03b8C) takes the fused features Z\u1d62 as input and outputs the final classification result Y.  The figure also highlights that information entropy changes in each of these stages.", "section": "3 Theory"}, {"figure_path": "XgkrXpHl5j/figures/figures_13_1.jpg", "caption": "Figure 2: Structure of GMF. The input is taken from f(Xi, 0) and the output is taken as Zi. This is done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be directly used for classification or can be connected to other fusion modules. See Appendix J", "description": "This figure illustrates the architecture of the Generalized Multimodal Fusion (GMF) method. The input to GMF is the feature vector f(Xi, 0) from unimodal feature extractors.  GMF operates in three stages: 1) Dissociation, where the features are separated into modality-invariant and modality-specific subspaces.  2) Concentration, where these subspaces are independently processed and potentially dimensionality reduced. 3) Reconstruction, where these components are recombined to form the output feature vector Zi. The GMF method can be used independently as a front-end, integrated directly into a downstream classification task or as a pre-processing stage before another fusion method.", "section": "4 Methodology"}, {"figure_path": "XgkrXpHl5j/figures/figures_15_1.jpg", "caption": "Figure 5: Schematic diagram of the electrolytic cell, + (orange) and - (black) represent the charged species (ions and electrodes). There is a boundary b (black line) in the electrolytic cell, assuming that the positive potential is Uo, the negative potential is -U0, and the boundary b is the zero potential.", "description": "This figure is a schematic diagram that illustrates the concept of an electrolytic cell used in the analogy to explain the multimodal feature fusion method. The cell is divided into two parts by a boundary (b), with one side having a positive potential (Uo) and the other a negative potential (-Uo).  The positive and negative ions (charged particles representing features) are distributed within the cell. The boundary (b) represents the point of equilibrium where the net charge is zero, separating modality-invariant features from modality-specific features. An external voltage (also stimulation) is applied, affecting the distribution of charged particles.", "section": "3.3 Modality Feature Dissolution and Concentration"}, {"figure_path": "XgkrXpHl5j/figures/figures_16_1.jpg", "caption": "Figure 6: Representation of ion distribution. The ordinate represents the ion concentration and the abscissa represents the electrolytic cell position. O is the position of the positive electrode, l is the position of the negative electrode, and b is the potential equilibrium boundary. The green line represents a uniform distribution of initial state ions to conform to macroscopic electrical neutrality, the yellow line represents the ideal electrolysis target, that is, the foreign ions are completely divided at the equilibrium boundary, and the red line represents the practically possible situation.", "description": "This figure shows the ion distribution in an electrolytic cell, illustrating the initial uniform distribution (green), ideal separated distribution (yellow), and the actual distribution achieved (red) at equilibrium.  The x-axis represents the position within the cell, and the y-axis represents the ion concentration. The figure helps to visualize the concept of ion separation and concentration used in the GMF method to fuse multimodal features.", "section": "3.3 Modality Feature Dissolution and Concentration"}, {"figure_path": "XgkrXpHl5j/figures/figures_17_1.jpg", "caption": "Figure 7: Example diagram of loop guidance. The modes are excited by each other.", "description": "This figure illustrates the concept of loop guidance in the context of multimodal feature fusion.  The system is represented as a series of electrochemical cells, each containing positively and negatively charged particles representing modality-invariant and modality-specific features respectively.  The movement of these particles is guided by external stimuli, mimicking the effect of different modalities on each other in a real-world scenario. The figure shows how the process of dissociation, concentration, and reconstruction results in the reunification of the features at the opposite end of the loop.", "section": "3.4 Poisson-Nernst-Planck Equation"}, {"figure_path": "XgkrXpHl5j/figures/figures_18_1.jpg", "caption": "Figure 1: Stages of information entropy change. Where Z\u2081 might be a set of vectors ({ ZA, . . ., ZM }) or a vector, depending on the fusion method F(\u00b7), and C(\u00b7) stands for classifier.", "description": "This figure illustrates the three main stages of multimodal learning: feature extraction, fusion, and classification.  In the feature extraction stage, unimodal feature extractors process the input from different modalities (e.g., image, audio, text). The fusion stage combines these extracted features into a unified representation Z, using a fusion method F(\u00b7). Finally, the classification stage employs a classifier C(\u00b7) to map the fused features to the final output Y.  The figure highlights the flow of information and the changes in information entropy at each stage. The entropy reduction is a key objective of multimodal learning, and this figure visually represents how the approach aims to achieve that.", "section": "3 Theory"}, {"figure_path": "XgkrXpHl5j/figures/figures_19_1.jpg", "caption": "Figure 1: Stages of information entropy change. Where Z\u2081 might be a set of vectors ({ ZA, . . ., ZM }) or a vector, depending on the fusion method F(\u00b7), and C(\u00b7) stands for classifier.", "description": "This figure illustrates the three stages of multimodal learning: extraction, fusion, and classification.  In the extraction stage, unimodal feature extractors process individual modality inputs (X(j)) producing features f(j)(X(j),\u03b8(j)).  These features are then fused using a method F(.,\u03b8F) to produce the fused features Z. Finally, a classifier C(.,\u03b8C) is used to produce the final output Y. The figure highlights how information entropy changes throughout these stages, particularly focusing on the reduction of entropy achieved through the fusion stage. This reduction is crucial for improved downstream task performance.", "section": "3 Theory"}, {"figure_path": "XgkrXpHl5j/figures/figures_20_1.jpg", "caption": "Figure 8: Evaluate ResNet, MobileNet and ViT test accuracy and the ratio of test accuracy to training accuracy (denote as ratio) on the CIFAR-10 dataset.", "description": "This figure presents the evaluation results of three different model architectures (ResNet, MobileNet, and ViT) on the CIFAR-10 dataset.  It shows two subplots for each model architecture. The left subplot displays the test accuracy, illustrating the model's performance on unseen data. The right subplot shows the ratio of test accuracy to training accuracy, providing insight into the model's generalization capability (the closer the ratio is to 1, the better the generalization). The x-axis represents different feature dimensions (increased dimensionality means more parameters), while the y-axis represents the test accuracy or the accuracy ratio. This figure is used to illustrate Theorem 3.2, which argues for the existence of an optimal feature dimension for each task. By observing the trends in accuracy and the accuracy ratio across different feature dimensions, the authors aim to showcase how the performance and generalization ability of the models vary.", "section": "C Proof of Theorem 3.2"}, {"figure_path": "XgkrXpHl5j/figures/figures_21_1.jpg", "caption": "Figure 8: Evaluate ResNet, MobileNet and ViT test accuracy and the ratio of test accuracy to training accuracy (denote as ratio) on the CIFAR-10 dataset.", "description": "This figure displays the results of experiments evaluating the impact of feature dimension on the performance of three different model architectures (ResNet, MobileNet, and ViT) trained on the CIFAR-10 dataset.  Two key metrics are presented: test accuracy and the ratio of test accuracy to training accuracy. The ratio provides insight into the model's generalization capability\u2014a ratio closer to 1 indicates better generalization.  The plots show how these metrics change as the feature dimension increases. The results suggest the existence of an optimal feature dimension for each architecture, beyond which performance may plateau or even decrease, highlighting the importance of selecting an appropriate feature dimension to balance model performance and generalization.", "section": "C Proof of Theorem 3.2"}, {"figure_path": "XgkrXpHl5j/figures/figures_23_1.jpg", "caption": "Figure 2: Structure of GMF. The input is taken from f(Xi, 0) and the output is taken as Zi. This is done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be directly used for classification or can be connected to other fusion modules. See Appendix J", "description": "This figure illustrates the Generalized Multimodal Fusion (GMF) method's architecture. It shows how the input features from different modalities are processed in three main stages: dissociation, concentration, and reconstruction.  The dissociation stage separates modality-specific and modality-invariant features. The concentration stage focuses these separated features. Finally, the reconstruction stage combines these features to produce a fused output (Zi) that can be directly used for classification or integrated with other fusion methods.", "section": "4 Methodology"}, {"figure_path": "XgkrXpHl5j/figures/figures_25_1.jpg", "caption": "Figure 2: Structure of GMF. The input is taken from f(Xi, 0) and the output is taken as Zi. This is done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be directly used for classification or can be connected to other fusion modules. See Appendix J", "description": "This figure illustrates the Generalized Multimodal Fusion (GMF) method proposed in the paper.  The GMF method involves three main steps: dissociation, concentration, and reconstruction.  The input to the GMF is the features extracted by unimodal feature extractors f(Xi, 0). The dissociation step separates the features into modality-invariant and modality-specific subspaces. The concentration step focuses the features. Finally, the reconstruction step combines the features.  The resulting GMF output (Zi) can be used directly for classification, or integrated with other modules as a front-end for further processing.", "section": "4 Methodology"}, {"figure_path": "XgkrXpHl5j/figures/figures_26_1.jpg", "caption": "Figure 2: Structure of GMF. The input is taken from f(Xi, 0) and the output is taken as Zi. This is done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be directly used for classification or can be connected to other fusion modules. See Appendix J", "description": "This figure illustrates the Generalized Multimodal Fusion (GMF) method proposed in the paper. The GMF method consists of three main steps: dissociation, concentration, and reconstruction. The input features from different modalities are first disassociated into modality-invariant and modality-specific subspaces.  Then, the modality-invariant features are concentrated, and finally, the disassociated features are reconstructed into a fused feature representation. The fused feature representation, Z<sub>i</sub>, can be directly used for classification tasks or combined with other fusion methods as a front-end module.", "section": "4 Methodology"}]