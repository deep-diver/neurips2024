[{"type": "text", "text": "Generalized Multimodal Fusion via Poisson-Nernst-Planck Equation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Previous studies have highlighted significant advancements in multimodal fusion.   \n2 Nevertheless, such methods often encounter challenges regarding the efficacy of fea  \n3 ture extraction, data integrity, consistency of feature dimensions, and adaptability   \n4 across various downstream tasks. This paper proposes a generalized multimodal fu  \n5 sion method (GMF) via the Poisson-Nernst-Planck (PNP) equation, which adeptly   \n6 addresses the aforementioned issues. Theoretically, the optimization objective for   \n7 traditional multimodal tasks is formulated and redefined by integrating information   \n8 entropy and the flow of gradient backward step. Leveraging these theoretical   \n9 insights, the PNP equation is applied to feature fusion, rethinking multimodal   \n10 features through the framework of charged particles in physics and controlling their   \n11 movement through dissociation, concentration, and reconstruction. Building on   \n12 these theoretical foundations, GMF disassociated features which extracted by the   \n13 unimodal feature extractor into modality-specific and modality-invariant subspaces,   \n14 thereby reducing mutual information and subsequently lowering the entropy of   \n15 downstream tasks. The identifiability of the feature\u2019s origin enables our approach to   \n16 function independently as a frontend, seamlessly integrated with a simple concate  \n17 nation backend, or serve as a prerequisite for other modules. Experimental results   \n18 on multiple downstream tasks show that the proposed GMF achieves performance   \n19 close to the state-of-the-art (SOTA) accuracy while utilizing fewer parameters and   \n20 computational resources. Furthermore, by integrating GMF with advanced fusion   \n21 methods, we surpass the SOTA results. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 The world is inherently multimodal; individuals perceive and integrate diverse sensory inputs to form   \n24 a more comprehensive understanding of their surroundings. Similarly, multimodal learning processes   \n25 inputs from multiple modalities, offering potential applications in complex downstream tasks such as   \n26 cross-modal retrieval and multi-modal classification. Nevertheless, features from different modalities   \n27 often differ significantly, even when describing the same event [1, 2]. Consequently, fusing features   \n28 from different modalities is challenging, requiring a dedicated fusion phase before being applied in   \n29 tasks, bridging the semantic gap between different modalities is crucial for valid feature fusion.   \n30 Theoretical works on multimodal fusion have proposed more generalized schemes. MBT [3] ex  \n31 changes mutual information between different modalities to enhance understanding. Perceiver [4]   \n32 stacks various features and extracts fusion features from transformer blocks to condense task-related   \n33 features. Uni-Code [2] distinguishes between modality-invariant and modality-specific features,   \n34 optimizing feature utilization. Moreover, in downstream tasks, innovative fusion methods are applied.   \n35 MAP-IVR [5] considered that image features belong to the subset of video features, UAVM [6] fuses   \n36 different modalities using an independent fusion block.   \n37 Although existing methods for feature fusion show considerable improvements, they often rely on   \n38 several incomplete assumptions: 1)Feature dimension consistency: Feature dimensions across   \n39 different modalities are perfectly aligned [7, 8], leading to inefficient representations, thus impairing   \n40 model performance; 2)Data reliability: In reality, poor quality data (e.g. missing modalities) directly   \n41 degrades performance [9, 10], even though datasets are assumed to be complete; 3)Downstream   \n42 task applicability: Feature fusion requirements are uniform across different tasks, but matching   \n43 tasks [11, 12, 13, 14, 5] require modality-invariant features (common to all modalities), whereas   \n44 detection tasks [15, 16] necessitate modality-specific features (specific to each modality) additionally;   \n45 4)Feature extraction effectiveness: Loss function in feature fusion does not affect the feature   \n46 extractor\u2019s gradients [17, 18] (See Appendix A), often results in feature extractor homogenization [17],   \n47 deteriorating performance in downstream tasks [1]. Furthermore, the fixed quantity of modal features   \n48 often limit the generalizability of proposed fusion methods [2].   \n49 This paper introduces a generalized multimodal fusion method (GMF) that operates independently   \n50 of the usual constraints. We formulate the learning objectives for traditional multimodal tasks and   \n51 propose new definitions based on information entropy theory [19, 20]. Taking inspiration from the   \n52 Poisson-Nernst-Planck equation (PNP) [21], treating features as charged particles to disassociate them,   \n53 employing GMF for multimodal feature fusion. Leveraging the principles of the PNP equation, GMF   \n54 orchestrates the guided migration of features within a high-dimensional space, segregating modality  \n55 invariant from modality-specific features within the disassociated feature landscape, reducing the   \n56 mutual information between features further decreases the relevant entropy of downstream tasks.   \n57 Specifically, the proposed method incorporates a reversible feature dissociation-concentration step   \n58 and applies reasonable regional constraints to the reconstruction gradient, emphasizing the connection   \n59 between the feature extractor and the loss of a downstream task, enabling GMF to generalize   \n60 effectively and serve as the frontend for other fusion modules. We evaluated our method on multiple   \n61 datasets across specific downstream tasks. It consistently demonstrated significant performance and   \n62 generalization capabilities. In summary, our contributions are as follows:   \n63 (1) We propose a novel theory for multimodal feature fusion based on the Poisson-Nernst-Planck   \n64 equation and information entropy with an exhaustive proof, demonstrating its effectiveness   \n65 through theoretical analysis and preliminary experiments.   \n66 (2) We have devised a generalized feature fusion method GMF, grounded in entropy theory and the   \n67 PNP equation, which stands independent of both feature extractors and downstream tasks.   \n68 (3) Experiments demonstrate that GMF achieves comparable performance to SOTA with fewer   \n69 computational demands and parameters, while also showing robustness to missing modalities.   \n70 Moreover, when integrated with advanced fusion methods, its performance and robustness are   \n71 notably enhanced, surpassing SOTA and ensuring greater reliability in real-world applications. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "72 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "73 Innovative advancements in multimodal fusion methods, both theoretically [2] and structurally [4],   \n74 have significantly propelled the progress of generalized multimodal tasks (denote as GMTs). Some   \n75 SOTA methods focusing on downstream tasks propose fusion methods specifically tailored for   \n76 them. However, the fusion challenges vary with the diversity of downstream tasks. In this paper,   \n77 we categorize multimodal tasks into two types: Native Multimodal Tasks (denote as NMTs) and   \n78 Extended Multimodal Tasks (denote as EMTs), based on whether corresponding single-modal tasks   \n79 exist. Specifically, cross-modal retrieval and matching tasks such as Image-Video retrieval [14, 5]   \n80 and Image-Text matching [12, 13, 11] usually belong to NMT and only require the similarity of   \n81 modalities. For example, CLIP [22] transforms the image classification task into an image-text   \n82 retrieval task, achieving stunning zero-shot performance. Multi-modal classification, recognition, and   \n83 detection tasks such as emotion recognition [16] and event classification [6] usually belong to EMT.   \n84 Different modalities often have inconsistent perspectives, and fully aligned features will affect the   \n85 performance of such tasks.   \n86 To illustrate the generalization capabilities of these methods and their impact on downstream tasks,   \n87 Tab 1 is presented. The \"Type\" column categorizes methods by GMT support. \"Align.\" indicates   \n88 feature alignment across modalities. \"Grad. Ref.\" assesses if fusion affects feature extractor gradients.   \n89 \"Gene.\" denotes uniformity of fusion requirements across tasks. \"Avail.\" indicates handling of missing   \n90 modalities during inference. Lastly, \"Complexity\" reflects computational complexity regarding (n)   \n91 modalities. Perceiver [4] does not report multimodal correlation experiments.   \n92 It is worth noting that the evaluation of gradient correlation is simply whether there is an explicit   \n93 excitation of the loss function. Some downstream methods introduce ways such as concat (e.g.,   \n94 classifier of AVoiD-DF [15]) in the classification stage, and the modal missing adaptation in the fusion   \n95 stage does not represent the adaptation for this task. In addition, for NMTs, the complete modal input   \n96 is necessary, so the conclusion of this part is \"-\"; Here, the complexity takes the highest value, which   \n97 does not represent the final computation cost. (e.g., the disentangled loss of MISA [16] is $O(n^{2})$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/e7ddc7531e414adbe93f79cea4e679f2fec5b74e73e87b0d32b21e30ef98fef3.jpg", "table_caption": ["Table 1: Comparison of multimodal method proposed in the fusion phase. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "98 3 Theory ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "99 In this subsection, we briefly introduce the notation system used in this paper and the general structure   \n100 of multimodal tasks, representing the information entropy at different stages of multimodal learning.   \n101 After that, we generalize the information entropy to multi-modality and redefine the entropy reduction   \n102 objective for multi-modal learning. Finally, we evaluate the impact of linear dimension mapping on   \nthe performance of downstream tasks and present the preamble theorem. ", "page_idx": 2}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/f78d78b38828f12ffb67dbdf0530dfafeac2ea6e17724dd61c57fa2eabf48932.jpg", "img_caption": ["Figure 1: Stages of information entropy change. Where $Z_{i}$ might be a set of vectors $(\\{Z_{i}^{A},\\ldots,Z_{i}^{M}\\})$ or a vector, depending on the fusion method $\\mathbf{\\bar{\\rho}}_{F\\left(\\cdot\\right)}$ , and $C(\\cdot)$ stands for classifier. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "103 104 3.1 Formulation and Traditional Objective Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "105 Consider inputs with $d$ modalities, where $j\\,\\in\\,\\{1,2,\\ldots,d\\}$ represents different modalities. Ex  \n106 amine a dataset comprising $n$ samples. Let the input be $\\bar{X^{\\widehat{\\mathbf{\\alpha}}}}=\\{X_{1},X_{2},\\ldots,X_{n}\\}$ , where a   \n107 specific sample i \u2208 {1, 2, . . . , n} is represented as Xi = {Xi(1), Xi(2), . . The output   \n108 is $Y~=~\\{Y_{1},Y_{2},\\ldots,Y_{n}\\}$ , and each $\\{X_{i},Y_{i}\\}$ forms a sample pair. $X_{i}^{(j)}$ represents the orig  \n109 inal sample of modality $j$ with varying shapes, while the shape of $Y_{i}$ depends on the specific   \n110 datasets and downstream tasks. For each modality $j$ , specific feature extractors $f^{(j)}(\\cdot,\\theta^{(\\bar{j})})$ and   \n111 parameters $\\theta^{(j)}$ are employed for feature extraction. The fused features capturing multimodal   \n112 interactions for sample $i$ are denoted as $Z_{i}~=~\\{Z_{i}^{(1)},Z_{i}^{(2)},\\cdot\\cdot\\cdot\\,,Z_{i}^{(d)}\\}$ . The set of global fea  \n113 tures is expressed as $f(X,\\theta)\\;=\\;[f^{(1)}(X^{(1)},\\theta^{(1)});f^{(2)}(X^{(2)},\\theta^{(2)});\\ldots;f^{(d)}(X^{(d)},\\theta^{(d)})]$ , where   \n114 $\\theta=\\{\\theta^{(1)},\\bar{\\theta}^{(2)},\\dots,\\theta^{(\\dot{d})}\\}$ .   \n115 The multimodal task is depicted in Figure 1, delineating three key parameters: the feature extractor   \n116 $\\theta$ , fusion parameter $\\theta^{F}$ , and classifier parameter $\\theta^{C}$ . Optimization of these parameters aims at   \n117 maximizing performance. Regarding entropy, $F(\\cdot)$ represents the fused mapping, extending the   \n118 learning objective from feature extraction to fusion: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta,\\theta^{F}}\\{H(F(f(X,\\theta),\\theta^{F})\\mid F(f(X,\\theta))\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "119 Similarly, we employ $C(\\cdot)$ to represent the mapping for downstream tasks and generalize it to embody   \n120 the learning objective fused with downstream tasks: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta,\\theta^{F},\\theta^{C}}\\{H(Y\\mid C(F[f(X,\\theta),\\theta^{F}],\\theta^{C})])\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "121 In Eq. ( 2), these parameters are optimized by downstream task losses. If there is a loss in the fusion   \n122 stage, then it optimizes the parameters in Eq. ( 1). ", "page_idx": 3}, {"type": "text", "text": "123 3.2 Information Entropy and Objective Redefinition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "124 Feature extraction through dimensionality reduction involves reducing data uncertainty [19], as   \n125 quantified by information entropy $H$ . In Figure 1, we show a simplified approach to single-modal   \n126 learning. The feature extractor and classifier (dotted arrow) directly minimize the information   \n127 entropy of both the input $X_{i}^{(j)}$ and the output $Y_{i}$ by adjusting the parameters of the feature extractor   \n128 $f^{(j)}(\\cdot,\\theta^{(j)})$ and the classifier $C^{(j)}(\\cdot,\\theta^{C^{(j)}})$ for modality $j$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta^{(j)},\\theta^{C}}H[Y_{i}|C^{(j)}(f^{(j)}(X^{(j)},\\theta^{(j)}),\\theta^{C^{(j)}})]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 This process, facilitated by feature extractors, condenses data samples into a feature space, preserving   \n130 pertinent attributes for downstream tasks. Think loss as stimulation of entropy reduction, maximize   \n131 mutual information about related features [18]. Expanding to the multimodal fusion stage, the   \n132 objective is to minimize the entropy of the fused features compared to the sum of the entropy of   \n133 each input feature. In the context of multimodal fusion, where outputs from disparate modalities are   \n134 integrated post-feature extraction, the total information entropy of the system can be estimated using   \n135 the joint entropy formula, and for constant $X$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nH(f(X,\\theta))=\\sum_{j=1}^{d}H(f^{(j)}(X^{(j)},\\theta^{(j)}))-\\underbrace{I(f(X,\\theta))}_{\\mathrm{Mutual\\,information}}\\Longrightarrow\\operatorname*{min}_{\\theta}H(f(X,\\theta))\\Leftrightarrow\\operatorname*{max}_{\\theta}I(f(X,\\theta)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "136 Downstream objectives are typically structured to minimize mutual information, consequently leading   \n137 to a reduction in entropy. However, in fusion stage, disparities observed among the equations (1),   \n138 (2), and (3) suggest that certain fusion-method might not establish a straightforward correspondence   \n139 between network inputs and outputs. Achieving complete consistency between modalities, where   \n140 mutual information is zero, may not always lead to optimal outcomes [1, 20], potentially increasing   \n141 entropy in downstream task-related features [17]. This observation is substantiated by the diminishing   \n142 performance of certain multimodal methods [3, 15] compared to earlier unimodal methods, indicating   \n143 a decline in their capacity to extract distinctive features from individual modalities when confronted   \n144 with the absence of certain modalities. Thus, optimization objectives for multimodal tasks should   \n145 balance minimizing entropy during fusion with maintaining or reducing entropy in downstream   \n146 task-related features. This highlights the necessity of aligning deep learning tasks with downstream   \n147 objectives and minimizing information entropy when designing loss functions for these tasks.   \n148 Theorem 3.1: The overarching objective of multimodal tasks lies in minimizing entropy during the   \n149 fusion stage without amplifying the entropy of downstream task-related features: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta,\\theta^{F},\\theta^{C}}{\\operatorname*{min}}\\{H(Y\\mid C(F[f(X,\\theta),\\theta^{F}],\\theta^{C})])\\}}\\\\ &{\\mathrm{s.t.}\\quad\\forall j\\in\\{1,2,\\ldots,d\\},\\quad\\theta^{(j)}\\in\\arg\\underset{\\theta^{(j)}}{\\operatorname*{min}}H(Y|f^{(j)}(X^{(j)},\\theta^{(j)}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "150 Some approaches introduce the fused results as residuals, which demonstrate a certain degree of   \n151 improvement, and this theory provides a better rationale for such enhancement. However, given that   \n152 the forward pass necessarily involves the operation of $F(\\cdot)$ , it becomes challenging to fully meet this   \n153 precondition. During gradient backward, the loss incurred during the fusion stage for the feature   \n154 extractor should align with the loss of the downstream task or be zero. ", "page_idx": 4}, {"type": "text", "text": "155 3.3 Modality Feature Dissolution and Concentration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "156 Adding too many parameters, or overcharacterization, can improve the model\u2019s ability to fti the data,   \n157 acting like a parameterized memory function [23]. However, it\u2019s important to balance this with the   \n158 amount of data available for the next task to prevent learning too much noise and overftiting [7]. On   \n159 the other hand, having too few parameters may weaken the model\u2019s ability to represent complex   \n160 patterns, resulting in lower performance across different methods (See Appendix C).   \n161 Theorem 3.2: The dimension of the feature that is best suited to the downstream task varies, and   \n162 there is always an optimal value for this feature. The dimension multiple relationship between each   \n163 layer of the feature extractor is fixed, and the initial dimension is adjusted. Too low dimension of the   \n164 final output will lead to inefficient representation, and too high dimension will introduce noise. The   \n165 existence of an integer $l_{\\mathrm{best}}$ such that for any integer $l$ distinct from $l_{\\mathrm{best}}$ , the conditional entropy of   \n166 the model\u2019s predictions $f_{l}(X,\\theta_{l})$ is greater than that of the model\u2019s predictions $f_{l_{\\mathrm{best}}}(X,\\theta_{l_{\\mathrm{best}}})$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\exists l_{\\mathrm{best}}\\in\\mathbb{N},\\forall l\\in\\mathbb{N},l\\neq l_{\\mathrm{best}},H(Y|f_{l}(X,\\theta_{l}))>H(Y|f_{l_{\\mathrm{best}}}(X,\\theta_{l_{\\mathrm{best}}}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 Theorem 3.3: The feature extractor is fixed, and its original output feature dimension $l$ is mapped   \n168 to $n l$ , and finally back to $l$ . The mapping result is used as the basis for the downstream task. The   \n169 performance of downstream tasks is infinitely close to the original performance as $n$ increases, but   \n170 never greater than the original performance. For magnification $n\\,>\\,1,n\\,\\in\\,\\mathbb{Z}$ , mapping matrix   \n171 $\\mathbf{U}_{1}\\in\\breve{\\mathbb{R}}^{l\\times n l}$ and $\\mathbf{U}_{2}\\in\\mathbb{R}^{n l\\times l}$ , For the output features $\\overline{{f(X,\\theta)}}\\in\\mathbb{R}^{l}$ and $Y$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(Y|f(X,\\theta))<H(Y|\\mathbf{U}_{1}\\cdot(\\mathbf{U}_{2}\\cdot f(X,\\theta)))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 ", "page_idx": 4}, {"type": "equation", "text": "$$\nl i m_{n\\to\\infty}H(Y|\\mathbf{U}_{1}\\cdot(\\mathbf{U}_{2}\\cdot f(X,\\theta))))=H(Y|f(X,\\theta)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "173 Conjecture 3.1: Rely on Theorem 3.1, 3.2, 3.3, we propose an conjecture that a boundary of perfor  \n174 mance limitation exists, determined by downstream-related entropy. Theoretically, by establishing a   \n175 direct correspondence between the extractor and classifier, fusion method can enhance the limitation   \n176 boundary, further improve performance. ", "page_idx": 4}, {"type": "text", "text": "177 3.4 Poisson-Nernst-Planck Equation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "178 The Nernst-Planck equation represents a mass conservation equation that characterizes the dynamics   \n179 of charged particles within a fluid medium. This equation modifies Fick\u2019s law of diffusion to include   \n180 scenarios where particles are also mobilized by electrostatic forces relative to the fluid. The equation   \n181 accounts for the total flux of particle $p\\in\\{+,-\\}$ , denoted as $\\mathbf{J}_{p}$ , of charged particles, encompassing   \n182 both diffusion driven by concentration gradients and migration induced by electric fields. Since fusion   \n183 features are usually one-dimensional, we only consider the $x$ direction here. For a given charged   \n184 particle $i$ , the equation describes its movement as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{J}_{p}=\\underbrace{-D_{p}\\nabla c_{p}(x,t)}_{\\mathrm{Diffusion}}+\\underbrace{c_{p}(x,t)\\mathbf{v}}_{\\mathrm{Advection}}+\\underbrace{\\frac{D_{p}z_{p}e}{k_{B}T}c_{p}(x,t)\\mathbf{E}}_{\\mathrm{Electromigration}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "185 $p$ is abstracted as elements in the modality-invariant feature and the modality-specific feature. Here,   \n186 $c_{p}(\\boldsymbol{x},t)$ denotes the concentration of particle, while $D_{p}$ (diffusivity of $p$ ), $k_{B}$ (Boltzmann constant),   \n187 $z_{p}$ (valence also electric charge), and $e$ (elementary charge) are constants. $T$ is a hyperparameter,   \n188 represent temperature. $\\mathbf{E}$ represents the electric field of the entire system, and $\\mathbf{v}$ represents the flow   \n189 rate. The Poisson equation describes the relationship between the distribution of a field and the   \n190 potential energy it induces, represented by the expression: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla^{2}\\phi(x)=-\\frac{\\rho}{\\varepsilon_{0}},\\rho=e(z_{+}c_{+}(x,t)+z_{-}c_{-}(x,t))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "191 $\\phi$ signifies the potential, considered as an external excitation, $\\varepsilon_{0}$ represent dielectric constant. By   \n192 integrating the relationship between the concentration of charged particles and the electromigration   \n193 term in the Poisson equation, we derive the Poisson-Nernst-Planck (PNP) equation. Assuming that   \n194 the dissociation process approaches equilibrium, for feature elements without magnetic field and flow   \n195 velocity, we can consider the time-dependent change in concentration $c_{p}(\\boldsymbol{x},t)$ of the charged particle   \n196 $i$ over time $t$ is negligible: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial c_{p}(x,t)}{\\partial t}=D_{p}(\\frac{\\partial^{2}c_{p}(x,t)}{\\partial x^{2}}-\\frac{z_{p}e F}{k_{B}T\\epsilon_{0}}c_{p}(x,t)(z_{+}c_{+}(x,t)+z_{-}c_{-}(x,t)+\\frac{z_{p}e}{k_{B}T}\\frac{\\partial c_{p}(x,t)}{\\partial x}\\frac{d\\phi(x)}{d x})\\approx0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "197 When the final state is stable, a sufficiently large 1D electrolytic cell of length $l$ , at the potential   \n198 equilibrium boundary $b$ , it can be equivalent to (See Appendix B): ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\phi(0)-\\phi(b))-e\\int_{0}^{b}c_{-}(x,t)z_{-}d x\\approx e\\int_{0}^{l}c_{+}(x,t)z_{+}-c_{-}(x,t)z_{-}d x\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "199 In this context, $\\phi(x)$ represents an external influence from another modality feature. We assume that   \n200 modality-invariant feature elements have a positive charge, while modality-specific feature elements   \n201 have a negative charge. The difference $\\phi(0)-\\phi(b)$ indicates the enrichment potential of modality  \n202 invariant feature elements for the excitation modality. This potential attracts modality-specific feature   \n203 elements in dissociated modality towards dissociation.   \n204 Theorem 3.4: Following dissociation and Theorem3.3, in line with the principles of matter and   \n205 information conservation, the excitation and attraction features can revert back to their original state.   \n206 A cyclic feature electrolytic cell is generalized, using a loss function as stimulation: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{Z}_{i}^{(j)}={\\bf U}_{d i s}f^{(j)}(X_{i}^{(j)},\\theta^{(j)})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=||\\mathbf{U}_{c o n}^{(j)}[\\hat{Z}_{i}^{(j)}(1:b^{j});\\hat{Z}_{i}^{(j+1)}(b^{(j+1)}+1:n l^{(j+1)})]-f^{(j)}(X_{i}^{(j)},\\theta^{(j)})||^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "208 $\\mathcal{L}$ is loss function. $l^{(j)}$ and $b^{(j)}$ are feature dimension and dissociation boundary of modality $j$ ,   \n209 respectively. Around this boundary, features are explicitly distinguished. The mapping matrix   \n210 $\\mathbf{U}_{d i s}^{(j)}\\in\\mathbb{R}^{n\\dot{l}^{(j)}\\times l^{(j)}}$ , $\\mathbf{U}_{c o n}^{(j)}\\,\\in\\,\\mathbb{R}^{l^{(j)}\\times\\big(\\!n l^{(j+1)}+b^{(j)}-b^{(j+1)}\\big)}$ is learnable. $\\hat{Z}_{i}^{(j)}\\in\\mathbb{R}^{n l^{(j)}}$ is the result of   \n211 $f^{(j)}(X_{i}^{(j)},\\theta^{(j)})\\in\\mathbb{R}^{l^{(j)}}$ being linearly mapped (dissolved) into a higher dimensional space. ", "page_idx": 5}, {"type": "text", "text": "212 4 Methodology ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "213 Set the dissociation boundary $b^{(j)}$ and feature dimension $l^{(j)}$ of modality $j$ . The feature with the   \n214 smallest dimension is denoted as $l^{*}$ . The feature dimension of the dissociation is $n l^{(j)}$ , with a uniform   \n215 magnification of $n>2$ .   \n216 Combining information entropy theory with the PNP equation, we propose GMF method to optimize   \n217 fusion feature mutual information on the premise of maintaining the downstream task related infor  \n218 mation of input features. Following Assumption3.1, GMF has only four learnable matrices for each   \n219 modality, enforces correlations without complex structure, as shown in Fig 2.   \n220 GMF is divided into three stages, for each modality $j$ , applying different learnable mapping ma  \n221 trices: dissolve matrix P(dji)s $\\mathbf{P}_{d i s}^{(j)}\\,\\in\\,\\mathbb{R}^{n l^{(j)}\\times l^{(j)}}$ ), concentrate matrix P(cji)nv $\\mathbf{P}_{c i n v}^{(j)}\\,\\in\\,\\mathbb{R}^{b^{(j)}\\times l^{*}}$ and $\\mathbf{P}_{c s p e c}^{(j)}\\;\\in$   \n222 (nl(j)\u2212b(j))\u00d7l(j), reconstruct matrix P(rje)con \u2208Rl(j)\u00d7(l(j)+l\u2217). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nZ_{i}=\\mathrm{GMF}(f(X_{i},\\theta),\\theta^{G M F}),\\:\\:\\:\\theta^{G M F}=\\{\\mathbf{P}_{d i s}^{(j)},\\mathbf{P}_{c i n v}^{(j)},\\mathbf{P}_{c s p e c}^{(j)},\\mathbf{P}_{r e c o n}^{(j)}\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "223 First, to make sure the features move, we map (dissolve) them to higher dimensions. Next, for the   \n224 feature of each modality, after dimension elevation, the goal is explicitly divided as specific and   \n225 invariant by abstracting different kinds of features into positive and negative charged particles: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{Z}_{i}^{(j)}={\\bf P}_{d i s}^{(j)}(f^{(j)}(X_{i}^{(j)},\\theta^{(j)})),\\quad(\\hat{Z}_{i}^{(j)})_{i n v}=\\hat{Z}_{i}^{(j)}(1:b^{(j)}),\\quad(\\hat{Z}_{i}^{(j)})_{s p e c}=\\hat{Z}_{i}^{(j)}(b^{(j)}+1:n l^{(j)})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "226 $f^{(j)}(\\boldsymbol{X}_{i}^{(j)},\\boldsymbol{\\theta}^{(j)})\\in\\mathbb{R}^{l^{(j)}}$ , and $\\hat{Z}_{i}^{(j)}\\in\\mathbb{R}^{n l^{(j)}}$ . Referencing Eq.( 4), irrespective of the initial length   \n227 $l^{(j)}$ of a feature, partitioning it into invariant $(Z_{i}^{(j)})_{i n v}\\ \\in\\ \\mathbb{R}^{l^{*}}$ and specific $(Z_{i}^{(j)})_{s p e c}\\:\\in\\:\\mathbb{R}^{l^{(j)}}$   \n228 components aims to minimize output feature dimensions, thereby mitigating entropy disturbance.   \n229 After concentrate, finally, the output $Z_{i}^{(j)}\\in\\mathbb{R}^{(l^{(j)}+l^{*})}$ is obtained: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(Z_{i}^{(j)})_{i n v}=\\mathbf{P}_{c i n v}^{(j)}(\\hat{Z}_{i}^{(j)})_{i n v},\\quad(Z_{i}^{(j)})_{s p e c}=\\mathbf{P}_{s p e c}^{(j)}(\\hat{Z}_{i}^{(j)})_{s p e c},\\quad Z_{i}^{(j)}=[(Z_{i}^{(j+1)})_{i n v};(Z_{i}^{(j)})_{s p e c}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/d963298b2418ffbeab7b0dc9e8e60cda904ac2ddb0a7512a8bf1ab3f90a2f1ab.jpg", "img_caption": ["Figure 2: Structure of GMF. The input is taken from $f(X_{i},\\theta)$ and the output is taken as $Z_{i}$ . This is done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be directly used for classification or can be connected to other fusion modules. See Appendix J "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "230 Eventually the entire system can be restored to its original state. A loss function is given as an   \n231 external incentive to force the features to move in different directions. Following the Theorem3.4, we   \n232 use P(rje)c $\\mathbf{P}_{r e c o n}^{(j)}$ to map the features back to $f^{(j)}(\\ensuremath{\\boldsymbol{X}}_{i}^{(j)},\\theta^{(j)})$ and apply the disassociation loss. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{d i s}=\\sum_{j=1}^{d}||(f^{(j)}(X_{i}^{(j)},\\theta^{(j)})-\\mathbf{P}_{r e c o n}^{(j)}Z_{i}^{(j)}||^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "233 ", "page_idx": 6}, {"type": "text", "text": "234 5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "235 In this section we briefly introduce the experimental dataset, evaluation metrics, implementation   \n236 details, experimental results and analysis. Our evaluation focuses on solving the limitations mentioned   \n237 in Section 1 and verifying our theory and hypothesis, so we pay more attention to the fusion   \n238 performance under the same feature extraction ability. ", "page_idx": 6}, {"type": "text", "text": "239 5.1 Datasets and experimental tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "240 We performed the NMT task for image-video retrieval on ActivityNet [24] dataset and the EMT task   \n241 for audio-video event classification on VGGSound [25] and deepfake detection on FakeAVCeleb [26],   \n242 and compared the NMT, EMT and GMT methods (as defined in the Related Work) respectively. We   \n243 conduct three sets of comparison experiments:   \n244 (1) Input the same features to simulate the freezing of the feature extractor, and evaluate the entropy   \n245 reduction effect of the fusion method on the existing information.   \n246 (2) Complete the training of the whole model including the same feature extractor, and evaluate the   \n247 impact of the fusion method on the gradient of the feature extractor.   \n248 (3) Select a set of method-specific feature extractors to test the limitation performance.   \n249 For EMTs, VGGSound dataset evaluate (1) and $(2)^{1}$ , the evaluation metric is the classification   \n250 accuracy $\\operatorname{ACC}(\\%)$ . FakeAVCeleb dataset evaluate (3), due to the imbalance of data samples, the   \n251 evaluation focuses on the Area under the Curve of ROC (AUC). For NMTs, ActivityNet dataset   \n252 evaluate (4), the evaluation metric is the matching accuracy mAP, $\\scriptstyle{\\mathrm{mAP}}\\,\\ @\\,n$ represents that the   \n253 matching task target is selected from $n$ samples.   \n255 For the methods proposed in different papers, we only compare the fusion structures except feature   \n256 extractor and classifier. During the evaluation, we set $n$ to 4 and $b^{(j)}$ to always be $\\frac{1}{2}$ of $l^{(j)}$ . All   \n257 experiments were performed on a single $\\mathrm{RTX}4090@2.64\\mathrm{GHz}$ , the CPU for testing the inference   \n258 time is R $\\mathrm{:9\\5900X@4.5GHz}$ , and the random seed was fixed to \u20191\u2019 except for dropout proposed   \n259 by baseline and some transformer [27]-based methods. There was no data augmentation (such as   \n260 cropping, rotation) or introduction of any pre-training parameters in the data preprocessing process.   \n261 See the Appendix G for details of the training parameters.   \n262 The baseline of the multi-modal is all the direct connection of the features of the output of the   \n263 single-modal baseline. GMF stands for simple connection as the back-end. \"G-method name\" stands   \n264 for GMF as a front-end for the method, See Figure 13 in Appendix for the detailed structure. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "265 5.3 Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "266 For EMTs, our experiments, detailed in Table 2 and conducted on the VGGSound dataset [25],   \n267 employ $\\mathsf{R}(2{+}1)\\mathsf{D}$ [28] as the video feature extractor and ResNet-18 [29] as the audio feature extractor.   \n268 The \u2019Training Extractor\u2019 label indicates trainable parameters, while \u2019Frozen Extractor\u2019 denotes   \n269 fixed parameters. Columns \u2019A\u2019 and \u2019V\u2019 represent audio and video inputs, respectively, while a   \n270 value of $\\because0^{\\circ}$ for the other modality input indicates its absence. For trainable feature extractors, we   \n271 introduce additional columns \u2019A(uni)\u2019 and \u2019V(uni)\u2019 to evaluate the direct use of extracted features for   \n272 classification, thereby assessing feature extraction efficacy.   \n273 UAVM [6] emphasizes unified expression, highlighting the importance of modality absence. In   \n274 contrast, AVoiD-DF [15] and MBT [3] prioritize exchanging feature semantics, making them par  \n275 ticularly sensitive to missing modalities; MBT further distinguishes itself through the incorporation   \n276 of bottlenecks. Notably, DrFuse [8] and MISA [16] marginally outperform our method, possibly   \n277 due to the abundance of learnable cross-modal parameters enabled by their self-attention modules,   \n278 which also magnifies the impact of modality absence. Perceiver [4], characterized by stacked features   \n279 without explicit modal differentiation, is notably susceptible to missing modalities. In cases where   \n280 the feature extractor is trainable, the impact of modality absence becomes more pronounced. At this   \n281 juncture, this influence arises not only from modal fusion but also from the homogenization of features   \n282 extracted by the feature extractor. GMF stands out for its minimal parameters and computational load,   \n283 yet it achieves competitive performance while significantly reducing sensitivity to modality absence.   \n284 This remarkable trait can be harnessed by integrating it with other methods, imparting them with   \n285 similar characteristics. This integration leads to performance enhancement and decreased sensitivity   \n286 to modality absence, showcasing the versatility and applicability of GMF.   \n287 For NMTs, our performance report on the ActivityNet [24] dataset is presented in Table 3. To be   \n288 fair, we utilize the features same as AP-IVR [14] (4096-dimensional for video, 128-dimensional for   \n289 images) as input. We map image features to 4096 dimensions or video features to 128 dimensions.   \n290 Three combinations are obtained: Image-Video feature dimensions are (1) 128-4096 (denoted as   \n291 128-4096), (2) 4096-4096 (denoted as 4096), and (3) 128-128 (denoted as 128).   \n292 We employ CLIP [22] as the baseline, which only requires computing cosine similarity of mapped   \n293 features without introducing parameters. METER [13] introduces the cross-attention module on this   \n294 basis, but the improvement is limited due to the sparse features. MAP-IVR [5] employs fixed-length   \n295 mappings, while Perceiver [4] inputs an indistinguishable feature mapping, so the actual number of   \n296 parameters relative to input dimensions is not apparent. GMF achieving competitive performance   \n297 in (128) with minimal additional parameters and computations. Furthermore, the experiments (128-   \n298 4096) demonstrate the necessity of unequal-length fusion, ensuring not only the flexibility of the   \n299 method but also profoundly impacting its performance and additional parameters. In the experiments   \n300 of unequal-length fusion, GMF achieved state-of-the-art performance. Given that GMF is composed   \n301 of linear layers, an increase in input dimensionality leads to an escalation in parameter count.   \n302 We performed a theoretical performance evaluation on FakeAVCeleb [26], as shown in Table 4. We   \n303 use a feature extractor that is more compatible with the proposed method and remove the linear layer,   \n304 denote as GMF-MAE (in Appendix, Fig. 14). For other SOTA methods involved in the comparison,   \n305 we choose the feature extractor proposed in the original paper as much as possible (MISA utilizes   \n306 sLSTM [30], UAVM adopts ConvNeXT-B [31], GMF-MAE employs MAE [32, 33]). The remaining   \n307 methods, including Baseline employs $\\mathrm{R}(2{+}1)\\mathrm{D}{-}18$ [28]. Due to the imbalance in the dataset, with   \n308 a ratio of approximately 1:39, the audio ratio is 1:1 and the video ratio is 1:19. UAVM [6] learns a   \n309 unified representation, thus the easier classification of audio significantly impacts the overall results.   \n310 Both DrFuse [8] and MISA [16] perform below our expectations; one potential explanation could be   \n311 the influence of sample imbalance on their performance.   \n312 The performance of GMF remains consistent with the conclusions drawn from Table 2. Furthermore,   \n313 GMF\u2019s insensitivity to missing modalities effectively mitigates the impact of sample imbalance,   \n314 avoiding an excessive emphasis on any particular modality. The combination of GMF and MAE [32,   \n315 33] demonstrates optimal performance limits, validating our approach\u2019s effectiveness in addressing   \n316 the challenges posed by downstream tasks. We provide a more comprehensive comparison with   \n317 methods focused on deepfake detection in Table 7 (in Appendix). ", "page_idx": 7}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/622cf4ef3c6d0a9e9420f0e695b781e756d06232e150622268c2c421ef2b6833.jpg", "table_caption": ["Table 2: Comparison of EMTs and GMTs methods on VGGSound. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/9b30b89dfe3cf1f2f8b1bbfd6daed9b6ac98e686dbae865a675d58e6479e87d4.jpg", "table_caption": ["Table 3: Comparison of NMTs and GMTs methods on ActivityNet. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/5f7e9a14a3219ab45f238eb14ca34a0df86520b8fe2242f9f031b5444fb191ab.jpg", "table_caption": ["Table 4: Comparison of fusion methods based on different feature extractors on FakeAVCeleb. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "318 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "319 In this paper, we combine the PNP equation with information entropy theory to introduce a multimodal   \n320 fusion method for unrelated input features and downstream task features. The aim is to reduce the   \n321 joint entropy of input features while decreasing the downstream task-related information entropy.   \n322 Experimental results demonstrate that the proposed method takes a step forward in the generalization   \n323 and robustness of multimodal tasks. Meanwhile, the additional burden can be negligible.   \n324 GMF comprises basic linear layers and is consequently susceptible to the inherent characteristics of   \n325 linear operations, which exhibit growth in parameter count relative to input dimensionality. However,   \n326 as per our theoretical framework, the effective component is proportional to the feature dimension. In   \n327 forthcoming research, we intend to concentrate on sparsifying mapping matrices to further diminish   \n328 parameter count. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "329 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "330 [1] Liang, V. W., Y. Zhang, Y. Kwon, et al. Mind the gap: Understanding the modality gap in   \n331 multi-modal contrastive representation learning. In Advances in Neural Information Processing   \n332 Systems, pages 17612\u201317625. 2022.   \n333 [2] Xia, Y., H. Huang, J. Zhu, et al. Achieving cross modal generalization with multimodal unified   \n334 representation. In Conference and Workshop on Neural Information Processing Systems, pages   \n335 63529\u201363541. 2023.   \n336 [3] Nagrani, A., S. Yang, A. Arnab, et al. Attention bottlenecks for multimodal fusion. In   \n337 Conference and Workshop on Neural Information Processing Systems, pages 14200\u201314213.   \n338 2021.   \n339 [4] Jaegle, A., F. Gimeno, A. Brock, et al. Perceiver: General perception with iterative attention. In   \n340 International Conference on Machine Learning. 2021.   \n341 [5] Liu, L., J. Li, L. Niu, et al. Activity image-to-video retrieval by disentangling appearance and   \n342 motion. In Association for the Advancement of Artificial Intelligence. 2021.   \n343 [6] Gong, Y., A. H. Liu, A. Rouditchenko, et al. Uavm: Towards unifying audio and visual models.   \n344 IEEE Signal Processing Letters, pages 2437\u20132441, 2022.   \n345 [7] Ying, X. An overview of overfitting and its solutions. Journal of Physics: Conference Series,   \n346 page 022022, 2019.   \n347 [8] Yao, W., K. Yin, W. K. Cheung, et al. Drfuse: Learning disentangled representation for clinical   \n348 multi-modal fusion with missing modality and modal inconsistency. In Association for the   \n349 Advancement of Artificial Intelligence, pages 16416\u201316424. 2024.   \n350 [9] Ma, M., J. Ren, L. Zhao, et al. Are multimodal transformers robust to missing modality? In   \n351 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.   \n352 [10] Wang, H., Y. Chen, C. Ma, et al. Multi-modal learning with missing modality via shared-specific   \n353 feature modelling. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n354 pages 15878\u201315887. 2023.   \n355 [11] Kim, W., B. Son, I. Kim. Vilt: Vision-and-language transformer without convolution or region   \n356 supervision. In International Conference on Machine Learning. 2021.   \n357 [12] Li, J., R. R. Selvaraju, A. D. Gotmare, et al. Align before fuse: Vision and language representa  \n358 tion learning with momentum distillation. In NeurIPS, pages 9694\u20139705. 2021.   \n359 [13] Dou, Z.-Y., Y. Xu, Z. Gan, et al. An empirical study of training end-to-end vision-and-language   \n360 transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n361 18145\u201318155. 2021.   \n362 [14] Xu, R., L. Niu, J. Zhang, et al. A proposal-based approach for activity image-to-video retrieval.   \n363 In Association for the Advancement of Artificial Intelligence, pages 12524\u201312531. 2020.   \n364 [15] Yang, W., X. Zhou, Z. Chen, et al. Avoid-df: Audio-visual joint learning for detecting deepfake.   \n365 IEEE Transactions on Information Forensics and Security, pages 2015\u20132029, 2023.   \n366 [16] Hazarika, D., R. Zimmermann, S. Poria. Misa: Modality-invariant and -specific representations   \n367 for multimodal sentiment analysis. In ACM International Conference on Multimedia, pages   \n368 1122\u20131131. 2020.   \n369 [17] Wang, F., H. Liu. Understanding the behaviour of contrastive loss. pages 2495\u20132504. 2020.   \n370 [18] Boudiaf, M., J. Rony, I. M. Ziko, et al. A unifying mutual information view of metric learning:   \n371 Cross-entropy vs. pairwise losses. In European Conference on Computer Vision, page 548\u2013564.   \n372 2020.   \n373 [19] Shang, Z. W., W. Li, M. Gao, et al. An intelligent fault diagnosis method of multi-scale deep   \n374 feature fusion based on information entropy. Chinese Journal of Mechanical Engineering, 2021.   \n375 [20] Jiang, Q., C. Chen, H. Zhao, et al. Understanding and constructing latent modality structures in   \n376 multi-modal representation learning. In IEEE/CVF Conference on Computer Vision and Pattern   \n377 Recognition, pages 7661\u20137671. 2023.   \n378 [21] Granada, J. R. G., V. A. Kovtunenko. Entropy method for generalized poisson\u2013nernst\u2013planck   \n379 equations. Analysis and Mathematical Physics, pages 603\u2013619, 2018.   \n380 [22] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural   \n381 language supervision. In International Conference on Machine Learning, pages 8748\u20138763.   \n382 2021.   \n383 [23] Frankle, J., M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.   \n384 In International Conference on Learning Representations. 2019.   \n385 [24] Heilbron, F. C., V. Escorcia, B. Ghanem, et al. Activitynet: A large-scale video benchmark   \n386 for human activity understanding. In 2015 IEEE Conference on Computer Vision and Pattern   \n387 Recognition, pages 961\u2013970. 2015.   \n388 [25] Chen, H., W. Xie, A. Vedaldi, et al. Vggsound: A large-scale audio-visual dataset. In 2020   \n389 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 721\u2013725.   \n390 2020.   \n391 [26] Khalid, H., S. Tariq, M. Kim, et al. Fakeavceleb: A novel audio-video multimodal deepfake   \n392 dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and   \n393 Benchmarks. 2021.   \n394 [27] Vaswani, A., N. M. Shazeer, N. Parmar, et al. Attention is all you need. In Neural Information   \n395 Processing Systems. 2017.   \n396 [28] Tran, D., H. Wang, L. Torresani, et al. A closer look at spatiotemporal convolutions for   \n397 action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern   \n398 Recognition, pages 6450\u20136459. 2018.   \n399 [29] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In 2016 IEEE   \n400 Conference on Computer Vision and Pattern Recognition, pages 770\u2013778. 2016.   \n401 [30] Hochreiter, S., J. Schmidhuber. Long short-term memory. Neural Computation, pages 1735\u2013   \n402 1780, 1997.   \n403 [31] Todi, A., N. Narula, M. Sharma, et al. Convnext: A contemporary architecture for convolutional   \n404 neural networks for image classification. In 2023 3rd International Conference on Innovative   \n405 Sustainable Computational Technologies (CISCT), pages 1\u20136. 2023.   \n406 [32] He, K., X. Chen, S. Xie, et al. Masked autoencoders are scalable vision learners. In Proceedings   \n407 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009.   \n408 2022.   \n409 [33] Huang, P.-Y., H. Xu, J. B. Li, et al. Masked autoencoders that listen. In Advances in Neural   \n410 Information Processing Systems. 2022.   \n411 [34] Kingma, D. P., M. Welling. Auto-encoding variational bayes. In International Conference on   \n412 Learning Representations. 2014.   \n413 [35] Hinton, G. E., R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.   \n414 Science, pages 504\u2013507, 2006.   \n415 [36] Mittal, T., U. Bhattacharya, R. Chandra, et al. Emotions don\u2019t lie: An audio-visual deepfake   \n416 detection method using affective cues. In Proceedings of the 28th ACM International Conference   \n417 on Multimedia, page 2823\u20132832. 2020.   \n418 [37] Nagrani, A., S. Albanie, A. Zisserman. Seeing voices and hearing faces: Cross-modal biometric   \n419 matching. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n420 8427\u20138436. 2018.   \n421 [38] Hu, Y., C. Chen, R. Li, et al. Mir-gan: Refining frame-level modality-invariant representations   \n422 with adversarial network for audio-visual speech recognition. In Proceedings of the 61st Annual   \n423 Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages   \n424 11610\u201311625. 2023.   \n425 [39] Shankar, S., L. Thompson, M. Fiterau. Progressive fusion for multimodal integration, 2024.   \n426 [40] Huang, G., Z. Liu, L. Van Der Maaten, et al. Densely connected convolutional networks. In   \n427 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261\u20132269.   \n428 2017.   \n429 [41] Skoog, D., D. West, F. Holler, et al. Fundamentals of Analytical Chemistry. 2021.   \n430 [42] Nakkiran, P., G. Kaplun, Y. Bansal, et al. Deep double descent: Where bigger models and more   \n431 data hurt. In International Conference on Learning Representations, page 124003. 2020.   \n432 [43] Krizhevsky, A. Learning multiple layers of features from tiny images, 2009.   \n433 [44] Howard, A., M. Sandler, B. Chen, et al. Searching for mobilenetv3. In 2019 IEEE/CVF   \n434 International Conference on Computer Vision, pages 1314\u20131324. 2019.   \n435 [45] Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al. An image is worth 16x16 words: Transformers   \n436 for image recognition at scale. In International Conference on Learning Representations. 2021.   \n437 [46] Han, S., J. Pool, J. Tran, et al. Learning both weights and connections for efficient neural   \n438 network. In Advances in Neural Information Processing Systems. 2015.   \n439 [47] Paszke, A., S. Gross, F. Massa, et al. Pytorch: An imperative style, high-performance deep   \n440 learning library. In Advances in Neural Information Processing Systems, pages 8024\u20138035.   \n441 2019.   \n442 [48] Deng, J., W. Dong, R. Socher, et al. Imagenet: A large-scale hierarchical image database. In   \n443 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. 2009.   \n444 [49] Zhou, Y., S.-N. Lim. Joint audio-visual deepfake detection. In IEEE International Conference   \n445 on Computer Vision, pages 14780\u201314789. 2021.   \n446 [50] Cheng, H., Y. Guo, T. Wang, et al. Voice-face homogeneity tells deepfake. ACM Transactions   \n447 on Multimedia Computing, Communications, and Applications, 2023.   \n448 [51] Mittal, T., U. Bhattacharya, R. Chandra, et al. Emotions don\u2019t lie: An audio-visual deepfake   \n449 detection method using affective cues. In ACM International Conference on Multimedia, page   \n450 2823\u20132832. 2020.   \n451 [52] Zadeh, A., P. P. Liang, N. Mazumder, et al. Memory fusion network for multi-view sequential   \n452 learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence   \n453 and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI   \n454 Symposium on Educational Advances in Artificial Intelligence, page 5634\u20135641. 2018.   \n455 [53] Chugh, K., P. Gupta, A. Dhall, et al. Not made for each other- audio-visual dissonance-based   \n456 deepfake detection and localization. In ACM International Conference on Multimedia, page   \n457 439\u2013447. 2020.   \n458 [54] Hara, K., H. Kataoka, Y. Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and   \n459 imagenet? In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n460 6546\u20136555. 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "462 A Gradient Backward Flow ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "463 A.1 Definition and Explaination ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "464 In the gradient backward stage, the gradient is propagated from the output to the input direction according to the adjustment of the downstream task loss. The specific gradient backward diagram is ", "page_idx": 12}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/e0828d88cb0be84a30f4c2bafac559ad5b6193c19f9ffcc731bfe36e626c85e1.jpg", "img_caption": ["Figure 3: The gradient diagram extended from Figure 1, the notation system is consistent with Figure 1. The blue arrow represents the loss in the fusion stage $(\\mathcal{L}_{f u s i o n})$ , and the red arrow represents the loss in the downstream task $(\\mathcal{L}_{t a s k})$ . The green arrow is related to our redefined optimization objective, and the meaning is consistent with the green dashed arrow in Figure 1. Not all multimodal fusion methods have gradients with blue arrows and green arrows. These are not specific losses, nor are they necessarily individual losses. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "465   \n466 shown in Figure 3. The gradient generated by the downstream task loss is propagated through the   \n467 entire network to the input, and the gradient of the fusion stage loss (if any) is propagated from the   \n468 fusion output feature to the downstream task. The parameter adjustment of the feature extractor is   \n469 affected by the gradient backward of the loss in the fusion stage and the loss of the downstream task.   \n470 It is worth discussing that the gradient adjustments from downstream task classification loss and   \n471 fusion-related loss may not necessarily align. Hence, there typically exists a set of hyperparameters   \n472 to balance the impacts of different losses. For instance, in VAE [34], the KL divergence loss and the   \n473 reconstruction loss serve distinct purposes. The KL divergence loss facilitates model generalization,   \n474 a significant divergence between VAE and AE [35], while the reconstruction loss is task-specific,   \n475 reconstructing a sample from the latent space. However, both the KL divergence loss and the   \n476 reconstruction loss in VAE often cannot simultaneously be zero. The KL divergence loss encourages   \n477 some randomness in the latent space features, whereas the reconstruction loss favors more consistency   \n478 in the latent space features. This balancing act is commendable, yet weighting between the losses   \n479 poses a significant challenge. Hence, when all losses in multi-stage learning bear significance and   \n480 the gradient descent directions of feature extractors are incongruent, balancing a hyperparameter   \n481 becomes necessary to harmonize diverse learning objectives.   \n482 However, not all losses bear significance. Take contrastive loss, for example. It is a downstream   \n483 task loss in some NMT tasks [22], yet in most EMT tasks, contrastive loss typically operates in   \n484 the fusion stage, complementing downstream task-relevant cross-entropy losses, to narrow the gap   \n485 between positive samples in the latent space and push away negative samples. Some studies [1, 20]   \n486 have demonstrated the existence of gaps between modalities, and smaller gaps are not necessarily   \n487 better. There are also analyses of the behavior of contrastive loss [17], aiming to minimize mutual   \n488 information for positive sample pairs and maximize mutual information for negative sample pairs [18].   \n489 In EMT tasks, if positive and negative sample pairs coexist, as in Audio-Visual Deepfake Detec  \n490 tion [15], the contrastive loss in the fusion stage aims to extract consistent information from positive   \n491 sample pairs (representing real samples) while ensuring inconsistency in negative sample pairs   \n492 (representing fake samples). It must be emphasized that the significant advantage of EMT tasks lies   \n493 in modality commonality. Some studies have proven the existence of commonality [36, 37], but   \n494 this doesn\u2019t alter the fact that auditory and visual modalities are fundamentally distinct (not only   \n495 the semantic gap), with their enriched information not entirely consistent. In action recognition   \n496 tasks, there is currently no work that effectively achieves this through audio; in speech recognition   \n497 tasks [38], even with more complex, advanced feature extractors for extracting video features, or   \n498 introducing priors to isolate video features solely for lip movements, the results are far inferior to   \n499 audio single modality. While contrastive loss constrains the feature extractor to extract the most   \n500 effective synchronous-related features, in the absence of a modality [15], it leads to a significant   \n501 performance decline.   \n502 Moreover, not all tasks in EMT tasks involve positive and negative sample contrastive learning, so   \n503 sometimes contrastive loss is equivalent to operating mutual information. For example, in some   \n504 EMT methods\u2019 decoupling works [8, 16], each modality enjoys a common encoder and a specific   \n505 encoder, minimizing mutual information for different modalities\u2019 common encoders to homogenize   \n506 the extracted content and maximizing mutual information for the same modality\u2019s common encoder   \n507 and specific encoder to heterogenize them, adapting well to the environment of modality absence.   \n508 However, this method fixes the dimensions of each feature part, and the introduced losses directly   \n509 manipulate the behavior of the feature extractor, compelling it to extract a predetermined quantity of   \n510 common and specific features. The design of hyperparameters (encoder dimensions) will alter the   \n511 behavior of the feature extractor. Additionally, when expanding to more modalities, the training cost   \n512 of this method is also worth discussing. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "513 A.2 Combine With Residual ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "514 ResNet [29] solves the bottleneck of the number of network layers, and this epoch-making work 515 allows the number of network layers to be stacked into thousands. A plausible explanation is that 516 it reduces gradient disappearance or gradient explosion in deep networks. We try to explain this problem based on our information entropy related theory (Theory 3.1). ", "page_idx": 13}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/2aa96cad0f3a610a14f3170a6e79e27e7ff9a93053d81a8e914d728643071251.jpg", "img_caption": ["Figure 4: Structure of Residual in Networks. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "518 The basic block structure of ResNet [29] and the gradient propagation are illustrated in Figure 4. We   \n519 abstract it into a more general structure, where the downsampling block is considered as an arbitrary   \n520 function $f(\\cdot,\\theta^{f})$ , and the residual block is considered as another arbitrary function $g(\\cdot,\\theta^{g})$ . Here,   \n521 both of these arbitrary functions represent a type of network structure (in fact, this structure can be   \n522 further generalized), with $\\theta^{f}$ and $\\theta^{g}$ representing the parameters of the functions $f$ and $g$ , respectively.   \n523 Same as Eq.(2), the objective of gradient optimization is to optimize these parameters to minimize   \n524 the conditional entropy of Input $X,Y$ and Output $Y_{p r e d}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nY_{p r e d}=(g(f(X,\\theta^{f})\\;,\\theta^{g}),\\;\\;\\;\\;\\mathcal{L}=H[Y\\;|\\;g(f(X,\\theta^{f})\\;,\\theta^{g})]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "525 The expression for gradient descent can be derived by computing the partial derivatives of the loss   \n526 function with respect to the parameters $\\theta^{f}$ and $\\theta^{g}$ . Denote the loss function as Eq.( 19), the gradient   \n527 descent expressions are: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\partial{\\mathcal{L}}}{\\partial(\\theta^{f},\\theta^{g})}}={\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta^{f}}}+{\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta^{g}}},\\qquad{\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta^{f}}}={\\frac{\\partial{\\mathcal{L}}}{\\partial g}}\\cdot{\\frac{\\partial g}{\\partial f}}\\cdot{\\frac{\\partial f}{\\partial\\theta^{f}}},\\qquad{\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta^{g}}}={\\frac{\\partial{\\mathcal{L}}}{\\partial g}}\\cdot{\\frac{\\partial g}{\\partial\\theta^{g}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "528 For functions $f$ positioned further back, their ultimate gradients are influenced by the partial deriva  \n529 tives of the loss function with respect to functions $\\mathrm{g}$ positioned earlier. If network $\\mathrm{g}$ is composed of   \n530 $g_{1},g_{2},...,g_{n}$ , then during backward, it will be multiplied by numerous coefficients, making it more   \n531 prone to gradient vanishing or exploding. The introduction of residuals can alleviate this problem. It   \n532 is expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nY_{p r e d}=\\left(g(f(X,\\theta^{f})\\;,\\theta^{g})+f(X,\\theta^{f})\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "533 These derivatives represent the directions of steepest descent with respect to the parameters $\\theta^{f}$ and   \n534 $\\theta^{g}$ , guiding the optimization process towards minimizing the loss function. Rethinking the associated   \n535 gradient of $f$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta^{f}}}={\\frac{\\partial{\\mathcal{L}}}{\\partial g}}\\cdot{\\frac{\\partial g}{\\partial f}}\\cdot{\\frac{\\partial f}{\\partial\\theta^{f}}}+{\\frac{\\partial{\\mathcal{L}}}{\\partial f}}\\cdot{\\frac{\\partial f}{\\partial\\theta^{f}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "536 For the two elements of addition, compared to no residual, the first half of the gradient is numerically   \n537 consistent, and the second half of the gradient is used as the residual. Obviously, this gradient is   \n538 going to be direct.   \n539 Even in multimodal tasks, there exist challenges akin to residual issues yet to be resolved [39]. For   \n540 instance, the association between feature extractors and downstream tasks may be compromised by   \n541 the presence of feature fusion modules, manifested particularly in the introduction of intermediate   \n542 gradients by deep fusion mechanisms, leading to gradient explosion or vanishing gradients. One   \n543 approach to addressing this is through the incorporation of residuals. Indeed, some experimental   \n544 endeavors have already undertaken this step, demonstrating its efficacy. These inferences may serve   \n545 as a possible explanation, offering a generalized perspective.   \n546 However, residuals alone cannot entirely resolve the issue. Residuals, as a vector addition method,   \n547 demand strict consistency in dimensions between inputs and outputs; moreover, excessive layer-by  \n548 layer transmission of residuals may result in the accumulation of low-level semantics onto high-level   \n549 semantics, thereby blurring the representations learned by intermediate layers. While it may be   \n550 feasible to employ residuals in a smaller phase within the fusion stage, utilizing residuals across   \n551 the entire stage not only imposes stringent constraints on inputs and outputs but also risks semantic   \n552 ambiguity.   \n553 Another method of applying residuals is akin to DenseNet [40], directly stacking channels. This still   \n554 necessitates consistency in residual dimensions across different stages but circumvents the issue of   \n555 semantic confusion. However, the final classifier remains a linear layer, requiring the flattening of   \n556 multiple channels. Based on our theory, regardless of semantic sophistication, their initial origins   \n557 remain consistent. As dimensions accumulate, elements describing the same set of features proliferate,   \n558 inevitably leading to mutual information and subsequently reducing the conditional entropy relevant   \n559 to downstream tasks.   \n560 In light of the foregoing analysis, residual connections at the skip-fusion stage can effectively alleviate   \n561 the prevalent gradient issues in deep networks. However, this phased residual connection directly   \n562 linking feature extractors to downstream tasks rigorously constrains the form of inputs and outputs,   \n563 necessitating equilength features and overly blurred semantics, thus failing to achieve optimal effects.   \n564 Furthermore, the nature of multimodal tasks diverges from simple downsampling-residual networks,   \n565 as gradients stem not only from downstream tasks but also from multiple sources before the fusion   \n566 stage. Our proposed method entails reducing the network layers in the fusion stage to align the fusion   \n567 gradients with the descent direction of downstream task gradients. Alternatively, the scope of the   \n568 fusion stage loss function gradient can be restricted. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "569 B Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "570 We explain the derivation of the PNP equation to the proposed loss in detail. As before, let\u2019s assume that the cell is one-dimensional, and only the direction $x$ exists. ", "page_idx": 15}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/b342defeec3a608e2deeb7c01a52ab30b437a959d4477c26b479f7500955d11b.jpg", "img_caption": ["Figure 5: Schematic diagram of the electrolytic cell, $^+$ (orange) and - (black) represent the charged species (ions and electrodes). There is a boundary $b$ (black line) in the electrolytic cell, assuming that the positive potential is $U_{0}$ , the negative potential is $-U_{0}$ , and the boundary $b$ is the zero potential. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "571   \n572 For the basic Nernst-Planck equation, as shown in Figure 5, the ion $p\\in\\{+,-\\}$ in the cell system   \n573 conforms to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{J}_{p}=\\underbrace{-D_{p}\\nabla c_{p}(x,t)}_{\\mathrm{Diffusion}}+\\underbrace{c_{p}(x,t)\\mathbf{v}}_{\\mathrm{Advection}}+\\underbrace{\\frac{D_{p}z_{p}e}{k_{B}T}c_{p}(x,t)\\mathbf{E}}_{\\mathrm{Electromigration}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "574 We abstract the feature vector into a one-dimensional electrolytic cell and need to correspond each   \n575 term of the equation to it. Throughout the system, the fluid remains stationary; The electric field $\\mathbf{E}$   \n576 that guides the movement of ions is generated by the electric potential $\\phi$ and the magnetic field A.   \n577 We need to externally excite $\\phi$ and do not additionally apply a magnetic field. The actual learning   \n578 rate is usually not very large $(<100)$ , and the charge of the ion is assumed to be very small. This   \n579 gradient can be neglected as the magnetic field generated by the excitation. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{E}\\!=\\!-\\nabla\\phi-\\frac{\\partial\\mathbf{A}}{\\partial t}}{\\mathbf{v}\\!\\equiv\\!0,\\mathbf{A}\\!\\equiv\\!0}\\underbrace{-D_{p}\\nabla c_{p}(x,t)}_{\\mathrm{Diffusion}}+\\underbrace{\\frac{D_{p}z_{p}e}{k_{B}T}c_{p}(x,t)(-\\nabla\\phi)}_{\\mathrm{Electromigration}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "580 Our external excitation electric field is constant, so the potential expression can be expressed by the   \n581 ion concentration. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi(x)=U_{0}+e\\int_{0}^{x}(c_{+}(y,t)z_{+}+c_{-}(y,t)z_{-})d y\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "582 The final state of the system is that the flux is fixed with respect to time, that is, the partial differential   \n583 is zero. From the ion point of view, diffusion and electromigration are in equilibrium. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial c_{p}(x,t)}{\\partial t}=-\\nabla\\cdot\\mathbf{J}_{p}\\approx0}\\\\ &{\\qquad\\qquad\\implies-\\nabla\\dot{\\{}-D_{p}\\nabla c_{p}(x,t)+\\frac{D_{p}z_{p}e}{k_{B}T}c_{p}(x,t)[-\\nabla\\phi(x)]\\}\\approx0}\\\\ &{\\qquad\\qquad\\frac{\\nabla^{2}\\phi(x)=-\\frac{\\rho(x)}{\\epsilon_{0}},\\rho(x)=\\sum_{j}z_{j}c_{j}(x,t)}{\\epsilon_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{p}(\\frac{\\partial^{2}c_{p}(x,t)}{\\partial x^{2}}-\\frac{z_{p}e F}{k_{B}T\\epsilon_{0}}c_{p}(x,t)\\sum_{j}z_{j}c_{j}(x,t)+\\frac{z_{p}e}{k_{B}T}\\frac{\\partial c_{p}(x,t)}{\\partial x}\\frac{d\\phi(x)}{d x})\\approx0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/55d2decd4b0e75b4a83da3c7b6bf65420b4a59f79d12dbefffeb9220dea804f9.jpg", "img_caption": ["Figure 6: Representation of ion distribution. The ordinate represents the ion concentration and the abscissa represents the electrolytic cell position. 0 is the position of the positive electrode, $l$ is the position of the negative electrode, and $b$ is the potential equilibrium boundary. The green line represents a uniform distribution of initial state ions to conform to macroscopic electrical neutrality, the yellow line represents the ideal electrolysis target, that is, the foreign ions are completely divided at the equilibrium boundary, and the red line represents the practically possible situation. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "584 In the initial condition, ions undergo spontaneous and uniform distribution through diffusion driven   \n585 by Brownian motion (the green line in Figure 6). This dynamic process leads to the establishment of a   \n586 heterogeneous distribution of ions within the system. However, as the system approaches the potential   \n587 equilibrium boundary $b$ , the electrostatic forces acting on ions become increasingly influential. At   \n588 this boundary, denoted as the end condition [41], the principles of electroneutrality come into play.   \n589 Here, positive and negative ions are balanced such that their net charge is neutral, resulting in an   \n590 electrically neutral region around the potential equilibrium boundary: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{j}z_{j}c_{j}(x,t)\\approx0,\\quad\\frac{\\partial^{2}c_{p}(x,t)}{\\partial x^{2}}+\\frac{z_{p}e}{k_{B}T}\\frac{\\partial c_{p}(x,t)}{\\partial x}\\frac{d\\phi(x)}{d x}\\approx0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "591 ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi(0)-\\phi(B)\\approx-e\\int_{0}^{B}c_{-}(x,t)z_{-}d x|_{t=T}\\approx\\phi(B+1)-\\phi(L)\\approx e\\int_{B+1}^{L}c_{+}(x,t)z_{+}d x|_{t=T}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "592 The positive and negative properties of diffusion and electromigration are always opposite. If the   \n593 ion species used as the external electrode is the same as that of the original solution, then we can   \n594 approximately assume that the ion on either side of the zero potential boundary $b$ , combined with the   \n595 ion equivalent to the external electrode, can reduce the initial solute.   \n596 Assuming features from another modality are perfectly ordered, they can serve as a constant stimulus   \n597 guiding the ionization of the awaiting electrolytic modality. However, unlike in deep learning, where   \n598 the loss function can be equivalent to an external potential, both serve as stimuli capable of guiding   \n599 the respective fundamental ion directional motion.   \n600 Beginning with two modalities, initially disordered features prompt GMF to attempt cyclic con  \n601 nections, as depicted in the diagram. The imposition of external guidance induces the movement   \n602 of feature particles of different polarities in distinct directions, ultimately coalescing at one end.   \n603 According to the law of conservation of mass, these aggregated features can be fully reconstructed   \n604 into the original modality representation of the guided modality particles at the opposite end. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "605 Expanding to multiple modalities, electrochemical cells allow for parallel multi-level connectivity, 606 where applying a set of stimuli can simultaneously guide the movement of ions across multiple cell 607 groups. These potentials, as per the principles of basic circuitry, are distributed across each cell, as shown in Figure 7. ", "page_idx": 17}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/4e51cc1dfbead87086d8f4653bbb9a08e3dccdcdd4681a0944526306aef925de.jpg", "img_caption": ["Figure 7: Example diagram of loop guidance. The modes are excited by each other. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "608 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "609 The PNP equation provides a theoretical basis for GMF, and then we can propose to model material   \n610 conservation with a reconstruction loss. The reconstruction loss can well simulate the motion of   \n611 particles, and its reduction condition does not lead to ambiguity due to the existence of modality  \n612 specific, as expressed in Eq.( 31). ", "page_idx": 17}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/d30a4c62e219f342e92e0498959dd2bf2536308476d2150b018c41eb4fca634c.jpg", "img_caption": ["Figure 8: Evaluate ResNet, MobileNet and ViT test accuracy and the ratio of test accuracy to training accuracy (denote as ratio) on the CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "613 C Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "614 Theorem 3.2: The dimension of the feature that is best suited to the downstream task varies, and   \n615 there is always an optimal value for this feature. The dimension multiple relationship between each   \n616 layer of the feature extractor is fixed, and the initial dimension is adjusted. Too low dimension of the   \n617 final output will lead to inefficient representation, and too high dimension will introduce noise. The   \n618 existence of an integer $l_{\\mathrm{best}}$ such that for any integer $l$ distinct from $l_{\\mathrm{best}}$ , the conditional entropy of   \n619 the model\u2019s predictions $f_{l}(X,\\theta_{l})$ is greater than that of the model\u2019s predictions $f_{l_{\\mathrm{best}}}(X,\\theta_{l_{\\mathrm{best}}})$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\exists l_{\\mathrm{best}}\\in\\mathbb{N},\\forall l\\in\\mathbb{N},l\\neq l_{\\mathrm{best}},H(Y|f_{l}(X,\\theta_{l}))>H(Y|f_{l_{\\mathrm{best}}}(X,\\theta_{l_{\\mathrm{best}}}))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "620 C.1 Experiment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "621 There was some previous work [42] that demonstrated that this optimal dimension exists. However,   \n622 existing methods do not account particularly well for the conditions under which poor ftiting occurs,   \n623 so we conduct experiments to demonstrate the existence of this phenomenon. At the end we present   \n624 a possible conjecture. The existence of this optimal dimension is universal and at the same time   \n625 inconsistent. Specifically, each type of feature extractor, each type of dataset, and each corresponding   \n626 downstream task have different optimal dimensions.   \n627 Our set of experiments is shown in Fig 8. In addition to the intuitive visualization of the validation   \n628 accuracy, we also show the ratio of the validation accuracy to the training accuracy, aiming to measure   \n629 the validation accuracy and reflect the fitting effect of the model. The closer the ratio is to 1, the   \n630 stronger the generalization ability is, and the better the fit is.   \n631 In Figure 8 (a) and (b), the evaluation results of ResNet [29] on CIFAR-10 [43] are presented. As the   \n632 dimensionality increases, the testing performance of the model improves, and the performance range   \n633 stabilizes. However, with a twofold increase in dimensionality, the variation in testing performance   \n634 diminishes, approaching zero. In other words, doubling the parameter count does not yield any   \n635 improvement. Additionally, for larger networks like ResNet110, performance begins to decline.   \n636 Furthermore, while absolute performance is increasing, the ratio is declining, indicating a weakening   \n637 in generalization capability.   \n638 Figure 8 (c) and (d) depict the evaluation results of MobileNetV3 [44] on CIFAR-10 [43], showing   \n639 conclusions similar to those of ResNet. For larger networks like MobileNetV3-Large, at lower   \n640 dimensionalities, its generalization capability is significantly lower compared to simpler networks.   \n641 Figure 8 (e) and (f) illustrate the evaluation results of ViT [45] on CIFAR-10 [43]. As ViT is based   \n642 on transformers [27] and possesses a global receptive field, its base dimensionality is significantly   \n643 larger than that of convolutional neural networks. Both in terms of absolute performance and ratio,   \n644 its optimal representation dimensionality approaches 256, distinct from other networks.   \n645 However, it is worth noting that the presence of optimal features is not only closely related to network   \n646 type and structure, but also to the dataset and downstream tasks. We chose CIFAR-100 [43] for this   \n647 set of comparative experiments. This is because its data volume is consistent with CIFAR-10, but   \n648 with more categories and greater difficulty. The experimental results of ResNet [29] evaluated on   \n649 CIFAR-100 are shown in Figure 9. Compared to the results shown in Figure 8(a) and (b), firstly, the   \n650 impact of different dimensions on accuracy is more significant (for example, the maximum difference   \n651 in test performance of ResNet-20 on CIFAR-10 is about $25\\%$ , exceeding $40\\%$ here); for ResNet-110,   \n652 excessive dimensions no longer lead to performance stabilization, but rather a visible performance   \n653 decline.   \n654 The experimental results demonstrate the existence of an optimal dimensionality. This dimensionality   \n655 may vary based on the different structures of networks. Hence, the concept of optimal dimensionality   \n656 should be discussed in consideration of multiple external conditions. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/2e533ae36f0b2c525a0df284851a742637750ec6c36575d9dc82228677a2a528.jpg", "img_caption": ["Figure 9: Evaluate ResNet test accuracy and the ratio of test accuracy to training accuracy (denote as ratio) on the CIFAR-100 dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "657 D Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "658 Theorem 3.3: The feature extractor is fixed, and its original output feature dimension $l$ is mapped   \n659 to $n l$ , and finally back to $l$ . The mapping result is used as the basis for the downstream task. The   \n660 performance of downstream tasks is infinitely close to the original performance as $n$ increases, but   \n661 never greater than the original performance. For magnification $n\\,>\\,1,n\\,\\in\\,\\mathbb{Z}$ , mapping matrix   \n662 $\\mathbf{U}_{1}\\in\\breve{\\mathbb{R}}^{l\\times n l}$ and $\\mathbf{U}_{2}\\in\\mathbb{R}^{n l\\times l}$ , For the output features $\\overline{{f(X,\\theta)}}\\in\\mathbb{R}^{l}$ and $Y$ : ", "page_idx": 19}, {"type": "text", "text": "663 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{H(Y|f(X,\\theta))<H(Y|\\mathbf{U}_{1}\\cdot(\\mathbf{U}_{2}\\cdot f(X,\\theta)))}\\\\ {l i m_{n\\to\\infty}H(Y|\\mathbf{U}_{1}\\cdot(\\mathbf{U}_{2}\\cdot f(X,\\theta))))=H(Y|f(X,\\theta)}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/2adbf42b70bd77d060e316364f4ff1dc376486a910d1449e558cfbe84d7d0110.jpg", "img_caption": ["Figure 10: Evaluate ResNet test accuracy and the ratio of test accuracy to training accuracy (denote as ratio) on the CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "664 D.1 Theoretically ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "665 Denote $V=f(X,\\theta)$ , the rank of each stage: ", "page_idx": 20}, {"type": "equation", "text": "$$\nV=\\left[\\!\\!\\begin{array}{l}{v_{1}}\\\\ {v_{2}}\\\\ {\\vdots}\\\\ {v_{l}}\\end{array}\\!\\!\\right],\\quad r(V)\\le l,\\quad r(\\mathbf{U}_{2})\\le l,\\quad r(\\mathbf{U}_{2}\\cdot V)\\le\\operatorname*{min}(r(V),r(\\mathbf{U}_{2}))\\le l,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "666 ", "page_idx": 20}, {"type": "equation", "text": "$$\nr(\\mathbf{U}_{1})\\le l\\quad r(\\mathbf{U}_{1}\\cdot(\\mathbf{U}_{2}\\cdot f(X,\\theta))\\leq l\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "667 The mapped rank is always less than or equal to the original rank. That is, downstream task-relevant   \n668 features may be compressed while not generating features out of thin air. Eq. (33) gets the certificate.   \n669 For Eq.(34), we discuss the problem from pruning, linear algebra and probability theory. Neural   \n670 networks are often overparameterized, requiring more network parameters than needed to get a   \n671 good fit. In theory [23], however, only a subset of these parameters are useful in practice. Hence,   \n672 some knowledge distillation methods such as teacher-student networks and pruning [46]. These   \n673 tested models maintain good performance while removing most of the parameters, which proves that   \n674 overparameterization is a common phenomenon. We interpret it as a probabilistic problem, that is,   \n675 the effective parameters are generated with a certain probability. Overparameterization significantly   \n676 improves the effective parameter generation, and knowledge distillation removes these redundant and   \n677 invalid parameters.   \n678 Let $\\mathbf{A}\\in\\mathbb{R}^{n d\\times d}$ be a learnable matrix $(n>>1)$ ). Act on $\\mathbf{v}\\in\\mathbb{R}^{d}$ to complete the mapping from   \n679 lower dimension to higher dimension: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{A}=[\\mathbf{a}_{1},\\mathbf{a}_{2},\\ldots,\\mathbf{a}_{\\mathbf{n}\\mathbf{d}}],\\quad\\hat{\\mathbf{v}}=\\mathbf{A}\\mathbf{v}=[\\hat{\\mathbf{v}}_{1},\\hat{\\mathbf{v}}_{2},\\ldots,\\hat{\\mathbf{v}}_{n d}]^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "680 Denote $\\hat{\\mathbf{v}}\\in\\mathbb{R}^{n d}$ as the mapping result. $\\hat{\\mathbf{v}}_{k}$ represents the $k$ -th row element. For any two of these   \n681 row vectors ${\\bf a_{i}}$ and ${\\bf a_{j}}$ $(i\\neq j)$ . They have a ratio c for their first elements.A necessary and sufficient   \n682 condition for linearity between two vectors can be extended to the following: for any element in the   \n683 same row of these two vectors, the ratio should be $c$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{i}}=[a_{i1},a_{i2},\\dots,a_{i d}],\\quad\\mathbf{a}_{\\mathbf{j}}=[a_{j1},a_{j2},\\dots,a_{j d}],\\quad c={\\frac{a_{i1}}{a_{j1}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "684 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{d}{\\frac{a_{j t}}{a_{i t}}}=c\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "685 If A is learnable, each element will have a different weight for each parameter adjustment. denote   \n686 $\\mathbf{P}({\\frac{a_{j}t}{a_{i t}}}\\ =\\ c)$ as the probability that the proportion of the $t$ -th element of $\\mathbf{a_{i}}$ and $\\mathbf{a_{j}}$ is equal to   \n687 $c$ , which cannot be determined directly because the input sample is uncertain. In the context of   \n688 neural networks, the adjustment of gradients can be regarded as following a continuous probability   \n689 distribution. Consequently, the probability of the adjustment taking on a specific constant value   \n690 is zero (does not imply impossibility). By cumulatively multiplying this probability, we get the   \n691 probability that the two column vectors are linearly related in gradient descent. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\prod_{t=1}^{d}\\operatorname{P}({\\frac{a_{j t}}{a_{i t}}}=c)\\approx0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "692 However, for a $d_{\\cdot}$ -dimensional vector, there cannot be more than $d$ linearly independent features. To   \n693 simplify the expression, we assume that Eq.(40) is a fixed value on the interval (0,1). The probability   \n694 that exactly ${\\mathrm d}$ -dimensional features are linearly dependent is given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{(n d)!}{d!(n d-d)!}(\\prod_{t=1}^{d}\\mathbb{P}(\\frac{a_{j t}}{a_{i t}}=c))^{d}(1-\\prod_{t=1}^{d}\\mathbb{P}(\\frac{a_{j t}}{a_{i t}}=c))^{n d-d}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "695 ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{(n d+1)!}{d!(n d+1-d)!}(\\prod_{t=1}^{d}\\mathrm{P}(\\frac{a_{j t}}{a_{i t}}=c))^{d}(1-\\prod_{t=1}^{d}\\mathrm{P}(\\frac{a_{j t}}{a_{i t}}=c))^{n d+1-d}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "696 In deep learning methods, the feature dimension is usually not set too small, $d$ is sufficiently   \n697 large. Combined with gradient descent, the parameter adjustment is random, the linear correlation   \n698 probability of two random features is close to 0. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\prod_{t=1}^{d}\\operatorname{P}(\\frac{a_{j t}}{a_{i t}}=c)\\approx0,\\quad\\frac{E q.(42)}{E q.(41)}=\\frac{n d+1}{n d+1-d}\\prod_{t=1}^{d}\\operatorname{P}(\\frac{a_{j t}}{a_{i t}}=c)\\approx1+\\frac{d}{n d+1-d}\\geq1\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "699 Consider mapping matrix $\\mathbf{U}_{2}\\in\\mathbb{R}^{n l\\times l}$ . As n increases, the probability of rank $l$ increases. The same   \n700 is true for the matrix $\\mathbf{U}_{1}\\in\\mathbb{R}^{l\\times n l}$ . Therefore, as the probability of two correlation matrices being full   \n701 rank becomes larger, a larger $n$ helps to restore the original representation under the premise that the   \n702 network does not involve unexpected situations such as gradient explosion and vanishing gradients.   \n703 However, it can be determined that when n is less than 1 $({\\mathfrak{n}}>0)$ , there must be information loss. This   \n704 is because the upper limit of the rank of a matrix depends on the smaller value of the number of rows,   \n705 columns. Furthermore, it is not appropriate to increase the number of parameters blindly, which will   \n706 lead to an exponential number of parameters. ", "page_idx": 21}, {"type": "text", "text": "707 D.2 Experiment ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/1449672195596dbd3e9a7d6bc9af5906c0a8372b2da9b72d3db69b437c5ba813.jpg", "img_caption": ["Figure 11: Theoretical validation on ImageNet on the performance impact of raising and then reducing the original features. The horizontal coordinate represents the mapped feature dimension, the upper bound is the best performance, the lower bound is the worst performance, and the ordinate represents the validation set accuracy. The dashed line represents the results reported for directly validating the performance of the pretrained model. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "708 We employed pre-trained ResNet-18, ResNet-34, ResNet-50, and ResNet-101 [29] models provided   \n709 by PyTorch [47], removing their classifiers to obtain raw features with dimensions of 512, 512, 2048,   \n710 and 2048 respectively. After freezing the other layers, we mapped these original features to another   \n711 dimension and subsequently retrained the classifiers based on these new features. As depicted in   \n712 Figure 11, where the abscissa represents the dimensions of the mapped features and the ordinate   \n713 represents the classification accuracy of the new classifier on the ImageNet [48] validation set. Our   \n714 experimental hyperparameter design and optimizer were identical to those reported in the original   \n715 paper. We recorded the validation accuracy every 400 iterations, and if the accuracy did not improve   \n716 for 10 consecutive validations, training was terminated prematurely. The final results are depicted in   \n717 a bar chart, where the upper and lower bounds represent the maximum and minimum values of the   \n718 validation accuracy.   \n719 It can be observed that larger mapping dimensions lead to faster convergence and yield better results.   \n720 Smaller mapping dimensions, especially when they are smaller than the original dimensions, not   \n721 only exhibit significant differences in upper and lower bounds of validation accuracy but also witness   \n722 a substantial decrease in the upper limit. This observation aligns with our theoretical expectations.   \n723 When the scaling factor $n$ is close to 4, the performance loss has entered the acceptable range. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "724 E Different Between Theorem 3.2 and Theorem 3.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "725 Both Theorem 3.2 and Theorem 3.3 focus on the dimension of presentation. The most significant   \n726 difference between the two theories is what the original input was.   \n727 Theorem 3.2 is for the case where the sample is known and the representation is unknown, and this   \n728 representation contains relevant information and irrelevant information. Therefore, this theory is   \n729 more about the number of parameters needed to characterize, the minimum dimension needed to get   \n730 the best performance, or the best performance in the minimum dimension. In this paper, this theory   \n731 emphasizes the necessity of unequal-length fusion, and points out and proves through experiments   \n732 that equal-length fusion may bring the problem of feature redundancy or feature missing, which not   \n733 only increases the unnecessary amount of computation, but also affects the performance to some   \n734 extent.   \n735 Theorem 3.3 is to analyze the influence of linear mapping on the representation in the case of known   \n736 representation and unknown samples. Our proposed GMF method is very simple and contains only a   \n737 number of linear layers, achieving the performance of larger parameter fusion methods of previous   \n738 works. However, our original intention is not to be guided by experimental results, but to theoretically   \n739 analyze whether the possible information loss is acceptable. We expect our work to be interpretable   \n740 and applicable. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "741 F Derivation of Conjecture 3.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "742 Conjecture 3.1: Rely on Theorem 3.1, 3.2, 3.3, we propose an conjecture that a boundary of perfor  \n743 mance limitation exists, determined by downstream-related entropy. Theoretically, by establishing a   \n744 direct correspondence between the extractor and classifier, fusion method can enhance the limitation boundary, further improve performance.   \n746 Based on the proof of Theorem 3.2, one of the foundations of learning in neural networks is gradient   \n747 descent, which presuppositions that gradients can be backpropagated. Every tuning of the learnable   \n748 parameters will eventually be implemented on the original input. Assuming that the feature extractor is fixed, the original input at this time is the feature output by the feature extractor. For any learnable   \n750 parameter, the value of a certain sample can be expressed by an exact formula. For a completely consistent input, it is assumed that its downstream task-related information entropy can be efficiently   \n752 calculated, and its information entropy minimum is certain. Therefore, there is a performance upper   \n753 bound, depending on how the existing features are utilized. In practical deep learning tasks, the input features are often not fixed, and gradients need to propagate   \n755 to be able to fully determine the original samples\u2014which must also be fully determined. We continue   \n756 to analyze the feature layers outputted by the feature extractor, assuming that the relevant information   \n757 entropy of downstream tasks can be manually calculated. Thus, for the output features at a certain   \n758 moment, the lower bound of the conditional entropy of downstream tasks can still be computed,   \n759 which represents the performance upper bound. ", "page_idx": 22}, {"type": "text", "text": "760 Therefore, the entire multimodal learning network is divided into two parts: one is the lower bound of   \n761 the conditional entropy of the feature extractor output relative to the original samples, and the other   \n762 is the lower bound of the conditional entropy of downstream tasks relative to the feature extractor   \n763 output. The former is a prerequisite for the latter sequentially. However, as stated in the formulas,   \n764 assuming the existence of fusion loss and downstream task loss, and the gradient descent directions   \n765 are not completely consistent, let the weight of the fusion loss $\\mathcal{L}_{f u s i o n}$ be $\\lambda_{1}$ , and the loss of the   \n766 downstream task $\\mathcal{L}_{t a s k}$ be $\\lambda_{2}$ , the total loss can be expressed as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\lambda_{1}\\mathcal{L}_{f u s i o n}+\\lambda_{2}\\mathcal{L}_{t a s k}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "767 The learning task is to minimize the training loss. Assuming that $\\lambda_{1}\\mathcal{L}_{f u s i o n}>\\lambda_{2}\\mathcal{L}_{t a s k}$ , then the   \n768 gradient of the feature extractor will tend more toward the fusion loss. In severe cases (such as opposite   \n769 gradient descent directions), the downstream task-related loss will be completely overshadowed.   \n770 This also leads to an increase in the lower bound of the conditional entropy of downstream tasks   \n771 and a decrease in the theoretical performance upper limit. Therefore, we assume that there exists a   \n772 boundary, which is determined by the theoretical performance upper bound based on a fixed feature   \n773 and the conditional entropy of downstream tasks. Regardless of how outstanding the fusion method   \n774 design is, just like the principle of energy conservation law for features, the final task performance of   \n775 this method cannot exceed this upper bound.   \n776 The reason why our proposed GMF achieves performance improvement is not due to the performance   \n777 enhancement brought by the complex fusion network, but rather from a higher upper bound. However,   \n77 in reality, we are still far from this upper bound, and demonstrating our method as a precursor to   \n779 other methods can prove this point well. As shown in the Figure 12, we have drawn a hypothetical   \n780 graph based on the data reported in the paper. Assuming GMF as the precondition method for the   \n781 Perceiver [4], the result that GMF can be on par with complex networks with almost no resource   \n782 consumption is interpretable. ", "page_idx": 23}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/74f3281c4a01342c24d212f059caa0ec0afef6046b0039e9d85af4972d98f560.jpg", "img_caption": ["Figure 12: Visualizing performance improvements based on conjectures. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "783 G Experiment Supplement ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "784 G.1 Implement Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "785 For all experiments, we use apex to optimize the v-memory and the parameter is set to \u2019O1\u2019. The   \n786 random seed fixed \u20191\u2019 for all GMF related implementation. However, for some dropout design   \n787 methods, the reported experimental results may not be fully reproducible. More details are listed in   \n788 Table 5   \n789 (1) torch.manual_seed(seed)   \n790 (2) torch.cuda.manual_seed_all(seed)   \n791 (3) np.random.seed(seed) ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/5830c9583cb707851ed48f61113d61be1a49ba51dc98409b316ec3539f546e7e.jpg", "table_caption": ["Table 5: Details of GMF. Momentun of $\\mathrm{{SGD}}=0.9$ , weight delay $\\equiv$ 1e-4. Lr_scheduler $=$ ReduceLROnPlateau, facto ${\\it\\omega}=0.1$ , patience $\\mathord{=}1000$ . "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "793 G.2 Information About Preprocess and Baseline ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "794 For the VGGSound dataset, we downsample all currently available samples to 5fps, with videos of   \n795 size $192^{*}256$ and audio sampled at $16000\\,\\mathrm{Hz}$ , while retaining only the first 9 seconds to accommodate   \n796 most samples that are not exactly 10 seconds in duration. Samples without audio or video are removed.   \n797 As for FakeAVCeleb, since the fabricated samples exhibit a global range of fabrication, with lengths   \n798 distributed from 0.8 seconds and above, and a frame rate between 15 to 30 fps, we only select the   \n799 first 8 frames along with their corresponding audio to ensure adaptability to the dataset.   \n800 We employ the default testing-training split provided by VGGSound. For FakeAVCeleb, consistent   \n801 with much of the prior work focused on audio-visual deepfake detection, we first sort each class   \n802 (real audio-real video, real audio-fake video, fake audio-real video, fake audio-fake video), and then   \n803 allocate the first $70\\%$ of each class to the training set and the remaining $30\\%$ to the testing set.   \n804 The baseline of VGGSound pretrained on KINETICS400V1. Momentun of $\\mathrm{{SGD}=0.9}$ , weight   \n805 delay=1e-4. Adam betas $=$ (0.5, 0.9). lr_scheduler $=$ ReduceLROnPlateau, facto $=\\!0.1$ , patience $=\\!1000$   \n806 on VGGSound, facto ${\\mathrel{\\left/{\\right/}}}10.5$ , patience $\\mathrel{\\left<=}50\\right>$ , verbose=True, min $\\scriptstyle\\mathrm{lr}=1\\mathrm{e}-8$ on FakeAVCeleb. The generated   \n807 audio sequence is quite long, and the receptive field of the convolutional network is not global. To   \n808 address this potential issue, we stack the audio into a timing sequence (144000 to $9\\,\\times\\,16000)$ .   \n809 Audio wave transform to input tensor by MelSpectrogram(sample_rate $=16000$ , n_ff $=\\!400$ ,   \n810 win_length $\\scriptstyle=400$ , hop_length $=160$ , n_mel $\\scriptstyle=192$ ) for VGGSound and log (abs (STFT(n_ff $=\\!1024$ ,   \n811 hop_length $=256$ , win_length $_{1=1024}$ ,window $r_{=}$ blackman_window $(1024))+1\\mathrm{e}{-8})$ for FakeAVCeleb.   \n812 Video frame directly as the input of network without any preprocess. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "813 The hyperparameter as shown in Table 6 ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/9ba1e92d7900e12e455191a8b572e02641a4e28d78c98632a3cb3bb4dd5a30dc.jpg", "table_caption": ["Table 6: Model Details of Baseline. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "814 G.3 Compared Method Structure ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "815 The integration of our method with others is depicted in Figure 1. By bypassing modality-invariant   \n816 features and focusing solely on modality-specific features for fusion, the input represents a representa  \n817 tion with reduced mutual information. This leads to a reduction in the conditional entropy magnitude   \n818 during the initial stages. The backend component may consist of a simple concatenation or modules   \n819 proposed by other methods. Consequently, the inherent characteristics of GMF are constrained by   \n820 the limitations of the backend module. Comparatively, the limitations are minimal with a simple   \n821 concatenation approach. ", "page_idx": 24}, {"type": "text", "text": "822 H More Comparison on the FakeAVCeleb Dataset ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "823 We expanded the experimental table of FakeAVCeleb (Tab. 4) in the main text, incorporating additional   \n824 comparisons focused on deepfake detection methods. Apart from the experiments reported in the   \n825 original text, the remaining data were sourced from the original paper proposing the method. Here,   \n826 VFD [50], Emo-Foren [51], and MDS [53] are grouped together because these methods transform   \n827 EMT into NMT. Specifically, these methods emphasize certain aspects of multimodal performance:   \n828 VFD emphasizes identity, Emo-Foren emphasizes emotion, and MDS, while not emphasizing a   \n829 specific mode, relies on computing confidence in matching a certain segment. Therefore, the modal   \n830 absence evaluation for these methods is marked as \u2019-\u2019, indicating absence. Importantly, our method   \n831 effectively connects representations of different modalities without additional overhead for AE-based   \n832 feature extractors, resulting in a highly competitive outcome. ", "page_idx": 24}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/4f6bd90c43e4fce6ceb2843bd21acca00701499806164dd811763a97e68efede.jpg", "img_caption": ["Figure 13: G-structure schematic diagram. Yellow feature vectors represent modality-invariant features, while other colors represent modality-specific features for each modality. Modality-invariant features are directly connected to downstream task classifiers, while modality-specific features serve as new inputs to the fusion module. "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/7a6294b94f4b285f18b2e14c2847bdfdb78413bae673af0ea1a4b7d883ca40db.jpg", "table_caption": ["Table 7: Performance on the FakeAVCeleb dataset. \u2019A\u2019, \u2019V\u2019 represents the separate audio and video modality, and the input of the other modality is 0. \u2019AV\u2019 stands for the full sample. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "833 H.1 The reason of choose FakeAVCeleb ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "834 The FakeAVCeleb dataset is atypical, characterized by severe class imbalance posing significant   \n835 challenges to methods. Specifically, the ratio of positive to negative samples is 1:1 for audio and 1:19   \n836 for video, resulting in an overall ratio of 1:39. While audio often possesses discriminative capabilities   \n837 less susceptible to the impact of sample proportions, most methods evaluated in our tests struggle to   \n838 effectively address this bias.   \n839 Addressing this imbalance necessitates multimodal methods to learn weight disparities across modali  \n840 ties to mitigate the effects of sample bias. This manifests in high accuracy (ACC) juxtaposed with   \n841 mismatched area under the curve (AUC). Methods capable of mitigating this bias often underutilize it,   \n842 resulting in suboptimal ACC. However, in real-world scenarios, the distribution of genuine and fake   \n843 samples may not be balanced, and a single segment may not adequately represent an event. Hence,   \n844 the adaptability of methods to publicly available datasets warrants thorough investigation. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "845 I GMF with AutoEncoder (GMF-AE/MAE) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "846 AutoEncoder [35] (AE) was initially proposed as a feature dimensionality reduction method, com  \n847 pressing samples into a latent space and then reconstructing them to retain the details of the entire   \n848 sample in the latent space features. Masked AutoEncoder [32] (MAE) is a more powerful feature   \n849 extraction variant of AE, masking most of the original samples and reconstructing them, allowing   \n850 the model to learn more sample features. An intriguing point is that this concept can be seamlessly   \nintegrated with GMF (proposed Generalized Multimodal Fusion). GMF applies reconstruction loss ", "page_idx": 26}, {"type": "image", "img_path": "XgkrXpHl5j/tmp/859093aadb5501bb5312286a01ba7ed73a9955f198a4b0e52388a58f6f4b33e0.jpg", "img_caption": ["Figure 14: Simplified GMF frame diagram with MAE as feature extractor. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "851 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "852 as an incentive, directing the movement of different types of features towards a relatively ordered   \n53 representation. Combining with PNP equations and our theoretical framework, this requires two   \n54 additional linear layers for feature dimensionality reduction, expansion, and a linear layer for recon  \n55 struction. Thus, the additional overhead includes a reconstruction loss and the mentioned linear layers.   \n56 However, due to the nature of AE, this feature-directional movement process can be accomplished   \n57 during AE\u2019s self-supervised learning. Specifically, instead of feeding complete latent space features   \n58 into the Decoder, a combination of features from the corresponding Encoder and another modality   \n59 Encoder is used. This allows us to achieve our goal without any additional overhead. However, if   \n860 done so, explicit boundary delineation is necessary, which may affect model performance; moreover,   \n861 this learning process must be conducted in a multi-modal task, and features must be intact during the   \n862 learning process.   \n863 The specific structural diagram is shown in Figure 14. Here, we also consider the transformer [27]   \n864 initially used for text as a variant of MAE, video encoder is MAE [32] and the Audio encoder is   \n865 Audio-MAE [33]. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "866 J GMF Architecture ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "XgkrXpHl5j/tmp/76f53502ba5f4f1aa65273ec1748056450e6014fd8294165167cd8db12428c0d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "867 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We propose a generalized multimodal fusion model via Poisson-Nernst-Planck Equation, which can greatly improve the fusion performance. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "884 2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide an outlook on future work in the conclusion section and write a separate subsection in the appendix to state limitations. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "916 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "17 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n18 a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Justification: We briefly introduce the theory in the main text and present the necessary formulas that will help the reader to understand. For each proposed theory and hypothesis, the necessary derivations and experimental results are proved in the corresponding subsections of the experimental section and appendix. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "935 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We fixed the random seed, and used source code that can be directly used as a class, rather than pseudocode, during the introduction to the algorithm. For fixed features, we will provide pre-trained models with the results of feature extraction. In addition, we have added our code in the attachment, and annotate the reference projects in detail. In addition, we present the detailed hyperparameters of the replication method in a tabular form in the appendix, and open source this part of the code. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 29}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We will open source all original code (such as the implementation and reproduction method of the proposed method), for non-original code, we will mark the reference project. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "973   \n974   \n975   \n976   \n977   \n978   \n979   \n980   \n981   \n982   \n983   \n984   \n985   \n986   \n987   \n988   \n989   \n990   \n991   \n992   \n993   \n994   \n995   \n996   \n997   \n998   \n999   \n1000   \n1001   \n1002   \n1003   \n1004   \n1005   \n1006   \n1007   \n1008   \n1009   \n1010   \n1011   \n1012   \n1013   \n1014   \n1015   \n1016   \n1017   \n1018   \n1019   \n1020   \n1021   \n1022   \n1023 ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We briefly describe the hyperparameter Settings and experimental equipment used in the experiment section of the main text. For methods not previously available on the corresponding dataset, our preset hyperparameters are described in detail in the appendix. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "1024 Justification: We fixed all random seeds to 1, and no data augmentation was applied to the   \n1025 original data, so the results should be similar across multiple runs. Furthermore, feature   \n1026 extractors are mostly aligned, which has nothing to do with dataset integrity.   \n1027 Guidelines:   \n1028 \u2022 The answer NA means that the paper does not include experiments.   \n1029 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n1030 dence intervals, or statistical significance tests, at least for the experiments that support   \n1031 the main claims of the paper.   \n1032 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n1033 example, train/test split, initialization, random drawing of some parameter, or overall   \n1034 run with given experimental conditions).   \n1035 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1036 call to a library function, bootstrap, etc.)   \n1037 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1038 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1039 of the mean.   \n1040 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1041 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1042 of Normality of errors is not verified.   \n1043 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1044 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1045 error rates).   \n1046 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1047 they were calculated and reference the corresponding figures or tables in the text.   \n1048 8. Experiments Compute Resources   \n1049 Question: For each experiment, does the paper provide sufficient information on the com  \n1050 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1051 the experiments?   \n1052 Answer: [Yes]   \n1053 Justification: For each set of experiments in the main text, we report not only the evaluation   \n1054 metrics, but also the number of parameters, the amount of computation, and the time required   \n1055 for inference.   \n1056 Guidelines:   \n1057 \u2022 The answer NA means that the paper does not include experiments.   \n1058 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1059 or cloud provider, including relevant memory and storage.   \n1060 \u2022 The paper should provide the amount of compute required for each of the individual   \n1061 experimental runs as well as estimate the total compute.   \n1062 \u2022 The paper should disclose whether the full research project required more compute   \n1063 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1064 didn\u2019t make it into the paper).   \n1065 9. Code Of Ethics   \n1066 Question: Does the research conducted in the paper conform, in every respect, with the   \n1067 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1068 Answer: [Yes]   \n1069 Justification: We\u2019ve read the spec and followed it to the letter. Our paper does not involve   \n1070 human subjects and the datasets used are all open source datasets. These data sets are all   \n1071 instant downloads, and we cannot obtain them when the sample provider sets the sample   \n1072 private.   \n1073 Guidelines:   \n1074 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1075 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1076 deviation from the Code of Ethics.   \n1077 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1078 eration due to laws or regulations in their jurisdiction).   \n1079 10. Broader Impacts   \n1080 Question: Does the paper discuss both potential positive societal impacts and negative   \n1081 societal impacts of the work performed?   \n1082 Answer: [NA]   \n1083 Justification: Our approach is a deep learning architecture, and the selection of downstream   \n1084 tasks does not require a natural person to do it. This is not directly related to society.   \n1085 Guidelines:   \n1086 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1087 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1088 impact or why the paper does not address societal impact.   \n1089 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1090 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1091 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1092 groups), privacy considerations, and security considerations.   \n1093 \u2022 The conference expects that many papers will be foundational research and not tied   \n1094 to particular applications, let alone deployments. However, if there is a direct path to   \n1095 any negative applications, the authors should point it out. For example, it is legitimate   \n1096 to point out that an improvement in the quality of generative models could be used to   \n1097 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1098 that a generic algorithm for optimizing neural networks could enable people to train   \n1099 models that generate Deepfakes faster.   \n1100 \u2022 The authors should consider possible harms that could arise when the technology is   \n1101 being used as intended and functioning correctly, harms that could arise when the   \n1102 technology is being used as intended but gives incorrect results, and harms following   \n1103 from (intentional or unintentional) misuse of the technology.   \n1104 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1105 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1106 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1107 feedback over time, improving the efficiency and accessibility of ML).   \n1108 11. Safeguards   \n1109 Question: Does the paper describe safeguards that have been put in place for responsible   \n1110 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1111 image generators, or scraped datasets)?   \n1112 Answer: [NA]   \n1113 Justification: The ultimate goal of our proposed method is not to propose any model,   \n1114 but to propose a valuable theory of multi-modal learning. The training data used only   \n1115 includes matching, classification and detection, and the data sets are all open source data   \n1116 sets. Therefore, as far as this article is concerned, there is no risk of abuse.   \n1117 Guidelines:   \n1118 \u2022 The answer NA means that the paper poses no such risks.   \n1119 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1120 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1121 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1122 safety filters.   \n1123 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1124 should describe how they avoided releasing unsafe images.   \n1125 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1126 not require this, but we encourage authors to take this into account and make a best   \n1127 faith effort.   \n1128 12. Licenses for existing assets ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "1129 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1130 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1131 properly respected?   \n1132 Answer: [Yes]   \n1133 Justification: We annotated any sources in detail.   \n1134 Guidelines:   \n1135 \u2022 The answer NA means that the paper does not use existing assets.   \n1136 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1137 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1138 URL.   \n1139 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1140 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1141 service of that source should be provided.   \n1142 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1143 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1144 has curated licenses for some datasets. Their licensing guide can help determine the   \n1145 license of a dataset.   \n1146 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1147 the derived asset (if it has changed) should be provided.   \n1148 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1149 the asset\u2019s creators.   \n1150 13. New Assets   \n1151 Question: Are new assets introduced in the paper well documented and is the documentation   \n1152 provided alongside the assets?   \n1153 Answer: [No]   \n1154 Justification: For the code and resources involved in the full text, we only provide our   \n1155 original parts, such as our methods and our reproduced methods. For resources that already   \n1156 exist (e.g., the feature extractor code), the reader should follow the documentation. Since   \n1157 the concept we propose contains some conclusions that should be tried by the reader (such   \n1158 as the optimal dimension), the specific training procedure should also be designed by the   \n1159 reader.   \n1160 Guidelines:   \n1161 \u2022 The answer NA means that the paper does not release new assets.   \n1162 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1163 submissions via structured templates. This includes details about training, license,   \n1164 limitations, etc.   \n1165 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1166 asset is used.   \n1167 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1168 create an anonymized URL or include an anonymized zip file.   \n1169 14. Crowdsourcing and Research with Human Subjects   \n1170 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1171 include the full text of instructions given to participants and screenshots, if applicable, as   \n1172 well as details about compensation (if any)?   \n1173 Answer: [NA]   \n1174 Justification: Our work does not involve any human subjects.   \n1175 Guidelines:   \n1176 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1177 human subjects.   \n1178 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1179 tion of the paper involves human subjects, then as much detail as possible should be   \n1180 included in the main paper.   \n1181 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1182 or other labor should be paid at least the minimum wage in the country of the data   \n1183 collector.   \n1184 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1185 Subjects   \n1186 Question: Does the paper describe potential risks incurred by study participants, whether   \n1187 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1188 approvals (or an equivalent approval/review based on the requirements of your country or   \n1189 institution) were obtained?   \n1190 Answer: [NA]   \n1191 Justification: Our work does not involve any human subjects.   \n1192 Guidelines:   \n1193 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1194 human subjects.   \n1195 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1196 may be required for any human subjects research. If you obtained IRB approval, you   \n1197 should clearly state this in the paper.   \n1198 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1199 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1200 guidelines for their institution.   \n1201 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1202 applicable), such as the institution conducting the review. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}]