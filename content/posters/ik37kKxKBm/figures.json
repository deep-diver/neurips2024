[{"figure_path": "ik37kKxKBm/figures/figures_4_1.jpg", "caption": "Figure 1: The structure of a one-layer transformer with multi-head softmax attention.", "description": "This figure shows the architecture of a one-layer transformer with multi-head softmax attention.  The input consists of a prompt matrix E<sup>P</sup> containing N query-answer pairs and a query matrix E<sup>Q</sup> containing K-N unseen tokens. The prompt and query embeddings are processed through multi-head attention mechanism, where the query, key, and value matrices are W<sup>Q</sup>, W<sup>K</sup>, and W<sup>V</sup>, respectively.  The output is a prediction matrix \u0177 containing K predicted labels for all tokens. The figure highlights the use of softmax function for attention weight calculation and multi-head attention for richer representation learning.  The main processing steps involve query-key matrix multiplication, softmax normalization, value matrix multiplication, and the final linear transformation.", "section": "Transformer architecture"}, {"figure_path": "ik37kKxKBm/figures/figures_12_1.jpg", "caption": "Figure 2: Training and inference losses of (a) 1-layer and (b) 4-layer transformers, which validate Theorem 2, as well as the transformer's contextual generalization to unseen examples and to unseen tasks.", "description": "This figure shows the training and inference losses for both one-layer and four-layer transformers.  The training loss curves show a clear downward trend indicating successful learning.  The inference loss is measured both for in-domain (seen during training) and out-of-domain (unseen during training) examples. The results support Theorem 2 in the paper and demonstrate the model's ability to generalize to both unseen examples and unseen tasks.  Note that the out-of-domain inference loss is consistently higher than the in-domain inference loss, as expected.", "section": "A Experiments"}, {"figure_path": "ik37kKxKBm/figures/figures_13_1.jpg", "caption": "Figure 3: The performance gap \\(||\\hat{y} - \\hat{y}_{\\text{best}}\\||_2\\) with different N when m = 100, which validates that the closer N is to m, the better the transformer\u2019s prediction is.", "description": "The figure shows the performance gap between the transformer's prediction and the best possible prediction using ridge regression, plotted against the number of examples in the prompt (N). The vertical dashed line indicates when N equals m (number of basis functions). The plot shows that the performance gap is smallest when N is close to m, suggesting that the transformer's performance improves as the number of examples in the prompt approaches the number of basis functions.", "section": "Experiments"}, {"figure_path": "ik37kKxKBm/figures/figures_13_2.jpg", "caption": "Figure 4: Training losses of the 1-layer transformer with different number of attention heads H, where H should be large enough to guarantee the convergence of the training loss, but setting H too large leads to instability and slower divergence.", "description": "This figure shows how the number of attention heads (H) in a one-layer transformer affects its training performance.  The x-axis represents the number of training iterations, and the y-axis shows the population loss.  Multiple lines are plotted, each representing a different value of H. The results show that a small H leads to slower convergence, while too large an H leads to instability and divergence.  There is an optimal range of H values for best training performance. This is consistent with the paper's theoretical analysis demonstrating that the number of heads in the transformer affects its training and generalization performance.", "section": "A Experiments"}, {"figure_path": "ik37kKxKBm/figures/figures_14_1.jpg", "caption": "Figure 4: Training losses of the 1-layer transformer with different number of attention heads H, where H should be large enough to guarantee the convergence of the training loss, but setting H too large leads to instability and slower divergence.", "description": "This figure shows the training losses for a one-layer transformer model with varying numbers of attention heads (H).  The x-axis represents the number of attention heads, and the y-axis shows the final training loss after a fixed number of training iterations.  The plot shows that a minimum number of attention heads is required to achieve convergence. However, using too many heads leads to instability and slower convergence. This highlights a trade-off between model capacity and training stability in the context of multi-head attention mechanisms.", "section": "A Experiments"}, {"figure_path": "ik37kKxKBm/figures/figures_14_2.jpg", "caption": "Figure 5: Training losses of the 4-layer transformer with different number of attention heads H, where H should be large enough to guarantee the convergence of the training loss, but setting H too large leads to instability and slower divergence.", "description": "This figure shows how the number of attention heads (H) affects the training loss of a 4-layer transformer.  It demonstrates that a sufficiently large H is needed for convergence, but excessively large values of H lead to instability and slower convergence. The experiment fixes the wall-clock time to 100 seconds and plots the training loss for different values of H, illustrating the trade-off between convergence speed and model stability.", "section": "A Experiments"}]