{"references": [{"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "publication_date": "2020-05-01", "reason": "This paper provides a comprehensive overview of offline reinforcement learning, establishing the foundation and context for the current research."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Stabilizing off-policy Q-learning via bootstrapping error reduction", "publication_date": "2019-12-01", "reason": "This paper addresses the critical issue of off-policy evaluation instability in offline RL, a problem directly tackled by the current work."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-12-01", "reason": "This paper introduces a conservative Q-learning algorithm that is highly relevant to the current work's focus on improving offline RL performance."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "A minimalist approach to offline reinforcement learning", "publication_date": "2021-12-01", "reason": "This paper offers a simplified yet effective offline RL approach, providing a relevant baseline and context for evaluating the proposed method."}, {"fullname_first_author": "Misha Laskin", "paper_title": "Reinforcement learning with augmented data", "publication_date": "2020-12-01", "reason": "This paper explores data augmentation in reinforcement learning, providing a direct foundation for the current work's generative trajectory augmentation approach."}]}