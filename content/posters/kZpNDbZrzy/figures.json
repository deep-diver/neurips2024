[{"figure_path": "kZpNDbZrzy/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of noise injection [11, 12], generative data augmentation [13] and GTA.", "description": "This figure compares three different data augmentation methods for offline reinforcement learning: noise injection (e.g., RAD, S4RL), generative data augmentation (e.g., SynthER), and the proposed Generative Trajectory Augmentation (GTA).  It visually represents how each method augments the original data distribution (represented by beige ellipses) to include high-rewarding datapoints (represented by blue ellipses). Noise injection methods add minimal noise to existing data points, while generative methods synthesize new data points, often with limited success in exploring high-reward areas.  GTA is shown to generate new high-rewarding datapoints that are plausibly connected to the original data, better aligning with the aims of offline RL.", "section": "1 Introduction"}, {"figure_path": "kZpNDbZrzy/figures/figures_3_1.jpg", "caption": "Figure 2: Overall framework of the GTA comprises 3 major stages. In the first stage, we train a conditional diffusion model designed for generating trajectories. Following this, We perturb the original trajectory and subsequently denoise it using the trained diffusion model, conditioned by amplified return. Lastly, we employ the augmented dataset to train various offline RL algorithms.", "description": "This figure illustrates the three main stages of the Generative Trajectory Augmentation (GTA) method. Stage A involves training a conditional diffusion model to generate trajectories.  Stage B augments the trajectories by partially adding noise using a diffusion model's forward process and then denoising it with amplified return guidance (pushing trajectories towards higher rewards). Stage C uses the augmented dataset to train offline reinforcement learning (RL) algorithms (TD3BC, IQL, CQL, MCQ).", "section": "4 Method"}, {"figure_path": "kZpNDbZrzy/figures/figures_4_1.jpg", "caption": "Figure 3: Mechanism of the partial noising and denoising framework. The extent of exploration increases with \u03bc (\u03bc1 < \u03bc2 < \u03bc3). During denoising, amplified return guidance shifts trajectories towards the high-rewarding region.", "description": "This figure illustrates the mechanism of GTA's data augmentation process, which involves two main steps: partial noising and guided denoising.  The partial noising process adds noise to the original trajectory, controlled by the parameter \u03bc, allowing for exploration of new state-action spaces.  The level of exploration increases as \u03bc increases. The guided denoising process uses amplified return guidance to direct the trajectory towards high-rewarding areas, enhancing exploitation of already learned knowledge.  The figure visually represents this process by showing how trajectories are shifted towards the high-rewarding region during denoising, while preserving plausibility by starting from a partially noised version of the original trajectory. ", "section": "4.2 Stage B: Augment Trajectory-level Data"}, {"figure_path": "kZpNDbZrzy/figures/figures_7_1.jpg", "caption": "Figure 4: (a), (b) D4RL normalized score across different noise levels over the course of training TD3BC on halfcheetah-medium-v2 and halfcheetah-medium-expert-v2. (c) Comparison oracle reward sum of subtrajectory between conditioning strategy on halfcheetah-medium-v2.", "description": "This figure demonstrates the ablation study on the impact of partial noising (different noise levels) and amplified return guidance. (a) and (b) show the training curves of TD3BC on two different D4RL datasets (halfcheetah-medium-v2 and halfcheetah-medium-expert-v2) with various noise levels. (c) shows the distribution of oracle rewards (sum of subtrajectories) for three different conditioning strategies: amplified conditioning (GTA), fixed conditioning, and unconditioning.", "section": "5.3 Ablation Studies"}, {"figure_path": "kZpNDbZrzy/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison of noise injection [11, 12], generative data augmentation [13] and GTA.", "description": "This figure compares three different data augmentation methods: noise injection, generative data augmentation, and the proposed Generative Trajectory Augmentation (GTA). Noise injection adds small noise to existing data, leading to only minimal improvement. Generative data augmentation generates new data points, but it is limited by the original data distribution.  GTA leverages a diffusion model to create trajectories that are both high-rewarding and dynamically plausible, outperforming the other methods.", "section": "1 Introduction"}, {"figure_path": "kZpNDbZrzy/figures/figures_22_1.jpg", "caption": "Figure 3: Mechanism of the partial noising and denoising framework. The extent of exploration increases with \u03bc (\u03bc1 < \u03bc2 < \u03bc3). During denoising, amplified return guidance shifts trajectories towards the high-rewarding region.", "description": "This figure illustrates the mechanism of GTA's partial noising and denoising framework. The left part shows how partial noising with different noise levels (\u03bc) increases the exploration of the trajectory generation process. The right part shows how the denoising with amplified return guidance pushes the noised trajectory towards the high-rewarding region. The manifold of feasible trajectories is shown to indicate the relationship between the original trajectory and the generated trajectory.", "section": "4.2 Stage B: Augment Trajectory-level Data"}, {"figure_path": "kZpNDbZrzy/figures/figures_25_1.jpg", "caption": "Figure 1: Comparison of noise injection [11, 12], generative data augmentation [13] and GTA.", "description": "This figure compares three different data augmentation methods: noise injection, generative data augmentation, and the proposed Generative Trajectory Augmentation (GTA). Noise injection adds minimal noise to the existing data, resulting in small local changes. Generative data augmentation synthesizes new data points, but these points tend to cluster near the original data distribution. GTA generates high-rewarding trajectories that are dynamically plausible while exploring novel states or actions.", "section": "1 Introduction"}, {"figure_path": "kZpNDbZrzy/figures/figures_25_2.jpg", "caption": "Figure 2: Overall framework of the GTA comprises 3 major stages. In the first stage, we train a conditional diffusion model designed for generating trajectories. Following this, We perturb the original trajectory and subsequently denoise it using the trained diffusion model, conditioned by amplified return. Lastly, we employ the augmented dataset to train various offline RL algorithms.", "description": "The figure illustrates the three main stages of the Generative Trajectory Augmentation (GTA) method. Stage A involves training a conditional diffusion model that can generate trajectories. Stage B uses this model to augment the offline data. It partially adds noise to the original trajectories using the diffusion forward process, then denoises them with classifier-free guidance conditioned on amplified return values, guiding the trajectories towards high-rewarding regions. Finally, Stage C trains an offline reinforcement learning algorithm using the augmented dataset.", "section": "4 Method"}, {"figure_path": "kZpNDbZrzy/figures/figures_26_1.jpg", "caption": "Figure 2: Overall framework of the GTA comprises 3 major stages. In the first stage, we train a conditional diffusion model designed for generating trajectories. Following this, We perturb the original trajectory and subsequently denoise it using the trained diffusion model, conditioned by amplified return. Lastly, we employ the augmented dataset to train various offline RL algorithms.", "description": "This figure illustrates the three main stages of the Generative Trajectory Augmentation (GTA) method.  Stage A involves training a conditional diffusion model to generate trajectories. Stage B augments the trajectories by partially adding noise using a diffusion process, then denoising using amplified return guidance to direct the trajectories towards higher rewards.  Finally, Stage C uses the augmented dataset to train various offline reinforcement learning (RL) algorithms. The figure shows a visual representation of data flow between the stages and the processes involved.", "section": "4 Method"}, {"figure_path": "kZpNDbZrzy/figures/figures_26_2.jpg", "caption": "Figure 4: (a), (b) D4RL normalized score across different noise levels over the course of training TD3BC on halfcheetah-medium-v2 and halfcheetah-medium-expert-v2. (c) Comparison oracle reward sum of subtrajectory between conditioning strategy on halfcheetah-medium-v2.", "description": "This figure shows the ablation study of the noising ratio (\u03bc) and amplified return guidance on the halfcheetah environment.  (a) and (b) are learning curves of TD3BC trained with different levels of noising ratios on halfcheetah-medium-v2 and halfcheetah-medium-expert-v2. (c) shows the distribution of oracle rewards from the offline dataset, generated trajectories using fixed guidance, amplified return guidance and unconditioning.", "section": "5.3 Ablation Studies"}, {"figure_path": "kZpNDbZrzy/figures/figures_27_1.jpg", "caption": "Figure 1: Comparison of noise injection [11, 12], generative data augmentation [13] and GTA.", "description": "This figure compares three different data augmentation methods: noise injection, generative data augmentation, and the proposed method, Generative Trajectory Augmentation (GTA). Noise injection adds minimal noise to the existing data, resulting in only minor changes and limited exploration. Generative data augmentation uses a generative model to create new data, but it is constrained by the distribution of the existing data. GTA, on the other hand, uses a diffusion model to generate new and high-rewarding trajectories that are both plausible and dynamic. The figure shows that GTA is able to generate data that is significantly different from the original data and that it is better at exploring the state-action space.", "section": "1 Introduction"}]