[{"figure_path": "kZpNDbZrzy/tables/tables_6_1.jpg", "caption": "Table 1: Normalized average scores on Gym locomotion and maze tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 8 seeds.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning algorithms on various Gym locomotion and maze tasks.  The algorithms are tested with and without data augmentation techniques (None, S4RL, SynthER, and GTA).  The scores represent the mean and standard deviation across eight independent random seeds, highlighting the effectiveness of different methods. The highest score for each task and algorithm is shown in bold.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_6_2.jpg", "caption": "Table 2: Normalized average scores on complex robotics tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 8 seeds.", "description": "This table presents the average performance scores of different offline reinforcement learning algorithms on complex robotics tasks.  The algorithms are tested with and without data augmentation using GTA, S4RL, and SynthER. The table shows the mean and standard deviation of the scores across 8 different random seeds.  The tasks included are Adroit (pen-human and door-human) and FrankaKitchen (partial, mixed, and complete). The bold values represent the best-performing methods for each task.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_7_1.jpg", "caption": "Table 3: Normalized average scores on pixel-based observation tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 6 seeds.", "description": "This table presents the results of experiments conducted on pixel-based observation tasks using different data augmentation methods and offline reinforcement learning algorithms. The table shows the average scores achieved by each method across various datasets (medium, medium-replay, medium-expert, and expert).  Higher scores indicate better performance.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_8_1.jpg", "caption": "Table 1: Normalized average scores on Gym locomotion and maze tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 8 seeds.", "description": "This table presents the results of experiments conducted on various Gym locomotion and maze tasks using different offline reinforcement learning algorithms.  The algorithms were tested with and without data augmentation techniques (None, S4RL, SynthER, and GTA).  The table shows the normalized average scores achieved by each algorithm on each task. The highest scores for each task are bolded. The mean and standard deviation of the scores across eight different random seeds are reported for each algorithm and augmentation method.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_8_2.jpg", "caption": "Table 5: D4RL normalized score on medium and medium-replay quality locomotion environments with fixed \u03b1 and \u03bc. The experiments are conducted with TD3BC.", "description": "This table presents the D4RL normalized scores achieved by different offline RL algorithms (None, SynthER, GTA with \u03bc=0.5 and \u03b1=1.3, GTA with \u03bc=0.75 and \u03b1=1.3) on three locomotion tasks (Halfcheetah, Hopper, Walker2d) using medium and medium-replay datasets.  The scores represent the average performance across multiple trials, along with standard deviations, indicating the variability in performance for each algorithm on each task and dataset.", "section": "5.5 Guideline for Selecting \u03bc and \u03b1"}, {"figure_path": "kZpNDbZrzy/tables/tables_14_1.jpg", "caption": "Table 6: Hyperparameter setting for backbone of diffusion model for GTA", "description": "This table shows the hyperparameters used for training the backbone diffusion model in the Generative Trajectory Augmentation (GTA) method.  It includes parameters such as batch size, optimizer (Adam), learning rate, learning rate schedule (Cosine Annealing Warmup), number of training steps, and conditional dropout rate. These settings are crucial for effectively training the diffusion model to generate high-quality trajectories.", "section": "B Hyperparameters"}, {"figure_path": "kZpNDbZrzy/tables/tables_14_2.jpg", "caption": "Table 7: Hyperparameter setting of diffusion model of GTA for trajectory sampling", "description": "This table lists the hyperparameters used for sampling trajectories from the diffusion model in the Generative Trajectory Augmentation (GTA) method.  The hyperparameters are related to the Stochastic Differential Equation (SDE) sampler from the Elucidated Diffusion Model (EDM), including the number of diffusion steps, noise schedule parameters (\u03c3min, \u03c3max, Schurn, Smin, Smax, Snoise), and the guidance scale (w) for classifier-free guidance.  These settings control aspects of the trajectory generation process, balancing exploration and exploitation.", "section": "4.1 Stage A: Train Diffusion Model"}, {"figure_path": "kZpNDbZrzy/tables/tables_15_1.jpg", "caption": "Table 8: Hyperparameter setting for partial noising level \u03bc and guidance multiplier \u03b1", "description": "This table shows the hyperparameter search space used in the Generative Trajectory Augmentation (GTA) method.  It lists the ranges of values explored for three key hyperparameters: \u03b1 (guidance multiplier), \u03bc (partial noising level), and H (horizon length) across different environments. The reweighting strategy used is also specified for each environment.  The hyperparameter ranges reflect a balance between exploration and exploitation of the data during the augmentation process, with different ranges considered depending on task characteristics. This table is critical for understanding the experimental setup and how the algorithm was configured for different environments.", "section": "4.2 Stage B: Augment Trajectory-level Data"}, {"figure_path": "kZpNDbZrzy/tables/tables_15_2.jpg", "caption": "Table 9: Hyperparameter setting for reweighted sampling", "description": "This table shows the hyperparameters used for reweighted sampling in different locomotion environments.  Reweighting is applied to focus on high-rewarding regions in the dataset.  The hyperparameters include whether reweighting is used (indicated by 'O' for yes and 'X' for no), the number of bins (NB) used for splitting the return, a smoothing parameter (u), and a parameter (q) that determines the weighting between high and low-score bins.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_17_1.jpg", "caption": "Table 10: Ablations on the length of horizon H in halfcheetah-medium-v2", "description": "This table presents the results of ablation studies on the length of the horizon (H) used for trajectory-level generation in the GTA method. It shows the impact of varying H on both D4RL score and Dynamic MSE (dynamic mean squared error), a measure of how well the generated trajectories adhere to the environment dynamics.  The results demonstrate the importance of using sequential relationships within transitions (H > 1) for effective data augmentation, as shorter horizons lead to a significant increase in dynamic MSE and a decrease in performance.", "section": "E.1 Ablation on trajectory-level generation"}, {"figure_path": "kZpNDbZrzy/tables/tables_18_1.jpg", "caption": "Table 11: D4RL score of the TD3BC on sparse expert dataset. The results are calculated with 10 evaluations over 8 seeds.", "description": "This table presents the results of the TD3BC algorithm on a sparse expert dataset.  The performance is evaluated under various data augmentation techniques (None, Naive Duplication, S4RL, SynthER, GTA) using different ratios of expert to non-expert data (1:20 and 1:10). The results show that GTA significantly outperforms other methods and that even on a dataset of expert-only data, GTA provides a performance boost over other augmentation techniques. The experiment is done with 10 evaluation over 8 seeds.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_19_1.jpg", "caption": "Table 12: D4RL score of the TD3BC on small dataset. The results are calculated with 10 evaluations over 4 seeds.", "description": "This table presents the D4RL scores obtained using the TD3BC algorithm with varying amounts of offline data (5%, 10%, 15%, 20%, and 100%).  The results show the performance of TD3BC both with and without the proposed GTA augmentation method.  The scores represent the mean and standard deviation across 10 evaluations with 4 random seeds for each data size.  It demonstrates the sample efficiency of GTA when the size of the offline dataset is limited.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_19_2.jpg", "caption": "Table 1: Normalized average scores on Gym locomotion and maze tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 8 seeds.", "description": "This table presents the results of experiments conducted on various Gym locomotion and maze tasks.  The scores are normalized, meaning 0 represents a random policy and 100 an expert policy.  The table compares the performance of different offline reinforcement learning (RL) algorithms, with and without data augmentation techniques.  Each entry shows the mean and standard deviation of the normalized scores, averaged across 8 different random seeds.  The bold entries highlight the highest scores achieved for each task and augmentation method.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_19_3.jpg", "caption": "Table 14: Normalized evaluation scores of Decision Transformer on D4RL Maze 2d tasks. The results are calculated with 100 evaluations over 4 seeds.", "description": "This table presents the average scores achieved by the Decision Transformer algorithm on three different variations of the Maze2d environment from the D4RL benchmark dataset. The results are categorized by the augmentation method used (None, S4RL, GTA) and across three difficulty levels (umaze, medium, large). Each score represents the average performance over 100 evaluations with 4 random seeds, showcasing the impact of data augmentation on the performance of the Decision Transformer.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_20_1.jpg", "caption": "Table 15: Ablations on the reweighted sampling techniques", "description": "This table shows the ablation study result on reweighted sampling techniques. It shows the D4RL scores and oracle rewards with and without reweighted sampling on Hopper-medium-v2 and Walker2d-medium-v2 environments. The results indicate that reweighted sampling improves performance by increasing the D4RL scores and oracle rewards.", "section": "E.5 Ablation on Reweighted Sampling"}, {"figure_path": "kZpNDbZrzy/tables/tables_20_2.jpg", "caption": "Table 16: D4RL score comparison between two conditioning strategies, amplified return guidance of GTA and amplified reward inpainting", "description": "This table compares the performance of two different conditioning strategies for data augmentation using a diffusion model. The first strategy uses amplified return guidance as in the proposed GTA method, while the second uses amplified reward inpainting. The performance is measured using the D4RL score on three locomotion tasks: Halfcheetah-medium, Hopper-medium, and Walker2d-medium. The results show that amplified return guidance generally leads to higher D4RL scores compared to amplified reward inpainting, suggesting that the amplified return guidance approach is more effective in improving the performance of offline reinforcement learning algorithms.", "section": "E.2 Mixed-quality Dataset"}, {"figure_path": "kZpNDbZrzy/tables/tables_20_3.jpg", "caption": "Table 2: Normalized average scores on complex robotics tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 8 seeds.", "description": "This table presents the average performance results of different offline reinforcement learning algorithms on four complex robotics tasks: pen-human, door-human, FrankaKitchen-partial, and FrankaKitchen-complete.  The algorithms tested include the baseline with no data augmentation (None), data augmentation using S4RL, SynthER, and the proposed GTA method.  The highest average score for each task is highlighted in bold, and the mean and standard deviation are shown across 8 separate experimental runs (seeds).  This allows for a comparison of the effectiveness of the data augmentation techniques on complex robotics tasks.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_20_4.jpg", "caption": "Table 1: Normalized average scores on Gym locomotion and maze tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 8 seeds.", "description": "This table presents the results of experiments conducted on various Gym locomotion and maze tasks using different offline RL algorithms.  The algorithms were tested with and without data augmentation techniques (None, S4RL, SynthER, and GTA).  The table shows the normalized average scores achieved by each algorithm on each task, with the highest score for each task and augmentation method highlighted in bold.  The scores represent the mean and standard deviation across 8 separate trials (seeds).", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_21_1.jpg", "caption": "Table 19: Data quality comparison on halfcheetah-medium-v2 between distinct conditioning strategies: fixed conditioning, unconditioning, and amplified return conditioning.", "description": "This table presents a comparison of data quality metrics for different conditioning strategies used in the generative trajectory augmentation process.  The metrics include Dynamic Mean Squared Error (Dynamic MSE), which assesses the fidelity of the generated trajectories compared to real-world dynamics; Oracle reward, which measures the average reward of the generated trajectories; and Novelty, which quantifies the uniqueness of the generated trajectories compared to the original data.  The table contrasts the performance of three methods: a baseline (no conditioning), a fixed conditioning strategy (conditioning on a constant reward value), an unconditioned strategy (no conditioning), and the proposed amplified return conditioning strategy (conditioning on an amplified reward value).", "section": "F.1 Comparing Conditioning Strategies"}, {"figure_path": "kZpNDbZrzy/tables/tables_21_2.jpg", "caption": "Table 20: Welch's t-test result demonstrates statistically significant performance improvements of GTA over SynthER", "description": "This table presents the results of Welch's t-tests comparing the performance of GTA against SynthER across four different offline RL algorithms (TD3BC, IQL, CQL, and MCQ).  The p-values indicate the statistical significance of the performance differences, with smaller p-values suggesting stronger evidence that GTA outperforms SynthER.  All p-values are less than 0.05 indicating statistically significant improvement by GTA across all four algorithms.", "section": "F.2 Statistical Test Results for The Performance Boosting Effects of GTA"}, {"figure_path": "kZpNDbZrzy/tables/tables_22_1.jpg", "caption": "Table 21: Comparison of oracle rewards across gym locomotion tasks.", "description": "This table presents a quantitative comparison of the optimality achieved by three different data augmentation methods (S4RL, SynthER, and GTA) across various Gym locomotion tasks.  Optimality is measured using the oracle reward, representing the average reward obtained by querying the environment with generated states and actions. The results show the average oracle reward for each method across several different tasks, providing insights into the effectiveness of each augmentation strategy in generating high-reward trajectories.", "section": "5.4 Data Quality Analysis"}, {"figure_path": "kZpNDbZrzy/tables/tables_23_1.jpg", "caption": "Table 1: Normalized average scores on Gym locomotion and maze tasks, with the highest scores highlighted in bold. Each cell displays the mean and standard deviation across 8 seeds.", "description": "This table presents the results of experiments conducted on various Gym locomotion and maze tasks.  The table shows the normalized average scores achieved by different offline reinforcement learning (RL) algorithms with and without data augmentation techniques (None, S4RL, SynthER, and GTA). Each cell contains the mean and standard deviation of the scores obtained across eight independent seeds. The highest scores for each task and algorithm are highlighted in bold, allowing for easy comparison of the performance of different methods.", "section": "5 Experiments"}, {"figure_path": "kZpNDbZrzy/tables/tables_23_2.jpg", "caption": "Table 23: Comparison of Dynamic MSE across gym locomotion tasks.", "description": "This table presents a comparison of the Dynamic Mean Squared Error (Dynamic MSE) across various gym locomotion tasks.  Dynamic MSE is a metric used to assess the congruence of generated subtrajectories with the environment dynamics. Lower values indicate better agreement between generated and true dynamics. The table compares the Dynamic MSE for three different data augmentation methods: S4RL, SynthER, and GTA (the proposed method). The results are shown for different datasets, reflecting various difficulty levels in terms of data quality (medium, medium-replay, medium-expert).", "section": "5.3 Ablation Studies"}, {"figure_path": "kZpNDbZrzy/tables/tables_23_3.jpg", "caption": "Table 21: Comparison of the novelty of state and action across gym locomotion tasks.", "description": "This table presents a comparison of novelty scores achieved by three different data augmentation methods (S4RL, SynthER, and GTA) across various gym locomotion tasks.  Novelty, as a data quality metric, measures the uniqueness of the augmented trajectories compared to the original offline data.  Higher novelty scores indicate that the augmentation method generated trajectories that are more distinct from the original data.", "section": "5.4 Data Quality Analysis"}, {"figure_path": "kZpNDbZrzy/tables/tables_23_4.jpg", "caption": "Table 22: Comparison of the novelty of state and action across gym locomotion tasks.", "description": "This table presents a comparison of the novelty scores achieved by three different data augmentation methods (S4RL, SynthER, and GTA) across various gym locomotion tasks.  Novelty is a measure of how unique the generated trajectories are compared to the original dataset. The table shows the novelty scores for the state, action, and combined state-action for each augmentation method and task. Higher scores indicate greater novelty.", "section": "5.1 Experimental Setup"}, {"figure_path": "kZpNDbZrzy/tables/tables_24_1.jpg", "caption": "Table 26: Comparison with diffusion planners and offline model-based RL baselines with GTA. For baselines, we take results from their original papers. For GTA, we report score of TD3BC and CQL as base algorithms.", "description": "This table compares the performance of Generative Trajectory Augmentation (GTA) against several baseline methods, including diffusion planners (Diffuser, DD, AdaptDiffuser) and offline model-based RL algorithms (MOPO, MOREL, COMBO).  The comparison is done across various locomotion tasks from the D4RL benchmark, showing the average normalized scores for each method. TD3BC and CQL are used as the offline RL algorithms in the GTA approach. The table highlights GTA's superior performance compared to the baselines.", "section": "F.5 Comparisons of GTA with Offline Model-based RL and Diffusion Planners"}, {"figure_path": "kZpNDbZrzy/tables/tables_24_2.jpg", "caption": "Table 27: Comparison of computational cost between Diffusion planners, offline model-based RL baselines, and GTA. The elapsed time was measured using a single RTX 3090 on the HalfCheetah-medium-v2 environment.", "description": "This table compares the computational cost of GTA against other methods (diffusion planners and model-based RL algorithms).  It breaks down the time spent in each phase: model training, RL policy training, synthetic data generation, and policy evaluation.  The results highlight GTA's efficiency, especially in test-time evaluation.", "section": "F.5 Comparisons of GTA with Offline Model-based RL and Diffusion Planners"}, {"figure_path": "kZpNDbZrzy/tables/tables_26_1.jpg", "caption": "Table 28: Normalized average scores on D4RL MuJoCo locomotion tasks. Training with a larger batch size makes no difference without modification on the offline dataset.", "description": "This table presents the results of training TD3BC, CQL, and IQL algorithms on D4RL MuJoCo locomotion tasks using different batch sizes (256 and 1024).  It compares the performance (normalized average scores) across various datasets (medium, medium-replay, medium-expert) for each algorithm and batch size.  The results show that increasing the batch size doesn't significantly impact performance when there's no data augmentation.", "section": "G.1 Batch Size and Training Epochs for Offline RL Training"}, {"figure_path": "kZpNDbZrzy/tables/tables_27_1.jpg", "caption": "Table 28: Normalized average scores on D4RL MuJoCo locomotion tasks. Training with a larger batch size makes no difference without modification on the offline dataset.", "description": "This table presents the results of experiments conducted to assess the impact of varying batch sizes on the performance of different offline reinforcement learning algorithms. The experiments were performed on the D4RL MuJoCo locomotion tasks using TD3BC, CQL, and IQL algorithms with batch sizes of 256 and 1024. The results indicate that increasing the batch size does not significantly affect the performance when no modifications are made to the offline dataset.  The table is structured to show the average normalized scores for each algorithm and batch size across different tasks (Halfcheetah, Hopper, and Walker2d) with different data qualities (medium, medium-replay, and medium-expert).", "section": "G.1 Batch Size and Training Epochs for Offline RL Training"}]