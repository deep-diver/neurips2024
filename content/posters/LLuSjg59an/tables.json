[{"figure_path": "LLuSjg59an/tables/tables_14_1.jpg", "caption": "Table 1: A single continuous input sequence presented to the model for decoding a single test source sentence \"After you become comfortable with formatting ..\". Given the entire sequence as input, the model proceeds to generate the target sequence.", "description": "This table shows an example of a single continuous input sequence used for prompt examples in machine translation. The model receives the entire sequence as input and is expected to generate the target sequence.", "section": "4.1 Analysis Methodology: Layer-from Context Masking"}, {"figure_path": "LLuSjg59an/tables/tables_16_1.jpg", "caption": "Table 2: Performance when using trained attention head gates for Lo with regularisation \u03bb = .01. \u03bb = 0 refers to training without regularisation. 0 and 5 prompts were used in the context for training. We highlight values which are greater or worse than 0.5 BLEU points from baseline. Note that as these are compression experiments, we do not expect Lo to perform better than baseline.", "description": "This table presents the BLEU scores achieved using different training methods on the English-to-French and English-to-Portuguese translation tasks. The training methods involve using 0 or 5 prompts, with or without L0 regularization (\u03bb=0 or \u03bb=0.01). The table highlights the performance differences (greater than or less than 0.5 BLEU points) compared to the baseline.", "section": "A.5.3 Generalisability of Lo gate training"}, {"figure_path": "LLuSjg59an/tables/tables_17_1.jpg", "caption": "Table 3: 0-prompts with instructions, masking layer by layer of GPTNEO2.7B", "description": "This table presents the results of an experiment where the GPT-NEO 2.7B model was tested with 0 prompts (no examples) and instructions, while masking out different layers of the model.  The table shows the layer ID, language of translation, BLEU score, and the generated text for each layer. This helps illustrate the performance change at each layer when the context (examples and instructions) is incrementally removed from the attention process in the model.", "section": "4.2 Results"}, {"figure_path": "LLuSjg59an/tables/tables_18_1.jpg", "caption": "Table 3: 0-prompts with instructions, masking layer by layer of GPTNEO2.7B", "description": "This table presents the results of an experiment using the GPTNEO2.7B model.  The experiment involved masking different layers of the model while performing machine translation from English to French.  The prompt consisted of instructions but no examples.  The table shows the layer ID, language, BLEU score, and the model's generated text for each layer, allowing analysis of how masking different layers impacted translation performance.  The presence of \"NaN\" in the text column indicates that the model did not generate any coherent French translation.", "section": "A.6 Qualitative Analysis of Layer-wise Masking"}, {"figure_path": "LLuSjg59an/tables/tables_19_1.jpg", "caption": "Table 3: 0-prompts with instructions, masking layer by layer of GPTNEO2.7B", "description": "This table presents the results of an experiment where the GPTNEO 2.7B model was tested with 0 prompts and instructions, with layer-wise masking applied. Each row represents a specific layer, and the columns show the layer ID, language of the generated text, BLEU score, and the generated text itself. The experiment aims to understand how the model's performance changes as different layers of the model are masked.", "section": "4.2 Results"}, {"figure_path": "LLuSjg59an/tables/tables_20_1.jpg", "caption": "Table 3: 0-prompts with instructions, masking layer by layer of GPTNEO2.7B", "description": "This table presents the results of an experiment where the GPTNEO 2.7B model was tested with zero prompts and instructions, with layer-wise masking applied. The table shows the layer ID, language, BLEU score, and the generated text for each layer. This allows for an analysis of how the model's performance changes as different layers of the network are masked, providing insights into the role of individual layers in the translation process.", "section": "4.2 Results"}]