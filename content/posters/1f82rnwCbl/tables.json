[{"figure_path": "1f82rnwCbl/tables/tables_9_1.jpg", "caption": "Table 1: Win rates and average votes of our agents when playing the five-player ONUW game as Player 3.", "description": "This table presents the results of experiments conducted using five-player ONUW games. The RL-instructed (RL-ins.) and LLM-instructed (LLM-ins.) agents are compared against the baseline ReAct agent and two ablated versions (Belief and Random).  The table shows the win rates and average number of votes received by each agent when playing as Player 3. The results demonstrate that the RL-ins. agent generally outperforms other agents, achieving higher win rates and fewer votes received. The difference in performance between RL-ins. and LLM-ins. highlights the effectiveness of the RL-trained discussion policy.", "section": "6.4 Generalizability of the Discussion Policy"}, {"figure_path": "1f82rnwCbl/tables/tables_21_1.jpg", "caption": "Table 2: Statistics on game logs generated by GPT-4. Here the E, A and D represent the discussion tactic of Evidence, Accusation and Defense, respectively.", "description": "This table shows the percentage of different discussion tactics that GPT-4 selects for each initial role in the dataset.  It demonstrates GPT-4's tendency to be deceptive when playing on Team Werewolf and honest when playing on Team Village, aligning with game objectives.  The Robber and Insomniac show higher deceptive frequencies than other Team Village roles, potentially due to their ability to re-check their roles and adapt their discussion strategy.", "section": "E.1 Data Collection and Statistics"}, {"figure_path": "1f82rnwCbl/tables/tables_22_1.jpg", "caption": "Table 3: Training hyperparameters for CQL.", "description": "This table lists the hyperparameters used for training the Conservative Q-Learning (CQL) algorithm, including the learning rate, discount factor, mini-batch size, trade-off factor, critic number, target critic update interval, epoch number, steps per epoch, state dimension, and action dimension.", "section": "E Implementation and Experiment Details"}, {"figure_path": "1f82rnwCbl/tables/tables_22_2.jpg", "caption": "Table 4: Role changes in easy setting.", "description": "This table shows the initial and final roles of each player in the easy setting of the five-player ONUW game.  The changes reflect the actions taken during the night phase, such as the Robber switching roles with another player and the Troublemaker swapping the roles of two other players. This setting is used in the experiments to evaluate the performance of the proposed RL-instructed LLM-based agent.", "section": "6.3 Game Settings in Section 6.3"}, {"figure_path": "1f82rnwCbl/tables/tables_22_3.jpg", "caption": "Table 5: Role changes in hard setting.", "description": "This table shows the initial and final roles of each player in a hard setting of the five-player ONUW game.  The hard setting introduces additional complexity compared to the easy setting due to specific actions taken by the Robber and Troublemaker during the night phase, which changes the final roles of the players.", "section": "E.3 Game Settings in Section 6.3"}]