[{"figure_path": "g5DyqerUpX/figures/figures_9_1.jpg", "caption": "Figure 1: The test accuracy on hyper-cleaning with various SPARKLE-based algorithms using different corruption rates p. (Left: p = 0.1, Middle: p = 0.2, Right: p = 0.3.)", "description": "This figure displays the test accuracy achieved by different decentralized stochastic bilevel optimization algorithms on a hyper-cleaning task with varying corruption rates (p = 0.1, 0.2, 0.3).  The algorithms compared include various versions of SPARKLE (using Gradient Tracking, EXTRA, and Exact Diffusion), as well as D-SOBA, MA-DSBO-GT, and MDBO.  The plots show how the test accuracy changes over the number of gradient evaluations for each algorithm and corruption level. The results demonstrate that SPARKLE generally outperforms the other algorithms, and different strategies are suitable under different corruption rate.", "section": "4 Numerical experiments"}, {"figure_path": "g5DyqerUpX/figures/figures_9_2.jpg", "caption": "Figure 1: The test accuracy on hyper-cleaning with various SPARKLE-based algorithms using different corruption rates p. (Left: p = 0.1, Middle: p = 0.2, Right: p = 0.3.)", "description": "This figure displays the test accuracy achieved by various SPARKLE-based algorithms (SPARKLE-GT, SPARKLE-EXTRA, SPARKLE-ED, SPARKLE-ED-GT, SPARKLE-EXTRA-GT) in comparison with other decentralized SBO algorithms (D-SOBA, MA-DSBO-GT, MDBO) on the FashionMNIST hyper-cleaning task.  The x-axis represents the number of gradient evaluations, and the y-axis shows the test accuracy.  Three different corruption rates (p = 0.1, 0.2, 0.3) are shown across three subfigures.  It illustrates the superior performance of SPARKLE-EXTRA and SPARKLE-ED, and also demonstrates that using mixed strategies (ED/EXTRA with GT) can lead to similar accuracy compared with using ED or EXTRA only.", "section": "4 Numerical experiments"}, {"figure_path": "g5DyqerUpX/figures/figures_21_1.jpg", "caption": "Figure 4: The estimation error of D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLE-EXTRA under different networks and data heterogeneity.", "description": "This figure compares the performance of four different decentralized stochastic bilevel optimization algorithms (D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLE-EXTRA) under various network topologies (fully connected, 2D torus, and adjusted ring) and levels of data heterogeneity (severe and mild). The y-axis represents the estimation error, while the x-axis represents the number of gradient evaluations.  The results show that SPARKLE-ED and SPARKLE-EXTRA generally outperform D-SOBA and SPARKLE-GT across different network structures and heterogeneity levels, demonstrating their robustness and efficiency.", "section": "4 Numerical experiments"}, {"figure_path": "g5DyqerUpX/figures/figures_64_1.jpg", "caption": "Figure 4: The estimation error of D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLE-EXTRA under different networks and data heterogeneity.", "description": "This figure compares the performance of four decentralized stochastic bilevel optimization algorithms (D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLE-EXTRA) across different network topologies (fully connected, 2D torus, and adjusted ring) and levels of data heterogeneity (severe and mild).  The y-axis represents the estimation error (\u03a3N\u1d62=\u2081||x\u1d62(t) - x*||\u00b2), and the x-axis represents the number of gradient evaluations.  The results illustrate the relative performance of these algorithms under varying network structures and data heterogeneity, highlighting the robustness of SPARKLE-ED and SPARKLE-EXTRA.", "section": "4 Numerical experiments"}, {"figure_path": "g5DyqerUpX/figures/figures_65_1.jpg", "caption": "Figure 1: The test accuracy on hyper-cleaning with various SPARKLE-based algorithms using different corruption rates p. (Left: p = 0.1, Middle: p = 0.2, Right: p = 0.3.)", "description": "This figure shows the test accuracy achieved by various SPARKLE algorithms (SPARKLE-GT, SPARKLE-EXTRA, SPARKLE-ED, SPARKLE-ED-GT, and SPARKLE-EXTRA-GT) compared with other decentralized stochastic bilevel optimization algorithms (D-SOBA, MA-DSBO-GT, and MDBO) on the FashionMNIST dataset with different corruption rates (p = 0.1, 0.2, 0.3).  The x-axis represents the number of gradient evaluations and the y-axis represents the test accuracy. Each plot corresponds to a specific corruption rate, showing the algorithms' performance in data hyper-cleaning tasks under different levels of data corruption. ", "section": "4 Numerical experiments"}, {"figure_path": "g5DyqerUpX/figures/figures_66_1.jpg", "caption": "Figure 6: The average test accuracy of SPARKLE-EXTRA and SPARKLE-EXTRA-GT on hyper-cleaning with different communicating strategy of x, y, z.", "description": "This figure shows the results of hyperparameter cleaning experiments on the FashionMNIST dataset.  Two versions of the SPARKLE algorithm (EXTRA and EXTRA-GT) were tested with different communication strategies for updating the model parameters. In the first set of experiments, the upper level parameters (x) were updated using a fixed topology, while the lower level parameters (y, z) were updated using various topologies (different ring graphs). The second set of experiments reversed these settings:  The lower level (y, z) used a fixed topology, while the upper level (x) employed various topologies. The results demonstrate that the performance is more sensitive to the topology of the lower level parameters (y, z) than to that of the upper level parameters (x).", "section": "4 Numerical experiments"}, {"figure_path": "g5DyqerUpX/figures/figures_67_1.jpg", "caption": "Figure 7: The test loss against samples generated by one agent of different algorithms in the policy evaluation. (Left: n = 20, Right: n = 10.)", "description": "The figure shows the test loss for different decentralized bilevel optimization algorithms (SPARKLE-ED, SPARKLE-EXTRA, MDBO, SLDBO, and single-level ED) across various sample sizes (x-axis) for policy evaluation tasks. Two different network sizes are presented: n=10 (right panel) and n=20 (left panel). The test loss (y-axis) represents the average training loss over the last 500 iterations. The results highlight the convergence performance of different algorithms under varying data sizes and network configurations.", "section": "Numerical experiments"}, {"figure_path": "g5DyqerUpX/figures/figures_68_1.jpg", "caption": "Figure 8: The accuracy on training and testing set of different algorithms for the meta-learning problem.", "description": "The figure shows the training and testing accuracy of SPARKLE-ED, D-SOBA, and Decentralized MAML on the meta-learning problem.  It illustrates the performance of these algorithms over time, indicating the convergence and generalization abilities of each method.  SPARKLE-ED demonstrates superior performance compared to the other two algorithms, achieving higher accuracy on both training and testing datasets.", "section": "D.4 Decentralized meta-learning"}]