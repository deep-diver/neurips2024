[{"type": "text", "text": "Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ju $\\ensuremath{\\mathbf{Q}}^{\\ensuremath{\\mathbf{i}},2,\\dagger}$ , Hei $\\mathbf{Falin^{1,2}}$ , Feng $\\mathbf{Ting^{1,2}}$ , Yi Dengbing1,2, Fang Zhemei1,2,\u2020, Luo Yunfeng1,2 1. School of Artificial Intelligence and Automation, Huazhong University of Science and Technology 2. National Key Laboratory of Science and Technology on Multispectral Information Processing {juqi, heifalin, fenting, yidengbing, zmfang2018, yfluo}@hust.edu.cn \u2020 Corresponding author. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Counterfactual Regret Minimization (CFR) and its variants are widely recognized as effective algorithms for solving extensive-form imperfect information games. Recently, many improvements have been focused on enhancing the convergence speed of the CFR algorithm. However, most of these variants are not applicable under Monte Carlo (MC) conditions, making them unsuitable for training in largescale games. We introduce a new MC-based algorithm for solving extensive-form imperfect information games, called MCCFVFP (Monte Carlo Counterfactual Value-Based Fictitious Play). MCCFVFP combines CFR\u2019s counterfactual value calculations with fictitious play\u2019s best response strategy, leveraging the strengths of fictitious play to gain significant advantages in games with a high proportion of dominated strategies. Experimental results show that MCCFVFP achieved convergence speeds approximately $20\\%{\\sim}50\\%$ faster than the most advanced MCCFR variants in games like poker and other test games. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Game theory investigates mathematical models of strategic interactions among rational agents, aiming to identify a Nash Equilibrium (NE) where no participant can gain by deviating from the established strategy. For complete information games, it is possible to segment the game into smaller sub-games and apply backward induction to determine the equilibrium. In contrast, in incomplete information games, the inability to directly apply sub-game payoffs in the backward induction algorithm significantly increases the complexity of finding an equilibrium. ", "page_idx": 0}, {"type": "text", "text": "The Counterfactual Regret Minimization (CFR) algorithm Zinkevich et al. [2007] is crucial for solving extensive-form games with incomplete information. CFR has numerous variants, such as $\\mathrm{CFR+}$ Tammelin [2014], Lazy CFR Zhou et al. [2018], PCFR Farina et al. [2021], DCFR Brown and Sandholm [2019a], and PDCFR Xu et al. [2024]. Nevertheless, these variants typically require a full traversal of the entire game tree, which is impractical for real-world applications. In contrast, games like no-limit Texas hold\u2019em, DouDizhu, and Mahjong have $10^{162}$ Morav\u00edk et al. [2017], $10^{83}$ Zha et al. [2021], and $10^{121}$ Li et al. [2020] information sets, respectively, making full traversal infeasible. ", "page_idx": 0}, {"type": "text", "text": "To address this, Lanctot et al. Lanctot et al. [2010] proposed Monte Carlo CFR (MCCFR), which reduces the number of game tree nodes visited in each iteration through sampling. This makes MCCFR the preferred algorithm for training large-scale games. However, most CFR variants cannot improve the convergence speed in the case of MCCFR. Only a few variants, such as Discount MCCFR Brown and Sandholm [2019a] and VR-MCCFR Schmid et al. [2019], are currently suitable for MCCFR. Therefore, it is crucial to study the characteristics of large-scale games and how to leverage these characteristics to accelerate the convergence of MC-based game learning algorithms. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We believe that the Fictitious Play (FP) algorithm is highly suitable for addressing large-scale incomplete information games, due to the characteristics of using the best response (BR) strategy. FP was first introduced in Brown\u2019s 1951 article Berger [2007]; Brown [1951]. The treatise The Theory of Learning in Games Fudenberg and Levine [1998] consolidated prior research, setting a standardized framework for FP . Generalized Weakened Fictitious Play (GWFP) Leslie and Collins [2006] further established that under specific perturbations and errors, convergence to NE is attainable in a manner consistent with FP . Hendon et al. Hendon et al. [1996] expanded FP to the domain of extensive-form games. The development of the Full-Width Extensive-Form Fictitious Play (XFP) algorithm Heinrich et al. [2015], predicated on GWFP, has facilitated faster convergence in these games. ", "page_idx": 1}, {"type": "text", "text": "Our algorithm, Monte Carlo Counterfactual Value-Based Fictitious Play (MCCFVFP), optimizes FP in extensive-form games by incorporating counterfactual value into the BR strategy calculations. We have theoretically proven that MCCFVFP can converge to a NE, and experimentally demonstrated that in large-scale games, MCCFVFP fully leverages the high proportion of dominated strategies to achieve faster convergence speeds than MCCFR. Our code can be found at GitHub. ", "page_idx": 1}, {"type": "text", "text": "2 Notation and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Game Theory ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1.1 Normal-Form Game ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The normal-form game serves as the foundational model in Game Theory. Let ${\\mathcal N}=1,2,\\ldots,i,\\ldots$ represent the set of players, where each player $i$ has a finite action set $A^{i}$ . Player $i$ \u2019s strategy, denoted $\\sigma^{i}$ , is a probability distribution over $A^{i}$ and is represented by a $(|\\mathcal{A}^{i}|-\\mathbf{\\dot{1}{)}}$ -dimensional simplex. Here, $|\\cdot|$ indicates the cardinality of the set, and $\\sigma^{i}(a^{\\prime})$ denotes the probability with which player $i$ selects action $a^{\\prime}$ . Let $\\Sigma^{i}$ denote the strategy set for player $i$ , such that $\\sigma^{i}\\in\\Sigma^{i}$ . A strategy profile, $\\sigma\\ =\\textstyle\\prod_{i\\in\\mathcal{N}}\\sigma^{i}$ , represents the collection of strategies for all players, while $\\sigma^{-i}=(\\sigma^{1},\\dots,\\sigma^{i-1},\\sigma^{i+1},\\dots)$ includes all strategies in $\\sigma$ except for that of player $i$ . The entire set of strategy profiles is denoted by $\\begin{array}{r}{\\Sigma=\\prod_{i\\in\\mathcal{N}}\\Sigma^{i}}\\end{array}$ , where $\\sigma\\in\\Sigma$ . The payoff function for player $i$ , defined as $u^{i}:\\Sigma\\to\\mathbb{R}$ , is finite. The notation $u^{i}(\\sigma^{i},\\sigma^{-i})$ indicates the expected payoff to player $i$ when they select the pure strategy $\\sigma^{i}$ and all other players follow the strategy profile $\\sigma^{-i}$ . Finally, define the payoff interval of the game as $\\begin{array}{r}{L=\\operatorname*{max}_{\\sigma\\in\\Sigma,i\\in\\mathcal{N}}u^{i}(\\sigma)-\\operatorname*{min}_{\\sigma\\in\\Sigma,i\\in\\mathcal{N}}u^{i}(\\sigma).}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "2.1.2 Extensive-Form Games ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In extensive-form games, typically represented as game trees, the player set is $\\mathcal{N}=1,2,\\ldots$ Each node $s$ represents a possible state in the game, forming a set $\\boldsymbol{S}$ , with terminal states represented by leaf nodes $z\\in{\\mathcal{Z}}$ . At each state $s\\in S$ , the action set $\\boldsymbol{\\mathcal{A}}(s)$ includes all possible actions available to a player or chance. The player function $P:S\\to N\\cup c$ assigns each state an acting party, with $c$ indicating a chance event. Information sets $I\\in\\mathcal{T}^{i}$ group states that player $i$ cannot distinguish among, reflecting their uncertainty. The payoff function $\\bar{R_{\\mathrm{:}}}\\,\\bar{\\mathcal{Z}_{\\mathrm{~\\rightarrow~}}}\\mathbb{R}^{|\\mathcal{N}|}$ maps each terminal state to a payoff vector for the players. For each information set $I\\in\\mathcal{T}^{i}$ , the behavioral strategy $\\sigma^{i}(I)\\in\\bar{\\mathbb{R}}^{|\\bar{\\mathcal{A}}(I)|}$ defines a probability distribution over available actions. ", "page_idx": 1}, {"type": "text", "text": "2.1.3 Nash Equilibrium ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The BR strategy of player $i$ to their opponents\u2019 strategies $\\sigma^{-i}$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\nb^{i}(\\sigma^{-i})=\\arg\\operatorname*{max}_{a^{i}\\in\\mathcal{A}^{i}}u^{i}(a^{i},\\sigma^{-i}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "although a mixed strategy may also become a BR strategy, it is much easier to find a pure BR strategy than a mixed BR strategy in engineering implementation, so the BR strategies discussed in this article are all pure BR strategies. If there are multiple BR strategies at the same time, one of them will be returned randomly. Define the exploitability $\\overline{{\\epsilon}}^{i}$ of player $i$ in strategy profile $\\sigma$ as: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\epsilon^{i}=u^{i}(b^{i}(\\sigma^{-i}),\\sigma^{-i})-u^{i}(\\sigma),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "and total exploitability as $\\textstyle\\epsilon=\\sum_{i\\in{\\mathcal{N}}}\\epsilon^{i}$ . A Nash equilibrium is a strategy profile $\\sigma$ such that $\\epsilon=0$ ", "page_idx": 1}, {"type": "text", "text": "During iterations, if the strategy\u2019s exploitability satisfies $\\epsilon\\,\\propto\\,T^{-\\frac{1}{2}}$ , the convergence rate of the algorithm is $O\\left(T^{-{\\frac{1}{2}}}\\right)$ . Additionally, the time complexity within one iteration is a critical factor affecting the convergence rate. We use $\\mathcal{O}(\\cdot)$ to describe the time complexity needed for one iteration. ", "page_idx": 2}, {"type": "text", "text": "2.1.4 Dominated Strategy and Clear Games ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In game theory, strategic dominance occurs when one action (or strategy) is better than another action (or strategy) for one player, regardless of the opponents\u2019 actions. A classic example is the Prisoner\u2019s Dilemma, where choosing to testify always yields a higher payoff than staying silent, regardless of the opponent\u2019s decision. Here, staying silent is a dominated strategy. Formally, For player $i$ , if there is a pure strategy $a^{i*}$ and a strategy $\\bar{\\sigma^{i*}}\\in\\Sigma^{i}$ satisfies: ", "page_idx": 2}, {"type": "equation", "text": "$$\nu^{i}(a^{i*},\\sigma^{-i})\\leq u^{i}(\\sigma^{i*},\\sigma^{-i}),\\forall\\sigma^{-i}\\in\\Sigma^{-i},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "then the pure strategy $a^{i*}$ is a dominated strategy of player $i$ . Rational players will invariably avoid choosing a dominated strategy, allowing the elimination of dominated strategy from the action set $\\boldsymbol{A}^{i}$ without impacting the game\u2019s NE Samuelson [1992]. ", "page_idx": 2}, {"type": "text", "text": "In our analysis, the proportion of dominated strategies in a game is crucial, significantly impacting the convergence speed of various algorithms. We categorize games into two distinct types based on this factor. Games where the number of dominated strategies is less than $\\sqrt{|A|}$ are classified as clear games. This term captures a strategic landscape where the preferable choices are evident, owing to the relatively few dominated strategies. On the other hand, tangled games are defined by having a number of dominated strategies greater than $\\sqrt{|A|}$ , reflecting a more complex and nuanced strategic environment with less obvious choices. The reason for defining these categories in this way can be found in Appendix B.3. Such classification aids in comprehending a game\u2019s nature and assessing the effectiveness of different algorithmic approaches. ", "page_idx": 2}, {"type": "text", "text": "2.2 Regret Matching and Counterfactual Regret Minimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In normal-form games, let $\\sigma_{t}^{i}$ be the strategy used by player $i$ on round $t$ . The regret of player $i$ \u2019s action $a^{i}$ at time $T$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{T}^{i}(a^{i})=\\sum_{t=1}^{T}u^{i}\\left(a^{i},\\sigma_{t}^{-i}\\right)-u^{i}\\left(\\sigma_{t}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The new strategy is produced by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sigma_{T+1}^{i}(a^{i})=\\left\\{\\frac{R_{T}^{i,+}(a)}{\\sum_{a\\in A^{i}}R_{T}^{i,+}(a)}\\right.\\quad\\mathrm{if~}\\sum_{a\\in A^{i}}R_{T}^{i,+}(a)>0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $R_{T}^{i,+}(a)=\\mathrm{max}\\left(R_{T}^{i}(a),0\\right)$ . Since the probability of selecting action $\\sigma_{t+1}^{i}(a)$ is proportional to the non-negative regret value $R_{T}^{i,+}(a)$ for this action, this algorithm is referred to as the regret matching algorithm. Define the average strategy as $\\begin{array}{r}{\\bar{\\sigma}_{T}=\\frac{1}{T}\\sum_{t=1}^{T}\\sigma_{t}}\\end{array}$ , which converges to the NE as $T\\rightarrow\\infty$ , with a convergence rate of $O\\left(L{\\sqrt{|A|/T}}\\right)$ . In each iteration, RM requires a matrix multiplication operation, resulting in an iteration time complexity of $\\mathcal{O}\\left(\\lvert\\mathcal{A}\\rvert^{2}\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "CFR is a variant of RM specifically adapted for extensive-form games. The counterfactual value $u(I,\\sigma)$ is defined as the expected payoff assuming that information set $I$ is reached and all players follow strategy $\\sigma$ , with the exception that player $i$ acts specifically to reach $I$ . For each action $a\\in A^{i}(I)$ , let $\\sigma|_{I\\to a}$ represent a strategy profile identical to $\\sigma$ except that player $i$ consistently chooses action $a$ within information set $I$ . The counterfactual regret is then given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{T}^{i}(I,a)=\\sum_{t=1}^{T}\\pi_{\\sigma_{t}}^{-i}(I)\\left(u^{i}\\left(I,\\sigma_{t}|_{I\\rightarrow a}\\right)-u^{i}\\left(I,\\sigma_{t}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pi_{\\sigma_{t}}^{-i}(I)$ is the probability of information set $I$ occurring if all players (including chance, except $i$ ) choose actions according to $\\sigma_{t}$ . Let $R_{T}^{i,+}(I,a)=\\operatorname*{max}\\left(R_{T}^{i}(I,a),0\\right)$ , the strategy at time $T+1$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sigma_{T+1}^{i}(I,a)=\\left\\{\\frac{R_{T}^{i,+}(I,a)}{\\sum_{a\\in A(I)}R_{T}^{i,+}(I,a)}\\right.\\quad\\mathrm{if~}\\sum_{a\\in A^{i}}R_{T}^{i,+}(I,a)>0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The average strategy $\\bar{\\sigma}_{T}^{i}$ for an information set $I$ on iteration $T$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\sigma}_{T}^{i}(I)=\\frac{\\sum_{t=1}^{T}\\pi_{\\sigma_{t}}^{i}(I)\\sigma_{t}^{i}(I)}{\\sum_{t=1}^{T}\\pi_{\\sigma_{t}}^{i}(I)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Eventually, $\\bar{\\sigma}_{T}$ will converge to NE with $T\\to\\infty$ , the convergence rate of CFR is $\\mathcal{O}\\left(L|\\mathcal{Z}|\\sqrt{|A|/T}\\right)$ and the time complexity of one iteration of the CFR algorithm is $\\mathcal{O}\\left(|\\mathcal{A}||\\mathcal{T}|\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "There are many forms of sampling MCCFR, but the most common is external sampling MCCFR (ES-MCCFR) Brown [2020] because of its simplicity and powerful performance. In ES-MCCFR, some players are designated as traversers and others are samplers during an iteration. Traversers follow the CFR algorithm to update the regret and the average strategy of the experienced information set. On the rest of the sampler\u2019s nodes and the chance node, only one action is explored (sampled according to the player\u2019s strategy for that iteration on the information set), and the regret and the average strategy are not updated. ", "page_idx": 3}, {"type": "text", "text": "2.3 Fictitious Play ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "FP assuming all players start with random strategy profile $\\bar{\\sigma}_{t=1}$ , the strategy profile is updated following the iterative function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\sigma}_{t+1}=\\left(1-\\alpha_{t}\\right)\\bar{\\sigma}_{t}+\\alpha_{t}\\sigma_{t+1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where in vanilla $\\mathrm{FP}\\,\\alpha_{t}=1/(t+1)$ , $\\sigma_{t+1}=b(\\bar{\\sigma}_{t})$ . The convergence rate of vanilla FP in a two-player zero-sum game may be $\\mathcal{O}\\left(\\sqrt{T}\\right)$ Daskalakis and Pan [2014], and the time complexity of one iteration of the vanilla FP algorithm is $\\mathcal{O}\\left(\\lvert\\mathcal{A}\\rvert^{2}\\right)$ . To speed up iteration, define the average Q-value of player $i$ \u2019s action $a^{i}$ at time $T$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bar{Q}}_{T}^{i}(a^{i})=\\frac{1}{T}\\sum_{t=1}^{T}u^{i}\\left(a^{i},\\sigma_{t}^{-i}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If the game is a two-player game, or if $u^{i}$ is an affine function (e.g., in a potential game), we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\bar{Q}_{T}^{i}\\left(a^{i}\\right)=\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}u^{i}\\left(a^{i},\\sigma_{t}^{-i}\\right)=u\\left(a^{i},\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\sigma_{t}^{-i}\\right)}}\\\\ {{=u^{i}\\left(a^{i},\\bar{\\sigma}_{t}^{-i}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "therefore, we can conclude that ", "page_idx": 3}, {"type": "equation", "text": "$$\nb^{i}(\\sigma^{-i})=\\operatorname*{max}_{a^{i}\\in\\mathcal{A}^{i}}\\bar{Q}_{T}^{i}\\left(a^{i}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In 2.1.3, it is noted that BR strategy $\\sigma_{t}^{i}$ in FP is a pure strategy. As a result, calculating $u^{i}(a^{i},\\sigma_{t}^{-i})$ involves simply selecting a row (or column) from the payoff matrix, eliminating the need for matrix multiplication as required in RM. Consequently, the time complexity of each iteration in the FP algorithm decreases to $\\mathcal{O}(\\lvert\\mathcal{A}\\rvert)$ , representing a significant improvement over the $\\mathcal{O}(|A|^{2})$ complexity in RM. ", "page_idx": 3}, {"type": "text", "text": "3 Motivation of CFVFP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Currently, the mainstream approach for large-scale games is a combination of \u201cpre-trained blueprint strategy $^+$ real-time search.\u201d For example, Pluribus initially employs the MCCFR algorithm to establish a blueprint strategy, which it applies in the early stages of the game, and then transitions to real-time search as the game progresses Brown and Sandholm [2019b]. Similarly, in the game of Go, AlphaGo primarily used reinforcement learning to train its policy and value networks, incorporating a limited number of MCTS rollouts during training. However, in actual matches against human experts, AlphaGo significantly increased the number of MCTS rollouts to enhance its real-time decisionmaking Silver et al. [2016, 2017]. This phenomenon can be understood from two perspectives: ", "page_idx": 3}, {"type": "text", "text": "1. The complexity of real-world games is so high that it is impossible for any single strategy (even with deep networks) to perfectly handle all game scenarios. Due to this extreme complexity, only sampling-based training methods are feasible. Many current improvements to CFR, such as $\\mathrm{CFR+}$ and Predictive CFR, are designed for full traversal of the game tree. While these methods accelerate convergence, they are unsuitable for sampling-based approaches because the inexact payoffs introduced by sampling significantly disrupt the convergence direction of each strategy update. ", "page_idx": 4}, {"type": "text", "text": "2. Training a blueprint strategy usually starts with sampling from the initial nodes, resulting in a higher probability of exploring early game nodes compared to leaf nodes. Consequently, while the blueprint strategy might be less effective in the later stages of the game, it often rapidly converges towards the optimal solution in the early stages due to thorough exploration. ", "page_idx": 4}, {"type": "text", "text": "After clarifying the specific ideas for training large-scale game AI. Next, we need to think about the characteristics of large-scale games and how to design corresponding MC-type solving algorithms based on these characteristics. ", "page_idx": 4}, {"type": "text", "text": "One prominent characteristic of large-scale games is that the majority of strategies are dominated. For instance, in chess and Go, top AI systems and human experts often opt for fixed openings. Similarly, professional poker players tend to fold most hands in the early rounds. Theoretically, DeepMind likens game strategies to a spinning top Czarnecki et al. [2020], implying that only a limited subset of strategies can be considered non-dominated. MCCFR experiments show that up to $96\\%$ of strategies can be pruned in certain games Lanctot et al. [2010]. Likewise, the supplementary materials of Pluribus Brown and Sandholm [2019c] reveal that around $50\\%$ of information sets are never encountered during training (indicating that unplayed strategies are almost certainly dominated, with many traversed ones also being dominated). In other words, while it cannot be proven with absolute certainty, the choices of professional players and the experimental outcomes from current advanced AIs suggest that the larger the game, the higher the likelihood that it can be classified as a \u201cclear game.\u201d ", "page_idx": 4}, {"type": "text", "text": "Additionally, our toy experiment clearly demonstrates that the FP algorithm is more effective for clear games compared to the RM algorithm 1. We attribute this advantage to the fact that FP employs the BR strategy rather than the RM strategy during iterations, which is why we aim to incorporate this feature into the iterations of CFR. ", "page_idx": 4}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/57e9fa3dff1f53026d2c47861549a251780dad612438e6a709f3d97bfbdc3335.jpg", "img_caption": ["Figure 1: The figure compares the convergence rates of the RM and FP algorithms in a $100\\times100$ random payoff matrix game generated from a $N(0,1)$ distribution. In right figure, the convergence for a standard random payoff matrix is shown, while left figure illustrates the convergence in $100\\times100$ random payoff matrix where the payoffs for actions 1 to 10 are uniformly increased by 5 (causing actions 11 to 100 to have a high probability of being dominated strategies). It can be observed that in this setting, the convergence rate of the FP is very close to that of RM. Considering that the complexity of one FP iteration is only $\\mathcal{O}(\\lvert\\mathcal{A}\\rvert)$ compared to the complexity of RM, which is $\\mathcal{O}(|A|^{2})$ , in a clear game, the overall convergence rate of FP can actually surpass that of RM. Each scenario tested an average of 30 rounds. The shaded areas represent the $90\\%$ confidence intervals for these trials. The experiments in Appendix A can also confirm our view from another perspective. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 CFVFP Method ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Counterfactual Value Fictitious Play Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our method can be easily adapted from CFR. First, there is no need to compute the counterfactual regret; instead, we define the counterfactual value: ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{t}^{i}(I,a)=Q_{t-1}^{i}(I,a)+\\pi_{\\sigma_{t}}^{-i}(I)u^{i}\\left(I,\\sigma_{t}|_{I\\to a}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Second, the strategy in the next iteration is a BR strategy rather than RM strategy: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{t+1}^{i}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}^{i}(I)}Q_{t}^{i}(I,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The main distinction between $Q_{t}^{i}$ and $R_{t}^{i}$ is that $Q_{t}^{i}$ omits the average payoff term $u^{i}\\left(I,\\sigma_{t}\\right)$ , as this term does not affect identifying the maximum value of $Q_{t}^{i}$ . This omission leads to a significant reduction in computation time. Since arg max selects the action with the highest counterfactual value, the resulting strategy in each iteration is a pure strategy, similar to the FP algorithm. Thus, we refer to this algorithm as Counterfactual Value-Based Fictitious Play. Additionally, the time complexity of calculating the BR strategy is notably lower than that required for computing the RM strategy. ", "page_idx": 5}, {"type": "text", "text": "Finally, since the update $\\sigma_{t+1}^{i}$ is a pure strategy, $\\pi_{\\sigma_{t}}^{-i}(I)$ is either 0 or 1. This significantly increases the likelihood of triggering naive pruning. From equation 6, we see that if $\\pi_{\\sigma_{t}}^{-i}(I)=0$ , it is unnecessary to enter the sub-game tree to calculate $Q_{t}^{i}(I,a)$ . Similarly, equation 8 shows that if $\\pi_{\\sigma_{t}}^{i}(I)=0$ , it is unnecessary to update the average strategy $\\bar{\\sigma}_{t}^{i}(I)$ . This pruning greatly enhances the algorithm\u2019s efficiency. ", "page_idx": 5}, {"type": "text", "text": "CFVFP employs a simple yet effective approach by using BR strategy instead of regret-matching strategy for next iteration. The convergence of CFVFP can be easily proven. First, FP\u2019s convergence to a NE classifies it as a regret minimizer Abernethy et al. [2011], which also implies that FP satisfies Blackwell approachability Blackwell [1956]. The CFR framework shows that if an algorithm satisfies Blackwell approachability, its counterfactual regrets will converge to zero Zinkevich et al. [2007]. Therefore, CFVFP inherently adheres to Blackwell approachability. ", "page_idx": 5}, {"type": "text", "text": "Moreover, we can directly begin with the definition of Blackwell approachability and prove that FP fulfills this criterion. The convergence rate is $\\mathcal{O}\\left(L|\\mathcal{A}|T^{-\\frac{1}{2}}\\right)$ , and the detailed proof can be found in B.1. The pseudocode for the CFVFP algorithm is provided in Appendix C.1. ", "page_idx": 5}, {"type": "text", "text": "Since CFVFP follows Blackwell approachability, it is fully compatible with various CFR variants, including MCCFR, $\\mathrm{CFR+}$ , and different averaging schemes Brown and Sandholm [2019a], resulting in a range of CFVFP variants. Consequently, we conducted a series of experiments to examine the convergence rates of these CFVFP variants. As detailed in Appendix B, the vanilla-weighted MCCFVFP, a combination of MCCFR and CFVFP, shows the fastest convergence among the variants. The pseudocode for the MCCFVFP algorithm is provided in Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "4.2 Theoretical Analysis of MCCFVFP Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We discuss the advantages of the MCCFVFP algorithm in terms of the computing resources required per information set and the algorithm\u2019s pruning efficiency. ", "page_idx": 5}, {"type": "text", "text": "The MCCFVFP algorithm is highly efficient in conserving computing resources. For example, if an information set has $|{\\mathcal{A}}(I)|=x$ possible actions, MCCFVFP only requires $2x+1$ additions to process this information set, whereas MCCFR needs $6x-2$ additions and $3x$ multiplications to complete the same calculation. This means that, for a single information set, MCCFVFP requires only about $2/9$ of the computational time compared to MCCFR. Considering that a Blueprint training typically traverses at least 1 billion nodes, MCCFVFP offers significant advantages over MCCFR in terms of engineering implementation. For a detailed proof, refer to Appendix E.1. ", "page_idx": 5}, {"type": "text", "text": "Additionally, CFVFP is highly efficient in game tree pruning. Pruning is a common optimization technique used in all tree search algorithms. For instance, the Alpha-Beta algorithm in completeinformation extensive-form games is a pruned version of the Min-Max algorithm, and it was a key factor in Deep Blue\u2019s success Hsu [2002]. Naive pruning is the simplest pruning method in CFR. In naive pruning, if no player has a probability of reaching the current state $s$ $\\overline{{{\\mathrm{:}}}}\\;(\\forall i,\\overline{{{\\pi}}}_{\\sigma_{t}}^{-i}(s)=0)$ , the entire subtree at that state can be pruned for the current iteration without affecting regret calculations. ", "page_idx": 5}, {"type": "text", "text": "By analyzing the frequency of different node types in the game tree, we theoretically prove that CFVFP can significantly reduce the number of nodes that need to be processed. Compared to CFR, which must traverse all $\\mathcal{O}\\left(\\left\\vert S\\right\\vert\\right)$ nodes, CFVFP only needs to traverse $\\theta\\left({\\sqrt[||{\\mathcal{N}}|]{S}}\\right)$ nodes. This is especially beneficial in multiplayer games, where it greatly improves the efficiency of algorithm iterations. For detailed proof, please refer to Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Description of the Game and Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We employed various game models, such as Kuhn-extension poker, Leduc-extension poker, the princess and monster game Lanctot et al. [2010], and Texas Hold\u2019em, to evaluate the performance of different algorithms. These games are widely used benchmarks for comparing algorithmic convergence rates. Kuhn-extension poker, an expanded version of the classic Kuhn poker Kuhn [1950], features an increased card count of $x$ , a broader range of betting actions with $y$ options, and up to $z$ raising opportunities. Similarly, Leduc-extension poker is an advanced version of the original Leduc poker Shi and Littman [2002], allowing for flexible scaling. The princess and monster game represents a classic pursuit-evasion problem. Texas Hold\u2019em, one of the most popular poker games in the world, was also included in our analysis. ", "page_idx": 6}, {"type": "text", "text": "In Kuhn, Leduc, and princess and monster games, we use a random distribution as the initial strategy to initialize all algorithms. In each iteration, the strategies and regrets of all players are updated simultaneously. In MCCFR, when Rit,max = 0, the next stage strategy is set to a random pure strategy. In addition, the settings in our comparison experiment are consistent with those in previous groundbreaking works, including MCCFR Lanctot et al. [2010], $\\mathrm{DCFR+}$ Brown and Sandholm [2019a], and PCFR Farina et al. [2021]. For engineering implementation, any node with a probability less than $10^{-20}$ during iteration will be pruned. Our Texas Hold\u2019em experiments employed a variant of the multivalued state technique Brown et al. [2018], with an experimental setup closely mirroring that of Pluribus Brown and Sandholm [2019b]. The Texas Hold\u2019em experiments were run on a 32-core, 128GB memory server, while the other experiments were conducted on a single core. It is important to note that all times mentioned below have been converted to reflect execution on a single CPU core. ", "page_idx": 6}, {"type": "text", "text": "For a comprehensive exposition of these games and additional experimental findings, please refer to Appendix F. In Appendix G, we compare time, number of iterations, and nodes touched as an indicator. Finally, we use nodes touched and time as an indicator. ", "page_idx": 6}, {"type": "text", "text": "5.2 Experimental results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As shown in the Figure 2, we can demonstrate the core findings of our paper: ", "page_idx": 6}, {"type": "text", "text": "\u2022 In small-scale problems like vanilla Kuhn, algorithms such as DCFR, PCFR, and $\\mathrm{CFR+}$ may outperform MCCFR and MCCFVFP. However, as the game scale increases, the convergence speed of sampling-based algorithms (MCCFR/MCCFVFP) gradually surpasses that of full-traversal algorithms (DCFR/PCFR/CFR $^+$ ).   \n\u2022 In our experiments, MCCFVFP consistently converged faster than MCCFR. While theoretically, our algorithm might be less effective than MCCFR in tangled games, the acceleration in its implementation and the tendency of large-scale games to be clear games have led to MCCFVFP outperforming MCCFR in all tested scenarios. ", "page_idx": 6}, {"type": "text", "text": "Specifically, when using the number of nodes touched as a metric, MCCFVFP demonstrates a slightly faster convergence rate compared to ES-MCCFR. However, in terms of processing the same number of nodes, MCCFVFP\u2019s computation time is only about 2/9 of MCCFR\u2019s (the underlying reasons are discussed in Section E.1). As a result, when factoring in the time required for game simulation, MCCFVFP achieves approximately $50\\%$ time savings compared to ES-MCCFR for similar levels of exploitability in these games. ", "page_idx": 6}, {"type": "text", "text": "As previously noted, algorithms such as $\\mathrm{CFR+}$ , PCFR, and DCFR are not well-suited for large-scale games due to their reliance on full traversal, which is impractical in complex settings like Texas ", "page_idx": 6}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/45441a7c3d0e4b4bbba7ebd3fde21815243163af93c07a7e4fe705e34935a0fb.jpg", "img_caption": ["Figure 2: Convergence rates in Kuhn-extension, Leduc-extension, and princess-and-monster games are shown. In the first two rows, time is measured in milliseconds (ms). The last two rows reflect the same running time but with the horizontal axis representing the number of nodes touched during iteration. All experiments tested over an average of 30 rounds. The shaded areas indicate $90\\%$ confidence intervals for the trials. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Hold\u2019em. Therefore, in our Texas Hold\u2019em experiments, we focused on comparing the performance of our algorithm specifically against the traditional MCCFR method, excluding these full-traversal algorithms from testing. ", "page_idx": 7}, {"type": "text", "text": "In our experiments, we used 2, 3, and 6-player Texas Hold\u2019em game setups, where each player started with 25 Big Blinds (BB) in chips. We assumed that all players would check from the start of the game through to the river stage, simulating a conservative gameplay scenario to examine strategic strengths under limited betting dynamics. ", "page_idx": 7}, {"type": "text", "text": "In the two-player Texas Hold\u2019em game, convergence speed can be directly measured using exploitability. The results are shown in Table 1. The public cards in the experiment were 5d2s9d2c7c, and after abstracting the hands, there were 69 hand strength ranks. This sub-game contains approximately $89\\mathrm{k}$ information sets, with exploitability measured in BB/100. Compared to MCCFR, MCCFVFP achieved a $20\u201330\\%$ faster convergence speed within the same time frame in the two-player Texas Hold\u2019em game. ", "page_idx": 7}, {"type": "table", "img_path": "eFD9N5zdFC/tmp/14cf67ad66df36e9070317325586404d508b2dde21adce4a52fe0d4e76dc0285.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "eFD9N5zdFC/tmp/d81446af71cad4b9ce2002cf25c6b98118ab3cf9b70c401de307283f42b6e732.jpg", "table_caption": ["Table 1: The convergence rates in two-player Texas Hold\u2019em ", "Table 2: The results of different AIs competing against each other in multiplayer games "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In multiplayer situations, the exploitability cannot be directly calculated. Therefore, we use the method of mutual battles to measure the strength of different algorithms. As shown in Table 2, in the competition 1, MCCFVFP AI is randomly set as player $i$ , and all other players except player $i$ are set as MCCFR AI. Define $r_{1}$ as the rewards of the MCCFVFP AI player at competition 1. The second setting is exactly the opposite. Player $i$ is randomly set as MCCFR AI, and the remaining players are set as MCCFVFP AI. Define $r_{2}$ as the rewards of the MCCFR AI player at competition 2. By comparing $r_{1}$ and $r_{2}$ , we can roughly compare the convergence speeds of different algorithms. ", "page_idx": 8}, {"type": "text", "text": "In these scenarios, the MCCFVFP AI, trained for the same duration, significantly outperforms the MCCFR algorithm across all aspects. The 30 to 40 seconds training experiment is particularly important, as it closely mirrors the setup of the Pluribus experiment. In the 6-player Pluribus experiment, the AI trained with MCCFR gained an average of 3.2 BB/100 per game, with a standard error of 1.5 BB/100. In our experiment, using community cards KsQsJs3h2h, MCCFVFP gained an average of 0.932 BB/100 compared to MCCFR\u2014a substantial improvement, especially considering Pluribus gained 3.2 BB/100 against human players. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper introduces a novel method for solving large-scale incomplete information zero-sum games: Monte Carlo Counterfactual Value-Based Fictitious Play (MCCFVFP). By implementing the BR strategy in place of the regret-matching strategy, MCCFVFP achieved convergence speeds approximately $20\\%$ to $50\\%$ faster than the most advanced MCCFR variants. ", "page_idx": 8}, {"type": "text", "text": "In future research, we aim to evaluate the scalability of our method and its compatibility with different CFR variants, including those incorporating deep networks Brown et al. [2019]. Furthermore, given MCCFVFP\u2019s efficacy in clear games, we envision developing a warm start algorithm. This approach would initially use MCCFVFP to eliminate dominated strategies and then switch to algorithms that can further accelerate convergence in the later stages of training. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (Grant No. 62103158). We would also like to extend our gratitude to Thomas Tellier for his invaluable assistance with our Texas Hold\u2019em experiment. For more information, he can be reached via email at thomas@gtoking.com or through his website at gtoking.com. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jacob Abernethy, Peter Bartlett, and Elad Hazan. Blackwell approachability and no-regret learning are equivalent. 2011.   \nUlrich Berger. Brown\u2019s original fictitious play. Journal of Economic Theory, 135(1):572\u2013578, 2007.   \nDavid Blackwell. An analog of the minmax theorem for vector payoffs. Pacific Journal of Mathematics, 65(1):1\u20138, 1956.   \nNoam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted regret minimization. Proceedings of the AAAI Conference on Artificial Intelligence, 33:1829\u20131836, 2019.   \nNoam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885\u2013 890, 2019.   \nNoam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):eaay2400, 2019.   \nNoam Brown, Tuomas Sandholm, and Brandon Amos. Depth-limited solving for imperfectinformation games. Advances in Neural Information Processing Systems (NeurIPS), 31, 2018.   \nNoam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret minimization. In International conference on machine learning, pages 793\u2013802. PMLR, 2019.   \nG. W. Brown. Iterative solution of games by fictitious play. activity analysis of production and allocation, 1951.   \nNoam Brown. Equilibrium finding for large adversarial imperfect-information games. PhD thesis, 2020.   \nWojciech M Czarnecki, Gauthier Gidel, Brendan Tracey, Karl Tuyls, Shayegan Omidshafiei, David Balduzzi, and Max Jaderberg. Real world games look like spinning tops. Advances in Neural Information Processing Systems, 33:17443\u201317454, 2020.   \nConstantinos Daskalakis and Qinxuan Pan. A counter-example to karlin\u2019s strong conjecture for fictitious play. In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pages 11\u201320. IEEE, 2014.   \nGabriele Farina, Christian Kroer, and Tuomas Sandholm. Faster game solving via predictive blackwell approachability: Connecting regret matching and mirror descent. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 5363\u20135371, 2021.   \nDrew Fudenberg and David K. Levine. The theory of learning in games. MIT Press Books, 1, 1998.   \nJohannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In International Conference on Machine Learning, 2015.   \nEbbe Hendon, Hans J\u00f8rgen Jacobsen, and Birgitte Sloth. Fictitious play in extensive form games. Games and Economic Behavior, 15(2):177\u2013202, 1996.   \nHoane Feng Hsiung Hsu. Deep blue. Artificial Intelligence, 2002.   \nH. W Kuhn. Simplified two-person poker. Contributions to the Theory of Games, 1950.   \nMarc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H. Bowling. Monte carlo sampling for regret minimization in extensive games. In Advances in Neural Information Processing Systems 22, 2010.   \nDavid S. Leslie and E. J. Collins. Generalised weakened fictitious play. Games and Economic Behavior, 56(2):285\u2013298, 2006.   \nJunjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, and Hsiao Wuen Hon. Suphx: Mastering mahjong with deep reinforcement learning. 2020.   \nMatej Morav\u00edk, Martin Schmid, Neil Burch, Viliam Lis, and Michael Bowling. Deepstack: Expertlevel artificial intelligence in no-limit poker. Science, 356(6337):508, 2017.   \nLarry Samuelson. Dominated strategies and common knowledge. Games and Economic Behavior, 4(2):284\u2013313, 1992.   \nMartin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, and Michael Bowling. Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form games using baselines. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 2157\u20132164, 2019.   \nJiefu Shi and Michael L. Littman. Abstraction methods for game theoretic poker. In International Conference on Computers and Games, 2002.   \nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484\u2013489, 2016.   \nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550(7676):354\u2013359, 2017.   \nOskari Tammelin. Solving large imperfect information games using cfr+. Eprint Arxiv, 2014.   \nHang Xu, Kai Li, Bingyun Liu, Haobo Fu, Qiang Fu, Junliang Xing, and Jian Cheng. Minimizing weighted counterfactual regret with optimistic online mirror descent. arXiv preprint arXiv:2404.13891, 2024.   \nDaochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, and Ji Liu. Douzero: Mastering doudizhu with self-play deep reinforcement learning, 2021.   \nYichi Zhou, Tongzheng Ren, Jialian Li, Dong Yan, and Jun Zhu. Lazy-cfr: fast and near optimal regret minimization for extensive games with imperfect information. 2018.   \nM. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. Regret minimization in games with incomplete information. Oldbooks.nips.cc, 20:1729\u20131736, 2007. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A A Toy Example of RM not Performing Well in Clear Game ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In training, RM and its variants do not perform well in the clear games, which we attribute to excessive exploration of dominated strategies by RM. For instance, consider the rock-paper-scissors (RPS) game, where an additional action, Leaky Rock (LR), is introduced for player 1. Set the payoff of action LR to be 0.1 less than that of action Rock (R) (making LR a dominated strategy that no rational player would choose): ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c}{{0}}&{{-1}}&{{1}}\\\\ {{1}}&{{0}}&{{-1}}\\\\ {{-1}}&{{1}}&{{0}}\\end{array}\\right]\\xrightarrow{\\mathrm{add\\,action\\,LR}}\\left[\\begin{array}{c c c}{{0}}&{{-1}}&{{1}}\\\\ {{1}}&{{0}}&{{-1}}\\\\ {{-1}}&{{1}}&{{0}}\\\\ {{-0.1}}&{{-1.1}}&{{0.9}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Assuming that player 1 starts with an average strategy and player2 takes scissors in the first round, player 1\u2019s payoff is $u^{1}\\,=\\,0.225$ . The regret value is $R^{1}\\mathbf{\\dot{(R)}}\\,=\\,1\\,-\\,0.225\\,=\\,0.775$ , $R^{1}(\\mathbf{S})\\,=$ $0\\mathrm{~-~}0.225\\mathrm{~=~-0.225~}$ , $R^{1}(\\mathbf{P})=-1-0.225=-1.225$ , $R^{1}(\\mathrm{LR})=0.9-0.225=0.675$ , Under the RM algorithm, the probability of selecting Rock in the next iteration becomes: $\\sigma^{1}(\\mathbf{R})\\approx0.534$ , and for the Leaky Rock action: $\\sigma^{1}(\\mathrm{LR})\\approx\\bar{0}.466$ . Despite LR being a dominated strategy, RM still assigns it nearly half the selection probability, arguably wasting nearly half the computational resources. However, in Fictitious Play (FP), the LR action would never be chosen. ", "page_idx": 11}, {"type": "text", "text": "B Convergence of CFVFP ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Blackwell Approachability Game ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Definition 1 A Blackwell approachability game in normal-form two-player games can be described as a tuple (\u03a3, u, S1, S2), where \u03a3 is a strategy profile, u is the payoff function, and Si = R|\u2264A0i| i s $a$ closed convex target cone. The Player $i$ \u2019s regret vector of the strategy proflie $\\sigma$ is $R^{i}(\\sigma)\\in\\mathbb{R}^{|A^{i}|}$ , for each component $R^{i}(\\sigma)(a_{x})=u^{i}\\left(a_{x},\\sigma^{-i}\\right)-u^{i}\\left(\\sigma\\right)$ , $a_{x}\\in A^{i}$ the average regret vector for players i to take actions at $T$ time $a$ is $\\Bar{R}_{T}^{i}$ ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\bar{R}_{T}^{i}=\\frac{1}{T}\\sum_{t=1}^{T}R^{i}(\\sigma_{t})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "At each time $t$ , the two players interact in this order: ", "page_idx": 11}, {"type": "text", "text": "\u2022 Player 1 chooses a strategy $\\sigma_{t}^{1}\\in\\Sigma^{1}$ ;   \n\u2022 Player 2 chooses an action $\\sigma_{t}^{2}\\in\\Sigma^{2}$ , which can depend adversarially on all the $\\sigma^{t}$ output so far;   \n\u2022 Player 1 gets the vector value payoff $R^{1}(\\sigma_{t})\\in\\mathbb{R}^{l^{1}}$ . ", "page_idx": 11}, {"type": "text", "text": "The goal of Player $^{\\,l}$ is to select actions $\\sigma_{1}^{1},\\sigma_{2}^{1},\\dots~\\in~\\Sigma^{1}$ such that no matter what actions $\\sigma_{1}^{2},\\bar{\\sigma_{2}^{2}},\\dots\\overset{\\cdot}{\\in}\\Sigma^{2}$ played by Player 2, the average payoff vector converges to the target set $S^{1}$ . ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{s}\\in S^{1}}\\left\\|\\hat{s}-\\bar{R}_{T}^{1}\\right\\|_{2}\\to0\\quad a s\\quad T\\to\\infty\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Before explaining how to choose the action $\\sigma_{t}$ to ensure this goal achieve, we first need to define the forceable half-space: ", "page_idx": 11}, {"type": "text", "text": "Definition 2 Let $\\mathcal{H}~~\\subseteq~~\\mathbb{R}^{d}$ as half-space, that is, for some $\\textbf{\\textit{a}}\\in\\ \\mathbb{R}^{d}$ , $b~\\in~\\mathbb{R},~\\mathcal{H}~=$ $\\left\\{x\\in\\mathbb{R}^{d}:\\pmb{a}^{\\top}\\pmb{x}\\leq b\\right\\}$ . In Blackwell approachability games, the halfspace $\\mathcal{H}$ is said to be forceable if there exists a strategy $\\sigma^{i*}\\in\\Sigma^{i}$ of Player i that guarantees that the regret vector $R^{i}(\\sigma)$ is in $\\mathcal{H}$ no matter the strategy played by Player $-i$ , such that ", "page_idx": 11}, {"type": "equation", "text": "$$\nR^{i}\\left(\\sigma^{i\\ast},\\hat{\\sigma}^{-i}\\right)\\in\\mathcal{H}\\quad\\forall\\hat{\\sigma}^{-i}\\in\\Sigma^{-i}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "And $\\sigma^{i*}$ is forcing action for $\\mathcal{H}$ . ", "page_idx": 11}, {"type": "text", "text": "Blackwell\u2019s approachability theorem states the following. ", "page_idx": 11}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/8ff526abcb3ae7c72d17becc4704b940e2576e6a78c567db7d694f48afdb56d0.jpg", "img_caption": ["Figure 3: The difference between RM and FP (CFVFP in normal-form game) in a two-dimensional plane "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Theorem 1 Goal 17 can be attained if and only if every halfspace $\\mathcal{H}_{t}\\supseteq S$ is forceable. ", "page_idx": 12}, {"type": "text", "text": "The relationship between Blackwell approachability and no-regret learning is: ", "page_idx": 12}, {"type": "text", "text": "Theorem 2 Any strategy (algorithm) that achieves Blackwell approachability can be converted into an algorithm that achieves no-regret, and vice versa Abernethy et al. [2011] ", "page_idx": 12}, {"type": "text", "text": "Let $\\bar{\\sigma}_{T}^{i}$ be the average strategy of player $i$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\bar{\\sigma}_{T}^{i}(a)=\\frac{\\sum_{t=1}^{T}\\sigma_{t}^{i}(a)}{T}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In a two-player zero-sum game, the exploitability of the average strategy $\\bar{\\sigma}_{T}^{i}$ at time $T$ of player $i$ is $\\epsilon_{T}^{i}=\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}^{i}}\\bar{R}_{T}^{i}(a^{\\prime})$ Brown [2020]. Obviously, the regret value is always greater than the exploitability: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\epsilon_{T}=\\operatorname*{lim}_{T\\to\\infty}\\sum_{i\\in\\mathcal{N}}\\epsilon_{T}^{i}\\leq\\operatorname*{lim}_{T\\to\\infty}\\sum_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\hat{s}\\in S^{i}}\\left\\|\\hat{s}-R_{T}^{i}\\right\\|_{2}=0\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "So, if the algorithm achieves Blackwell approachability, the average strate\u221agy $\\bar{\\sigma}_{T}^{i}$ will converge to equilibrium with $T\\to\\infty$ . The rate of convergence is $\\epsilon_{T}^{i}\\leq\\bar{R}_{T}^{i}\\leq L\\sqrt{|A|}/\\sqrt{T}$ . ", "page_idx": 12}, {"type": "text", "text": "B.2 Fictitious Play achieves Blackwell Approachability ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We first prove that FP achieves Blackwell approachability in a two-player zero-sum game. Define $\\bar{R}_{t}^{i,\\mathrm{max}}$ be the maximum portion of vector $\\bar{R}_{t}^{i,+}$ . If $\\bar{R}_{t}^{i,\\operatorname*{max}}\\neq\\mathbf{0}$ , we find the point $\\psi_{t}=\\bar{R}_{t}^{i}-\\bar{R}_{t}^{i,\\mathrm{max}}$ in $\\mathbb{R}^{|A^{i}|}$ and let | RR\u00af\u00afiit\u2212\u2212\u03c8\u03c8tt| be the normal vector, then we can determine half-space by| RR\u00af\u00afiit\u2212\u2212\u03c8\u03c8tt| a nd $\\psi_{t}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t}^{\\mathrm{P}}=\\left\\{z\\in\\mathbb{R}^{l^{-i}}:(\\bar{R}_{t}^{i}-\\psi_{t})^{\\top}z\\leq(\\bar{R}_{t}^{i}-\\psi_{t})^{\\top}\\psi_{t}\\right\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Because $\\bar{R}_{t}^{i}-\\psi_{t}=\\bar{R}_{t}^{i,\\mathrm{max}}$ , $(\\bar{R}_{t}^{i}-\\pmb{\\psi}_{t})^{\\top}\\pmb{\\psi}_{t}=0$ , therefore: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t}^{\\mathrm{P}}=\\left\\{z\\in\\mathbb{R}^{l^{-i}}:\\left\\langle\\bar{R}_{t}^{i,\\operatorname*{max}},z\\right\\rangle\\leq0\\right\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For any point $s^{\\prime}\\,\\in\\,S^{i}$ there is $\\left\\langle\\bar{R}_{t}^{i,\\mathrm{max}},s^{\\prime}\\right\\rangle\\leq\\,0$ . Then we need to prove that a forcing action for $\\mathcal{H}_{t}^{\\mathrm{P}}$ indeed exists. According to Definition 2, we need to find a $\\sigma_{t+1}^{i*}\\;\\in\\;\\Sigma^{i}$ that achieves $R^{i}\\left(\\sigma_{t+1}^{i*},\\hat{\\sigma}_{t+1}^{-i}\\right)\\in\\mathcal{H}_{t+1}^{i,\\mathrm{P}}$ for any $\\hat{\\sigma}_{t+1}^{-i}\\in\\Sigma^{-i}$ . For simplicity, let $\\ell=[u^{i}\\left(a_{1},\\sigma^{-i}\\right),\\ldots]^{\\top}\\in\\mathbb{R}^{|A^{i}|}$ , ", "page_idx": 12}, {"type": "text", "text": "we rewrite the regret vector as $R^{i}\\left(\\sigma_{t+1}^{i*},\\hat{\\sigma}_{t+1}^{-i}\\right)=\\ell-\\left\\langle\\ell,\\sigma_{t+1}^{i*}\\right\\rangle\\mathbf{1}$ , we are looking for a $\\sigma_{t+1}^{i*}\\in\\Sigma^{i}$ such that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad R^{i}\\left(\\sigma_{t+1}^{i},\\hat{\\sigma}_{t+1}^{-i}\\right)\\in\\mathcal{H}_{t}^{\\psi}}\\\\ &{\\Longleftrightarrow\\left\\langle\\bar{R}_{t}^{i,\\mathrm{nux}},\\ell-\\left\\langle\\ell,\\sigma_{t+1}^{i*}\\right\\rangle1\\right\\rangle\\le0}\\\\ &{\\Longleftrightarrow\\left\\langle\\bar{R}_{t}^{i,\\mathrm{nux}},\\ell\\right\\rangle-\\left\\langle\\ell,\\sigma_{t+1}^{i*}\\right\\rangle\\left\\langle\\bar{R}_{t}^{i,\\mathrm{nux}},1\\right\\rangle\\le0}\\\\ &{\\Longleftrightarrow\\left\\langle\\bar{R}_{t}^{i,\\mathrm{nux}},\\ell\\right\\rangle-\\left\\langle\\ell,\\sigma_{t+1}^{i*}\\right\\rangle\\left\\|\\bar{R}_{t}^{i,\\mathrm{nux}}\\right\\|_{1}\\le0}\\\\ &{\\Longleftrightarrow\\left\\langle\\ell,\\frac{\\bar{R}_{t}^{i,\\mathrm{nux}}}{\\sqrt{\\kappa}}\\right\\|_{1}\\biggr\\rangle-\\left\\langle\\ell,\\sigma_{t+1}^{i*}\\right\\rangle\\le0}\\\\ &{\\Longleftrightarrow\\left\\langle\\ell,\\frac{\\left\\|\\bar{R}_{t}^{i,\\mathrm{nux}}\\right\\|_{1}}{\\sqrt{\\kappa}}-\\sigma_{t+1}^{i*}\\right\\rangle\\le0}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We obtain the strategy \u03c3ti\u2217+1 = $\\begin{array}{r}{\\sigma_{t+1}^{i*}\\,=\\,\\frac{\\bar{R}_{t}^{i,\\operatorname*{max}}}{\\left\\lVert\\bar{R}_{t}^{i,\\operatorname*{max}}\\right\\rVert_{1}}}\\end{array}$ that guarantees $\\mathcal{H}_{t+1}^{\\mathrm{P}}$ to be forceable half space. And the action with the highest regret value is actually the BR strategy Brown [2020], so FP achieves Blackwell approachability. Figure 3 shows the difference in forceable half spaces for BR and RM strategies in the two-dimensional plane. According to 2, BR strategy is also a regret minimizer in normal-form game. Therefore, replacing the RM strategy with the BR strategy in CFR does not affect the convergence of the algorithm. ", "page_idx": 13}, {"type": "text", "text": "B.3 The convergence speed of FP and clear games ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "However, it is important to note that the convergence point of FP differs from that of RM . Specifically, the convergence point for FP is $\\psi_{t}=\\bar{R}_{t}^{i}-\\bar{R}_{t}^{i,\\mathrm{max}}$ , while for RM it is $\\psi_{t}^{\\mathrm{RM}}=\\bar{R}_{t}^{i}-\\bar{R}_{t}^{i,+}=\\bar{R}_{t}^{-i}$ . The relationship between these two points can be expressed as $\\|\\psi_{t}^{\\mathrm{{RM}}}\\|_{2}\\leq\\|\\psi_{t}\\|_{2}\\sqrt{|A|}$ . The convergence rate of RM is $\\mathcal{O}\\left(L\\sqrt{|A|/T}\\right)$ . Consequently, the overall convergence rate for FP is ${\\mathcal{O}}\\left(L|A|/{\\sqrt{T}}\\right)$ . In Section 3, we have analyzed that the RM strategy may select a dominated strategy, whereas FP will not. Therefore, representing the complexity of the problem using $|{\\mathcal{A}}|$ for FP is inaccurate. If an action $a^{\\prime}$ is a dominated strategy, then it cannot be the best response, i.e., $b(\\sigma)\\neq a^{\\prime}$ . Hence, in FP, we can replace $|{\\mathcal{A}}|$ with $\\mathrm{\\mathcal{A}n d}$ , where $A_{\\mathrm{nd}}^{i}\\subseteq A^{i}$ is the set of non-dominated strategies in the game. This adjustment leads to the FP convergence rate being $\\mathcal{O}\\left(L|\\mathcal{A}_{\\mathrm{nd}}|/\\sqrt{T}\\right)$ . ", "page_idx": 13}, {"type": "text", "text": "We now define a \u201cclear game\u201d as follows: when the number of non-dominated strategies in a game satisfies $|{\\mathcal{A}}_{\\mathrm{nd}}|\\leq{\\sqrt{|A|}}$ , the game is considered a clear game. In Figure 1, the square root of 100 is 10, aligning with theoretical expectations. In clear games, even when the number of iterations is used as a measure, the convergence speed of FP will outperform that of RM. ", "page_idx": 13}, {"type": "text", "text": "C Pseudocode ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 CFVFP ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The pseudocode 1 here does not join the rest of the variants, nor does it consider cases other than two-player zero-sum. ", "page_idx": 13}, {"type": "text", "text": "C.2 MCCFVFP ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Since MCCFVFP will directly sample actions on opportunity nodes, the efficiency of the MCCFVFP algorithm will be higher than CFVFP. The pseudocode of MCCFVFP is shown in pseudocode 2. ", "page_idx": 13}, {"type": "text", "text": "1: if $\\pi^{1}=\\pi^{2}=0$ or $\\pi^{c}=0$ then   \n2: return [0, 0]   \n3: end if   \n4: if $s\\in{\\mathcal{Z}}$ then   \n5: return $\\left[u^{1}(s)\\pi^{2}\\pi^{c},u^{2}(s)\\pi^{1}\\pi^{c}\\right]$   \n6: end if   \n7: Define $I$ as the information set to which $s$ belongs   \n8: $r=[0,0]$   \n9: if $P(s)=c$ then   \n10: for $\\begin{array}{r l}&{a\\in\\mathcal{A}(s)\\;\\mathbf{do}}\\\\ &{r^{\\prime}=\\mathrm{PCFR}(s+a,\\pi,\\pi^{c}\\sigma^{c}(s)(a))}\\\\ &{r=r+r^{\\prime}(p_{s})}\\end{array}$   \n11:   \n12:   \n13: end for   \n14: return r   \n15: else   \n16: if $\\begin{array}{c}{{P(s)=p_{1}\\;{\\bf t h e n}}}\\\\ {{\\pi_{s},\\pi_{o}=\\pi^{1},\\pi^{2}}}\\\\ {{p_{s},p_{o}=p_{1},p_{2}}}\\end{array}$   \n17:   \n18:   \n19: else   \n20: $\\begin{array}{c}{{\\pi_{s},\\pi_{o}=\\pi^{2},\\pi^{1}}}\\\\ {{p_{s},p_{o}=p_{2},p_{1}}}\\end{array}$   \n21:   \n22: end if   \n23: if $\\pi_{o}=0$ then   \n24: $\\begin{array}{r l}&{\\tilde{r}^{\\prime}=\\mathrm{CFVP}(s+\\sigma_{t}(I),\\pi,\\pi^{c})}\\\\ &{r(p_{o})=r^{\\prime}(p_{o})}\\\\ &{\\bar{\\sigma}_{t}^{p_{s}}(I)=((t-1)\\bar{\\sigma}_{t-1}^{p_{s}}(I)+\\sigma_{t}^{p_{s}}(I))/t}\\end{array}$   \n25:   \n26:   \n27: else if $\\pi_{s}=0$ then   \n28: for $a\\in\\mathcal{A}(I)$ do   \n29: $\\begin{array}{r l}&{r^{\\prime}=\\mathbf{C}\\mathbf{\\dot{F}}\\mathbf{\\dot{V}}\\mathbf{P}(s+a,\\pi,\\pi^{c})}\\\\ &{Q_{t}^{p_{s}}(I,a)=Q_{t-1}^{p_{s}}(I,a)+r^{\\prime}(p_{s})}\\\\ &{\\mathbf{if}\\;a=\\sigma^{p_{s}}(I)\\;\\mathbf{then}}\\\\ &{\\qquad r(p_{o})=r^{\\prime}(p_{o})}\\end{array}$   \n30:   \n31:   \n32:   \n33: end if   \n34: end for   \n35: $\\begin{array}{r}{\\sigma_{t+1}^{p_{s}}(I)=\\operatorname*{max}_{a\\in\\Sigma_{\\mathrm{P}}^{i}(I)}Q_{t}^{p_{s}}(I,a)}\\end{array}$   \n36: else   \n37: for $a\\in\\mathcal{A}(I)$ do   \n38: $\\begin{array}{r l}&{\\overbrace{\\pi^{p_{s}}}=\\sigma_{t}^{p_{s}}(I,a)}\\\\ &{\\quad\\pi^{\\prime}=\\mathrm{CFV}\\mathrm{FP}(s+a,\\pi,\\pi^{c})}\\\\ &{\\quad Q_{t}^{p_{s}}(I,a)=Q_{t-1}^{p_{s}}(I,a)+r^{\\prime}(p_{s})}\\\\ &{\\quad r(p_{o})=r(p_{o})+r^{\\prime}(p_{o})}\\\\ &{\\quad r(p_{s})=r(p_{s})+r^{\\prime}(p_{s})\\sigma_{t}^{p_{s}}(I,a)}\\end{array}$   \n39:   \n40:   \n41:   \n42:   \n43: end for   \n44: $\\begin{array}{r l}&{\\bar{\\sigma}_{t}^{p_{s}}(I)=((t-1)\\bar{\\sigma}_{t-1}^{p_{s}}(I)+\\sigma_{t}^{p_{s}}(I))/t}\\\\ &{\\sigma_{t+1}^{p_{s}}(I)=\\operatorname*{max}_{a\\in\\Sigma_{\\mathrm{p}}^{i}(I)(I)}Q_{t}^{p_{s}}(I,a)}\\end{array}$   \n45:   \n46: end if   \n47: end if   \n48: return r ", "page_idx": 14}, {"type": "text", "text": "D Comparison between variants of CFVFP ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 CFVFP variants ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We integrated CFVFP with $\\mathbf{RM+}$ and MC variants, resulting in four distinct combinations: CFVFP, $\\mathrm{CFVFP+}$ , MCCFVFP, and MCCFVFP $^+$ . Figure 4 clearly illustrates that MCCFVFP and MCCFVFP+ consistently outperform CFVFP and $\\mathrm{CFVFP+}$ in terms of results. However, the performance of ", "page_idx": 14}, {"type": "text", "text": "1: if $\\pi^{1}=\\pi^{2}=0$ then   \n2: return [0, 0]   \n3: end if   \n4: if $s\\in{\\mathcal{Z}}$ then   \n5: return $\\left[u^{1}(s)\\pi^{2},u^{2}(s)\\pi^{1}\\right]$   \n6: end if   \n7: Define $I$ as the information set to which $s$ belongs   \n8: $r=[0,0]$   \n9: if $P(s)=c$ then   \n10: $a\\sim\\sigma^{c}(s)$   \n11: $r=\\mathrm{PMCCFR}(s+a,\\pi)$   \n12: return r   \n13: else   \n14: if $P(s)=p_{1}$ then   \n15: \u03c0s, \u03c0o = \u03c01, \u03c02   \n16: $p_{s},p_{o}=p_{1},p_{2}$   \n17: else   \n18: $\\begin{array}{c}{{\\tau_{s},\\pi_{o}=\\pi^{2},\\pi^{1}}}\\\\ {{p_{s},p_{o}=p_{2},p_{1}}}\\end{array}$   \n19:   \n20: end if   \n21: if $\\pi_{o}=0$ then   \n22: $\\begin{array}{r l}&{\\dot{\\boldsymbol{r}}^{\\prime}=\\mathbf{MCCFVP}(s+\\sigma_{t}(I),\\pi)}\\\\ &{\\boldsymbol{r}(\\boldsymbol{p}_{o})=\\boldsymbol{r}^{\\prime}(\\boldsymbol{p}_{o})}\\\\ &{\\bar{\\sigma}_{t}^{p_{s}}(I)=((\\underline{{t}}-1)\\bar{\\sigma}_{t-1}^{p_{s}}(I)+\\sigma_{t}^{p_{s}}(I))/t}\\end{array}$   \n23:   \n24:   \n25: else if $\\pi_{s}=0$ then   \n26: for $a\\in\\mathcal{A}(I)$ do   \n27: 28: $\\begin{array}{r l}&{r^{\\prime}=\\mathbf{MCCFV}\\mathrm{P}(s+a,\\pi)}\\\\ &{Q_{t}^{p_{s}}(I,a)=Q_{t-1}^{p_{s}}(I,a)+r^{\\prime}(p_{s})}\\\\ &{\\mathbf{if}\\,a=\\sigma^{p_{s}}(I)\\ \\mathbf{then}}\\\\ &{\\qquad r(p_{o})=r^{\\prime}(p_{o})}\\end{array}$   \n29:   \n30:   \n31: end if   \n32: end for   \n33: $\\begin{array}{r}{\\sigma_{t+1}^{p_{s}}(I)=\\operatorname*{max}_{a_{\\mathrm{P}}\\in\\Sigma_{\\mathrm{P}}(I)}Q_{t}^{p_{s}}(I,a)}\\end{array}$   \n34: else   \n35: for $a\\in\\mathcal{A}(I)$ do   \n36: $\\begin{array}{r l}&{\\mathrm{~\\widetilde{\\tau}^{\\omega}\\stackrel{\\sim}{=}~}\\omega_{t}^{\\mathcal{P}_{s}}\\left(I,a\\right)}\\\\ &{r^{\\prime}=\\mathrm{MCFVP}(s+a,\\pi)}\\\\ &{Q_{t}^{p_{s}}(I,a)=Q_{t-1}^{p_{s}}(I,a)+r^{\\prime}(p_{s})}\\\\ &{r(p_{o})=r(p_{o})+r^{\\prime}(p_{o})}\\\\ &{r(p_{s})=r(p_{s})+r^{\\prime}(p_{s})\\sigma_{t}^{p_{s}}(I,a)}\\end{array}$   \n37:   \n38:   \n39:   \n40:   \n41: end for   \n42: $\\begin{array}{r l}&{\\bar{\\sigma}_{t}^{p_{s}}(I)=((t-1)\\bar{\\sigma}_{t-1}^{p_{s}}(I)+\\sigma_{t}^{p_{s}}(I))/t}\\\\ &{\\sigma_{t+1}^{p_{s}}(I)=\\arg\\operatorname*{max}_{a_{\\mathrm{P}}\\in\\Sigma_{\\mathrm{P}}(I)}Q_{t}^{p_{s}}(I,a)}\\end{array}$   \n43:   \n44: end if   \n45: end if   \n46: return r ", "page_idx": 15}, {"type": "text", "text": "MCCFVFP and $\\mathrm{MCCFVFP+}$ varies across different problem sets. Based on these observations, we ultimately selected the more conservative MCCFVFP method. ", "page_idx": 15}, {"type": "text", "text": "D.2 Weighted Averaging Schemes for CFVFP ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Different weights of CFVFP will also have a significant impact on the results. The weight $w_{t}$ is introduced into $Q_{t}^{i}(I)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ_{t}^{i}(I)=\\left\\{Q_{t-1}^{i}(I)+w_{t}u^{i}(I,\\sigma_{t})\\quad\\mathrm{if}\\;\\pi_{\\sigma_{t}}^{-i}(I)=1\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/55e9e649cebaf041296d4f039b0bf4159b00035dc4ca0a647e2ada7ebaa70624.jpg", "img_caption": ["Figure 4: Convergence rate of MCCFVFP variants in different games "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Common $w_{t}$ values are $w_{t}=1$ , $w_{t}=\\log t$ , $w_{t}=t$ , $w_{t}=t^{2}$ . The Figures 5 show the experimental results with different weights. In the comparison of four common $w_{t}$ , convergence is faster when ", "page_idx": 16}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/086f73571e2d1e15b77beeb9c1524f1de2d3782867f0c14f0e841c9408b28578.jpg", "img_caption": ["Figure 5: Convergence rate of different weighted average schemes for CFVFP "], "img_footnote": [], "page_idx": 16}, {"type": "equation", "text": "$$\nw_{t}=1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E Proof of conclusion in theoretical analysis of CFVFP algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Time Complexity of CFR and CFVFP in an Information Set ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Due to its significantly streamlined calculation process, CFVFP requires far less computation per information set compared to CFR . Consider an information set $I$ with $x$ possible actions, denoted as $\\mathcal{A}(I)=a_{1},a_{2},\\dotsc,\\bar{a}_{x}$ . If the counterfactual payoff $u(I,a)$ for each action is known, then: ", "page_idx": 16}, {"type": "text", "text": "The calculations required for CFVFP in one information set are: ", "page_idx": 16}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/eb10c2a81c4c277ce4012adebc891f92a46d0e1bd5865095a8409c0db9e669c8.jpg", "img_caption": ["Figure 6: Game tree when each node has $h=4$ levels and $g=3$ actions. The number $(\\pi^{1}(s),\\pi^{2}(s))$ in each node represents the probability of player1 and player2 reaching this node respectively. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "1. Add the counterfactual payoff $u^{i}\\left(I,\\sigma_{t}\\big|_{I\\rightarrow a}\\right)$ to the counterfactual value $Q_{T}^{i}(I,a)$ , this step requires $x$ additions; 2. Find the maximum value of $Q_{T}^{i}(I)$ (get BR strategy), this step requires $x$ comparisons; 3. Update the BR strategy to the average strategy, this step requires 1 addition; ", "page_idx": 17}, {"type": "text", "text": "A total of $2x+1$ additions. ", "page_idx": 17}, {"type": "text", "text": "The calculations required for CFR in one information set are: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Get average counterfactual payoff $u^{i}(I)$ from $u^{i}\\left(I,\\sigma_{t}\\big|_{I\\rightarrow a}\\right)$ and strategy $\\sigma_{t}(I)$ , this step needs $x$ multiplications and $x-1$ additions;   \n2. Get regret value $R(I,a)$ , this step needs $x$ additions;   \n3. Multiply the regret value $R(I,a)$ by $\\pi_{\\sigma_{t}}^{-i}(I)$ and add it to the average regret value $\\bar{R}_{T}(I,a)$ , this step needs $x$ additions and $x$ multiplications;   \n4. Compared Regret value with 0, this step needs $x$ comparisons;   \n5. Add up the positive regret values, this step needs $x-1$ additions;   \n6. Get regret matching strategy, this step $x$ multiplications;   \n7. Update the RM strategy to the average strategy, this step needs $x$ additions; ", "page_idx": 17}, {"type": "text", "text": "A total of $6x-2$ additions and $3x$ multiplications. It can be seen that in one information set, CFVFP only takes 2/9 of the time of CFR . ", "page_idx": 17}, {"type": "text", "text": "E.2 Number of nodes touched in one iteration ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In a deterministic game tree with $h$ levels and $g$ actions per node (no chance nodes), such as a $g\\ =\\ 3,h\\ =\\ 4$ scenario depicted in Figure 6, nodes are categorized into four distinct types $(s_{\\mathrm{all}}=(s_{\\mathrm{r}},s_{\\mathrm{g}},s_{\\mathrm{b}},s_{\\mathrm{y}}))$ . ", "page_idx": 17}, {"type": "text", "text": "\u2022 The probability of all players reaching this node is 1 (red node $s_{\\mathrm{r}}$ );   \n\u2022 The probability of all players reaching this node is 0 (green node $s_{\\mathrm{g}.}$ ), and these nodes can be pruned;   \n\u2022 The probability of the current player reaching this node is 1, and the probability of other players is 0 (blue node $s_{\\mathrm{b}}$ );   \n\u2022 The probability that the current player reaches this node is 0, and the probability of other players is 1 (yellow node $s_{\\mathrm{y}}$ ); ", "page_idx": 17}, {"type": "text", "text": "We define the nodes that must be touched as $^{S_{\\mathrm{pass}=(s_{\\mathrm{r}},s_{\\mathrm{b}},s_{\\mathrm{y}})}}$ . The figure shows that each level has a single red node, with yellow and green nodes following a blue node, which is derived from red and yellow nodes. The function $F_{h}(s_{x})$ quantifies the count of each node type across layers 1 to $h$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{h}\\left(s_{\\mathrm{r}}\\right)=1}\\\\ &{F_{h}\\left(s_{\\mathrm{g}}\\right)=g F_{h-1}\\left(s_{\\mathrm{g}}\\right)+(g-1)F_{h-1}\\left(s_{\\mathrm{b}}\\right)}\\\\ &{F_{h}\\left(s_{\\mathrm{b}}\\right)=(g-1)F_{h-1}\\left(s_{\\mathrm{r}}\\right)+g F_{h-1}\\left(s_{\\mathrm{y}}\\right)}\\\\ &{F_{h}\\left(s_{\\mathrm{y}}\\right)=F_{h-1}\\left(s_{\\mathrm{b}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the general term formula for blue node $s_{\\mathrm{b}}$ in the $h$ layer is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{h}\\left(s_{\\mathfrak{b}}\\right)=(g-1)F_{h-1}\\left(s_{\\mathfrak{r}}\\right)+g F_{h-1}\\left(s_{\\mathfrak{y}}\\right)}\\\\ &{\\qquad\\qquad=(g-1)1+g F_{h-2}\\left(s_{\\mathfrak{b}}\\right)}\\\\ &{\\qquad\\quad=(g-1)+g\\left((g-1)+g F_{h-4}\\left(s_{\\mathfrak{b}}\\right)\\right)}\\\\ &{\\qquad\\quad=...}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "after simplification: ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{h\\geq2}(s_{\\mathfrak{b}})=(g-1)\\sum_{i=0}^{\\lfloor(h-2)/2\\rfloor}g^{i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The number of nodes that need to be touched $s_{\\mathrm{pass}}$ in the $h$ level game tree is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{h}\\left(s_{\\mathrm{pass}}\\right)=F_{h}\\left(s_{\\mathrm{r}}\\right)+F_{h}\\left(s_{\\mathrm{b}}\\right)+F_{h}\\left(s_{\\mathrm{y}}\\right)}\\\\ &{\\qquad\\qquad=1+F_{h}\\left(s_{\\mathrm{b}}\\right)+F_{h-1}\\left(s_{\\mathrm{b}}\\right)\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{h\\geq3}(s_{\\mathrm{pass}})=1+(g-1)\\sum_{i=0}^{\\lfloor(h-2)/2\\rfloor}g^{i}+(g-1)\\sum_{i=0}^{\\lfloor(h-3)/2\\rfloor}g^{i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The number of node $s_{\\mathrm{all}}$ of the entire $h$ level game tree is: ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{h}(s_{\\mathrm{all}})=\\sum_{i=0}^{h}g^{i}=\\frac{1-g^{h}}{1-g}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In a two-player game, it can be approximately considered that $F_{h}(s_{\\mathrm{all}})\\propto\\sqrt{F_{h}(s_{\\mathrm{pass}})}$ , so CFVFP only needs to touch $\\mathcal{O}\\left(\\sqrt{|S|}\\right)$ nodes in one iteration. Extending the results to multi-player games, CFVFP only needs to touch $\\mathcal{O}\\left(\\sqrt[|\\mathcal{N}|]{S}|\\right)$ nodes. At the same time, CFR must traverse all $\\mathcal{O}\\left(\\vert\\boldsymbol{S}\\vert\\right)$ nodes. ", "page_idx": 18}, {"type": "text", "text": "F Experiment Supplementary Notes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Description of the game ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1.1 Kuhn-extension/Leduc-extension poker ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We have made improvements to Kuhn and Leduc Poker: ", "page_idx": 18}, {"type": "text", "text": "1. The original Leduc and Kuhn poker types are 3 cards, and in the improved game the types are $x\\geq3$ cards;   \n2. The original Leduc and Kuhn can only raise one fixed chip, but in the improved game it is allowed to raise $y\\geq1$ chips of various sizes. The Bet Action raise size here adopts an equal proportional sequence in multiples of 2. For example, when the blind bet is 1 and $y=4$ , the allowed raise action is [1, 2, 4, 8];   \n3. The original Leduc and Kuhn can only raise once in a round. After one player raises, the rest of the players can only choose to call or fold, and cannot raise a larger bet. In the improved game, it can be raised up to z times. ", "page_idx": 18}, {"type": "text", "text": "F.1.2 Princess and Monster ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Princess and monster (PAM) is a game in which two players chase and escape in a dungeon with obstacles (a 4-connected grid diagram of $m\\times n$ ). The game rule is: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The two players are monster and princess, and each player knows the structure of the dungeon and which rooms can be accessed.   \n\u2022 They can only exist in one particular dungeon at a time, and they all know the number of their current dungeon (grid).   \n\u2022 The monster\u2019s goal is to find and capture the princess as soon as possible.   \n\u2022 The princess\u2019s goal is to escape the monster as much as possible. ", "page_idx": 19}, {"type": "text", "text": "The actions they can choose are: ", "page_idx": 19}, {"type": "text", "text": "\u2022 In the initial stage, monster and princess can choose any birth room to start the game; \u2022 In the following process, monster and princess can choose to move one space per step in four directions, up, down, left and right, or stay in place. When moving, they cannot exceed the dungeon boundary or enter impassable rooms. ", "page_idx": 19}, {"type": "text", "text": "Result of the game: ", "page_idx": 19}, {"type": "text", "text": "\u2022 At the same time, if the monster and princess appear in the same dungeon, the game ends. The princess survives the $n$ round and gets $n$ utilities, and the corresponding monster gets $-n$ utilities.   \n\u2022 If the monster does not capture the princess within a certain number of steps (such as $N$ move), the game ends directly. The princess earns $N$ utilities and the monster earns $-N$ utilities. ", "page_idx": 19}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/8ef4e12879e435558a93dafb7bbbe64690c3c65ce614442d416f0f2586e546e4.jpg", "img_caption": ["Figure 7: The structure of the dungeon. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "As shown in Figure 7, we set the dungeon to a $3\\!\\times\\!3$ grid, with inaccessible rooms in the upper left and right corners. ", "page_idx": 19}, {"type": "text", "text": "F.2 The Rest of the Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We measured the number of information sets and the number of nodes in different games in Table 3, and there are rest of the experimental results in Figure 8. ", "page_idx": 19}, {"type": "text", "text": "G The difference between time, nodes touched and iterations as indicator ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In previous papers, many convergence analyses were performed based on the number of iterations $T$ , which is obviously unfair to the sampling CFR/CFVFP algorithm. For sampling algorithms, many studies have used the number of nodes to analyze the convergence of the algorithm, but these studies have ignored the inconsistent calculation time of different nodes and the inconsistent number of information sets that have passed through the same node. The cost of an iteration in CFR/CFVFP is divided into two parts: the game tree traversal cost and the RM/BR strategy cost calculated on the information set: ", "page_idx": 19}, {"type": "text", "text": "\u2022 On each information set: It is necessary to calculate the RM/BR strategy and update the average strategy for the passed information set. These costs have been detailed in Section E.1. ", "page_idx": 19}, {"type": "table", "img_path": "eFD9N5zdFC/tmp/8c70e04c7be5b9703e25881093327cc81aec760e12e981dc7e19185bcdd6635f.jpg", "table_caption": [], "table_footnote": ["Table 3: Information sets and node number record for different games "], "page_idx": 20}, {"type": "image", "img_path": "eFD9N5zdFC/tmp/8f057c405a89e5e5a9d41e2d6d4474a26bb9585889ea21f592dc1b95f49eee8c.jpg", "img_caption": ["Figure 8: Convergence rate in Leduc-extension, Kuhn-extension, Here Action and Len are both 1. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "\u2022 There are three types of nodes on each game tree node, all of which need to get the counterfactual utility of each player. The difference is: \u2013 On the player node, the counterfactual utility needs to be obtained on this type of node $\\bar{u^{i}\\left(s,\\sigma_{t}\\bar{|}_{I\\rightarrow a}\\right)}$ and multiplied by the strategy return. \u2013 On opportunity nodes, in this type of node need to get virtual utility $u^{i}\\left(I,\\sigma_{t}\\big|_{I\\rightarrow a}\\right)$ and multiplied by opportunity node probability. \u2013 On the end point node, it is necessary to solve the final income according to the node on this type of node. ", "page_idx": 20}, {"type": "text", "text": "When passing through the same number of nodes, different algorithms do not pass through the same number of information sets. For example, when using the CFR algorithm to train vanilla Kuhn poker (assuming no pruning), one iteration passes through 55 nodes (24 player nodes, 1 random node, 30 leaf nodes), and calculate regret matching strategies on 12 information sets, at this time, the ratio of information set calculation and node calculation is 12:55. While using MCCFR the calculation passes through 10 nodes (4 player nodes, 1 random node, 5 leaf nodes), and calculate regret matching strategies on 4 information sets, the ratio of information set calculation and node calculation is 4:10. And the computing time is different at different decision, opportunity, and end point nodes. Therefore, measuring the performance of the algorithm by the number of nodes passed does not objectively reflect the ability of the algorithm. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: I believe that the abstract and introduction accurately reflect the contribution and scope of the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: At the end of the paper we pointed out that our algorithm needs larger-scale experiments and is combined with more CFR variants. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: I think the theory about CFVFP has been well proven, but it is just a conjecture that the large game is a clear game. The correctness of this conjecture needs to be verified in more problems. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have made our code available for inspection and reproduction ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code we provide can ensure that the results of this paper are reproduced, but this paper relies on the assumption that large-scale games are a clear game, which is not necessarily $100\\%$ certain in large-scale games. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our Texas Hold\u2019em experiments were conducted using a commercial solver, so we are unable to share this part of the code. However, the experiments on smaller-scale games have been made publicly available. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All our experiments are labeled with confidence intervals. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide a detailed account of the resources required for all experiments. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We believe that our research complies with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work is very theoretical and there is no need to discuss social implications. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our research does not cover this aspect. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: together in the submitted code. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]