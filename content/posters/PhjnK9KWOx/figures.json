[{"figure_path": "PhjnK9KWOx/figures/figures_4_1.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "Figure 1(a) compares the shape of different activation functions used in the proposed Pairwise Softmax Loss (PSL) and the original Softmax Loss (SL).  It shows that PSL's activation functions (tanh, atan, relu) provide a closer approximation to the Heaviside step function compared to the exponential function used in SL. Figure 1(b) contrasts the weight distribution in SL and PSL. It highlights how PSL's various activation functions help to mitigate the excessive impact of negative instances with high prediction scores, a key limitation of SL.", "section": "4 Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_8_1.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "This figure compares different surrogate activation functions and their effect on the weight distribution in Softmax Loss (SL) and Pairwise Softmax Loss (PSL). The left panel shows the curves of three surrogate activations (tanh, atan, relu) against the Heaviside step function and the exponential function used in SL.  The right panel shows how the weight distribution changes in SL and PSL with the different activation functions.  It illustrates that PSL offers more balanced weight distributions, mitigating the potential for excessive influence of false negative instances.", "section": "Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_21_1.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "This figure shows two subfigures. Subfigure (a) compares the shapes of different surrogate activation functions used in PSL against the exponential function used in SL and the Heaviside step function. Subfigure (b) shows how the weight distribution changes for SL and PSL with three different activation functions (Tanh, Atan, ReLU). It demonstrates PSL's ability to control the weight distribution, mitigating the excessive impact of false negatives, compared to SL which is highly sensitive to false negatives.", "section": "Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_21_2.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "This figure compares different surrogate activation functions and their effects on the weight distribution in Softmax Loss (SL) and Pairwise Softmax Loss (PSL).  Panel (a) shows a graphical comparison of the exponential function used in SL versus three alternative activation functions (Tanh, Atan, ReLU) used in PSL. Panel (b) illustrates how the choice of activation function influences the weighting of different data points during training. It demonstrates how PSL mitigates the excessive influence of false negatives in recommendation systems by providing better control over the weight distribution compared to SL.", "section": "Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_22_1.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "This figure shows a comparison of different activation functions used in the proposed Pairwise Softmax Loss (PSL) and the original Softmax Loss (SL). Subfigure (a) illustrates the curves of different activation functions (Tanh, Atan, ReLU) and their relationship with the Heaviside step function, which is crucial for ranking. Subfigure (b) compares the weight distribution of SL and PSL using three different surrogate activations.  It demonstrates that PSL offers better control over weight distribution compared to SL, which is particularly sensitive to the impact of false negatives. This visualization supports the authors' claim that PSL mitigates the excessive impact of false negatives while still approximating ranking metrics.", "section": "Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_22_2.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "This figure shows a comparison of different surrogate activation functions and their effect on the weight distribution in the Softmax Loss (SL) and the proposed Pairwise Softmax Loss (PSL).  The left panel (a) illustrates the shapes of the exponential function (used in SL), and three alternative functions (Tanh, Atan, ReLU) used in PSL. The right panel (b) visualizes the resulting weight distributions for each function, highlighting how PSL leads to more balanced weights compared to the skewed distribution of SL, making PSL more robust to noisy data.  A temperature hyperparameter (\u03c4) of 0.2 is used.", "section": "4 Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_23_1.jpg", "caption": "Figure C.5: Noise results of PSL-softplus on Amazon-Book dataset.", "description": "This figure shows the performance of PSL-softplus (a variant of PSL using the softplus activation function) compared to PSL-relu and SL under different noise ratios (p) on the Amazon-Book dataset.  The plot demonstrates the impact of noise on Recall@20 for each model.  It reveals that PSL-softplus performs worse than both PSL-relu and the original SL across all noise levels.", "section": "C.2 PSL-softplus Results"}, {"figure_path": "PhjnK9KWOx/figures/figures_23_2.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "Figure 1(a) shows a comparison of different activation functions used in Softmax Loss (SL) and Pairwise Softmax Loss (PSL), including exponential, ReLU, arctangent, and hyperbolic tangent functions. The figure highlights how PSL with various activation functions approximates the Heaviside step function better than SL. Figure 1(b) demonstrates how PSL modifies the weight distribution of training instances in comparison to SL, mitigating the excessive influence of false negative instances. In particular, it shows that PSL with different activation functions (ReLU, arctangent, and hyperbolic tangent) better balances the data contributions during training than the exponential function used in SL.", "section": "4 Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_24_1.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "This figure shows two subfigures. Subfigure (a) compares the shapes of different activation functions that are used in the PSL model and the SL model. These include exponential, ReLU, arctan and tanh functions. The exponential function is used in the SL model, while others are used in the PSL model. The y axis shows the values of these activation functions and the x axis shows the score difference between positive and negative items. Subfigure (b) shows the weight distributions of SL and PSL using three different activation functions: tanh, atan, and relu. This figure shows that PSL assigns weights more evenly to each pair than SL, which uses an exponential function that assigns weights unevenly.", "section": "4 Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_24_2.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "The figure shows a comparison of different surrogate activation functions (a) and the weight distributions (b) of the Softmax Loss (SL) and Pairwise Softmax Loss (PSL) functions.  The surrogate activation functions considered are ReLU, arctan, and tanh.  The figure highlights that PSL, using alternative activation functions, better balances the weights assigned to training instances compared to SL, which is heavily influenced by the exponential function and thus sensitive to noise and outliers.", "section": "4 Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_24_3.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "Figure 1(a) compares the shapes of various activation functions used in the paper, including the exponential function (used in Softmax Loss), and other functions explored as substitutes in the proposed Pairwise Softmax Loss.  Figure 1(b) shows how the weight given to training instances varies as a function of the score difference between positive and negative instances (duij)  for Softmax Loss (SL) and the three variations of Pairwise Softmax Loss (PSL). This illustrates how PSL mitigates the disproportionate influence of false negatives.", "section": "Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_24_4.jpg", "caption": "Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \u03c4 = 0.2, which typically achieves optimal results in practice.", "description": "This figure consists of two subfigures. Subfigure (a) shows different surrogate activation functions used in the paper, such as Tanh, Atan, and Relu. These functions are compared against the Heaviside step function and the exponential function. Subfigure (b) shows the weight distributions of Softmax Loss (SL) and Pairwise Softmax Loss (PSL) with different activation functions against duij which is the score gap between positive-negative pairs. This visualization illustrates how PSL better balances the contributions of different instances compared to SL, especially in mitigating the influence of noisy instances.", "section": "Methodology"}, {"figure_path": "PhjnK9KWOx/figures/figures_24_5.jpg", "caption": "Figure C.4: Noise results on Gowalla dataset.", "description": "This figure shows the performance comparison of SL and PSL in terms of Recall@20 and NDCG@20 on Gowalla dataset with different false negative noise ratios (p). The shaded area represents the confidence interval for each loss function.  The results demonstrate how the performance of each loss changes under varying levels of noise, allowing for a comparison of their noise resistance.", "section": "C.1 Noise Results"}]