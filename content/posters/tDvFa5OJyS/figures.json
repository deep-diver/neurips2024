[{"figure_path": "tDvFa5OJyS/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.", "description": "This figure compares the performance of four different Gaussian Process (GP) models on a synthetic dataset: an exact GP (CholeskyGP) and three scalable approximations (SVGP, CaGP-CG, and CaGP-Opt). It showcases how the proposed computation-aware GP model (CaGP-Opt) provides a more accurate approximation of the posterior distribution, especially in data-sparse regions where SVGP fails to capture uncertainty.", "section": "1 Introduction"}, {"figure_path": "tDvFa5OJyS/figures/figures_5_1.jpg", "caption": "Figure 2: Visualization of action vectors defining the data projection. We perform model selection using two CaGP variants, with CG and learned sparse actions\u2014denoted as CaGP-CG, and CaGP-Opt\u2014on a toy 2-dimensional dataset. Left: For each x \u2208 {x1,...,xn}, we plot the magnitude of the entries of the top-5 eigenvectors of K and of the first five action vectors. Yellow denotes larger magnitudes; blue denotes smaller magnitudes. Right: We compare the span of the actions S against the top-i eigenspace throughout training by measuring the Grassman distance between the two subspaces (see also Section S5.2). CaGP-CG actions are closer to the kernel eigenvectors than the CaGP-Opt actions, both of which are more closely aligned than randomly chosen actions.", "description": "This figure visualizes how the learned sparse actions (CaGP-Opt) and the actions derived from the conjugate gradient method (CaGP-CG) compare to the top eigenvectors of the kernel matrix K.  The left panel shows a heatmap of the action vectors and eigenvectors, illustrating their similarities and differences in terms of magnitude. The right panel shows the Grassman distance between the subspace spanned by the actions and the subspace spanned by the top eigenvectors over training epochs.  The smaller the Grassman distance, the closer the alignment between the two subspaces. The results demonstrate that CaGP-CG actions are more closely aligned with the kernel's eigenvectors than CaGP-Opt actions, suggesting a potential trade-off between optimization efficiency and alignment to the optimal action selection based on information theory.", "section": "Choice of Actions"}, {"figure_path": "tDvFa5OJyS/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.", "description": "This figure compares the performance of an exact GP posterior with three scalable approximations: SVGP, CaGP-CG, and CaGP-Opt.  It highlights the overconfidence issue of SVGP in data-sparse regions, where it expresses almost no posterior variance near the inducing point and attributes most variance to noise. In contrast, CaGP-CG and CaGP-Opt, especially the proposed CaGP-Opt, show significantly more posterior variance, indicating better uncertainty quantification in these areas.  The plot shows that although none of the methods perfectly recover the data-generating process, CaGP-CG and CaGP-Opt show better posterior predictive distributions compared to SVGP.", "section": "1 Introduction"}, {"figure_path": "tDvFa5OJyS/figures/figures_8_2.jpg", "caption": "Figure 4: Uncertainty quantification for CaGP-Opt and SVGP. Difference between the desired coverage (95%) and the empirical coverage of the GP 95% credible interval on the \"Power\" dataset. After training, CaGP-Opt has better empirical coverage than SVGP.", "description": "This figure compares the uncertainty quantification of CaGP-Opt and SVGP on the \"Power\" dataset.  Specifically, it shows the difference between the desired 95% credible interval coverage and the actual empirical coverage achieved by each method.  The x-axis represents training time, and the y-axis shows the absolute difference in coverage percentage.  The plot demonstrates that CaGP-Opt achieves better calibration (closer to the desired 95% coverage) than SVGP after training.", "section": "5 Experiments"}, {"figure_path": "tDvFa5OJyS/figures/figures_19_1.jpg", "caption": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.", "description": "This figure compares the performance of an exact GP posterior with three scalable approximations: SVGP, CaGP-CG, and CaGP-Opt.  It highlights how CaGP-CG and CaGP-Opt, unlike SVGP, maintain significant posterior variance even in data-sparse regions, leading to better uncertainty quantification. The figure also demonstrates how the different methods handle posterior mean predictions, showing CaGP-CG and CaGP-Opt are closer to the exact GP's predictions.", "section": "1 Introduction"}, {"figure_path": "tDvFa5OJyS/figures/figures_19_2.jpg", "caption": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.", "description": "This figure compares the performance of an exact GP posterior with three scalable approximations (SVGP, CaGP-CG, and CaGP-Opt).  It highlights the overconfidence issue of SVGP in data-sparse regions, where it attributes most variance to observational noise.  In contrast, CaGP-CG and CaGP-Opt show more accurate uncertainty representation, even in areas lacking data. Hyperparameters for all methods were optimized using appropriate techniques.", "section": "1 Introduction"}, {"figure_path": "tDvFa5OJyS/figures/figures_23_1.jpg", "caption": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.", "description": "This figure compares the performance of an exact Gaussian Process (CholeskyGP) with three scalable approximations: SVGP, CaGP-CG, and CaGP-Opt.  It highlights how CaGP-CG and CaGP-Opt provide more accurate posterior estimates, especially in data-sparse regions where SVGP shows overconfidence by attributing variation to noise rather than uncertainty.", "section": "1 Introduction"}, {"figure_path": "tDvFa5OJyS/figures/figures_25_1.jpg", "caption": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.", "description": "This figure compares the performance of an exact Gaussian Process (GP) with three scalable GP approximation methods: SVGP, CaGP-CG, and CaGP-Opt.  The plot shows the posterior predictive distributions for each method, highlighting the differences in uncertainty quantification.  CaGP-CG and CaGP-Opt, unlike SVGP, demonstrate more realistic uncertainty estimates, especially in regions with sparse data, showing their ability to better capture approximation error.", "section": "1 Introduction"}, {"figure_path": "tDvFa5OJyS/figures/figures_26_1.jpg", "caption": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.", "description": "This figure compares the performance of an exact GP posterior with three scalable approximations: SVGP, CaGP-CG, and CaGP-Opt.  The plot shows posterior predictive distributions for each method.  While none perfectly recover the true process, CaGP-CG and CaGP-Opt show much better agreement with the exact posterior than SVGP. Notably, SVGP exhibits near zero posterior variance in data-sparse regions, indicating overconfidence, unlike CaGP-CG and CaGP-Opt, which show appropriate uncertainty.", "section": "1 Introduction"}]