[{"figure_path": "7CMNSqsZJt/figures/figures_1_1.jpg", "caption": "Figure 1: CONTEXTCITE. Our context attribution method, CONTEXTCITE, traces any specified generated statement back to the parts of the context that are responsible for it.", "description": "The figure shows a simple example of how CONTEXTCITE works. Given a context (a PDF about the 2024 solar eclipse), and a query (\"I live in Boston, MA. When and where should I go to see the eclipse?\"), a language model generates a response.  CONTEXTCITE then highlights the portion of the context that was used by the model to generate a specific part of the response. In this case, the response's statement about Maine being on the path of totality is linked to the relevant section in the provided PDF.", "section": "1.1 Our contributions"}, {"figure_path": "7CMNSqsZJt/figures/figures_4_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure demonstrates an example of how CONTEXTCITE, a linear surrogate model, is used for context attribution.  The left side shows the context, query, and generated response. The middle shows the attribution scores (weights of the linear model). The right displays a scatter plot comparing the model's predictions of logit-scaled probabilities against the actual values, for various context ablations. The plot shows a strong linear correlation, indicating the model's accuracy.", "section": "Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_5_1.jpg", "caption": "Figure 3: Inducing sparsity improves the surrogate model's sample efficiency. In CNN DailyMail [28], a summarization task, and Natural Questions [29], a question answering task, we observe that the number of sources that are \u201crelevant\u201d for a particular statement generated by Llama-3-8B [22] is small, even when the context comprises many sources (Figure 3a). Therefore, inducing sparsity via LASSO yields an accurate surrogate model with just a few ablations (Figure 3b). See Appendix A.4 for the exact setup.", "description": "This figure demonstrates how inducing sparsity in the surrogate model improves sample efficiency for context attribution.  It shows that even with many sources, only a small subset is truly relevant in predicting model responses for both summarization and question answering tasks.  LASSO regularization effectively identifies these crucial sources with fewer ablations, leading to a more efficient and accurate surrogate model.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_7_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure demonstrates an example of how CONTEXTCITE, a linear surrogate model, is used for context attribution.  The left side shows the context, query, and generated response. The middle shows the attribution scores (weights of the linear model) assigned by CONTEXTCITE to each source. The right side displays a scatter plot comparing the surrogate model's predictions to the actual logit probabilities obtained by ablating different parts of the context.  The linear relationship shown indicates the model effectively captures the model's behavior.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_7_2.jpg", "caption": "Figure 4: Evaluating context attributions. We report the top-k log-probability drop (Figure 4a) and linear datamodeling score (Figure 4b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-8B and Phi-3-mini on up to 1,000 randomly sampled validation examples from each of three benchmarks. We find that CONTEXTCITE using just 32 context ablations consistently matches or outperforms the baselines-attention, gradient norm, semantic similarity and leave-one-out-across benchmarks and models. Increasing the number of context ablations to {64, 128, 256} can further improve the quality of CONTEXTCITE attributions in this setting as well.", "description": "This figure compares the performance of CONTEXTCITE against several baseline methods for context attribution across different datasets and language models. Two metrics are used for evaluation: top-k log-probability drop and linear datamodeling score.  CONTEXTCITE demonstrates superior performance, even with a limited number of context ablations, suggesting its effectiveness in identifying relevant context sources.", "section": "Evaluating CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_8_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure illustrates an example of how CONTEXTCITE uses a linear surrogate model to perform context attribution.  It shows a context, query, and response, along with the weights (attribution scores) assigned by the surrogate model to each part of the context. The plot demonstrates the model's ability to accurately predict the logit-scaled probability of the response given different context ablations, indicating a strong linear relationship between context features and response generation.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_9_1.jpg", "caption": "Figure 6: Improving response quality by constructing query-specific contexts. On the left, we show that filtering contexts by selecting the top-{2, ..., 16} query-relevant sources (via CONTEXTCITE) improves the average F\u2081-score of Llama-3-8B on 1,000 randomly sampled examples from the Hotpot QA dataset. Similarly, on the right, simply replacing the entire context with the top-{8,..., 128} query-relevant sources boosts the average F\u2081-score of Llama-3-8B on 1,000 randomly sampled examples from the Natural Questions dataset. In both cases, CONTEXTCITE improves response quality by extracting the most query-relevant information from the context.", "description": "This figure shows the results of two experiments evaluating the impact of pruning irrelevant information from the context before generating responses using a language model. The left graph shows that selecting only the top 2 to 16 most relevant sources based on CONTEXTCITE's attribution scores improved the F1 score on the HotpotQA dataset.  The right graph shows similar improvements on the Natural Questions dataset when using the top 8 to 128 sources. This demonstrates that CONTEXTCITE can effectively identify and select the most relevant parts of the context, leading to improved response quality.", "section": "5 Applying context attribution"}, {"figure_path": "7CMNSqsZJt/figures/figures_12_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure demonstrates an example of how CONTEXTCITE, a linear surrogate model, is used for context attribution. It shows the weights of the model (attribution scores), the actual vs predicted logit-scaled probabilities for random context ablations, and how these two sources primarily determine the response. The linearity of the model is highlighted, showing the additive effects of removing the sources.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_12_2.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model\u2019s predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model\u2019s behavior.", "description": "This figure demonstrates an example of how CONTEXTCITE, a linear surrogate model, is used for context attribution. It shows a context, query, and response generated by Llama-3-8B about the weather in Antarctica. The figure highlights the model's weights (attribution scores) and a plot comparing the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. The results show a strong linear relationship between the model\u2019s predictions and the actual probabilities, indicating that the surrogate model effectively captures the language model's behavior. This example shows two main sources that primarily contribute to the response.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_14_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model\u2019s predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model\u2019s behavior.", "description": "This figure illustrates an example of CONTEXTCITE's linear surrogate model.  The left shows a context, query, and model-generated response. The middle displays the weights of the linear surrogate model (interpreted as attribution scores). The right shows a scatter plot comparing the surrogate model's predictions of logit-scaled probabilities against actual logit-scaled probabilities from random context ablations. The strong linear correlation demonstrates the model's accuracy in capturing the language model's behavior.  The example highlights how a few key context sources strongly influence the response.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_15_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure shows an example of how CONTEXTCITE uses a linear surrogate model to perform context attribution.  The left side displays the context, query, and response generated by a language model. The middle shows the weights (attribution scores) of the linear surrogate model. The right side displays a scatter plot comparing the surrogate model's predictions against the actual probabilities from context ablations, illustrating the model's accuracy in capturing the language model's behavior. The example highlights the linear interaction between two key sources in generating the response.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_15_2.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure shows an example of how CONTEXTCITE uses a linear surrogate model to perform context attribution.  It demonstrates the model's ability to accurately predict the logit probability of a response based on ablating different parts of the context. The strong correlation between predicted and actual probabilities highlights the effectiveness of the linear surrogate model in capturing the language model's behavior. The example focuses on an Antarctica weather query.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_16_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model\u2019s behavior.", "description": "This figure shows an example of how CONTEXTCITE uses a linear surrogate model to attribute a response to parts of the input context.  The left panel shows the context, query and generated response. The middle panel shows the weights of the linear model, which are interpreted as attribution scores for each source. The right panel compares the surrogate model's predictions to the actual logit probabilities for various context ablations, demonstrating the model's accuracy in capturing the language model's behavior.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_16_2.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure demonstrates how CONTEXTCITE uses a linear surrogate model to perform context attribution.  It shows an example of a context, query, and generated response, along with the attribution scores (weights of the surrogate model) assigned to different parts of the context. The plot on the right visually shows the strong correlation between the surrogate model's predictions and the actual logit-scaled probabilities, indicating the model's accuracy in capturing the language model's behavior.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_17_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model\u2019s behavior.", "description": "This figure shows an example of how CONTEXTCITE uses a linear surrogate model to perform context attribution.  The left side shows the context, query, and generated response. The middle displays the weights of the linear model, which are interpreted as attribution scores. The right shows a scatter plot comparing the surrogate model's predictions with the actual logit probabilities from various context ablations. The close correlation between predicted and actual values demonstrates the model's accuracy in capturing the relationship between context and response. The example highlights the linear interaction between two key sources in generating the response.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_19_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure illustrates an example of how CONTEXTCITE, a context attribution method, uses a linear surrogate model to estimate the logit-scaled probability of a response. It shows the weights of the linear model as attribution scores and compares the model's predictions against actual probabilities for random context ablations.  The close match between predicted and actual probabilities demonstrates the model's accuracy in capturing the language model's behavior.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_21_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure illustrates how CONTEXTCITE uses a linear surrogate model to estimate the logit-scaled probability of a response based on context ablations.  It shows an example with a context, query, and response about the weather in Antarctica.  The model weights are displayed as attribution scores, and a scatter plot demonstrates the model's accuracy in predicting the actual logit-scaled probabilities. The linearity of the model's behavior and its ability to capture the effect of ablations are highlighted.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_22_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure shows an example of how CONTEXTCITE uses a linear surrogate model to attribute a response to parts of the context.  The left panel shows the context, query, and generated response. The middle panel shows the weights (attribution scores) assigned by the linear surrogate model to each source in the context. The right panel shows a scatter plot comparing the model's predictions to the actual log-probabilities, demonstrating the model's accuracy in capturing the language model's behavior.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_23_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model\u2019s behavior.", "description": "This figure demonstrates an example of how CONTEXTCITE, a linear surrogate model, is used for context attribution. It shows the weights (attribution scores) assigned to different parts of the context for a given response. The plot on the right shows the strong correlation between the surrogate model's predictions and the actual log-probabilities, indicating the model's accuracy.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_24_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure illustrates an example of how CONTEXTCITE, a method for context attribution, uses a linear surrogate model to approximate the relationship between context ablations and the logit-scaled probability of a response. The left shows the context, query and generated response. The middle shows the attribution scores derived from the weights of the linear surrogate model. The right plots the surrogate model predictions against the actual logit-scaled probabilities, demonstrating a strong correlation and showcasing the method's effectiveness.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_25_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure shows an example of how CONTEXTCITE uses a linear surrogate model to approximate the language model's behavior, illustrating its effectiveness.  It highlights the key components of the method: using context ablation vectors, learning a linear surrogate model, and interpreting the model weights as attribution scores. The plot demonstrates the strong correlation between predicted and actual logit-scaled probabilities, suggesting the surrogate model accurately captures the relationship between context and response.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_26_1.jpg", "caption": "Figure 4: Evaluating context attributions. We report the top-k log-probability drop (Figure 4a) and linear datamodeling score (Figure 4b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-8B and Phi-3-mini on up to 1,000 randomly sampled validation examples from each of three benchmarks. We find that CONTEXTCITE using just 32 context ablations consistently matches or outperforms the baselines-attention, gradient norm, semantic similarity and leave-one-out-across benchmarks and models. Increasing the number of context ablations to {64, 128, 256} can further improve the quality of CONTEXTCITE attributions in this setting as well.", "description": "The figure displays the results of evaluating CONTEXTCITE and several baseline methods for context attribution.  It shows the top-k log-probability drop and linear datamodeling score for each method across three different benchmarks (HotpotQA, TyDi QA, and CNN DailyMail) using two language models (Llama-3-8B and Phi-3-mini).  The results demonstrate that CONTEXTCITE, even with a small number of context ablations (32), performs competitively with or better than the baseline methods.", "section": "4 Evaluating CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_27_1.jpg", "caption": "Figure 12: Evaluating word-level context attributions. We report the top-k log-probability drop (Figure 12a) and linear datamodeling score (Figure 12b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-70B on 1,000 randomly sampled validation examples from each of CNN DailyMail and Hotpot QA.", "description": "This figure displays the results of evaluating word-level context attributions using two metrics: top-k log-probability drop and linear datamodeling score.  The evaluation is performed on the Llama-3-70B model for CNN DailyMail and HotpotQA datasets.  It compares CONTEXTCITE against baselines (Average Attention and Similarity). The top-k log-probability drop measures the impact of removing the top-scoring words on the generation probability, indicating the relevance of identified words. The linear datamodeling score assesses how well the attribution scores predict the effect of random word ablations.  The figure shows that CONTEXTCITE generally outperforms the baselines in both metrics.", "section": "B.4.1 Evaluation of CONTEXTCITE for Llama-3-70B"}, {"figure_path": "7CMNSqsZJt/figures/figures_27_2.jpg", "caption": "Figure 12: Evaluating word-level context attributions. We report the top-k log-probability drop (Figure 12a) and linear datamodeling score (Figure 12b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-70B on 1,000 randomly sampled validation examples from each of CNN DailyMail and Hotpot QA.", "description": "This figure shows the results of evaluating CONTEXTCITE's performance on word-level context attribution using two metrics: top-k log-probability drop and linear datamodeling score.  The evaluation was performed on Llama-3-70B, a large language model, with data from the CNN DailyMail and HotpotQA datasets.  The figure visually compares CONTEXTCITE against several baseline methods to demonstrate its effectiveness in identifying relevant words within the context that contribute to the generation of specific statements in the model's output.", "section": "B.4 CONTEXTCITE for larger models"}, {"figure_path": "7CMNSqsZJt/figures/figures_29_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly-the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure demonstrates the linear surrogate model used in CONTEXTCITE.  The left shows example context, query, and generated response. The middle shows attribution scores (weights of the linear model). The right shows a scatter plot comparing the model's predictions to the actual logit probabilities for various context ablations, demonstrating the model's accuracy in capturing the language model's behavior.", "section": "Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_30_1.jpg", "caption": "Figure 4: Evaluating context attributions. We report the top-k log-probability drop (Figure 4a) and linear datamodeling score (Figure 4b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-8B and Phi-3-mini on up to 1,000 randomly sampled validation examples from each of three benchmarks. We find that CONTEXTCITE using just 32 context ablations consistently matches or outperforms the baselines-attention, gradient norm, semantic similarity and leave-one-out-across benchmarks and models. Increasing the number of context ablations to {64, 128, 256} can further improve the quality of CONTEXTCITE attributions in this setting as well.", "description": "This figure compares the performance of CONTEXTCITE against other baseline methods for context attribution on multiple datasets and language models.  Two metrics are used for evaluation: Top-k log-probability drop (measures how much the probability of generating the original response decreases when removing the top-k highest-scoring sources) and Linear Datamodeling Score (measures how well attribution scores predict the effect of randomly removing sources).  The results show that CONTEXTCITE consistently outperforms the baselines, particularly when using only 32 context ablations. Increasing the number of ablations improves performance further.", "section": "4 Evaluating CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_30_2.jpg", "caption": "Figure 4: Evaluating context attributions. We report the top-k log-probability drop (Figure 4a) and linear datamodeling score (LDS) (2) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-8B and Phi-3-mini on up to 1,000 randomly sampled validation examples from each of three benchmarks. We find that CONTEXTCITE using just 32 context ablations consistently matches or outperforms the baselines-attention, gradient norm, semantic similarity and leave-one-out-across benchmarks and models. Increasing the number of context ablations to {64, 128, 256} can further improve the quality of CONTEXTCITE attributions in this setting as well.", "description": "This figure evaluates the performance of CONTEXTCITE and several baseline methods for context attribution using two metrics: top-k log-probability drop and linear datamodeling score.  The results show CONTEXTCITE's superiority, particularly when using only 32 context ablations, indicating efficiency and effectiveness.", "section": "Evaluating CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_31_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure illustrates an example of how CONTEXTCITE uses a linear surrogate model to approximate the language model's behavior. The left shows the context, query, and generated response. The middle shows the weights (attribution scores) learned by the surrogate model. The right shows the surrogate model's predictions plotted against actual logit-scaled probabilities for random context ablations. The linear relationship between the model's predictions and the actual probabilities indicates that the surrogate model accurately captures the language model's behavior.", "section": "3 Context attribution with CONTEXTCITE"}, {"figure_path": "7CMNSqsZJt/figures/figures_34_1.jpg", "caption": "Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model's predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model's behavior.", "description": "This figure demonstrates the linear surrogate model used by CONTEXTCITE.  The left panel shows an example context, query, and generated response. The middle panel displays the weights of a linear surrogate model, which are interpreted as attribution scores. The right panel shows a scatter plot comparing the surrogate model's predictions with actual logit-scaled probabilities, showing a high degree of correlation and indicating the model's accuracy.", "section": "3 Context attribution with CONTEXTCITE"}]