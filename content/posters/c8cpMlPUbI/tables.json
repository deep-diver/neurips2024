[{"figure_path": "c8cpMlPUbI/tables/tables_4_1.jpg", "caption": "Table 3: The average reward per episode in Frozen Lake (POMDP) after 90,000 training steps.", "description": "This table presents the average reward per episode obtained in the Frozen Lake environment (partially observable Markov decision process) after 90,000 training steps.  It compares the performance of several algorithms, including ExPerior-MaxEnt and ExPerior-Param (the authors' proposed algorithms), a Na\u00efve Bootstrapped Deep Q-Network (DQN) baseline, and the EXPLORE algorithm. The results are broken down by the number of hazards on the frozen lake map (5, 7, and 9), and the competence parameter (beta) of the expert policy used to train ExPerior.  The \"Optimal\" row shows the maximum achievable average reward.", "section": "6 Learning in Markov Decision Processes (MDPs)"}, {"figure_path": "c8cpMlPUbI/tables/tables_6_1.jpg", "caption": "Table 1: Ablation experiments to assess the robustness of ExPerior to misspecified expert models. Random-optimal experts choose the optimal action with probability \u03b3 and choose random actions with probability 1 - \u03b3. ExPerior-MaxEnt achieves consistent out-performance by setting the hyperparameter \u03b2 = 10, while ExPerior-Param gets almost similar results for \u03b2 = 1 and \u03b2 = 2.5.", "description": "This table presents ablation study results on the robustness of the ExPerior algorithm to different expert model specifications.  It compares the performance of ExPerior-MaxEnt and ExPerior-Param under three expert types: optimal, noisily rational, and random-optimal. The random-optimal experts act optimally with a probability \u03b3 (varying from 0.0 to 0.75), and randomly otherwise.  The results show the effect of the hyperparameter \u03b2 on the algorithms' performance across different expert types and levels of optimality.", "section": "Experiments"}, {"figure_path": "c8cpMlPUbI/tables/tables_7_1.jpg", "caption": "Table 2: Superiority of ExPerior-MaxEnt compared to ExPerior-Param with misspecified parametric prior.", "description": "This table presents the Bayesian regret for different prior distributions (Low, Mid, and High Entropy) using ExPerior-Param and ExPerior-MaxEnt methods with various parametric priors (Gamma, Beta-SGLD, Normal).  It also includes results from Oracle-TS, a method with access to the true prior.  The results demonstrate that ExPerior-MaxEnt, which employs a non-parametric maximum entropy approach, consistently outperforms ExPerior-Param, particularly when the parametric prior is misspecified. This highlights the robustness and advantage of the non-parametric method in scenarios where prior knowledge is uncertain or inaccurate.", "section": "5 Learning in Bandits"}, {"figure_path": "c8cpMlPUbI/tables/tables_8_1.jpg", "caption": "Table 3: The average reward per episode in Frozen Lake (POMDP) after 90,000 training steps.", "description": "This table presents the average reward per episode achieved by different reinforcement learning algorithms in the Frozen Lake environment after 90,000 training steps. The Frozen Lake environment is a partially observable Markov decision process (POMDP) where an agent needs to navigate to a goal while avoiding hazards. The table compares the performance of ExPerior-MaxEnt, ExPerior-Param, Na\u00efve Boot-DQN, and EXPLORE across different settings with varying numbers of hazards and competence levels (\u03b2) of the expert demonstrations.  The results show the average reward and standard deviation for each algorithm and setting, indicating the effectiveness of ExPerior in leveraging expert data to enhance performance.", "section": "6 Learning in Markov Decision Processes (MDPs)"}]