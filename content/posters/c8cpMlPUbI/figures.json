[{"figure_path": "c8cpMlPUbI/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of ExPerior in a goal-oriented task. Step 1 (Offline): The experts demonstrate their policies for different goal types while observing them. Step 2 (Offline): The expert data De only contains the trajectories states/actions goal types are not collected. We form an informative prior distribution over the goal types (unobserved factors) using DE. Step 3 (Online): The goal type is unknown but drawn from the same distribution of goals in Step 1. The learner uses the learned prior for posterior sampling.", "description": "This figure illustrates the three steps of the Experts-as-Priors (ExPerior) algorithm in a goal-oriented task.  Step 1 shows experts demonstrating policies while observing unobserved contextual variables (goals). Step 2 shows how an informative prior distribution is learned from expert data that does not include the goals. Step 3 shows how this prior guides an online Bayesian RL agent to use the learned distribution to perform posterior sampling and select actions in an environment where the goal is unknown.", "section": "4 Experts-as-Priors Framework for Unobserved Heterogeneity"}, {"figure_path": "c8cpMlPUbI/figures/figures_5_1.jpg", "caption": "Figure 2: The Bayesian regret of ExPerior and baselines for K-armed Bernoulli bandits (K = 10). We consider three categories of prior distributions based on the entropy of the optimal action.", "description": "This figure compares the Bayesian regret (a measure of the algorithm's performance) of different algorithms for solving a multi-armed bandit problem with 10 arms. Each arm has a probability of success (reward) that's unknown to the algorithm, and the goal is to maximize the cumulative reward over a series of pulls.  The algorithms are categorized into three groups based on the entropy of their prior distribution over these unknown probabilities.  The x-axis represents the different algorithms including ExPerior, Oracle-TS, and several baselines. The y-axis represents the Bayesian regret. The bars are colored and grouped by entropy level (low, mid, high) to visually show the effect of entropy on regret.  The results show that ExPerior achieves the lowest regret across the range of entropy levels. ", "section": "5 Learning in Bandits"}, {"figure_path": "c8cpMlPUbI/figures/figures_6_1.jpg", "caption": "Figure 3: (a) Empirical analysis of ExPerior\u2019s regret in Bernoulli bandits based on the (left) number of arms, (middle) entropy of the optimal action, and (right) number of episodes. (b) The regret bound from Theorem 2 vs. the entropy of the optimal action. The linear relationship is consistent with the middle panel of (a).", "description": "This figure presents an empirical analysis of the Bayesian regret achieved by the Experts-as-Priors algorithm (ExPerior) in Bernoulli bandit settings.  Panel (a) shows three subplots illustrating how the regret changes with the number of arms (K), the entropy of the optimal action, and the number of episodes (T).  Panel (b) plots the theoretical regret bound derived in Theorem 2 against the entropy of the optimal action, demonstrating a linear relationship that aligns with the results observed in the middle subplot of panel (a).", "section": "Empirical regret analysis for Experts-as-Priors"}, {"figure_path": "c8cpMlPUbI/figures/figures_9_1.jpg", "caption": "Figure 4: The average reward per episode over 2,000 episodes in \"Deep Sea.\" The goal is located at the right column, uniformly at the right-most quarter of the columns, uniformly at the right-most half, and uniformly at random over all the columns, respectively. ExPerior outperforms the baselines in all instances.", "description": "This figure shows the average reward per episode achieved by different reinforcement learning algorithms over 2000 episodes in the Deep Sea environment. The Deep Sea environment is a grid world where the agent starts at the top left and must navigate to a goal at the bottom. The goal's location varies across four different conditions, with the goal being located at the rightmost column, uniformly at the rightmost quarter of columns, uniformly at the rightmost half of columns, and uniformly at random across all columns.  The figure compares the performance of ExPerior (with both maximum entropy and parametric prior approaches) to several baselines, including Na\u00efve Boot-DQN and EXPLORE. The results demonstrate that ExPerior consistently outperforms these baselines across all four goal location distributions.", "section": "Deep Sea Results"}]