{"references": [{"fullname_first_author": "Abbasi-Yadkori", "paper_title": "Improved algorithms for linear stochastic bandits", "publication_date": "2011", "reason": "This paper provides foundational algorithms for linear stochastic bandits, which are crucial to the theoretical underpinnings of the RoME algorithm."}, {"fullname_first_author": "Greenewald", "paper_title": "Action centered contextual bandits", "publication_date": "2017", "reason": "This paper introduces the action-centered contextual bandit algorithm, a key component upon which the RoME algorithm builds, addressing non-stationarity in mHealth rewards."}, {"fullname_first_author": "Abeille", "paper_title": "Linear Thompson sampling revisited", "publication_date": "2017", "reason": "This paper revisits and improves upon the theoretical analysis of Thompson sampling for linear contextual bandits, which is important for establishing the regret bound in RoME."}, {"fullname_first_author": "Chernozhukov", "paper_title": "Double/debiased machine learning for treatment and structural parameters", "publication_date": "2018", "reason": "This paper introduces the double/debiased machine learning framework, which is used in RoME for robust estimation of baseline rewards, improving the algorithm's performance in complex settings."}, {"fullname_first_author": "Yang", "paper_title": "Laplacian-regularized graph bandits: Algorithms and theoretical analysis", "publication_date": "2020", "reason": "This paper introduces the concept of nearest-neighbor regularization in contextual bandit settings, a technique adopted in RoME to efficiently pool information across users and improve the algorithm's performance."}]}