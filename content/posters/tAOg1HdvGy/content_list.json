[{"type": "text", "text": "Interpolating Item and User Fairness in Multi-Sided Recommendations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qinyi Chen1 Jason Cheuk Nam Liang1 Negin Golrezaei1 Djallel Bouneffouf2 1Massachusetts Institute of Technology 2IBM Research {qinyic,jcnliang,golrezae}@mit.edu djallel.bouneffouf@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Today\u2019s online platforms heavily lean on algorithmic recommendations for bolstering user engagement and driving revenue. However, these recommendations can impact multiple stakeholders simultaneously\u2014the platform, items (sellers), and users (customers)\u2014each with their unique objectives, making it difficult to find the right middle ground that accommodates all stakeholders. To address this, we introduce a novel fair recommendation framework, Problem (FAIR), that flexibly balances multi-stakeholder interests via a constrained optimization formulation. We next explore Problem (FAIR) in a dynamic online setting where data uncertainty further adds complexity, and propose a low-regret algorithm FORM that concurrently performs real-time learning and fair recommendations, two tasks that are often at odds. Via both theoretical analysis and a numerical case study on real-world data, we demonstrate the efficacy of our framework and method in maintaining platform revenue while ensuring desired levels of fairness for both items and users. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online recommendation systems have become essential components of various digital platforms and marketplaces, playing a critical role in user experience and revenue generation. These platforms usually operate as multi-sided entities, recommending items (services, products, contents) to users (consumers). Examples include e-commerce (Amazon, eBay, Etsy), service platforms (Airbnb, Google Local Services), job portals (LinkedIn, Indeed), streaming services (Netfilx, Spotify), and social media (Facebook, TikTok). On the platform, there are typically two main groups of stakeholders: items (products, services, contents) and users (those engaging with items), with the platform itself acting as an independent stakeholder due to its unique objectives. ", "page_idx": 0}, {"type": "text", "text": "However, as these platforms play a more vital role in societal and economic realms, fairness concerns have prompted increasing regulatory action. Notably, the European Union proposed the Digital Markets Act [54], which emphasizes the need for contestable and fair markets in the digital sector, designating recommendation systems as a key area of focus; the U.S. Algorithmic Accountability Act [47] requires companies to assess the impacts of their automated decision systems to ensure they are not biased or discriminatory. It has also become increasingly evident, to the platforms, that upholding fairness across their stakeholder groups not only strengthens relationships with these stakeholders, but also enhances long-term platform sustainability [67]. ", "page_idx": 0}, {"type": "text", "text": "Fairness concerns within digital platforms significantly impact both users and items, manifesting in various forms of disparities. For users, issues like racial discrimination in Airbnb\u2019s host-guest matching [25] and gender disparity in career ads [43] highlight the critical need for enforcing user fairness. Similarly, items on the platforms also experience unfair allocation of opportunities induced by algorithmic decisions, such as e-commerce platforms favoring their own private-label products [19] and social media prioritizing influencers\u2019 contents over others [63], indicating a disparity that affects items (sellers, content creators) alike. These underscore the necessity to address biases and create a more inclusive environment for all stakeholders involved. ", "page_idx": 0}, {"type": "text", "text": "Despite the many efforts by companies to mitigate fairness issues, most of the existing measures solely focus on one stakeholder group, sometimes even at the expense of another. For example, Airbnb has implemented strategies to mitigate racial discrimination among its users [4], yet their listings of comparable relevance might still not receive similar levels of visibility. Etsy implements measures to promote market diversity and supporting new entrants, which contributes to equality of opportunities for its items (sellers) [27, 13]; however, such a strategy may inadvertently disadvantage users by potentially overwhelming them with choices and hindering their ability to quickly find their preferred products. Similar to industrial practices, the majority of ongoing research on fairness in recommender systems (e.g., [13, 55, 71]) also focus solely on either users or items, but seldom both. ", "page_idx": 1}, {"type": "text", "text": "Addressing multi-sided fairness is inherently complex due to the competing interests and objectives of different stakeholders [39, 12]. If algorithms solely prioritize the platform\u2019s proftis, this can lead to inequitable exposure for those more niche items and users who prefer them. In addition, what users perceive as fair may be viewed as unfair by items, and vice versa [16]. ", "page_idx": 1}, {"type": "text", "text": "In face of these challenges, our work aims to answer the following two questions: (1) What constitutes a fair recommendation within a multi-sided platform? and (2) How would a platform implement a fair recommendation in a practical online setting? Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. A novel fair recommendation framework, Problem (FAIR). Our fair recommendation problem, framed as a constrained optimization problem, adopts a novel multi-sided perspective that first achieves within-group fairness among items/users, and then enforces cross-group fairness that addresses the trade-offs across groups. Notably, Problem (FAIR) enables the platform to (i) flexibly define fair solutions for items/users rather than relying on a single predefined notion (see Section 2.2); (ii) adjust trade-offs between its own business goals and fairness for stakeholders (see, e.g., our case study in Section 4); (iii) flexibly accommodate additional operational considerations. ", "page_idx": 1}, {"type": "text", "text": "2. A Fair Online Recommendation algorithm for Multi-sided platforms (FORM). In Section 3, we study an online setting where the platform must ensure fairness for a sequence of arriving users amidst data uncertainty. We present FORM, a low-regret algorithm tailored for fair online recommendation, whose efficacy is further validated via a real-world case study (Section 4). The design of FORM contributes both methodologically and technically. (i) Methodologically, contrary to prior works (e.g., [55, 71]) that treats learning and enforcing fairness as two separate tasks, we recognize that these two tasks, when conducted simultaneously, are often at odds. FORM well balances learning and fairness via properly relaxing fairness constraints and introducing randomized exploration. (ii) Technically, FORM overcomes a non-trivial challenge of managing uncertain fairness constraints when solving a constrained optimization problem in an online setting with bandit feedback. While existing works on online constrained optimization all would require certain access to constraint feedback (see discussion in Section 1.1), our setup disallows even verifying the satisfaction of item/user fairness constraints, demanding novel design in FORM. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work primarily contributes to the emerging area of algorithmic fairness in recommender systems. Additionally, from a technical aspect, our algorithm contributes to the field of constrained optimization with bandit feedback. We highlight the literature most relevant to our work in both areas. ", "page_idx": 1}, {"type": "text", "text": "Algorithmic fairness in recommender systems. Algorithmic fairness is an emerging topic [8, 2, 42] explored in various contexts such as supervised learning [14, 24], resource allocation [9, 37], opportunity allocation [36], scheduling [50], online matching [46], bandits [6], facility location [34], refugee assignment [29], assortment planning [17], online advertising [21], search [5], and online combinatorial optimization [32]. Our research is primarily aligned with works investigating fairness in recommender systems; see [67, 20] for comprehensive surveys. ", "page_idx": 1}, {"type": "text", "text": "The prior works on fairness in recommender systems can be categorized into three streams based on their subjects: item fairness, user fairness and joint fairness. Item fairness focuses on whether the decision treats items fairly, including fair ranking and display of search results [73, 11, 48, 10, 60, 66, 17, 72], similar prediction errors for items\u2019 ratings [57], and long-term fair exposure [31]. User fairness examines whether the recommendation is fair to different users, encompassing aspects like similar recommendation quality [26, 45, 68] and comparable explainability [30] across users. ", "page_idx": 1}, {"type": "text", "text": "The third stream, also the stream that our work ftis in, focuses on joint fairness that concerns whether both items and users are treated fairly. However, the number of works in this stream remain scarce, as suggested in [1]. Existing works mainly focuses on achieving item and user fairness based on certain pre-specified fairness notions: [13] seeks to balance item and user neighborhoods; [55] proposes a method that guarantees maxmin fair exposure for items and envy-free fairness for users; [71, 52] promote fairness with respect to item exposures and user normalized discounted cumulative gains. Additionally, [70] proposes and compares a family of joint multi-sided fairness metrics to tackle systematic biases in content exposure, and [69] introduces a multi-objective optimization framework that jointly optimizes accuracy and fairness for consumers and producers. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our work distinguishes from the above works on multi-sided fairness in several aspects. (i) We not only impose fairness for multi-stakeholders (items/users), but also take the platform\u2019s revenue, which is often neglected, into consideration. (ii) Our framework is not confined to a single, pre-specified fairness/outcome notion. (iii) Most importantly, unlike works that solely focused on multi-sided fairness, we recognize the challenge of jointly handling fairness and learning, and propose an algorithm with theoretical guarantees (see Section 3). ", "page_idx": 2}, {"type": "text", "text": "Constrained optimization with bandit feedback. Our work formulates the fair recommendation problem in an online setting as a constrained optimization with bandit feedback (see Section 3.1). Our proposed algorithm (Algorithm 1) makes a notable contribution to the literature of constrained optimization with bandit feedback by addressing the challenge of having uncertain constraints. ", "page_idx": 2}, {"type": "text", "text": "Previous research has explored constrained optimization with bandit feedback in various contexts. Notably, works on online learning with knapsack [38, 15, 61] involves maximizing total reward while adhering to a resource consumption constraint, using feedback on rewards and resource usage. Other relevant studies include online bidding [28, 22], online allocation [7], and safe sequential decision-making [62], which develop algorithms to monitor and maintain \u201cconstraint balances\u201d or address adversarial objectives and constraints. Our setting crucially differs from prior works by not assuming availability of constraint feedback or even the ability to monitor constraint satisfaction, making our problem more challenging (see, also, Section 3.2 for more discussions). ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Throughout this paper, boldface symbols denote vectors or matrices, while regular symbols represent scalars. For matrix $A\\in\\mathbb{R}^{n\\times m}$ , $A_{i,j}$ is the element at the $i$ -th row and $j$ -th column; $A_{i,}$ : and $A_{:,j}$ are the $i$ -th row and $j$ -th column vectors, respectively. We set $[n]:=\\{1,2,\\dots,n\\}$ , $\\Delta_{n}$ as the probability distribution space over $[n]$ , and $\\Delta_{n}^{m}=\\Delta_{n}\\times\\cdot\\cdot\\times\\Delta_{n}$ (m times). For any $v\\in\\mathbb R$ , $(v)^{+}:=\\operatorname*{\\bar{max}}(v,0)$ ", "page_idx": 2}, {"type": "text", "text": "2.1 Platform\u2019s Recommendation Problem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a platform performing recommendations for $T$ rounds, each indexed by $t\\in[T]$ , where one user visits the platform at each round. There are $N$ items on the platform, each indexed by $i\\in[N]$ , and the users are classified into $M$ types, each indexed by $j\\in[M]$ . At each round $t$ , a type- $J_{t}$ user arrives and the platform observes the user\u2019s type.1 Here, we consider a stochastic setting where the probability of having a type- $j$ user arrival is $\\overline{{\\mathbb{P}}}[J_{t}=j]=p_{j}$ , where $p=(p_{j})_{j\\in[M]}\\in\\bar{\\Delta}_{M}$ denotes the arrival probabilities2. The platform needs to select an item to display to the user, denoted by $I_{t}$ . If a type- $j$ user is presented with item $i$ , he/she would choose/purchase the item with probability $y_{i,j}\\in(0,1)$ , where $\\pmb{y}=(y_{i,j})_{\\mathbf{\\epsilon}_{j\\in[M]}}$ denotes the purchase probabilities. The platform then observes the user\u2019s purchase decision $z_{t}\\in\\{0,1\\}$ . If item $i$ gets chosen/purchased, it generates revenue $r_{i}>0$ , where $\\pmb{r}=(r_{i})_{i\\in[N]}$ denotes the revenues. ", "page_idx": 2}, {"type": "text", "text": "Let $\\pmb{\\theta}=(p,y,r)\\in\\Theta$ , where $\\Theta=\\Delta_{M}\\times(0,1)^{N\\times M}\\times\\mathbb{R}^{N}$ , define an instance of the platform\u2019s recommendation problem. Given the problem instance $\\pmb{\\theta}$ , the platform needs to determine its recommendation probabilities $\\pmb{x}~\\in~\\Delta_{N}^{M}$ , where $x_{i,j}$ is the probability of offering item $i$ upon observing a type- $j$ user\u2019s arrival.3In Section 2, we first fix the problem instance $\\pmb{\\theta}$ and omit variables\u2019 dependency on $\\pmb{\\theta}$ whenever the context allows. Later, we will transition to an online setting where the problem instance $\\pmb{\\theta}$ becomes unknown (see Section 3) reestablish the dependency in our discussion. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Here, we focus on a single-item recommendation setting, where the platform highlights one item to each arriving user. This approach applies to various real-world scenarios, such as Spotify\u2019s \u201cSong of the Day\u201d, Amazon\u2019s \u201cBest Seller\u201d, Medium\u2019s daily \u201cMust-Read\u201d article, etc. Later in Section 3.6 and our case study on Amazon review data in Section 4, we will show that the core concepts of our model and approach can naturally extend to recommending an assortment of items. ", "page_idx": 3}, {"type": "text", "text": "2.2 Single-Sided Fair Solutions: Within-Group Fairness for Items and Users ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To properly address multi-sided fairness, we begin by first considering within-group fairness. That is, we seek to answer: what constitutes a fair solution within our item/user group? To address this question, it is important to understand the different outcomes that items and users respectively care about, and the fairness notions they would like to consider. ", "page_idx": 3}, {"type": "text", "text": "Single-Sided Item-Fair Solutions. Let $O_{i}^{\\tt I}({\\pmb x})$ be the expected outcome received by item $i$ at a round with recommendation probabilities $\\textbf{\\em x}$ . We let $O_{i}^{\\bar{\\mathrm{I}}}(x)$ take the following general form: $O_{i}^{\\mathrm{I}}({\\pmb x})\\,=\\,L_{i,:}^{\\top}{\\pmb x}_{i,:}$ , where $L\\,=\\,(L_{i,j})\\,_{\\,i\\in[N]}$ and $L_{i,j}$ can be any proxy for the expected outcome received by item $i$ from type- $j$ user if it gets offered. One can consider any of the following metrics or their weighted combinations as the item\u2019s outcome: (1) visibility: $L_{i,j}=p_{j}$ and $\\begin{array}{r}{O_{i}^{\\tt I}({\\pmb x})=\\overline{{\\sum_{j}p_{j}x_{i,j}}}}\\end{array}$ ; (2) marketshare: $L_{i,j}=p_{j}y_{i,j}$ and $\\begin{array}{r}{O_{i}^{\\mathrm{I}}(\\pmb{x})=\\sum_{j}p_{j}y_{i,j}x_{i,j}}\\end{array}$ ; (3) expected revenue: $L_{i,j}=r_{i}p_{j}y_{i,j}$ and OiI (x) = j ripjyi,jxi,j . ", "page_idx": 3}, {"type": "text", "text": "For items, a common way to achieve fairness is via the optimization of a social welfare function (SWF), denoted as $W$ , which merges fairness and efficiency metrics into a singular objective (see [18]). The maximization of SWF can incorporate a broad spectrum of fairness notions commonly adopted in practice, including maxmin fairness [58], Kalai-Smorodinsky (K-S) bargaining solution [41], Hooker-Williams fairness [37], Nash bargaining solution [53], demographic parity, etc. (See Section A.2 for an expanded discussion of these fairness notions and their social welfare functions.) ", "page_idx": 3}, {"type": "text", "text": "To preserve the generality of our framework, we let the platform freely determine the outcome function and fairness notion for items, by broadly defining the item-fair solution as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Item-Fair Solution) Given items\u2019 outcome matrix ${\\pmb L}({\\pmb\\theta})$ , and a SWF $W:\\Delta_{N}^{M}\\rightarrow\\mathbb{R},$ , an item-fair solution is given by: $f^{I}\\in\\arg\\operatorname*{max}_{\\mathbf{x}\\in\\Delta_{N}^{M}}$ $W({\\pmb O}^{I}({\\pmb x}))$ . ", "page_idx": 3}, {"type": "text", "text": "One popular fairness notion that can be captured by Definition 2.1 is maxmin fairness, which ensures that the most disadvantaged item is allocated a fair portion of the outcome. A few prior works [55, 72] on fair recommendation solely focus on achieving maxmin fairness for the visibilities (exposure) received by the items. Our model, however, can flexibly accommodate any outcome functions as listed above and any fairness notions supported via SWF maximization (see Section A.2). ", "page_idx": 3}, {"type": "text", "text": "Single-Sided User-Fair Solutions. Let $O_{j}^{\\tt U}({\\pmb x})$ be the expected outcome received by a type- $j$ user at a round with recommendation probabilities $\\textbf{\\em x}$ . We again let $O_{j}^{\\tt U}({\\pmb x})$ take a general form: $O_{j}^{\\mathrm{U}}({\\pmb x})=U_{:,j}^{\\top}{\\pmb x}_{:,j}$ , where $U=\\left(U_{i,j}\\right)_{\\mathscr{i}\\in[N]}$ and $U_{i,j}$ is the expected outcome received by a type- $j$ user if offered item $i$ . Here, the specific form of the user\u2019s outcome matrix $U(\\pmb\\theta)$ is determined by user\u2019s utility model, which can vary depending on contexts. See Section A.1 for some example forms of $U(\\pmb\\theta)$ based on discrete choice models (multinomial logit (MNL), probit) used in demand modeling [65] and valuation-based models used in online auction design [51]. For generality, we do not impose restrictions on the form of $U(\\pmb\\theta)$ , but merely assume knowledge of it. ", "page_idx": 3}, {"type": "text", "text": "For users, achieving fairness is more straightforward. Since the platform can personalize recommendations based on user types, given the users\u2019 outcome matrix $U(\\pmb\\theta)$ , it is best for type- $j$ users to consistently receive the item that offers the highest utility. We thus define the user-fair solution as: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (User-Fair Solution) Given users\u2019 outcome matrix $U(\\pmb\\theta)$ , the user-fair solution $\\pmb{f}^{U}\\in$ $\\Delta_{N}^{M}$ is given by $\\pmb{f}_{i,j}^{\\upsilon}=1\\;i f\\,i=\\arg\\operatorname*{max}_{i}U_{i,j}$ and $\\pmb{f}_{i,j}^{\\upsilon}=0$ otherwise, for all $i\\in[N],j\\in[M]$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Drawbacks of a Single-Sided Solution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While a single-sided (fair) solution for one stakeholder group can be straightforward to identify, a recommendation policy that solely beneftis a single stakeholder group (platform, items, users) could result in extensive costs for the rest of the stakeholders, as suggested by the following proposition. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.1 Let $\\epsilon_{1}=\\operatorname*{min}{y/\\operatorname*{max}{y}}$ , $\\epsilon_{2}=\\operatorname*{min}U/\\operatorname*{max}U$ , and $\\epsilon=\\operatorname*{max}\\{\\epsilon_{1},\\epsilon_{2},1/N\\}$ . There exists a problem instance such that: $(I)$ The platform\u2019s revenue-maximizing solution results in zero outcomes for some items and all users attaining only \u03f5 of their maximum attainable outcome. (2) The item-fair solution leads to the platform receiving at most $2\\epsilon$ of its maximum attainable revenue and any user achieving at most $2\\epsilon$ of their maximum attainable outcome. (3) The user-fair solution results in zero outcomes for some items and the platform receiving \u03f5 of its maximum attainable revenue. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.1 shows that single-sided solutions that can lead to extremely unfair outcomes for some stakeholders under certain scenarios. This motivates us to investigate the concept of cross-group fairness in the following section, where we aim to balance the needs of all stakeholders. ", "page_idx": 4}, {"type": "text", "text": "2.4 Multi-Sided Solutions: From Within-Group Fairness to Cross-Group Fairness ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we proceed to investigate the concept of cross-group fairness by proposing a fair recommendation framework, Problem (FAIR). Specifically, we seek to answer the following: what constitutes a fair recommendation policy across all stakeholder groups on a multi-sided platform? ", "page_idx": 4}, {"type": "text", "text": "The platform, our main stakeholder, primarily wishes to optimize its expected revenue, denoted by $\\begin{array}{r}{\\mathrm{REV}(\\pmb{x})=\\sum_{i,j}r_{i}p_{j}y_{i,j}x_{i,j}}\\end{array}$ . Whenever a type- $j$ user arrives, a revenue-maximizing platform with full knowledge of the instance would show the most profitable item, i.e., $\\begin{array}{r}{i_{j}^{\\star}=\\arg\\operatorname*{max}_{i\\in[N]}r_{i}y_{i,j}}\\end{array}$ with probability one. We let OPT- $\\mathrm{REV}=\\operatorname*{max}_{\\mathbf{\\boldsymbol{x}}}\\mathrm{REV}(\\mathbf{\\boldsymbol{x}})$ be platform\u2019s maximum attainable expected revenue in the absence of fairness. Such a deterministic revenue-maximizing approach, as shown in Section 2.3, is evidently undesirable for the less profitable items and any user who prefer them. ", "page_idx": 4}, {"type": "text", "text": "To create a fair ecosystem, the platform wishes to impose certain levels of fairness for all items/users. In an offilne setting with full knowledge of the instance $\\pmb{\\theta}$ , the platform can first compute item-fair and user-fair solutions $\\pmb{f}^{\\tt I},\\pmb{f}^{\\tt U}$ and solve the following fair recommendation problem, Problem (FAIR), to prioritize its revenue maximization goal, while achieving cross-group fairness via fairness constraints: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{FAIR-REV}=\\underset{x\\in\\Delta_{N}^{M}}{\\mathrm{max}}\\,\\,\\,\\mathrm{REV}(\\pmb{x})}&{\\mathrm{s.t.}\\,\\,\\,\\,O_{i}^{\\intercal}(\\pmb{x})\\geq\\delta^{\\intercal}\\cdot O_{i}^{\\intercal}(\\pmb{f}^{\\intercal})\\,\\,\\,\\forall\\,\\,i\\in[N]}\\\\ &{}&{O_{j}^{\\intercal}(\\pmb{x})\\geq\\delta^{\\intercal}\\cdot O_{j}^{\\intercal}(\\pmb{f}^{\\intercal})\\,\\,\\,\\forall\\,\\,j\\in[M]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\delta^{\\mathtt{I}},\\delta^{\\mathtt{U}}\\in[0,1]$ are the fairness parameters for items and users respectively. Here, the item/user fairness constraints ensure that the outcome received by any item/user is at least of a certain proportion of the outcome that they would otherwise receive under their within-group fair solution. One can think of $\\delta^{\\tt I}$ and $\\delta^{\\tt U}$ as tunable handles that the platform can adjust to regulate the tradeoff between fairness and its own revenue (see how a platform can tune these parameters in practice in Sections 4 and C). If a platform wishes to incorporate other operational considerations such as diversity constraints $(x_{\\mathrm{min}}\\leq x_{i,j}\\leq x_{\\mathrm{max}})$ or fairness for more stakeholder groups beyond items/users (e.g., DoorDash drivers, Airbnb hosts), additional constraints can be incorporated in a similar fashion. ", "page_idx": 4}, {"type": "text", "text": "We let $x^{\\star}$ denote the optimal solution to Problem (FAIR), which serves as the benchmark that we compare our algorithm against when we next work with the online setting (see Section 3). ", "page_idx": 4}, {"type": "text", "text": "Remark 2.1 (Constrained optimization versus fairness regularizers) An alternative method of integrating fairness is to use fairness regularizers (i.e., Lagrangian multipliers) and optimize a weighted sum of platform\u2019s revenue and items\u2019/users\u2019 outcomes. Nonetheless, our constrained optimization formulation offers several clear beneftis: (i) while it is possible to translate our constrained optimization formulation into a weighted sum with fairness regularizers via dualization, the reverse process is not straightforward; (ii) our fairness parameters $\\delta^{I^{-}},\\delta^{\\boldsymbol{U}}$ are directly interpretable; (iii) our framework accommodates additional operational considerations, with results remaining valid. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.2 (Selecting fairness parameters via \u201cprice of fairness\u201d) To choose the right fairness parameters $\\delta^{I},\\delta^{U},$ , it is important to understand both (i) the extent of fairness needed for items/users, and (ii) the cost of implementing fairness constraints to balance its revenue and stakeholder interests, often known as the \u201cprice of fairness\u201d (PoF). In Section $C$ , we investigate the concept of PoF under our framework (Problem (FAIR)), and show a piecewise linear dependency of the PoF on both (i) the amount of misalignment in the platform\u2019s and its stakeholders\u2019 objectives, (ii) the fairness parameters $\\delta^{I}$ , $\\overset{\\cdot}{\\delta^{U}}$ (see Thoerem C.1). We further provide guidelines on how a platform can effectively tune its fairness parameters based on its desired fairness level and acceptable PoF via online A/B experimentation (see Section C for an extended discussion). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3 Fair Online Recommendation Algorithm for Multi-Sided Platforms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In practice, user data are often missing or inaccurate, so solving Problem (FAIR) with flawed data could inadvertently result in unfair outcomes. Our online setting (Section 3.1) allows the platform to improve its estimates of user data via a data collection process over time. However, this simultaneous process of learning to be fair and understanding user preferences also introduces new challenges. In this section, we address the more intricate online setting and introduce an algorithm for this scenario. ", "page_idx": 5}, {"type": "text", "text": "3.1 Online Setting and Goals ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In an online setting, the platform no longer has prior knowledge of user preference $\\textit{\\textbf{y}}$ and arrival rates $\\pmb{p}$ . At each round $t$ , some non-anticipating algorithm $\\boldsymbol{\\mathcal{A}}$ generates the recommendation probabilities $\\bar{\\mathbf{\\boldsymbol{x}}}_{t}\\in\\Delta_{N}^{M}$ , only based on past recommendation $\\{x_{t^{\\prime}}\\}_{t^{\\prime}<t}$ and purchase decisions $\\bar{\\{z_{t^{\\prime}}\\}}_{t^{\\prime}<t}$ . After observing the type of the arriving user $J_{t}$ , the platform offers item $i$ with probability $x_{t,i,J_{t}}$ . Following this recommendation, the platform only observes the user\u2019s binary purchase decision $z_{t}\\in\\{0,1\\}$ for the offered item. We will measure the performance of our algorithm using the following metrics: ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (Revenue and fairness regrets) Consider Problem (FAIR) under a problem instance $\\theta\\in\\Theta$ , item outcome function $L(\\pmb\\theta):\\Theta\\stackrel{\\subset}{\\to}\\mathbb{R}^{N}$ , user outcome function $U(\\pmb{\\theta}):\\Theta\\stackrel{\\textstyle-}{\\rightarrow}\\mathbb{R}^{M}$ , item social welfare function $W:\\!\\mathbb{R}^{N}\\rightarrow\\mathbb{R}$ , and fairness parameters $\\mathbf{\\Psi}^{;I},\\dot{\\delta}^{U}\\in[0,1]$ . Let $\\boldsymbol{\\mathcal{A}}$ be a non-anticipating algorithm that generates recommendation probabilities $\\pmb{x}_{t}$ at each round $t\\,\\in\\,[T]$ . We define the revenue regret, denoted by $\\mathcal{R}(T)$ , and the fairness regret, denoted by $\\mathcal{R}_{F}(T)$ , of $\\boldsymbol{\\mathcal{A}}$ respectively as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}(T)=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\mathtt{R E V}(\\pmb{x}^{\\star},\\theta)-\\mathtt{R E V}(\\pmb{x}_{t},\\theta)\\right)\\quad a n d\\quad\\mathcal{R}_{F}(T)=\\operatorname*{max}\\{\\mathcal{R}_{F}^{I}(T),\\mathcal{R}_{F}^{U}(T)\\}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{R}_{F}^{I}(T)=\\operatorname*{max}_{i}\\frac{1}{T}\\sum_{t=1}^{T}(\\delta^{I}{\\cdot}\\mathcal{O}_{i}^{I}(f^{I}(\\pmb{\\theta}),\\pmb{\\theta}){-}\\mathcal{O}_{i}^{I}(\\pmb{x}_{t},\\pmb{\\theta}))^{+}}\\end{array}$ and $\\begin{array}{r}{\\mathcal{R}_{F}^{U}(T)=\\operatorname*{max}_{j}\\frac{1}{T}\\sum_{t=1}^{T}(\\delta^{U}\\mathrm{.~}}\\end{array}$ \u00b7 $O_{j}^{U}(\\pmb{f}^{U}(\\pmb{\\theta}),\\pmb{\\theta})-O_{j}^{U}(\\pmb{x}_{t},\\pmb{\\theta}))^{+}$ are respectively the maximum time-averaged violation of item/user-fair constraints. Here, $\\begin{array}{r}{\\mathrm{REV}({\\pmb x},{\\pmb\\theta})=\\sum_{i,j}r_{i}p_{j}y_{i,j}x_{i,j}}\\end{array}$ is platform\u2019s expected revenue under instance $\\pmb{\\theta}$ ; $\\begin{array}{r}{O_{i}^{I}(\\pmb{x},\\pmb{\\theta})=L_{i,:}(\\pmb{\\theta})^{\\top}\\pmb{x}_{i},}\\end{array}$ : and $O_{j}^{\\upsilon}(\\pmb{x},\\pmb{\\theta})=U_{:,j}(\\pmb{\\theta})^{\\top}\\pmb{x}_{:,j}$ are the item/user outcomes under $\\pmb{\\theta}$ ; $\\pmb{f}^{I}(\\pmb\\theta)$ and $\\pmb{f}^{U}(\\pmb\\theta)$ are the item/user-fair solutions w.r.t. ${\\cal L}(\\pmb\\theta),{\\cal U}(\\pmb\\theta)$ and $S W F W$ , per Definition 2.1 and 2.2. ", "page_idx": 5}, {"type": "text", "text": "Observe that in Definition 3.1, we reintroduce the dependency of our variables on the instance $\\pmb{\\theta}$ . This is because in the online setting, we typically work with an estimated instance, which in turn impacts our estimates for the platform\u2019s revenue, item/user outcomes as well as item/user-fair solutions. ", "page_idx": 5}, {"type": "text", "text": "3.2 Challenges of the Online Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Before proceeding, we highlight the two main challenges unique to the online setting. ", "page_idx": 5}, {"type": "text", "text": "(1) Data uncertainty and partial feedback can interfere with evaluating fairness. Due to lack of knowledge of the instance $\\pmb{\\theta}$ and limited feedback (as we only observe the purchase decision for the offered items), it is difficult to assess the quality of our recommendations $\\pmb{x}_{t}$ at each round. In particular, verifying whether our fairness constraints are satisfied and/or measuring the amount of constraint violations require evaluating the item/user-fair solutions ${\\pmb f}^{\\mathrm{I}}(\\pmb\\theta),f^{\\mathrm{I}}(\\pmb\\theta)$ and item/user outcomes $O_{i}^{\\mathrm{I}}(f^{\\mathrm{I}}(\\pmb\\theta),\\pmb\\theta),O_{i}^{\\dagger}(\\pmb x_{t},\\pmb\\theta)$ and $\\bar{O_{j}^{\\intercal}}(f^{\\mathrm{{y}}}(\\pmb{\\theta}),\\pmb{\\theta}),O_{j}^{\\mathrm{{y}}}(\\pmb{x}_{t},\\pmb{\\theta})$ , all of which heavily depend on $\\pmb{\\theta}$ . ", "page_idx": 5}, {"type": "text", "text": "This sets our work apart from prior works on fairness in recommender systems, which assume full knowledge of the problem instance (e.g., [55, 71]), and from works on constrained optimization with bandit feedback (e.g., [38, 15, 61]), which rely on the availability of constraint feedback. As we discussed in Section 1.1, the latter works need to access the amount of constraint violation or verify constraint satisfaction after each decision to update their policies. For example, in online learning with knapsack, the constraint is a resource budget, so constraint violations can be directly evaluated. Our work contributes to the literature on constrained optimization with bandit feedback by directly handling uncertain constraints and providing a sublinear regret bound (see Theorem 3.1). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "(2) Fairness can interfere with the quality of learning. To ensure fairness for all stakeholder groups, some items (e.g., low-revenue or low-utility items) would necessarily receive lower recommendation probabilities than the others. Nonetheless, if we barely offer these items to the users, the lack of exploration could also lead to poor estimation for their purchase probability. ", "page_idx": 6}, {"type": "text", "text": "3.3 Algorithm Description ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present our algorithm, called FORM (Fair Online Recommendation algorithm for Multi-sided platforms), which handles the aforementioned challenges by adopting a relaxation-then-exploration technique, allowing it to achieve both low revenue regret and fairness regret. The design of FORM is outlined in Algorithm 1, consisting of the following. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Fair Online Recommendation Algorithm for Multi-Sided Platforms (FORM) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: (i) $N$ items with revenues $\\boldsymbol{r}\\in\\mathbb{R}^{N}$ , item outcome $L(\\pmb{\\theta}):\\Theta\\rightarrow\\mathbb{R}^{N\\times M}$ , item SWF $W:\\Delta_{N}^{M}\\to\\mathbb{R}$ ; (ii) $M$ types of users with user outcome $U(\\pmb{\\theta}):\\Theta\\rightarrow\\mathbb{R}^{N\\times M}$ ; (iii) fairness parameters $\\delta^{\\mathtt{I}},\\delta^{\\mathtt{U}}\\in[0,1]$ . 1. Initialization. Set $\\hat{y}_{1,i,j}=1/2$ and $\\hat{p}_{j}=1/M$ for $i\\in[N],j\\in[M]$ . Let the magnitude of exploration be $\\epsilon_{t}=\\operatorname*{min}\\left\\{N^{-1},N^{-\\frac{2}{3}}t^{-\\frac{1}{3}}\\right\\}$ , and define the magnitude of relaxation as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta_{t}=M\\log(T)\\operatorname*{max}\\{\\Gamma_{y,t},\\Gamma_{p,t}\\}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Gamma_{y,t}=2\\log(T)/\\sqrt{\\epsilon_{t}\\cdot\\operatorname*{max}\\{1,t/\\log(T)-\\sqrt{t\\log(T)/2}\\}}$ and $\\Gamma_{p,t}=5\\sqrt{\\log(3T)/t}$ . 2. For $t=1,\\dots,T$ ", "page_idx": 6}, {"type": "text", "text": "(a) Solve a relaxed version of Problem (FAIR) under data uncertainty. Given estimated instance $\\pmb{\\hat{\\theta}}_{t}=(\\hat{p}_{t},\\hat{y}_{t},\\pmb{r})$ and relaxation $\\eta_{t}$ , let $\\hat{\\pmb x}_{t}$ be the optimal solution to Problem (FAIR-RELAX $(\\widehat{\\pmb{\\theta}}_{t},\\eta_{t}))$ . ", "page_idx": 6}, {"type": "text", "text": "\u2022 Let the recommendation probability be $\\pmb{x}_{t,i,j}=(1-N\\epsilon_{t})\\hat{\\pmb{x}}_{t,i,j}+\\epsilon_{t}$ for all $i\\in[N],j\\in[M]$ .   \n\u2022 Observe the type of the arriving user $J_{t}$ and offer item $I_{t}$ based on probabilities $I_{t}\\sim{\\pmb x}_{t,:,J_{t}}$ .   \n\u2022 Observe purchase decision $z_{t}\\in\\{0,1\\}$ and update the number of user arrivals: $n_{J_{t},t}=n_{J_{t},t-1}+1$ . ", "page_idx": 6}, {"type": "text", "text": "(c) Update estimates for purchase and arrival probabilities. Let ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{y}_{t+1,i,j}=\\frac{1}{n_{j,t}}\\sum_{k=1}^{n_{j,t}}\\mathbb{I}\\{I_{\\tau_{j,k}}=i,z_{\\tau_{j,k}}=1\\}/x_{\\tau_{j,k},i,j},\\hat{p}_{t+1,j}=\\frac{1}{t}n_{j,t},\\hat{\\theta}_{t+1}=\\left(\\hat{p}_{t+1},\\hat{y}_{t+1},\\boldsymbol{r}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\tau_{j,k}\\in[T]$ denote the round at which the $k\\mathrm{th}$ type- $j$ user arrives. ", "page_idx": 6}, {"type": "text", "text": "Solve a relaxed version of Problem (FAIR) under the estimated instance. Recall, from our first challenge, that we cannot directly verify if the fairness constraints have been satisfied due to having data uncertainty and partial feedback. If the platform solves Problem (FAIR) using the estimated instance, the flaw in estimation could easily lead to failure of maintaining fairness for some stakeholders. In face of this, FORM solves a relaxed version of Problem (FAIR), defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in\\Delta_{N}^{M}}{\\operatorname*{max}}}&{\\mathsf{R E V}(x,\\hat{\\theta}_{t})\\quad\\mathrm{s.t.}\\quad O_{i}^{\\mathbb T}(x,\\hat{\\theta}_{t})\\geq\\delta^{\\mathbb T}\\cdot O_{i}^{\\mathbb T}(f^{\\mathbb T}(\\hat{\\theta}_{t}),\\hat{\\theta}_{t})-\\eta_{t}\\quad\\forall i\\in[N]}\\\\ &{}&\\\\ &{\\quad O_{j}^{\\mathbb U}(x,\\hat{\\theta}_{t})\\geq\\delta^{\\mathbb U}\\cdot O_{j}^{\\mathbb U}(f^{\\mathbb U}(\\hat{\\theta}_{t}),\\hat{\\theta}_{t})-\\eta_{t}\\quad\\forall j\\in[M]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\widehat{\\pmb{\\theta}}_{t}$ is the estimated instance at round $t$ and $\\eta_{t}>0$ is a parameter that regulates the magnitude of relaxation on our fairness constraints (Eq. (1)). Here, Problem (FAIR-RELAX $(\\hat{\\pmb{\\theta}}_{t},\\eta_{t}))$ differs from Problem (FAIR) in that (i) it uses the estimated instance $\\widehat{\\pmb{\\theta}}_{t}$ rather than the ground-truth instance $\\pmb{\\theta}$ , and (ii) it relaxes all fairness constraints by the amount of $\\eta_{t}$ . At a high level, the relaxation here ensures that the solution fair to all stakeholders (i.e., $x^{\\star}$ ) would be captured even under data uncertainty. The magnitude of relaxation $\\eta_{t}$ depends on $\\Gamma_{y,t}$ and $\\Gamma_{p,t}$ , which are respectively confidence bounds associated with estimated purchase/arrival probabilities (see Definition D.1). As our estimates become more accurate, both confidence bounds shrink, hence decreasing the magnitude of fairness relaxation. ", "page_idx": 6}, {"type": "text", "text": "Recommend with randomized exploration. In order to handle the second challenge of some items being inadequately explored, we incorporate randomized exploration by sampling from a distribution that perturbs the estimated solution to Problem (FAIR), $\\hat{\\pmb x}_{t}$ , by a carefully tailored amount $\\epsilon_{t}$ . This allows ongoing exploration of all items, with the magnitude of exploration $\\epsilon_{t}$ also decreasing over time as our parameter estimates improve, shifting from exploration towards greater exploitation. ", "page_idx": 7}, {"type": "text", "text": "Unbiased estimators for our problem instance. In Algorithm 1, for simplicity, we estimate purchase probabilities $\\textit{\\textbf{y}}$ using an inverse probability weighted estimator and estimate arrival probabilities $\\textbf{\\emph{p}}$ with the sample mean (See Eq. (2)). For sufficiently large $t$ , our estimates $\\hat{\\pmb y}_{t}$ and $\\hat{\\pmb{p}}_{t}$ will be accurate with high probability (see Lemma D.2). In practice, the platform, potentially with access to historical data, can freely use any learning mechanism that yield unbiased estimators for user preferences and arrival rates. As long as the estimates get sufficiently accurate over time with high probability, FORM would ensure low revenue/fairness regrets, all while keeping the rest of its design unchanged. ", "page_idx": 7}, {"type": "text", "text": "3.4 Theoretical Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We theoretically analyze the performance of FORM, under a mild local Lipschitzness assumption. ", "page_idx": 7}, {"type": "text", "text": "Assumption 3.1 Given instance $\\pmb\\theta\\in\\Theta$ , there exists constants $B,\\zeta>0$ such that for any $\\tilde{\\pmb{\\theta}}\\in\\Theta$ where $-\\,\\theta\\|_{\\infty}\\leq\\zeta,\\,\\mathrm{max}\\{\\|U(\\theta)-U(\\tilde{\\theta})\\|_{\\infty},\\|f^{I}(\\theta)-f^{I}(\\tilde{\\theta})\\|_{\\infty}\\}\\leq B\\|\\theta-\\tilde{\\theta}\\|_{\\infty}$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 3.1 is well justified in practice. In terms of users\u2019 outcome matrix, the assumption readily holds for all prevalent users\u2019 choice models and valuation-based models (see examples in Section A.1) as long as $\\pmb{\\theta}$ is bounded away from the boundaries of $\\Theta$ . For item-fair solutions $\\pmb{f}^{\\mathrm{I}}(\\pmb\\theta)$ , a wide range of standard outcome functions (visibility, revenue, etc.) and fairness notions (maxmin, K-S, etc.) readily induce locally Lipschitz item-fair solutions; see Section A.3 for some examples. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.1 is the main result of this section, which states that FORM achieves both sublinear revenue regret and fairness regret, as desired. The proof of Theorem 3.1 is deferred to Section D. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.1 (Performance of FORM) Given any problem instance $\\pmb\\theta\\in\\Theta$ and assume that Assumption 3.1 holds, for $T$ sufficiently large, we have that ", "page_idx": 7}, {"type": "text", "text": "\u2022 the revenue regret of FORM is at most $\\mathbb{E}[\\mathcal{R}(T)]\\le\\mathcal{O}(M N^{\\frac{1}{3}}T^{-\\frac{1}{3}})$ ;   \n\u2022 the fairness regret of FORM is at most $\\mathbb{E}[\\mathcal{R}_{F}(T)]\\le\\mathcal{O}(M N^{\\frac{1}{3}}T^{-\\frac{1}{3}})$ . ", "page_idx": 7}, {"type": "text", "text": "3.5 Computational complexity and scalability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In terms of the computational complexity of FORM, the dominant runtime cost in each iteration of FORM arises from solving Problem (FAIR-RELAX $(\\hat{\\pmb{\\theta}}_{t},\\eta_{t}))$ . Note that for a wide variety of commonly used item-fairness notions (e.g., maxmin, K-S, demographic parity; see Table 1) and item outcome functions (e.g., visibility, revenue), solving Problem $\\big(\\mathrm{FAIR-RELAX}\\big(\\hat{\\pmb{\\theta}}_{t},\\eta_{t}\\big)\\big)$ involves solving two linear programs with $M N$ variables, which is solvable in polynomial runtime $O^{*}((M N)^{2+1/18})$ [40]. The remaining operations in each iteration of FORM takes $O(M N)$ time. Consequently, each iteration of FORM has a worst-case complexity of $O^{*}((M N)^{2+1/18})$ . In practice, however, much better performance can often be achieved by advanced LP solvers such as Gurobi and CPLEX. ", "page_idx": 7}, {"type": "text", "text": "For real-world deployments, FORM can be further adapted with scalability in consideration. First, in practice, there is no need to solve Problem (FAIR-RELAX $(\\hat{\\pmb{\\theta}}_{t},\\eta_{t}))$ at every user arrival. Instead, platforms can resolve the problem after a given number of user arrivals or periodically at fixed time intervals, while updating user data in real-time. This allows majority of the iterations to run in $O(M N)$ time and removes the computational overhead.4 Second, we do not always encounter a large-scale optimization problem when applying our framework. Real-world recommendation systems often narrow down items through lightweight pre-filtering stages based on criteria like keywords or price range (e.g., [49]), allowing us to enforce fairness within smaller, context-specific subsets. Our fairness framework is also particularly impactful at this final stage, where items with similar attributes compete for visibility and a revenue-maximizing strategy could lead to extremely unfair outcomes. ", "page_idx": 7}, {"type": "text", "text": "3.6 Extensions to Additional Setups ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our framework, Problem (FAIR), and the proposed algorithm can be extended to accommodate other variations of our setup. Below, we briefly introduce these extensions; see Section E for more details. ", "page_idx": 8}, {"type": "text", "text": "Periodic arrivals. While our current model focuses on stochastic arrivals with fixed user arrival probabilities $\\textbf{\\emph{p}}$ , real-world recommendation systems often observe non-stationary user arrivals with hourly, daily or weekly periodicity. In Section E.1, we show that by additionally integrating a sliding window mechanism, FORM can seamlessly accommodate periodic arrivals, and attain the same $\\mathcal{O}(M\\bar{N}^{1/3}T^{-1/3})$ guarantees for both revenue and fairness regrets. ", "page_idx": 8}, {"type": "text", "text": "Recommending an assortment. As remarked in Section 2, while our model adopts a single-item recommendation setting, the high-level ideas behind our framework/method naturally extend to recommending an assortment of size at most $K$ . To extend our framework, Problem (FAIR), we will let $\\{q_{j}(S):S^{*}\\!\\in[N],|S|\\leq K,j\\in[M]\\}$ be the decision variables, where $q_{j}(S)$ is the likelihood of proposing assortment $S$ to a type- $j$ user. We can then apply the same relaxation-then-exploration techniques to solve the fair recommendation problem in a dynamic online setting. See Section E.2 for details of our extension. In the case study that follows (Section 4), we also validate the efficacy of our framework/method in a real-world assortment recommendation problem. ", "page_idx": 8}, {"type": "text", "text": "4 Case Studies on Amazon Review Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In our case study on Amazon review data, we act as an e-commerce platform displaying featured products to incoming users, aiming to maximize revenue while ensuring fairness for items and users. Our experiments numerically validate the efficacy of our framework/method. All algorithms were implemented in Python 3.7 and run on a MacBook with a $1.4\\:\\mathrm{GHz}$ Quad-Core Intel Core i5 processor. ", "page_idx": 8}, {"type": "text", "text": "Data and setup. We use an Amazon review dataset [71] from the \u201cClothing, Shoes and Jewelry\u201d category. Product reviews provide relevance scores between each item and user, serving as a proxy for purchase likelihood. Users are classified into $M=5$ types using matrix factorization and $k$ -means clustering on user feature vectors. The arrival probability $p_{j}$ is set to the proportion of type- $j$ users. We select $N=30$ items with the highest variance in relevance scores across user types, indicating a discrepancy between item and user interests and making this a challenging instance. Item revenues $r_{i}$ are uniformly drawn from [0.5, 1.5]. The purchase probability and users\u2019 utilities are defined based on the multinomial logit (MNL) model [65]. For a type- $j$ user presented with assortment $S$ , the probability of purchasing item $i\\in S$ is $\\begin{array}{r}{y_{i,j}=\\frac{e^{v_{i,j}}}{1+\\sum_{i^{\\prime}\\in S}e^{v_{i^{\\prime},j}}}}\\end{array}$ , where $v_{i,j}$ is the relevance score. The user\u2019s perceived utility is $\\begin{array}{r}{\\log(1+\\sum_{i^{\\prime}\\in S}e^{v_{i^{\\prime},j}})}\\end{array}$ . Each instance simulates user arrivals. Upon arrival, each type- $^j$ user is shown an assortment $S$ of up to $K=3$ items. ", "page_idx": 8}, {"type": "text", "text": "The platform\u2019s primary goal is to maximize its revenue while ensuring maxmin fairness for items w.r.t. item revenue, and fairness for users w.r.t. utilities from the MNL model. We apply the extension of FORM for recommending assortments, using relaxation-then-exploration techniques to produce fair recommendations while learning user data (see Section E.2). To establish generality of our framework/method, we have also performed additional experiments under alternative outcomes and fairness notions (see Section F.2) and an alternative movie recommendation setting using MovieLens data (see Section F.3). In all cases, our experiments yielded consistent results. ", "page_idx": 8}, {"type": "text", "text": "Baselines. We consider six baselines for comparisons. Since all baselines assume full knowledge of the instance and lack a learning phase, we let them use our unbiased estimator to update their estimated instance as they observe purchase decisions, and recommend based on these estimates. (i) greedy: offers $K$ items with the highest expected revenue, prioritizing platform\u2019s goal; (ii) max-utility: offers $K$ items with the highest user utilities, a user-centric approach; (iii) min-revenue: offers $K$ items generating the least revenue so far, promoting maxmin fairness for items w.r.t. revenue; (iv) random: offers $K$ items uniformly at random, promoting maxmin fairness for items w.r.t. visibility; (v) FairRec [55]: an algorithm that addresses two-sided fairness in a static setting. While it\u2019s not designed for online settings, we adapt it for online arrivals by duplicating users and using the singleshot recommendation solution. It ensures maxmin fairness for item visibility and envy-free fairness for users; (vi) TFROM [71]: addresses two-sided fairness in an online setting, focusing on uniform item visibility and similar user normalized discounted cumulative gain. However, neither FairRec nor TFROM considers platform\u2019s revenue; see Section F.1 for more details on these two baselines. ", "page_idx": 8}, {"type": "image", "img_path": "tAOg1HdvGy/tmp/886f9fc8d4ff56f2a190a352191d8264b2f953ffdf5db7c62ee4261e5af7a22f.jpg", "img_caption": ["Figure 1: Experiment results for Amazon review data. FAIR-REV $(\\delta^{\\tt I},\\delta^{\\tt I})$ is the platform\u2019s revenue from solving Problem (FAIR) in hindsight with fairness parameters $\\delta^{\\tt I},\\delta^{\\tt U}$ and $\\mathrm{\\dot{F}O R M}(\\mathrm{\\dot{\\delta}^{I}},\\mathrm{\\dot{\\delta}^{U}})$ is FORM when adopting fairness parameters $\\delta^{\\tt I},\\delta^{\\tt U}$ . In Figures 1c and 1d, item (user) outcomes are shown in ascending order. All resul\u221ats are averaged over 10 simulations, with the line indicating the mean and shaded region showing mean $\\pm{\\mathrm{~std}}/{\\sqrt{10}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Platform\u2019s revenue. We first assess FORM\u2019s efficacy in achieving a low-regret solution. Figure 1a shows that the time-averaged revenue of FORM, i.e., tT=1 REV(xt), converges rapidly to the optimal revenue for Problem (FAIR), complementing Theorem 3.1. Figure 1b compares the timeaveraged revenue (normalized by OPT-REV) of FORM with other baselines. As expected, greedy achieves revenue close to OPT-REV, while all other baselines result in significant revenue loss. FairRec and TFROM, in particular, reduce platform revenue by about $38\\%$ as their sole focus is on ensuring two-sided fairness. In contrast, FORM offers tunable parameters to balance platform and stakeholder interests. As shown in Figure 1b, adjusting $\\delta^{\\mathtt{I}}$ and $\\delta^{\\bar{\\mathrm{U}}}$ allows platforms to control revenue loss (e.g., choosing $\\delta^{\\mathtt{I}}$ , $\\delta^{0}=0.2$ keeps loss within $10\\%$ ). See, also, Section C for how a platform can tune fairness parameters to control its \u201cprice of fairness\u201d in practice. ", "page_idx": 9}, {"type": "text", "text": "Item and user fairness. Figure 1c shows average outcomes for each item $i$ , normalized by the outcome under item-fair solution, $\\operatorname*{max}\\{\\textstyle\\frac{1}{T}O_{i}^{\\tt I}({\\pmb x}_{t}\\bar{\\bf\\Delta})/O_{i}^{\\tt I}({\\pmb f}^{\\tt I}),1\\}$ . As expected, since our item-fair solution $f^{\\mathrm{I}}$ adopts maxmin fairness w.r.t. item revenues, min-revenue achieves the highest level of item fairness, though at a high cost to the platform. Methods such as random, FairRec, TFROM also achieve high item fairness but with some discrepancies in maximum and minimum item outcomes. greedy and max-utility show extremely skewed allocations, with some items receiving minimal or no revenue. In comparison, our algorithm FORM strikes a good balance, ensuring all items nearly attain or surpass the specified fairness levels, whether the level is high $\\delta_{\\mathrm{I}}=0.8)$ ) or moderate $(\\delta_{\\mathrm{I}}=0.2)$ ). ", "page_idx": 9}, {"type": "text", "text": "Figure 1d shows the average outcomes for each user type, normalized by their outcome under the user-fair solution, $\\operatorname*{max}\\{\\bar{\\L}_{T}^{1}\\bar{O}_{j}^{\\mathrm{U}}(\\pmb x_{T})/O_{j}^{\\mathrm{U}}(\\pmb f_{\\ L}^{\\mathrm{U}}),1\\}$ . In the Amazon review data, user interests align well with the platform\u2019s objectives (as validated in Section C), leading to most baselines performing fairly well and achieving high levels of fairness for the users. FORM again ensures good user outcomes, all while maintaining high platform revenue and desired item fairness levels. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work introduced a novel fair recommendation framework that maintains platform revenue while addressing multi-stakeholder fairness, as well as a low-regret algorithm that effectively produces fair recommendations amidst data uncertainty. It is worth noting that the high-level ideas behind our versatile framework has the potential to be applied in settings beyond recommender systems, such as dynamic pricing and online advertising, ensuring fairness across different stakeholders and promoting stable market conditions in these applications. ", "page_idx": 9}, {"type": "text", "text": "There are several future directions worth investigating. (i) Our current framework calibrates recommendation policies within a shorter time period when user preferences and item attributes are relatively fixed. Future research can explore long-term effects of our method by developing adaptive fairness notions that account for evolving user and item attributes and quantifying the long-term multi-stakeholder fairness. (ii) It\u2019d be interesting to pursue real-world deployments of our framework/algorithm and evaluate their impact using an expanded set of metrics, such as user satisfaction, retention rates, and recommendation diversity. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "N.G. and Q.C. were partially supported by funding from the Office of Naval Research (ONR) (Award Number: N00014-23-1-2584) and the MIT-IBM Watson AI Lab. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] H. Abdollahpouri and R. Burke. Multi-stakeholder recommendation and its connection to multi-sided fairness. arXiv preprint arXiv:1907.13158, 2019.   \n[2] R. Abebe, S. Barocas, J. Kleinberg, K. Levy, M. Raghavan, and D. G. Robinson. Roles for computing in social change. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 252\u2013260, 2020.   \n[3] S. Agrawal, V. Avadhanula, V. Goyal, and A. Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453\u20131485, 2019.   \n[4] Airbnb. A six-year update on airbnb\u2019s work to fight discrimination. https://news.airbnb. com/sixyearadupdate/, 2022. Accessed: 2023-10-01.   \n[5] M. R. Aminian, V. Manshadi, and R. Niazadeh. Fair markovian search. Available at SSRN 4347447, 2023.   \n[6] J. Baek and V. F. Farias. Fair exploration via axiomatic bargaining. arXiv preprint arXiv:2106.02553, 2021. [7] S. R. Balseiro, H. Lu, and V. Mirrokni. The best of many worlds: Dual mirror descent for online allocation problems. Operations Research, 71(1):101\u2013119, 2023.   \n[8] S. Barocas and A. D. Selbst. Big data\u2019s disparate impact. California law review, pages 671\u2013732, 2016.   \n[9] D. Bertsimas, V. F. Farias, and N. Trichakis. The price of fairness. Operations research, 59(1):17\u201331, 2011.   \n[10] A. Beutel, J. Chen, T. Doshi, H. Qian, L. Wei, Y. Wu, L. Heldt, Z. Zhao, L. Hong, E. H. Chi, et al. Fairness in recommendation ranking through pairwise comparisons. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2212\u20132220, 2019.   \n[11] A. J. Biega, K. P. Gummadi, and G. Weikum. Equity of attention: Amortizing individual fairness in rankings. In The 41st international acm sigir conference on research & development in information retrieval, pages 405\u2013414, 2018.   \n[12] R. Burke. Multisided fairness for recommendation. arXiv e-prints, pages arXiv\u20131707, 2017.   \n[13] R. Burke, N. Sonboli, and A. Ordonez-Gauger. Balanced neighborhoods for multi-sided fairness in recommendation. In Conference on fairness, accountability and transparency, pages 202\u2013214. PMLR, 2018.   \n[14] T. Calders, F. Kamiran, and M. Pechenizkiy. Building classifiers with independency constraints. In 2009 IEEE International Conference on Data Mining Workshops, pages 13\u201318. IEEE, 2009.   \n[15] M. Castiglioni, A. Celli, and C. Kroer. Online learning with knapsacks: the best of both worlds. In International Conference on Machine Learning, pages 2767\u20132783. PMLR, 2022.   \n[16] H. A. Chaudhari, S. Lin, and O. Linda. A general framework for fairness in multistakeholder recommendations. arXiv preprint arXiv:2009.02423, 2020.   \n[17] Q. Chen, N. Golrezaei, and F. Susan. Fair assortment planning. arXiv preprint arXiv:2208.07341, 2022.   \n[18] V. Chen and J. Hooker. Welfare-based fairness through optimization. Technical report, Working Paper, Carnegie-Mellon University, 2021.   \n[19] A. Dash, A. Chakraborty, S. Ghosh, A. Mukherjee, and K. P. Gummadi. When the umpire is also a player: Bias in private label product recommendations on e-commerce marketplaces. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 873\u2013884, 2021.   \n[20] Y. Deldjoo, D. Jannach, A. Bellogin, A. Difonzo, and D. Zanzonelli. Fairness in recommender systems: research landscape and future directions. User Modeling and User-Adapted Interaction, pages 1\u201350, 2023.   \n[21] Y. Deng, N. Golrezaei, P. Jaillet, J. C. N. Liang, and V. Mirrokni. Individual welfare guarantees in the autobidding world with machine-learned advice. arXiv preprint arXiv:2209.04748, 2022.   \n[22] Y. Deng, N. Golrezaei, P. Jaillet, J. C. N. Liang, and V. Mirrokni. Multi-channel autobidding with budget and roi constraints. arXiv preprint arXiv:2302.01523, 2023.   \n[23] L. Devroye. The equivalence of weak, strong and complete convergence in l1 for kernel density estimates. The Annals of Statistics, pages 896\u2013904, 1983.   \n[24] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214\u2013226, 2012.   \n[25] B. Edelman, M. Luca, and D. Svirsky. Racial discrimination in the sharing economy: Evidence from a field experiment. American economic journal: applied economics, 9(2):1\u201322, 2017.   \n[26] M. D. Ekstrand, M. Tian, I. M. Azpiazu, J. D. Ekstrand, O. Anuyah, D. McNeill, and M. S. Pera. All the cool kids, how do they fti in?: Popularity and demographic biases in recommender evaluation and effectiveness. In Conference on fairness, accountability and transparency, pages 172\u2013186. PMLR, 2018.   \n[27] Etsy. Search engine optimization (seo) for shop and listing pages. https://help.etsy.com/hc/en-us/articles/ 115015663987-Search-Engine-Optimization-SEO-for-Shop-and-Listing-Pages? segment $=$ selling. Accessed: 2024-02-28.   \n[28] Z. Feng, S. Padmanabhan, and D. Wang. Online bidding algorithms for return-on-spend constrained advertisers. arXiv preprint arXiv:2208.13713, 2022.   \n[29] D. Freund, T. Lykouris, E. Paulson, B. Sturt, and W. Weng. Group fairness in dynamic refugee assignment. arXiv preprint arXiv:2301.10642, 2023.   \n[30] Z. Fu, Y. Xian, R. Gao, J. Zhao, Q. Huang, Y. Ge, S. Xu, S. Geng, C. Shah, Y. Zhang, et al. Fairness-aware explainable recommendation over knowledge graphs. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 69\u201378, 2020.   \n[31] Y. Ge, S. Liu, R. Gao, Y. Xian, Y. Li, X. Zhao, C. Pei, F. Sun, J. Ge, W. Ou, et al. Towards longterm fairness in recommendation. In Proceedings of the 14th ACM international conference on web search and data mining, pages 445\u2013453, 2021.   \n[32] N. Golrezaei, R. Niazadeh, K. K. Patel, and F. Susan. Online combinatorial optimization with group fairness constraints. Available at SSRN 4824251, 2024.   \n[33] O. G\u00fcler, A. J. Hoffman, and U. G. Rothblum. Approximations to solutions to systems of linear inequalities. SIAM Journal on Matrix Analysis and Applications, 16(2):688\u2013696, 1995.   \n[34] S. Gupta, J. Moondra, and M. Singh. Socially fair and hierarchical facility location problems. arXiv preprint arXiv:2211.14873, 2022.   \n[35] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst., 5(4), dec 2015.   \n[36] H. Heidari and J. Kleinberg. Allocating opportunities in a dynamic model of intergenerational mobility. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 15\u201325, 2021.   \n[37] J. N. Hooker and H. P. Williams. Combining equity and utilitarianism in a mathematical programming model. Management Science, 58(9):1682\u20131693, 2012.   \n[38] N. Immorlica, K. Sankararaman, R. Schapire, and A. Slivkins. Adversarial bandits with knapsacks. Journal of the ACM, 69(6):1\u201347, 2022.   \n[39] D. Jannach and G. Adomavicius. Price and profit awareness in recommender systems. arXiv preprint arXiv:1707.08029, 2017.   \n[40] S. Jiang, Z. Song, O. Weinstein, and H. Zhang. Faster dynamic matrix inverse for faster lps. arXiv preprint arXiv:2004.07470, 2020.   \n[41] E. Kalai and M. Smorodinsky. Other solutions to nash\u2019s bargaining problem. Econometrica: Journal of the Econometric Society, pages 513\u2013518, 1975.   \n[42] M. Kasy and R. Abebe. Fairness, equality, and power in algorithmic decision-making. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 576\u2013586, 2021.   \n[43] A. Lambrecht and C. Tucker. Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads. Management science, 65(7):2966\u20132981, 2019.   \n[44] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. Lars: A location-aware recommender system. In 2012 IEEE 28th international conference on data engineering, pages 450\u2013461. IEEE, 2012.   \n[45] Y. Li, H. Chen, Z. Fu, Y. Ge, and Y. Zhang. User-oriented fairness in recommendation. In Proceedings of the Web Conference 2021, pages 624\u2013632, 2021.   \n[46] W. Ma, P. Xu, and Y. Xu. Fairness maximization among offline agents in online-matching markets. arXiv preprint arXiv:2109.08934, 2021.   \n[47] M. MacCarthy. An examination of the algorithmic accountability act of 2019. Available at SSRN 3615731, 2019.   \n[48] R. Mehrotra, J. McInerney, H. Bouchard, M. Lalmas, and F. Diaz. Towards a fair marketplace: Counterfactual evaluation of the trade-off between relevance, fairness & satisfaction in recommendation systems. In Proceedings of the 27th acm international conference on information and knowledge management, pages 2243\u20132251, 2018.   \n[49] Meta. Powered by ai: Instagram\u2019s explore recommender system. https://ai.meta.com/ blog/powered-by-ai-instagrams-explore-recommender-system/. Accessed: 2024- 10-01.   \n[50] J. Mulvany and R. S. Randhawa. Fair scheduling of heterogeneous customer populations. Available at SSRN 3803016, 2021.   \n[51] R. B. Myerson. Optimal auction design. Mathematics of operations research, 6(1):58\u201373, 1981.   \n[52] M. Naghiaei, H. A. Rahmani, and Y. Deldjoo. Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 770\u2013779, 2022.   \n[53] J. F. Nash Jr. The bargaining problem. Econometrica: Journal of the econometric society, pages 155\u2013162, 1950.   \n[54] Official Journal of the European Union. Regulation (eu) 2022/1925 of the european parliament and of the council of 14 september 2022 on contestable and fair markets in the digital sector and amending directives (eu) 2019/1937 and (eu) 2020/1828 (digital markets act). https:// eur-lex.europa.eu/legal-content/EN/TXT/?uri $=$ CELEX%3A32022R1925, 2023. Accessed: 2023-04-19.   \n[55] G. K. Patro, A. Biswas, N. Ganguly, K. P. Gummadi, and A. Chakraborty. Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms. In Proceedings of the web conference 2020, pages 1194\u20131204, 2020.   \n[56] J. Pena, J. C. Vera, and L. F. Zuluaga. New characterizations of hoffman constants for systems of linear constraints. Mathematical Programming, 187:79\u2013109, 2021.   \n[57] B. Rastegarpanah, K. P. Gummadi, and M. Crovella. Fighting fire with fire: Using antidote data to improve polarization and fairness of recommender systems. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 231\u2013239, 2019.   \n[58] J. Rawls. A theory of justice: Revised edition. Harvard university press, 2020.   \n[59] Y. Seldin, C. Szepesv\u00e1ri, P. Auer, and Y. Abbasi-Yadkori. Evaluation and analysis of the performance of the exp3 algorithm in stochastic environments. In European Workshop on Reinforcement Learning, pages 103\u2013116. PMLR, 2013.   \n[60] A. Singh and T. Joachims. Policy learning for fairness in ranking. Advances in neural information processing systems, 32, 2019.   \n[61] A. Slivkins, K. A. Sankararaman, and D. J. Foster. Contextual bandits with packing and covering constraints: A modular lagrangian approach via regression. In The Thirty Sixth Annual Conference on Learning Theory, pages 4633\u20134656. PMLR, 2023.   \n[62] W. Sun, D. Dey, and A. Kapoor. Safety-aware algorithms for adversarial contextual bandit. In International Conference on Machine Learning, pages 3280\u20133288. PMLR, 2017.   \n[63] The New York Times. Food businesses lose faith in instagram after algorithm changes. https: //www.nytimes.com/2022/03/22/dining/instagram-algorithm-reels.html, 2022. Accessed: 2022-03-22.   \n[64] The Wall Street Journal. On orbitz, mac users steered to pricier hotels. https://www.wsj. com/articles/SB10001424052702304458604577488822667325882. Accessed: 2024-08- 01.   \n[65] K. E. Train. Discrete choice methods with simulation. Cambridge university press, 2009.   \n[66] L. Wang, Y. Bai, W. Sun, and T. Joachims. Fairness of exposure in stochastic bandits. In International Conference on Machine Learning, pages 10686\u201310696. PMLR, 2021.   \n[67] Y. Wang, W. Ma, M. Zhang, Y. Liu, and S. Ma. A survey on the fairness of recommender systems. ACM Transactions on Information Systems, 41(3):1\u201343, 2023.   \n[68] C. Wu, F. Wu, X. Wang, Y. Huang, and X. Xie. Fairness-aware news recommendation with decomposed adversarial learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 4462\u20134469, 2021.   \n[69] H. Wu, C. Ma, B. Mitra, F. Diaz, and X. Liu. A multi-objective optimization framework for multi-stakeholder fairness-aware recommendation. ACM Transactions on Information Systems, 41(2):1\u201329, 2022.   \n[70] H. Wu, B. Mitra, C. Ma, F. Diaz, and X. Liu. Joint multisided exposure fairness for recommendation. In Proceedings of the 45th International ACM SIGIR Conference on research and development in information retrieval, pages 703\u2013714, 2022.   \n[71] Y. Wu, J. Cao, G. Xu, and Y. Tan. Tfrom: A two-sided fairness-aware recommendation model for both customers and providers. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1013\u20131022, 2021.   \n[72] C. Xu, S. Chen, J. Xu, W. Shen, X. Zhang, G. Wang, and Z. Dong. P-mmf: Provider max-min fairness re-ranking in recommender system. In Proceedings of the ACM Web Conference 2023, pages 3701\u20133711, 2023.   \n[73] M. Zehlike, F. Bonchi, C. Castillo, S. Hajian, M. Megahed, and R. Baeza-Yates. Fa\\* ir: A fair top-k ranking algorithm. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 1569\u20131578, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Interpolating Item and User Fairness for Multi-Sided Recommendations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Example Choices of Outcomes, Fairness Notions and Fair Solutions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Example Utility Functions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Below are some common example utility functions $U(\\pmb\\theta)$ based on common discrete choice models (multinomial logit (MNL), probit) used in demand modeling [65] and valuation-based models used in online auction design [51]. For all of these examples, $U_{i,j}$ is strictly increasing w.r.t. the purchase probability yi,j. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Multinomial logit (MNL) model. Let the utility of item $i$ for a type $j$ user be $v_{i,j}+\\epsilon_{i,j}$ , where $v_{i,j}\\geq0$ and $\\epsilon_{i,j}$ is the random part drawn i.i.d. from the standard Gumbel distribution. Let $\\epsilon_{0,j}$ be the utility of no-purchase option, again drawn i.i.d. from the standard Gumbel distribution. The expected utility is $\\begin{array}{r}{U_{i,j}=\\mathbb{E}\\left[\\operatorname*{max}\\{v_{i,j}+\\epsilon_{i,j},\\epsilon_{0,j}\\}\\right]=\\log(1+\\exp(v_{i,j}))\\_{\\tau}\\gamma}\\end{array}$ , where $\\gamma$ is the Euler-Mascheroni constant. The purchase probability is given by $y_{i,j}=\\mathbb{P}\\left[v_{i,j}+\\epsilon_{i,j}>\\epsilon_{0,j}\\right]=$ $\\frac{\\exp(v_{i,j})}{1\\!+\\!\\exp(v_{i,j})}$ . Hence, the expected utility can be viewed as $\\begin{array}{r}{U_{i,j}=\\log(\\frac{1}{1-y_{i,j}})+\\gamma}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 Probit model. Let the utility of item $i$ for a type $j$ user again take the form of $v_{i,j}+\\epsilon_{i,j}$ , where the random part $\\epsilon_{i,j}$ and the utility of the no-purchase option $\\epsilon_{0,j}$ are drawn i.i.d. from a normal distribution ${\\mathcal{N}}(0,\\sigma)$ . Here, the expected utility $U_{i,j}\\,=\\,\\mathbb{E}\\left[\\operatorname*{max}\\{v_{i,j}+\\epsilon_{i,j},\\epsilon_{0,j}\\}\\right]\\,=$ $\\begin{array}{r}{v_{i,j}\\Phi\\left(\\frac{v_{i,j}}{\\sqrt{2}\\sigma}\\right)\\,+\\,\\sqrt{2}\\sigma\\phi\\left(\\frac{v_{i,j}}{\\sqrt{2}\\sigma}\\right)}\\end{array}$ , where $\\Phi(.)$ and $\\phi(.)$ are the CDF and PDF of a standard normal distribution. The purchase probability is $\\begin{array}{l c l}{{y_{i,j}}}&{{=}}&{{\\Phi(\\frac{v_{i,j}}{\\sqrt{2}\\sigma})}}\\end{array}$ . We thus have $U_{i,j}\\;\\;=\\;\\;$ $\\sqrt{2}\\sigma\\Phi^{-1}(y_{i,j})y_{i,j}+\\sqrt{2}\\sigma\\phi(\\Phi^{-1}(y_{i,j}))\\,.$ ", "page_idx": 14}, {"type": "text", "text": "\u2022 Valuation-based model. Let $v_{i,j}\\sim F_{i,j}$ be the value of item $i$ for a type $j$ user, where $F_{i,j}$ is the distribution of $v_{i,j}$ . Then, $y_{i,j}^{\\setminus}=\\mathbb{P}[(v_{i,j}-r_{i})\\geq0]$ and $U_{i,j}=\\mathbb{E}[(v_{i,j}-r_{i})^{+}]$ , where $r_{i}$ is the price (revenue) of item $i$ . If the valuations follow exponential distribution $v_{i,j}\\sim\\mathrm{Exp}(\\lambda)$ for some $\\lambda>0$ , we have purchase probability $y_{i,j}=\\exp(-\\lambda r_{i})$ and expected utility $U_{i,j}=$ $\\textstyle{\\frac{1}{\\lambda}}\\exp(-\\lambda r_{i})=y_{i,j}/\\lambda$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Example Fairness Notions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given a group of $S$ stakeholders, each indexed by $s\\in[S]$ , and any function that maps the platform\u2019s decision $\\textbf{\\em x}$ to the stakeholders\u2019 outcome $O(x)\\in\\mathbb{R}_{+}^{S}$ , we can solve the following optimization problem to ensure that each of the $S$ stakeholders are offered a fair outcome: $\\pmb{f}=\\arg\\operatorname*{max}_{\\pmb{x}}~W(\\pmb{O}(\\pmb{x}))$ . where the social welfare function $W$ determines which fairness notion we adopt. In Table 1, we list some example fairness notions commonly adopted in practice, as well as their corresponding social welfare functions $W$ . Among them, we have ", "page_idx": 14}, {"type": "text", "text": "\u2022 Maxmin fairness [58]: It maximizes the minimum outcome across all stakeholders, ensuring that the stakeholder with the least favorable outcome is as well-off as possible. \u2022 Kalai-Smorodinsky (K-S) fairness [41]: It seeks an equitable outcome where each stakeholder receives a proportional share of their maximum possible outcome. This is represented by a proportional allocation up to a common factor $\\beta$ . \u2022 Hooker-Williams fairness $I37J$ : It aims to balance the efficiency/equity tradeoff using parameter $\\Delta$ , by prioritizing stakeholders whose outcomes are within $\\Delta$ of the minimum outcome. \u2022 Nash bargaining solution: It maximizes the product of stakeholders\u2019 outcomes, ensuring an equitable distribution that reflects their relative negotiating power and achieves proportional fairness. \u2022 Demographic parity: It ensures that outcomes are equally distributed across different demographic groups, by minimizing the disparity in outcomes between a subset of stakeholders $S^{\\prime}$ and the rest. ", "page_idx": 14}, {"type": "table", "img_path": "tAOg1HdvGy/tmp/f7f98dcc9d1b4e89329975ac4bf56719167a8d17a33c1ee1e5e9cf62ca93f489.jpg", "table_caption": ["Table 1: Fairness notions and their social welfare functions (SWF). "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Choosing the right fairness notion is non-trivial and very much depends on the context. Some important considerations include: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Stakeholder needs and outcomes. Understanding the desired outcomes for items and users is crucial. Our framework, Problem (FAIR), is designed to handle various outcome functions (e.g., revenue, marketshare, visibility), which can differ across platforms. For example, a video streaming platform might aim to ensure fair visibility for both independent content creators and popular studios, while an e-commerce platform might focus more on fair marketshare/revenue for small sellers versus large brands. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Implication of fairness notion. Each fairness notion has a different implication, and platforms need to evaluate which best suits their goals and stakeholder needs. For example, maxmin fairness maximizes the outcome received by the most disadvantaged stakeholder, which can be ideal for video streaming platforms like Netflix or YouTube that wish to ensure independent and lesserknown content creators receive fair visibility alongside popular creators. K-S fairness ensures that each individual receives a fair share of his/her maximum attainable outcome. This can be suitable for platforms like LinkedIn or Indeed that wish to ensure fair opportunities for their job seekers relative to their qualifications and experience. Platforms may also need to experiment with different fairness notions to understand how their choices impact their \"price of fairness\" (see discussion in Section C). ", "page_idx": 15}, {"type": "text", "text": "\u2022 Regulatory requirements. As we discussed in Section 1, one important motivation for imposing fairness in online recommendations is the increasing regulatory action. Therefore, the choice of fairness notions can also depend on legislative or regulatory requirements. For example, since the Digital Markets Act [54] calls for a fair and open digital marketplace, maxmin fairness can be potentially suitable as it ensures even the most disadvantaged item receive a fair level of exposure. ", "page_idx": 15}, {"type": "text", "text": "A.3 Example Item-Fair Solutions with Local Lipschitzness ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we discuss a number of item-fair solutions that adopt different outcome functions and fairness notions, and show that they satisfy Assumption 3.1 in Section 3.4. In the following, we will assume that the item-fair solution satisfies the following condition: ", "page_idx": 15}, {"type": "text", "text": "Condition A.1 Under problem instance $\\theta\\in\\Theta$ , the item outcome $\\textbf{\\emph{L}}$ and the social welfare function $W$ are chosen such that ", "page_idx": 15}, {"type": "text", "text": "Note that statement (i) in Condition A.1 is satisfied by all of the common definitions of item outcome, such as (1) visibility: $L_{i,j}\\,=\\,p_{j}$ ; (2) marketshare: $L_{i,j}\\,=\\,p_{j}\\,y_{i,j}$ ; (3) expected revenue: $L_{i,j}\\,=$ $r_{i}p_{j}y_{i,j}$ . On the other hand, the uniqueness condition can be achieved via adding regularization or lexicographic optimization to the social welfare function. In light of Condition A.1, we enumerate several item-fair solutions adopting prevalent fairness notions (see Section A.2 for definitions) such that local Lipschitzness is satisfied, as outlined in Assumption 3.1. ", "page_idx": 15}, {"type": "text", "text": "Example 1: Item-Fair Solution with Maxmin Fairness. Suppose that an item-fair solution $f^{\\mathrm{I}}$ adopts maxmin fairness, $f^{\\mathrm{I}}$ would be the solution to the following linear program (LP): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}\\in\\Delta_{N}^{M},z\\in\\mathbb{R}}z\\quad\\mathrm{s.t.}\\quad\\pmb{L}_{i,:}^{\\top}\\pmb{x}_{i,:}\\geq z\\quad\\forall i\\in[N]\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\pmb{\\theta}=(\\pmb{p},\\pmb{y},\\pmb{r}),\\tilde{\\pmb{\\theta}}=(\\tilde{\\pmb{p}},\\tilde{\\pmb{y}},\\tilde{\\pmb{r}})\\in\\Theta$ be two problem instances such that $\\begin{array}{r}{\\|\\pmb{\\theta}-\\tilde{\\pmb{\\theta}}\\|_{\\infty}\\leq\\zeta}\\end{array}$ for some $\\zeta>0$ . We consider the two item-fair solutions enforcing maxmin fairness under the two problem instances. For simplicity of notation, we let $f^{\\tt I}=f^{\\tt I}(\\pmb\\theta)$ and $\\tilde{\\pmb{f}}^{\\mathrm{I}}=\\pmb{f}^{\\mathrm{I}}(\\tilde{\\pmb{\\theta}});\\pmb{L}=\\pmb{L}(\\pmb{\\theta})$ and $\\tilde{L}=L(\\tilde{\\pmb{\\theta}})$ . The item-fair solutions are obtained via the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\pmb{f}^{\\intercal},z^{\\star})=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\Delta_{N}^{M},z\\in\\mathbb{R}}z\\quad\\mathrm{s.t.}\\quad\\pmb{L}_{i,:}^{\\intercal}\\pmb{x}_{i,:}\\geq z\\quad\\forall i\\in[N]\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\tilde{f}^{\\mathrm{I}},\\tilde{z}^{\\star})=\\arg\\operatorname*{max}_{\\substack{\\mathbf{x}\\in\\Delta_{N}^{M},\\,z\\in\\mathbb{R}}}z\\quad\\mathrm{s.t.}\\quad\\tilde{L}_{i,:}^{\\top}\\mathbf{x}_{i,:}\\geq z\\quad\\forall i\\in\\left[N\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|f^{\\tt I}-\\tilde{f}^{\\tt I}\\|_{\\infty}\\leq\\|(f^{\\tt I},z^{\\star})-(\\tilde{f}^{\\tt I},\\tilde{z}^{\\star})\\|_{\\infty}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider the region $\\mathcal{D}=\\{(\\mathbf{\\mathit{x}},z):L_{i,:}^{\\top}\\mathbf{\\mathit{x}}_{i,:}\\geq z$ for all $i\\in[N],z\\geq z^{\\star}\\}$ . Under Condition A.1, we know that $(f^{\\tt I},z^{\\star})$ is only feasible point in $\\mathcal{D}$ . We can then invoke Lemma G.1 (with $\\pmb{x}=(\\pmb{f}^{\\mathrm{I}},z^{\\star})$ and ${\\pmb x}^{\\prime}=(\\tilde{f}^{\\tt I},\\tilde{z}^{\\star})$ and the feasibility region $\\mathcal{D}$ ) to bound the distance between $(\\pmb{f}^{\\mathrm{I}},z^{\\star})$ and $(\\tilde{\\pmb{f}}^{\\tt I},\\tilde{z}^{\\star})$ using a Hoffman constant $H>0$ and a constraint violation term: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\boldsymbol{f}^{\\tt T},\\tilde{\\boldsymbol{z}}^{\\star})-(\\tilde{\\boldsymbol{f}}^{\\tt I},\\tilde{\\boldsymbol{z}}^{\\star})\\|_{\\infty}\\leq H\\cdot\\operatorname*{max}\\{\\operatorname*{max}_{i}\\|(\\tilde{\\boldsymbol{z}}^{\\star}-L_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\tt I})^{+}\\|_{\\infty},(\\boldsymbol{z}^{\\star}-\\tilde{\\boldsymbol{z}}^{\\star})^{+}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the Hoffman constant can be characterized by invoking Lemma G.2 for the matrix defining region $\\mathcal{D}$ . ", "page_idx": 16}, {"type": "text", "text": "Now, let us bound the two terms (i.e. $\\operatorname*{max}_{i}\\|(\\tilde{z}^{\\star}-L_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\tt I})^{+}\\|_{\\infty}$ and $(z^{\\star}-\\tilde{z}^{\\star})^{+})$ , on the right-hand side of (6) respectively. Let us first denote $E=\\|\\boldsymbol{L}-\\boldsymbol{\\tilde{L}}\\|_{\\infty}$ . ", "page_idx": 16}, {"type": "text", "text": "To bound the first term, given that for all $i\\in[N],\\tilde{f}^{\\mathtt{I}}$ satisfies $\\tilde{L}_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\mathrm{I}}\\geq\\tilde{z}^{\\star}$ , we must have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{z}^{\\star}-L_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\tt I}\\leq(\\tilde{z}^{\\star}-\\tilde{L}_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\tt I})+(\\tilde{L}_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\tt I}-L_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\tt I})\\leq M E\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To bound the second term, let us additionally consider the following auxiliary problem: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\hat{f}^{\\mathrm{^I}},\\hat{z}^{\\star}\\right)=\\arg\\operatorname*{max}_{x\\in{\\Delta_{N}^{M},z\\in\\mathbb{R}}}z\\quad\\mathrm{s.t.}\\quad L_{i,:}^{\\top}x_{i,:}\\geq z+M E\\quad\\forall i\\in\\left[N\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that if $L_{i,:}^{\\top}x_{i,:}\\,\\geq\\,z+M E$ , we must also have $\\tilde{L}_{i,:}^{\\top}x_{i,:}\\geq L_{i,:}^{\\top}x_{i,:}-M E\\geq z$ . Hence, the feasibility region of Problem (8) is included in the feasibility region of Problem (4), which implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{z}^{\\star}\\leq\\tilde{z}^{\\star}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, note that Problem (8) is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{{\\boldsymbol x}\\in\\Delta_{N}^{M},{\\boldsymbol z}^{\\prime}\\in\\mathbb{R}}}{\\boldsymbol z}^{\\prime}-M{\\boldsymbol E}\\quad\\mathrm{s.t.}\\quad L_{i,:}^{\\top}{\\boldsymbol x}_{i,:}\\geq{\\boldsymbol z}^{\\prime}\\quad\\forall i\\in[N]\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "if we perform change of variable $z^{\\prime}=z+M E$ . This implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\nz^{\\star}={\\hat{z}}^{\\star}+M E\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Equations (9) and (11) together imply that ", "page_idx": 16}, {"type": "equation", "text": "$$\nz^{\\star}-\\tilde{z}^{\\star}\\leq M E\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, combining Equations (5), (6), (7) and (12), we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f^{\\tt T}-\\tilde{f}^{\\tt T}\\|_{\\infty}\\leq\\|(f^{\\tt T},\\tilde{z}^{\\star})-(\\tilde{f}^{\\tt T},\\tilde{z}^{\\star})\\|_{\\infty}\\leq H\\cdot\\operatorname*{max}\\{\\|(\\tilde{z}^{\\star}-L_{i,:}^{\\top}\\tilde{f}_{i,:}^{\\tt T})^{+}\\|_{\\infty},(z^{\\star}-\\tilde{z}^{\\star})^{+}\\}}\\\\ &{\\qquad\\qquad\\leq H\\cdot M E=H\\cdot M\\|L-\\tilde{L}\\|_{\\infty}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By statement (i) in Condition A.1, we thus establish the local Lipschitzness of item-fair solution $f^{\\mathrm{I}}$ that enforces maxmin fairness. ", "page_idx": 16}, {"type": "text", "text": "Example 2: Item-Fair Solution with Hooker-Williams Fairness. If the platform adopts an item-fair solution $f^{\\mathrm{I}}$ w.r.t. Hooker-Williams fairness with some given $\\Delta>0$ (see definition in Section A.2), ", "page_idx": 16}, {"type": "text", "text": "$f^{\\mathrm{I}}$ would be the solution to a mixed linear integer program, with the following LP relaxation (it is shown that the LP relaxation describes the convex hull of the feasibility set; see [37]): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l l l l}{\\displaystyle\\operatorname*{max}_{\\substack{x\\in\\Delta_{N}^{M},\\;}}}&{\\;z}&{\\mathrm{s.t.}}&{(N-1)\\Delta+\\displaystyle\\sum_{i=1}^{N}v_{i}\\ge z}\\\\ {\\displaystyle z,v_{i},w\\in\\mathbb{R}^{+},\\delta_{i}\\in[0,1]}&{}&&{\\displaystyle L_{i,:}^{\\top}x_{i,:}-\\Delta\\le v_{i}\\le L_{i,:}^{\\top}x_{i,:}-\\Delta\\delta_{i}}&{}&{\\forall i\\in[0,1]}\\\\ &{}&&{w\\le v_{i}\\le w+(\\Gamma-\\Delta)\\delta_{i}}&{}&{\\forall i}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $\\Delta>0$ is a constant that regulates the equity/efficiency tradeoff, and $\\Gamma$ can be any constant such that $\\Gamma\\geq\\bar{L}$ where $\\bar{L}=\\|\\pmb{L}\\|_{\\infty}^{\\top}$ . We would fix $\\dot{\\Gamma}=2M\\bar{L}\\dot{+}\\Delta$ in the following. ", "page_idx": 17}, {"type": "text", "text": "Let $\\theta,\\widetilde{\\pmb{\\theta}}\\in\\Theta$ be two problem instances such that $\\lVert\\pmb{\\theta}-\\tilde{\\pmb{\\theta}}\\rVert_{\\infty}\\leq\\zeta$ for some $\\zeta>0$ . For simplicity of notation, we let ${\\pmb L}={\\pmb L}({\\pmb\\theta})$ and $\\tilde{L}=L(\\tilde{\\pmb{\\theta}})$ . Here, using the same techniques as in maxmin fairness, we let $(f^{\\tt I},z^{\\star},{\\boldsymbol v}^{\\star},w^{\\star},\\dot{\\delta}^{\\star})$ denote the solution to Problem (13) under the problem instance $\\pmb{\\theta}$ , and $(\\tilde{f}^{\\mathrm{I}},\\tilde{z}^{\\star},\\tilde{v},\\tilde{w},\\tilde{\\pmb\\delta})$ denote the solution to Problem (13) under the problem instance $\\tilde{\\pmb{\\theta}}$ . We can bound the difference in the two item-fair solutions similarly by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{\\mathbb{I}}-\\tilde{f}^{\\mathbb{I}}\\|_{\\infty}\\leq\\|(f^{\\mathbb{I}},z^{\\star},v,w,\\delta)-(\\tilde{f}^{\\mathbb{I}},\\tilde{z}^{\\star},\\tilde{v},\\tilde{w},\\tilde{\\delta})\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq H\\cdot\\operatorname*{max}\\{\\operatorname*{max}_{i}\\|(\\tilde{v}_{i}-(L_{i,:}^{\\top}\\tilde{x}_{i,:}-\\Delta\\tilde{\\delta}_{i}))^{+}\\|_{\\infty}\\,,\\operatorname*{max}_{i}\\|((L_{i,:}^{\\top}\\tilde{x}_{i,:}-\\Delta)-\\tilde{v}_{i})^{+}\\|_{\\infty}\\,,(z^{\\star}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality follows from uniqueness of $f^{\\mathrm{I}}$ in Condition A.1 and applying Lemma G.1 to the feasibility region of Problem (13) with the additional constraint $z\\geq z^{\\star}$ . The argument here is similar to what we did for item-fair solution with maxmin fairness. Here, $H$ is the Hoffman constant associated with Problem (13) under instance $\\pmb{\\theta}$ , which can be characterized using Lemma G.2. ", "page_idx": 17}, {"type": "text", "text": "It suffices to bound the three terms on the right hand side of (14). The first two terms can be bounded using similar techniques as done in the case of maxmin fairness. Using ${\\cal E}=\\|{\\cal L}-\\tilde{\\cal L}\\|_{\\infty}$ and the fact that $\\tilde{L}_{i,:}^{\\top}\\tilde{\\pmb{x}}_{i,:}-\\Delta\\leq\\tilde{v}_{i}\\leq\\tilde{L}_{i,:}^{\\top}\\tilde{\\pmb{x}}_{i,:}-\\Delta\\tilde{\\delta}_{i}$ for all $i\\in[N]$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{v}_{i}-(\\widetilde{L}_{i,:}^{\\top}\\widetilde{x}_{i,:}-\\Delta\\widetilde{\\delta}_{i})\\leq M E\\quad\\mathrm{and}\\quad(\\widetilde{L}_{i,:}^{\\top}\\widetilde{x}_{i,:}-\\Delta)-\\widetilde{v}_{i}\\leq M E}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $i\\in[N]$ . To bound the third term in (14), we again adopt a similar technique as above and consider an auxiliary problem ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l l l l l}{\\displaystyle\\operatorname*{max}_{\\substack{\\mathbf{x}\\in\\Delta_{N},\\,}}}&{\\;z}&{\\mathrm{s.t.}}&{(N-1)\\Delta+\\displaystyle\\sum_{i=1}^{N}v_{i}\\ge z}\\\\ {\\displaystyle\\qquad\\mathrm{s.},\\,v_{i}\\mathrm{\\in}\\mathrm{e}^{\\mathrm{i}\\cdot\\delta_{i}}\\mathrm{\\in}[0,1]}&{}&&{\\displaystyle\\mathbf{L}_{i,:}^{\\top}x_{i,:}-\\Delta+M E\\le v_{i}\\le L_{i,:}^{\\top}x_{i,:}-\\Delta\\delta_{i}-M E}&{}&{\\forall i\\in\\{1,\\ldots,N\\}}\\\\ &{}&&{w\\le v_{i}\\le w+(\\Gamma-\\Delta)\\delta_{i}}&{}&{\\forall i}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $(\\hat{\\pmb{f}}^{\\mathrm{I}},\\hat{z}^{\\star},\\hat{\\pmb{v}},\\hat{w},\\hat{\\pmb{\\delta}})$ denote the solution to Problem (13). Consider the feasibility region of Problem (13) under the problem instance $\\tilde{\\pmb{\\theta}}$ , which contains the feasibility region of Problem (15). This then gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{z}^{\\star}\\leq\\tilde{z}^{\\star}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We note that if we solve Problem (13) under problem instance, we claim that we must have $\\delta_{i}^{\\star}\\leq1/2$ . This is because if $\\delta_{i}^{\\star}>1/2$ , we would have $w+(\\Gamma-\\Delta)\\delta_{i}^{\\star}\\geq2M\\bar{L}\\delta_{i}^{\\star}>L_{i,:}^{\\top}f_{i,:}^{\\mathrm{I}}-\\Delta\\delta_{i}$ . That is, the upper bound in the third constraint is not tight, so the upper bound in the second constraint should be tight. However, setting $\\delta_{i}^{\\star}=1/2$ would yield a better objective. ", "page_idx": 17}, {"type": "text", "text": "Having this in mind, we choose $\\zeta>0$ sufficiently small such that $M E<\\textstyle{\\frac{1}{4}}\\Delta$ (this is doable since $\\ell_{i}(\\pmb\\theta)$ is Lipschitz in $\\pmb{\\theta}$ . Then, Problem (15) is feasible. We note that $(\\pmb{f}^{\\mathrm{I}},\\bar{\\hat{z}},\\hat{\\pmb{v}},\\hat{w},\\pmb{\\delta}^{\\star})$ is a feasible solution to Problem (15), where $\\hat{v}_{i}\\,=\\,v_{i}^{\\star}\\,-\\,M E$ and $\\hat{w}\\,=\\,w\\,-\\,M E,\\hat{z}\\,=\\,z^{\\star}\\,-\\,N M E$ . This is because under the optimal solution $(f^{\\mathrm{I}},z^{\\star},{\\boldsymbol v}^{\\star},w^{\\star},\\delta^{\\star})$ to Problem (13), both upper bounds for $v_{i}$ should be tight (otherwise, there exists a $\\delta$ that yields a better objective). We thus have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{z}^{\\star}\\geq\\hat{z}=z^{\\star}-N M E\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Equations (16) and (17) together give ", "page_idx": 18}, {"type": "equation", "text": "$$\nz^{\\star}-\\tilde{z}^{\\star}\\leq N M E\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which bounds the third term in the right-hand side of (14). ", "page_idx": 18}, {"type": "text", "text": "Having established the bounds in the right-hand side of (14), we have shown that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|f^{\\mathrm{I}}-\\tilde{f}^{\\mathrm{I}}\\|_{\\infty}\\le H\\cdot N M E=H\\cdot N M\\|L-\\tilde{L}\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By statement (i) in Condition A.1, we thus establish the local Lipschitzness of item-fair solution $f^{\\mathrm{I}}$ that enforces Hooker-Williams fairness. ", "page_idx": 18}, {"type": "text", "text": "Example 3: Item-Fair Solution with Kalai-Smorodinsky (K-S) Fairness. To obtain item-fair solution $f^{\\mathrm{I}}$ under K-S fairness, we solve the following problem given problem instance $\\pmb{\\theta}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{{\\pmb x}\\in\\Delta_{N}^{M},\\beta\\in\\left[0,1\\right]}}\\beta\\quad\\mathrm{s.t.}\\quad\\frac{{\\pmb L}_{i,:}^{\\top}{\\pmb x}_{i,:}}{{\\pmb L}_{i}^{\\star}}\\geq\\beta\\quad\\forall i\\in\\left[N\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{L_{i}^{\\star}\\,=\\,\\sum_{j}L_{i,j}}\\end{array}$ denotes the maximum outcome that item $i$ can receive. Note that this is achievable if the platform always show item $i$ regardless of the type of the arriving user. ", "page_idx": 18}, {"type": "text", "text": "By similar arguments as in our discussion for maxmin fairness, we would get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|f^{\\boldsymbol{\\mathrm{U}}}-\\tilde{f}^{\\boldsymbol{\\mathrm{U}}}\\|_{\\infty}\\le H\\frac{E}{\\operatorname*{min}_{i}L_{i}^{\\star}}=\\frac{H}{\\operatorname*{min}_{i}L_{i}^{\\star}}\\cdot\\|\\boldsymbol{L}-\\tilde{\\boldsymbol{L}}\\|_{\\infty}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $H$ is the Hoffman constant that can be characterized by invoking Lemma G.2. By statement (i) in Condition A.1, we thus have the local Lipschitzness of item-fair solution $f^{\\mathrm{I}}$ that enforces K-S fairness. ", "page_idx": 18}, {"type": "text", "text": "B Proof of Proposition 2.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As we discussed in Section 2.3, if a platform adopts a recommendation that solely benefits a single stakeholder group, this could result in extensive costs for the rest of the stakeholders, including the platform itself. The following Example B.1 establishes Proposition 2.1, where we consider a problem instance with one highly popular item and another notably profitable item. Under such a problem instance, the perceptions of a \u201cfair solution\u201d can vary widely among different stakeholders. ", "page_idx": 18}, {"type": "text", "text": "Example B.1 (A single-sided solution can be extremely unfair to the other sides.) Consider $a$ problem instance with $N$ items and $M$ types of users. For $i\\in[M],j\\in[N],$ , let the probability of purchases $y_{1,j}\\,=\\,1\\,$ and $y_{i,j}\\,=\\,\\epsilon_{1}$ where $\\epsilon_{1}\\ll1$ for all $i\\neq1$ ; probability of arrival $p_{j}=1/M$ ; revenue $r_{2}=1/\\epsilon_{1}^{2}$ and $r_{i}=1$ , for all $i\\neq2$ . For simplicity, let the utility of the users $U_{i,j}=y_{i,j}$ . Let $\\epsilon=\\operatorname*{max}\\{\\epsilon_{1},1/N\\}$ . Under such a instance, we have the following. ", "page_idx": 18}, {"type": "text", "text": "Platform\u2019s revenue-maximizing solution. A platform that seeks to maximize its revenue would always recommend item 2 to any arriving user, which yields expected revenue of $1/\\epsilon_{1}\\gg1$ . However, such $a$ solution is extremely unfair for any items $i\\neq2$ that receive zero outcome. This is also unfair to all users, as they would receive utility 1 from item 1 but only receives utility $\\epsilon_{1}$ from item 2. ", "page_idx": 18}, {"type": "text", "text": "An item-fair solution. Suppose items consider visibility as their outcome and adopts maxmin fairness, offering all items equal visibility $x_{i,j}=1/N$ is an item-fair solution. However, this can be unfair for all users who prefers item 1, whose utility under the item-fair solution is $1/N+(N-1)/N\\dot{\\epsilon}_{1}<2\\epsilon$ . This is also unfair to the platform that prefers item 2, as its current revenue becomes $1/N\\cdot1/\\epsilon_{1}+$ $1/N+(N-2)/N\\cdot\\epsilon_{1}<1/N\\cdot1/\\epsilon_{1}+1=1/N\\cdot1/\\epsilon_{1}+\\epsilon_{1}\\cdot1/\\epsilon_{1}\\leq2\\epsilon\\cdot1/\\epsilon_{1}$ . ", "page_idx": 18}, {"type": "text", "text": "A user-fair solution. A user-fair solution would always display item 1 to all types of users. Nonetheless, such a solution is extremely unfair to the rest of the items that receives zero outcome. It is also unfair to the platform, as always displaying item 1 results in expected revenue of 1, while if it displays item 2 it would gain $1/\\epsilon_{1}$ . ", "page_idx": 18}, {"type": "text", "text": "C Price of Fairness ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we formally characterize the confilcting interests among different stakeholders using a concept called the price of fairness, and show that our formulation of Problem (FAIR) provides the platform with flexible handles to find the right middleground. ", "page_idx": 18}, {"type": "text", "text": "The concept of price of fairness was first introduced by [9], which we formally define as follows. ", "page_idx": 19}, {"type": "text", "text": "Definition C.1 (Price of Fairness) Given a problem instance $\\pmb\\theta\\in\\Theta$ , an item-fair solution $f^{I}$ and a user-fair solution $\\boldsymbol{f}^{U}$ . We let OPT- $\\mathrm{REV}\\,=\\,\\operatorname*{max}_{\\mathbf{x}}\\mathrm{\\,REV}(\\pmb{x})$ be the maximum achievable expected revenue in the absence of fairness, and let FAIR-REV be the solution to Problem (FAIR). The price of fairness $(P o F)$ is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{PoF}=\\frac{\\mathrm{OPT-REV-FAIR-REV}}{\\mathrm{OPT-REV}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The price of fairness, which depends on the problem instance $\\pmb{\\theta}$ and $(\\delta^{\\tt I},\\delta^{\\tt U})$ , quantifies the trade-off between item/user fairness and platform interests, where a lower value is favored by the platform. In Theorem C.1, we formally upper bound the price of fairness introduced by Problem (FAIR) when we interpolate item and user fairness with parameters $\\delta^{\\tt I},\\delta^{\\tt U}$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem C.1 Given a problem instance $\\pmb\\theta\\in\\Theta$ , item-fair solution $f^{I}$ and user-fair solution $\\boldsymbol{f}^{U}$ . Let $\\pmb{x}^{\\mathrm{opT}}=\\arg\\operatorname*{max}_{\\pmb{x}}\\mathrm{REV}(\\pmb{x})$ be the revenue-maximizing solution in the absence of fairness. If we solve Problem (FAIR) with parameters $\\delta^{I}$ and $\\delta^{U}$ and assuming that the problem is feasible, the price of fairness is at most ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{PoF}\\leq H\\cdot\\operatorname*{max}\\left\\{\\operatorname*{max}_{i\\in[N]}(\\delta^{I}\\cdot O_{i}^{I}(f^{I})-O_{i}^{I}(x^{\\mathrm{ovr}}))^{+},\\operatorname*{max}_{j\\in[M]}(\\delta^{U}\\cdot O_{j}^{U}(f^{U})-O_{j}^{U}(x^{\\mathrm{ovr}}))^{+}\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where constant $H$ is the Hoffman constant associated with Problem (FAIR) under instance $\\pmb{\\theta}$ (see definition in Lemma G.2). ", "page_idx": 19}, {"type": "text", "text": "There are two important takeaways from Theorem C.1: (1) The price of fairness arises from the misalignment of objectives, captured by the difference in items\u2019/users\u2019 outcomes under the singlesided item/user-fair solution and the platform\u2019s optimal revenue solution, $\\pmb{x}^{\\mathrm{{oPT}}}$ (i.e., $\\left(\\delta^{\\tt I}\\cdot O_{i}^{\\tt I}(f^{\\tt I}\\right)-$ $O_{i}^{\\mathrm{I}}(x^{\\mathrm{OPT}}))^{+}$ and $(\\delta^{\\mathrm{U}}\\!\\cdot\\!O_{j}^{\\mathrm{U}}(\\pmb{f}^{\\mathrm{U}})-O_{j}^{\\mathrm{U}}(\\pmb{x}^{\\mathrm{{\\acute{o}P T}}}))^{+}$ . A high price of fairness can result from high divergence of platform\u2019s goals from item/user interests (as illustrated by Proposition 2.1 and Example B.1). (2) Theorem C.1 also underscores the value of the parameters $\\mathbf{\\dot{\\delta}}^{\\mathrm{I}}$ and $\\delta^{\\tt U}$ in achieving a balance among stakeholder interests. Both takeaways are further supported empirically, as we next investigate the price of fairness associated with our case study on Amazon review data in Section C.1. ", "page_idx": 19}, {"type": "text", "text": "C.0.1 Proof for Theorem C.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall from Section 2.4 that $\\begin{array}{r}{i_{j}^{\\star}=\\arg\\operatorname*{max}_{i\\in[N]}r_{i}y_{i,j}}\\end{array}$ is the item with the maximum expected revenue, if an arriving user is of type $j$ . (Here, we assumed that $i_{j}^{\\star}$ is unique without loss of generality.) The platform\u2019s revenue-maximizing solution is xiO,PjT $x_{i,j}^{\\mathrm{opT}}=1$ if $i=i_{j}^{\\star}$ and $x_{i,j}^{\\mathrm{opr}}=0$ otherwise. ", "page_idx": 19}, {"type": "text", "text": "Now, suppose that the platform solves Problem (FAIR) with some fairness parameters $\\delta^{\\mathtt{I}},\\delta^{\\mathtt{U}}\\in[0,1]$ and assuming Problem (FAIR) is feasible. Let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}=\\{\\pmb{x}\\in\\Delta_{N}^{M}\\,:O_{i}^{\\intercal}(\\pmb{x})\\geq\\delta^{\\mathrm{I}}\\cdot O_{i}^{\\intercal}(\\pmb{f}^{\\intercal});\\quad O_{j}^{\\intercal}(\\pmb{x})\\geq\\delta^{\\mathrm{U}}\\cdot O_{j}^{\\intercal}(\\pmb{f}^{\\intercal})\\quad\\forall i\\in[N],j\\in[M]\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "denote the feasibility region of Problem (FAIR). We define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{x}^{\\prime}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathcal{F}}\\|\\pmb{x}-\\pmb{x}^{\\mathrm{{OPT}}}\\|_{\\infty}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "That is, $\\mathbf{\\nabla}x^{\\prime}$ is a feasible solution to Problem (FAIR) that is closest to $\\pmb{x}^{\\mathrm{{oPT}}}$ w.r.t. $\\|\\cdot\\|_{\\infty}$ . Then, by Lemma G.1, we have the following bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{x}^{\\prime}-\\pmb{x}^{\\mathrm{opr}}\\|_{\\infty}\\leq\\pmb{H}\\cdot\\operatorname*{max}\\left\\{\\operatorname*{max}_{i\\in[N]}(\\delta^{\\intercal}\\cdot\\pmb{L}_{i,:}^{\\top}f_{i,:}^{\\intercal}-\\pmb{L}_{i,:}^{\\top}x_{i,:}^{\\mathrm{opr}})^{+},\\operatorname*{max}_{j\\in[M]}(\\delta^{\\intercal}\\cdot U_{:,j}^{\\top}f_{:,j}^{\\intercal}-U_{:,j}^{\\top}x_{:,j}^{\\mathrm{opr}})^{+},0\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first term $H$ is the Hoffman constant associated with the feasibility region $\\mathcal{F}$ , and the second term encapsulates how much $\\pmb{x}^{\\mathrm{{OPT}}}$ violates the item/user-fairness constraints. ", "page_idx": 19}, {"type": "text", "text": "Now, note that we also have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|x^{\\prime}-x^{\\mathrm{opr}}\\|_{\\infty}\\overset{(a)}{=}\\underset{j\\in[M]}{\\operatorname*{max}}\\,\\operatorname*{max}\\{1-x_{i_{j}^{\\star},j}^{\\prime}\\ ,\\ \\underset{i\\neq i_{j}^{\\star}}{\\operatorname*{max}}\\,x_{i,j}^{\\prime}\\}\\overset{(b)}{=}\\underset{j\\in[M]}{\\operatorname*{max}}\\ 1-x_{i_{j}^{\\star},j}^{\\prime}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (a) follows from the form of $\\pmb{x}^{\\mathrm{{oPT}}}$ , and (b) follows from $\\begin{array}{r}{1-x_{i_{j}^{\\star},j}^{\\prime}=\\sum_{i\\not=i_{j}^{\\star},j}x_{i,j}^{\\prime}\\geq x_{i,j}^{\\prime}}\\end{array}$ for any $i\\neq i^{\\star}$ . This then allows us to bound the PoF as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{o F}=\\frac{\\mathsf{O P T-R E V-F A I R-R E V}}{\\mathrm{OPT-REV}}\\leq1-\\frac{\\mathsf{R E V}(\\pmb{x}^{\\prime})}{\\mathrm{OPT-REV}}\\leq1-\\frac{\\sum_{j\\in[M]}R_{i j}\\pmb{x}_{i j,j}^{\\prime}}{\\sum_{j\\in[M]}R_{i j}}\\leq\\operatorname*{max}_{j\\in[M]}1-\\boldsymbol{x}_{i j,j}^{\\prime}=\\|\\pmb{x}^{\\prime}-\\pmb{x}^{\\mathrm{op}}\\|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality follows from $\\pmb{x}^{\\prime}$ being a feasible solution to Problem (FAIR), the second inequality follows from $\\begin{array}{r}{\\mathrm{REV}(\\pmb{x}^{\\prime})\\geq\\sum_{j\\in[M]}\\bar{R_{i_{j}^{\\star}}}x_{i_{j}^{\\star},j}^{\\prime}}\\end{array}$ , and OPT-R $\\begin{array}{r}{\\mathrm{iv}=\\sum_{j\\in[M]}R_{i_{j}^{\\star}}}\\end{array}$ , and the final equality follows from (21). ", "page_idx": 20}, {"type": "text", "text": "Finally, combining Eq. (20) and Eq. (22) finishes the proof. ", "page_idx": 20}, {"type": "text", "text": "C.1 Price of Fairness for Our Case Study on Amazon Review Data ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we investigate the price of fairness from our case study on the Amazon review data (Section 4) and shed light on how our fair recommendation framework, Problem (FAIR), can help the platform achieve the right middleground among multiple stakeholders. ", "page_idx": 20}, {"type": "text", "text": "In our case study, we act as an e-commerce site (such as Amazon) recommending a collection of at most $K=3$ products to incoming users. There are a total of $N=30$ items to be shown to $M=5$ types of users, where the relevance score between items and users as well as users\u2019 arrival rates are both obtained from an Amazon review dataset [71]; see Section 4 for a complete description of the dataset. Assuming that the platform has full knowledge of the problem instance, we solve Problem (FAIR-ASSORT) (here, we consider the assortment extension of Problem (FAIR); see Section E.2 for details) under different values of fairness parameters $\\delta^{\\tt I},\\delta^{\\tt U}$ to investigate how much loss the platform needs to endure in order to achieve various levels of fairness for its items/users. ", "page_idx": 20}, {"type": "text", "text": "The left-hand side of Figure 2 shows the price of fairness (POF) endured by the platform given different $(\\delta^{\\tt I},\\delta^{\\tt U})$ . Note that in our recommendation problem based on Amazon review data, item fairness is much more difficult to achieve than user fairness, since (i) the number of items is much larger, making the fair outcome of many items differentiate a lot from the outcome attained under platform\u2019s revenue-maximizing solution; and (ii) the objective of users (MNL utility) aligns fairly well with the platform\u2019s objective (revenue). As a result, the item-fair constraints are the binding constraints in majority of the cases. This can be seen in the left-hand side plot, where we see that POF increases more significantly when we increase $\\delta^{\\mathtt{I}}$ . This concurs with the first takeaway from Theorem C.1, which suggests that the misalignment of objectives directly impacts the POF. ", "page_idx": 20}, {"type": "text", "text": "In the right-hand side of Figure 2, we plot the evolution of PoF when we fix $\\delta^{\\tt U}=0$ (upper-right plot) and $\\delta^{\\tt I}\\bar{=}\\,0$ (lower-right plot) respectively. Given that the item fairness constraints are primarily the binding constraints, as discussed above, we observe a linear increase in PoF for the platform as the fairness parameter $\\delta^{\\tt I}$ for items increases. This observation again corroborates the findings presented in Theorem C.1. For the users, in the case of Amazon review data, the misalignment of objectives $(\\delta^{\\tt U}\\cdot O_{j}^{\\tt U}({\\pmb f}^{\\tt U})-O_{j}^{\\tt U}({\\pmb x}^{\\tt O P T}))_{.}^{+}$ only becomes noteworthy as $\\delta^{\\tt U}$ gets close to 1. This indicates that under the Amazon review data, the platform can potentially obtain a high level of user-fairness at very little cost, which also matches our findings in our case study in Section 4. ", "page_idx": 20}, {"type": "image", "img_path": "tAOg1HdvGy/tmp/9ddf0ef6da0ac547982c89e83b1475fa38b4fe73c0c43bf1f59758fe2c2a4c0d.jpg", "img_caption": ["Figure 2: Price of Fairness (POF) in our case study on the Amazon review data. Left: PoF when solving Problem (FAIR-ASSORT) under different fairness parameters $(\\delta^{\\tt I},\\delta^{\\tt U})$ . The grid is colored black if the problem is infeasible. Upper-right: PoF when $\\dot{\\delta}^{\\mathrm{U}}=0$ . Lower-right: PoF when $\\delta^{\\tt I}=0$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Insights from the case study. Our empirical analysis provides practical guidelines for selecting appropriate fairness parameters $\\delta^{\\tt I},\\delta^{\\tt U}$ in real-world settings. Specifically, platforms should focus on (i) identifying which fairness constraints are binding and (ii) evaluating the degree of objective misalignment, typically captured by the slope between the PoF and the fairness parameters. ", "page_idx": 21}, {"type": "text", "text": "One potential approach is to first identify the binding constraints by assessing which stakeholders experience the most unfair outcomes under the current recommendation policy. Then, using the piecewise linear relationship established in Theorem C.1, the platform can estimate the tradeoff between the binding constraints and PoF. Based on the desired fairness levels and acceptable PoF, the platform can now narrow down the range of fairness parameters to experiment with. Once a small subset of promising fairness parameters is identified, a platform can conduct online A/B tests by splitting its traffic to experiment with these parameters in parallel, thus selecting the optimal fairness parameters efficiently without extensive trial-and-error. ", "page_idx": 21}, {"type": "text", "text": "D Proof for Theorem 3.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Definitions and Lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first state a few definitions and lemmas that are useful for the proof of Theorem 3.1. ", "page_idx": 21}, {"type": "text", "text": "Good event and cutoff time. We define the good event at round $t$ , denoted by $\\mathcal{E}_{t}$ , as the event under which our estimates for $\\textit{\\textbf{y}}$ and $\\textbf{\\emph{p}}$ are accurate w.r.t. $t$ . ", "page_idx": 21}, {"type": "text", "text": "Definition D.1 (Good event) At round $t$ , a good event $\\mathcal{E}_{t}$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}=\\left\\{\\|\\hat{\\pmb{p}}_{t}-\\pmb{p}\\|_{1}\\leq\\Gamma_{p,t}\\right\\}\\cap\\left\\{\\|\\hat{\\pmb{y}}_{t}-\\pmb{y}\\|_{\\infty}\\leq\\Gamma_{y,t}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{y,t}=2\\log(T)/\\sqrt{m(t)\\epsilon_{t}}\\,a n d\\,m(t)=\\operatorname*{max}\\left\\{1,t/\\log(T)-\\sqrt{t\\log(T)/2}\\right\\}}\\\\ &{\\Gamma_{p,t}=5\\sqrt{\\log(3T)/t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Observe that $\\Gamma_{p,t}$ strictly decreases to 0 as $t\\to\\infty$ . This is also the case for $\\Gamma_{y,t}$ . To see that, note that $m(t)$ , used to define $\\Gamma_{y,t}$ , strictly increases in $t$ and goes to $\\infty$ as $t\\to\\infty$ . Hence, we can additionally define the cutoff time $\\tau$ , which is time after which the good event becomes \u201csufficiently good\u201d: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T=\\operatorname*{min}\\left\\{t\\in[T]:\\Gamma_{y,t}<1-\\|y\\|_{\\infty}\\,,\\,\\operatorname*{max}\\{\\Gamma_{y,t},\\Gamma_{p,t}\\}\\leq\\zeta,\\mathrm{and}\\,\\,m(t)>1\\right\\}=\\mathcal{O}(1)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\zeta$ is defined in Assumption 3.1. Here we also used $y_{i,j}\\,\\in\\,(0,1)$ for all $i\\,\\in\\,[N],j\\,\\in\\,[M]$ , which is assumed in Section 2.1, so we must have $\\|\\pmb{y}\\|_{\\infty}<1$ . Here, note that $\\Gamma_{y,t}\\,=\\,\\mathcal{O}(t^{-\\frac{1}{3}})$ , $\\Gamma_{p,t}=\\mathcal{O}(t^{-\\frac{1}{2}})$ so $\\mathcal{T}=\\mathcal{O}(\\operatorname*{max}\\{\\zeta\\;,\\;\\Vert\\pmb{y}\\Vert_{\\infty}\\}^{3})$ and is $\\mathcal{O}(1)$ w.r.t. $T$ . ", "page_idx": 21}, {"type": "text", "text": "The following lemma states that under the good event, for $t\\,>\\,0$ that is sufficiently large, the optimal solution $x^{\\star}$ to Problem (FAIR) is feasible to the relaxed problem under the estimated instance, Problem $\\big(\\mathrm{FAIR-RELAX}\\big(\\hat{\\pmb{\\theta}}_{t},\\eta_{t}\\big)\\big)$ , while our solution $\\hat{\\pmb{x}}_{t}$ is also feasible to the relaxed problem under the ground-truth instance, Problem (FAIR-RELAX $(\\theta,\\eta_{t}))$ . The proof is presented in Section D.3. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.1 (Feasibility under the good event) Given problem instance $\\pmb\\theta\\ \\in\\ \\Theta$ . Suppose that Assumption 3.1 holds, where $B$ is the Lipschitz constant. Let constant $\\bar{U}$ be the maximum utility in the small neighborhood around $\\pmb{\\theta}$ ; that is, $\\|\\pmb{U}(\\tilde{\\pmb{\\theta}})\\|_{\\infty}\\leq\\bar{U}$ for all $\\tilde{\\pmb{\\theta}}\\in\\Theta$ such that $\\lvert|\\pmb{\\theta}-\\tilde{\\pmb{\\theta}}\\rvert|\\leq\\zeta$ . Under the good event $\\mathcal{E}_{t}$ , when $t>\\tau$ (defined in (24)) and $\\log(T)>(2+\\|r\\|_{\\infty})B_{}$ , we have the following: ", "page_idx": 21}, {"type": "text", "text": "(i) $\\|\\hat{\\pmb y}_{t}\\|_{\\infty}<1$ , so $\\pmb{x}_{t,i,j}=(1-N\\epsilon_{t})\\hat{\\pmb{x}}_{t,i,j}+\\epsilon_{t}$ according to FORM. (ii) $x^{*}$ is feasible to Problem $\\big(\\mathrm{FAIR-RELAX}\\big(\\hat{\\pmb{\\theta}}_{t},\\eta_{t}\\big)\\big)$ . (iii) $\\hat{\\pmb x}_{t}$ is feasible to Problem (FAIR-RELAX $(\\pmb\\theta,2\\eta_{t}),$ ", "page_idx": 21}, {"type": "text", "text": "where $\\widehat{\\pmb{\\theta}}_{t}$ and $\\eta_{t}$ are defined in Eq. (1) and Eq. (2). ", "page_idx": 21}, {"type": "text", "text": "The second Lemma (Lemma D.2) shows that the good event happens with high probability. Its proof is deferred to Section D.4. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.2 (Concentration bounds for estimates) Assume $\\log(T)>\\,1/\\operatorname*{min}_{j\\in[M]}p_{j}$ . Then for any $t>\\tau$ defined in Eq. (24), we have $\\mathbb{P}(\\mathcal{E}_{t+1}^{c}\\,)\\le(M N+2)/T$ . ", "page_idx": 21}, {"type": "text", "text": "D.2 Complete Statement and Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem D.3 (Complete statement of Theorem 3.1) Given problem instance $\\pmb\\theta\\in\\Theta$ and assume that Assumption 3.1 holds. Then, for $\\begin{array}{r}{\\log(T)>\\operatorname*{max}\\left\\{(2+\\|\\boldsymbol{r}\\|_{\\infty})B,\\ \\frac{1}{\\operatorname*{min}_{j\\in[M]}p_{j}}\\right\\}}\\end{array}$ , we have ", "page_idx": 22}, {"type": "text", "text": "\u2022 the revenue regret of FORM is at most $\\mathbb{E}[\\mathcal{R}(T)]\\le\\mathcal{O}(M N^{\\frac{1}{3}}T^{-\\frac{1}{3}})$ \u2022 the fairness regret of FORM is at most $\\mathbb{E}[\\mathcal{R}_{F}(T)]\\le\\mathcal{O}(M N^{\\frac{1}{3}}T^{-\\frac{1}{3}})$ . ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem D.3. We bound the revenue regret and the fairness regret respectively. ", "page_idx": 22}, {"type": "text", "text": "(1) Bounding revenue regret. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For a given $t\\,>\\,\\tau$ , assume that the good event $\\mathcal{E}_{t}$ defined in Definition D.1 holds. Let $\\scriptstyle R\\;=$ $\\left(R_{i,j}\\right)_{j\\in[N]}$ , where $R_{i,j}\\,=\\,r_{i}p_{j}y_{i,j}$ denotes the expected revenue that the platform receives from offering item $i$ to user of type- $j$ . That is, $\\begin{array}{r}{\\mathrm{REV}(\\pmb{x},\\pmb{\\theta})=\\sum_{j\\in[N]}\\:R_{i,j}x_{i,j}}\\end{array}$ . Similarly, for simplicity of notation, let $\\hat{R}_{t}=(\\hat{R}_{t,i,j})\\mathbf{\\Phi}_{i\\in[N]}$ , where $\\hat{R}_{t,i,j}=r_{i}\\hat{p}_{t,j}\\hat{y}_{t,i,j}$ . ", "page_idx": 22}, {"type": "text", "text": "Let us first rewrite the revenue regret at round $t$ as the following: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(x^{\\star},\\theta)-\\underset{\\stackrel{i\\in[N]}{i\\in[N]}}{\\mathrm{REV}}(x_{t},\\theta)=\\displaystyle\\sum_{i\\in[N]}R_{i,j}x_{i,j}^{\\star}-\\sum_{i\\in[N]}R_{i,j}x_{t,i,j}}\\\\ {=\\displaystyle\\sum_{i\\in[N]}\\hat{R}_{t,i,j}(x_{i,j}^{\\star}-x_{t,i,j})+\\displaystyle\\sum_{i\\in[N]}x_{i,j}^{\\star}(R_{i,j}-\\hat{R}_{t,i,j})-\\sum_{i\\in[N]}x_{t,i,j}(R_{i,j}-\\hat{R}_{t,i,j})}\\\\ {=\\displaystyle\\sum_{i\\in[N]}\\hat{R}_{t,i,j}(x_{i,j}^{\\star}-x_{t,i,j})+\\displaystyle\\sum_{i\\in[N]}(x_{i,j}^{\\star}-x_{t,i,j})(R_{i,j}-\\hat{R}_{t,i,j})}\\\\ {\\overset{j\\in[N]}{\\underset{j\\in[N]}{j\\in[N]}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We then have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{REV}(\\mathbf{x}^{\\star},\\theta)-\\mathsf{R E V}(\\mathbf{x}_{t},\\theta)}\\\\ &{\\stackrel{(a)}{=}(1-N\\epsilon_{t})\\displaystyle\\sum_{i\\in[N]}\\hat{R}_{t,i,j}(x_{i,j}^{\\star}-\\hat{x}_{t,i,j})+N\\epsilon_{t}\\displaystyle\\sum_{i\\in[N]}\\hat{R}_{t,i,j}x_{i,j}^{\\star}-\\epsilon_{t}\\displaystyle\\sum_{i\\in[N]}\\hat{R}_{t,i,j}+\\sum_{i\\in[N]}\\displaystyle(\\hat{x}_{t,i,j}^{\\star}-x_{t,i,j})}\\\\ &{\\stackrel{i\\in[N]}{\\leq}N\\epsilon_{t}\\displaystyle\\sum_{i\\in[N]}\\hat{R}_{t,i,j}x_{i,j}^{\\star}-\\epsilon_{t}\\displaystyle\\sum_{i\\in[N]}\\hat{R}_{t,i,j}+\\sum_{i\\in[N]}\\displaystyle(x_{i,j}^{\\star}-x_{t,i,j})(R_{i,j}-\\hat{R}_{t,i,j})}\\\\ &{\\stackrel{(b)}{\\leq}N\\epsilon_{t}\\displaystyle\\sum_{i\\in[M]}\\hat{R}_{t,i,j}x_{i,j}^{\\star}-\\epsilon_{t}\\displaystyle\\sum_{j\\in[M]}\\hat{R}_{t,i,j}}\\\\ &{\\leq N M\\epsilon_{t}\\|\\hat{R}_{t}\\|_{\\infty}+2M\\cdot\\|{R}-\\hat{R}_{t}\\|_{\\infty}}\\\\ &{\\leq N M\\epsilon_{t}\\|{R}-\\hat{R}_{t}\\|_{\\infty}+N M\\epsilon_{t}\\|{R}\\|_{\\infty}+2M\\cdot\\|{R}-\\hat{R}_{t}\\|_{\\infty}}\\\\ &{\\stackrel{(c)}{\\leq}3M\\|{R}-\\hat{R}_{t}\\|_{\\infty}+N M\\epsilon_{t}\\|{R}\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (a) follows from statement (i) in Lemma D.1, i.e., $\\mathbf{\\boldsymbol{x}}_{t,i,j}=(1-N\\epsilon_{t})\\hat{\\mathbf{\\boldsymbol{x}}}_{t,i,j}+\\epsilon_{t}$ ; (b) follows from the fact that $\\begin{array}{r}{\\epsilon_{t}\\leq\\frac{1}{N}}\\end{array}$ so $1-N\\epsilon_{t}\\geq0$ , and by statement (ii) Lemma D.1, we have that $x^{\\star}$ and $\\hat{\\pmb{x}}_{t}$ are both feasible to Problem (FAIR-RELAX $(\\hat{\\pmb{\\theta}}_{t},\\eta_{t}))$ , and hence $\\operatorname{REV}(\\pmb{x}^{\\star},\\hat{\\pmb{\\theta}}_{t})\\leq\\operatorname{REV}(\\hat{\\pmb{x}}_{t},\\hat{\\pmb{\\theta}}_{t})$ ; (c) follows from the fact that \u03f5t \u2264 N1 . ", "page_idx": 22}, {"type": "text", "text": "Note that since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hat{R}_{t,i,j}-R_{i,j}\\ =r_{i}\\hat{y}_{t,i,j}\\hat{p}_{t,j}-r_{i}y_{i,j}p_{j}}&{}\\\\ {\\ =r_{i}\\left(\\hat{y}_{t,i,j}\\hat{p}_{t,j}-y_{i,j}\\hat{p}_{t,j}+y_{i,j}\\hat{p}_{t,j}-y_{i,j}p_{j}\\right)}&{}\\\\ {\\ =r_{i}\\Big((\\hat{y}_{t,i,j}-y_{i,j})\\hat{p}_{j}+y_{i,j}(\\hat{p}_{t,j}-p_{j})\\Big)}&{}\\\\ {\\ \\le r_{i}\\Big(\\lVert\\hat{\\pmb{y}}_{t}-\\pmb{y}\\rVert_{\\infty}+\\lVert\\pmb{y}\\rVert_{\\infty}\\cdot\\lVert\\hat{\\pmb{p}}_{t}-\\pmb{p}\\rVert_{1}\\Big)}&{}\\\\ {\\ \\le r_{i}\\Big(\\Gamma_{y,t}+\\Gamma_{p,t}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We thus have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{R E V}({\\pmb x}^{\\star},{\\pmb\\theta})-\\mathtt{R E V}({\\pmb x}_{t},{\\pmb\\theta})\\leq3M\\bar{r}(\\Gamma_{y,t}+\\Gamma_{p,t})+N M\\epsilon_{t}\\|{\\pmb R}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, for any $t>\\tau$ , let $\\mathcal{F}_{t-1}$ be the sigma-algbra generated from all randomness up to round $t-1$ , and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\mathbb{R}\\mathrm{E}\\mathbb{V}\\big(\\mathbf{x}^{\\star},\\theta\\big)-\\mathbb{R}\\mathrm{E}\\mathbb{V}\\big(\\mathbf{x}_{t},\\theta\\big)\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbb{R}\\mathrm{E}\\mathbb{V}\\big(\\mathbf{x}^{\\star},\\theta\\big)-\\mathbb{R}\\mathrm{E}\\mathbb{V}\\big(\\mathbf{x}_{t},\\theta\\big)\\,\\Big|\\,\\mathcal{F}_{t-1}\\right]\\right]}\\\\ &{\\le\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbb{R}\\mathrm{E}\\mathbb{V}\\big(\\mathbf{x}^{\\star},\\theta\\big)-\\mathbb{R}\\mathrm{E}\\mathbb{V}\\big(\\mathbf{x}_{t},\\theta\\big)\\mathbb{1}\\big\\{\\mathcal{E}_{t}\\big\\}\\,\\Big|\\,\\mathcal{F}_{t-1}\\right]\\right]+\\mathbb{E}\\left[\\mathbb{R}\\mathrm{E}\\mathbb{V}\\big(\\mathbf{x}^{\\star},\\theta\\big)\\mathbb{I}\\big\\{\\mathcal{E}_{t}^{c}\\big\\}\\right]}\\\\ &{\\stackrel{(a)}\\le3M\\bar{r}\\bigg(\\Gamma_{y,t}+\\Gamma_{p,t}\\bigg)+N M\\epsilon_{t}\\|\\mathbf{R}\\|_{\\infty}+\\|\\mathbf{R}\\|_{\\infty}\\mathbb{P}(\\mathcal{E}_{t}^{c})}\\\\ &{\\stackrel{(b)}\\le3M\\bar{r}\\bigg(\\Gamma_{y,t}+\\Gamma_{p,t}\\bigg)+N M\\epsilon_{t}\\|\\mathbf{R}\\|_{\\infty}+\\frac{\\left(M N+2\\right)\\|\\mathbf{R}\\|_{\\infty}}{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) follows from (27); (b) follows from the high probability bound for event $\\mathcal{E}_{t}$ in Lemma D.2, given that $t>\\tau$ holds. ", "page_idx": 23}, {"type": "text", "text": "Finally, by (28), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t\\in[T]}\\mathbb{E}\\Big[\\mathtt{R E V}\\big(\\mathbf{x}^{\\star},\\theta\\big)-\\mathtt{R E V}\\big(\\mathbf{x}_{t},\\theta\\big)\\Big]\\leq\\mathcal{O}\\Big(\\frac{T}{T}+\\frac{M}{T}\\sum_{t>T}(\\Gamma_{p,t}+\\Gamma_{y,t})+\\frac{N M}{T}\\sum_{t>T}\\epsilon_{t}\\Big)=\\mathcal{O}\\big(M N^{\\frac{1}{3}}\\big)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the final equality we recall $\\epsilon_{t}=\\operatorname*{min}\\left\\{N^{-1},N^{-\\frac{2}{3}}t^{-\\frac{1}{3}}\\right\\}$ and $\\Gamma_{y,t}=2\\log(T)/\\sqrt{m(t)\\epsilon_{t}}<$ $\\begin{array}{r}{\\frac{2\\log(T)}{\\sqrt{(t/\\log(T)-\\sqrt{t\\log(T)/2})\\epsilon_{t}}}=\\mathcal{O}(N^{\\frac13}t^{-\\frac13})}\\end{array}$ since $m(t)>1$ for all $t>\\tau$ defined in Eq. (24). ", "page_idx": 23}, {"type": "text", "text": "(2) Bounding constraint violation. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Item-Fair Constraints. For any item $i\\in[N]$ , and any $t>\\tau$ , under the good event $\\mathcal{E}_{t}$ defined in Definition D.1, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\colon\\boldsymbol{L}_{i,:}(\\theta)^{\\top}f_{i,:}^{\\top}(\\theta)-\\boldsymbol{L}_{i,:}(\\theta)^{\\top}\\boldsymbol{x}_{t,i,:}\\stackrel{(a)}{=}\\delta^{\\top}\\boldsymbol{L}_{i,:}(\\theta)^{\\top}f_{i,:}^{\\top}(\\theta)-\\boldsymbol{L}_{i,:}(\\theta)^{\\top}\\Big((1-N\\epsilon_{t})\\hat{\\boldsymbol{x}}_{t,i,:}+\\epsilon_{t}\\boldsymbol{e}_{M}\\Big)}\\\\ {\\le\\delta^{\\top}\\cdot\\boldsymbol{L}_{i,:}(\\theta)^{\\top}f_{i,:}^{\\top}(\\theta)-\\boldsymbol{L}_{i,:}(\\theta)^{\\top}\\hat{\\boldsymbol{x}}_{t,i,:}+N M\\|\\boldsymbol{L}_{i,:}(\\theta)\\|_{\\infty}\\epsilon_{t}}\\\\ {\\overset{(b)}{\\le}2M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}+N M\\|\\boldsymbol{L}_{i,:}(\\theta)\\|_{\\infty}\\epsilon_{t}}&{\\hphantom{(b).}\\sim}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) follows from Lemma D.1 part (i), which states that under the good event $\\mathcal{E}_{t}$ for $t>\\tau$ , we have $\\pmb{x}_{t,i,:}=(1\\!-\\!N\\epsilon_{t})\\hat{\\pmb{x}}_{t,i,:}\\!+\\!\\epsilon_{t}\\pmb{e}_{M}$ according to Algorithm 1; (b) follows from Lemma D.1 part (iii), which states $\\hat{\\pmb{x}}_{t}$ is feasible to Problem (FAIR-RELAX $\\!\\,[\\theta,\\eta_{t}))$ , where $\\eta_{t}=\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t}^{\\mathring{}},\\Gamma_{y,t}\\}$ according to Eq. (1). ", "page_idx": 23}, {"type": "text", "text": "Hence, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\displaystyle\\frac{T}{\\Gamma\\displaystyle\\operatorname*{lims}\\{1,\\hat{r}\\}}\\Big[(\\vartheta^{\\dagger}\\cdot L_{i\\cdot}(\\theta)^{\\top}f_{t_{**}(\\theta)}^{\\dagger}-L_{i\\cdot}(\\theta)^{\\top}\\pi_{t_{**}(\\cdot)})^{+}\\Big]}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\frac{T}{T}\\operatorname*{max}\\{1,\\hat{r}\\}+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{\\ell>T}\\mathbb{E}\\Big[(\\delta^{\\dagger}\\cdot L_{i\\cdot}(\\theta)^{\\top}f_{t_{*}(\\theta)}^{\\dagger}-L_{i\\cdot}(\\theta)^{\\top}x_{t_{**}(\\cdot)})^{+}\\Big]}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\frac{T}{T}\\operatorname*{max}\\{1,\\hat{r}\\}+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{\\ell>T}\\mathbb{E}\\Big[\\Big(\\delta^{\\dagger}\\cdot L_{i\\cdot}(\\theta)^{\\top}f_{t_{*}(\\theta)}^{\\dagger}-L_{i\\cdot}(\\theta)^{\\top}x_{t_{**}(\\cdot)}\\Big)^{+}\\mathbb{I}\\{\\xi\\}\\Big]+\\mathbb{P}(\\xi_{t}^{\\vee})\\operatorname*{max}\\{1,\\hat{r}\\}}\\\\ &{\\overset{(c)}{\\leq}\\displaystyle\\frac{T}{T}\\operatorname*{max}\\{1,\\hat{r}\\}+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{\\ell>T}\\mathbb{I}\\mathcal{M}\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}+{\\cal N}M\\|{\\cal G}(\\theta)\\|_{\\infty^{\\epsilon}\\epsilon}\\Big]+\\mathbb{P}(\\xi_{t}^{\\vee})\\operatorname*{max}\\{1,\\hat{r}\\}}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\frac{T}{T}\\operatorname*{max}\\{1,\\hat{r}\\}+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{\\ell>T}\\mathbb{I}\\mathcal{M}\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}+{\\cal N}M\\|{\\cal G}(\\theta)\\|_{\\infty^{\\epsilon}\\epsilon}|+\\displaystyle\\frac{({\\cal N}M+2)\\operatorname*{max}\\{1,\\hat{r}\\}}{T}}\\\\ &{={\\cal O}({\\cal M}^{\\mathrm{N}}{\\bf1}^{*}T^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) and (b) follows from $\\delta^{\\tt I}\\leq1$ and $L_{i,:}(\\pmb\\theta)^{\\top}f_{i,:}^{\\tt I}(\\pmb\\theta)\\leq\\operatorname*{max}\\{1,\\bar{r}\\}$ depending on the item\u2019s outcome; (c) follows from Eq. (30); (d) follows from Lemma D.2 for sufficiently large $T$ . Therefore, the time-averaged item-fairness constraint violation for any item $i$ is at most $\\mathcal{O}(M\\bar{N}^{\\frac{1}{3}}T^{-\\frac{1}{3}})$ . ", "page_idx": 24}, {"type": "text", "text": "We can perform the same analysis for user-fairness constraints and obtain the same upper bound. The proof is thus omitted. ", "page_idx": 24}, {"type": "text", "text": "D.3 Proof of Lemma D.1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We prove the three statements in Lemma D.1 respectively as follows. ", "page_idx": 24}, {"type": "text", "text": "Proof for (i). Consider the following under the good event $\\mathcal{E}_{t}$ , defined in Eq. (D.1): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\pmb{y}}_{t}\\|_{\\infty}-\\|\\pmb{y}\\|_{\\infty}\\leq\\|\\hat{\\pmb{y}}_{t}-\\pmb{y}\\|_{\\infty}\\,\\leq\\,\\Gamma_{y,t}\\,\\overset{(a)}{<}1-\\|\\pmb{y}\\|_{\\infty}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (a) follows from $t>\\tau$ and the definition of $\\tau$ in (24). This gives $\\|\\hat{\\pmb{y}}_{t}\\|_{\\infty}<1$ , and hence given the design of FORM, $\\pmb{x}_{t,i,j}=(1-N\\epsilon_{t})\\hat{\\pmb{x}}_{t,i,j}+\\epsilon_{t}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof for (ii). Here, we would like to show that $x^{*}$ is feasible for Problem (FAIR-RELAX $(\\widehat{\\pmb{\\theta}}_{t},\\eta_{t}))$ , where $\\widehat{\\pmb{\\theta}}_{t}$ and $\\eta_{t}$ are defined in Eq. (1) and Eq. (2). ", "page_idx": 24}, {"type": "text", "text": "Item-Fair Constraints. Let $\\bar{L}=\\|\\pmb{L}\\|_{\\infty}$ . For any $i\\in[N]$ and $\\pmb{x}\\in\\Delta_{N}^{M}$ , under Assumption 3.1, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{i,\\cdot}(\\hat{\\theta}_{t})^{\\top}x_{i,\\cdot}-L_{i,\\cdot}(\\theta)^{\\top}x_{i,\\cdot}\\Big|\\leq\\|L_{i,\\cdot}(\\hat{\\theta}_{t})-L_{i,\\cdot}(\\theta)\\|_{\\infty}\\|x_{i,\\cdot}\\|_{1}\\leq M B\\cdot\\|\\hat{\\theta}_{t}-\\theta\\|_{\\infty}=M B\\cdot\\operatorname*{max}\\{|x_{i,\\cdot}|_{\\infty}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, for any $i\\in[N]$ , we also have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|L_{i,\\cdot}(\\hat{\\theta}_{t})^{\\top}f_{i,\\cdot}^{\\top}(\\hat{\\theta}_{t})-L_{i,\\cdot}(\\theta)^{\\top}f_{i,\\cdot}^{\\top}(\\theta)\\right|}\\\\ &{=\\left|L_{i,\\cdot}(\\hat{\\theta}_{t})^{\\top}f_{i,\\cdot}^{\\top}(\\hat{\\theta}_{t})-L_{i,\\cdot}(\\hat{\\theta}_{t})^{\\top}f_{i,\\cdot}^{\\top}(\\theta)+L_{i,\\cdot}(\\hat{\\theta}_{t})^{\\top}f_{i,\\cdot}^{\\top}(\\theta)-L_{i,\\cdot}(\\theta)^{\\top}f_{i,\\cdot}^{\\top}(\\theta)\\right|}\\\\ &{\\le\\left|L_{i,\\cdot}(\\hat{\\theta}_{t})^{\\top}\\left(f_{i,\\cdot}^{\\top}(\\hat{\\theta}_{t})-f_{i,\\cdot}^{\\top}(\\theta)\\right)\\right|+\\Big|\\left(L_{i,\\cdot}(\\hat{\\theta}_{t})-L_{i,\\cdot}(\\theta)\\right)^{\\top}f_{i,\\cdot}^{\\top}(\\theta)\\Big|}\\\\ &{\\le M\\bar{L}\\cdot\\|f_{i,\\cdot}^{\\top}(\\hat{\\theta}_{t})-f_{i,\\cdot}^{\\top}(\\theta)\\|_{\\infty}+M\\cdot\\|L_{i,\\cdot}(\\hat{\\theta}_{t})-L_{i,\\cdot}(\\theta)\\|_{\\infty}}\\\\ &{\\le M\\bar{L}B\\cdot\\|\\hat{\\theta}_{t}-\\theta\\|_{\\infty}+M B\\cdot\\|\\hat{\\theta}_{t}-\\theta\\|_{\\infty}}\\\\ &{\\le M B(\\bar{L}+1)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality from the fact that for all $t>\\tau$ (see definition in (24)), under the good event $\\mathcal{E}_{t}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{t}-\\theta\\|_{\\infty}=\\operatorname*{max}\\{\\|\\hat{y}_{t}-y\\|_{\\infty},\\|\\hat{p}_{t}-p\\|_{\\infty}\\}\\leq\\operatorname*{max}\\{\\|\\hat{y}_{t}-y\\|_{\\infty},\\|\\hat{p}_{t}-p\\|_{1}\\}\\leq\\operatorname*{max}\\{\\Gamma_{y,t},\\Gamma_{p,t}\\}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $x^{\\star}\\,\\in\\,\\Delta_{N}^{M}$ is the optimal solution to Problem (FAIR), we know that $L_{i,:}(\\pmb\\theta)^{\\top}\\pmb x_{i,:}^{\\star}\\,\\geq\\,\\delta^{\\mathrm{I}}$ $L_{i,:}(\\pmb\\theta)^{\\top}f_{i,:}^{\\mathrm{I}}(\\pmb\\theta)$ . We thus have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{r}_{i,:}(\\widehat{\\theta}_{t})^{\\top}\\pmb{x}_{i,:}^{\\star}+M B\\cdot\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\geq L_{i,:}(\\theta)^{\\top}\\pmb{x}_{i,:}^{\\star}\\geq\\delta^{\\top}\\cdot L_{i,:}(\\theta)^{\\top}\\pmb{f}_{i,:}^{\\mathsf{I}}(\\theta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\delta^{\\top}\\cdot L_{i,:}(\\widehat{\\theta}_{t})^{\\top}\\pmb{f}_{i,:}^{\\mathsf{I}}(\\widehat{\\theta}_{t})-\\delta^{\\top}M B(\\bar{L}+1)\\cdot\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality holds because of Eq. (33) and the third inequality holds because of Eq. (34). Hence we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad L_{i,:}(\\widehat{\\theta}_{t})^{\\top}x_{i,:}^{\\star}\\geq\\delta^{\\top}\\cdot L_{i,:}(\\widehat{\\theta}_{t})^{\\top}f_{i,:}^{\\mathrm{I}}(\\widehat{\\theta}_{t})-\\big(M B+\\delta^{\\tt T}M B(\\bar{L}+1)\\big)\\cdot\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\\\ &{\\overset{(a)}{\\Longrightarrow}L_{i,:}(\\widehat{\\theta}_{t})^{\\top}x_{i,:}^{\\star}\\geq\\delta^{\\tt I}\\cdot L_{i,:}(\\widehat{\\theta}_{t})^{\\top}f_{i,:}^{\\mathrm{I}}(\\widehat{\\theta}_{t})-M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (a) follows from $\\log(T)>(2+{\\bar{r}})B>B+\\delta^{\\mathtt{I}}B({\\bar{L}}+1)$ . Since the amount of relaxation is $\\eta_{t}\\,=\\,M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}$ , as defined in Eq. 1, we show that $x^{\\star}$ satisfies the item-fair constraints for Problem (FAIR-RELAX $\\{(\\hat{\\pmb\\theta}_{t},\\eta_{t})\\}$ ). ", "page_idx": 24}, {"type": "text", "text": "User-Fair Constraints. For any $j\\in[M]$ and $\\pmb{x}\\in\\Delta_{N}^{M}$ , under Assumption 3.1, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|U_{:,j}(\\widehat{\\theta}_{t})^{\\top}x_{:,j}-U_{:,j}(\\theta)^{\\top}x_{:,j}\\right|\\leq\\|U_{:,j}(\\widehat{\\theta}_{t})-U_{:,j}(\\theta)\\|_{\\infty}\\|x_{:,j}\\|_{1}\\leq B\\|\\widehat{\\theta}_{t}-\\theta\\|_{\\infty}=B\\operatorname*{max}\\{\\Gamma_{p,t},\\|\\theta\\}_{1leq t}\\leq C\\frac{1}{B},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Also note that for any problem instance $\\pmb\\theta^{\\prime}\\in\\Theta$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\nU_{:,j}(\\pmb\\theta^{\\prime})^{\\top}\\pmb f_{:,j}^{\\mathrm{U}}(\\pmb\\theta^{\\prime})=\\operatorname*{max}_{i\\in[N]}U_{i,j}(\\pmb\\theta^{\\prime})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, we have the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{U_{:,j}(\\hat{\\theta}_{t})^{\\top}f_{:,j}^{\\mathrm{U}}(\\hat{\\theta}_{t})-U_{:,j}(\\theta)^{\\top}f_{:,j}^{\\mathrm{U}}(\\theta)\\Big|=\\Big|\\displaystyle\\operatorname*{max}_{i\\in[N]}U_{i,j}(\\hat{\\theta}_{t})-\\displaystyle\\operatorname*{max}_{i\\in[N]}U_{i,j}(\\theta)\\Big|}&{}\\\\ {\\le\\lVert U_{:,j}(\\hat{\\theta}_{t})-U_{:,j}(\\theta)\\rVert_{\\infty}\\le B\\lVert\\hat{\\theta}_{t}-\\theta\\rVert_{\\infty}=B\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y}\\}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second inequality follows from local Lipschitzness of $U(\\pmb\\theta)$ . ", "page_idx": 25}, {"type": "text", "text": "Now, since $x^{\\star}\\,\\in\\,\\Delta_{N}^{M}$ is the optimal solution to Problem (FAIR), we know that $U_{:,j}(\\pmb{\\theta})^{\\top}\\pmb{x}_{:,j}^{\\star}\\ge$ $\\delta^{\\boldsymbol{\\mathrm{U}}}\\cdot{\\boldsymbol{U}}_{:,j}(\\pmb{\\theta})^{\\top}\\pmb{f}_{:,j}^{\\boldsymbol{\\mathrm{U}}}(\\pmb{\\theta})$ . Combining this with the Eq. (37) and Eq. (38), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{U_{:,j}(\\widehat{\\theta}_{t})^{\\top}x_{:,j}^{\\star}+B\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\;\\geq\\;U_{:,j}(\\theta)^{\\top}x_{:,j}^{\\star}\\;\\geq\\;\\delta^{\\boldsymbol{\\mathrm{U}}}\\cdot U_{:,j}(\\theta)^{\\top}f_{:,j}^{\\boldsymbol{\\mathrm{U}}}(\\theta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\delta^{\\boldsymbol{\\mathrm{U}}}\\cdot U_{:,j}(\\widehat{\\theta}_{t})^{\\top}f_{:,j}^{\\boldsymbol{\\mathrm{U}}}(\\widehat{\\theta}_{t})-\\delta^{\\boldsymbol{\\mathrm{U}}}B\\cdot\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{U_{:,j}(\\widehat{\\theta}_{t})^{\\top}{x}_{:,j}^{\\star}\\geq U_{:,j}(\\widehat{\\theta}_{t})^{\\top}f_{:,j}^{\\mathrm{U}}(\\widehat{\\theta}_{t})-\\left(1+\\delta^{\\mathrm{U}}\\right)B\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\\\ &{}&{\\implies U_{:,j}(\\widehat{\\theta}_{t})^{\\top}{x}_{:,j}^{\\star}\\geq U_{:,j}(\\widehat{\\theta}_{t})^{\\top}f_{:,j}^{\\mathrm{U}}(\\widehat{\\theta}_{t})-M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of (iii). We can show $\\hat{\\pmb x}_{t}$ is feasible to Problem (FAIR-RELAX $(\\theta,\\eta_{t}))$ via a similar argument as our proof for (ii) ", "page_idx": 25}, {"type": "text", "text": "Item-Fair Constraints. First note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad L_{i,:}(\\theta)^{\\top}\\hat{x}_{t,i,:}+M B\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\\\ &{\\stackrel{(a)}{\\geq}L_{i,:}(\\hat{\\theta}_{t})^{\\top}\\hat{x}_{t,i,:}}\\\\ &{\\stackrel{(b)}{\\geq}\\delta^{\\mathrm{T}}\\cdot L_{i,:}(\\hat{\\theta}_{t})^{\\top}f_{i,:}^{\\mathrm{I}}(\\hat{\\theta}_{t})-M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\\\ &{\\stackrel{(c)}{\\geq}\\delta^{\\mathrm{I}}\\cdot L_{i,:}(\\theta)^{\\top}f_{i,:}^{\\mathrm{I}}(\\theta)-M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}-\\delta^{\\mathrm{I}}M B(\\bar{L}+1)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here, (a) follows from an identical argument as shown in (33), while replacing $\\textbf{\\em x}$ with $\\hat{\\pmb x}_{t}$ ; (b) follows from feasibility of $\\hat{\\pmb{x}}_{t}$ to Problem (FAIR-RELAX $(\\theta,\\eta_{t}))$ ; (c) follows from (34). Hence, since $\\log(T)>(2+{\\bar{r}})B>\\dot{B}+\\delta^{\\mathtt{I}}B(\\bar{L}+1)$ , we conclude ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{i,:}(\\pmb\\theta)^{\\top}\\hat{\\pmb x}_{t,i,:}\\geq\\delta^{\\mathrm{I}}\\cdot{\\pmb L}_{i,:}(\\pmb\\theta)^{\\top}f_{i,:}^{\\mathrm{I}}(\\pmb\\theta)-2M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "User-Fair Constraints. We again follow the same arguments as in our proof for (ii) and establish the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad U_{:,j}(\\theta)^{\\top}\\hat{x}_{t,:,j}+B\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\\\ &{\\geq U_{:,j}(\\hat{\\theta}_{t})^{\\top}\\hat{x}_{t,:,j}}\\\\ &{\\geq\\delta^{\\mathrm{U}}\\cdot U_{:,j}(\\hat{\\theta}_{t})^{\\top}f_{:,j}^{\\mathrm{U}}(\\hat{\\theta}_{t})-M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}}\\\\ &{\\geq\\delta^{\\mathrm{U}}\\cdot U_{:,j}(\\theta)^{\\top}f_{:,j}^{\\mathrm{U}}(\\theta)-M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}-\\delta^{\\mathrm{U}}B\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which then gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{:,j}(\\pmb\\theta)^{\\top}\\hat{\\boldsymbol x}_{t,:,j}\\geq\\delta^{\\mathrm{U}}\\cdot U_{:,j}(\\pmb\\theta)^{\\top}f_{:,j}^{\\mathrm{U}}(\\pmb\\theta)-2M\\log(T)\\operatorname*{max}\\{\\Gamma_{p,t},\\Gamma_{y,t}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.4 Proof of Lemma D.2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For completeness, let us recall the definitions of $\\Gamma_{y,t}$ and $m(t)$ in Eq. (23): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Gamma_{y,t}=\\frac{2\\log(T)}{\\sqrt{m(t)\\epsilon_{t}}}\\;,\\;m(t)=\\operatorname*{max}\\left\\{1,\\frac{t}{\\log(T)}-\\sqrt{t\\log(T)/2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We define related variables $\\widetilde{\\Gamma}_{y,j,t}$ and $\\widetilde{m}_{j}(t)$ as followed: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde{\\Gamma}_{y,j,t}=\\frac{2\\log(T)}{\\sqrt{\\widetilde{m}_{j}(t)\\epsilon_{t}}}\\;,\\;\\widetilde{m}_{j}(t)=\\operatorname*{max}\\{1,t p_{j}-\\sqrt{t\\log(T)/2}\\}\\;.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since for $\\log(T)\\;>\\;{\\frac{1}{\\operatorname*{min}_{j\\,\\in\\,[M]}p_{j}}}$ and $t>\\tau$ (see definition of the cutoff time $\\tau$ in (24)) such that $m(t)>1$ , we know $\\widetilde{m}_{j}(t)\\geq m(t)>1$ and thus $\\Gamma_{y,t}\\ge\\widetilde{\\Gamma}_{y,j,t}$ for any $j\\in[M]$ . We thus have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\hat{y}_{i,j,t+1}-y_{i,j}\\right|\\geq\\Gamma_{y,t}\\right)\\leq\\mathbb{P}\\left(\\left|\\hat{y}_{i,j,t+1}-y_{i,j}\\right|\\geq\\widetilde{\\Gamma}_{y,j,t}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\boldsymbol{\\mathcal{E}}_{t}^{c})\\,\\le\\mathbb{P}\\!\\left(\\|\\boldsymbol{\\hat{y}}_{t}-\\boldsymbol{y}\\|_{\\infty}\\ge\\Gamma_{y,t}\\right)+\\mathbb{P}\\!\\left(\\|\\boldsymbol{\\hat{p}}_{t}-\\boldsymbol{p}\\|_{1}\\ge\\Gamma_{p,t}\\right)}\\\\ &{\\le\\displaystyle\\sum_{i\\in[N]}\\sum_{j\\in[M]}\\mathbb{P}\\!\\left(|\\boldsymbol{\\hat{y}}_{i,j,t+1}-\\boldsymbol{y}_{i,j}|\\ge\\Gamma_{y,t}\\right)+\\mathbb{P}\\!\\big(\\|\\boldsymbol{\\hat{p}}_{t}-\\boldsymbol{p}\\|_{1}\\ge\\Gamma_{p,t}\\big)}\\\\ &{\\overset{(a)}{\\le}\\displaystyle\\sum_{i\\in[N]}\\sum_{j\\in[M]}\\mathbb{P}\\!\\left(|\\boldsymbol{\\hat{y}}_{i,j,t+1}-\\boldsymbol{y}_{i,j}|\\ge\\tilde{\\Gamma}_{y,j,t}\\right)+\\mathbb{P}\\!\\big(\\|\\boldsymbol{\\hat{p}}_{t}-\\boldsymbol{p}\\|_{1}\\ge\\Gamma_{p,t}\\big)}\\\\ &{\\overset{(b)}{\\le}\\displaystyle\\sum_{i\\in[N]}\\sum_{j\\in[M]}\\mathbb{P}\\!\\left(|\\boldsymbol{\\hat{y}}_{i,j,t+1}-\\boldsymbol{y}_{i,j}|\\ge\\tilde{\\Gamma}_{y,j,t}\\right)+\\frac{1}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (a) follows from (46); (b) follows directly from the concentration inequality to bound empirical distribution estimates; in particular, we use Lemma G.3 and take $\\begin{array}{r}{\\delta=\\frac{1}{T}}\\end{array}$ . Hence, in the following, it suffices to bound $\\mathbb{P}\\left(|\\hat{y}_{i,j,t+1}-y_{i,j}|\\geq\\widetilde{\\Gamma}_{y,j,t}\\right)$ for any $i\\in[N],j\\in[M]$ . ", "page_idx": 26}, {"type": "text", "text": "Recall that $n_{j,t}$ is the total number of type $j$ arrivals up to time $t$ . Then, consider the following ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(|\\hat{y}_{i,j,t+1}-y_{i,j}|\\geq\\widetilde{\\Gamma}_{y,j,t}\\right)\\!}&{=\\mathbb{P}\\left(|\\hat{y}_{i,j,t+1}-y_{i,j}|\\geq\\widetilde{\\Gamma}_{y,j,t},n_{j,t}\\geq\\widetilde{m}_{j}(t)\\right)}\\\\ &{+\\mathbb{P}\\left(|\\hat{y}_{i,j,t+1}-y_{i,j}|\\geq\\widetilde{\\Gamma}_{y,j,t},n_{j,t}<\\widetilde{m}_{j}(t)\\right)}\\\\ &{\\leq\\underbrace{\\mathbb{P}\\left(|\\hat{y}_{i,j,t+1}-y_{i,j}|\\geq\\widetilde{\\Gamma}_{y,j,t},n_{j,t}\\geq\\widetilde{m}_{j}(t)\\right)}_{A}+\\underbrace{\\mathbb{P}\\left(n_{j,t}<\\widetilde{m}_{j}(t)\\right)}_{B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We would bound terms $A$ and $B$ respectively. ", "page_idx": 26}, {"type": "text", "text": "Bounding $A$ . Recall that $\\tau_{j,k}\\in[T]$ is the round at which the $k$ th consumer of type $j$ arrived. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\mathbb{P}\\Big(\\Big|\\hat{y}_{t+1,i,j}-y_{i,j}\\Big|\\geq\\widetilde{\\Gamma}_{y,j,i},n_{j,i}\\geq\\widetilde{m}_{j}(t)\\Big)}\\\\ &{\\quad\\leq\\mathbb{P}\\Big(\\Big|\\operatorname*{max}_{\\bar{m}(i)\\leq n\\leq i}\\frac{1}{n_{k-1}}\\underbrace{\\mathbb{I}\\{I_{\\gamma,i}=i,z_{\\bar{\\tau}_{j,k}}=1\\}}_{x_{\\tau_{j,k},i,j}}-y_{i,j}\\Big|\\geq\\widetilde{\\Gamma}_{y,j,i},n_{j,t}\\geq\\widetilde{m}_{j}(t)\\Big)}\\\\ &{\\quad\\leq\\mathbb{P}\\Big(\\Big|\\operatorname*{max}_{\\bar{m}(i)\\leq n\\leq i}\\displaystyle\\frac{1}{n_{k-1}}\\sum_{k=1}^{n}\\frac{\\mathbb{I}\\{I_{\\gamma_{j,k}}=i,z_{\\bar{\\tau}_{j,k}}=1\\}}{x_{\\tau_{j,k},i,j}}-y_{i,j}\\Big|\\geq\\widetilde{\\Gamma}_{y,j,i}\\Big)}\\\\ &{\\quad\\leq\\displaystyle\\sum_{\\bar{m}_{j}(i)\\leq n\\leq i}\\mathbb{P}\\Big(\\Big|\\frac{1}{n}\\sum_{k=1}^{n}\\frac{\\mathbb{I}\\{I_{\\gamma_{j,k}}=i,\\tau_{\\tau_{j,k}}=1\\}}{x_{\\tau_{j,k},i,j}}-y_{i,j}\\Big|\\geq\\widetilde{\\Gamma}_{y,j,i}\\Big)}\\\\ &{\\quad\\leq\\displaystyle\\sum_{\\bar{m}_{i}(i)\\leq n\\leq i}\\mathbb{I}\\Big(\\Big|\\frac{1}{n}\\sum_{k=1}^{n}\\frac{\\mathbb{I}\\{I_{\\gamma_{j,k}}=i,z_{\\tau_{j,k}}=1\\}}{x_{\\tau_{j,k},i,j}}-y_{i,j}\\Big|\\geq\\frac{2\\log(T)}{\\sqrt{n}\\epsilon_{i}}\\Big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the final inequality we used the definition of $\\widetilde{\\Gamma}_{y,j,t}$ in (45) and the fact that $n\\geq\\widetilde{m}_{j}(t)$ . Note that for any $k\\in\\mathbb{N}$ , we have $\\mathbb{E}\\left[\\frac{\\mathbb{I}\\{I_{\\tau_{j,k}}=i,z_{\\tau_{j,k}}=1\\}}{x_{\\tau_{j,k},i,j}}\\;\\Big|\\;\\mathcal{F}_{\\tau_{j,k}}\\right]\\;=\\;y_{i,j}$ , where $\\mathcal{F}_{\\tau_{j,k}}$ is the sigma-algebra generated from all randomness up to round $\\tau_{j,k}$ . Therefore, $\\begin{array}{r}{\\mathcal{M}_{k}\\,=\\,\\frac{\\mathbb{I}\\{I_{\\tau_{j,k}}=i,z_{\\tau_{j,k}}=1\\}}{x_{\\tau_{j,k},i,j}}-y_{i,j}}\\end{array}$ is a Martingale difference sequence. Further, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\,1\\,\\le\\,\\mathcal{M}_{k}\\,\\le\\,\\frac{1}{x_{\\tau_{j,k},i,j}}\\,\\overset{(a)}{\\le}\\,\\frac{1}{\\epsilon_{\\tau_{j,k}}}\\le\\frac{1}{\\epsilon_{t}}\\overset{(b)}{\\Longrightarrow}|\\mathcal{M}_{k}|\\le\\frac{1}{\\epsilon_{t}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (a) follows from the fact that $\\begin{array}{r}{x_{\\tau_{j,k},i,j}\\;=\\;\\frac{1}{N}\\;\\ge\\;\\epsilon_{\\tau_{j,k}}}\\end{array}$ if $\\|\\hat{\\pmb{y}}_{t}\\|_{\\infty}\\geq1$ and $x_{\\tau_{j,k},i,j}\\,=\\,(1\\,-\\,$ $N\\epsilon_{t})\\hat{x}_{\\tau_{j,k},i,j}\\,+\\epsilon_{\\tau_{j,k}}\\,\\mathrm{~\\,~\\geq~}\\epsilon_{\\tau_{j,k}}$ if $\\|\\hat{\\pmb{y}}_{t}\\|_{\\infty}<1$ according to FORM (Algorithm 1); (b) follows from $\\begin{array}{r}{\\epsilon_{t}\\leq\\frac{1}{N}}\\end{array}$ so $\\scriptstyle{\\frac{1}{\\epsilon_{t}}}\\geq N\\geq1$ , and hence $\\begin{array}{r}{\\mathcal{M}_{k}\\geq-1\\geq-\\frac{1}{\\epsilon_{t}}}\\end{array}$ . Also, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[M_{k}^{2}\\Big|\\mathcal{F}_{\\tau_{j,k}}\\right]=\\mathbb{E}\\left[\\frac{\\mathbb{I}\\{I_{\\tau_{j,k}}=i,z_{\\tau_{j,k}}=1\\}}{x_{\\tau_{j,k},i,j}^{2}}\\Big|\\mathcal{F}_{\\tau_{j,k}}\\right]-y_{i,j}^{2}\\leq\\frac{1}{x_{\\tau_{j,k},i,j}}\\leq\\frac{1}{\\epsilon_{t}}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, by Bernstein\u2019s inequality (see Lemma G.4), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\mathbb{P}\\Big(\\Big|\\displaystyle\\sum_{k\\in[n]}\\mathcal{M}_{k}\\Big|\\geq\\sqrt{(e-2)V_{n}\\log(2/\\delta)}\\Big)\\leq\\delta}\\\\ &{\\stackrel{(a)}{\\Longrightarrow}\\mathbb{P}\\Big(\\Big|\\displaystyle\\frac{1}{n}\\sum_{k=1}^{n}\\frac{\\mathbb{I}\\{I_{\\tau_{j,k}}=i,z_{\\tau_{j,k}}=1\\}}{x_{\\tau_{j,k},i,j}}-y_{i,j}\\Big|\\geq\\frac{2\\log(T)}{\\sqrt{n\\epsilon_{t}}}\\Big)\\leq\\displaystyle\\frac{2}{T^{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (a) follows from invoking Lemma G.4 by taking $\\begin{array}{r}{c_{k}=\\frac{1}{\\epsilon_{t}}}\\end{array}$ \u03f51 and Vk = $\\begin{array}{r}{V_{k}=\\frac{2k\\log(T)}{(e-2)\\epsilon_{t}}}\\end{array}$ , and $\\begin{array}{r}{\\delta=\\frac{1}{2T^{2}}}\\end{array}$ . Combining Eq. (49) and Eq. (52) yields ", "page_idx": 27}, {"type": "equation", "text": "$$\nA\\;\\leq\\;\\sum_{\\tilde{m}_{j}(t)\\leq n\\leq t}\\frac{2}{T^{2}}\\;\\leq\\;\\frac{2t}{T^{2}}\\;\\leq\\;\\frac{2}{T}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Bounding $B$ . Denote $\\mathcal{M}_{t}=\\mathbb{I}\\{J_{t}=j\\}-p_{j}$ and it is easy to see $\\mathcal{M}_{t}$ is a Martingale difference sequence bounded between $[-1,1]$ . Hence, by recognizing $\\begin{array}{r}{\\dot{n}_{j,t}-t\\cdot p_{j}=\\sum_{\\tau\\in[t]}\\dot{\\mathcal{M}_{\\tau}}}\\end{array}$ and applying the Azuma-Hoeffding inequality, for any $\\epsilon>0$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{P}\\Big(n_{j,t}-t\\cdot p_{j}<-\\epsilon\\Big)=\\mathbb{P}\\Big(\\sum_{\\tau\\in[t]}\\mathcal{M}_{\\tau}<-\\epsilon\\Big)\\leq\\exp\\left(-\\frac{2\\epsilon^{2}}{t}\\right)}\\\\ {\\displaystyle\\Longrightarrow B=\\mathbb{P}\\Big(n_{j,t}<\\widetilde{m}_{j}(t)\\Big)=\\mathbb{P}\\Big(n_{j,t}-t\\cdot p_{j}<-\\sqrt{t\\log(T)/2}\\Big)\\,\\,\\stackrel{(a)}{\\leq}\\,\\,\\frac{1}{T}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where in (a) we take $\\epsilon=\\sqrt{t\\log(T)/2}$ . ", "page_idx": 27}, {"type": "text", "text": "E Extensions to Additional Setups ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our fair online recommendation framework, Problem (FAIR), and method FORM can be extended to accommodate additional variants of our setup. In Section E.1, we discuss how our framework/method can additionally handle periodic arrivals with the same performance guarantees. In Section E.2, we detail how our framework/method extends to handling the assortment recommendation setting. ", "page_idx": 27}, {"type": "text", "text": "E.1 Fair Online Recommendation with Periodic Arrivals ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In real-world scenarios, user arrivals are often non-stationary, displaying variations over time due to periodic user preferences and behavior. This is particularly evident in many recommendation systems that exhibit daily or weekly periodicity, with consistent patterns of user arrivals and interactions throughout the day or week. For instance, in an e-commerce setting, higher user activity and engagement may be observed during lunch breaks, evenings, or late-night browsing. Similarly, recommendation systems may experience increased user engagement during weekends compared to weekdays. Recognizing the presence of these periodic patterns, we extend our concept of a fair solution and our algorithm FORM to accommodate the online setting with periodic arrivals. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Consider a scenario where users arrive in each round $t\\in[T]$ according to a probability distribution $p_{t}\\,\\in\\,\\Delta_{M}$ . Additionally, suppose that there exists a period length $Q\\,\\in\\,(0,T)$ and distributions $\\widetilde{p}^{(1)}\\ldots\\widetilde{p}^{(Q)}$ such that $p_{m Q+q}=\\widetilde{p}^{(q)}$ for any $m=0,1,2\\ldots$ and $q=0,1\\ldots Q-1$ . By defining $\\textbf{\\emph{p}}$ as the time-averaged arrival distribution, represented by $\\begin{array}{r}{{\\pmb{p}}=\\frac{1}{T}\\sum_{t\\in[T]}{\\pmb{p}}_{t}}\\end{array}$ , we can generalize the fair recommendation problem, Problem (FAIR), to incorporate this periodic arrival setup. ", "page_idx": 28}, {"type": "text", "text": "Yet, similar to the stationary setting, the platform does not have prior knowledge of the sequence of user arrival distributions $p_{1}\\ldots p_{T}$ , nor the time-averaged distribution $\\pmb{p}$ . Although solving Problem (FAIR) w.r.t. the time-averaged distribution $\\textbf{\\emph{p}}$ seems to be more challenging compared to that under stationary arrivals, we can in fact still obtain accurate estimates by slightly modifying the design of FORM as follows: instead of estimating $\\pmb{p}$ using all historical data (as done in Eq. (2)), we only obtain estimates over a sliding window of length $\\mathcal{W}>0$ . That is, we estimate the arrival rates of type- $j$ user at round $t+1$ via the following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{p}_{t+1,j}=\\frac{1}{\\operatorname*{min}\\{t,\\mathcal{W}\\}}\\sum_{\\tau=\\operatorname*{max}\\{1,t-\\mathcal{W}+1\\}}^{t}\\mathbb{I}\\{J_{t}=j\\}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By doing so and keeping all other procedures unchanged, we show in Theorem E.1 that FORM would yield the same theoretical guarantees for its final output under the setting with periodic arrivals, when the size of the sliding window size $\\mathcal{W}$ is chosen appropriately. ", "page_idx": 28}, {"type": "text", "text": "Theorem E.1 (Performance of FORM under Periodic Arrivals) If we apply FORM (\u221aAlgorithm $^{\\,l}$ ) with an estimated arriving probability $\\hat{\\pmb{p}}_{t+1,j}$ taking the form in Eq. (55) and $\\begin{array}{r}{\\Gamma_{p,t}=\\frac{5\\sqrt{\\log(3T)}}{\\sqrt{\\mathcal{W}}}\\ w i t h}\\end{array}$ a sliding window of size $\\mathcal{W}=Q T^{\\frac{2}{3}}$ . Then, for sufficiently large $T$ , we have the following: ", "page_idx": 28}, {"type": "text", "text": "\u2022 the revenue regret is at most $\\mathbb{E}[\\mathcal{R}(T)]\\le\\mathcal{O}(M N^{\\frac{1}{3}}T^{-\\frac{1}{3}})$ \u2022 the fairness regret is at most $\\mathbb{E}[\\mathcal{R}_{F}(T)]\\le\\mathcal{O}(M N^{\\frac{1}{3}}T^{-\\frac{1}{3}})$ ", "page_idx": 28}, {"type": "text", "text": "The proof of Theorem E.1 is provided in Section E.1.1. ", "page_idx": 28}, {"type": "text", "text": "Remark E.1 (Lack of knowledge of the period length) We remark that in the periodic setup, we assume knowledge of the period length $Q>0$ . This assumption aligns with practical scenarios where platforms are aware of intraday, daily, weekly, or seasonal user arrival cycles. However, if the period length $Q$ is unknown, one might consider employing a standard meta \u201cexpert\u201d algorithm from the bandit literature on top of FORM. This approach involves using sliding window lengths as experts, representing different expert windows in the set $A\\,=\\,a T^{\\frac{2}{3}}:a=1,2\\dots\\bar{Q}$ for some $\\bar{Q}>\\mathbf{\\bar{Q}}$ . Each expert produces estimates of $\\pmb{p}$ and the fair solution based on their window length. We maintain weights over these estimates using an EXP3 algorithm. However, while this approach may eventually approximate the cumulative revenue of the best expert with the correct period length $Q$ , the expected constraint violation w.r.t. the true parameters $\\pmb{\\theta}$ may not be sublinear or vanishing over time. This is because the estimates from experts with incorrect period lengths can significantly violate constraints, resulting in overall large constraint violations. Therefore, optimizing the fair solution under a periodic arrival setup with an unknown period length remains an open question. ", "page_idx": 28}, {"type": "text", "text": "E.1.1 Proof of Theorem E.1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "First, similar to Lemma D.2 that presents a concentration bound for empirical estimates of $\\pmb{p}$ using all historical data, if we construct an empirical estimate for $\\pmb{p}$ with Eq. (55) using sliding window length $\\mathcal{W}$ , again applying Lemma G.3, we have for any $t>\\mathcal{W}$ s.t. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\lVert\\pmb{p}-\\hat{\\pmb{p}}_{t}\\rVert_{1}>\\frac{5\\sqrt{\\log(3T)}}{\\sqrt{\\mathcal{W}}}\\Big)\\leq\\frac{1}{T}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Recall the definition of $\\tau$ in Eq. (24). We additionally define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\overline{{\\mathcal{T}}}=\\operatorname*{max}\\{\\mathcal{W}+1,\\mathcal{T}\\}=\\mathcal{O}(\\mathcal{W}T^{\\frac{2}{3}})\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Following the same analysis as bo\u221aunding the regret for Theorem 3.1 in Eq. (29), while replacing $\\tau$ with $\\overline{{\\tau}}$ and considering $\\dot{\\Gamma_{p,t}}=\\frac{5\\sqrt{\\log(3T)}}{\\sqrt{\\mathcal{W}}}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t\\in[T]}\\mathbb{E}\\Big[\\mathrm{REV}\\big(\\pmb{x}^{\\star},\\pmb{\\theta}\\big)-\\mathrm{REV}\\big(\\pmb{x}_{t},\\pmb{\\theta}\\big)\\Big]\\leq\\mathcal{O}\\Big(\\frac{\\overline{{T}}}{T}+\\frac{M}{T}\\sum_{t>\\overline{{T}}}\\big(\\Gamma_{p,t}+\\Gamma_{y,t}\\big)+\\frac{M N}{T}\\sum_{t>\\overline{{T}}}\\epsilon_{t}\\Big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\,\\mathcal{O}\\big(M N^{\\frac{1}{3}}T^{-\\frac{1}{3}}\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "A similar analysis as Eq. (31) applies to bounding the fairness regret $\\mathcal{R}_{F}(T)$ . \u25a0 ", "page_idx": 29}, {"type": "text", "text": "E.2 Fair Online Assortment Recommendation ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Our model in Section 2 primarily considered a setting where a single item is offered to each user at each round, which applies to various real-world settings. In this section, we discuss an extension of our framework/method to a setting in which the platform might wish to recommend an assortment to its users, which applies to real-world scenarios such as job recommendation, where a number of most relevant jobs are recommended to a candidate, or e-commerce sites, where a small assortment of items might be featured on the front page whenever a user arrives. ", "page_idx": 29}, {"type": "text", "text": "Extension of our framework. To consider an assortment recommendation setting, we let $w_{i,j}>0$ be the weight associated with each item $i$ for a type- $j$ user. If a type- $j$ user arrives onto the platform and gets offered assortment $S$ , he/she will choose/purchase item $i$ from the assortment with probability 1+wwij,j(S), where wj(S) = i\u2208S wi,j, based on the MNL model [65]. This is different from the single-item recommendation setting, where we have well-defined purchase probability $y_{i,j}$ for each item-user pair. When we recommend assortments, the purchase probability not only depends on an item\u2019s weight, but also the assortment it gets presented in. Given that, we define the problem instance as $\\pmb\\theta=(p,w,r)$ . ", "page_idx": 29}, {"type": "text", "text": "Our fair recommendation framework, Problem (FAIR), can be readily extended by letting our decision variable be $\\pmb q\\,=\\,\\{q_{j}(S)\\,:\\,S\\,\\subseteq\\,[N],|S|\\,\\leq\\,K,j\\,\\in\\,[M]\\}$ , where $q_{j}(S)$ denote the probability of presenting assortment $S$ to a type- $^{\\,j}$ user. Then, we can formulate the fair recommendaton problem under the same idea: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\b{q}}{\\operatorname*{max}}\\ \\ \\mathrm{REV}(\\b{q})\\ \\ \\mathrm{s.t.}\\ \\ }&{O_{i}^{\\tt I}(\\b{q})\\geq\\delta^{\\tt I}\\cdot O_{i}^{\\tt I}(\\b{f}^{\\tt I})\\ \\forall\\ \\it i\\in[N]}\\\\ &{O_{j}^{\\tt U}(\\b{q})\\geq\\delta^{\\tt U}\\cdot O_{j}^{\\tt U}(\\b{f}^{\\tt U})\\ \\forall\\ \\it j\\in[M]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathsf{R E V}(q)=\\sum_{j\\in[M]}p_{j}\\sum_{S\\in[N],|S|\\leq K}q_{j}(S)\\frac{\\sum_{i\\in S}r_{i}w_{i,j}}{1+w_{j}(S)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "denote the expected revenue under recommendation $\\pmb q$ . On the other hand, the forms of the item/user outcome functions $O_{i}^{\\mathrm{I}}({\\pmb q},{\\pmb\\theta}),O_{j}^{\\mathrm{U}}({\\pmb q},{\\pmb\\theta})$ as well as their respective fair solutions ${f}^{\\mathrm{I}}(\\theta),{f}^{\\mathrm{U}}(\\theta)$ can again take any forms that depend on the problem instance $\\pmb{\\theta}$ , based on their own needs. ", "page_idx": 29}, {"type": "text", "text": "Extension of our algorithm for the online setting. Given the extended framework Problem (FAIR-ASSORT), we can similarly extend the algorithm FORM for the assortment recommendation setting, which is outlined in Algorithm 2. ", "page_idx": 29}, {"type": "text", "text": "Here, Algorithm 2 follows the same relaxation-then-exploration design as discussed in Section 3.3 and Algorithm 1. Whenever our estimate of the problem instance $\\pmb{\\theta}$ is updated, the algorithm first solves a relaxed version of Problem (FAIR-ASSORT) using the updated estimate, and then adds randomized exploration to stimulate learning; see Steps 2(b) and 2(c) in Algorithm 2.5 ", "page_idx": 29}, {"type": "text", "text": "The main difference between Algorithm 2 and Algorithm 1 lies in the learning mechanism. In order to learn the weights $\\mathbf{\\nabla}w$ in the assortment setting, we adopt the learning mechanism from MNL-Bandit [3]. The rounds in which type- $j$ arrives are divided into epochs. In each epoch $\\ell_{j}$ , the same assortment $S_{j}$ is offered until a no-purchase option is observed. We then apply Eq. (58) to estimate $\\pmb{w}_{:,j}$ , which is an unbiased estimator according to Corollary A.1 in [3]. To estimate arrival probabilities, we ", "page_idx": 29}, {"type": "text", "text": "Input: (i) $N$ items with revenues ${\\pmb r}\\in\\mathbb{R}^{N}$ , item outcome $O_{i}^{\\mathrm{I}}(.~,.),i\\in[N]$ and item-fair solution $\\boldsymbol{f^{0}}(.)$ ; (ii) $M$ types of users with user outcome $O_{j}^{\\mathrm{U}}(.~,.),j\\in[M]$ and user-fair solution $\\mathbf{\\bar{\\boldsymbol{f}}^{\\intercal}(\\boldsymbol{\\cdot})}$ (iii) fairness parameters $\\delta^{\\mathtt{I}},\\delta^{\\mathtt{U}}\\in[0,1]$ . (iv) parameters $\\epsilon_{t}$ and $\\eta_{t}$ . ", "page_idx": 30}, {"type": "text", "text": "1. Initialization and Setting the Parameters. For all $i\\in[N]$ , $j\\in[M]$ , initialize $\\hat{w}_{1,i,j}=1/2$ and $\\hat{p}_{1,j}=1/M$ . Randomly select $S_{j}\\sim{\\mathcal{S}}$ , where ${\\cal{S}}=\\{{\\cal{S}}\\subseteq[N],|{\\cal{S}}|\\leq K\\}$ denote the collection of all possible assortments. Let $\\ell_{j}=0,T_{i,j}=\\emptyset$ . ", "page_idx": 30}, {"type": "text", "text": "2. While $t\\leq T$ . \u2022 Observe the type of the arriving user $J_{t}$ and offer assortment $S_{J_{t}}$ . Update number of arrivals $n_{J_{t},t}\\gets n_{J_{t},t}+1$ . \u2022 Observe purchase decision $z_{t}\\in\\{0\\}\\cup S_{J_{t}}$ (where 0 means no-purchase) \u2013 If $z_{t}=0$ , (a) Let $\\mathcal{T}_{i,J_{t}}=\\mathcal{T}_{i,J_{t}}\\cup\\{\\ell_{J_{t}}\\}$ for $i\\in S_{J_{t}}$ , where $\\mathcal{T}_{i,J_{t}}$ are the epochs in which item $i$ gets shown to user type $J_{t}$ . Then, increment $\\ell_{J_{t}}\\gets\\ell_{J_{t}}+1$ , where $\\ell_{J_{t}}$ denotes the number of epochs for user type $J_{t}$ . (b) Update estimates for item weights and arrival probabilities. Let ${\\hat{w}}_{t,i,J_{t}}={\\frac{1}{|T_{i,J_{t}}|}}\\sum_{\\tau\\in T_{i,J_{t}}}\\sum_{t\\in{\\mathcal{E}}_{\\tau,J_{t}}}\\mathbb{I}\\{z_{t}=i\\}\\quad\\forall i\\in[N]\\quad{\\mathrm{and}}\\quad{\\hat{p}}_{t,j}={\\frac{1}{t}}n_{j,t}\\ \\ \\forall j\\in[M]$ (58) (c) Solve a Relaxed Problem (FAIR-ASSORT) with the Estimated Instance. Given estimated instance $\\widehat{\\pmb{\\theta}}=(\\widehat{\\pmb{p}_{t}},\\widehat{\\pmb{w}}_{t},\\pmb{r})$ and the magnitude of relaxation $\\eta_{t}$ , let $\\hat{\\pmb q}_{t}$ be the optimal solution to the following: $\\begin{array}{r l}{\\underset{q}{\\operatorname*{max}}\\ \\ \\mathsf{R E V}(q,\\hat{\\theta}_{t})}&{\\mathrm{s.t.}\\ \\ \\ O_{i}^{\\mathbb{I}}(q,\\hat{\\theta}_{t})\\geq\\delta^{\\mathbb{I}}\\cdot O_{i}^{\\mathbb{I}}(f^{\\mathbb{I}}(\\hat{\\theta}_{t}),\\hat{\\theta}_{t})-\\eta_{t}\\ \\forall\\ i\\in[N]}\\\\ &{\\ \\ \\ \\ }&{O_{j}^{\\mathbb{I}}(q,\\hat{\\theta}_{t})\\geq\\delta^{\\mathbb{I}}\\cdot O_{j}^{\\mathbb{I}}(f^{\\mathbb{I}}(\\hat{\\theta}_{t}),\\hat{\\theta}_{t})-\\eta_{t}\\ \\forall\\ j\\in[M]\\,,}\\end{array}$ (59) (d) Recommend with Randomized Exploration. Update the assortment to be recommended to type- $J_{t}$ user based on the recommendation probabilities $S_{J_{t}}\\sim q_{t,J_{t}}$ , where $\\pmb{q}_{t,J_{t}}(S)=(1-|S|\\epsilon_{t})\\hat{\\pmb{q}}_{t,J_{t}}(S)+\\epsilon_{t}\\quad\\forall S\\in S,j\\in[M]\\,.$ (60) \u2013 Else, E\u2113Jt ,Jt \u2190E\u2113Jt ,Jt \u222a{t} \u2022 t \u2190t + 1 . ", "page_idx": 30}, {"type": "text", "text": "again use the sample mean for $\\pmb{p}$ . As discussed in Section 3.3, any learning mechanism that provides accurate estimates for sufficiently large $T$ would be suitable here. The magnitudes of exploration and relaxation, $\\epsilon_{t}$ and $\\eta_{t}$ , are adjustable inputs for platforms to fine-tune in practice. ", "page_idx": 30}, {"type": "text", "text": "While a detailed theoretical analysis of the expanded algorithm is reserved for future work, it is clear that our high-level ideas behind FORM, including incorporating relaxations for fair recommendations and adding exploration to stimulate learning, seamlessly transition to the assortment recommendation context. The efficacy of our extended framework/method in the assortment recommendation setting is further validated in our case study on Amazon review data (see Section 4). ", "page_idx": 30}, {"type": "text", "text": "F Supplements for Case Study on Amazon Review Data ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "F.1 More Details of Our Baselines ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we discuss the baselines introduced in existing works, specifically FairRec [55] and TFROM [71], which are state-of-the-art approaches targeting multi-sided fairness. We detail the main setups of both approaches and highlight the key differences between these methods and ours. ", "page_idx": 30}, {"type": "text", "text": "FairRec [55]. As we briefly remarked in Section 4, FairRec targets a single-shot recommendation problem where there are $N$ items and $M$ users, and makes a single recommendation decision that displays $K$ items to each of the $M$ users, assuming full knowledge of the exact relevance score between each pair of item and user. Let ${\\cal A}\\,=\\,\\{(\\bar{A}_{j})_{j\\in[M]}\\,:\\,A_{j}\\,\\,\\bar{\\subset}\\,\\,[N],|A_{j}|\\,\\le\\,K\\}$ denote the the algorithm\u2019s recommendation decision. The authors of [55] show that FairRec can achieve (i) maxmin share (MMS) of visibility (exposure) for most of the items and non-zero visibility for the rest; (ii) envy-free up to one item (EF1) fairness for every user, meaning that for every pair of users $j,j^{\\prime}\\in[M]$ , there should exist an item $p\\in A_{j^{\\prime}}$ such that the utility that user $j$ receives from $A_{j}$ is at least the utility that user $j^{\\prime}$ receives from $A_{j^{\\prime}}\\backslash\\{p\\}$ . ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "There are several key differences between our setting/method and FairRec. (i) FairRec focuses on an offline setting where the platform makes a single-shot recommendation with full knowledge of user preferences (i.e., relevance scores). In contrast, our work targets the more challenging online setting where user data is unknown and must be continuously learned. We maintain fairness in this online setting, dealing with users who arrive stochastically, and ensuring fairness over a longer time horizon. (ii) FairRec assumes specific fairness notions (i.e., maxmin fairness regarding visibility for items and envy-free fairness up to one item for users). Our framework, on the other hand, is much more flexible, accommodating a wide range of outcome functions and fairness notions, as discussed in Section 2.2. See, also, Section F.2 where we conduct additional experiments on the Amazon review data under alternative outcome functions and fairness notions, and demonstrate that FORM remains effective in making fair recommendations in an online environment. ", "page_idx": 31}, {"type": "text", "text": "TFROM [71]. Similar to FairRec, the authors of [71] consider a recommendation problem where relevance scores between items and users are known in advance. To impose fairness for items, TFROM enforces either uniform fairness or quality-weighted fairness regarding item visibility (see Definitions 1 and 2 in [71]). For user fairness, TFROM aims to achieve uniform normalized discounted cumulative gain (NDCG) among users. [71] does not provide theoretical performance results, but numerically shows that TFROM achieves low variance in item visibility and NDCG among users in online settings, thus ensuring two-sided fairness. TFROM is provided in two versions: one for the offline setting and one for the online setting, with the latter dynamically maintaining low variance in item exposure and user NDCG scores over time. We compare our algorithm with the online version of TFROM in our experiments in Section 4. ", "page_idx": 31}, {"type": "text", "text": "While TFROM addresses online user arrivals, it again differs significantly from our work in several ways: (i) Like FairRec, it assumes full knowledge of user preferences (i.e., relevance scores) and lacks a learning stage, ignoring potential biases from corrupted data; (ii) It focuses on pre-specified outcomes and fairness notions for items/users, lacking the flexibility of our framework; (iii) No theoretical guarantees are provided for TFROM\u2019s performance, whereas we offer full theoretical guarantees for our algorithm despite data uncertainty. ", "page_idx": 31}, {"type": "text", "text": "Finally, it is important to note that neither FairRec nor TFROM considers the platform\u2019s revenue, which is another key focus of our work. While the aforementioned works focus on two-sided fairness in recommender systems, our framework takes a step forward and adopts a multi-sided perspective, balancing the interests of the platform, items, users, and potentially other stakeholders. The efficacy of our framework is highlighted in Figure 1b, showing how our algorithm manages to maintain high platform revenue while ensuring fairness for other stakeholders. ", "page_idx": 31}, {"type": "text", "text": "F.2 Additional Experiments under Alternative Outcomes and Fairness Notions ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To demonstrate the generality of our results, we have replicated our experiments on Amazon review data from Section 4 under alternative fairness notions and outcomes for items. In particular, we have considered the following definitions of the item-fair solutions $f^{\\mathrm{I}}$ , while keeping all other setups and baselines the same as in Section 4: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Maxmin fairness w.r.t. item visibility. Results are shown in Figure 3.   \n\u2022 K-S fairness w.r.t. item revenue. Results are shown in Figure 4.   \n\u2022 K-S fairness w.r.t. item visibility. Results are shown in Figure 5. ", "page_idx": 31}, {"type": "text", "text": "Overall, the results from Figures 3, 4, and 5 are consistent with those previously observed in Figure 1. In terms of the platform\u2019s revenue, we again observe the convergence of time-averaged revenue towards the optimal revenue in hindsight, confirming FORM\u2019s low regret. Regarding normalized revenue, item outcomes, and user outcomes, FORM consistently strikes an effective balance among multiple stakeholders\u2019 interests under the specified fairness parameters $\\delta^{\\mathtt{I}}$ and $\\delta^{\\tt U}$ , regardless of the fairness notion or outcome that stakeholders care about. The performance of our baselines, however, is more affected by the fairness notion or outcomes when defining within-group fairness. For instance, while min-revenue achieves high levels of fairness for all items when the item-fair solution considers maxmin fairness w.r.t. item revenues, its performance deteriorates when items instead care about visibility or adopt K-S fairness. In comparison, FORM quickly adapts to various fairness notions and outcomes, while maintaining its good performance. ", "page_idx": 31}, {"type": "image", "img_path": "tAOg1HdvGy/tmp/e6af3e895a61c14c018eec194ae719433345973a13883c15074419a2d8d4940c.jpg", "img_caption": ["Figure 3: Additional experiment results for Amazon review data. Here, the item-fair solution adopts maxmin fairness w.r.t. item visibility. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "tAOg1HdvGy/tmp/e692f1902fa467c71978f1f52b3749b2b2dcb2a2eb4a533833cc2412e9160d7d.jpg", "img_caption": ["Figure 4: Additional experiment results for Amazon review data. Here, the item-fair solution adopts K-S fairness w.r.t. item revenue. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "F.3 Additional Experiments on MovieLens Data ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To show generality of our framework/method, we have conducted additional experiments in an alternative setting (movie recommendation) using the MovieLens data, which complements our case study on Amazon review data in Section 4. Overall, the results are consistent with our Amazon case study, showing our method\u2019s effectiveness in balancing platform revenue and stakeholder fairness in various types of recommender systems. ", "page_idx": 32}, {"type": "text", "text": "Here, we act as a movie recommendation platform that shows a \u201ctrending action movie\u201d to any arriving user. We considered the $N\\,=\\,50$ action movies from MovieLens (ML-100K) data [35] with the highest number of ratings and clustered users into 6 types based on their preferences. As the movies are not associated with revenues, we let $r_{i}=1$ for all $i\\,\\in\\,[N]$ . Here, the platform\u2019s main objective is to maximize its expected marketshare. The item-fair solution adopts maxmin fairness w.r.t. each movie\u2019s marketshare, with user utilities captured by the MNL model. In this set of experiments, we consider a total of $T=2000$ rounds, where at each round there are 100 user arrivals. During each round $t$ , we would solve the relaxed constrained optimization problem and update our recommendation probabilities only once, given scalability considerations as stated in Section 3.5; however, we will update our estimates for item weights and arrival probabilities based on Eq. (58) throughout the user arrivals. ", "page_idx": 32}, {"type": "text", "text": "Figure 6 shows the results obtained from our MovieLens experiments. It is evident the results are largely in line with what we observed in our case study on Amazon review data (Section 4), both in terms of the platform\u2019s revenue and the outcomes received by items/users. Our framework and method also remain effective when we only resolve the relaxed constrained optimization problem after a given number of user arrivals. It is noteworthy that in a movie recommendation setting with homogeneous revenues, the interests of the platform and the users completely align. This explains why the curves of greedy and max-utility completely overlaps with each other in our figures. However, greedy still suffers from $7.8\\%$ loss in marketshare (Figure 6b)), which is precisely because inadequate exploration of user data makes it overlook potentially more popular items and stick with a sub-optimal item. Overall, our algorithm FORM again adeptly balances the interests of both the platform and its stakeholders, while handling the tradeoff between learning and fair recommendation. ", "page_idx": 32}, {"type": "text", "text": "G Supplementary Lemma ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we state some useful lemmas that would be invoked in this work. ", "page_idx": 32}, {"type": "image", "img_path": "tAOg1HdvGy/tmp/cb241439737446d7f32fa2c4804b8d0adc8cea052246d1a79d7a5f280ea7b335.jpg", "img_caption": ["(a) Convergence of revenue. (b) Normalized revenue. (c) Item outcomes. (d) User outcomes. ", "Figure 5: Additional experiment results for Amazon review data. Here, the item-fair solution adopts K-S fairness w.r.t. item visibility. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "tAOg1HdvGy/tmp/60c65ac7d6c7a36bac168a2659addb34c2303029cd43cd82ae676abdafbf6109.jpg", "img_caption": ["(a) Convergence of revenue. (b) Normalized revenue. (c) Item outcomes. (d) User outcomes. ", "Figure 6: Additional experiment results on MovieLens data. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Lemma G.1 (Approximations to solutions to systems of linear equations ([33], Theorem 2.1)) Let $\\begin{array}{r l r}{A}&{{}\\in}&{\\mathbb{R}^{m\\times n}}\\end{array}$ , and $\\begin{array}{r l r}{H(\\pmb{A})}&{{}=}&{\\operatorname*{max}\\{\\|\\lambda\\|_{1}}\\end{array}$ : $\\lambda$ is an extreme point of $\\sigma(A)\\}$ , where $\\begin{array}{r c l c r c l}{{\\sigma(\\pmb{A})}}&{{=}}&{{\\{\\lambda}}&{{\\in}}&{{\\mathbb{R}^{m}}}&{{:}}&{{\\lambda}}&{{\\geq}}&{{0,\\|\\lambda^{\\top}\\pmb{A}\\|_{1}}}&{{\\leq}}&{{1\\}}\\end{array}$ . Then, for each $\\textbf{\\textit{b}}\\in\\mathbb{R}^{m}$ such that $\\{x\\in\\mathbb{R}^{n}:A x\\leq b\\}\\neq\\emptyset$ and for each $\\pmb{x}^{\\prime}\\in\\mathbb{R}^{n}$ , we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A x\\leq b}\\|x-x^{\\prime}\\|_{\\infty}\\leq H(A)\\cdot\\left\\|(A x^{\\prime}-b)^{+}\\right\\|_{\\infty}\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $H(A)\\geq0$ is known as the Hoffman constant. ", "page_idx": 33}, {"type": "text", "text": "Lemma G.2 (Characterization of Hoffman constant ([56], Proposition 2)) Let $A\\in\\mathbb{R}^{m\\times n}$ . The Hoffman constant $H(A)$ defined in Lemma $G.l$ can be characterized as follows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{4})=\\operatorname*{max}_{\\underset{A,J\\,\\mathrm{f}i l l\\,r o w\\,r a n k}{\\operatorname*{max}}}\\operatorname*{max}\\left\\{\\|v\\|_{1}:v\\in\\mathbb{R}_{+}^{J},\\|A_{J}^{\\top}v\\|_{1}\\leq1\\right\\}=\\operatorname*{max}_{\\underset{A,J\\,\\mathrm{f}i l l\\,r o w\\,r a n k}{\\operatorname*{max}}}\\frac{1}{\\operatorname*{min}_{v\\in\\mathbb{R}_{+}^{J},\\|v\\|_{1}=1}\\left\\|A_{J}^{\\top}v\\right\\|_{1}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $A_{J}$ denotes the submatrix of $\\pmb{A}$ that only includes the rows in $J\\subseteq\\{1,\\ldots,m\\}$ . ", "page_idx": 33}, {"type": "text", "text": "Lemma G.3 (Empirical distribution concentration bound w.r.t. \u2113-1 norm ([23], Lemma 3)) Let $\\pmb{p}\\in\\Delta_{N}$ and $\\hat{p}\\sim$ Multinomia $\\left(n,p\\right)$ . Then, for any $\\delta\\in[0,3\\exp(-4N/5)]$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\lVert\\pmb{p}-\\hat{\\pmb{p}}\\rVert>\\frac{5\\sqrt{\\log(3/\\delta)}}{\\sqrt{n}}\\Big)\\leq\\delta\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma G.4 (Bernstein\u2019s inequality ([59], Theorem 8)) Let $\\mathcal{M}_{1},\\mathcal{M}_{2}\\ldots$ . be a martingale difference sequence. Assume there exists deterministic sequences $c_{1},c_{2},\\ldots$ and $V_{1},V_{2},.\\,.$ . such that $|\\mathcal{M}_{k}|\\le c_{k}$ and $\\mathbb{E}[\\sum_{k^{\\prime}\\in[k]}\\mathcal{M}_{k^{\\prime}}^{2}]\\le V_{k}$ for all $k$ . Then, $\\begin{array}{r}{i f\\sqrt{\\frac{\\log(2/\\delta)}{(e-2)V_{n}}}\\le\\frac{1}{c_{n}}}\\end{array}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\Big|\\sum_{k\\in[n]}\\mathcal{M}_{k}\\Big|\\geq\\sqrt{(e-2)V_{n}\\log(2/\\delta)}\\Big)\\leq\\delta\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: As discussed in our abstract and Section 1, our main contributions include (i) introducing a novel fair recommendation framework, Problem (FAIR), that flexibly balances multi-stakeholder interests, and (ii) introducing a low-regret algorithm FORM that simultaneously learns user data and performs fair recommendation in a dynamic online setting, whose efficacy is further validated in a real-world case study. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We have clearly stated the assumptions needed for our theoretical results, the computational complexity of the proposed algorithm, and the setups adopted for our numerical experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have provided all necessary assumptions and proofs for our theoretical results (e.g., see Section D for the proof our main result, Theorem 3.1). We have also provided proofs for any theorems and lemmas that show up in our appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have provided code and data for our numerical experiments in the supplementary materials. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 35}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have provided code and data for our numerical experiments in the supplementary materials. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have detailed the setups of our numerical experiments in Section 4. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: As stated in Section 4, all results are averaged over 10 simulations. In our figures, the line indicates the mean value and the shaded region shows mean $\\pm{\\mathrm{~std}}/{\\sqrt{10}}$ . Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: As stated in Section 4, all algorithms were implemented in Python 3.7 and run on a MacBook with a 1.4 GHz Quad-Core Intel Core i5 processor. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: This work contributes to enhancing fairness in algorithmic recommendation decisions, and we do not anticipate any negative societal impact from the research. If any concerns arise regarding the use of sensitive user or item attributes in determining user types, the platform can exclude such information and instead rely on non-sensitive attributes, as noted in Footnote 1. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All baselines used in our numerical experiments have been cited. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]