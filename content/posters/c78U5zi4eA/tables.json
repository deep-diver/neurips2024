[{"figure_path": "c78U5zi4eA/tables/tables_4_1.jpg", "caption": "Table 1: Performance of LightGCN variants.", "description": "This table presents the performance of different LightGCN variants on the Yelp-2018 and Gowalla datasets, comparing the full LightGCN model to variants without gradient updates from message passing, without neighbor information from message passing, and without message passing at all.  The results demonstrate the relative contribution of forward pass information and gradient updates to the overall performance improvement achieved through message passing in LightGCN.", "section": "3 How Does Message Passing Improve Collaborative Filtering?"}, {"figure_path": "c78U5zi4eA/tables/tables_7_1.jpg", "caption": "Table 2: Recommendation performance (i.e., NDCG@20 and Recall@20) of all models across users with different numbers of interactions. The lower percentile indicates the set of nodes whose degrees are ranked in the lower 30% population. Bold and underline indicate the best and second best model respectively. LightGCN and MF are trained with DirectAU [57].", "description": "This table presents the NDCG@20 and Recall@20 scores for various recommendation models, categorized by user interaction count (degree) and overall performance.  It compares the performance of different models across users with varying numbers of interactions. The table highlights the top-performing models for low-degree users (those with fewer interactions) and all users, showcasing the effectiveness of TAG-CF in improving recommendation performance, especially for users with few interactions.", "section": "5.2 Performance Improvement to Matrix Factorization Methods"}, {"figure_path": "c78U5zi4eA/tables/tables_7_2.jpg", "caption": "Table 3: Running time (1 \u00d7 103 seconds) for MF methods and TAG-CF. Time % is the percentage of running time TAG-CF takes w.r.t. the time for corresponding MF methods. Speed\u2191 refers to the ratio of running times between training-time aggregation (i.e., LightGCN) and TAG-CF. All training steps are timed and terminated by an early stopping strategy (see Appendix C).", "description": "This table presents the running time comparison between different methods.  It shows the running time of MF methods (ENMF, UltraGCN, LightGCN) and the additional time taken by TAG-CF. The \"Time %\" column indicates the percentage increase in runtime introduced by TAG-CF for each method. The \"Speed\u2191\" column shows how much faster TAG-CF is compared to the training-time aggregation method, LightGCN. All methods use an early stopping strategy to prevent overfitting.", "section": "5 Experiments"}, {"figure_path": "c78U5zi4eA/tables/tables_8_1.jpg", "caption": "Table 2: Recommendation performance (i.e., NDCG@20 and Recall@20) of all models across users with different numbers of interactions. The lower percentile indicates the set of nodes whose degrees are ranked in the lower 30% population. Bold and underline indicate the best and second best model respectively. LightGCN and MF are trained with DirectAU [57].", "description": "This table compares the performance of various collaborative filtering methods, including those enhanced with graph neural networks, across different user groups categorized by their number of interactions (degree).  It shows NDCG@20 and Recall@20 scores for low-degree users (bottom 30%) and all users.  The best and second-best performing models are highlighted for each category.  LightGCN and standard Matrix Factorization (MF) models were both trained using the DirectAU loss function.", "section": "5.2 Performance Improvement to Matrix Factorization Methods"}, {"figure_path": "c78U5zi4eA/tables/tables_15_1.jpg", "caption": "Table 2: Recommendation performance (i.e., NDCG@20 and Recall@20) of all models across users with different numbers of interactions. The lower percentile indicates the set of nodes whose degrees are ranked in the lower 30% population. Bold and underline indicate the best and second best model respectively. LightGCN and MF are trained with DirectAU [57].", "description": "This table presents the NDCG@20 and Recall@20 scores for various recommendation models, categorized by user interaction counts (low-degree vs. overall).  It compares different collaborative filtering (CF) methods, both with and without graph-based enhancements (TAG-CF). The results highlight performance differences based on the number of user interactions and the impact of the proposed TAG-CF method.", "section": "5.2 Performance Improvement to Matrix Factorization Methods"}, {"figure_path": "c78U5zi4eA/tables/tables_16_1.jpg", "caption": "Table 7: Improvement of TAG-CF+ to TAG-CF. Degree cutoffs are selected according to Figure 3.", "description": "This table presents the performance improvement achieved by TAG-CF+ over TAG-CF for different datasets (Yelp-2018, Gowalla, Amazon-book, Anime) and different training objectives (BPR, DirectAU). The degree cutoffs used for TAG-CF+ were selected based on the results shown in Figure 3. The table reports the percentage improvement in NDCG@20 and Recall@20, as well as the percentage reduction in running time, for each dataset and objective function.", "section": "5.5 Performance w.r.t. User Degree"}, {"figure_path": "c78U5zi4eA/tables/tables_16_2.jpg", "caption": "Table 8: Improvement (NDCG@20) brought by TAG-CF at different degree cutoffs and upsampling rates on ML-1M.", "description": "This table shows the NDCG@20 improvement brought by applying TAG-CF at different user degree cutoffs and varying the upsampling rate of low-degree users in the MovieLens-1M dataset.  It demonstrates how the performance gain from TAG-CF changes as the proportion of low-degree users increases during training.", "section": "5.5 Performance w.r.t. User Degree"}, {"figure_path": "c78U5zi4eA/tables/tables_17_1.jpg", "caption": "Table 2: Recommendation performance (i.e., NDCG@20 and Recall@20) of all models across users with different numbers of interactions. The lower percentile indicates the set of nodes whose degrees are ranked in the lower 30% population. Bold and underline indicate the best and second best model respectively. LightGCN and MF are trained with DirectAU [57].", "description": "This table presents the NDCG@20 and Recall@20 scores for various recommendation models across different user groups categorized by the number of interactions.  It compares models with and without TAG-CF, highlighting the performance improvement achieved by TAG-CF, especially for low-degree (less active) users.  The best and second-best performing models are indicated for each metric and user group.", "section": "5.2 Performance Improvement to Matrix Factorization Methods"}]