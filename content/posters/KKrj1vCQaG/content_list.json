[{"type": "text", "text": "RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhicheng $\\mathbf{Sun}^{1}$ , Zhenhao Yang3, Yang $\\mathbf{Jin}^{1}$ , Haozhe $\\mathbf{Chi^{1}}$ , Kun $\\mathbf{Xu^{2}}$ , $\\mathbf{Kun\\,Xu^{2}}$ , Liwei Chen2, Hao Jiang1, Yang Song, Kun Gai2, Yadong $\\mathbf{M}\\mathbf{u}^{1}\\cdot$ \u2217 ", "page_idx": 0}, {"type": "text", "text": "1Peking University, 2Kuaishou Technology, 3University of Electronic Science and Technology of China {sunzc,myd}@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Customizing diffusion models to generate identity-preserving images from userprovided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at https://github.com/feifeiobama/RectifID. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in diffusion models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020) have ignited a surge of research into their customizability. A prominent example is personalized image generation, which aims to integrate user-defined subjects into the generated image. This plays a pivotal role in AI art creation, empowering users to produce identity-consistent images with greater customizability beyond text prompts. Nevertheless, there remain significant challenges in accurately preserving the subject\u2019s identity and being flexible to a variety of personalization needs. ", "page_idx": 0}, {"type": "text", "text": "Existing personalization methods are limited in these two aspects, as they require an extra finetuning or pre-training stage. For example, the pioneering works (Gal et al., 2023; Ruiz et al., 2023a) finetune conditional embeddings or model parameters per subject, resulting in suboptimal efficiency and identity consistency due to lack of domain knowledge. On the other hand, the recently prevailing tuning-free methods (Wei et al., 2023; Ye et al., 2023; Li et al., 2024; Wang et al., 2024b) pre-train a conditioning adapter to encode subject features into the generation process. However, their models must be pre-trained on extensive domain-specific data, e.g. LAION-Face 50M (Zheng et al., 2022), which is costly in the first place, and cannot be transferred flexibly across different data domains, e.g. from human faces to common live subjects and objects, and even to multiple subjects. ", "page_idx": 0}, {"type": "text", "text": "To address both challenges of identity consistency and flexibility, we advocate a training-free approach that utilizes the guidance of a pre-trained discriminator without extra training of the generative model. This methodology is well-known as classifier guidance (Dhariwal and Nichol, 2021), which modifies an existing denoising process using the gradient from a pre-trained classifier. The rationale behind our exploitation is twofold: first, it directly harnesses the discriminator\u2019s domain knowledge for identity preservation, which may be a cost-effective substitute for training on domain-specific datasets; secondly, keeping the diffusion model intact allows for plug-and-play combination with different discriminators, as shown in Fig. 1, which enhances its flexibility across various personalization tasks. However, the original classifier guidance is largely limited in the reliance on a special classifier trained on noised inputs. Despite recent efforts to approximate the guidance (Kim et al., 2022a; Liu et al., 2023b; Wallace et al., 2023; Ben-Hamu et al., 2024), they have mainly focused on computational efficiency, and have yet to achieve sophisticated performance on personalization tasks. ", "page_idx": 0}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/8580b4d4ff7430ccaecd97a6c7d14c2edc77509f9dfe53272a636a91220b55dd.jpg", "img_caption": ["Figure 1: Illustration of training-free classifier guidance. Left: an off-the-shelf discriminator can be reused to steer the existing diffusion model, e.g. rectified flow, to generate identity-preserving images. Right: personalized image generation results for human faces and objects using our proposed method. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Technically, to extend classifier guidance for personalized image generation, our work builds on a recent framework named rectified flow (Liu et al., 2023a) featuring strong theoretical properties, e.g. the straightness of its sampling trajectory. By approximating the rectified flow to be ideally straight, the original classifier guidance is reformulated as a simple fixed-point problem concerning only the trajectory endpoints, thus naturally overcoming its reliance on a special noise-aware classifier. This allows flexible reuse of image discriminators for identity preservation in personalization tasks. Furthermore, we propose to anchor the classifier-guided flow trajectory to a reference trajectory to improve the stability of its solving process, which provides a convergence guarantee in theoretical scenarios and proves even more crucial in practice. Lastly, a clear connection is established between our derived anchored classifier guidance and the existing approximation practices. ", "page_idx": 1}, {"type": "text", "text": "The derived method is implemented for a practical class of rectified flow (Yan et al., 2024) assumed to be piecewise straight, in combination with face or object discriminators. This provides flexibility for a range of personalization tasks on human faces, live subjects, certain objects, and multiple subjects. Extensive experimental results on these tasks clearly validate the effectiveness of our approach. Our contributions are summarized as follows: (1) We propose a training-free approach to flexibly personalize rectified flow, based on a fixed-point formulation of classifier guidance. (2) To improve its stability, we anchor the flow trajectory to a reference trajectory, which yields a theoretical convergence guarantee when the flow is ideally straight. (3) The proposed method is implemented on a relaxed piecewise rectified flow and demonstrates advantageous results in various personalization tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Personalized image generation studies incorporating user-specified subjects into the text-to-image generation pipeline. To preserve the subject\u2019s identity, the seminal works Textual Inversion (Gal et al., 2023) and DreamBooth (Ruiz et al., 2023a) finetune conditional embeddings or model parameters for each subject, which imposes high computational costs. Subsequent literature resorts to more efficient parameters (Hu et al., 2022; Han et al., 2023; Yuan et al., 2023) or a pre-trained subject encoder (Wei et al., 2023; Ye et al., 2023) to allow personalization within a few minutes or even without tuning. At the other end, a recent trend is the reuse of existing discriminators to improve identity consistency, such as extracting discriminative face features as the condition (Ye et al., 2023; Wang et al., 2024b) or as a training objective for the encoder (Peng et al., 2024; Gal et al., 2024; Guo et al., 2024). However, these models require extensive pre-training on domain-specific data, e.g. LAION-Face 50M (Zheng et al., 2022). In contrast, our method is a training-free approach that exploits existing discriminators based on the recent rectified flow model, allowing flexible personalization for a variety of tasks. ", "page_idx": 1}, {"type": "text", "text": "Rectified flow is an instance of flow-based generative models (Song et al., 2021; Xu et al., 2022; Liu et al., 2023a; Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023). They aim to learn a velocity field $\\pmb{v}$ that maps random noise $z_{0}\\sim\\pi_{0}$ to samples from a complex distribution $z_{1}\\sim\\pi_{\\mathrm{data}}$ via an ordinary differential equation (ODE): ", "page_idx": 2}, {"type": "equation", "text": "$$\nd{\\boldsymbol{z}}_{t}={\\boldsymbol{v}}({\\boldsymbol{z}}_{t},t)d t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Instead of directly solving the ODE (Chen et al., 2018), rectified flow (Liu et al., 2023a) simply learns a linear interpolation between the two distributions by minimizing the following objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{v}}\\int_{0}^{1}\\mathbb{E}\\left[\\|(\\pmb{z}_{1}-\\pmb{z}_{0})-\\pmb{v}(\\pmb{z}_{t},t)\\|^{2}\\right]d t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This procedure straightens the flow trajectory and thus allows faster sampling. Ideally, a well-trained rectified flow is a straight flow with uniform velocity $\\pmb{v}(z_{t},t)=\\pmb{v}(z_{0},\\bar{0})$ following: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{z}_{t}=\\boldsymbol{z}_{0}+\\boldsymbol{v}(\\boldsymbol{z}_{t},t)t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Recently, rectified flow has shown promising efficiency (Liu et al., 2024b) and quality (Esser et al., 2024; Yan et al., 2024) in text-to-image generation. Our work extends its capabilities and theoretical properties to personalized image generation via classifier guidance. ", "page_idx": 2}, {"type": "text", "text": "Classifier guidance, initially proposed for class-conditioned diffusion models (Dhariwal and Nichol, 2021), introduces a test-time mechanism to adjust the predicted noise $\\epsilon(z_{t},t)$ based on the guidance from a classifier. Given condition $c$ and classifier output $p(c|z_{t})$ , the adjustment is formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}(z_{t},t)=\\epsilon(z_{t},t)+s\\cdot\\sigma_{t}\\nabla_{z_{t}}\\log p(c|z_{t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where s denotes the guidance scale, and $\\sigma_{t}$ is determined by the noise schedule. Noteworthy, the condition $c$ is not restricted to class labels, but can be extended to text (Nichol et al., 2022) and beyond. However, it is largely limited by the reliance on a noise-aware classifier for noised inputs $\\boldsymbol{z}_{t}$ , which restricts the use of most pre-trained discriminators that only predict the likelihood $p(c|z_{1})$ on clean images. Consequently, its usefulness is limited in practice. See Appendix B for more related work. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This work aims at customizing rectified flow with classifier guidance. We show that the above limit of classifier guidance may be solved with a simple fixed-point solution for rectified flow (Section 3.1). To improve its stability, Section 3.2 proposes a new anchored classifier guidance with a convergence guarantee. Lastly, the implementation and applications are described in Sections 3.3 and 3.4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Classifier Guidance for Rectified Flow ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section first derives the vanilla classifier guidance for rectified flow, and then present an initial attempt to remove the need for the noise-aware classifier $p(c|z_{t})$ , which is based on a new fixed-point solution of classifier guidance assuming that the rectified flow is ideally straight. ", "page_idx": 2}, {"type": "text", "text": "The classifier guidance can be derived as modifying the potential associated with the rectified flow. According to the Helmholtz decomposition, a velocity field $v$ may be decomposed into: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{v}(z_{t},t)=\\nabla_{z_{t}}\\phi(z_{t},t)+\\pmb{r}(z_{t},t),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi$ is a scaler potential and $\\pmb{r}$ is a divergence-free rotation field. They can be determined by solving the Poisson\u2019s equation $\\nabla^{2}\\phi(z_{t},t)=\\bar{\\nabla}\\cdot\\mathbf{v}(z_{t},t)$ , but this is beyond our focus. We directly add a new potential proportional to the log-likelihood to simulate classifier guidance, as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol v}(\\boldsymbol z_{t},t)=\\nabla_{\\boldsymbol z_{t}}\\left[\\phi(\\boldsymbol z_{t},t)+\\boldsymbol s\\cdot\\log p(\\boldsymbol c|\\boldsymbol z_{t})\\right]+\\boldsymbol r(\\boldsymbol z_{t},t),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s$ denotes the guidance scale, and\u02c6is used to distinguish the new flow from the original one. Subtracting the above two equations yields the vanilla classifier guidance, similar in form to Eq. (4): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{v}(z_{t},t)=v(z_{t},t)+s\\cdot\\nabla_{z_{t}}\\log p(c|z_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While this classifier guidance should allow for test-time conditioning of rectified flow, it cannot be applied in the absence of noise-aware classifier $p(c|z_{t})$ . In the following, we show that this limitation may be overcome by exploiting the straightness property of rectified flow. ", "page_idx": 2}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/d0fd16758e54accd993bcaa54209e93c80ec5e17c78f81dbc4d6f0f346276d51.jpg", "img_caption": ["Figure 2: Illustration of anchored classifier guidance for rectified flow. Left: we propose to guide the flow trajectory while implicitly enforcing it to flow straight and stay close to a reference trajectory. Right: comparison of the new trajectory with the reference trajectory (in the last three sampling steps). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Attempt to bypass noise-aware classifier. We make a key observation that the intermediate classifier guidance $\\nabla_{z_{t}}\\log{p(c|z_{t})}$ can be circumvented by approximating the new flow trajectory to be straight (an ideal guidance should preserve the properties of rectified flow) and focusing on the endpoint $z_{1}$ . Formally, substituting $t=1$ in Eqs. (3) and (7) allows skipping any intermediate guidance terms: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{1}=z_{0}+\\hat{v}(z_{1},1)}\\\\ &{\\quad=z_{0}+v(z_{1},1)+s\\cdot\\nabla_{z_{1}}\\log p(c|z_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Interestingly, this turns out to be a fixed-point problem w.r.t. $z_{1}$ , suggesting that the classifier-guided flow trajectory could be solved iteratively by numerical methods such as the fixed-point iteration, without knowing the noise-aware classifier. This greatly enhances the flexibility of classifier guidance to a variety of off-the-shelf image discriminators. However, our further analysis reveals both empirical (Section 4.3) and theoretical evidence questioning the convergence of this iterative approach: ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. There exist Lipschitz continuous functions $\\pmb{v}(\\pmb{z}_{1},1)$ and $\\nabla_{z_{1}}\\log{p(c|z_{1})}$ , such that the fixed-point iteration for solving the target trajectory based on Eq. (8) is not guaranteed to converge by the Banach fixed-point theorem (Banach, 1922), irrespective of the choice of $s>0$ . ", "page_idx": 3}, {"type": "text", "text": "Proof. Consider the following construction. Let $\\pmb{v}(\\pmb{z}_{1},1)$ and $\\nabla z_{1}\\log p(c|z_{1})$ be identical functions with a Lipschitz constant greater than 1. Then, the Lipschitz constant of the right-hand side of the fixed-point equation is greater than 1 for any $s>0$ . This violates the Banach fixed-point theorem\u2019s requirement for a Lipschitz constant strictly less than 1, thus convergence is not guaranteed. \u518f\u53e3 ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 shows that the derived fixed-point solution may not always be practical. Intuitively, even with a small perturbation at $z_{1}$ , the target flow trajectory estimated by Eq. (8) could diverge significantly after iterated updates, which hinders the controllability of rectified flow. This motivates us to anchor the target flow trajectory to a reference trajectory to stabilize its solving process. ", "page_idx": 3}, {"type": "text", "text": "3.2 Anchored Classifier Guidance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section establishes a new type of classifier guidance based on a reference trajectory. The idea is to constrain the new trajectory to be straight and near the reference trajectory, as illustrated in Fig. 2. It provides a better convergence guarantee and a certain degree of interpretability. ", "page_idx": 3}, {"type": "text", "text": "Let $\\hat{z}_{t}$ and $\\boldsymbol{z}_{t}$ represent two flow trajectories originating from the common starting point $\\scriptstyle z_{0}$ with or without classifier guidance. The symbol \u02c6 denotes the new trajectory with classifier guidance. Assuming the two trajectories are close and straight (ideally preserving the characteristics of rectified flow), their difference can be estimated based on Eq. (7) and the first-order Taylor expansion: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}(\\hat{z}_{t},t)-v(z_{t},t)=v(\\hat{z}_{t},t)+s\\cdot\\nabla_{\\hat{z}_{t}}\\log p(c|\\hat{z}_{t})-v(z_{t},t)}\\\\ &{\\qquad\\qquad\\qquad\\approx\\left[\\nabla_{z_{t}}v(z_{t},t)\\right](\\hat{z}_{t}-z_{t})+s\\cdot\\nabla_{\\hat{z}_{t}}\\log p(c|\\hat{z}_{t})}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\left[\\nabla_{z_{t}}v(z_{t},t)t\\right]\\left(\\hat{v}(\\hat{z}_{t},t)-v(z_{t},t)\\right)+s\\cdot\\nabla_{\\hat{z}_{t}}\\log p(c|\\hat{z}_{t})}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\left[I-\\nabla_{z_{t}}z_{0}\\right]\\left(\\hat{v}(\\hat{z}_{t},t)-v(z_{t},t)\\right)+s\\cdot\\nabla_{\\hat{z}_{t}}\\log p(c|\\hat{z}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the final step is derived from Eq. (3). From here, a new form of classifier guidance is obtained: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{\\hat{v}}(\\boldsymbol{\\hat{z}}_{t},t)=\\boldsymbol{v}(\\boldsymbol{z}_{t},t)+\\boldsymbol{s}\\cdot\\left[\\nabla_{\\boldsymbol{z}_{0}}\\boldsymbol{z}_{t}\\right]\\nabla_{\\boldsymbol{\\hat{z}}_{t}}\\log{p(c|\\boldsymbol{\\hat{z}}_{t})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This new classifier guidance anchors the target velocity to a predetermined reference velocity $\\pmb{v}(\\pmb{z}_{t},t)$ that is dependent only on $t$ and irrelevant to the current state $\\hat{z}_{t}$ , thereby constraining the target flow trajectory near the reference trajectory and improving its controllability. Next, we extend its applicability to the more common scenarios where the noise-aware classifier $p(c|\\hat{z}_{t})$ is absent. ", "page_idx": 4}, {"type": "text", "text": "Bypassing noise-aware classifier. To circumvent the intermediate classifier guidance, we follow the previous practice of substituting $t=1$ into Eqs. (3) and (10), yielding a fixed-point problem w.r.t. $\\hat{z}_{1}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{z}_{1}=z_{1}+s\\cdot\\left[\\nabla_{z_{0}}z_{1}\\right]\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As can be seen, the target endpoint $\\hat{z}_{1}$ is also anchored to a known reference point $z_{1}$ , which should enhance its stability in the solving process via fixed-point iteration or alternative numerical methods. Below, we exemplify its favorable theoretical property using the fixed-point iteration: ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. Suppose $\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1})$ is Lipschitz continuous w.r.t. $\\hat{z}_{1}$ , the fixed-point iteration to solve the target trajectory by Eq. (11) exhibits at least linear convergence with a properly chosen s. ", "page_idx": 4}, {"type": "text", "text": "Proof. Denote the Frobenius norm of $\\nabla_{z_{0}}z_{1}$ as $L_{1}$ , and the Lipschitz constant of $\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1})$ as $L_{2}$ . The Lipschitz constant of the right side of the equation w.r.t. $\\hat{z}_{1}$ is upper bounded by $s\\cdot L_{1}\\cdot L_{2}$ . By choosing a sufficiently small $s\\dot{<}1/(L_{1}\\cdot L_{2})$ , the Lipschitz constant of the right side is reduced to less than 1, thus ensuring linear convergence by the Banach fixed-point theorem. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Interpretation of new classifier guidance. In addition to the above convergence guarantee, our new classifier guidance can be interpreted by connecting with gradient backpropagation. From Eq. (10) one could obtain an estimate of the intermediate classifier guidance (see Appendix A for derivation): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\hat{z}_{t}}\\log p(c|\\hat{z}_{t})=[\\nabla_{z_{t}}z_{1}]\\,\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This suggests that our method is secretly estimating the intermediate classifier guidance with gradient backpropagation. While this is implicitly assumed or directly used in recent works that adapt classifier guidance to flow-based models (Wallace et al., 2023; Liu et al., 2023b; Ben-Hamu et al., 2024), it is explicitly derived here based on a very different assumption (the straightness of the flow trajectory). Such a connection helps to rationalize both our adopted assumption and the existing practice. ", "page_idx": 4}, {"type": "text", "text": "3.3 Practical Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Extension to piecewise rectified flow. The above analyses are performed based on the assumption that the rectified flow is well-trained and straight, which is often not the case in reality. In fact, existing rectified flow usually require multiple sampling steps due to the inherent curvature in the flow trajectory. Inspired by Yan et al. (2024), we adopt a relaxed assumption during implementation that the rectified flow is piecewise linear. Let there be $K$ time windows $\\{[t_{k},t_{k-1}^{-})\\}_{k=K}^{1}$ where $1=t_{K}>\\cdot\\cdot\\cdot>t_{k}>t_{k-1}>\\cdot\\cdot\\cdot>t_{0}=0$ , and the flow trajectory is assumed straight within each time window, then the inference procedure can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{t}=z_{t_{k-1}}+v(z_{t},t)(t-t_{k-1}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $k$ is the index of the time window $[t_{k},t_{k-1})$ that $t$ belongs to. Note that this framework is also compatible with the vanilla rectified flow by setting $K$ to the number of sampling steps. ", "page_idx": 4}, {"type": "text", "text": "The previously derived fixed-point iteration in Eq. (11) cannot be applied directly, since its assumption that the target and reference trajectory segments share the same starting point (e.g. $\\hat{z}_{t_{k-1}}=z_{t_{k-1}})$ may be violated after updates. A quick fix is to reinitialize the reference trajectory every round with predictions for updated target starting points. This allows to formulate the following problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{z}_{t_{k}}=z_{t_{k}}^{e}+s\\cdot\\left[\\nabla_{z_{t_{k-1}}}z_{t_{k}}^{e}\\right]\\nabla_{\\hat{z}_{t_{k}}}\\log p(c|\\hat{z}_{t_{k}})}\\\\ &{\\qquad=z_{t_{k}}^{e}+s\\cdot\\left[\\nabla_{z_{t_{k-1}}}z_{1}^{e}\\right]\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the last step is obtained by recursively applying Eq. (12) to backpropagate the guidance signal, and a superscript $e$ is introduced to denote the endpoint of the previous trajectory segment, as the above fix may disconnect different segments of the reference trajectory. Meanwhile, a straight-through estimator (Bengio et al., 2013) is applied to allow computing the Jacobian across different trajectory segments by estimating the Jacobian between the adjacent points $z_{t_{k}}$ and $\\boldsymbol{z}_{t_{k}}^{e}$ with $\\boldsymbol{\\mathit{I}}$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Anchored Classifier Guidance ", "page_idx": 5}, {"type": "text", "text": "Input: rectified flow $v$ , classifier $p(c|\\cdot)$ , sampling steps $K$ , iterations $N$ . Initialize reference trajectory zt[0k] from v. Initialize target trajectory z\u02c6t[0k] $\\hat{z}_{t_{k}}^{[0]}\\gets z_{t_{k}}^{[0]}$ for $i\\leftarrow0$ to $N-1$ do ", "page_idx": 5}, {"type": "text", "text": "Update reference trajectory with predicted starting points $z_{t_{k}}^{[i+1]}$ .   \nUpdate target trajectory z\u02c6t[ik+ $\\hat{z}_{t_{k}}^{[i+1]}$ with classifier output $p(c|\\hat{\\boldsymbol{z}}_{1}^{[i]})$ . ", "page_idx": 5}, {"type": "text", "text": "Output: target trajectory $\\hat{z}_{t_{k}}^{[N]}$ subject to condition $c$ ", "page_idx": 5}, {"type": "text", "text": "Solving target flow trajectory. The target trajectory under classifier guidance, subject to Eq. (14), can be estimated iteratively by starting with $\\hat{z}_{t_{k}}^{[0]}=z_{t_{k}}^{[0]}$ zt[0k] and performing the following iterations: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{t_{k}}^{[i+1]}=z_{t_{k}}^{[i]}+\\underline{{z}}_{t_{k}}^{e[i+1]}-z_{t_{k}}^{e[i]}+\\hat{\\underline{{z}}}_{t_{k}}^{[i]}-\\underline{{z}}_{t_{k}}^{e[i]},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{z}_{t_{k}}^{[i+1]}=z_{t_{k}}^{e[i+1]}+s\\cdot\\left[\\nabla_{z_{t_{k-1}}^{[i+1]}}z_{1}^{e[i+1]}\\right]\\nabla_{\\hat{z}_{1}^{[i]}}\\log p(c|\\hat{z}_{1}^{[i]}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the superscript $[i]$ is used to indicate the target and reference trajectories at the $i$ -th iteration. Specifically, Eq. (15) implements the prediction of updated target starting points by extrapolating from history updates, and Eq. (16) tackles the derived problem. Note that there are more sophisticated methods for predicting target starting points and solving this problem, e.g. quasi-Newton methods, but we opt for simplicity here and leave their exploration to future work. The complete procedure for implementing the proposed classifier guidance is summarized by Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "3.4 Applications ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The proposed algorithm is flexible for various personalized image generation tasks on human faces and common subjects. Given a reference image $z_{\\mathrm{ref}}$ and our generated image $\\hat{z}_{1}$ , we use their feature similarity on an off-the-shelf discriminator $f$ , e.g. the face specialist ArcFace (Deng et al., 2019) or a self-supervised backbone DINOv2 (Oquab et al., 2023), as classifier guidance. In addition, to improve the guidance signal, a face detector or an open-vocabulary object detector $g$ is employed to locate the identity-relevant region for feature extraction. Formally, the classifier output is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\np(c|\\hat{z}_{1}^{[i]})=\\sin\\left(f\\circ g(\\hat{z}_{1}^{[i]}),f\\circ g(z_{\\mathrm{ref}})\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "More details are described in Appendix C. Notably, both configurations can be flexibly extended to a multi-subject scenario by incorporating a bipartite matching step between multiple detected subjects. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. Our method does not involve training data, as it operates only at test time. For face-centric evaluation, we follow Pang et al. (2024) to evaluate on 20 prompts with the first 200 images from CelebA-HQ (Liu et al., 2015; Karras et al., 2018) as reference images. For subject-driven generation, we conduct qualitative studies on a subset of examples from the DreamBooth dataset (Ruiz et al., 2023b), spanning 10 subjects across two live subject categories and three object categories. ", "page_idx": 5}, {"type": "text", "text": "Metrics. Three metrics are considered: identity similarity, prompt consistency, and computation time. The first two are measured using an ArcFace model (Deng et al., 2019) and CLIP encoders (Radford et al., 2021), while the latter is tested on an NVIDIA A800 GPU. We reproduce the latest methods IP-Adapter (Ye et al., 2023), PhotoMaker (Li et al., 2024), and InstantID (Wang et al., 2024b) for a comprehensive comparison, and also include the existing baselines in Pang et al. (2024). ", "page_idx": 5}, {"type": "text", "text": "Implementation details. We experiment with a frozen piecewise rectified flow (Yan et al., 2024) finetuned from Stable Diffusion 1.5 (Rombach et al., 2022) with 4 equally divided time windows. The number of sampling steps is set to a minimum $K=4$ given the memory overhead of backpropagation. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Quantitative comparison for face-centric personalization. The inference time is measured in seconds on an NVIDIA A800. Unlike the previous state-of-the-art methods that require training on large face datasets (the number of images is listed for reference), our method achieves superior performance in a training-free manner, by exploiting the guidance from an off-the-shelf discriminator. ", "page_idx": 6}, {"type": "table", "img_path": "KKrj1vCQaG/tmp/a637eea0c9952b7d91028510a3d61fc7bf7eb23fcb72613cb2abc7f2b0ff887e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/c33c78c12825d9ba7b4a3f982e09c1d14add1d7fba03446e26386094598fd9ba.jpg", "img_caption": ["Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "A naive implementation takes 14GB of GPU memory, which ftis on a range of consumer-grade GPUs. More results on alternative rectified flows can be found in Appendix D.4. For hyperparameters, the guidance scale is fixed to $s=1$ in quantitative evaluation. Meanwhile, for stability, the gradient is normalized following Karunratanakul et al. (2024). The number of iterations is set to $N=100$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Face-centric personalization. Table 1 and Fig. 3 compare our method (denoted RectifID) with extensive baselines. Overall, our training-free approach achieves state-of-the-art performance in quantitative evaluations. Specifically, we observe that: (1) our SD 1.5-based implementation yields the highest identity similarity of all, and leads in prompt consistency among SD 1.x-based methods. It is also computationally efficient, e.g. taking less time than existing tuning-based methods, and outperforming the training-based IP-Adapter (Ye et al., 2023) in a near inference time of 9 seconds vs. 2 seconds. (2) By simply replacing the base diffusion model with SD 2.1 at its default image size, our prompt consistency further surpasses SDXL (Podell et al., 2024)-based models. Note, however, that the rest of this paper still uses SD 1.5 for a fair comparison to SD 1.x-based baselines in various personalization tasks, excluding potential improvements from using better base models. (3) In general, our method takes a big step towards bridging the substantial performance gap with training-based personalization methods by exploring the effectiveness of training-free classifier guidance. ", "page_idx": 6}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/42f04c72d6f6330e1775fbd774e66760229aa28c70025e55ef78ac934c3983ec.jpg", "img_caption": ["Figure 4: Qualitative comparison for subject-driven generation. \u2217denotes finetuned with multiple images of the target subject to achieve sufficient identity consistency. See Fig. 13 for more samples. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/7d62fad6a94eb73cd1202239c60e1590140ee8ea8b9e0d5c2fd38c47a05cae64.jpg", "img_caption": ["Figure 5: Qualitative comparison for multi-subject personalization. See Fig. 14 for more samples. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For face-centric qualitative comparison in Fig. 3, our method remains advantageous as its generated images by the guidance of the face discriminator exhibit high identity consistency. In comparison, InstantID (Wang et al., 2024b) delivers a near level of consistency by controlling face landmarks, but sometimes distorts the face shape (the first and third images) and contains much less natural variation. More generated samples are provided in Figs. 11 and 12 in the appendix. ", "page_idx": 7}, {"type": "text", "text": "Subject-driven generation. Our approach is flexibly extended beyond human faces towards more subjects, including certain common animals and regularly shaped objects. To validate our flexibility, Fig. 4 qualitatively compares it on three cats or dogs and a regularly shaped can, where the images generated by our method achieve highly competitive identity and prompt consistency. In comparison, the state-of-the-art method Emu2 (Sun et al., 2024), as a generalist multimodal large language model, yields high identity similarity largely by reconstructing the input image, which limits its usefulness. The tuning-based Textual Inversion (Gal et al., 2023) and DreamBooth (Ruiz et al., 2023a) only work well with multiple images and exhibit inferior prompt consistency due to finetuned model parameters or prompt embeddings. See Fig. 13 in the appendix for additional results from more subjects. ", "page_idx": 7}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/7d45338ea42868a11129ca998cc02dd546c01334ae8fd65f2e1890f5412d75cf.jpg", "img_caption": ["Figure 6: Comparison with alternative designs at varying guidance scale (or learning rate) and iterations. The prompts are \u201ccave mural depicting a person\u201d and \u201ca person as a priest in blue robes\u201d. The base learning rate for gradient descent is 0.4, with momentum of 0.9 and an $\\ell_{2}$ regularizer of 1.0. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Multi-subject personalization. Our method can be further extended to multi-subject scenarios via a bipartite matching step. Figure 5 compares it to the domain experts FastComposer (Xiao et al., 2023) and Cones 2 (Liu et al., 2023c) on composing multiple faces, live subjects and objects. As can be seen, our method achieves overall advantageous identity consistency, in spite of differences in non-persistent attributes such as hairstyle. Image semantics and quality are also well preserved, as exemplified by the amusement park details in the first image, with some others even surpassing the SD 2.1-based specialized model Cones 2. More generated samples can be found in Fig. 14 in the appendix. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To justify the effectiveness of our proposed classifier guidance, Fig. 6 and Table 2 compare it with two variants: the previously derived guidance without anchor, namely using Eq. (8), and a gradient descent method on the initial noise similar to DOODL (Wallace et al., 2023) and D-Flow (Ben-Hamu et al., 2024). The figure depicts that the gradient descent is unstable (left) and converges relatively slowly (right) despite using momentum and $\\ell_{2}$ regularization. ", "page_idx": 8}, {"type": "table", "img_path": "KKrj1vCQaG/tmp/2438b20234042b719c47164f4358be5d6cfe51df415f144b1838d51581ef3be1.jpg", "table_caption": ["Table 2: Quantitative comparison with alternative designs. The number of iterations is 100, and the remaining settings for gradient descent follow Fig. 6. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "And its identity preservation is sensitive to the learning rate. Though our new fixed-point formulation allows for a more stable layout, the initial version fails to converge as the face feature keeps drifting. In contrast, our full method exhibits better stability (left) and faster convergence (right) by implicitly regularizing the flow trajectory to be close and straight. This is further supported by the quantitative comparison, where our method delivers better identity and prompt consistency than the alternatives. Further analysis for hyperparameter sensitivity is provided in Appendix D.3. ", "page_idx": 8}, {"type": "text", "text": "4.4 Generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate the generalizability of our approach to broader application scenarios, we have extended it to more controllable generation tasks by directly using the guidance functions from Universal Guidance (Bansal et al., 2024). The experimental results under the guidance of segmentation map or style image are illustrated in Fig. 7. As shown, our classifier guidance can perform both tasks without any additional tuning, faithfully following the various forms of control signals provided by the user. This confirms the adaptability of our approach for various controllable generation tasks. Additional generalization analysis of our method for broader diffusion models is presented in Appendix D.1. ", "page_idx": 8}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/a742271fdc0b08bd14d6857716a05e0f561d838cd0b96a23ae3a643293789492.jpg", "img_caption": ["(b) Style transfer "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 7: Experimental results for more controllable generation tasks. The first column shows the guidance, and the rest are the generated results. Our method is extended to various controllable generation tasks by incorporating the guidance functions from Universal Guidance (Bansal et al., 2024). ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work presents a training-free personalized image generation method using anchored classifier guidance. It extends the applicability of the original classifier guidance based on two key findings: first, by developing on a rectified flow framework assuming ideal straightness, the classifier guidance can be transformed into a new fixed-point formulation involving only clean image-based discriminators; secondly, anchoring the flow trajectory to a reference trajectory greatly improves its solving stability. The derived anchored classifier guidance allows flexible reuse of existing image discriminators to improve identity consistency, as validated by extensive experiments on various personalized image generation tasks for human faces, live subjects, certain objects, and multiple subjects. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement: This research work is supported by National Key R&D Program of China (No.   \n2022ZD0160305), a research grant from China Tower Corporation Limited, an internal grant (No.   \n2024JK28) and a grant from Beijing Aerospace Automatic Control Institute. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. A neural space-time representation for text-to-image personalization. ACM Transactions on Graphics, 42(6):1\u201310, 2023. ", "page_idx": 9}, {"type": "text", "text": "Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations, 2023. ", "page_idx": 9}, {"type": "text", "text": "Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, et al. Partial FC: Training 10 million identities on a single machine. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 1445\u20131449, 2021. ", "page_idx": 9}, {"type": "text", "text": "Stefan Banach. Sur les op\u00e9rations dans les ensembles abstraits et leur application aux \u00e9quations int\u00e9grales. Fundamenta Mathematicae, 3(1):133\u2013181, 1922. ", "page_idx": 9}, {"type": "text", "text": "Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In International Conference on Learning Representations, 2024.   \nHeli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-Flow: Differentiating through flows for controlled generation. In International Conference on Machine Learning, 2024.   \nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \nLi Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, et al. PhotoVerse: Tuning-free image customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793, 2023.   \nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, pages 6572\u20136583, 2018.   \nHyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. In Advances in Neural Information Processing Systems, pages 25683\u201325696, 2022.   \nHyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In International Conference on Learning Representations, 2023.   \nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2019.   \nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, pages 8780\u20138794, 2021.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024.   \nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations, 2023.   \nRinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. LCM-lookahead for encoder-based text-to-image personalization. In Proceedings of the European Conference on Computer Vision, 2024.   \nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2426\u20132436, 2023.   \nJia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for efficient face detection. In International Conference on Learning Representations, 2022.   \nZinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. PuLID: Pure and lightning ID customization via contrastive alignment. In Advances in Neural Information Processing Systems, 2024.   \nLigong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. SVDiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7323\u20137334, 2023.   \nYutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, WeiHsiang Liao, Yuki Mitsufuji, J Zico Kolter, Ruslan Salakhutdinov, et al. Manifold preserving guided diffusion. In International Conference on Learning Representations, 2024.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, pages 6840\u20136851, 2020.   \nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \nHaibo Jin, Shengcai Liao, and Ling Shao. Pixel-in-pixel net: Towards efficient facial landmark detection in the wild. International Journal of Computer Vision, 129(12):3174\u20133194, 2021.   \nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.   \nKorrawe Karunratanakul, Konpat Preechakul, Emre Aksan, Thabo Beeler, Supasorn Suwajanakorn, and Siyu Tang. Optimizing diffusion noise can serve as universal motion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \nGwanghyun Kim, Taesung Kwon, and Jong Chul Ye. DiffusionCLIP: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435, 2022a.   \nKihong Kim, Yunho Kim, Seokju Cho, Junyoung Seo, Jisu Nam, Kychul Lee, Seungryong Kim, and KwangHee Lee. DiffFace: Diffusion-based face swapping with facial guidance. arXiv preprint arXiv:2212.13344, 2022b.   \nNupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22691\u201322702, 2023.   \nDongxu Li, Junnan Li, and Steven Hoi. BLIP-Diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. In Advances in Neural Information Processing Systems, pages 30146\u201330166, 2023.   \nZhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. PhotoMaker: Customizing realistic human photos via stacked ID embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023.   \nHanwen Liu, Zhicheng Sun, and Yadong Mu. Countering personalized text-to-image generation with influence watermarks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12257\u201312267, 2024a.   \nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023a.   \nXingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, and Qiang Liu. FlowGrad: Controlling the output of generative ODEs with gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24335\u201324344, 2023b.   \nXingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. InstaFlow: One step is enough for high-quality diffusion-based text-to-image generation. In International Conference on Learning Representations, 2024b.   \nZhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. In Advances in Neural Information Processing Systems, pages 57500\u201357519, 2023c.   \nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3730\u20133738, 2015.   \nMatthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In Proceedings of the European Conference on Computer Vision, pages 728\u2013755, 2022.   \nChong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. DiffEditor: Boosting accuracy and flexibility on diffusion-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8488\u20138497, 2024.   \nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pages 16784\u201316804, 2022.   \nOpenAI. GPT-4V(ision) system card. https://openai.com/research/gpt-4v-system-card, 2023.   \nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023.   \nLianyu Pang, Jian Yin, Haoran Xie, Qiping Wang, Qing Li, and Xudong Mao. Cross initialization for personalized text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \nXu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. PortraitBooth: A versatile portrait model for fast identitypreserved personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations, 2024.   \nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763, 2021.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023a.   \nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. HyperDreamBooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023b.   \nAxel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In Proceedings of the European Conference on Computer Vision, 2024.   \nPatrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22522\u201322531, 2023.   \nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, pages 25278\u201325294, 2022.   \nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265, 2015.   \nJiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 32483\u201332498, 2023.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pages 11918\u201311930, 2019.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \nZhentao Tan and Yadong Mu. Learning solution-aware transformers for efficiently solving quadratic assignment problem. In International Conference on Machine Learning, 2024.   \nThanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc N Tran, and Anh Tran. AntiDreamBooth: Protecting users from personalized text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2116\u20132127, 2023.   \nBram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7280\u20137290, 2023.   \nFu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency model. In Advances in Neural Information Processing Systems, 2024a.   \nQixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. InstantID: Zero-shot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024b.   \nYinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In International Conference on Learning Representations, 2023.   \nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15943\u201315953, 2023.   \nGuangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00e9do Durand, and Song Han. FastComposer: Tuning-free multi-subject image generation with localized attention. arXiv preprint arXiv:2305.10431, 2023.   \nYilun Xu, Ziming Liu, Max Tegmark, and Tommi Jaakkola. Poisson flow generative models. In Advances in Neural Information Processing Systems, pages 16782\u201316795, 2022.   \nHanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. PeRFlow: Piecewise rectified flow as universal plug-and-play accelerator. In Advances in Neural Information Processing Systems, 2024.   \nLingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, and Ye Shi. Guidance with spherical gaussian constraint for conditional diffusion. In International Conference on Machine Learning, 2024.   \nHaotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, and Stefano Ermon. TFG: Unified training-free guidance for diffusion models. In Advances in Neural Information Processing Systems, 2024.   \nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.   \nJiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. FreeDoM: Trainingfree energy-guided conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23174\u201323184, 2023.   \nGe Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, and Huicheng Zheng. Inserting anybody in diffusion models via celeb basis. In Advances in Neural Information Processing Systems, pages 72958\u201372982, 2023.   \nYinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen. General facial representation learning in a visual-linguistic manner. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18697\u201318709, 2022.   \nYuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. Denoising diffusion models for plug-and-play image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1219\u20131229, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Detailed Derivations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Most of the equations in our paper are accompanied by their derivations, except for Eqs. (11) and (12).   \nFor the sake of completeness, their derivations are supplemented here. ", "page_idx": 15}, {"type": "text", "text": "Equation (11): We substitute $t=1$ into Eqs. (3) and (10) to obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{z}_{1}=z_{0}+\\hat{v}(\\hat{z}_{1},1)}\\\\ &{\\quad=z_{0}+v(z_{1},1)+s\\cdot\\left[\\nabla_{z_{0}}z_{1}\\right]\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1})}\\\\ &{\\quad=z_{1}+s\\cdot\\left[\\nabla_{z_{0}}z_{1}\\right]\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Equation (12): By applying Eq. (10) twice and utilizing the straightness property of rectified flow, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\hat{z}_{t}}\\log p(c|\\hat{z}_{t})=1/s\\cdot\\left[\\nabla_{z_{t}}z_{0}\\right]\\left(\\hat{v}(\\hat{z}_{t},t)-v(z_{t},t)\\right)}\\\\ &{\\qquad\\qquad\\qquad=1/s\\cdot\\left[\\nabla_{z_{t}}z_{0}\\right]\\left(\\hat{v}(\\hat{z}_{1},1)-v(z_{1},1)\\right)}\\\\ &{\\qquad\\qquad\\quad=\\left[\\nabla_{z_{t}}z_{0}\\right]\\left[\\nabla_{z_{0}}z_{1}\\right]\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1})}\\\\ &{\\qquad\\qquad\\quad=\\left[\\nabla_{z_{t}}z_{1}\\right]\\nabla_{\\hat{z}_{1}}\\log p(c|\\hat{z}_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Related Work on Classifier Guidance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since the proposal of classifier guidance (Dhariwal and Nichol, 2021) which uses a special noiseaware classifier as training-free guidance for diffusion models, many efforts have been made in order to extend its applicability to off-the-shelf loss guidance. They can be grouped into three categories: (1) Early literature focuses on simpler objectives for linear inverse problems such as image superresolution, deblurring, and inpainting (Chung et al., 2022, 2023; Wang et al., 2023; Zhu et al., 2023). (2) These methods can be extended to more complex discriminators through various approximations. Yu et al. (2023); Song et al. (2023) use Tweedie\u2019s formula and Monte Carlo method, respectively, to estimate the integrated classifier guidance. Bansal et al. (2024); He et al. (2024) perform updates directly in the clean data space, with the latter imposing an additional manifold constraint. Similarly, Gaussian spherical constraint is explored in Yang et al. (2024). Mou et al. (2024) advance these techniques to more versatile editing tasks. The above methods are further unified in Ye et al. (2024). (3) A recent line of work directly uses gradient descent with specific diffusion models. To enable their gradient computation, DiffusionCLIP (Kim et al., 2022a) relies on shortened ODE trajectories, while FlowGrad (Liu et al., 2023b) adopts a non-uniform ODE discretization and decomposed gradient computation. DOODL (Wallace et al., 2023), DNO (Karunratanakul et al., 2024) and D-Flow (BenHamu et al., 2024) use invertible models or flow models to backpropagate gradient to the initial noise. Our proposed method is related to the third category, focusing on rectified flow whose approximation remains understudied. Moreover, it features a fixed-point formulation with a convergence guarantee for ideal rectified flow, which allows potentially better stability over the existing approaches, e.g. gradient descent on initial noise, as empirically validated in Section 4.3. ", "page_idx": 15}, {"type": "text", "text": "Recent studies also explore the use of pre-trained classifiers to guide personalized image generation, but mostly during model training. DiffFace (Kim et al., 2022b) and PhotoVerse (Chen et al., 2023) directly apply a face discriminator to noised images to compute an identity loss. PortraitBooth (Peng et al., 2024) improves loss quality by computing it at less noisy stages. More relevant to our work, LCM-Lookahead (Gal et al., 2024) and PuLID (Guo et al., 2024) utilize distilled diffusion models to generate images in few steps, allowing for direct gradient backpropagation of image-space losses. However, these personalization methods must first be trained on extensive face recognition data, e.g. LAION-Face 50M (Zheng et al., 2022). In comparison, we harness off-the-shelf face discriminators at test time based on the methodology of classifier guidance, enabling more flexible customization without training. And it can be generalized to other subjects by simply replacing the classifier. ", "page_idx": 15}, {"type": "text", "text": "C Experimental Settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Method. We mainly experiment on the recently proposed piecewise rectified flow (Yan et al., 2024). The model is finetuned from Stable Diffusion 1.5 (Rombach et al., 2022) on the LAION-Aesthetic- $^{5+}$ dataset (Schuhmann et al., 2022) without special pre-training on human faces or subjects. We adopt a fixed image size of $512\\times512$ , number of sampling step $K=4$ , and a classifier-free guidance scale of 3.0 during quantitative evaluation. The newly incorporated anchored classifier guidance uses a default guidance scale $s=1.0$ and number of iterations $N=100$ . For qualitative studies, a few results are generated with a slightly different guidance scale $s=0.5$ or 2.0 for better visual quality or identity consistency. But in general, our method is not very sensitive to these hyperparameters. In terms of computational and memory overhead, our method takes less than 0.5s per iteration on an NVIDIA A800 GPU and fits on consumer-grade GPUs such as NVIDIA RTX 4080, the latter of which may be further improved with gradient checkpointing or the techniques in Liu et al. (2023b). ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "For face-centric personalization, our method is implemented with the antelopev2 model pack from the InsightFace library.2 Specifically, it detects and crops the face regions with an SCRFD model (Guo et al., 2022), and then extracts face features using an ArcFace model (Deng et al., 2019) trained on Glint360K (An et al., 2021), consistent with most personalization methods that use a face model (Ye et al., 2023; Wang et al., 2024b; Gal et al., 2024). The resulting face feature is compared with the reference image to compute a cosine loss, which serves as the classifier guidance signal. ", "page_idx": 16}, {"type": "text", "text": "For subject-driven generation, we use an open-vocabulary object detector OWL-ViT (Minderer et al., 2022) to detect the region of interest, and extract visual features with DINOv2 (Oquab et al., 2023). The extracted feature is compared with the reference to calculate a cosine loss as the guidance signal. While this guidance already works well for various live subjects including multiple dogs and cats, we add an optional $\\ell_{1}$ loss with a coefficient of 10.0 to help preserve the identity of certain objects, such as cans, vases and duck toys. The current implementation is still limited in not capturing the details of some irregularly shaped objects, e.g. plush toys, and we expect to resolve this issue with an improved discriminator, or by combining with existing image prompt techniques (Ye et al., 2023). ", "page_idx": 16}, {"type": "text", "text": "For multi-subject personalization, we consider a simplified case of exactly two subjects and perform the following: first detect the two subjects, then enumerate all possible bipartite matches with the reference subjects to minimize the matching cost. For more complex scenarios, a possible workaround is to formulate it as an quadratic assignment problem and apply efficient solvers (Tan and Mu, 2024). ", "page_idx": 16}, {"type": "text", "text": "Evaluation. For face-centric personalization, we follow Pang et al. (2024) in evaluating on the first 200 images in the CelebA-HQ dataset (Liu et al., 2015; Karras et al., 2018) with 20 text prompts including 15 realistic prompts and 5 stylistic prompts. The evaluation process reuses the code from Celeb Basis (Yuan et al., 2023),3 which first detects the face region using a PIPNet (Jin et al., 2021) with a threshold of 0.5 and then computes the cosine similarity on face features extracted by an ArcFace model (Deng et al., 2019). It should be noted that our method adopts a different face detector and different alignment and cropping methods, so it does not overfti the evaluation protocol. For the baselines, in addition to those compared in Pang et al. (2024), we also evaluate the recently proposed IP-Adapter (Ye et al., 2023), PhotoMaker (Li et al., 2024), and InstantID (Wang et al., 2024b) on their recommended settings. The number of their sampling steps is set to 30 for a fair comparison. The checkpoint version of IP-Adapter is ip-adapter-full-face_sd15. The image size is set to $512\\!\\times\\!512$ for IP-Adapter and $1024\\!\\times\\!1024$ for PhotoMaker and InstantID based on SDXL (Podell et al., 2024). In the qualitative analysis, we also include Celeb Basis (Yuan et al., 2023) as a baseline method. ", "page_idx": 16}, {"type": "text", "text": "For subject-driven generation, we perform a qualitative rather than a quantitative comparison on a subset of the DreamBooth dataset (Ruiz et al., 2023a) due to the previously mentioned limitations. Nevertheless, many subjects are considered during the qualitative study, including 7 live subjects from two categories (cats and dogs) and 3 regularly shaped objects from different categories (cans, vases, and teapots). For the baselines, we incorporate Textual Inversion (Gal et al., 2023), DreamBooth (Ruiz et al., 2023a), BLIP-Diffusion (Li et al., 2023), and Emu2 (Sun et al., 2024) for extensive comparison. Their diffusion models and hyperparameter settings follow the official or Diffusers implementation.4 ", "page_idx": 16}, {"type": "text", "text": "Licenses. The piecewise rectified flow (Yan et al., 2024) used in the main experiments is released under the BSD-3-Clause License and the 2-rectified flow (Liu et al., 2023a, 2024b) used in Appendix D.4 is released under the MIT License. The InsightFace library for face detection and recognition is released under the MIT License, while its pre-trained models are available for non-commercial research purposes only. The OWL-ViT (Minderer et al., 2022) and DINOv2 (Oquab et al., 2023) models for object detection and feature extraction are released under the Apache-2.0 License. For evaluation, the code of Celeb Basis (Yuan et al., 2023) is licensed under the MIT License. The ", "page_idx": 16}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/3a9a2e0f4a4bf4cc5ca50c1045c3697ad46a5a7fa5fa603921c9efbf46591388.jpg", "img_caption": ["Figure 8: Generalization to few-step diffusion models, including SD-Turbo (Sauer et al., 2024) and phased consistency model (Wang et al., 2024a), both distilled from SD and using 4 sampling steps in inference. The results show that our method is effective for personalizing broader diffusion models. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "CelebA-HQ (Liu et al., 2015; Karras et al., 2018) and DreamBooth (Ruiz et al., 2023a) datasets for quantitative and qualitative evaluation are released under the CC BY-NC 4.0 and CC-BY-4.0 licenses. ", "page_idx": 17}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Generalization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "While our classifier guidance is derived based on rectified flow, the same idea can be generalized to some few-step diffusion models by assuming straightness of their trajectories within each time step. We empirically demonstrate this in Fig. 8 with two popular few-step diffusion models, SDTurbo (Sauer et al., 2024) and phased consistency model (Wang et al., 2024a). As the results indicate, our method effectively personalizes these diffusion models to generate identity-preserving images. We will continue to explore this approach for other generative models in future research. ", "page_idx": 17}, {"type": "text", "text": "D.2 More Visualizations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "More examples of our generated image are provided in Figs. 9 to 14. For face-centric personalized image generation, it is shown that our method can follow a variety of text prompts to generate both realistic or stylistic images while preserving the user-specified identity from a diverse group of people. Although there exist minor differences in the person\u2019s age and hairstyles, the face looks very similar to the reference image. In particular, the method demonstrates good generalizability among different piecewise rectified flows based on SDXL (Podell et al., 2024) and SD 1.5 (Rombach et al., 2022). For subject-driven generation, new results are presented from additional subject types, including different breeds of cats and dogs, and some regularly shaped objects such as vases, demonstrating the flexibility of our approach across different use cases. For multi-subject generation, it naturally blends multiple human faces or live subjects into a single image while maintaining the visual quality and semantics, revealing a wider range of potentially interesting applications. Overall, our method demonstrates to be effective and flexible for various personalization tasks. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "D.3 Ablation Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In addition to the ablation experiments in the main paper, we perform a sensitivity analysis of the two hyperparameters in our method, namely the guidance scale $s$ and the number of iterations $N$ . Specifically, we study the effect of different $s$ under a fixed $N=20$ and then the effect of different $N$ under a fixed $s=1.0$ . The results are summarized in Fig. 15. As can be observed, increasing both hyperparameters from 0 leads to a significant improvement in identity consistency, confirming the effectiveness of our proposed classifier guidance. Also, the performance is stable over a fairly wide range of hyperparameters, indicating that our approach is not very sensitive to hyperparameters. Furthermore, we find that the use of a small classifier guidance scale is actually beneficial for prompt consistency, possibly because it enhances the visual features, as demonstrated in Fig. 2. ", "page_idx": 18}, {"type": "text", "text": "D.4 Experiments with 2-Rectified Flow ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figures 16 and 17 present additional qualitative results on a vanilla 2-rectified flow (Liu et al., 2023a, 2024b) finetuned on Stable Diffusion 1.4 (Rombach et al., 2022). As can be seen, our method continues to deliver satisfactory identity preservation when moving to a different model, despite a noticeable drop in the generation quality. Concretely, it integrates target subjects with some quite challenging prompts, such as a person swimming or getting a haircut, while showing very little interference with the original background, e.g. jungles and cityscapes. These results clearly validate the effectiveness of our proposed method in alternative rectified flow models. ", "page_idx": 18}, {"type": "text", "text": "Note that we also experimented with $K=1$ on a single-step InstaFlow (Liu et al., 2024b) distilled from this 2-rectified flow, but found that it tended to converge to slightly distorted images. This may be attributed to the larger modeling error inherent in InstaFlow\u2019s distillation process, which reduces the effectiveness of our approach assuming each flow trajectory segment is well-trained and straight. ", "page_idx": 18}, {"type": "text", "text": "E Broader Impacts and Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Broader impacts. The proposed method can be integrated with the emerging rectified flow-based models to enhance identity preservation and versatile control over existing AI art production pipelines. However, as a training-free personalization technique, it may increase the risk of image faking and have negative societal effects. Some immediate remedies include text-based prompt filters and AIgenerated image detection, but it remains an open problem for a more principled solution, for which we advocate further research on data watermarking and model unlearning as potential mitigations. To further clarify it, we provide a more detailed explanation of these mitigations below: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Prompt filtering, model unlearning: Since our method keeps the diffusion model intact, existing techniques for regulating diffusion models can be applied seamlessly, including prompt filters or unlearning methods. The former can be applied explicitly like the text filters in SD models, or implicitly via CFG as in Schramowski et al. (2023). The latter approaches involve finetuning the diffusion model to remove the ability to generate harmful content (Gandikota et al., 2023; Kumari et al., 2023).   \n\u2022 Data watermarking: To prevent misuse of personal images, one could add a protective watermark to their images (Van Le et al., 2023; Liu et al., 2024a). With this watermark or perturbation, the image can no longer be learned by common personalization methods. However, it is unclear how robust these watermarks are to training-free methods such as ours. An alternative watermarking scheme is to embed special watermark to the images generated by our proposed model, which would be invisible to the users yet identifiable by us (i.e., the tech provider). Images with such watermarks will be marked as being artificial.   \n\u2022 AI-generated image detection: As a post-hoc safety measure, it helps to distinguish fake images generated by the attackers from real images. Beyond above watermark-based scheme, more sophisticated data-driven methods have attracted increasing interest from the AI community. Despite that current methods still lack accuracy, we believe that developing reliable and widely available AI-generated image detectors is an important research direction. ", "page_idx": 18}, {"type": "text", "text": "Limitations. Our theoretical guarantee is limited to ideal rectified flow and cannot be generalized to more complex flow-based models. Empirically, anchoring the new flow trajectory to a reference trajectory only proves effective for faces, live subjects and certain regularly shaped objects, and remains insufficient for many objects with large structural variations, e.g. plush toys. Furthermore, while our method is training-free, its inference time has yet to match several training-based baselines, which may be addressed by applying more advanced numerical solvers to the derived problem. ", "page_idx": 19}, {"type": "text", "text": "Another important issue is the lack of pre-trained discriminators. To address this in the short term, we suggest first training a specialized discriminator and then applying our classifier guidance. There are two reasons for doing this instead of finetuning the generator directly: (1) training/finetuning a discriminator is usually more efficient and stable than training/finetuning a generator; (2) it can take full advantage of domain images that have no captions or even labels by using standard contrastive learning loss. In the future, scaling up vision-language models may be a general solution for these domains. The current models such as GPT-4V (OpenAI, 2023) have demonstrated certain generalizability across visual understanding tasks. As they continue to improve in generalizability and robustness, they will become a viable source for guiding diffusion models in new domains. ", "page_idx": 19}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/55a3869d325799b32bf4f2741ee773d0c991ac202b2928d21b02370a82dfacc1.jpg", "img_caption": ["Input "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "graduating afterwear a magician hat and a blue   \nfinishing PhD coat in a garden ", "page_idx": 20}, {"type": "text", "text": "as White Queen ", "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/6102747ccaff6b3b613c223d973af5ece3c778a0e3700ced236a97bb767f8115.jpg", "img_caption": ["wearing concert poster headphones ", "pencil drawing "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/3ccfe311452638fb571f1290a06e447e8172da87ba8c4bd1e5f5ef693db2ec14.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/952b8897e4089a4730f56f41cd0481d15e15aef840ef2ec3482f629fe92308b6.jpg", "img_caption": ["reading on buckled in his sipping coffee the train seat on a plane at a cafe "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "holding roses wear a magician in front of the hat and a blue Eiffel Tower coat in a garden ", "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/171b615392a89a10f1a0e9768ed2ce4e733a8cebb0393a956b674527b6a4a0f0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/4e37658c6b3be059bfb6ed90a8c43a0408600bfe3bcaf37f1655770724dc469a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/48e12fbaabcc9b53c1fce0d33de4931ebb3d69ff1511306c4b2e73b180cfc2d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/1e27267c8c8459f7f51da7048f91dd6d1e634eaf46097dea0dc0a6228b38bbe4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 9: Additional face-centric personalization results with piecewise rectified flow (Yan et al., 2024) based on SDXL (Podell et al., 2024). Our method is compatible with more advanced base models and provides sophisticated personalization results. ", "page_idx": 20}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/b1116557846ad4352c16c6471c0cd3299e9c000a9afe810d1540fd9ff5b61d1c.jpg", "img_caption": ["Figure 10: Additional face-centric personalization results with piecewise rectified flow (Yan et al., 2024) based on SDXL (Podell et al., 2024). Our method is compatible with more advanced base models and provides sophisticated personalization results. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/f74ac77b92af9034fa28cc06d9803e7978ae69dc60070398b4ad2378a5208da7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 11: Additional face-centric personalization results with piecewise rectified flow (Yan et al., 2024), which is based on Stable Diffusion 1.5 (Rombach et al., 2022). Our method achieves high identity consistency. See Fig. 16 for results on the vanilla 2-rectified flow (Liu et al., 2023a, 2024b). ", "page_idx": 22}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/7383f0c217c3fe0e3b28326a6a60b7287f2fbbf2be670d140b3171c0ccfb210a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/dc7914b444b3c91b413e17cd0088911fa1482cd0af96a65283ad0d704d22be5a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 12: Additional face-centric personalization results with piecewise rectified flow (Yan et al., 2024), which is based on Stable Diffusion 1.5 (Rombach et al., 2022). Our method achieves high identity consistency. See Fig. 16 for results on the vanilla 2-rectified flow (Liu et al., 2023a, 2024b). ", "page_idx": 23}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/c3cdbcee0ba8ef512cbe98fbe65edae1a8b412a7b07805d5452e6a211bc19bd0.jpg", "img_caption": ["Figure 13: Additional subject-driven generation results with piecewise rectified flow (Yan et al., 2024), which is based on Stable Diffusion 1.5 (Rombach et al., 2022). Our approach preserves the identity of both live subjects and some regularly shaped objects. Please see Fig. 17 for more examples using the vanilla 2-rectified flow (Liu et al., 2023a, 2024b). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/dea6163f2cf14819f5cb74d15c9b095a8ca49901d27702f5e163fa12d7987bdc.jpg", "img_caption": ["Figure 14: Additional multi-subject personalization results with piecewise rectified flow (Yan et al., 2024), which is based on Stable Diffusion 1.5 (Rombach et al., 2022). Our approach can naturally compose multiple subjects into the generated image while preserving their identities. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/43bf39875a54ec2da268074eb4e15867b3826cf5644667e6f6bb57f4deab4e31.jpg", "img_caption": ["Figure 15: Ablation study of hyperparameters. Left: ablation study of guidance scale $s$ under $N=20$ . Right: ablation study of the number of iterations $N$ under $s=1.0$ . Our method remains effective over a reasonably wide range of hyperparameters. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/0b37e768c643a314718ffd0b26635b039131cfa5421db5aa43c472f788ce0b1b.jpg", "img_caption": ["Figure 16: Face-centric personalization results with vanilla 2-rectified flow (Liu et al., 2023a, 2024b), which is based on Stable Diffusion 1.4 (Rombach et al., 2022). Our method preserves their identities well while remaining faithful to the text prompt during generation. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "KKrj1vCQaG/tmp/e4465a96a7c4178b5c39bd3a32979f92a690e65196f53a007c82a4d602a88f87.jpg", "img_caption": ["Figure 17: Subject-driven generation results with vanilla 2-rectified flow (Liu et al., 2023a, 2024b), which is based on Stable Diffusion 1.4 (Rombach et al., 2022). Examples for additional categories of cats, dogs, and objects are included to demonstrate the effectiveness of our approach. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The main claims and contributions of this paper are summarized at the end of the introduction (Section 1). ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The limitations are discussed in Appendix E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The assumptions and proofs for Propositions 1 and 2 are provided in the main paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have detailed these information in Section 4.1 and Appendix C. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The code to reproduce all results is available at this anonymized link https: //github.com/feifeiobama/RectifID, with sufficient instructions in the README file. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The experimental settings are specified in Section 4.1 and Appendix C. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: We report the results from a single experimental run, following many baselines in the field. The experiments use a fixed seed of 42. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The compute resources are detailed in Section 4.1 and Table 1. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and confirm that our research complies with the Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The potential societal impacts are discussed in Appendix E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not release any datasets or pre-trained models ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We have cited the original paper that produced the assets used in this paper and summarized their licenses in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The code is available at this anonymized repository https://github.com/ feifeiobama/RectifID, along with a README documentation. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or human subjects research. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or human subjects research. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]