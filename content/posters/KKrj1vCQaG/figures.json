[{"figure_path": "KKrj1vCQaG/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of training-free classifier guidance. Left: an off-the-shelf discriminator can be reused to steer the existing diffusion model, e.g. rectified flow, to generate identity-preserving images. Right: personalized image generation results for human faces and objects using our proposed method.", "description": "The figure illustrates the proposed training-free classifier guidance method.  The left panel shows the process:  A user provides a reference image ('ref img').  A pre-trained diffusion model (based on rectified flow) is used, guided by an off-the-shelf classifier (either a face or object discriminator).  This avoids the need for training a domain-specific classifier. The right panel displays example results of the personalized image generation for both human faces and objects, demonstrating the effectiveness of the method.", "section": "1 Introduction"}, {"figure_path": "KKrj1vCQaG/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of anchored classifier guidance for rectified flow. Left: we propose to guide the flow trajectory while implicitly enforcing it to flow straight and stay close to a reference trajectory. Right: comparison of the new trajectory with the reference trajectory (in the last three sampling steps).", "description": "This figure illustrates the concept of anchored classifier guidance in the context of rectified flow. The left panel shows a schematic representation of how a target trajectory is guided towards a desired endpoint (z1) by incorporating classifier guidance, while simultaneously being constrained to remain close to a reference trajectory. This constraint helps to ensure the stability and convergence of the process. The right panel presents visual results, showcasing the last three steps of the sampling process, where the new trajectory (resulting from anchored classifier guidance) is compared to the reference trajectory. This comparison highlights the effectiveness of the approach in achieving both accurate guidance and stable convergence.", "section": "3.2 Anchored Classifier Guidance"}, {"figure_path": "KKrj1vCQaG/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure compares the results of face-centric personalization using different methods.  It shows a set of input images, and the results generated by the Celeb Basis, IP-Adapter, PhotoMaker, InstantID, and RectifID methods. The caption indicates that more examples can be found in figures 9 through 12 of the paper.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparison for subject-driven generation. * denotes finetuned with multiple images of the target subject to achieve sufficient identity consistency. See Fig. 13 for more samples.", "description": "This figure compares the results of several methods for generating images with a user-specified subject.  The methods compared include Textual Inversion, DreamBooth, BLIP-Diffusion, Emu2, and the proposed RectifID method.  The top row shows the input reference image for each subject, and the subsequent rows show the generated images for different prompts. The asterisk (*) indicates that fine-tuning was performed on multiple images of the subject.  The figure highlights the ability of the proposed RectifID method to generate images that accurately preserve the identity of the subject and are consistent with the given prompt, even when compared to methods that use finetuning.", "section": "4 Experiments"}, {"figure_path": "KKrj1vCQaG/figures/figures_7_2.jpg", "caption": "Figure 5: Qualitative comparison for multi-subject personalization. See Fig. 14 for more samples.", "description": "This figure compares the results of multi-subject image generation using three different methods: FastComposer, Cones2, and the proposed RectifID method.  The input shows two different reference images, one of a person and one of a dog. Each column shows the generation results for each method, demonstrating how each approach handles the task of integrating multiple subjects into a single generated image.", "section": "4 Experiments"}, {"figure_path": "KKrj1vCQaG/figures/figures_8_1.jpg", "caption": "Figure 6: Comparison with alternative designs at varying guidance scale (or learning rate) and iterations. The prompts are \u201ccave mural depicting a person", "description": "This ablation study compares the proposed anchored classifier guidance against two alternatives: gradient descent on the noise and classifier guidance without an anchor.  Three different guidance scales (s \u00d7 0.5, s \u00d7 1, s \u00d7 2) and three different numbers of iterations (N = 20, N = 50, N = 100) are tested for each method. The results demonstrate that the proposed method is more stable, converges faster, and achieves better results in terms of identity preservation and prompt consistency.", "section": "4.3 Ablation Study"}, {"figure_path": "KKrj1vCQaG/figures/figures_9_1.jpg", "caption": "Figure 7: Experimental results for more controllable generation tasks. The first column shows the guidance, and the rest are the generated results. Our method is extended to various controllable generation tasks by incorporating the guidance functions from Universal Guidance (Bansal et al., 2024).", "description": "This figure demonstrates the flexibility of the proposed method by incorporating guidance functions from Universal Guidance.  The leftmost column displays the guidance (segmentation map or style transfer). The remaining columns present images generated using the method with different guidance types.  This shows the method's capability to extend beyond simple identity preservation to more controlled image generation scenarios.", "section": "4.4 Generalization"}, {"figure_path": "KKrj1vCQaG/figures/figures_17_1.jpg", "caption": "Figure 8: Generalization to few-step diffusion models, including SD-Turbo (Sauer et al., 2024) and phased consistency model (Wang et al., 2024a), both distilled from SD and using 4 sampling steps in inference. The results show that our method is effective for personalizing broader diffusion models.", "description": "This figure demonstrates the generalization of the proposed method to different diffusion models.  Two few-step diffusion models (SD-Turbo and Phased Consistency Model) were tested. The results show that the method effectively personalizes these models to generate identity-preserving images, highlighting its adaptability and broader applicability beyond the initially used rectified flow model.", "section": "4.4 Generalization"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_1.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between the proposed RectifID method and several baselines (Celeb Basis, IP-Adapter, PhotoMaker, InstantID).  The comparison is based on the generation of images according to a set of prompts, using a face as the reference image.  The figure highlights the RectifID method's superior ability to maintain identity while fulfilling the prompt's requirements.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_2.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between RectifID and other methods.  The input column shows the reference image and prompt used.  The remaining columns display the generated images from different methods, allowing visual comparison of identity preservation and prompt adherence.  The caption directs the reader to figures 9-12 for more generated image samples.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_3.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure compares the results of face-centric personalization using different methods.  It shows several example images generated from the same input prompt by different methods.  The goal is to generate images that accurately preserve the identity of the target face while also matching the provided text prompt. RectifID is shown along with other state-of-the-art methods for comparison. The additional figures mentioned (9-12) likely provide a more extensive set of examples.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_4.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure compares the results of face-centric personalization using different methods.  The input is a reference image. The results show generated images of the same person in various styles, demonstrating the ability of each method to maintain identity consistency while varying the style.  Additional examples are available in Figures 9-12.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_5.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure presents a qualitative comparison of face-centric personalization results from several different methods.  It shows several examples of the input image (reference image), and results generated from different methods. The methods shown include Celeb Basis, IP-Adapter, PhotoMaker, InstantID, and the method proposed in this paper (RectifID). The images are meant to illustrate the capabilities of each method in creating identity-preserving images, highlighting differences in quality, accuracy of identity preservation, etc.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_6.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure compares the results of face-centric personalization using different methods.  The input is a reference image and text prompt.  Each column represents a different method (Celeb Basis, IP-Adapter, PhotoMaker, InstantID, and RectifID).  The results show the generated images for each method, allowing for a visual comparison of identity preservation and adherence to the prompt.  Further samples are provided in Figures 9-12. ", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_7.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between several different methods including the proposed RectifID method.  The input is the reference image, while the output of different methods for generating images using the same prompt is shown. The RectifID method demonstrates successful preservation of identity and flexibility.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_20_8.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between several methods, including the proposed RectifID method.  It displays generated images for various prompts, allowing a visual comparison of identity preservation and adherence to the prompt's style.  The additional figures mentioned (9-12) likely contain further examples.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_21_1.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between the proposed RectifID method and several other state-of-the-art methods.  Different prompts are used as input, and the generated images from each method are displayed alongside the original input image and prompt.  This allows for visual comparison of identity preservation and overall image quality.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_22_1.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between RectifID and other methods.  Each method is given the same input (a text prompt and reference image), and the generated results are displayed. This allows for a visual comparison of identity preservation and prompt consistency.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_23_1.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results using different methods.  The input is a reference image. The columns show results from several different state-of-the-art methods along with the results from the proposed RectifID method.  The results demonstrate the ability of RectifID to generate high-fidelity images while preserving the identity of the subject.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_23_2.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between RectifID and several other methods.  It visually demonstrates the ability of RectifID to generate images that accurately preserve the identity of the input face, even when applying different prompts.  The additional figures referenced (9-12) contain more examples of the generated images for a more thorough comparison.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_24_1.jpg", "caption": "Figure 2: Illustration of anchored classifier guidance for rectified flow. Left: we propose to guide the flow trajectory while implicitly enforcing it to flow straight and stay close to a reference trajectory. Right: comparison of the new trajectory with the reference trajectory (in the last three sampling steps).", "description": "This figure illustrates the concept of anchored classifier guidance. The left panel shows how the proposed method guides the flow trajectory while keeping it close to a reference trajectory to maintain stability.  The right panel presents a visual comparison, highlighting how the new trajectory stays close to the reference in its final stages.", "section": "3.2 Anchored Classifier Guidance"}, {"figure_path": "KKrj1vCQaG/figures/figures_25_1.jpg", "caption": "Figure 14: Additional multi-subject personalization results with piecewise rectified flow (Yan et al., 2024), which is based on Stable Diffusion 1.5 (Rombach et al., 2022). Our approach can naturally compose multiple subjects into the generated image while preserving their identities.", "description": "This figure shows example results of applying the RectifID method to generate images with multiple subjects.  The method successfully integrates multiple subjects into a single image while preserving the identity of each subject, demonstrating the capability of the method for multi-subject personalization tasks.  The results are based on the piecewise rectified flow model (Yan et al., 2024) built on Stable Diffusion 1.5 (Rombach et al., 2022).", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_25_2.jpg", "caption": "Figure 15: Ablation study of hyperparameters. Left: ablation study of guidance scale s under N = 20. Right: ablation study of the number of iterations N under s = 1.0. Our method remains effective over a reasonably wide range of hyperparameters.", "description": "This figure shows the results of an ablation study on the hyperparameters of the proposed method. The left panel shows how identity and prompt similarity change as the guidance scale (s) varies, while keeping the number of iterations (N) constant at 20. The right panel shows how these metrics change as N varies, while keeping s constant at 1.0.  The results demonstrate the robustness of the method to changes in both hyperparameters within a reasonable range.", "section": "4.3 Ablation Study"}, {"figure_path": "KKrj1vCQaG/figures/figures_26_1.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between different methods, including the proposed RectifID method.  Each row represents a different prompt, and each column showcases the output images from various methods. The RectifID method is shown to produce images with higher identity preservation and better prompt consistency compared to the other methods.", "section": "4.2 Main Results"}, {"figure_path": "KKrj1vCQaG/figures/figures_27_1.jpg", "caption": "Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.", "description": "This figure shows a qualitative comparison of face-centric personalization results between several methods including the proposed RectifID.  Each row represents a different prompt and each column a different method. The results illustrate the ability of RectifID to generate images that maintain identity consistency while adhering to the prompt.", "section": "4.2 Main Results"}]