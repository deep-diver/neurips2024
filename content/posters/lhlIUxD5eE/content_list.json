[{"type": "text", "text": "Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chen-Hao Chao\u22171,2 Chien Feng\u22171 Wei-Fang Sun2 ", "page_idx": 0}, {"type": "text", "text": "Cheng-Kuang Lee2 Simon See2 Chun-Yi Lee\u20201 ", "page_idx": 0}, {"type": "text", "text": "1 Elsa Lab, National Tsing Hua University, Hsinchu City, Taiwan 2 NVIDIA AI Technology Center, NVIDIA Corporation, Santa Clara, CA, USA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing Maximum-Entropy (MaxEnt) Reinforcement Learning (RL) methods for continuous action spaces are typically formulated based on actor-critic frameworks and optimized through alternating steps of policy evaluation and policy improvement. In the policy evaluation steps, the critic is updated to capture the soft Q-function. In the policy improvement steps, the actor is adjusted in accordance with the updated soft Q-function. In this paper, we introduce a new MaxEnt RL framework modeled using Energy-Based Normalizing Flows (EBFlow). This framework integrates the policy evaluation steps and the policy improvement steps, resulting in a single objective training process. Our method enables the calculation of the soft value function used in the policy evaluation target without Monte Carlo approximation. Moreover, this design supports the modeling of multi-modal action distributions while facilitating efficient action sampling. To evaluate the performance of our method, we conducted experiments on the MuJoCo benchmark suite and a number of high-dimensional robotic tasks simulated by Omniverse Isaac Gym. The evaluation results demonstrate that our method achieves superior performance compared to widely-adopted representative baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maximum-Entropy (MaxEnt) Reinforcement Learning (RL) [1\u201317] has emerged as a prominent method for modeling stochastic policies. Different from standard RL, MaxEnt RL integrates the entropy of policies as rewards, which leads to a balanced exploration-exploitation trade-off during training. This approach has demonstrated improved robustness both theoretically and empirically [17\u201319]. Building on this foundation, many studies leveraging MaxEnt RL have shown superior performance on continuous-control benchmark environments [8, 9] and real-world applications [20\u201322]. ", "page_idx": 0}, {"type": "text", "text": "An active research domain in MaxEnt RL concentrates on the learning of the soft Q-function [8\u201315]. These methods follow the paradigm introduced in soft Q-learning (SQL) [8]. They parameterize the soft Q-function as an energy-based model [23] and optimize it based on the soft Bellman error [8] calculated from rewards and the soft value function. However, this approach presents two challenges. First, sampling from an energy-based model requires a costly Monte Carlo Markov Chain (MCMC) [24, 25] or variational inference [26] process, which can result in inefficient interactions with environments. Second, the calculation of the soft value function can involve computationally infeasible integration, which requires an effective approximation method. To tackle these issues, various methods [8\u201315] were proposed, all grounded in a common design philosophy. To address the first challenge, these methods suggest operating on an actor-critic framework and optimizing it through alternating steps of policy evaluation and policy improvement. For the second challenge, they resort to Monte Carlo methods to approximate the soft value function using sets of random samples. Although these two issues can be circumvented, these methods still have their drawbacks. The actor-critic design introduces an additional optimization process for training the actor, which may lead to optimization errors in practice [27]. Moreover, the results of Monte Carlo approximation may be susceptible to estimation errors and variances when there are an insufficient number of samples [28\u201330]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Instead of using energy-based models to represent MaxEnt RL frameworks, this paper investigates an alternative method employing normalizing flows (i.e., flow-based models), which offer solutions to the aforementioned challenges. Our framework is inspired by the recently introduced Energy-Based Normalizing Flows (EBFlow) [31]. This design facilitates the derivation of an energy function from a flow-based model while supporting efficient sampling, which enables a unified representation of both the soft Q-function and its corresponding action sampling process. This feature allows the integration of the policy evaluation and policy improvement steps into a single objective training process. In addition, the probability density functions (pdf) of flow-based models can be calculated efficiently without approximation. This characteristic permits the derivation of an exact representation for the soft value function. Our experimental results demonstrate that the proposed framework exhibits superior performance on the commonly adopted MuJoCo benchmark [32, 33]. Furthermore, the evaluation results on the Omniverse Isaac Gym environments [34] indicate that our framework excels in performing challenging robotic tasks that simulate real-world scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we walk through the background material and the related works. We introduce the objective of MaxEnt RL in Section 2.1, describe existing actor-critic frameworks and soft value estimation methods in Section 2.2, and elaborate on the formulation of Energy-Based Normalizing Flow (EBFlow) in Section 2.3. ", "page_idx": 1}, {"type": "text", "text": "2.1 Maximum Entropy Reinforcement Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we consider a Markov Decision Process (MDP) defined as a tuple $(S,\\mathcal{A},p_{T},\\mathcal{R},\\gamma,p_{0})$ , where $\\boldsymbol{S}$ is a continuous state space, $\\boldsymbol{\\mathcal{A}}$ is a continuous action space, $p_{T}:S\\times S\\times A\\to\\mathbb{R}_{\\geq0}$ is the pdf of a next state $\\mathbf{s}_{t+1}$ given a current state $\\mathbf{s}_{t}$ and a current action ${\\mathbf a}_{t}$ at timestep $t,$ , $\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ is the reward function, $0<\\gamma<1$ is the discount factor, and $p_{0}$ is the pdf of the initial state $\\mathbf{s}_{0}$ . We adopt $r_{t}$ to denote $\\mathscr{R}(\\mathbf{s}_{t},\\mathbf{a}_{t})$ , and use $\\rho_{\\pi}\\big(\\mathbf{s}_{t},\\mathbf{a}_{t}\\big)$ to represent the state-action marginals of the trajectory distribution induced by a policy $\\pi(\\mathbf{a}_{t}|\\mathbf{s}_{t})$ [8]. ", "page_idx": 1}, {"type": "text", "text": "Standard RL defines the objective as $\\begin{array}{r}{\\pi^{*}=\\operatorname{argmax}_{\\pi}\\;\\sum_{t}\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{a}_{t})\\sim\\rho_{\\pi}}[r_{t}]}\\end{array}$ and has at least one deterministic optimal policy [35, 36]. In contrast, MaxEnt RL [4] augments the standard RL objective with the entropy of a policy at each visited state $\\mathbf{s}_{t}$ . The objective of MaxEnt RL is written as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{MaxEnt}}^{*}=\\underset{\\pi}{\\mathrm{argmax}}\\,\\sum_{t}\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{a}_{t})\\sim\\rho_{\\pi}}\\left[r_{t}+\\alpha\\mathcal{H}(\\pi(\\mathbf{\\pi}\\cdot|\\mathbf{s}_{t}))\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{H}(\\pi(\\mathbf{\\mu}\\cdot|\\mathbf{s}_{t}))\\triangleq\\mathbb{E}_{\\mathbf{a}\\sim\\pi(\\cdot|\\mathbf{s}_{t})}[-\\log\\pi(\\mathbf{a}|\\mathbf{s}_{t})]$ and $\\alpha\\in\\mathbb{R}_{>0}$ is a temperature parameter for determining the relative importance of the entropy term against the reward. An extension of Eq. (1) defined with $\\gamma$ is discussed in [8]. To obtain $\\pi_{\\mathrm{MaxEnt}}^{*}$ described in Eq. (1), the authors in [8] proposed to minimize the soft Bellman error for all states and actions. The solution can be expressed using the optimal soft Q-function $Q_{\\mathrm{soft}}^{*}:S\\times A\\to\\mathbb{R}$ and soft value function $V_{\\mathrm{soft}}^{*}:S\\rightarrow\\mathbb{R}$ as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{MaxEnt}}^{*}(\\mathbf{a}_{t}|\\mathbf{s}_{t})=\\exp\\left({\\frac{1}{\\alpha}}(Q_{\\mathrm{soft}}^{*}(\\mathbf{s}_{t},\\mathbf{a}_{t})-V_{\\mathrm{soft}}^{*}(\\mathbf{s}_{t}))\\right),\\ {\\mathrm{where}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "equation", "text": "$$\nQ_{\\mathrm{soft}}^{*}(\\mathbf{s}_{t},\\mathbf{a}_{t})=r_{t}+\\gamma\\mathbb{E}_{\\mathbf{s}_{t+1}\\sim p_{T}}\\left[V_{\\mathrm{soft}}^{*}(\\mathbf{s}_{t+1})\\right],\\quad V_{\\mathrm{soft}}^{*}(\\mathbf{s}_{t})=\\alpha\\log\\int\\exp\\left(\\frac{1}{\\alpha}Q_{\\mathrm{soft}}^{*}(\\mathbf{s}_{t},\\mathbf{a})\\right)d\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In practice, a policy can be modeled as $\\begin{array}{r}{\\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})=\\exp(\\frac{1}{\\alpha}(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})-V_{\\theta}(\\mathbf{s}_{t}))}\\end{array}$ with parameter $\\theta$ , where the soft Q-function and the soft value function are expressed as $Q_{\\theta}\\big(\\mathbf{s}_{t},\\mathbf{a}_{t}\\big)$ and ${\\cal V}_{\\theta}({\\bf s}_{t})=$ ", "page_idx": 1}, {"type": "text", "text": "$\\begin{array}{r}{\\alpha\\log\\int\\exp\\left(\\frac{1}{\\alpha}Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\right)d\\mathbf{a}_{t}}\\end{array}$ , respectively. Given an experience reply buffer $\\mathcal{D}$ that stores transition tuples $(\\mathbf{s}_{t},\\mathbf{a}_{t},r_{t},\\mathbf{s}_{t+1})$ , the training objective of $Q_{\\theta}$ (which can then be used to derive $V_{\\theta}$ and $\\pi_{\\theta}$ ) can be written as the following equation according to the soft Bellman errors: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{a}_{t},r_{t},\\mathbf{s}_{t+1})\\sim\\mathcal{D}}\\left[\\frac{1}{2}\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})-\\left(r_{t}+\\gamma V_{\\theta}(\\mathbf{s}_{t+1})\\right)\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Nonetheless, directly using the objective in Eq. (4) presents challenges for two reasons. First, drawing samples from an energy-based model (i.e., $\\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\propto\\exp(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})/\\alpha))$ requires a costly MCMC or variational inference process [26, 37], which makes the interaction with the environment inefficient. Second, the calculation of the soft value function involves integration, which require stochastic approximation methods [28\u201330] to accomplish. To address these issues, the previous MaxEnt RL methods [8\u201314] adopted actor-critic frameworks and introduced a number of techniques to estimate the soft value function. These methods are discussed in the next subsection. ", "page_idx": 2}, {"type": "text", "text": "2.2 Actor-Critic Frameworks and Soft Value Estimation in MaxEnt RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Previous MaxEnt RL methods [8\u201315] employed actor-critic frameworks, in which the critic aims to capture the soft Q-function, while the actor learns to sample actions based on this soft Q-function. Available choices for modeling the actor include Gaussian models [9], Gaussian mixture models [38], variational autoencoders (VAE) [15, 13, 39], normalizing flows [10, 11], and amortized SVGD (A-SVGD) [8, 40], all of which support efficient sampling. The separation of the actor and the critic prevents the need for costly MCMC processes during sampling. However, this design induces additional training steps aimed to minimize the discrepancy between them. Let $\\begin{array}{r}{\\pi_{\\theta}(\\Breve{\\mathbf{a}_{t}}|\\mathbf{s}_{t})\\propto\\exp(\\frac{1}{\\alpha}Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t}))}\\end{array}$ and $\\pi_{\\phi}\\big(\\mathbf{a}_{t}\\big|\\mathbf{s}_{t}\\big)$ denote the pdfs defined through the critic and the actor, respectively. The objective of this additional training process is formulated according to the reverse KL divergence $\\mathbb{D}_{\\mathrm{KL}}\\big[\\pi_{\\phi}(\\mathbf{\\boldsymbol{\\cdot}}|\\mathbf{s}_{t})||\\pi_{\\theta}(\\mathbf{\\boldsymbol{\\cdot}}|\\mathbf{s}_{t})\\big]$ between $\\pi_{\\phi}$ and $\\pi_{\\theta}$ , and is typically reduced as follows [9]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\phi)=\\mathbb{E}_{\\mathbf{s}_{t}\\sim\\mathcal{D}}\\left[-\\mathbb{E}_{\\mathbf{a}_{t}\\sim\\pi_{\\phi}}[Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})-\\alpha\\log\\pi_{\\phi}(\\mathbf{a}_{t}|\\mathbf{s}_{t})]\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The optimization processes defined by the objective functions ${\\mathcal{L}}(\\theta)$ and ${\\mathcal{L}}(\\phi)$ in Eqs. (4) and (5) are known as the policy evaluation steps and policy improvement steps [9], respectively. Through alternating updates according to $\\nabla_{\\theta}\\mathcal{L}(\\theta)$ and $\\nabla_{\\phi}\\mathcal{L}(\\phi)$ , the critic learns directly from the reward signals to estimate the soft Q-function, while the actor learns to draw samples based on the distribution defined by the critic. ", "page_idx": 2}, {"type": "text", "text": "Although the introduction of the actor enhances sampling efficiency, calculating the soft value function in Eq. (3) still requires Monte Carlo approximations for the computationally infeasible integration operation. Prior soft value estimation methods can be categorized into two groups: soft value estimation in Soft Q-Learning (SQL) and that in Soft Actor-Critic (SAC), with the former yielding a larger estimate than the latter, derived from Jensen\u2019s inequality (i.e., Proposition A.1 in Appendix A.1). These two soft value estimation methods are discussed in the following paragraphs. ", "page_idx": 2}, {"type": "text", "text": "Soft Value Estimation in SQL. Soft Q-Learning [8] leverages importance sampling to convert the integration in Eq. (3) into an expectation, which can be estimated using a set of independent and identically distributed (i.i.d.) samples. To ensure the estimation variance is small, the authors in [8] proposed to utilize samples drawn from $\\pi_{\\phi}$ . Let $\\{\\mathbf{a}^{(i)}\\}_{i=1}^{M}$ be a set of $M$ samples drawn from $\\pi_{\\phi}$ The soft value function is approximated based on the following formula: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{\\theta}({\\bf s}_{t})=\\alpha\\log\\int\\exp\\left(Q_{\\theta}({\\bf s}_{t},{\\bf a})/\\alpha\\right)d{\\bf a}=\\alpha\\log\\int\\pi_{\\phi}({\\bf a}|{\\bf s}_{t})\\frac{\\exp\\left(Q_{\\theta}({\\bf s}_{t},{\\bf a})/\\alpha\\right)}{\\pi_{\\phi}({\\bf a}|{\\bf s}_{t})}d{\\bf a}\\ ~~}}\\\\ {{\\displaystyle~~~~~~=\\alpha\\log\\mathbb{E}_{{\\bf a}\\sim\\pi_{\\phi}}\\left[\\frac{\\exp\\left(Q_{\\theta}({\\bf s}_{t},{\\bf a})/\\alpha\\right)}{\\pi_{\\phi}({\\bf a}|{\\bf s}_{t})}\\right]\\approx\\alpha\\log\\left(\\frac{1}{M}\\sum_{i=1}^{M}\\frac{\\exp\\left(Q_{\\theta}({\\bf s}_{t},{\\bf a}^{(i)})/\\alpha\\right)}{\\pi_{\\phi}({\\bf a}^{(i)}|{\\bf s}_{t})}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Eq. (6) has the least variance when $\\pi_{\\phi}(\\cdot\\,|\\mathbf{s}_{t})\\propto\\exp(Q_{\\theta}(\\mathbf{s}_{t},\\cdot)/\\alpha)$ [29]. In addition, as $M\\rightarrow\\infty$ , the law of large numbers ensures that this estimation converge to $V_{\\theta}(\\mathbf{s}_{t})$ [41]. ", "page_idx": 2}, {"type": "text", "text": "Soft Value Estimation in SAC. Soft Actor-Critic [9] and its variants [10, 11, 13, 14, 12] reformulated the soft value function $\\begin{array}{r}{V_{\\theta}(\\mathbf{s}_{t})\\,=\\,\\alpha\\log\\int\\exp\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)d\\mathbf{a}}\\end{array}$ as its equivalent form $\\mathbb{E}_{\\mathbf{a}\\sim\\pi_{\\theta}}\\big[Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})-\\alpha\\log\\pi_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})\\big]$ based on the relationship that $\\begin{array}{r}{\\pi_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})=\\exp(\\frac{1}{\\alpha}(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}))-}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "$V_{\\theta}(\\mathbf{s}_{t}))$ . By assuming that the policy improvement loss ${\\mathcal{L}}(\\phi)$ is small (i.e., $\\pi_{\\theta}\\approx\\pi_{\\phi}$ ), the soft value function $V_{\\theta}$ can be estimated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{\\theta}({\\bf s}_{t})=\\mathbb{E}_{{\\bf a}\\sim\\pi_{\\theta}}[Q_{\\theta}({\\bf s}_{t},{\\bf a})-\\alpha\\log\\pi_{\\theta}({\\bf a}|{\\bf s}_{t})]}\\ ~}\\\\ {{\\displaystyle~~~~~~~~\\approx\\mathbb{E}_{{\\bf a}\\sim\\pi_{\\phi}}[Q_{\\theta}({\\bf s}_{t},{\\bf a})-\\alpha\\log\\pi_{\\phi}({\\bf a}|{\\bf s}_{t})]\\approx\\frac{1}{M}\\sum_{i=1}^{M}\\left(Q_{\\theta}({\\bf s}_{t},{\\bf a}^{(i)})-\\alpha\\log\\pi_{\\phi}({\\bf a}^{(i)}|{\\bf s}_{t})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "An inherent drawback of the estimation in Eq. (7) is its reliance on the assumption $\\pi_{\\phi}\\approx\\pi_{\\theta}$ . In addition, the second approximation involves Monte Carlo estimation with $M$ samples $\\{\\mathbf{a}^{(i)}\\}_{i=1}^{M}$ , where the computational cost increases with the number of samples . ", "page_idx": 3}, {"type": "text", "text": "2.3 Energy-Based Normalizing Flows ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Normalizing flows (i.e., flow-based models) are universal representations for pdf [42]. Given input data $\\mathbf{x}\\in\\mathbb{R}^{\\breve{D}}$ , a latent variable $\\mathbf{z}\\in\\mathbb{R}^{D}$ with prior pdf $p_{\\mathbf{z}}$ , and an invertible function $g_{\\theta}=g_{\\theta}^{L}\\circ\\cdots\\circ g_{\\theta}^{1}$ modeled as a neural network with $L$ layers, where $g_{\\theta}^{i}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D}$ , $\\forall i\\in\\{1,\\cdots\\,,L\\}$ . According to the change of variable theorem and the distributive property of the determinant operation, a parameterized pdf $p_{\\theta}$ can be described as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x})=p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{x})\\right)\\prod_{i=1}^{L}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{x}^{i-1})\\right)\\right|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{x}^{0}\\triangleq\\mathbf{x}$ is the input, $\\mathbf{x}^{i}=g_{\\theta}^{i}\\circ\\cdots\\circ g_{\\theta}^{1}(\\mathbf{x})$ is the output of the $i$ -th layer, and $\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{x}^{i-1})\\triangleq$ $\\frac{\\partial}{\\partial\\mathbf{x}^{i-1}}g_{\\theta}^{i}(\\mathbf{x}^{i-1})$ represents the Jacobian of the $i$ -th layer of $g_{\\theta}$ with respect to $\\mathbf{x}^{i-1}$ . To dra\u03b8w samples from $p_{\\theta}$ , one can first sample $\\mathbf{z}$ from $p_{\\mathbf{z}}$ and then derive $g_{\\boldsymbol{\\theta}}^{-1}(\\mathbf{z})$ . To facilitate efficient computation of the pdf and the inverse of $g_{\\boldsymbol{\\theta}}$ , one can adopt existing architectural designs [43\u201348] for $g_{\\theta}$ . Popular examples involve autoregressive layers [43\u201345] and coupling layers [46\u201348], which utilizes specially designed architectures to speed up the calculation. ", "page_idx": 3}, {"type": "text", "text": "Energy-Based Normalizing Flows (EBFlow) [31] were recently introduced to reinterpret flow-based models as energy-based models. In contrast to traditional normalizing flow research [46, 47, 49, 50] that focuses on the use of effective non-linearities, EBFlow emphasizes the use of both linear and non-linear transformations in the invertible transformation $g_{\\theta}$ . Such a concept was inspired by the development of normalizing flows with convolution layers [48, 51\u201354] or fully-connected layers [55, 56], linear independent component analysis (ICA) models [57, 58], as well as energy-based training techniques [58\u201360]. Let $S_{l}=\\left\\{i\\left|\\right.g_{\\theta}^{i}\\right.$ is linear $\\}$ and $S_{n}=\\left\\{i\\left|\\right.g_{\\theta}^{i}\\right.$ is non-linear} represent the sets of indices of the linear and non-linear transformations in $g_{\\theta}$ , respectively. As shown in [31], the Jacobian determinant product in Eq. (8) can be decomposed according to ${\\mathcal{S}}_{n}$ and $\\mathcal{S}_{l}$ . This decomposition allows a flow-based model to be reinterpreted as an energy-based model, as illustrated in the following equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x})=\\underbrace{p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{x})\\right)\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{x}^{i-1})\\right)\\right|}_{\\mathrm{Unormalized~Density}}\\underbrace{\\prod_{i\\in S_{l}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}\\right)\\right|}_{\\mathrm{Const.}}\\triangleq\\underbrace{\\exp\\left(-E_{\\theta}(\\mathbf{x})\\right)}_{\\mathrm{Unnormalized~Density}}\\underbrace{Z_{\\theta}^{-1}}_{\\mathrm{Const.}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In EBFlow, the energy function $E_{\\theta}(\\mathbf{x})$ is defined as $\\begin{array}{r}{-\\log(p_{\\mathbf{z}}\\left(g_{\\boldsymbol{\\theta}}(\\mathbf{x})\\right)\\prod_{i\\in S_{n}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\boldsymbol{\\theta}}^{i}}(\\mathbf{x}^{i-1}))\\vert)}\\end{array}$ and the normalizing constant $\\begin{array}{r}{Z_{\\theta}=\\int\\exp(-E_{\\theta}(\\mathbf{x}))d\\mathbf{x}=\\prod_{i\\in S_{l}}\\vert\\operatorname*{det}{\\mathbf{J}_{g_{\\theta}^{i}}}\\vert^{-1}}\\end{array}$ is independent of $\\mathbf{x}$ . The input-independence of $Z_{\\theta}$ holds since $g_{\\theta}^{i}$ is either a first-degree or zero-degree polynomial for any $i\\in S_{l}$ , and thus its Jacobian is a constant to $\\mathbf{x}^{i-1}$ . This technique was originally proposed to reduce the training cost of maximum likelihood estimation for normalizing flows. However, we discovered that EBFlow is ideal for MaxEnt RL. Its unique capability to represent a parametric energy function with an associated sampler $g_{\\theta}^{-1}$ , and to calculate a normalizing constant $Z_{\\theta}$ without integration are able to address the challenges mentioned in Section 2.2. We discuss our insights in the next section. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce our proposed MaxEnt RL framework modeled using EBFlow. In Section 3.1, we describe the formulation and discuss its training and inference processes. In Section 3.2, ", "page_idx": 3}, {"type": "text", "text": "we present two techniques for improving the training of our framework. Ultimately, in Section 3.3, we offer an algorithm summary. ", "page_idx": 4}, {"type": "text", "text": "3.1 MaxEnt RL via EBFlow ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose a new framework for modeling MaxEnt RL using EBFlow, which we call MEow. This framework possesses several unique features. First, as EBFlow enables simultaneous modeling of an unnormalized density and its sampler, MEow can unify the actor and the critic previously separated in MaxEnt RL frameworks. This feature facilitates the integration of policy improexvement steps with policy evaluation steps, and results in a single objective training process. Second, the normalizing constant of EBFlow is expressed in closed form, which enables the calculation of the soft value function without resorting to the approximation methods mentioned in Eqs. (6) and (7). Third, given that normalizing flow is a universal approximator for probability density functions, our policy\u2019s expressiveness is not constrained, and can model multi-modal action distributions. ", "page_idx": 4}, {"type": "text", "text": "In MEow, the policy is described as a state-conditioned EBFlow, with its pdf presented as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})=p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\right)\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}\\big(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}\\big)\\right)\\right|\\prod_{i\\in S_{l}}\\left|\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t}))\\right|\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\triangleq\\underbrace{\\exp\\left(\\frac{1}{\\alpha}Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\right)}_{\\mathrm{Unnormalized\\:Density}}\\underbrace{\\exp\\left(-\\frac{1}{\\alpha}V_{\\theta}(\\mathbf{s}_{t})\\right)}_{\\mathrm{Norm.\\:Const.}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the soft Q-function and the soft value function are selected as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n2\\theta(\\mathbf{s}_{t},\\mathbf{a}_{t})\\triangleq\\alpha\\log p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\right)\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t})\\right)\\right|,V_{\\theta}(\\mathbf{s}_{t})\\triangleq-\\alpha\\log\\prod_{i\\in S_{l}}\\left|\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t}))\\right|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Such a selection satisfies $\\begin{array}{r}{V_{\\theta}(\\mathbf{s}_{t})=\\alpha\\log\\int\\exp(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha)d\\mathbf{a}}\\end{array}$ based on the property of EBFlow. In addition, both $Q_{\\theta}$ and ${V}_{\\theta}$ have a common output codomain $\\mathbb{R}$ , which enables them to learn to output arbitrary real values. These properties are validated in Proposition 3.1, with the proof provided in Appendix A.2. The training and inference processes of MEow are summarized as follows. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. Eq. $(I I)$ satisfies the following statements: $(I)$ Given that the Jacobian of g\u03b8 is non-singular, $V_{\\theta}(\\mathbf{s}_{t})\\ \\in\\ \\mathbb{R}$ and $Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\in\\ \\mathbb R$ , $\\forall\\mathbf{a}_{t}\\ \\in\\ A,\\forall\\mathbf{s}_{t}\\ \\in\\ S$ . (2) $V_{\\theta}(\\mathbf{s}_{t})\\ =$ $\\begin{array}{r}{\\operatorname{\\acute{\\alpha}}\\log\\int\\exp\\left(\\bar{Q_{\\theta}}(\\mathbf{s}_{t},\\mathbf{a})/\\dot{\\alpha}\\right)\\acute{d}}\\end{array}$ a. ", "page_idx": 4}, {"type": "text", "text": "Training. With $Q_{\\theta}$ and $V_{\\theta}$ defined in Eq. (11), the loss ${\\mathcal{L}}(\\theta)$ in Eq. (4) can be calculated without using Monte Carlo approximation of the soft value function target. Compared to the previous MaxEnt RL frameworks that rely on Monte Carlo estimation (i.e., Eqs. (6) and (7)), our framework offers the advantage of avoiding the errors induced by the approximation. In addition, MEow employs a unified policy rather than two separate roles (i.e., the actor and the critic), which eliminates the need for minimizing an additional policy improvement loss ${\\mathcal{L}}(\\phi)$ to bridge the gap between $\\pi_{\\theta}$ and $\\pi_{\\phi}$ . This simplifies the training process of MaxEnt RL, and obviates the requirement of balancing between the two optimization loops. ", "page_idx": 4}, {"type": "text", "text": "Inference. The sampling process of $\\pi_{\\theta}$ can be efficiently performed by deriving the inverse of $g_{\\boldsymbol{\\theta}}$ , as supported by several normalizing flow architectures [43\u201348]. In addition, unlike previous actor-critic frameworks susceptible to discrepancies between $\\pi_{\\theta}$ and $\\pi_{\\phi}$ , the distribution established via $g_{\\theta}^{-1}(\\mathbf{z}|\\mathbf{s}_{t})$ , where $\\mathbf{z}\\sim p_{\\mathbf{z}}$ , is consistently aligned with the pdf defined by $Q_{\\theta}$ . As a result, the actions taken by MEow can precisely reflect the learned soft Q-function. ", "page_idx": 4}, {"type": "text", "text": "3.2 Techniques for Improving the Training and Inference Processes of MEow ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we introduce a number of training and inference techniques aimed at improving MEow while preserving its key features discussed in the previous subsection. For clarity, we refer to the MEow framework introduced in the last section as \u2018MEow (Vanilla)\u2019. ", "page_idx": 4}, {"type": "text", "text": "Learnable Reward Shifting (LRS). Reward shifting [61\u201365] is a technique for shaping the reward function. This technique enhances the learning process by incorporating a shifting term in the reward function, which leads to a shifted optimal soft Q-function in MaxEnt RL. Inspired by this, this work proposes modeling a reward shifting function $b_{\\theta}:\\mathcal{S}\\rightarrow\\mathbb{R}$ with a neural network to enable the automatic learning of a reward shifting term. For notational simplicity, the parameters are denoted using $\\theta$ , and the details of the architecture are presented in Appendix A.5.1. The soft Q-function $Q_{\\theta}^{b}$ augmented by $b_{\\theta}$ is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{\\theta}^{b}(\\mathbf{s}_{t},\\mathbf{a}_{t})=Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})+b_{\\theta}(\\mathbf{s}_{t}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The introduction of $Q_{\\theta}^{b}$ results in a corresponding shifted soft value function $V_{\\theta}^{b}(\\mathbf{s}_{t})$ \u225c $\\begin{array}{r l r}{\\alpha\\log\\int\\exp(Q_{\\theta}^{b}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha)d\\mathbf{a}}&{=}&{V_{\\theta}(\\mathbf{s}_{t})\\;+\\;b_{\\theta}(\\mathbf{s}_{t})}\\end{array}$ (i.e., Proposition A.3 in Appendix A.2), which can be calculated without Monte Carlo estimation. Moreover, with the incorporation of $b_{\\theta}$ , the policy $\\pi_{\\theta}$ remains invariant since $\\begin{array}{r l}{\\exp(\\frac{1}{\\alpha}(Q_{\\theta}^{b}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\ -\\ V_{\\theta}^{b}(\\mathbf{s}_{t})))}&{{}=}\\end{array}$ $\\begin{array}{r}{\\exp(\\frac{1}{\\alpha}((Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\,+\\,b_{\\theta}(\\mathbf{s}_{t}))\\,-\\,(V_{\\theta}(\\mathbf{s}_{t})\\,+\\,b_{\\theta}(\\mathbf{s}_{t}))))\\;=\\;\\exp(\\frac{1}{\\alpha}(\\bar{Q}_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\,-\\,V_{\\theta}(\\mathbf{s}_{t}))).}\\end{array}$ ), which allows the use of $g_{\\theta}^{-1}$ for efficiently sampling actions. As evidenced in Fig. 1, this method effectively addresses the issues of the significant growth and decay of Jacobian determinants of $g_{\\boldsymbol{\\theta}}$ (discussed in Appendix A.3). In Section 4.4, we further demonstrate that the performance of MEow can be significantly improved through this technique. ", "page_idx": 5}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/07c6f7e9c076f76bca0fe2a9aafdd331ea79f1bee025ee2c6c10cd0f58f55175.jpg", "img_caption": ["Figure 1: The Jacobian determinant products for (a) the non-linear and (b) the linear transformations, evaluated during training in the Hopper-v4 environment. Subfigure (b) is presented on a log scale for better visualization. This experiment adopt the affine coupling layers [47] as the nonlinear transformations. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Shifting-Based Clipped Double Q-Learning (SCDQ). As observed in [66], the overestimation of value functions often occurs in training. To address this issue, the authors in [66] propose clipped double Q-learning, which employs two separate Q-functions and uses the one with the smaller output to estimate the value function during training. This technique is also used in MaxEnt RL frameworks [9\u201313]. Inspired by this and our proposed learnable reward shifting, we further propose a shifting-based method that adopts two learnable reward shifting functions, $\\bar{b}_{\\theta}^{(1)}$ and $b_{\\theta}^{(2)}$ , without duplicating the soft Q-function $Q_{\\theta}$ and soft value function $V_{\\theta}$ defined by $g_{\\theta}$ . The soft Q-functions $Q_{\\theta}^{(\\bar{1})}$ and $\\bar{Q}_{\\theta}^{(2)}$ with corresponding learnable reward shifting functions $b_{\\theta}^{(1)}$ and $b_{\\theta}^{(2)}$ can be obtained using Eq. (12), while the soft value function $V_{\\theta}^{\\mathrm{clip}}$ is written as the following formula: ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{\\theta}^{\\mathrm{clip}}(\\mathbf{s}_{t})=\\mathrm{min}\\left(V_{\\theta}(\\mathbf{s}_{t})+b_{\\theta}^{(1)}(\\mathbf{s}_{t}),V_{\\theta}(\\mathbf{s}_{t})+b_{\\theta}^{(2)}(\\mathbf{s}_{t})\\right)=V_{\\theta}(\\mathbf{s}_{t})+\\mathrm{min}\\left(b_{\\theta}^{(1)}(\\mathbf{s}_{t}),b_{\\theta}^{(2)}(\\mathbf{s}_{t})\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This design also prevents the production of two policies in MEow, as having two policies can complicate the inference procedure. In our ablation analysis presented in Section 4.4, we demonstrate that this technique can effectively improve the training process of MEow. ", "page_idx": 5}, {"type": "text", "text": "Deterministic Policy for Inference. As observed in [8], deterministic actors typically performed better as compared to its stochastic variant during the inference time. Such a problem can be formalized as finding an action a that maximizes $Q(\\mathbf{s}_{t},\\mathbf{a})$ for a given $\\mathbf{s}_{t}$ . Since $\\boldsymbol{\\mathcal{A}}$ is a continuous space, finding such a value would require extensive calculation. In the MEow framework, this value can be derived by making assumptions on the model architecture construction. Our key observation is that if the Jacobian determinants of the non-linearities (i.e., $g_{\\theta}^{i}\\in\\mathcal{S}_{n})$ are constants with respect to its inputs, and that $\\operatorname{argmax}_{\\mathbf{z}}p_{\\mathbf{z}}(\\mathbf{z})$ can be directly obtained, then the action a that maximizes $Q(\\mathbf{s}_{t},\\mathbf{a})$ can be efficiently derived according to the following proposition. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.2. Given that $|\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}^{i-1}|\\mathbf{s}_{t}))|$ is a constant with respect to $\\mathbf{a}^{i-1}$ , then $\\begin{array}{r}{g_{\\boldsymbol\\theta}^{-1}(\\operatorname{argmax}_{\\mathbf{z}}p_{\\mathbf{z}}(\\mathbf{z})|\\mathbf{s}_{t})=\\operatorname{argmax}_{\\mathbf{a}}Q_{\\boldsymbol\\theta}(\\mathbf{s}_{t},\\mathbf{a})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The proof is provided in Appendix A.2. It is important to note that $g_{\\theta}^{i}$ can still be a non-linear transformation, given that $|\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}^{i-1}|\\mathbf{s}_{t}))|$ is a constant. To construct such a model, a Gaussian prior with the additive coupling transformations [46] can be used as non-linearities. Under such a design, an action can be derived by calculating $g_{\\boldsymbol{\\theta}}^{-1}\\big(\\mu|\\mathbf{s}_{t}\\big)$ , where $\\mu$ represents the mean of the ", "page_idx": 5}, {"type": "text", "text": "Input: Learnable parameters $\\theta$ and shadow parameters $\\theta^{\\prime}$ . Target smoothing factor $\\tau$ . Learning rate $\\beta$ .   \nNeural networks $g_{\\theta}\\big(\\cdot\\,\\big|\\cdot\\big),b_{\\theta}^{(1)}\\big(\\cdot\\big)$ , and $b_{\\theta}^{(2)}(\\cdot)$ . Temperature parameter $\\alpha$ . Discount factor $\\gamma$ .   \n1: for each training step do   \n2: $\\triangleright$ Extend the Replay Buffer.   \n3: $\\mathbf{a}_{t}=g_{\\theta}^{-1}(\\mathbf{z}|\\mathbf{s}_{t}),\\,\\mathbf{z}\\sim p_{\\mathbf{z}}(\\cdot).$ .   \n4: $\\mathbf{s}_{t+1}\\sim p_{T}\\big(\\cdot|\\mathbf{s}_{t},\\mathbf{a}_{t}\\big)$ .   \n5: $\\begin{array}{r}{\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{(\\mathbf{s}_{t},\\mathbf{a}_{t},r_{t},\\mathbf{s}_{t+1})\\}.}\\end{array}$ .   \n6: $\\triangleright$ Update Policy.   \n7: $(\\mathbf{s}_{t},\\mathbf{a}_{t},r_{t},\\mathbf{s}_{t+1})\\sim\\mathcal{D}.$ .   \n8: $\\begin{array}{r l r}&{\\mathsf{\\Gamma}_{Q\\theta}^{(\\mathsf{v}_{t},\\mathsf{a}_{t}),\\mathsf{\\Gamma}_{t},\\mathsf{e}_{t+1}+\\imath\\mathsf{\\Gamma}_{f}^{\\sim}\\cdot\\mathsf{\\Gamma}_{\\theta}^{\\sim}}&{\\quad\\forall\\mathsf{E q}\\left(\\mathbf{a}_{t}|\\mathbf{s}_{t}\\right)\\prod_{i\\in S_{n}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}))\\vert).}&{\\quad\\mathrm{~b~Eq.~}}\\\\ &{\\quad}&{V_{\\theta^{\\prime}}(\\mathbf{s}_{t+1})=-\\alpha\\log\\prod_{i\\in S_{t}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta^{\\prime}}^{i}}(\\mathbf{s}_{t+1}))\\vert.}&{\\quad\\mathrm{~b~Eq.~}}\\\\ &{\\quad}&{Q_{\\theta}^{(1)}(\\mathbf{s}_{t},\\mathbf{a}_{t})=Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})+b_{\\theta}^{(1)}(\\mathbf{s}_{t})\\mathrm{~and~}Q_{\\theta}^{(2)}(\\mathbf{s}_{t},\\mathbf{a}_{t})=Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})+b_{\\theta}^{(2)}(\\mathbf{s}_{t}).}&{\\quad\\mathrm{b~Eq.~}}\\\\ &{\\quad}&{V_{\\theta^{\\prime}}^{\\mathrm{chp}}(\\mathbf{s}_{t+1})=V_{\\theta^{\\prime}}(\\mathbf{s}_{t+1})+\\operatorname*{min}\\left(b_{\\theta^{\\prime}}^{(1)}(\\mathbf{s}_{t+1}),b_{\\theta^{\\prime}}^{(2)}(\\mathbf{s}_{t+1})\\right).}&{\\quad\\mathrm{b~Eq.~}}\\\\ &{\\quad}&{\\mathcal{L}(\\theta)=\\frac{1}{2}(Q_{\\theta}^{(1)}(\\mathbf{s}_{t},\\mathbf{a}_{t})-(r_{t}+\\gamma V_{\\theta^{\\prime}}^{\\mathrm{dip}}(\\mathbf{s}_{t+1})))^{2}+\\frac{1}{2}(Q_{\\theta}^{(2)}(\\mathbf{s}_{t},\\mathbf{a}_{t})-(r_{t}+\\gamma V_{\\theta^{\\prime} $ (11)   \n9: (11)   \n10: (12)   \n11: (13)   \n12: (4)   \n13:   \n14:   \n15: end for ", "page_idx": 6}, {"type": "text", "text": "Gaussian distribution. We elaborate on our model architecture design in Appendix A.5.1, and provide a performance comparison between MEow evaluated using a stochastic policy (i.e., $\\mathbf{a}_{t}\\sim\\pi_{\\theta}\\big(\\cdot|\\mathbf{s}_{t}\\big))$ and a deterministic policy (i.e., $\\mathbf{a}_{t}=\\operatorname{argmax}_{\\mathbf{a}}Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}))$ in Section 4.4. ", "page_idx": 6}, {"type": "text", "text": "3.3 Algorithm Summary ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We summarize the training process of MEow in Algorithm 1. The algorithm integrates the policy evaluation steps with the policy improvement steps, resulting in a single loss training process. This design differs from previous actor-critic frameworks, which typically perform two consecutive updates in each training step. In Algorithm 1, the learning rate is denoted as $\\beta$ . A set of shadow parameters $\\theta^{\\prime}$ is maintained for calculating the delayed target values [67], and is updated according to the Polyak averaging [68] of $\\theta$ , i.e., $\\theta^{\\prime}\\bar{\\leftarrow}\\,(1-\\tau\\bar{)}\\theta^{\\prime}+\\tau\\bar{\\theta}$ , where $\\tau$ is the target smoothing factor. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the following sections, we first present an intuitive example of MEow trained in a two-dimensional multi-goal environment [8] in Section 4.1. We then compare MEow\u2019s performance against several continuous-action RL baselines in five MuJoCo environments [32, 33] in Section 4.2. Next, in Section 4.3, we evaluate MEow\u2019s performance on a number of Omniverse Isaac Gym environments [34] simulated based on real-world robotic application scenarios. Lastly, in Section 4.4, we provide an ablation analysis to inspect the effectiveness of each proposed technique. Among all experiments, we maintain the same model architecture, while adjusting inputs and outputs according to the state space and action space for each environment. We construct $g_{\\theta}$ using the additive coupling layers [46] with element-wise linear transformations, utilize a unit Gaussian as $p_{\\mathbf{z}}$ , and model the learnable adaptive reward shifting functions $b_{\\theta}$ as multi-layer perceptrons (MLPs). For detailed descriptions of the experimental setups, please refer to Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "4.1 Evaluation on a Multi-Goal Environment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we present an example of MEow trained in a two-dimensional multi-goal environment [8]. The environment involves four goals, indicated by the red dots in Fig. 2 (a). The reward function is defined by the negative Euclidean distance from each state to the nearest goal, and the corresponding reward landscape is depicted using contours in Fig. 2 (a). The gradient map in Fig. 2 (a) represents the soft value function predicted by our model. The blue lines extending from the center represent the trajectories produced using our policy. ", "page_idx": 6}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/e1dd690041fd9366f91a46e4166ad9d9ee5e2a057061e597521118e8db11cc26.jpg", "img_caption": ["Figure 3: The results in terms of total returns versus the number of training steps evaluated on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the $95\\%$ confidence intervals, derived from five independent runs with different seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "As illustrated in Fig. 2 (a), our model\u2019s soft value function predicts higher values around the goals, suggesting successful learning of the goal positions through rewards. In addition, the trajectories demonstrate our agent\u2019s correct transitions towards the goals, which validates the effectiveness of our learned policy. To illustrate the potential impact of approximation errors that might emerge when employing previous soft value estimation methods, we compare three calculation methods for the soft value function: (I) Our approach (i.e., Eq. (11)): $V_{\\theta}(\\mathbf{s}_{t})$ , (II) SQL-like (i.e., Eq. (6)): \u03b1 log( M1  iM=1exp(\u03c0Q\u03d5\u03b8((as(ti,)a|(sit)))/\u03b1)), ", "page_idx": 7}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/4fb34e6e03bc5c6161c60d8712ca88dec75adf8f1e7ecf0b1c57affebcdece4a.jpg", "img_caption": ["Figure 2: (a) The soft value function and the trajectories generated using our method on the multi-goal environment. (b) The estimation error evaluated at the initial state under different choices of $M$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "and (III) SAC-like (i.e., Eq. (7)): $\\begin{array}{r}{\\frac{1}{M}\\sum_{i=1}^{M}(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}^{(i)})-\\alpha\\log\\pi_{\\phi}(\\mathbf{a}^{(i)}|\\mathbf{s}_{t}))}\\end{array}$ , where $\\{\\mathbf{a}^{(i)}\\}_{i=1}^{M}$ is sampled from $\\pi_{\\phi}$ . The approximation errors of the soft value functions at the initial state are calculated using the Euclidean distances between (I) and (II), and between (I) and (III), for various values of $M$ . As depicted in Fig. 2 (b), the blue line and the orange line decreases slowly with respect to $M$ . These results suggest that Monte Carlo estimation converges slowly, making approximation methods such as Eqs. (6) and (7) challenging to achieve accurate predictions. ", "page_idx": 7}, {"type": "text", "text": "4.2 Performance Comparison on the MuJoCo Environments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this experiment, we compare MEow with several commonly-used continuous control algorithms on five MuJoCo environments [32] from Gymnasium [33]. The baseline algorithms include SQL [8], SAC [9], deep deterministic policy gradient (DDPG) [69], twin delayed deep deterministic policy gradient (TD3) [66], and proximal policy optimization (PPO) [70]. The results for SAC, DDPG, TD3, and PPO were reproduced using Stable Baseline 3 (SB3) [71], utilizing SB3\u2019s refined hyperparameters. The results for SQL were reproduced using our own implementation, as SB3 does not support SQL and the official code is not reproducible. Our implementation adheres to SQL\u2019s original paper. Each method is trained independently under five different random seeds, and the evaluation curves for each environment are presented in the form of the means and the corresponding confidence intervals. ", "page_idx": 7}, {"type": "text", "text": "As depicted in Fig. 3, MEow performs comparably to SAC and outperforms the other baseline algorithms in most of the environments. Furthermore, in environments with larger action and state dimensionalities, such as \u2018Ant-v4\u2019 and \u2018Humanoid-v4\u2019, MEow offers performance improvements over SAC and exhibits fewer spikes in the evaluation curves. These results suggest that MEow is capable of performing high-dimensional tasks with stability. To further investigate the performance difference between MEow and SAC, we provide a thorough comparison between MEow, SAC [9], Flow-SAC [10, 11], and their variants in Appendix A.4.2. The results indicate that the training process involving policy evaluation and policy improvement steps may be inferior to our proposed training process with a single objective. In the next subsection, we provide a performance examination using the simulation environments from the Omniverse Isaac Gym [34]. ", "page_idx": 7}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/aa1a0ca8c6cb5731eab8333d9fa312a09df1c57ca0362c957ad97e8f8ad98694.jpg", "img_caption": ["Figure 4: A comparison on six Isaac Gym environments. Each curve represents the mean performance of five runs, with shaded areas indicating the $95\\%$ confidence intervals. \u2018Steps\u2019 in the $\\mathbf{X}_{\\mathrm{~}}$ -axis represents the number of training steps, each of which consists of $N$ parallelizable interactions with the environments. ", "Figure 5: A demonstration of the six Isaac Gym environments introduced in Section 4.3. The dimensionalities of the state and action for each environment are denoted below each subfigure. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Performance Comparison on the Omniverse Issac Gym Environments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we examine the performance of MEow on a variety of robotic tasks simulated by Omniverse Isaac Gym [34], a GPU-based physics simulation platform. In addition to \u2018Ant\u2019 and \u2018Humanoid\u2019, we employ four additional tasks: \u2018Ingenuity\u2019, \u2018ANYmal\u2019, \u2018AllegroHand\u2019, and \u2018FrankaCabinet\u2019. All of them are designed based on real-world robotic application scenarios. \u2018Ingenuity\u2019 and \u2018ANYmal\u2019 are locomotion environments inspired by NASA\u2019s Ingenuity helicopter and ANYbotics\u2019 industrial maintenance robots, respectively. On the other hand, \u2018AllegroHand\u2019 and \u2018FrankaCabinet\u2019 focus on executing specialized manipulative tasks with robotic hands and arms, respectively. A demonstration of these tasks is illustrated in Fig. 5. ", "page_idx": 8}, {"type": "text", "text": "In this experimental comparison, we adopt SAC as a baseline due to its excellent performance in the MuJoCo environments. The evaluation results are presented in Fig. 4. The results demonstrate that MEow exhibits superior performance on \u2018Ant (Isaac)\u2019 and \u2018Humanoid (Isaac)\u2019. In addition, MEow consistently outperforms SAC across the four robotic environments (i.e., \u2018Ingenuity\u2019, \u2018ANYmal\u2019, \u2018AllegroHand\u2019, and \u2018FrankaCabinet\u2019), indicating that our algorithm possesses the ability to perform challenging robotic tasks simulated based on real-world application scenarios. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we provide an ablation analysis to examine the effectiveness of each technique introduced in Section 3.2. ", "page_idx": 8}, {"type": "text", "text": "Training Techniques. Fig. 6 compares the performance of three variants of MEow: \u2018MEow (Vanilla)\u2019, \u2018MEow $\\mathrm{(+LRS)}^{\\ast}$ , and \u2018MEow $\\scriptstyle\\mathbf{+LRS}$ & SCDQ)\u2019, across five MuJoCo environments. The results show that \u2018MEow (Vanilla)\u2019 consistently underperforms, with its total returns demonstrating negligible or no growth throughout the training period. In contrast, the variants incorporating translation functions demonstrate significant performance enhancements. This observation highlights the importance of including $b_{\\theta}$ in the model design. In addition, the comparison between \u2018MEow $(\\mathrm{+LRS})^{\\mathrm{,}}$ and \u2018MEow (+LRS & SCDQ)\u2019 suggests that our reformulated approach to clipped double Q-learning [66] improves the final performance by a noticeable margin. ", "page_idx": 8}, {"type": "text", "text": "Inference Technique. Fig. 7 compares the performance of two variants of MEow: \u2018MEow (Stochastic)\u2019 and \u2018MEow (Deterministic)\u2019. The former samples action based on $\\mathbf{a}_{t}\\sim\\pi_{\\theta}(\\cdot\\left|\\mathbf{s}_{t}\\right)$ while the latter derive action according to $\\begin{array}{r}{\\mathbf{a}_{t}=\\operatorname*{argmax}_{\\mathbf{a}}Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})=g_{\\theta}^{-1}(\\mu|\\mathbf{s}_{t})}\\end{array}$ . As shown in the figure, MEow with a deterministic policy outperforms its stochastic variant, suggesting that a deterministic policy may be more effective for MEow\u2019s inference. ", "page_idx": 8}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/d8449de794f8e7930989380e9ace0896b57c03ccd8d0c75b46b91fb39f22aa70.jpg", "img_caption": ["Figure 6: The performance comparison of MEow\u2019s variants (i.e., \u2018MEow (Vanilla)\u2019, \u2018MEow $(+\\mathrm{LRS})^{\\mathrm{;}}$ \u2019, and \u2018MEow $+\\mathrm{LRS}$ & SCDQ)\u2019) on five MuJoCo environments. Each curve represents the mean performance of five runs, with shaded areas indicating the $95\\%$ confidence intervals. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/02ed98884300ebc61c8f9afa14f59a38d4c3b32efcd8772efeedd0839278aada.jpg", "img_caption": ["Figure 7: Performance comparison between MEow with a deterministic policy and MEow with a stochastic policy on five MuJoCo environments. Each curve represents the mean performance of five runs, with shaded areas indicating the $95\\%$ confidence intervals. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce MEow, a unified MaxEnt RL framework that facilitates exact soft value calculations without the need for Monte Carlo estimation. We demonstrate that MEow can be optimized using a single objective function, which streamlines the training process. To further enhance MEow\u2019s performance, we incorporate two techniques, learnable reward shifting and shiftingbased clipped double Q-learning, into the design. We examine the effectiveness of MEow via experiments conducted in five MoJoCo environments and six robotic tasks simulated by Omniverse Isaac Gym. The results validate the superior performance of MEow compared to existing approaches. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As discussed in Section 3.2, deterministic policies typically offer better performance compared to their stochastic counterparts. Although our implementation of MEow supports deterministic inference, this capability is based on the assumptions that the Jacobian determinants of the non-linear transformations are constants with respect to their inputs, and that argmaxz $p_{\\mathbf{z}}(\\mathbf{z})$ can be efficiently derived. These assumptions may not hold for certain types of flow-based models. Therefore, exploring effective architectural choices for MEow represents a promising direction for further investigation. ", "page_idx": 9}, {"type": "text", "text": "On the other hand, the training speed of MEow is around $2.3\\times$ slower than that of SAC, even though updates according to ${\\mathcal{L}}(\\phi)$ are bypassed in MEow. According to our experimental observations, the computational bottleneck of MEow may lie in the inference speed of the flow-based model during interactions with environments. While this speed is significantly faster than many iterative methods, such as MCMC or variational inference, it is still slower compared to the inference speed of Gaussian models. As a result, enhancing the inference speed of flow-based models represents a potential avenue for further improving the training efficiency of MEow. ", "page_idx": 9}, {"type": "text", "text": "Finally, our hyperparameter sensitivity analysis, as presented in A.4.5, indicates that our current approach requires different values of $\\tau$ to achieve optimal performance. Since hyperparameter tuning often demands significant computational resources, establishing a more generalized parameter setting or developing an automatic tuning mechanism for $\\tau$ presents an important direction for future exploration. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors gratefully acknowledge the support from the National Science and Technology Council (NSTC) in Taiwan under grant numbers MOST 111-2223-E-002-011-MY3, NSTC 113-2221-E-002- 212-MY3, and NSTC 113-2640-E-002-003. The authors would also like to express their appreciation for the computational resources from NVIDIA Corporation and NVIDIA AI Technology Center (NVAITC) used in this work. Furthermore, the authors extend their gratitude to the National Center for High-Performance Computing (NCHC) for providing the necessary computational and storage resources. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] H J Kappen. Path Integrals and Symmetry Breaking for Optimal Control Theory. Journal of Statistical Mechanics: Theory and Experiment, 2005.   \n[2] Brian Ziebart, Andrew Maas, J. Bagnell, and Anind Dey. Maximum Entropy Inverse Reinforcement Learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2008.   \n[3] Marc Toussaint. Robot Trajectory Optimization using Approximate Inference. In Proceedings of the International Conference on Machine Learning (ICML), 2009.   \n[4] Brian D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, USA, 2010.   \n[5] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2012.   \n[6] Roy Fox, Ari Pakman, and Naftali Tishby. Taming the Noise in Reinforcement Learning via Soft Updates. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2016.   \n[7] Brendan O\u2019Donoghue, R\u00e9mi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. PGQ: Combining Policy Gradient and Q-learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.   \n[8] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with Deep Energy-Based Policies. In Proceedings of the International Conference on Machine Learning (ICML), 2017.   \n[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Proceedings of the International Conference on Machine Learning (ICML), 2017.   \n[10] Tuomas Haarnoja, Kristian Hartikainen, P. Abbeel, and Sergey Levine. Latent Space Policies for Hierarchical Reinforcement Learning. In Proceedings of the International Conference on Machine Learning (ICML), 2018.   \n[11] Bogdan Mazoure, Thang Doan, Audrey Durand, R Devon Hjelm, and Joelle Pineau. Leveraging Exploration in Off-policy Algorithms via Normalizing Flows. In Proceedings of the Conference on Robot Learning (CoRL), 2019.   \n[12] Patrick Nadeem Ward, Ariella Smofsky, and A. Bose. Improving Exploration in Soft-ActorCritic with Normalizing Flows Policies. 2019.   \n[13] Dinghuai Zhang, Aaron Courville, Yoshua Bengio, Qinqing Zheng, Amy Zhang, and Ricky T. Q. Chen. Latent State Marginalization as a Low-cost Approach to Improving Exploration. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[14] Safa Messaoud, Billel Mokeddem, Zhenghai Xue, Linsey Pang, Bo An, Haipeng Chen, and Sanjay Chawla. S2AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic. In Proceedings of the International Conference on Learning Representations (ICLR), 2024.   \n[15] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.   \n[16] Wenjie Shi, Shiji Song, and Cheng Wu. Soft Policy Gradient Method for Maximum Entropy Deep Reinforcement Learning. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2019.   \n[17] Benjamin Eysenbach and Sergey Levine. Maximum Entropy RL Provably Solves Some Robust RL Problems. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.   \n[18] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a Posteriori Policy Optimisation. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.   \n[19] Kyungjae Lee, Sungyub Kim, Sungbin Lim, Sungjoon Choi, and Songhwai Oh. Tsallis Reinforcement Learning: A Unified Framework for Maximum Entropy Reinforcement Learning. ArXiv, abs/1902.00137, 2019.   \n[20] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, G. Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, P. Abbeel, and Sergey Levine. Soft Actor-Critic Algorithms and Applications. ArXiv, abs/1812.05905, 2018.   \n[21] Kwan-Woo Park, MyeongSeop Kim, Jung-Su Kim, and Jae-Han Park. Path Planning for Multi-Arm Manipulators Using Soft Actor-Critic Algorithm with Position Prediction of Moving Obstacles via LSTM. Applied Sciences, 2022.   \n[22] Junior Costa de Jesus, Victor Augusto Kich, Alisson Henrique Kolling, Ricardo Bedin Grando, Marco Antonio de Souza Leite Cuadros, and Daniel Fernando Tello Gamarra. Soft Actor-Critic for Navigation of Mobile Robots. Journal of Intelligent and Robotic Systems, 2021.   \n[23] Yann LeCun, Sumit Chopra, Raia Hadsell, Aurelio Ranzato, and Fu Jie Huang. A Tutorial on Energy-Based Learning. 2006.   \n[24] Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 1996.   \n[25] Gareth O. Roberts and Jeffrey S. Rosenthal. Optimal scaling of discrete approximations to Langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 1998.   \n[26] Qiang Liu and Dilin Wang. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2016.   \n[27] Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. 2015.   \n[28] Malvin H. Kalos and Paula A. Whitlock. Monte Carlo methods. Vol. 1: basics. WileyInterscience, 1986. ISBN 0471898392.   \n[29] Surya T. Tokdar and Robert E. Kass. Importance Sampling: A Review. Wiley Interdisciplinary Reviews: Computational Statistics, 2, 2010.   \n[30] Michael B. Giles. Multilevel Monte Carlo Methods. Acta Numerica, 24:259 \u2013 328, 2013.   \n[31] Chen-Hao Chao, Wei-Fang Sun, Yen-Chang Hsu, Zsolt Kira, and Chun-Yi Lee. Training EnergyBased Normalizing Flow with Score-Matching Objectives. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[32] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In Proceedings of the International Conference on Intelligent Robots and Systems (IROS), 2012.   \n[33] Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel Goul\u00e3o, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo PerezVicente, Andrea Pierr\u00e9, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, 2023. URL https://zenodo.org/record/8127025.   \n[34] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, N. Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning. Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS) Dataset and Benchmark Track, 2021.   \n[35] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1994. ISBN 0471619779.   \n[36] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, 2018. ISBN 0262039249.   \n[37] Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In Proceedings of the International Conference on Machine Learning (ICML), 2011.   \n[38] Iman Nematollahi, Erick Rosete-Beas, Adrian Roefer, Tim Welschehold, Abhinav Valada, and Wolfram Burgard. Robot Skill Adaptation via Soft Actor-Critic Gaussian Mixture Models. Proceedings of IEEE International Conference on Robotics and Automation (ICRA), 2022.   \n[39] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. 2013.   \n[40] Dilin Wang and Qiang Liu. Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.   \n[41] Michel Dekking. A Modern Introduction to Probability and Statistics. 2007.   \n[42] George Papamakarios, Eric T. Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning Research (JMLR), 2019.   \n[43] Mathieu Germain, Karol Gregor, Iain Murray, and H. Larochelle. MADE: Masked Autoencoder for Distribution Estimation. 2015.   \n[44] Diederik P. Kingma, Tim Salimans, and Max Welling. Improved Variational Inference with Inverse Autoregressive Flow. Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2016.   \n[45] George Papamakarios, Iain Murray, and Theo Pavlakou. Masked Autoregressive Flow for Density Estimation. Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2017.   \n[46] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components Estimation. Workshop at the International Conference on Learning Representations (ICLR), 2015.   \n[47] Laurent Dinh, Jascha Narain Sohl-Dickstein, and Samy Bengio. Density Estimation using Real NVP. Proceedings of the International Conference on Learning Representations (ICLR), 2017.   \n[48] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2018.   \n[49] Thomas M\u00fcller, Brian McWilliams, Fabrice Rousselle, Markus H. Gross, and Jan Nov\u00e1k. Neural Importance Sampling. ACM Transactions on Graphics (TOG), 2018.   \n[50] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural Spline Flows. Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2019.   \n[51] Emiel Hoogeboom, Rianne van den Berg, and Max Welling. Emerging Convolutions for Generative Normalizing Flows. Proceedings of the International Conference on Machine Learning (ICML), 2019.   \n[52] Xuezhe Ma and Eduard H. Hovy. MaCow: Masked Convolutional Generative Flow. Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2019.   \n[53] You Lu and Bert Huang. Woodbury Transformations for Deep Generative Flows. Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020.   \n[54] Chenlin Meng, Linqi Zhou, Kristy Choi, Tri Dao, and Stefano Ermon. ButterflyFlow: Building Invertible Layers with Butterfly Matrices. Proceedings of the International Conference on Machine Learning (ICML), 2022.   \n[55] L. Gresele, G. Fissore, A. Javaloy, B. Sch\u00f6lkopf, and A. Hyv\u00e4rinen. Relative Gradient Optimization of the Jacobian Term in Unsupervised Deep Learning. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2020.   \n[56] T. Anderson Keller, Jorn W. T. Peters, Priyank Jaini, Emiel Hoogeboom, Patrick Forr\u2019e, and Max Welling. Self Normalizing Flows. In Proceedings of the International Conference on Machine Learning (ICML), 2020.   \n[57] A. Hyv\u00e4rinen and E. Oja. Independent Component Analysis: Algorithms and Applications. Neural Networks: the Official Journal of the International Neural Network Society, 13 4-5: 411\u201330, 2000.   \n[58] A. Hyv\u00e4rinen. Estimation of Non-Normalized Statistical Models by Score Matching. Journal of Machine Learning Research (JMLR), 2005.   \n[59] Yee Whye Teh, Max Welling, Simon Osindero, and Geoffrey E. Hinton. Energy-based models for sparse overcomplete representations. Journal of Machine Learning Research (JMLR), 4: 1235\u20131260, 2003.   \n[60] Will Grathwohl, Kuan-Chieh Jackson Wang, J\u00f6rn-Henrik Jacobsen, David Kristjanson Duvenaud, and Richard S. Zemel. Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling. In Proceedings of the International Conference on Machine Learning, 2020. URL https://api.semanticscholar.org/CorpusID:220042193.   \n[61] Jette Randl\u00f8v and Preben Alstr\u00f8m. Learning to Drive a Bicycle Using Reinforcement Learning and Shaping. In Proceedings of the International Conference on Machine Learning (ICML), 1998.   \n[62] A. Ng, Daishi Harada, and Stuart J. Russell. Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping. In Proceedings of the International Conference on Machine Learning (ICML), 1999.   \n[63] Adam Laud. Theory and Application of Reward Shaping in Reinforcement Learning. 2004.   \n[64] Hao Sun, Lei Han, Rui Yang, Xiaoteng Ma, Jian Guo, and Bolei Zhou. Optimistic Curiosity Exploration and Conservative Exploitation with Linear Reward Shaping. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2022.   \n[65] Zhe Zhang and Xiaoyang Tan. Adaptive reward shifting based on behavior proximity for offilne reinforcement learning. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2023.   \n[66] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error in Actor-Critic Methods. In Proceedings of the International Conference on Machine Learning (ICML), 2018.   \n[67] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Kirkeby Fidjeland, Georg Ostrovski, Stig Petersen, Charlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. Nature, 518:529\u2013533, 2015.   \n[68] Xiang Li, Wenhao Yang, Jiadong Liang, Zhihua Zhang, and Michael I. Jordan. A Statistical Analysis of Polyak-Ruppert Averaged Q-Learning. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTAT), 2021.   \n[69] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. Proceedings of the International Conference on Learning Representations (ICLR), 2016.   \n[70] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. ArXiv, abs/1707.06347, 2017.   \n[71] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of Machine Learning Research (JMLR), 22(268):1\u20138, 2021.   \n[72] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. 2019.   \n[73] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for Activation Functions. arXiv:1710.05941, 2017.   \n[74] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. $A r X i\\nu$ , abs/1607.06450, 2016.   \n[75] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. ArXiv, abs/1207.0580, 2012.   \n[76] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Jo\u00e3o G.M. Ara\u00fajo. CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms. Journal of Machine Learning Research (JMLR), 23 (274):1\u201318, 2022.   \n[77] Vincent Stimper, David Liu, Andrew Campbell, Vincent Berenz, Lukas Ryll, Bernhard Sch\u00f6lkopf, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. normflows: A PyTorch Package for Normalizing Flows. Journal of Open Source Software, 8(86):5361, 2023.   \n[78] Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.   \n[79] Antonio Serrano-Mu\u00f1oz, Dimitrios Chrysostomou, Simon B\u00f8gh, and Nestor AranaArexolaleiba. skrl: Modular and Flexible Library for Reinforcement Learning. Journal of Machine Learning Research (JMLR), 24(254):1\u20139, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this Appendix, we begin with a discussion of the soft value estimation methods used in SQL and SAC in Section A.1. We then derive a number of theoretical properties of MEow in Section A.2. Next, we discuss the issue of numerical instability in Section A.3. Then, we present additional experimental results in Section A.4, and summarize the experimental setups in Section A.5. Finally, we elaborate on the potential impacts of this work in Section A.6. ", "page_idx": 15}, {"type": "text", "text": "A.1 The Soft Value Estimation Methods in SAC and SQL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we elaborate on the soft value estimation methods mentioned in Section 2.2. We first show that $V_{\\theta}(\\mathbf{s}_{t})$ approximated using Eq. (6) is greater than that approximated using Eq. (7) for any given state $\\mathbf{s}_{t}$ in Proposition A.1 and Remark A.2. Then, we discuss their practical implementation. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.1. For any $\\mathbf{s}_{t}\\in\\mathcal{S}$ and $\\alpha\\in\\mathbb{R}_{>0}$ , the following inequality holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha\\log\\mathbb{E}_{\\mathbf{a}\\sim\\pi_{\\phi}}\\left[\\frac{\\exp\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)}{\\pi_{\\phi}(\\mathbf{a}|\\mathbf{s}_{t})}\\right]\\geq\\mathbb{E}_{\\mathbf{a}\\sim\\pi_{\\phi}}[Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})-\\alpha\\log\\pi_{\\phi}(\\mathbf{a}|\\mathbf{s}_{t})].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha\\log\\mathbb{E}_{\\mathbf{a}\\sim\\pi_{\\phi}}\\left[\\frac{\\exp\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)}{\\pi_{\\phi}(\\mathbf{a}|\\mathbf{s}_{t})}\\right]\\overset{(i)}{\\geq}\\alpha\\mathbb{E}_{\\mathbf{a}\\sim\\pi_{\\phi}}\\left[\\log\\left(\\frac{\\exp\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)}{\\pi_{\\phi}(\\mathbf{a}|\\mathbf{s}_{t})}\\right)\\right]}\\\\ {=\\alpha\\mathbb{E}_{\\mathbf{a}\\sim\\pi_{\\phi}}\\left[Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha-\\log\\pi_{\\phi}(\\mathbf{a}|\\mathbf{s}_{t})\\right]}\\\\ {=\\mathbb{E}_{\\mathbf{a}\\sim\\pi_{\\phi}}\\left[Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})-\\alpha\\log\\pi_{\\phi}(\\mathbf{a}|\\mathbf{s}_{t})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(i)$ is due to Jensen\u2019s inequality. ", "page_idx": 15}, {"type": "text", "text": "Remark A.2. The inequality in Eq. (A1) preserves after applying the Monte Carlo estimation. Namely, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha\\log\\left(\\frac{1}{M}\\sum_{i=1}^{M}\\frac{\\exp\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}^{(i)})/\\alpha\\right)}{\\pi_{\\phi}(\\mathbf{a}^{(i)}|\\mathbf{s}_{t})}\\right)\\geq\\frac{1}{M}\\sum_{i=1}^{M}\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}^{(i)})-\\alpha\\log\\pi_{\\phi}(\\mathbf{a}^{(i)}|\\mathbf{s}_{t})\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\{\\mathbf{a}^{(i)}\\}_{i=1}^{M}$ represents a set of samples drawn from $\\pi_{\\phi}$ . ", "page_idx": 15}, {"type": "text", "text": "Unlike the estimation in Eq. (7), the estimation in Eq. (6) is guaranteed to converge to $V_{\\theta}(\\mathbf s_{t})$ as $M\\to\\infty$ . However, empirically, the estimation method in Eq. (7) is preferred and widely used in the contemporary MaxEnt framework. One potential reason could be the required number of samples needed for effective approximation. According to [8], $M=32$ is an effective choice for Eq. (6), whereas $M=1$ works well for Eq. (7), as adopted by many previous works. [9\u201312, 14]. ", "page_idx": 15}, {"type": "text", "text": "A.2 Theoretical Properties of MEow ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we examine a number of key properties of the MEow framework. We begin by presenting a proposition to verify MEow\u2019s capability in modeling the soft Q-function and the soft value function. Then, we present a proposition to derive a deterministic policy in MEow. Finally, we offer a discussion of the impact of incorporating learnable reward shifting functions. ", "page_idx": 15}, {"type": "text", "text": "Proposition 3.1 Eq. $(I I)$ satisfies the following statements: $(I)$ Given that the Jacobian of g\u03b8 is non-singular, $V_{\\theta}(\\mathbf{s}_{t})\\ \\in\\ \\mathbb{R}$ and $Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\;\\;\\in\\;\\;\\mathbb{R},\\;\\forall\\mathbf{a}_{t}\\;\\;\\in\\;\\;{\\cal A},\\forall\\mathbf{s}_{t}\\;\\;\\in\\;\\;{\\cal S}$ . (2) $V_{\\theta}(\\mathbf{s}_{t})\\;\\mathrm{~=~}$ $\\begin{array}{r}{\\stackrel{\\cdot}{\\alpha}\\log\\int\\exp\\left(\\dot{Q_{\\theta}}(\\mathbf{s}_{t},\\mathbf{a})/\\dot{\\alpha}\\right)d\\mathbf{a}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. (1) Given that the Jacobian of $g_{\\boldsymbol{\\theta}}$ is non-singular, $V_{\\theta}(\\mathbf{s}_{t})\\in\\mathbb{R}$ and $Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\,\\in\\,\\mathbb{R},\\,\\forall\\mathbf{a}_{t}\\in$ $\\mathbf{\\mathcal{A}},\\forall\\mathbf{s}_{t}\\in\\mathcal{S}$ . ", "page_idx": 15}, {"type": "text", "text": "If the Jacobian of $g_{\\theta}$ is non-singular, then both $\\begin{array}{r l r}{\\prod_{i\\in S_{l}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t}))\\vert}&{{}\\in}&{\\mathbb{R}_{>0}}\\end{array}$ and $\\begin{array}{r}{\\prod_{i\\in S_{n}}|\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}))|\\ \\in\\ \\mathbb{R}_{>0}}\\end{array}$ . This suggests that $\\begin{array}{r}{\\alpha\\log\\prod_{i\\in S_{l}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t}))\\vert\\ \\in\\ \\mathbb{R}}\\end{array}$ and $\\begin{array}{r}{\\alpha\\log p_{\\mathbf{z}}\\left(g_{\\boldsymbol{\\theta}}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\right)\\prod_{i\\in S_{n}}|\\operatorname*{det}(\\mathbf{J}_{g_{\\boldsymbol{\\theta}}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}))|\\in\\mathbb{R}}\\end{array}$ . As a result, $V_{\\theta}(\\mathbf{s}_{t})\\in\\mathbb{R}$ and $Q_{\\theta}\\big(\\mathbf{s}_{t},\\mathbf{a}_{t}\\big)\\in\\mathbb{R}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\theta}(\\mathbf{s}_{t})=\\alpha\\log\\int\\exp\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)d\\mathbf{a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1=\\displaystyle\\int\\pi\\big(\\mathbf a|\\mathbf s_{t}\\big)d\\mathbf a=\\int p_{\\pi}\\left(g_{\\theta}\\big(\\mathbf a|\\mathbf s_{t}\\big)\\right)\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf J_{g_{\\theta}}(\\mathbf a^{i-1}|\\mathbf s_{t})\\right)\\right|\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\big(\\mathbf J_{g_{\\theta}^{i}}(\\mathbf s_{t})\\big)\\right|d\\mathbf a.}\\\\ &{\\Leftrightarrow\\ \\left(\\prod_{i\\in S_{t}}\\left|\\operatorname*{det}(\\mathbf J_{g_{\\theta}^{i}}(\\mathbf s_{t})\\big)\\right|\\right)^{-1}=\\displaystyle\\int p_{\\pi}\\left(g_{\\theta}\\big(\\mathbf a|\\mathbf s_{t}\\big)\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf J_{g_{\\theta}^{i}}(\\mathbf a^{i-1}|\\mathbf s_{t})\\right)\\right|d\\mathbf a.}\\\\ &{\\Leftrightarrow\\ \\alpha\\log\\left(\\prod_{i\\in S_{t}}\\left|\\operatorname*{det}(\\mathbf J_{g_{\\theta}^{i}}(\\mathbf s_{t}))\\right|\\right)^{-1}=\\alpha\\log\\int p_{\\pi}\\left(g_{\\theta}\\big(\\mathbf a|\\mathbf s_{t}\\big)\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf J_{g_{\\theta}^{i}}(\\mathbf a^{i-1}|\\mathbf s_{t})\\right)\\right|d\\mathbf a.}\\\\ &{\\Leftrightarrow\\ -\\alpha\\log\\prod_{i\\in S_{t}}\\left|\\operatorname*{det}(\\mathbf J_{g_{\\theta}^{i}}(\\mathbf s_{t}))\\right|=\\alpha\\log\\int p_{\\pi}\\left(g_{\\theta}\\big(\\mathbf a|\\mathbf s_{t}\\big)\\right)\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf J_{g_{\\theta}^{i}}(\\mathbf a^{i-1}|\\mathbf s_{t})\\right)\\right|d\\mathbf a.}\\\\ &{\\Leftrightarrow\\ \\nu_{\\theta}(s)=\\alpha\\log\\int\\exp\\left(Q_{\\theta}(s_{t},\\mathbf a)/\\alpha\\right)d\\mathbf a.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In Section 3.2, we demonstrate that $\\operatorname{argmax}_{\\mathbf{a}}Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})$ can be efficiently obtained through $g_{\\theta}^{-1}(\\mathrm{argmax}_{\\mathbf{z}}\\,p_{\\mathbf{z}}(\\mathbf{z})|\\mathbf{s}_{t})$ . To provide theoretical support for this result, we include a proof for Proposition 3.2. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3.2 Given that $|\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}^{i-1}|\\mathbf{s}_{t}))|$ is a constant with respect to $\\mathbf{a}^{i-1}$ , then $\\begin{array}{r}{g_{\\boldsymbol\\theta}^{-1}(\\operatorname{argmax}_{\\mathbf{z}}p_{\\mathbf{z}}(\\mathbf{z})|\\mathbf{s}_{t})=\\operatorname{argmax}_{\\mathbf{a}}Q_{\\boldsymbol\\theta}(\\mathbf{s}_{t},\\mathbf{a})}\\end{array}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{a}}{\\mathrm{argmax}}\\mathcal{G}_{\\theta}(\\mathbf{s}_{t},\\mathbf{a})=\\underset{\\mathbf{a}}{\\mathrm{argmax}}\\,\\alpha\\log\\left(p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})\\right)\\prod_{i}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}^{i-1}|\\mathbf{s}_{t})\\right)\\right|\\right)}&{}\\\\ {=\\underset{\\mathbf{a}}{\\mathrm{argmax}}\\,\\alpha\\log\\left(p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})\\right)c\\right)}&{}\\\\ {\\overset{(i)}{=}\\underset{\\mathbf{a}}{\\mathrm{argmax}}\\,p_{z}\\left(g_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})\\right)c}&{}\\\\ {=\\underset{\\mathbf{a}}{\\mathrm{argmax}}\\,p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})\\right)}&{}\\\\ {=\\underset{\\mathbf{a}}{\\mathrm{argmax}}\\,p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})\\right)}&{}\\\\ {\\overset{(i i)}{=}\\underset{\\mathbf{a}}{\\mathrm{argmax}}\\,p_{\\mathbf{z}}\\left(\\mathbf{z}\\right)}&{}\\\\ {\\overset{(i i i)}{=}\\underset{\\mathbf{g}}{\\mathrm{argmax}}\\,p_{\\mathbf{z}}\\left(\\mathbf{z}\\right)\\biggr|\\mathbf{s}_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(i)$ is because logarithm is strictly increasing, $(i i)$ is due to $\\mathbf{z}=g_{\\theta}(\\mathbf{a}|\\mathbf{s}_{t})$ , and $(i i i)$ is due to $\\mathbf{a}=g_{\\theta}^{-1}(\\mathbf{z}|\\mathbf{s}_{t})$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "In Section 3.2, we incorporate the learnable reward shifting function $b_{\\theta}$ in $Q_{\\theta}$ and $V_{\\theta}$ . This incorporation results in redefined soft Q-function $Q_{\\theta}^{b}$ and soft value function ${V}_{\\theta}^{b}$ . In Proposition A.3, we verify that $\\begin{array}{r}{V_{\\theta}^{b}(\\mathbf{s}_{t})=\\alpha\\log\\int\\exp(Q_{\\theta}^{b}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha)d\\mathbf{a}=V_{\\theta}(\\mathbf{s}_{t})+b_{\\theta}(\\mathbf{s}_{t})}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Proposition A.3. Given that $Q$ and $V$ satisfy $\\begin{array}{r}{V(\\mathbf{s}_{t})\\triangleq\\alpha\\log\\int\\exp\\left(Q(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)d\\mathbf{a}}\\end{array}$ . The augmented functions, $Q^{b}(\\mathbf{s}_{t},\\mathbf{a}_{t})=Q(\\mathbf{s}_{t},\\mathbf{a}_{t})+b(\\mathbf{s}_{t})$ and $\\begin{array}{r}{V^{b}(\\mathbf{s}_{t})\\triangleq\\alpha\\log\\int\\exp(Q^{b}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha)d\\mathbf{a},}\\end{array}$ , where $b(\\mathbf{s}_{t})$ is the the reward shifting function, satisfy $V^{b}(\\mathbf{s}_{t})=V(\\mathbf{s}_{t})+b(\\mathbf{s}_{t})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{b}(\\mathbf{s}_{t})=\\alpha\\log\\int\\exp\\left(Q^{b}(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)d\\mathbf{a}.}\\\\ &{\\qquad\\qquad=\\alpha\\log\\left(\\int\\exp\\left((Q(\\mathbf{s}_{t},\\mathbf{a})+b(\\mathbf{s}_{t}))/\\alpha\\right)d\\mathbf{a}\\right)}\\\\ &{\\qquad\\qquad=\\alpha\\log\\left(\\exp(b(\\mathbf{s}_{t})/\\alpha)\\int\\exp\\left(Q(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)d\\mathbf{a}\\right)}\\\\ &{\\qquad=\\alpha\\log\\int\\exp\\left(Q(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)d\\mathbf{a}+\\alpha\\log\\left(\\exp(b(\\mathbf{s}_{t})/\\alpha)\\right)}\\\\ &{\\qquad=\\alpha\\log\\int\\exp\\left(Q(\\mathbf{s}_{t},\\mathbf{a})/\\alpha\\right)d\\mathbf{a}+b(\\mathbf{s}_{t})}\\\\ &{\\qquad=V(\\mathbf{s}_{t})+b(\\mathbf{s}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3 The Issue of Numerical Instability ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide the motivation for employing the learnable reward shifting function described in Section 3.2. We show that while $Q_{\\theta}$ and $V_{\\theta}$ defined in Eq. (11) have the theoretical capability to learn arbitrary real values (i.e., Proposition 3.1), they may experience numerical instability in practice. This instability arises due to the exponential growth of $\\begin{array}{r}{\\overline{{\\prod_{i\\in S_{n}}|}}\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}))|}\\end{array}$ and the exponential decay of $\\begin{array}{r}{\\prod_{i\\in{\\cal S}_{l}}\\vert\\operatorname*{det}({\\bf J}_{g_{\\theta}^{i}}({\\bf s}_{t}))\\vert}\\end{array}$ . We first examine the relationship between $V_{\\theta}(\\mathbf{s}_{t})$ and $\\begin{array}{r}{\\prod_{i\\in{\\cal S}_{l}}\\vert\\operatorname*{det}({\\bf J}_{g_{\\theta}^{i}}({\\bf s}_{t}))\\vert}\\end{array}$ according to the following equations: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{\\theta}(\\mathbf{s}_{t})=-\\log\\prod_{i\\in S_{l}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t})\\right)\\right|\\ \\ \\Leftrightarrow\\ \\ \\exp(-V_{\\theta}(\\mathbf{s}_{t}))=\\prod_{i\\in S_{l}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t})\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The above equation suggests that the value of $\\begin{array}{r}{\\prod_{i\\in S_{l}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t}))\\vert}\\end{array}$ decreases exponentially with respect to $V_{\\theta}(\\mathbf{s}_{t})$ , which may lead to numerical instability during training. On the other hand, the relationship between $Q_{\\theta}\\big(\\mathbf{s}_{t},\\mathbf{a}_{t}\\big)$ and $\\begin{array}{r}{\\prod_{i\\in S_{n}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}))\\vert}\\end{array}$ can be expressed according to the following equations: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})=\\log p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\right)+\\log\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t})\\right)\\right|.}\\\\ {\\Leftrightarrow}&{\\displaystyle Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})-\\log p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\right)=\\log\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t})\\right)\\right|.}\\\\ {\\Leftrightarrow}&{\\displaystyle\\exp\\left(Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})-\\log p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\right)\\right)=\\prod_{i\\in S_{n}}\\left|\\operatorname*{det}\\left(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t})\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The equations indicate that $\\begin{array}{r}{\\prod_{i\\in S_{n}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}))\\vert}\\end{array}$ increases exponentially with respect to $Q_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})-\\log p_{\\mathbf{z}}\\left(g_{\\theta}(\\mathbf{a}_{t}\\vert\\mathbf{s}_{t})\\right)$ . Therefore, increasing $Q_{\\theta}$ may also lead to an exponential growth of $\\begin{array}{r}{\\prod_{i\\in S_{n}}\\vert\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{a}_{t}^{i-1}|\\mathbf{s}_{t}))\\vert}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "LRS makes our model less susceptible to numerical calculation errors since the learnable reward shifting function $b_{\\theta}$ , unlike $Q_{\\theta}$ and ${\\mathit{V}}_{\\theta}$ , is not represented in logarithmic scale. Consider a case where FP32 precision is in use, \u2018MEow (Vanilla)\u2019 could fail to learn a target $V_{\\theta^{*}}(\\mathbf{s}_{t})\\,>\\,38,\\forall\\mathbf{s}_{t}$ , since $\\begin{array}{r}{\\prod_{i\\in S_{l}}\\bar{|}\\operatorname*{det}(\\mathbf{J}_{g_{\\theta}^{i}}(\\mathbf{s}_{t}))|=\\exp(-V_{\\theta^{*}}(\\mathbf{s}_{t}))<2^{-126}}\\end{array}$ cannot be represented using FP32 precision. Therefore, without shifting the reward function, the loss sometimes becomes undefined values, and can lead to ineffective training (e.g., the green lines in Fig. 6). The reward shifting term can be designed as a (state-conditioned) function or a (non-state-conditioned) value. It can also be learnable or non-learnable. All of these designs (i.e., $b_{\\theta}(\\mathbf{s}_{t}),b(\\mathbf{s}_{t}),b_{\\theta}$ , and $b$ ) can be directly applied to MEow since none of them influences the action distribution. Based on our preliminary experiments, we identified that a learnable state-conditioned reward shifting delivers the best performance. ", "page_idx": 17}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/0f172e248dfc87988d27cee36e710aa0736cab3a887486de4e4e00a9c2a75485.jpg", "img_caption": ["Figure A1: Performance comparison between MEow with additive coupling transformations in $g_{\\theta}$ and MEow with affine coupling transformations in $g_{\\theta}$ on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the $95\\%$ confidence intervals, derived from five independent runs with different seeds. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/b0f77c14cb1052a9397110e22301b1011e839cc601293a331834ece919434093.jpg", "img_caption": ["Figure A2: Performance comparison between \u2018MEow\u2019, \u2018Energy Critic $^{+}$ Gaussian Actor\u2019 (ECGA), \u2018Energy Critic $^{+}$ Flow Actor\u2019 (ECFA), \u2018Flow Critic $^+$ Gaussian Actor\u2019 (FCGA), and \u2018Flow Critic $+$ Flow Actor\u2019 (FCFA) on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the $95\\%$ confidence intervals, derived from five independent runs with different seeds. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.4 Supplementary Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide additional experimental results. In Section A.4.1, we offer a comparison between MEow with $g_{\\boldsymbol{\\theta}}$ modeled using additive coupling layers and that using affine coupling layers. In Section A.4.2, we compare the performance of MEow with four distinct types of actor-critic frameworks formulated based on prior works [9\u201311]. In Section A.4.3, we provide an example illustrating the ability of flow-based models to represent multi-modal distributions as policies. In Section A.4.4, we present a performance comparison between SAC and its variant with LRS. Finally, in Section A.4.5, we provide a sensitivity examination for the target smoothing parameter. ", "page_idx": 18}, {"type": "text", "text": "A.4.1 Comparison of Additive and Affine Transformations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we evaluate the performance of MEow with two commonly-adopted non-linear transformations, additive [46] and affine [47] coupling layers, for constructing $g_{\\theta}$ . The results are presented in Fig. A1. The results show that MEow with additive coupling layers achieves better performance than that with affine coupling layers. Based on this observation, we adopt additive coupling layers for constructing $g_{\\theta}$ throughout the experiments in Section 4 of the main manuscript. ", "page_idx": 18}, {"type": "text", "text": "A.4.2 Influences of Parameterization in MaxEnt RL Actor-Critic Frameworks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we compare the performance of MEow against four different actor-critic frameworks formulated based on prior works [9\u201311]. The first framework is the same as SAC [9], with the critic modeled as an energy-based model and the actor as a Gaussian. The second framework follows the approaches of [10, 11], where the critic is also an energy-based model, but the actor is a flow-based model. The third and fourth frameworks both utilize a flow-based model for the critic, with the actor modeled as a Gaussian and a flow-based model, respectively. These frameworks are denoted as: \u2018Energy Critic $^{+}$ Gaussian Actor\u2019 (ECGA), \u2018Energy Critic+Flow Actor\u2019 (ECFA), \u2018Flow Critic $^+$ Gaussian Actor\u2019 (FCGA), and \u2018Flow Critic+Flow Actor\u2019 (FCFA), respectively. Regarding the soft value calculation during training, the first and second frameworks adopt the value estimation method in SAC (i.e., Eq. (7)). For the third and the fourth frameworks, their training adopts the exact value calculation (i.e., Eq. (11)), which is the same as MEow. The results are presented in Fig. A2. ", "page_idx": 18}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/ba39343205410ad2135ca9cdacede2c803dcf2897993fa9679251cb7ec8ac982.jpg", "img_caption": ["Figure A4: Performance comparison between \u2018SAC\u2019 and \u2018SAC $\\mathrm{(+LRS)}^{\\star}$ . Each curve represents the mean performance, with shaded areas indicating the $95\\%$ confidence intervals, derived from five independent runs with different seeds. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "As depicted in Fig. A2, MEow exhibits superior performance and stability compared to other actor-critic frameworks in the \u2018Hopper-v4\u2019, \u2018Ant-v4\u2019, and \u2018Walker2d-v4\u2019 environments, and shows comparable performance with ECGA in most environments. In addition, the results that compare the frameworks with flow-based models as actors (i.e., ECFA and FCFA) to those with Gaussians as actors (i.e., ECGA and FCGA) suggest that Gaussians are more effective for modeling actors. This finding is similar to that in [10]. On the other hand, the comparisons between ECGA and FCGA, and between FCGA and FCFA, do not show a clear trend. These findings suggest that both flow-based and energy-based models can be suitable for modeling the soft Q-function. Furthermore, the comparison between FCFA and MEow reveals that the training process involving alternating policy evaluation and improvement steps may be inferior to our proposed training process with a single objective. ", "page_idx": 19}, {"type": "text", "text": "A.4.3 Modeling Multi-Modal Distributions using Flow-based Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we use a one-dimensional example to demonstrate that flow-based models are capable of learning multi-modal action distributions. We employ a state-conditioned neural spline flow (NSF) [50] as the model, and train it in a single-step environment with one-dimensional state and action spaces. Fig. A3 (a) illustrates the reward landscape with the state and action denoted on the xaxis and y-axis, respectively. Fig. A3 (b) illustrates the probability density function (pdf) predicted by the model. The result demon", "page_idx": 19}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/cdef73b8ad63ae1a9f8fcca081ef59e59513815b84a96502ffe1035ed1986ffd.jpg", "img_caption": ["Figure A3: (a) The reward landscape of the one-step environment described in Section A.4.3. (b) The conditional pdf prediction using an NSF model. effectively learn multi-modal distributions. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.4.4 Applying Learnable Reward Shifting to SAC ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we examine the performance of SAC with the proposed LRS technique. Since the original implementation of SAC involves the clipped double Q-Learning technique, SAC with LRS is equivalent to SAC with the shifting-based clipped double Q-Learning (SCDQ) technique discussed in Section 3.2 of the main manuscript. The performance of \u2018SAC\u2019 and \u2018SAC (+LRS)\u2019 is presented in Fig. A4. The results indicate that applying LRS does not improve SAC\u2019s performance. Therefore, for a fair evaluation, the original implementation of SAC is adopted in the comparison in Section 4 of the main manuscript. ", "page_idx": 19}, {"type": "text", "text": "A.4.5 Sensitivity Examination for the Target Smoothing Parameter ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide a performance comparison of SAC and MEow trained with different target smoothing parameter values (i.e., $\\tau=0.005$ , 0.003, 0.0005, and 0.0001). The results shown in ", "page_idx": 19}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/585f0484bdf18aebeaf51277acb479d6f3dd6f5f83f84172f97bd26a1047f676.jpg", "img_caption": ["Figure A5: A performance comparison between MEow and SAC under different trained $\\tau$ on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the $95\\%$ confidence intervals, derived from five independent runs with different seeds. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Fig. A5 indicate that SAC performs the best when $\\tau=0.005$ , while MEow requires different $\\tau$ values to achieve good performance across different tasks. Although both algorithms exhibit significant performance variations with different $\\tau$ values, SAC demonstrates a more consistent trend in terms of the total returns among the tested values of $\\tau$ . ", "page_idx": 20}, {"type": "text", "text": "A.5 Experimental Setups ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we elaborate on the experimental configurations and provide the detailed hyperparameter setups for the experiments presented in Section 4 of the main manuscript. The code is implemented using PyTorch [72] and is available in the following repository: https: //github.com/ChienFeng-hub/meow. ", "page_idx": 20}, {"type": "text", "text": "A.5.1 Model Architecture ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Among all experiments presented in Section 4 of the main manuscript, we maintain the same model architecture, while adjusting inputs and outputs according to the state space and action space for each environment. An illustration of this architecture is presented in Fig. A6. The model architecture comprises three main components: (I) normalizing flow, (II) hypernetwork, and (III) reward shifting function. For the first component, the transformation $g_{\\theta}$ includes four additive coupling layers [46] followed by an element-wise linear layer. The prior distribution $p_{\\mathbf{z}}$ is modeled as a unit Gaussian. For the second component, the hypernetwork involves two types of multi-layer perceptrons (MLPs), labeled as (a) and (b) in Fig. A6, which produce weights for the non-linear and linear transformations, respectively. Both MLPs employ swish activation functions [73] and have a hidden layer size of 64. The MLPs labeled as (a) incorporate layer normalization [74] and a dropout layer [75] with a dropout rate of 0.1. For the third component, the reward shifting functions (i.e., b(\u03b81)a nd $b_{\\theta}^{(2)}$ ) are implemented using MLPs with swish activation and a hidden layer size of 256. The parameters used in these components are collectively referred to as $\\theta$ , and are optimized using the same objective function $\\mathcal{L}(\\bar{\\theta)}$ defined in Eq. (4), with the soft Q-function and the soft value function replaced by $Q_{\\theta}^{b}$ and ${V}_{\\theta}^{b}$ , respectively. Please note that, for the sake of notational simplicity and conciseness, the parameters of each network are all represented using $\\theta$ instead of distinct symbols (e.g., $\\theta_{\\mathrm{(II)}}$ -(a), $\\theta_{\\mathrm{(II)}}$ -(b), $\\theta_{\\mathrm{(III)}}$ -(1), and $\\theta_{\\mathrm{(III)}-(2)},$ . ", "page_idx": 20}, {"type": "text", "text": "A.5.2 Experiments on the Multi-Goal Environment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The experiments in Section 4.1 are performed on a two-dimensional multi-goal environment [8]. This environment consists of four goals positioned at [0, 5], $[0,-5]$ , [5, 0], and $[-5,0]$ , denoted as $\\mathbf{g}_{1}$ , $\\mathbf{g}_{2}$ , ${\\bf g}_{3}$ , and $\\mathbf{g}_{4}$ , respectively. The reward is the sum of two components, $r_{1}(\\mathbf{s}_{t})$ and $r_{2}(\\mathbf{a}_{t})$ , which are ", "page_idx": 20}, {"type": "image", "img_path": "lhlIUxD5eE/tmp/e3016581f3d7d00a8aec434a79c2033b9f95583bd811f411053155a7183ff144.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure A6: The architecture adopted in MEow. This architecture consists of three primary components: (I) normalizing flow, (II) hypernetwork, and (III) reward shifting function. The hypernetwork includes two distinct types of networks, labeled as (a) and (b), which are responsible for generating weights for the non-linear and linear transformations within the normalizing flow, respectively. Layer normalization is denoted as \u2018L. Norm\u2019 in (a). ", "page_idx": 21}, {"type": "table", "img_path": "lhlIUxD5eE/tmp/e07b592f075d35aa8aafe440c995ebc2645f019836295cdf9e6d06d07e82075b.jpg", "table_caption": ["Table A1: Shared hyperparameters of MEow. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "lhlIUxD5eE/tmp/2370ec34c925a7f814d7ae1df2ee5e284d4116d1ba00efcd0c131e325dac3741.jpg", "table_caption": ["Table A2: Shared hyperparameters of SAC. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "formulated as follow: ", "page_idx": 21}, {"type": "equation", "text": "$$\nr_{1}(\\mathbf{s}_{t})=\\operatorname*{max}_{i}-\\left\\|\\mathbf{s}_{t}-\\mathbf{g}_{i}\\right\\|\\mathrm{~and~}r_{2}(\\mathbf{a}_{t})=-30\\times\\left\\|\\mathbf{a}_{t}\\right\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to Eq. (A3), $r_{1}(\\mathbf{s}_{t})$ encourages policies to reach states near the goals. On the other hand, $r_{2}(\\mathbf{a}_{t})$ encourages policies to produce actions with small magnitudes. ", "page_idx": 21}, {"type": "text", "text": "In this experiment, we adopt a temperature parameter $\\alpha=2.5$ , a target smoothing factor $\\tau=0.0005$ , a learning rate $\\beta\\,=\\,0.001$ , a discount factor $\\gamma\\,=\\,0.9$ , and a total of 4, 000 training steps. The computation was carried out on NVIDIA TITAN V GPUs equipped with 12GB of memory. The training takes approximately four minutes. ", "page_idx": 21}, {"type": "text", "text": "A.5.3 Experiments on the MuJoCo Environments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Software and Hardware Setups. For the experiments on the MuJoCo environments, our implementation is built on CleanRL [76], with the normalizing flow component adapted from [77]. The computation was carried out on NVIDIA V100 GPUs equipped with 16GB of memory. The training takes approximately 13 hours per 1 million steps, with each GPU capable of executing four training sessions simultaneously. ", "page_idx": 21}, {"type": "text", "text": "Hyperparameter Setups. The shared and the environment-specific hyperparameters of MEow are summarized in Tables A1 and A3, respectively. The hyperparamers for the baseline methods are directly borrowed from Stable Baseline 3 (SB3) [71]. ", "page_idx": 21}, {"type": "table", "img_path": "lhlIUxD5eE/tmp/7895bcc1efb5cf0e41fd9d4adfc9288f371d7ffa2c2514370fc2e3f4bc1b9a1e.jpg", "table_caption": ["Table A3: A list of environment-specific hyperparameters used in MEow. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "lhlIUxD5eE/tmp/efbf2925545aaab5ddea3b399fe59ef339ecbeba02ed77353789655cac79e403.jpg", "table_caption": ["Table A4: A list of environment-specific hyperparameters used in SAC. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.5.4 Experiments on the Omniverse Isaac Gym Environments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Software and Hardware Setups. For the experiments performed on Omniverse Isaac Gym, the implementation is built on SKRL [79] due to its compatibility with Omniverse Issac Gym [34]. The computation was carried out on NVIDIA L40 GPUs equipped with 48GB of memory. The training takes approximately 22 hours per 1 million training steps, with each GPU capable of executing three training sessions simultaneously. For \u2018Ant\u2019, \u2018Humanoid\u2019, \u2018Ingenuity\u2019, and \u2018ANYmal\u2019, each training step consists of 128 parallelizable interactions with the environments. For \u2018AllegroHand\u2019 and \u2018FrankaCabinet\u2019, each training step consists of 512 parallelizable interactions with the environments. ", "page_idx": 22}, {"type": "text", "text": "Hyperparameter Setups. The shared and the environment-specific hyperparameters of MEow are summarized in Tables A1 and A3, respectively. Those of SAC are summarized in Tables A2 and A4, respectively. Both SAC and MEow were tuned using the same search space for $\\tau$ and $\\alpha$ to ensure a fair comparison. Specifically, a grid search was conducted with $\\tau$ values ranging from 0.1 to 0.00025 and $\\alpha$ values from 0.8 to 0.0005 for both algorithms. The setups with the highest average return were selected for each environment. ", "page_idx": 22}, {"type": "text", "text": "A.6 Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This work represents a new research direction for MaxEnt RL. It discusses a unified method that can be trained using a single objective function and can avoid Monte Carlo estimation in the calculation of the soft value function, which addresses two issues in the existing MaxEnt RL methods. From a practical perspective, our experiments demonstrate that MEow can achieve superior performance compared to widely adopted representative baselines. In addition, the experimental results conducted in the Omniverse Isaac environments show that our framework can perform robotic tasks simulated based on real-world application scenarios. These results indicate the potential for deploying MEow in real robotic tasks. Given MEow\u2019s potential to be extended to perform challenging tasks, it is unlikely to have negative impacts on society. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main claims in the abstract and Section 1 accurately reflect the contributions of this paper. The theoretical results are discussed in Section 3. The experiments in Section 4 provide empirical justification for these claims. Finally, Section 5 summarizes both the theoretical and empirical contributions of this work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The limitations of this work are discussed in the main manuscript. The discussion covers the assumptions made in this paper and the computational efficiency of the proposed framework. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The assumptions and the proofs of our theoretical results are presented in detail in Appendices $_\\mathrm{A.l\\simA.3}$ . ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper fully disclose all the information needed to reproduce the experimental results. The experimental configurations, detailed hyperparameter setups, and hardware requirements for the experiments presented in this paper are elaborated in Appendix A.5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 24}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code and installation instructions are available in an anonymous repository, with the link provided in Appendix A.5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The experimental configurations, detailed hyperparameter setups, and hardware requirements for the experiments presented in this paper are elaborated in Appendix A.5. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The evaluation curves in Figs. 3, 4, 6, 7, A1, and A2) present the mean performance, with the shaded areas indicating the $95\\%$ confidence intervals. Each of them is derived from five independent runs with different seeds. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The hardware requirements (e.g., the computational hardware configurations and the execution time) for each experiment are elaborated in Appendix A.5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All authors have reviewed the NeurIPS Code of Ethics and confirmed that the research conducted in this paper complies with it. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This paper discusses its potential impacts in Appendix A.6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no risk for misuse (e.g., pretrained language models, image generators, or scraped datasets). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The creators of assets are properly credited through citations, and the licence is included in the asset (see Appendix A.5). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The code, installation instructions, and running commands are summarized in an anonymous repository, with the link provided in Appendix A.5. The environments are publicly available, and the experiments are performed on them with the default setup. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]