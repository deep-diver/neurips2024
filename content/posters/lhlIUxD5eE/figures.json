[{"figure_path": "lhlIUxD5eE/figures/figures_5_1.jpg", "caption": "Figure 1: The Jacobian determinant products for (a) the non-linear and (b) the linear transformations, evaluated during training in the Hopper-v4 environment. Subfigure (b) is presented on a log scale for better visualization. This experiment adopt the affine coupling layers [47] as the nonlinear transformations.", "description": "This figure shows the Jacobian determinant products of the linear and non-linear transformations used in the MEow model during training on the Hopper-v4 environment.  The non-linear transformations exhibit a significant growth and decay, while the linear transformations show more stability. The learnable reward shifting (LRS) technique is shown to mitigate the instability in the non-linear transformations.", "section": "3.2 Techniques for Improving the Training and Inference Processes of MEow"}, {"figure_path": "lhlIUxD5eE/figures/figures_7_1.jpg", "caption": "Figure 3: The results in terms of total returns versus the number of training steps evaluated on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the 95% confidence intervals, derived from five independent runs with different seeds.", "description": "This figure compares the performance of MEow against several other reinforcement learning algorithms on five different MuJoCo environments.  Each environment is represented by a separate graph. The x-axis represents the number of training steps, and the y-axis shows the total returns. Each line represents the mean performance of MEow and its comparison algorithms, averaged over five independent training runs with different random seeds. The shaded areas indicate the 95% confidence intervals, providing a measure of uncertainty in the results. The graphs illustrate how the total return varies over time and how the performance of MEow compares with that of other algorithms.", "section": "4.2 Performance Comparison on the MuJoCo Environments"}, {"figure_path": "lhlIUxD5eE/figures/figures_7_2.jpg", "caption": "Figure 2: (a) The soft value function and the trajectories generated using our method on the multi-goal environment. (b) The estimation error evaluated at the initial state under different choices of M.", "description": "Figure 2(a) shows a contour plot of the soft value function learned by the proposed method in a 2D multi-goal environment.  The blue lines represent the trajectories followed by the agent, clearly demonstrating the successful learning and proper transitions toward the goals. Figure 2(b) illustrates the impact of using Monte Carlo methods to estimate the soft value function by plotting the estimation errors with respect to the number of samples (M) used for different approximation methods. The results highlight the slow convergence of Monte Carlo estimation.", "section": "4.1 Evaluation on a Multi-Goal Environment"}, {"figure_path": "lhlIUxD5eE/figures/figures_8_1.jpg", "caption": "Figure 4: A comparison on six Isaac Gym environments. Each curve represents the mean performance of five runs, with shaded areas indicating the 95% confidence intervals. \u2018Steps\u2019 in the x-axis represents the number of training steps, each of which consists of N parallelizable interactions with the environments.", "description": "The figure shows the performance comparison between MEow and SAC on six different robotic tasks simulated using Omniverse Isaac Gym.  The tasks vary in complexity and dimensionality, showcasing MEow's ability to handle high-dimensional and complex robotic control problems.  The plots show the total return over training steps for each environment.  The shaded regions represent 95% confidence intervals, indicating statistical significance. The number of parallelizable interactions (N) differs between some tasks, illustrating variations in computational load.", "section": "4.3 Performance Comparison on the Omniverse Isaac Gym Environments"}, {"figure_path": "lhlIUxD5eE/figures/figures_9_1.jpg", "caption": "Figure 6: The performance comparison of MEow\u2019s variants (i.e., \u2018MEow (Vanilla)', \u2018MEow (+LRS)', and 'MEow (+LRS & SCDQ)') on five MuJoCo environments. Each curve represents the mean performance of five runs, with shaded areas indicating the 95% confidence intervals.", "description": "This figure compares the performance of three variants of the MEow algorithm across five MuJoCo benchmark environments.  The three variants are: MEow (Vanilla), MEow with Learnable Reward Shifting (+LRS), and MEow with both Learnable Reward Shifting and Shifting-Based Clipped Double Q-Learning (+LRS & SCDQ). Each line shows the mean average return over five independent runs with different random seeds, and the shaded area represents the 95% confidence interval.  The figure demonstrates the significant performance improvement achieved by incorporating the proposed training techniques (LRS and SCDQ).", "section": "4.4 Ablation Analysis"}, {"figure_path": "lhlIUxD5eE/figures/figures_9_2.jpg", "caption": "Figure 3: The results in terms of total returns versus the number of training steps evaluated on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the 95% confidence intervals, derived from five independent runs with different seeds.", "description": "This figure displays the results of the experiments conducted on five different MuJoCo environments. For each environment, the total returns over the training steps are presented in terms of mean performance and 95% confidence intervals. Five independent runs with different random seeds were used for each environment. The x-axis represents the number of training steps (in millions), and the y-axis represents the total return.", "section": "4.2 Performance Comparison on the MuJoCo Environments"}, {"figure_path": "lhlIUxD5eE/figures/figures_18_1.jpg", "caption": "Figure A1: Performance comparison between MEow with additive coupling transformations in g\u03b8 and MEow with affine coupling transformations in g\u03b8 on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the 95% confidence intervals, derived from five independent runs with different seeds.", "description": "This figure compares the performance of MEow using two different types of coupling layers in the normalizing flow: additive coupling and affine coupling.  The results are shown across five MuJoCo benchmark environments (Hopper-v4, HalfCheetah-v4, Walker2d-v4, Ant-v4, and Humanoid-v4).  Each line represents the average performance across five independent training runs, with the shaded area showing the 95% confidence interval, illustrating the variability in performance.", "section": "A.4.1 Comparisons of Additive and Affine Transformations"}, {"figure_path": "lhlIUxD5eE/figures/figures_18_2.jpg", "caption": "Figure A2: Performance comparison between \u2018MEow\u2019, \u2018Energy Critic+Gaussian Actor\u2019 (ECGA), \u2018Energy Critic+Flow Actor\u2019 (ECFA), \u2018Flow Critic+Gaussian Actor\u2019 (FCGA), and \u2018Flow Critic+Flow Actor\u2019 (FCFA) on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the 95% confidence intervals, derived from five independent runs with different seeds.", "description": "This figure compares the performance of MEow against four different actor-critic frameworks formulated based on prior works [9-11]. The frameworks include: (1) SAC [9], with the critic modeled as an energy-based model and the actor as a Gaussian; (2) [10, 11], where the critic is also an energy-based model, but the actor is a flow-based model; (3) flow-based model for the critic and Gaussian for the actor; (4) flow-based model for both the critic and the actor. The results are presented for five MuJoCo environments: Hopper-v4, HalfCheetah-v4, Walker2d-v4, Ant-v4, and Humanoid-v4. Each curve represents the mean performance derived from five independent runs, with shaded areas indicating the 95% confidence intervals.", "section": "A.4 Supplementary Experiments"}, {"figure_path": "lhlIUxD5eE/figures/figures_19_1.jpg", "caption": "Figure 3: The results in terms of total returns versus the number of training steps evaluated on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the 95% confidence intervals, derived from five independent runs with different seeds.", "description": "This figure presents the learning curves of five different reinforcement learning algorithms on five benchmark MuJoCo environments. The y-axis shows the total returns (average reward per episode), while the x-axis shows the number of training steps. Each line represents the average performance of an algorithm across five independent runs, and the shaded region represents the 95% confidence interval.  This demonstrates the performance comparison and stability of the algorithms across various tasks.", "section": "4.2 Performance Comparison on the MuJoCo Environments"}, {"figure_path": "lhlIUxD5eE/figures/figures_19_2.jpg", "caption": "Figure 2: (a) The soft value function and the trajectories generated using our method on the multi-goal environment. (b) The estimation error evaluated at the initial state under different choices of M.", "description": "Figure 2(a) shows a contour plot of the soft value function learned by the model in a multi-goal environment. The blue lines represent the agent's trajectories, clearly showing that it successfully learns to navigate to the goals.  Figure 2(b) compares the soft value function estimates using different methods (the proposed method and two approximations, SQL-like and SAC-like) for varying numbers of Monte Carlo samples (M). The estimation error is calculated as the Euclidean distance between the true value and the approximation. This demonstrates that the proposed method is more efficient than the approximations. ", "section": "4.1 Evaluation on a Multi-Goal Environment"}, {"figure_path": "lhlIUxD5eE/figures/figures_20_1.jpg", "caption": "Figure 3: The results in terms of total returns versus the number of training steps evaluated on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the 95% confidence intervals, derived from five independent runs with different seeds.", "description": "This figure compares the performance of the proposed MEow algorithm against several baselines on five different MuJoCo environments (Hopper-v4, HalfCheetah-v4, Walker2d-v4, Ant-v4, and Humanoid-v4).  Each environment's results are shown in a separate subplot. The y-axis shows the total return, and the x-axis represents the number of training steps.  Each line represents the average performance across five independent runs with different random seeds, and the shaded regions represent the 95% confidence intervals, giving a measure of uncertainty in the results.", "section": "4.2 Performance Comparison on the MuJoCo Environments"}, {"figure_path": "lhlIUxD5eE/figures/figures_21_1.jpg", "caption": "Figure A6: The architecture adopted in MEow. This architecture consists of three primary components: (I) normalizing flow, (II) hypernetwork, and (III) reward shifting function. The hypernetwork includes two distinct types of networks, labeled as (a) and (b), which are responsible for generating weights for the non-linear and linear transformations within the normalizing flow, respectively. Layer normalization is denoted as 'L. Norm' in (a).", "description": "This figure shows the architecture of the MEow model, which consists of three main parts: a normalizing flow for modeling the policy, a hypernetwork to generate parameters for the normalizing flow, and learnable reward shifting functions. The hypernetwork has two branches: one for non-linear transformations and one for linear transformations within the normalizing flow.  The reward shifting functions help to stabilize the training process.", "section": "A.5.1 Model Architecture"}]