[{"figure_path": "RB1F2h5YEx/tables/tables_27_1.jpg", "caption": "Table 1: Metaworld Tasks", "description": "This table lists the 19 Metaworld tasks used in the Metaworld20-10 benchmark.  These tasks were selected because the RPO agent achieved high success rates after 1 million training steps.  The tasks represent a variety of manipulation challenges involving a robotic arm.", "section": "C.4 Metaworld environments"}, {"figure_path": "RB1F2h5YEx/tables/tables_29_1.jpg", "caption": "Table 2: Hyperparameters for RPO and PPO", "description": "This table shows the hyperparameters used for both the RPO and PPO algorithms across four different reinforcement learning environments: Metaworld, Gridworld, Quadruped, and Lunar Lander.  For each environment, the table lists the learning rate, number of environments, minibatch size, number of minibatches, update epochs, GAE lambda, maximum gradient norm, entropy regularization, RPO alpha, network width, number of hidden layers, and the type of additional parameters used (Diagonal Layer, None, Input Scale).  The hyperparameters were tuned for optimal performance in each environment.", "section": "4 Experiments"}, {"figure_path": "RB1F2h5YEx/tables/tables_30_1.jpg", "caption": "Table 3: Runtimes for Parseval regularization and baseline methods in minutes. Adding Parseval regularization only leads to a mild computational cost over the standard agent.", "description": "This table presents the runtimes for different algorithms across four different continual reinforcement learning environments.  The runtimes are given in minutes for each algorithm (no Parseval regularization, with Parseval regularization, and shrink-and-perturb). The percentage increase in runtime for Parseval regularization compared to the no-Parseval baseline is provided. The data highlights that Parseval regularization adds minimal computational overhead compared to the baseline and even slightly less than the shrink-and-perturb method.", "section": "C.7 Runtime analysis"}]