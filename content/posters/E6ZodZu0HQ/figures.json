[{"figure_path": "E6ZodZu0HQ/figures/figures_0_1.jpg", "caption": "Figure 1: We introduce PuLID, a tuning-free ID customization approach. PuLID maintains high ID fidelity while effectively reducing interference with the original model's behavior.", "description": "This figure shows several examples of identity customization using different methods, including IPAdapter, InstantID, and the proposed PuLID method.  Each column represents a different image prompt and style, and each row shows the results generated using a different technique.  The \"Input\" row shows the original image prompt.  The goal is to show that PuLID (the authors' method) generates images with high identity fidelity while retaining the style and other details of the original prompt better than competing methods.  This illustrates the core contribution of the paper: a high-fidelity, tuning-free identity customization technique.", "section": "Introduction"}, {"figure_path": "E6ZodZu0HQ/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of PuLID framework. The upper half of the framework illustrates the conventional diffusion training process. The face extracted from the same image is employed as the ID condition Cid. The lower half of the framework demonstrates the Lightning T2I training branch introduced in this study. It leverages the recent fast sampling methods to iteratively denoise from pure noise to high-quality images in a few steps (4 in this paper). In this branch, we construct contrastive paths with and without ID injection and introduce an alignment loss to instruct the model on how to insert ID condition without disrupting the original model's behavior. As this branch can produce photo-realistic images, it implies that we can achieve a more accurate ID loss for optimization.", "description": "This figure shows the PuLID framework, which consists of two branches: a conventional diffusion branch and a Lightning T2I branch.  The conventional branch uses standard diffusion methods for image generation. The Lightning branch uses fast sampling to generate high-quality images from noise in a few steps, allowing for the calculation of accurate ID loss. A contrastive alignment loss is used to minimize interference with the original model's behavior.  The figure highlights how the ID is incorporated and how the loss functions help maintain ID fidelity while preserving the original model's characteristics.", "section": "3 Methods"}, {"figure_path": "E6ZodZu0HQ/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration and Effect of the alignment loss.", "description": "This figure visually explains the alignment loss function used in PuLID.  Panel (a) shows how the alignment loss compares UNet features (Qt and Qtid) from two image generation paths (one with and one without ID embedding).  Specifically, it uses textual features (K) to query the UNet features (Q) and calculates the correlation, aggregating Q based on this matrix. This aims to ensure that ID insertion doesn't disrupt the model's response to the prompt.  The semantic alignment loss (Lalign-sem) is calculated based on the similarity of these responses, while the layout alignment loss (Lalign-layout) measures the difference between Qt and Qtid to maintain consistent layout.  Panel (b) shows the effect of the alignment loss, demonstrating its effectiveness in mitigating the issue of ID information contaminating the model's behavior by aligning UNet features, leading to improved ID insertion without disrupting the model's original behavior.", "section": "3 Methods"}, {"figure_path": "E6ZodZu0HQ/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparisons. T2I w/o ID represents the output generated by the original T2I model without inserting ID, which reflects the behavior of the original model. Our PuLID achieves higher ID fidelity while causing less disruption to the original model. As the disruption to the model is reduced, results generated by PuLID accurately reproduce the lighting (1st row), style (4th row), and even layout (5th row) of the original model. This unique advantage broadens the scope for a more flexible application of PuLID.", "description": "This figure compares the image generation results of three different methods: IPAdapter, InstantID, and the proposed PuLID method.  Each method is tested on several prompts with various styles and scenarios. The results show that PuLID maintains high ID (identity) fidelity while minimizing changes to the original image's style, lighting, and layout compared to the other two methods. The consistent preservation of style, lighting, and layout demonstrates PuLID's effectiveness and superior performance.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "E6ZodZu0HQ/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison for ablation study on alignment loss.", "description": "This figure shows a qualitative comparison of image generation results with and without the alignment loss (L_align).  The left half demonstrates the impact of removing L_align, showing how the ID insertion disrupts the original model's behavior.  This disruption is evident in the inability of the prompt to precisely control style and orientation, and the tendency for the face to dominate the image layout. Conversely, the right half illustrates the improved image generation results obtained with the alignment loss (L_align), showcasing that it effectively mitigates this disruptive effect by enabling precise and style-consistent ID insertion without hindering prompt adherence.", "section": "4.5 Ablation"}, {"figure_path": "E6ZodZu0HQ/figures/figures_13_1.jpg", "caption": "Figure 6: More applications. Including style changes, IP fusion, accessory modification, recontextualization, attribute editing, transformation from non-photo-realistic domain to photo-realistic domain, and ID mixing. Note that all these high-quality images are generated in just 4 steps with SDXL-Lightning model, without the need for additional Lora.", "description": "This figure showcases the versatility of the PuLID model by demonstrating its ability to handle various image manipulation tasks.  It shows examples of changing styles, combining identities (IP fusion), modifying accessories, altering contexts, editing attributes, converting images from non-photorealistic styles to photorealistic ones, and even mixing multiple identities.  The key point is that all these high-quality results are achieved in only 4 steps using the SDXL-Lightning model, without the need for additional techniques like LoRA, highlighting the efficiency of the PuLID approach.", "section": "More Applications of PuLID"}, {"figure_path": "E6ZodZu0HQ/figures/figures_14_1.jpg", "caption": "Figure 4: Qualitative comparisons. T2I w/o ID represents the output generated by the original T2I model without inserting ID, which reflects the behavior of the original model. Our PuLID achieves higher ID fidelity while causing less disruption to the original model. As the disruption to the model is reduced, results generated by PuLID accurately reproduce the lighting (1st row), style (4th row), and even layout (5th row) of the original model. This unique advantage broadens the scope for a more flexible application of PuLID.", "description": "This figure shows a qualitative comparison of the proposed PuLID method against two state-of-the-art baselines, InstantID and IPAdapter.  The top row shows the input image and the prompts used, followed by the results from the base model without ID insertion (T2I w/o ID), InstantID, IPAdapter and finally, the proposed PuLID. Each column represents different editing tasks or styles applied to the same base input image.  The results demonstrate that PuLID achieves higher identity fidelity while maintaining consistency in other aspects like lighting, composition, and style, compared to the other methods which show more style degradation or lower ID fidelity.", "section": "4 Experiments"}, {"figure_path": "E6ZodZu0HQ/figures/figures_15_1.jpg", "caption": "Figure 4: Qualitative comparisons. T2I w/o ID represents the output generated by the original T2I model without inserting ID, which reflects the behavior of the original model. Our PuLID achieves higher ID fidelity while causing less disruption to the original model. As the disruption to the model is reduced, results generated by PuLID accurately reproduce the lighting (1st row), style (4th row), and even layout (5th row) of the original model. This unique advantage broadens the scope for a more flexible application of PuLID.", "description": "This figure compares the qualitative results of different methods for identity customization in text-to-image generation.  The leftmost column shows the original image without any identity (ID) insertion. The following columns show results from InstantID, PuLID (the proposed method), and IPAdapter. Each method is tested using both SDXL-Lightning and SDXL-base models, resulting in two images per method. The comparison highlights PuLID's ability to maintain higher ID fidelity while preserving the original model's style, lighting, and layout.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "E6ZodZu0HQ/figures/figures_16_1.jpg", "caption": "Figure 4: Qualitative comparisons. T2I w/o ID represents the output generated by the original T2I model without inserting ID, which reflects the behavior of the original model. Our PuLID achieves higher ID fidelity while causing less disruption to the original model. As the disruption to the model is reduced, results generated by PuLID accurately reproduce the lighting (1st row), style (4th row), and even layout (5th row) of the original model. This unique advantage broadens the scope for a more flexible application of PuLID.", "description": "This figure compares the image generation results of PuLID against two other methods and a baseline (original model). PuLID shows higher ID (identity) fidelity and less disruption to the original model's style and layout compared to the other methods.", "section": "4 Experiments"}, {"figure_path": "E6ZodZu0HQ/figures/figures_17_1.jpg", "caption": "Figure 2: Overview of PuLID framework. The upper half of the framework illustrates the conventional diffusion training process. The face extracted from the same image is employed as the ID condition Cid. The lower half of the framework demonstrates the Lightning T2I training branch introduced in this study. It leverages the recent fast sampling methods to iteratively denoise from pure noise to high-quality images in a few steps (4 in this paper). In this branch, we construct contrastive paths with and without ID injection and introduce an alignment loss to instruct the model on how to insert ID condition without disrupting the original model's behavior. As this branch can produce photo-realistic images, it implies that we can achieve a more accurate ID loss for optimization.", "description": "This figure shows the PuLID framework. The upper half shows a conventional diffusion process where the extracted face is used as the ID condition.  The lower half shows the Lightning T2I branch which uses fast sampling methods to generate images from noise with a contrastive alignment loss to preserve the original model's behavior while maintaining high ID fidelity.", "section": "3 Methods"}, {"figure_path": "E6ZodZu0HQ/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative comparisons. T2I w/o ID represents the output generated by the original T2I model without inserting ID, which reflects the behavior of the original model. Our PuLID achieves higher ID fidelity while causing less disruption to the original model. As the disruption to the model is reduced, results generated by PuLID accurately reproduce the lighting (1st row), style (4th row), and even layout (5th row) of the original model. This unique advantage broadens the scope for a more flexible application of PuLID.", "description": "This figure shows qualitative comparisons between the results of the original model, PuLID, InstantID, and IPAdapter. It highlights PuLID's ability to maintain high ID fidelity while minimizing disruption to the original model's behavior, preserving aspects like lighting, style, and layout.  This contrasts with the other methods which may show style degradation or reduced editability.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "E6ZodZu0HQ/figures/figures_19_1.jpg", "caption": "Figure 4: Qualitative comparisons. T2I w/o ID represents the output generated by the original T2I model without inserting ID, which reflects the behavior of the original model. Our PuLID achieves higher ID fidelity while causing less disruption to the original model. As the disruption to the model is reduced, results generated by PuLID accurately reproduce the lighting (1st row), style (4th row), and even layout (5th row) of the original model. This unique advantage broadens the scope for a more flexible application of PuLID.", "description": "This figure shows a qualitative comparison of PuLID against two other state-of-the-art methods, InstantID and IPAdapter. The comparison demonstrates that PuLID achieves higher identity (ID) fidelity while better preserving the original model's behavior (e.g., style, lighting, and layout).  The results suggest that PuLID's ability to minimize interference with the original model makes it more versatile for various applications.", "section": "4 Experiments"}]