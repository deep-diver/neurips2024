[{"figure_path": "dRJJt0Ji48/tables/tables_3_1.jpg", "caption": "Table 1: Performance comparisons on four real-world datasets in terms of MSE, MAE, and CRPS. The best is in bold, while the second best is underlined.", "description": "This table presents a comparison of the proposed RATD model's performance against various baseline methods on four real-world datasets. The performance is evaluated using three metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Continuous Ranked Probability Score (CRPS).  The best performing model for each metric on each dataset is shown in bold, and the second-best is underlined, allowing for easy comparison of the different approaches across various datasets and evaluation metrics.", "section": "5 Experiments"}, {"figure_path": "dRJJt0Ji48/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparisons on four real-world datasets in terms of MSE, MAE, and CRPS. The best is in bold, while the second best is underlined.", "description": "This table compares the performance of the proposed RATD model against several baseline models on four real-world datasets.  The performance is evaluated using three metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Continuous Ranked Probability Score (CRPS). The best performing model for each metric and dataset is highlighted in bold, while the second-best is underlined.  This allows for a clear comparison of the relative strengths and weaknesses of each model across different datasets and evaluation metrics.", "section": "5 Experiments"}, {"figure_path": "dRJJt0Ji48/tables/tables_8_1.jpg", "caption": "Table 2: Performance comparisons on MIMIC datasets with popular time series forecasting methods. Here, \"MIMIC-IV (All)\" refers to the model\u2019s testing results on the complete test set, while \"MIMIC(Rare)\" indicates the model\u2019s testing results on a rare disease subset.", "description": "This table compares the performance of RATD with other popular time series forecasting methods on the MIMIC-IV dataset.  It presents results for the entire test set and a subset of rare cases (less than 2% of the total). The \"Rare\" subset helps evaluate the model's ability to handle complex, less-frequent data.", "section": "5.3 Model Analysis"}, {"figure_path": "dRJJt0Ji48/tables/tables_8_2.jpg", "caption": "Table 1: Performance comparisons on four real-world datasets in terms of MSE, MAE, and CRPS. The best is in bold, while the second best is underlined.", "description": "This table presents a comparison of the proposed RATD model against various baseline methods across four real-world datasets.  The performance is evaluated using three metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Continuous Ranked Probability Score (CRPS).  The best performing model for each metric and dataset is highlighted in bold, with the second-best underlined.  This allows for a comprehensive comparison of the model's effectiveness against existing state-of-the-art methods.", "section": "5 Experiments"}, {"figure_path": "dRJJt0Ji48/tables/tables_9_1.jpg", "caption": "Table 4: Performance comparison(MSE) between CSDI-based methods, CSDI represents the basic network framework, CSDI+Linear denotes the approach where inputs and references are concatenated via a linear layer and fed into the network together, CSDI+CrossAttention signifies the use of cross attention to fuse features from inputs and references, and finally, CSDI+RMA, which incorporates an additional RMA.", "description": "This table presents a comparison of Mean Squared Error (MSE) achieved by different models on various datasets.  The models compared are CSDI (baseline), CSDI+Linear (adding linear concatenation of inputs and references), CSDI+Cross Attention (adding cross-attention fusion), and CSDI+RMA (the proposed method incorporating the Reference Modulated Attention module).  Lower MSE values indicate better performance.", "section": "5.3 Model Analysis"}, {"figure_path": "dRJJt0Ji48/tables/tables_9_2.jpg", "caption": "Table 1: Performance comparisons on four real-world datasets in terms of MSE, MAE, and CRPS. The best is in bold, while the second best is underlined.", "description": "This table presents a comparison of the performance of the proposed RATD model against several baseline time series forecasting methods across four real-world datasets: Exchange, Wind, Electricity, and Weather. The performance is evaluated using three metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Continuous Ranked Probability Score (CRPS).  The best performing model for each metric and dataset is highlighted in bold, while the second-best is underlined. This allows for a direct comparison of the different models' accuracy and uncertainty quantification capabilities across various datasets.", "section": "5 Experiments"}, {"figure_path": "dRJJt0Ji48/tables/tables_9_3.jpg", "caption": "Table 6: Ablation study on different RMA positions. The best is in bold.", "description": "This table presents the ablation study results on the impact of different RMA positions (front, middle, and back) on the model's performance.  The results are presented in terms of MSE, MAE, and CRPS metrics across four datasets (Exchange, Wind, Electricity, and Weather).  The best performing RMA position for each dataset and metric is shown in bold, highlighting the optimal placement of the RMA module within the network architecture for enhanced time series forecasting accuracy.", "section": "5.3 Model Analysis"}]