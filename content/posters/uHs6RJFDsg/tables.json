[{"figure_path": "uHs6RJFDsg/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of CLIP vs. state-of-the-art task-specific vision encoders. Our evaluation criteria encompass a variety of dimensions: comprehensive benchmarks [16], text-oriented Visual Question Answering (VQA) [17, 18], general VQA [19], object hallucination [20], Referring Expression Comprehension (REC) [21], Referring Expression Segmentation (RES) [21], and medical VQA benchmark SLAKE [22]. We use the same data for each model.", "description": "This table compares the performance of CLIP, a general-purpose vision encoder, against several state-of-the-art task-specific vision encoders across a range of tasks and benchmarks.  The benchmarks cover various aspects of visual understanding, including image-text matching, visual grounding, object detection, image segmentation, text recognition, chart understanding, document parsing, and medical image understanding. The table highlights that while CLIP excels in general image understanding, task-specific encoders often outperform it on more specialized tasks. This demonstrates the need for adaptive and task-aware vision encoding in multimodal large language models.", "section": "1 Introduction"}, {"figure_path": "uHs6RJFDsg/tables/tables_6_1.jpg", "caption": "Table 3: Performance comparison with current state-of-the-art frameworks on popular MLLM benchmarks. PT and SFT indicate the number of multimodal training samples in pretraining and finetuning stage. #IMG means the number of image tokens processed by LLM.", "description": "This table compares the performance of MoVA against other state-of-the-art multimodal large language models (MLLMs) across several benchmark datasets.  The benchmarks cover a range of tasks, and the table shows various metrics including performance scores on each benchmark.  The table also includes details about the language models used (LLM), the number of training samples used in both the pretraining and finetuning stages, and the number of image tokens processed by each model.  This allows for a comparison of MoVA's performance based on different LLMs and training scales.", "section": "4.2 MLLM Benchmarks"}, {"figure_path": "uHs6RJFDsg/tables/tables_7_1.jpg", "caption": "Table 3: Performance comparison with current state-of-the-art frameworks on popular MLLM benchmarks. PT and SFT indicate the number of multimodal training samples in pretraining and finetuning stage. #IMG means the number of image tokens processed by LLM.", "description": "This table compares the performance of MoVA against other state-of-the-art multimodal large language models (MLLMs) across various benchmarks.  It shows the model used (LLM), the number of parameters, the number of training samples used in pre-training (PT) and fine-tuning (SFT), the number of image tokens processed, and the performance scores on several benchmark tasks (MME, MMB, MMBCN, QBench, MathVista, MathVerse, POPE). The results highlight MoVA's competitive performance, particularly its ability to achieve significant gains in challenging tasks.", "section": "4.2 MLLM Benchmarks"}, {"figure_path": "uHs6RJFDsg/tables/tables_7_2.jpg", "caption": "Table 5: Performance comparison (Acc@0.5) on RefCOCO REC task. Specialists are specifically designed for the grounding task or finetuned on RefCOCO data.", "description": "This table compares the performance of various models on the RefCOCO Referring Expression Comprehension (REC) task.  The accuracy (Acc@0.5) is reported for three variations of the RefCOCO dataset (val, test-A, test-B). The models are categorized as either 'Generalist' (models with general-purpose capabilities) or 'Specialist' (models specifically designed for grounding tasks or finetuned on the RefCOCO dataset).  The table aims to demonstrate MoVA's performance relative to other generalist and specialist models, showcasing its effectiveness in a visual grounding task.", "section": "4.4 Visual Grounding"}, {"figure_path": "uHs6RJFDsg/tables/tables_8_1.jpg", "caption": "Table 6: Comparisons on the biomedical VQA datasets.", "description": "This table compares the performance of MoVA against LLaVA-Med and LLaVA-1.5 on two biomedical visual question answering (VQA) benchmark datasets: VQA-RAD and SLAKE.  The results are broken down by whether the evaluation was open-set (Open) or closed-set (Close).  LLaVA-Med (ft) represents the performance of LLaVA-Med after full finetuning on the benchmarks.  MoVA shows improvements in both open- and closed-set settings, indicating its ability to generalize and perform well on unseen data.", "section": "4.5 Medical Visual Question Answering"}, {"figure_path": "uHs6RJFDsg/tables/tables_8_2.jpg", "caption": "Table 7: Results of component-wise ablation studies.", "description": "This table presents the results of ablation experiments conducted on the MoVA model.  It shows the performance of MoVA compared to variations where key components are removed or replaced.  Specifically, it examines the effect of removing the context-aware expert routing, removing the MoV-Adapter, and using random routing instead of context-aware routing. The results are presented in terms of accuracy on the GQA, ChartQA, and DocVQA benchmarks, highlighting the contribution of each component to the overall model performance.", "section": "4.6 Ablation Study"}, {"figure_path": "uHs6RJFDsg/tables/tables_8_3.jpg", "caption": "Table 8: Results of K varying from 1 to 3.", "description": "This table presents the performance comparison of MoVA with different numbers of activated experts (K).  The results are shown for two tasks: GQA and ChartQA.  A dynamic selection of experts is compared against scenarios where a fixed number (1, 2, or 3) of experts are used. This helps illustrate the effectiveness of the model's adaptive expert routing strategy.", "section": "4.2 MLLM Benchmarks"}, {"figure_path": "uHs6RJFDsg/tables/tables_8_4.jpg", "caption": "Table 7: Results of component-wise ablation studies.", "description": "This table presents the results of ablation studies conducted on MoVA's components.  It shows the performance of MoVA on GQA, ChartQA, and DocVQA when using various configurations: The original MoVA design, a version with random routing instead of context-aware routing, a version without the routing mechanism entirely, and finally, a version without the MoV-Adapter.  By comparing these configurations, the table helps to assess the contribution of each component of MoVA and determine the impact of the coarse-to-fine expert routing and fusion strategy.", "section": "4.6 Ablation Study"}, {"figure_path": "uHs6RJFDsg/tables/tables_8_5.jpg", "caption": "Table 10: Open-world experiments.", "description": "This table presents the results of open-world experiments evaluating the performance of MoVA's expert routing in scenarios with novel vision experts.  It compares MoVA's performance (measured by accuracy) against baselines using different numbers of training samples (2K vs. 5K) and alternative routing designs (removing data augmentation, using an MLP classifier instead of an LLM). The results demonstrate MoVA's robustness and generalization ability in open scenarios.", "section": "4.6 Ablation Study"}, {"figure_path": "uHs6RJFDsg/tables/tables_17_1.jpg", "caption": "Table 12: Vision expert model configurations of vision experts in MoVA. Methods with * use a convolution layer to compress the output feature.", "description": "This table details the configurations of the various vision experts used in the MoVA model.  It shows the number of parameters (Params), the input resolution, width, depth, and the shape of the output feature map for each expert. The asterisk (*) indicates that a convolution layer is used to compress the output feature for that specific expert.", "section": "A Appendix / supplemental material"}, {"figure_path": "uHs6RJFDsg/tables/tables_17_2.jpg", "caption": "Table 3: Performance comparison with current state-of-the-art frameworks on popular MLLM benchmarks. PT and SFT indicate the number of multimodal training samples in pretraining and finetuning stage. #IMG means the number of image tokens processed by LLM.", "description": "This table compares the performance of MoVA against other state-of-the-art Multimodal Large Language Models (MLLMs) across several popular benchmarks.  It shows the performance (measured by different metrics depending on the benchmark) of various models, including different versions of MoVA using various LLMs (Vicuna-7B, Llama3-8B, and Hermes-Yi-34B).  The table also provides information on the number of training samples used (both in pre-training and supervised fine-tuning stages) and the number of image tokens processed by the LLM for each model.  This allows for a comparison of MoVA's performance relative to other models, considering both model size and training data.", "section": "4.2 MLLM Benchmarks"}, {"figure_path": "uHs6RJFDsg/tables/tables_18_1.jpg", "caption": "Table 3: Performance comparison with current state-of-the-art frameworks on popular MLLM benchmarks. PT and SFT indicate the number of multimodal training samples in pretraining and finetuning stage. #IMG means the number of image tokens processed by LLM.", "description": "This table compares the performance of MoVA against other state-of-the-art multimodal large language models (MLLMs) across various benchmarks.  It shows the model used (LLM), the number of training samples used in the pre-training (PT) and supervised fine-tuning (SFT) stages, the number of image tokens processed by the language model (#IMG), and the performance scores on several MLLM benchmarks: MME, MMB, MMBCN, QBench, MathVista, MathVerse, and POPE. The table helps demonstrate MoVA's competitive performance against other models, especially considering its efficiency in processing a relatively smaller number of image tokens.", "section": "4.2 MLLM Benchmarks"}, {"figure_path": "uHs6RJFDsg/tables/tables_18_2.jpg", "caption": "Table 3: Performance comparison with current state-of-the-art frameworks on popular MLLM benchmarks. PT and SFT indicate the number of multimodal training samples in pretraining and finetuning stage. #IMG means the number of image tokens processed by LLM.", "description": "This table compares the performance of MoVA against other state-of-the-art multimodal large language models (MLLMs) on several popular benchmarks.  It shows that MoVA achieves significant performance gains, particularly when using larger language models like Hermes-Yi-34B. The table also provides details on the number of training samples used for pretraining (PT) and finetuning (SFT), as well as the number of image tokens processed by the language model (#IMG). This demonstrates MoVA's efficiency and effectiveness.", "section": "4.2 MLLM Benchmarks"}]