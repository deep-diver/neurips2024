[{"figure_path": "XuWWq3gy7W/tables/tables_2_1.jpg", "caption": "Table 1: Comparison between BitDelta and a SVD based method, with Llama 2-7B and Llama 2-7B Chat as the base and fine-tuned models. BitDelta is performant across the board, whereas the SVD-based method fails to sufficiently capture the fine-tuned information.", "description": "This table compares the performance of BitDelta against a Singular Value Decomposition (SVD)-based method for capturing the weight delta between a base Llama 2-7B model and its fine-tuned counterpart (Llama 2-7B Chat).  The comparison is done across several evaluation metrics (TruthfulQA, GSM8K, MT-Bench, and an adjusted average).  The results show that BitDelta achieves consistently good performance across all metrics, while the SVD method, even with higher dimensionality (r=128), fails to adequately capture the information added during fine-tuning.", "section": "1-bit quantization"}, {"figure_path": "XuWWq3gy7W/tables/tables_4_1.jpg", "caption": "Table 2: BitDelta works on Llama-2 and Mistral families and on a wide range of model sizes ranging from 7B to 70B parameters. BitDelta works for many types of fine-tuned information, including SFT-based methods, RLHF-based methods, and context extension methods (RoPE scaling). Scale distillation is effective, raising TruthfulQA/GSM8K scores to within 1-2 points of the baseline fine-tune, and MT-Bench scores to within 0.1-0.2 points.", "description": "This table presents the results of BitDelta applied to various Llama-2 and Mistral models ranging from 7B to 70B parameters.  It demonstrates BitDelta's performance across different fine-tuning methods (SFT, RLHF, RoPE scaling) and model sizes, showcasing its effectiveness in preserving performance after 1-bit quantization of the fine-tuning delta.  The results show that scale distillation improves the scores on TruthfulQA, GSM8K, and MT-Bench, demonstrating that BitDelta maintains performance comparable to the baseline fine-tuned models.", "section": "3. Experiments"}, {"figure_path": "XuWWq3gy7W/tables/tables_5_1.jpg", "caption": "Table 2: BitDelta works on Llama-2 and Mistral families and on a wide range of model sizes ranging from 7B to 70B parameters. BitDelta works for many types of fine-tuned information, including SFT-based methods, RLHF-based methods, and context extension methods (RoPE scaling). Scale distillation is effective, raising TruthfulQA/GSM8K scores to within 1-2 points of the baseline fine-tune, and MT-Bench scores to within 0.1-0.2 points.", "description": "This table presents the results of BitDelta's performance across various model families (Llama-2 and Mistral), model sizes (7B to 70B parameters), and types of fine-tuning (SFT, RLHF, RoPE scaling).  It demonstrates BitDelta's effectiveness and the impact of scale distillation in improving model performance, showing minimal performance degradation compared to baseline fine-tuned models.", "section": "3.1 Method"}, {"figure_path": "XuWWq3gy7W/tables/tables_6_1.jpg", "caption": "Table 4: Comparison of model responses from Zephyr-7B-B for Question 9 in MT-Bench, a concise advertisement task. BitDelta-Initial is unable to follow the instructions, producing an advertisement that is overly formal and makes no attempt to adhere to the word limit. With the addition of scale distillation, BitDelta successfully produces a concise, catchy advertisement slightly over the word limit. *Prompt slightly modified for clarity.", "description": "This table compares the performance of BitDelta and BitDelta-Initial on a concise advertisement generation task from the MT-Bench benchmark using the Zephyr-7B-B model.  It demonstrates that the addition of scale distillation in BitDelta significantly improves the model's ability to follow instructions, generating more concise, catchy, and appropriate advertisements compared to BitDelta-Initial.", "section": "4.2 Accurate Quantization"}, {"figure_path": "XuWWq3gy7W/tables/tables_6_2.jpg", "caption": "Table 5: BitDelta achieves over 10\u00d7 compression. We can further compress the embedding and LM head layers, but leave this to future work due to inconsistencies in tokenizer vocabularies.", "description": "This table presents the compression factors achieved by BitDelta for several base language models.  It shows the original size of the model, the size of the delta after applying BitDelta's 1-bit quantization, and the resulting compression ratio.  The compression is substantial, exceeding a factor of 10 in all cases.  The table also notes that further compression could be achieved by applying the method to embedding and LM head layers; however, this was not pursued in the study due to variations in tokenizer vocabulary sizes across different models.", "section": "4.2 Accurate Quantization"}, {"figure_path": "XuWWq3gy7W/tables/tables_7_1.jpg", "caption": "Table 6: We apply BitDelta to Llama 2-7B Chat (with corresponding base model Llama 2-7B), and find it holds up when the underlying base model is quantized at various levels.", "description": "This table demonstrates the robustness of BitDelta even when the base model is quantized using different methods (FP16, INT8 RTN, GPTQ, QuIP#).  It shows that applying BitDelta to a quantized base model maintains performance, indicating the effectiveness of BitDelta's approach across various quantization techniques.  The results are presented in terms of TruthfulQA, GSM8K, MT-Bench scores, and an adjusted average.  The \"+\" symbol indicates the addition of BitDelta to the base model quantization method.", "section": "4.3 Latency Improvement"}, {"figure_path": "XuWWq3gy7W/tables/tables_14_1.jpg", "caption": "Table 7: We train a r = 16 LoRA finetune of Llama 2-7B on 1 epoch of UltraChat [17] and apply BitDelta with minimal performance degradation. This further shows the generality of BitDelta, which works on parameter-efficient fine-tunes in addition to full-parameter fine-tunes.", "description": "This table presents the results of applying BitDelta to a Llama 2-7B model fine-tuned using LoRA on the UltraChat dataset.  The table shows that BitDelta maintains performance comparable to the original fine-tuned model, indicating its effectiveness even with parameter-efficient fine-tuning methods. It highlights BitDelta's broad applicability across different fine-tuning techniques.", "section": "A.2 Additional Experiments"}, {"figure_path": "XuWWq3gy7W/tables/tables_14_2.jpg", "caption": "Table 6: We apply BitDelta to Llama 2-7B Chat (with corresponding base model Llama 2-7B), and find it holds up when the underlying base model is quantized at various levels.", "description": "This table presents the results of applying BitDelta to Llama 2-7B Chat, using different quantization methods for the base model (Llama 2-7B).  It demonstrates the robustness of BitDelta across various quantization levels (FP16, INT8 RTN, GPTQ, QuIP#). The table shows that BitDelta maintains its performance even when the base model is quantized, highlighting its effectiveness and adaptability to different quantization schemes.", "section": "4.2 Accurate Quantization"}, {"figure_path": "XuWWq3gy7W/tables/tables_14_3.jpg", "caption": "Table 9: Full results of the ablation over the fidelity of A, corresponding to Figure 3.", "description": "This table presents a detailed breakdown of the results from an ablation study on the fidelity of the delta (\u0394) in the BitDelta method.  It shows the performance across various metrics (ARC, BBH, HellaSwag, TruthfulQA, LAMBADA, WinoGrande, GSM8K, and Average) as the number of bits used to represent the delta is varied from 1 to 8 bits.  The results for Llama 2-7b and Vicuna-7b v1.5 models are shown separately, allowing for a comparison of model performance under different quantization levels.", "section": "4.2 Accurate Quantization"}, {"figure_path": "XuWWq3gy7W/tables/tables_15_1.jpg", "caption": "Table 2: BitDelta works on Llama-2 and Mistral families and on a wide range of model sizes ranging from 7B to 70B parameters. BitDelta works for many types of fine-tuned information, including SFT-based methods, RLHF-based methods, and context extension methods (RoPE scaling). Scale distillation is effective, raising TruthfulQA/GSM8K scores to within 1-2 points of the baseline fine-tune, and MT-Bench scores to within 0.1-0.2 points.", "description": "This table presents the results of BitDelta applied to various Llama-2 and Mistral models ranging from 7B to 70B parameters.  It demonstrates BitDelta's performance across different fine-tuning methods (SFT, RLHF, RoPE scaling) and model sizes, showing minimal performance degradation compared to the baseline models. The results highlight the effectiveness of scale distillation in improving the accuracy scores on TruthfulQA, GSM8K, and MT-Bench.", "section": "3. Experiments"}]