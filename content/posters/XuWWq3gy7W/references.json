{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper is foundational to the field of large language models (LLMs) and the pretrain-finetune paradigm, which is a key concept in BitDelta."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "BERT is a highly influential LLM architecture that is often used as a base model for fine-tuning, and understanding its impact is crucial to evaluating BitDelta's effectiveness."}, {"fullname_first_author": "Edward Beeching", "paper_title": "Open LLM leaderboard", "publication_date": "2023-01-01", "reason": "The Open LLM Leaderboard is a critical resource for evaluating the performance of LLMs, and the paper's inclusion of results from this benchmark strengthens the credibility of BitDelta's findings."}, {"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-26", "reason": "GPTQ is a widely used post-training quantization method, and comparing BitDelta's performance to GPTQ is essential for establishing BitDelta's novelty and efficiency."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-01-01", "reason": "LoRA is a popular parameter-efficient fine-tuning method, and the paper compares BitDelta to LoRA to show its improvement in efficiency and performance."}]}