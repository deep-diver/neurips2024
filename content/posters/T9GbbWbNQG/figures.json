[{"figure_path": "T9GbbWbNQG/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of LAST for two layers. Matrices are divided by lines on a per-state basis, and subsystems are sorted in descending order by their H\u221e norms. LAST scores are obtained by normalizing each H\u221e norm by the sum of all H\u221e norms in a layer when the states with lower H\u221e norms are excluded. Since LAST scores correlate with model-level output energy loss, we prune all parameters corresponding to states with low LAST scores.", "description": "This figure illustrates the Layer-Adaptive STate pruning (LAST) method for deep state space models (SSMs).  It shows how LAST works across two layers.  Each layer's matrices (A, B, C) are divided into sub-matrices representing individual states.  The H\u221e norm, a measure of subsystem energy, is calculated for each state. States are then sorted by their H\u221e norms. LAST scores are computed by normalizing each state's H\u221e norm by the total H\u221e norm within its layer.  This normalization allows for cross-layer comparison of state importance. Finally, states with the lowest LAST scores (representing the least significant states) are pruned, reducing the model's state dimensionality and computational cost.", "section": "3 LAST: Layer-adaptive state pruning for SSMs"}, {"figure_path": "T9GbbWbNQG/figures/figures_7_1.jpg", "caption": "Figure 2: Efficiency-accuracy trade-off curves of pruned S5 models for tasks in LRA benchmark. LAST maintained accuracy better than other methods, Uniform H\u221e and Global H\u221e (LAST without energy normalization), demonstrating its superior ability to identify insignificant states.", "description": "This figure displays the efficiency-accuracy trade-off curves for pruned S5 models across various tasks within the Long Range Arena (LRA) benchmark.  It compares the performance of LAST against three other pruning methods: Uniform H\u221e, Global H\u221e, and random pruning. The x-axis represents the state dimension (number of states after pruning), and the y-axis represents the accuracy. The results show that LAST consistently outperforms the other methods, maintaining higher accuracy even with significant state dimension reduction.  The comparison highlights the effectiveness of LAST's layer-adaptive state pruning and the benefit of its energy normalization technique in identifying and removing less important states.", "section": "5 Experiments"}, {"figure_path": "T9GbbWbNQG/figures/figures_8_1.jpg", "caption": "Figure 3: (Top) Evaluated state importance score and (Bottom) remaining state dimensions in an S5 model for Path-X task. The state indices are sorted by H\u221e scores, evaluated once for each conjugate pair.", "description": "This figure shows a comparison between Global H\u221e and LAST methods in terms of state importance scores and remaining state dimensions for an S5 model trained on the Path-X task. The top row displays the importance scores for each state across the six layers of the model, illustrating how LAST assigns higher scores to significant states compared to Global H\u221e. The bottom row presents the state dimensions remaining after pruning at different rates (0%, 20%, 40%, 60%, and 80%), showing that LAST effectively reduces the state dimension while preserving model performance. The rightmost plot visualizes the poles (complex numbers representing the system's dynamics) for Layer 6 at different pruning ratios, further illustrating LAST's ability to prune less significant states without affecting the model's stability.", "section": "5.4 Analysis"}, {"figure_path": "T9GbbWbNQG/figures/figures_14_1.jpg", "caption": "Figure 5: Search space in the two-dimensional coefficient space for stability.", "description": "This figure illustrates the stability region for a second-order system in the two-dimensional coefficient space (a1, a2). The larger blue-shaded triangle represents the entire stability region determined by the Schur-Cohn criterion (a2 < 1 and (1 + a2)\u00b2 - a1\u00b2 > 0).  The smaller, dark-red diamond shape within this triangle shows the stability region constrained by Montel's criterion (|a1| + |a2| \u2264 1). The black 'x' marks the zero initialization point (a1 = 0, a2 = 0), highlighting that this initialization is within both stability regions. The difference in size between the regions shows the more restrictive search space imposed by Montel's constraint in comparison to the broader Schur-Cohn criterion.", "section": "A.1 Stability of state space models"}, {"figure_path": "T9GbbWbNQG/figures/figures_21_1.jpg", "caption": "Figure 6: Efficiency-accuracy trade-off curves of pruned (Upper) S4D (Lower) S5 models for pixel-level image classification tasks. LAST obtained more efficient models that maintain performance compared to Uniform H\u221e, which was observed more stably and consistently than Global H\u221e (LAST w/o score normalization).", "description": "This figure compares the efficiency and accuracy of different pruning methods (random, uniform H\u221e, global H\u221e, and LAST) applied to S4D and S5 models on pixel-level image classification tasks (sMNIST, psMNIST, and sCIFAR).  The x-axis represents the remaining state dimension after pruning, and the y-axis represents the accuracy. The plot shows that LAST consistently outperforms other methods in maintaining accuracy while reducing the state dimension.", "section": "5.4 Analysis"}, {"figure_path": "T9GbbWbNQG/figures/figures_23_1.jpg", "caption": "Figure 2: Efficiency-accuracy trade-off curves of pruned S5 models for tasks in LRA benchmark. LAST maintained accuracy better than other methods, Uniform H\u221e and Global H\u221e (LAST without energy normalization), demonstrating its superior ability to identify insignificant states.", "description": "This figure shows the performance of different pruning methods (Random, Uniform H\u221e, Global H\u221e, and LAST) on various tasks from the Long Range Arena (LRA) benchmark.  The x-axis represents the remaining state dimension after pruning, while the y-axis represents the accuracy.  The plot demonstrates that LAST consistently outperforms the other methods in maintaining accuracy even with significant state dimension reduction, highlighting its effectiveness in identifying and removing less important states.", "section": "5 Experiments"}, {"figure_path": "T9GbbWbNQG/figures/figures_25_1.jpg", "caption": "Figure 2: Efficiency-accuracy trade-off curves of pruned S5 models for tasks in LRA benchmark. LAST maintained accuracy better than other methods, Uniform H\u221e and Global H\u221e (LAST without energy normalization), demonstrating its superior ability to identify insignificant states.", "description": "This figure shows the efficiency-accuracy trade-off curves for pruned S5 models across various tasks in the Long Range Arena (LRA) benchmark.  The x-axis represents the remaining state dimension after pruning, and the y-axis represents the accuracy.  The plot compares four methods: Random pruning, Uniform H\u221e pruning, Global H\u221e pruning, and LAST (Layer-Adaptive State pruning).  The results demonstrate that LAST consistently outperforms the other methods, maintaining high accuracy even with significant state dimension reduction.  The figure highlights LAST's effectiveness in identifying and removing less important states, preserving model performance while reducing model complexity.", "section": "5 Experiments"}]