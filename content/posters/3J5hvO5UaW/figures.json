[{"figure_path": "3J5hvO5UaW/figures/figures_6_1.jpg", "caption": "Figure 1: Profile risk for classifying two Gaussian centered in \u03bc0 = (0,0) and \u03bc1 = (\u22121,1) with quadratic loss and various values of \u03bb for the diagonal coefficients of \u03a0. The performative risk remains convex as long as \u03a0 is positive semidefinite i.e., \u03bb \u2265 0, and becomes non-convex whenever some of the \u03bbi are negative.", "description": "This figure shows contour plots of the performative risk for a binary classification problem with two Gaussian distributions.  The x and y axes represent the parameters \u03b81 and \u03b82 of a linear classifier. Different subplots show the performative risk with varying diagonal elements (\u03bb) of the matrix \u03a0, which represents the performative effect. The plots illustrate how the convexity of the performative risk depends on the values of \u03bb.  Specifically, the risk is convex when all \u03bbi are non-negative, and becomes non-convex when some \u03bbi are negative.", "section": "4 Convexity of Performative Risk"}, {"figure_path": "3J5hvO5UaW/figures/figures_8_1.jpg", "caption": "Figure 2: (a) Logistic regression to classify two Gaussian distributions centered in (0,0) and (-1,-1) and different magnitudes of performative effects \u03b3. We report the accuracy for three different magnitudes of the performative effects, from no performative effect (\u03b3 = 0) to a strong one (\u03b3 = 1). (b) we report the position of the parameter \u03b8 in its 2D-space, starting from (0,0) and following different paths depending on the algorithm. (c) Accuracy of a classification with quadratic loss on two Gaussian distributions of dimension 7 with various levels of variance \u03c3 of the distributions. (d) Same experiments but using the learnt \u03a0 for RPPerfGD. (e) In this case, distance between the true matrix \u03a0 and the estimated version. Note that in RGD and RRGD the estimation of \u03a0 is not used in the algorithm. (f) Logistic regression for the Housing dataset with various magnitude of performative shift \u03bb on the coordinates 0, 4 and 6. Accuracy is averaged over 20 runs.", "description": "This figure displays the results of several experiments evaluating the performance of different algorithms (RPPerfGD, RGD, RRGD, SFPerfGD) under different scenarios.  Subfigure (a) shows the accuracy of logistic regression for classifying two Gaussian distributions with varying strengths of performative effects. Subfigure (b) visualizes the parameter trajectories in a 2D parameter space.  Subfigure (c) illustrates classification accuracy with varying data variance, while subfigure (d) repeats the experiment using a learned parameter.  Subfigure (e) shows the difference between the true and estimated parameters. Finally, subfigure (f) demonstrates the performance on the Housing dataset with varying levels of performative shift.", "section": "6 Experiments"}, {"figure_path": "3J5hvO5UaW/figures/figures_14_1.jpg", "caption": "Figure 3: (a) Learning a logistic regression between two Gaussian distributions centered in (0,0) and (\u22121,\u22121) and different magnitude of performative effects \u03b3. (b) Accuracy of a classification with quadratic loss on two Gaussian of dimension 7 with various level of noise \u03c3", "description": "Figure 3 shows the result of using the Repeated Risk Minimization (RRM) algorithm on two different tasks.  Subfigure (a) illustrates the algorithm's performance in a logistic regression task with two Gaussian distributions, varying the magnitude of performative effects (\u03b3).  The plot shows the accuracy over iterations for three different levels of performative effects. Subfigure (b) demonstrates the algorithm's performance in a classification task with a quadratic loss, varying the noise level (\u03c3) in two 7-dimensional Gaussian distributions. Both subfigures show that RRM performs poorly in the presence of even moderate performative effects or noise.", "section": "6 Experiments"}]