[{"figure_path": "x4HMnqs6IE/figures/figures_2_1.jpg", "caption": "Figure 1: The forward pass of ID\u00b3 in terms of loss computation. Given an image, its face attributes, and its face embedding, ID\u00b3 obtains the image's noised version after t diffusion steps and employs a denoising network to denoise it. This denoising process is conditioned on the predicted attributes and the ID embedding. Optimization proceeds by minimizing a loss function comprised of a denoising term, a one-step reconstruction term, an inner-product term, and a constant.", "description": "This figure illustrates the forward pass of the ID\u00b3 model, showing how it uses an image, its attributes, and its embedding to generate a denoised image.  The process involves adding noise to the image, using a diffusion model to process it, and a denoising network to reconstruct a cleaned image conditioned on the identity and attributes.  The loss function is composed of terms for denoising, one-step reconstruction, an inner product term related to identity preservation, and a constant.", "section": "3 Methodology"}, {"figure_path": "x4HMnqs6IE/figures/figures_4_1.jpg", "caption": "Figure 1: The forward pass of ID\u00b3 in terms of loss computation. Given an image, its face attributes, and its face embedding, ID\u00b3 obtains the image's noised version after t diffusion steps and employs a denoising network to denoise it. This denoising process is conditioned on the predicted attributes and the ID embedding. Optimization proceeds by minimizing a loss function comprised of a denoising term, a one-step reconstruction term, an inner-product term, and a constant.", "description": "This figure illustrates the forward pass of the ID\u00b3 model during training. It shows how the model takes an image, its corresponding identity embedding (from a face recognition model), and its predicted attributes as input. The image is then noised using a diffusion process, and a denoising network is used to reconstruct the original image from this noisy version, conditioned on the identity and attributes. The training loss is a combination of a denoising term, a one-step reconstruction term, an inner-product term, and a constant, aiming to preserve identity while generating diverse facial attributes.", "section": "3 Methodology"}, {"figure_path": "x4HMnqs6IE/figures/figures_5_1.jpg", "caption": "Figure 2: Qualitative comparison of face images generated by the adjusted score function \u2207 log p(x|y, s) and the original score function \u2207 log p(x|y, s).", "description": "This figure compares the quality of face images generated using two different score functions: the adjusted score function (\u2207 log p(x|y, s)) and the original score function (\u2207 log p(x|y, s)).  The adjusted score function, a key innovation in the ID\u00b3 model, incorporates an additional term to preserve identity while increasing diversity. The figure visually demonstrates that the adjusted score function produces significantly higher-quality images compared to the original score function, showcasing the benefits of ID\u00b3's approach.", "section": "3.4 ID-Preserving Sampling"}, {"figure_path": "x4HMnqs6IE/figures/figures_6_1.jpg", "caption": "Figure 3: Uncurated samples generated by ID\u00b3 (Top) and those by IDiff-Face (Bottom).", "description": "This figure displays a qualitative comparison of face images generated by the proposed ID\u00b3 model (top row) and the existing IDiff-Face model (bottom row).  The goal is to show the visual differences between the two models in terms of identity preservation, intra-class diversity, and overall image quality. The top row, representing ID\u00b3, shows a variety of images showcasing different facial attributes and poses, yet maintaining consistent identity within each identity group. The bottom row, representing IDiff-Face, serves as a comparison, highlighting potential differences in the quality and diversity of generated images. This visual comparison is used to support the claim that ID\u00b3 generates higher-quality images with better identity preservation and diversity.", "section": "4.3.1 Qualitative Results"}, {"figure_path": "x4HMnqs6IE/figures/figures_13_1.jpg", "caption": "Figure A.1: An illustration of the dataset-generating algorithm.", "description": "The figure illustrates the dataset generation process.  First, N anchor embeddings (wi) are generated on a unit hypersphere, aiming for maximal separation. For each anchor, m identity embeddings (yij) are generated, clustering near the anchor but with some variance, controlled by the variables (vij).  These embeddings serve as conditioning signals along with attributes (sij) for generating diverse yet identity-consistent face images using the ID\u00b3 model. Each colored group on the sphere represents one identity, and the corresponding face images are shown to the right.", "section": "Appendix C"}, {"figure_path": "x4HMnqs6IE/figures/figures_14_1.jpg", "caption": "Figure A.2: The distribution of inter-class and intra-class similarities.", "description": "This figure shows the distribution of cosine similarity scores for both inter-class and intra-class comparisons of face embeddings generated by the ID\u00b3 model. The top panel shows the distribution for [lb, ub] = [0.5, 0.7], where lb and ub are lower and upper bounds of uniform distributions used for sampling, while the bottom panel displays the distribution for [lb, ub] = [0.7, 0.9].  The x-axis represents the cosine similarity, ranging from -1 to 1, and the y-axis shows the frequency counts.  The distributions demonstrate the model's ability to generate both diverse (inter-class) and consistent (intra-class) face images.", "section": "Appendix A"}, {"figure_path": "x4HMnqs6IE/figures/figures_15_1.jpg", "caption": "Figure A.3: Pose distribution of datasets generated by different models.", "description": "This figure compares the distribution of pose angles in datasets generated by different models.  The distributions from IDiffFace, ID\u00b3 PoseOnly 1, ID\u00b3 PoseOnly 2, and ID\u00b3 PoseOnly 3 are shown, alongside the distribution of pose angles in the FFHQ dataset. The purpose of this comparison is to illustrate the impact of varying the degree of control over the pose attribute during dataset generation on the final distribution of pose angles within the generated datasets.  Different distributions can affect the performance of face recognition models trained on these datasets.", "section": "F Attribute Analysis of Generated Datasets"}, {"figure_path": "x4HMnqs6IE/figures/figures_15_2.jpg", "caption": "Figure A.4: Pose distribution (pitch, yaw, roll) and age distribution on FFHQ", "description": "This figure presents the distributions of pitch, yaw, roll angles, and age within the FFHQ dataset.  Each subplot shows a histogram representing the frequency of different values for a specific attribute (pitch, yaw, roll, or age). These distributions demonstrate the range and commonality of these attributes within the training dataset used for the ID\u00b3 model, providing context for the model's performance in generating images with diverse facial characteristics.", "section": "Appendix F"}]