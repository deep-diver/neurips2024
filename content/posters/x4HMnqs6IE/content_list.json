[{"type": "text", "text": "$\\mathbf{ID}^{3}$ : Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jianqing $\\mathrm{Xu^{*1}}$ , Shen Li\u22172, Jiaying $\\mathrm{W}\\mathbf{u}^{2}$ , Miao Xiong2, Ailin Deng2, Jiazhen $\\mathrm{{Ji^{1}}}$ , Yuge Huang#1, Guodong $\\mathrm{M}\\mathrm{u}^{1}$ , Wenjie Feng2, Shouhong $\\mathrm{Ding}^{1}$ , and Bryan Hooi2 ", "page_idx": 0}, {"type": "text", "text": "1Tencent Youtu Lab 2National University of Singapore ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Synthetic face recognition (SFR) aims to generate synthetic face datasets that mimic the distribution of real face data, which allows for training face recognition models in a privacy-preserving manner. Despite the remarkable potential of diffusion models in image generation, current diffusion-based SFR models struggle with generalization to real-world faces. To address this limitation, we outline three key objectives for SFR: (1) promoting diversity across identities (interclass diversity), (2) ensuring diversity within each identity by injecting various facial attributes (intra-class diversity), and (3) maintaining identity consistency within each identity group (intra-class identity preservation). Inspired by these goals, we introduce a diffusion-fueled SFR model termed $\\mathrm{{ID}^{3}}$ . $\\bar{\\mathrm{ID}}^{3}$ employs an ID-preserving loss to generate diverse yet identity-consistent facial appearances. Theoretically, we show that minimizing this loss is equivalent to maximizing the lower bound of an adjusted conditional log-likelihood over ID-preserving data. This equivalence motivates an ID-preserving sampling algorithm, which operates over an adjusted gradient vector field, enabling the generation of fake face recognition datasets that approximate the distribution of real-world faces. Extensive experiments across five challenging benchmarks validate the advantages of $\\mathrm{{ID}^{3}}$ . Code is released at: https://github.com/hitspring2015/ID3-SFR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the introduction of various regulations restricting the use of large-scale facial data in recent years, such as GDPR, synthetic-based face recognition (SFR) (Boutros et al., 2023) has received widespread attention from the academic community (Qiu et al., 2021; Wood et al., 2021; Wang et al., 2023). The goal of SFR is to generate synthetic face datasets that mimic the distribution of real face images, and use it to train a face recognition (FR) model such that the model can recognize real face images as effectively as possible. ", "page_idx": 0}, {"type": "text", "text": "There exist numerous efforts to address SFR, which can be categorized into $G A N$ -based models and diffusion models. GAN-based models utilize adversarial training to learn to generate synthetic data for FR training. Recently, with the empirical advantages of diffusion models over GANs, many works have attempted to use diffusion models to generate synthetic face data in place of authentic data. However, the reported results by these state-of-the-art (SoTA) SFR generative models (Bae et al., 2023; Boutros et al., 2022; Kolf et al., 2023; Qiu et al., 2021; Boutros et al., 2023) show significant degradation in the verification accuracy in comparison to FR models trained by authentic data. We deduce the degradation might be due to two reasons. First, while previous works adopt diffusion models, they operate in the original score vector field without injecting the direction with regards to identity information, which makes them unable to guarantee identity-preserving sampling. Second, they fail to consider the structure of face manifold in terms of diversity during sampling. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We thus argue that the crux of SFR is to automatically generate a training dataset that has the following characteristics: (i) inter-class diversity: the training dataset covers sufficiently many distinct identities; (ii) intra-class diversity: each identity has diverse face samples with various facial attributes such as poses, ages, etc; (iii) intra-class identity preservation: samples within each class should be identity-consistent. Also note that, critically, the SFR dataset generation process should be fully automated without manual filtering or introducing auxiliary real face samples. ", "page_idx": 1}, {"type": "text", "text": "To this end, in this paper, we propose a novel IDentity-preserving-yet-Diversified Diffusion generative model termed $\\mathrm{\\dot{ID}^{3}}$ and a sampling algorithm for inference. Jointly leveraging identity and face attributes as conditioning signals, $\\mathrm{{ID^{3}}}$ can synthesize diversified face images that conform to desired attributes while preserving intra-class identity. Specifically, $\\mathrm{{ID}^{3}}$ generates a new sample based upon two conditioning signals: a target face embedding and a specific set of face attributes. The target face embedding enforces identity preservation while face attributes enrich intra-class diversity. To optimize $\\mathrm{{ID}^{3}}$ , we propose a new loss function that involves an explicit term to preserve identity. Theoretically, we show that with the addition of this term, minimizing the proposed loss function is equivalent to maximizing the lower bound of the likelihood of an adjusted conditional data log-likelihood. Consequently, this theoretical analysis motivates a new ID-preserving sampling algorithm that generates desired synthetic face images. To generate an SFR dataset, we further propose a new dataset-generating algorithm. This algorithm ensures inter-class diversity by solving the Tammes problem (Tammes, 1930), which maximally separates identity embeddings on the face manifold. In the meantime, it encourages intra-class diversity by perturbing identity embeddings randomly within prescribed areas. It works in conjunction with identity embeddings and diverse attributes to ensure inter-/intra-class diversity while preserving identity. Extensive experiments show that $\\mathrm{{ID}^{3}}$ outperforms other existing methods in multiple challenging benchmarks. ", "page_idx": 1}, {"type": "text", "text": "To sum up, our major contributions are listed as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Model with Theoretical Guarantees: We propose $\\mathrm{{ID}^{3}}$ , an identity-preserving-yetdiversified diffusion model for SFR. Theoretically, optimizing $\\mathrm{{ID}^{3}}$ is equivalent to shifting the original data likelihood to cover ID-preserving data.   \n\u2022 Algorithm Design: Motivated by this theoretical equivalence, we design a novel sampling algorithm for face image generation, together with a face dataset-generating algorithm, which effectively generates fake face datasets that approximate real-world faces.   \n\u2022 Effectiveness: Compared with SoTA SFR approaches, $\\mathrm{{ID}^{3}}$ improves SFR performance by $\\sim2.4\\%$ on average across five challenging benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The scope of this paper is synthetic-based face recognition (SFR), which focuses on generating high-quality training data (i.e., face images) for FR models. Generally, we aim to address SFR by generating face images that conform to diverse facial attributes while preserving identity within each class, in an automated manner. Technically, we break down this objective into the following two research questions (RQs) to be answered: ", "page_idx": 1}, {"type": "text", "text": "\u2022 RQ1: How can we effectively train a SFR generative model that preserves identity within each class, while boosting inter-class and intra-class diversity? ", "page_idx": 1}, {"type": "text", "text": "\u2022 RQ2: Once the generative model is trained, what sampling strategy can be employed to generate a synthetic face dataset that enables state-of-the-art face recognition models to perform well on real face benchmarks? ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper aims to answer these two questions, respectively, in order to improve synthetic face recognition performance. ", "page_idx": 1}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/f02ba8ca430ffe62d7d9813a10882a6a7393cba509a819fe7ad1d4f99799a95d.jpg", "img_caption": ["Figure 1: The forward pass of $\\mathrm{{ID}^{3}}$ in terms of loss computation. Given an image, its face attributes, and its face embedding, $\\mathrm{{ID}^{3}}$ obtains the image\u2019s noised version after $t$ diffusion steps and employs a denoising network to denoise it. This denoising process is conditioned on the predicted attributes and the ID embedding. Optimization proceeds by minimizing a loss function comprised of a denoising term, a one-step reconstruction term, an inner-product term, and a constant. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose $\\mathrm{{ID}^{3}}$ , a conditional diffusion model that generates diverse yet identity-preserving face images. $\\mathrm{{ID}^{3}}$ solves RQ1 by introducing two conditioning signals (identity embeddings and face attributes) into a diffusion model which is trained using a novel loss function. The loss function, together with identity embeddings, ensures intra-class identity preservation, while generation upon various face attributes give rise to intra-/inter-class diversity of face appearances. Our theoretical result regarding this loss function leads to an ID-preserving sampling algorithm and, further, an effective dataset-generating algorithm. ", "page_idx": 2}, {"type": "text", "text": "Notations. Throughout the rest of the the paper, we let $\\mathcal{D}$ denote a real face dataset that contains face images $\\mathbf{x}_{0}\\in\\check{\\mathbb{R}}^{H\\times W\\times3}$ . Let y denote a desired identity embedding and s be face attributes. ", "page_idx": 2}, {"type": "text", "text": "3.1 Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We build up our generative model, $\\mathrm{{ID}^{3}}$ , upon denoising diffusion probabilistic models (diffusion models for short) (Ho et al., 2020; Song et al., 2022; Rombach et al., 2022) as they empirically exhibit SoTA performance in the field of image generation. Diffusion models can be seen as a hierarchical VAE whose optimization objective is to minimize the KL divergence between the true data distribution and the model distribution $p_{\\theta}$ , which is equivalent to minimizing the expected negative log-likelihood (NLL), $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[-\\log p_{\\pmb{\\theta}}(\\mathbf{x})]$ . However, directly minimizing the expected NLL is intractable, therefore diffusion models instead minimize its evidence lower bound (ELBO), where the ELBO term can further simply to a denoising task with several model assumptions: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\log p(\\mathbf{x})\\geq\\underbrace{\\mathbb{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})}\\left[\\log\\frac{p(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})}\\right]}_{\\mathrm{ELBO}}}\\\\ {\\displaystyle=\\mathbb{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})}\\left[-\\frac{1}{2}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{1},1)\\right\\|_{2}^{2}\\right]-\\frac{1}{T-1}\\sum_{t=2}^{T}\\mu_{t}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{t},t)\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where \u00b5t := $\\begin{array}{r}{\\mu_{t}:=\\frac{T-1}{2\\sigma_{q}^{2}(t)}\\cdot\\frac{\\bar{\\alpha}_{t-1}(1-\\alpha_{t})^{2}}{(1-\\bar{\\alpha}_{t})^{2}},\\bar{\\alpha}_{t}=\\prod_{\\tau=1}^{t}\\alpha_{\\tau}}\\end{array}$ . Specifically, given a sample $\\mathbf{x}_{\\mathrm{0}}$ (or interchangeably, $\\mathbf{x}$ ) from the image distribution, a sequence $\\mathbf{x}_{1}$ , $\\mathbf{x}_{2}$ , ..., $\\mathbf{x}_{T}$ of noisy images is produced by progressively adding Gaussian noise according to a variance schedule $\\alpha_{1},...,\\alpha_{T}$ . This process is called the forward diffusion process $q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}\\bar{})$ . At the final time step, $\\mathbf{x}_{T}$ is assumed to be pure Gaussian noise: $\\mathbf{x}_{T}\\sim\\mathcal{N}(\\bar{0_{,}}I)$ . The objective is to train a denoising network $\\hat{\\mathbf{x}}_{\\pmb{\\theta}}$ that is able to predict the original image from the noisy image $\\mathbf{x}_{t}$ and the time step $t$ . To sample a new image, we sample $\\mathbf{x}_{T}\\sim\\bar{\\mathcal{N}}(0,I)$ and iteratively denoise it, producing a sequence $\\mathbf{x}_{T}$ $\\mathbf{\\Phi},\\,\\mathbf{x}_{T-1},\\,...,\\,\\mathbf{x}_{1},\\,\\mathbf{x}_{0}$ . The final image, $\\mathbf{x}_{\\mathrm{0}}$ , should resemble the training data. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Although the naive diffusion models are powerful in generating images, they do not deliver the promise of generating face images of the same identity (i.e. identity preservation) without direct corresponding information; nor are they aware of diverse desired facial attributes during inference. To achieve intra-class diversity and intra-class identity preservation, we would like to gain control of generating desired identities, each of which exhibits various attributes, including poses, ages and background variations. Hence, our aim is to design a diffusion model that conditions on specific identities and attributes throughout the generation of face images. ", "page_idx": 3}, {"type": "text", "text": "3.2 $\\mathbf{ID}^{3}$ as Conditional Diffusion Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose a conditional diffusion model, $\\mathrm{{ID^{3}}}$ (see Figure 1 for details). Specifically, we extend the denoising network by conditioning it on two sources of signals: identity signals y and face attribute signals s. The identity signals capture discernible faces in generated images, whereas face attribute signals specify the identity-irrelevant attributes, including poses, ages, etc. We introduce how to obtain these two conditioning signals, respectively, in the next two subsections. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Identity Conditioning Signal ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To obtain identity conditioning signals, we assume access to a pretrained face recognition model $f_{\\phi}$ : $\\mathbb{R}^{H\\times W\\times3}\\mapsto\\mathbb{S}^{d-1}$ , which maps the domain of face images to a feature space $\\mathbb{S}^{d-1}$ . This mapping $f_{\\phi}$ is parameterized by the learnable parameter $\\phi$ , which is obtained by training the model on a real face dataset in the face recognition task. We follow the latest advancement of face recognition by setting the output space to be a unit hypersphere $\\mathbb{S}^{d-1}$ . Then, given a face image $\\mathbf{x}_{\\mathrm{0}}$ drawn from the dataset $\\mathcal{D}$ , we obtain its identity embedding $\\mathbf{y}\\in\\mathbb{S}^{d-1}$ by feeding it into a face recognition model $f_{\\phi}\\colon\\mathbf{y}=f_{\\phi}{\\big(}\\mathbf{x}_{0}{\\big)}$ , which serves as the identity conditioning signals for $\\mathrm{{ID^{3}}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Face Attribute Conditioning Signal ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Face attributes capture identity-irrelevant information about face images, such as age, face poses, etc. To obtain face attribute as conditioning signals, we employ pretrained attribute predictors (Serengil and Ozpinar, 2021) which output these attributes when given a face image as input. The pretrained attribute predictors are a collection of ad-hoc domain experts in age estimation and pose estimation. After obtaining each of these attribute values, $\\mathbf{s}_{\\mathrm{age}}\\in[0,100]$ , $\\mathbf{s}_{\\mathrm{pose}}\\in[-90^{\\circ},90^{\\circ}]^{3}$ , we concatenate them as the overall attribute $\\mathbf{s}=[\\mathbf{s}_{\\mathrm{age}},\\mathbf{s}_{\\mathrm{pose}}]$ which is then fed into the diffusion model as conditioning signals. ", "page_idx": 3}, {"type": "text", "text": "3.3 Optimization Objective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Now the denoising network in Eq. (1) becomes $\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{y},\\mathbf{s})$ that takes as input the noised $\\mathbf{x}_{t}$ , the time step $t$ , and the conditioning signals $\\mathbf{y}$ and s. To optimize $\\mathrm{{ID}^{3}}$ , we construct a training objective upon the ELBO of $\\log p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})$ , ensuring that $\\mathrm{{ID}^{3}}$ generates identity-preserving yet diversified faces: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\theta}{\\operatorname*{min}}\\,\\mathbb{E}_{(\\mathbf{x}_{0},\\mathbf{y},\\mathbf{s})\\sim\\mathcal{D}^{\\prime}}\\bigg[\\mathcal{L}_{\\theta,\\phi}(\\mathbf{x}_{0},\\mathbf{y},\\mathbf{s})\\bigg]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\pmb{\\theta}$ is the learnable parameter of the denoising network and the datapoint-wise loss is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{L}_{\\theta,\\phi}(\\mathbf{x}_{0},\\mathbf{y},\\mathbf{s})}\\\\ &{}&{=\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\left[\\underbrace{\\mu_{t}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(t)}\\right\\|_{2}^{2}}_{\\mathrm{denoising~term}}-\\underbrace{\\lambda_{t}\\kappa_{\\mathbf{x}_{0}}\\mathbf{y}^{T}f_{\\phi}\\left(\\hat{\\mathbf{x}}_{0}^{(t)}\\right)}_{\\mathrm{inner.\\,product~term}}\\right]+\\mathbb{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})}\\left[\\underbrace{\\frac{1}{2}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(1)}\\right\\|_{2}^{2}}_{\\mathrm{one-ster~reconstruction~term}}\\right]+C_{\\theta,\\phi}(\\mathbf{x}_{1}|\\mathbf{x}_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/d9224f0b9a5c32a95558f7343863cea8a7d914dafe407c91bf4095241ea337b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where \u02c6x0 is the output of the denoising network that takes as input the conditioning signals $\\mathbf{y},\\mathbf{s}.$ , the time $t$ and the $t$ -step noisified image $\\mathbf{x}_{t}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{0}^{(t)}:=\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{y},\\mathbf{s}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Symbolically, $\\hat{\\mathbf{x}}_{0}^{(t)}$ denotes the denoised image predicted by the denoising network when given the $t$ -step noisified $\\mathbf{x}_{t}$ , the time $t$ and the associated conditioning signals $\\mathbf{y},\\mathbf{s}$ . The coefficients, $\\kappa_{\\mathbf{x}_{\\mathrm{0}}}$ and $\\lambda_{t}$ are scalars depending on $\\mathbf{x}_{\\mathrm{0}}$ and $t$ , respectively, and $C$ is a constant that does not depend on the learnable parameters $\\pmb{\\theta}$ . The specific value of $C$ will be elaborated in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "To summarize, our proposed loss function consists of four terms: the one-step reconstruction term, the denoising term, the inner-product term, and a constant. Intuitively, the denoising term, along with the one-step reconstruction term, aims to improve the generative quality by denoising the $t$ -step noisified face images while the inner-product term encourages the face embedding of the denoisified images to get close to the groundtruth identity embedding. To understand this loss function systematically, we theoretically find that minimizing this proposed loss function is equivalent to the maximization of the lower bound of an adjusted conditional log-likelihood over identity-preserving face images, which further leads us to an ID-preserving sampling algorithm. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Minimizing $\\mathcal{L}$ with regard to $\\pmb{\\theta}$ is equivalent to minimizing the upper bound of an adjusted conditional data negative log-likelihood $-\\log\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})$ , i.e.: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathcal{L}(\\mathbf{x}_{0},\\mathbf{y},\\mathbf{s})\\geq-\\log\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\\propto p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\\cdot p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})^{\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. The proof can be found in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Remark. We have just shown that our proposed loss is the upper bound of an adjusted conditional negative data log-likelihood. This adjusted likelihood $\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})$ can be factorized into the original likelihood $p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})$ and a reversed likelihood $p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})$ with some positive power. We term it as \u201cadjusted\u201d since the original likelihood is discounted by the reversed likelihood. Intuitively, the reversed likelihood shifts the original likelihood such that the adjusted likelihood covers ID-preserving data, which is attributed to the inner-product term we introduce into the loss function in Eq. (2). ", "page_idx": 4}, {"type": "text", "text": "3.4 ID-Preserving Sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 provides insights for designing a novel sampling algorithm in the spirit of Langevin dynamics applied on the adjusted conditional likelihood $\\tilde{p}(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})$ . We note that Langevin dynamics can generate new samples from a probability density $p$ by virtue of its score function (i.e., the gradient of the logarithm of the probability density w.r.t. the sample, $\\nabla_{\\mathbf{x}}\\mathrm{log}\\,p)$ . Motivated by this observation, we aim to find the score function of the adjusted likelihood for sample generation. Specifically, taking the logarithm and the gradient w.r.t. $\\mathbf{x}$ on both sides of Eq. (6) yields ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla\\log\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})=\\nabla\\log p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})+\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}\\nabla\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Input: Denoising network $\\hat{\\mathbf{x}}_{\\theta}$ ; recognition model $f_{\\phi}$ ; the number of identities $N$ . ", "page_idx": 5}, {"type": "text", "text": "Output: A synthetic dataset $\\mathcal{D}_{\\mathrm{syn}}$ . ", "page_idx": 5}, {"type": "equation", "text": "$\\mathbf{x}_{0}\\gets\\mathrm{Alg.~}2(\\hat{\\mathbf{x}}_{\\theta},f_{\\phi},\\mathrm{norm}(\\mathbf{y}_{i j}^{*}),\\mathbf{s}_{i j});$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "return $\\mathcal{D}_{\\mathrm{syn}}$ ", "text_level": 1, "page_idx": 5}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/3939dfae9948d8c6d603737ba101a5a0e8e0a18afdaab5e7a2c981319260a710.jpg", "img_caption": ["Figure 2: Qualitative comparison of face images generated by the adjusted score function $\\nabla\\log\\tilde{p}(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})$ and the original score function $\\nabla\\log p(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Then, our ID-preserving sampling algorithm first draws a Gaussian sample $\\mathbf{x}_{T}\\sim\\mathcal{N}(0,I)$ . Afterwards, sequentially, the algorithm performs the following update for $t$ iterating from $T$ backwards to 1: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t-1}\\leftarrow\\mathbf{x}_{t}+\\gamma\\nabla\\log\\tilde{p}(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})+\\sqrt{2\\gamma}\\epsilon\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\log\\tilde{p}(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})=\\underbrace{\\nabla\\log p(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})}_{\\mathrm{original\\,likelihood\\,score}}+\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}\\underbrace{\\nabla\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x}_{t})}_{\\mathrm{reversed\\,likelihood\\,score}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the original likelihood score in Eq. (8) can be evaluated by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\log p(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})=\\frac{\\sqrt{\\bar{\\alpha}_{t}}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\left(\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{y},\\mathbf{s})-\\frac{\\mathbf{x}_{t}}{\\sqrt{\\bar{\\alpha}_{t}}}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and the reversed likelihood score is given by a scaled inner product: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x}_{t})=\\kappa_{\\mathbf{x}_{t}}\\mathbf{y}^{T}\\nabla f_{\\phi}(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "See Appendix B for the derivation of the above equations. As such, our ID-preserving sampling algorithm performs sampling by searching a trajectory in the vector field $\\nabla\\log\\tilde{p}(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})$ that can maximize the adjusted conditional likelihood $\\tilde{p}(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})$ . See Algorithm 2 for the specific procedure. ", "page_idx": 5}, {"type": "text", "text": "Remark. Our proposed adjusted likelihood score differs from the original score by adding an extra scaled reversed likelihood score in Eq. (8). Consequently, as shown in Figure 2 , the resulting vector field differs from the original vector field, which leads to different Langevin sampling trajectories and thus different sampling quality. ", "page_idx": 5}, {"type": "text", "text": "3.5 Synthetic Dataset Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In terms of the second question (RQ2): after training $\\mathrm{{ID^{3}}}$ , with what sampling strategy is it possible to generate a synthetic face dataset on which SoTA face recognition models can be trained and perform well on challenging benchmarks? ", "page_idx": 5}, {"type": "text", "text": "Our proposed dataset-generating algorithm goes as follows: given $N$ target identities, we generate $N$ anchor embeddings distributed on the sphere: $\\mathbf{w}_{1},\\mathbf{w}_{2},...,\\mathbf{\\bar{w}}_{N}\\in\\mathbb{S}^{d-1}$ as uniformly as possible in the sense that each pair of the embeddings are maximally separated on the unit sphere#. For each anchor $\\mathbf{w}_{i}$ , we would like to generate $m$ identity embeddings perturbed around $\\mathbf{w}_{i}$ while ensuring that these $m$ identity embeddings get close to but different than $\\mathbf{w}_{i}$ . Specifically, to find these $m$ identity embeddings, we solve the following optimization problem $\\mathbf{P}_{i}$ : ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{y}_{i j},j=1,...,m}\\left\\|\\left[-\\mathbf{w}_{i}^{T}-\\right]\\mathrm{norm}\\left(\\underbrace{\\left[\\begin{array}{l l l l}{|}&{|}&{\\ddots}&{|}\\\\ {\\mathbf{y}_{i1}}&{\\mathbf{y}_{i2}}&{\\dots}&{\\mathbf{y}_{i m}}\\\\ {|}&{|}&{\\ddots}&{|}\\end{array}\\right]}_{\\mathbf{Y}_{i}}\\right)-\\left[\\nu_{i1},\\nu_{i2},\\dots,\\nu_{i m}\\right]\\right|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the operator norm is column-wise normalization which normalizes each column of $\\mathbf{Y}_{i}$ into a unit vector, and the desired similarity scores $\\nu_{i1},\\nu_{i2},...,\\nu_{i m}$ are randomly generated from a continuous uniform distribution $\\mathcal{U}[l b,u b)$ . After solving Eq. (9), we are able to retrieve the $m$ optimal unnormalized vector yi\u22171, ..., yi\u2217m. These $m$ vectors are then normalized, yielding $m$ identity embeddings: $\\mathrm{norm}(\\mathbf{y}_{i j}^{*})$ , for $j\\,=\\,1,...,m$ . Then, the resulting identity embeddings, along with face attributes, are fed into our generative models to generate face images. Finally, the entire dataset is generated by solving each $\\mathbf{P}_{i},i\\,=1,...,N$ , which yields $N$ identities, each with $m$ face images. The entire algorithm is summarized in Algorithm 3. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we verify the effectiveness of $\\mathrm{{ID^{3}}}$ through empirical evaluation of the face dataset that $\\mathrm{{ID^{3}}}$ generates, and verify the performance of the SoTA face recognition model trained on this dataset in comparison with other baseline methods. ", "page_idx": 6}, {"type": "text", "text": "4.1 Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Training Dataset: We train our proposed $\\mathrm{{ID^{3}}}$ on FFHQ (Karras et al., 2019) dataset. The FFHQ (FaceForensics $^{++}$ ) dataset is a large-scale dataset used for benchmarking and evaluating the performance of deep learning models in the field of face forensics. It is an extension of the original FaceForensics dataset, which was designed to facilitate the development and comparison of methods for detecting and preventing face manipulation and deepfakes. In order to compare with DCFace (Kim et al., 2023), we also train $\\mathrm{{ID^{3}}}$ on CASIA-WebFace (Yi et al., 2014). The CASIA-WebFace dataset is used for face verification and face recognition tasks. This dataset contains 494,414 face images of 10,575 real identities collected from the web. ", "page_idx": 6}, {"type": "text", "text": "Benchmarks: The performance of face recognition models is evaluated on various benchmark datasets: LFW (Huang et al., 2008), CFP-FP (Sengupta et al., 2016), CPLFW (Zheng and Deng, 2018), AgeDB (Moschoglou et al., 2017) and CALFW (Zheng et al., 2017). They are used to measure the impact of different factors on face image, such as pose changes and age variations. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For our $\\mathrm{{ID^{3}}}$ , we implement the denoising network with a U-net architecture and the projection module with a three-layer perceptron (hidden-layer size (512, 256, 768)) with ReLU activation. All models are implemented with PyTorch and trained from scratch using 8 NVIDIA Tesla V100 GPUs. Specifically, we set $\\lambda_{t}\\kappa_{\\mathbf{x}_{t}}=0.5\\cdot(1-1/(1+\\exp{(-t/T)})$ for the loss coefficients in Eq. (3), and use $T=1,000$ for the diffusion model; training batch size is set to 16 and the total training steps 500, 000. We directly use a pre-trained face recognition (FR) model sourced from $\\mathsf{p S p}$ (Richardson et al., 2021) as the identity feature extractor. Throughout the entire training process, these pre-trained models are frozen. In addition, we set $\\#$ of identity embeddings $m=25$ in Eq. (9) for each ID and match their embeddings with randomly selected attributes as conditioning signals for the diffusion model. For face recognition, we use LResNet50-IR (Deng et al., 2019), a variant of ResNet (He et al., 2016), as the backbone framework and follow the original configurations. ", "page_idx": 6}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/1a5a86aa694cd5f6fa818d8590dd7fcb4380b09f297a20f117e546ab1461312d.jpg", "img_caption": ["Figure 3: Uncurated samples generated by $\\mathrm{{ID}^{3}}$ (Top) and those by IDiff-Face (Bottom). "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "x4HMnqs6IE/tmp/a019779a1a2466daacf5d272ce7041c067251639f9221a22d7145e349b78984d.jpg", "table_caption": ["Table 1: SoTA Comparison. Face verification accuracy $(\\%)$ of LResNet50-IR on different benchmarks when trained on synthetic datasets from $\\mathrm{{ID}^{3}}$ and state-of-the-art SFR generative models. For fairness, all methods generate face datasets of 10K identities each of which has 50 face images. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Performance Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We test the performance of the face recognition model trained on synthetic face data generated by $\\mathrm{{ID^{3}}}$ and compare against SoTA SFR generative models, including IDiff-Face (Boutros et al., 2023), ID-Net (Kolf et al., 2023), DigiFace (Bae et al., 2023), SFace (Boutros et al., 2022), SynFace (Qiu et al., 2021) and DCFace (Kim et al., 2023). ", "page_idx": 7}, {"type": "text", "text": "4.3.1 Qualitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we illustrate a collection of face images generated by $\\mathrm{{ID^{3}}}$ as qualitative evaluation. Figure 3 shows the results for randomly sampled identities (IDs) under various attribute conditions; Obviously, when comparing different identities (inter-class), the essential intrinsic key information of each identity is still retained and can be easily identified. Also, different samples of each identity (intra-class) exhibit distinct diversity, stemming from variations in similarity scores $(\\nu_{i j}\\,^{\\circ}\\mathrm{s})$ and differences in face attributes as conditioning signals. In terms of the effect of our proposed adjusted score and the original score on the sampling algorithm, we observe that the face images generated by our proposed $\\mathrm{\\breve{ID}^{3}}$ exhibits much better quality and identity preservation than those generated by the original score function, as shown in Figure 2. ", "page_idx": 7}, {"type": "text", "text": "4.3.2 Quantitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare the accuracies of FR models trained on the synthetic face datasets generated by different generative models and demonstrate the results in Table 1. ", "page_idx": 7}, {"type": "text", "text": "As shown in Table 1, $\\mathrm{{ID^{3}}}$ demonstrates consistent superior performance, achieving the highest average accuracy of $86.50\\%$ , and outperforms other baselines in all benchmarks, notably scoring $83.78\\%$ in AgeDB and $85.00\\%$ in CFP-FP. This demonstrates the effectiveness of $\\mathrm{{ID^{3}}}$ in gaining pose and age control. Other methods, while effective to varying degrees, attain average scores below $86.50\\%$ and are inferior to $\\mathrm{{ID^{3}}}$ . ", "page_idx": 7}, {"type": "text", "text": "It is worth mentioning that $\\mathrm{{ID^{3}}}$ , apart from using real data during training, does not introduce any real images as auxiliary data during the sampling phase. The synthetic data is directly used in the training of the face recognition model without undergoing any secondary or manual filtering. Additionally, when training the face recognition model using the synthetic data, no real images are introduced as auxiliary data. On the other hand, DCFace, as described and reported in (Kim et al., 2023), introduces real face images as auxiliary data during the training phase for face recognition. ", "page_idx": 7}, {"type": "table", "img_path": "x4HMnqs6IE/tmp/2f8e5b7c9283127104c74d7eb3c9680849a1ca04e6770551f6ec97c1cdb5a9fa.jpg", "table_caption": ["Table 2: Ablation Study. Face verification accuracy $(\\%)$ of LResNet50-IR when trained on synthetic datasets from $\\mathrm{{ID}^{3}}$ and other model variants. $\\mathrm{ID}^{3}{-}[l b,u b]$ represents an $\\mathrm{{ID}^{3}}$ variant using $l b$ and $u b$ as lower- and upper-bound for sampling $\\nu_{i j}$ \u2019s. $\\mathrm{{ID}^{3}}$ -random denotes a model variant that randomly sets anchors on the unit hypersphere for sample generation. $\\mathrm{{ID}^{3}}$ -w/o-attribute denotes one that does not use attributes as conditioning signals. $\\mathrm{{ID}^{3}}$ -w/o-reversed denotes one that removes the reversed likelihood score from Eq. (8) in the proposed ID-preserving sampling algorithm. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "This helps enhance the diversity of the training data and leads to slightly better results than $\\mathrm{{ID^{3}}}$ in the two benchmarks. ", "page_idx": 8}, {"type": "text", "text": "Ablation study. We further investigate the impact of each contributing component of $\\mathrm{{ID^{3}}}$ in generating a synthetic face dataset on SFR. This includes three ablation studies shown in Table 2: the effect of the reversed likelihood score in Eq. (8) on ID-preserving sampling algorithm $\\mathrm{{ID}^{3}}$ vs. $\\mathrm{{ID^{3}}}$ - w/o-reversed), the effect of using anchors in $\\mathrm{{ID^{3}}}$ $\\mathrm{{ID}^{3}}$ vs. $\\mathrm{{ID^{3}}}$ -random), and the effect of lower- and upper-bound of Uniform distribution for sampling $\\nu_{i j}$ \u2019s. ", "page_idx": 8}, {"type": "text", "text": "In the first study, we compare $\\mathrm{{ID^{3}}}$ with $\\mathrm{{ID^{3}}}$ -w/o-reversed, which removes the reversed likelihood score from Eq. (8) in the proposed ID-preserving sampling algorithm. We observed $\\mathrm{{ID^{3}}}$ consistently outperforms $\\mathrm{{ID^{3}}}$ -w/o-reversed with large margins. This suggests the necessity of the inner-product term in the proposed loss function Eq. (3) and the reversed likelihood score in the adjusted likelihood score Eq. (8). ", "page_idx": 8}, {"type": "text", "text": "In the second study (cf. Appendix E), an appropriate smaller value of $l b$ , if not exceeding a certain range, can increase the intra-class diversity, resulting in more diverse intra-class face images. This aligns with our objective of increasing intra-class diversity in the generated data to enhance the effectiveness of SFR. As per the constraints of Eq. (9), each generated identity embedding $\\mathbf{y}_{i j}$ maintains the same identity as the anchor $\\mathbf{w}_{i}$ . This, along with our proposed inner-product term in Eq. (3), ensures consistent intra-class identities while introducing a significant amount of diversity. ", "page_idx": 8}, {"type": "text", "text": "In the third study, we demonstrate how effective it is to use maximally-separated anchors in $\\mathrm{{ID^{3}}}$ as compared to $\\mathrm{{\\dot{ID}}^{3}}$ -random that randomly sets anchors on the unit hypersphere for sample generation. Clearly, $\\mathrm{{ID^{3}}}$ -random does not yield as good results as $\\mathrm{{ID^{3}}}$ . This is because the random sampling method only introduces one identity signal per ID, while the model requires a combination of attributes and identity signals. Attribute signals can only control explicit attributes, whereas identity signals control implicit properties. Introducing only one identity signal per identity implies insufficient intra-class diversity. Additionally, $\\mathrm{{ID^{3}}}$ -random fails to regularize the relationship among different identities, leading to inadequate diversity among classes or aliasing issues with different identity signals. The identity signals obtained using $\\mathrm{{ID^{3}}}$ resolves the problem of aliasing between identity signals across classes, effectively improving intra-/inter-class diversity. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have proposed $\\mathrm{{ID}^{3}}$ , an identity-preserving-yet-diversified diffusion generative model for SFR. Our theoretical analysis regarding the training of $\\mathrm{{ID}^{3}}$ induces a new ID-preserving sampling algorithm and further, a dataset-generating algorithm that generates identity-preserving face images with inter-/intra-class diversity. Extensive experiments show that $\\mathrm{{ID}^{3}}$ outperforms existing methods in challenging multiple benchmarks. ", "page_idx": 8}, {"type": "text", "text": "Limitations. While $\\mathrm{{ID}^{3}}$ , designed for the sake of privacy protection, achieves SoTA performance in SFR, there remains clear margins as compared to the FR performance when training with real-world face datasets such as MS1M. This suggests that the fake face dataset generated by $\\mathrm{{ID}^{3}}$ does not fully approximate the real-world faces. Future work might include closing this gap. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Gwangbin Bae, Martin de La Gorce, Tadas Baltru\u02c7saitis, Charlie Hewitt, Dong Chen, Julien Valentin, Roberto Cipolla, and Jingjing Shen. Digiface-1m: 1 million digital face images for face recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3526\u20133535, 2023.   \nFadi Boutros, Marco Huber, Patrick Siebke, Tim Rieber, and Naser Damer. Sface: Privacy-friendly and accurate face recognition using synthetic data. In 2022 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u201311. IEEE, 2022.   \nFadi Boutros, Jonas Henry Grebe, Arjan Kuijper, and Naser Damer. Idiff-face: Synthetic-based face recognition through fizzy identity-conditioned diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19650\u201319661, 2023.   \nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019.   \nYu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. Disentangled and controllable face image generation via 3d imitative-contrastive learning. In IEEE Computer Vision and Pattern Recognition, 2020.   \nYandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 87\u2013102. Springer, 2016.   \nMd Abul Hasnat, Julien Bohne\u00b4, Jonathan Milgram, St\u00b4ephane Gentric, and Liming Chen. von mises-fisher mixture model-based deep learning: Application to face verification. arXiv preprint arXiv:1706.04264, 2017.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \nGary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and recognition, 2008.   \nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4396\u20134405, 2019.   \nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in neural information processing systems, 33:12104\u2013 12114, 2020.   \nMinchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu. Dcface: Synthetic face generation with dual condition diffusion model, 2023.   \nJan Niklas Kolf, Tim Rieber, Jurek Elliesen, Fadi Boutros, Arjan Kuijper, and Naser Damer. Identity-driven three-player generative adversarial network for synthetic-based face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 806\u2013816, 2023.   \nShen Li, Jianqing Xu, Xiaqing Xu, Pengcheng Shen, Shaoxin Li, and Bryan Hooi. Spherical confidence learning for face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15629\u201315637, 2021.   \nZhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding\u2013supplementary materials\u2013.   \nJianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Conditional image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5524\u20135532, 2018.   \nCalvin Luo. Understanding diffusion models: A unified perspective, 2022.   \nPascal Mettes, Elise van der Pol, and Cees G M Snoek. Hyperspherical prototype networks. In Advances in ", "page_idx": 9}, {"type": "text", "text": "Neural Information Processing Systems, 2019. ", "page_idx": 9}, {"type": "text", "text": "Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 51\u201359, 2017.   \nHaibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, and Dacheng Tao. Synface: Face recognition with synthetic data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10880\u201310890, 2021.   \nElad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel CohenOr. Encoding in style: a stylegan encoder for image-to-image translation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo\u00a8rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022.   \nSoumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M Patel, Rama Chellappa, and David W Jacobs. Frontal to profile face verification in the wild. In 2016 IEEE winter conference on applications of computer vision (WACV), pages 1\u20139. IEEE, 2016.   \nSefik Ilkin Serengil and Alper Ozpinar. Hyperextended lightface: A facial attribute analysis framework. In 2021 International Conference on Engineering and Emerging Technologies (ICEET), pages 1\u20134. IEEE, 2021.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022.   \nPieter Merkus Lambertus Tammes. On the origin of number and arrangement of the places of exit on the surface of pollen-grains. Recueil des travaux botaniques n\u00b4eerlandais, 27(1):1\u201384, 1930.   \nDani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. Face0: Instantaneously conditioning a text-to-image model on a face. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u201310, 2023.   \nHao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5265\u20135274, 2018.   \nQixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.   \nWenqing Wang, Lingqing Zhang, Chi-Man Pun, and Jiu-Cheng Xie. Boosting face recognition performance with synthetic data and limited real data. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135, 2023.   \nErroll Wood, Tadas Baltru\u02c7saitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3681\u20133691, 2021.   \nJianqing Xu, Shen Li, Ailin Deng, Miao Xiong, Jiaying Wu, Jiaxiang Wu, Shouhong Ding, and Bryan Hooi. Probabilistic knowledge distillation of face ensembles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3489\u20133498, 2023.   \nYuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, Gang Yu, and Bin Fu. Facestudio: Put your face everywhere in seconds. arXiv preprint arXiv:2312.02663, 2023.   \nDong Yi, Zhen Lei, Shengcai Liao, and Stan Z. Li. Learning face representation from scratch, 2014.   \nTianyue Zheng and Weihong Deng. Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments. Beijing University of Posts and Telecommunications, Tech. Rep, 5(7), 2018.   \nTianyue Zheng, Weihong Deng, and Jiani Hu. Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments. arXiv preprint arXiv:1708.08197, 2017.   \nZheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, et al. Webface260m: A benchmark unveiling the power of million-scale deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10492\u201310502, 2021. ", "page_idx": 10}, {"type": "text", "text": "$\\mathbf{ID}^{3}$ : Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A Appendix A ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We first present a lemma (Lemma A.1) that will be further used to prove Theorem 3.1. ", "page_idx": 11}, {"type": "text", "text": "Lemma A.1. Given any two conditional probability density functions, $p(a|b)$ and $p(b|a)$ , and $a$ positive scalar $w>0$ , there exists another conditional probability density function $\\tilde{p}$ such that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\tilde{p}(a|b)=\\frac{1}{Z_{b,w}}p(a|b)p(b|a)^{w},w h e r e\\;Z_{b,w}=\\int p(a|b)p(b|a)^{w}d a.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. To show this result is equivalent to showing $\\tilde{p}(a|b)\\,\\propto\\,p(a|b)\\,\\cdot\\,p(b|a)^{w},w\\,>\\,0$ , which is equivalent to showing that, for any $b=b_{0}$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\tilde{p}(a|b=b_{0})\\propto p(a|b=b_{0})\\cdot p(b=b_{0}|a)^{w},w>0\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Note that ${\\tilde{p}}(a|b=b_{0})$ is a function of $a$ , i.e. there exists a function $f_{b_{0},w}$ such that $p(b=b_{0}|a)^{w}:=$ $f_{b_{0},w}(a)$ . Let ", "page_idx": 11}, {"type": "equation", "text": "$$\nZ_{b_{0},w}:=\\int p(a|b=b_{0})f_{b_{0},w}(a)d a\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Then, ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{p(a|b=b_{0})\\cdot p(b=b_{0}|a)^{w}}{Z_{b_{0},w}}=\\frac{p(a|b=b_{0})\\cdot f_{b_{0},w}(a)}{Z_{b_{0},w}}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "is a proper probability density function of $a$ with $b=b_{0}$ given. Therefore, Eq. (A.4) can be written as $\\bar{p}(a|\\bar{b}=\\bar{b}_{0})$ . The above proof holds true for any $b_{0}$ , which concludes the proof for $\\tilde{p}(a|b)\\;\\propto$ $p(a|b)\\,\\cdot\\,p(b|a)^{w},w\\;>\\;0$ . Note that this result can be trivially extended to multivariate random variables. ", "page_idx": 11}, {"type": "text", "text": "Theorem 3.1. Minimizing $\\mathcal{L}$ with regard to $\\pmb{\\theta}$ is equivalent to minimizing the upper bound of an adjusted conditional data negative log-likelihood $-\\log\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})$ , i.e.: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathcal{L}_{\\theta}(\\mathbf{x}_{0},\\mathbf{y},\\mathbf{s})\\geq-\\log\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\\propto p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\\cdot p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})^{\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and $\\mathbf{x}_{\\mathrm{0}}$ and $\\mathbf{x}$ both refer to a raw image interchangeably. ", "page_idx": 11}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{z}_{\\theta}(\\mathbf{x}_{0},\\mathbf{y},\\mathbf{s})=\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\bigg[\\underbrace{\\mu_{t}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(t)}\\right\\|_{2}^{2}}_{\\mathrm{denoising~term}}-\\underbrace{\\lambda_{t}\\kappa_{\\mathbf{x}}\\mathbf{y}^{T}f_{\\phi}\\left(\\hat{\\mathbf{x}}_{0}^{(t)}\\right)}_{\\mathrm{inner:product:}}\\bigg]+\\mathbb{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})}\\left[\\underbrace{\\frac{1}{2}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(1)}\\right\\|_{2}^{2}}_{\\mathrm{one-step~reconstuction~term}}\\right]}\\\\ {=\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\bigg[\\underbrace{\\mu_{t}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(t)}\\right\\|_{2}^{2}}_{\\mathrm{denoising~term}}\\bigg]+\\mathbb{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})}\\left[\\underbrace{\\frac{1}{2}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(1)}\\right\\|_{2}^{2}}_{\\mathrm{one-step~reconstruction~term}}\\right]+C-\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\bigg[\\underbrace{\\lambda_{t}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(t)}\\right\\|_{2}^{2}}_{\\mathrm{one-step~reconstruction~term}}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "It can be shown that the reversed likelihood $p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})$ is a joint vMF density (Xu et al., 2023; Hasnat et al., 2017): ", "page_idx": 11}, {"type": "equation", "text": "$$\np(\\mathbf{y},\\mathbf{s}|\\mathbf{x})\\overset{\\footnotesize(1)}{=}p(\\mathbf{y}|\\mathbf{s},\\mathbf{x})p(\\mathbf{s}|\\mathbf{x})\\overset{\\footnotesize(2)}{=}p(\\mathbf{y}|\\mathbf{x})p(\\mathbf{s}|\\mathbf{x})\\overset{\\footnotesize(3)}{=}J_{\\kappa_{\\mathbf{x}}}^{2}\\exp\\left(\\kappa_{\\mathbf{x}}(\\mathbf{y}^{T}f_{\\phi}\\left(\\mathbf{x}\\right)+\\mathbf{s}^{T}F_{a}(\\mathbf{x}))\\right)\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $J_{\\kappa_{\\times}}$ is the normalizing constant and $F_{a}$ is the pretrained attribute predictor. Note that Equality (1) is obtained by the product rule of probability; Equality (2) is obtained by observing that $\\mathbf{y}$ and s are conditionally independent when $\\mathbf{x}$ is given; and Equality (3) is obtained by assuming that ", "page_idx": 12}, {"type": "equation", "text": "$$\np(\\mathbf{y}|\\mathbf{x})=J_{\\kappa_{\\mathbf{x}}}\\exp{\\left(\\kappa_{\\mathbf{x}}\\cdot\\mathbf{y}^{T}f_{\\phi}\\left(\\mathbf{x}\\right)\\right)},\\quad p(\\mathbf{s}|\\mathbf{x})=J_{\\kappa_{\\mathbf{x}}}\\exp{\\left(\\kappa_{\\mathbf{x}}\\cdot\\mathbf{s}^{T}F_{a}(\\mathbf{x})\\right)}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that these reasonable assumptions are also held in ( $\\mathrm{\\DeltaXu}$ et al., 2023; Li et al., 2021; Hasnat et al., 2017). Now we can specify the value of the scalar $C$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nC=-\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\left[\\lambda_{t}\\left(\\log{J_{\\kappa_{\\times}}^{2}}+\\mathbf{s}^{T}F_{a}(\\mathbf{x})\\right)\\right]-\\frac{n}{2}\\log(2\\pi)+D_{\\mathrm{KL}}\\left(q(\\mathbf{x}_{T}|\\mathbf{x}_{0})||p(\\mathbf{x}_{T})\\right)+\\log Z\\right].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $n=3H W$ is the dimensionality of $\\mathbf{x}$ , and ", "page_idx": 12}, {"type": "equation", "text": "$$\nZ=\\int_{\\mathbf{x}}p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\\cdot p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})^{\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}}d\\mathbf{x}=Z\\left(\\mathbf{y},\\mathbf{s},{\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that $Z$ only depends on $\\mathbf{y}$ , s and $\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}$ and hence $C$ is a scalar that does not depend on the learnable parameter $\\pmb{\\theta}$ . Therefore, $\\mathcal{L}$ can be rewritten into a sum of two parts: $\\mathcal{L}=\\mathcal{L}_{1}+\\mathcal{L}_{2}$ , where ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\Sigma}_{1}=\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\left[\\underbrace{\\mu_{t}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(t)}\\right\\|_{2}^{2}}_{\\mathrm{denoising~term}}\\right]+\\mathbb{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})}\\left[\\underbrace{\\frac{1}{2}\\left\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{0}^{(1)}\\right\\|_{2}^{2}}_{\\mathrm{cne~step~reconstruction~term}}\\right]-\\frac{n}{2}\\log(2\\pi)+D_{\\mathrm{KL}}\\left(q(\\mathbf{x}_{T}|\\mathbf{x}_{0})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}=-\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\bigg[\\lambda_{t}\\log J_{\\kappa_{\\times}}^{2}+\\lambda_{t}\\kappa_{\\mathbf{x}}\\mathbf{s}^{T}F_{a}(\\mathbf{x})+\\underbrace{\\lambda_{t}\\kappa_{\\mathbf{x}}\\mathbf{y}^{T}f_{\\phi}\\left(\\hat{\\mathbf{x}}_{0}^{(t)}\\right)}_{\\mathrm{inner-product\\,term}}\\bigg]+\\log Z\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We recognize $-\\mathcal{L}_{1}$ is the evidence lower bound (ELBO) of $\\log p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})$ (Luo, 2022), i.e. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}\\geq-\\log p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "As for ${\\mathcal{L}}_{2}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{2}=-\\mathbb{E}_{t\\sim\\mathcal{U}[2,T]}\\left[\\lambda_{t}\\left(\\log{J_{\\kappa_{\\mathrm{x}}}^{2}}+\\underbrace{\\kappa_{\\mathrm{x}}\\mathbf{y}^{T}f_{\\phi}\\left(\\hat{\\mathbf{x}}_{0}^{(t)}\\right)}_{\\mathrm{inner:product:term}}+\\kappa_{\\mathrm{x}}\\mathbf{s}^{T}F_{a}(\\mathbf{x})\\right)\\right]+\\log{Z}}\\\\ &{\\quad=-\\displaystyle\\frac{1}{T-1}\\sum_{t=2}^{T}\\left[\\lambda_{t}\\left(\\log{J_{\\kappa_{\\mathrm{x}}}^{2}}+\\underbrace{\\kappa_{\\mathrm{x}}\\mathbf{y}^{T}f_{\\phi}\\left(\\hat{\\mathbf{x}}_{0}^{(t)}\\right)}_{\\mathrm{inner:product:term}}+\\kappa_{\\mathrm{x}}\\mathbf{s}^{T}F_{a}(\\mathbf{x})\\right)\\right]+\\log{Z}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here we assume access to a perfect denoising module such that $\\hat{\\mathbf{x}}_{0}^{(t)}=\\mathbf{x}_{0}=\\mathbf{x}$ , for all $t$ \u2019s. Hence, ${\\mathcal{L}}_{2}$ can be written as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal{L}_{2}=-\\frac{1}{T-1}\\sum_{t=2}^{T}\\left[\\lambda_{t}\\left(\\log J_{\\kappa_{\\bf x}}^{2}+\\kappa_{\\bf x}\\mathbf{y}^{T}f_{\\phi}\\left(\\mathbf{x}\\right)+\\kappa_{\\bf x}\\mathbf{s}^{T}F_{a}(\\mathbf{x})\\right)\\right]+\\log Z}}\\\\ {\\displaystyle{\\qquad=-\\frac{1}{T-1}\\sum_{t=2}^{T}\\left[\\lambda_{t}\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})\\right]+\\log Z}}\\\\ {\\displaystyle{\\qquad=-\\frac{1}{T-1}\\left(\\sum_{t=2}^{T}\\lambda_{t}\\right)\\cdot\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})+\\log Z}}\\\\ {\\displaystyle{\\qquad=-\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}+\\log Z}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/fc1412cd6d8ec2b5b79673cc32d353a77cfd0ff11899222a3629ad9a19064764.jpg", "img_caption": ["Figure A.1: An illustration of the dataset-generating algorithm. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\mathcal{L}_{1}+\\mathcal{L}_{2}}\\\\ &{\\quad\\geq-\\left[\\log p(\\mathbf{x}|\\mathbf{y},\\mathbf{s})+\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x})^{\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}}\\right]+\\log Z}\\\\ &{\\quad=-\\log\\left(Z\\cdot\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})\\right)+\\log Z}\\\\ &{\\quad=-\\log\\tilde{p}(\\mathbf{x}|\\mathbf{y},\\mathbf{s})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where Equality (A.17c) is obtained by applying Lemma A.1. This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "B Appendix B ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we show the derivations of the following equations: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\log p(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})=\\frac{\\sqrt{\\bar{\\alpha}_{t}}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\left(\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{y},\\mathbf{s})-\\frac{\\mathbf{x}_{t}}{\\sqrt{\\bar{\\alpha}_{t}}}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x}_{t})=\\kappa_{\\mathbf{x}_{t}}\\mathbf{y}^{T}\\nabla f_{\\phi}(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that in the main text, we showed that the adjusted likelihood score is a summation of the original likelihood score and the scaled reversed likelihood score: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\underbrace{\\nabla\\log\\tilde{p}(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})}_{\\mathrm{adjusted\\;likelihood\\;score}}=\\underbrace{\\nabla\\log p(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})}_{\\mathrm{original\\;likelihood\\;score}}+\\frac{\\sum_{t=2}^{T}\\lambda_{t}}{T-1}\\underbrace{\\nabla\\log p(\\mathbf{y},\\mathbf{s}|\\mathbf{x}_{t})}_{\\mathrm{reversed\\;likelihood\\;score}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the original likelihood score, we note that our proposed $\\mathrm{{ID^{3}}}$ itself is a conditional diffusion model. By virtue of the relation between the score and the denoising module (i.e. the Tweedie\u2019s Formula) in diffusion models (cf. Equation (133) in (Luo, 2022)), we are able to show that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\log p(\\mathbf{x}_{t}|\\mathbf{y},\\mathbf{s})=\\frac{\\sqrt{\\bar{\\alpha}_{t}}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\left(\\mathbf{x}_{0}-\\frac{\\mathbf{x}_{t}}{\\sqrt{\\bar{\\alpha}_{t}}}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, $\\mathbf{x}_{\\mathrm{0}}$ can be approximated by denoising $\\mathbf{x}_{t}$ via the trained denoising module ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{x}_{0}\\approx\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{y},\\mathbf{s})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "C An Illustration of the Dataset Generating Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We illustrate the dataset generating algorithm in Figure A.1. First, $N$ anchor embeddings are generated on the sphere as uniformly as possible. Then, for each anchor, $m$ identity embeddings are generated around the anchor. This strategy ensures inter-class diversity while intra-class identity preservation is guaranteed. Colors show the correspondence between the generation procedure on the left and the generated samples on the right. ", "page_idx": 13}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/145799f7a4dbb717dc4951d43c35601d5ff0bebae825b172bd29c17f61f64a0b.jpg", "img_caption": ["Figure A.2: The distribution of inter-class and intra-class similarities. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Face Recognition. Face Recognition (FR) is the task of matching query imagery to an enrolled identity database. SoTA FR models are trained using margin-based softmax losses (Wang et al., 2018; Deng et al., 2019) on large-scale web-crawled datasets (Guo et al., 2016; Zhu et al., 2021). These datasets encompasses three characteristics in common (as mentioned in the introduction): (i) sufficient inter-class diversity; (ii) intra-class diversity; (iii) intra-class identity preservation. However, due to the introduction of various regulations restricing the use of authentic face data, researchers switch their attention to synthetic face recognition (SFR). We argue that the crux of SFR is to generate a training dataset that inherits the three characteristics above. ", "page_idx": 14}, {"type": "text", "text": "GAN-based SFR models. Most of the deep generative models for synthetic faces generation are based on GANs. DigiFace (Bae et al., 2023) utilizes a digital rendering pipeline to generate synthetic images based on a learned model of facial geometry and attributes. SFace (Boutros et al., 2022) and ID-Net (Kolf et al., 2023) train a StyleGAN-ADA (Karras et al., 2020) under a classconditional setting. SynFace (Qiu et al., 2021) extends DiscoFaceGAN (Deng et al., 2020) using synthetic identity mix-up to enhance the intra-class diversity. However, the reported results shown by these models show significant performance degradation in comparison to FR trained on real data. This performance gap is mainly due to inter-class discrimination and small intra-class diversity in their generated synthetic training datasets. Diffusion models for SFR. Recently, Diffusion Models (DMs) (Ho et al., 2020; Lin et al., 2018; Song et al., 2020) gained attention for both research and industry due to their potential to rival GANs on image synthesis, as they are easier to train without stability issues, and stem from a solid theoretical foundation. Among SFR diffusion models, IDiff-Face (Boutros et al., 2023) achieves SoTA performance. On the basis of a diffusion model, it incorporates Contextual Partial Dropout to generate diverse intra-class images. However, IDiff-Face fails to regularize the relationship among different identities. ", "page_idx": 14}, {"type": "text", "text": "Latent Diffusion Models. There exist many diffusion-based models (LDMs) (e.g., Face0 (Valevski et al., 2023), PhotoMaker (Li et al.), FaceStudio (Yan et al., 2023), InstantID (Wang et al., 2024)) which use ID attributes to assist in generating images. However, we note that these LDMs are designed for image generation but not for SFR. Our empirical findings further suggests these LDMs do not perform reasonably well even when applied to SFR. We found that although these LDMs claim to be ID-preserving in the pixel space, their feature embeddings are not discriminative enough for face recognition, since there is no inductive bias (neither loss functions nor architectures) to achieve face discriminativeness. ", "page_idx": 14}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/5a0c6f1f08098744a384093accf07a1128a5d8a500cca00f617736f97ed9fccd.jpg", "img_caption": ["Figure A.3: Pose distribution of datasets generated by different models. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "x4HMnqs6IE/tmp/0a53e607fb22c3d04b632b4116f79e5385968c812893f0a4c655fc9a247252dc.jpg", "img_caption": ["Figure A.4: Pose distribution (pitch, yaw, roll) and age distribution on FFHQ "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "E Ablation Study (ii) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We show the inter-class and intra-class similarity in Figure A.2when using [0.5, 0.7] and [0.7, 0.9] as the lower- and upper-bound $[l b,u b]$ for sampling $\\nu_{i j}$ \u2019s in our proposed dataset-generating algorithm. ", "page_idx": 15}, {"type": "text", "text": "F Attribute Analysis of Generated Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we perform an attribute analysis of the generated face datasets by $\\mathrm{{ID^{3}}}$ . As shown in Figure A.3 and Figure A.4, from the distribution, we observe that the highest age in our training set is 70; the largest pose is 60 in degree and the smallest -60 in degree. Our model can interpolate within these ranges but is less likely to extrapolate outside of these ranges. To generate face images of large pose and high age, one can collect more such data and add them to the training dataset, which increases their occurrences during training. ", "page_idx": 15}, {"type": "text", "text": "To examine whether the attributes we used in our paper are fit for SFR tasks, we perform the following ablation study: as we increase intra-class pose variation, the SFR performances on cross-pose test sets (including CFPFP and CPLFW) are boosted whereas the performances on cross-age test sets (including AgeDB and CALFW) remain almost unchanged. Results and distribution plots are shown in Table A.1, Figure A.3 and Figure A.4. From these results, we observe that the distribution of pose angle on FFHQ, Dataset by IDiffFace, Dataset by ID PoseOnly 1 and 2 are all unimodal. And their performances are inferior to Dataset by ID PoseOnly 3 which exhibits multimodal distribution of pose angle. ", "page_idx": 15}, {"type": "table", "img_path": "x4HMnqs6IE/tmp/e11969cbb11f945e342cc79bd763078ae424817591ab8ac890f726ef6c3be97a.jpg", "table_caption": ["Table A.1: Datasets generated by different models (Column 1), the attribute statistics for each dataset (Column 2, 3), and the FR performance of FR models trained on them, respectively (Column 4-8). "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: As shown in the abstract and introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: As shown in the conclusion. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: As shown in the Appendix A and B. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: As shown in the experiment and appendix ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: see Abstract. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: As shown in the experiments and appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: As shown in the experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: NA ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited. The license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: NA. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]