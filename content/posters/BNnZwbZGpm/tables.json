[{"figure_path": "BNnZwbZGpm/tables/tables_1_1.jpg", "caption": "Table 1: Comparisons of the Bilevel Opt. & Conditional Bilevel Opt. algorithms for finding an e-stationary point. We include comparison with stochastic gradient descent type of methods and a more comprehensive comparison including other acceleration methods can be found in Table 2 of Appendix. An e-stationary point is defined as ||\u2207h(x)|| \u2264 \u20ac. Gc(f, e) and Gc(g, e) denote the number of gradient evaluations w.r.t. f(x, y) and g(x, y); JV(g, e) denotes the number of Jacobian-vector products; HV(g, e) is the number of Hessian-vector products. m and n are the number of data examples for the outer and inner problems, in particular, n is the maximum number of inner problems for conditional bilevel optimization. Our methods have a dependence over example numbers (max(m, n))q, q is a value decided by without-replacement sampling strategy and can have value in [0, 1] (A herding-based permutation [33] can let q = 0).", "description": "This table compares the computational complexity of various bilevel optimization algorithms in terms of gradient evaluations, Jacobian-vector products, and Hessian-vector products needed to find an e-stationary point. It shows the complexity for both standard and conditional bilevel optimization problems, highlighting the impact of different sampling strategies (independent vs. without-replacement). The table includes existing algorithms and the authors' proposed methods (WiOR-BO and WiOR-CBO), demonstrating the improved convergence rate of the new algorithms.", "section": "1 Introduction"}, {"figure_path": "BNnZwbZGpm/tables/tables_29_1.jpg", "caption": "Table 1: Comparisons of the Bilevel Opt. & Conditional Bilevel Opt. algorithms for finding an e-stationary point. We include comparison with stochastic gradient descent type of methods and a more comprehensive comparison including other acceleration methods can be found in Table 2 of Appendix. An e-stationary point is defined as ||\u2207h(x)|| \u2264 \u20ac. Gc(f, e) and Gc(g, e) denote the number of gradient evaluations w.r.t. f(x, y) and g(x, y); JV (g, e) denotes the number of Jacobian-vector products; HV(g, e) is the number of Hessian-vector products. m and n are the number of data examples for the outer and inner problems, in particular, n is the maximum number of inner problems for conditional bilevel optimization. Our methods have a dependence over example numbers (max(m, n))q, q is a value decided by without-replacement sampling strategy and can have value in [0, 1] (A herding-based permutation [33] can let q = 0).", "description": "This table compares the convergence rates of different bilevel optimization algorithms in terms of gradient evaluations, Jacobian-vector products, and Hessian-vector products.  It shows the computational complexity for finding an epsilon-stationary point for both standard and conditional bilevel optimization problems. The algorithms are categorized by their sampling strategy (independent vs. without-replacement). The table highlights the superior convergence rate of the proposed without-replacement sampling algorithms.", "section": "1 Introduction"}]