[{"figure_path": "dqT9MC5NQl/tables/tables_7_1.jpg", "caption": "Table 1: Average test log-likelihoods (\u2191) for the synthetic 1-D GP and 2-D smoke experiments. For the 1-D dataset we used the regular TNP, while for the 2-D experiment we used the PT-TNP. Results are grouped together by model class. Best in-class result is bolded.", "description": "This table presents the average test log-likelihoods achieved by various neural process models on two different experiments: a synthetic 1-D Gaussian process regression task and a 2-D smoke plume simulation.  The table is organized by model type (TNP, ConvCNP, RelaxedConvCNP, EquivCNP), showing performance for both in-distribution and out-of-distribution settings (where applicable). The best performing model within each model class is highlighted in bold. Note that different Neural Process variants were used for the 1D and 2D experiments.", "section": "5 Experiments"}, {"figure_path": "dqT9MC5NQl/tables/tables_9_1.jpg", "caption": "Table 2: Average test log-likelihoods (\u2191) for the 2-D and 4-D environmental regression experiment. Results are grouped together by model class. Best in-class result is bolded.", "description": "This table presents the results of the 2D and 4D environmental regression experiments.  It compares the average test log-likelihoods achieved by various neural process models (non-equivariant, equivariant, and approximately equivariant) on two different geographical regions (Europe and the US).  The models are grouped by their underlying architecture, and the best performing model within each group is highlighted in bold. The table allows for comparison of model performance across different levels of equivariance (strict, approximate, and none) and dimensionality of the input data (2D vs 4D).", "section": "5 Experiments"}, {"figure_path": "dqT9MC5NQl/tables/tables_23_1.jpg", "caption": "Table 3: Average test log-likelihoods (\u2191) for the synthetic 1-D GP experiment when tested on context sets. Ground truth log-likelihood is 0.2806 \u00b1 0.0005.", "description": "This table presents the average test log-likelihoods achieved by different neural process models on the synthetic 1-D Gaussian Process dataset when only tested on the context sets (i.e., without the target sets).  The models include the non-equivariant TNP, equivariant versions of the TNP, ConvCNP, and EquivCNP, and approximately equivariant counterparts of the TNP and ConvCNP models. The ground truth log-likelihood is provided for comparison. This allows to measure the model's ability to accurately reconstruct the context set, which is a key component of the neural process.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/tables/tables_23_2.jpg", "caption": "Table 4: Equivariance deviation (\u0394equiv) of the approximately equivariant models in the 1-D synthetic GP experiment.", "description": "This table presents the equivariance deviation (\u0394equiv) for each of the approximately equivariant models evaluated in the 1D synthetic GP experiment.  The equivariance deviation is calculated as the L1 norm of the difference between the predictive mean of the approximately equivariant model and the predictive mean of the strictly equivariant model (obtained by zeroing out the fixed inputs). This metric quantifies how much the approximately equivariant model deviates from strict equivariance. Lower values indicate a closer adherence to strict equivariance.", "section": "E.1 Synthetic 1-D Regression"}, {"figure_path": "dqT9MC5NQl/tables/tables_24_1.jpg", "caption": "Table 5: Average test log-likelihoods (\u2191) of the TNP (T) and the ConvCNP (T) models when varying the number of fixed inputs. All standard deviations are 0.004.", "description": "This table presents the average test log-likelihoods achieved by two different models, TNP (T) and ConvCNP (T), under various numbers of fixed inputs (0, 1, 2, 4, 8, 16).  It shows how the model performance changes as the number of fixed inputs increases, illustrating the impact of adjusting the degree of approximation to equivariance.  All standard deviations across the different numbers of fixed inputs are consistently 0.004.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/tables/tables_27_1.jpg", "caption": "Table 6: Average test log-likelihoods (\u2191) for the 2-D environmental regression experiment. p denotes the probability of dropping out the fixed inputs during training.", "description": "This table presents the average test log-likelihoods achieved by two different ConvCNP models in a 2D environmental regression experiment.  The models differ in the probability (p) of dropping out the fixed inputs during training. The results show the log-likelihoods for both Europe and the US regions.  The values show that using a dropout probability of 0.5 improves out-of-distribution generalization to the US.", "section": "5 Experiments"}, {"figure_path": "dqT9MC5NQl/tables/tables_27_2.jpg", "caption": "Table 2: Average test log-likelihoods (\u2191) for the 2-D and 4-D environmental regression experiment. Results are grouped together by model class. Best in-class result is bolded.", "description": "This table presents the results of the 2D and 4D environmental regression experiments.  It compares the average test log-likelihoods achieved by different neural process models (PT-TNP, ConvCNP, RelaxedConvCNP, EquivCNP) under different conditions of equivariance: strictly equivariant, approximately equivariant, and non-equivariant.  The best performing model within each class is highlighted in bold. The data used is ERA5 surface air temperature data for Europe and the central US.", "section": "5 Experiments"}, {"figure_path": "dqT9MC5NQl/tables/tables_28_1.jpg", "caption": "Table 8: Equivariance deviation (\u0394equiv) of the approximately equivariant models in the 2-D environ-mental regression experiment.", "description": "This table presents the equivariance deviation (\u0394equiv) for each of the approximately equivariant models used in the 2-D environmental regression experiment.  The equivariance deviation is a measure of how much the approximately equivariant model deviates from a strictly equivariant model.  A lower value indicates a closer approximation to strict equivariance. The table shows that the approximately equivariant models generally have low equivariance deviation, indicating that they are only slightly deviating from strict equivariance.", "section": "5 Experiments"}, {"figure_path": "dqT9MC5NQl/tables/tables_28_2.jpg", "caption": "Table 9: Average test log-likelihoods (\u2191) of the ConvCNP (T) with different number of additional fixed inputs for the 2-D environmental regression experiment. We show the results when the models are tested on Europe.", "description": "This table shows the average test log-likelihoods for the ConvCNP (T) model in the 2-D environmental regression experiment.  The results are broken down by the number of additional fixed inputs used in the model (0, 1, 2, 4, 8, and 16), with the likelihoods calculated for the Europe region.  The table demonstrates how the model performance changes as the number of fixed inputs increases.", "section": "5 Experiments"}]