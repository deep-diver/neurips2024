[{"figure_path": "dqT9MC5NQl/figures/figures_7_1.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model's predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of different neural process models on a single synthetic 1D regression dataset.  It highlights the performance of standard (non-equivariant), strictly equivariant, and approximately equivariant models.  The approximately equivariant models are shown with and without their additional fixed inputs, illustrating how those inputs affect the predictive distribution and allow for flexibility beyond strict equivariance. The shaded area represents the uncertainty.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_9_1.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model's predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of three different types of neural process models (TNP, ConvCNP, and EquivCNP) on a single synthetic 1D regression dataset.  It highlights how the approximately equivariant models (those incorporating the approach described in the paper) balance strict equivariance with the ability to model non-equivariant aspects of the data. The plot shows both the standard model prediction and a prediction without utilizing the fixed inputs (representing the strictly equivariant case).", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_16_1.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model's predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of three different types of neural process models (TNP, ConvCNP, and EquivCNP) on a single synthetic 1D regression dataset.  It highlights the impact of incorporating approximate equivariance. The blue lines represent the predictive distribution of the approximately equivariant model, while the red lines show the distribution obtained when the additional fixed inputs that break strict equivariance are removed. The dotted black lines show the range of target values.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_17_1.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model's predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of three different types of neural process models (TNP, ConvCNP, and EquivCNP) on a single 1D synthetic regression dataset.  The key takeaway is the performance of approximately equivariant models; they balance the benefits of equivariance (generalization) with the flexibility to account for deviations from strict symmetry in real-world data, achieving better performance than both strictly equivariant and non-equivariant models.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_18_1.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model's predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of various neural process models on a single synthetic 1-D regression dataset.  It highlights the differences between non-equivariant, strictly equivariant, and approximately equivariant models. The approximately equivariant models are shown with and without the additional fixed inputs that break strict equivariance.  The comparison allows visualization of how these modifications affect model predictions and uncertainty estimation.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_22_1.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model\u2019s predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of various neural process (NP) models on a single synthetic 1D regression dataset.  The models shown include standard non-equivariant NPs, strictly equivariant NPs, and approximately equivariant NPs (the latter created using the method presented in the paper).  The figure highlights how the approximately equivariant models balance between the flexibility of non-equivariant models and the efficiency of equivariant models.  The use of red lines to illustrate the results without fixed inputs emphasizes the impact of the method developed for achieving approximate equivariance.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_22_2.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model's predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of several neural process models (TNP, ConvCNP, EquivCNP) on a single synthetic 1D regression dataset.  It highlights the performance of approximately equivariant models by contrasting their predictive distributions with those of their strictly equivariant and non-equivariant counterparts.  The use of fixed inputs in the approximately equivariant models is shown by comparing their full predictive distribution with the distribution obtained when these fixed inputs are zeroed out. The target range is also indicated.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_25_1.jpg", "caption": "Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the model's predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.", "description": "This figure compares the predictive distributions of three different types of neural process models (TNP, ConvCNP, and EquivCNP) on a single synthetic 1D regression dataset.  It highlights how the approximately equivariant models (shown in blue) balance flexibility (departing from strict equivariance) with generalizability (closely matching the predictions of their strictly equivariant counterparts). The red lines in the approximately equivariant model plots show what the predictions would have been if the non-equivariant components of the models were not used.", "section": "5.1 Synthetic 1-D Regression With the Gibbs Kernel"}, {"figure_path": "dqT9MC5NQl/figures/figures_26_1.jpg", "caption": "Figure 6: A comparison between the predictive distributions of the equivariant and approximately equivariant versions of the three classes of models: PT-TNP, ConvCNP, and EquivCNP. From left to right we show: the ground-truth (GT), the non-equivariant PT-TNP, PT-TNP (T), PT-TNP (T), ConvCNP (T), ConvCNP (T), RelaxedConvCNP (T), EquivCNP (E), and EquivCNP (E). The top row shows the mean of the predictions, while the bottom row shows the absolute difference between the predicted mean of each model and the ground-truth.", "description": "This figure compares the predictive means of different models on a smoke plume dataset.  The top row displays the predicted means for several models: ground truth, non-equivariant, equivariant, and approximately equivariant versions of PT-TNP, ConvCNP, and EquivCNP. The bottom row shows the absolute differences between each model's prediction and the ground truth. This allows for a visual comparison of how well each model's predictions match the ground truth, and how approximate equivariance affects performance.", "section": "5.2 Smoke Plumes"}]