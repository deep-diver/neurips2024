[{"figure_path": "zqLAMwVLkt/figures/figures_1_1.jpg", "caption": "Figure 1: Left: An exemplar graph with the edge width indicates the level of affinity connecting two nodes, in which normal nodes (e.g., Uni and Unj) have stronger affinity to its neighboring normal nodes than anomaly nodes (e.g., Va\u2081 and va\u2081) due to homophily relation within the normal class. Our approach GGAD aims to generate outliers (e.g., Voi and vo\u2081) that can well assimilate the anomaly nodes. Right: The outliers generated by methods like AEGIS [7] that ignore their structural relation often mismatch the distribution of abnormal nodes (a), due to their false local affinity (c). By contrast, GGAD incorporates two important priors about anomaly nodes to generate outliers so that they well assimilate the (b) feature representation and (d) local structure of abnormal nodes.", "description": "This figure illustrates the concept of GGAD and compares it with AEGIS.  The left panel shows an example graph where normal nodes have stronger connections to other normal nodes than to anomalous nodes. GGAD aims to generate synthetic \"outlier\" nodes that mimic the characteristics of true anomalies, both structurally (local affinity) and representationally (features). The right panel contrasts GGAD with AEGIS, highlighting how AEGIS-generated outliers poorly match the distribution and local structure of real anomalies, while GGAD's approach produces more realistic synthetic data.", "section": "1 Introduction"}, {"figure_path": "zqLAMwVLkt/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the overall framework of the GGAD model.  First, it initializes outlier nodes using the feature representations of the ego networks of labeled normal nodes. Then, it refines these initial outliers by incorporating two important priors about anomaly nodes: asymmetric local affinity and egocentric closeness. These priors are enforced through loss functions that guide the outlier generation process. Finally, these generated outlier nodes are used as negative samples to train a discriminative one-class classifier on the labeled normal nodes, producing anomaly scores.", "section": "3.2 Overview of the Proposed GGAD Approach"}, {"figure_path": "zqLAMwVLkt/figures/figures_5_1.jpg", "caption": "Figure 3: (a-c) t-SNE visualization of the node representations and (d-f) histograms of local affinity yielded by GGAD and its two variants on a GAD dataset T-Finance [50].", "description": "This figure compares the performance of GGAD and its variants (using only local affinity prior or only egocentric closeness prior) in terms of node representation visualization (t-SNE) and local affinity distribution.  The visualization shows how well the generated outlier nodes (by GGAD and its variants) match the real anomaly nodes in the feature space and in their local graph structure. The histograms show how the local affinity of the generated outliers is different from the real anomalies and normal nodes, highlighting the effectiveness of GGAD in generating more realistic outlier nodes.", "section": "3.4 Incorporating the Egocentric Closeness Prior"}, {"figure_path": "zqLAMwVLkt/figures/figures_7_1.jpg", "caption": "Figure 4: AUPRC results w.r.t the size of training normal nodes (R% of |V|). 'Baseline' denotes the performance of the best unsupervised GAD method per dataset.", "description": "The figure shows the AUPRC (Area Under the Precision-Recall Curve) performance of GGAD and other methods across six datasets as the percentage of training normal nodes (R%) varies.  It demonstrates how the performance changes with different sizes of training data, illustrating GGAD's effectiveness in leveraging labeled normal nodes.", "section": "Experiments"}, {"figure_path": "zqLAMwVLkt/figures/figures_8_1.jpg", "caption": "Figure 5: AUPRC w.r.t. contamination.", "description": "This figure shows the AUPRC (Area Under the Precision-Recall Curve) results for different anomaly contamination rates on two datasets: Amazon and T-Finance.  It demonstrates the robustness of GGAD (Generative Graph Anomaly Detection) compared to other methods (DOMINANT, AnomalyDAE, OCGNN, AEGIS, GAAN, TAM) when dealing with contaminated data, illustrating the impact of having incorrect labels in training data on model performance.  GGAD consistently shows higher AUPRC, maintaining its effectiveness even as more anomalies are introduced in training data.", "section": "Experiments"}, {"figure_path": "zqLAMwVLkt/figures/figures_16_1.jpg", "caption": "Figure 3: (a-c) t-SNE visualization of the node representations and (d-f) histograms of local affinity yielded by GGAD and its two variants on a GAD dataset T-Finance [50].", "description": "This figure visualizes the node representations and local affinity distributions generated by GGAD and its variants (using only local affinity loss, only egocentric closeness loss).  The t-SNE plots (a-c) show how the different methods position normal, outlier, and abnormal nodes in the feature space. The histograms (d-f) show the distribution of local affinity values for each node type.  The comparison highlights the effectiveness of GGAD in generating outliers that closely resemble the characteristics of true anomalies.", "section": "3.4 Incorporating the Egocentric Closeness Prior"}, {"figure_path": "zqLAMwVLkt/figures/figures_16_2.jpg", "caption": "Figure 3: (a-c) t-SNE visualization of the node representations and (d-f) histograms of local affinity yielded by GGAD and its two variants on a GAD dataset T-Finance [50].", "description": "This figure compares the performance of GGAD with two of its variants using t-SNE visualization and histograms of local affinity. The t-SNE visualizations show the distribution of normal, outlier, and abnormal nodes in the feature representation space. The histograms illustrate the distribution of local affinity scores for normal, outlier, and abnormal nodes. By comparing these visualizations and histograms, the figure showcases how GGAD generates outlier nodes that closely resemble the characteristics of abnormal nodes, particularly in terms of both feature representation and local affinity.", "section": "3.4 Incorporating the Egocentric Closeness Prior"}, {"figure_path": "zqLAMwVLkt/figures/figures_17_1.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the GGAD approach.  It begins by initializing outlier nodes based on the feature representations of neighboring nodes to a labeled normal node.  Then, it leverages two priors about anomaly nodes (asymmetric local affinity and egocentric closeness) to refine these outlier nodes, aligning them more closely with real anomalies. Finally, these generated outlier nodes serve as negative samples to train a one-class classifier using labeled normal nodes.", "section": "3.2 Overview of the Proposed GGAD Approach"}, {"figure_path": "zqLAMwVLkt/figures/figures_17_2.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the GGAD approach.  It begins by initializing outlier nodes based on the features of labeled normal nodes' ego networks (a). Two loss functions then refine the outlier nodes: one incorporating asymmetric local affinity (b), and the other employing egocentric closeness (c). The resulting refined outliers (d) serve as negative samples for training a one-class classifier.", "section": "3.2 Overview of the Proposed GGAD Approach"}, {"figure_path": "zqLAMwVLkt/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the GGAD approach for semi-supervised graph anomaly detection.  It shows the process of initializing outlier nodes based on the features of labeled normal nodes' ego networks, then incorporating two anomaly priors (asymmetric local affinity and egocentric closeness) to optimize these outlier nodes. Finally, these optimized outliers are used as negative samples to train a one-class classifier on the normal nodes.", "section": "3.2 Overview of the Proposed GGAD Approach"}, {"figure_path": "zqLAMwVLkt/figures/figures_18_2.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the GGAD (Generative Graph Anomaly Detection) approach.  It starts by initializing outlier nodes using the features of the labeled normal nodes' ego networks.  These initial outliers are then refined using two key priors: asymmetric local affinity (making sure the outliers' local connections are weaker than those of normal nodes) and egocentric closeness (ensuring outliers' feature representations are close to similar normal nodes).  Finally, these optimized outliers serve as negative samples to train a one-class classifier on the normal nodes.", "section": "3.2 Overview of the Proposed GGAD Approach"}, {"figure_path": "zqLAMwVLkt/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the overall process of the GGAD model.  It starts by initializing outlier nodes based on the features of labeled normal nodes' ego networks.  Then, the model leverages two priors on anomaly nodes (asymmetric local affinity and egocentric closeness) to optimize these outlier nodes. Finally, the optimized outlier nodes are used as negative examples to train a one-class classifier.", "section": "3.2 Overview of the Proposed GGAD Approach"}, {"figure_path": "zqLAMwVLkt/figures/figures_19_2.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the GGAD approach. It starts by initializing outlier nodes based on the features of labeled normal nodes' ego networks.  Then, it refines these outlier nodes using two priors: asymmetric local affinity (making outlier nodes less similar to their neighbors than normal nodes) and egocentric closeness (pulling outlier node features towards similar normal nodes). Finally, these refined outlier nodes are used as negative samples to train a one-class classifier on the labeled normal nodes.", "section": "3.2 Overview of the Proposed GGAD Approach"}, {"figure_path": "zqLAMwVLkt/figures/figures_21_1.jpg", "caption": "Figure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.", "description": "This figure illustrates the GGAD (Generative Graph Anomaly Detection) approach.  It starts by initializing outlier nodes using the feature representations of the ego networks (neighbors) of labeled normal nodes. Then, two key priors about anomaly nodes are incorporated: asymmetric local affinity (making sure outliers have weaker local affinity than normal nodes) and egocentric closeness (making sure outliers have similar feature representations to normal nodes with similar local structure). Finally, these optimized outlier nodes serve as negative samples to train a one-class classifier on the labeled normal nodes.", "section": "3.2 Overview of the Proposed GGAD Approach"}]