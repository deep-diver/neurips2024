{"references": [{"fullname_first_author": "David Ha", "paper_title": "Recurrent world models facilitate policy evolution", "publication_date": "2018-00-00", "reason": "This paper introduces the concept of recurrent world models, which is foundational to the current work's model-based approach to reinforcement learning."}, {"fullname_first_author": "Danijar Hafner", "paper_title": "Dream to control: Learning behaviors by latent imagination", "publication_date": "2019-00-00", "reason": "This work introduces the Dream to Control framework, which is directly compared to and improved upon by the current work, serving as a major point of reference and comparison."}, {"fullname_first_author": "Thomas Kipf", "paper_title": "Contrastive learning of structured world models", "publication_date": "2019-00-00", "reason": "The current work draws heavily from this paper's contrastive learning method, extending it with a parsimonious regularization technique to improve the quality of latent dynamics models."}, {"fullname_first_author": "Max Schwarzer", "paper_title": "Data-efficient reinforcement learning with self-predictive representations", "publication_date": "2020-00-00", "reason": "The proposed methodology builds upon self-predictive representations, a key technique used for data-efficient reinforcement learning that is further improved by the current work's parsimonious dynamics approach."}, {"fullname_first_author": "Nicklas Hansen", "paper_title": "Temporal difference learning for model predictive control", "publication_date": "2022-00-00", "reason": "This paper introduces the TD-MPC algorithm, which is one of the model-based RL approaches used in the experimental evaluation of the proposed method, acting as a baseline."}]}