[{"type": "text", "text": "What makes unlearning hard and what to do about it ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kairan Zhao\u2217 Meghdad Kurmanji George-Octavian Barbulescu University of Warwick University of Cambridge University of Warwick ", "page_idx": 0}, {"type": "text", "text": "Eleni Triantafillou\u2020 Peter Triantafillou\u2020 Google DeepMind University of Warwick ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine unlearning is the problem of removing the effect of a subset of training data (the \u201cforget set\u201d) from a trained model e.g. to comply with users\u2019 requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data. With unlearning research still being at its infancy, many fundamental open questions exist: Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms? We present the first investigation into these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Our evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don\u2019t materialize on random forget sets. Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) refining the forget set into homogenized subsets, according to different characteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in deepening our scientific understanding of unlearning and revealing new pathways to improving the state-of-the-art. \u2021 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning models have generated impressive success stories recently by leveraging increasingly large and data-hungry neural networks that are also increasingly expensive to train. This trend has led to reusing previously-trained models for a wide range of tasks more than ever before. However, the heavy reliance of deep models on training data, together with the difficulty of removing data from trained models after-the-fact, has exacerbated concerns on perpetuating harmful or outdated information, violating user privacy and other issues. Specifically, deep networks are highly nonconvex, making it difficult to trace (and thus attempt to remove) the effect of a given subset of training data on the model weights. We are therefore faced with important technical challenges when it comes to building machine learning pipelines that are performant while efficiently supporting deletion requests. Machine unlearning [29] is a growing field that aims to address this important issue. ", "page_idx": 0}, {"type": "text", "text": "While unlearning is receiving increasing attention [34, 33], it is still a young area of research and the factors affecting the success of different approaches remain poorly-understood. Understanding what makes an unlearning problem easy or hard is crucial for several reasons. First, knowledge of behaviours of unlearning algorithms on different types of forgetting requests may inform which unlearning method to choose for a given request. In fact, for some requests it may be that all current methods are inadequate, suggesting that one should pay the cost of retraining from scratch rather than opting for \u201capproximate unlearning\u201d that imperfectly removes information after-the-fact. Further, deepening our understanding of unlearning can illuminate pathways for improving both unlearning algorithms as well as evaluation protocols by focusing on relevant factors that affect difficulty. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To this end, we present the first investigation into different factors that characterize the difficulty of an unlearning problem. We find that the unlearning problem becomes harder i) the more entangled the retain and forget sets are and ii) the more memorized the forget set examples are. Our investigation reveals that different unlearning algorithms suffer disproportionately as the difficulty level increases and surfaces previously-unknown behaviours and failure modes of state-of-the-art unlearning algorithms. Inspired by our findings, we propose a Refined-Unlearning Meta-algorithm (RUM) for improving unlearning pipelines. RUM contains two steps: i) a refinement procedure that divides the given forget set into subsets that are homogeneous with respect to relevant factors that influence algorithms\u2019 behaviours, and ii) a meta-algorithm that dictates how to unlearn each of those subsets and compose the resulting models to arrive at one that has unlearned the entire forget set. Our thorough investigation shows that RUM boosts unlearning performance of several state-of-the-art algorithms and addresses issues that our investigation of unlearning difficulty has uncovered. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Unlearning problem formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\theta^{o}=\\mathcal{A}(D_{t r a i n})$ be the weights obtained by applying a training algorithm $\\boldsymbol{\\mathcal{A}}$ on a training dataset $\\mathcal{D}_{t r a i n}$ . We will refer to $\\theta^{o}$ as the \u201coriginal model\u201d. Further, let $\\mathcal{S}\\subseteq\\mathcal{D}_{t r a i n}$ denote a subset of the training data referred to as the \u201cforget set\u201d. For convenience, we will refer to its complement as the \u201cretain set\u201d $\\mathcal{R}=\\mathcal{D}_{t r a i n}\\:\\backslash\\:\\mathcal{S}$ . Informally, the goal of an unlearning algorithm $\\boldsymbol{\\mathcal{U}}$ is to utilize $\\theta^{o}$ , $\\boldsymbol{S}$ and $\\mathcal{R}$ to produce an unlearned model $\\theta^{u}=\\mathcal{U}(\\theta^{o},S,\\mathcal{R})$ from which the influence of $\\boldsymbol{S}$ is removed. ", "page_idx": 1}, {"type": "text", "text": "This idea has been formalized by considering the distributional similarity between the model $\\theta^{u}$ produced by $\\boldsymbol{\\mathcal{U}}$ and the model $\\theta^{r}$ produced by the optimal unlearning approach: retraining from scratch on an adjusted training dataset that excludes the forget set: $\\theta^{r}=A(D_{t r a i n}\\setminus S)$ . Note that we refer to distributions here since rerunning $\\boldsymbol{\\mathcal{U}}$ and $\\boldsymbol{\\mathcal{A}}$ with different random seeds (that control e.g. the initialization and the order of mini-batches) will lead to slightly different weights. The ideal unlearning algorithm then, according to this viewpoint, is one that yields the same distribution of weights as retraining from scratch. Of course, for an unlearning algorithm to be practical, we would additionally desire it to be significantly more computationally efficient than retraining the model. ", "page_idx": 1}, {"type": "text", "text": "The following definition, borrowed from [33] (and similar to [14]) formalizes this idea in a framework inspired by differential privacy [8]. ", "page_idx": 1}, {"type": "text", "text": "Definition 2.1. Unlearning. An unlearning algorithm $\\boldsymbol{\\mathcal{U}}$ is an $(\\epsilon,\\delta)$ -unlearner (for $\\boldsymbol{\\mathcal{A}}$ , $\\mathcal{D}_{t r a i n}$ and $\\boldsymbol{S}$ ) if the distributions of $\\mathcal{A}(\\mathcal{D}_{t r a i n}\\setminus S)$ and $\\mathcal{U}(\\bar{\\theta}^{o},S,\\mathcal{D}_{t r a i n}\\setminus S)$ are $(\\epsilon,\\delta)$ -close. ", "page_idx": 1}, {"type": "text", "text": "where we say two distributions $\\mu,\\nu$ are $(\\epsilon,\\delta)$ -close if $\\mu(B)\\leq e^{\\epsilon}\\nu(B)+\\delta$ and $\\nu(B)\\leq e^{\\epsilon}\\mu(B)+\\delta$ for all measurable events $B$ . ", "page_idx": 1}, {"type": "text", "text": "According to the above definition, an unlearning algorithm is said to be exact unlearning if it satisfies the above definition for $\\epsilon=\\delta=0$ , i.e., it yields a distribution of models identical to that of retraining from scratch. For neural networks, the only known exact solutions involve retraining, either naively, or in the context of mixtures where one can retrain only a subset of models affected by the deletion request [3]. These approaches unfortunately are inefficient; in the worst-case, even clever schemes suffer inefficiency similar to naive retraining, and may also yield poorer performance. To address this, a plethora of approximate unlearning algorithms have been recently proposed, whose $\\epsilon$ and $\\delta$ values aren\u2019t known in general, but are substantially more efficient and may have higher utility. ", "page_idx": 1}, {"type": "text", "text": "Evaluating approximate unlearning. Since the success of (most) approximate unlearning algorithms cannot be proved within tight $(\\epsilon,\\delta)$ bounds, the community has considered various empirical measurements of success, guided by three desiderata: i) good forgetting quality, 2) high utility, and 3) efficiency. An unlearning algorithm thus is faced with a complex balancing act, as there are well-known trade-offs both between forgetting quality and utility, as well as forgetting quality and efficiency, and a good unlearning metric should capture these nuances. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Utility and efficiency are straightforward to measure, and, in the context of classifiers, can be represented by the accuracy on the retain and test sets, and time in seconds, respectively. Measuring forgetting quality, on the other hand, is more complex and several proxies have been proposed. The simplest one is to inspect the accuracy on the forget set, with the goal of matching the accuracy on the forget set that would have been obtained by retraining from scratch. Alternatively, inspired from privacy literature [4, 28], Membership Inference Attacks (MIAs) have been adopted by the unlearning community [25, 26, 20] to measure forgetting quality. In essence, an MIA is designed to infer from the model\u2019s characteristics (e.g. loss, confidence) whether a data point has been used in training, and then unlearned, versus was never trained on in the first place. Intuitively, the failure of an attacker to tell apart unlearned examples from never-seen examples marks a success for the unlearning algorithm in terms of this metric. We will consider both of these proxies in our experimental investigation. ", "page_idx": 2}, {"type": "text", "text": "To holistically evaluate an unlearning algorithm, we desire a single metric that captures both forgetting quality and utility. We will later introduce a \u201ctug-of-war\u201d metric for this purpose, inspired by [33]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Memorization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Deep neural networks are known to \u201cmemorize\u201d (a subset of) their training data, with a recent theory showing that label memorization is in fact necessary for achieving close-to-optimal generalization error in classifiers [11] when the data distribution is long-tailed. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.2. Memorization score [11]. The memorization score for an example $i\\,\\in\\,{\\mathcal{D}}$ , with respect to a training dataset $\\mathcal{D}$ and training algorithm $\\boldsymbol{\\mathcal{A}}$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{mem}(\\mathcal{A},\\mathcal{D},i)=\\operatorname*{Pr}_{f\\sim\\mathcal{A}(\\mathcal{D})}\\left[f\\big(x_{i}\\big)=y_{i}\\right]\\,-\\,\\operatorname*{Pr}_{f\\sim\\mathcal{A}(\\mathcal{D}\\backslash i)}[f\\big(x_{i}\\big)=y_{i}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{i}$ and $y_{i}$ are the feature and label, respectively, of example $i$ . ", "page_idx": 2}, {"type": "text", "text": "The first term in the above equation considers models trained on all of $\\mathcal{D}$ whereas the second term considers models trained on $\\mathcal{D}$ excluding example $i$ . Intuitively, the memorization score for an example $i$ is high if including it in training yields a different distribution of predictions on that example than excluding it from training would have. Recent works [11, 12, 23] identify atypical examples or outliers of the data distribution as examples that are more highly memorized: if an example has a noisy or incorrect label, the model is required to memorize it in order to predict it correctly. ", "page_idx": 2}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Approximate unlearning algorithms. A plethora of algorithms have been proposed that aim to identify effective data scrubbing procedures post-training. We now describe representative methods. ", "page_idx": 2}, {"type": "text", "text": "Fine-tune [36, 15] relies on catastrophic forgetting to diminish the confidence of the original model $\\theta^{o}$ on $\\boldsymbol{S}$ . Catastrophic forgetting is induced by simply fine-tuning on the retain set $\\bar{D_{t r a i n}}\\setminus S$ . On the other hand, NegGrad [15, 17, 31] instead directly maximizes the loss on $\\boldsymbol{S}$ . This approach has been found empirically to cause a large drop in the utility of the model. To address this, $\\mathbf{NegGrad+}$ [25] combines fine-tuning and gradient ascent, by jointly minimizing the loss function on the retain set, and maximizing the loss function with respect to the forget set. SCRUB, proposed by the same authors as $\\mathrm{NegGrad+}$ , extends the contrastive learning behind NegGrad+ by framing it as a student-teacher problem. Concretely, SCRUB is a bi-optimization algorithm, where the student aims to mimic the teacher\u2019s behaviour on $\\mathcal{R}$ and to disobey the teacher\u2019s output with respect to $\\boldsymbol{S}$ . L1-sparse [26] infuses weight \u201csparsity\u201d into the unlearning algorithm by fine-tuning on the retain set with an L1-penalty, drawing inspiration from the model pruning literature [13, 27]. Influence Unlearning [21, 24] arrives at the important model\u2019s weights by estimating how removing a data point affects $\\theta^{o}$ via influence functions [7], and draws connections to $(\\epsilon,\\delta)$ -forgetting [35, 19]. ", "page_idx": 2}, {"type": "text", "text": "A different line of work is \u201crelabelling-based\u201d methods that trick the model to learn new labels for $S$ . This can be achieved by finetuning the model with respect to a dataset ${\\cal D}_{r e l a b e l}=(X_{S},Y)$ , where $X_{S}$ are the features and labels $Y$ are sampled from a prior distribution of the label space. Saliency Unlearning (SalUn) [10] learns $\\mathcal{D}_{r e l a b e l}$ by optimising only the salient parameters of the model. ", "page_idx": 2}, {"type": "text", "text": "Concretely, the authors argue that the model\u2019s weights $\\theta^{o}$ can be decomposed into salient weights and \u201cintact\u201d model weights, by investigating the weight space with respect to the forget set $S$ ala [30, 1]. ", "page_idx": 3}, {"type": "text", "text": "Difficulty of Unlearning. The closest research to ours is the contemporaneous work of [9], where the authors study adversarial unlearning cases, i.e. \u201cworst-case\u201d forget sets. [9] arrives at difficult forget sets by solving a bi-level optimization based on fine-tuning (i.e. catastrophic forgetting). Instead, we arrive at difficult partitions through the lens of interpretable factors: the degree of entanglement between the retain and forget set and memorization. While the primary aim of [9] is to construct more pessimistic evaluation benchmarks, our primary aim is to deepen our understanding of unlearning problems and of the behaviour of state-of-the-art algorithms when operating on forget set of different identified characteristics, ultimately improving unlearning pipelines. ", "page_idx": 3}, {"type": "text", "text": "Catastrophic forgetting, atypical examples and privacy. [22] and [32] study catastrophic forgetting during training. [22] finds that, when training on large datasets, examples that were only seen early in training may enjoy better privacy, in terms of MIAs and extraction attacks, compared to examples seen recently. [32] investigate \u201cforgetting events\u201d, where an example that was previously correctly predicted becomes incorrectly predicted later in training. They find that examples with noisy labels witness a larger number of these forgetting events. Further, [5] find that models trained with Differential Privacy (DP) find it primarily hard to correctly predict atypical examples. We build on this literature by studying the difficulty of unlearning after-the-fact, rather than (passive) forgetting during training and draw connections to memorization, a notion closely related to atypicality in the data distribution. ", "page_idx": 3}, {"type": "text", "text": "4 What Makes Unlearning Hard? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we identify and empirically examine two factors that affect the difficulty of unlearning. Before diving in, we first define a simple proxy for unlearning difficulty that we will use in this section. Our goal is to capture the difficulty of performing the \u201cbalancing act\u201d of forgetting $\\boldsymbol{S}$ while retaining the ability to perform well on $\\mathcal{R}$ and generalize well to the test set. We propose a metric to capture this \u201ctug-of-war\u201d (ToW) using the relative difference between the accuracies of the unlearned and the retrained model on the forget, retain and test sets, in a manner inspired by [33]. ", "page_idx": 3}, {"type": "text", "text": "$\\mathrm{ToW}(\\theta^{u},\\theta^{r},\\mathcal{S},\\mathcal{R},\\mathcal{D}_{t e s t})=(1-\\mathrm{da}(\\theta^{u},\\theta^{r},\\mathcal{S}))\\cdot(1-\\mathrm{da}(\\theta^{u},\\theta^{r},\\mathcal{R}))\\cdot(1-\\mathrm{da}(\\theta^{u},\\theta^{r},\\mathcal{D}_{t e s t}))$ where $\\begin{array}{r}{\\mathtt{a}(\\theta,\\mathcal{D})=\\frac{1}{|\\mathcal{D}|}{\\sum_{(x,y)\\in\\mathcal{D}}}[f(x;\\theta)=y]}\\end{array}$ is the accuracy on $\\mathcal{D}$ of a model $f$ parameterized by $\\theta$ and $\\mathtt{d a}(\\theta^{u},\\theta^{r},\\mathcal{D})\\,=\\,|{\\mathtt{a}}(\\theta^{u},\\mathcal{D})\\,-\\,{\\mathtt{a}}(\\theta^{r},\\mathcal{D})|$ is the absolute difference between the accuracy of models $\\theta^{u}$ and $\\theta^{r}$ on $\\mathcal{D}$ . Therefore, ToW rewards unlearned models that match the accuracy of the retrained-from-scratch model, on each of the forget, retain, and test sets. ToW ranges from 0 to 1, with higher values associated with better unlearning. ", "page_idx": 3}, {"type": "text", "text": "4.1 The more entangled the forget and retain sets are, the harder unlearning becomes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Prior research (e.g., by Feldman et al [11] and Carlini et al [5]) has tried to identify prototypical (or atypical) examples and their impact on learning. Primarily, this depended on the position of examples within the overall data space distribution. In contrast, here we focus on the embedding space, as unlearning depends heavily on how the model has learned to represent training data. Furthermore, instead of looking at isolated examples, we delve into how \u201centangled\u201d the retain and forget sets are in embedding space. We hypothesize that higher \u201centanglement\u201d leads to harder unlearning: if the two sets are highly entangled, attempting to erase $\\boldsymbol{S}$ will cause accidentally erasing $\\mathcal{R}$ too. ", "page_idx": 3}, {"type": "text", "text": "We propose to measure entanglement between the retain and forget sets via the below Entanglement Score (ES), inspired by a measure previously introduced in [16] to study learned representations. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{ES}(\\mathcal{R},S;\\theta^{o})=\\frac{\\frac{1}{|\\mathcal{R}|}\\sum_{i\\in\\mathcal{R}}(\\phi_{i}-\\mu_{\\mathcal{R}})^{2}+\\frac{1}{|S|}\\sum_{j\\in\\mathcal{S}}(\\phi_{j}-\\mu_{S})^{2}}{\\frac{1}{2}\\big((\\mu_{\\mathcal{R}}-\\mu)^{2}+(\\mu_{\\mathcal{S}}-\\mu)^{2}\\big)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi_{i}=g(x_{i};\\theta^{o})$ is the embedding of example $x_{i}$ according to the \u201coriginal model\u201d $f$ , parameterized by $\\theta^{o}$ ; where $g$ denotes the forward pass through $f$ up till the penultimate layer, i.e. excluding the ", "page_idx": 3}, {"type": "image", "img_path": "QAbhLBF72K/tmp/268c0a618476947dddc2211180d675cad0efd37eb6ab74dfa85c6c0300305db0.jpg", "img_caption": ["(a) Entanglement Score (ES) vs ToW ", "(b) Memorization vs ToW "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Uncovering two factors that affect unlearning difficulty according to ToW (where higher is better). Left: the more entangled the retain and forget sets are in the embedding space, the harder it is to unlearn. Right: the less memorized a forget set is (thus having influenced the model less), the easier it is to unlearn (for most algorithms). Error bars correspond to $95\\%$ confidence intervals from running each algorithm 3 times (6 times for relabelling-based that had higher variance). ", "page_idx": 4}, {"type": "text", "text": "classifier layer. Further, $\\begin{array}{r}{\\mu_{\\mathcal{R}}=\\frac{1}{|\\mathcal{R}|}\\sum_{i\\in\\mathcal{R}}\\phi_{i}}\\end{array}$ is the mean embedding of the retain set, and analogously, $\\mu_{S}$ the mean embedding of the forget set, while $\\mu$ is the mean embedding over all of $\\mathcal{D}_{t r a i n}=\\mathcal{R}\\cup\\mathcal{S}$ . Intuitively, ES measures entanglement between $\\boldsymbol{S}$ and $\\mathcal{R}$ in the embedding space of the original model (before unlearning begins). The numerator measures intra-class variance, capturing the tightness of each of those two sets, independently, while the denominator measures inter-class variance between those two sets. Higher ES score corresponds to higher entanglement in the embedding space. ", "page_idx": 4}, {"type": "text", "text": "Our investigation hinges on generating three different forget/retain partitions with different degrees of entanglement: low, medium, and high. But, Equation (2) does not directly suggest a procedure for generating retain/forget partitions with a desired ES score. Hence, we achieved this indirectly using a proxy. Specifically, let $d(i,\\mu;\\theta^{o})=||\\phi_{i}-\\mu||^{2}$ denote the l2-distance in the original model\u2019s embedding space between example $i$ and centroid $\\mu$ , as defined above. We compute this distance for each example in $\\mathcal{D}_{t r a i n}$ and sort those examples according to their $d\\!\\cdot$ -values. We then form each forget set to contain a contiguous subset of examples from different ranges of that sorted list. We find that this procedure allows us to construct retain/forget partitions of varying ES values. The ES values for our low, medium and high partitions are $309.94{\\scriptstyle\\pm98.56}$ , 1076.99\u00b178.64, 1612.21\u00b1110.82 for CIFAR-10, and 963.82\u00b1113.53, $2831.24{\\scriptstyle\\pm558.63}$ , and $3876.90{\\pm}426.92$ for CIFAR-100. We include details of this procedure in the Section A.3, along with visualizations and Maximum Mean Discrepancy (MMD) analysis to further validate ES, confirming that the degree of retain/forget entanglement aligns with the computed scores. We experiment with four dataset/architecture settings: CIFAR-10/ResNet-18, CIFAR-100/ResNet-50, Tiny-ImageNet/ResNet-18 and Tiny-ImageNet/VGG16, using $\\vert{\\cal S}\\vert=3000$ . Refer to Section A.2 for implementation details and Section A.8 for more detailed results on all datasets. ", "page_idx": 4}, {"type": "text", "text": "We observe from Figure 1a that it is harder to unlearn when the retain and forget sets are more entangled: all unlearning algorithms have poorer performance for highly-entangled vs lower-entangled settings. Further, we notice that different unlearning algorithms suffer disproportionately as the entanglement increases. Notably, methods based on relabelling (SalUn and Random-label) perform very poorly when the entanglement is high. We hypothesize that this is because, if two examples $i$ and $j$ are close neighbours in embedding space, with $i$ in the forget set and $j$ in the retain set, forcing example $i$ to be confidently predicted as an incorrect class (as relabelling algorithms do) will also cause $j$ to be predicted as that incorrect class, too, thus causing a drop in retain accuracy, which is captured by ToW. This effect will be less pronounced if $i$ and $j$ are far from each other. ", "page_idx": 4}, {"type": "text", "text": "4.2 The more memorized the forget examples are, the harder unlearning becomes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Feldman et al [11] have already established that models must memorize some atypical examples in order to perform well. Further, prior literature has also established that noisy examples (that are more likely to be memorized) witness more \u201cforgetting events\u201d during training (their predicted label flips to an incorrect one) [32] and that models trained with Differential Privacy (DP), a procedure where noise is added to the gradients (making it harder to memorize), find it primarily hard to correctly predict atypical examples [5]. In this section, we build upon these prior insights by investigating the connection between the degree of memorization of the forget set and difficulty of unlearning. ", "page_idx": 4}, {"type": "text", "text": "Let\u2019s begin by inspecting Definition 2.2: if an example is not really memorized, the predictions of the model on that example will not change much whether the example was included in training or not. This implies that even the original model (no unlearning) is similar to retrain-from-scratch in terms of predictions on those examples, making unlearning unnecessary or trivial. On the other hand, for highly-memorized examples, the predictions between the original and retrained models will differ significantly, implying that an unlearning algorithm has \u201cmore work\u201d to do to turn the original model into one that resembles the retrained one. We now investigate how the level of memorization of the forget set affects the behaviour of state-of-the-art unlearning algorithms. We hypothesize, based on our above intuition, that unlearning is easier when the forget set contains less-memorized examples. ", "page_idx": 5}, {"type": "text", "text": "To investigate this, we first compute the memorization score $\\mathsf{m e m}(\\mathcal{A},\\mathcal{D}_{t r a i n},i)$ of each example $i\\,\\in\\,{\\mathcal{D}}_{t r a i n}$ and we sort all examples according to their scores. We then use that sorted list to create three different forget sets, corresponding to the lowest $N$ scores (\u201clow-mem\u201d), the highest $N$ (\u201chigh-mem\u201d), and the $N$ that are nearest to 0.5, i.e. the midpoint of the range of memorization scores (\u201cmedium-mem\u201d), where $N=3000$ . We then apply different unlearning algorithms on each of these forget sets and compute ToW. We perform this experiment on CIFAR-10 using ResNet-18 and on CIFAR-100 using ResNet-50. Refer to Section A.2 for implementation details. ", "page_idx": 5}, {"type": "text", "text": "We first emphasize two key sets of conclusions. First, in terms of ToW, Figure 1b shows that, indeed, for most algorithms, the lower the memorization level of the forget set, the easier the problem is. In line with our prior discussion, even the original model performs well on \u201clow-mem\u201d, but performs very poorly on \u201chigh-mem\u201d. Interestingly though, the two relabelling-based algorithms (SalUn and Random-label) follow an inverse trend: they perform better for higher-memorized forget sets. Second, breaking down ToW into its parts, we find interesting trends in terms of the forget set accuracy, in Figure 12. Specifically, for several unlearning algorithms, the forget accuracy for \u201clow-mem\u201d is still very high after unlearning them, as the model can infer the correct labels for such examples even when they weren\u2019t included in training; this follows directly by Definition 2.2 if unlearning is done by retraining, and is shown here for the first time for approximate unlearning algorithms. On the other hand, we find that, for \u201chigh-mem\u201d, different unlearning algorithms can (to varying degrees) cause the forget set accuracy to drop substantially; this is consistent with both [32] and [5], but shown here for the first time for approximate unlearning algorithms. Notably, we find that relabelling-based algorithms cause a larger drop in the accuracy of the forget set, relative to other approaches. This benefits ToW in the case of \u201chigh-mem\u201d forget sets, where retraining has poor accuracy on this set (so they get rewarded by matching it), but it hurts on \u201clow-mem\u201d, since it causes a large discrepancy from retraining, which has high accuracy on this set (since it makes similar predictions to the original model on this set, by definition, and the original model has high accuracy on all of $\\mathcal{D}_{t r a i n.}$ ). ", "page_idx": 5}, {"type": "text", "text": "Overall, we have presented the first investigation into the behaviour of unlearning algorithms applied on forget sets of different degrees of memorization. A key finding is that different algorithms outshine others for different forget sets. Most notable is the failure of relabelling-based algorithms on the \u201clow-mem\u201d forget set, which is easy for other algorithms and, in fact, even no unlearning in that case might be an acceptable solution. We intuit that this is due to their aggressive unlearning strategy yielding \u201coverforgetting\u201d (producing a forget set accuracy that is lower than that of retraining from scratch) as discussed above. Furthermore, we observe that different unlearning algorithms work best for the \u201clow-mem\u201d, \u201cmedium-mem\u201d and \u201chigh-mem\u201d forget sets. Concretely, from Figure 1b we note that Finetune is best for \u201cmedium-mem\u201d, SalUn is best for \u201chigh-mem\u201d, and a number of algorithms are top-performers for \u201clow-mem\u201d (including no unlearning). This reveals a possible pathway for improving unlearning based on using different algorithms for different forget sets. So, how can one build on these insights to further improve unlearning algorithms performance? ", "page_idx": 5}, {"type": "text", "text": "5 Refined-Unlearning Meta-algorithm (RUM) for Improved Unlearning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Previously, we observed that unlearning algorithms have different behaviours on forget sets with different properties. For example, while \u201clow mem\u201d forget sets are almost trivial to unlearn (and even doing nothing may be acceptable), SalUn and Random-label perform poorly on them. On the other hand, SalUn and Random-label evidently outperform other unlearning algorithms on \u201chigh mem\u201d. These observations suggest that the optimal unlearning algorithm to use is dependent on the properties of the forget set. One could therefore pick the best unlearning algorithm for each unlearning request, based on these factors. However, in practical scenarios, forget sets may be distributed differently than in our preliminary experiments, that were designed to cleanly separate different factors of interest. ", "page_idx": 5}, {"type": "text", "text": "Indeed, real-world forget sets would likely contain a mixture of examples from different modes of the data distribution, some rare or highly-memorized while others common and not memorized at all. So, what can be done about these expected heterogeneous forget sets? How can our insights above be leveraged to improve unlearning for such cases? ", "page_idx": 6}, {"type": "text", "text": "To address this, we first propose a refinement procedure that divides forget sets into homogeneous subsets (with respect to the factors that we have found to affect the difficulty of unlearning and behaviours of existing algorithms). Second, we propose to utilize a pool of state-of-the-art algorithms to unlearn different subsets. Put together, we propose a Refined-Unlearning Metaalgorithm (RUM), comprised of two steps: 1) Refinement and 2) Meta-unlearning. Figure 2 overviews RUM. ", "page_idx": 6}, {"type": "image", "img_path": "QAbhLBF72K/tmp/fda074d3a058621de2796079765c91b76c626d0f4dbcce72801f4ca358488506.jpg", "img_caption": ["Figure 2: Overview of RUM. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Step 1: Refinement. We introduce a function $\\mathcal{F}$ that partitions the forget set $S$ into $K$ subsets: $\\{S_{i}\\}_{i=1}^{K^{\\bullet}}=\\mathcal{F}(S)$ such that each forget set example appears in exactly one such subset. The ", "page_idx": 6}, {"type": "text", "text": "intention of $\\mathcal{F}$ is to generate homogeneous subsets w.r.t factor(s) that affect difficulty / algorithm\u2019s behaviours. ", "page_idx": 6}, {"type": "text", "text": "Step 2: Meta-Unlearning. Having obtained the subsets $\\{S_{i}\\}_{i=1}^{K}$ of $\\boldsymbol{S}$ , we now require a \u201cmetaalgorithm\u201d $\\mathcal{M}$ that dictates how to perform the individual unlearning requests and how to compose the resulting unlearned models to arrive to a model that has unlearned all of $\\boldsymbol{S}$ . In this work, we focus on meta-algorithms that tackle unlearning of subsets in a sequence, leaving other designs for future work. It therefore remains for our \u201cmeta-algorithm\u201d to decide: i) what unlearning algorithm to apply for each subset, ii) what order should the unlearning requests be executed in. ", "page_idx": 6}, {"type": "text", "text": "More concretely, we assume access to a pool of existing unlearning algorithms $\\mathcal{U}_{1}\\dots\\mathcal{U}_{N}$ , like the ones described in related work, for instance. Let $\\mathcal{M}^{\\mathcal{U}}(\\bar{S}_{i})$ denote a procedure that takes as input a subset $S_{i}$ and returns an unlearning algorithm $\\mathcal{U}\\in\\{\\mathcal{U}_{1}\\,.\\,.\\,.\\,\\mathcal{U}_{N}\\}$ that will be used for that subset. This selection can be done by leveraging insights such as those in Section 4. Further, let ${\\mathcal{M}}^{O}$ denote a procedure that takes as input the $K$ subsets of $\\boldsymbol{S}$ and returns a sorted list $S^{\\prime}=\\mathcal{M}^{\\mathcal{O}}(\\mathcal{F}(\\mathcal{S}))$ containing the $\\mathbf{K}$ subsets in the desired order of execution. ", "page_idx": 6}, {"type": "text", "text": "Given the above ingredients, RUM proceeds by executing $K$ unlearning requests in a sequence, with step $i$ of that sequence corresponding to unlearning subset $S^{\\prime}[i]$ by applying $\\mathcal{U}_{i}\\big(\\theta^{o},S^{\\prime}[i],\\mathcal{R}_{i}\\big)=\\theta_{i}^{u}$ , where $\\theta_{i}^{u}$ denotes the unlearned model up to step $i$ and $\\mathcal{U}_{i}\\,=\\,\\mathcal{M}^{\\mathcal{U}}(S^{\\prime}[i])$ and $\\mathcal{R}_{i}\\,=\\,\\mathcal{R}\\cup\\{S^{\\prime}[i\\,+$ $1],\\dots S^{\\dot{\\prime}}[K]\\}$ is the retain set for step $i$ , containing $\\mathcal{R}$ as well as all other subsets of $\\boldsymbol{S}$ that have not yet been unlearned in the sequence so far. We finally return the unlearned model of the last step $\\theta_{K}^{u}$ . ", "page_idx": 6}, {"type": "text", "text": "Our RUM framework is meant as an analysis framework, surfacing new problems to be solved and offering new pathways into future state-of-the-art algorithms. Nonetheless, we contribute below specific top-performing RUM instantiations, with specific choices for $\\mathcal{F}$ and for $\\mathcal{M}$ . ", "page_idx": 6}, {"type": "text", "text": "6 Experimenting with RUM flavours ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present RUM instantiations using a refinement strategy based on memorization scores and experimental evaluations answering the following questions: Q1: How useful is refinement alone? That is, for a given unlearning algorithm $\\boldsymbol{\\mathcal{U}}$ , does applying $\\boldsymbol{\\mathcal{U}}$ sequentially on the $K$ homogeneous subsets of $\\boldsymbol{S}$ outperform applying $\\boldsymbol{\\mathcal{U}}$ once on all of $S^{\\prime}$ ? Q2: Can we obtain further gains by additionally selecting the best-performing unlearning algorithm for each forget set subset? Q3: Are there interpetable factors behind the boost obtained by sequential unlearning of homogeneous subsets? ", "page_idx": 6}, {"type": "text", "text": "Experimental setup We experiment with a refinement strategy based on memorization scores where $K=3$ . Specifically, we study unlearning a forget set $\\boldsymbol{S}$ that is the union of the three sets containing the $N$ lowest, the $N$ closest to 0.5, and the $N$ highest memorized examples in the dataset, ", "page_idx": 6}, {"type": "image", "img_path": "QAbhLBF72K/tmp/59015432d7e3b7d5e7eecb7ec253596e6a63a81550ea0e1a50a811362b06961c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: From subplots a and c, we observe that $\\mathbf{RUM}^{\\mathcal{F}}$ improves each unlearning algorithm. Vanilla corresponds to unlearning $\\boldsymbol{S}$ in one go, whereas Shuffle and $\\mathbf{RUM}^{\\mathcal{F}}$ operate sequentially on 3 subsets of $\\boldsymbol{S}$ . In the case of $\\mathbf{R}\\mathbf{U}\\mathbf{\\bar{M}}^{\\mathcal{F}}$ , the 3 subsets are the result of applying $\\mathcal{F}$ and the order is low $\\rightarrow$ medium $\\rightarrow$ high, whereas Shuffle uses equal-sized random subsets, serving as a control experiment. Further, from subplot $\\mathbf{b}$ , we observe that full RUM, equipped with the best algorithm for each subset (do nothing $\\rightarrow$ Fine-tune $\\rightarrow\\mathrm{{SalUn})}$ ), yields the overall best results (note: in CIFAR-100, NegGrad+ is the best algorithm, so full RUM corresponds to the $\\mathbf{RUM}^{\\mathcal{F}}$ variant of $\\mathrm{NegGrad+}$ ). ", "page_idx": 7}, {"type": "text", "text": "where $N=1000$ , making the overall size of the forget set 3000. We conduct experiments on two different datasets: CIFAR-10 and CIFAR-100, using ResNet-18 and ResNet-50, respectively. We evaluate unlearning algorithms both in terms of ToW, as before, and using a commonly-used MIA [10, 26] that is a binary classifier trained to separate $\\mathcal{R}$ from $\\mathcal{D}_{t e s t}$ and then queried on examples from $\\boldsymbol{S}$ . Following previous work, we report as \u201cMIA\u201d the fraction of examples from $\\boldsymbol{S}$ that were predicted to be held-out. The goal is to match the \u201cMIA\u201d score of retrain-from-scratch. For convenience, we report the \u201cMIA gap\u201d: the absolute difference of the MIA score of unlearning from the MIA score of retrain-from-scratch (lower is better). Further details on MIA setup are in Section A.4. We also introduce a new metric\u201cToW-MIA\u201d, which mirrors ToW but replaces its \"forget quality\" component (i.e., its accuracy on $\\boldsymbol{S}$ ) with the MIA score. Results for ToW-MIA are presented in Section A.4 and A.8 and largely follow the same trends. To gain a finer understanding of the differences between the unlearned and retrained models, we additionally analyze the number of examples where their predictions disagree (see Section A.7). We also consider and analyze the effect of two different orderings: i) low $\\rightarrow$ medium $\\rightarrow$ high and i) high $\\rightarrow$ medium $\\rightarrow$ low. ", "page_idx": 7}, {"type": "text", "text": "How useful is refinement alone? To investigate this, we selected a subset of highest-performing algorithms and apply each algorithm $\\boldsymbol{\\mathcal{U}}$ in three ways: i) \u201cvanilla\u201d, i.e. applying $\\mathcal{U}(\\theta^{o},\\mathcal{S},\\mathcal{R})$ as usual, ii) \u201cshuffle\u201d where $\\boldsymbol{\\mathcal{U}}$ is applied sequentially on three equal-sized subsets of $\\boldsymbol{S}$ that were determined randomly, and iii) $\\mathbf{RUM}^{\\mathcal{F}}$ , where $\\boldsymbol{\\mathcal{U}}$ is applied sequentially on the subsets obtained by $\\mathcal{F}(\\bar{S)}$ , i.e. utilizing only the refinement step of RUM and applying the same $\\boldsymbol{\\mathcal{U}}$ on each subset. We include \u201cshuffle\u201d as a control experiment, so that any gain of $\\mathbf{RUM}^{\\mathcal{F}}$ over shuffle is due to homogenization rather than simply reducing the size of the forget set or other effects of sequential unlearning. We observe from Figure 3 and Tables 16a and 16b that, for four different unlearning algorithms and on two different datasets, $\\mathbf{RUM}^{\\mathcal{F}}$ significantly outperforms \u201cvanilla\u201d and \u201cshuffle\u201d, indicating that operating sequentially on homogenized subsets according to memorization can boost the performance of unlearning algorithms. Interestingly, in most cases, \u201cshuffle\u201d actually performs worse than \u201cvanilla\u201d, indicating that the difficulty of the problem may increase rather than decrease given a poor refinement strategy. ", "page_idx": 7}, {"type": "table", "img_path": "QAbhLBF72K/tmp/f81ffb9393edd5dd752f3ac949cfaf954a887d6fef9bf0a03a5f3757b3bccc00.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Each algorithm $\\boldsymbol{\\mathcal{U}}$ is applied in three ways: i) in one-go (\u201cvanilla\u201d), ii) on a random partition of $\\boldsymbol{S}$ into 3 equal-sized subsets, sequentially (\u201cshuffle\u201d), and iii) on three equalsized subsets obtained by $\\mathcal{F}$ in $\\mathrm{low}\\to\\mathrm{med}\\to$ high order $({}^{\\bullet}\\mathbf{RUM}^{\\mathcal{F}^{\\bullet}})$ . In last row, RUM additionally chooses the best algorithm for each subset: none $\\rightarrow$ Fine-tune $\\rightarrow\\mathrm{{SalUn}}$ in CIFAR10 and $\\mathsf{N e g G r a d}+\\mathsf{R U M}^{\\mathcal{F}}$ in CIFAR-100. ", "page_idx": 7}, {"type": "text", "text": "Can we further boost performance by per-subset algorithm selection? To answer this, we leverage our findings from Section 4 to identify the best unlearning algorithm for each of the \u201clow", "page_idx": 7}, {"type": "image", "img_path": "QAbhLBF72K/tmp/c8d4d895b6c8cc2ed796d76ec26a32cb44a9dbb7c89ad4d7a2d63c24839dfeb1.jpg", "img_caption": ["(a) C-proxy vs ToW "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "QAbhLBF72K/tmp/80bd11272842972c929187e1f93635dbe4c7b9c3035d66b55ff81ccd4194b7be.jpg", "img_caption": ["(b) $\\mathbf{RUM}^{\\mathcal{F}}$ on CIFAR-10 using C-proxy "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Replacing mem scores with the efficient C-proxy yields similar trends and performance gains, carving a path for practical deployment of RUM. Left: forget sets with lower C-proxy values (i.e., higher mem scores, since C-proxy and memorization are negatively correlated) are harder to unlearn, consistent with the trend in Figure 1b. Right: $\\mathbf{RUM}^{\\mathcal{F}}$ using C-proxy in the refinement step enhances unlearning performance across algorithms, comparable to using the memorization score in Figure 3a. Error bars correspond to $95\\%$ confidence intervals, with each algorithm run 3 times. ", "page_idx": 8}, {"type": "text", "text": "mem\u201d, \u201cmedium-mem\u201d and \u201chigh-mem\u201d scenarios. On CIFAR-10, this corresponds to doing nothing for low-mem (i.e. using the original model directly), using Fine-tune for medium-mem and SalUn for high-mem (based on Figure 1b). From Figure 3b (and Table 16a), we observe that we get the overall best results by far, both in terms of ToW and MIA, by applying RUM with \u201cnothing $\\rightarrow$ Fine-tune $\\rightarrow$ SalUn\u201d, demonstrating the value of incorporating our insights into unlearning pipelines. In fact, lets revisit our previous observation that SalUn and Random-Label perform uncharacteristically poorly on the \u201clow-mem\u201d forget set. In line with our hypothesis, we notice that \u201cnothing $\\rightarrow\\mathrm{SalUn}\\rightarrow\\mathrm{SalUn}^{\\ast}$ outperforms applying SalUn on all three subsets (a.k.a. SalUn $\\mathbf{RUM}^{\\mathcal{F}}$ ). Note that, for CIFAR-100, the best algorithm for all subsets is NegGrad+, so full RUM corresponds to $\\mathbf{NegGrad+RUM}^{\\mathcal{F}}$ . ", "page_idx": 8}, {"type": "text", "text": "Can we obtain similar performance boosts with compute-efficient proxies? Given the computational cost of calculating memorization scores, we aim to find a more efficient proxy to enable practical deployment of RUM. We discuss a confidence-based memorization metric, termed \u201cCproxy\u201d, as a proxy for memorization [23, 32]. Section A.6 provides a detailed explanation of the proxy and the complete results across various datasets and architectures. Figure 4a shows that the observed unlearning difficulty pattern\u2014specifically, that forget examples with higher memorization scores (i.e., lower C-proxy values, as they are negatively correlated; see Table 4) are harder to unlearn\u2014remains consistent when using C-proxy, paralleling the results in Figure 1b. This pattern is further confirmed on Tiny-ImageNet with both ResNet-18 and the VGG-16 architectures, as shown in Figure 9. We then examine whether using C-proxy achieves similar performance gains within RUM as the original memorization score. Figure 4b presents results on CIFAR-10 with ResNet-18, using C-proxy in place of memorization score during the refinement step. This analysis is also extended to Tiny-ImageNet with both ResNet-18 and VGG-16 architectures, as shown in Figure 10 and Table 17. Together, these results suggest that C-proxy is a practical and compute-efficient alternative, delivering significant performance gains comparable to those achieved with the memorization score. Detailed analysis of the use and appropriateness of various memorization proxies and their effect on RUM can be found in [37]. ", "page_idx": 8}, {"type": "text", "text": "Analysis of sequence dynamics We report ToW and MIA with different orderings in Tables 16a and 16b. We find that, while ToW is similar for different orderings, MIA can vary. To better understand these dynamics, we inspect the accuracies on $\\boldsymbol{S}$ and its subsets after each step in Figure 5 for $\\operatorname{SalUn}^{\\mathcal{F}}$ on CIFAR-10 and in Figure 13 for $\\scriptstyle\\mathrm{NegGrad}+{^{F}}$ CIFAR-100. The former reveals why $\\operatorname{SalUn}^{\\mathcal{F}}$ with l $\\mathrm{ow}\\to\\mathrm{med}\\to\\mathrm{high}$ order greatly outperforms vanilla SalUn. Recall that we identified in Section 4.2 that SalUn \u201coverforgets\u201d low-mem examples (its forget accuracy is lower than that of retraining). We observe from Figure 5 that future steps of the sequence neutralize that overforgetting effect on low-mem, leading to better ToW (see Figure 3). Interestingly, in line with our previous insights (Section 4.2), we find (from both Figures 5 and 13) that it is hard to cause the \u201clow-mem\u201d accuracy to become low and stay low. NegGrad $^{+}$ does not drop it for any order of execution; SalUn drops it in the first ordering, but that drop is later reversed. We leave it to future work to further study these sequential dynamics and their helpful or harmful effects on Tow and MIA. The fact that MIA results differ based on the sequence may tie in with the recently-identified \u201cprivacy onion effect\u201d [6]. ", "page_idx": 8}, {"type": "image", "img_path": "QAbhLBF72K/tmp/a3452ab2c5e524c38b9cbdb5ee666e1b4c676da33513304157f66567e1bf1edb.jpg", "img_caption": ["Figure 5: Sequence dynamics for SalUn $\\mathbf{RUM}^{\\mathcal{F}}$ on CIFAR-10. We report the accuracy on overall $\\boldsymbol{S}$ , $\\mathcal{R}$ and $\\mathcal{D}_{t e s t}$ , and subsets of $\\boldsymbol{S}$ after each step. Both orderings yield similar Tow (see Table 16a). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Discussion and conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented the first investigation into interpretable factors that affect the difficulty of unlearning. We found that unlearning gets harder i) the more entangled the forget and retain sets are in embedding space and ii) the more memorized the forget set is. Our investigation led into uncovering previouslyunknown behaviours of state-of-the-art algorithms that were not surfaced when considering random forget sets: when do they fail, when do they excel and when do they exhibit different trends from each other. Notably, we discovered that relabelling-based methods suffer disproportionately as the embedding-space entanglement increases, and exhibit a reverse trend compared to other methods in their behaviour for different memorization levels. Armed with these insights, we then proposed the RUM framework surfacing new (sub)problems within unlearning, whose solution may lead to greater performance. Finally, we derived specific instantiations of RUM and analyzed how its different components can improve performance. We found that sequential unlearning of homogenized forget set subsets improves all considered state-of-the-art unlearning algorithms and investigated the dynamics of sequential unlearning to glean insights as to why that is. We also found that we can further boost performance by selecting the best unlearning algorithm per subset. ", "page_idx": 9}, {"type": "text", "text": "Efficiency How is this important aspect affected in our sequential framework? We remark that it depends on the unlearning algorithm. For instance, applying Fine-tune three times is much more expensive than applying it once, because Fine-tune performs (at least) one epoch over the entire retain set. But for other algorithms the overall cost does not increase significantly. Further, a key observation from our results is that we can do well by actually doing nothing on a subset of the forget set, which can really boost efficiency (especially since the vast majority of examples are \u201clow mem\u201d). Additionally, our results with the C-proxy demonstrate that significant performance improvements in unlearning can be achieved with minimal computational cost, avoiding the heavy expense of computing memorization scores. ", "page_idx": 9}, {"type": "text", "text": "Data-space vs embedding-space outliers How does the embedding space entanglement interact with the level of memorization of the forget set? We analyzed this and found in Table 3 that all of our memorization buckets have relatively high ES, indicating that separating out (data-space) outliers in the forget set doesn\u2019t lead to lower entanglement between the two sets in the embedding space. We leave it to future work to study the interaction of these factors. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work We hope future work explores other refinement strategies (e.g. for a notion that captures embedding-space entanglement) and investigates privacy implications of sequential RUM, e.g. in terms of the \u201cprivacy onion effect\u201d [6]. We also hope to see how our RUM framework can be adopted and adapted for unlearning in LLMs, especially given the findings from the contemporaneous paper [2] where unlearning is performed only on the highest-memorized examples in the forget set (albeit, memorization is defined differently for LLMs). We hope our framework continues to enable progress in understanding and improving unlearning and that our identified factors of difficulty and associated behaviours of existing algorithms continue to improve the state-of-the-art and inform the development of strong evaluation metrics that consider forget sets that vary in terms of relevant identified characteristics. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Vincent Dumoulin and Fabian Pedregosa for valuable conversations and feedback at various stages of the project. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. Advances in neural information processing systems, 31, 2018.   \n[2] George-Octavian Barbulescu and Peter Triantafillou. To each (textual sequence) its own: Improving memorized-data unlearning in large language models. In International conference on machine learning, 2024.   \n[3] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141\u2013159. IEEE, 2021.   \n[4] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy $(S P)$ , pages 1897\u20131914. IEEE, 2022.   \n[5] Nicholas Carlini, Ulfar Erlingsson, and Nicolas Papernot. Distribution density, tails, and outliers in machine learning: Metrics and applications. arXiv preprint arXiv:1910.13427, 2019.   \n[6] Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer. The privacy onion effect: Memorization is relative. Advances in Neural Information Processing Systems, 35:13263\u201313276, 2022.   \n[7] R Dennis Cook and Sanford Weisberg. Criticism and influence analysis in regression. Sociological methodology, 13:313\u2013361, 1982.   \n[8] Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and programming, pages 1\u201312. Springer, 2006.   \n[9] Chongyu Fan, Jiancheng Liu, Alfred Hero, and Sijia Liu. Challenging forgets: Unveiling the worst-case forget sets in machine unlearning. arXiv preprint arXiv:2403.07362, 2024.   \n[10] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. arXiv preprint arXiv:2310.12508, 2023.   \n[11] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954\u2013959, 2020.   \n[12] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems, 33:2881\u20132891, 2020.   \n[13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.   \n[14] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data deletion in machine learning. Advances in neural information processing systems, 32, 2019.   \n[15] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304\u20139312, 2020.   \n[16] Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Goldstein. Unraveling meta-learning: Understanding feature representations for few-shot tasks. In International Conference on Machine Learning, pages 3607\u20133616. PMLR, 2020.   \n[17] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11516\u201311524, 2021.   \n[18] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013773, 2012.   \n[19] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. arXiv preprint arXiv:1911.03030, 2019.   \n[20] Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot. Inexact unlearning needs more careful evaluations to avoid a false sense of privacy. arXiv preprint arXiv:2403.01218, 2024.   \n[21] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion from machine learning models. In International Conference on Artificial Intelligence and Statistics, pages 2008\u20132016. PMLR, 2021.   \n[22] Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, et al. Measuring forgetting of memorized training examples. arXiv preprint arXiv:2207.00099, 2022.   \n[23] Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. arXiv preprint arXiv:2002.03206, 2020.   \n[24] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pages 1885\u20131894. PMLR, 2017.   \n[25] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, PRANAY SHARMA, Sijia Liu, et al. Model sparsity can simplify machine unlearning. Advances in Neural Information Processing Systems, 36, 2024.   \n[27] Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu, Minghai Qin, Sijia Liu, Zhangyang Wang, et al. Sanity checks for lottery tickets: Does your winning ticket really win the jackpot? Advances in Neural Information Processing Systems, 34:12749\u201312760, 2021.   \n[28] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Sch\u00f6lkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. arXiv preprint arXiv:2305.18462, 2023.   \n[29] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022.   \n[30] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.   \n[31] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understanding factors influencing machine unlearning. In 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P), pages 303\u2013319. IEEE, 2022.   \n[32] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.   \n[33] Eleni Triantafillou, Peter Kairouz, Fabian Pedregosa, Jamie Hayes, Meghdad Kurmanji, Kairan Zhao, Vincent Dumoulin, Julio Jacques Junior, Ioannis Mitliagkas, Jun Wan, Lisheng Sun Hosoya, Sergio Escalera, Gintare Karolina Dziugaite, Peter Triantafillou, and Isabelle Guyon. Are we making progress in unlearning? findings from the first neurips unlearning competition. arXiv preprint arXiv:2406.09073, 2024.   \n[34] Eleni Triantafillou, Fabian Pedregosa, Jamie Hayes, Peter Kairouz, Isabelle Guyon, Meghdad Kurmanji, Gintare Karolina Dziugaite, Peter Triantafillou, Kairan Zhao, Lisheng Sun Hosoya, Julio C. S. Jacques Junior, Vincent Dumoulin, Ioannis Mitliagkas, Sergio Escalera, Jun Wan, Sohier Dane, Maggie Demkin, and Walter Reade. Neurips 2023 - machine unlearning, 2023.   \n[35] Junxiao Wang, Song Guo, Xin Xie, and Heng Qi. Federated unlearning via class-discriminative pruning. In Proceedings of the ACM Web Conference 2022, pages 622\u2013632, 2022.   \n[36] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. arXiv preprint arXiv:2108.11577, 2021.   \n[37] Kairan Zhao and Peter Triantafillou. Scalability of memorization-based machine unlearning. In NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Broader impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Unlearning research can have profound broader impact in allowing users to request their data to be deleted from models or making models safer or more accurate by removing harmful or outdated information. Our work is exploratory: we identify factors affecting difficulty that are interpretable and can drive improvements to unlearning pipelines. As such, we don\u2019t see any direct harmful impact of our work. ", "page_idx": 13}, {"type": "text", "text": "A.2 Implementation details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Datasets and models We use the CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets for our evaluation. CIFAR-10 consists of $60\\small{,}000\\,32\\mathrm{x}32$ color images across 10 classes, with 6,000 images per class. CIFAR-100 contains 100 classes, each with $600\\ 32\\mathrm{x}32$ color images. Tiny-ImageNet includes 100,000 64x64 color images across 200 classes, with each class containing 500 training images, 50 validation images, and 50 test images (for a total of 100,000 training, 10,000 validation, and 10,000 test images). For CIFAR-10 and CIFAR-100, the train, validation, and test set sizes are 45,000, 5,000, and 10,000 images, respectively. We use ResNet-18, ResNet-50, and VGG-16 as model architectures. Specifically, we use ResNet-18 for CIFAR-10, ResNet-50 for CIFAR-100, and both ResNet-18 and VGG-16 for Tiny-ImageNet. ", "page_idx": 13}, {"type": "text", "text": "Training details for original models We trained four models across different datasets and architectures: ResNet-18 on CIFAR-10, ResNet-50 on CIFAR-100, ResNet-18 on Tiny-ImageNet, and VGG-16 on Tiny-ImageNet. For CIFAR-10, the ResNet-18 model was trained for 30 epochs using the SGD optimizer. The learning rate was initialized at 0.1 and scheduled with cosine decay. For CIFAR-100, the ResNet-50 model was trained for 150 epochs using the SGD optimizer, with the learning rate initialized at 0.1 and decayed by a factor of 0.2 at epochs 60 and 120. For Tiny-ImageNet, the ResNet-18 model was trained for 80 epochs, and the VGG-16 model was trained for 100 epochs, both using the SGD optimizer with an initial learning rate of 0.1 and cosine decay. All models were trained with a weight decay of 0.0005, a momentum of 0.9, and a batch size of 256. Additionally, data augmentations, including random cropping and random horizontal filpping, were applied during training on CIFAR-100 and Tiny-ImageNet. All training was conducted on Nvidia RTX A5000 GPUs. ", "page_idx": 13}, {"type": "text", "text": "Training details for machine unlearning To ensure optimal performance, we carefully set the hyperparameters for each unlearning method across different datasets and architectures. For retrainfrom-scratch, we followed the exact same training procedure as the original model, but only trained on the retain set, excluding the forget set. For Fine-tune, we trained the model for 10 epochs with a learning rate in the range [0.01, 0.1]. L1-sparse was run for 10 epochs with a learning rate in the range [0.005, 0.1] and a sparsity-promoting regularization parameter $\\gamma$ in the range $[\\bar{10}^{-6},10^{-4}]$ . NegGrad was also trained for 10 epochs, with a learning rate in the range $[10^{-4}$ , 0.1]. For $\\mathrm{NegGrad+}$ , we used a $\\beta$ value in the range [0.85, 0.99] as a weighting factor that balances two components of the loss, training for 5 epochs with a learning rate in the range [0.01, 0.05]. Influence unlearning involved tuning the parameter $\\alpha$ for the woodfisher Hessian Inverse approximation within the range [1, 100]. SalUn was trained for 5-10 epochs with a learning rate in the range [0.005, 0.1] and sparsity ratios in the range [0.3, 0.7]. Random-label was trained for 10 epochs with a learning rate in the range [0.01, 0.1]. ", "page_idx": 13}, {"type": "text", "text": "In our experiments with forget / retain set partitions at varied levels of memorization or ES, we tuned the hyperparameters to achieve the best ToW performance for each unlearning algorithm. The results are reported as averages with $95\\%$ confidence intervals over 3 runs, except for relabeling-based methods, which had higher variance and were therefore run 6 times. For the RUM experiment, we adjusted the hyperparameters at each step to ensure that the accuracy after each step closely matched the accuracy obtained by retraining from scratch. This procedure was repeated for all algorithms, with results reported as averages over 3 runs with $95\\%$ confidence intervals. ", "page_idx": 13}, {"type": "text", "text": "A.3 Procedure for creating retain / forget partitions with varying ES ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To create retain / forget partitions with varied levels of ES, we followed a systematic procedure. Initially, we trained the original model $\\theta^{o}$ on the entire training dataset $\\mathcal{D}_{t r a i n}$ . Using $\\theta^{o}$ , we then extracted embeddings for each data point in $\\mathcal{D}_{t r a i n}$ . The global centroid for $\\mathcal{D}_{t r a i n}$ , denoted as $\\mu$ , was determined by calculating the mean of all example embeddings. For each example $i$ in $\\mathcal{D}_{t r a i n}$ , we then computed its l2-distance from the global centroid $\\mu$ in the original model\u2019s embedding space as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nd(i,\\mu;\\theta^{o})=||\\phi_{i}-\\mu||^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We ranked these distances for all data examples in $\\mathcal{D}_{t r a i n}$ and selected the 3000 examples with the highest distances to form the low ES bucket. Subsequently, we moved further down the ranked list, selecting 3000 examples with progressively lower distances to form the medium and high ES buckets, until we achieved the desired levels of ES variation. This approach allowed us to form forget sets, each with a size of 3000, categorized into low, medium, and high ES levels. The rationale behind this selection is that examples with high distances from the global centroid are considered \u201cdistant\" from the overall data distribution in the embedding space and are therefore less entangled with the rest of the dataset, i.e., the retain set. Various unlearning algorithms were then deployed on $\\theta^{o}$ across different forget / retain partitions. Their performance was measured using ToW along with forget, retain, and test accuracy, as well as MIA. ", "page_idx": 14}, {"type": "text", "text": "This procedure enabled us to create retain / forget partitions with varying ES values. The ES values for our low, medium, and high ES partitions are shown in Table 2. As observed from the table, the ES values increase from low to high ES partitions for both CIFAR-10 and CIFAR-100, confirming the effectiveness of our procedure. We also use Maximum Mean Discrepancy (MMD) [18], a widely-used metric, to further validate ES. The MMD is computed as follows: Given a pre-trained model $\\theta^{o}$ (the original model in our case) trained on $\\mathcal{D}_{t r a i n}$ , we first extract features for the forget set $\\boldsymbol{S}$ and the retain set $\\mathcal{R}$ using $\\theta^{o}$ , denoted as $\\phi(S)$ and $\\phi(\\mathcal{R})$ , respectively. We then apply an RBF kernel to map these features to another space $\\mathcal{H}$ such that $\\dot{\\phi^{\\prime}}(\\bar{S}),\\phi^{\\prime}(\\mathcal{R})\\in\\mathcal{H}$ . The MMD between $\\phi(S)$ and $\\phi(\\mathcal{R})$ is then calculated using the following formula: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{MMD}^{2}(\\phi(S),\\phi(\\mathcal{R}))=\\left\\lVert\\frac{1}{|S|}\\sum_{i\\in\\mathcal{S}}\\phi_{i}^{\\prime}-\\frac{1}{|\\mathcal{R}|}\\sum_{j\\in\\mathcal{R}}\\phi_{j}^{\\prime}\\right\\rVert_{\\mathcal{H}}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Our experiments reveal a negative correlation between ES and MMD scores, as reported in Table 2. For CIFAR-10/ResNet-18 and CIFAR-100/ResNet-50, low (high) ES values correspond to high (low) MMD values, supporting the consistency of the ES. ", "page_idx": 14}, {"type": "text", "text": "Additionally, Figure 6 presents the data representation of low, medium, and high ES partitions, confirming that the degree of entanglement between the retain and forget sets aligns with the computed ES values. As we move from low to high ES partitions, the forget set (yellow) and the retain set (blue) become increasingly entangled. This indicates that higher ES partitions reflect greater complexity in distinguishing between the two sets. ", "page_idx": 14}, {"type": "text", "text": "A.4 Description of MIA and ToW-MIA ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We adopted a MIA based on prediction confidence, following the procedure described by [26]. To conduct this attack, we first sampled equal-sized data from the retain set and the test set, using these to train a binary classifier as the MIA model. This model is designed to distinguish whether a data example was involved in the training stage or not. ", "page_idx": 14}, {"type": "text", "text": "Next, we applied this attack model to the forget set to evaluate unlearning performance during the testing phase, after an unlearning method was implemented. Intuitively, for successful unlearning, we want forget set examples to be classified as \u201cnon-training\u201d data. We define \u201ctraining\" data as the positive class and \u201cnon-training\" data as the negative class, and measured the performance of MIA by calculating the ratio of true negatives (i.e., the number of the forgetting samples predicted as non-training examples) predicted by the MIA model to the total size of the forget set, as shown in (3), following the same procedure as prior work [26]. ", "page_idx": 14}, {"type": "table", "img_path": "QAbhLBF72K/tmp/19beadea7c98abe149a0364e829bb959037db05bfcaa4deffc0c213a49aa9697.jpg", "table_caption": ["Table 2: ES and MMD scores for the low, medium and high forget / retain partitions for CIFAR-10 and CIFAR-100. ", "(b) CIFAR-100 "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "QAbhLBF72K/tmp/eca4cc3c5f69dc520cf57efacfeea62dcb260b9e04e6259582a972485ab35d34.jpg", "img_caption": ["Figure 6: Data representation visualization for forget / retain partitions with low, medium, and high ES. We used PCA to reduce the dimensionality for visualization. In each figure, the data examples from the forget set are shown in yellow, while those from the retain set are in blue. The global centroid is marked in red at the center of the figures. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{MIA\\,Performance}=\\frac{T N_{S}}{|S|},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In this context, $\\mathcal{S}\\subseteq\\mathcal{D}_{t r a i n}$ represents the forget set and $|{\\cal S}|$ is the total size of this set. The term $T N_{S}$ denotes the number of true negatives predicted by the MIA model, indicating examples identified as \u201cnon-training\u201d data. This metric ranges from 0 to 1, with higher values signifying a larger portion of the forget set predicted as \u201cnon-training\u201d data. The ideal MIA score, for an unlearing algorithm, is one that matches the MIA score of retraining-from-scratch. Note that, even if applying this MIA on retrain-from-scratch, some portion of the forget set will be classified as \u201ctraining data\u201d due to heavily resembling the retain set (e.g. examples that are not really memorized may have similar confidences regardless on whether or not they were trained on). And we want the model confidences of the unlearned model to resemble as closely as possible those of retrain-from-scratch. For this reason, we additionally report the \u201cMIA gap\u201d (the absolute differentce between the MIA score for unlearning compared to that of retrain-from-scratch) as a easier-to-interpret metric, where lower is better, and the ideal score there is 0. ", "page_idx": 15}, {"type": "text", "text": "Building on the MIA setup above, we introduce another metric \u201cToW-MIA\u201d to evaluate unlearning performance, similar to ToW described in Section 4. The metric is defined as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{?oW-MIA}\\big(\\theta^{u},\\theta^{r},\\mathcal{S},\\mathcal{R},\\mathcal{D}_{t e s t}\\big)=\\big(1-\\mathrm{d}\\mathrm{m}(\\theta^{u},\\theta^{r},\\mathcal{S})\\big)\\cdot\\big(1-\\mathrm{d}\\mathrm{a}\\big(\\theta^{u},\\theta^{r},\\mathcal{R}\\big)\\big)\\cdot\\big(1-\\mathrm{d}\\mathrm{a}\\big(\\theta^{u},\\theta^{r},\\mathcal{D}_{t e s t}\\big)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathfrak{m}(\\theta,\\mathcal{D})$ represents the MIA performance of the model $\\theta$ trained on $\\mathcal{D}$ , as defined in Equation (3). The first term, $\\mathrm{d}\\mathrm{m}(\\theta_{u},\\theta_{r},S)=|\\mathrm{m}(\\theta_{u},S)-\\mathrm{m}(\\theta_{r},S)|$ , denotes the absolute difference in MIA performance on $\\boldsymbol{S}$ between the unlearned model $\\theta_{u}$ and the retrained-from-scratch model $\\theta_{r}$ . This term is the key distinction between ToW and ToW-MIA, where \u201cforget quality\u201d is measured by MIA (ToW-MIA) rather than accuracy (ToW). The other two terms remain identical to those in ToW (see the equation in Section 4), representing the absolute difference in accuracy between $\\theta^{u}$ and $\\theta^{r}$ on $\\mathcal{R}$ and $\\mathcal{D}_{t e s t}$ , respectively, which reflect \u201cmodel utility\u201d. Like ToW, ToW-MIA rewards unlearned models that approximate the performance of the retrained model across the forget, retain, and test sets. ToW-MIA ranges from 0 to 1, with higher values indicating better unlearning performance. Figure 7 show the same trends with ToW-MIA as for ToW, both for unlearning difficulty analysis and RUM. More detailed results using ToW-MIA can be found in Table 14. ", "page_idx": 15}, {"type": "image", "img_path": "QAbhLBF72K/tmp/2d1b90cb06ce4d1a4302b3b50469c6b6ab7d3d33a818c3e12a91cfc7f2fa679b.jpg", "img_caption": ["Figure 7: Evaluating performance with ToW-MIA (higher is better) for unlearning difficulty analysis (a, b) and $\\mathbf{RUM}^{\\mathcal{F}}$ (c). Error bars represent $95\\%$ confidence intervals from 3 runs per algorithm. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A.5 Data-space vs embedding-space outliers ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Building on our discussion in Section 4, we identify two factors that affect the difficulty of unlearning: the entanglement between the forget and retain sets, and the memorization level of the forget examples. This raises the question: how do these two factors interact with each other? Are the outliers in data space the same as those in embedding space? Our findings in this section suggest that these factors are indeed distinct. ", "page_idx": 16}, {"type": "text", "text": "Memorization within ES-based partitions We analyzed the memorization scores within each forget set categorized by different ES values. Figure 8 shows the distribution and average memorization scores in the forget sets of low, medium, and high ES categories. It can be seen that each ES category\u2019s forget set contains a mix of memorization levels. Specifically, while the mean memorization score of forget examples increases from low to high ES categories, the memorization scores still span the entire range from 0 to 1. ", "page_idx": 16}, {"type": "text", "text": "Entanglement within memorization-based partitions We examined the ES values for each memorization category to understand the entanglement between the forget and retain sets in the embedding space. Table 3 presents the ES values corresponding to low, medium, and high memorization buckets. The findings reveal that all the memorization bucket exhibits relatively high ES, suggesting that separating outliers in the data space does not reduce the entanglement between the forget and retain sets in the embedding space. ", "page_idx": 16}, {"type": "table", "img_path": "QAbhLBF72K/tmp/1866be5f3ad004151925685c58cfac559ba473b48fcbba725abeee3ad7d68ef1.jpg", "table_caption": ["Table 3: ES values for forget / retain partitions across varied memorization levels for CIFAR-10 and CIFAR-100. ", "(b) CIFAR-100 "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "QAbhLBF72K/tmp/ad3faef15df14b547a668531f4d09f8aff0e04ea96124af6185b344ca4dea9cc.jpg", "img_caption": ["Figure 8: Memorization distribution for low, medium, high ES forget / retain partitions. The mean memorization score for low, medium, high ES partitions are $0.084{\\scriptstyle\\pm0.203}$ , 0.134\u00b10.235, 0.390\u00b10.326 for CIFAR-10, and $0.159{\\scriptstyle\\pm0.283}$ , $0.222{\\scriptstyle\\pm0.329}$ , and $0.317{\\scriptstyle\\pm0.364}$ for CIFAR-100. ", ""], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "These findings demonstrate that embedding space entanglement and the memorization level of the forget set are distinct concepts, not merely different aspects of the same phenomenon. ", "page_idx": 17}, {"type": "text", "text": "A.6 Confidence-based proxy for memorization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To improve the efficiency of calculating memorization scores, we leverage insights from previous works [23, 32] and introduce a model confidence-based metric, referred to as the C-proxy, which serves as a proxy for the memorization score. The computation of the C-proxy proceeds as follows: For each data point $(x_{i},y_{i})\\in\\mathcal{D}$ , we track the softmax probability of the model\u2019s prediction $\\theta(x_{i})$ for the ground-truth label $y_{i}$ across all training epochs, as the model $\\theta$ is trained on $\\mathcal{D}$ using algorithm $\\boldsymbol{\\mathcal{A}}$ . These probabilities are then averaged over all epochs at the end of training to capture the training dynamics. ", "page_idx": 17}, {"type": "text", "text": "Table 4 presents the fidelity (measured by Spearman correlation) and efficiency (measured by additional computational cost) of the C-proxy in relation to memorization. This evaluation is conducted on both CIFAR-10 and CIFAR-100 datasets. As shown, the C-proxy demonstrates a strong negative Spearman correlation with memorization scores while significantly reducing computational overhead compared to both computing memorization scores and retraining the model from scratch. This suggests that the C-proxy is an effective proxy for memorization while also offering substantial efficiency gains. ", "page_idx": 17}, {"type": "text", "text": "Table 4: Comparison of C-proxy based on Spearman correlation with memorization and relative computation time percentages in comparison to memorization computation / retraining the model from scratch, evaluated on CIFAR-10 and CIFAR-100 using ResNet-18 and ResNet-50, respectively. ", "page_idx": 17}, {"type": "table", "img_path": "QAbhLBF72K/tmp/d664d4e2f5f685e8ad3e21f09129b7c082209f6c54514746351e2d88d8a927c9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "QAbhLBF72K/tmp/00f7cd05c9351c99b6e27b0d39060871b90383c7bd33a548b47845e487d2ad93.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 9: Impact of C-proxy on unlearning difficulty: C-proxy vs ToW on Tiny-ImageNet across low, medium, and high C-proxy levels, using ResNet-18 and VGG-16. Error bars represent $95\\%$ confidence intervals from 3 runs per algorithm. ", "page_idx": 18}, {"type": "image", "img_path": "QAbhLBF72K/tmp/f56940a24689d8539fdb6142dcb89c29eee31592407efcaf9bb2430f019cc52d.jpg", "img_caption": ["Figure 10: Impact of C-proxy on unlearning performance on Tiny-ImageNet with ResNet-18 and VGG-16. Each unlearning algorithm is applied in three ways: $\\mathbf{RU}\\mathbf{\\dot{M}}^{\\mathcal{F}}$ , as well as vanilla and shuffle as comparison. Error bars represent $95\\%$ confidence intervals from 3 runs per algorithm. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 4a and Figure 9 present results across different datasets and model architectures, demonstrating that the pattern identified in Section 4.2 remains consistent when using the C-proxy instead of the memorization score. Specifically, the lower the C-proxy value, the harder it is to unlearn (note that the C-proxy is negatively correlated with memorization). Furthermore, Figure 4b and Figure 10 show the results of $\\mathbf{R}\\bar{\\mathbf{U}}\\mathbf{\\bar{M}}^{\\mathcal{F}}$ across various datasets and model architectures, using the C-proxy in place of memorization for the refinement step described in Section 5. More detailed results, including accuracy and MIA, are available in Table 17. These results demonstrate that $\\mathbf{RUM}^{\\mathcal{F}}$ still achieves a significant unlearning performance gain when C-proxy is used, comparable to those obtained with the memorization score. A detailed analysis of various memorization proxies, their suitability, and their impact on RUM can be found in [37]. ", "page_idx": 18}, {"type": "text", "text": "A.7 Per-example differences between the unlearned and retrained models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "While accuracy is a commonly used metric in the unlearning literature [29], it reflects an average over a data distribution and may overlook finer details. To gain a more granular understanding of the similarities/differences between the unlearned model and the retrained model, we examine their behavior at the level of individual examples. Specifically, we collect predictions from both models for each example and count the examples where their predictions differ. Table 5 presents these results for a representative setting: CIFAR-10 dataset with ResNet-18 architecture. ", "page_idx": 18}, {"type": "text", "text": "We observe that, in all cases, the percentage of disagreements between the unlearned and retrained models remains low. For highly memorized data (which typically makes unlearning harder), the disagreement rate increases but stays modest, ranging between ca. $[7\\%,15\\%]$ . For low-memorized data, the disagreement rate is lower, within ca. $[3\\%,5\\%]$ . ", "page_idx": 18}, {"type": "text", "text": "Interestingly, for the vast majority of unlearning algorithms, these per-example disagreement trends align with those observed using ToW: low-memorized examples are generally easier to unlearn, as indicated by both ToW and the lower disagreement rate, while high-memorized examples are harder to unlearn, with higher disagreement rates. A similar trend is observed across different ES levels as well. This consistency between average accuracies (ToW) and per-example disagreements supports the robustness of our findings and underscores the validity of ToW as a measure of unlearning difficulty. ", "page_idx": 18}, {"type": "table", "img_path": "QAbhLBF72K/tmp/603184eb10682d30611ee78acc50a6c2685a8fc7640f4767206b6a39ad7b91f3.jpg", "table_caption": ["Table 5: ToW vs percentage of different predictions across ES or memorization levels on CIFAR10/ResNet-18, averaged over 3 runs with $95\\%$ confidence intervals. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "QAbhLBF72K/tmp/fe5e2d5959ade1f171045252e74ea08d9d4e0347a181bdb9428feaf8e168773a.jpg", "table_caption": ["(a) Per-example difference vs ES ", "(b) Per-example difference vs memorization "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.8 Detailed results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we show the results that were used to construct all figures in the paper, as well as additional results and analyses. ", "page_idx": 19}, {"type": "text", "text": "Entanglement of forget and retain sets affects unlearning difficulty The first factor affecting unlearning difficulty, as discussed in Section 4.1, is the degree of entanglement between the forget and retain sets in the embedding space. Specifically, the more entangled these sets are, the harder it becomes to unlearn. We present detailed results on how this factor affects unlearning difficulty in Table 6, 7, 8, 9, 10 and Figure 11. Table 6 displays the ToW results for different ES partitions across various settings, including CIFAR-10, CIFAR-100 and Tiny-ImageNet datasets, as well as ResNet and VGG architectures. Additionally, comprehensive data\u2014covering forget, retain, and test accuracy along with MIA performance\u2014are provided in Figure 11 and in Table 7 through Table 10 for these datasets and model architectures. All the results are averaged over 3 runs for each algorithm (6 runs for relabelling-based algorithms due to their higher variance), along with $95\\%$ confidence intervals. ", "page_idx": 19}, {"type": "text", "text": "Memorization of forget examples affects unlearning difficulty In Section 4.2, we discussed another critical factor affecting unlearning difficulty: the memorization level of the forget examples. Specifically, when a forget set consists of examples that are more highly memorized by the model, it becomes more difficult to unlearn (for most algorithms). The detailed results that support this observation are presented in Table 11, Table 12, Table 13, and Figure 12. ", "page_idx": 19}, {"type": "text", "text": "Table 11 illustrates the ToW for forget sets with varying memorization levels. Additionally, Figure 12 and Table 12 to Table 13 provide extensive data on forget, retain, and test accuracy, as well as MIA performance. For all the experiments, the average results are reported with $95\\%$ confidence intervals over 6 runs for relabelling-based algorithms and 3 runs for others. ", "page_idx": 19}, {"type": "text", "text": "RUM experiments We present the details of our RUM experiment in this section. Table 15 presents the distribution of examples in the forget set for each class in the RUM experiments for both CIFAR10 and CIFAR-100 datasets. The size of the forget set is 3000 in all the experiments. For CIFAR-10, the table lists the number of forget set examples for each class. For CIFAR-100, the table arranges the number of forget set examples in a 20x5 format for better readability. As shown in Table 15, the forget set covers examples from all classes in both CIFAR-10 and CIFAR-100 experiments. ", "page_idx": 19}, {"type": "image", "img_path": "QAbhLBF72K/tmp/a09e49a7b6787c8a93ceab189c1c9708a7efca2241ee37448dcf7da04bfe1c95.jpg", "img_caption": ["(b) CIFAR-100 with ResNet-50 "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 11: Forget, retain, test accuracy and MIA performance for forget / retain partitions with varied ES, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the ES increases, there is a notable decline in forget accuracy and a significant rise in MIA performance. This trend indicates that as the forget and retain sets become more entangled, more information from the forget set is effectively removed after unlearning. ", "page_idx": 20}, {"type": "text", "text": "Tables 16 provide detailed results when applying RUM to different unlearning algorithms for CIFAR10 using ResNet-18 and CIFAR-100 using ResNet-50. We selected the top-performing algorithms from previous experiments (i.e., Fine-tune, L1-sparse, NegGrad+, SalUn) and applied the refinement strategy to each algorithm in three ways: i) \u201cvanilla\": unlearning $\\boldsymbol{S}$ in one go, ii)\u201cshuffle\": sequentially applying the algorithm on 3 equal-sized random subsets, serving as a control experiment, iii) $\\dot{}^{\\mathbf{\\mu}}\\mathbf{R}\\mathbf{U}\\mathbf{M}^{\\mathcal{F}\\ddot{\\mathbf{\\eta}}}$ : sequentially applying the algorithm on 3 subsets of $\\boldsymbol{S}$ in $\\mathrm{low}\\to\\mathrm{med}\\to$ high memorization order. Each experiment was conducted 3 times, with average values and $95\\%$ confidence intervals reported. Our results indicate that $\\mathbf{RUM}^{\\mathcal{F}}$ improves the performance of each unlearning algorithm, and the full RUM approach, which uses the best algorithm for each subset, achieves the overall best results. ", "page_idx": 20}, {"type": "text", "text": "Furthermore, to gain a deeper understanding of the dynamics involved in applying RUM, we plotted the sequence dynamics for SalUn $\\mathbf{RUM}^{\\mathcal{F}}$ on CIFAR-10 (Figure 5) and $\\mathrm{NegGrad}+\\mathrm{\\overline{{RUM}}}^{\\mathcal{F}}$ on CIFAR100 (Figure 13). These plots show the accuracy of the entire forget set, the retain set, the test set, and subsets of the forget set after each step. They demonstrate that while both orderings $\\mathrm{{1ow\\tomed\\to}}$ high and $\\mathrm{igh}\\rightarrow\\mathrm{med}\\rightarrow\\mathrm{low}$ memorization) yield similar ToW according to Table 16, their sequence dynamics during the unlearning phase are different. This phenomenon is discussed in Section 6 in the main paper, specifically in the Analysis of Sequence Dynamics paragraph. ", "page_idx": 20}, {"type": "image", "img_path": "QAbhLBF72K/tmp/f1cc57cc0b86403ca8d83d2b462bc411590c28346cc77220db9980c43a81a1a0.jpg", "img_caption": ["(b) CIFAR-100 with ResNet-50 "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 12: Forget, retain, test accuracy and MIA performance for forget / retain partitions with varying levels of memorization, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the memorization level of the forget sets increases, forget accuracy significantly decreases while MIA performance increases. This trend indicates that as the forget examples become more memorized by the model, unlearning becomes more effective in removing the effect of these examples. ", "page_idx": 21}, {"type": "image", "img_path": "QAbhLBF72K/tmp/f9a43e012d8261ccf48e1996b206e6777fb6b8ed82646f2671592edaeca446cb.jpg", "img_caption": ["(a) low $\\rightarrow$ med $\\rightarrow$ high (Tow: 0.92, MIA gap: 0.059) (b) high $\\rightarrow$ med $\\rightarrow$ low (ToW: 0.93, MIA gap: 0.022) "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 13: Analysis of sequence dynamics for two different orderings. We apply $\\mathrm{NegGrad+}$ RUM on CIFAR-100 dataset using ResNet-50 and show the accuracy on overall forget set (and each of its subsets), the retain set and test set after each step in the RUM sequence. ", "page_idx": 21}, {"type": "text", "text": "Table 6: ToW for different unlearning algorithms applied to forget / retain sets with varying ES, evaluated on various datasets and model architectures. Across all algorithms, including the baseline without any unlearning performed (denoted as \u201cOriginal\u201d), we observe that ToW decreases from low to high ES partitions, indicating that unlearning becomes harder as the forget and retain sets become more entangled. ", "page_idx": 22}, {"type": "table", "img_path": "QAbhLBF72K/tmp/bac7a9975ccb7875780426683eda0f9c89600d906f0746ba176e980820a58d80.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "QAbhLBF72K/tmp/25d7d9e85b2340e03fcc72033e0e27e776033c3b9076b69b011e45bac427f857.jpg", "table_caption": ["(a) CIFAR-10 with ResNet-18 "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "QAbhLBF72K/tmp/c6526dfed0405a998095bb369d8037c731926ff69f82c447fddcc99d2822bcf0.jpg", "table_caption": ["(b) CIFAR-100 with ResNet-50 "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "QAbhLBF72K/tmp/5524de26a7f46ec4d8af7f609ad5bbd87f80a34fa196d222029faf6c58cfc48b.jpg", "table_caption": ["(c) Tiny-ImageNet with ResNet-18 "], "table_footnote": ["(d) Tiny-ImageNet with VGG-16 "], "page_idx": 22}, {"type": "table", "img_path": "QAbhLBF72K/tmp/7ae26079aaad074e0879d8268eade41e6fece534c0134d49874a39dd9bca7794.jpg", "table_caption": ["Table 7: Accuracy and MIA performance for different unlearning algorithms applied to forget / retain sets with varying ES for CIFAR-10 using ResNet-18. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "QAbhLBF72K/tmp/08144dc597357c24f9e7d895130517559a70e9191d168a69428873e8a4e84ae7.jpg", "table_caption": ["(a) Low ES "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "QAbhLBF72K/tmp/8fca6a9f72d03959bb2edcfd4cfd49e13b8d0982bfe425568acad8aa58b930e6.jpg", "table_caption": ["(b) Medium ES ", "(c) High ES "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "QAbhLBF72K/tmp/67962d7c95509c63b29fb0166ec1c89c47701b1f3dcdf34e61a5076f0722513b.jpg", "table_caption": ["Table 8: Accuracy and MIA performance of different unlearning algorithms on forget / retain sets with varying ES for CIFAR-100 using ResNet-50. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "QAbhLBF72K/tmp/0a6b2ec6f7d73c3c3854e8ffffa504a6b658ca9058f0cd0a806e6d8e93bbf181.jpg", "table_caption": ["(a) Low ES "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "QAbhLBF72K/tmp/92bab6861ce626ca80ede2ab0683ea96cbfa978aa72155153453b40f370bd080.jpg", "table_caption": ["(b) Medium ES ", "(c) High ES "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "QAbhLBF72K/tmp/1385e26413852e88e7d9baeb1295227aea833ac2fc38d0f40bcb6c8c9017f902.jpg", "table_caption": ["Table 9: Accuracy and MIA performance of different unlearning algorithms on forget / retain sets with varying ES for Tiny-ImageNet using ResNet-18. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "QAbhLBF72K/tmp/8594d3be139528efc11510c93495a6535644d08cb4bdb8f8f6a1c801f9a187ec.jpg", "table_caption": ["(a) Low ES "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "QAbhLBF72K/tmp/1ccf43fd59de8a58e8fdf47e9e8e6bf80631a913265d9662c1cb1c60e99ffd79.jpg", "table_caption": ["(b) Medium ES "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "QAbhLBF72K/tmp/aa0aa5dbc367f2ae73244cae445428ddbaba53e638cf32f20035edccdf5e25c2.jpg", "table_caption": ["Table 10: Accuracy and MIA performance of different unlearning algorithms on forget / retain sets with varying ES for Tiny-ImageNet using VGG-16. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "QAbhLBF72K/tmp/da2cb7c0f87180f76ca5e1c8178abef59e16cfabbc383e39cc3697fe519d8bfd.jpg", "table_caption": ["(a) Low ES "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "QAbhLBF72K/tmp/39115678a063ce7cc3e339bb5c4e34830843fa9f05ded4074d69adbd1740ee9a.jpg", "table_caption": ["(b) Medium ES "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "QAbhLBF72K/tmp/fbfd842edd66e3f718771113e79db5d3215ab27e8b87b0e91965d48820f2dc39.jpg", "table_caption": ["Table 11: ToW for different unlearning algorithms applied to forget sets with varying levels of memorization, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the memorization level of the forget examples increases, the ToW significantly decreases for most algorithms, indicating that unlearning becomes harder when the forget examples are more memorized by the model. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "QAbhLBF72K/tmp/ed914edb5e270baeda435dd437046a13f1512c5715e53da17406c55c289debfb.jpg", "table_caption": ["(a) CIFAR-10 with ResNet-18 "], "table_footnote": ["(b) CIFAR-100 with ResNet-50 "], "page_idx": 27}, {"type": "table", "img_path": "QAbhLBF72K/tmp/e2a3e7f7a4e6495953fc1aad212dffeea3fa93f4473689bbcfd4f6f499450e18.jpg", "table_caption": ["Table 12: Accuracy and MIA results for various unlearning algorithms applied on forget / retain sets with different memorization levels for CIFAR10 using ResNet-18. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "QAbhLBF72K/tmp/841631a7ecdc21ecfe25695fea79a2d8118743602343c17349d79df027e604d2.jpg", "table_caption": ["(a) Low memorization "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "QAbhLBF72K/tmp/5281c36f7c46e4ac7cdadbed518179809493fd7b3fc024bcea92d0b91b2cbd34.jpg", "table_caption": ["(b) Medium memorization "], "table_footnote": ["(c) High memorization "], "page_idx": 28}, {"type": "table", "img_path": "QAbhLBF72K/tmp/e1fca27f198e08fc3fb446cd047f55b585081cc87d085552ad09d314ad5b6142.jpg", "table_caption": ["Table 13: Accuracy and MIA performance for different unlearning algorithms applied on forget / retain sets of varying levels of memorization for CIFAR100 using ResNet-50. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "QAbhLBF72K/tmp/b5d2f2f0715a627ee87f96c15ad690e55fdf33159d05bd11e0102f8ec558bf08.jpg", "table_caption": ["(a) Low memorization "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "QAbhLBF72K/tmp/006381570f3fc31bba094539c84c4a0a393df26a9aba7a68d9fa81cfafe5ce93.jpg", "table_caption": ["(b) Medium memorization "], "table_footnote": ["(c) High memorization "], "page_idx": 29}, {"type": "table", "img_path": "QAbhLBF72K/tmp/8904eb446bd0cd38ebdd384e045fdecb5bd9af6ec263d074207c1b62866a1dcc.jpg", "table_caption": ["Table 14: Unlearning performance evaluated by ToW-MIA, with ToW results included for comparison. Subtables (a, b): ToW-MIA for different unlearning algorithms applied to forget / retain sets with varying ES or memorization levels on CIFAR-10 using ResNet-18, with ToW results from Tables 6 and 11 for comparison. Subtable (c): $\\mathbf{RUM}^{\\mathcal{F}}$ and RUM results evaluated by ToW-MIA on CIFAR-10 using ResNet-18, with ToW results from Table 16 for comparison. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "QAbhLBF72K/tmp/a75d7f058259a5a78c0be5f88dfb86aa1d1d37f702c4a2b78c09f03da143db20.jpg", "table_caption": ["(a) ToW-MIA vs ES "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "QAbhLBF72K/tmp/7a2df2c9bff718cbec02c3f62252ca6b247b51d6c5fa4cb976ced5bd474b624c.jpg", "table_caption": ["(b) ToW-MIA vs memorization ", "(c) ToW-MIA vs RUM "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 15: Class distribution of the forget set in the RUM experiment for CIFAR-10 and CIFAR-100, with 3000 examples in the forget set for each dataset. The forget set includes examples from all classes in both datasets. ", "page_idx": 31}, {"type": "table", "img_path": "QAbhLBF72K/tmp/c754d888e880450cfd7e6931c24daeb5121ab2a4854fc490798470eb3b66d331.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "QAbhLBF72K/tmp/27689d01c4374bf2d5157e62abe291ed9077817790ec1f8d0cba1b7c3f20d843.jpg", "table_caption": ["(a) CIFAR-10 ", "(b) CIFAR-100 "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Table 16: RUM results on CIFAR-10 and CIFAR-100. Results obtained by applying RUM with different algorithms, according to ToW (higher is better), its constituent ingredients, and MIA (lower is better for MIA gap). The top section compares applying an unlearning algorithm $\\boldsymbol{\\mathcal{U}}$ in three ways: i) in one-go, as usual (e.g. Fine-tune), ii) on three randomly-determined equal-sized subsets of $\\boldsymbol{S}$ , sequentially (e.g. Fine-tune shuffle), and iii) on three equal-sized buckets obtained by refinement ${\\mathcal{F}}(S)$ according to memorization scores, in l $\\mathrm{ow\\tomed\\tohigh}$ order (e.g. Fine-tune $\\begin{array}{r}{\\dot{\\mathbf{R}}\\mathbf{U}\\mathbf{M}^{\\mathcal{F}}}\\end{array}$ ). The middle section of Table 16a further experiments with picking a different unlearning algorithm for each subset of ${\\mathcal{F}}(S)$ . Here $\\mathrm{A}\\rightarrow\\mathrm{B}\\rightarrow\\mathrm{C}$ denotes applying algorithm A on the first subset, B on the second subset, and C on the third subset, where the subsets appear in low $\\rightarrow$ medium $\\rightarrow$ high order. The bottom section shows different orderings. ", "page_idx": 32}, {"type": "table", "img_path": "QAbhLBF72K/tmp/a228ee9b6797d2ce7f3ecf171972adb90c05570f4927932444ebf37f8b0a52f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "QAbhLBF72K/tmp/e8d2d80da08d0fb88cfd67ec16ee2f05c1478d1652792f918403f8952e7e4276.jpg", "table_caption": ["(a) RUM results on CIFAR-10 using ResNet-18 ", "(b) RUM results on CIFAR-100 using ResNet-50 "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "QAbhLBF72K/tmp/83b1482d784bd823ed76e3997458c8dd4ac3f6e460a82905d610035d93c30cbf.jpg", "table_caption": ["Table 17: RUM results using C-proxy for the refinement step, evaluated by ToW on CIFAR-10 and Tiny-ImageNet datasets. Each algorithm is applied in three ways: $\\mathbf{RUM}^{\\mathcal{F}}$ , as well as vanilla and shuffle as comparison. Results are averaged over 3 runs with $95\\%$ confidence intervals. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "QAbhLBF72K/tmp/d3dd90882996a89241bcf52327bfbab92db12bcd865a441d3b922e04ce35c611.jpg", "table_caption": ["(a) ToW vs RUM on CIFAR-10 with ResNet-18 "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "QAbhLBF72K/tmp/f524eeaca14d999d5f4fb4cf5791f9d321af1d1bb8deac5f3c9ce21aad8c45fb.jpg", "table_caption": ["(b) ToW vs RUM on Tiny-ImageNet with ResNet-18 ", "(c) ToW vs RUM on Tiny-ImageNet with VGG-16 "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction accurately outline the paper\u2019s contributions, aligning well with the scope and findings presented in the study (we study interpretable factors affecting unlearning difficulty, uncover previously-unknown behaviours of state-of-the-art algorithms and propose a framework that leads to improvements of unlearning performance). ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper includes a dedicated \"Discussion and conclusion\" section that discuss the limitations of the work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not include any theoretical results, hence no assumptions or proofs are required. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper provides detailed descriptions of the experimental setup, including the datasets used, model architectures, hyperparameters, and evaluation metrics. This ensures that the main experimental results can be reproduced accurately, supporting the paper\u2019s claims and conclusions. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 35}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have included a GitHub repository containing our code and detailed instructions to enable other researchers to faithfully reproduce the main experimental results. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The experimental setting and details are described in Section 6 in the main paper and in Section A.2. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper reports the results with $95\\%$ confidence intervals and provides error bars, which appropriately reflect the statistical significance and variability of the experimental findings. This ensures that the reliability and significance of the results are clearly communicated. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper provides detailed information on the computational resources used for the experiments in A.2. This ensures that other researchers can accurately reproduce the experiments. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research adheres to the NeurIPS Code of Ethics, ensuring compliance with ethical standards in terms of data usage, experiment conduct, and the overall research process. Anonymity and confidentiality have been maintained where required, and all ethical considerations have been duly addressed. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We discuss this is Section A.1. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper poses no significant risks for misuse of data or models. Therefore, specific safeguards are not necessary. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper properly credits the creators and original owners of all assets used, including code, data, and models. The licenses and terms of use are explicitly mentioned and adhered to, ensuring compliance with the original creators\u2019 requirements. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]