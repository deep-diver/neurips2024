[{"type": "text", "text": "Robust Preference Optimization through Reward Model Distillation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Language model (LM) post-training (or alignment) involves maximizing a   \n2 reward function that is derived from preference annotations. Direct Preference   \n3 Optimization (DPO) is a popular offline alignment method that trains a policy   \n4 directly on preference data without the need to train a reward model or apply   \n5 reinforcement learning. However, typical preference datasets have only a single, or   \n6 at most a few, annotation per preference pair, which causes DPO to overconfidently   \n7 assign rewards that trend towards infinite magnitude. This frequently leads to   \n8 degenerate policies, sometimes causing even the probabilities of the preferred   \n9 generations to go to zero. In this work, we analyze this phenomenon and propose   \n0 distillation to get a better proxy for the true preference distribution over generation   \n11 pairs: we train the LM to produce probabilities that match the distribution induced   \n2 by a reward model trained on the preference data. Moreover, to account for   \n13 uncertainty in the reward model we are distilling from, we optimize against a   \n4 family of reward models that, as a whole, is likely to include at least one reasonable   \n15 proxy for the preference distribution. Our results show that distilling from such   \n16 a family of reward models leads to improved robustness to distribution shift in   \n17 preference annotations, while preserving the simple supervised nature of DPO. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Language model (LM) post-training (or alignment) aims to steer language model policies towards   \n20 responses that agree with human preferences. Early state-of-the-art approaches have focused on   \n21 reward learning from human feedback. In this paradigm, preference annotations are used to train   \n22 reward models, which then guide the optimization of the language model policy through online   \n23 reinforcement learning (an approach broadly referred to as RLHF). Recent research on offilne \u201cDirect   \n24 Preference Optimization\u201d [DPO; 23] and extensions thereof [3; 31], however, has demonstrated that   \n25 it is also possible to directly optimize policies on the preference data, which bypasses the need for a   \n26 separate reward model\u2014and its offline nature also leads to faster, and simpler, training frameworks.   \n27 While this direct approach to preference optimization is attractive in terms of its simplicity and   \n28 efficiency, it also raises important questions about the effectiveness and robustness of the resulting   \n29 policies\u2014as well as the broader utility of using an explicit reward model. In this paper, we argue that   \n30 explicit reward modeling can, in fact, offer substantial practical advantages that are not captured by   \n31 DPO\u2019s formulation. In particular, we theoretically show that relying solely on the preference data   \n32 can be a precarious strategy, with few natural brakes in place to prevent policies trained under the   \n33 DPO objective from careening off towards degenerate policies when the preference data exhibits   \n34 certain idiosyncratic properties. On the other hand, explicit reward models can easily be regularized   \n35 and understood\u2014regardless of whether they are Bradley-Terry models [4], margin-based ranking   \n36 models [40], or simply any other kind of function that correlates well with human preferences [31; 17].   \n37 Taking a step back from pure direct preference optimization, we propose a method that merges the   \n38 best of both worlds: an efficient reward model distillation algorithm that (i) operates effectively in the   \n39 offline setting, (ii) makes minimal assumptions about the true, optimal reward we aim to maximize,   \n40 and (iii) demonstrates greater robustness to the specific distribution of prompt/response data used for   \n4 policy alignment. Drawing inspiration from prior knowledge distillation techniques [14; 26; 35; 10],   \n42 we leverage the same change of variables trick employed in DPO to express the language model   \n43 policy in terms of its implicit reward model [23]. We then train the policy to match our desired,   \n44 explicit reward via an $L_{2}$ loss that directly regresses the pairwise differences in target rewards for   \n45 any two generation pairs $(x,y_{1})$ and $(x,y_{2})$ . We theoretically establish the equivalence between   \n46 optimizing this distillation loss over a sufficiently diverse offline dataset of unlabeled examples and   \n47 optimizing the traditional online RLHF objective.   \n48 Our reward model distillation approach, however, is not immune to some of the same challenges   \n49 facing DPO-style learning of policies. In particular, reward model distillation requires having a   \n50 reliable reward model\u2014but having a reliable reward requires having a reliable method for extracting   \n51 a reward model from a potentially noisy preference dataset. To address the uncertainty surrounding   \n52 the \u201cright\u201d reward model, we introduce a pessimistic extension to our approach. This extension aims   \n53 to maximize the worst-case improvement of our model across a plausible family of reward models   \n54 (e.g., those sufficiently consistent with annotated preference data). This strategy aligns with that of   \n55 existing work in conservative offline reinforcement learning [5; 16]. Interestingly, we derive that   \n56 this pessimistic objective can be equivalently expressed and optimized by adding a simple additional   \n57 KL-divergence regularization to the original distillation objective.   \n58 Empirically, we find that reward model distillation, particularly pessimistic reward model distillation,   \n59 leads to similar performance to prior direct preference optimization methods in settings where the   \n60 preference datasets used are unbiased, but significantly better performance in settings where the   \n61 preference datasets are biased, when compared to DPO and the Identity Preference Optimization   \n62 (IPO) framework of [3], which was introduced as a more robust alternative to DPO. To further support   \n63 these empirical observations, we provide an extensive theoretical analysis that both (i) sheds more   \n64 light on the degenerative tendencies of DPO and issues inherent to its objective, and (ii) highlights   \n65 relative advantages of our explicitly regularized approaches. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "66 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "67 We begin with a brief review of Direct Preference Optimization (DPO) [23] and its analysis. Proofs   \n68 of all theoretical results provided here, and in the rest of the paper, are deferred to Appendix A. ", "page_idx": 1}, {"type": "text", "text": "69 2.1 The preference alignment problem ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 Let $x$ be an input prompt, and let $y\\sim\\pi_{\\theta}(\\cdot\\mid x)$ be the language model policy $\\pi_{\\theta}$ \u2019s response to $x$ .   \n71 Given some reward function $r^{\\ast}(x,y)$ and another reference policy $\\pi_{\\mathrm{ref}}(y\\mid x)$ , the goal of alignment   \n72 is to solve for the \u201caligned\u201d policy $\\pi_{\\theta^{*}}(y\\mid x)$ that maximizes the following RLHF objective, i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pi_{\\theta^{*}}\\left(y\\mid x\\right)=\\operatorname*{argmax}_{\\pi_{\\theta}}\\mathbb{E}_{\\mu\\left(x\\right)}\\left[\\mathbb{E}_{\\pi_{\\theta}\\left(y\\mid x\\right)}[r^{*}(x,y)]-\\beta\\mathbb{D}_{\\mathrm{KL}}[\\pi_{\\theta}(\\cdot\\mid x)\\|\\pi_{\\mathrm{ref}}(\\cdot\\mid x)]\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "73 where $\\mu(x)$ is a fixed distribution over prompts, and the KL-divergence term prevents the aligned   \n74 policy from being dramatically different from the anchoring reference policy, $\\pi_{\\mathrm{ref}}(y\\mid x)$ . Here,   \n75 the reward function $r^{*}$ is typically not known in advance, but rather inferred from collected human   \n76 preference data in the form of $(x,y^{w},y^{\\ell})$ , where $x$ is the prompt, $y^{w}$ is the \u201cwinning\u201d, or preferred,   \n77 response, and $y^{\\ell}$ is the \u201closing\u201d, or dispreferred, response. A common approach is to assume that   \n78 pairs $(y_{1},y_{2})$ follow a Bradley-Terry model [4], under which the probability that $y_{1}$ is preferred to $y_{2}$   \ngiven the reward function $r^{*}$ and prompt $x$ is $p^{*}(y_{1}\\succ y_{2}\\mid x)\\stackrel{*}{=}\\sigma(r^{*}(\\dot{x},y_{1})\\stackrel{}{-}r^{*}(\\dot{x},y_{2}))$ , where   \n80 $\\sigma(\\cdot)$ is the sigmoid function and $\\succ$ denotes preference. Under this model, we can use the preference   \n81 data $(x,y^{w},y^{\\ell})\\sim\\mathcal{D}_{\\mathrm{pref}}$ to estimate $r^{*}$ via maximum likelihood estimation, i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{r}\\in\\mathop{\\mathrm{argmin}}_{r}\\mathbb{E}_{(y^{w},y^{\\ell},x)\\sim{\\mathcal{D}}_{\\mathrm{pref}}}\\left[-\\log\\sigma(r(x,y^{w})-r_{\\phi}(x,y^{\\ell}))\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "82 With $\\hat{r}$ in hand, Eq. (1) can be optimized using standard reinforcement learning algorithms [27; 29; 6]. ", "page_idx": 1}, {"type": "text", "text": "83 2.2 Direct preference optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "84 DPO is a simple approach for offline policy optimization that uses preferences to directly align the   \n85 language model policy, without training an intermediate reward model. Specifically, DPO leverages   \n86 the fact that the optimal solution to the KL-constrained objective in (1) takes the form [15] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{\\theta^{*}}(y\\mid x)=\\frac{1}{Z(x)}\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r^{*}(x,y)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "87 where $\\begin{array}{r}{Z(x)=\\sum_{y}\\pi_{\\mathrm{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r^{\\ast}(x,y))}\\end{array}$ is the partition function. DPO reparameterizes the   \n88 true reward function $r^{*}$ in terms of the optimal policy $\\pi_{\\theta^{*}}$ that it induces, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\nr^{*}(x,y)=\\beta\\log\\left({\\frac{\\pi_{\\theta^{*}}(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}}\\right)+\\beta\\log Z(x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "89 Under the Bradley-Terry model, the likelihood that $y_{1}\\succ y_{2}$ can then be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\np^{*}(y_{1}\\succ y_{2}\\mid x)=\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta^{*}}(y_{1})\\pi_{\\mathrm{ref}}(y_{2})}{\\pi_{\\theta^{*}}(y_{2})\\pi_{\\mathrm{ref}}(y_{1})}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "90 where now $\\pi_{\\theta^{*}}$ can be directly estimated on $\\mathcal{D}_{\\mathrm{pref}}$ following the objective in (2), in place of the   \n91 intermediate reward model $\\hat{r}$ , i.e., $\\begin{array}{r}{\\pi_{\\hat{\\theta}}(y\\mid x)\\in\\mathop{\\mathrm{argmin}}_{\\pi_{\\theta}}\\mathcal{L}_{\\mathrm{dpo}}(\\pi_{\\theta};\\mathcal{D}_{\\mathrm{pref}})}\\end{array}$ where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{dpo}}(\\pi_{\\theta};\\mathcal{D}_{\\mathrm{pref}})=\\mathbb{E}_{(y^{w},y^{\\ell},x)\\sim\\mathcal{D}_{\\mathrm{pref}}}\\left[-\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta^{*}}(y^{w})\\pi_{\\mathrm{ref}}(y^{\\ell})}{\\pi_{\\theta^{*}}(y^{\\ell})\\pi_{\\mathrm{ref}}(y^{w})}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "92 2.3 Pitfalls of direct preference optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 As argued in [3], the Bradley-Terry assumption that DPO strongly relies on for maximum likelihood   \n94 estimation is sensitive to the underlying preference data. Specifically, if we have any two responses $y_{1}$   \n95 and $y_{2}$ where $p^{*}(y_{1}\\succ y_{2}\\mid x)=1$ , then the Bradley-Terry model dictates that $r^{*}(\\bar{y_{1}}){-}r^{*}(\\bar{y_{2}})=+\\bar{\\infty}$ ,   \n96 and therefore $\\pi_{\\theta^{*}}(y_{2}\\mid x)=0$ for any finite KL-regularization strength $\\beta$ . ", "page_idx": 2}, {"type": "text", "text": "97 We can illustrate this phenomenon on a broader level with the following example. ", "page_idx": 2}, {"type": "text", "text": "98 Assumption 1. Suppose we are given a preference dataset of (context-free) pairs $\\begin{array}{r l}{\\mathcal{D}_{\\mathrm{pref}}}&{{}=}\\end{array}$   \n99 $\\{(y_{i}^{w},y_{i}^{\\ell})\\}_{i=1}^{n}$ , the pairs $(y_{i}^{w},y_{i}^{\\ell})$ are mutually disjoint in both the elements. Further suppose   \n100 that we optimize the DPO objective on $\\mathcal{D}_{\\mathrm{pref}}$ with a single parameter $\\theta_{y}$ for each $y$ .   \n101 Proposition 1. Under Assumption $^{\\,l}$ , for any $(y,y^{\\prime})$ such that $y=y_{i}^{w}$ and $y^{\\prime}=y_{i}^{\\ell}$ for some $i$ , we   \n102 have \u03c0\u03c0\u03b8\u2217((yy\u2032))\u03c0\u03c0ref((yy\u2032)) , for all global minimizers $\\pi\\theta^{*}$ of the DPO objective in (6), for any $\\beta>0$ .   \n103 Corollary 1. Under Assumption $^{\\,l}$ , further assume that $0<\\pi_{\\mathrm{ref}}(y)<1$ for all $y$ . Then $\\pi_{\\theta^{*}}$ is $a$   \n104 global minimizer of the DPO objective in (6) iff $\\pi_{\\theta^{*}}(\\mathcal{C}(y^{\\ell})^{c})\\to1$ with $\\pi_{\\theta^{*}}(y_{i}^{w})>0\\,\\forall i\\in[n].$ , where   \n105 $\\mathcal{C}(y^{\\ell})^{c}$ is the complement of the set of all responses $y$ that appear as a dispreferred $y_{i}^{\\ell}$ for any $i\\in[n]$ .   \n106 Additional analysis of the training dynamics of DPO is also provided in $\\S5$ . A significant, and non  \n107 obvious, implication of Corollary 1 is that the set of global optima of the DPO loss also includes poli  \n108 cies that can shift nearly all probability mass to responses that never even appear in the training set\u2014   \n109 and even assign near zero probability to all of the training data responses that do in fact correspond to   \n110 winning generations, $y^{w}$ , a phenomenon that has been observed empirically [e.g., 20]. Stated differ  \n111 ently, Corollary 1 implies that any $\\theta^{*}$ merely satisfying $\\pi_{\\theta^{*}}(y_{i}^{\\ell})=0$ with $\\pi_{\\theta^{*}}(y_{i}^{w})>0\\,\\forall i\\in[n]$ is a   \n112 global minimizer of the DPO objective in this setting. Though simplistic, the scenario in Assumption 1   \n113 is closer to reality than might first be appreciated: in many practical situations we can almost always   \n114 expect the finite-sample preference data to contain one (or at most a few) preference annotations per   \n115 example $\\left(x,y_{1},y_{2}\\right)$ , while the policies $\\pi_{\\theta}$ can have billions of parameters $(\\gg n)$ . Of course, this issue   \n116 can also be viewed as a classic instance of overftiting\u2014with the additional caveat that as opposed to   \n117 overpredicting responses within the training set, we might overfti to almost never producing anything   \n118 like the \u201cgood\u201d responses that do appear within the training set. Furthermore, without additional regu  \n119 larization (beyond $\\beta$ ), we can expect this degeneration to easily happen in typical preference datasets. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "120 3 Uncertainty-aware reward model distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "121 As discussed in the previous section, a core issue in preference optimization is that the true preference   \n122 distribution $p^{*}(y_{1}\\succ y_{2}\\mid x)$ is not known. Attempting to infer it from finite-sample preference data   \n123 (that may further be biased or out-of-distribution with respect to the target domain) can then result   \n124 in a failure to learn reasonable policies. In this section, we now propose an inherently regularized   \n125 approach to direct preference optimization that uses uncertainty-aware reward model distillation. ", "page_idx": 3}, {"type": "text", "text": "126 3.1 Reward model distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 Suppose for the moment that the reward function $r^{*}$ was in fact known, and did not have to be   \n128 inferred from sampled preference data. Under this setting, we can then define an efficient offline   \n129 optimization procedure that is similar in spirit to DPO, but no longer relies directly on a preference   \n130 dataset. Concretely, given unlabeled samples $(x,y_{1},y_{2})\\sim\\rho$ (where the number of samples can be   \n131 potentially unlimited), we can define a simple \u201cdistillation\u201d loss, $\\mathcal{L}_{\\mathrm{distill}}(r^{*},\\pi_{\\theta})$ , as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{distill}}(r^{*},\\pi_{\\theta};\\rho)=\\mathbb{E}_{\\rho(x,y_{1},y_{2})}\\left[\\left(r^{*}(x,y_{1})-r^{*}(x,y_{2})-\\beta\\log\\frac{\\pi_{\\theta}(y_{1}\\mid x)\\pi_{\\mathrm{ref}}(y_{2}\\mid x)}{\\pi_{\\theta}(y_{2}\\mid x)\\pi_{\\mathrm{ref}}(y_{1}\\mid x)}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 Intuitively, the distillation loss seeks to exactly match differences in reward model scores across   \n133 all generation pairs $\\left(x,y_{1},y_{2}\\right)$ . It is then easy to see that under the Bradley-Terry model, this is   \n134 equivalent to matching the strength of the preference relationship, $y_{1}\\succ y_{2}$ . Furthermore, by only   \n135 matching differences, we can still conveniently ignore the log partition term, $\\log Z(x)$ , in the implicit   \n136 reward formulation for $\\pi_{\\theta}$ as shown in (4), as it is constant across different $y$ for any given $x$ . Finally,   \n137 similar to the motivation in DPO, we can show that minimizing $\\mathcal{L}_{\\mathrm{distill}}(r^{*},\\pi_{\\theta};\\rho)$ indeed results in an   \n138 optimally aligned policy $\\pi\\theta^{*}$ , as long as the data distribution $\\rho$ has sufficient support.   \n139 Theorem 1. Let $\\boldsymbol{\\wp}$ denote the set of all possible responses for any model $\\pi_{\\theta}$ . Assume that   \n140 $\\operatorname{supp}(\\pi_{\\operatorname{ref}}(y\\mid x))=\\mathcal{Y}$ , i.e., the reference policy may generate any outcome with non-zero probability.   \n141 Further, let $\\begin{array}{r}{\\mathrm{supp}(\\rho(x,y_{1},y_{2}))=\\mathrm{supp}(\\mu(x))\\times\\mathcal{V}\\!\\times\\!\\mathcal{V}.}\\end{array}$ . Let $\\begin{array}{r}{\\pi_{\\boldsymbol\\theta^{\\ast}}(\\boldsymbol{y}\\mid\\boldsymbol{x})\\in\\mathrm{argmin}_{\\pi_{\\boldsymbol\\theta}}\\,\\mathcal{L}_{\\mathrm{distill}}(\\boldsymbol{r}^{\\ast},\\pi_{\\boldsymbol\\theta};\\boldsymbol{\\rho})}\\end{array}$   \n142 be a minimizer over all possible policies, of the implicit reward distillation loss in (7), for which   \n143 $r^{\\ast}(x,y)$ is assumed to be deterministic, and finite everywhere. Then for any $\\beta~>~0$ , $\\pi\\theta^{*}$ also   \n144 maximizes the alignment objective in (1).   \n145 The above result holds for a broad class of data distributions $\\rho(x,y_{1},y_{2})$ , and makes no assumptions   \n146 on $r^{*}$ (e.g., it is no longer necessary for it to be defined using a Bradley-Terry model). In fact, this   \n147 result can also be seen as strict generalization of the IPO framework of [3] when taking $r^{*}(x,y)\\triangleq$   \n148 $\\mathbf{1}\\{y=y_{w}\\}$ , if labeled pairs $(x,y_{w},y_{l})$ are provided instead of the unlabeled pairs $(x,y_{1},y_{2})$ .   \n149 Of course, the true reward $r^{*}$ is usually not known in practice. Still, as in standard RLHF, we can   \n150 go about constructing good proxies by using the preference data to identify plausible target reward   \n151 models $r_{\\mathrm{tgt}}$ \u2014further guided by any amount of regularization and inductive bias that we desire. A   \n152 natural choice is to first learn $r_{\\mathrm{tgt}}$ on the preference data $\\mathcal{D}_{\\mathrm{pref}}$ using standard methods, and then reuse   \n153 $\\mathcal{D}_{\\mathrm{pref}}$ to distill $\\pi_{\\theta}$ , which is similar to classical settings in teacher-based model distillation [14; 26].   \n154 Furthermore, as $r_{\\mathrm{tgt}}$ is a real-valued model, at a bare minimum it is guaranteed to induce a regularized   \n155 Bradley-Terry preference distribution $p_{\\mathrm{tgt}}(y_{1}\\succ y_{2}\\mid x)>0$ , $\\forall x,y_{1},y_{2}\\in\\mathcal{X}\\times\\mathcal{Y}$ , and thereby avoid   \n156 some of the degeneracies identified in $\\S2.3$ for the maximum likelihood estimate under DPO. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "157 3.2 Pessimistic reward model distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "158 Choosing a single reward model $r_{\\mathrm{tgt}}$ for anchoring the LM policy can naturally still lead to degenerate   \n159 behavior if $r_{\\mathrm{tgt}}$ is a poor approximation of the true $r^{*}$ that accurately reflects human preferences.   \n160 However, we can easily extend our framework to handle uncertainty in the right target reward function   \n161 by defining a confidence set of $k\\geq1$ plausible target reward models, rtgt, . . . , rtgt , and   \n162 training $\\pi_{\\theta^{*}}(y\\mid x)$ to maximize the following \u201cpessimistic\u201d form of the objective in (1): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi_{\\theta}}\\operatorname*{min}_{r_{\\mathrm{tef}}^{i}\\in S}\\mathbb{E}_{\\mu(x)}\\Big[\\underbrace{\\mathbb{E}_{\\pi_{\\theta}(y|x)}[r_{\\mathrm{tgt}}^{i}(x,y)]-\\mathbb{E}_{\\pi_{\\mathrm{ref}}(y|x)}[r_{\\mathrm{tgt}}^{i}(x,y)]}_{\\mathrm{~}}-\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\theta}(\\cdot\\mid x)\\|\\pi_{\\mathrm{ref}}(\\cdot\\mid x))\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "163 In this pessimistic objective we are no longer op  \n164 timizing $\\pi_{\\theta}$ for a single reward, but optimizing   \n165 $\\pi_{\\theta}$ to produce generations that are scored favor  \n166 ably on average, even by the worst-case reward   \n167 model in the set $\\boldsymbol{S}$ , relative to the generations of   \n168 the baseline policy $\\pi_{\\mathrm{ref}}$ .When the set $\\boldsymbol{S}=\\{r^{*}\\}$   \n169 consists of only the ground-truth reward, the ob  \n170 jective (8) is equivalent to standard RLHF (1),   \n171 up to a constant offset independent of $\\theta$ . More   \n172 generally, whenever $\\boldsymbol{S}$ includes a good proxy   \n173 $\\widetilde r$ for $r^{*}$ , the pessimistic advantage evaluation   \n174 ensures that the the policy $\\pi_{\\theta}^{*}$ that maximizes   \n175 eq. (8) still has a large advantage over $\\pi_{\\mathrm{ref}}$ under   \n176 all $r\\,\\in\\,S$ , including $\\widetilde r$ . This use of pessimism   \n177 to handle uncertainty in the knowledge of the   \n178 true reward is related to similar techniques in   \n179 the offline RL literature [16; 5]. ", "page_idx": 4}, {"type": "image", "img_path": "VajjTXRj6J/tmp/a00a8e8e2e8ae1b0af4abdf0a28fa2055152c5f628b0e352dcf75189f7d8da82.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: A toy illustration of Theorem 2, which states that the optimal $\\pi_{\\theta^{*}}$ for (8) is the policy in $\\mathcal{P}_{\\beta}(S)$ with the lowest forward-KL from $\\pi_{\\mathrm{SFT}}$ . The set ${\\mathcal{P}}_{\\beta}(S)$ contains a (potentially infinite) set of policies $\\pi_{1},\\pi_{2},\\ldots$ corresponding to target reward models. Here, $\\pi_{\\mathrm{SFT}}$ assigns equal mass to $y^{w}$ and $y^{\\ell}$ , $\\pi_{\\mathrm{MLE}}$ is the MLE solution for the DPO objective, which puts all probability mass on $y^{w}$ , and $\\pi_{3}$ is the policy in $\\mathcal{P}_{\\beta}(S)$ with lowest forward-KL. ", "page_idx": 4}, {"type": "text", "text": "180 For the objective to be meaningful, the set $\\boldsymbol{S}$ has to be chosen carefully. When $\\boldsymbol{S}$ is small, it might   \n181 not include any good proxy for $r^{*}$ . Conversely, if $\\boldsymbol{S}$ is too rich, it forces $\\pi_{\\theta^{*}}$ to be nearly identical to   \n182 $\\pi_{\\mathrm{ref}}$ , since any deviations from $\\pi_{\\mathrm{ref}}$ might be penalized by some reward model in $\\boldsymbol{S}$ . Consequently,   \n183 we want to design $\\boldsymbol{S}$ to be the smallest possible set which contains a reasonable approximation to $r^{*}$ .   \n184 To optimize (8), it turns out that we can formulate it as an equivalent constrained offilne optimization   \n185 problem, that we will show to conveniently admit a similar loss form as (7). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "186 Theorem 2 (Pessimistic distillation). Define the constrained minimizer ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\theta^{*}}\\!\\left(y\\mid x\\right)\\in\\operatorname*{argmin}_{\\pi_{\\theta}\\in\\mathcal{P}_{\\beta}(S)}\\beta\\mathbb{E}_{\\mu(x)}\\mathbb{D}_{\\mathrm{KL}}\\big(\\pi_{\\mathrm{ref}}(\\cdot\\mid x)\\big\\|\\pi_{\\theta}\\big(\\cdot\\mid x\\big)\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "187 where $\\mathcal{P}_{\\beta}(S)$ is the set of all possible policies with implicit reward models that are consistent with   \n188 any target reward model $r_{\\mathrm{tgt}}^{i}\\in\\mathcal{S}$ , i.e., $\\mathcal{P}_{\\beta}(S)\\triangleq\\{\\pi_{\\theta_{i}}\\}_{i=1}^{|S|}$ where $\\pi_{\\theta_{i}}\\propto\\pi_{\\mathrm{ref}}(y\\mid x)\\exp{\\textstyle{\\frac{1}{\\beta}}r_{\\mathrm{tgt}}^{i}(x,y)}$ .   \n189 Then for any $\\beta>0$ , $\\pi_{\\theta^{*}}$ also maximizes the pessimistic alignment objective in (8).   \n190 To unpack this result, Theorem 2 stipulates that the $\\pi_{\\theta}$ that maximizes the pessimistic objective in (8)   \n191 is the policy in $\\mathcal{P}_{\\beta}(S)$ that is closest in forward KL-divergence to $\\pi_{\\mathrm{ref}}$ (see Figure 1).1In addition,   \n192 this policy also maximizes the expected reward of one of the $r_{\\mathrm{tgt}}^{i}\\in\\mathcal{S}$ (minus the additional weighted   \n193 reverse KL-divergence penalty term). Intuitively, the forward KL-divergence term serves the role of   \n194 biasing the model towards optimizing for reward models that are similar to the implicit reward that   \n195 $\\pi_{\\mathrm{ref}}$ already maximizes. Otherwise, there might exist a target reward model rtigt\u2208S for which the   \n196 advantage of $\\pi_{\\theta}$ relative to $\\pi_{\\mathrm{ref}}$ will be low, or even negative (a solution that we would like to avoid). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "197 3.2.1 Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "198 The constraint in (9) can then be relaxed and approximately optimized by introducing an objective   \n199 with a Lagrangian-style penalty with strength $\\alpha>0$ on a form of distillation loss as (7), i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{\\theta}}\\beta\\mathbb{E}_{\\mu(x)}\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\mathrm{ref}}(y\\mid x)\\|\\pi_{\\theta}(y\\mid x))+\\alpha\\operatorname*{min}_{r_{\\mathrm{tgt}}^{i}\\in\\mathcal{S}}\\mathcal{L}_{\\mathrm{distill}}(r_{\\mathrm{tgt}}^{i},\\pi_{\\theta};\\rho),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "200 where in practice we divide by $\\alpha$ and instead optimize2 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{pdistill}}(S,\\pi_{\\theta};\\rho)=\\operatorname*{min}_{r_{\\mathrm{tgt}}^{i}\\in S}\\mathcal{L}_{\\mathrm{distill}}(r_{\\mathrm{tgt}}^{i},\\pi_{\\theta};\\rho)+\\gamma\\mathbb{E}_{\\mu(x)}\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\mathrm{ref}}(\\cdot\\mid x)\\|\\pi_{\\theta}(\\cdot\\mid x)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "201 where $\\gamma\\,=\\,\\beta\\alpha^{-1}$ . In reality, minimizing (11) for $\\gamma>0$ is equivalent to solving the constrained   \n202 optimization problem in (9) with an implicitly larger set of possible reward models $\\mathcal{S}_{\\gamma}\\supseteq S$ indexed   \n203 by $\\gamma$ . More specifically, $\\mathcal{S}_{\\gamma}$ also contains all reward models $\\tilde{r}$ that are approximately consistent with   \n204 the anchoring reward models $r_{\\mathrm{tgt}}^{i}$ contained in $\\boldsymbol{S}$ , as the following result states.   \n205 Proposition 2 (Soft pessimistic distillation). Assume the same conditions as Theorem $^{\\,l}$ . Then for   \n206 any $0<\\gamma<\\infty$ , there exists a $\\lambda\\geq0$ such that $\\begin{array}{r}{\\pi_{\\theta^{*}}(y\\mid x)\\in\\mathrm{argmin}_{\\pi_{\\theta}}\\,\\mathcal{L}_{\\mathrm{pdistill}}(S,\\pi_{\\theta};\\rho)}\\end{array}$ , where $\\pi_{\\theta^{\\ast}}$   \n207 is a minimizer over all possible policies, is a solution to (9) for the effective reward model set ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{\\gamma}=\\bigcup_{r_{\\mathrm{tgt}}^{i}\\in S}\\Big\\{\\tilde{r}\\colon\\mathbb{E}_{\\rho(x,y_{1},y_{2})}\\left[(r_{\\mathrm{tgt}}^{i}(x,y_{1})-r_{\\mathrm{tgt}}^{i}(x,y_{2})-\\tilde{r}(x,y_{1})+\\tilde{r}(x,y_{2}))^{2}\\right]\\leq\\lambda\\Big\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "208 As a result, optimizing (11) even when using the singleton $S=\\{r_{\\mathrm{tgt}}\\}$ yields an implicitly pessimistic   \n209 objective, in which the pessimism is over all reward models $\\tilde{r}$ that are consistent up to $\\lambda$ with $r_{\\mathrm{tgt}}$ . ", "page_idx": 5}, {"type": "text", "text": "210 3.3 Pessimistic DPO ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "211 We can also observe that Proposition 2 can be leveraged to obtain an alternative, implicitly pessimistic,   \n212 objective that uses DPO directly instead of distillation. Consider the following regularized DPO loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{pdpo}}(\\pi_{\\theta};\\mathcal{D}_{\\mathrm{pref}})=\\mathcal{L}_{\\mathrm{dpo}}(\\pi_{\\theta};\\mathcal{D}_{\\mathrm{pref}})+\\gamma\\mathbb{E}_{\\mu(x)}\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\mathrm{ref}}(y\\mid x)\\|\\pi_{\\theta}(y\\mid x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "213 Following a similar analysis as in Proposition 2, we can derive that this implicitly corresponds to   \n214 maximizing the pessimistic objective in (8) for the reward model set ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\gamma}=\\Bigl\\{r_{\\pi_{\\theta}}:\\mathcal{L}_{\\mathrm{dpo}}(\\pi_{\\theta};\\mathcal{D}_{\\mathrm{pref}})\\leq\\underset{\\pi_{\\theta}^{\\prime}}{\\operatorname*{min}}\\mathcal{L}_{\\mathrm{dpo}}(\\pi_{\\theta}^{\\prime};\\mathcal{D}_{\\mathrm{pref}})+\\lambda\\Bigr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "215 where $r_{\\pi_{\\theta}}(x,y)\\triangleq\\beta\\log\\pi_{\\theta}(y\\mid x)/\\pi_{\\mathrm{ref}}(y\\mid x)+\\beta\\log Z(x)$ is the implicit reward model defined by   \n216 $\\pi_{\\theta}.\\,S_{\\gamma}$ then corresponds to the set of reward models $r_{\\pi_{\\theta}}$ that are all approximate minimizers of the   \n217 DPO loss. This not only includes the MLE, but also all other estimators that obtain nearly the same   \n218 loss. In principle, this can be expected to help ameliorate some of the issues of $\\S2.3$ : since driving the   \n219 reward to $\\pm\\infty$ only marginally decreases the $\\mathcal{L}_{\\mathrm{dpo}}$ loss past a certain point, the set $\\boldsymbol{S}$ will also include   \n220 finite reward functions $\\bar{|r_{\\pi_{\\theta}}}(x,y)|<\\infty$ for any $\\gamma>0$ . These rewards would then be preferred if they   \n221 induce a policy with a smaller (forward) KL-divergence to $\\pi_{\\mathrm{ref}}$ than the degenerate, infinite rewards. ", "page_idx": 5}, {"type": "text", "text": "222 4 Experimental results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "223 The main motivation for reward distillation and pessimism is to increase alignment robustness   \n224 in challenging settings where it is difficult to learn good policies directly from the preference   \n225 data. To demonstrate the effectiveness of our approach, we run experiments on the popular TL;DR   \n226 summarization task [29; 32], in which we simulate a scenario where the preference data has a spurious   \n227 correlation between the length of a summary and whether or not it is preferred.3 ", "page_idx": 5}, {"type": "text", "text": "228 4.1 Experimental setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "229 We first train an \u201coracle\u201d reward model on the TL;DR preference data training set [29] and relabel   \n230 all preference pairs with this oracle. This enables us to use the oracle reward model for evaluation,   \n231 without worrying about the gap to true human preferences. After relabeling, longer responses (where   \n232 longer is defined as $y_{1}$ having at least $10\\%$ more tokens than $y_{2}$ ) are preferred in $\\bar{6}1\\%$ of the examples.   \n233 To test the effect of a spurious correlation on preference-based policy optimization, we select as a   \n234 training set 30K examples from the relabeled data such that the longer output is preferred in $\\rho$ fraction   \n235 of examples, with $\\rho\\ {\\stackrel{.}{\\in}}\\ \\{0.2,0.3,0.4,0.5,0.6,0.7,0.8\\}$ . Each such training set is denoted $\\mathcal{D}_{\\rho}$ . At   \n236 each $\\mathcal{D}_{\\rho}$ , we compare our approach to DPO [23] and IPO [3], which are currently the most commonly   \n237 used offline alignment methods. We test the following variants of distillation and pessimism:   \n238 \u2022 Distilled DPO (d-DPO): Trains a reward model $r_{\\rho}$ on $\\mathcal{D}_{\\rho}$ , and then optimizes $\\mathcal{L}_{\\mathrm{distill}}(\\boldsymbol{r}_{\\rho},\\pi_{\\theta};\\rho)$ .   \n239 \u2022 Pessimistic DPO (p-DPO): A pessimistic version of DPO as described in $\\S3.3$ , trained on $\\mathcal{D}_{\\rho}$ .   \n240 \u2022 Pessimistic Distilled DPO (pd-DPO): Combines the above two by training a reward model $r_{\\rho}$ on   \n241 $\\mathcal{D}_{\\rho}$ and optimizing the pessimistic distillation objective (Eq. (11)) with confidence set $\\begin{array}{r}{S=\\{r_{\\mathrm{tgt}}\\}}\\end{array}$ .   \n242 \u2022 Pessimistic Ensemble DPO (e-DPO): To create ensembles of reward models, we subsample from   \n243 each $\\mathcal{D}_{\\rho}$ five preference datasets, $\\mathcal{D}_{\\rho,b}$ , at $b\\in B=\\{0.2,0.4,0.5,0.6,0.8\\}$ , such that the fraction   \n244 of pairs where the longer response is preferred is $b$ , and train reward models $r_{\\rho,b}$ on those subsets.   \n245 Consequently, sensitivity to length should vary across ensemble members. We then apply the   \n246 same procedure as pd-DPO above, with a confidence set $\\boldsymbol{S}_{\\rho}=\\{r_{\\rho,b}\\}_{b=1}^{\\beta}$ .   \n247 All reward models and policies are initialized from Palm-2-XS [2]. Policies also go through a   \n248 supervised finetuning step on human-written summaries from the original TL;DR training set [32]   \n249 prior to alignment, and we term this policy $\\pi_{\\mathrm{SFT}}$ . We evaluate performance by sampling summaries   \n250 for test set prompts, evaluating the average reward according to the oracle reward model, and   \n251 computing the advantage in average reward compared to $\\pi_{\\mathrm{SFT}}$ (before alignment). We train policies   \n252 for $\\bar{1}0^{4}$ steps with batch size 16 and learning rate $10^{-6}$ , and reward models for $3k$ steps with   \n253 batch size 64 and learning rate $4\\times10^{-6}$ . We use the validation set for model selection during   \n254 policy training and to choose the following hyperparameters. For all DPO variants, we sweep over   \n255 $\\beta\\,\\in\\,\\{.01,.1,1,3,10,30,100\\}$ . For IPO, we sweep over $\\tau\\,\\in\\,\\{0.01,0.1,1,3,5,10,25\\}$ . For all   \n256 pessimistic methods we anneal $\\gamma=\\alpha/\\beta$ from $10^{-4}$ to $10^{-2}$ linearly during the $10k$ training steps. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "VajjTXRj6J/tmp/0581ee5d2f71d55e9ff32e3c56eb36fcd627793d1bf3660fd3537bd4b3962f3a.jpg", "img_caption": ["Figure 2: Main results, showing the advantage in oracle reward compared to the initial finetuned policy. Errorbars correspond to bootstrap $95\\%$ confidence intervals for finite sample variance. Ensemble DPO (e-DPO) is significantly better than DPO and IPO in the challenging setup where shorter responses are preferred $(\\rho\\leq0.5)$ , and is generally the best-performing method overall in this regime. Distilled DPO (d-DPO) performs best when longer responses are preferred $(\\rho>0.6)$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "257 4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "258 We present the results of our experiment in Figure 2. As can be seen in the plot, the more challenging   \n259 setting is when $\\rho<0.5$ , which corresponds to a sample of preference annotations in which shorter   \n260 outputs are generally preferred. This distribution shift is more difficult because as mentioned the oracle   \n261 reward model (trained on human annotations) has a bias in favor of longer outputs [28]. Nevertheless   \n262 we get sizable improvements compared to the reference policy $\\pi_{\\mathrm{SFT}}$ for all length bias values.   \n263 All approaches that invoke distillation (d-DPO, e-DPO, dp-DPO) outperform IPO and DPO $(p<.01$   \n264 by a Wald test) for $\\rho\\,\\leq\\,0.5$ , where shorter responses are preferred. Pessimistic ensemble DPO   \n265 (e-DPO) performs particularly well in these settings, generally outperforming all methods that use   \n266 a single reward model. When longer responses are preferred $(\\rho>0.6)$ , single reward distillation   \n267 (d-DPO) leads to the highest performance, significantly outperforming both DPO and IPO $(p<.01$   \n268 by a Wald test). Interestingly, p-DPO does not provide empirical benefits relative to the distillation   \n269 based methods, indicating that the distillation loss itself is quite important. For the effect of   \n270 hyper-parameter selection, see Figure D.1. In DPO-based methods, the optimal value of $\\beta$ is inversely   \n271 correlated with the bias; in IPO the same holds for the $\\tau$ hyperparameter.   \n272 To better understand the utility of reward ensembles in e-DPO, in particular when $\\rho\\,<\\,0.5$ , we   \n273 examine the role of each reward model in the ensemble across different biases. Specifically, given   \n274 the final e-DPO policy per length bias, for each example we identify the reward model $r_{\\rho,b}$ that   \n275 best matches the implicit reward of this policy, i.e., for which reward model is $\\mathcal{L}_{\\mathrm{distill}}$ minimized on   \n276 that example (see Eq. (7) and (11)). We find that when the policy is trained on data where shorter   \n277 preference are preferred $(\\rho<.5)$ , the reward model that best matches the policy often has the opposite   \n278 bias $\\mathit{b}$ is high), and vice versa. Thus, the success of e-DPO may be explained by its ability to distill   \n279 from reward models that do not suffer from the bias in the policy training data, which is particularly   \n280 helpful when $\\rho\\leq.5$ as this bias is also not shared by the oracle RM. We provide the full distribution   \n281 over reward models for all $\\rho$ and $\\beta$ in App. C. Overall, these results demonstrate the efficacy of   \n282 training a policy by distilling from a reward model in the presence of distribution shifts, and that a   \n283 careful design of an ensemble to mitigate spurious correlations can lead to further performance gains.4 ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "284 5 Theoretical analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "285 This section characterizes problems with the DPO objective and solutions offered by pessimistic DPO   \n286 and distillation, focusing on the simplified scenario in which we optimize with respect to a single   \n287 preference pairs $(y^{w},y^{\\ell})$ . Once again, all proofs are deferred to Appendix A.   \n288 In its Lagrangian formulation, pessimistic DPO adds a forward KL term to the DPO objective (\u00a73.3).   \n289 For the sake of analysis, we assume that the preference annotations are sampled from the reference   \n290 distribution, $\\mu(x)\\stackrel{.}{\\times}\\pi_{\\mathrm{ref}}(y\\mid x)\\times\\pi_{\\mathrm{ref}}(y\\mid x)$ . Then a finite-sample approximation of the forward   \n291 KL term is $\\begin{array}{r}{\\hat{\\Omega}(\\Theta):=\\sum_{(y^{w},y^{\\ell})\\in{\\mathcal D}_{\\mathrm{Pref}}}-\\left(\\log\\pi_{\\theta}(y^{\\ell})+\\log\\pi_{\\theta}(y^{w})\\right)}\\end{array}$ . By applying this finite-sample   \n292 approximation, $p$ -DPO has a finite optimum, unlike DPO, as shown in Proposition 1. Note that this   \n293 analysis is limited in two ways: (1) as mentioned, we compute the KL term over the completions   \n294 in the preference data; (2) we directly optimize the probability ratios $\\psi_{w}=\\pi_{\\theta}(y^{w})/\\pi_{\\mathrm{ref}}(\\bar{y}^{w})$ and   \n295 $\\psi_{\\ell}=\\bar{\\pi}_{\\theta}(y^{\\ell})/\\pi_{\\mathrm{ref}}(y^{\\ell})$ , rather than optimizing them jointly through the parameters. For sufficiently ex  \n296 pressive $\\pi_{\\theta}$ , however, this approximation captures the behavior of the two algorithms reasonably well.   \n297 Proposition 3. Let $\\hat{\\mathcal{L}}_{\\mathrm{pdpo}}$ represent a finite-sample approximation to $\\mathcal{L}_{\\mathrm{pdpo}}$ with the empir  \n298 ical forward $K L$ term $\\hat{\\Omega}(\\Theta)$ . For a fixed $\\hat{\\pi}_{\\boldsymbol{\\theta}}(y_{i}^{w})$ and $\\alpha\\mathrm{~\\ensuremath~{~>~}~}1$ , the $\\mathrm{argmin}_{\\pi_{\\theta}(y^{\\ell})}\\,\\hat{\\mathcal{L}}_{\\mathrm{pdpo}}$ is   \n299 min 1 \u2212\u03c0\u02c6\u03b8(yiw ), \u03c0\u02c6\u03b8(yi\u2113) , with log \u03c0\u02c6\u03b8(yi\u2113) = \u2212\u03b21 log (\u03b1 \u22121) + log \u03c0\u02c6\u03b8(yiw ) + log \u03c0\u03c0rreeff((yyiiw\u2113  )).   \n300 The optimum in Proposition 3 corresponds to $\\log\\psi_{w}/\\psi_{\\ell}=\\beta^{-1}\\log(\\alpha-1)$ . Recall that IPO seeks   \n301 to assign a constant value to this ratio by minimizing $\\begin{array}{r}{(\\log\\frac{\\psi_{w}}{\\psi_{\\ell}}-\\tau^{-1})^{2}}\\end{array}$ ; the (unconstrained) optima   \n302 are identical for $\\tau^{-1}:=\\beta^{-1}\\log(\\alpha-1)$ , but the loss surfaces are different (see Appendix B). DPO   \n303 sets $\\pi_{\\theta}(y_{i}^{\\ell})\\to0$ , as shown in Corollary 1; this is due not only to competition from $\\pi_{\\theta}(y_{i}^{w})$ but from   \n304 DPO penalizing positive probability on $y_{i}^{\\ell}$ . Analysis of the distilled loss gives a similar result:   \n305 Proposition 4. For any fixed $\\hat{\\pi}_{\\boldsymbol{\\theta}}(y_{i}^{u v})$ and $\\beta>0$ , the argmin of the distilled DPO objective (eq. (7))   \n306 is min(1\u2212\u03c0\u02c6\u03b8(yiw ), \u03c0\u02c6\u03b8(yi\u2113), with log \u03c0\u02c6\u03b8(yi\u2113) = \u03b21 (rt(x, yi\u2113)\u2212rt(x, yiw ))+log \u03c0\u02c6\u03b8(yiw )+log \u03c0\u03c0rreeff((yyiiw\u2113  )).   \n307 While the setting is simplistic, the results are comforting: here the additional regularization effects of   \n308 both distillation and pessimism (in the case of $\\mathbf{p}$ -DPO) clearly help to avoid degenerate optima.   \n309 Why DPO can drive $\\pi(y^{w})$ to zero. In $\\S2.3$ we pointed out a peculiarity of the DPO global optima:   \n310 in certain cases, it can include policies where $\\pi(y^{w})$ may be nearly 0 for all $y^{w}$ in the training set. This   \n311 undesirable behavior has also been observed in practice [20; 22; 30]. For intuition on why this may   \n312 happen, consider the simplified case where the policy is a bag-of-words model, $\\pi_{\\theta}(y)\\propto\\exp\\left(c(y)\\cdot\\theta\\right)$   \n313 for $c(y)$ representing a vector of counts in $y$ and $\\theta_{i}$ representing the unnormalized log-probability of   \n314 token $i$ . Then we can formally show that DPO optimization monotonically decreases an upper bound   \n315 on the probability of the preferred completion, $\\tilde{\\pi}_{\\theta^{(t-1)}}(y^{w})\\geq\\tilde{\\pi}_{\\theta^{(t)}}(y^{w})\\geq\\pi_{\\theta^{(t)}}(y^{w})$ .   \n316 Proposition 5. Let $y^{w},y^{\\ell}~\\in~\\mathcal{V}^{n}$ be preferred vs. dispreferred outputs of length $n_{i}$ , with   \n317 $\\pi_{\\mathrm{ref}}\\bar{(}y^{w})$ , $\\pi_{\\mathrm{ref}}(y^{\\ell})>\\breve{0}$ and corresponding count vectors $c(\\dot{y}^{w}),c(y^{\\ell})$ . Let $\\log\\pi_{\\theta}(y)=c(y)\\cdot\\theta-$   \n318 $n Z(\\theta)$ for $\\begin{array}{r}{Z(\\theta)=\\log\\sum_{i}^{\\mathcal{V}}e^{\\theta_{i}}}\\end{array}$ , with upper bound $\\log\\tilde{\\pi}_{\\boldsymbol{\\theta}}(\\boldsymbol{y})=c(\\boldsymbol{y})\\!\\cdot\\!\\boldsymbol{\\theta}\\!-\\!n\\operatorname*{max}_{j}\\theta_{j}$ . Let $\\theta^{(t)}$ represent   \n319 the parameters of $\\pi$ after $t$ steps of gradient descent on $\\mathcal{L}_{\\mathrm{dpo}}(\\{y^{\\ell},y^{w},x\\})$ , with $\\theta^{(0)}\\,=\\,0$ . Then   \n320 $\\pi_{\\theta^{(t)}}(y^{w})\\leq\\tilde{\\pi}_{\\theta^{(t)}}(y^{w})\\leq\\tilde{\\pi}_{\\theta^{(t-1)}}(y^{w})$ for all $t$ .   \n321 Where does the probability mass go? If $\\pi_{\\boldsymbol{\\theta}^{(t)}}(y^{w})$ decreases in $t$ , what other strings become   \n322 more probable? In the following proposition, we show that under the bag-of-words model, DPO   \n323 optimization moves probability mass away from $y^{w}$ to sequences that contain only the tokens that   \n324 maximize the difference between $y^{w}$ and $\\dot{y}^{\\ell}$ . This is a concrete example of the type of undesirable   \n325 optima described in $\\S2.3$ , now shown here to be realizable.   \n326 Proposition 6. Let $y^{w}$ and $y^{\\ell}$ be preferred / dispreferred outputs of length $n$ . Let $\\Delta=c(y^{w})-c(y^{\\ell})$   \n327 be the difference in unigram counts. Let $\\hat{y}=[i,i,\\dots,i]$ , for $i\\,\\in\\,a r g\\operatorname*{max}\\Delta$ , with $||c(\\hat{y})||_{1}=n$ .   \n328 Then $\\pi_{\\theta^{(t)}}(y^{w})-\\pi_{\\theta^{(t)}}(\\hat{y})=\\tau(t)k$ for some $k\\leq0$ and some non-decreasing $\\tau:\\mathbb{Z}_{+}\\to\\mathbb{R}_{+}$ .   \n329 We have $k=0$ when $c(y^{w})=c(\\hat{y})$ , and $k\\ll0$ when $||c(y^{w})||_{2}\\ll||c(\\hat{y})||_{2}=n$ (dense $c(y^{w}))$ and   \n330 $||\\Delta||_{2}=||\\Delta||_{\\infty}$ (sparse $\\Delta$ ). This implies that when $y^{w}$ and $y^{\\ell}$ are similar, $\\pi_{\\boldsymbol{\\theta}}(y^{w})$ will degrade more   \n331 rapidly. Early stopping will therefore tradeoff between reaching the degenerate solution on such   \n332 cases, and underfitting other cases in which $y^{w}$ and $y^{\\ell}$ are more distinct. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "333 6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "334 Recent work in offline alignment has focused on DPO [23] as a simpler alternative for aligning   \n335 language models from preference data. Subsequent work has identified issues with DPO, including   \n336 weak regularization [3] and a tendency to decrease the probability of winning generations during   \n337 training [20]. Other methods have explored various avenues for improvement. These include   \n338 analyzing the impact of noise on DPO alignment [11], proposing to update the reference policy   \n339 during training [12], and suggesting a variant of IPO with a per-context margin [1]. Additional   \n340 research has focused on token-level alignment methods [38; 22] and on developing a unified view of   \n341 various offline alignment methods [31]. This work builds upon several these findings, and provides   \n342 further analysis, as well as a solution based on pessimism and reward distillation.   \n343 While offilne alignment methods are popular, recent evidence suggests that online alignment methods   \n344 such as RLHF [6; 29], may lead to more favorable outcomes [13; 30; 8; 34]. Notably, Zhu et al. [41]   \n345 proposed iterative data smoothing, which uses a trained model to softly label data during RLHF.   \n346 Whether online or offline, however, policies are still succeptible to overfitting to certain degenerate   \n347 phenomena. To this end, reward ensembles have been widely investigated recently as a mechanism   \n348 for tackling reward hacking in RLHF [9; 7; 39; 25], and in the context of multi-objective optimization   \n349 [19; 24]. We use an ensemble of rewards to represent the uncertainty with respect to reward models   \n350 that are suitable given preference data. Moskovitz et al. [19] focus on \u201ccomposite\u201d rewards, with the   \n351 goal of achieving high task reward while ensuring that every individual component is above some   \n352 threshold\u2014also by applying a Lagrangian relaxation. In this work, we also consider multiple reward   \n353 models, but we only focus on cases where there is no known, obvious reward decomposition.   \n354 Finally, the question of using a small amount of offline data to learn high-quality policies, instead   \n355 of online access to reward feedback, has been widely studied in the offline reinforcement learning   \n356 (RL) literature. The predominant approach here is to use pessimism, that is, to learn a policy with   \n357 the highest reward under all plausible environment models consistent with the data, with an extensive   \n358 theoretical [18; 37; 33] and empirical [16; 5; 36] body of supporting work. The key insight in this   \n359 literature is that without pessimism, the RL algorithm learns undesirable behaviors which are not   \n360 explicitly ruled out in the training data, and pessimism provides a robust way of preventing such   \n361 undesirable extrapolations, while still preserving generalization within the support of the data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "362 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "363 LM alignment is crucial for deploying safe and helpful assistants, but is difficult due to lack of   \n364 access to perfect preference oracles. We presented a thorough theoretical analysis of some of   \n365 the degeneracies that DPO is susceptible to when learning from sampled human preference data.   \n366 Furthermore, our findings suggest that explicit reward modeling remains a powerful vehicle for   \n367 introducing regularization into post-training. By distilling the reward assigned by a single, explicit   \n368 reward model\u2014or a family of explicit reward models\u2014directly into the implicit reward maximized   \n369 by our policies using offline data, we demonstrated that we can achieve improved robustness to   \n370 variations in preference dataset quality, while maintaining the simplicity of the DPO framework.   \n371 Limitations. The empirical results in the paper are based on one dataset and form of distribution shift.   \n372 For deeper understanding of pessimism and ensembling, additional settings should be explored. The   \n373 theoretical aspects of the paper are sometimes based on restrictive assumptions and simplifications.   \n374 Nonetheless, they provide potential explanations for phenomena observed in real-world settings.   \n375 Broader impact. We introduce new ideas to the active field of research on preference-based post  \n376 training, which we hope will help facilitate the alignment of large models, and improve understanding   \n377 of current approaches\u2014ultimately supporting the development of capable and reliable AI systems. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "378 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "379 [1] Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset.   \n380 arXiv preprint arXiv:2402.10571, 2024.   \n381 [2] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,   \n382 Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,   \n383 Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira,   \n384 Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing   \n385 Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,   \n386 James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin   \n387 Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave,   \n388 Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg,   \n389 Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas   \n390 Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,   \n391 Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia,   \n392 Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin   \n393 Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao   \n394 Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra,   \n395 Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,   \n396 Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan   \n397 Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee   \n398 Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha   \n399 Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang,   \n400 John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu,   \n401 Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui   \n402 Wu. Palm 2 technical report, 2023.   \n403 [3] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland,   \n404 Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning   \n405 from human preferences. In International Conference on Artificial Intelligence and Statistics,   \n406 pages 4447\u20134455. PMLR, 2024.   \n407 [4] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the   \n408 method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. ISSN 00063444. URL   \n409 http://www.jstor.org/stable/2334029.   \n410 [5] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor   \n411 critic for offline reinforcement learning. In International Conference on Machine Learning,   \n412 pages 3852\u20133878. PMLR, 2022.   \n413 [6] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep   \n414 reinforcement learning from human preferences. Advances in neural information processing   \n415 systems, 30, 2017.   \n416 [7] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help   \n417 mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.   \n418 [8] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen   \n419 Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf.   \n420 2024.   \n421 [9] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\u2019Amour, DJ Dvi  \n422 jotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping   \n423 or herding? Reward model ensembles mitigate but do not eliminate reward hacking. arXiv   \n424 preprint arXiv:2312.09244, 2023.   \n425 [10] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.   \n426 Born again neural networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the   \n427 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine   \n428 Learning Research, pages 1607\u20131616. PMLR, 10\u201315 Jul 2018. URL https://proceedings.   \n429 mlr.press/v80/furlanello18a.html.   \n430 [11] Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment   \n431 performance of generative language models. arXiv preprint arXiv:2404.09824, 2024.   \n432 [12] Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Ak  \n433 senov, Ian Maksimov, Nikita Balagansky, and Daniil Gavrilov. Learn your reference model for   \n434 real good alignment. arXiv preprint arXiv:2404.09656, 2024.   \n435 [13] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexan  \n436 dre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from   \n437 online ai feedback. arXiv preprint arXiv:2402.04792, 2024.   \n438 [14] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.   \n439 In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.   \n440 org/abs/1503.02531.   \n441 [15] Tomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better viewed   \n442 as bayesian inference. In Findings of the Association for Computational Linguistics: EMNLP   \n443 2022, pages 1083\u20131091, 2022.   \n444 [16] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning   \n445 for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:   \n446 1179\u20131191, 2020.   \n447 [17] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,   \n448 Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human   \n449 feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.   \n450 [18] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch   \n451 off-policy reinforcement learning without great exploration. Advances in neural information   \n452 processing systems, 33:1264\u20131274, 2020.   \n453 [19] Ted Moskovitz, Aaditya K Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D   \n454 Dragan, and Stephen McAleer. Confronting reward model overoptimization with constrained   \n455 rlhf. arXiv preprint arXiv:2310.04373, 2023.   \n456 [20] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White.   \n457 Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint   \n458 arXiv:2402.13228, 2024.   \n459 [21] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from   \n460 quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024.   \n461 [22] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$ to $q^{*}$ : Your language model   \n462 is secretly a $\\boldsymbol{\\mathrm{q}}$ -function. 2024.   \n463 [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and   \n464 Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.   \n465 Advances in Neural Information Processing Systems, 36, 2024.   \n466 [24] Alexandre Ram\u00e9, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya,   \n467 Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by   \n468 interpolating weights fine-tuned on diverse rewards, 2023.   \n469 [25] Alexandre Ram\u00e9, Nino Vieillard, L\u00e9onard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier   \n470 Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models. 2024.   \n471 [26] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and   \n472 Yoshua Bengio. Fitnets: Hints for thin deep nets. In In Proceedings of ICLR, 2015.   \n473 [27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal   \n474 policy optimization algorithms. 2017.   \n475 [28] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating   \n476 length correlations in rlhf. 2023.   \n477 [29] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec   \n478 Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.   \n479 Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n480 [30] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie,   \n481 Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage   \n482 suboptimal, on-policy data, 2024.   \n483 [31] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, R\u00e9mi Munos, Mark   \n484 Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo \u00c1vila Pires, and Bilal Piot.   \n485 Generalized preference optimization: A unified approach to offline alignment. 2024.   \n486 [32] Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit   \n487 to learn automatic summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini,   \n488 and Fei Liu, editors, Proceedings of the Workshop on New Frontiers in Summarization, pages   \n489 59\u201363, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.   \n490 doi: 10.18653/v1/W17-4508. URL https://aclanthology.org/W17-4508.   \n491 [33] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman  \n492 consistent pessimism for offline reinforcement learning. Advances in neural information   \n493 processing systems, 34:6683\u20136694, 2021.   \n494 [34] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao   \n495 Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. 2024.   \n496 [35] Chenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan L. Yuille. Training deep neural networks   \n497 in generations: a more tolerant teacher educates better students. In Proceedings of the Thirty  \n498 Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of   \n499 Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in   \n500 Artificial Intelligence, AAAI\u201919/IAAI\u201919/EAAI\u201919. AAAI Press, 2019. ISBN 978-1-57735-809-   \n501 1. doi: 10.1609/aaai.v33i01.33015628. URL https://doi.org/10.1609/aaai.v33i01.   \n502 33015628.   \n503 [36] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea   \n504 Finn. Combo: Conservative offline model-based policy optimization. Advances in neural   \n505 information processing systems, 34:28954\u201328967, 2021.   \n506 [37] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic   \n507 methods for offilne reinforcement learning. Advances in neural information processing systems,   \n508 34:13626\u201313640, 2021.   \n509 [38] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token  \n510 level direct preference optimization. 2024.   \n511 [39] Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng, Bo Ding, and Huaimin   \n512 Wang. Uncertainty-penalized reinforcement learning from human feedback with diverse reward   \n513 lora ensembles. 2023.   \n514 [40] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu.   \n515 Slic-hf: Sequence likelihood calibration with human feedback. 2023.   \n516 [41] Banghua Zhu, Michael I. Jordan, and Jiantao Jiao. Iterative data smoothing: Mitigating reward   \n517 overfitting and overoptimization in rlhf. 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "518 A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "519 A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "520 Proof. Since all the preference pairs $(y,y^{\\prime})$ are mutually disjoint, and $\\theta_{y}$ is specific to each $y$ , the   \n521 DPO objective over $\\mathcal{D}_{\\mathrm{pref}}$ is convex in $\\Delta=\\{\\Delta_{1},\\ldots,\\Delta_{n}\\}$ , where ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Delta_{i}=\\beta\\log\\frac{\\pi_{\\theta}(y_{i}^{w})\\pi_{\\mathrm{ref}}(y_{i}^{\\ell})}{\\pi_{\\theta}(y_{i}^{\\ell})\\pi_{\\mathrm{ref}}(y_{i}^{w})}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "522 Furthermore, the different $\\Delta_{i}$ are completely independent from each other due to the preference pairs   \n523 being disjoint, so they can be optimized over separately. ", "page_idx": 12}, {"type": "text", "text": "524 In particular, for every $i$ we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\Delta_{i}\\rightarrow\\infty}-\\log\\left(\\sigma\\left(\\Delta_{i}\\right)\\right)=0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "525 which implies that $\\Delta^{*}=\\{\\infty\\}^{n}$ is the unique global minimizer of the DPO loss over $\\mathcal{D}_{\\mathrm{pref}}$ in the   \n526 space of $\\Delta$ \u2019s, and any $\\theta^{*}$ that is a global minimizer must therefore satisfy ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\log\\frac{\\pi_{\\theta}(y_{i}^{w})\\pi_{\\mathrm{ref}}(y_{i}^{\\ell})}{\\pi_{\\theta}(y_{i}^{\\ell})\\pi_{\\mathrm{ref}}(y_{i}^{w})}=\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "527 ", "page_idx": 12}, {"type": "text", "text": "528 A.2 Proof of Corollary 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "529 Proof. Following the same argument of the proof of Proposition 1, we have that all global minimizers   \n530 $\\theta^{*}$ of the DPO satisfy $\\Delta_{i}^{*}=\\infty$ , which in turn implies that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\pi_{\\theta^{*}}(y_{i}^{w})\\pi_{\\mathrm{ref}}(y_{i}^{\\ell})}{\\pi_{\\theta^{*}}(y_{i}^{\\ell})\\pi_{\\mathrm{ref}}(y_{i}^{w})}=\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "531 Since $\\pi_{\\mathrm{ref}}(y)$ is assumed to satisfy $0<\\pi_{\\mathrm{ref}}(y)<1$ for all $y$ , this implies that all $\\theta^{*}$ satisfy ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\pi_{\\theta^{*}}(y_{i}^{w})}{\\pi_{\\theta^{*}}(y_{i}^{\\ell})}=\\infty,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "532 which further implies that $\\pi_{\\theta^{*}}(y_{i}^{\\ell})=0$ and $\\pi_{\\theta^{*}}(y_{i}^{w})>0$ for all $i\\in[n]$ , as $\\pi_{\\theta^{*}}(y_{i}^{w})\\leq1$ for any $y_{i}^{w}$ .   \n533 Aggregating ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{C}(y_{\\ell})=\\{y\\colon\\exists i\\in[n]\\;\\mathrm{s.t}\\;y_{i}^{\\ell}=y\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "534 then gives that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\pi_{\\theta^{*}}(\\mathcal{C}(y_{\\ell}))=\\sum_{y\\in\\mathcal{C}(y_{\\ell})}\\pi_{\\theta^{*}}(y)=0\\Longrightarrow\\pi_{\\theta^{*}}(\\mathcal{C}(y_{\\ell})^{c})=1.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "535 ", "page_idx": 12}, {"type": "text", "text": "536 To prove the converse, let $\\pi_{\\theta^{\\prime}}$ be a policy that satisfies $\\pi_{\\theta^{\\prime}}(\\mathcal{C}(y^{\\ell})^{c})=1$ , with $\\pi_{\\theta^{\\prime}}(y_{i}^{w})>0,\\forall i\\in[n].$ ,.   \n537 As $\\pi_{\\theta^{\\prime}}(y)\\geq0$ for all $y$ , this implies that $\\pi_{\\theta^{\\prime}(y_{i}^{\\ell})}=0\\,\\forall i\\in[n]$ . Then, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\pi_{\\theta^{\\prime}}(y_{i}^{w})}{\\pi_{\\theta^{\\prime}}(y_{i}^{\\ell})}=\\infty,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "538 which by Proposition 1 implies that $\\pi_{\\theta^{\\prime}}$ is a global optimum. ", "page_idx": 12}, {"type": "text", "text": "539 A.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "540 Proof. We know that the optimal policy for the RLHF objective (1) is given by $\\pi_{\\theta^{\\ast}}(y|x)\\;\\propto\\;$   \n541 $\\pi_{\\mathrm{ref}}(y|x)\\exp(r^{*}(x,y)/\\beta)$ . Plugging this policy into the distillation objective (7), we see that   \n542 $\\mathcal{L}_{\\mathrm{distill}}(r^{*},\\pi_{\\theta^{*}},\\rho)\\;=\\;0$ for all $\\rho$ . In fact, the loss is equal to 0 pointwise, meaning that $\\pi_{\\theta^{*}}$ is   \n543 a global minimizer of the distillation objective (7). Further, let $\\pi$ be some other minimizer of   \n544 $\\mathcal{L}_{\\mathrm{distill}}(r^{*},\\cdot,\\rho)$ . Then $\\pi$ also has to attain a loss of 0 at all $(x,y,y^{\\prime})$ in the support of $\\rho$ , meaning   \n545 that $\\log\\pi(y|x)-\\log\\pi(y^{\\prime}|x)\\,=\\,\\log\\pi_{\\theta^{*}}(y|x)-\\log\\pi_{\\theta^{*}}(y|x)$ for all $(x,y,y^{\\prime})$ in the support of $\\rho$ .   \n546 Consequently, the two policies coincide in the support of $\\rho$ (due to the normalization constraint, there   \n547 is no additional offset term allowed as the support of $\\rho$ covers all of $\\mathcal{V}_{.}$ ). Finally, noting that the   \n548 support of the chosen $\\rho$ is such that $\\pi\\theta^{*}$ puts no mass outside its support due to the KL constraint   \n549 in (1), we complete the proof. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "550 A.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "551 Proof. Consider the pessimistic objective: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi_{\\theta}}\\operatorname*{min}_{r_{\\mathrm{tgt}}\\in S}\\mathbb{E}_{\\mu(x)}\\Big[\\mathbb{E}_{\\pi_{\\theta}(y|x)}[r_{\\mathrm{tgt}}(x,y)]-\\mathbb{E}_{\\pi_{\\mathrm{ref}}(y|x)}[r_{\\mathrm{tgt}}(x,y)]\\Big]-\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\theta}\\|\\pi_{\\mathrm{ref}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "552 As it is linear in $r_{\\mathrm{tgt}}$ and convex in $\\pi$ , we can switch the order of min and max: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r_{\\mathrm{tgt}}\\in S}\\;\\left[\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{\\mu(x)}\\left[\\mathbb{E}_{\\pi(y\\mid x)}[r_{\\mathrm{tgt}}(x,y)]-\\mathbb{E}_{\\pi_{\\mathrm{ref}}(y\\mid x)}[r_{\\mathrm{tgt}}(x,y)]\\right]-\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "553 Note that every $r_{\\mathrm{tgt}}\\in S$ can be written in terms of the KL-constrained policy $\\pi_{r_{\\mathrm{tgt}}}^{*}$ it induces, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\nr_{\\mathrm{tgt}}(x,y)=\\beta\\log\\frac{\\pi_{r_{\\mathrm{tgt}}}^{*}(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}+\\beta\\log Z(x,r_{\\mathrm{tgt}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "554 where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pi_{r_{\\mathrm{tgt}}}^{*}=\\operatorname*{argmax}_{\\pi_{\\theta}}\\mathbb{E}_{\\mu(x)}\\mathbb{E}_{\\pi_{\\theta}(y\\vert x)}[r_{\\mathrm{tgt}}(x,y)]-\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\theta}\\Vert\\pi_{\\mathrm{ref}})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "555 which has the form ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pi_{r_{\\mathrm{tgt}}}^{*}(y\\mid x)={\\frac{1}{Z(x,r_{\\mathrm{tgt}})}}\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\left({\\frac{1}{\\beta}}r_{\\mathrm{tgt}}(x,y)\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "556 where $Z(x,r_{\\mathrm{tgt}})$ is the partition function: ", "page_idx": 13}, {"type": "equation", "text": "$$\nZ(x,r_{\\mathrm{tgt}})=\\sum_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r_{\\mathrm{tgt}}(x,y)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "557 Substituting $\\pi_{r_{\\mathrm{tgt}}}^{*}$ in for $\\mathrm{max}_{\\pi}$ and writing $r_{\\mathrm{tgt}}$ in terms of \u03c0r\u2217tgt, we get the simplified objective ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r_{\\mathrm{gc}}\\in S}{\\operatorname*{min}}\\ \\Bigg[\\frac{\\mathrm{max}}{\\pi\\epsilon}\\Bigg[\\mathbb{E}_{\\mu\\propto\\pi}\\Bigg[\\mathbb{E}_{\\mathbf{g}(\\mathbf{r}_{\\mathrm{g}})}\\Big[r_{\\mathbf{g}(\\mathbf{r})}(\\mathbf{r},\\mathbf{g})\\Big]-\\mathbb{E}_{\\pi\\times\\pi(\\mathbf{g}(\\mathbf{r})\\mid\\mathbf{r})}\\Big[r_{\\mathbf{g}(\\mathbf{r})}(\\mathbf{r},\\mathbf{g})\\Big]\\Bigg]-\\beta\\mathbb{D}_{\\mathbf{KL}}\\Big(\\pi\\Big\\vert\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}\\Big]\\Bigg]}\\\\ &{\\qquad\\qquad=\\underset{r_{\\mathrm{gc}}\\in S}{\\operatorname*{min}}\\ \\Bigg[\\mathbb{E}_{\\mu\\mid\\mathbf{r}_{\\mathrm{g}}}\\Bigg[\\mathbb{E}_{\\mathbf{r}_{\\mathrm{gc}}^{\\prime}(\\mathbf{r})\\mid\\mathbf{r}}\\Bigg[\\beta\\log\\frac{\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}}{\\pi_{\\mathbf{r}\\in\\mathbf{r}}(\\mathbf{r})\\mid\\mathbf{r}_{\\mathbf{g}}}+\\beta\\log Z(\\mathbf{r},r_{\\mathrm{gc}})\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad-\\mathbb{E}_{\\pi\\times\\pi(\\mathbf{g}(\\mathbf{r})\\mid\\mathbf{r})}\\Bigg[\\beta\\log\\frac{\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}^{\\star}}{\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}}+\\beta\\log Z(\\mathbf{r},r_{\\mathrm{gc}})\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad-\\beta\\mathbb{D}_{\\mathbf{KL}}\\Big(\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}^{\\star}\\Big\\vert\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}\\Big\\vert\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\underset{r_{\\mathrm{gc}}\\in S}{\\operatorname*{min}}\\ \\beta\\Bigg[\\mathbb{E}_{\\mu(\\mathbf{r})}\\Big[\\frac{\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}}{\\mathbb{D}_{\\mathbf{KL}}\\big(\\pi_{\\mathbf{r}\\mathbf{g}(\\mathbf{r})}\\mid\\mathbf{r}_{\\mathbf{g}}\\big)}\\Big\\vert\\ x\\Bigg]}\\\\ &{\\qquad\\qquad\\quad=\\underset{r_{\\mathrm{gc}}\\in S}{\\operatorname*{min}}\\ \\beta\\mathbb{E}_{\\mu(\\mathbf{\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "559 A.5 Proof of Proposition 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "560 Proof. The proof is a standard Lagrangian duality argument, which we reproduce here for complete  \n561 ness. For two functions $f(z)$ and $g(z)$ , let us define ", "page_idx": 14}, {"type": "equation", "text": "$$\nz^{*}=\\operatorname*{argmin}_{z}f(z)+\\alpha g(z).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "562 Let us also consider the constrained problem ", "page_idx": 14}, {"type": "equation", "text": "$$\nz^{\\prime}=\\underset{z}{\\operatorname{argmin}}\\,f(z)\\quad\\mathrm{s.t.}\\ g(z)\\leq g(z^{*}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "563 Suppose by contradiction that $z^{*}$ is not a minimizer of (31). Since $z^{*}$ is feasible for the constraint by   \n564 construction, we get that $f(z^{\\prime})<f(z^{*})$ . Consequently, we further have ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(z^{\\prime})+\\alpha g(z^{\\prime})<f(z^{*})+\\alpha g(z^{*}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "565 where the inequality follows from the feasibility of $z^{\\prime}$ in (31). This contradicts the optimality   \n566 of $z^{*}$ in (30), meaning that $z^{*}$ must be a minimizer of (31). Applying this general result with   \n567 $f=\\beta\\mathbb{E}_{\\mu(x)}\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\mathrm{ref}}(y\\mid x)\\Vert\\pi_{\\theta}(y\\mid x))$ , $\\begin{array}{r}{g=\\mathrm{min}_{r_{\\mathrm{tgt}}^{i}\\in\\mathcal{S}}\\,\\mathcal{L}_{\\mathrm{distill}}(r_{\\mathrm{tgt}}^{i},\\pi_{\\theta};\\rho)}\\end{array}$ , and $z=\\pi_{\\theta}$ completes   \n568 the proof, since we recognize the set $\\mathcal{S}_{\\gamma}$ in (12) to be equivalent to $\\begin{array}{r}{\\bigcup_{r_{\\mathrm{tgt}}^{i}\\in\\mathcal{S}}\\mathcal{L}_{\\mathrm{distill}}(r_{\\mathrm{tgt}}^{i},\\pi_{\\theta};\\rho)\\le\\lambda}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "570 A.6 Proof of Proposition 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "571 Proof. We differentiate $\\mathcal{L}_{\\mathrm{pdpo}}$ with respect to $\\psi_{\\ell}=\\pi_{\\theta}(y^{\\ell})/\\pi_{\\mathrm{ref}}(y^{\\ell})$ with $i$ implicit, obtaining, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{\\mathrm{pdpo}}}{\\partial\\psi_{\\ell}}=\\!\\beta\\frac{\\psi_{\\ell}^{\\beta}}{\\psi_{w}^{\\beta}+\\psi_{\\ell}^{\\beta}}\\psi_{\\ell}^{-1}-\\frac{\\beta}{\\alpha}\\psi_{\\ell}^{-1}=\\beta\\psi_{\\ell}^{-1}\\left(\\frac{\\psi_{\\ell}^{\\beta}}{\\psi_{w}^{\\beta}+\\psi_{\\ell}^{\\beta}}-\\alpha^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "572 which is zero when, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\alpha\\psi_{\\ell}^{\\beta}=\\psi_{w}^{\\beta}+\\psi_{\\ell}^{\\beta}}\\\\ &{\\quad\\displaystyle\\psi_{\\ell}=\\left(\\frac{1}{\\alpha-1}\\right)^{1/\\beta}\\psi_{w}}\\\\ &{\\quad\\displaystyle\\log\\psi_{\\ell}=-\\,\\frac{1}{\\beta}\\log(\\alpha-1)+\\log\\psi_{w}}\\\\ &{\\quad\\log\\pi_{\\hat{\\theta}}(y^{\\ell})=\\log\\pi_{\\mathrm{ref}}(y^{\\ell})-\\frac{1}{\\beta}\\log\\left(\\alpha-1\\right)+\\log\\pi_{\\theta}(y^{w})-\\log\\pi_{\\mathrm{ref}}(y^{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "573 By the second-order condition, the critical point is a minimum. The objective $\\mathcal{L}_{\\mathrm{pdpo}}$ is the sum of two   \n574 components: the negative log sigmoid term for $\\mathcal{L}_{i}$ and the negative log probability for \u2126\u02c6. Because   \n575 each component is a convex function of $\\psi_{i}$ , so is $\\mathcal{L}_{\\mathrm{pdpo}}$ . As a result, the local minimum $\\log\\hat{\\pi}_{\\boldsymbol{\\theta}}(y^{\\ell})$ is   \n576 also a global minimum. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "577 A.7 Proof of Proposition 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "578 Proof. This follows directly from differentiating eq. (7) with respect to $\\pi_{\\theta}(y_{2})$ . ", "page_idx": 14}, {"type": "text", "text": "579 A.8 Proof of Proposition 5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "580 Proof. Let $\\Delta\\,=\\,[c(y^{w})\\,-\\,c(y^{\\ell})]$ and $\\rho=\\pi_{\\mathrm{ref}}(y^{w})/\\pi_{\\mathrm{ref}}(y^{\\ell})$ . The theorem assumes $|y^{w}|\\,=\\,|y^{\\ell}|$ .   \n581 Then $\\mathcal{L}_{\\mathrm{dpo}}=-\\log\\sigma\\left(\\beta(\\Delta\\cdot\\theta)+\\beta\\log\\rho\\right)$ . The derivative with respect to $\\theta$ is, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{\\beta}(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}}=-\\left(1-\\sigma(\\beta(\\Delta\\cdot\\boldsymbol{\\theta})+\\beta\\log\\rho)\\right)\\!\\beta\\Delta=-\\operatorname*{Pr}(y^{\\ell}\\succ y^{w};\\boldsymbol{\\theta})\\beta\\Delta\\prec0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "582 Let $\\delta_{t}=\\beta\\operatorname*{Pr}(y^{\\ell}\\succ y^{w};\\theta^{(t)})$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\pi}_{\\theta^{(t)}}=\\theta^{(t)}\\cdot c(y^{w})-n\\operatorname*{max}_{j}\\theta_{j}^{(t)}}\\\\ &{\\quad\\quad=(\\theta^{(t-1)}+\\delta_{t}\\Delta)\\cdot c(y^{w})-n\\operatorname*{max}_{j}(\\theta_{j}^{(t-1)}+\\delta_{t}\\Delta_{j})}\\\\ &{\\quad\\quad=\\theta^{(t-1)}\\cdot c(y^{w})-n\\operatorname*{max}_{j}\\theta_{j}^{(t-1)}+\\delta_{t}\\Delta\\cdot c(y^{w})-n\\delta_{t}\\operatorname*{max}_{j}\\Delta_{j}}\\\\ &{\\quad\\quad=\\widetilde{\\pi}_{\\theta^{(t-1)}}+\\delta_{t}\\left(\\Delta\\cdot c(y^{w})-n\\operatorname*{max}_{j}\\Delta_{j}\\right)}\\\\ &{\\quad\\quad\\quad=\\widetilde{\\pi}_{\\theta^{(t-1)}}+\\delta_{t}\\sum_{j}c_{j}(y^{w})(\\Delta_{j}-\\operatorname*{max}_{j^{\\prime}}\\Delta_{j^{\\prime}})\\leq\\widetilde{\\pi}_{\\theta^{(t-1)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "583 We obtain $\\begin{array}{r}{\\operatorname*{max}_{j}\\left(\\theta_{j}^{(t-1)}+\\delta_{t}\\Delta_{j}\\right)=\\operatorname*{max}_{j}\\theta_{j}^{(t-1)}+\\operatorname*{max}_{j}\\delta_{t}\\Delta_{j}}\\end{array}$ from the fact that $\\theta^{(0)}\\,=\\,0$ and   \n584 therefore $j\\,\\in\\,\\arg\\operatorname*{max}\\Delta$ implies $j\\,\\in\\,\\arg\\operatorname*{max}\\theta^{(t^{\\prime})}$ for all $t^{\\prime}\\,>\\,0$ . The second-to-last step uses   \n585 $\\begin{array}{r}{n=\\sum_{j}^{\\nu}c_{j}(y^{w})}\\end{array}$ and the final step uses $\\Delta_{j}\\,\\le\\,\\operatorname*{max}_{j}^{\\prime}\\Delta_{j^{\\prime}}$ . Finally, we have $\\pi_{\\theta^{(t)}}(y)\\,\\leq\\,\\tilde{\\pi}_{\\theta^{(t)}}\\big(y^{w}\\big)$   \n586 because $\\begin{array}{r}{Z(\\theta)=\\log\\sum_{j}\\exp\\theta_{j}\\ge\\log\\operatorname*{max}_{j}\\exp\\theta_{j}=\\operatorname*{max}_{j}\\theta_{j}}\\end{array}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "587 A.9 Proof of Proposition 6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "588 Proof. Applying gradient descent with learning rate $\\eta$ to the gradient from Equation (37), at each   \n589 step $t$ the parameters are, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\theta^{(t)}=\\theta^{(t-1)}+\\eta\\beta\\operatorname*{Pr}(y^{\\ell}\\succ y^{w};\\theta^{(t-1)})\\Delta=\\left(\\sum_{\\iota^{\\prime}=1}^{t}\\eta\\beta\\operatorname*{Pr}(y^{\\ell}\\succ y^{w};\\theta^{(t^{\\prime})})\\right)\\Delta=\\tau(t)\\Delta.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "590 Plugging these parameters into the likelihoods, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\theta^{(t)}}(c(y^{w}))-\\ell_{\\theta^{(t)}}(\\hat{y})=c(y^{w})\\cdot\\theta^{(t)}-n Z(\\theta^{(t)})-c(\\hat{y})\\cdot\\theta^{(t)}+n Z(\\theta^{(t)})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(c(y^{w})-c(\\hat{y}))\\cdot\\theta^{(t)}=(c(y^{w})-c(\\hat{y}))\\cdot(\\tau(t)\\Delta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\tau(t)(c(y^{w})\\cdot\\Delta-n\\operatorname*{max}\\Delta)=\\tau(t)k,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "591 with $k\\leq0$ by $c(y^{w})\\cdot\\Delta\\leq||c(y^{w})||_{1}\\times||\\Delta||_{\\infty}=n\\operatorname*{max}\\Delta.$ ", "page_idx": 15}, {"type": "text", "text": "592 B Transitive closure ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "593 Both p-DPO and IPO target a constant ratio for $\\log{\\psi_{w}}/\\psi_{l}$ . However, the loss surfaces are different.   \n594 To see this, we consider a simplified setting with three possible outputs, $y_{1},\\,y_{2},\\,y_{3}$ . We observe either   \n595 $\\mathcal{D}=\\{(y_{1}\\,\\prec\\,y_{2}),(y_{2}\\,\\prec\\,y_{3})\\}$ or $\\overline{{D}}=\\mathit{{D}}\\cup\\{(y_{1}\\prec y_{3})\\}$ . If we treat this problem as a multi-arm   \n596 bandit, the goal is to assign a weight to each arm, which we denote $\\psi_{i}=\\log\\pi_{\\theta}(y_{i}|x)+Z_{x}$ , with $Z_{x}$   \n597 an underdetermined log-partition function.   \n598 Proposition 7. Let $\\mathcal{D}=\\{(i,i\\!+\\!1):i\\in1,2,\\ldots,n\\}$ for $n>2$ . Let $\\overline{{\\mathcal{D}}}$ be the dataset arising from the   \n599 transitive closure of $\\mathcal{D}$ . Assume $\\pi_{\\mathrm{ref}}$ is indifferent to all $(y_{i},y_{j})$ . Let $\\psi_{\\infty}^{({\\cal D})}=\\operatorname*{max}_{i}\\psi_{i}^{({\\cal D})}-\\operatorname*{min}_{i}\\psi_{i}^{({\\cal D})}$ .   \n600 Then $\\begin{array}{r}{\\psi_{\\infty}^{(\\mathcal{D})}=(n-1)\\tau^{-1}>\\psi_{\\infty}^{(\\overline{{\\mathcal{D}}})}=2\\frac{n-1}{n}\\tau^{-1}}\\end{array}$ .   \n601 Proof. For $\\mathcal{D}$ , the IPO objective can be minimized at zero, so that \u03c8(\u221eD)= (n \u22121)\u03c4 \u22121. For D,   \n602 each adjacent pair of completions is separated by $\\gamma$ , and the objective is $\\begin{array}{r}{\\sum_{i=1}^{n-1}(n-i)(i\\gamma-\\tau^{-1})^{2}}\\end{array}$ .   \n603 The minimum is $\\begin{array}{r}{\\gamma\\,=\\,\\frac{n(n+1)(n-1)/6}{n^{2}(n+1)(n-1)/12}\\tau^{-1}\\,=\\,\\frac{2}{n}\\tau^{-1}}\\end{array}$ , so that $\\begin{array}{r}{\\psi_{\\infty}^{(\\overline{{D}})}\\,=\\,(n\\,-\\,1)\\gamma\\,=\\,2\\frac{n-1}{n}\\tau^{-1}\\,<\\,}\\end{array}$   \n604 $(n-1)\\tau^{-1}=\\psi_{\\infty}^{(\\mathcal{D})}$ for $n>2$ .   \n605 Intuitively, the observation of $(y_{1}\\prec y_{3})$ should increase confidence that $y_{3}$ is superior to $y_{1}$ , but   \n606 in IPO it has the opposite effect, drawing their scores closer together. While pessimistic DPO also   \n607 has a target ratio between each preference pair, its loss surface is different: in particular, it does not   \n608 increase quadratically as we move away from the target. We find empirically that pessimistic DPO is   \n609 robust to the transitive closure of preference annotations in the multi-arm bandit setting, as shown in   \n610 Figure B.1. As discussed above, DPO will set $\\psi_{1}\\rightarrow-\\infty$ because $y_{1}$ is never preferred.   \n611 In our empirical experiments we solve the p-DPO and IPO objectives for both $\\mathcal{D}=$   \n612 $\\{(y_{1},y_{2}),(y_{2},y_{3})\\}$ and $\\overline{{D}}=\\mathcal{D}\\cup\\{(y_{1},y_{3})\\}$ , solving with respect to $\\{\\pi_{\\theta}(y_{i})\\}$ . IPO is solved analyti  \n613 cally as a quadratic program; for pessimistic DPO we used projected gradient descent. We consider   \n614 $\\beta\\in(1,3,10,30)$ and $\\alpha\\in(5,10,20,50,100,1000)$ . As shown in Figure B.1, there are significant   \n615 differences in the IPO solutions with and without transitive closure, while for p-DPO these differences   \n616 are imperceptible. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "VajjTXRj6J/tmp/ea38dc70a63b8ac2c839d5c84e91ee00a6284af541f743013d18dc644a4a1c46.jpg", "img_caption": ["Figure B.1: Effect of transitive closure on $\\mathbf{p}$ -DPO and IPO solutions to preference learning in a multi-arm bandit. Each column shows the learned policy probability for a given arm, based on the preferences $y_{1}\\prec y_{2}\\prec y_{3}$ . The top row shows that in ${\\bf p}$ -DPO, the probabilities are not materially affected by the transitive closure $y_{1}\\prec y_{3}$ . The bottom row shows that in IPO, transitive closure causes the probabilities to be compressed. In each subfigure, we sweep a range of effective values of $\\tau^{-1}$ , shown on the $\\mathbf{X}$ -axis. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "617 C Distribution over reward models for e-DPO ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "618 Figure C.1 investigates the reason for the success of e-DPO, especially when $\\rho<.5.$ . For every length   \n619 bias, we show across all training examples the fraction of cases where a certain reward model, $r_{\\rho,b}$ ,   \n620 best matched the implicit reward of the final e-DPO policy. The policy matches different reward   \n621 models in different examples. Moreover, there is inverse correlation between the data bias for policy   \n622 training $(\\rho)$ and the data bias for training the reward models $(b)$ . This suggests that the ensemble   \n623 in e-DPO helps as the policy is distilling from reward models that do not share the data bias of the   \n624 policy training set. ", "page_idx": 16}, {"type": "image", "img_path": "VajjTXRj6J/tmp/0fff79ed0cbd213f9a5efaf7f9eecd7d290601426cbcae9b5fb1db998007c98b.jpg", "img_caption": ["Figure C.1: We show for every length bias, $\\rho$ , the distribution over reward models that best match the final policy trained by e-DPO across all training examples. We observe that the e-DPO policy matches different reward models across examples. Moreover, when the policy is trained with data biased towards preferring short responses, the reward model that was trained on longer responses is often preferred and vice versa. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "625 D Hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "626 Validation set performance across the range of hyperparameter settings is shown in Figure D.1. In   \n627 pilot studies we found that these results were relatively robust to variation in the random seed, but did   \n628 not conduct extensive investigation of this effect across all methods and hyperparameters due to cost. ", "page_idx": 17}, {"type": "text", "text": "629 E Compute resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "630 We train policies on 32 TPU v3 chips and reward models on 16 TPU v3 chips. We obtain roughly 0.1   \n631 steps per second when training, for both the policy and reward models. ", "page_idx": 17}, {"type": "image", "img_path": "VajjTXRj6J/tmp/5767b85425329c3dac97ea74744f607c7b544cf2ebc87b76f7b4bfae310bb10b.jpg", "img_caption": ["Figure D.1: Validation set results across hyperparameters for each method. For all methods, different values of $\\rho$ induce different optimal hyperparameters $\\beta$ and $\\tau^{-1}$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "632 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "633 The checklist is designed to encourage best practices for responsible machine learning research,   \n634 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n635 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n636 follow the references and precede the (optional) supplemental material. The checklist does NOT   \n637 count towards the page limit.   \n638 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n639 each question in the checklist:   \n640 \u2022 You should answer [Yes] , [No] , or [NA] .   \n641 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n642 relevant information is Not Available.   \n643 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n644 The checklist answers are an integral part of your paper submission. They are visible to the   \n645 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n646 (after eventual revisions) with the final version of your paper, and its final version will be published   \n647 with the paper.   \n648 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n649 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n650 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n651 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n652 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n653 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n654 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n655 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n656 please point to the section(s) where related material for the question can be found. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "657 IMPORTANT, please: ", "page_idx": 19}, {"type": "text", "text": "658 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n659 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n660 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n661 1. Claims   \n662 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n663 paper\u2019s contributions and scope?   \n664 Answer: [Yes]   \n665 Justification: In our view, the abstract and introduction accurately summarize the contribu  \n666 tions of the paper.   \n667 Guidelines:   \n668 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n669 made in the paper.   \n670 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n671 contributions made in the paper and important assumptions and limitations. A No or   \n672 NA answer to this question will not be perceived well by the reviewers.   \n673 The claims made should match theoretical and experimental results, and reflect how   \n674 much the results can be expected to generalize to other settings.   \n675 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n676 are not attained by the paper.   \n677 2. Limitations   \n678 Question: Does the paper discuss the limitations of the work performed by the authors?   \nAnswer: [Yes]   \n680 Justification: See Section 7   \n681 Guidelines:   \n682 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n683 the paper has limitations, but those are not discussed in the paper.   \n684 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n685 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n686 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n687 model well-specification, asymptotic approximations only holding locally). The authors   \n688 should reflect on how these assumptions might be violated in practice and what the   \n689 implications would be.   \n690 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n691 only tested on a few datasets or with a few runs. In general, empirical results often   \n692 depend on implicit assumptions, which should be articulated.   \n693 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n694 For example, a facial recognition algorithm may perform poorly when image resolution   \n695 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n696 used reliably to provide closed captions for online lectures because it fails to handle   \n697 technical jargon.   \n698 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n699 and how they scale with dataset size.   \n700 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n701 address problems of privacy and fairness.   \n702 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n703 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n704 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n705 judgment and recognize that individual actions in favor of transparency play an impor  \n706 tant role in developing norms that preserve the integrity of the community. Reviewers   \n707 will be specifically instructed to not penalize honesty concerning limitations.   \n708 3. Theory Assumptions and Proofs   \n709 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n710 a complete (and correct) proof?   \n711 Answer: [Yes]   \n712 Justification: See Appendix A   \n713 Guidelines:   \n714 \u2022 The answer NA means that the paper does not include theoretical results.   \n715 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n716 referenced.   \n717 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n718 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n719 they appear in the supplemental material, the authors are encouraged to provide a short   \n720 proof sketch to provide intuition.   \n721 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n722 by formal proofs provided in appendix or supplemental material.   \n723 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n724 4. Experimental Result Reproducibility   \n725 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n726 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n727 of the paper (regardless of whether the code and data are provided or not)?   \n728 Answer: [Yes]   \n729 Justification: Details are provided in Section 4.1 and Appendix D.   \n730 Guidelines:   \n731 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Experiments are on publicly-available data, but it is not possible for us to share code. We believe that the implementation should be relatively straightforward, given the mathematical descriptions presented here. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 21}, {"type": "text", "text": "788 \u2022 Providing as much information as possible in supplemental material (appended to the   \n789 paper) is recommended, but including URLs to data and code is permitted.   \n790 6. Experimental Setting/Details   \n791 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n792 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n793 results?   \n794 Answer: [Yes]   \n795 Justification: These details are provided in Section 4.1.   \n796 Guidelines:   \n797 \u2022 The answer NA means that the paper does not include experiments.   \n798 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n799 that is necessary to appreciate the results and make sense of them.   \n800 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n801 material.   \n802 7. Experiment Statistical Significance   \n803 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n804 information about the statistical significance of the experiments?   \n805 Answer: [Yes]   \n806 Justification: Section 4.2 includes bootstrap $95\\%$ confidence intervals on the main figure   \n807 and hypothesis tests for specific comparisons between methods.   \n808 Guidelines:   \n809 \u2022 The answer NA means that the paper does not include experiments.   \n810 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n811 dence intervals, or statistical significance tests, at least for the experiments that support   \n812 the main claims of the paper.   \n813 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n814 example, train/test split, initialization, random drawing of some parameter, or overall   \n815 run with given experimental conditions).   \n816 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n817 call to a library function, bootstrap, etc.)   \n818 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n819 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n820 of the mean.   \n821 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n822 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n823 of Normality of errors is not verified.   \n824 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n825 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n826 error rates).   \n827 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n828 they were calculated and reference the corresponding figures or tables in the text.   \n829 8. Experiments Compute Resources   \n830 Question: For each experiment, does the paper provide sufficient information on the com  \n831 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n832 the experiments?   \n833 Answer: [Yes]   \n834 Justification: Please see Section 4.1.   \n835 Guidelines:   \n836 \u2022 The answer NA means that the paper does not include experiments.   \n837 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n838 or cloud provider, including relevant memory and storage.   \n839 \u2022 The paper should provide the amount of compute required for each of the individual   \n840 experimental runs as well as estimate the total compute.   \n841 \u2022 The paper should disclose whether the full research project required more compute   \n842 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n843 didn\u2019t make it into the paper).   \n844 9. Code Of Ethics   \n845 Question: Does the research conducted in the paper conform, in every respect, with the   \n846 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n847 Answer: [Yes]   \n848 Justification: The research does not involve human subjects and does not introduce new data.   \n849 Its main impact should be to improve effectiveness and understanding of preference-based   \n850 post-training.   \n851 Guidelines:   \n852 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n853 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n854 deviation from the Code of Ethics.   \n855 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n856 eration due to laws or regulations in their jurisdiction).   \n857 10. Broader Impacts   \n858 Question: Does the paper discuss both potential positive societal impacts and negative   \n859 societal impacts of the work performed?   \n860 Answer: [Yes]   \n861 Justification: See Section 7   \n862 Guidelines:   \n863 \u2022 The answer NA means that there is no societal impact of the work performed.   \n864 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n865 impact or why the paper does not address societal impact.   \n866 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n867 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n868 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n869 groups), privacy considerations, and security considerations.   \n870 \u2022 The conference expects that many papers will be foundational research and not tied   \n871 to particular applications, let alone deployments. However, if there is a direct path to   \n872 any negative applications, the authors should point it out. For example, it is legitimate   \n873 to point out that an improvement in the quality of generative models could be used to   \n874 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n875 that a generic algorithm for optimizing neural networks could enable people to train   \n876 models that generate Deepfakes faster.   \n877 \u2022 The authors should consider possible harms that could arise when the technology is   \n878 being used as intended and functioning correctly, harms that could arise when the   \n879 technology is being used as intended but gives incorrect results, and harms following   \n880 from (intentional or unintentional) misuse of the technology.   \n881 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n882 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n883 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n884 feedback over time, improving the efficiency and accessibility of ML).   \n885 11. Safeguards   \n886 Question: Does the paper describe safeguards that have been put in place for responsible   \n887 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n888 image generators, or scraped datasets)?   \n889 Answer: [NA]   \n2 \u2022 The answer NA means that the paper poses no such risks.   \n3 Released models that have a high risk for misuse or dual-use should be released with   \n4 necessary safeguards to allow for controlled use of the model, for example by requiring   \n5 that users adhere to usage guidelines or restrictions to access the model or implementing   \n6 safety filters.   \n7 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n98 should describe how they avoided releasing unsafe images.   \n9 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n0 not require this, but we encourage authors to take this into account and make a best   \n1 faith effort. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "902 12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "03 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n04 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n05 properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The main external resource is the TLDR dataset, which we cite. Its license is CC BY 4.0. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "928 Answer: [NA]   \n929 Justification: No new assets are introduced.   \n930 Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \nResearchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "39 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "40 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n41 include the full text of instructions given to participants and screenshots, if applicable, as   \n42 well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}, {"type": "text", "text": "943   \n944   \n945   \n946   \n947   \n948   \n949   \n950   \n951   \n952   \n953   \n954   \n955   \n956   \n957   \n958   \n959   \n960   \n961   \n962   \n963   \n964   \n9 65   \n966   \n967   \n968   \n969   \n970   \n971   \n972 ", "page_idx": 25}]