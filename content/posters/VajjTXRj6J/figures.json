[{"figure_path": "VajjTXRj6J/figures/figures_4_1.jpg", "caption": "Figure 1: A toy illustration of Theorem 2, which states that the optimal \u03c0\u03c1* for (8) is the policy in PB(S) with the lowest forward-KL from \u03c0SFT. The set PB(S) contains a (potentially infinite) set of policies \u03c01, \u03c02,... corresponding to target reward models. Here, \u03c0SFT assigns equal mass to yw and yl, \u03c0MLE is the MLE solution for the DPO objective, which puts all probability mass on yw, and \u03c03 is the policy in PB(S) with lowest forward-KL.", "description": "This figure illustrates Theorem 2, which states that the optimal policy (\u03c0\u03c1*) for the pessimistic objective (8) is the policy within the set of possible policies consistent with implicit reward models (PB(S)) that has the lowest forward Kullback-Leibler (KL) divergence from the reference policy (\u03c0SFT).  The figure shows a toy example with three policies in PB(S) (\u03c01, \u03c02, and \u03c03), the maximum likelihood estimate policy from direct preference optimization (\u03c0MLE), and the reference policy (\u03c0SFT).  \u03c03 is identified as the optimal policy because it minimizes the KL divergence from \u03c0SFT while also being consistent with the pessimistic objective.", "section": "3.2 Pessimistic reward model distillation"}, {"figure_path": "VajjTXRj6J/figures/figures_6_1.jpg", "caption": "Figure 2: Main results, showing the advantage in oracle reward compared to the initial finetuned policy. Errorbars correspond to bootstrap 95% confidence intervals for finite sample variance. Ensemble DPO (e-DPO) is significantly better than DPO and IPO in the challenging setup where shorter responses are preferred (p \u2264 0.5), and is generally the best-performing method overall in this regime. Distilled DPO (d-DPO) performs best when longer responses are preferred (p > 0.6).", "description": "This figure displays the performance of different preference optimization methods across various levels of length bias in the TL;DR summarization task.  The x-axis represents the length bias (proportion of longer responses preferred), and the y-axis shows the advantage in oracle reward compared to the initial fine-tuned policy. Error bars represent the 95% confidence interval. The figure highlights that Ensemble DPO significantly outperforms other methods when shorter summaries are preferred, while Distilled DPO performs best when longer summaries are preferred.  Overall, it shows the robustness of the proposed methods to distribution shifts in preference data.", "section": "4 Experimental results"}, {"figure_path": "VajjTXRj6J/figures/figures_16_1.jpg", "caption": "Figure B.1: Effect of transitive closure on p-DPO and IPO solutions to preference learning in a multi-arm bandit. Each column shows the learned policy probability for a given arm, based on the preferences y1 < y2 < y3. The top row shows that in p-DPO, the probabilities are not materially affected by the transitive closure y1 < y3. The bottom row shows that in IPO, transitive closure causes the probabilities to be compressed. In each subfigure, we sweep a range of effective values of \u03c4\u22121, shown on the x-axis.", "description": "This figure compares the performance of pessimistic direct preference optimization (p-DPO) and identity preference optimization (IPO) in a multi-arm bandit setting.  It shows how the learned policy probabilities for three options (y1, y2, y3) change with different levels of regularization (\u03b2 for p-DPO and \u03c4 for IPO) and whether or not a transitive closure (y1 < y3) is considered. The key takeaway is that p-DPO is more robust to the transitive closure than IPO, maintaining more distinct probabilities for each option. IPO, on the other hand, shows a compression of probabilities as the regularization changes when the transitive closure is added.", "section": "B Transitive closure"}, {"figure_path": "VajjTXRj6J/figures/figures_17_1.jpg", "caption": "Figure C.1: We show for every length bias, p, the distribution over reward models that best match the final policy trained by e-DPO across all training examples. We observe that the e-DPO policy matches different reward models across examples. Moreover, when the policy is trained with data biased towards preferring short responses, the reward model that was trained on longer responses is often preferred and vice versa.", "description": "This figure visualizes the distribution of reward models selected by the ensemble DPO (e-DPO) method for different length biases in the training data.  It shows that e-DPO dynamically chooses different reward models depending on the bias present in the training data.  Specifically, when shorter responses are preferred in the training data, the model often selects reward models trained on datasets with a bias towards longer responses and vice versa, suggesting a form of self-correction against biases in the training data.", "section": "C Distribution over reward models for e-DPO"}, {"figure_path": "VajjTXRj6J/figures/figures_18_1.jpg", "caption": "Figure 2: Main results, showing the advantage in oracle reward compared to the initial finetuned policy. Errorbars correspond to bootstrap 95% confidence intervals for finite sample variance. Ensemble DPO (e-DPO) is significantly better than DPO and IPO in the challenging setup where shorter responses are preferred (p \u2264 0.5), and is generally the best-performing method overall in this regime. Distilled DPO (d-DPO) performs best when longer responses are preferred (p > 0.6).", "description": "This figure displays the performance of various preference optimization methods (DPO, IPO, d-DPO, dp-DPO, e-DPO, p-DPO) across different levels of length bias in the training data.  The y-axis represents the advantage in oracle reward compared to a baseline policy, showing how well each method aligns the language model's responses with human preferences. The x-axis indicates the proportion of examples where longer responses were preferred in the training data.  The figure demonstrates that e-DPO generally outperforms other methods, especially when shorter responses are preferred, while d-DPO excels when longer responses are preferred. Error bars show the 95% confidence interval.", "section": "4 Experimental results"}]