[{"figure_path": "NIcIdhyfQX/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of QDQ and the other baselines on the three Gym-MuJoCo tasks. All the experiment are performed on the MuJoCo \"-v2\" dataset. The results are calculated over 5 random seeds.med = medium, r = replay, e = expert, ha = halfcheetah, wa = walker2d, ho=hopper", "description": "This table compares the performance of the proposed QDQ algorithm with several state-of-the-art offline reinforcement learning algorithms on three MuJoCo tasks from the D4RL benchmark.  The results are presented for different dataset variations (medium, replay, expert) and averaged over five random seeds.  The table highlights QDQ's performance relative to other methods.", "section": "5.1 Performance on D4RL benchmarks for Offline RL"}, {"figure_path": "NIcIdhyfQX/tables/tables_8_1.jpg", "caption": "Table 2: Comparison of QDQ and the other baselines on the AntMaze tasks. All the experiment are performed on the Antmaze \"-v0\" dataset for the comparison comfortable with previous baseline. The results are calculated over 5 random seeds.", "description": "This table compares the performance of the proposed QDQ algorithm against several other state-of-the-art offline reinforcement learning methods on AntMaze tasks from the D4RL benchmark.  The results are shown for different AntMaze environments (umaze, umaze-diverse, medium-play, medium-diverse, large-play, and large-diverse), and the total score is also given.  The table indicates the average score over 5 random seeds for each algorithm and environment.", "section": "5.1 Performance on D4RL benchmarks for Offline RL"}, {"figure_path": "NIcIdhyfQX/tables/tables_28_1.jpg", "caption": "Table 1: Comparison of QDQ and the other baselines on the three Gym-MuJoCo tasks. All the experiment are performed on the MuJoCo \"-v2\" dataset. The results are calculated over 5 random seeds.med = medium, r = replay, e = expert, ha = halfcheetah, wa = walker2d, ho=hopper", "description": "This table compares the performance of the proposed QDQ algorithm against several state-of-the-art offline reinforcement learning algorithms on three MuJoCo tasks from the D4RL benchmark.  The results are averaged across five random seeds and show QDQ's performance relative to others across different dataset variations (medium, replay, expert).  Abbreviations are provided for clarity.", "section": "5.1 Performance on D4RL benchmarks for Offline RL"}, {"figure_path": "NIcIdhyfQX/tables/tables_29_1.jpg", "caption": "Table 1: Comparison of QDQ and the other baselines on the three Gym-MuJoCo tasks. All the experiment are performed on the MuJoCo \"-v2\" dataset. The results are calculated over 5 random seeds.med = medium, r = replay, e = expert, ha = halfcheetah, wa = walker2d, ho=hopper", "description": "This table compares the performance of the proposed QDQ algorithm with several state-of-the-art offline reinforcement learning algorithms on three MuJoCo tasks from the D4RL benchmark.  The results are presented for different dataset variations (medium, replay, expert) for each task (halfcheetah, hopper, walker2d). The table shows the average normalized scores across 5 random seeds, indicating the relative performance of each algorithm on these tasks and datasets.", "section": "5.1 Performance on D4RL benchmarks for Offline RL"}, {"figure_path": "NIcIdhyfQX/tables/tables_30_1.jpg", "caption": "Table 1: Comparison of QDQ and the other baselines on the three Gym-MuJoCo tasks. All the experiment are performed on the MuJoCo \"-v2\" dataset. The results are calculated over 5 random seeds.med = medium, r = replay, e = expert, ha = halfcheetah, wa = walker2d, ho=hopper", "description": "This table compares the performance of the proposed QDQ algorithm against other state-of-the-art offline reinforcement learning methods on three MuJoCo tasks from the D4RL benchmark.  It shows the average normalized scores achieved by each algorithm across five random seeds, with results broken down by dataset type (medium, replay, expert) for each task.", "section": "5.1 Performance on D4RL benchmarks for Offline RL"}]