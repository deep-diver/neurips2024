[{"figure_path": "NIcIdhyfQX/figures/figures_1_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the challenges in offline reinforcement learning caused by distribution shift.  Panel (a) shows how Q-value overestimation in out-of-distribution (OOD) actions leads to a learning policy diverging from the behavior policy.  Panel (b) highlights the trade-off between pessimism (avoiding overestimation) and optimism (approaching the optimal policy), showing that overly conservative methods may hinder performance.", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_14_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the problem of Q-value overestimation in offline RL.  Panel (a) shows how the learning policy's Q-value diverges from the behavior policy's in out-of-distribution (OOD) regions due to overestimation. Panel (b) compares different Q-value learning approaches, highlighting the trade-off between pessimism (avoiding OOD overestimation) and accuracy (approaching the optimal Q-value).", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_25_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates two key problems in offline RL. (a) shows how Q-value overestimation in out-of-distribution (OOD) actions leads to divergence between the learning policy and the behavior policy. (b) demonstrates the trade-off between being overly pessimistic in OOD regions and underestimating Q-values in the in-distribution region.", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_25_2.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy\u2019s action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the problem of Q-value overestimation in offline reinforcement learning.  Panel (a) shows how the learning policy's Q-values diverge from the behavior policy's action space in out-of-distribution (OOD) regions, leading to the prioritization of risky actions.  Panel (b) compares different Q-value training approaches, highlighting the challenges of balancing safety (pessimistic estimates in OOD regions) with optimality (approaching the optimal Q-value in the in-distribution region).", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_26_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure shows two plots illustrating the challenges of offline reinforcement learning.  The left plot (a) demonstrates how Q-value overestimation in out-of-distribution (OOD) regions can lead to the learning policy prioritizing risky actions. The right plot (b) compares the Q-values of an optimal policy, the behavior policy, and learning policies using different training approaches. It highlights the difficulty of balancing safety (pessimism in OOD regions) and accuracy (approaching optimal Q-values in the data distribution).", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_26_2.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the challenges in offline reinforcement learning due to the distribution shift problem.  (a) shows how Q-value overestimation in out-of-distribution (OOD) actions can lead the learning policy astray. (b) contrasts the optimal Q-value with the behavior policy's Q-value and demonstrates the trade-off between pessimism and accuracy in addressing the OOD issue.", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_27_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the challenges of offline RL training due to distribution shift.  Panel (a) shows how Q-value overestimation in out-of-distribution (OOD) actions leads to the learning policy prioritizing risky actions, ultimately hindering performance. Panel (b) highlights the difficulty of balancing Q-value safety and optimality: overly pessimistic adjustments can hinder performance while overly optimistic ones lead to overestimation issues.", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_27_2.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure shows two plots illustrating the problem of Q-value overestimation in offline reinforcement learning. (a) shows how the maximum Q-value often occurs for out-of-distribution (OOD) actions due to the distribution shift problem. (b) compares different Q-value functions, including the optimal Q-value, behavior policy's Q-value, in-sample Q-learning Q-value, and a more conservative Q-learning Q-value, highlighting the challenges of balancing safety and optimality.", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_30_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates two common problems in offline reinforcement learning.  (a) shows how Q-value overestimation in out-of-distribution (OOD) actions can cause the learning policy to diverge from the behavior policy. (b) shows that overly conservative Q-value estimations, while avoiding overestimation, can be overly pessimistic and fail to reach optimal Q-values.", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_31_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates two key issues in offline reinforcement learning. Subfigure (a) shows how Q-value overestimation in out-of-distribution (OOD) actions leads to the learning policy prioritizing risky actions, causing divergence from the behavior policy's action space. Subfigure (b) compares the optimal Q-value, behavior policy Q-value, in-sample trained Q-value and a more conservative Q-value.  It demonstrates the challenge of balancing safety (pessimism in OOD regions) and optimality (approaching optimal Q-values in the data distribution).", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_32_1.jpg", "caption": "Figure G.1: The derived Q-value distribution when using difference sliding step and same window width to scan over the trajectory's on halfcheetah-medium dataset.The width of the sliding window is set to 200. The Q-value is scaled to facilitate comparison.", "description": "This figure shows the effect of different sliding window step sizes (k=1, k=10, k=50) on the distribution of the derived Q-values.  A smaller step size leads to a more concentrated Q-value distribution, while a larger step size results in a sparser distribution.  The window width (T) is held constant at 200 across all three scenarios. The Q-values are normalized for easier comparison.", "section": "G.1 Real Q dataset generation"}, {"figure_path": "NIcIdhyfQX/figures/figures_33_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the problem of Q-value overestimation in offline reinforcement learning.  Panel (a) shows how the learning policy's Q-values diverge from the behavior policy's action space in out-of-distribution (OOD) regions, leading to overestimation. Panel (b) compares different Q-value estimation methods, highlighting the challenge of balancing safety (pessimistic estimates) with optimality (approaching the true optimal Q-values).", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_34_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the problem of Q-value overestimation in offline RL. (a) shows how the learning policy's Q-values diverge from the behavior policy's action space due to overestimation of out-of-distribution (OOD) actions. (b) compares different Q-value training approaches, highlighting the trade-off between pessimism and optimality.", "section": "1 Introduction"}, {"figure_path": "NIcIdhyfQX/figures/figures_35_1.jpg", "caption": "Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the \u201cdistribution shift\u201d problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy's action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.", "description": "This figure illustrates the problem of Q-value overestimation in offline reinforcement learning.  (a) Shows how the learning policy's Q-values diverge from the behavior policy's action space in out-of-distribution (OOD) regions, leading to the selection of risky actions. (b) Compares different Q-value training approaches, highlighting the trade-off between pessimism (avoiding OOD actions) and optimism (approaching optimal performance).", "section": "1 Introduction"}]