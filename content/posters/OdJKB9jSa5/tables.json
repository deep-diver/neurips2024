[{"figure_path": "OdJKB9jSa5/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparison of different sorting algorithms and our method.", "description": "This table compares the average time taken by different sorting algorithms (BubbleSort, HeapSort, ATk) and the proposed STk method to solve the Top-k problem.  The time is measured in seconds and represents the average of 50 experiments.  The results demonstrate that STk achieves linear time complexity while maintaining stability, significantly outperforming other methods.", "section": "4.1 Time Cost"}, {"figure_path": "OdJKB9jSa5/tables/tables_6_1.jpg", "caption": "Table 2: Accuracy and ParaF1-Score on the synthetic dataset.", "description": "This table presents the accuracy and ParaF1 score achieved when different aggregate loss functions are combined with the LR model and the cross-entropy loss, trained to convergence.  It shows the results for Average, Maximum, ATk, MATk losses, and for the proposed STk loss with different ReLU variants (ELU, SoftPlus, Leaky-ReLU, and SReLU). The table also shows the computation time taken by each method.", "section": "4.2 Gaussian Distributed Dataset"}, {"figure_path": "OdJKB9jSa5/tables/tables_6_2.jpg", "caption": "Table 3: Misclassification Rate(%) and Standard Derivation of Various Aggregate Losses Combined with Individual Logistic Loss.", "description": "This table presents the results of a binary classification experiment comparing different aggregate loss functions combined with individual logistic loss.  The misclassification rate and its standard deviation are reported for various datasets using Average, Maximum, ATk, MATk and the proposed STk loss functions.  The results illustrate the performance improvement achieved by using STk Loss, especially in terms of reducing the standard deviation of the error rate.", "section": "5.1 Binary Classification"}, {"figure_path": "OdJKB9jSa5/tables/tables_7_1.jpg", "caption": "Table 4: Misclassification Rate (%) and Standard Derivation of Various Aggregate Losses Combined with Individual Hinge Loss.", "description": "This table presents the results of experiments comparing different aggregate loss functions (Average, Maximum, ATk, MATk, and STk) combined with the individual Hinge loss for binary classification tasks.  The misclassification rate and standard deviation are shown for several datasets (appendicitis, wisconsin, australian, german, titanic, phoneme, spambase).  The STk method consistently shows either the lowest or a near-lowest misclassification rate and often demonstrates the lowest standard deviation, indicating improved stability and robustness.", "section": "5.1 Binary Classification"}, {"figure_path": "OdJKB9jSa5/tables/tables_9_1.jpg", "caption": "Table 5: Results on large long-tailed datasets are presented, where the first three columns indicate accuracy, and the last two columns show the BLEU scores. Values marked with an '*' represent the state-of-the-art (SOTA) on the leaderboard.", "description": "This table presents the results of the proposed STk method on several large, long-tailed datasets.  The first three columns show the accuracy achieved on CIFAR-100-LT, ImageNet-LT, and Places-LT, respectively. The last two columns display the BLEU scores obtained for machine translation tasks on IWSLT2014 and WMT2017 datasets.  The results demonstrate that the STk method surpasses the state-of-the-art (indicated by '*') on several benchmarks.", "section": "5.2 Long-Tailed Classification"}, {"figure_path": "OdJKB9jSa5/tables/tables_9_2.jpg", "caption": "Table 6: Comparison of Average RMSE and Standard Deviation for Different Aggregate Losses Combined with Square and Absolute Loss.", "description": "This table presents a comparison of the root mean squared error (RMSE) and standard deviation for different aggregate loss functions (Average, MATk, and STk) combined with both square and absolute individual losses.  The results are shown for four different regression datasets (Sinc, Housing, Abalone, and Cpusmall). It demonstrates the performance of the proposed STk loss compared to other aggregation methods.", "section": "5.3 Regression Tasks"}, {"figure_path": "OdJKB9jSa5/tables/tables_13_1.jpg", "caption": "Table 7: Ablation Study on Long-Tailed Learning Algorithms.", "description": "This table presents the results of an ablation study on long-tailed learning algorithms.  It shows the performance (accuracy on ImageNet-LT and CIFAR-100-LT datasets) achieved by using different combinations of techniques: MAE or CLIP pre-training, cost-sensitive learning (CS), Parameter-Efficient Long-Tailed (PEL) Recognition, and the proposed Smoothed Top-k (STk) module. The table helps to understand the individual and combined contributions of these methods to improving the performance on long-tailed image classification tasks.", "section": "5.2 Long-Tailed Classification"}, {"figure_path": "OdJKB9jSa5/tables/tables_13_2.jpg", "caption": "Table 8: Sensitive Analysis of the Smoothing Coefficient \u03b4.", "description": "This table presents the results of an ablation study on the smoothing coefficient (\u03b4) used in the STk loss function.  It shows how the performance metrics (accuracy for image classification datasets and BLEU score for translation datasets) vary across different values of \u03b4 on five real-world datasets: CIFAR-100-LT, ImageNet-LT, Place-LT, IWSLT2014, and WMT2017. The purpose is to demonstrate the sensitivity of the model's performance to this hyperparameter and identify an optimal or near-optimal value of \u03b4. The results help to demonstrate the robustness of STk loss to small variations in the choice of \u03b4. ", "section": "A.3 More Experiments"}, {"figure_path": "OdJKB9jSa5/tables/tables_13_3.jpg", "caption": "Table 9: Statistics of Benchmarks.", "description": "This table provides a detailed statistical overview of the datasets used in the paper's experiments. It includes the number of samples (n), features (d), and classes (c) for both regression and binary classification datasets.  The regression datasets are Sinc, Housing, Abalone, and Cpusmall, while the binary classification datasets are Appendicitis, Australian, German, Phoneme, Spambase, Titanic, and Wisconsin.", "section": "A.4 Benchmarks Detailed Information"}]