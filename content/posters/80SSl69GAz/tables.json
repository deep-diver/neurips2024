[{"figure_path": "80SSl69GAz/tables/tables_3_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) models and a standard Transformer model on the Wikitext-103 dataset.  It highlights SwitchHead's ability to match or exceed the performance of other models while using significantly less compute and memory resources. The table is sorted by model perplexity, showing the trade-off between performance and resource efficiency.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_5_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention (MoA) models and a standard Transformer on the Wikitext 103 dataset.  It demonstrates that SwitchHead achieves comparable or better perplexity with significantly less compute and memory usage than both MoA and the standard Transformer.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_6_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) models and a standard Transformer model on the Wikitext 103 dataset.  It highlights that while MoA can sometimes achieve slightly better perplexity scores, it requires significantly more computational resources (MACs and memory). SwitchHead consistently outperforms the standard Transformer while using substantially less compute and memory.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_7_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) and a standard Transformer on the Wikitext-103 dataset.  It demonstrates that SwitchHead achieves comparable or better perplexity (a lower value is better) than MoA and the standard Transformer while using significantly less compute and memory. The table highlights the trade-off between performance and resource consumption, showing that SwitchHead offers a better balance.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_7_2.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) and a standard Transformer model on the Wikitext-103 dataset. It demonstrates SwitchHead's superior performance in terms of perplexity, while using significantly less compute and memory compared to MoA and the baseline Transformer.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_13_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) models and a standard Transformer on the Wikitext-103 dataset.  It shows that while MoA can sometimes achieve slightly better perplexity, it requires significantly more compute and memory resources than SwitchHead.  Importantly, SwitchHead matches or exceeds the performance of the standard Transformer baseline while using substantially less compute and memory.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_14_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against different Mixture of Attention Heads (MoA) variants and a standard Transformer on the Wikitext-103 dataset. It demonstrates that SwitchHead achieves comparable or better perplexity (a measure of language model accuracy) while using significantly less compute and memory than MoA and the baseline Transformer.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_14_2.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) models and a standard Transformer baseline on the Wikitext-103 dataset.  The comparison is done in a parameter-matched setting, meaning the models have roughly the same number of parameters. The table shows that SwitchHead achieves comparable or better perplexity (a measure of language model performance) than MoA and the baseline Transformer while using significantly less compute and memory resources.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_15_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) models and a standard Transformer on the Wikitext 103 dataset.  It demonstrates that SwitchHead achieves comparable or better performance than MoA and the baseline Transformer while using significantly less compute and memory. The models are sorted by their perplexity, a measure of language model performance.", "section": "3 Experiments"}, {"figure_path": "80SSl69GAz/tables/tables_20_1.jpg", "caption": "Table 1: Performance of SwitchHead compared to different MoA variants. MoA can outperform the baseline, but only at a price of using significantly more compute and memory. Also, SwitchHead outperforms the baseline dense Transformer. These results are on Wikitext 103. Table sorted by model perplexity.", "description": "This table compares the performance of SwitchHead against various Mixture of Attention Heads (MoA) models and a standard Transformer model on the Wikitext 103 dataset.  It demonstrates that SwitchHead achieves comparable or better performance than other methods while using significantly less compute and memory. The table is sorted by model perplexity, a measure of language model performance.", "section": "3 Experiments"}]