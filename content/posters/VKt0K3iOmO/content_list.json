[{"type": "text", "text": "Spiking Graph Neural Network on Riemannian Manifolds ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Li Sun\u2217 North China Electric Power University Beijing 102206, China ccesunli@ncepu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Zhenhao Huang North China Electric Power University Beijing 102206, China huangzhenhao@necpu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Qiqi Wan North China Electric Power University Beijing 102206, China wanqiqi@ncepu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Hao Peng   \nBeihang University   \nBeijing 100191, China   \npenghao@buaa.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Philip S. Yu University of Illinois at Chicago IL, USA psyu@uic.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph neural networks (GNNs) have become the dominant solution for learning on graphs, the typical non-Euclidean structures. Conventional GNNs, constructed with the Artificial Neuron Network (ANN), have achieved impressive performance at the cost of high computation and energy consumption. In parallel, spiking GNNs with brain-like spiking neurons are drawing increasing research attention owing to the energy efficiency. So far, existing spiking GNNs consider graphs in Euclidean space, ignoring the structural geometry, and suffer from the high latency issue due to Back-Propagation-Through-Time (BPTT) with the surrogate gradient. In light of the aforementioned issues, we are devoted to exploring spiking GNN on Riemannian manifolds, and present a Manifold-valued Spiking GNN (MSG). In particular, we design a new spiking neuron on geodesically complete manifolds with the diffeomorphism, so that BPTT regarding the spikes is replaced by the proposed differentiation via manifold. Theoretically, we show that MSG approximates a solver of the manifold ordinary differential equation. Extensive experiments on common graphs show the proposed MSG achieves superior performance to previous spiking GNNs and energy efficiency to conventional GNNs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphs are the ubiquitous, non-Euclidean structures that describe the relationship among objects. Graph neural networks (GNNs), constructed with the floating-point Artificial Neuron Network (ANN), have achieved state-of-the-art accuracy for learning on graphs [1; 2; 3; 4]. However, they raise the concerns about computation and energy consumption, particularly when dealing with real-world graphs of considerable scale [5; 6]. In contrast, Spiking Neuron Networks (SNNs), inspired by the biological mechanism of brains, utilize neurons that communicate using sparse and discrete spikes, showcasing their superiority in energy efficiency [7; 8]. Attempting to bring the best of both worlds, spiking GNNs are drawing increasing research attention. ", "page_idx": 0}, {"type": "text", "text": "In the literature of spiking GNNs, recent efforts have been made to design different architectures with spiking neurons, e.g., graph convolution [5], attention mechanism [9], variational autoencoder [10] and continuous GNN [11]. While achieving encouraging results, existing spiking GNNs still face several fundamental issues: (1) Representation Space. Spiking GNNs consider the graph in Euclidean space, ignoring the inherent geometry of graph structures. Unlike the Euclidean structures (e.g., pixel matrix and grid structures), graphs cannot be embedded in Euclidean space with bounded distortion [12]. Instead, Riemannian manifolds have been shown as the promising spaces to model graphs in recent years [3; 13; 4] (e.g., hyperbolic space, a type of Riemannian manifolds, is well aligned with the graphs dominated by hierarchical structures). However, none of the existing works study SNN on Riemannian manifolds, to the best of our knowledge. It is thus an interesting and urgent problem to consider how to endow the spiking GNN with a Riemannian manifold. (2) Training Algorithm. Training spiking GNN is challenging, since the spikes are non-differentiable. Existing studies consider the spiking GNN as a recurrent neural network and apply Backward-PassingThrough-Time (BPTT) with the surrogate gradient [5; 9; 10; 11]. They recurrently compute the backward gradient at each time step, and thus suffer from the high latency issue [14; 15; 16; 6] especially when the spike trains are long. ", "page_idx": 1}, {"type": "text", "text": "Present work. Deviating from previous spiking GNNs in Euclidean space, in this paper, we open a new direction to explore spiking GNNs on Riemannian manifolds, and propose a novel Manifold-valued Spiking GNN (MSG) sketched in Fig. 1. It is not realistic to place spike trains in a manifold such as hyperbolic or hyperspherical space, given the fact that spike trains cannot align with the defining domain. Instead, we design a Manifold Spiking Layer that conducts parallel forwarding of spike trains and manifold representations. Specifically, we first incorporate the structural information ", "page_idx": 1}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/8befa1b17c0f73a5d0f2756fc8133712e402805df73ec908e0c6e43ec83a42cb.jpg", "img_caption": ["Figure 1: MSG conducts parallel forwarding and enables a new training algorithm alleviating the high latency issue. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "into spike trains by graph convolution. Then, a new manifold spiking neuron is proposed to emit spike trains and relate them to manifold representations with diffeomorphism, where the spike train generates a momentum that forwards manifold representation along the geodesic. Instead of applying BPTT in spike domain, the proposed neuron provides us with an alternative of Differentiation via Manifold $(D\\nu M)$ . (The red dashed line in Fig. 1.) Yet, differentiation in Riemannian manifold is nontrivial. We leverage the properties of pullback and derive the closed-form backward gradient (Theorem 4.1). $D\\nu M$ enables the recurrence-free gradient backpropagation, which no longer needs to perform recurrent computation of time steps as in BPTT. Theoretically, MSG is essentially related to manifold Ordinary Differential Equation (ODE). Each layer creates a chart of the manifold, and MSG approximates the dynamic chart solver [17] of manifold ODE (Theorem 5.2). ", "page_idx": 1}, {"type": "text", "text": "Contributions. Overall, the key contributions are summarized as follows: (1) To the best of our knowledge, we propose the first spiking neural network on Riemannian manifolds (MSG)2, and show its connection to manifold ODE theoretically. (2) We design a new training algorithm of differentiation via manifold, which avoids the high latency of BPTT methods. (3) Extensive experiments show the superior effectiveness and energy efficiency of the proposed MSG. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We briefly overview the ANN-based GNNs (i.e., the conventional, floating-point GNNs living in either Euclidean space or Riemannian manifolds) and SNN-based GNNs (i.e., spiking GNNs). ", "page_idx": 1}, {"type": "text", "text": "ANN-based GNNs (Euclidean and Riemannian). The majority of GNNs are built with floatingpoint ANN, conducting message passing on the graphs [18; 2; 19]. The Euclidean space has been the workhorse for graph representation learning for decades, and the popular GCN [18], GAT [2] and SGC [19] are also designed in the Euclidean space. In recent years, Riemannian manifolds have emerged as an exciting alternative considering the geometry of graph structures [20; 21]. Among Riemannian manifolds, hyperbolic space is recognized for its alignment with the graphs of hierarchical structures, and a series of hyperbolic GNNs (e.g., HGNN [22], HGCN [3]) show superior performance to their Euclidean versions. Beyond hyperbolic space, hyperspherical space is well suited for cyclical structures [23], and recent studies further investigate the constant curvature spaces [13], product spaces [24; 25; 26; 27], quotient spaces [28], SPD manifolds [29; 30], etc. Riemannian manifolds achieve remarkable success in graph clustering [31; 32], structural learning [33], graph dynamics [34; 35; 36; 37], information diffusion [38] and graph generation [39; 40], but have rarely been touched yet in the SNN counterpart. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Spiking Neural Networks (SNNs) & Spiking GNNs. Mimicking the biological neural networks, SNNs [7; 8] utilize the spiking neuron to process spike trains, and offer the advantage of energy efficiency. Despite the wide application of SNN in computer vision [41; 42], SNNs are still at an early stage in the graph domain. The basic idea of spiking GNNs is adapting ANN-based GNNs to the SNN framework by substituting the activation functions with spiking neurons. Pioneering works study the graph convolution [43; 5], and efforts have also been made to the graph attention [9], variational graph autoencoder [10], graph differential equations [44], etc. SpikeGCL [6] is a recent endeavor to conduct graph contrastive learning with SNN. In parallel, spiking GNNs are extended to model the dynamic graphs [45; 46; 47]. We focus on the static graph in this work. In both dynamic and static cases, previous spiking GNNs are trained with the surrogate gradient, leading to high latency, and consider the graphs in the Euclidean space. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Different from aforementioned spiking GNNs, we study the spiking GNN on Riemannian manifolds. Thus, we formally introduce the basic concepts of Riemannian geometry and SNN. Throughout this paper, the lowercase boldfaced $\\textbf{\\em x}$ and uppercase $\\mathbf{X}$ denote vector and matrix, respectively. Important notations are summarized in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Riemannian Geometry & Riemannian Manifold. Riemannian geometry provides elegant framework to study structures and manifolds. A Riemannian manifold is described as a smooth and real manifold $\\mathcal{M}$ endowed with a Riemannian metric. Each point $\\textbf{\\em x}$ in the manifold is associated with the tangent space $T_{\\mathbf{x}}\\mathcal{M}$ that \u201clooks Euclidean\u201d, and the Riemannian metric is given by the inner product in the tangent space, so that geometric properties (e.g, angle, length) can be defined. A geodesic between two points on the manifold is the smooth path connecting them with the minimal length. There exist three types of isotropic manifold, namely, the Constant Curvature Space (CCS): hyperbolic space $\\mathbb{H}$ , hyperspherical space $\\mathbb{S}$ and the special case of Euclidean space with \u201cflat\u201d geometry $\\mathbb{E}$ . ", "page_idx": 2}, {"type": "text", "text": "Graph & Riemannian Graph Representation Learning. A graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E},\\mathbf{F},\\mathbf{A})$ is defined on the node set $\\mathcal{V}$ and edge set $\\mathcal{E}\\subset\\mathcal{V}\\times\\mathcal{V}$ , and $\\mathbf{A}\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ is the adjacency matrix describing the structure information. Each node $v_{i}$ is associated with a feature $\\boldsymbol{\\mathscr{f}}_{i}$ , and node features are summarized in $\\mathbf{F}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ . In this paper, we resolve the problem of Riemannian Graph Representation Learning with SNN. Specifically, we seek a graph encoder $\\mathcal{F}_{\\theta}\\;:\\;v\\;\\mapsto\\;z$ where $z\\,\\in\\,{\\mathcal{M}}$ is a point on the manifold, instead of Euclidean space, and $\\mathcal{F}_{\\theta}$ is defined with an energy-efficient SNN. ", "page_idx": 2}, {"type": "text", "text": "Spiking Neural Network. SNNs are constructed by spiking neurons that communicate with each other by spike trains. Concretely, a spiking neuron is conceptualized as \u201ca capacitor of the membrane potential\u201d, and processes the spike trains by the following 3 phases [48]. First, the incoming current $I[t]$ is accumulated in the capacitor, leading to the potential buildup (integrate). When the membrane potential $V[t]$ reaches or exceeds a specific threshold $V_{t h}$ , the neuron emits a spike (fire). After that, the membrane potential is reset to the resting potential $V_{r e s e t}$ (reset). There are two popular spiking neurons: IF model and LIF model [49]. In particular, the three phases of IF model are formalized as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{Integrate:}\\ \\ V[t]=g(V[t-1],I[t])=V[t-1]+I[t]}\\\\ &{\\quad}&{\\mathrm{Fire:}\\ \\ S[t]=H(V[t]-V_{t h})}\\\\ &{\\mathrm{Reset:}\\ \\ V[t]=\\{(1-S[t])V[t]+S[t]V_{r e s t},}&{F i x e d-r e s e t,}\\\\ &{\\quad}&{\\mathrm{Subtraction-reset.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the incoming current $I[t]$ is related to the input spike train, and $V_{r e s e t}$ is lower than $V_{t h}$ . $t$ denotes the time index of the spike. The Heaviside function $H(\\cdot)$ is non-differentiable, $H(x)=1$ if $x\\geq0$ , and 0 otherwise. There are two options for reset, and fixed-reset is adopted in this paper. Overall, an IF model is given as $S[t]=\\operatorname{IFModel}(I[t])$ , and the only difference between IF model and LIF model lies in the definition of $g(\\cdot)$ in Eq. (1). In this paper, we are interested in designing a new spiking neuron on Riemannian manifold. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology: Manifold-valued Spiking GNN ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present a simple yet effective Manifold-valued Spiking GNN (MSG), which can be applied to any geodesically complete manifolds, e.g., the Constant Curvature Space (CCS), including hyperbolic space and hyperspherical space, or the product of CCS. In particular, we design a spiking neuron on Riemannian manifolds (named as Manifold Spiking Neuron) that allows for the differentiation via manifold. It provides a new perspective of training spiking GNN, so that we avoid the high latency of typical backward-passing-through-time (BPTT) training. ", "page_idx": 3}, {"type": "text", "text": "4.1 Manifold Spiking Layer ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/ecc568de134bd3d7cb9ece8f2c41dcfdea8a937d29c068f69c623ecd11d19c14.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "We elaborate on the sole building block of the proposed model \u2014 Manifold Spiking Layer. Note that, the spike train or spiking representation in existing spiking GNNs [43; 5; 9; 10; 6; 45; 46; 47] cannot align with the defining domain of Riemannian manifolds (e.g., hyperbolic space and hyperspherical space), thus posing a fundamental challenge. Our solution is to generate node representation on the manifold (referred to as manifold representation) in parallel, and leverage the notion of Diffeomorphism to create the alignment between the two domains. We formulate the parallel forwarding of spike trains and manifold representations as follows. ", "page_idx": 3}, {"type": "text", "text": "Unified Formulation. The forward pass of the spiking layer consists of a graph convolution and one proposed manifold spiking neuron. Without loss of generality, for each node $v_{i}\\in\\mathcal{G}$ , the l\u2212th spiking layer is formulated as follows, ", "page_idx": 3}, {"type": "text", "text": "Figure 2: Manifold Spiking Layer. It conducts parallel forwarding of spike trains and manifold representations, and creates an alternative backward pass (red dashed line). The backward gradient with \u2202\u2202vlW\u2212l1 , Dvl\u22121\u03c6l\u22121 and \u2207zlL will be introduced in Sec. 4.2. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i}^{l-1}[t]=\\operatorname{GCN}(\\mathbf{s}_{i}^{l-1}[t];\\mathbf{W}^{l}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n[\\mathbf{s}_{i}^{l},\\mathbf{z}_{i}^{l}]=\\mathrm{MSNeuron}(\\mathbf{x}_{i}^{(l-1)},\\mathbf{z}_{i}^{(l-1)}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{x}$ is the incoming current to generate spike trains. s and $\\mathbf{z}$ denote the spike trains and manifold representation, respectively. $\\mathrm{GCN}(\\cdot)$ is a GNN in the Euclidean space, and $\\mathbf{W}^{l}$ is the learnable parameter in the layer. Different from the neuron of previous spiking GNNs, we design a novel manifold spiking neuron (MSNeuron) as shown in Fig. 2. It emits spike trains relates them to manifold representations simultaneously, which is formulated as follows, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{s}_{i}^{l}=\\mathrm{IFModel}(\\{\\mathbf{x}_{i}^{l-1}[t]\\}_{t=1,\\dots,T})}\\\\ &{\\mathbf{v}_{i}^{l-1}=\\mathrm{Pooling}(\\mathbf{x}_{i}^{l-1}[t])}\\\\ &{\\quad\\mathbf{z}_{i}^{l}=f(\\mathbf{z}_{i}^{l-1},\\boldsymbol{\\epsilon}\\mathbf{v}_{i}^{l-1})\\in\\mathcal{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t$ is the time step of spike trains. The IF model can be replaced by LIF model, and we utilize IF model by default for simplicity. Pooling is defined as the average pooling of the current $\\mathbf{x}$ over $t$ , and $\\mathbf{v}$ is given as the Euclidean vector. $f$ denotes the diffeomorphism to the Riemannian manifold in which $\\epsilon$ is the step size. ", "page_idx": 3}, {"type": "text", "text": "Incorporating structural information. We inject the structural information when the received spikes transform into the incoming current of the neuron. A GNN is leveraged to define the current, conducting message-passing over the graph. Each node\u2019s representation is derived recursively by neighborhood aggregation [18; 2; 1]. Accordingly, GCN in the proposed neuron is given as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{GCN}(\\mathbf{s}_{i}^{l-1}[t];\\mathbf{W}^{l})=\\operatorname{combine}(\\mathbf{s}_{i}^{l-1}[t],\\operatorname{aggregate}(\\{\\mathbf{s}_{j}[t]:j\\in\\Omega_{i}\\};\\mathbf{W}^{l})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the neighborhood $\\Omega_{i}$ is the set of immediate neighbors centering at node $v_{i}$ . The aggregate function aggregates the messages from neighborhood $\\Omega_{i}$ , where we create the message of a node by $\\mathbf{W}^{l}\\mathbf{s}_{j}[t]$ . combine $(\\cdot)$ denotes the combination of the center node\u2019s message and aggregated message. We utilize GCN [18] as the backbone to define aggregate and combine. ", "page_idx": 4}, {"type": "text", "text": "Diffeomorphism between manifolds. In the proposed neuron, we bridge the spikes and manifold representation with the notion of diffeomorphism in differential geometry. A diffeomorphism connects two smooth manifolds, saying $\\mathcal{M}$ and $\\mathcal{N}$ . Formally, a map $f:\\mathcal{M}\\rightarrow\\mathcal{N}$ is a diffeomorphism between $\\mathcal{M}$ and $\\mathcal{N}$ if the smooth $f$ is bijective and its inverse $f^{-1}$ is also smooth. ", "page_idx": 4}, {"type": "text", "text": "Recall that the tangent space is locally Euclidean. We propose to place the Euclidean $\\mathbf{v}$ , a representation of the spikes, in the tangent space $T_{\\mathbf{z}}\\mathcal{M}$ of the point $\\mathbf{z}$ . In MSG, we choose the exponential map to act as the diffeomorphism between the tangent space and manifold. With a step size $\\epsilon$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\mathbf{z}_{i}^{(l-1)},\\epsilon\\mathbf{v}_{i}^{(l-1)})=\\mathrm{Exp}_{\\mathbf{z}_{i}^{(l-1)}}(\\epsilon\\mathbf{v}_{i}^{(l-1)})\\in\\mathcal{M}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Concretely, given $\\mathbf{z}\\in\\mathcal{M}$ and $\\mathbf{v}\\in T_{\\mathbf{z}}\\mathcal{M}$ , the exponential $\\mathrm{map}^{3}$ of $\\mathbf{v}$ at point $\\mathbf{z}$ , $\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{v}):T_{\\mathbf{z}}\\mathcal{M}\\rightarrow$ $\\mathcal{M}$ , maps tangent vector $\\mathbf{v}$ onto the manifold $\\mathcal{M}$ . The map pushes $\\mathbf{z}$ along the geodesic $\\gamma_{\\mathbf{z},\\mathbf{v}}(t):$ $[0,1]\\rightarrow{\\mathcal{M}}$ starting at $\\gamma_{\\mathbf{z},\\mathbf{v}}(0)\\,=\\,\\mathbf{z}$ and ending at $\\mathbf{y}\\,=\\,\\gamma_{\\mathbf{z},\\mathbf{\\bar{v}}}(1).\\;\\,\\dot{\\gamma}_{\\mathbf{z},\\mathbf{v}}(t)$ denotes the velocity of $\\gamma_{\\mathbf{z},\\mathbf{v}}(t)$ , and the direction of geodesic at the beginning is given as ${\\dot{\\gamma}}_{\\mathbf{z},\\mathbf{v}}(0)=\\mathbf{v}$ . That is, the tangent vector $\\mathbf{v}$ , derived from the spikes, pushes the manifold representation along the geodesic via the exponential map. The advantage of our choice is that we are able to define the diffeomorphism in arbitrary geodesically complete manifold (detailed in Appendix D). ", "page_idx": 4}, {"type": "text", "text": "Note that, our idea is inherently different from the exponential/logarithmic based Riemannian GNNs [3; 4; 22], which leverage the tangent space of the origin for neighborhood aggregation. In contrast, we consider the successive process over the tangent spaces of manifold representations, which will be further studied in Sec. 5. ", "page_idx": 4}, {"type": "text", "text": "Model Initialization In MSG, we need to simultaneously initialize the spiking input $\\mathbf{S}^{0}$ and manifold representation ${\\bf Z}^{0}$ , which is a collection of points on the given manifold. Given a graph $\\mathcal{G}(\\mathcal{V},\\mathcal{E},\\mathbf{F},\\mathbf{A})$ , the node features are first encoded by one graph convolution layer ${\\bf H}=\\mathrm{GCN}({\\bf A},{\\bf F};{\\dot{\\bf W}}^{0})$ , and we generate $T$ copies of the node encodings $\\mathbf{H}$ , where $T$ is the number of time steps in spike trains. Then, we complete model initialization with the proposed manifold neuron $[\\mathbf{S}^{0},\\mathbf{Z}^{0}]^{\\circ}]=\\mathrm{M}\\hat{\\mathrm{SNeuron}}(\\mathbf{H},\\mathbf{O})$ , where the encoding $\\mathbf{H}$ is regarded as the incoming current that charges the neuron in each time step. $\\mathbf{O}$ consists of the original points of the manifold, e.g., in the sphere model of hyperspherical space, the original point is given as the south pole $\\mathbf{o}=[-\\bar{1},0,...,0]_{.}^{\\bar{\\intercal}}$ and $\\mathbf{O}=[\\mathbf{o}^{\\top},...,\\mathbf{o}^{\\top}]^{\\top}$ . Note that, the exponential map in the proposed neuron guarantees that $\\bar{\\mathbf{Z}}^{0}$ lives in the manifold. ", "page_idx": 4}, {"type": "text", "text": "4.2 Learning Approach: Differentiation via Manifold ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Optimizing SNNs is challenging, as the Heaviside step function is non-differentiable. In the literature, existing spiking GNNs typically regard SNN as the recurrent neural network, and leverage backwardpassing-through-time (BPTT) to train the model [50; 51; 52]. Concretely, given a real loss function $\\mathcal{L}$ , the gradient backpropagation conducts Differentiation via Spikes $(D\\nu S)$ s as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{W}^{l}}\\mathcal{L}=\\sum_{t}[\\frac{\\partial\\mathbf{s}^{l}[t]}{\\partial\\mathbf{W}^{l}}]^{*}\\nabla_{\\mathbf{s}^{l}[t]}\\mathcal{L},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where W is the parameter, and $t$ denotes the time step. The surrogate gradient [51] is required for $D_{\\mathbf{W}^{l}}\\mathbf{s}^{l}[t]$ , where Heaviside step function is replaced by a differentiable surrogate, e.g., sigmoid function. The differentiation via spikes presents high latency in the backward pass [14; 15; 16], as it needs to recur all the time steps in BPTT. We notice that, in the computer vision domain, the sub-gradient method [15] is proposed to address such issues in Euclidean space. However, it cannot be generalized to the Riemannian manifold since the linearity does not hold in Riemannian geometry. ", "page_idx": 4}, {"type": "text", "text": "In MSG, we decouple the forward pass and backward pass, and propose Differentiation via Manifold $(D\\nu M)$ to avoid the high latency in differentiation via spikes. The overall procedure of training MSG by the proposed learning approach is summarized in Algorithm 1. Thanks to the parallel forwarding of spikes and manifold representation, the proposed neuron provides us with an alternative of studying $\\nabla_{\\mathbf{W}^{l}}\\mathcal{L}$ through the forwarding pass on the manifold (i.e., differentiation via manifold). Nevertheless, it is nontrivial and it requires to derive the pullback between different dual spaces. ", "page_idx": 5}, {"type": "text", "text": "Pushforward, Pullback and Dual Space. We first introduce the differentiation in Riemannian geometry which is essentially different from that in Euclidean space. In Riemannian geometry, a pushforward refers to a derivative of a map connecting two manifolds $\\mathcal{M}$ and $\\mathcal{N}$ . Concretely, given $f:\\mathcal{M}\\to\\mathcal{N}$ and a point $\\textbf{z}\\in\\mathcal{M}$ , the pushforward $D_{\\mathbf{z}}f$ maps a tangent vector $\\mathbf{v}\\in T_{\\mathbf{z}}\\mathcal{M}$ to the tangent vector $D_{\\mathbf{z}}f(\\mathbf{v})\\in T_{f(\\mathbf{z})}\\mathbf{\\mathcal{N}}$ . On the notation, for a manifold-valued function $f(\\mathbf{z})=$ $\\mathbf{p}\\in\\mathcal{N},\\partial\\mathbf{p}/\\partial\\mathbf{z}$ is equivalent to $D_{\\mathbf{z}}f$ ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Training MSG by the proposed Differentiation via Manifold   \nInput: Graph $\\mathcal{G}(\\mathcal{V},\\mathcal{E},\\mathbf{F},\\mathbf{A})$ , Manifold $\\mathcal{M}$ , Loss function over the manifold $\\mathcal{L}(\\cdot)$ , Number of spiking layers $L$ , Original points $\\mathbf{O}$ .   \nOutput: Parameters $\\{\\mathbf{W}^{l}\\}_{l=0,\\cdots,L}$   \n1: while not converging do   \n2: $\\vartriangleright$ forward pass 3: Input current ${\\bf X}^{0}=\\mathrm{GCN}({\\bf A},{\\bf F};{\\bf W}^{0})$ ; 4: Initialize $[{\\bf S}^{0},{\\bf Z}^{0}]=\\mathrm{MSNeuron}({\\bf X}^{0},{\\bf O})$ ; 5: for each spiking layer $l=1$ to $L$ do   \n6: $\\mathbf{X}^{(l-1)}\\mathbf{\\bar{\\alpha}}=\\mathrm{GCN}(\\mathbf{A},\\mathbf{S}^{(l-1)};\\mathbf{W}^{l})$ ;   \n7: $[\\mathbf{S}^{l},\\mathbf{Z}^{l}]=\\mathrm{MSNeuron}(\\mathbf{X}^{(l-1)},\\mathbf{Z}^{(l-1)});$ ; 8: end for   \n9: $\\vartriangleright$ backward pass 10: Compute $\\nabla_{\\mathbf{z}^{L}}\\mathcal{L}$ from $\\mathcal{L}(\\mathbf{Z}^{L})$ .   \n11: for layer $l=L-1$ to 1 do   \n12: Compute Dzl\u03c8l, Dvl\u22121\u03c6l\u22121, \u2202\u2202vlW\u2212l1 . 13: Compute $\\nabla_{\\mathbf{z}^{l}}\\mathcal{L},\\nabla_{\\mathbf{W}^{l}}\\mathcal{L}$ as Eq. 12.   \n14: Update $\\mathbf{W}^{l}$ .   \n15: end for   \n16: end while ", "page_idx": 5}, {"type": "text", "text": ". For a scalar function $f,D_{\\mathbf{z}}f$ is interchangeable with $\\nabla_{\\mathbf{z}}f$ . ", "page_idx": 5}, {"type": "text", "text": "In the proposed MSG, we consider a scalar loss function on the manifold $\\mathcal{L}:\\mathcal{M}\\rightarrow\\mathbb{R}$ . The pushforward $\\mathit{D}_{\\mathbf{z}}\\mathcal{L}$ at point $\\mathbf{z}\\in\\mathcal{M}$ maps tangent vector $\\mathbf{v}\\in T_{\\mathbf{z}}\\mathcal{M}$ to a scalar value and, correspondingly, $\\b{D_{\\mathbf{z}}}\\b{\\mathcal{L}}$ belongs to the dual space of the tangent space $T_{\\mathbf{z}}^{*}\\mathcal{M}$ , which is a vector space consisting all linear functional $F\\,:\\,T_{\\mathbf{z}}\\mathcal{M}\\,\\rightarrow\\,\\mathbb{R}$ . As the tangent spaces at different points of the manifold are different, it requires a pullback that maps the dual space $T_{\\mathbf{z}^{l}}^{*}M$ to the dual space $T_{\\mathbf{z}^{(l+1)}}^{*}M$ . ", "page_idx": 5}, {"type": "text", "text": "We derive the backward gradient with properties of differential 1\u2212form (Lemma B.1), communication (Lemma B.2), and pullback of a sum and a product (Lemma B.3) detailed in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Backward Gradient). Let $\\mathcal{L}$ be the scalar-valued function, and $\\mathbf{z}^{l}$ is the output of l-th layer with parameter $\\mathbf{W}^{l}$ , which is delivered by tangent vector $\\dot{\\mathbf{v}}^{l}$ . Then, the gradient of function $\\mathcal{L}$ $w.r t\\,{\\bf W}^{l}$ is given as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{W}^{l}}\\mathcal{L}=[\\frac{\\partial\\mathbf{v}^{l-1}}{\\partial\\mathbf{W}^{l}}]^{*}[D_{\\mathbf{v}^{l-1}}\\phi^{l-1}]^{*}\\nabla_{\\mathbf{z}^{l}}\\mathcal{L},\\quad\\nabla_{\\mathbf{z}^{l}}\\mathcal{L}=[D_{\\mathbf{z}^{l}}\\psi^{l}]^{*}\\nabla_{\\mathbf{z}^{l+1}}\\mathcal{L},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi^{l-1}(\\cdot)=\\mathrm{Exp}_{{\\bf z}^{l-1}}(\\cdot),\\,\\psi^{l}(\\cdot)=\\mathrm{Exp}_{(\\cdot)}({\\bf v}^{l}),$ , and $[\\cdot]^{*}$ means the matrix form of pullback. ", "page_idx": 5}, {"type": "text", "text": "The detailed proof is given in Appendix B.1, and we derive the two Jacobian matrices $D_{\\mathbf{v}^{l-1}}\\phi^{l-1}$ and $D_{\\mathbf{z}^{l}}\\psi^{l}$ in Appendix C. There are three key advantages of the proposed $D\\nu M.$ . First, every term in Equation (12) is differentiable, and thereby the surrogate gradient is no longer needed. Second, $D\\nu M$ enables the recurrence-free backward pass alleviating the high latency training. We specify that both $D\\nu M$ and the previous $D\\nu S$ recurrently compute every time step in the forward pass, and the difference lies in the backward pass. In particular, we only conduct recurrence-free gradient backpropagation layer by layer, while the previous $D\\nu S$ recurs every time step of each layer in BPTT. In addition to the differentiable and recurrence-free properties, $D\\nu M$ does not suffer from gradient vanishing/explosion, and the empirical evidence is provided in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "5 Theory: MSG as Neural ODE Solver ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we demonstrate the theoretical aspects of our model that MSG approximates a solver of manifold Ordinary Differential Equations (ODEs). ", "page_idx": 5}, {"type": "text", "text": "We leverage the notion of chart to study the relationship between MSG and manifold ODE. A manifold ODE defined as ", "page_idx": 6}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/af57645402904eb3d6de0bec50dd0b71d728c6b0addb48119fad4792854fcea6.jpg", "img_caption": ["Figure 3: Charts given by the logarithmic map. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$\\mathbf{y}(t):[\\tau,\\tau+\\epsilon]\\rightarrow\\mathbb{R}^{n}$ is the solution of ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{z}(t)}{d t}=u(\\mathbf{z}(t),t),\\quad\\mathbf{z}(t)\\in\\mathcal{M},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "describes a vector field $u$ that maps a smooth path $\\mathbf{z}(t):[0,1]\\rightarrow\\mathcal{M}$ to the tangent bundle $T\\mathcal{M}^{\\mathrm{~4~}}$ . In other words, the vector field $u$ assigns each point $\\mathbf{z}(t)\\in\\mathcal{M}$ to a tangent vector $u(\\mathbf{z}(t),t)\\in T_{\\mathbf{z}(t)}\\dot{\\mathcal{M}}$ . A chart at ${\\bf z}$ , denoted as $(U_{\\mathbf{z}},\\phi_{\\mathbf{z}})$ , is a smooth bijection $\\phi_{\\mathbf{z}}$ between ${\\bf z}$ \u2019s neighborhood $U_{\\mathbf{z}}\\subset\\mathcal{M}$ and a subspace of Euclidean space. Thus, the chart relates Eq. (13) to ODE in Euclidean space [17]. Specifically, if ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{y}(t)}{d t}=\\big(D_{\\phi_{i}^{-1}(\\mathbf{y}(t))}\\phi_{i}\\big)u\\big(\\phi_{i}^{-1}(\\mathbf{y}(t)),t\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then $\\mathbf{y}(t)=\\phi_{i}(\\mathbf{z}(t))$ is a valid solution of Eq. (13) on $t\\in[\\tau,\\tau+\\epsilon]$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 5.1 (Dynamic Chart Solver [17]). The manifold ODE in Eq. (13) with initial condition $\\mathbf{z}(0)=\\mathbf{z}$ can be solved with a finite collection of successive charts $\\{(U_{i},\\phi_{i})\\}_{i=1,...,L}$ . $I f_{0}\\mathrm{de}_{i}$ is the numerical solver to Euclidean ODE corresponding to the $i$ -th chart, $\\mathbf{y}(t)=\\mathrm{ode}_{i}(t)$ on $[\\tau_{i},\\tau_{i}+\\epsilon_{i}]$ , then ${\\bf z}(t)$ in Eq. (13) is given as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\phi_{L}^{-1}\\circ\\mathrm{ode}_{L}\\circ(\\phi_{L}\\circ\\phi_{L-1}^{-1})\\circ\\dots\\circ(\\phi_{2}\\circ\\phi_{1}^{-1})\\circ\\mathrm{ode}_{1}\\circ\\phi_{1})(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "That is, a manifold ODE can be solved in Euclidean subspaces given by a series of successive charts. In MSG, we consider the charts given by the logarithmic map as illustrated in 3, and we prove that MSG approximates a dynamic chart solver of manifold ODE (Theorem 5.2). ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2 (MSG as Dynamic Chart Solver). If $\\mathbf{\\hat{y}}(t):[\\tau,\\tau+\\epsilon]\\rightarrow\\mathbb{R}^{n}$ is the solution of ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{y}(t)}{d t}=(D_{\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{y}(\\mathbf{t}))}\\,\\mathrm{Log}_{\\mathbf{z}})u(\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{y}(t)),t),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then $\\mathbf{z}(t)=\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{y}(t))$ is a valid solution to the manifold ODE of Eq. (13) on $t\\in[\\tau,\\tau+\\epsilon]$ , where $\\mathbf{z}=\\mathbf{z}(\\tau)$ . If ${\\bf y}(t)$ is given by the first-order approximation with the \u03f5 small enough, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{y}(\\tau+\\epsilon)=\\epsilon\\cdot(D_{\\mathbf{z}}\\log_{\\mathbf{z}})u(\\mathbf{z}(\\tau),\\tau),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then the update process of Eqs. (4) and (8) in MSG is equivalent to Dynamic Chart Solver in Eq. (15). ", "page_idx": 6}, {"type": "text", "text": "Proof. The proof utilizes some facts of Riemannian manifolds and is detailed in Appendix B.2. ", "page_idx": 6}, {"type": "text", "text": "In other words, in MSG, the transformation of manifold input and output is described as some manifold ODE, whose vector field is governed by a spiking-related neural network in the tangent bundle. To solve the manifold ODE, MSG leverages the Dynamic Chart Solver (Definition 5.1). Specifically, each manifold spiking layer corresponds to a chart, and thus the number of spiking layers equals to the number of charts. Each layer solves the ODE of a smooth path $\\mathbf{y}(t):[\\tau,\\tau+\\epsilon]\\rightarrow\\mathbb{R}^{n}$ in the tangent space that centered at the manifold layer input. With the first-order approximation in Theorem 5.2, given a step size $\\epsilon$ , the endpoint ${\\bf y}(\\tau+\\epsilon)$ of the path is parameterized by a GNN related to the spikes. Layer-by-layer forwarding solves the manifold ODE from the current chart to the successive chart. Consequently, the manifold output of MSG approximates the solution to the manifold ODE. ", "page_idx": 6}, {"type": "text", "text": "We notice that a recent work [11] connects spiking GNN to an ODE in Euclidean space. In contrast, the proposed MSG is essentially related to the manifold ODE. ", "page_idx": 6}, {"type": "text", "text": "The Appendix contains the proofs, the derivation of Jacobian, necessary facts on Riemannian geometry (i.e., Lorentz/Sphere model, stereographic projection and $\\kappa$ -stereographic model, and Cartesian product and product space), empirical details and additional results. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Node Classification (NC) in terms of classification accuracy $(\\%)$ and Link Prediction in terms of AUC $(\\%)$ on Computers, Photo, CS and Physics datasets. The best results are boldfaced, and the runner-ups are underlined. The standard derivations are given in the subscripts. ", "page_idx": 7}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/c22460b15dad7f7d6b2736d3261719fa679f6aff1d8ee93e2be4e975319a6bc4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct extensive experiments with 12 strong baselines to evaluate the proposed MSG in terms of (1) the representation effectiveness, (2) the energy efficiency, and (3) the advantages of the proposed components. Additional results are presented in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "6.1 Experimental Setups ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. Our experiments are conducted on 4 commonly used benchmark datasets including two popular co-purchase graphs: Computers and Photo[53], and two co-author graphs: CS and Physics [53]. Datasets are detailed in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We compare the proposed MSG with 12 strong baselines of three categories: (1) ANNbased Euclidean GNNs: the popular GCN [18], GAT [2], GraphSAGE [1] and SGC [19], (2) ANN-based Riemannian GNNs: HGCN [3] and HyboNet [54] of hyperbolic spaces, $\\kappa{\\mathrm{-GCN}}$ [13] of the constant curvature space, and the recent $Q{\\mathrm{-}}\\mathrm{GCN}$ [4] of the quotient space, (3) The previous Euclidean Spiking GNNs: SpikeNet [45], SpikeGCN [5], SipkeGraphormer [55] (termed as SpikeGT for short) and the recent SpikeGCL [6]. Note that, we focus on the graph representation learning on static graphs, and thereby graph models for the dynamic ones are out of the scope of this paper. SpikeNet [45] was originally designed for dynamic graphs, and we utilize its variant for static graphs according to [6]. So far, spiking GNN has not yet been connected to Riemannian manifolds, and we are devoted to bridging this gap. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Protocol. All models are evaluated by node classification and link prediction tasks. The evaluation metrics of node classification is classification accuracy; we employ the popular Area Under Curve (AUC) for link prediction. The hyperparameter setting is the same as the original papers. We perform 10 independent runs for each case, and report the mean with standard derivations. Experiments are conducted on the hardware of NVIDIA GeForce RTX 4090 GPU 24GB memory, and AMD EPYC 9654 CPU with 96-Core Processor. Our model is built upon GeoOpt [56], SpikingJelly [56] and PyTorch [57]. ", "page_idx": 7}, {"type": "text", "text": "Model Instantiation & Configuration. Note that, the proposed MSG applies to any Constant Curvature Space (CCS) or the product of CCS. We instantiate MSG in the Lorentz model of hyperbolic space by default (whose Riemannian metric, exponential map, and the derived Jacobian is given in Appendix C), and study the impact of representation space in the Ablation Study. The dimension of the representation space is set as 32. The manifold spiking neuron is based on the IF model [49] by default, and it is ready to switch to the LIF model [49] whose results are given in Appendix F. The time steps $T$ for neurons is set to 5 or 15. The step size $\\epsilon$ in Eq. 8 is set to 0.1. The hyperparameters are tuned with grid search, in which the learning rate is $\\left\\lbrace0.01,0.003\\right\\rbrace$ for node classification and $\\{0.003,0.001\\}$ for link prediction, and the dropout rate is in $\\{0.1,0.3,0.5\\}$ . We provide the source code of MSG at the anonymous link https://github.com/ZhenhHuang/MSG. ", "page_idx": 7}, {"type": "text", "text": "6.2 Results & Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effectiveness. We evaluate the effectiveness of MSG in both node classification and link prediction tasks. Specifically, for node classification, we cannot directly feed the manifold representations of Riemannian baselines to a softmax layer with Euclidean measure. We bridge the manifold representation and Euclidean softmax with the logarithmic map of respective manifold. For link prediction, we utilize the generalized sigmoid for all the baselines, i.e., the Fermi-Dirac decoder [3] in which the distance function is defined under the respective geometry. In the proposed MSG, the model inference does not need the expensive successive exponential maps, and only limited float-point operations (i.e., addition) are involved. Accordingly, we leverage the tangent vectors for the downstream tasks. The performance of both learning tasks on Computer, Photo, CS and Physics datasets are collected in Table 1. Note that, SpikeNet and SpikeGT cannot do link prediction, since they are designed for node classification and do not offer spiking representation. The proposed MSG consistently achieves the best results among SNN-based models. In addition, MSG generally outperforms the best ANN-based baselines in node classification, and has competitive results to the recent ANN-based Riemannian baselines in link prediction. ", "page_idx": 8}, {"type": "text", "text": "Ablation Study. Here, we examine the impact of representation space and the effectiveness of the proposed Differentiation via Manifold $(D\\nu M)$ . For the former goal, we instantiate 6 geometric variants of MSG in hyperbolic space $\\mathbb{H}^{32}$ , hyperspherical space $\\mathbb{S}^{32}$ , Euclidean space ${\\mathbb{E}}^{{\\bar{3}}2}$ and the products of $\\mathbb{H}^{16}\\!\\times\\!\\mathbb{H}^{16}$ , $\\mathbb{H}^{16}\\!\\times\\!\\mathbb{S}^{16}$ and $\\bar{\\mathbb{S}}^{16}\\!\\times\\!\\mathbb{S}^{16}$ . ", "page_idx": 8}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/60fd9aff3fbd9b31e8188243a728698236944881b25be8b672941cf56ca7f687.jpg", "table_caption": ["Table 2: Ablation study of geometric variants. Results of node classification in terms of ACC $(\\%)$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "of representation space, and we leverage $D\\nu M$ for optimization. Manifold variants generally achieve superior results to the Euclidean one, thus verifying our motivation. On CS dataset, the performance of geometric variants is aligned with that of Euclidean and Riemannian baselines in Table 1. The proposed MSG is ready to switch among H, S, $\\mathbb{E}$ , and their products, matching the geometry of graphs. ", "page_idx": 8}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/086de0fc436a08e3f0deb2b6e21aac98dc556bbc929fed9c4d39fa040c197cdb.jpg", "img_caption": ["Figure 4: Backward time and gradient norm for node classification on Computer. "], "img_footnote": ["The superscript denotes the dimension "], "page_idx": 8}, {"type": "text", "text": "To examine the effectiveness of $D\\nu M$ , we design the optimization variant (named as Surrogate) for a given representation space. In the variant, we conduct differentiation via spikes and leverage BPTT for optimization, same as previous spiking GNNs. The training time of the optimization variants in different representation spaces are given in Fig. 4(a). Backward time of $D\\nu M$ is significantly less than that of BPTT algorithm. The reason is that $D\\nu M$ no longer needs recurrent gradient calculation of each time step (recurrence-free), while BPTT leads to high training time especially when the time step is large. In addition, we examine the backward gradient of $D\\nu M$ , and plot the gradient norm of each layer in Fig. 4(b). It demonstrates that $D\\nu M$ does not suffer from gradient vanishing/explosion. ", "page_idx": 8}, {"type": "text", "text": "Energy Cost. We investigate the energy cost of the graph models in terms of theoretical energy consumption (mJ) [5; 6], whose formula is specified in Appendix E. We summarize the results for node classification in Table 3 in which the number of parameters at the running time is listed as a reference. It shows that SNN-based models generally enjoy less energy cost than ANN-based ones. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Energy cost. The number of parameters at the running time (KB) and theoretical energy consumption (mJ) on Computers, Photo, CS and Physics datasets. The best results are boldfaced, and the runner ups are underlined. ", "page_idx": 9}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/4b5f2d7298208458c18c9f123d4dccf5017b2c7959c19a5f08a8d9b07b28f30d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Note, MSG achieves the best energy efficiency among SNN-based models except Photo dataset. In addition, it has at least $1/20$ energy cost to the Riemannian baselines. ", "page_idx": 9}, {"type": "text", "text": "Visualization & Discussion. We empirically study the connection between the proposed MSG and manifold ODE. In particular, we visualize a toy example of Zachary Karate Club dataset [58] on a $\\mathbb{S}^{1}\\times\\Bar{\\mathbb{S}}^{1}$ in Fig. 5, where we plot each layer output on the manifold. The red curve is the path connecting the layer input and layer output, and the blue one is the direction of the geodesic. As shown in Fig. 5, the red and blue curves are coincided, that is, each layer solves an ODE describing the geodesic on the manifold. ", "page_idx": 9}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/4de49935cdc59bc970e34a0219601b4d82e8325f1b10c33f7d711f58f6384b8f.jpg", "img_caption": ["Figure 5: Visualization on ${\\mathbb S}^{1}\\times{\\mathbb S}^{1}$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study spiking GNN from a fundamentally different perspective of Riemannian geometry, and present a simple yet effective Manifold-valued Spiking GNN (MSG). Concretely, we design a manifold spiking neuron which leverages the diffeomorphism to bridge spiking representations and manifold representations. With the proposed neuron, we propose a new training algorithm with Differentiation via Manifold, which no longer needs to recur the backward gradient and thus alleviates the high latency of previous methods. An interesting theoretical result is that, MSG is essentially related to manifold ODE. Extensive empirical results on benchmark datasets demonstrate the superior effectiveness and energy efficiency of the proposed MSG. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Broader Impact and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work brings together two previously separate domains: spiking neural network and Riemannian geometry, and presents a novel Manifold-valued Spiking GNN for energy-efficiency graph learning, especially for the large graphs. Our work is mainly a theoretical exploration, and not tied to particular applications. A positive societal impact is the possibility of decreasing carbon emissions in training large models. None of negative societal impacts we feel must be specifically highlighted here. ", "page_idx": 9}, {"type": "text", "text": "Limitation. Our work as well as the previous spiking GNNs considers the undirected, homophilous graphs, while the spiking GNN on directed or heterophilous graphs still remains open. Also, readers may find it challenging to implement the proposed method. However, we provide downloadable code and will offer an easy-to-use interface. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported in part by NSFC under grants 62202164 and 62322202. Philip S. Yu is supported in part by NSF under grants III-2106758, and POSE-2346158. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hamilton, W., Z. Ying, J. Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[2] Veli\u02c7ckovi\u00b4c, P., G. Cucurull, A. Casanova, et al. Graph Attention Networks. International Conference on Learning Representations, 2018. Accepted as poster.   \n[3] Chami, I., Z. Ying, C. R\u00e9, et al. Hyperbolic graph convolutional neural networks. In Advances in NeurIPS, vol. 32. 2019.   \n[4] Xiong, B., S. Zhu, N. Potyka, et al. Pseudo-riemannian graph convolutional networks. In Advances in NeurIPS, vol. 32. 2022.   \n[5] Zhu, Z., J. Peng, J. Li, et al. Spiking graph convolutional networks. In Proceedings of the 31st IJCAI, pages 2434\u20132440. ijcai.org, 2022.   \n[6] Li, J., H. Zhang, R. Wu, et al. A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. arXiv preprint arXiv:2305.19306, 2023.   \n[7] Maass, W. Networks of spiking neurons: The third generation of neural network models. Neural networks, 10(9):1659\u20131671, 1997.   \n[8] Brette, R., M. Rudolph, T. Carnevale, et al. Simulation of networks of spiking neurons: A review of tools and strategies. Journal of computational neuroscience, 23:349\u2013398, 2007.   \n[9] Wang, B., B. Jiang. Spiking GATs: Learning graph attentions via spiking neural network. arXiv preprint arXiv:2209.13539, 2022.   \n[10] Yang, H., R. Zhang, Q. Kong, et al. Spiking variational graph auto-encoders for efficient graph representation learning. arXiv preprint arXiv:2211.01952, 2022.   \n[11] Yin, N., M. Wang, L. Shen, et al. Continuous spiking graph neural networks. CoRR, abs/2404.01897, 2024.   \n[12] Sarkar, R. Low distortion delaunay embedding of trees in hyperbolic plane. In Proceedings of the 19th International Symposium on Graph Drawing, vol. 7034 of Lecture Notes in Computer Science, pages 355\u2013366. Springer, 2011.   \n[13] Bachmann, G., G. B\u00e9cigneul, O. Ganea. Constant curvature graph convolutional networks. In Proceedings of the 37th ICML, vol. 119, pages 486\u2013496. PMLR, 2020.   \n[14] Wu, H., Y. Zhang, W. Weng, et al. Training Spiking Neural Networks with Accumulated Spiking Flow. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):10320\u201310328, 2021.   \n[15] Meng, Q., M. Xiao, S. Yan, et al. Training high-performance low-latency spiking neural networks by differentiation on spike representation. In Proceedings of CVPR, pages 12434\u2013 12443. IEEE, 2022.   \n[16] Wu, J., Y. Chua, M. Zhang, et al. A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 34(1):446\u2013460, 2023.   \n[17] Lou, A., D. Lim, I. Katsman, et al. Neural manifold ordinary differential equations. In Advances in NeurIPS. 2020.   \n[18] Kipf, T. N., M. Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of the 5th ICLR. OpenReview.net, 2017.   \n[19] Wu, F., A. Souza, T. Zhang, et al. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861\u20136871. PMLR, 2019.   \n[20] Feng, Y., H. You, Z. Zhang, et al. Hypergraph Neural Networks. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3558\u20133565, 2019.   \n[21] Guo, K., Y. Hu, Y. Sun, et al. Hierarchical Graph Convolution Network for Traffic Forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 35(1):151\u2013159, 2021.   \n[22] Liu, Q., M. Nickel, D. Kiela. Hyperbolic Graph Neural Networks. In Advances in Neural Information Processing Systems, vol. 32. Curran Associates, Inc., 2019.   \n[23] Coors, B., A. P. Condurache, A. Geiger. SphereNet: Learning spherical representations for detection and classification in omnidirectional images. In Proceedings of ECCV, pages 518\u2013533. 2018.   \n[24] Gu, A., F. Sala, B. Gunel, et al. Learning mixed-curvature representations in product spaces. In Proceedings of the 7th ICLR. OpenReview.net, 2019.   \n[25] Zhang, S., Y. Tay, W. Jiang, et al. Switch Spaces: Learning product spaces with sparse gating. CoRR, abs/2102.08688, 2021.   \n[26] Sun, L., Z. Zhang, J. Ye, et al. A self-supervised mixed-curvature graph neural network. In Proceedings of the 36th AAAI, pages 4146\u20134155. 2022.   \n[27] Sun, L., Z. Huang, Z. Wang, et al. Motif-aware riemannian graph neural network with generativecontrastive learning. In Proceedings of the 38th AAAI, pages 9044\u20139052. 2024.   \n[28] Law, M. Ultrahyperbolic neural networks. In Advances in NeurIPS. 2021.   \n[29] Gao, Z., Y. Wu, Y. Jia, et al. Learning to optimize on SPD manifolds. In Proceedings of CVPR, pages 7697\u20137706. Computer Vision Foundation / IEEE, 2020.   \n[30] Dong, Z., S. Jia, C. Zhang, et al. Deep manifold learning of symmetric positive definite matrices with application to face recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31. 2017.   \n[31] Sun, L., Z. Huang, H. Peng, et al. Lsenet: Lorentz structural entropy neural network for deep graph clustering. In Proceedings of the 41st ICML. OpenReview.net, 2024.   \n[32] Sun, L., J. Hu, S. Zhou, et al. Riccinet: Deep clustering via a riemannian generative model. In Proceedings of the ACM Web Conference 2024 (WWW\u201924), pages 4071\u20134082. 2024.   \n[33] Sun, L., Z. Huang, H. Wu, et al. Deepricci: Self-supervised graph structure-feature co-refinement for alleviating over-squashing. In Proceedings of the 23rd ICDM, pages 558\u2013567. 2023.   \n[34] Sun, L., J. Ye, H. Peng, et al. Self-supervised continual graph learning in adaptive riemannian spaces. In Proceedings of the 37th AAAI, pages 4633\u20134642. 2023.   \n[35] Sun, L., Z. Zhang, J. Zhang, et al. Hyperbolic variational graph neural network for modeling dynamic graphs. In Proceedings of the 35th AAAI, pages 4375\u20134383. 2021.   \n[36] Sun, L., J. Ye, H. Peng, et al. A self-supervised riemannian gnn with time varying curvature for temporal graph learning. In Proceedings of the 31st ACM CIKM, pages 1827\u20131836. 2022.   \n[37] Sun, L., J. Ye, J. Zhang, et al. Contrastive sequential interaction network learning on co-evolving riemannian spaces. Int. J. Mach. Learn. Cybern., 15(4):1397\u20131413, 2024.   \n[38] Sun, L., J. Hu, M. Li, et al. R-ode: Ricci curvature tells when you will be informed. In Proceedings of the ACM SIGIR. 2024.   \n[39] Wang, Y., S. Zhang, J. Ye, et al. A mixed-curvature graph diffusion model. In Proceedings of the 33rd ACM CIKM, page 2482\u20132492. ACM, 2024.   \n[40] Fu, X., Y. Gao, Y. Wei, et al. Hyperbolic geometric latent diffusion model for graph generation. In Proceedings of the 41st ICML. OpenReview.net, 2024.   \n[41] Cao, Y., Y. Chen, D. Khosla. Spiking deep convolutional neural networks for energy-efficient object recognition. International Journal of Computer Vision, 113:54\u201366, 2015.   \n[42] Cao, J., Z. Wang, H. Guo, et al. Spiking denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4912\u20134921. 2024.   \n[43] Xu, M., Y. Wu, L. Deng, et al. Exploiting spiking dynamics with spatial-temporal feature normalization in graph learning. In Proceedings of the 30th IJCAI, pages 3207\u20133213. ijcai.org, 2021.   \n[44] Poli, M., S. Massaroli, J. Park, et al. Graph neural ordinary differential equations. arXiv preprint arXiv:1911.07532, 2019.   \n[45] Li, J., Z. Yu, Z. Zhu, et al. Scaling up dynamic graph representation learning via spiking neural networks. In Proceedings of the 37th AAAI, vol. 37, pages 8588\u20138596. 2023.   \n[46] Zhao, H., X. Yang, C. Deng, et al. Dynamic reactive spiking graph neural network. In Proceedings of the 38th AAAI, vol. 38, pages 16970\u201316978. 2024.   \n[47] Yin, N., M. Wang, Z. Chen, et al. Dynamic spiking graph neural networks. In Proceedings of the 38th AAAI, vol. 38, pages 16495\u201316503. 2024.   \n[48] Salinas, E., T. J. Sejnowski. Integrate-and-fire neurons driven by correlated stochastic input. Neural computation, 14(9):2111\u20132155, 2002.   \n[49] Burkitt, A. N. A review of the integrate-and-fire neuron model: I. homogeneous synaptic input. Biological Cybernetics, 95(1):1\u201319, 2006.   \n[50] Huh, D., T. J. Sejnowski. Gradient Descent for Spiking Neural Networks. In Advances in Neural Information Processing Systems, vol. 31. 2018.   \n[51] Neftci, E. O., H. Mostafa, F. Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Process. Mag., 36(6):51\u201363, 2019.   \n[52] Li, Y., Y. Guo, S. Zhang, et al. Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks. In Advances in Neural Information Processing Systems, vol. 34. 2021.   \n[53] Shchur, O., M. Mumme, A. Bojchevski, et al. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.   \n[54] Chen, W., X. Han, Y. Lin, et al. Fully hyperbolic neural networks. In Proceedings of the 60th ACL, pages 5672\u20135686. Association for Computational Linguistics, 2022.   \n[55] Sun, Y., D. Zhu, Y. Wang, et al. SpikeGraphormer: A high-performance graph transformer with spiking graph attention. arXiv preprint arXiv:2403.15480, 2024.   \n[56] B\u00e9cigneul, G., O. Ganea. Riemannian adaptive optimization methods. In Proceedings of ICLR. OpenReview.net, 2019.   \n[57] Paszke, A., S. Gross, F. Massa, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems, vol. 32. 2019.   \n[58] Zachary, W. W. An information flow model for confilct and fission in small groups. Journal of anthropological research, 33(4):452\u2013473, 1977.   \n[59] Tu, L. W. An Introduction to Manifolds. Springer New York, 2011.   \n[60] Crouch, P. E., R. Grossman. Numerical integration of ordinary differential equations on manifolds. Journal of Nonlinear Science, 3(1):1\u201333, 1993.   \n[61] Petersen, P. Riemannian Geometry, 3rd edition. Springer-Verlag, 2016.   \n[62] Zhou, Z., Y. Zhu, C. He, et al. Spikformer: When spiking neural network meets transformer. arXiv preprint arXiv:2209.15425, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "The appendix is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "1. Notation table,   \n2. Proofs of Theorem 4.1 and Theorem 5.2,   \n3. Derivation of the Jacobian,   \n4. Riemannian geometry including geodesically complete manifold, stereographic projection and $\\kappa$ -stereographic model, and Cartesian product and product manifold,   \n5. Empirical details, i.e., datasets/baselines description, theoretical energy consumption and implementation notes,   \n6. Additional results of link prediction, layer-wise gradient and the visualization. ", "page_idx": 13}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We summarize the important notations of our paper in Table 4. ", "page_idx": 13}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/8efa293eea6ad7193c73e1e7ba6f71abbb258e9a8429bcbe8c3e73c783a5d46e.jpg", "table_caption": ["Table 4: Notations. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we demonstrate the proofs of Theorem 4.1 (Backward Gradient) and Theorem 5.2 (MSG as Dynamic Chart Solver). ", "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First, we give the formal definition of the pullback. Given a smooth map $\\phi:\\mathcal{M}\\to\\mathcal{N}$ connecting two manifolds $\\mathcal{M}$ and $\\mathcal{N}$ , and a real, smooth function $f:\\mathcal{N}\\rightarrow\\mathbb{R}$ , the pullback of $f$ by $\\phi$ is the smooth function $\\phi^{*}f$ on $\\mathcal{M}$ defined by $(\\phi^{*}f)(\\mathbf{x})=f(\\phi(\\mathbf{x}))$ . ", "page_idx": 13}, {"type": "text", "text": "Next, we introduce some properties of the pullback in the smooth manifold [59] (i.e., differential 1-form of a smooth function, communication, and pullback of a sum and a product), supporting the derivation of the backward gradient (Theorem 4.1). ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1 (Differential 1-form of a smooth function). For a point $\\mathbf{z}\\in\\mathcal{N}$ related with a coordinate chart $(U,z^{1},...,z^{n}).$ , there is a series of covectors $\\{(d z^{1})_{\\mathbf{z}},...,(d z^{n})_{\\mathbf{z}}\\}$ forming a basis of $T_{\\mathbf{z}}^{*}\\!N$ dual to the basis $\\{(\\partial/\\partial\\acute{z}^{1})|\\mathbf{z},...,(\\partial/\\partial z^{n})|\\mathbf{\\dot{z}}\\}$ of tangent space $T_{\\mathbf{z}}\\mathcal{N}$ . Then, for any smooth function $f$ on $\\mathcal{N}$ restrict to $U$ , the differential $^{\\,I}$ -form of $f$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\nd f=\\sum_{i=1}^{n}{\\frac{\\partial f}{\\partial z^{i}}}d z^{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma B.2 (Communication). Let $F:\\mathcal{N}\\to\\mathcal{M}$ be a smooth map, for any smooth function g on $\\mathcal{M}$ , we have $F^{*}(d g)=d(F^{*}g)$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma B.3 (Pullback of a sum and a product). Let $F:\\mathcal{N}\\to\\mathcal{M}$ be a smooth map, $g$ is a smooth scalar function on $\\mathcal{M}$ , and $\\omega,\\gamma$ are differential $^{\\,l}$ -forms on $\\mathcal{M}$ . Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F^{*}(\\omega+\\gamma)=F^{*}\\omega+F^{*}\\gamma}\\\\ {F^{*}(g\\omega)=(F^{*}g)(F^{*}\\omega).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given the properties of the pullback, we derive the closed-form backward gradient of the real function on the manifold, and prove Theorem 4.1. ", "page_idx": 14}, {"type": "text", "text": "Theorem 4.1 (Backward Gradient) Let $\\mathcal{L}$ be the scalar-valued function, and $\\mathbf{z}^{l}$ is the output of $l$ -th layer with parameter $\\mathbf{W}^{l}$ , which is delivered by tangent vector $\\mathbf{\\widetilde{v}}^{l}$ . Then, the gradient of function $\\mathcal{L}$ $w.r.t.\\textbf{W}^{l}$ is given as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{W}^{l}}\\mathcal{L}=[\\frac{\\partial\\mathbf{v}^{l-1}}{\\partial\\mathbf{W}^{l}}]^{*}[D_{\\mathbf{v}^{l-1}}\\phi^{l-1}]^{*}\\nabla_{\\mathbf{z}^{l}}\\mathcal{L},\\quad\\nabla_{\\mathbf{z}^{l}}\\mathcal{L}=[D_{\\mathbf{z}^{l}}\\psi^{l}]^{*}\\nabla_{\\mathbf{z}^{l+1}}\\mathcal{L},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\phi^{l-1}(\\cdot)=\\mathrm{Exp}_{{\\bf z}^{l-1}}(\\cdot)$ , $\\psi^{l}(\\cdot)=\\mathrm{Exp}_{(\\cdot)}(\\mathbf{v}^{l})$ , and $[\\cdot]^{*}$ means the matrix form of pullback. ", "page_idx": 14}, {"type": "text", "text": "Proof. Given $\\mathbf{z}^{l},\\mathbf{z}^{l+1}$ in $\\mathcal{M}$ , and $F\\,:\\,\\mathcal{M}\\,\\rightarrow\\,\\mathcal{M}$ be the smooth map such that $F({\\bf z}^{l})\\;=\\;{\\bf z}^{l+1}$ . Consider scalar loss function $L:\\mathcal{M}\\rightarrow\\mathbb{R}$ , if we relate $\\mathbf{z}^{l}$ with a chart $(U,x^{1},...,x^{m})$ and $\\mathbf{z}^{l+1}$ with $(V,y^{1},...,y^{m})$ , the gradients of $L$ at $\\mathbf{z}^{l+1}$ and $\\mathbf{z}^{l}$ are given by Lemma. B.1, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{z}^{l+1}}{\\mathcal{L}}=\\sum_{i}{\\frac{\\partial{\\mathcal{L}}}{\\partial y^{i}}}{\\bigg|}_{\\mathbf{z}^{l+1}}d y^{i}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{z}^{l}}(\\mathcal{L}\\circ F)=\\sum_{i}\\frac{\\partial\\mathcal{L}\\circ F}{\\partial x^{i}}\\bigg|_{\\mathbf{z}^{l}}d x^{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we apply the pullback $F^{*}$ on $\\nabla_{\\mathbf{z}^{l+1}}L$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F^{*}(\\nabla_{\\pi^{+}}\\mathcal{L})=F^{*}\\displaystyle\\sum_{i}\\frac{\\partial\\mathcal{L}}{\\partial y^{i}}\\Bigg|_{\\alpha^{+}=1}\\;\\;\\mathrm{d}y^{i}|_{\\alpha^{+}=1}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i}(F^{*}\\frac{\\partial\\mathcal{L}}{\\partial y^{i}}|_{\\alpha^{+}=1})(F^{*}\\partial y^{i}|_{\\alpha^{+}=1}^{*})\\quad\\mathrm{from~}\\mathbf{Lemma},\\;\\mathbf{B},3}\\\\ &{\\qquad=\\displaystyle\\sum_{i}(\\frac{\\partial\\mathcal{L}}{\\partial y^{i}}\\in F)|_{\\alpha^{+}}(d(F^{*}y^{i})|_{\\alpha^{+}})\\quad\\mathrm{from~}\\mathbf{Lemma},\\;\\mathbf{B},2}\\\\ &{\\qquad=\\displaystyle\\sum_{i}(\\frac{\\partial\\mathcal{L}}{\\partial y^{i}}\\circ F)|_{\\alpha^{+}}(d(y^{i}\\circ F))|_{\\alpha^{+}}}\\\\ &{\\qquad=\\displaystyle\\sum_{i}(\\frac{\\partial\\mathcal{L}}{\\partial y^{i}}\\Bigg|_{\\alpha^{+}=1})(\\sum_{j}\\frac{\\partial F^{*}}{\\partial x^{j}}\\mathrm{d}x^{j})|_{\\alpha^{+}}\\quad\\mathrm{from~}\\mathbf{Lemma},\\;\\mathbf{B},1}\\\\ &{\\qquad=\\displaystyle\\sum_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial y^{i}}\\Bigg|_{\\alpha^{+}=1}\\frac{\\partial F^{*}}{\\partial x^{j}}\\Bigg|_{\\alpha^{+}}d x^{j}\\Bigg|_{\\alpha^{+}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we can find that the matrix form of the pullback $F^{*}$ can be written as the transpose of the Jacobian matrix of $F$ , denoted as $\\Big[\\frac{\\partial F^{i}}{\\partial x^{j}}\\Big|_{{\\bf Z}^{l}}\\Big]^{*}$ or $[D_{\\mathbf{z}^{l}}F]^{*}$ . The derivation of $\\nabla_{\\mathbf{W}^{l}}\\mathcal{L}$ is similar, we only need to use an addition process like above on $\\nabla_{\\mathbf{v}^{l}}\\!-\\!1\\,\\mathcal{L}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Note that, we give the closed-form expression of exponential map, logarithmic map, and parallel transport for hyperbolic and hyperspherical space, and derive the corresponding Jacobian in Sec. C. ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 5.2 (MSG as Dynamic Chart Solver) $I f\\mathbf{y}(t):[\\tau,\\tau+\\epsilon]\\rightarrow\\mathbb{R}^{n}$ is the solution of ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{y}(t)}{d t}=(D_{\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{y}(\\mathbf{t}))}\\,\\mathrm{Log}_{\\mathbf{z}})u(\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{y}(t)),t),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then $\\mathbf{z}(t)=\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{y}(t))$ is a valid solution to the manifold ODE of Eq. (13) on $t\\in[\\tau,\\tau+\\epsilon]$ , where $\\mathbf{z}=\\mathbf{z}(\\tau)$ . If ${\\bf y}(t)$ is given by the first-order approximation with the $\\epsilon$ small enough, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{y}(\\tau+\\epsilon)=\\epsilon\\cdot(D_{\\mathbf{z}}\\log_{\\mathbf{z}})u(\\mathbf{z}(\\tau),\\tau),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then the update process of Eqs. (4) and (8) in MSG is equivalent to Dynamic Chart Solver in Eq. (15). ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $t\\in[\\tau,\\tau+\\epsilon]$ , then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d\\mathbf{z}(t)}{d t}=(D_{\\mathbf{y}(\\mathbf{t})}\\operatorname{Exp}_{\\mathbf{z}})\\frac{d\\mathbf{y}(t)}{d t}}\\\\ &{\\qquad=(D_{\\mathbf{y}(\\mathbf{t})}\\operatorname{Exp}_{\\mathbf{z}})(D_{\\operatorname{Exp}_{\\mathbf{z}}(\\mathbf{y}(\\mathbf{t}))}\\operatorname{Log}_{\\mathbf{z}})u(\\operatorname{Exp}_{\\mathbf{z}}(\\mathbf{y}(t)),t)}\\\\ &{\\qquad=(D_{\\mathbf{y}(\\mathbf{t})}\\operatorname{Exp}_{\\mathbf{z}})(D_{\\mathbf{z}(t)}\\operatorname{Log}_{\\mathbf{z}})u(\\operatorname{Exp}_{\\mathbf{z}}(\\mathbf{y}(t)),t)}\\\\ &{\\qquad=u(\\mathbf{z}(t),t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider two adjacent charts $(U_{1},\\mathrm{Log}_{{\\bf z}_{1}})$ and $(U_{2},\\mathrm{Log}_{{\\bf z}_{2}})$ , such that $\\mathbf{z}_{1}\\in U_{1}$ and $\\mathbf{z}_{2}\\in U_{1}\\cap U_{2}$ . Note that, in interval $[\\tau,\\tau+\\epsilon],\\,\\mathbf{z}(\\tau)=\\dot{\\mathbf{z}}_{1}$ and $\\mathbf{z}(\\tau+\\epsilon)\\,{\\bar{\\mathbf{\\alpha}}}=\\mathbf{z}_{2}$ , we have $\\mathbf{y}(\\tau)=\\mathrm{Exp}_{\\mathbf{z}_{1}}(\\mathbf{z}_{1})=\\mathbf{0}$ With the first-order approximation, ${\\bf y}(\\tau+\\epsilon)$ is thus given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{y}(\\tau+\\epsilon)=\\mathbf{y}(\\tau)+\\epsilon\\cdot(D_{\\mathrm{Exp}_{\\mathbf{z}_{1}}(\\mathbf{y}(\\tau))}\\,\\mathrm{Log}_{\\mathbf{z}_{1}})u(\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{y}(\\tau)),\\tau)}\\\\ &{\\qquad\\qquad=\\epsilon\\cdot(D_{\\mathbf{z}_{1}}\\,\\mathrm{Log}_{\\mathbf{z}_{1}})u(\\mathbf{z}(\\tau),\\tau)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Also, Eq. (37) can be treated as a step in the Euler solver [60] for a small $\\epsilon$ . Finally, we have $\\mathbf{z}(\\tau+\\epsilon)\\,=\\,\\mathrm{Exp}_{\\mathbf{z}_{1}}(\\mathbf{y}(\\tau+\\epsilon))\\,=\\,\\mathbf{z}_{2}$ , ending the process of dynamic chart solver. That is, MSG considers the logarithmic map to define the charts, and is equivalent to Dynamic Chart Solver (Definition 5.1), completing the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Deviation of Jacobian ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We instantiate the proposed MSG in the Lorentz model $\\mathbb{H}$ of hyperbolic space, sphere model $\\mathbb{S}$ of hyperspherical space, and the products of $\\mathbb{H}$ \u2019s or/and $\\mathbb{S}$ \u2019s. Accordingly, we derive the Jacobian in $\\mathbb{H}$ and $\\mathbb{S}$ , and introduce the construction in the products in D.3. ", "page_idx": 15}, {"type": "text", "text": "C.1 Hyperbolic Space ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lorentz Model The $d$ -dimensional Lorentz model $\\mathbb{H}^{d}$ is defined on the $(d+1)$ -dimensional manifold of $\\begin{array}{r}{\\{\\mathbf{z}=[z_{0},z_{1},\\cdots,z_{d}]^{T}\\in\\mathbb{R}^{d+1}|\\langle\\mathbf{z},\\mathbf{z}\\rangle_{\\mathcal{L}}=-1,z_{0}>0\\}^{\\mathcal{L}}}\\end{array}$ 5, equipped with the Minkowski inner product, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle{\\bf u},{\\bf v}\\rangle_{\\mathcal{L}}=-u_{0}v_{0}+\\sum_{i=1}^{d}u_{i}v_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The tangent space at point $\\textbf{z}\\in\\mathbb{H}^{d}$ is $T_{\\mathbf{z}}\\mathbb{H}^{d}\\;=\\;\\{\\mathbf{v}\\;\\in\\;\\mathbb{R}^{d+1}|\\langle\\mathbf{z},\\mathbf{v}\\rangle_{\\mathcal{L}}\\;=\\;0\\}$ , and ${\\mathrm{Proj}}_{\\mathbf{z}}(\\mathbf{u})~=$ $\\mathbf{u}+\\langle\\mathbf{z},\\mathbf{u}\\rangle_{\\mathcal{L}}\\mathbf{z}$ is to project a vector $u\\in\\mathbb{R}^{d+1}$ into the tangent space $T_{\\mathbf{z}}\\mathbb{H}^{d}$ . The Lorentz norm of tangent vector is defined as $\\|\\mathbf{v}\\|_{\\mathcal{L}}=\\sqrt{\\langle\\mathbf{z},\\mathbf{z}\\rangle_{\\mathcal{L}}}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem 4.1 requires the Jocabian of $\\phi(\\cdot)=\\mathrm{Exp}_{\\mathbf{z}}(\\cdot)$ and $\\psi(\\cdot)=\\mathrm{Exp}_{(\\cdot)}(\\mathbf{v})$ , and Lorentz model has the closed-form exponential map given as follows, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{v})=\\cosh(\\|\\mathbf{v}\\|_{\\mathcal{L}})\\mathbf{z}+{\\frac{\\sinh(\\|\\mathbf{v}\\|_{\\mathcal{L}})}{\\|\\mathbf{v}\\|_{\\mathcal{L}}}}\\mathbf{v}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The inverse of exponential map (i.e, the logarithmic map) is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Log}_{\\mathbf{z}}(\\mathbf{x})=\\frac{\\mathrm{arcosh}(\\langle\\mathbf{z},\\mathbf{x}\\rangle_{\\mathcal{L}})}{\\mathrm{sinh}(\\mathrm{arcosh}(\\langle\\mathbf{z},\\mathbf{x}\\rangle_{\\mathcal{L}}))}(\\mathbf{x}-\\langle\\mathbf{z},\\mathbf{x}\\rangle_{\\mathcal{L}}\\mathbf{z})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Deviation of Jacobian We first calculate the Jacobian of $\\psi$ , and it is given as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\mathbf{z}}\\psi=\\cosh(\\|\\mathbf{v}\\|_{\\mathcal{L}})\\mathbf{I}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that, Jacobian of $\\phi$ needs the Jacobian of $\\|\\mathbf{v}\\|_{\\mathcal{L}}$ , which is derived as ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathbf{v}}\\|\\mathbf{v}\\|_{\\mathcal{L}}=\\frac{d}{d\\langle\\mathbf{v},\\mathbf{v}\\rangle_{\\mathcal{L}}}(\\sqrt{\\langle\\mathbf{v},\\mathbf{v}\\rangle}_{\\mathcal{L}})D_{\\mathbf{v}}(\\langle\\mathbf{v},\\mathbf{v}\\rangle_{\\mathcal{L}})=\\frac{1}{\\|\\mathbf{v}\\|_{\\mathcal{L}}}\\hat{\\mathbf{v}}^{T},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\hat{\\mathbf{v}}=[-v_{0},v_{1},...,v_{d}]^{T}$ . Then, we compute the derivative of the first term of Eq. 39. ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathbf{v}}\\cosh(\\|\\mathbf{v}\\|_{\\mathcal{L}})\\mathbf{z}=\\frac{d}{d\\|\\mathbf{v}\\|_{\\mathcal{L}}}(\\cosh(\\|\\mathbf{v}\\|_{\\mathcal{L}}))\\mathbf{z}(D_{\\mathbf{v}}\\|\\mathbf{v}\\|_{\\mathcal{L}})=\\frac{\\sinh(\\|\\mathbf{v}\\|_{\\mathcal{L}})}{\\|\\mathbf{v}\\|_{\\mathcal{L}}}\\mathbf{z}\\hat{\\mathbf{v}}^{T},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the derivative of the second term is derived as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathbf{v}}\\frac{\\mathrm{sinh}\\left(\\|\\mathbf{v}\\|\\|_{\\mathcal{L}}\\right)}{\\|\\mathbf{v}\\|_{\\mathcal{L}}}\\mathbf{v}=D_{\\mathbf{v}}(\\frac{\\mathrm{sinh}\\left(\\|\\mathbf{v}\\|\\|_{\\mathcal{L}}\\right)}{\\|\\mathbf{v}\\|_{\\mathcal{L}}})\\mathbf{v}+\\frac{\\mathrm{sinh}\\left(\\|\\mathbf{v}\\|\\|_{\\mathcal{L}}\\right)}{\\|\\mathbf{v}\\|_{\\mathcal{L}}}D_{\\mathbf{v}}\\mathbf{v}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\|\\mathbf{v}\\|_{\\mathcal{L}}\\cosh\\left(\\|\\mathbf{v}\\|_{\\mathcal{L}}\\right)-\\sinh\\left(\\|\\mathbf{v}\\|_{\\mathcal{L}}\\right)}{\\|\\mathbf{v}\\|_{\\mathcal{L}}^{3}}\\mathbf{v}\\hat{\\mathbf{v}}^{T}+\\frac{\\sinh\\left(\\|\\mathbf{v}\\|_{\\mathcal{L}}\\right)}{\\|\\mathbf{v}\\|_{\\mathcal{L}}}\\mathbf{I}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing up above equations, we finally have ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathbf{v}}\\phi=\\frac{\\|\\mathbf{v}\\|_{\\mathcal{L}}\\cosh(\\|\\mathbf{v}\\|_{\\mathcal{L}})-\\sinh(\\|\\mathbf{v}\\|_{\\mathcal{L}})}{\\|\\mathbf{v}\\|_{\\mathcal{L}}^{3}}\\mathbf{v}\\hat{\\mathbf{v}}^{T}+\\frac{\\sinh(\\|\\mathbf{v}\\|_{\\mathcal{L}})}{\\|\\mathbf{v}\\|_{\\mathcal{L}}}(\\mathbf{I}+\\mathbf{z}\\hat{\\mathbf{v}}^{T}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{I}$ is the identity matrix. ", "page_idx": 16}, {"type": "text", "text": "C.2 Hyperspherical Space ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Sphere Model The sphere model $\\mathbb{S}^{d}$ is defined on the $(d+1)$ -dimensional manifold of $\\mathbf{\\{z=}}$ $[z_{0},z_{1},\\cdot\\cdot\\cdot\\ ,z_{d}]^{T}\\in\\mathbb{R}^{d+1}|\\langle\\mathbf{z},\\mathbf{z}\\rangle=1,z_{0}>0\\}$ with the standard inner product $\\textstyle\\langle\\mathbf{x},\\mathbf{y}\\rangle=\\sum_{i=0}^{d}x_{i}y_{i}$ and norm $\\|\\mathbf{x}\\|\\,=\\,\\sqrt{\\textstyle\\sum_{i=0}^{d}x_{i}^{2}}$ . The tangent space at point ${\\bf z}$ is $T_{\\mathbf{z}}\\mathbb{S}^{d}\\,=\\,\\{{\\mathbf{v}}\\,\\in\\,\\mathbb{R}^{d+1}|\\langle{\\mathbf{z}},{\\mathbf{v}}\\rangle\\,=\\,0\\}$ Similar to Lorentz model, we have ${\\mathrm{Proj}}_{\\mathbf{z}}(\\mathbf{u})=\\mathbf{u}-\\langle\\mathbf{z},\\mathbf{u}\\rangle\\mathbf{z}$ projecting a vector $u\\in\\mathbb{R}^{d+1}$ into $T_{\\mathbf{z}}\\mathbb{S}^{d}$ The exponential map in the sphere model is given as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Exp}_{\\mathbf{z}}(\\mathbf{v})=\\cos(\\|\\mathbf{v}\\|)\\mathbf{z}+{\\frac{\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|}}\\mathbf{v},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the logarithmic map is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Log}_{\\mathbf{z}}(\\mathbf{x})={\\frac{\\operatorname{arccos}(\\langle\\mathbf{z},\\mathbf{x}\\rangle)}{\\sin(\\operatorname{arccos}(\\langle\\mathbf{z},\\mathbf{x}\\rangle))}}(\\mathbf{x}-\\langle\\mathbf{z},\\mathbf{x}\\rangle\\mathbf{z})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Derivation of Jacobian We first calculate the Jacobian of $\\psi$ , and it is given as ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathbf{z}}\\psi=\\cos(\\|\\mathbf{v}\\|)\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similar to that in Lorentz model, the Jacobian of $\\phi$ needs the Jacobian of $\\|\\mathbf{v}\\|$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathbf{v}}\\|\\mathbf{v}\\|=\\frac{d}{d\\langle\\mathbf{v},\\mathbf{v}\\rangle}(\\sqrt{\\langle\\mathbf{v},\\mathbf{v}\\rangle})D_{\\mathbf{v}}(\\langle\\mathbf{v},\\mathbf{v}\\rangle)=\\frac{1}{\\|\\mathbf{v}\\|}\\mathbf{v}^{T}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we compute the derivative of the first term of Eq. 47. ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathbf{v}}\\cos(\\|\\mathbf{v}\\|)\\mathbf{z}=\\frac{d}{d\\|\\mathbf{v}\\|}(\\cos(\\|\\mathbf{v}\\|))\\mathbf{z}(D_{\\mathbf{v}}\\|\\mathbf{v}\\|)=\\frac{-\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|}\\mathbf{z}\\mathbf{v}^{T},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the derivative of the second term is ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{D_{\\mathbf{v}}{\\frac{\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|}}\\mathbf{v}=D_{\\mathbf{v}}({\\frac{\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|}})\\mathbf{v}+{\\frac{\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|}}D_{\\mathbf{v}}\\mathbf{v}}\\\\ &{\\qquad\\qquad\\qquad={\\frac{\\|\\mathbf{v}\\|\\cos(\\|\\mathbf{v}\\|)-\\sinh(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|^{3}}}\\mathbf{v}\\mathbf{v}^{T}+{\\frac{\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|}}\\mathbf{I}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing up above equations, we finally have ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathbf{v}}\\phi=\\frac{\\|\\mathbf{v}\\|\\cos(\\|\\mathbf{v}\\|_{\\mathcal{L}})-\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|^{3}}\\mathbf{v}\\mathbf{v}^{T}+\\frac{\\sin(\\|\\mathbf{v}\\|)}{\\|\\mathbf{v}\\|}(\\mathbf{I}-\\mathbf{z}\\mathbf{v}^{T}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Riemannian Geometry ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Some Notations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we give the formal descriptions of the notions mentioned in the main paper, and please refer to [61] for systematic elaborations. ", "page_idx": 17}, {"type": "text", "text": "Geodesically Complete Manifold. A manifold is said to be geodesically complete if the maximal defining interval of any geodesic is $\\mathbb{R}$ . For any Riemannian manifold $(\\mathcal{M},g)$ admitting a metric structure given by the length of geodesic ", "page_idx": 17}, {"type": "equation", "text": "$$\nd(p,q)=\\operatorname*{inf}\\{L(\\gamma)|\\gamma{\\mathrm{~is~a~piecewise~smooth~curve~connecting~}}p{\\mathrm{~to~}}q\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "the completeness of $d$ can be described as a metric space is complete if any Cauchy sequence in it converges. For instance, hyperbolic space as well as hyperspherical space is geodesically complete. ", "page_idx": 17}, {"type": "text", "text": "Tangent Bundle. Given an $n$ -dimensional smooth manifold $\\mathcal{M}$ , the tangent bundle $T\\mathcal{M}$ is the disjoint union of all the tangent spaces of the manifold $\\begin{array}{r}{T\\mathcal{M}=\\bigsqcup_{\\mathbf{z}\\in\\mathcal{M}}T_{\\mathbf{z}}\\mathcal{M}}\\end{array}$ , and the tangent bundle with the projection $\\pi(\\mathbf{v})=\\mathbf{p}$ for all $\\mathbf{v}\\in T_{\\mathbf{p}}\\mathcal{M}$ is a vector bundle of rank $n$ . ", "page_idx": 17}, {"type": "text", "text": "Chart. A chart of a manifold is a pair $(U,\\phi)$ where $U$ is an open set in the manifold and $\\phi:U\\to\\mathbb{R}^{n}$ is homeomorphism onto it image, giving a local coordinate of the manifold. In other words, it provides a way of identifying the manifold locally with a Euclidean space. Given two charts $(U_{1},\\phi_{1})$ and $(U_{2},\\phi_{2})$ , if the overlap ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi_{2}\\circ\\phi_{1}^{-1}:\\phi_{1}(U_{1}\\cap U_{2})\\to\\phi_{2}(U_{1}\\cap U_{2})\\mathrm{~and~}\\phi_{1}\\circ\\phi_{2}^{-1}:\\phi_{2}(U_{1}\\cap U_{2})\\to\\phi_{1}(U_{1}\\cap U_{2}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "the two charts are said to be compatible. ", "page_idx": 17}, {"type": "text", "text": "Curvature and Sectional Curvature. The curvature is a notion describing the extent of how a manifold derivatives from being \u201cflat\u201d. In particular, the curvature of a Riemannian manifold $\\mathcal{M}$ should be viewed as a measure $R(X,Y)Z$ of the extent to which the operator $(X,Y)\\rightarrow\\nabla_{X}\\nabla_{Y}Z$ is symmetric, where $\\nabla$ is a connection on $\\mathcal{M}$ (where $X,Y,Z$ are vector fields, with $Z$ fixed). Sectional curvature is simpler object of curvature and is defined on two independent vector unit in the tangent space. When $\\nabla$ is the Levi-Civita connection induced by a Riemannian metric on $\\mathcal{M}$ , it turns out that the curvature operator $R$ can be recovered from the sectional curvature. ", "page_idx": 17}, {"type": "text", "text": "Constant Curvature Space, Hyperbolic Space, Hyperspherical Space. A Riemannian manifold is said to be a constant curvature space (CCS) if the sectional curvature is constant scalar everywhere on the manifold. When the CCS has a negative constant curvature, it is referred to as hyperbolic space, and the CCS is hyperspherical when its constant curvature is positive. ", "page_idx": 17}, {"type": "text", "text": "D.2 $\\kappa$ -stereographic model and Stereographic Projection ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "$\\kappa$ -stereographic model It gives a unified formalism for both positive and negative constant curvatures. For a positive curvature, it is the hyperspherical model for the hyperspherical space, and for a negative curvature, it switches to the Poincar\u00e9 ball model. ", "page_idx": 17}, {"type": "text", "text": "Specifically, for a curvature $\\kappa$ and a dimension $d\\geq2$ , the $\\kappa$ -stereographic model $\\mathfrak{s t}_{\\kappa}^{d}$ is defined on the manifold of $\\{\\mathbf{x}\\in\\mathbb{R}^{d}|-\\kappa\\|\\mathbf{x}\\|^{2}<1\\}$ , which is equipped with a Riemannian metric $\\begin{array}{r}{\\tilde{\\mathfrak{g}}_{\\mathbf{x}}^{\\kappa}=\\frac{4}{(1+\\kappa\\|\\mathbf{x}\\|^{2})^{2}}\\mathbf{I}}\\end{array}$ for any constant curvature $\\kappa$ . When $\\kappa\\geq0$ , the defining domain is $\\mathbb{R}^{d}$ in which the stereographic projection of the Sphere model of hyperspherical space is endowed. When $\\kappa<0$ , the manifold $\\hat{\\mathfrak{s t}}_{\\kappa}^{d}$ is represented in an open ball of radius $\\frac{1^{\\mathbf{\\lambda}}}{\\sqrt{-\\kappa}}$ , and is the stereographic projection of the Lorentz model of hyperbolic space. ", "page_idx": 17}, {"type": "text", "text": "The $\\kappa$ -stereographical model is a gyrovector space in which a non-associative vector operator system is defined. For $\\mathbf{x},\\mathbf{y}\\in\\mathbb{G}^{n}$ , $a\\in\\mathbb R$ , the $\\kappa$ -addition (a.k.a. M\u00f6bius addition) is given as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{x}\\oplus_{\\kappa}\\mathbf{y}={\\frac{(1-2\\kappa\\mathbf{x}^{T}\\mathbf{y}-\\kappa\\|\\mathbf{y}\\|^{2})\\mathbf{x}+(1+\\kappa\\|\\mathbf{x}\\|^{2})\\mathbf{y}}{1-2\\kappa\\mathbf{x}^{T}\\mathbf{y}+\\kappa^{2}\\|\\mathbf{x}\\|^{2}\\|\\mathbf{y}\\|^{2}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The distance function given by $\\kappa$ -addition is thus formulated as ", "page_idx": 18}, {"type": "equation", "text": "$$\nd_{\\kappa}(\\mathbf v,\\mathbf y)=2\\tan_{\\kappa}^{-1}(\\|(-\\mathbf x)\\oplus_{\\kappa}\\mathbf y\\|)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The $\\kappa$ -scaling for any real scalar $c$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\nc\\otimes_{\\kappa}\\mathbf{x}=\\tan_{\\kappa}\\bigl(c\\cdot\\tan_{\\kappa}^{-1}(\\|\\mathbf{x}\\|)\\bigr)\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The unit-speed geodesic from $\\mathbf{x}$ to $\\mathbf{y}$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma_{\\mathbf{x}\\to\\mathbf{y}}(t)=\\mathbf{x}\\oplus_{\\kappa}{\\big(}t\\otimes_{\\kappa}{\\big(}(-\\mathbf{x})\\oplus_{\\kappa}\\mathbf{y}{\\big)}{\\big)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With the unit-speed geodesic, the exponential map as well as its inverse (i.e., the logarithmic map) has the closed-form expression as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Exp}_{\\mathbf{x}}^{\\kappa}(\\mathbf{v})=\\mathbf{x}\\oplus_{\\kappa}(\\tan_{\\kappa}(|\\kappa|^{\\frac{1}{2}}\\frac{\\lambda_{\\mathbf{x}}^{\\kappa}\\|\\mathbf{v}\\|}{2})\\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|})}\\\\ &{\\mathrm{Log}_{\\mathbf{x}}^{\\kappa}(\\mathbf{y})=\\frac{2|\\kappa|^{-\\frac{1}{2}}}{\\lambda_{\\mathbf{x}}^{\\kappa}}\\tan_{\\kappa}^{-1}(\\|(-\\mathbf{x})\\oplus_{\\kappa}\\mathbf{y}\\|)\\frac{(-\\mathbf{x})\\oplus_{\\kappa}\\mathbf{y}}{\\|(-\\mathbf{x})\\oplus_{\\kappa}\\mathbf{y}\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the curvature-aware trigonometric function is utilized, e.g., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tan_{\\kappa}(x)=\\left\\{\\begin{array}{l l}{\\frac{1}{\\sqrt{\\kappa}}\\tan(x)}&{\\kappa>0,}\\\\ {x}&{\\kappa=0,}\\\\ {\\frac{1}{\\sqrt{-\\kappa}}\\operatorname{tanh}(x)}&{\\kappa<0.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Stereographic Projection The stereographic projection is a diffeomorphism connecting the different model spaces of Riemannian manifold. In particular, it is defined as a map $\\pi:\\mathbb{L}_{\\kappa}^{d}/\\tilde{\\mathbb{S}_{\\kappa}^{d}}\\to\\mathfrak{s t}_{\\kappa}^{d}$ taking the form of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi:\\mathbb{L}_{\\kappa}^{d}/\\mathbb{S}_{\\kappa}^{d}\\to{\\mathfrak{s t}}_{\\kappa}^{d},\\quad\\mathbf{x}=\\frac{1}{1+\\sqrt{|\\kappa|}\\mathbf{x}_{d+1}^{\\prime}}\\mathbf{x}_{1:d}^{\\prime},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{x}^{\\prime}$ is a point on the Lorentz model $\\mathbb{L}_{\\kappa}^{d}$ or Sphere model $\\mathbb{L}_{\\kappa}^{d}$ , and $\\mathbf{x}$ , the image of the projection, is the corresponding point in the gyrovector ball of $\\kappa-$ stereographic model. The inverse projection is given as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi^{-1}:\\mathfrak{s t}_{\\kappa}^{d}\\to\\mathbb{L}_{\\kappa}^{d}/\\mathbb{S}_{\\kappa}^{d},\\quad\\mathbf{x}^{\\prime}=\\left(\\lambda_{\\mathbf{x}}^{\\kappa}\\mathbf{x},\\frac{1}{\\sqrt{\\left|\\kappa\\right|}}\\left(\\lambda_{\\mathbf{x}}^{\\kappa}-1\\right)\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where \u03bb\u03bax =1+\u03ba2\u2225x\u22252 is known as the conformal factor. ", "page_idx": 18}, {"type": "text", "text": "D.3 Cartesian Product and Product Manifold ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The concept of product manifolds allows for creating a new manifold from a finite collection of existing ones. Given a set of smooth manifolds $\\mathcal{M}_{1},\\mathcal{M}_{2},\\ldots,\\mathcal{M}_{k}$ , the product manifold $\\mathbb{P}$ is given as the Cartesian product of these manifolds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}=\\mathcal{M}_{1}\\times\\mathcal{M}_{2}\\times...\\times\\mathcal{M}_{k},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\otimes$ denotes the Cartesian product. Specifically, with the Cartesian product construction, a point $\\mathbf{x}\\in\\mathbb{P}$ are represented by a concatenation of $\\mathbf{x}=[\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{k}]$ , where $\\mathbf{x}_{i}\\in\\mathcal{M}_{i}$ . A tangent vector $\\mathbf{v}\\in T_{\\mathbf{x}}\\mathbb{P}$ at a point $\\mathbf{x}$ can be given as $\\mathbf{v}=[\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{k}]$ , where $\\mathbf{v}_{i}\\in T_{\\mathbf{x}_{i}}\\mathcal{M}_{i}$ . If each manifold $\\mathcal{M}_{i}$ is equipped with a metric tensor $\\mathbf{g}_{i}$ , the product metric $\\mathbf{g}$ decomposes into the direct sum of the individual metrics $\\mathbf{g}=\\oplus_{i=1}^{k}\\mathbf{g}^{i}$ , which can be expressed as $\\mathrm{Diag}(\\dot{\\mathbf{g}^{1}},...,\\mathbf{g}^{k})$ . For $\\mathbf{x}$ and $\\textbf y\\in\\mathbf{P}$ , the distance between them is defined as $\\begin{array}{r}{d_{\\mathbb{P}}(\\mathbf{x},\\mathbf{y})=\\sum_{i=1}^{k}d_{\\mathcal{M}_{i}}(\\mathbf{x}_{i},\\mathbf{y}_{i})}\\end{array}$ . Accordingly, the exponential map is given as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{Exp}_{\\mathbf{x}}([\\mathbf{v}_{1},\\dots,\\mathbf{v}_{k}])=\\left[\\operatorname{Exp}_{\\mathbf{x}_{1}}(\\mathbf{v}_{1}),\\operatorname{Exp}_{\\mathbf{x}_{2}}(\\mathbf{v}_{2}),\\dots,\\operatorname{Exp}_{\\mathbf{x}_{k}}(\\mathbf{v}_{k})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/e8d5d66fe4a5ce713f7ab1e036bbdfe2e3d8214bce4ce59bf6d714a4b326595e.jpg", "table_caption": ["Table 5: Dataset statitics. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Experimental Setups ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use four common benchmark datasets to evaluate our model and Table 5 show the details of the datasets. There are two co-purchase graphs including Amazon-Photo and Amazon-Computers [53] and two co-author network including CS and Physics [53]. ", "page_idx": 19}, {"type": "text", "text": "E.2 Baselines ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/92a319e39a3dd9f5a92bb64e6ce59c8eeb62c277b2d6d1a046863c9c32610ef2.jpg", "table_caption": ["Table 6: The categories of the baselines "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "As shown in Table 6, we divide the baselines into three categories: ANN-based Euclidean GNNs, ANN-based Riemannian GNNs and SNN-based Euclidean GNNs. Note that, none of the existing work studies the SNN-based GNN in Riemannian space, to the best of our knowledge. ", "page_idx": 19}, {"type": "text", "text": "ANN-based Euclidean GNNs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 GCN [18]: It defines graph convolution on the spectral domain.   \n\u2022 SAGE [1]: It gives the aggregate-and-combine formulation for the message passing over the graph.   \n\u2022 GAT [2]: It introduces the attention mechanism for the learning on graphs.   \n\u2022 SGC [19]: It reformulates GCN [18] with feature propagation and linear layer, acting as a low-pass filter. ", "page_idx": 19}, {"type": "text", "text": "ANN-based Riemannian GNNs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 HGCN [3] $:$ It generalizes GAT [2] in the Lorentz model of hyperbolic space in which the graph convolution is conducted in the tangent space.   \n\u2022 $\\kappa{\\mathrm{-}}\\mathbf{GCN}$ [13]: It generalizes GCN [18] to the $\\kappa$ \u2212stereographical model of constant curvature spaces where several gyrovector operators are given in the unified formalism.   \n\u2022 HyboNet [54]: It introduces a parameterized Lorentz transformation for hyperbolic graph modeling without the tangent space.   \n\u2022 $\\mathcal{Q}$ \u2212GCN [4]: It studies the graph convolution network in the Pseudo-Riemannian manifold. ", "page_idx": 19}, {"type": "text", "text": "SNN-based Euclidean GNNs. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 SpikeGCN [5]: It integrates SNN and graph convolution network in which SNN acts as an activation function.   \n\u2022 SpikeGraphormer [55] (termed as SpikeGT for short in our main paper): It generalizes a kind of graph transformer with spiking neurons, accompanied by an ANN for improving the performance.   \n\u2022 SpikeGCL [6]: It introduces a method to perform graph contrastive learning with the spiking GNN.   \n\u2022 SpikeNet [45]: It is original designed for dynamic graph modeling, and we utilize its version for static graph following [6]. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Note that, existing SNN-based GNNs work with Euclidean space, and leverage the BPTT training with surrogate gradient, suffering from high latency. ", "page_idx": 20}, {"type": "text", "text": "E.3 Theoretical Energy Consumption ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Following the previous works [6; 5; 62], we calculate the theoretical energy consumption for each model, instead of measuring actual electricity usage, for fair comparison. ", "page_idx": 20}, {"type": "text", "text": "\u2022 For the SNN-based models, the energy consumption involves encoding energy $E_{\\mathrm{encoding}}$ and spiking process energy $E_{\\mathrm{spiking}}$ . The former is calculated by the number of multiply-andaccumulate (MAC) operations, and the latter is given by the number of SOP operations. The energy consumption is thus defined as follows, ", "page_idx": 20}, {"type": "equation", "text": "$$\nE=E_{\\mathrm{encoding}}+E_{\\mathrm{spiking}}=E_{\\mathrm{MAC}}\\sum_{t=1}^{T}N d S_{t}+E_{\\mathrm{SOP}}\\sum_{t=1}^{T}\\sum_{l=1}^{L}S_{t}^{l},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the scaling constant $E_{\\mathrm{MAC}}$ and $E_{\\mathrm{SOP}}$ are set to $4.6p J$ and $3.7p J$ , respectively. $N$ is the number of nodes in the graph, $d$ is the dimension of node features, $L$ is the number of layers in the neural model. $T$ is the time steps of the spikes, and $S_{t}^{l}$ denotes the output spikes at time step $t$ and layer $l$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 For the ANN-based models, the energy consumption is given by embedding generation step and aggregation step, and both of them are calculated by MAC operations. In the embedding generation step, the feature transformation with a weight matrix of $\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ executes $N d_{i n}d_{o u t}$ multiplication and $N d_{i n}d_{o u t}$ addition operations. In the aggregation step, $|\\mathcal{E}|d_{i n}$ is the number of multiplication and $|\\mathcal{E}|d_{o u t}$ is the number of addition operations. Supposing $|\\mathcal{E}|d_{i n}=|\\mathcal{E}|d_{o u t}$ , the energy consumption is given as follows, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E=E_{\\mathrm{MAC}}(N d_{i n}d_{o u t}+|\\mathcal{E}|d_{o u t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the scaling constant $E_{\\mathrm{MAC}}$ is set to $4.6p J,|\\mathcal{E}|$ is the number of edge, $d_{i n}$ and $d_{o u t}$ are the input and output dimensions. ", "page_idx": 20}, {"type": "text", "text": "E.4 Implementation Notes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Model Instantiation. The proposed MSG is instantiated in the Lorentz model $\\mathbb{H}$ of hyperbolic space or Sphere model $\\mathbb{S}$ of hyperspherical space as well as the products over $\\mathbb{H}$ and $\\mathbb{S}$ . Note that, MSG can be equivalently instantiated on the $\\kappa-$ stereographic model (i.e., Poincar\u00e9 model of hyperbolic space or Hyperspherical model of hyperspherical space), given the closed form Riemannian metric and exponential map. The equivalence can be achieved by scaling the stereographical projection. We leverage the Lorentz model $\\mathbb{H}$ by default. ", "page_idx": 20}, {"type": "text", "text": "Hyperparameters. The dimension of the manifold is set as 32. When we instantiate MSG on the product manifold, the sum of factor manifold\u2019s dimensions is defined as 32. The manifold spiking neuron is based on the IF model [49] by default, and it is ready to switch to the LIF model [49]. The time latency $T$ for neurons is set to 5 or 15. The step size $\\epsilon$ in Eq. 8 is set to 0.1. The hyperparameters are tuned with grid search, in which the learning rate is $\\{0.01,0.003\\}$ for node classification and $\\{0.003,0.001\\}$ for link prediction, and the dropout rate is in $\\{0.1,0.3,\\dot{0}.5\\}$ . ", "page_idx": 20}, {"type": "text", "text": "Hardware. Experiments are conducted on the NVIDIA GeForce RTX 4090 GPU 24GB memory, and AMD EPYC 9654 CPU 315 with 96-Core Processor. ", "page_idx": 20}, {"type": "text", "text": "F Additional Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show the additional results on backward gradient, comparison between IF and LIF model, link prediction and visualization. ", "page_idx": 20}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/c2ee448f76c40a40e411d8448fc2c72a541bf92ebc0a75ca4f19ab998cb04b48.jpg", "img_caption": ["(a) The norm of backward gradient of $L$ with respect to $\\mathbf{z}$ in each spiking layers. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/0cba57fb85fbf96fb53efff5a8163c16dc17c22b1781fa23310ae1f37ccb1fde.jpg", "img_caption": ["(b) The norm of backward gradient of $L$ with respect to $\\mathbf{v}$ in each spiking layers. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/a88515e7f5b0237cd3fa9db2e24ef3a94d3fc0ea675aab8c5eecc529d03d69b9.jpg", "img_caption": ["Figure 6: Visualizations of the training process for node classification on Computer dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Backward Gradient. Previous studies compute backward gradients though the Differentiation via Spikes $(D\\nu S)$ . Distinguishing from the previous studies, we compute backward gradients though the Differentiation via Manifold $(D\\nu M)$ . In order to examine the backward gradients, we visualize the training process for node classification on Computer dataset. Concretely, we plot the norm of backward gradients in each iteration in Figs. 6 (a) and (b) together with the value of loss function in Figs. 6 (c). As shown in Fig. 6, the proposed algorithm with $D\\nu M$ converges well, and the backward gradients do not suffer from gradient vanishing or gradient explosion. ", "page_idx": 21}, {"type": "text", "text": "Comparison between IF and LIF model. In the main paper, the proposed MSG is built with the IF model, and it is applicable to LIF model as well. We compare the performance between IF and LIF model in different algorithms ( $D\\nu M$ and $D\\nu S$ ) and in different manifolds (hyperbolic $\\mathbb{H}$ , hyperspherical $\\mathbb{S}$ , Euclidean $\\mathbb{E}$ and the product spaces among $\\mathbb{H}$ and $\\mathbb{S}$ ) for a comprehensive evaluation. ", "page_idx": 21}, {"type": "text", "text": "The results of node classification on Computer, Photo, CS and Physics datasets are summarized in Table 7 and Table 8. Note that, IF model and LIF model achieves competitive performance in every case. We opt for IF model in the model instantiation for simplicity. ", "page_idx": 22}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/8197b5ef0e52de5c927e2f5faa59059a5210ba54fa24e47390f9b78d0adb3cc3.jpg", "table_caption": ["Table 7: Comparison between IF and LIF model in Node Classification, qualified by classification accuracy $(\\%)$ . The proposed model is trained by Differentiation via Spikes (i.e., BPTT with the surrogate gradient). "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/def328195c4478168d83914d053d9f9d566904cb013be6fa6500c6ae26b101e1.jpg", "table_caption": ["Table 8: Comparison between IF and LIF model in Node Classification, qualified by classification accuracy $(\\%)$ . The proposed model is trained by Differentiation via Manifold. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Link Prediction. The performance of link prediction in terms of AUC is provided in the main paper. We show the results on Computer, Photo, CS and Physics datasets in terms of AP $(\\%)$ in Table 9. In particular, we feed the spiking representation of SpikingGCN and SpikeGCL into the Fermi-Dirac decoder same as the proposed MSG, while SpikeNet and SpikeGT are designed for node classification specially. As shown in Table 9, the proposed spiking MSG consistently achieves the best results among the spiking GNNs on all the four datasets, and even achieves the competitive performances with the strong Riemannian baselines. ", "page_idx": 22}, {"type": "text", "text": "Table 9: Link Prediction in terms of AP $(\\%)$ on Computers, Photo, CS and Physics datasets. The best results are boldfaced, and the runner-ups are underlined. The standard derivations are given in the subscripts. OOM denotes Out-Of-Memory. ", "page_idx": 22}, {"type": "table", "img_path": "VKt0K3iOmO/tmp/4b29fd6d8b26c8308fd900c74c25470c98da903025b343b18f9463ebf099ede4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Visualization. Here, we visualize the forward pass of the proposed MSG and empirically demonstrate the connection between MSG and manifold ordinary differential equation (ODE). ", "page_idx": 22}, {"type": "text", "text": "We choose a toy example of KarateClub dataset. The proposed MSG are instantiated on the 2D manifold for the ease of visualization. Specifically, we plot the node representation of each spiking layer in Fig. 7(a) and Fig. 7(b) in which the curve connecting the outputs of successive layer is marked in red, and blue arrow is the direction of the geodesic. It is shown that a spiking layer forwards the node along the geodesic on the manifold. In other words, each layer, constructing a chart given by the exponential map, is a solver of the ODE describing the geodesic. ", "page_idx": 22}, {"type": "image", "img_path": "VKt0K3iOmO/tmp/a6b10be270c18d4997995d2422ddd331a116a7816bfb3f51c2795e9b2f9ad70d.jpg", "img_caption": ["Figure 7: Visualizations of node representations on Zachary karateClub datasets [58]. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We propose a novel Manifold-valued Spiking GNN (MSG), and design a new training algorithm with Differentiation via Manifold with theoretical gaurantee. Extensive experiments show the effectiveness of the proposed approach. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations in Sec. 8. The proposed model is applicable to any geodesically complete manifold (e.g., hyperbolic space, hyperspherical space and their products), and its generalization to more generic manifold leaves as the future work. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide the complete proof in Appendix B. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Experimental details are given in Sec. 6.1, and further introduced in Appendix E. Also, we give pseudocodes in Algorithm 1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The datasets are publicly available. We properly cite and introduce the datasets in Sec. 6.1 and Appendix E.1. Our source code is at the anonymous link https: //anonymous.4open.science/r/MSG-16E9. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We give experimental settings in Sec. 6.1, which is further detailed in Appendix E. Implementation details can be found in the anonymous link at https://anonymous. 4open.science/r/MSG-16E9. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In the experiment, we perform 10 independent runs for each case, and report the mean with standard derivations in Sec. 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In Sec. 6.1, we provide the information on the computer resources: NVIDIA GeForce RTX 4090 GPU 24GB memory, and AMD EPYC 9654 CPU with 96-Core Processor. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts of the work performed in Sec. 8. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We properly cite the public datasets and open source Python library (e.g., Geoopt and SpikingJelly) in Sec. 6.1 and Appendix E.1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We give the source code of the proposed model with documentation at https: //anonymous.4open.science/r/MSG-16E9. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]