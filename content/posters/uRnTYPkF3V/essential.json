{"importance": "This paper is **crucial** for researchers in online learning and information theory. It provides **novel theoretical insights** into sequential probability assignment, offers a **new minimax optimal algorithm**, and **sharpens existing regret bounds**. This work opens up exciting new avenues for research and has significant implications for developing more efficient and effective online learning algorithms.", "summary": "This paper introduces contextual Shtarkov sums, a new complexity measure characterizing minimax regret in sequential probability assignment with contexts, and derives the minimax optimal algorithm, contextual NML.", "takeaways": ["Contextual Shtarkov sums precisely characterize minimax regret in sequential probability assignment with contexts.", "The paper introduces contextual Normalized Maximum Likelihood (cNML), a novel minimax optimal algorithm.", "The study provides a sharpened regret upper bound using sequential entropy, unifying and improving state-of-the-art results."], "tldr": "Sequential probability assignment, a fundamental problem in online learning and information theory, seeks algorithms minimizing regret against the best expert from a hypothesis class.  Existing complexity measures like sequential entropy fail to fully characterize minimax regret, particularly with contexts (side information).  The minimax optimal strategy remains elusive.\nThis research introduces contextual Shtarkov sums, a new complexity measure based on projecting Shtarkov sums onto context trees.  It proves that the minimax regret equals the worst-case log contextual Shtarkov sum.  Using this, it derives the minimax optimal algorithm, contextual NML, extending Normalized Maximum Likelihood to contextual settings. This work also offers a simplified proof for a tighter regret upper bound based on sequential entropy.", "affiliation": "University of Toronto", "categories": {"main_category": "AI Theory", "sub_category": "Optimization"}, "podcast_path": "uRnTYPkF3V/podcast.wav"}