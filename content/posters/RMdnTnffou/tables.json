[{"figure_path": "RMdnTnffou/tables/tables_6_1.jpg", "caption": "Table 1: Classification Accuracy and Average Percentage of Activated Concepts (Sparsity). By bold blue/red, we denote the best-performing high/low level sparsity-inducing concept-based model.", "description": "This table presents a comparison of different models' performance on three benchmark datasets (CUB, SUN, and ImageNet).  The models are categorized into several groups: Baseline (using raw image embeddings), non-interpretable models using CLIP embeddings, concept-based models (both whole-image and patch-based), and the proposed Coarse-to-Fine Concept Bottleneck Model (CF-CBM). For each model, the table shows classification accuracy and the average percentage of activated concepts (a measure of sparsity).  The best-performing models in each category for both high and low-level sparsity are highlighted in bold.", "section": "4 Experimental Evaluation"}, {"figure_path": "RMdnTnffou/tables/tables_7_1.jpg", "caption": "Table 2: Attribute matching accuracy. We compare our approach to the recent CDM model trained with the considered AL set. Then, we predict the matching between the inferred per-example concept indicators to: (i) class-wise and (ii) per-example ground truth attributes found in both SUN and CUB.", "description": "This table presents the results of attribute matching accuracy experiments.  The authors compare their Coarse-to-Fine Concept Bottleneck Model (CF-CBM) against a recent Concept Bottleneck Model (CDM) using two attribute sets: class-wise and example-wise.  The matching accuracy and Jaccard index are reported for both SUN and CUB datasets, providing a quantitative measure of the model's ability to discover relevant concepts.", "section": "4 Experimental Evaluation"}, {"figure_path": "RMdnTnffou/tables/tables_7_2.jpg", "caption": "Table 1: Classification Accuracy and Average Percentage of Activated Concepts (Sparsity). By bold blue/red, we denote the best-performing high/low level sparsity-inducing concept-based model.", "description": "This table presents a comparison of the classification accuracy and sparsity (average percentage of activated concepts) achieved by different models on three benchmark datasets: CUB, SUN, and ImageNet.  The models compared include a baseline using only image information, a non-interpretable CLIP embedding model, label-free CBMs, concept-discovery high level models (CDMH), concept discovery low level models (CDML), and the proposed Coarse-to-Fine CBMs (CF-CBMs) at both high and low levels. The table shows that the CF-CBM approach generally outperforms other methods in terms of accuracy, while also achieving high sparsity.", "section": "4 Experimental Evaluation"}, {"figure_path": "RMdnTnffou/tables/tables_13_1.jpg", "caption": "Table 1: Classification Accuracy and Average Percentage of Activated Concepts (Sparsity). By bold blue/red, we denote the best-performing high/low level sparsity-inducing concept-based model.", "description": "This table compares the classification accuracy and sparsity (average percentage of activated concepts) of different models on three benchmark datasets: CUB, SUN, and ImageNet.  The models include a baseline using only image information, non-interpretable CLIP embeddings, Label-Free CBMs, and concept-based models with and without the proposed high and low-level concept discovery mechanisms.  The table highlights the performance of the proposed Coarse-to-Fine CBM (CF-CBM) approach, showing its superior accuracy and sparsity compared to other methods.", "section": "4 Experimental Evaluation"}, {"figure_path": "RMdnTnffou/tables/tables_13_2.jpg", "caption": "Table 1: Classification Accuracy and Average Percentage of Activated Concepts (Sparsity). By bold blue/red, we denote the best-performing high/low level sparsity-inducing concept-based model.", "description": "This table compares the classification accuracy and sparsity (average percentage of activated concepts) of different models on three benchmark datasets: CUB, SUN, and ImageNet.  It compares the proposed Coarse-to-Fine Concept Bottleneck Model (CF-CBM) against several baselines and state-of-the-art methods, including non-interpretable models, CLIP embeddings (both image and patch level), label-free CBMs, and concept-discovery models. The table shows both high-level and low-level results for the CF-CBM, demonstrating its performance across different levels of granularity and sparsity settings.", "section": "4 Experimental Evaluation"}]