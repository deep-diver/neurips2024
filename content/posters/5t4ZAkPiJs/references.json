{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduces the concept of few-shot learning in large language models, which is foundational to the current research in LLM compression and efficient inference."}, {"fullname_first_author": "T. Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "This paper presents FlashAttention, a crucial technique for accelerating attention mechanisms in LLMs, directly impacting the efficiency of KV cache compression methods."}, {"fullname_first_author": "E. Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-26", "reason": "This paper introduces GPTQ, a state-of-the-art post-training quantization method for LLMs, which is highly relevant to the efficient compression of KV cache."}, {"fullname_first_author": "S. Ge", "paper_title": "Model tells you what to discard: Adaptive KV cache compression for LLMs", "publication_date": "2024-01-01", "reason": "This paper focuses on adaptive KV cache compression, a technique highly relevant to the approach presented in the main paper and provides a comparative baseline."}, {"fullname_first_author": "H. Kang", "paper_title": "GEAR: An efficient KV cache compression recipe for near-lossless generative inference of LLM", "publication_date": "2024-03-01", "reason": "This paper proposes another competitive KV cache compression method, GEAR, which serves as a strong baseline for comparison in evaluating the performance of the proposed ZipCache method."}]}