[{"figure_path": "2AIwiIkE0s/tables/tables_4_1.jpg", "caption": "Table 1: An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. We utilize the widely used ViT NAS search space, Autoformer [6], which includes three different search spaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample 3,000 architectures from these search spaces to populate our benchmark. To ensure comprehensiveness, we evaluate these architectures across 8 of the most common SOTA OOD datasets. Following prior OoD generalization works [57, 30, 31, 15], we employ three metrics for our benchmark: ID Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).", "description": "This table summarizes the experimental setup used to create the OoD-ViT-NAS benchmark. It details the search space (Autoformer with three variations: Tiny, Small, and Base), the eight out-of-distribution (OoD) datasets used for evaluation, the number of classes and images in each dataset, and the metrics used (ID accuracy, OoD accuracy, and AUPR). The table provides a comprehensive overview of the benchmark's design and scope.", "section": "3 OoD-VIT-NAS: NAS Benchmark for ViT's OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the Kendall \u03c4 correlation coefficients between the OoD accuracy and the predictions from nine different training-free neural architecture search (NAS) methods and two simple proxies (#Param and #Flops).  The results show that existing training-free NAS methods are not very effective at predicting OoD accuracy, even those specifically designed for Vision Transformers (ViTs). Surprisingly, simple proxies like #Param and #Flops show better correlation with OoD accuracy than the more sophisticated training-free NAS methods. This indicates a need for improved training-free NAS methods for predicting the out-of-distribution (OoD) generalization performance of ViTs.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_9_1.jpg", "caption": "Table 1: An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. We utilize the widely used ViT NAS search space, Autoformer [6], which includes three different search spaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample 3,000 architectures from these search spaces to populate our benchmark. To ensure comprehensiveness, we evaluate these architectures across 8 of the most common SOTA OOD datasets. Following prior OoD generalization works [57, 30, 31, 15], we employ three metrics for our benchmark: ID Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).", "description": "This table details the experimental setup of the OoD-ViT-NAS benchmark.  It shows the search space used (Autoformer-Tiny, Small, Base), the number of architectures sampled (3000), the eight out-of-distribution (OOD) datasets used for evaluation, and the three metrics used to measure performance: In-Distribution (ID) accuracy, OoD accuracy, and Area Under the Precision-Recall Curve (AUPR).", "section": "3 OoD-ViT-NAS: NAS Benchmark for ViT's OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_16_1.jpg", "caption": "Table 1: An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. We utilize the widely used ViT NAS search space, Autoformer [6], which includes three different search spaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample 3,000 architectures from these search spaces to populate our benchmark. To ensure comprehensiveness, we evaluate these architectures across 8 of the most common SOTA OOD datasets. Following prior OoD generalization works [57, 30, 31, 15], we employ three metrics for our benchmark: ID Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).", "description": "This table details the experimental setup used to create the OoD-ViT-NAS benchmark. It outlines the search space (Autoformer-Tiny/Small/Base), the number of architectures sampled (3000), the eight out-of-distribution (OOD) datasets used for evaluation (ImageNet-C, ImageNet-P, ImageNet-D, Stylized ImageNet, ImageNet-R, ImageNet-Sketch, ImageNet-A, ImageNet-O), and the three metrics employed to assess the model's performance: ID Accuracy, OoD Accuracy, and AUPR.", "section": "3 OoD-ViT-NAS: NAS Benchmark for ViT's OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_17_1.jpg", "caption": "Table 1: An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. We utilize the widely used ViT NAS search space, Autoformer [6], which includes three different search spaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample 3,000 architectures from these search spaces to populate our benchmark. To ensure comprehensiveness, we evaluate these architectures across 8 of the most common SOTA OOD datasets. Following prior OoD generalization works [57, 30, 31, 15], we employ three metrics for our benchmark: ID Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).", "description": "This table summarizes the experimental setup used in the OoD-ViT-NAS benchmark. It specifies the search space (Autoformer-Tiny, -Small, -Base) used to generate 3,000 ViT architectures,  the eight standard Out-of-Distribution (OoD) datasets employed for evaluation, and the three metrics (ID Accuracy, OoD Accuracy, AUPR) used for assessing performance.", "section": "3 OoD-VIT-NAS: NAS Benchmark for ViT's OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_25_1.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the Kendall's \u03c4 correlation coefficients between the OoD accuracies and different training-free NAS proxy values on eight common large-scale out-of-distribution (OoD) datasets.  The results demonstrate that existing training-free NAS methods are largely ineffective at predicting OoD accuracy for Vision Transformers (ViTs).  Surprisingly, simple proxies such as the number of parameters (\"#Param\") or floating-point operations (\"#Flops\") significantly outperform more complex training-free NAS methods in predicting both OoD and in-distribution (ID) accuracy for ViTs.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_27_1.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the Kendall \u03c4 correlation coefficients between the OoD accuracies and the predictions from nine different training-free Neural Architecture Search (NAS) methods.  The methods are evaluated across eight common out-of-distribution (OoD) datasets and three different ViT architecture search spaces. The results show that existing training-free NAS methods are not very effective at predicting OoD accuracy, even those recently proposed and specifically designed for Vision Transformers (ViTs). Surprisingly, simple metrics like the number of parameters (#Param) and floating-point operations (#Flops) outperform the more complex training-free NAS methods in predicting both OoD and in-distribution (ID) accuracy for ViTs.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_27_2.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the Kendall \u03c4 ranking correlation coefficients between the out-of-distribution (OoD) accuracies and the predictions of nine training-free neural architecture search (NAS) methods on eight common large-scale OoD datasets.  The results show that existing training-free NAS methods are not very effective at predicting OoD accuracy for Vision Transformers (ViTs), even recently proposed ones. Surprisingly, simple proxies like the number of parameters (#Param) and the number of floating-point operations (#Flops) surprisingly outperform more complex training-free NAS methods in predicting both OoD and in-distribution (ID) accuracy.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_28_1.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the Kendall \u03c4 ranking correlation coefficients between the out-of-distribution (OoD) accuracies and various training-free Neural Architecture Search (NAS) proxies across eight common large-scale OoD datasets.  It shows that existing training-free NAS methods are not very effective at predicting OoD accuracy for Vision Transformers (ViTs), even recent ones.  Surprisingly, simple proxies like the number of parameters (#Param) or floating point operations (#Flops) surprisingly outperform more complex training-free NAS methods.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_28_2.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the results of a study comparing the effectiveness of various training-free Neural Architecture Search (NAS) methods in predicting out-of-distribution (OoD) accuracy for Vision Transformers (ViTs).  It compares the Kendall \u03c4 correlation between the OoD accuracy and the prediction from each NAS method, using 8 common OoD datasets.  The table notably shows that simple proxies like parameter count (\"#Param\") and floating-point operations (\"#Flops\") surprisingly outperform more sophisticated training-free NAS methods in accurately predicting OoD accuracy.  This finding challenges the current state-of-the-art in training-free NAS for ViTs and suggests that simpler measures might be more effective for predicting OoD robustness.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_29_1.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the Kendall's \u03c4 ranking correlation between the out-of-distribution (OoD) accuracy and predictions from nine different zero-cost training-free neural architecture search (NAS) methods across eight common OoD datasets. The results show that existing training-free NAS methods are not very effective at predicting OoD accuracy, even for methods specifically designed for Vision Transformers (ViTs). Surprisingly, simple proxies such as the number of parameters (#Param) or floating point operations (#Flops) surprisingly outperform the more complex NAS methods in predicting both OoD and in-distribution (ID) accuracy for ViTs.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_29_2.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents a comparison of the Kendall \u03c4 rank correlation between the out-of-distribution (OoD) accuracies and the predictions from nine different training-free neural architecture search (NAS) methods.  The methods are evaluated on eight common large-scale OoD datasets using the OoD-ViT-NAS benchmark.  The table highlights that existing training-free NAS methods are not very effective at predicting OoD accuracy, even those recently proposed and specifically designed for Vision Transformers (ViTs). Surprisingly, simple proxies like the number of parameters (#Param) or floating point operations (#Flops) significantly outperform the more complex training-free NAS methods in predicting both OoD and in-distribution (ID) accuracy for ViTs.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_30_1.jpg", "caption": "Table 2: Comparison of Kendall \u03c4 ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. Bold and underline stand for the best and second, respectively. We show that existing Training-free NAS's predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OOD/ID accuracy.", "description": "This table presents the Kendall \u03c4 correlation coefficients between the out-of-distribution (OoD) accuracies and the predictions from nine different training-free neural architecture search (NAS) methods, along with two simple proxies (#Param and #Flops), across eight common OoD datasets.  The results show that existing training-free NAS methods are largely ineffective at predicting OoD accuracy.  Surprisingly, the simple proxies of #Param and #Flops outperform more sophisticated training-free NAS methods in this task.", "section": "4.3 Explore Training-free NAS for OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_34_1.jpg", "caption": "Table 1: An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. We utilize the widely used ViT NAS search space, Autoformer [6], which includes three different search spaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample 3,000 architectures from these search spaces to populate our benchmark. To ensure comprehensiveness, we evaluate these architectures across 8 of the most common SOTA OOD datasets. Following prior OoD generalization works [57, 30, 31, 15], we employ three metrics for our benchmark: ID Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).", "description": "This table details the experimental setup for the OoD-ViT-NAS benchmark.  It lists the search space used (Autoformer-Tiny/Small/Base), the number of architectures sampled (3000), the eight out-of-distribution (OoD) datasets used for evaluation, and the three metrics employed to assess performance: In-Distribution (ID) Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).", "section": "3 OoD-VIT-NAS: NAS Benchmark for ViT's OoD Generalization"}, {"figure_path": "2AIwiIkE0s/tables/tables_34_2.jpg", "caption": "Table 1: An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. We utilize the widely used ViT NAS search space, Autoformer [6], which includes three different search spaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample 3,000 architectures from these search spaces to populate our benchmark. To ensure comprehensiveness, we evaluate these architectures across 8 of the most common SOTA OOD datasets. Following prior OoD generalization works [57, 30, 31, 15], we employ three metrics for our benchmark: ID Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).", "description": "This table details the experimental setup for the OoD-ViT-NAS benchmark.  It specifies the search space used (Autoformer with Tiny, Small, and Base variations), the number of architectures sampled (3000), the eight out-of-distribution (OOD) datasets used for evaluation, and the three metrics employed to assess performance: In-Distribution (ID) accuracy, OOD accuracy, and Area Under the Precision-Recall Curve (AUPR).", "section": "3 OoD-VIT-NAS: NAS Benchmark for ViT's OoD Generalization"}]