[{"type": "text", "text": "Communication-Efficient Federated Group Distributionally Robust Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhishuai Guo, Tianbao Yang\u2217   \nDepartment of Computer Science and Engineering Texas A&M University   \nzhishguo@tamu.edu,tianbao-yang@tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning faces challenges due to the heterogeneity in data volumes and distributions at different clients, which can compromise model generalization ability to various distributions. Existing approaches to address this issue based on group distributionally robust optimization (GDRO) often lead to high communication and sample complexity. To this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO). Our contributions are threefold: Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average top-K losses while reducing communication complexity to $\\bar{O}(1\\bar{/}\\epsilon^{4})$ , where $\\epsilon$ denotes the desired precision level. Secondly, our FGDRO-KL algorithm is crafted to optimize KL regularized FGDRO, cutting communication complexity to $O(1/\\epsilon^{3}\\bar{)}$ . Lastly, we propose FGDRO-KL-Adam to to utilize Adam-type local updates in FGDRO-KL, which not only maintains a communication cost of $O(1/\\bar{\\epsilon}^{3})$ but also shows potential to surpass SGD-type local steps in practical applications. The effectiveness of our algorithms has been demonstrated on a variety of real-world tasks, including natural language processing and computer vision. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning enables effective model training without the need to share raw data [38, 48]. It is essential in contexts where data privacy and ownership are paramount, such as in inter-hospital collaborations [54] and mobile device networks [20]. However, clients often have data of varying volumes and distinct distributions, which poses notable challenges in maintaining generalization behavior [50, 27]. Generalization here refers to the model\u2019s ability to perform consistently across different clients, including those that have not participated in the training [23, 79]. ", "page_idx": 0}, {"type": "text", "text": "In this study, we tackle the issue using federated group distributionally robust optimization (FGDRO), formulated as follows: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}F(\\mathbf{w}):=\\operatorname*{max}_{\\mathbf{p}\\in\\Delta_{N}}\\sum_{i=1}^{N}\\mathbf{p}_{i}\\ell_{i}(\\mathbf{w})-\\lambda\\phi(\\mathbf{p}).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here, w denotes a machine learning model, and $N$ represents the number of clients. For each client $i$ , $\\mathcal{D}_{i}$ represents its local data distribution, and $\\ell_{i}(\\mathbf{w})=\\mathbb{E}_{\\mathbf{z}\\sim\\mathcal{D}_{i}}\\ell(\\mathbf{w};\\mathbf{z})$ represents the loss calculated from that local distribution. $\\Delta_{N}$ denotes a $N$ -dimensional simplex, which constrains $\\textstyle\\sum_{i}p_{i}=1$ . The vector $\\mathbf{p}=[\\mathbf{p}_{1},...,\\mathbf{p}_{N}]$ comprises the weights assigned to each of the $N$ clients. The function $\\phi(\\mathbf{p})$ acts as a regularization term, with $\\lambda>0$ being an adjustable parameter. This framework aims to assign higher weights to machines with greater losses while discouraging substantial deviations of these weights from a specified distribution. ", "page_idx": 0}, {"type": "text", "text": "Our study concentrates on two particular forms of the regularization term $\\phi$ , which are well-established regularization techniques [11, 39], each suited to different tasks and data distributions. Specifically, CVaR is defined as $\\phi(\\mathbf{p})=\\mathbb{I}_{[0,1/K]}(\\mathbf{p})$ . In this scenario, $\\phi(\\mathbf{p})$ is set to 0 if each weight $\\mathbf{p}_{i}$ falls within the range of $[0,1/K]$ , and is infinite otherwise. FGDRO-CVaR focuses on optimizing for worst-case scenarios or the average of the worst-case losses, making it particularly effective in high-stakes applications like healthcare and finance, where avoiding extreme losses is crucial. However, it can be sensitive to outliers or malicious client attacks. FGDRO-KL, on the other hand, uses Kullback-Leibler (KL) divergence,expressed as $\\begin{array}{r}{\\phi(\\mathbf{p})=\\sum_{i=1}^{N}\\mathbf{p}_{i}\\log(N\\mathbf{p}_{i})}\\end{array}$ . This version of $\\phi$ penalizes deviations of the weight distribution $\\mathbf{p}$ from a uniform distribution. Fundamentally, when $\\phi(\\mathbf{p})$ is strongly convex, as in the case of KL divergence, $F(\\mathbf{w})$ can enjoy a smoothness property, while non-strongly convex $\\phi(\\mathbf{p})$ would result in non-smooth $F(\\mathbf{w})$ [6]. In contrast to CVaR, KL is a softer regularizer to promote smoother and more stable learning. Thus, it can be beneficial in scenarios where robustness to outliers or malicious clients is needed. FGDRO-KL-Adam further enhances FGDRO-KL by incorporating Adam-type updates. ", "page_idx": 1}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/583feb6b162c5d9967e7709342401b07843f83e948c5e6cc7ab5407c19505070.jpg", "table_caption": ["Table 1: Comparison of communication cost and sample complexity on each machine to achieve $\\epsilon$ -stationary point or near to $\\epsilon$ -stationary point, where $\\epsilon_{}$ -stationary point has a (sub-)gradient $\\lVert\\partial F(\\mathbf{w})\\rVert^{2}\\,\\dot{\\leq}\\,\\,\\epsilon^{2}$ . NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Previous research addressing these optimization problems in federated learning has struggled with high communication and sample complexity issues, which are basically due to inefficient updates of $\\mathbf{p}$ on local machines. For example, Deng et al. [11] examined a particular case of the general formula (1) using a $\\mathrm{CVaR}$ constraint with $K=1$ , and developed an algorithm for KL regularization as well. They update $\\mathbf{p}$ only in global communication rounds, while local steps only optimize the local loss function using stochastic gradient descent (SGD). To achieve a $\\epsilon$ -stationary point or a point near to an $\\epsilon_{}$ -stationary point, where an $\\epsilon$ -stationary point has a (sub)gradient $\\lVert\\partial F(\\mathbf{\\dot{w}})\\rVert^{2}\\leq\\epsilon^{2}$ , their methods required a communication cost of $O(1/\\epsilon^{\\bar{1}2})$ and a sample complexity of $\\dot{O}(1/\\epsilon^{16})$ on each client. For FGDRO with KL regularization, [30] achieves a communication cost of ${\\cal O}(1/\\epsilon^{3})$ , but it requires the use of large data batches in local update steps in order to get good approximation for the surrogate of p, resulting in a total sample complexity of ${\\cal O}(1/\\epsilon^{6})$ per machine. ", "page_idx": 1}, {"type": "text", "text": "To overcome these limitations, this paper presents specialized algorithms FGDRO-CVaR and FGDROKL for FGDRO with CVaR constraint and KL regularization, respectively. Instead of dealing with the constrained primal-dual formulation in (1), we consider their equivalent forms with a compositional structure that get rid of the high-dimensional constrained variable p. We summarize the complexity results in Table 1. ", "page_idx": 1}, {"type": "text", "text": "For FGDRO with CVaR constraint, we are the first to consider a constraint-free equivalent form and develop a communication-efficient algorithm for it, significantly reducing communication costs, as shown in Table 1. In addition to sharing machine learning models, we only introduce an additional scalar threshold to select participants in each round, minimizing additional costs. The equivalent compositional form is a non-smooth two-level compositional function with one auxiliary variable $s$ , which works as a threshold. Only machines whose local losses are greater than $s$ are supposed to contribute to updating the model. In this way, we can simply update the constraint-free scalar variable $s$ locally in each client and average $s$ in communication rounds. However, we do face challenges with non-smooth compositional optimization problems. Our first algorithm FGDRO-CVaR effectively addresses this issues and achieves a communication cost of ${\\cal O}(1/\\epsilon^{4})$ and a sample complexity of ${\\cal O}(1/\\epsilon^{8})$ on each machine. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "For FGDRO with KL regularization, while previous literature has explored constraint-free compositional reformulations, they often require large batch sizes on each machine to estimate gradients, making this approach impractical and leading to high sample complexity. In contrast, we utilize moving averages that can work with smaller data batches while still providing accurate gradient estimates, enhancing the efficiency of our method. The equivalent compositional form we consider is a smooth three-level compositional function. In this case, the weights for the clients depend on both local loss functions and global loss functions. We use moving average estimators of these statistics and update the estimators locally. In communication rounds, in addition to averaging the model w, the machines will average the estimator of the global loss function. We have reduced the communication cost and the computation cost compared to the literature as presented in Table 1. ", "page_idx": 2}, {"type": "text", "text": "To further enhance our approach, we have developed an adaptive algorithm for solving FGDRO with KL regularization, named FGDRO-KL-Adam. Stochastic adaptive methods apply variable step sizes for each coordinate based on historical gradient information, often yielding better results than non-adaptive techniques, as evidenced by a wealth of research [13, 36, 49, 69]. In federated learning, while Reddi et al. [59] have developed a federated adaptive algorithm and shown its effectiveness in various tasks. However, it limits adaptive steps to global updates on the server, with local updates relying on standard SGD, which may lead to suboptimal results. Moreover, their method is primarily designed for Empirical Risk Minimization (ERM) and is not applicable to address compositional optimization problems. Our FGDRO-KL-Adam allows local updates to use Adam-type updates, which introduces the challenge of handling unbiased gradients, further complicated by the use and updating of the second-order moment. To this end, we update the first-order momentum and secondorder momentum locally and then average them globally during communication rounds. Moreover, our analysis carefully manages the moving estimates of the first and second-order moments, ensuring that the solution provably converges. ", "page_idx": 2}, {"type": "text", "text": "Our FGDRO-KL-Adam enables local updates with Adam-type methods, which raises the challenge of maintaining unbiased gradients, especially with the adjustment of the second-order moment. Our analysis meticulously handles the moving estimates of both first and second-order moments to guarantee provable convergence. The first-order momentum and second-order momentum are updated locally and then averaged during communication rounds. ", "page_idx": 2}, {"type": "text", "text": "In summary, our paper contributes in three main areas. First, our FGDRO-CVaR algorithm greatly reduces both communication costs and sample complexity for FGDRO with CvaR constraint problems. Second, our FGDRO-KL algorithm achieves a better sample complexity while maintaining the same communication costs as the existing results. Third, our FGDRO-KL-Adam integrates adaptive step sizes with Adam-type updates, which has the potential to surpass the performance of conventional SGD-based approaches. Extensive testing on diverse real-world datasets has shown that our approach achieves superior performance while substantially reducing communication overhead. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Federated learning has gained significant attention due to its potential to train machine learning models using data from various sources while ensuring data privacy [38, 48, 54]. Two central challenges to this field are communication cost and client heterogeneity, which have been extensively explored in the literature [63, 76, 77, 73, 62, 34, 64, 2, 31, 68, 4, 33, 35, 71, 70, 34, 18]. This section will dive into the body of literature that focuses on these specific challenges. ", "page_idx": 2}, {"type": "text", "text": "Non-IID Clients in Federated Learning (FL) One of the key challenges in Federated Learning (FL) is managing client heterogeneity, particularly the issue of non-IID (nonindependently and identically distributed) data across client networks. Efforts to overcome the negative implications of data diversity have led to the development of model personalization techniques [47, 10, 44, 82, 45, 43, 75, 19, 41]. However, these approaches face challenges when dealing with data from unseen or unidentifiable groups. For a comprehensive examination of the challenges and strategies concerning non-IID clients in federated learning, the readers are directed to [27]. ", "page_idx": 2}, {"type": "text", "text": "Federated Group Distributionally Robust Optimization Since Group Distributionally Robust Optimization has shown effectivenss in addressing non-iid data in centralized setting [14, 51, 12, 56], previous research has investigated Federated Group Distributionally Robust Optimization (FGDRO) to address the challenges posed by non-IID clients in federated settings [50, 11]. The DRFA algorithm [11] focuses on a specific instance of (2), applying a CVaR constraint on $\\mathbf{p}$ with $K=1$ . It samples machines based on updated probabilities to allow local updates, reducing the need for communication, with these probabilities managed by a central server. However, this approach results in significant communication costs of ${\\cal O}(1/\\epsilon^{12})$ and sample complexity of ${\\cal O}(1/\\epsilon^{16})$ per machine to achieve an $\\epsilon$ -stationary point. Recent developments in [22] introduced algorithms for handling Group DRO with a CVaR constraint in centralized settings, but adapting these to federated learning entails substantial communication overheads of ${\\cal O}(1/\\epsilon^{6})$ . Moreover, [78] introduced the SCAFF-PD algorithm, which is only applicable in convex scenarios and requires the use of the complete dataset in each training round. Regarding KL regularization, [30] achieved a communication cost of ${\\cal O}(1/\\epsilon^{3})$ , but required the use of large data batches, resulting in a total sample complexity of ${\\cal O}(1/\\epsilon^{6})$ per machine. [80] has studied FGDRO with KL regularization in a decentralized setting, which would incur a communication cost of ${\\cal O}(1/\\epsilon^{4})$ if directly applied to a centralized federated learning setting. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Federated Adaptive Algorithm Stochastic adaptive methods for minimization in non-convex stochastic optimization have garnered significant interest in recent years [13, 36, 49, 69, 42, 84, 65, 7, 46, 25, 16, 81]. These methods, known for assigning unique step sizes to each coordinate, often outperform their non-adaptive counterparts. In federated learning, Reddi et al. [59] have advanced the field with an adaptive algorithm. However, their methodology predominantly applies adaptive steps at the global server level, with local updates still dependent on SGD. This could lead to suboptimal performance. Furthermore, their approach was tailored for Empirical Risk Minimization (ERM) problems and could not be applied for problems considered in this work. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A function $f$ is $C$ -Lipschitz if $f(\\mathbf{x})-f(\\mathbf{y})\\leq C\\|\\mathbf{x}-\\mathbf{y}\\|$ . A differentiable function $f$ is $L$ -smooth if $\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|$ , where $\\nabla f(\\mathbf{x})$ denotes the gradient. For a non-differentiable function $f$ , its subdifferential $\\partial f(\\mathbf{x})$ is defined as a set of all subgradients as $\\partial f(\\mathbf{x})=\\{\\mathbf{v}|f(\\mathbf{y})\\geq$ $f(\\mathbf{x})+\\langle\\mathbf{v},\\mathbf{y}-\\mathbf{x}\\rangle+o(\\|\\mathbf{y}-\\mathbf{x}\\|)\\}$ as $\\textbf{y}\\to\\textbf{x}$ . When the context is clear, we also overload the notation $\\partial f(\\mathbf{x})$ to denote one subgradient from the subdifferential set. We use $\\nabla f(\\mathbf{x};\\mathbf{z})$ or $\\partial f(\\mathbf{x};\\mathbf{z})$ to represent an unbiased estimator of gradient or subgradient with a randomly drawn sample $\\mathbf{z}$ . Additionally, a function $f$ is $\\rho$ -weakly convex if $\\begin{array}{r}{f(\\mathbf x)\\dot{\\geq f}(\\mathbf y)+\\langle\\partial f(\\mathbf y),\\mathbf y-\\mathbf x\\rangle-\\frac{\\rho}{2}\\|\\mathbf y-\\mathbf x\\|^{2}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "For a smooth function $f(\\mathbf{x})$ , x is an $\\epsilon$ -stationary point if $\\|\\nabla f(\\mathbf{x})\\|^{2}\\leq\\epsilon^{2}$ . For non-smooth functions, $\\mathbf{x}$ is an $\\epsilon$ -stationary point if $\\|d i s t(\\mathbf{0},\\partial f(\\mathbf{x}))\\mathbf{\\dot{\\|}^{2}}\\,\\leq\\,\\epsilon^{2}$ , where $\\begin{array}{r}{d i s t(\\mathbf x,S)\\,=\\,\\operatorname*{min}_{\\mathbf x^{\\prime}\\in S}\\|\\mathbf x-\\mathbf x^{\\prime}\\|_{2}}\\end{array}$ measures the distance between a point $\\mathbf{x}$ and a set $S$ . For non-smooth functions, since it is usually difficult or even impossible to find an $\\epsilon$ -stationary point, we instead seek an $\\epsilon$ -near stationary point. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. $\\mathbf{x}$ is an $\\epsilon$ -near stationary point of $f(\\cdot)$ if $\\exists\\mathbf{x}^{\\prime}$ such that $\\|\\mathbf{x}\\mathrm{~-~}\\mathbf{x}^{\\prime}\\|_{2}\\;\\leq\\;\\epsilon$ and $d i s t(\\mathbf{0},\\partial f(\\mathbf{x}^{\\prime}))\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "4 FGDRO-CVaR ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our algorithm designed to tackle Federated Group Distributionally Robust Optimization (FGDRO) with a CVaR constraint. This problem poses substantial challenges due to the complexity of both CVaR and simplex constraints. Typically, during local updates, individual machines do not have access to adequate information to appropriately adjust the weight vector p. Prior approaches, such as the one proposed by [11], mitigate this issue by updating $\\mathbf{p}$ during global communication rounds, but results in slower convergence rates. To this end, we reformulate the problem into an equivalent two-level compositional optimization problem without constraints: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\operatorname*{min}_{s}F(\\mathbf{w},s):=\\frac{1}{N}\\sum_{i=1}^{N}f\\bigl(g_{i}(\\mathbf{w})-s\\bigr)+\\frac{K}{N}s.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f(\\cdot)=(\\cdot)_{+}$ and $g_{i}(\\mathbf{w})=\\mathbb{E}_{\\mathbf{z}\\sim\\mathcal{D}_{i}}\\ell(\\mathbf{w},\\mathbf{z})$ is a local loss and $s$ is intended to serve as a threshold value. With $\\begin{array}{r}{s=\\arg\\operatorname*{min}_{s^{\\prime}}\\frac{1}{N}\\sum_{i=1}^{N}f\\big(g_{i}(\\mathbf{w})-s^{\\prime}\\big)+\\frac{K}{N}s^{\\prime}}\\end{array}$ , only the $K$ clients with the highest losses will have losses greater than $s$ [53, 83]. During the training phase, $K$ clients with the highest losses are expected to predominantly influence the optimization process. ", "page_idx": 3}, {"type": "text", "text": "The formulation (2) replaces the constrained high-dimensional vector $\\mathbf{p}$ with a single unconstrained scalar variable $s$ . However, this adjustment introduces new challenges due to the compositional structure and the non-smooth nature of the outer function $f$ . As a result, it is biased to estimate subgradient $\\partial f(g_{i}(\\mathbf{w})-s)\\nabla g_{i}(\\mathbf{w})$ using a batch of samples. To address this, it is common practice to create an accurate estimate of $g_{i}(\\mathbf{w})$ [67, 24, 22, 66, 32, 55, 57]. Specifically, we employ a moving average u as our accurate estimate: ", "page_idx": 4}, {"type": "equation", "text": "$$\nu_{i,t}^{r}=(1-\\beta_{1})u_{i,t-1}^{r}+\\beta_{1}\\ell(\\mathbf{w};\\mathbf{z}_{i,t}^{r}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "And then the estimators for sub-gradient of w and $s$ , namely $\\mathbf{m},v$ are computed using $u$ . It is notable that for $s$ , it is updated locally using local data and averaged between clients in communication rounds. It will converge alongside w to an $\\epsilon$ -near stationaty point. This is a fundamental reason why our method achieves a lower communication complexity compared to [11], as the latter can only update the weight variables at the server node in the communication rounds. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 FGDRO-CVaR ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Initialization: $\\bar{\\bf w}^{1}$ , $\\bar{s}^{1}=0$ , $u_{i,t}^{0}=0$ ,   \n2: for $r=1,...,R$ do   \n3: $\\mathbf{w}_{i,0}^{r}=\\bar{\\mathbf{w}}^{r}$ , $s_{i,0}^{r}=\\bar{s}^{r}$ , $u_{i,0}^{r}=u_{0,I}^{r-1}$   \n4: for $t=1,...,I\\;{\\bf d o}$   \n5: Each machine samples data $\\mathbf{z}_{i,t}^{r}$   \n6: $u_{i,t}^{r}=(1-\\beta_{1})u_{i,t-1}^{r}+\\beta_{1}\\ell(\\dot{\\mathbf{w}}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})$   \n7: $\\begin{array}{r}{v_{i,t}^{r}=-\\partial f(u_{i,t}^{r}-s_{i,t-1}^{r})+\\frac{K}{N}}\\end{array}$ , and $s_{i,t}^{r}=s_{i,t-1}^{r}-\\eta_{2}v_{i,t}^{r}$   \n8: $\\mathbf{m}_{i,t}^{r}=\\partial f(u_{i,t}^{r}-s_{i,t-1}^{r})\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r},\\mathbf{z}_{i,t}^{r}),$ , and $\\mathbf{w}_{i,t}^{r}=\\mathbf{w}_{i,t-1}^{r}-\\eta_{1}\\mathbf{m}_{i,t}^{r}$   \n9: end for   \n10: $\\bar{\\mathbf{w}}^{r+1}=\\textstyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{w}_{i,I}^{r},\\,\\bar{s}^{r+1}=\\textstyle\\frac{1}{N}\\sum_{i=1}^{N}s_{i,I}^{r}$   \n11: end for   \n12: Output: $\\begin{array}{r}{\\tilde{\\mathbf{w}}=\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\mathbf{w}_{i,t^{\\prime}}^{r^{\\prime}}}\\end{array}$ , where $r^{\\prime}$ and $t^{\\prime}$ are sampled from $[1,R]$ and $[1,I]$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "We present the formalization of our FGDRO-CVaR method in Algorithm 1. Next, we show the convergence results of FGDRO-CVaR . We make the following assumptions regarding problem (2). ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1. (1) $\\forall i$ and $\\forall\\mathbf{z}\\in D_{i}$ , $\\ell(\\cdot,\\mathbf{z})$ is $C_{g}$ -Lipschitz and $L_{g}$ -smooth. (2) $\\mathbb{E}_{\\mathbf{z}\\in\\mathcal{D}_{i}}\\|\\nabla\\ell(\\mathbf{w};\\mathbf{z})-$ $\\nabla g_{i}(\\mathbf{w})\\|^{2}\\leq\\sigma^{2}$ , $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{z}\\in\\mathcal{D}_{i}}\\|\\ell(\\mathbf{w};\\mathbf{z})-g_{i}(\\mathbf{w})\\|^{2}\\leq\\sigma^{2}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark. The first assumption about Lipschitz continuity and smoothness of $g_{i}$ is standard in compositional optimization [22, 66, 67]. The second assumption of bounded variance is also common. Assumption 4.1 leads to $F(\\mathbf{w},s)$ being weakly convex, which is a key step in the analysis as shown in Appendix A.2 ", "page_idx": 4}, {"type": "text", "text": "The behavior of the estimator $u$ is examined through the following lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.2. Under Assumption 4.1, by setting $\\eta\\,=\\,O\\,\\bigl(1/(R)^{3/2}\\bigr),\\;\\beta_{1}\\,=\\,O\\,(1/R),\\;I\\,=\\,O(R),$ , Algorithm $^{\\,l}$ ensures that ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{i,t}-g_{i}(\\bar{\\bf w}_{t}^{r})\\|^{2}\\le(1-\\beta_{1})\\mathbb{E}\\|u_{i,t-1}^{r}-g_{i}(\\bar{\\bf w}_{t-1}^{r})\\|^{2}+2\\beta_{1}^{2}\\sigma^{2}+3\\beta_{2}\\eta^{2}I^{2}C_{g}^{2}+\\frac{5}{\\beta_{1}}C_{g}^{2}\\|\\bar{\\bf w}_{t}^{r}-\\bar{\\bf w}_{t-1}^{r}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The convergence result of FGDRO-CVaR is given in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.3. Under Assumption 4.1, by setting $\\eta=O\\left(1/(R)^{3/2}\\right)$ , $\\beta_{1}=O\\left(1/R\\right)$ , $I=O(R)$ and $\\hat{\\rho}=2L_{g}$ , the Algorithm 1 ensures that for the output $\\left(\\tilde{\\mathbf{w}},\\tilde{s}\\right)$ , there exists $(\\mathbf{w}^{\\prime},s^{\\prime})$ that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|d i s t(\\mathbf{0},F(\\mathbf{w}^{\\prime},s^{\\prime}))\\|^{2}\\leq1/\\hat{\\rho}(\\|\\tilde{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|_{2}^{2}+\\|\\tilde{s}-s^{\\prime}\\|_{2}^{2})\\leq O\\left(\\frac{1}{R^{1/2}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark. The analysis has utilized Moreau envelop to address the nonsmooth issue [52, 9]. To achieve an $\\epsilon$ -near stationary point of $F(\\cdot)$ , we need to set $R={\\cal O}(1/\\epsilon^{4})$ and $I=O(1/\\epsilon^{4})$ , and thus the sample complexity on each machine is $R I=O(1/\\epsilon^{8})$ . The total sample complexity of $O(n/\\epsilon^{6})$ ", "page_idx": 4}, {"type": "text", "text": "by [22] is achieved by the STORM estimator [8] which incurs additional memory and computational costs due to the requirement of computing gradients using two models at each iteration. Without the STORM estimator, [22] would exhibit a total sample complexity of $O(n/\\epsilon^{8})$ . When deployed in a federated setting, the complexity for each machine would be $O(1/\\epsilon^{8})$ , aligning with our results and demonstrating that our approach achieves a linear speed-up in terms of number of machines. Additionally, although we aggregate certain scalar variables (in FGDRO-CVaR, the scalar variable s; in FGDRO-KL and FGDRO-KL-Adam to be presented later, the scalar variable v), similar to the technique in the Remark 3.1 of [61], we can aggregate these variables using homomorphic encryption, ensuring that their exact values remain confidential. ", "page_idx": 5}, {"type": "text", "text": "5 FGDRO-KL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present our FGDRO-KL algorithm for solving problem (1) with a KL regularization. Unlike the CVaR constraints that focus on the top K clients, KL regularization takes into account all clients, assigning them varying weights. Additionally, FGDRO with KL regularization is smooth, and strongly concave with respect to $\\mathbf{p}$ . Nevertheless, it is subject to the simplex constraint on p. To address this, we use an equivalent form derived from the KKT conditions, as referenced in [56, 30]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}F(\\mathbf{w})=\\lambda\\log(\\frac{1}{N}\\sum_{i=1}^{N}\\exp(\\mathbb{E}_{\\mathbf{z}\\sim\\mathcal{D}_{i}}\\ell(\\mathbf{w};\\mathbf{z})/\\lambda)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This formulation eliminates the constrained vector $\\mathbf{p}$ , and $F(\\mathbf{w})$ is smooth since KL regularization is strongly concave [6]. However, this formulation has a three-level composition structure and thus, using a batch of data in a three-level composition can result in biased gradient estimation. Furthermore, the gradients on one machine are depend on other machines. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we denote $g_{i}(\\mathbf{w})=\\exp(\\mathbb{E}_{\\mathbf{z}\\in\\mathcal{D}_{i}}\\ell(\\mathbf{w};\\mathbf{z})/\\lambda)$ and $\\begin{array}{r}{g(\\mathbf{w})=\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}g_{i}(\\mathbf{w})}\\end{array}$ , with $\\ell(\\mathbf{w};\\mathcal{D}_{i})=$ $\\mathbb{E}_{\\mathbf{z}\\in\\mathcal{D}_{i}}\\ell(\\mathbf{w};\\mathbf{z})$ , then, the gradient of $F(\\mathbf{w})$ in (5) is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla F(\\mathbf{w})=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{g_{i}(\\mathbf{w})}{g(\\mathbf{w})}\\nabla\\ell(\\mathbf{w};D_{i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is crucial to recognize that the gradient for machine $i$ , i.e., $\\nabla\\ell(\\mathbf{w};\\mathcal{D}_{i})$ , is scaled by $g_{i}(\\mathbf{w})/g(\\mathbf{w})$ . This scaling indicates that machines experiencing larger loss functions exert more influence over the training process. To mitigate the biased gradient estimation, we approximate $g_{i}(\\mathbf{w})$ and $g(\\mathbf{w})$ based on moving average estimators $u$ and $v$ . On each machine, $u$ serves as a moving average estimator for the local loss function $\\ell(\\mathbf{w};\\mathcal{D}_{i})$ , with $\\exp(u_{i,t}^{r}/\\lambda)$ providing a local approximation of $g_{i}(\\mathbf{w})$ . $u$ is updated and maintained locally without need for averaging during communication rounds. $v$ estimates the global statistic $g(\\mathbf{w})$ , and is updated locally but averaged during global communication rounds. Subsequently, a moving average estimator of the gradient, denoted as $\\mathbf{m}$ is constructed using $u$ and $v$ . For specific update rules, please refer to Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "For analysis, we make the following assumptions regarding problem (1) with a KL regularization: ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.1. $(1)\\,\\forall i$ and $\\forall\\mathbf{z}\\in D_{i}$ , $g_{i}(\\cdot)$ is $C_{g}$ -Lipschitz and $L_{g}$ -smooth. (2) $\\mathbb{E}_{\\mathbf{z}\\in\\mathcal{D}_{i}}\\|\\nabla\\ell(\\mathbf{w};\\mathbf{z})-$ $\\nabla\\ell(\\mathbf{w};D_{i})\\|^{2}\\leq\\sigma^{2}$ , $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{z}\\in\\mathcal{D}_{i}}\\|\\ell(\\mathbf{w};\\mathbf{z})-\\ell(\\mathbf{w};\\mathcal{D}_{i})\\|^{2}\\leq\\sigma^{2}}\\end{array}$ . (3) $f$ is $C_{f}$ -Lipschitz and $L_{f}$ -smooth. (4) $\\forall i$ and $\\forall\\mathbf{z}\\in D_{i}$ , $0\\leq\\ell(\\cdot;\\mathbf{z})\\leq C_{0},\\ell(\\cdot)$ is $C_{\\ell}$ -Lipschitz and $L_{\\ell}$ -smooth. ", "page_idx": 5}, {"type": "text", "text": "The behavior of the $u$ and $v$ estimators can be bounded similar to the previous section and are shown in Appendix B. The estimator $\\mathbf{m}$ for gradient can be bounded as ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.2. Under Assumption 5.1, with proper constants $C_{1}$ and $G$ , by setting $\\begin{array}{r}{\\eta=O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , $\\begin{array}{r}{\\beta_{1}=O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , the Algorithm 2 ensures that \u2225 $\\begin{array}{l}{{\\displaystyle{\\bar{\\bf m}}_{t}^{r}-\\nabla F(\\bar{\\bf w}_{t}^{r})\\|^{2}\\leq(1-\\frac{\\beta_{3}}{2})\\|\\bar{\\bf m}_{t-1}^{r}-\\nabla F(\\bar{\\bf w}_{t-1}^{r})\\|^{2}+\\beta_{3}C_{\\ell}^{2}C_{1}^{2}\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\|u_{i,t-1}^{r}-\\ell(\\bar{\\bf w}_{t-1}^{r};\\mathcal{D}_{i})\\|^{2}}}\\\\ {~~}\\\\ {{\\displaystyle+\\;\\beta_{3}C\\|\\bar{v}_{t}^{r}-g(\\bar{\\bf w}_{t}^{r})\\|^{2}+3\\eta\\|\\nabla F(\\bar{\\bf w}_{t-1}^{r})\\|^{2}+\\beta_{3}^{2}C_{1}^{2}\\frac{\\sigma^{2}}{N}+2\\beta_{3}C_{1}^{2}L_{\\ell}^{2}\\eta^{2}I^{2}G^{2}}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 FGDRO-KL ", "page_idx": 6}, {"type": "text", "text": "1: Initialization: $\\bar{\\bf w}^{1}$ , $u_{i,I}^{0},\\bar{v}^{1},\\bar{\\mathbf{m}}^{1}$   \n2: for $r=1,...,R$ do   \n3: $\\mathbf{w}_{i,0}^{r}=\\bar{\\mathbf{w}}^{r}$ , $\\mathbf{m}_{i,0}^{r}=\\bar{\\mathbf{m}}^{r}$ , $u_{i,0}^{r}=u_{i,I}^{r-1},v_{i,0}^{r}=\\bar{v}^{r}$   \n4: for $t=1,...,I$ do   \n5: Each machine samples data $\\mathbf{z}_{i,t}^{r}$   \n6: $u_{i,t}^{r}=(1-\\beta_{1})u_{i,t-1}^{r}+\\beta_{1}\\ell(\\dot{\\mathbf{w}}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})$ , and $v_{i,t}^{r}=(1-\\beta_{2})v_{i,t-1}^{r}+\\beta_{2}\\exp(u_{i,t}^{r}/\\lambda)$   \n7: hir,t expv(ruir,t)\u2207\u2113(wir,t\u22121; zir,t), and mir,t =(1\u2212\u03b23)mir,t\u22121 + \u03b23hir,t   \n8: $\\mathbf{w}_{i,t}^{r}=\\mathbf{w}_{i,t-1}^{r}-\\eta\\mathbf{m}_{i,t}^{r}$   \n9: end for   \n10 ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{w}}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{w}_{i,I}^{r},\\bar{v}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}v_{i,I}^{r},\\mathrm{and}\\,\\bar{\\mathbf{m}}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{m}_{i,I}^{r}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "12: Output: w\u02dc = N1 $\\begin{array}{r}{\\tilde{\\mathbf{w}}=\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\mathbf{w}_{i,t^{\\prime}}^{r^{\\prime}}}\\end{array}$ , where $r^{\\prime}$ and $t^{\\prime}$ are sampled from $[1,R]$ and $[1,I]$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 3 FGDRO-KL-Adam ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Initialization: $\\bar{\\bf w}^{1}$ , $u_{i,I}^{0},\\bar{v}^{1},\\bar{\\mathbf{m}}^{1},\\bar{\\mathbf{q}}^{1}$   \n2: for $r=1,...,R$ do   \n3: $\\mathbf{w}_{i,0}^{r}=\\bar{\\mathbf{w}}^{r}$ , $\\mathbf{m}_{i,0}^{r}=\\bar{\\mathbf{m}}^{r}$ , $\\mathbf{q}_{i,0}^{r}=\\bar{\\mathbf{q}}^{r}$ , $u_{i,0}^{r}=u_{i,I}^{r-1}$ , and $v_{i,0}^{r}=\\bar{v}^{r}$   \n4: for $t=1,...,I$ do   \n5: Each machine samples data $\\mathbf{z}_{i,t}^{r}$   \n6: $u_{i,t}^{r}=(1-\\beta_{1})u_{i,t-1}^{r}+\\beta_{1}\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})$ , and $v_{i,t}^{r}=(1-\\beta_{2})v_{i,t-1}^{r}+\\beta_{2}\\exp(u_{i,t}^{r}/\\lambda)$   \n7: $\\begin{array}{r}{\\mathbf{h}_{i,t}^{r}=\\frac{\\exp(u_{i,t}^{r})}{v_{i,t}^{r}}\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})}\\end{array}$   \n8: $\\mathbf{m}_{i,t}^{r}\\!=\\!(1\\!-\\!\\beta_{3})\\mathbf{m}_{i,t-1}^{r}+\\beta_{3}\\mathbf{h}_{i,t}^{r}$ , and $\\mathbf{q}_{i,t}^{r}=(1\\!-\\!\\beta_{4})\\mathbf{q}_{i,t-1}^{r}+\\beta_{4}(\\mathbf{h}_{i,t}^{r})^{2}$   \n9: $\\begin{array}{r}{\\mathbf{w}_{i,t}^{r}=\\mathbf{w}_{i,t-1}^{r}-\\eta\\frac{\\mathbf{m}_{i,t}^{r}}{\\sqrt{\\mathbf{q}_{i,t}^{r}}+\\tau}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "10: end for ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{w}}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{w}_{i,I}^{r},\\bar{v}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}v_{i,I}^{r},\\bar{\\mathbf{m}}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{m}_{i,I}^{r}\\;\\mathrm{and}\\;\\mathbf{q}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{q}_{i,I}^{r}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "12: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "13: Output: $\\begin{array}{r}{\\tilde{\\mathbf{w}}=\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\mathbf{w}_{i,t^{\\prime}}^{r^{\\prime}}}\\end{array}$ , where $r^{\\prime}$ and $t^{\\prime}$ are sampled from $[1,R]$ and $[1,I]$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "The precisions of the $u,v,\\mathbf{m}$ estimators depend on each other. The idea is to get $\\mathbb{E}\\|u_{i,t}^{r}-\\ell(\\bar{\\mathbf{w}}_{t}^{r};\\mathcal{D}_{i})\\|^{2}$ , $\\mathbb{E}\\|\\bar{v}_{t}^{r}-g(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}$ , ${\\left\\|{\\bar{\\bf{m}}_{t}^{r}-\\nabla F({\\bar{\\bf{w}}_{t}^{r}})}\\right\\|^{2}}$ and $\\mathbb{E}\\|\\nabla F(\\tilde{\\mathbf{w}})\\|^{2}$ jointly converge, and then we finally have the following theorem to guarantee the convergence: ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3. Under Assumption 5.1, by setting $\\begin{array}{r}{\\eta\\,=\\,O\\left(\\frac{1}{\\sqrt{R I}}\\right)\\mathrm{,~}\\,\\beta_{1}\\,=\\,O\\left(\\frac{1}{\\sqrt{R I}}\\right)\\mathrm{,~}I\\,=\\,R^{1/3},}\\end{array}$ , Algorithm 2 ensures that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\nabla F(\\tilde{\\mathbf{w}})\\|^{2}\\leq O\\left(\\frac{1}{R^{2/3}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark. To achieve an $\\epsilon$ -stationary point, i.e., $\\lVert\\nabla F(\\tilde{\\mathbf{w}})\\rVert^{2}\\leq\\epsilon^{2}$ , we need to set $R=O(1/\\epsilon^{3})$ , $I=$ ${\\cal O}(1/\\epsilon)$ , $\\eta=O(\\epsilon^{2})$ and $\\beta_{1}=O(\\epsilon^{\\bar{2}})$ . Compared to [30], our approach maintains a communication complexity of $\\mathcal{O}(1/\\epsilon^{3})$ , but significantly reduces the sample complexity on each machine from ${\\cal O}(1/\\epsilon^{6})$ from ${\\cal O}(1/\\epsilon^{4})$ , requiring only a batch size of $O(1)$ rather than a large batch size of ${\\cal O}(1/\\epsilon^{2})$ . Our results match the communication and sample complexity in [17], which tackles a simpler twolevel compositional problem and achieved sample complexity of ${\\cal O}(1/\\epsilon^{4})$ per machine. Considering that the sample complexity for a two-level compositional problem in a centralized setting would be $O(n/\\epsilon^{4})$ [66], our approach realizes a linear speed-up proportional to the number of machines. ", "page_idx": 6}, {"type": "text", "text": "6 FGDRO-KL-Adam ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we introduce an adaptive algorithm, FGDRO-KL-Adam, to address the problem (5) with KL regularization, as detailed in Algorithm 3. This algorithm incorporates Adam-type updates at local steps, which have been shown to outperform SGD in centralized settings. While previous studies [59] in federated settings have implemented Adam-type updates at the global step but retained SGD for local updates, which may be sub-optimal. ", "page_idx": 7}, {"type": "text", "text": "Similar to Algorithm 2, $u$ and $v$ are used to estimate the local loss and the global function $g(\\mathbf{w})$ , respectively. The variables $\\mathbf{h}$ and $\\mathbf{m}$ are updated in a manner consistent with Algorithm 2. Under Assumption 5.1, the behavior of the $u,v,\\mathbf{m}$ estimators is addressed as previously discussed. ", "page_idx": 7}, {"type": "text", "text": "The primary distinction in Algorithm 3 lies in its adaptive updates for the local model $\\mathbf{w}_{i,t}^{r}$ . Here, m serves a role akin to the first-order momentum in Adam, and we introduce $\\mathbf{q}_{i,t}^{r}$ to estimate the second-order momentum: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{q}_{i,t}^{r}=(1-\\beta_{4})\\mathbf{m}_{i,t-1}^{r}+\\beta_{4}\\mathbf{h}_{i,t}^{r}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Subsequently, the local models are updated adaptively using the formula: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{w}_{i,t}^{r}=\\mathbf{w}_{i,t-1}^{r}-\\eta\\frac{\\mathbf{m}_{i,t}^{r}}{\\sqrt{\\mathbf{q}_{i,t}^{r}}+\\tau},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where both the square root and division are performed element-wise. ", "page_idx": 7}, {"type": "text", "text": "A key step in the analysis is to address the coordinate-wise update, as in the following lemma. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6.1. Using $L$ -smooth of $F$ , for some proper constants $C$ and $G_{i}$ , by setting $\\begin{array}{r}{\\eta=O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , $\\begin{array}{r}{\\beta_{1}=O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\nF(\\bar{\\mathbf{w}}_{t}^{r})\\leq F(\\bar{\\mathbf{w}}_{t-1}^{r})+\\frac{\\eta}{\\tau}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})-\\bar{\\mathbf{m}}_{t-1}^{r}\\|^{2}+\\frac{\\eta}{\\tau}\\beta_{3}^{2}C-\\frac{\\eta}{2(G+\\tau)}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Finally, we show that FGDRO-KL-Adam has same convergence rate as FGDRO-KL. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6.2. Under Assumption 5.1, by setting of $\\begin{array}{r}{\\eta=O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , $\\begin{array}{r}{\\beta_{1}=O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , and $I=R^{1/3}$ , Algorithm 3 achieves: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\nabla F(\\tilde{\\mathbf{w}})\\|^{2}]\\leq O\\left(\\frac{1}{R^{2/3}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark. To achieve an $\\epsilon$ -stationary point, i.e., $\\|\\nabla F(\\tilde{\\mathbf{w}})\\|^{2}\\leq\\epsilon^{2}$ , we just need to set $R={\\cal O}(1/\\epsilon^{3})$ , $I=O(1/\\epsilon)$ , $\\eta=O(\\epsilon^{2})$ and $\\beta_{1}=\\Bar{O(\\epsilon^{2})}$ . The communication and sample complexities are the same as in Theorem 5.3. Our analysis, following the framework in [16], requires $\\bar{\\sqrt{\\mathbf{q}_{i,t}^{r}}}+\\tau$ to be both upper and lower bounded. It is achieved by the upper bound assumption and choice of $\\tau$ , ensuring $\\bar{\\tau}\\leq\\sqrt{\\mathbf{q}_{i,t}^{r}}+\\bar{\\tau}\\leq G+\\bar{\\tau}$ , which is utilized similarly in [59]. However, Guo et al. [16] did not cover the federated learning scenario or the compositional problems. It is important to note that we have not developed an Adam-type variant for FGDRO-CVaR. This is due to the need for accurate gradient estimation in the analysis of Adam-type updates, which is achieved using the moving estimator $\\mathbf{m}$ . But in CVaR variant, the nonsmooth nature renders a moving average for subgradient $\\partial F(\\mathbf{w})$ not provably accurate. ", "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and Neural Networks We use Pile [15], CivilComments [5], Camelyon17 [1], iWildCam2020 [3], and Poverty [74]. For Pile, we preprocess it as [72], for the others, we use the preprocessed version by [37]. We utilized natural data splits where data from the same hospital, web source, location, or demographic group were placed on the same client machine. These experiments have involved with highly imbalanced number of data on clients. Data statistics are presented in the Appendix E. Additiona experiments on Cifar 10 with Dirichlet distributions over 100 clients are reported in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "The Pile data set is a large language data set. We use the uncopyrighted version [29] which has 17 domains, and each domain is allocated to one machines. We use the GPT2 model [58] as implemented by [28] with 12 hidden layers, 12 attention heads, and 768 embeddings and hidden states. We measure the performance using worst log-perplexity and average log-perplexity of testing groups. CivilComments is a toxicity classification of the online comment task in diversified demographic identities. We train on four groups based on the presence of \u2019Black\u2019 and toxicity labels, deploying each on a separate machine, and use the DistilBERT base-uncased model [60] to predict toxicity. We measure the performance using worst group accuracy and average accuracy of testing groups. Camelyon17 focuses on tumor detection from lymph node images [1], with data from five hospitals split into training (3), validation (1), and testing (1) sets. Training uses three machines, each processing data from one hospital, using DenseNet-121 [26]. The iWildCam2020 dataset consists of wildlife images from various camera traps [3], the dataset is split into training, validation, and testing segments. We use ResNet50 [21] across all datasets and measure performance via Macro F1 score. The Poverty dataset contains geographic data aimed at predicting regional poverty levels [74]. We use ResNet50 [21] for our models and evaluate performance using both the Pearson correlation on the worst-performing region and the average across regions. ", "page_idx": 8}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/85fb42f6e787217f4a7bdc8f2bf538f5ee543141641807f0c37308eea3ab0bb4.jpg", "table_caption": ["Table 2: Experiments on Natural Language Task. PPL is abbrevation of perplexity "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/ff95d4e683704f36f8ad12d96e3b83eb8c4a9f7e8a209d50ff31d16c35bc2ae4.jpg", "table_caption": ["Table 3: Experiments on Image Classification Task "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Baselines We compare our algorithms FGDRO-CVaR, FGDRO-KL, and FGDRO-KL-Adam with four baselines: FedAvg [48], SCAFFOLD [34], FedProx[40], FedAdam [59], DRFA[11], and DRDSGD [30]. ", "page_idx": 8}, {"type": "text", "text": "We tune the initial step size in [1e-4, 1e-3, 1e-2, 1e-1]. All algorithms set the communication interval $I=32$ unless otherwise specified. The local mini-batch sizes are set to 32. Experiments are run for 20K local iterations except for Pile, which runs for 200K iterations. The $\\beta$ parameters of FGDRO-KL and FGDRO-KL-Adam are tuned in [0.01, 0.1, 0.2, 0.5]. For each algorithm, we repeat the experiments 3 times with different random seeds and report the averaged performance. Following [72], our FGDRO algorithms for Pile initially train for 20K iterations to obtain domain weights, which are then fixed during subsequent training phases. ", "page_idx": 8}, {"type": "text", "text": "Results We report the experimental results for natural language processing in Table 2 and those for computer vision in Table 3. We can see that our methods outperform the baselines in most tasks. Our approaches improve worst-case performance without hurting average case performance. Furthermore, FGDRO-KL-Adam has demonstrated superior performance compared to FGDRO-KL in most cases. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Ablation Studies Here we present some ablation study to examine some aspects of our algorithm design. First, in Figure 1(a), we vary the communication interval $I$ in experiments on the Camelyon dataset. We can see that both our FGDRO-CVaR and FGDRO-KL-Adam algorithms can tolerate skipping a large number of communications without degrading the performance. ", "page_idx": 9}, {"type": "text", "text": "To demonstrate the effect of the local adaptive updates. We develop a LocalAdam algorithm (see Appendix D), which optimizes ERM using our design of using Adam steps in local updates. The results are plotted in Figure 1(b). We can see that the LocalAdam algorithm outperforms FedAdam, which uses SGD in local steps and only uses adaptive steps in global communication rounds. ", "page_idx": 9}, {"type": "image", "img_path": "xNZEjFe0mh/tmp/101d325286d474afe14f10fa0aca5a8e073101913c3d744e69c9c432522bccc9.jpg", "img_caption": ["Figure 1: Ablation Experiments "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our algorithm provides a significant advantage in addressing federated group distributionally robust optimization while maintaining low communication and computational complexity. Furthermore, incorporating local adaptive steps has the potential to accelerate the training process beyond the capabilities of traditional approaches that employ SGD in local steps. Various experiments on natural lanugage processing and computer vision have confirmed our theoretical results and underscored the effectiveness of our algorithms. It remains to develop a provable adaptive algorithm for FGDROCVaR, which is currently absent due to the non-smoothness and compositional problem structure. ", "page_idx": 9}, {"type": "text", "text": "9 Impact Statements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper is meant to advance the field of federated machine learning. We do not see noticeable negative impact. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We appreciate the feedback provided by the anonymous reviewers. This work was partially supported by the National Science Foundation Career Award 2246753, the National Science Foundation Award 2246757, 2246756 and 2306572. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] P. Bandi, O. Geessink, Q. Manson, M. Van Dijk, M. Balkenhol, M. Hermsen, B. E. Bejnordi, B. Lee, K. Paeng, A. Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE transactions on medical imaging, 38(2):550\u2013560, 2018. ", "page_idx": 9}, {"type": "text", "text": "[2] D. Basu, D. Data, C. Karakus, and S. Diggavi. Qsparse-local-sgd: Distributed sgd with quantization, sparsification and local computations. Advances in Neural Information Processing Systems, 32, 2019. ", "page_idx": 9}, {"type": "text", "text": "[3] S. Beery, A. Agarwal, E. Cole, and V. Birodkar. The iwildcam 2021 competition dataset. arXiv preprint arXiv:2105.03494, 2021.   \n[4] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pages 560\u2013569. PMLR, 2018.   \n[5] D. Borkan, L. Dixon, J. Sorensen, N. Thain, and L. Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion proceedings of the 2019 world wide web conference, pages 491\u2013500, 2019.   \n[6] S. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[7] J. Chen, D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI), pages 3267\u20133275, 2020. [8] A. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex sgd. Advances in neural information processing systems, 32, 2019. [9] D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207\u2013239, 2019.   \n[10] Y. Deng, M. M. Kamani, and M. Mahdavi. Adaptive personalized federated learning. arXiv preprint arXiv:2003.13461, 2020.   \n[11] Y. Deng, M. M. Kamani, and M. Mahdavi. Distributionally robust federated averaging. Advances in neural information processing systems, 33:15111\u201315122, 2020.   \n[12] J. Duchi and H. Namkoong. Variance-based regularization with convex objectives. Journal of Machine Learning Research, 20(68):1\u201355, 2019.   \n[13] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.   \n[14] Y. Fan, S. Lyu, Y. Ying, and B. Hu. Learning with average top-k loss. Advances in neural information processing systems, 30, 2017.   \n[15] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[16] Z. Guo, Y. Xu, W. Yin, R. Jin, and T. Yang. A novel convergence analysis for algorithms of the adam family and beyond. arXiv preprint arXiv:2104.14840, 2021.   \n[17] Z. Guo, R. Jin, J. Luo, and T. Yang. Fedxl: provable federated learning for deep x-risk optimization. In International Conference on Machine Learning, pages 11934\u201311966. PMLR, 2023.   \n[18] F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. Cadambe. Local sgd with periodic averaging: Tighter analysis and adaptive synchronization. Advances in Neural Information Processing Systems, 32, 2019.   \n[19] F. Hanzely, S. Hanzely, S. Horv\u00e1th, and P. Richt\u00e1rik. Lower bounds and optimal algorithms for personalized federated learning. Advances in Neural Information Processing Systems, 33: 2304\u20132315, 2020.   \n[20] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and D. Ramage. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.   \n[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016.   \n[22] Q. Hu, D. Zhu, and T. Yang. Non-smooth weakly-convex finite-sum coupled compositional optimization. arXiv preprint arXiv:2310.03234, 2023.   \n[23] X. Hu, S. Li, and Y. Liu. Generalization bounds for federated learning: Fast rates, unparticipating clients and unbounded losses. In The Eleventh International Conference on Learning Representations, 2023.   \n[24] Y. Hu, S. Zhang, X. Chen, and N. He. Biased stochastic gradient descent for conditional stochastic optimization. arXiv preprint arXiv:2002.10790, 2020.   \n[25] F. Huang, J. Li, and H. Huang. Super-adam: Faster and universal framework of adaptive gradients. arXiv preprint arXiv:2106.08208, 2021.   \n[26] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[27] W. Huang, M. Ye, Z. Shi, G. Wan, H. Li, B. Du, and Q. Yang. Federated learning for generalization, robustness, fairness: A survey and benchmark. arXiv preprint arXiv:2311.06750, 2023.   \n[28] Huggingface. Open ai gpt2 by huggingface, howpublished $=$ https://huggingface.co/ docs/transformers/model_doc/gpt2\u201e .   \n[29] Huggingface. pile-uncopyrighted, howpublished $=$ https://huggingface.co/datasets/ monology/pile-uncopyrighted\u201e .   \n[30] C. B. Issaid, A. Elgabli, and M. Bennis. Dr-dsgd: A distributionally robust decentralized learning algorithm over graphs. arXiv preprint arXiv:2208.13810, 2022.   \n[31] P. Jiang and G. Agrawal. A linear speedup analysis of distributed deep learning with sparse and quantized communication. Advances in Neural Information Processing Systems, 31, 2018.   \n[32] W. Jiang, G. Li, Y. Wang, L. Zhang, and T. Yang. Multi-block-single-probe variance reduced estimator for coupled compositional optimization. In NeurIPS, 2022.   \n[33] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[34] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132\u20135143. PMLR, 2020.   \n[35] A. Khaled, K. Mishchenko, and P. Richt\u00e1rik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 4519\u20134529. PMLR, 2020.   \n[36] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.   \n[37] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021.   \n[38] J. Konec\u02c7ny\\`, H. B. McMahan, D. Ramage, and P. Richt\u00e1rik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.   \n[39] G. Lan and Z. Zhang. Optimal methods for convex risk-averse distributed optimization. SIAM Journal on Optimization, 33(3):1518\u20131557, 2023.   \n[40] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[41] T. Li, S. Hu, A. Beirami, and V. Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pages 6357\u20136368. PMLR, 2021.   \n[42] X. Li and F. Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In The 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), pages 983\u2013992, 2019.   \n[43] Z. Li, H. Zhao, B. Li, and Y. Chi. Soteriaf:l A unified framework for private federated learning with communication compression. Advances in Neural Information Processing Systems, 35: 4285\u20134300, 2022.   \n[44] P. P. Liang, T. Liu, L. Ziyin, N. B. Allen, R. P. Auerbach, D. Brent, R. Salakhutdinov, and L.-P. Morency. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020.   \n[45] G. Long, M. Xie, T. Shen, T. Zhou, X. Wang, and J. Jiang. Multi-center federated learning: clients clustering for better personalization. World Wide Web, 26(1):481\u2013500, 2023.   \n[46] L. Luo, Y. Xiong, Y. Liu, and X. Sun. Adaptive gradient methods with dynamic bound of learning rate. In 7th International Conference on Learning Representations (ICLR), 2019.   \n[47] Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.   \n[48] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[49] H. B. McMahan and A. Blum. Online geometric optimization in the bandit setting against an adaptive adversary. In Proceedings of the 17th Annual Conference on Learning Theory (COLT), pages 109\u2013123, 2004.   \n[50] M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. In International Conference on Machine Learning, pages 4615\u20134625. PMLR, 2019.   \n[51] H. Namkoong and J. C. Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. Advances in neural information processing systems, 29, 2016.   \n[52] Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical programming, 140(1):125\u2013161, 2013.   \n[53] W. Ogryczak and A. Tamir. Minimizing the sum of the k largest functions in linear time. Information Processing Letters, 85(3):117\u2013122, 2003.   \n[54] S. Pati, U. Baid, B. Edwards, M. Sheller, S.-H. Wang, G. A. Reina, P. Foley, A. Gruzdev, D. Karkada, C. Davatzikos, et al. Federated learning enables big data for rare cancer boundary detection. Nature communications, 13(1):7346, 2022.   \n[55] Q. Qi, J. Lyu, K.-S. Chan, E.-W. Bai, and T. Yang. Stochastic constrained dro with a complexity independent of sample size. Transactions on Machine Learning Research.   \n[56] Q. Qi, Z. Guo, Y. Xu, R. Jin, and T. Yang. An online method for a class of distributionally robust optimization with non-convex objectives. Advances in Neural Information Processing Systems, 34:10067\u201310080, 2021.   \n[57] Q. Qi, Y. Xu, W. Yin, R. Jin, and T. Yang. Attentional-biased stochastic gradient descent. Transactions on Machine Learning Research, 2022.   \n[58] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[59] S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Konec\u02c7ny\\`, S. Kumar, and H. B. McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.   \n[60] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.   \n[61] Z. Shen, J. Cervino, H. Hassani, and A. Ribeiro. An agnostic approach to federated learning with class imbalance. In International Conference on Learning Representations, 2021.   \n[62] V. Smith, S. Forte, M. Chenxin, M. Tak\u00e1\u02c7c, M. I. Jordan, and M. Jaggi. Cocoa: A general framework for communication-efficient distributed optimization. Journal of Machine Learning Research, 18:230, 2018.   \n[63] S. U. Stich. Local sgd converges fast and communicates little. In International Conference on Learning Representations, 2018.   \n[64] S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsified sgd with memory. Advances in Neural Information Processing Systems, 31, 2018.   \n[65] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report, 2012.   \n[66] B. Wang and T. Yang. Finite-sum coupled compositional stochastic optimization: Theory and applications. arXiv preprint arXiv:2202.12396, 2022.   \n[67] M. Wang, E. X. Fang, and H. Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Math. Program., 161(1-2):419\u2013449, 2017.   \n[68] J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for communication-efficient distributed optimization. Advances in Neural Information Processing Systems, 31, 2018.   \n[69] R. Ward, X. Wu, and L. Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex landscapes. In Proceedings of the 36th International Conference on Machine Learning (ICML), pages 6677\u20136686, 2019.   \n[70] B. Woodworth, K. K. Patel, S. Stich, Z. Dai, B. Bullins, B. Mcmahan, O. Shamir, and N. Srebro. Is local sgd better than minibatch sgd? In International Conference on Machine Learning, pages 10334\u201310343. PMLR, 2020.   \n[71] B. E. Woodworth, K. K. Patel, and N. Srebro. Minibatch vs local sgd for heterogeneous distributed learning. Advances in Neural Information Processing Systems, 33:6281\u20136292, 2020.   \n[72] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. Liang, Q. V. Le, T. Ma, and A. W. Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429, 2023.   \n[73] T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. Advances in Neural Information Processing Systems, 26, 2013.   \n[74] C. Yeh, A. Perez, A. Driscoll, G. Azzari, Z. Tang, D. Lobell, S. Ermon, and M. Burke. Using publicly available satellite imagery and deep learning to understand economic well-being in africa. Nature communications, 11(1):2583, 2020.   \n[75] K. Yi, L. Condat, and P. Richt\u00e1rik. Explicit personalization and local training: Double communication acceleration in federated learning. arXiv preprint arXiv:2305.13170, 2023.   \n[76] H. Yu, R. Jin, and S. Yang. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization. In International Conference on Machine Learning, pages 7184\u20137193. PMLR, 2019.   \n[77] H. Yu, S. Yang, and S. Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 5693\u20135700, 2019.   \n[78] Y. Yu, S. P. Karimireddy, Y. Ma, and M. I. Jordan. Scaff-pd: Communication efficient fair and robust federated learning. arXiv preprint arXiv:2307.13381, 2023.   \n[79] H. Yuan, W. Morningstar, L. Ning, and K. Singhal. What do we mean by generalization in federated learning? arXiv preprint arXiv:2110.14216, 2021.   \n[80] M. Zecchin, M. Kountouris, and D. Gesbert. Communication-efficient distributionally robust decentralized learning. arXiv preprint arXiv:2205.15614, 2022.   \n[81] J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. J. Reddi, S. Kumar, and S. Sra. Why adam beats sgd for attention models. 2019.   \n[82] M. Zhang, K. Sapra, S. Fidler, S. Yeung, and J. M. Alvarez. Personalized federated learning with first order model optimization. arXiv preprint arXiv:2012.08565, 2020.   \n[83] D. Zhu, G. Li, B. Wang, X. Wu, and T. Yang. When auc meets dro: Optimizing partial auc for deep learning with non-convex convergence guarantee. In International Conference on Machine Learning, pages 27548\u201327573. PMLR, 2022.   \n[84] F. Zou and L. Shen. On the convergence of adagrad with momentum for training deep neural networks. arXiv preprint arXiv:1808.03408, 2(3):5, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Analysis of FGDRO-CVaR ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For non-smooth functions, since it is usually difficult or even impossible to find an $\\epsilon$ -stationary point, we are interested in finding an $\\epsilon$ -near stationary point. Following a common technique for finding an $\\epsilon$ -near stationary point for weakly convex function, we use the Moreau envelope of a general non-smooth $\\rho$ -weakly-convex $F(\\mathbf{x})$ [9], defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\psi_{(1/\\hat{\\rho})}(\\mathbf x)=\\operatorname*{min}_{\\mathbf x^{\\prime}}[F(\\mathbf x^{\\prime})+\\frac{\\hat{\\rho}}{2}\\|\\mathbf x^{\\prime}-\\mathbf x\\|^{2}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the properties of Moreau envelop [52, 9], we have that if $\\hat{\\rho}=2\\rho$ , then $\\psi_{1/\\hat{\\rho}}(\\cdot)$ is smooth and an $\\epsilon$ -stationary point of $\\psi_{1/\\hat{\\rho}}(\\cdot)$ is an $\\epsilon$ -near stationary point of $F(\\cdot)$ . ", "page_idx": 15}, {"type": "text", "text": "Since $F(\\mathbf{w},s)$ is non-smooth, we investigate its Moreau envelop, which is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\psi_{(1/\\hat{\\rho})}(\\mathbf{w},s)=\\operatorname*{min}_{\\mathbf{w}^{\\prime},s^{\\prime}}[F(\\mathbf{w}^{\\prime},s^{\\prime})+\\frac{\\hat{\\rho}}{2}(\\lVert\\mathbf{w}^{\\prime}-\\mathbf{w}\\rVert^{2}+\\lVert s^{\\prime}-s\\rVert^{2})].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. By updating rule, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\Sigma}\\|u_{i,t}^{r}-g_{i}(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}=\\mathbb{E}\\bigg[\\|(1-\\beta_{1})u_{i,t-1}^{r}+\\beta_{1}\\ell(\\mathbf{w}_{i,t-1}^{r},\\mathbf{z}_{i,t}^{r})-g_{i}(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}\\bigg]}\\\\ &{\\leq\\mathbb{E}\\bigg[\\left(1+\\displaystyle\\frac{\\beta_{1}}{2}\\right)\\|(1-\\beta_{1})u_{i,t-1}^{r}+\\beta_{1}\\ell(\\mathbf{w}_{i,t-1}^{r},\\mathbf{z}_{i,t}^{r})-g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}\\bigg]+(1+\\displaystyle\\frac{2}{\\beta_{1}})\\mathbb{E}\\|g_{i}(\\bar{\\mathbf{w}}_{t}^{r})-g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r},\\bar{\\mathbf{z}}_{i,t-1}^{r})\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathfrak{x}}{\\mathfrak{t}}\\|(1-\\beta_{1})u_{i,t-1}^{r}+\\beta_{1}\\ell(\\mathbf{w}_{i,t-1}^{r},\\mathbf{z}_{i,t}^{r})-g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2}}\\\\ &{\\leq\\mathbb{E}\\|(1-\\beta_{1})(u_{i,t-1}^{r}-g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r}))+\\beta_{1}(\\ell(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\mathbf{z}_{i,t}^{r})-g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r}))+\\beta_{1}(\\ell(\\mathbf{w}_{i,t-1}^{r},\\mathbf{z}_{i,t}^{r})-\\ell(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\mathbf{z})}\\\\ &{\\leq(1+\\frac{\\beta_{1}}{2})\\mathbb{E}\\|(1-\\beta_{1})(u_{i,t-1}^{r}-g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r}))+\\beta_{1}(\\ell(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\mathbf{z}_{i,t}^{r})-g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r}))\\|^{2}}\\\\ &{\\quad+(1+\\frac{2}{\\beta_{1}})\\beta_{1}^{2}\\|\\ell(\\mathbf{w}_{i,t-1}^{r},\\mathbf{z}_{i,t}^{r})-\\ell(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\mathbf{z}_{i,t}^{r})\\|^{2}}\\\\ &{=(1+\\frac{\\beta_{1}}{2})\\mathbb{E}\\|(1-\\beta_{1})(u_{i,t-1}^{r}-g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r}))\\|^{2}+(1+\\frac{\\beta_{1}}{2})\\beta_{1}^{2}\\|\\ell(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\mathbf{z}_{i,t}^{r})-g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2}}\\\\ &{\\quad+\\,(1+\\frac{2}{\\beta_{1}})\\beta_{ \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last equality uses $\\mathbb{E}_{t-1}[\\ell(\\bar{\\mathbf{w}}_{t-1}^{r},\\mathbf{z}_{i,t}^{r})-g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})]=0.$ ", "page_idx": 15}, {"type": "text", "text": "Since $f(\\cdot,\\mathbf{z})$ and $g(\\cdot,\\mathbf{z})$ are Lipschitz and smooth, we know $\\|\\mathbf{m}_{i,t}^{r}\\|^{2}\\leq C_{g}^{2}$ . We also have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\bar{\\mathbf{w}}_{t}^{r}-\\bar{\\mathbf{w}}_{i,t}^{r}\\|^{2}=\\|(\\bar{\\mathbf{w}}^{r}-\\eta\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t^{\\prime}=1}^{t}\\mathbf{m}_{i,t^{\\prime}}^{r})-(\\bar{\\mathbf{w}}^{r}-\\eta\\sum_{t^{\\prime}=1}^{t}\\mathbf{m}_{i,t^{\\prime}}^{r})\\|^{2}}\\\\ {\\displaystyle\\leq2\\|\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t^{\\prime}=1}^{t}\\mathbf{m}_{i,t^{\\prime}}^{r}\\|^{2}+2\\|\\sum_{t^{\\prime}=1}^{t}\\mathbf{m}_{i,t^{\\prime}}^{r}\\|^{2}\\leq4\\eta^{2}I^{2}C_{g}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\|u_{i,t}^{r}-g_{i}(\\overline{{\\mathbf{w}}}_{t}^{r})\\|^{2}\\leq(1-\\beta_{1})\\mathbb{E}\\|u_{i,t-1}^{r}-g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}}\\\\ {\\displaystyle\\quad+\\,2\\beta_{1}^{2}\\sigma^{2}+4\\beta_{1}C_{\\ell}^{2}\\|\\bar{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{i,t-1}^{r}\\|^{2}+(1+\\displaystyle\\frac{2}{\\beta_{1}})\\|g_{i}(\\bar{\\mathbf{w}}_{t}^{r})-g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}}\\\\ {\\displaystyle\\leq(1-\\beta_{1})\\mathbb{E}\\|u_{i,t-1}^{r}-g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+2\\beta_{1}^{2}\\sigma^{2}+4\\beta_{1}\\eta^{2}I^{2}C_{g}^{2}+\\displaystyle\\frac{3}{\\beta_{1}}C_{g}^{2}\\|\\bar{\\mathbf{w}}_{t}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Since $f(\\cdot)$ is 1-Lipschitz, convex and monitonically nondecreasing, while $\\ell(\\cdot,\\mathbf{z})$ is $C_{g}$ - Lipschitz and $L_{g}$ -smooth, we know that $F(\\mathbf{w},s)$ is $\\rho_{F}:=L_{g}$ -weakly convex by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(g(\\mathbf w),s)\\geq f(g(\\mathbf w^{\\prime}),s^{\\prime})+\\langle\\partial_{g}(f(g(\\mathbf w^{\\prime}),s^{\\prime})),g(\\mathbf w)-g(\\mathbf w^{\\prime})\\rangle+\\langle\\nabla_{g}(f(g(\\mathbf w^{\\prime}),s^{\\prime})),s-s^{\\prime}\\rangle}\\\\ &{\\geq f(g(\\mathbf w^{\\prime}),s^{\\prime})+\\langle\\nabla_{g}(f(g(\\mathbf w^{\\prime}),s^{\\prime}))\\nabla g(\\mathbf w^{\\prime}),\\mathbf w-\\mathbf w^{\\prime}\\rangle-\\frac{L_{g}}{2}\\|\\mathbf w-\\mathbf w^{\\prime}\\|^{2}}\\\\ &{\\quad+\\left\\langle\\nabla_{g}(f(g(\\mathbf w^{\\prime}),s^{\\prime})),s-s^{\\prime}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality uses the convexity of $f(\\cdot)$ , and the second inequality uses that $\\partial f(\\cdot)\\geq=0$ and the $L_{g}$ -smoothness of $g(\\cdot)$ . ", "page_idx": 16}, {"type": "text", "text": "With $\\hat{\\rho}=\\operatorname*{max}2\\rho_{F},1,$ , we define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi_{(1/\\hat{\\rho})}(\\bar{\\mathbf{w}}_{t}^{r},\\bar{s}_{t}^{r})=\\operatorname*{min}_{\\mathbf{w}^{\\prime},s^{\\prime}}[F(\\mathbf{w}^{\\prime},s^{\\prime})+\\frac{\\hat{\\rho}}{2}(\\|\\mathbf{w}^{\\prime}-\\bar{\\mathbf{w}}_{t}^{r}\\|^{2}+\\|s^{\\prime}-\\bar{s}_{t}^{r}\\|^{2})]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\hat{\\mathbf{w}}_{t}^{r},\\hat{s}_{t}^{r})=\\arg\\operatorname*{min}_{\\mathbf{w}^{\\prime},s^{\\prime}}[F(\\mathbf{w}^{\\prime},s^{\\prime})+\\frac{\\hat{\\rho}}{2}(\\|\\mathbf{w}^{\\prime}-\\bar{\\mathbf{w}}_{t}^{r}\\|^{2}+\\|s^{\\prime}-\\bar{s}_{t}^{r}\\|^{2})],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then we have the following [9], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{w}}_{t}^{r}-\\bar{\\mathbf{w}}_{t}^{r}\\|_{2}^{2}+\\|\\hat{s}_{t}^{r}-\\bar{s}_{t}^{r}\\|^{2}=\\frac{1}{\\hat{\\rho}}\\|\\nabla\\psi_{(1/\\hat{\\rho})}(\\bar{\\mathbf{w}}_{t}^{r},\\bar{s}_{t}^{r})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{z}}\\{\\psi_{1/\\rho}(\\bar{\\mathbf{w}}_{t}^{r},\\bar{\\mathbf{s}}_{t}^{r}]\\}=\\mathbb{E}_{\\mathbf{w}_{t}^{\\mathrm{in}}}\\bigg[F(\\mathbf{w}^{r},s^{\\prime})+\\frac{\\hat{\\rho}}{2}\\big(\\|\\mathbf{w}^{r}-\\bar{\\mathbf{w}}_{t}^{r}\\|^{2}+\\|s^{\\prime}-\\bar{s}_{t}^{r}\\|^{2}\\big)\\bigg]}\\\\ &{\\leq\\mathbb{E}\\bigg[F(\\hat{\\mathbf{w}}_{t-1}^{r},\\hat{s}_{t-1}^{r})+\\frac{\\hat{\\rho}}{2}\\big(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t}^{r}\\|^{2}+\\|\\hat{s}_{t-1}^{r}-\\bar{s}_{t}^{r}\\|^{2}\\big)\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[F(\\hat{\\mathbf{w}}_{t-1}^{r},\\hat{s}_{t-1}^{r})+\\frac{\\hat{\\rho}}{2}\\big(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-(\\bar{\\mathbf{w}}_{t-1}^{r}-\\eta\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{m}_{t,t}^{r})\\|^{2}+\\|\\hat{s}_{t-1}^{r}-(\\bar{s}_{t-1}^{r}-\\eta\\frac{1}{N}\\sum_{i=1}^{N}\\boldsymbol{v}_{i,t}^{r})\\|^{2}\\big)\\bigg]}\\\\ &{\\leq\\mathbb{E}\\bigg[F(\\hat{\\mathbf{w}}_{t-1}^{r},\\hat{s}_{t-1}^{r})+\\frac{\\hat{\\rho}}{2}\\big(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}+\\|\\hat{s}_{t-1}^{r}-\\bar{s}_{t-1}^{r}\\|^{2}\\big)\\bigg]}\\\\ &{\\phantom{=}+\\hat{\\rho}\\mathbb{E}\\bigg[\\eta\\langle\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r},\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{m}_{t,t}^{r}\\rangle+\\eta\\langle\\hat{s}_{t-1}^\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "w $\\mathrm{\\.here~we~used~\\|{\\bfm}_{\\cal i,t}^{r}\\|^{2}~}\\le~C_{g}^{2}.~~\\mathrm{Denote~}\\hat{\\mathbf{m}}_{\\cal t}^{r}~=~\\frac{1}{N}\\sum_{i=1}^{N}\\partial_{u}f(u_{i,t}^{r},s_{i,t-1}^{r})\\nabla\\ell(\\bar{\\mathbf{w}}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r}),~\\hat{v}_{t}^{r}~=~\\sum_{i=1}^{N}\\hat{f}_{i,t}^{i},$ $-\\frac{1}{N}\\sum_{i=1}^{N}\\partial_{s}f(u_{i,t}^{r},s_{i,t-1}^{r})$ , where the sub-differential $\\partial_{u}f\\big(u_{i,t}^{r},s_{i,t-1}^{r}\\big)$ and $\\partial_{s}f\\big(u_{i,t}^{r},s_{i,t-1}^{r}\\big)$ are se", "page_idx": 16}, {"type": "text", "text": "lected the same in $\\mathbf{m}_{i,t}^{r}$ and $v_{i,t}^{r}$ , respectively. Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\vert\\hat{\\boldsymbol{v}}_{t}\\vert_{\\mathcal{H}}^{-}\\big\\vert\\hat{\\boldsymbol{w}}_{t}^{\\tau}\\big\\vert\\big]}\\\\ &{\\leq\\mathbb{E}\\bigg[\\mathbb{F}(\\hat{\\boldsymbol{w}}_{t-1}^{\\nu},\\hat{\\boldsymbol{v}}_{t-1}^{\\nu})+\\frac{\\hat{\\rho}}{2}(\\vert\\vert\\hat{\\boldsymbol{w}}_{t-1}^{\\nu}-\\overline{{\\boldsymbol{w}}}_{t-1}^{\\nu}\\vert\\|^{2}+\\|\\hat{\\boldsymbol{v}}_{t-1}^{\\nu}-\\tilde{\\boldsymbol{v}}_{t-1}^{\\tau}\\vert\\|^{2})\\bigg]}\\\\ &{\\quad+\\hat{\\rho}\\mathbb{E}\\bigg[\\eta\\langle\\hat{\\boldsymbol{w}}_{t-1}^{\\nu}-\\hat{\\boldsymbol{w}}_{t-1}^{\\nu},\\hat{\\boldsymbol{\\mathfrak{w}}}_{t}^{\\tau}\\rangle+\\eta\\langle\\hat{\\boldsymbol{s}}_{t-1}^{\\nu}-\\hat{\\boldsymbol{v}}_{t-1}^{\\nu},\\hat{\\boldsymbol{w}}_{t}^{\\tau}\\rangle\\bigg]}\\\\ &{\\quad+\\hat{\\rho}\\tilde{\\boldsymbol{E}}\\bigg[\\eta\\langle\\hat{\\boldsymbol{w}}_{t-1}^{\\nu}-\\hat{\\boldsymbol{w}}_{t-1}^{\\nu},\\hat{\\boldsymbol{\\mathfrak{w}}}_{t}^{\\tau}-\\hat{\\boldsymbol{w}}_{t}^{\\nu}\\rangle+\\eta\\langle\\hat{\\boldsymbol{s}}_{t-1}^{\\nu}-\\hat{\\boldsymbol{v}}_{t-1}^{\\nu},\\hat{\\boldsymbol{v}}_{t}^{\\tau}-\\hat{\\boldsymbol{v}}_{t}^{\\nu}\\rangle\\bigg]+\\hat{\\rho}\\eta^{2}C_{g}^{2}}\\\\ &{\\leq\\mathbb{E}\\bigg[F(\\hat{\\boldsymbol{w}}_{t-1}^{\\nu},\\hat{\\boldsymbol{w}}_{t-1}^{\\nu})+\\frac{\\hat{\\rho}}{2}(\\vert\\vert\\hat{\\boldsymbol{w}}_{t-1}^{\\nu}-\\hat{\\boldsymbol{w}}_{t-1}^{\\tau}\\vert\\|^{2}+\\|\\hat{\\boldsymbol{s}}_{t-1}^{\\nu}-\\hat{\\boldsymbol{s}}_{t-1}^{\\tau}\\vert\\|^{2})\\bigg]}\\\\ &{\\quad+\\hat{\\rho}\\tilde{\\boldsymbol{E}}\\bigg[\\eta\\langle\\hat{\\boldsymbol{w}}_{t-1}^{\\nu}-\\hat{\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $f$ is convex, $\\partial_{u}f\\geq0$ and $g_{i}$ is $\\rho_{g}:=L_{g}$ -weakly convex, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(g_{i}(\\hat{\\mathbf{w}}_{t-1}^{r}),\\hat{s}_{t-1}^{r})-f(u_{i,t}^{r},s_{i,t-1}^{r})}\\\\ &{\\geq\\partial_{u}f(u_{i,t}^{r},s_{i,t-1}^{r})(g_{i}(\\hat{\\mathbf{w}}_{t-1}^{r})-u_{i,t}^{r})+\\partial_{s}f(u_{i,t}^{r},\\bar{s}_{t-1}^{r})(\\hat{s}_{t-1}^{r}-s_{i,t-1}^{r})}\\\\ &{\\geq\\partial_{u}f(u_{i,t}^{r},s_{i,t-1}^{r})\\bigg[g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})-u_{i,t}^{r}+\\langle\\nabla g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r}),\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\rangle-\\frac{\\rho_{g}}{2}\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}\\bigg]}\\\\ &{\\quad+\\,\\partial_{s}f(u_{i,t}^{r},s_{i,t-1}^{r})(\\hat{s}_{t-1}^{r}-s_{i,t-1}^{r}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Noting $\\partial_{u}f\\le1$ , (22) yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\langle\\hat{\\mathbf{m}}_{t}^{r},\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\rangle+\\langle\\hat{v}_{t}^{r},\\hat{s}_{t-1}^{r}-\\bar{s}_{t-1}^{r}\\rangle}\\\\ {\\displaystyle=\\frac{1}{N}\\sum_{i=1}^{N}\\langle\\partial_{u}f(u_{i,t}^{r},s_{i,t-1}^{r})\\nabla g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r}),\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\rangle+\\frac{1}{N}\\sum_{i=1}^{N}\\partial_{s}f(u_{i,t}^{r},s_{i,t-1}^{r})(\\hat{s}_{t-1}^{r}-s_{i,t-1}^{r})}\\\\ {\\displaystyle\\leq\\frac{1}{N}\\sum_{i=1}^{N}\\left[f(g_{i}(\\hat{\\mathbf{w}}_{t-1}^{r}),\\hat{s}_{t-1}^{r})-f(u_{i,t}^{r},s_{i,t-1}^{r})-\\partial_{u}f(u_{i,t}^{r},s_{i,t-1}^{r})\\big[g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})-u_{i,t}^{r}\\big]\\right]+\\frac{\\rho_{g}}{2}\\|\\hat{\\mathbf{w}}_{t-1}^{r}\\|_{\\infty}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Putting (21) and (23) together, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}|\\psi_{1,\\rho}(\\overline{{\\mathbf{w}}}_{t}^{r},\\overline{{\\mathbf{x}}}_{t}^{r})|}\\\\ &{\\leq\\mathbb{E}\\|\\psi_{1,\\rho}(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\overline{{\\mathbf{x}}}_{t-1}^{r})+\\frac{\\eta\\hat{\\rho}}{2}\\|\\tilde{\\mathbf{w}}_{t-1}^{r}-\\overline{{\\mathbf{w}}}_{t-1}^{r}\\|^{2}+\\frac{\\eta\\hat{\\rho}}{2}\\|\\tilde{\\mathbf{x}}_{t-1}^{r}-\\overline{{\\mathbf{x}}}_{t-1}^{r}\\|^{2}+\\frac{\\eta\\hat{\\rho}}{2}\\|\\mathbf{\\tilde{n}}_{t}^{r}-\\mathbf{\\hat{m}}_{t}^{r}\\|^{2}+\\frac{\\eta\\hat{\\rho}}{2}\\|\\tilde{\\mathbf{n}}_{t}^{r}-\\mathbf{\\hat{m}}_{t-1}^{r}\\|^{2}}\\\\ &{+\\hat{\\rho}\\eta^{2}C_{g}^{2}+\\eta\\hat{\\rho}\\frac{1}{N}\\frac{N}{\\Gamma_{1}}\\leq\\mathbb{E}\\bigg[f(g_{t}(\\overline{{\\mathbf{w}}}_{t-1}^{r}),\\overline{{\\mathbf{x}}}_{t-1}^{r})-f(\\overline{{\\mathbf{u}}}_{t,t}^{r},\\overline{{\\mathbf{x}}}_{t-1}^{r})-\\partial_{t}f(\\overline{{\\mathbf{u}}}_{t,t}^{r},\\overline{{\\mathbf{x}}}_{t-1}^{r})\\|g_{t}(\\overline{{\\mathbf{w}}}_{t-1}^{r})-u_{t,1}^{r}\\|}\\\\ &{+\\frac{\\eta\\hat{\\rho}}{2}\\|\\tilde{\\mathbf{w}}_{t-1}^{r}-\\overline{{\\mathbf{w}}}_{t-1}^{r}\\|^{2}\\bigg]}\\\\ &{=\\mathbb{E}|\\psi_{1,\\rho}(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\overline{{\\mathbf{x}}}_{t-1}^{r})+\\frac{\\eta\\hat{\\rho}}{2}\\|\\tilde{\\mathbf{w}}_{t-1}^{r}-\\mathbf{w}_{t-1}^{r}\\|^{2}+\\frac{\\eta\\hat{\\rho}}{2}\\|\\tilde{\\mathbf{x}}_{t-1}^{\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $f(g_{i}(\\mathbf{w}),s)$ is $\\rho_{F}$ -weakly convex in w, s, $\\begin{array}{r}{f(g_{i}(\\mathbf{w}),s)+\\frac{\\hat{\\rho}}{2}(\\|\\mathbf{w}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}+\\|s-\\bar{s}_{t-1}^{r}\\|^{2})}\\end{array}$ is $\\hat{\\rho}-\\rho_{F}$ -strongly convex in $\\mathbf{w},s.$ . Therefore, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(g_{i}(\\hat{\\mathbf{w}}_{t-1}^{r}),\\hat{s}_{t-1}^{r})-f(g_{i}(\\hat{\\mathbf{w}}_{t-1}^{r}),s_{t,t-1}^{r})}\\\\ &{=[f(g_{i}(\\hat{\\mathbf{w}}_{t-1}^{r}),\\hat{s}_{t-1}^{r})+\\frac{\\hat{\\rho}}{2}(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}+\\|\\hat{s}_{t-1}^{r}-\\bar{s}_{t-1}^{r}\\|^{2})]}\\\\ &{\\quad-\\left[f(g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r}),s_{t,t-1}^{r})+\\frac{\\hat{\\rho}}{2}(\\|\\bar{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}+\\|s_{i,t-1}^{r}-\\bar{s}_{t-1}^{r}\\|^{2})\\right]}\\\\ &{\\quad-\\frac{\\hat{\\rho}}{2}(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}+\\|\\hat{s}_{t-1}^{r}-s_{i,t-1}^{r}\\|^{2})+\\frac{\\hat{\\rho}}{2}\\|s_{i,t-1}^{r}-\\bar{s}_{t-1}^{r}\\|^{2}}\\\\ &{\\leq(\\frac{\\rho F}{2}-\\hat{\\rho})(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}+\\|\\hat{s}_{t-1}^{r}-s_{i,t-1}^{r}\\|^{2})+\\frac{\\hat{\\rho}}{2}\\|s_{i,t-1}^{r}-\\bar{s}_{t-1}^{r}\\|^{2}}\\\\ &{\\leq-\\frac{\\rho F}{2}(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}+\\|\\hat{s}_{t-1}^{r}-\\bar{s}_{t-1}^{r}\\|^{2})+(\\hat{\\rho}+\\rho F)\\|s_{i,t-1}^{r}-\\bar{s}_{t-1}^{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\psi_{1/\\rho}(\\overline{{\\mathbf{w}}}_{t}^{\\nu},\\overline{{\\mathbf{x}}}_{t}^{\\nu})\\}}\\\\ &{\\le\\psi_{1/\\rho}(\\overline{{\\mathbf{w}}}_{t-1}^{\\nu},\\overline{{\\mathbf{x}}}_{t-1}^{\\nu})+\\frac{\\eta\\hat{\\beta}}{2}\\|\\hat{\\mathbf{w}}_{t-1}^{\\nu}-\\overline{{\\mathbf{w}}}_{t-1}^{\\nu}\\|^{2}+\\frac{\\eta\\hat{\\beta}}{2}\\|\\hat{\\mathbf{s}}_{t-1}^{\\nu}-\\overline{{s}}_{r-1}^{\\nu}\\|^{2}+\\frac{\\eta\\hat{\\beta}}{2}\\|\\hat{\\mathbf{w}}_{t}^{\\nu}-\\mathbf{\\hat{m}}_{t}^{\\nu}\\|^{2}+\\frac{\\eta\\hat{\\beta}}{2}\\|\\hat{\\mathbf{v}}_{t}^{\\nu}-\\mathbf{\\hat{m}}_{t-1}^{\\nu}\\|^{2}}\\\\ &{+\\hat{\\eta}\\eta^{2}C_{g}^{2}+\\eta\\hat{\\beta}\\frac{1}{N}\\sum_{i}\\Bigg[f(g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{\\nu}),\\overline{{s}}_{t-1}^{\\nu})-f(g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{\\nu}),\\overline{{s}}_{t-1}^{\\nu})+f(g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{\\nu}),s_{i-1}^{\\nu})-f(u_{i}^{\\nu},s_{i-1}^{\\nu})}\\\\ &{\\quad-\\,\\partial_{t}\\big(\\overline{{\\mathbf{w}}}_{t-1}^{\\nu},s_{i-1}^{\\nu}\\big)\\|g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{\\nu})-u_{i,t}^{\\nu}\\|+\\frac{\\rho_{0}}{2}\\|\\hat{\\mathbf{w}}_{t-1}^{\\nu}-\\mathbf{w}_{t-1}^{\\nu}\\|^{2}\\Bigg]}\\\\ &{\\le\\psi_{1/\\rho}(\\overline{{\\mathbf{w}}}_{t-1}^{\\nu},\\overline{{s}}_{t-1}^{\\nu})+\\frac{\\eta\\hat{\\beta}}{2}\\|\\hat{\\mathbf{w}}_{t-1}^{\\nu \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It follows that ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\psi_{1/\\hat{\\rho}}(\\overline{{\\mathbf{w}}}_{t}^{r},\\bar{s}_{t}^{r})]}\\\\ &{\\ \\leq\\psi_{1/\\hat{\\rho}}(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\bar{s}_{t-1}^{r})-\\frac{\\hat{\\rho}}{4}\\eta\\hat{\\rho}(\\|\\hat{\\mathbf{w}}_{t-1}^{r}-\\overline{{\\mathbf{w}}}_{t-1}^{r}\\|^{2}+\\|\\hat{s}_{t-1}^{r}-\\bar{s}_{t-1}^{r}\\|^{2})}\\\\ &{\\quad+\\hat{\\rho}\\eta^{2}C_{g}^{2}+\\eta\\hat{\\rho}C_{f}\\frac{1}{N}\\sum_{i}\\|g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r})-u_{i,t}^{r}\\|+\\frac{\\eta\\hat{\\rho}}{2}\\|\\hat{\\mathbf{m}}_{t-1}^{r}-\\overline{{\\mathbf{m}}}_{t-1}^{r}\\|^{2}+\\frac{\\eta\\hat{\\rho}}{2}\\|\\hat{v}_{t-1}^{r}-\\bar{v}_{t-1}^{r}\\|^{2}}\\\\ &{\\ \\leq\\psi_{1/\\hat{\\rho}}(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\bar{s}_{t-1}^{r})-\\frac{\\eta}{4}\\|\\nabla\\psi_{1/\\hat{\\rho}}(\\overline{{\\mathbf{w}}}_{t-1}^{r},\\bar{s}_{s-1}^{r})\\|^{2}+\\hat{\\rho}\\eta^{2}C_{g}^{2}}\\\\ &{\\quad+\\eta\\hat{\\rho}C_{f}\\frac{1}{N}\\sum_{i}\\|g_{i}(\\overline{{\\mathbf{w}}}_{t-1}^{r})-u_{i,t}^{r}\\|+\\frac{\\eta\\hat{\\rho}}{2}\\eta^{2}I^{2}C_{g}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{R I}\\sum_{r=1}^{R}\\displaystyle\\sum_{t=1}^{I}\\mathbb{E}\\|\\nabla\\psi_{1/\\hat{\\rho}}(\\bar{\\mathbf{w}}_{t-1}^{r},\\bar{s}_{s-1}^{r})\\|^{2}\\leq}\\\\ {\\displaystyle\\frac{\\psi_{1/\\hat{\\rho}}(\\bar{\\mathbf{w}}_{0}^{r},\\bar{s}_{0}^{r})}{\\eta R I}+\\eta\\hat{\\rho}C_{g}^{2}+\\frac{1}{N R I}\\displaystyle\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\sum_{i=1}^{N}\\mathbb{E}\\|g_{i}(\\bar{\\mathbf{w}}_{t-1}^{r})-u_{i,t}^{r}\\|+\\eta^{2}I^{2}C_{g}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, taking telescoping sum over Lemma 4.2 sum we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{R I N}\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\sum_{i=1}^{N}\\mathbb{E}\\|\\big(u_{i,t-1}^{r}-g_{i}\\big(\\Bar{\\mathbf{w}}_{t-1}^{r}\\big)\\big)\\|^{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{\\beta_{1}R I N}\\sum_{i=1}^{N}\\mathbb{E}\\|\\big(u_{i,0}^{0}-g_{i}\\big(\\Bar{\\mathbf{w}}_{0}^{0}\\big)\\big)\\|^{2})+2\\beta_{1}\\sigma^{2}+\\eta^{2}I^{2}C_{g}^{2}+\\frac{\\eta^{2}}{\\beta_{1}^{2}}C_{g}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{S}\\|\\nabla\\psi_{1/\\hat{\\rho}}(\\widetilde{\\mathbf w},\\widetilde{s})\\|^{2}=\\frac{1}{R I}\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\mathbb{E}\\|\\nabla\\psi_{1/\\hat{\\rho}}(\\widetilde{\\mathbf w},\\widetilde{s})\\|^{2}\\leq O\\left(\\frac{1}{\\eta R I}+\\eta+\\frac{1}{\\beta_{1}R I}+\\sqrt{\\beta_{1}}+\\eta I+\\frac{\\eta}{\\beta_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big(\\hat{\\mathbf{w}},\\hat{s}\\big)=\\arg\\operatorname*{min}_{\\mathbf{w}^{\\prime},s^{\\prime}}[F(\\mathbf{w}^{\\prime},s^{\\prime})+\\frac{\\hat{\\rho}}{2}(\\|\\mathbf{w}^{\\prime}-\\tilde{\\mathbf{w}}\\|^{2}+\\|s^{\\prime}-\\tilde{s}\\|^{2})],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We also have that $\\begin{array}{r l r}{\\|\\hat{\\bf w}\\ -\\ \\tilde{\\bf w}\\|^{2}\\;+\\;\\|\\hat{s}\\ -\\ \\tilde{s}\\|^{2}}&{=}&{\\hat{\\rho}\\|\\nabla\\psi_{1/\\hat{\\rho}}(\\tilde{\\bf w},\\tilde{s})\\|^{2},\\ |d i s t({\\bf0},\\partial F(\\hat{\\bf w},\\tilde{\\bf\\psi}))\\|^{2},}\\end{array}$ s\u02c6))|2 \u2264 $\\|\\nabla\\psi_{1/\\hat{\\rho}}(\\tilde{\\mathbf{w}},\\tilde{s})\\|^{2}$ [9]. We can conclude by setting parameters as in the theorem. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B Analysis of FGDRO-KL ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The behavior of $u$ , which is an estimator of $\\nabla g_{i}(\\mathbf{w})^{\\circ};$ s, is given in the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.1. Under Assumption 5.1, with some constant $G$ , by setting $\\begin{array}{r}{\\eta\\,=\\,O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , $\\beta_{1}\\,=$ $\\textstyle O\\left({\\frac{1}{\\sqrt{R I}}}\\right)$ , Algorithm 2 ensures that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|u_{i,t}^{r}-\\ell(\\bar{\\mathbf{w}}_{t}^{r};\\mathcal{D}_{i})\\|^{2}\\leq(1-\\displaystyle\\frac{\\beta_{1}}{2})\\mathbb{E}\\|u_{i,t-1}^{r}-\\ell(\\bar{\\mathbf{w}}_{t-1}^{r};\\mathcal{D}_{i})\\|^{2}+3\\beta_{1}\\eta^{2}\\beta_{3}^{2}I^{4}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle12\\beta_{1}C_{\\ell}^{2}\\eta^{2}I\\displaystyle\\sum_{\\tau=0}^{t-1}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{\\tau}^{r}\\|^{2}+\\beta_{1}^{2}\\sigma^{2}+\\displaystyle\\frac{3}{\\beta_{1}}C_{\\ell}^{2}\\eta^{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t}^{r}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The behavior of $v$ , which is an estimator of $\\begin{array}{r}{\\frac{1}{N}\\sum_{i}g_{i}(\\mathbf{w})}\\end{array}$ , is given in the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.2. Under Assumption 5.1, with some constant $C_{1},$ , by setting $\\begin{array}{r}{\\eta\\,=\\,O\\left(\\frac{1}{\\sqrt{R I}}\\right)}\\end{array}$ , $\\beta_{1}\\,=$ $\\textstyle O\\left({\\frac{1}{\\sqrt{R I}}}\\right)$ , Algorithm 2 ensures that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\|\\bar{v}_{t}^{r}-g(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}\\leq(1-\\beta_{2})\\mathbb{E}\\|\\bar{v}_{t-1}^{r}-g(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}}\\\\ {\\displaystyle+\\,3\\beta_{2}\\frac{1}{N}\\sum_{i=1}^{N}C_{1}\\mathbb{E}\\|u_{i,t}^{r}-\\ell(\\mathbf{w};\\mathcal{D}_{i})\\|^{2}+\\displaystyle\\frac{3}{\\beta_{2}}C_{g}^{2}\\mathbb{E}\\|\\bar{\\mathbf{w}}_{t}^{r}-\\bar{\\mathbf{w}}_{t-1}^{r}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Proof of Lemma B.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Denoting $\\bar{\\mathbf{w}}_{t}^{r}=\\frac{1}{N}\\sum_{i=1}^{N}\\bar{\\mathbf{w}}_{i,t}^{r}$ , we have $\\bar{\\mathbf{w}}_{t}^{r}=\\bar{\\mathbf{w}}_{t-1}^{r}-\\eta\\bar{\\mathbf{m}}_{t}^{r}$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\nabla_{x}\\|_{\\infty}^{2}-(\\mu_{x}^{*};\\mathcal{D}_{t})\\|^{2}=\\mathbb{E}\\|(1-\\beta_{t})\\|_{\\infty}^{2},~-(\\mu_{x}^{*};\\mathcal{D}_{t})+\\beta_{t}(\\mu_{x-1}^{*};\\mathcal{D}_{t})-\\hat{\\lambda}_{t}^{2}(\\mu_{x}^{*};\\mathcal{D}_{t})\\|^{2}}\\\\ &{=\\mathbb{E}\\big[(1-\\beta_{t})(u_{x-1}^{*}-(\\mu_{x-1}^{*};\\mathcal{D}_{t}))+\\beta_{t}(\\mu_{x-1}^{*};\\mathcal{D}_{t})-\\hat{\\lambda}_{t}^{2}(\\mu_{x-1}^{*};\\mathcal{D}_{t})+(\\mu_{x-1}^{*};\\mathcal{D}_{t})-\\hat{\\lambda}_{t}^{2}(\\mu_{x-1}^{*};\\mathcal{D}_{t})-\\hat{\\lambda}_{t}^{2}(\\mu_{x-1}^{*};\\mathcal{D}_{t})}\\\\ &{\\le\\mathbb{E}\\bigg(1+\\frac{\\beta_{t}}{2}\\bigg)\\bigg]\\|(1-\\beta_{t})\\|_{\\infty}^{2},~-[(\\mu_{x-1}^{*};\\mathcal{D}_{t})]+\\beta_{t}\\big(\\mu_{x-1}^{*};\\lambda_{t}^{2}\\big),~-[(\\mu_{x-1}^{*};\\mathcal{D}_{t})-\\hat{\\lambda}_{t}^{2}(\\mu_{x-1}^{*};\\mathcal{D}_{t})]\\|^{2}}\\\\ &{~+\\mathbb{E}\\bigg(1+\\frac{\\beta_{t}}{2}\\bigg)\\bigg\\}[(\\mu_{x-1}^{*};\\mathcal{D}_{t})-\\hat{\\lambda}(\\mu_{x}^{*};\\mathcal{D}_{t})\\bigg]^{2}}\\\\ &{\\lesssim\\Big(1+\\frac{\\beta_{t}}{2}\\bigg)\\|(1-\\beta_{t})(u_{x-1}^{*}-(\\mu_{x-1}^{*};\\mathcal{D}_{t}))+\\beta_{t}\\big(\\mu_{x-1}^{*};\\mathcal{D}_{t}\\big)-(\\mu_{x-1}^{*};\\mathcal{D}_{t})\\big)}\\\\\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (a) holds due to the fact that $\\mathbb{E}_{t-1}[\\ell(\\mathbf{w}_{i,t}^{r};\\mathbf{z}_{i,t}^{r})-\\ell(\\mathbf{w}_{i,t}^{r};\\mathcal{D}_{i})]=0$ . Moreover, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\ell({\\mathbf{w}}_{i,t-1}^{r};{\\mathcal{D}}_{i})-\\ell(\\bar{{\\mathbf{w}}}_{t-1}^{r};{\\mathcal{D}}_{i})\\|^{2}}\\\\ &{\\le C_{\\ell}^{2}\\mathbb{E}\\|{\\mathbf{w}}_{i,t-1}^{r}-\\bar{{\\mathbf{w}}}_{t-1}^{r}\\|^{2}}\\\\ &{\\le2C_{\\ell}^{2}\\mathbb{E}\\|{\\mathbf{w}}_{i,t-1}^{r}-\\bar{{\\mathbf{w}}}^{r}\\|^{2}+2C_{\\ell}^{2}\\mathbb{E}\\|\\bar{{\\mathbf{w}}}^{r}-\\bar{{\\mathbf{w}}}_{t-1}^{r}\\|^{2}}\\\\ &{\\le2C_{\\ell}^{2}\\eta^{2}\\mathbb{E}\\|\\displaystyle{\\sum_{\\tau=1}^{t-1}}{\\mathbf{m}}_{i,\\tau}^{r}\\|^{2}+2C_{\\ell}^{2}\\eta^{2}\\mathbb{E}\\|\\displaystyle{\\frac{1}{N}\\sum_{k=1}^{N}\\sum_{\\tau=1}^{t-1}{\\mathbf{m}}_{k,\\tau}^{r}}\\|^{2}}\\\\ &{\\le4C_{\\ell}^{2}\\eta^{2}I\\displaystyle{\\sum_{\\tau=1}^{t-1}}\\mathbb{E}\\|{\\mathbf{m}}_{i,\\tau}^{r}-\\bar{{\\mathbf{m}}}_{\\tau}^{r}\\|^{2}+6C_{\\ell}^{2}\\eta^{2}I\\displaystyle{\\sum_{\\tau=1}^{t-1}}\\mathbb{E}\\|\\bar{{\\mathbf{m}}}_{\\tau}^{r}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\|{\\bf m}_{i,\\tau}^{r}-\\bar{{\\bf m}}_{\\tau}^{r}\\|^{2}=\\mathbb{E}\\bigg\\|\\left(\\sum_{\\tau=1}^{t}(1-\\beta_{3})^{\\tau-1}\\beta_{3}\\frac{1}{v_{i,\\tau}^{r}}g(u_{i,\\tau}^{r})\\nabla\\ell({\\bf w}_{i,\\tau-1}^{r};{\\bf z}_{\\tau,t}^{r})+(1-\\beta_{3})^{\\tau}\\bar{{\\bf m}}_{0}^{r}\\right)}\\\\ {\\displaystyle-\\left(\\sum_{\\tau=1}^{t}(1-\\beta_{3})^{\\tau-1}\\beta_{3}\\frac{1}{N}\\sum_{k=1}^{N}\\frac{1}{v_{k,t}^{r}}g(u_{k,t}^{r})\\nabla\\ell({\\bf w}_{k,t-1}^{r};{\\bf z}_{k,t}^{r})+(1-\\beta_{3})^{\\tau}\\bar{{\\bf m}}_{0}^{r}\\right)\\bigg\\|^{2}}\\\\ {\\displaystyle\\leq2\\mathbb{E}\\|\\sum_{\\tau=1}^{t}(1-\\beta_{3})^{\\tau-1}\\beta_{3}\\frac{1}{v_{i,\\tau}^{r}}g(u_{i,\\tau}^{r})\\nabla\\ell({\\bf w}_{i,\\tau-1}^{r};{\\bf z}_{\\tau,t}^{r})\\|^{2}}\\\\ {\\displaystyle+\\,2\\mathbb{E}\\|\\sum_{\\tau=1}^{t}(1-\\beta_{3})^{\\tau-1}\\beta_{3}\\frac{1}{N}\\sum_{k=1}^{N}\\frac{1}{v_{k,t}^{r}}g(u_{k,t}^{r})\\nabla\\ell({\\bf w}_{k,t-1}^{r};{\\bf z}_{k,t}^{r})\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\mathbf{m}_{i,\\tau}^{\\tau}-\\mathbf{m}_{\\tau}^{\\tau}\\|^{2}}\\\\ &{\\quad\\quad\\le2t\\displaystyle\\sum_{\\tau=1}^{t}\\beta_{3}^{2}\\mathbb{E}\\|\\frac{1}{v_{i,\\tau}^{\\tau}}\\|^{2}\\|g(u_{i,\\tau}^{\\tau})\\|^{2}\\|\\nabla\\ell(\\mathbf{w}_{i,\\tau-1}^{\\tau};\\mathbf{z}_{i,\\tau}^{\\tau})\\|^{2}\\|}\\\\ &{\\quad\\quad+2t\\displaystyle\\sum_{\\tau=1}^{t}\\beta_{3}^{2}\\displaystyle\\frac{1}{N_{k}}\\sum_{k=1}^{N}\\mathbb{E}\\|\\|\\frac{1}{v_{k,t}^{\\tau}}\\|^{2}\\|g(u_{k,\\tau}^{\\tau})\\|^{2}\\|\\nabla\\ell(\\mathbf{w}_{k,t-1}^{\\tau};\\mathbf{z}_{i,\\tau}^{\\tau})\\|^{2}]}\\\\ &{\\quad\\quad\\le8t^{2}\\beta_{3}^{2}C_{1}^{2}\\Big(\\mathbb{E}\\|\\nabla\\ell(\\mathbf{w}_{i,\\tau-1}^{\\tau};\\mathbf{z}_{i,\\tau}^{\\tau})-\\nabla\\ell(\\mathbf{w}_{i,\\tau-1}^{\\tau};\\mathcal{D}_{i})\\|^{2}+\\mathbb{E}\\|\\nabla\\ell(\\mathbf{w}_{i,\\tau-1}^{\\tau};\\mathcal{D}_{i})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{N_{k-1}}\\sum_{k=1}^{N}(\\mathbb{E}\\|\\nabla\\ell(\\mathbf{w}_{k,\\tau-1}^{\\tau};\\mathbf{z}_{k,\\tau}^{\\tau})-\\nabla\\ell(\\mathbf{w}_{k,\\tau-1}^{\\tau};\\mathcal{D}_{k})\\|^{2}+\\mathbb{E}\\|\\nabla\\ell(\\mathbf{w}_{k,\\tau-1}^{\\tau};\\mathcal{D}_{k})\\|^{2})\\Big)}\\\\ &{\\quad\\quad\\le16t^{2}\\beta_{3}^{2}C_{1}^{2}(\\sigma^{2}+C_{2}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with $C_{1}=\\exp(C_{0}/\\lambda)$ . Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|u_{i,t}^{r}-\\ell(\\bar{\\mathbf{w}}_{t}^{r};\\mathcal{D}_{i})\\|^{2}\\leq(1-\\displaystyle\\frac{\\beta_{1}}{2})\\mathbb{E}\\|u_{i,t-1}^{r}-\\ell(\\bar{\\mathbf{w}}_{t-1}^{r};\\mathcal{D}_{i})\\|^{2}+3\\beta_{1}\\eta^{2}\\beta_{3}^{2}I^{4}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+12\\beta_{1}C_{\\ell}^{2}\\eta^{2}I\\displaystyle\\sum_{\\tau=0}^{t-1}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{\\tau}^{r}\\|^{2}+\\beta_{1}^{2}\\sigma^{2}+\\displaystyle\\frac{3}{\\beta_{1}}C_{\\ell}^{2}\\eta^{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t}^{r}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with $C_{2}=288(C_{\\ell}^{2}C_{1}^{2}(\\sigma^{2}+C_{\\ell}^{2}))$ . ", "page_idx": 21}, {"type": "text", "text": "B.3 Proof of Lemma B.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|v_{t}^{\\prime\\prime}-g(\\mathbf{w}_{t}^{\\prime})\\|^{2}=\\|(1-\\beta_{2})v_{t-1}^{\\prime}+\\beta_{2}\\frac{1}{N_{\\perp}}\\displaystyle\\sum_{i=1}^{N}\\exp(u_{i,t}^{\\prime}/\\lambda)-g(\\mathbf{w}_{t-1}^{\\prime})\\|^{2}}\\\\ &{=\\Big\\|(1-\\beta_{2})(v_{t-1}^{\\prime}-g(\\mathbf{w}_{t-1}^{\\prime}))+\\beta_{2}\\left(\\frac{1}{N_{\\perp}}\\displaystyle\\sum_{i=1}^{N}\\exp(u_{i,t}^{\\prime}/\\lambda)-g(\\mathbf{w}_{t-1}^{\\prime})\\right)-g(\\mathbf{w}_{t-1}^{\\prime})+g(\\mathbf{w}_{t-1}^{\\prime})\\Big\\|^{2}}\\\\ &{\\leq\\Big(1+\\frac{\\beta_{2}}{2}\\Big)\\left\\|(1-\\beta_{2})\\left(v_{t-1}^{\\prime}-g(\\mathbf{w}_{t-1}^{\\prime})\\right)+\\beta_{2}\\left(\\frac{1}{N_{\\perp}}\\displaystyle\\sum_{i=1}^{N}\\exp(v_{i,t}^{\\prime}/\\lambda)-g(\\mathbf{w}_{t-1}^{\\prime})\\right)\\right\\|^{2}}\\\\ &{\\qquad+\\left(1+\\frac{\\beta_{2}}{2}\\right)\\|g(\\mathbf{w}_{t}^{\\prime})-g(\\mathbf{w}_{t-1}^{\\prime})\\|^{2}}\\\\ &{\\leq\\Big(1+\\frac{\\beta_{2}}{2}\\Big)^{2}\\left(1-\\beta_{2}\\right)^{2}\\|v_{t-1}^{\\prime}-g(\\mathbf{w}_{t-1}^{\\prime})\\|^{2}+\\Big(1+\\frac{2}{\\beta_{2}}\\Big)\\beta_{2}\\left\\|\\frac{1}{N_{\\perp}}\\displaystyle\\sum_{i=1}^{N}\\exp(u_{i,t}^{\\prime}/\\lambda)-g(\\mathbf{w}_{t-1}^{\\prime})\\right\\|}\\\\ &{\\quad+\\frac{\\beta_{2}}{\\beta_{2}}c_{0}^{\\prime}\\|w_{t-1}^{\\prime}-g(\\mathbf{w}_{t-1}^{\\prime})\\|^{2}}\\\\ &{\\leq(1-\\beta_{2})\\|v_{t-1}^{\\prime}-g(\\mathbf{w}_{t-1}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{1}=\\exp(C_{0}/\\lambda)$ . ", "page_idx": 21}, {"type": "text", "text": "B.4 Proof of Lemma 5.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Here we analyze the $\\bar{\\bf m}$ , which is the moving average estimator of the gradient, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\nabla F(\\mathbf{w}_{t}^{r})\\|^{2}=\\left\\|(1-\\beta_{3})\\mathbf{m}_{t-1}^{r}+\\beta_{3}\\frac{1}{N}\\displaystyle\\frac{N}{\\sum_{i=1}^{N}\\overline{{v}}_{i,t}^{\\frac{1}{r}}}g(\\mathbf{u}_{i,t}^{r})\\nabla\\{\\mathbf{u}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r}\\}-\\nabla F(\\mathbf{w}_{t-1}^{r})\\right\\|^{2}}\\\\ {\\displaystyle(1-\\beta_{3})(\\mathbf{m}_{t-1}^{r}-\\nabla F(\\mathbf{w}_{t-1}^{r}))+\\beta_{3}(\\frac{1}{N}\\displaystyle\\frac{N}{N}\\displaystyle\\frac{1}{\\sum_{i=1}^{N}\\overline{{v}}_{i,t}^{\\frac{1}{r}}}\\exp(u_{i,t}^{r}/\\lambda)\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})-\\nabla F(\\mathbf{w}_{t-1}^{r}))}\\\\ {\\displaystyle+\\nabla F(\\mathbf{w}_{t-1}^{r})-\\nabla F(\\mathbf{w}_{t}^{r})\\right\\|^{2}}\\\\ {\\displaystyle_{\\mathrm{\\scriptsize\\textit{{V}}}}^{\\prime}+\\frac{\\beta_{3}}{2}\\displaystyle\\frac{1}{\\sqrt{1-\\beta_{3}}}\\displaystyle\\Bigg\\{\\frac{(1-\\beta_{3})(\\mathbf{m}_{t-1}^{r}-\\nabla F(\\mathbf{w}_{t-1}^{r}))+\\beta_{3}(\\displaystyle\\frac{1}{N}\\displaystyle\\frac{N}{\\sum_{i=1}^{N}\\overline{{v}}_{i,t}^{r}}\\exp(u_{i,t}^{r}/\\lambda)\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})-\\nabla F(\\mathbf{w}_{t}^{r})\\|^{2}}{\\displaystyle(1+\\frac{\\beta_{3}}{\\beta_{3}})\\|\\nabla F(\\mathbf{w}_{t-1}^{r})-\\nabla F(\\mathbf{w}_{t}^{r})\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\beta\\left(\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1}\\beta(\\mathbf{r}_{s}^{t},\\ldots,\\mathbf{r}_{i}^{t})(\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\cap\\mathcal{W}_{s})-\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1}\\beta(\\mathbf{r}_{s}^{t},\\ldots,\\mathbf{r}_{i}^{t})\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\cap\\mathcal{W}_{s}\\right)}\\\\ &{\\quad+\\beta\\left(\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1}\\alpha_{w}\\otimes_{(\\mathbf{r}_{s}^{t}-\\lambda_{w})}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\cap\\mathcal{W}_{s}\\right)}\\\\ &{\\quad+\\beta\\left(\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1}\\alpha_{w}\\otimes_{(\\mathbf{r}_{s}^{t}-\\lambda_{w})}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\cap\\mathcal{W}_{s}\\right)-\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1}\\alpha_{w}\\otimes_{(\\mathbf{r}_{s}^{t}-\\lambda_{w})}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}}\\\\ &{\\quad+\\beta\\left(\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1}\\alpha_{w}\\otimes_{(\\mathbf{r}_{s}^{t}-\\lambda_{w})}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}-\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1}\\alpha_{w}\\otimes_{(\\mathbf{r}_{s}^{t}-\\lambda_{w})}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\mathbb{P}(\\mathbf{u}_{s}^{t})_{i}\\right.}\\\\ &{\\quad\\left.+\\beta\\left(\\frac{1}{N_{w}}\\sum_{w\\in\\mathcal{W}_{s}}^{-1} \n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n+\\left(1+\\frac{2}{\\beta_{3}}\\right)\\beta_{3}^{2}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}\\exp(u_{i,t}^{r}/\\lambda)\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}\\exp(u_{i,t-1}^{r}/\\lambda)\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is followed by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A)\\leq\\displaystyle\\left(1+\\frac{\\beta_{3}}{2}\\right)\\left\\|(1-\\beta_{3})(\\mathbf{m}_{t-1}^{\\prime}-\\nabla F(\\mathbf{w}_{t-1}^{\\prime}))\\right.}\\\\ &{\\phantom{A A}\\displaystyle\\left.+\\beta_{3}\\left(\\frac{1}{N}\\,\\frac{\\nabla}{|\\mathbf{x}_{t}|}\\,\\frac{1}{v_{t,i}^{\\prime}}\\,\\mathrm{exp}(u_{t,t-1}^{\\prime}/\\lambda)\\nabla\\ell(\\mathbf{w}_{t,t-1}^{\\prime};\\mathcal{D}_{t})-\\frac{1}{N}\\,\\sum_{i=1}^{N}\\frac{1}{v_{t,i}^{\\prime}}\\,\\mathrm{exp}(\\ell(\\mathbf{w}_{t-1}^{\\prime};\\mathcal{D}_{t})/\\lambda)\\nabla\\ell(\\mathbf{w}_{t,t-1}^{\\prime};\\mathcal{D}_{t})\\right)\\right.}\\\\ &{\\phantom{A A}\\displaystyle\\left.+\\beta_{3}\\left(\\frac{1}{N}\\,\\frac{\\nabla}{|\\mathbf{x}_{t}|}\\,\\mathrm{exp}(\\ell(\\mathbf{w}_{t-1}^{\\prime};\\mathcal{D}_{t})/\\lambda)\\nabla\\ell(\\mathbf{w}_{t,t-1}^{\\prime};\\mathcal{D}_{t})-\\frac{1}{N}\\,\\sum_{i=1}^{N}\\frac{1}{v_{t,i}^{\\prime}}\\,\\mathrm{exp}(\\ell(\\mathbf{w}_{t-1}^{\\prime};\\mathcal{D}_{t})/\\lambda)\\nabla\\ell(\\mathbf{w}_{t-1}^{\\prime};\\mathcal{D}_{t})\\right)\\right.}\\\\ &{\\left.\\phantom{A A}\\displaystyle+\\beta_{3}\\left(\\frac{1}{N}\\,\\frac{1}{|\\mathbf{x}_{t}|}\\,\\mathrm{exp}(\\ell(\\mathbf{w}_{t-1}^{\\prime};\\mathcal{D}_{t})/\\lambda)\\nabla\\ell(\\mathbf{w}_{t-1}^{\\prime};\\mathcal{D}_{t})-\\nabla F(\\mathbf{w}_{t-1}^{\\prime})\\right)\\right\\|^{2},}\\\\ &{\\phantom{A A}\\displaystyle+\\left(1+\\frac{\\beta_{3}}{2}\\right)\\left\\|\\beta_{3}\\left(\\frac{1}{N}\\,\\frac{1}{|\\mathbf{x}_{t}|}\\,\\mathrm{exp}(u_{t,t-1}^{\\prime}/\\lambda)\\nabla\\ell(\\mathbf{w}_{t,t-1}^{\\prime};\\mathcal{D} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then it leads to ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathfrak{a}_{i\\alpha}-\\nabla F(\\mathbf{u}_{i}^{\\alpha})|^{s}}\\\\ &{\\le\\left(1+\\frac{\\beta}{2}\\right)^{2}\\left|1-\\beta\\right|(u_{i}^{\\alpha}-1)\\left(\\mathfrak{a}_{i\\alpha}^{\\tau}-\\nabla F(\\mathbf{u}_{i}^{\\alpha}-1)\\right)}\\\\ &{+\\beta_{s}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\nu_{i}^{2}}\\exp(u_{i-1}^{\\alpha}/\\lambda)\\nabla F(\\mathbf{u}_{i-1}^{\\alpha};D_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\nu_{i}^{2}}\\exp((\\mathfrak{w}_{i-1}^{\\alpha};D_{i})/\\lambda)\\nabla F(\\mathfrak{w}_{i-1}^{\\alpha};D_{i})\\right.}\\\\ &{\\left.+\\beta_{s}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\nu_{i}^{2}}\\exp(\\left(u_{i}^{\\alpha}-1/D_{i}\\right)/\\lambda)\\nabla F(\\mathbf{u}_{i-1}^{\\alpha};D_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\nu_{i}^{2}}\\exp(\\varphi_{i}^{\\alpha}/\\lambda)\\nabla F(\\mathfrak{w}_{i-1}^{\\alpha};D_{i})/\\lambda\\right)}\\\\ &{+\\beta_{s}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\nu_{i}^{2}}\\exp(\\left(u_{i-1}^{\\alpha};D_{i}\\right)/\\lambda)\\nabla F(\\mathfrak{w}_{i-1}^{\\alpha};D_{i})-\\nabla F(\\mathfrak{w}_{i-1}^{\\alpha};D_{i})\\right)\\right|^{2}}\\\\ &{+\\beta_{s}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\nu_{i}^{2}}\\exp(\\left(u_{i-1}^{\\alpha};D_{i}\\right)/\\lambda)\\nabla F(\\mathfrak{w}_{i-1}^{\\alpha};D_{i})-\\nabla F(\\mathfrak{w}_{i-1}^{\\alpha})\\right)\\Bigg|^{2}}\\\\ &{+\\left(1+\\frac{\\beta_{s}}{2}\\right)^{2}\\left|1\\frac{1}{N}\\sum_{i=1}^{N}\\exp(u_{i-1}^{\\\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, it follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\displaystyle\\frac{8}{\\beta_{3}}\\beta_{3}^{2}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i}^{2}}\\exp(u_{i,t-1}^{\\prime}/3)\\nabla\\ell(w_{i-1}^{\\prime}/3)-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{2}}\\exp(\\ell(\\overline{{w}}_{i-1}^{\\prime};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(w_{i-1}^{\\prime}/3)\\right\\|^{2}}\\\\ &{+\\displaystyle\\frac{8}{\\beta_{3}}\\beta_{3}^{2}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{2}}\\exp(\\ell(w_{i-1}^{\\prime};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(w_{i-1}^{\\prime};\\mathcal{D}_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i}^{2}}\\exp(\\ell(w_{i-1}^{\\prime};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(w_{i-1}^{\\prime}/3)\\right\\|^{2}}\\\\ &{+\\displaystyle\\frac{8}{\\beta_{3}}\\beta_{3}^{2}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i}^{2}}\\exp(\\ell(w_{i-1}^{\\prime};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(w_{i-1}^{\\prime};\\mathcal{D}_{i})-\\nabla F(\\overline{{w}}_{i-1}^{\\prime})\\right\\|^{2}}\\\\ &{+\\displaystyle\\beta_{3}^{2}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i}^{2}}\\exp(u_{i,t-1}^{\\prime}/3)\\sqrt{v_{i}(w_{i-1}^{\\prime};\\mathcal{D}_{i})}-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{2}}\\exp(u_{i,t-1}^{\\prime}/\\lambda)\\nabla\\ell(w_{i-1}^{\\prime};\\mathcal{D}_{i})\\right\\|^{2}}\\\\ &{+\\displaystyle\\frac{8}{\\beta_{3}}\\beta_{3}^{2}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We address each term as follows. $\\Phi$ can be bounded as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{\\sum}\\left[8\\beta_{3}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}\\exp(u_{i,t-1}^{r}/\\lambda)\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathcal{D}_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}\\exp(\\ell(\\bar{\\mathbf{w}}_{t-1}^{r};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathcal{D}_{i})\\right.\\right.}\\\\ &{\\displaystyle\\left.\\leq\\beta_{3}\\frac{1}{N}\\sum_{i=1}^{N}C_{\\ell}^{2}C_{1}^{2}\\|u_{i,t-1}^{r}-\\ell(\\bar{\\mathbf{w}}_{t-1}^{r};\\mathcal{D}_{i})\\|^{2}.\\right.}\\\\ &{\\displaystyle\\left.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\alpha_{4}\\times\\nabla\\bar{\\mathbf{\\Psi}}_{i,t}^{r}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\circleddash$ can be bounded as ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\left[8\\beta_{3}\\left|\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{2}}\\exp(\\ell(\\mathfrak{w}_{i-1}^{r};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(\\mathfrak{w}_{i,t-1}^{r};\\mathcal{D}_{i})-\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i}^{2}}\\exp(\\ell(\\mathfrak{w}_{i}^{r};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(\\mathfrak{w}_{i,t}^{r})\\right.}\\\\ &{\\displaystyle\\le\\mathbb{E}\\left[16\\beta_{3}\\left|\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{2}}\\exp(\\ell(\\mathfrak{w}_{i-1}^{r};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(\\mathfrak{w}_{i,t-1}^{r};\\mathcal{D}_{i})-\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{2}}\\exp(\\ell(\\mathfrak{w}_{i}^{r};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(\\mathfrak{w}_{i-1,t-1}^{r};\\mathcal{D}_{i})\\right.}\\\\ &{\\displaystyle\\left.+\\mathbb{E}\\left[16\\beta_{3}\\left|\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{2}}\\exp(\\ell(\\mathfrak{w}_{i}^{r};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(\\mathfrak{w}_{i,t-1}^{r};\\mathcal{D}_{i})-\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i}^{2}}\\exp(\\ell(\\mathfrak{w}_{i}^{r};\\mathcal{D}_{i})/\\lambda)\\nabla\\ell(\\mathfrak{w}_{i,t-1}^{r};\\mathcal{D}_{i})\\right.\\right.}\\\\ &{\\displaystyle\\left.\\left.\\le16\\beta_{3}c_{i}^{2}C_{i}^{2}\\mathbb{E}\\|\\mathfrak{w}_{i-1}^{r}-\\mathfrak{w}_{i}^{r}\\|^{2}+16\\beta_{3}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}C_{i}^{2}C_{i}^{2}C_{1}^{2}\\|v_{i,t}^{r}-\\bar{v}_{\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\circledast$ can be bounded as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[8\\beta_{3}\\left\\lVert\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{W_{i}^{\\epsilon}}\\exp(\\ell(\\widetilde w_{i-1}^{\\epsilon};\\mathcal D_{i})/\\lambda)\\nabla\\ell(w_{i-1}^{\\epsilon};\\mathcal D_{i})-\\nabla F(\\widetilde w_{i-1}^{\\epsilon})\\right\\rVert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[16\\beta_{3}\\Big\\lVert\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{W_{i}^{\\epsilon}}\\exp(\\ell(\\widetilde w_{i-1}^{\\epsilon};\\mathcal D_{i})/\\lambda)\\nabla\\ell(w_{i-1}^{\\epsilon};\\mathcal D_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{g}\\exp(\\ell(\\widetilde w_{i-1}^{\\epsilon};\\mathcal D_{i})/\\lambda)^{\\epsilon}}\\\\ &{\\quad+\\,\\mathbb{E}\\left[16\\beta_{3}\\Big\\lVert\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{g}\\Theta(\\ell(w_{i-1}^{\\epsilon};\\mathcal D_{i})/\\lambda)\\nabla\\ell(w_{i-1}^{\\epsilon};\\mathcal D_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{g}\\Theta(\\widetilde w_{i-1}^{\\epsilon})\\exp(\\ell(w_{i-1}^{\\epsilon};\\mathcal D_{i})/\\lambda)}\\\\ &{\\leq32\\beta_{3}C_{1}^{2}C_{1}^{2}\\|\\widetilde w_{i}^{\\epsilon}-g(\\widetilde w_{i}^{\\epsilon})\\|^{2}+32\\beta_{3}C_{1}^{2}C_{2}^{2}C_{3}^{2}\\|\\widetilde w_{i-1}^{\\epsilon}-\\widetilde w_{i}^{\\epsilon}\\|^{2}+16\\beta_{3}C_{1}^{2}L_{\\epsilon}^{2}\\frac{1}{N}\\sum_{i=1}^{N}\\|w_{i,i-1}^{\\epsilon}-\\widetilde w_{i-1}^{\\epsilon}\\|}\\\\ &{\\stackrel{(a)}{\\leq}32\\beta_{3}C_{1}^{2}C_{2}^{2}\\|\\widetilde w_{i}^{\\epsilon}-g(\\widetilde w_{i}^{\\epsilon})\\|^{2}+32\\beta_{3}C_{1}^{2}C_{4}^{2}C_{2}^{2}\\sigma_{2}^{2}\\|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) follows from (35). $\\textcircled{4}$ can be bounded as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle3_{3}^{2}\\mathbb{E}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}\\exp(u_{i,t-1}^{r}/\\lambda)\\nabla\\ell({\\mathbf w}_{i,t-1}^{r};{\\mathbf z}_{i,t}^{r})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}\\exp(u_{i,t-1}^{r}/\\lambda)\\nabla\\ell({\\mathbf w}_{i,t-1}^{r};{\\mathcal D}_{i})\\right\\|^{2}}\\\\ &{\\displaystyle=\\beta_{3}^{2}\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\mathbb{E}\\left\\|\\frac{1}{v_{i,t}^{r}}\\exp(u_{i,t-1}^{r}/\\lambda)\\nabla\\ell({\\mathbf w}_{i,t-1}^{r};{\\mathbf z}_{i,t}^{r})-\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}\\exp(u_{i,t-1}^{r}/\\lambda)\\nabla\\ell({\\mathbf w}_{i,t-1}^{r};{\\mathcal D}_{i})\\right\\|^{2}}\\\\ &{\\displaystyle\\leq\\beta_{3}^{2}C_{1}^{2}\\frac{\\sigma^{2}}{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as machines are independent conditioned on iteration $t-1$ . ", "page_idx": 25}, {"type": "text", "text": "$\\mathfrak{H}$ can be bounded as ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\8\\beta_{s}\\bigg|\\frac{1}{N}\\sum_{i=1}^{N}\\exp(\\mathrm{tr}_{i,t-1}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}-\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{\\nu_{i}^{2}}\\exp(\\mathrm{tr}_{i,t-1}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}}\\\\ &{\\leq\\mathbb{E}\\bigg[8\\beta_{s}\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}C_{i}^{2}\\left\\lVert\\exp(\\mathrm{tr}_{i,t}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}-\\exp(\\mathrm{tr}_{i,t-1}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}\\right\\rVert^{2}\\bigg]}\\\\ &{\\leq\\mathbb{E}\\bigg[24\\beta_{s}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}C_{i}^{2}\\left\\lVert\\exp(\\mathrm{tr}_{i,t}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}-\\exp(\\mathrm{tr}_{i,t}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}\\right\\rVert^{2}\\bigg]}\\\\ &{\\quad+\\mathbb{E}\\bigg[24\\beta_{s}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}C_{i}^{2}\\left\\lVert\\exp(\\mathrm{tr}_{i,t-1}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}-\\exp(\\mathrm{tr}_{i,t-1}^{\\prime}/\\lambda)\\nabla\\{(\\mathbf{w}_{i,t-1}^{\\prime};\\lambda)\\mathbf{z}_{i,t}^{\\tau}\\}\\right\\rVert^{2}\\bigg]}\\\\ &{\\quad+\\mathbb{E}\\bigg[24\\beta_{s}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^ \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality uses $v_{i,t}^{r}\\geq1$ as $\\ell(\\cdot)\\geq0$ . ", "page_idx": 25}, {"type": "text", "text": "$F(\\mathbf{w})$ is $L_{F}:=C_{f}L_{g}+C_{g}^{2}L_{f}$ -smooth. With $\\eta\\leq\\beta_{3}/(3L_{F}^{2}),$ $\\circled{6}$ can be bounded as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(1+\\frac{2}{\\beta_{3}}\\right)\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})-\\nabla F(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}\\leq\\frac{3}{\\beta_{3}}L_{F}^{2}\\mathbb{E}\\|\\bar{\\mathbf{w}}_{t-1}^{r}-\\bar{\\mathbf{w}}_{t}^{r}\\|^{2}=\\frac{3}{\\beta_{3}}L_{F}^{2}\\eta^{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t}^{r}\\|^{2}}\\\\ {\\displaystyle\\leq\\eta\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t}^{r}-\\bar{\\mathbf{m}}_{t-1}^{r}+\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})+\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}}\\\\ {\\displaystyle\\leq3\\eta\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t}^{r}-\\bar{\\mathbf{m}}_{t-1}^{r}\\|^{2}+3\\eta\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+3\\eta\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}}\\\\ {\\displaystyle\\overset{(a)}{\\leq}3\\eta(4\\beta_{3}^{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+4\\beta_{3}^{2}\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{m}}_{t-1}^{r})\\|^{2}+4\\beta_{3}^{2}C_{1}^{2}(C_{\\ell}^{2}+\\sigma^{2}))}\\\\ {\\displaystyle\\quad+3\\eta\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+3\\eta\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}}\\\\ {\\displaystyle\\leq4\\eta\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+4\\eta\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{w}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (a) uses ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t}^{r}-\\bar{\\mathbf{m}}_{t-1}^{r}\\|^{2}=\\mathbb{E}\\|(1-\\beta_{3})\\bar{\\mathbf{m}}_{t-1}^{r}+\\beta_{3}\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}g(u_{i,t}^{r})\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})-\\bar{\\mathbf{m}}_{t-1}^{r}\\|^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\leq2\\beta_{3}^{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t-1}^{r}\\|^{2}+2\\beta_{3}^{2}\\mathbb{E}\\|\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\frac{1}{v_{i,t}^{r}}g(u_{i,t}^{r})\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})\\|^{2}}\\\\ {\\displaystyle\\leq4\\beta_{3}^{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+4\\beta_{3}^{2}\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{m}}_{t-1}^{r})\\|^{2}+4\\beta_{3}^{2}C_{1}^{2}(C_{\\ell}^{2}+\\sigma^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\bar{\\mathbf{m}}_{t}^{r}-\\nabla F(\\bar{\\mathbf{v}}_{t}^{r})\\|^{2}\\leq(1-\\frac{\\beta_{3}}{2})\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+\\beta_{3}\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}C_{\\ell}^{2}C_{1}^{2}\\|u_{i,t-1}^{r}-\\ell(\\bar{\\mathbf{w}}_{t-1}^{r};\\mathcal{D}_{i})\\|}\\\\ {\\displaystyle+16\\beta_{3}C_{\\ell}^{2}C_{1}^{2}\\eta^{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t}^{r}\\|^{2}+16\\beta_{3}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}C_{\\ell}^{2}C_{1}^{2}\\|v_{i,t}^{r}-\\bar{v}_{t}^{r}\\|^{2}}\\\\ {\\displaystyle+32\\beta_{3}C_{1}^{2}C_{\\ell}^{2}\\|\\bar{\\mathbf{v}}_{t}^{r}-g(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}+32\\beta_{3}C_{1}^{2}C_{\\ell}^{2}C_{2}^{2}\\eta^{2}\\|\\bar{\\mathbf{m}}_{t}^{r}\\|^{2}}\\\\ {\\displaystyle\\quad+64C_{\\ell}^{2}\\eta^{4}I^{2}\\sum_{\\tau=1}^{t-1}\\mathbb{E}\\|\\mathbf{m}_{i,\\tau}^{r}-\\bar{\\mathbf{m}}_{\\tau}^{r}\\|^{2}+96C_{\\ell}^{2}\\eta^{2}I\\displaystyle\\sum_{\\tau=1}^{t-1}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{\\tau}^{r}\\|^{2}+\\beta_{3}^{2}C_{1}^{2}\\frac{\\sigma^{2}}{N}}\\\\ {\\displaystyle+48\\beta_{3}C_{2}^{2}C_{1}^{2}\\sigma^{2}+24\\beta_{3}L_{\\ell}^{2}C_{1}^{2}C_{0}^{2}\\beta_{1}^{2}}\\\\ {\\displaystyle+4\\eta\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We conclude the proof by setting the parameters as in the Lemma. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 5.3. Using Lemma 5.2, with $\\beta_{2}=O(\\beta_{3})$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{R I}\\displaystyle\\sum_{r=1}^{R}\\displaystyle\\sum_{t=1}^{I}\\displaystyle\\frac{\\beta_{3}}{2}\\|\\bar{\\mathbf{m}}_{t-1}^{0}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{0})\\|^{2}\\leq O\\bigg(\\frac{\\mathbb{E}\\|\\bar{\\mathbf{m}}_{0}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{0}^{r})\\|^{2}}{R I}+2\\beta_{3}^{3}I^{2}G^{2}+\\beta_{3}^{2}C_{2}\\frac{\\sigma^{2}}{N}}\\\\ &{\\displaystyle+\\,\\frac{1}{R K N}\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}\\|u_{i,0}^{0}-\\nabla F(\\bar{\\mathbf{w}}_{0}^{0})\\|^{2}+\\displaystyle\\frac{1}{R I}\\displaystyle\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\eta\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+\\beta_{1}^{2}\\sigma^{2}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using $L_{F}$ -smooth of $F$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle{\\cal F}(\\overline{{\\mathbf{w}}}_{t}^{r})\\leq{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\nabla{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})^{\\top}(\\overline{{\\mathbf{w}}}_{t}^{r}-\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\frac{L_{F}}{2}\\|\\overline{{\\mathbf{w}}}_{t}^{r}-\\overline{{\\mathbf{w}}}_{t-1}^{r}\\|^{2}}\\\\ &{\\displaystyle={\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})-\\eta\\nabla{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})^{\\top}\\overline{{\\mathbf{m}}}_{t}^{r}+\\frac{L_{F}}{2}\\eta^{2}\\|\\overline{{\\mathbf{n}}}_{t}^{r}\\|^{2}}\\\\ &{\\displaystyle\\leq{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})-\\eta\\nabla{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})^{\\top}(\\overline{{\\mathbf{m}}}_{t}^{r}-\\nabla{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\nabla{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r}))}\\\\ &{\\displaystyle~~+{\\cal L}_{F}\\eta^{2}(\\|\\overline{{\\mathbf{n}}}_{t}^{r}-\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2}+\\|\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2})}\\\\ &{\\displaystyle\\leq{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})-\\eta\\|\\nabla{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2}-\\eta\\nabla{\\cal F}(\\overline{{\\mathbf{w}}}_{t-1}^{r})^{\\top}(\\overline{{\\mathbf{m}}}_{t}^{r}-\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r}))}\\\\ &{\\displaystyle~~+\\frac{\\eta}{4}(\\|\\overline{{\\mathbf{n}}}_{t}^{r}-\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2}+\\|\\nabla F(\\overline{{\\mathbf{w}}}_{t-1 \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla F(\\tilde{\\mathbf{w}})\\Vert^{2}\\right]=\\mathbb{E}\\left[\\frac{1}{R I}\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\Vert\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\Vert^{2}\\right]\\le O\\left(\\frac{1}{\\eta R I}+\\beta_{1}\\sigma^{2}+\\beta_{3}^{2}I^{2}+\\eta L_{F}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We conclude the proof by setting the parameters as the theorem. ", "page_idx": 27}, {"type": "text", "text": "C Analysis of FGDRO-KL-Adam ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 6.2. Lemma B.1, Lemma B.2 and Lemma 5.2 still hold. Specifically, denoting $\\bar{\\mathbf{w}}_{t}^{r}=\\frac{1}{N}\\sum_{i=1}^{N}\\bar{\\mathbf{w}}_{i,t}^{r}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\|u_{i,t}^{r}-\\ell(\\bar{\\mathbf{w}}_{t}^{r};\\mathcal{D}_{i})\\|^{2}}\\\\ {\\displaystyle\\leq(1-\\beta_{1})\\|u_{i,t-1}^{r}-\\ell(\\bar{\\mathbf{w}}_{t-1}^{r};\\mathcal{D}_{i})\\|^{2}+6\\beta_{1}\\eta^{2}I^{2}C_{\\ell}^{2}G^{2}+\\beta_{1}^{2}\\sigma^{2}+\\frac{\\eta^{2}}{\\beta_{1}}C_{2}\\|\\bar{\\mathbf{m}}_{t}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}}\\\\ {\\displaystyle\\quad+\\,\\frac{\\eta^{2}}{\\beta_{1}}C_{2}\\|\\nabla F(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (a) holds due to the fact that $\\mathbb{E}_{t-1}[\\ell(\\mathbf{w}_{i,t}^{r};\\mathbf{z}_{i,t}^{r})-\\ell(\\mathbf{w}_{i,t}^{r};\\mathcal{D}_{i})]=0.$ . ", "page_idx": 27}, {"type": "text", "text": "And we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{v}_{t}^{r}-g(\\bar{\\mathbf{w}}_{t}^{r})\\|^{2}\\leq(1-\\beta_{2})\\|\\bar{v}_{t-1}^{r}-g(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+3\\beta_{2}\\frac{1}{N}\\sum_{i=1}^{N}C_{1}\\|u_{i,t}^{r}-\\ell(\\mathbf{w};\\mathcal{D}_{i})\\|^{2}+\\frac{3}{\\beta_{2}}C_{g}^{2}\\|\\bar{\\mathbf{w}}_{t}^{r}-\\bar{\\mathbf{v}}_{t}^{r}\\|^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $C_{1}=\\exp(C_{0}/\\lambda)$ . ", "page_idx": 27}, {"type": "text", "text": "Moreover, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{R I}\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\frac{\\beta_{3}}{2}\\mathbb{E}\\|\\bar{\\mathbf{m}}_{t-1}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}\\leq O\\bigg(\\frac{\\mathbb{E}\\|\\bar{\\mathbf{m}}_{0}^{r}-\\nabla F(\\bar{\\mathbf{w}}_{0}^{r})\\|^{2}}{R I}+2\\beta_{3}^{3}I^{2}G^{2}+\\beta_{3}^{2}C_{2}\\frac{\\sigma^{2}}{N}}\\\\ &{\\displaystyle+\\,\\frac{1}{R K N}\\sum_{i=1}^{N}\\mathbb{E}\\|u_{i,0}^{0}-\\nabla F(\\bar{\\mathbf{w}}_{0}^{0})\\|^{2}+\\frac{1}{R I}\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\eta\\mathbb{E}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}+\\beta_{1}^{2}\\sigma^{2}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using $L_{F}$ -smooth of $F$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\gamma}(\\overline{{\\mathbf{w}}}_{t}^{r})\\leq F(\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})^{\\top}(\\overline{{\\mathbf{w}}}_{t}^{r}-\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\frac{L}{2}\\|\\overline{{\\mathbf{w}}}_{t}^{r}-\\overline{{\\mathbf{w}}}_{t-1}^{r}\\|^{2}}\\\\ &{\\leq F(\\overline{{\\mathbf{w}}}_{t-1}^{r})-\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})^{\\top}(\\bar{\\eta}_{t}\\circ\\mathbf{\\hat{m}}_{t}^{r})+\\frac{L}{2}\\|\\bar{\\eta}^{2}\\circ\\mathbf{\\hat{m}}_{t}^{r}\\|^{2}}\\\\ &{=F(\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\frac{1}{2}\\|\\sqrt{\\bar{\\eta}_{t}}\\circ\\left(\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})-\\mathbf{\\hat{m}}_{t}^{r}\\right)\\|^{2}-\\frac{1}{2}\\|\\sqrt{\\bar{\\eta}_{t}}\\circ\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2}+\\frac{L}{2}\\|\\bar{\\eta}^{2}\\circ\\bar{\\mathbf{m}}_{t}^{r}\\|^{2}-\\frac{1}{2}\\|\\sqrt{\\bar{\\eta}_{t}}\\circ\\left(\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\right)}\\\\ &{\\leq F(\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\frac{\\eta}{2\\tau}\\|\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})-\\mathbf{\\hat{m}}_{t}^{r}\\|^{2}-\\frac{\\eta}{2(G+\\tau)}\\|\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})\\|^{2}+\\frac{\\eta^{2}L/\\tau^{2}-\\eta/(G+\\tau)}{2}\\|\\bar{\\mathbf{m}}_{t}^{r}\\|^{2}}\\\\ &{\\leq F(\\overline{{\\mathbf{w}}}_{t-1}^{r})+\\frac{\\eta}{\\tau}\\|\\nabla F(\\overline{{\\mathbf{w}}}_{t-1}^{r})- \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{R I}\\sum_{r=1}^{R}\\sum_{t=1}^{I}\\|\\nabla F(\\bar{\\mathbf{w}}_{t-1}^{r})\\|^{2}\\right]\\leq O\\left(\\frac{1}{\\eta R I}+\\beta_{1}\\sigma^{2}+\\beta_{3}^{2}I^{2}G^{2}+\\eta L G^{2}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We conclude the proof by setting the parameters as the theorem. ", "page_idx": 28}, {"type": "text", "text": "D LocalAdam Algorithm ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we present Algorithm 4, which uses Adam type updates in local steps to solve an ERM problem. ", "page_idx": 28}, {"type": "text", "text": "Algorithm 4 LocalAdam   \n1: Initialization: w\u00af1, m\u00af1, \u00afq1   \n2: for $r=1,...,R$ do   \n3: $\\mathbf{w}_{i,0}^{r}=\\bar{\\mathbf{w}}^{r}$ , $\\mathbf{m}_{i,0}^{r}=\\bar{\\mathbf{m}}^{r}$ , $\\mathbf{q}_{i,0}^{r}=\\bar{\\mathbf{q}}^{r}$   \n4: for $t=1,...,I$ do   \n5: Each machine samples data $\\mathbf{z}_{i,t}^{r}$   \n6: $\\begin{array}{r l}&{\\mathbf{h}_{i,t}^{r}=\\nabla\\ell(\\mathbf{w}_{i,t-1}^{r};\\mathbf{z}_{i,t}^{r})}\\\\ &{\\mathbf{m}_{i,t}^{r}\\!=\\!(1\\!-\\!\\beta_{3})\\mathbf{m}_{i,t-1}^{r}+\\beta_{3}\\mathbf{h}_{i,t}^{r},\\mathbf{q}_{i,t}^{r}=(1\\!-\\!\\beta_{4})\\mathbf{q}_{i,t-1}^{r}+\\beta_{4}(\\mathbf{h}_{i,t}^{r})^{2}}\\\\ &{\\mathbf{w}_{i,t}^{r}=\\mathbf{w}_{i,t-1}^{r}-\\eta\\frac{\\mathbf{m}_{i,t}^{r}}{\\sqrt{\\mathbf{q}_{i,t}^{r}}+\\tau}}\\\\ &{\\;.}\\end{array}$   \n7:   \n8:   \n9: end for   \n10: $\\bar{\\mathbf{w}}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{w}_{i,I}^{r},\\bar{\\mathbf{m}}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{m}_{i,I}^{r}\\;\\mathrm{and}\\;\\bar{\\mathbf{q}}^{r+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{q}_{i,I}^{r}$   \n11: end for ", "page_idx": 28}, {"type": "text", "text": "E Statistics of Datasets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Table 4 summarizes the sizes of the datasets used. Table 5 summarizes the client imbalance ratio and the class imbalance ratio. The client imbalance ratio represents the ratio between the number of training samples on the client with the most data and the client with the least data, and the class imbalance ratio reflects the ratio of training data in the largest to the smallest classes in classification tasks. ", "page_idx": 28}, {"type": "text", "text": "F Running Time ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Running time is reported in Tabel 6. Each algorithm was run on a high performance cluster where each machine uses a NVIDIA A100 GPU. ", "page_idx": 28}, {"type": "text", "text": "Table 4: Number of data for each split, with the number of training domains in the brackets. Number of training clients in the last column, with data from the same training domain to be on the same client. ", "page_idx": 29}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/2fe6f91c0b6aa4bb0fa2cf3c16304caae430b0708887d9f8798d84286c7dfb93.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/5efe4468418caa51f2e6b8f85a38bf9ff00c338622a306f3ec2aadf7021fe74b.jpg", "table_caption": ["Table 5: Imbalance Ratio "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "G Experiments on Cifar 10 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We create an imbalance dataset by reducing the data of 5 classes by $80\\%$ and then distributed the data across 100 clients according to two different Dirichlet distributions: Dirichlet (0.3) and Dirichlet (10), using code released by [61]. We use a two layer CNN as the model. Results of algorithms are summarized in Table 7. Figure 2 and Figure 2 illustrate the communication complexity of each method by comparing the worst-case testing accuracy against the number of local updates and the communicated data sizes. ", "page_idx": 29}, {"type": "image", "img_path": "xNZEjFe0mh/tmp/d37f699550e4a97ccd4ff9856c72fd73fe60688b017b809a6b85e4598d8e1bf1.jpg", "img_caption": ["Figure 2: Convergence on Imbalanced Cifar10 with Dirichlet(0.3) "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "H On the Statistical Significance of Proposed Algorithms ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In Table 8, 9 and 10, we evaluate the statistical significance of our algorithms\u2019 advantages over the baselines. In each cell, the three letters (representing FGDRO-CVaR, FGDRO-KL, and FGDRO-KLAdam, respectively) indicate whether each of our algorithms has outperformed the corresponding baseline in that row with a confidence level greater than $95\\%$ (p-value less than 0.05). \u2018Y\u2019 indicates Yes, and \u2018N\u2019 indicates No. ", "page_idx": 29}, {"type": "text", "text": "Table 6: Average running time of federated algorithms. We report running (in hours) for each algorithm to finish (200K iterations for Pile data and 20K for others). ", "page_idx": 30}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/1cd5316de2c507ceb6bb70b6c05310fae134fa045366f1943382f63c8f326a6d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/9152332f256400bd701f6c610d47daa9717a3e63b1205f3e087f004f67fb4cbe.jpg", "table_caption": ["Table 7: Imbalanced Cifar10, data alloacted to clients accoring to Dirichlet distributions "], "table_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "xNZEjFe0mh/tmp/bd72b52d9d07d6007dd6f8496b1a74c3d61664ae2289c432b5e07d19f2bc5d64.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/8943bab0ecf90266d0773f294bfde977134aaa0407d56b03cfdbfdf3f7ecdf5d.jpg", "table_caption": ["Figure 3: Convergence on Imbalanced Cifar10 with Dirichlet(10) ", "Table 8: Statistical Confidence on Imbalanced Cifar10 "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/26e6f19d504192e1e1c909213d24bb3d891f2cf27bc7808dc60cf391d8e2dbcf.jpg", "table_caption": ["Table 9: Statistical Confidence on Piles and CivilComments "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "xNZEjFe0mh/tmp/1f3d2976309d27fddc5c217593f95c38d890ff3459afe9b7411bcadb057721d5.jpg", "table_caption": ["Table 10: Statistical Confidence on Camelyon17, iWildCam2020, and PovertyMap "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The abstract and introduction are accurate summary of the paper and well supported by other sections of the paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Limitations are discussed in Section 8. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Assumptions are discussed in Section 3,4,5,6. Theorems are presented in Section 4,5,6. Proofs are shown in details in Appendix A, B, C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The setting and parameter tuning scope are discussed in Section 7. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: All used data are publicly available. Code will be released later. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: These details are shown in Section 7. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Experiments are repeated for multiple times with different random seed, and error bars are reported in experimental results in Section 7. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Computing resource and time of execution are reported in Appendix F. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper conform with the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The broader impacts have been discussed in Section 9 ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not release any new data or models. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The assets we use are all publicly available and properly credited. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: No new assets are introduced in this paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]