{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, a foundational language model, which is directly compared against in the efficiency experiments of the current paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper introduces Vision Transformers, a model architecture used as a baseline and compared against in multiple experiments in the current paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-00-00", "reason": "This paper introduces GPT-2, a language model that is used as a subject of the efficiency experiments of the current paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "Monarch: Expressive structured matrices for efficient and accurate training", "publication_date": "2022-00-00", "reason": "This paper introduces the Monarch matrix, a structured matrix design that is compared against in the current paper for its efficiency and flexibility."}, {"fullname_first_author": "Changwoo Lee", "paper_title": "Differentiable learning of generalized structured matrices for efficient deep neural networks", "publication_date": "2024-00-00", "reason": "This paper introduces the GBLR matrix, a structured matrix design that is compared against in the current paper for its efficiency and flexibility."}]}