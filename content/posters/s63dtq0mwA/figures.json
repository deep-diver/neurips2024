[{"figure_path": "s63dtq0mwA/figures/figures_1_1.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in two multi-modal large language models (MLLMs), LLaVA-7B and LLaMA (Vicuna)-7B, when performing a visual question answering task.  The x-axis represents the layers of the model, and the y-axis represents the causal importance, indicating how much each layer contributes to the final output.  The figure shows that MLLMs tend to retrieve information from much earlier layers (layers 1-4 for LLaVA-7B) compared to single-modal LLMs which use middle layers (layers 4-7 for LLaMA). The different window sizes needed to identify causal layers also highlight a difference in how these models utilize parametric memory.  The plot visualizes this using heatmaps, showing the most influential layers for the given constraints within the question and corresponding image.", "section": "3 A Multi-modal Constraint-based Framework"}, {"figure_path": "s63dtq0mwA/figures/figures_2_1.jpg", "caption": "Figure 2: We introduce MULTIMODALCAUSALTRACE, a causal tracing method to understand information storage in MLLMs. A clean model is corrupted by replacing the question's constraint with an incorrect one for the given image (e.g. \"This place\" -> \"Paris city\" for an image of \"Vinson Massif\"). The activations of windows of layers are then iteratively copied from the clean to the corrupted model until the corrupted model restores its output probability to match the clean model's.", "description": "This figure illustrates the MULTIMODALCAUSALTRACE method.  A clean model's response to a question with a visual constraint is compared to a corrupted model where the constraint has been changed (e.g., \"This place\" changed to \"Paris City\"). Iterative copying of layer activations from the clean model to the corrupted model helps identify the layers causally responsible for the correct answer. The plot shows the causal importance of layers for the clean model.", "section": "A Constraint-Based Framework for Studying Information Storage and Transfer in MLLMs"}, {"figure_path": "s63dtq0mwA/figures/figures_3_1.jpg", "caption": "Figure 3: Information to answer a visual question with a single constraint is mainly retrieved from early MLP and self-attention layers in MLLMs. MULTIMODALCAUSALTRACE obtains high indirect estimation effect values in LLaVa's early MLP and self-attention blocks corresponding to the visual constraint, across all 3 datasets in VQA-Constraints. This suggests these layers are causally important for information storage. The causal traces emerge with a window size of 3 (see results with a window size of 1 in Appendix C).", "description": "The figure shows the causal importance of different layers (MLP and self-attention) in two multi-modal large language models (MLLMs), LLaMA and LLaVA, when answering visual questions with single constraints.  It uses the MULTIMODALCAUSALTRACE method, demonstrating that early layers are crucial for information storage in MLLMs, unlike LLMs which rely more on mid-layer MLPs.  The results are consistent across three different datasets.", "section": "3.1 A Multi-modal Constraint-based Framework"}, {"figure_path": "s63dtq0mwA/figures/figures_4_1.jpg", "caption": "Figure 4: Information to answer a visual question with a visual and textual constraint is retrieved from early and middle MLP and self-attention layers in MLLMs. This suggests that meeting multiple constraint requires more parametric memory compared to single constraints. We show that MULTIMODALCAUSALTRACE obtains high indirect estimation effect values in the early and middle layers in LLaVa's on the OK-VQA dataset in VQA-Constraints (see multi-constraint results from the Movies dataset in Appendix F).", "description": "This figure visualizes the results of applying the MULTIMODALCAUSALTRACE method to visual questions that involve both visual and textual constraints. It shows that, unlike single-constraint questions where information retrieval happens mainly from early layers, multi-constraint questions require information from both early and middle MLP and self-attention layers. This difference indicates the necessity of more memory resources for processing questions with multiple constraints.", "section": "4 Key Findings in how MLLMs Store and Transfer Information"}, {"figure_path": "s63dtq0mwA/figures/figures_6_1.jpg", "caption": "Figure 5: The late visual tokens are primarily responsible for transferring information from the image to the early causal layers, via the first self-attention layer. We visualize attention contributions (see Eq.(1)) from the visual tokens to the visual constraint token averaged across the three datasets in VQA-Constraints. We also find that causal traces can be extracted from LLaVA with a minimum window size of 1, while LLaMa requires a minimum window size of 5 to obtain any significant causal traces.", "description": "This figure visualizes attention contributions from visual tokens to the visual constraint token across three datasets (Movies, Multi-Known, OKVQA). It shows that the late visual tokens (indices 540-576 out of 576) are most influential in transferring information to early causal layers via the first self-attention layer.  The difference in window size needed to extract causal traces between LLaVA (minimum 1) and LLaMA (minimum 5) is highlighted, suggesting a difference in how these models store information.", "section": "Studying Information Transfer in MLLMs with Attention Contributions"}, {"figure_path": "s63dtq0mwA/figures/figures_7_1.jpg", "caption": "Figure 6: MULTEDIT can correct error cases (left) and insert long-tailed factual knowledge (right) by editing the early causal MLP layers in an MLLM. We use the average probability of the correct token O* as the metric for Editing Efficacy and Generalization, and VQA-Accuracy for Specificity (higher the better for all).", "description": "This figure compares the performance of MULTEDIT against fine-tuning baselines (with and without constraints) for correcting errors and inserting long-tailed knowledge.  The results show MULTEDIT's superiority across various metrics: Editing Efficacy (correcting errors), Generalization (applying corrections to similar but slightly different questions), and Specificity (maintaining the accuracy on unrelated questions).  It highlights MULTEDIT's ability to effectively edit a model's causal layers for improved performance.", "section": "5 Correcting and Inserting Long-Tailed Information in MLLMs"}, {"figure_path": "s63dtq0mwA/figures/figures_12_1.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in two multi-modal large language models (MLLMs), LLaVA-7B and LLaMA (Vicuna)-7B, and contrasts them with their single-modal counterparts.  It shows that MLLMs tend to retrieve information from much earlier layers (layers 1-4) compared to LLMs (layers 4-7). The window size needed to identify these causal sites is also smaller for MLLMs (window size of 1) than for LLMs (window size of 5).  This suggests a difference in how MLLMs store and access factual information.", "section": "3.1 A Multi-modal Constraint-based Framework"}, {"figure_path": "s63dtq0mwA/figures/figures_13_1.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in two types of large language models (LLMs): Multimodal LLMs (MLLMs) and LLMs, using two specific models, LLaVa-7B and LLaMA (Vicuna)-7B respectively.  The causal importance is represented by the probability that manipulating the activation of specific layers affects the model's output. The figure shows that MLLMs (LLaVa-7B in this case) primarily use the early layers (layers 1-4) for information retrieval, whereas LLMs (LLaMA in this case) primarily utilize the middle layers (layers 4-7).  Further, the figure highlights the varying window sizes required to identify causal layers in these two model types.", "section": "Key Findings in how MLLMs Store and Transfer Information"}, {"figure_path": "s63dtq0mwA/figures/figures_13_2.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in two multi-modal large language models (MLLMs), LLaVA-7B and LLaMA (Vicuna)-7B, and contrasts them with a large language model (LLM), LLaMA.  The causal importance is measured by the indirect estimation effect on the model's output.  It shows that MLLMs utilize earlier layers (layers 1-4) for information retrieval compared to LLMs which rely on mid-layer MLPs (layers 4-7). The size of the layer window considered to identify causal sites also differs between MLLMs and LLMs.", "section": "4 Key Findings in how MLLMs Store and Transfer Information"}, {"figure_path": "s63dtq0mwA/figures/figures_14_1.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in two multi-modal large language models (MLLMs), LLaVa-7B and LLaMA (Vicuna)-7B, and contrasts them with a large language model (LLM). The causal importance is measured using a method called causal tracing. The figure shows that MLLMs tend to retrieve information from much earlier layers (layers 1-4) than LLMs (layers 4-7).  The window size needed to identify causal sites also differs between the models, with LLaVa-7B requiring a smaller window size (1) than LLaMA (Vicuna)-7B (5). This suggests that the information storage and retrieval mechanisms may be different for MLLMs compared to LLMs.", "section": "4 Key Findings in how MLLMs Store and Transfer Information"}, {"figure_path": "s63dtq0mwA/figures/figures_14_2.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in two multi-modal large language models (MLLMs), LLaVa-7B and LLaMA (Vicuna)-7B, and one large language model (LLM), LLaMA.  The causal importance is measured using a causal tracing method, and it shows that MLLMs utilize information from much earlier layers than LLMs. LLaVa-7B, in particular, exhibits high causal importance in the very first MLP layers, whereas LLaMA relies more on mid-layer MLPs. The difference in window sizes needed to observe the effects is another notable point.", "section": "Multi-modal Constraint-based Framework"}, {"figure_path": "s63dtq0mwA/figures/figures_15_1.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in two multi-modal large language models (MLLMs), LLaVA-7B and LLaMA (Vicuna)-7B, and contrasts them with a large language model (LLM).  The causal importance is measured by assessing the impact of each layer on the final output.  The figure shows that MLLMs rely on information stored in much earlier layers (layers 1-4 for LLaVA-7B), as opposed to LLMs, which typically use mid-layer MLPs.  It highlights the difference in the extent of parameter memory required for the two model types.", "section": "Key Findings in how MLLMs Store and Transfer Information"}, {"figure_path": "s63dtq0mwA/figures/figures_15_2.jpg", "caption": "Figure 1: MLLMs retrieve information from earlier internal layers compared to their LLM counterparts. We find that very early MLP layers [1-4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4-7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.", "description": "This figure compares the causal importance of different layers in Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs).  The causal importance is measured by how much the activation of a layer affects the model's final output.  The figure shows that MLLMs, specifically LLaVa-7B, rely on much earlier layers (MLP layers 1-4) for information storage compared to LLMs, which rely on mid-layer MLPs (layers 4-7).  Furthermore, LLMs may require a larger window size to identify these causal sites, indicating a difference in how information is stored and processed across these model types.", "section": "Key Findings in how MLLMs Store and Transfer Information"}, {"figure_path": "s63dtq0mwA/figures/figures_16_1.jpg", "caption": "Figure 12: MULTIMODALCAUSALTRACE with Window Size 6: Early and mid MLP layers are causal. Qualitative tracing results for examples with multiple constraint questions (containing a visual constraint and a text constraint). Here MULTIMODALCAUSALTRACE is performed at the text-constraint position. For multi-constraint questions, a larger window size is needed to recover any causal states.", "description": "This figure shows the results of applying MULTIMODALCAUSALTRACE with a window size of 6 to examples with multiple constraints (visual and textual). Unlike single-constraint examples, where early layers were causal, multi-constraint examples show causality in both early and mid-layer MLPs and self-attention blocks.  This suggests that more layers are involved in processing information when multiple constraints need to be satisfied.", "section": "F Multi-constraint Questions Plots"}, {"figure_path": "s63dtq0mwA/figures/figures_16_2.jpg", "caption": "Figure 14: Editing early causal layers leads to better editing efficacy, though editing non-causal layers also leads to non-trivial editing efficacy. Similar results for language models have been highlighted in [10]. The editing is performed at the position of the last token of the visual constraint.", "description": "This figure shows the editing efficacy of MULTEDIT when applied to different layers of the model. The x-axis represents the layers of the model, and the y-axis represents the editing efficacy, which is a measure of how well the model is able to correct errors or insert new information. The figure shows that editing the early causal layers (layers 1-4) leads to the best editing efficacy. However, editing the middle or later layers can also lead to some improvement in editing efficacy, although it is less effective than editing the early causal layers. This suggests that the early causal layers play a crucial role in the model's ability to store and retrieve information.", "section": "5. MULTEDIT"}, {"figure_path": "s63dtq0mwA/figures/figures_17_1.jpg", "caption": "Figure 15: Correct answers have stronger attention contributions from the constraint tokens (X-axis) to the last token, when compared to incorrect answers on an average. Computed on the Multimodal Known dataset in VQA-Constraints. In particular the layer 16 and layer 17 are the distinguishing layers which have higher attention contributions in the correct answers.", "description": "This figure visualizes the attention contributions from constraint tokens to the last token in different layers for both correct and incorrect answers.  It shows that correct answers exhibit stronger attention contributions, particularly in layers 16 and 17, highlighting the role of these layers in distinguishing correct from incorrect responses.", "section": "H Information Transfer and Early Failure Mode Detection - Qualitative Plots"}, {"figure_path": "s63dtq0mwA/figures/figures_18_1.jpg", "caption": "Figure 16: Correct answers have stronger attention contributions from the constraint tokens (X-axis) to the last token, when compared to incorrect answers on an average. Computed on the Multimodal Known dataset in VQA-Constraints. In particular the layer 16 and layer 17 are the distinguishing layers which have higher attention contributions in the correct answers.", "description": "The figure shows a comparison of attention contributions between correct and incorrect answers for a visual question answering task.  The top plots display average attention contributions across layers for both correct and incorrect responses, highlighting significantly higher contributions in mid-layers (16 and 17) for correct answers. Below, heatmaps visualize attention contributions from constraint tokens (x-axis) to the final token (last token) across layers for both correct and incorrect cases. These heatmaps reinforce the observation of substantially stronger attention contributions in mid-layers (16 and 17) when the model generates correct answers.", "section": "H Information Transfer and Early Failure Mode Detection - Qualitative Plots"}, {"figure_path": "s63dtq0mwA/figures/figures_18_2.jpg", "caption": "Figure 17: Attention Contribution Metric although lags behind Confidence Metric, it can be used as a coarse \"early\" failure mode detection metric. Dataset used: Multimodal Known from VQA-Constraints. We use the average attention contributions from Layer 16 and Layer 17 for computing the failure mode metric.", "description": "This figure shows the performance of two metrics for predicting whether a model will generate a correct answer: the attention contribution metric and the confidence metric.  The attention contribution metric uses the average attention contributions from layers 16 and 17, while the confidence metric uses the model's confidence in its answer.  The AUROC (Area Under the Receiver Operating Characteristic curve) is shown for both metrics, indicating that the confidence metric is a stronger predictor of correctness than the attention contribution metric, although the attention contribution metric can still provide a useful early indicator of model failure.", "section": "4.4 Finding 4: Mid-layer self-attention contributions can be used to predict whether a MLLM will generate a correct answer, but model confidence is a more reliable predictor"}, {"figure_path": "s63dtq0mwA/figures/figures_19_1.jpg", "caption": "Figure 18: Qualitative Examples from the long-tailed knowledge editing subset. The questions are sourced from the questions involving landmarks in Encyclopedia-VQA.", "description": "This figure shows six examples of images and questions from the Encyclopedia-VQA dataset used in the paper's experiments on inserting long-tailed knowledge into the model. Each example contains an image of a landmark and a question asking for the landmark's location.  These examples represent the type of challenging questions that the model-editing technique was designed to handle.", "section": "I Editing Dataset"}, {"figure_path": "s63dtq0mwA/figures/figures_19_2.jpg", "caption": "Figure 18: Qualitative Examples from the long-tailed knowledge editing subset. The questions are sourced from the questions involving landmarks in Encyclopedia-VQA.", "description": "This figure shows six example images from the Encyclopedia-VQA dataset used to test the MULTEDIT model's ability to insert long-tailed knowledge.  Each image is accompanied by a question about the location (country) of the landmark shown. The questions are designed to be challenging for large language models, as they involve less common geographic knowledge.", "section": "I Editing Dataset"}, {"figure_path": "s63dtq0mwA/figures/figures_19_3.jpg", "caption": "Figure 18: Qualitative Examples from the long-tailed knowledge editing subset. The questions are sourced from the questions involving landmarks in Encyclopedia-VQA.", "description": "This figure shows six example images from the Encyclopedia-VQA dataset used in the paper's long-tailed knowledge editing experiments. Each image is accompanied by a question asking for the country where the landmark is located.  These examples highlight the challenge of handling less frequently seen landmarks (long-tailed data) that are not well represented in the typical training sets used for multi-modal large language models (MLLMs).", "section": "I Editing Dataset"}, {"figure_path": "s63dtq0mwA/figures/figures_19_4.jpg", "caption": "Figure 18: Qualitative Examples from the long-tailed knowledge editing subset. The questions are sourced from the questions involving landmarks in Encyclopedia-VQA.", "description": "This figure shows six example images from the Encyclopedia-VQA dataset used in the paper's long-tailed knowledge editing experiments. Each image is accompanied by a question about the location (country) of the landmark shown in the image. These examples illustrate the types of questions used to evaluate the model's ability to handle less commonly seen factual information.", "section": "I Editing Dataset"}, {"figure_path": "s63dtq0mwA/figures/figures_19_5.jpg", "caption": "Figure 18: Qualitative Examples from the long-tailed knowledge editing subset. The questions are sourced from the questions involving landmarks in Encyclopedia-VQA.", "description": "This figure shows six example images from the Encyclopedia-VQA dataset used in the paper's experiments on inserting long-tailed knowledge into MLLMs. Each image is accompanied by a question asking the country where the landmark is located.  These examples highlight the challenges of handling less common or rare landmarks during MLLM training and evaluation.", "section": "I Editing Dataset"}, {"figure_path": "s63dtq0mwA/figures/figures_19_6.jpg", "caption": "Figure 18: Qualitative Examples from the long-tailed knowledge editing subset. The questions are sourced from the questions involving landmarks in Encyclopedia-VQA.", "description": "This figure shows six example images from the Encyclopedia-VQA dataset used in the paper's long-tailed knowledge editing experiments. Each image is accompanied by a question about the location (country) of the landmark shown.  These examples highlight the challenging nature of the long-tailed knowledge questions that the MULTEDIT algorithm was tested on.", "section": "I Editing Dataset"}]