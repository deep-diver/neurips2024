[{"figure_path": "gJxEiRcnao/figures/figures_4_1.jpg", "caption": "Figure 1: The Counter-Hebb update rule in comparison with the classical Hebb rule. The classical Hebb rule (on the left), with a focus on a single upstream synapse Wij (outlined by a circle), connecting a pre-synaptic neuron aj with a post-synaptic neuron bi. The synapse Wij is updated based on the activity of both associated neurons aj and bi. While neuron aj is directly associated with the synapse Wij, neuron bi is assumed to transmit its information through propagation down the dendritic tree to synapse Wij (orange arrow). In contrast, the Counter-Hebb update rule (on the right), relies on a contribution from the counterpart downstream (marked in orange), mediated via lateral connections. Compared with the Hebb rule, the signal from aj is combined with the signal from neuron bi rather than neuron bi. Notably, the resulting Counter-Hebb rule naturally applies an identical update to both Wij and its counter synapse Wji.", "description": "The figure compares the classical Hebb rule and the proposed Counter-Hebb rule for synaptic weight updates.  The Hebb rule updates a synapse based on the pre- and post-synaptic neuron activations. The Counter-Hebb rule incorporates lateral connectivity, using the activation of a \"counter neuron\" in the opposite stream to modify the synapse, effectively providing feedback signals from the top-down pathway for synaptic modification.", "section": "4 Counter-Hebbian learning"}, {"figure_path": "gJxEiRcnao/figures/figures_6_1.jpg", "caption": "Figure 2: The instruction-based learning algorithm. The three columns represent three passes of our model (left to right): TD \u2192 BU \u2192 TD, where the first two passes provide a prediction output given an image and a task, and the last TD pass (in green frame) is used for learning. In inference, The BU visual process is guided by the TD network according to the given task. More specifically, The TD network propagates instruction signals downward followed by a guided BU process of the input image to compute predictions. By applying ReLU non-linearity, the input task selectively activates a subset of neurons (i.e. non-zero values), composing a sub-network within the full network. The BU network then processes an input image using a composition of ReLU and GaLU. The GaLU function (dashed arrows) gates the BU computation to operate only on the selected sub-network that was activated by the task. For learning, the same TD network is then reused to propagate prediction error signals with GaLU exclusively (no ReLU). Finally, the 'Counter-Hebb' learning rule adjusts both networks' weights based on the activation values of their neurons. Therefore, in contrast with standard models, the entire computation, including the learning, is carried out by neurons in the network, and no additional computation is used for learning (e.g. backpropagation)", "description": "This figure illustrates the three steps of the instruction-based learning algorithm. First, a top-down (TD) pass uses the instruction head to propagate the task representation along the TD network, activating a task-specific sub-network.  Next, a bottom-up (BU) pass uses the prediction head with ReLU and GaLU activation to process the input image based on the selected sub-network. Finally, another TD pass uses the prediction head with GaLU to propagate the error signal and update the weights using the Counter-Hebb rule.  In inference, only the first two steps are used.", "section": "5 Instruction-based learning"}, {"figure_path": "gJxEiRcnao/figures/figures_19_1.jpg", "caption": "Figure 3: MNIST results: comparing different weight decay values and presenting the mean performance including std per training epoch averaged across 5 runs.", "description": "This figure shows the training and testing accuracy and loss curves for the MNIST dataset using different weight decay values. The results are averaged over five runs, with standard deviations shown as shaded areas.  It illustrates the impact of weight decay on the convergence and performance of the Counter-Hebb learning algorithm, comparing it to symmetric and asymmetric weight scenarios.", "section": "6.1 Unguided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_20_1.jpg", "caption": "Figure 3: MNIST results: comparing different weight decay values and presenting the mean performance including std per training epoch averaged across 5 runs.", "description": "This figure shows the training and testing accuracy and loss for a MNIST image classification task using different weight decay values.  The results are averaged over five runs, and error bars represent the standard deviation.  It compares the performance of the Counter-Hebb learning algorithm with different levels of weight decay (WD) and contrasts these results to the performance of both a symmetric (identical BU and TD weights) and asymmetric (different BU and TD weights) model, highlighting the effect of weight symmetry and weight decay on the model's ability to learn.", "section": "6 Empirical results"}, {"figure_path": "gJxEiRcnao/figures/figures_21_1.jpg", "caption": "Figure 3: MNIST results: comparing different weight decay values and presenting the mean performance including std per training epoch averaged across 5 runs.", "description": "This figure shows the training and testing accuracy and loss for different weight decay values on the MNIST dataset.  It compares the performance of the symmetric (equivalent to backpropagation) and asymmetric versions of the Counter-Hebb learning algorithm. The shaded area represents the standard deviation across the 5 runs.  The results illustrate the impact of weight decay on the convergence and generalization performance of the learning algorithm.", "section": "6.1 Unguided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_22_1.jpg", "caption": "Figure 3: MNIST results: comparing different weight decay values and presenting the mean performance including std per training epoch averaged across 5 runs.", "description": "This figure displays the training and testing accuracy and loss for the MNIST dataset across different weight decay values. It compares the performance of symmetric and asymmetric models, providing a visual representation of how weight decay affects the learning process and model performance.", "section": "6.1 Unguided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_23_1.jpg", "caption": "Figure 3: MNIST results: comparing different weight decay values and presenting the mean performance including std per training epoch averaged across 5 runs.", "description": "The figure shows the training and testing accuracy and loss for MNIST image classification using different weight decay values.  The results are averaged over 5 runs, and the standard deviation is also shown.  Different weight decay values result in different training and testing performance, with the optimal setting found somewhere in between the extremes of no weight decay (WD 0.0) and high weight decay (WD 0.5). The plots illustrate how different weight decay levels impact the learning process, showing a tradeoff between preventing overfitting and ensuring sufficient learning.", "section": "6.1 Unguided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_24_1.jpg", "caption": "Figure 10: Multi-MNIST results: mean and std of the average task accuracy and loss per training epoch (starting from the 10th). Comparing different numbers of channels. On the left is the number of channels at the first convolution layer, while on the right is the number of channels at the second layer", "description": "The figure displays the training and testing accuracy and loss for the Multi-MNIST dataset using the proposed Counter-Hebb learning method, comparing different numbers of channels in the first and second convolutional layers.  The left plots show the training and testing accuracy over epochs, while the right plots display the corresponding training and testing loss.  The results are presented for both symmetric and asymmetric weight settings, providing insights into the impact of network architecture on model performance for this multi-task learning setting.", "section": "A.4.5 Multi-MNIST"}, {"figure_path": "gJxEiRcnao/figures/figures_25_1.jpg", "caption": "Figure 3: MNIST results: comparing different weight decay values and presenting the mean performance including std per training epoch averaged across 5 runs.", "description": "The figure shows the training and testing accuracy and loss curves for a MNIST image classification task using a model trained with different weight decay values.  The results are averaged over 5 runs, and error bars represent the standard deviation.  The plot helps to visualize how different weight decay values affect the training dynamics and generalization performance of the model. Different lines represent different weight decay values, and separate plots are provided for training and testing metrics.", "section": "6.1 Unguided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_26_1.jpg", "caption": "Figure 10: Multi-MNIST results: mean and std of the average task accuracy and loss per training epoch (starting from the 10th). Comparing different numbers of channels. On the left is the number of channels at the first convolution layer, while on the right is the number of channels at the second layer", "description": "The figure shows the results of applying the Counter-Hebb learning algorithm to the Multi-MNIST dataset.  It compares the performance of the model with varying numbers of channels in the convolutional layers.  The left panel shows the test accuracy and the right shows the test loss.  The results indicate how the model's performance changes with different numbers of channels for both symmetric and asymmetric weight initializations. The results highlight the influence of model capacity and weight symmetry on the model's ability to learn and generalize.", "section": "A.4.5 Multi-MNIST"}, {"figure_path": "gJxEiRcnao/figures/figures_27_1.jpg", "caption": "Figure 10: Multi-MNIST results: mean and std of the average task accuracy and loss per training epoch (starting from the 10th). Comparing different numbers of channels. On the left is the number of channels at the first convolution layer, while on the right is the number of channels at the second layer", "description": "This figure shows the results of the Multi-MNIST experiment, comparing the performance of the model with different numbers of channels in the convolutional layers.  The left plots display training and test accuracy, while the right plots show training and test loss. The different line colors represent models with varying numbers of channels in the first and second convolutional layers. The dashed lines represent models with asymmetric weights, while the solid lines depict models with symmetric weights.  The figure illustrates how model capacity affects performance and highlights the impact of weight symmetry.", "section": "A.4.5 Multi-MNIST"}, {"figure_path": "gJxEiRcnao/figures/figures_28_1.jpg", "caption": "Figure 12: CelebA results: mean and std of the average task accuracy and loss on the test set per training epoch (sampled every 5 epochs).", "description": "This figure shows the performance of the CelebA experiment across different weight symmetry settings. The x-axis represents the training epoch, sampled every 5 epochs. The y-axis shows the average task accuracy and loss on the test set.  Three lines represent the performance of the three different symmetry settings: symmetric weights (blue), multi-decoders (red), and asymmetric weights (green). The shaded regions around the lines indicate the standard deviation.  The results indicate that symmetric weights generally perform better, closely followed by the multi-decoder setup, while the asymmetric weight setting lags behind. ", "section": "6.2 Guided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_28_2.jpg", "caption": "Figure 12: CelebA results: mean and std of the average task accuracy and loss on the test set per training epoch (sampled every 5 epochs).", "description": "The figure shows the results of the CelebA experiment. The mean and standard deviation of the average task accuracy and loss on the test set are plotted for each training epoch. The results are sampled every 5 epochs. The figure helps to visualize the training process and the performance of the model over time.", "section": "6.2 Guided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_30_1.jpg", "caption": "Figure 3: MNIST results: comparing different weight decay values and presenting the mean performance including std per training epoch averaged across 5 runs.", "description": "This figure presents the results of experiments conducted on the MNIST dataset to evaluate the impact of different weight decay values on model performance.  The plot shows the training and testing accuracy and loss over epochs. Multiple lines represent different weight decay values, and shaded regions illustrate standard deviations across 5 runs. The figure helps to understand the effect of weight decay (a regularization technique) on model performance and generalization.", "section": "6.1 Unguided visual processing"}, {"figure_path": "gJxEiRcnao/figures/figures_31_1.jpg", "caption": "Figure 2: The instruction-based learning algorithm. The three columns represent three passes of our model (left to right): TD \u2192 BU \u2192 TD, where the first two passes provide a prediction output given an image and a task, and the last TD pass (in green frame) is used for learning. In inference, The BU visual process is guided by the TD network according to the given task. More specifically, The TD network propagates instruction signals downward followed by a guided BU process of the input image to compute predictions. By applying ReLU non-linearity, the input task selectively activates a subset of neurons (i.e., non-zero values), composing a sub-network within the full network. The BU network then processes an input image using a composition of ReLU and GaLU. The GaLU function (dashed arrows) gates the BU computation to operate only on the selected sub-network that was activated by the task. For learning, the same TD network is then reused to propagate prediction error signals with GaLU exclusively (no ReLU). Finally, the 'Counter-Hebb' learning rule adjusts both networks' weights based on the activation values of their neurons. Therefore, in contrast with standard models, the entire computation, including the learning, is carried out by neurons in the network, and no additional computation is used for learning (e.g., backpropagation)", "description": "This figure illustrates the instruction-based learning algorithm used in the paper. It shows how the model uses the top-down (TD) and bottom-up (BU) streams to achieve both guidance and learning in a single process. In inference, task instructions guide the visual process, while in training, the Counter-Hebb learning rule adjusts synaptic weights based on both prediction errors and task instructions.", "section": "5 Instruction-based learning"}, {"figure_path": "gJxEiRcnao/figures/figures_32_1.jpg", "caption": "Figure 16: Multi-MNIST (weak symmetry) results: mean and std of the average task accuracy and loss per training epoch (starting from the 10th). Comparing different magnitudes of noise in the weak symmetry case. The dashed line indicates the final performance of the symmetric case.", "description": "This figure shows the results of applying the Counter-Hebb learning algorithm with varying levels of noise added to the weight updates in the Multi-MNIST dataset. The x-axis represents training epochs, and the y-axis shows the average task accuracy and loss. Different colored lines represent different levels of noise (0.05, 0.1, 0.5). The dashed line represents the performance of the symmetric case, where no noise is added to the weight updates. The figure demonstrates the robustness of the Counter-Hebb learning algorithm to noise in weight updates, even with a significant amount of noise, as the performance is similar to the symmetric case. This suggests that exact weight symmetry is not critical for achieving performance comparable to backpropagation. ", "section": "A.5 Asymmetric weights"}, {"figure_path": "gJxEiRcnao/figures/figures_33_1.jpg", "caption": "Figure 17: Comparing different weight symmetry using ResNet18 on CIFAR10 and presenting the mean accuracy including std per training epoch averaged across 5 runs.", "description": "The figure compares the performance of different weight symmetry settings (symmetric, asymmetric, asymmetric with weight decay, noisy symmetric, weak symmetric) using ResNet18 on the CIFAR10 dataset.  It shows training accuracy and loss for each setting across 5 runs, demonstrating how weight symmetry affects performance and convergence. The symmetric case serves as a baseline for comparison, while other settings explore the tradeoff between biological plausibility and accuracy.  Note that Feedback alignment is also presented for comparison.", "section": "A.5 Asymmetric weights"}, {"figure_path": "gJxEiRcnao/figures/figures_34_1.jpg", "caption": "Figure 17: Comparing different weight symmetry using ResNet18 on CIFAR10 and presenting the mean accuracy including std per training epoch averaged across 5 runs.", "description": "The figure compares different weight symmetry settings using ResNet18 on CIFAR10 dataset.  It shows the training and testing accuracy and loss across epochs for various conditions: symmetric, asymmetric, asymmetric with weight decay, noisy symmetric, and weak symmetric. The results illustrate the impact of weight symmetry (or lack thereof) on model performance and learning dynamics.", "section": "A.5 Asymmetric weights"}, {"figure_path": "gJxEiRcnao/figures/figures_36_1.jpg", "caption": "Figure 2: The instruction-based learning algorithm. The three columns represent three passes of our model (left to right): TD \u2192 BU \u2192 TD, where the first two passes provide a prediction output given an image and a task, and the last TD pass (in green frame) is used for learning. In inference, The BU visual process is guided by the TD network according to the given task. More specifically, The TD network propagates instruction signals downward followed by a guided BU process of the input image to compute predictions. By applying ReLU non-linearity, the input task selectively activates a subset of neurons (i.e. non-zero values), composing a sub-network within the full network. The BU network then processes an input image using a composition of ReLU and GaLU. The GaLU function (dashed arrows) gates the BU computation to operate only on the selected sub-network that was activated by the task. For learning, the same TD network is then reused to propagate prediction error signals with GaLU exclusively (no ReLU). Finally, the 'Counter-Hebb' learning rule adjusts both networks' weights based on the activation values of their neurons. Therefore, in contrast with standard models, the entire computation, including the learning, is carried out by neurons in the network, and no additional computation is used for learning (e.g. backpropagation)", "description": "This figure illustrates the three passes of the instruction-based learning algorithm: TD->BU->TD.  The first two passes generate a prediction given an image and task, while the final TD pass (in green) is for learning.  The TD network guides the BU process using task instructions. ReLU non-linearity on the task activates a task-specific sub-network within the BU network.  The BU processing uses both ReLU and GaLU, with GaLU gating based on the TD network, ensuring processing focuses only on the relevant sub-network. Learning uses a final TD pass and the counter-Hebbian rule to adjust weights based on neural activity, eliminating the need for backpropagation.", "section": "5 Instruction-based learning"}]