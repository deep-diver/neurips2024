[{"type": "text", "text": "Fairness-Aware Estimation of Graphical Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhuoping Zhou,\u2217 Davoud Ataee Tarzanagh,\u2217 Bojian Hou,\u2217 Qi Long,\u2020 Li Shen\u2020 ", "page_idx": 0}, {"type": "text", "text": "University of Pennsylvania {zhuopinz@sas., tarzanaq@}upenn.edu {bojian.hou, qlong, li.shen}@pennmedicine.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs\u2019 performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphical models (GMs) are probabilistic models that use graphs to represent dependencies between random variables [34]. They are essential in domains such as gene expression [91], social networks [17], computer vision [33], and recommendation systems [8]. The capacity of GMs to handle complex dependencies makes them crucial across various data-intensive disciplines. Therefore, as our society\u2019s reliance on machine learning grows, ensuring the fairness of these models becomes increasingly paramount; see Section 1.1 for further discussions. While significant research has addressed fairness in supervised learning [29], the domain of unsupervised learning, particularly in the estimation of GMs, remains less explored. ", "page_idx": 0}, {"type": "text", "text": "We address the fair estimation of sparse GMs where the number of variables $P$ is much larger than the number of observations $N$ [22, 16, 43]. We focus on three types of GMs: ", "page_idx": 0}, {"type": "text", "text": "I. Gaussian Graphical Model: Rows $\\mathbf{X}_{1:},\\allowbreak\\cdot\\cdot,\\allowbreak\\mathbf{X}_{N}$ : in the data matrix $\\mathbf{X}\\in\\mathbb{R}^{N\\times P}$ are i.i.d. from a multivariate Gaussian distribution $\\mathcal{N}(\\mathbf{0},\\Sigma)$ . The conditional independence graph is determined by the sparsity of the inverse covariance matrix $\\Sigma^{-1}$ , where $(\\Sigma^{-1})_{j j^{\\prime}}=0$ indicates conditional independence between the $j$ th and $j^{\\prime}$ th variables. ", "page_idx": 0}, {"type": "text", "text": "II. Gaussian Covariance Graph Model: Rows $\\mathbf{X}_{1:},\\allowbreak\\cdot\\cdot,\\allowbreak\\mathbf{X}_{N}$ : are i.i.d. from $\\mathcal{N}(\\mathbf{0},\\Sigma)$ . The marginal independence graph is determined by the sparsity of the covariance matrix $\\Sigma$ , where $\\Sigma_{j j^{\\prime}}=\\mathbf{\\bar{0}}$ indicates marginal independence between the $j$ th and $j^{\\prime}$ th variables. ", "page_idx": 0}, {"type": "text", "text": "III. Binary Ising Graphical Model: Rows $\\mathbf{X}_{1:},\\ldots,\\mathbf{X}_{N}$ : are binary vectors and i.i.d. with ", "page_idx": 0}, {"type": "equation", "text": "$$\np(\\mathbf{x};\\pmb{\\Theta})=(Z(\\pmb{\\Theta}))^{-1}\\exp\\big(\\sum_{j=1}^{P}\\theta_{j j}x_{j}+\\sum_{1\\leq j<j^{\\prime}\\leq P}\\theta_{j j^{\\prime}}x_{j}x_{j^{\\prime}}\\big).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here, $\\Theta$ is a symmetric matrix, and $Z(\\Theta)$ normalizes the density. $\\theta_{j j^{\\prime}}\\,=\\,0$ indicates conditional independence between the $j$ th and $j^{\\prime}$ th variables. The sparsity pattern of $\\Theta$ reflects the conditional independence graph. ", "page_idx": 0}, {"type": "image", "img_path": "WvWS8goWyR/tmp/94f44d00b70475f962742e4d7c91ef5428f14f8c50cd1fb00448e4c973583ce7.jpg", "img_caption": ["Figure 1: Illustration of a GM and its fair variant. (a) displays the entire dataset, split into Group Blue (b) and Group Orange (c). (d) and (e) show GMs for each group, detailing the relationships between variables. (f) uses a GM for the entire dataset. The fair model in (g) adjusts these relationships to ensure equitable representation and minimize biases in subgroup analysis. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In a data matrix $\\mathbf{X}\\in\\mathbb{R}^{N\\times P}$ , each column corresponds to a node in a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , where $\\mathcal{V}=\\{1,2,\\dots,P\\}$ are vertices and $\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}$ are edges. Column $\\mathbf{X}_{:i}$ $\\boldsymbol{i}\\in\\{1,2,\\dots,P\\})$ is a vector of length $N$ , representing the observations for the $i$ -th variable across all $\\dot{N}$ samples. The graph $\\mathcal{G}$ , represented by the symmetric matrix $\\Theta$ , has nonzero entries indicating edges and reflects the graph\u2019s independence properties. To obtain a sparse and interpretable graph estimate, we consider ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\underset{\\Theta}{\\mathrm{minimize}}\\;\\mathcal{L}\\left(\\Theta;\\mathbf{X}\\right)+\\lambda\\|\\Theta\\|_{1}\\;\\;\\mathrm{subj.}\\;\\mathfrak{w}\\;\\;\\Theta\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $\\mathcal{L}$ is a loss function; $\\lambda\\|\\cdot\\|_{1}$ is the $\\ell_{1}$ -norm regularization with parameter $\\lambda>0$ ; and $\\mathcal{M}$ is a convex constraint subset of $\\mathbb{R}^{P\\times P}$ . For example, in a Gaussian GM, $\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{\\Theta};\\mathbf{X})=-\\log\\operatorname*{det}(\\boldsymbol{\\Theta})+}\\end{array}$ $\\mathrm{trace}(\\mathbf{S}\\mathbf{\\Theta})$ , where $\\begin{array}{r}{{\\bf S}=n^{-1}\\sum_{i=1}^{n}{\\bf X}_{i:}^{\\top}{\\bf X}_{i:}}\\end{array}$ , and $\\mathcal{M}$ is the set of $P\\times P$ positive definite matrices. ", "page_idx": 1}, {"type": "text", "text": "1.1 Motivation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our motivations for obtaining a fair GM estimation are summarized as follows. i) Equitable Representation: Standard group-specific GM models may improve accuracy for targeted groups but do not ensure fairness and can reinforce biases present in the data [48]. A unified approach considering the entire dataset is essential for mitigating biases and promoting fairness across all groups. ii) Legal and Ethical Compliance: Ethical and legal considerations [12] require explicit consent for processing sensitive attributes in model selection. Thus, constructing a fair estimation approach that adheres to fairness practices, uses data with consent, and excludes sensitive attribute information during deployment ensures privacy and legal compliance. iii) Generalization across Groups: A unified fair GM captures differences across groups without segregating the model, enhancing generalizability and preventing overfitting to a specific group [30], a risk in training separate models for each group. ", "page_idx": 1}, {"type": "text", "text": "For further discussion, we compare a GM with its proposed Fair variant, as illustrated in Figure 1. Panel (a) shows the entire dataset, divided into Group Blue and Group Orange in panels (b) and (c). Panels (d) and (e) detail the GM for each group, highlighting variable relationships. Panel (f) demonstrates a conventional GM applied to the full dataset, revealing a bias towards Group Blue. Panel (g) introduces a Fair GM, including modifications (red dashed lines) to reduce bias and ensure balanced representation. These adjustments correct relationships within the model, promoting fairness by preventing disproportionate favor towards any group. This illustration highlights the bias challenge in GMs and the steps Fair GMs take to ensure fair and equal modeling outcomes across groups. ", "page_idx": 1}, {"type": "text", "text": "1.2 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "$\\diamond$ We propose a framework to mitigate bias in Gaussian, Covariance, and Ising models related to protected attributes. This is achieved by incorporating pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while preserving GMs performance. $\\diamond$ We develop a proximal gradient method with non-asymptotic convergence guarantees for nonsmooth multi-objective optimization, applicable to Gaussian, Covariance, and Ising models (Theorems 6\u20138). To our knowledge, this is the first work providing a multi-objective proximal gradient method for GM estimation, in contrast to existing single-objective GM methods [3, 87, 13]. $\\diamond$ We provide extensive experiments to validate the effectiveness of our GM framework in mitigating bias while maintaining model performance on synthetic data, the Credit Dataset, the Cancer Genome Atlas Dataset, Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI), and the binary LFM-1b Dataset for recommender systems3. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Estimation of Graphical Models. The estimation of network structures from high-dimensional data [89, 51, 1, 84, 19, 84] is a well-explored domain with significant applications in biomedical and social sciences [44, 59, 25]. Given the challenge of parameter estimation with limited samples, sparsity is imposed via regularization, commonly through an $\\ell_{1}$ penalty to encourage sparse network structures [22, 37, 25]. However, these approaches may overlook the complexity of real-world networks, which often have varying structures across scales, including densely connected subgraphs or communities [13, 26, 23]. Recent work extends beyond simple sparsity to estimate hidden communities within networks, reflecting homogeneity within and heterogeneity between communities [47]. This includes inferring connectivity and performing graph estimation when community information is known, as well as considering these tasks in the context of heterogeneous observations [42, 80, 24]. ", "page_idx": 2}, {"type": "text", "text": "Fairness. Fairness research in machine learning has predominantly focused on supervised methods [11, 4, 15, 39, 92, 79, 28]. Our work broadens this scope to unsupervised learning, incorporating insights from [65, 77, 56, 9, 10]. Notably, [41] has developed algorithms for fair clustering using the Laplacian matrix. Our approach diverges by not presupposing any graph and Laplacian structures. The most relevant works to this study are [78, 93, 52, 53]. Specifically, [78] initiated the learning of fair GMs using an $\\ell_{1}$ -regularized pseudo-likelihood method for joint GMs estimation and fair community detection. [93, 94] proposed a fair spectral clustering model that integrates graph construction, fair spectral embedding, and discretization into a single objective function. Unlike these models, which assume community structures, our study formulates fair GMs without such assumption. Concurrently with this work, [52] proposed a regularization method for fair Gaussian GMs assuming the availability of node attributes. Their methodology significantly differs from ours, as we focus on developing three classes of fair GMs (Gaussian, Covariance, and Ising models) for imbalanced groups without node attributes, aiming to automatically ensure fairness through non-smooth multi-objective optimization. ", "page_idx": 2}, {"type": "text", "text": "3 Fair Estimation of Graphical Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. $\\mathbb{R}^{d}$ denotes the $d$ -dimensional real space, and $\\mathbb{R}_{+}^{d}$ and $\\mathbb{R}_{++}^{d}$ its positive and negative orthants. Vectors and matrices are in bold lowercase and uppercase letters (e.g., a, A), with elements $a_{i}$ and $a_{i j}$ . Rows and columns of $\\mathbf{A}$ are $\\mathbf{A}_{i}$ : and $\\mathbf{A}_{:j}$ , respectively. For symmetric A, $\\mathbf A\\succ0$ and $\\mathbf A\\succeq0$ denote positive definiteness and semi-definiteness. $\\Lambda_{i}(\\mathbf{A})$ is the ith smallest eigenvalue of A. The matrix norms are defined as $\\begin{array}{r}{\\|\\mathbf{A}\\|_{1}=\\sum_{i j}|a_{i j}|}\\end{array}$ and $\\begin{array}{r}{\\|\\mathbf{A}\\|_{F}=(\\sum_{i j}|a_{i j}|^{2})^{1/2}}\\end{array}$ . For any positive integer $n$ , $[n]:=\\{1,\\ldots,n\\}$ . Any notation is defined upon its first use and summarized in Table 3. ", "page_idx": 2}, {"type": "text", "text": "3.1 Graph Disparity Error ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To evaluate the effects of joint GMs learning on different groups, we compare models trained on group-specific data with those trained on a combined dataset. Let a dataset $\\mathbf{X}$ be divided into $K$ sensitive groups, with the data for group $k\\,\\in\\,[K]$ represented as $\\mathbf{X}_{k}\\in\\mathbb{R}^{N_{k}\\times P}$ , where $N_{k}$ is the sample size for group $k$ , and $\\textstyle N=\\sum_{k=1}^{n}N_{k}$ . The performance of a GM, denoted b\u2217y $\\Theta$ , for group $k$ is measured by the loss function $\\mathcal{L}(\\Theta;\\mathbf{X}_{k})$ . Our goal is to find a global model that minimizes performance discrepancies across groups. We define graph disparity error to quantify fairness: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Graph Disparity Error). Given a dataset $\\mathbf{X}\\in\\mathbb{R}^{N\\times P}$ with $K$ sensitive groups, where $\\mathbf{X}_{k}$ represents the data for group $k\\in[K]$ , let ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Theta_{k}^{*}\\in\\ \\underset{\\Theta_{k}\\in\\mathcal{M}}{\\arg\\operatorname*{min}}\\ \\mathcal{L}(\\Theta_{k};\\mathbf{X}_{k})+\\lambda\\|\\Theta_{k}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The graph disparity error for group $k$ is then: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal E_{k}\\left(\\Theta\\right):=\\mathcal L(\\Theta;\\mathbf X_{k})-\\mathcal L(\\Theta_{k}^{\\ast};\\mathbf X_{k}),\\quad1\\leq k\\leq K.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This measures the loss difference between a global graph matrix $\\Theta$ and the optimal local graph matrix $\\Theta_{k}^{*}$ for each group\u2019s data $\\mathbf{X}_{k}$ . A fair GM, under Definition 1, seeks to balance $\\mathcal{E}_{k}$ across all groups. ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Fair Estimation of GMs (Fair GMs) ", "page_idx": 3}, {"type": "text", "text": "Require: Data Matrix $X=X_{1}\\cup X_{2}\\cup\\cdot\\cdot\\cdot\\cup X_{K}$ ; Parameters $\\lambda>0,\\epsilon>0,T>0$ , and $\\ell>L$ .   \nS1. Get local graph estimates $\\{\\Theta_{k}^{*}\\}_{k=1}^{K}$ using (3), and initialize global graph estimate $\\Theta^{(0)}$ .   \nS2. For $t=1$ to $\\bar{T}-1$ do: $\\Theta^{(t+1)}\\gets\\mathbf{P}_{\\ell}(\\Theta^{(t)})$ , where $\\mathbf{P}_{\\ell}$ is obtained by solving Subproblem (9).   \nOutput: Fair global graph estimate $\\Theta^{(t+1)}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Fair GM). A GM with graph matrix $\\Theta^{*}$ is called fair if the graph disparity errors among different groups are equal, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{E}_{1}\\left(\\boldsymbol{\\Theta}^{*}\\right)=\\mathcal{E}_{2}\\left(\\boldsymbol{\\Theta}^{*}\\right)=\\cdots=\\mathcal{E}_{K}\\left(\\boldsymbol{\\Theta}^{*}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To address the imbalance in graph disparity error among all groups, we introduce the idea of pairwise graph disparity error, which quantifies the variation in graph disparity between different groups. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Pairwise Graph Disparity Error). Let $\\phi:\\mathbb{R}\\to\\mathbb{R}_{+}$ be a penalty function such as $\\phi(x)=\\exp(x)$ or $\\phi(x)={\\textstyle\\frac{1}{2}}x^{2}$ . The pairwise graph disparity error for the group $k$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{k}\\left(\\Theta\\right):=\\sum_{s\\in\\left[K\\right],s\\neq k}\\phi\\left(\\mathcal{E}_{k}\\left(\\Theta\\right)-\\mathcal{E}_{s}\\left(\\Theta\\right)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The motivation for Definition 3 follows from the work of [35, 65, 95] in PCA and CCA. In our convergence analysis, we focus on smooth functions $\\phi$ , such as squared or exponential functions, while nonsmooth choices, such as $\\phi(x)=|x|$ , can be explored in the experimental evaluations. ", "page_idx": 3}, {"type": "text", "text": "3.2 Multi-Objective Optimization for Fair GMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section introduces a framework designed to mitigate bias in GMs (including Gaussian, Covariance, and Ising) related to protected attributes by incorporating pairwise graph disparity error into a nonsmooth multi-objective optimization problem. Smooth multi-objective optimization tackles fairness challenges in unsupervised learning [35, 95], proving particularly useful when decision-making involves multiple conflicting objectives. ", "page_idx": 3}, {"type": "text", "text": "We use non-smooth multi-objective optimization to balance two key factors: the loss in GMs and the pairwise graph disparity errors. To achieve this, let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{f_{1}\\left(\\Theta\\right)=\\mathcal{L}\\left(\\Theta;\\mathbf{X}\\right),\\qquad f_{k}\\left(\\Theta\\right)=\\Delta_{k-1}\\left(\\Theta\\right),}&&{\\mathrm{for~}2\\leq k\\leq K+1,}\\\\ &{\\qquad\\qquad F_{k}\\left(\\Theta\\right)=f_{k}\\left(\\Theta\\right)+g\\left(\\Theta\\right),}&&{\\mathrm{for~}1\\leq k\\leq M:=K+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g\\left(\\Theta\\right):=\\lambda\\|\\Theta\\|_{1}$ for some $\\lambda>0$ . ", "page_idx": 3}, {"type": "text", "text": "Consequently, we propose the following multi-objective optimization problem for Fair GMs: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\Theta}{\\mathrm{minimize}}\\;\\;\\mathbf{F}\\left(\\Theta\\right):=\\left[F_{1}\\left(\\Theta\\right),\\ldots,F_{M}\\left(\\Theta\\right)\\right]\\qquad\\mathrm{subj.\\;to}\\;\\;\\Theta\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathcal{M}$ is a convex constraint subset of $\\mathbb{R}^{P\\times P}$ and $\\mathbf{F}:\\Omega\\to\\mathbb{R}^{M}$ is a multi-objective function. ", "page_idx": 3}, {"type": "text", "text": "Assumption A. For some $L>0$ , al $\\begin{array}{r}{\\mathbf{\\Theta}^{\\bullet}\\mathbf{\\Phi}\\Phi\\in\\mathcal{M},k\\in\\left[M\\right],\\|\\nabla f_{k}\\left(\\Phi\\right)-\\nabla f_{k}\\left(\\Theta\\right)\\|_{F}\\leq L\\|\\Phi\\!-\\!\\Theta\\|_{F}.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Note that Assumption A holds for smooth $\\phi$ functions such as squared or exponential, as specified in Definition 6, and when $\\mathcal{L}$ is a smooth loss function. We demonstrate in Appendix C that this assumption holds for the Gaussian, Covariance, and Ising models studied in this work. To proceed, we provide the following definitions; see [20, 75, 73] for more details. ", "page_idx": 3}, {"type": "text", "text": "Definition 4 (Pareto Optimality). In Problem (8), a solution $\\Theta^{*}\\in\\mathcal{M}$ is Pareto optimal if there is no $\\Theta\\in\\mathcal{M}$ such that $\\mathbf{F}(\\mathbf{\\dot{\\Theta}})\\preceq\\dot{\\mathbf{F}}(\\mathbf{\\Theta}^{*})$ and $\\mathbf{F}(\\Theta)\\neq\\mathbf{F}(\\Theta^{*})$ . It is weakly Pareto optimal if there is no $\\Theta\\in\\mathcal{M}$ such that $\\mathbf{F}(\\Theta)\\prec\\mathbf{F}(\\Theta^{*})$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 5 (Pareto Stationary). We define a point $\\bar{\\Theta}\\in\\mathbb{R}^{P\\times P}$ as Pareto stationary (or critical) if it satisfies the following condition: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{k\\in[M]}F_{k}^{\\prime}(\\bar{\\Theta};\\mathbf{D}):=\\operatorname*{lim}_{\\alpha\\to0}\\frac{F_{k}(\\bar{\\Theta}+\\alpha\\mathbf{D})-F_{k}(\\bar{\\Theta})}{\\alpha}\\geq0\\quad\\mathrm{for~all}\\quad\\mathbf{D}\\in\\mathbb{R}^{P\\times P}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To solve Problem (8), we use the proximal gradient method and establish its convergence to a Pareto stationary point for the nonsmooth Problem (8). The procedure for our fairness-aware GMs (Fair GMs) is detailed in Algorithm 1. Given local graph estimates $\\{\\Theta_{k}^{*}\\}_{k=1}^{K}$ obtained in S1., and $\\ell>L$ where is a Lipschitz constant defined in Assumption A, the update of the global fair graph estimate $\\Theta$ is produced in S2. by solving: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbf{P}_{\\ell}\\left(\\Theta\\right):=\\underset{\\Phi\\in\\mathcal{M}}{\\arg\\operatorname*{min}}\\,\\varphi_{\\ell}\\left(\\Phi;\\Theta\\right),\\quad\\mathrm{with}\\quad}\\\\ {\\displaystyle\\varphi_{\\ell}\\left(\\Phi;\\Theta\\right):=\\operatorname*{max}_{k\\in\\left[M\\right]}\\left\\langle\\nabla f_{k}(\\Theta),\\Phi-\\Theta\\right\\rangle+g(\\Phi)-g(\\Theta)+\\frac{\\ell}{2}\\left\\Vert\\Phi-\\Theta\\right\\Vert_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the convexity of $g(\\Theta)=\\lambda\\|\\Theta\\|_{1}$ ensures a unique solution for Problem (9). We provide a simple yet efficient approach to solve Subproblem (9) through its dual in Appendix B. In addition, Proposition 11 in Appendix B characterizes the weak Pareto optimality for Problem (8). ", "page_idx": 4}, {"type": "text", "text": "3.3 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We apply Algorithm 1 to Gaussian, Covariance, and Ising models and provide theoretical guarantees. ", "page_idx": 4}, {"type": "text", "text": "Fair Graphical Lasso (Fair GLasso). Consider $\\mathbf{X}_{1:},\\ldots,\\mathbf{X}_{N}$ : as i.i.d. samples from $\\mathcal{N}(\\mathbf{0},\\Sigma)$ . In the GLasso method [22], the loss is defined as $\\begin{array}{r}{\\mathcal{L}_{G}\\left(\\boldsymbol{\\Theta};\\mathbf{X}\\right):=-\\log\\operatorname*{det}(\\boldsymbol{\\Theta})+\\operatorname{trace}(\\boldsymbol{\\mathbf{S}}\\boldsymbol{\\Theta})}\\end{array}$ , where $\\Theta$ is constrained to the set $\\mathcal{M}=\\{\\Theta:\\Theta\\succ\\mathbf{0},\\Theta=\\Theta^{\\top}\\}$ and $\\begin{array}{r}{\\mathbf{S}=n^{-1}\\sum_{i=1}^{n}\\mathbf{X}_{i:}\\mathbf{X}_{i:}^{\\top}\\in\\mathbb{R}^{N\\times N}}\\end{array}$ is the empirical covariance matrix of $\\mathbf{\\deltaX}$ . Extending this to fair GLasso and following (8), the multi-objective optimization problem is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{\\Theta}}{\\mathrm{minimize}}\\quad\\mathbf{F}(\\mathbf{\\Theta})=[\\mathcal{L}_{G}\\left(\\mathbf{\\Theta};\\mathbf{X}\\right)+\\lambda\\|\\mathbf{\\Theta}\\|_{1},F_{2}(\\mathbf{\\Theta}),\\cdots,F_{M}(\\mathbf{\\Theta})]}\\\\ &{\\quad\\quad\\mathbf{\\Theta}\\mathrm{subj.\\;to}\\quad\\mathbf{\\Theta}\\Theta\\in\\mathcal{M}=\\{\\Theta:\\Theta\\succ\\mathbf{0},\\Theta=\\Theta^{\\top}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption B. Let $\\mathcal{N}^{*}$ be the set of weakly Pareto optimal points for (8), and $\\Omega_{\\mathbf{F}}(\\alpha):=\\{\\Theta\\in$ $S\\mid\\mathbf{F}(\\Theta)\\preceq\\alpha\\}$ denote the the level set of $\\mathbf{F}$ for $\\alpha\\in\\mathbb{R}^{M}$ . For all $\\boldsymbol\\Theta\\in\\Omega_{\\mathbf{F}}(\\mathbf{F}(\\boldsymbol\\Theta^{(0)}))$ , there exists $\\Theta^{*}\\in\\mathcal{N}^{*}$ such that $\\mathbf{F}(\\mathbf{\\Theta}^{*})\\preceq\\mathbf{F}(\\mathbf{\\Theta}\\mathbf{\\Theta})$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\nR:=\\operatorname*{sup}_{\\mathbf{F}^{\\ast}\\in\\mathbf{F}(\\mathcal{N}^{\\ast}\\cap\\Omega_{\\mathbf{F}}(\\mathbf{F}(\\mathbf{\\Theta}^{0})))}\\quad\\operatorname*{inf}_{\\mathbf{\\Theta}\\in\\mathbf{F}^{-1}(\\{\\mathbf{F}^{\\ast}\\})}\\|\\mathbf{\\Theta}\\mathbf{\\Theta}-\\mathbf{\\Theta}^{(0)}\\|_{F}^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This assumption is satisfied when $\\Omega_{F}(\\mathbf{F}(\\pmb{\\Theta}^{(0)}))$ is bounded [75, 73]. When $M=1$ , it holds if the problem has at least one optimal solution. If $\\Omega_{F}(\\mathbf{F}(\\Theta^{(0)}))$ is bounded, Assumption B also holds, such as when $F_{k}$ is strongly convex for some $k\\in[M]$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 6. Suppose Assumptions $A$ and $B$ hold. Let $\\{\\Theta^{(t)}\\}_{t\\geq1}$ be the sequence generated by Algorithm 1 for solving (Fair GLasso). Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\Theta\\in\\mathcal{M}}\\operatorname*{min}_{k\\in[M]}\\left\\{F_{k}\\left(\\Theta^{(t)}\\right)-F_{k}(\\Theta)\\right\\}\\leq\\frac{\\ell R}{2t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Fair Covariance Graph (Fair CovGraph). For the Fair CovGraph, we assume $\\mathbf{X}_{1:},\\ldots,\\mathbf{X}_{N}$ : are i.i.d. samples from $\\mathcal{N}(\\mathbf{0},\\Sigma)$ . We use a sparse estimator for the covariance matrix, ensuring it remains positive definite and specifies the marginal independence graph. Following [62], we define the estimator\u2019s loss function as $\\begin{array}{r}{\\mathcal{L}_{C}(\\Sigma,\\mathbf{X}):=\\frac{\\bar{1}}{2}\\|\\Sigma-\\mathbf{S}\\|_{F}^{2}-\\tau\\log\\operatorname*{det}(\\Sigma)}\\end{array}$ with $\\tau>0$ . Building on this and using (7) and (8), for some nonegative constants $\\gamma_{C}$ and $\\lambda$ , we introduce the Fair CovGraph optimization problem, formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{\\Sigma}}{\\mathrm{minimize}}}&{\\mathbf{F}\\left(\\mathbf{\\Sigma}\\right)=\\left[F_{1}(\\Sigma),F_{2}(\\Sigma),\\ldots,F_{M}(\\Sigma)\\right]}\\\\ {\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}}\\\\ {\\mathrm{subj.~to}}&{\\Sigma\\in\\mathcal{M}=\\{\\Sigma:\\Sigma\\succ\\mathbf{0},\\Sigma=\\Sigma^{\\top}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, following (8), we have $f_{1}(\\Sigma)=\\mathcal{L}_{C}(\\Sigma;\\mathbf{X})$ and $f_{k}(\\Sigma)=\\Delta_{k-1}(\\Sigma)$ for $2\\leq k\\leq K+1$ . Also, $F_{1}\\left(\\boldsymbol{\\Sigma}\\right)=f_{1}\\left(\\boldsymbol{\\Sigma}\\right)+\\lambda\\|\\boldsymbol{\\Sigma}\\|_{1}$ and $F_{k}\\left(\\mathbf{\\Sigma}\\right)=f_{k}\\left(\\mathbf{\\Sigma}\\right)+\\lambda\\|\\mathbf{\\Sigma}\\mathbf{|}_{1}+\\gamma_{C}\\|\\mathbf{\\Sigma}\\|_{F}^{2}$ for $2\\leq k\\leq M=K+1$ . The parameter $\\gamma_{C}$ is used to convexify (Fair CovGraph) and is crucial for ensuring the convergence of Algorithm 1. The following theorem establishes the convergence of Algorithm 1 for (Fair CovGraph). ", "page_idx": 4}, {"type": "text", "text": "Theorem 7. Under conditions similar to Theorem $\\theta_{:}$ , by replacing $\\Theta$ with $\\Sigma$ and $\\mathcal{L}_{G}\\left(\\boldsymbol{\\Theta};\\mathbf{X}\\right)$ with $\\mathcal{L}_{C}(\\Sigma,\\mathbf{X})$ , for the sequence $\\{\\boldsymbol{\\Sigma}^{(t)}\\}_{t\\geq1}$ generated by Algorithm $^{\\,l}$ applied to (Fair CovGraph), and $f o r\\,\\gamma_{C}\\geq\\operatorname*{max}\\{0,-\\Lambda_{\\operatorname*{min}}(\\nabla^{2}f_{k}(\\Sigma))\\}$ for all $k\\in[K].$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\sum_{\\in\\mathcal{M}}k\\in\\left[M\\right]}\\left\\{F_{k}\\left(\\Sigma^{\\left(t\\right)}\\right)-F_{k}\\left(\\Sigma\\right)\\right\\}\\leq\\frac{\\ell R}{2t}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Fair Binary Ising Network (Fair BinNet). In this section, we focus on the binary Ising Markov random field as described by [58]. The model considers binary-valued, i.i.d. samples with probability density function defined in (1). Following [31], we consider the following loss function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{I}\\left(\\boldsymbol{\\Theta};\\mathbf{X}\\right)=-\\sum_{j=1}^{P}\\sum_{j^{\\prime}=1}^{P}\\theta_{j j^{\\prime}}(\\mathbf{X}^{\\top}\\mathbf{X})_{j j^{\\prime}}+\\sum_{i=1}^{N}\\sum_{j=1}^{P}\\log\\Big(1+\\exp\\left(\\theta_{j j}+\\sum_{j^{\\prime}\\neq j}\\theta_{j j^{\\prime}}x_{i j^{\\prime}}\\right)\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given some nonegative constants $\\gamma_{I}$ and $\\lambda$ , the Fair BinNet objective is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{\\Theta}}{\\mathrm{minimize}}}&{\\mathbf{F}\\left(\\mathbf{\\Theta}\\right)=\\left[F_{1}\\left(\\mathbf{\\Theta}\\right),F_{2}\\left(\\mathbf{\\Theta}\\right),\\ldots,F_{M}\\left(\\mathbf{\\Theta}\\right)\\right]}\\\\ {\\mathbf{\\Theta}}&{\\mathbf{\\Theta}}\\\\ {\\mathrm{subj.~to}}&{\\mathbf{\\Theta}\\Theta\\in\\mathcal{M}=\\{\\Theta:\\Theta=\\Theta^{\\top}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, following (8), we have $f_{1}(\\Theta)=\\mathcal{L}_{I}(\\Theta;\\mathbf{X})$ , and $f_{k}(\\Theta)=\\Delta_{k-1}(\\Theta)$ for $2\\leq k\\leq K\\!+\\!1$ . Also, $F_{1}\\left(\\Theta\\right)=f_{1}\\left(\\Theta\\right)+\\lambda\\|\\Theta\\|_{1}$ , and $F_{k}\\left(\\Theta\\right)=f_{k}\\left(\\Theta\\right)+\\lambda\\|\\Theta\\|_{1}+\\gamma_{I}\\|\\Theta\\|_{F}^{2}$ for $2\\leq k\\leq M=K+1$ . The parameter $\\gamma_{I}$ convexifies Problem (Fair BinNet) and ensures Algorithm 1 converges. The following theorem establishes the convergence of Algorithm 1 for Problem (Fair BinNet). ", "page_idx": 5}, {"type": "text", "text": "Theorem 8. Suppose Assumptions $A$ and $B$ hold, and that $\\gamma_{I}\\geq\\operatorname*{max}\\{0,-\\Lambda_{\\operatorname*{min}}(\\nabla^{2}f_{k}(\\Theta))\\}$ for all $k\\in[K]$ . Then, the sequence $\\{\\Theta^{(t)}\\}_{t\\geq1}$ generated by Algorithm $^{\\,l}$ for (Fair BinNet) satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\Theta\\in\\mathcal{M}}\\operatorname*{min}_{k\\in\\left[M\\right]}\\left\\{F_{k}\\left(\\Theta^{(t)}\\right)-F_{k}\\left(\\Theta\\right)\\right\\}\\leq\\frac{\\ell R}{2t}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 9 (Iteration Complexity of Algorithm 1). Theorems 6, 7, and 8 establish the global convergence rates of $O(1/t)$ for Algorithm 1 for Gaussian, Covariance, and Ising models, respectively. In contrast to Theorem $^{6}$ , Theorems 7 and 8 necessitate the inclusion of an additional convex regularization term with parameters $\\gamma_{C}$ and $\\gamma_{I}$ , respectively, to achieve Pareto optimality. ", "page_idx": 5}, {"type": "text", "text": "Remark 10 (Computational Complexity of Algorithm 1). Given the iteration complexity to achieve \u03f5-accuracy is $O(\\epsilon^{-1})$ , the overall time complexity of our optimization procedure becomes $O\\left(\\epsilon^{-1}\\operatorname*{max}(N P^{2},P^{3})\\right)$ . Assuming a small number of groups $(K<<N,P,1/\\epsilon)$ , the complexity aligns with that of standard proximal gradient methods used for covariance and inverse covariance estimation, making it feasible for large $N$ and $P$ . To further support the theoretical analysis, sensitivity analysis experiments are conducted to investigate the impact of varying $P$ , N, $K$ , and group imbalance on the performance of the proposed methods. Note that the complexity of Algorithm 1 applied to (Fair BinNet) depends on the choice of subproblem solver (e.g., first or second order) due to the nonlinearity of (10). Further experiments and discussions are detailed in Appendices D.6-D.9. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Baseline. The Iterative Shrinkage-Thresholding Algorithm (ISTA) is widely used for sparse inverse covariance estimation [60] due to its simplicity and efficiency. We adapt ISTA for the Covariance and Ising models and use them as a baseline to compare with our proposed Fair GMs. Note that our Fair GMs reduce to ISTA for Gaussian, Covariance, and Ising models if $M=1$ in (8). The detailed ISTA algorithm used in this study is provided in Appendix D for reference. ", "page_idx": 5}, {"type": "text", "text": "Parameters and Architecture. The initial iterate $\\Theta^{(0)}$ is chosen based on the highest graph disparity error among local graphs. This initialization can improve fairness by minimizing larger disparity errors. The $\\bar{\\ell}_{1}$ -norm coefficient $\\lambda$ is fixed for each dataset, searched over a grid in $\\{1\\bar{e}-$ $5,\\bar{\\dots},0.01,\\dots,0.1,1\\}$ . Tolerance $\\epsilon$ is set to $1e-5$ , with a maximum of $1e+7$ iterations. The initial value of $\\ell$ is $1e-2$ , undergoing a line search at each iteration $t$ with a decay rate of 0.1. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Numerical outcomes in terms of PCEE. The last row calculates the difference in PCEE between the two groups: the smaller, the better, and the best value is in bold. ", "page_idx": 6}, {"type": "table", "img_path": "WvWS8goWyR/tmp/62d1c46f44bf703416909f0cc4613a4a3c8073e933ddd72eccc7f897e194cfab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "WvWS8goWyR/tmp/3ae74e5c12448663eb05c01a9bdabd7b8cec0b099157945499e2c8325952f683.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Comparison of original graphs utilized in synthetic data creation for two groups, graph reconstruction using standard GMs, and fair graph reconstruction via Fair GMs. The diagonal elements are set to zero to enhance the visibility of the off-diagonal pattern. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Criteria. In our experiments, we introduce three metrics to evaluate the performance of our methods and the baseline methods: ", "page_idx": 6}, {"type": "text", "text": "1. Value of the objective function of GM: $F_{1}:=\\mathcal{L}(\\boldsymbol{\\Theta};\\mathbf{X})+\\lambda\\|\\boldsymbol{\\Theta}\\|_{1}$ .   \n2. Summation of pairwise graph disparity error for fairness: $\\Delta:=\\sum_{k=1}^{K}\\Delta_{k}$ .   \n3. Proportion of correctly estimated edges: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{PCEE}:=\\big(\\sum_{\\substack{j,j^{\\prime}\\in[P]}}\\mathbf{1}\\{\\hat{\\Theta}_{j j^{\\prime}}\\geq\\lambda\\,\\mathrm{and}\\,|\\Theta_{j j^{\\prime}}|\\geq\\lambda\\}\\big)/\\big(\\sum_{\\substack{j,j^{\\prime}\\in[P]}}\\mathbf{1}\\{|\\Theta_{j j^{\\prime}}|\\geq\\lambda\\}\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, $\\Theta$ and $\\hat{\\Theta}$ are the groundtruth and estimated graph. ", "page_idx": 6}, {"type": "text", "text": "4.2 Simulation Study of Fair GLasso and CovGraph ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the simulation, we construct two $100\\times100$ block diagonal covariance matrices, $\\pmb{\\Sigma}_{1}$ and $\\Sigma_{2}$ (Figures 2c and 2d). These matrices correspond to two sensitive groups and are created following the rigorous process in Appendix D.2. Each graph has three consistent diagonal blocks, with Group 1 also featuring two distinct blocks indicating bias. For each group, we derive the ground truth graphs by $\\Theta_{1}=\\Sigma_{1}^{-1}$ and $\\Theta_{2}=\\Sigma_{2}^{-1}$ (Figures 2a and 2b). Datasets are generated from normal distributions: 1000 samples from $\\mathcal{N}(\\mathbf{0},\\Sigma_{1})$ for the first group, and 1000 samples from $\\mathcal{N}(\\mathbf{0},\\Sigma_{2})$ for the second. ", "page_idx": 6}, {"type": "text", "text": "Results. Figure $2\\mathrm{g}$ shows the global graph derived using Standard GLasso on the entire dataset, where the two top-left blocks are not distinctly marked, suggesting bias towards $\\Theta_{2}$ . In contrast, Figure 2h shows a graph from our method that enhances block visibility, reducing bias. This improvement is supported by the results in Table 1, where the PCEE difference of Fair GLasso is smaller than that of Standard GLasso. Comparable efficacy in bias reduction for CovGraph is shown in Figures 2c, 2d, 2i, 2j, and Table 1, demonstrating our methods\u2019 effectiveness in achieving fairness. ", "page_idx": 6}, {"type": "text", "text": "4.3 Simulation Study of Fair BinNet ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We provided two simulation networks: $\\Theta_{1}$ for Group 1 with $P=50$ nodes and three hub nodes, and $\\mathbf{\\bar{\\Theta}}_{2}$ for Group 2 with one hub node (see Appendix D.3 for details). Adjacency matrices are shown in Figures 2e and 2f. We generate $N_{1}\\,\\dot{^{\\star}}=\\,500$ and $N_{2}\\,=\\,1000$ observations via Gibbs sampling, updating each variable $x_{j}^{(t+1)}$ at iteration $t+1$ using the Bernoulli distribution: $x_{j}^{(t+1)}\\sim$ ", "page_idx": 6}, {"type": "text", "text": "Bernoulli $\\left(z_{\\theta}/(1+z_{\\theta})\\right)$ ), where $\\begin{array}{r}{z_{\\theta}\\,=\\,\\exp(\\theta_{j j}+\\sum_{j^{\\prime}\\neq j}\\theta_{j j^{\\prime}}x_{j^{\\prime}}^{(t)})}\\end{array}$ . The first 10,000 iterations are designated as the burn-in period to ensure statistical independence among observations. Finally, observations are collected at every 100th iteration. ", "page_idx": 7}, {"type": "text", "text": "Results. Figure $2\\mathbf{k}$ illustrates the global graph from Standard BinNet, which is predominantly biased towards ${\\bf{\\Theta}}_{2}^{-}$ by identifying only one hub node. In contrast, Figure 2l, derived from Fair BinNet, presents a more balanced structure. While this improvement might not be visually evident, the quantitative results in Table 1 and Table 2 confirm it. Table 1 reveals that PCEE for Group 1 improved significantly, increasing from 0.4444 to 0.7485. Conversely, PCEE for Group 2 exhibited a decrease from 0.9481 to 0.7662. This convergence in performance metrics between the two groups indicates a more balanced distribution of predictive errors, thus enhancing the overall fairness of the model. ", "page_idx": 7}, {"type": "text", "text": "4.4 Application of Fair GLasso to Gene Regulatory Network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We apply GLasso to analyze RNA-Seq data from TCGA, focusing on lung adenocarcinoma. The data includes expression levels of 60,660 genes from 539 patients. From these, 147 KEGG pathway genes [36] are selected to construct a gene regulatory network. GLasso reveals conditional dependencies, aiding in understanding cancer genetics and identifying therapeutic targets. However, initially, this method, without accounting for sex-based differences, risks overlooking critical biological disparities, potentially skewing drug discovery and health outcomes across genders. Therefore, we divide the patient cohort into two groups based on sex: 248 males and 291 females. This stratification enables the use of Fair GLasso, which creates a more equitable gene regulatory network by accounting for these differences. The parameter $\\lambda$ is set to 0.03 for this experiment. Additionally, each variable is normalized to achieve a zero mean and a unit variance. ", "page_idx": 7}, {"type": "text", "text": "Results. The gene networks identified by GLasso and Fair GLasso are presented in Figures 3a-3b. GLasso identified several hub nodes, including NCOA1, BRCA1, FGF8, AKT1, NOTCH4, and CSNK1A1L. In contrast, Fair GLasso uniquely detected PIK3CD, suggesting its potential relevance in capturing sex-specific differences in lung adenocarcinoma. Although direct evidence linking PIK3CD exclusively to sex-specific traits in cancer is limited, this finding aligns with recent insights into sex-specific regulatory mechanisms in cancer [63, 45]. PIK3CD is a key component of the PI3K/Akt signaling pathway, which is involved in cell regulation and frequently implicated in various malignancies. The identification of PIK3CD by Fair GLasso demonstrates its potential to uncover biologically relevant genes that may be overlooked in conventional analyses, enhancing our understanding of lung adenocarcinoma and facilitating the development of personalized therapies. ", "page_idx": 7}, {"type": "text", "text": "4.5 Application of Fair GLasso to Amyloid / Tau Accumulation Network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performance of GLasso and Fair GLasso is evaluated using AV45 and AV1451 PET imaging data from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) [85, 86], focusing on amyloid- $\\bar{\\cdot}\\bar{\\beta}$ and tau deposition in the brain. The dataset includes standardized uptake value ratios of AV45 and AV1451 tracers in 68 brain regions, as defined by the Desikan-Killiany atlas [14], collected from 1,143 participants. An amyloid (or tau) accumulation network [68] is constructed to investigate the pattern of amyloid (or tau) accumulation. GLasso and Fair GLasso are used to uncover conditional dependencies between brain regions, providing insights into Alzheimer\u2019s disease progression and identifying potential biomarkers for early diagnosis and treatment response monitoring. To examine the influence of sensitive attributes on the network structure, marital status, and race are incorporated as exemplary sensitive attributes due to their reported association with dementia risk [71, 49]. Comprehensive details regarding the experiments, results, and analysis are provided in Appendix 4.5. ", "page_idx": 7}, {"type": "text", "text": "4.6 Application of Fair CovGraph to Credit Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performance of Fair CovGraph is evaluated using the Credit Datasets [90] from the UCI Machine Learning Repository [2]. These datasets have been previously used in research on Fair PCA [55, 83], which shows potential for improvement through sparse covariance estimation. The dataset composition is detailed in Table 5 in Appendix D.5, with categorizations based on gender, marital status, and education level. For this experiment, the parameters $\\tau$ and $\\lambda$ are set to 0.01 and 0.1, respectively. Each variable in the dataset is standardized to have a mean of zero and a variance of one. As shown in Table 2, our Fair CovGraph achieves a $53.75\\%$ increase in fairness with only a $0.42\\%$ decrease in the graph objective, demonstrating the strong ability of our method to attain fairness. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Outcomes in terms of the value of the objective function $\\left(F_{1}\\right)$ , the summation of the pairwise graph disparity error $(\\Delta)$ , and the average computation time in seconds ( $\\pm$ standard deviation) from 10 repeated experiments. \u201c $\\downarrow^{\\,,}$ means the smaller, the better, and the best value is in bold. Note that both $F_{1}$ and $\\Delta$ are deterministic. ", "page_idx": 8}, {"type": "table", "img_path": "WvWS8goWyR/tmp/8b0e47622e0989286048c47c8ea0f2bd8fded3bed54f09cb19d86a95edfd8b7d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "WvWS8goWyR/tmp/10b911167b44da7864ce5dad9d68205c35523656d5a5f2bbb5e90026a93b5e7f.jpg", "img_caption": ["Figure 3: (a)-(b) Comparison of graphs generated by standard GLasso and Fair GLasso on TCGA Dataset. Week edges are removed for visibility, and hub nodes that own at least 4 edges are highlighted. (c)-(d) Comparison of sub-graphs generated by standard BinNet and Fair BinNet on LFM-1b Dataset. Fair BinNet provides a more diversified recommendation network. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.7 Application of Fair BinNet to Music Recommendation Systems ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "LFM-1b Dataset4 contains over one billion listening events intended for use in recommendation systems [66]. In this experiment, we use the user-artist play counts dataset to construct a recommendation network of artists. Our analysis focuses on 80 artists intersecting the 2016 Billboard Artist 100 and 1,807 randomly selected users who listened to at least 400 songs. We transform the play counts into binary datasets for BinNet models, setting play counts above 0 to 1 and all others to 0. ", "page_idx": 8}, {"type": "text", "text": "This experiment examines male and female categories, stratifying the dataset into two groups with 1,341 and 466 samples, respectively. We set the BinNet models\u2019 parameter, $\\lambda$ , to $1e-5$ . ", "page_idx": 8}, {"type": "text", "text": "Results. Figures 3c-3d show the recommendation networks of the 2016 Billboard Top 10 popular music artists based on BinNet\u2019s and Fair BinNet\u2019s outputs. The comparative analysis reveals that Fair BinNet provides a more diversified recommendation network, particularly for the artist The Weeknd. Enhancing fairness fosters cross-group musical preference exchange, breaks the echo chamber effect, and broadens users\u2019 exposure to potentially intriguing music, enhancing the user-friendliness of the music recommendation system. ", "page_idx": 8}, {"type": "text", "text": "4.8 Trade-off Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In fairness studies, the trade-off between fairness and model accuracy presents a fundamental challenge. An effective fair method should achieve equitable outcomes while maintaining strong accuracy performance. We evaluate this balance by analyzing the percentage changes in both accuracy a, iarnned Sofp Fea GicMa l\u2212ly\u2206, owf .se changes as: $\\begin{array}{r}{\\%F_{1}=-\\frac{F_{1}\\ \\mathrm{of}\\ \\!\\mathrm{Fair}\\,\\mathrm{GM}\\,-F_{1}\\ \\mathrm{o}}{F_{1}\\ \\mathrm{of}\\,\\mathrm{GM}}}\\end{array}$ f $\\underline{{\\mathrm{GM}}}_{\\mathrm{~X~}}\\dot{}$ $100\\%$ $\\%\\Delta=-\\frac{\\Delta}{\\Delta}$ $\\Delta$ $\\frac{\\mathrm{GM}}{}\\times100\\%$ ", "page_idx": 8}, {"type": "text", "text": "Our empirical results (Tables 2, 4, and Figure 4) demonstrate that Fair GMs substantially reduce disparity error, thereby improving fairness, while incurring only minimal degradation in the objective function\u2019s value. This favorable trade-off validates the effectiveness of our approach. However, Fair GMs do face computational challenges, primarily stemming from two sources: local graph computation and multi-objective optimization. ", "page_idx": 8}, {"type": "image", "img_path": "WvWS8goWyR/tmp/fc75655136ea8f7dd3624c356b34bbd96c2b023a6514dd3d3d7d28492abd134a.jpg", "img_caption": ["Figure 4: Percentage change from GMs to Fair GMs (results from Tables 2 and 4). $\\%F_{1}$ is slight, while $\\%\\Delta$ changes are substantial, signifying fairness improvement without serious accuracy sacrifice. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "To address these limitations, we propose several solutions. The local graph learning phase can be accelerated using advanced graphical model algorithms such as QUIC [32], SQUIC [7], PISTA [69], GISTA [60], OBN [57], or ALM [67]. Moreover, to mitigate the increased complexity from multiple objectives, we introduce a stochastic objective selection strategy, randomly sampling a subset of objectives in each iteration. This approach effectively reduces computational overhead while maintaining model fairness and performance. To validate these computational considerations, we conducted additional experiments using GLasso, with detailed results presented in the Appendix D.10. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we tackle fairness in graphical models (GMs) such as Gaussian, Covariance, and Ising models, which interpret complex relationships in high-dimensional data. Standard GMs exhibit bias with sensitive group data. We propose a framework incorporating a pairwise graph disparity error term and a custom loss function into a nonsmooth multi-objective optimization. This approach enhances fairness without compromising performance, validated by experiments on synthetic and real-world datasets. However, it increases computational complexity and may be sensitive to the choice of loss function and balancing multiple objectives. Future research can include: ", "page_idx": 9}, {"type": "text", "text": "F1. Integrating our Fair GMs approach with supervised methods for downstream tasks, including spectral clustering [82], graph regularized dimension reduction [76]. F2. Developing novel group fairness notions based on sensitive attributes within our nonsmooth multi-objective optimization framework. F3. Extending fairness to ordinal data models, which are crucial for socioeconomic and healthrelated applications [27], neighborhood selection [50], and partial correlation estimation [40]. ", "page_idx": 9}, {"type": "text", "text": "Despite some limitations of Fair GMs for larger group sizes, this work demonstrates the potential of nonsmooth multi-objective optimization as a powerful tool for mitigating biases and promoting fairness in high-dimensional graph-based machine learning, contributing to the development of more equitable and responsible AI systems across a wide range of domains. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the NIH grants U01 AG066833, U01 AG068057, U19 AG074879, RF1 AG068191, RF1 AG063481, R01 LM013463, P30 AG073105, U01 CA274576, RF1-AG063481, R01-AG071174, and U01-CA274576. The ADNI data were obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative database (https://adni.loni.usc.edu), funded by NIH U01 AG024904. The authors thank Laura Balzano and Alfred O. Hero for their helpful suggestions and discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Bailey Andrew, David Westhead, and Luisa Cutillo. antglasso: An efficient tensor graphical lasso algorithm. In NeurIPS 2022 Workshop: New Frontiers in Graph Learning, 2022. ", "page_idx": 9}, {"type": "text", "text": "[2] Arthur Asuncion and David Newman. Uci machine learning repository, 2007. ", "page_idx": 9}, {"type": "text", "text": "[3] Onureena Banerjee, Laurent El Ghaoui, Alexandre d\u2019Aspremont, and Georges Natsoulis. Convex optimization techniques for ftiting sparse gaussian graphical models. In Proceedings of the 23rd international conference on Machine learning, pages 89\u201396, 2006.   \n[4] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019. http://www.fairmlbook.org.   \n[5] Dimitri P Bertsekas, W Hager, and O Mangasarian. Nonlinear programming. athena scientific belmont. Massachusets, USA, 1999.   \n[6] Foucaud du Boisgueheneuc, Richard Levy, Emmanuelle Volle, Magali Seassau, Hughes Duffau, Serge Kinkingnehun, Yves Samson, Sandy Zhang, and Bruno Dubois. Functions of the left superior frontal gyrus in humans: a lesion study. Brain, 129(12):3315\u20133328, 2006.   \n[7] Matthias Bollhofer, Aryan Eftekhari, Simon Scheidegger, and Olaf Schenk. Large-scale sparse inverse covariance matrix estimation. SIAM Journal on Scientific Computing, 41(1):A380\u2013A401, 2019.   \n[8] Sabri Boutemedjet and Djemel Ziou. A graphical model for context-aware visual content recommendation. IEEE Transactions on Multimedia, 10(1):52\u201362, 2007.   \n[9] Simon Caton and Christian Haas. Fairness in machine learning: A survey. arXiv preprint arXiv:2010.04053, 2020.   \n[10] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, pages 5029\u20135037, 2017.   \n[11] Alexandra Chouldechova and Aaron Roth. The frontiers of fairness in machine learning. arXiv preprint arXiv:1810.08810, 2018.   \n[12] Rachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the compatibility of privacy and fairness. In Adjunct publication of the 27th conference on user modeling, adaptation and personalization, pages 309\u2013315, 2019.   \n[13] Patrick Danaher, Pei Wang, and Daniela M Witten. The joint graphical lasso for inverse covariance estimation across multiple classes. Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(2):373\u2013397, 2014.   \n[14] Rahul S Desikan, Florent S\u00e9gonne, Bruce Fischl, Brian T Quinn, Bradford C Dickerson, Deborah Blacker, Randy L Buckner, Anders M Dale, R Paul Maguire, Bradley T Hyman, et al. An automated labeling system for subdividing the human cerebral cortex on mri scans into gyral based regions of interest. Neuroimage, 31(3):968\u2013980, 2006.   \n[15] Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil. Empirical risk minimization under fairness constraints. arXiv preprint arXiv:1802.08626, 2018.   \n[16] Noureddine EL KAROUI. Operator norm consistent estimation of large-dimensional sparse covariance matrices. Annals of statistics, 36(6):2717\u20132756, 2008.   \n[17] Alireza Farasat, Alexander Nikolaev, Sargur N Srihari, and Rachael Hageman Blair. Probabilistic graphical models in modern social network analysis. Social Network Analysis and Mining, 5:1\u201318, 2015.   \n[18] Yasmeen Faroqi-Shah, Therese Kling, Jeffrey Solomon, Siyuan Liu, Grace Park, and Allen Braun. Lesion analysis of language production deficits in aphasia. Aphasiology, 28(3):258\u2013277, 2014.   \n[19] Salar Fattahi and Somayeh Sojoudi. Graphical lasso and thresholding: Equivalence and closed-form solutions. Journal of machine learning research, 20(10):1\u201344, 2019.   \n[20] J\u00f6rg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods of operations research, 51:479\u2013494, 2000.   \n[21] Laura Fratiglioni, Stephanie Paillard-Borg, and Bengt Winblad. An active and socially integrated lifestyle in late life might protect against dementia. The Lancet Neurology, 3(6):343\u2013353, 2004.   \n[22] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432\u2013441, 2008.   \n[23] Lingrui Gan, Xinming Yang, Naveen N Nariestty, and Feng Liang. Bayesian joint estimation of multiple graphical models. Advances in Neural Information Processing Systems, 32, 2019.   \n[24] Mireille El Gheche and Pascal Frossard. Multilayer clustered graph learning. arXiv preprint arXiv:2010.15456, 2020.   \n[25] Jian Guo, Elizaveta Levina, George Michailidis, and Ji Zhu. Joint structure estimation for categorical markov networks. Unpublished manuscript, 3(5.2):6, 2010.   \n[26] Jian Guo, Elizaveta Levina, George Michailidis, and Ji Zhu. Joint estimation of multiple graphical models. Biometrika, 98(1):1\u201315, 2011.   \n[27] Jian Guo, Elizaveta Levina, George Michailidis, and Ji Zhu. Graphical models for ordinal data. Journal of Computational and Graphical Statistics, 24(1):183\u2013204, 2015.   \n[28] Nika Haghtalab, Michael Jordan, and Eric Zhao. A unifying perspective on multi-calibration: Game dynamics for multi-objective learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. arXiv preprint arXiv:1610.02413, 2016.   \n[30] Douglas M Hawkins. The problem of overftiting. Journal of chemical information and computer sciences, 44(1):1\u201312, 2004.   \n[31] Holger H\u00f6fling and Robert Tibshirani. Estimation of sparse binary pairwise markov networks using pseudo-likelihoods. Journal of Machine Learning Research, 10(4), 2009.   \n[32] Cho-Jui Hsieh, M\u00e1ty\u00e1s A Sustik, Inderjit S Dhillon, Pradeep Ravikumar, et al. Quic: quadratic approximation for sparse inverse covariance estimation. J. Mach. Learn. Res., 15(1):2911\u20132947, 2014.   \n[33] Michael Isard. Pampas: Real-valued graphical models for computer vision. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings., volume 1, pages I\u2013I. IEEE, 2003.   \n[34] Michael I Jordan. Graphical models. Statistical Science, pages 140\u2013155, 2004.   \n[35] Mohammad Mahdi Kamani, Farzin Haddadpour, Rana Forsati, and Mehrdad Mahdavi. Efficient fair principal component analysis. Machine Learning, pages 1\u201332, 2022.   \n[36] Minoru Kanehisa and Susumu Goto. Kegg: kyoto encyclopedia of genes and genomes. Nucleic acids research, 28(1):27\u201330, 2000.   \n[37] Noureddine El Karoui. Operator norm consistent estimation of large-dimensional sparse covariance matrices. The Annals of Statistics, pages 2717\u20132756, 2008.   \n[38] Philipp Kellmeyer, Wolfram Ziegler, Claudia Peschke, Eisenberger Juliane, Susanne Schnell, Annette Baumgaertner, Cornelius Weiller, and Dorothee Saur. Fronto-parietal dorsal and ventral pathways in the context of different linguistic manipulations. Brain and language, 127(2):241\u2013250, 2013.   \n[39] Aria Khademi, Sanghack Lee, David Foley, and Vasant Honavar. Fairness in algorithmic decision making: An excursion through the lens of causality. In The World Wide Web Conference, pages 2907\u20132914, 2019.   \n[40] Kshitij Khare, Sang-Yun Oh, and Bala Rajaratnam. A convex pseudolikelihood framework for high dimensional partial correlation estimation with convergence guarantees. Journal of the Royal Statistical Society: Series B: Statistical Methodology, pages 803\u2013825, 2015.   \n[41] Matth\u00e4us Kleindessner, Samira Samadi, Pranjal Awasthi, and Jamie Morgenstern. Guarantees for spectral clustering with fairness constraints. In International Conference on Machine Learning, pages 3458\u20133467. PMLR, 2019.   \n[42] Sandeep Kumar, Jiaxi Ying, Jos\u00e9 Vin\u00edcius de Miranda Cardoso, and Daniel P Palomar. A unified framework for structured graph learning via spectral constraints. Journal of Machine Learning Research, 21(22):1\u201360, 2020.   \n[43] Su-In Lee, Varun Ganapathi, and Daphne Koller. Efficient structure learning of markov networks using l_1-regularization. Advances in neural Information processing systems, 19, 2006.   \n[44] Fredrik Liljeros, Christofer R Edling, Luis A Nunes Amaral, H Eugene Stanley, and Yvonne \u00c5berg. The web of human sexual contacts. Nature, 411(6840):907\u2013908, 2001.   \n[45] Camila M Lopes-Ramos, John Quackenbush, and Dawn L DeMeo. Genome-wide sex and gender differences in cancer. Frontiers in oncology, 10:597788, 2020.   \n[46] Val J Lowe, Geoffry Curran, Ping Fang, Amanda M Liesinger, Keith A Josephs, Joseph E Parisi, Kejal Kantarci, Bradley F Boeve, Mukesh K Pandey, Tyler Bruinsma, et al. An autoradiographic evaluation of av-1451 tau pet in dementia. Acta neuropathologica communications, 4:1\u201319, 2016.   \n[47] Benjamin M Marlin and Kevin P Murphy. Sparse gaussian graphical models with unknown block structure. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 705\u2013712, 2009.   \n[48] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):1\u201335, 2021.   \n[49] Kala M Mehta and Gwen W Yeo. Systematic review of dementia prevalence and incidence in united states race/ethnic populations. Alzheimer\u2019s & Dementia, 13(1):72\u201383, 2017.   \n[50] Nicolai Meinshausen, Peter B\u00fchlmann, et al. High-dimensional graphs and variable selection with the lasso. Annals of statistics, 34(3):1436\u20131462, 2006.   \n[51] Karthik Mohan, Mike Chung, Seungyeop Han, Daniela Witten, Su-In Lee, and Maryam Fazel. Structured learning of gaussian graphical models. Advances in neural information processing systems, 25, 2012.   \n[52] Madeline Navarro, Samuel Rey, Andrei Buciulea, Antonio G Marques, and Santiago Segarra. Fair glasso: Estimating fair graphical models with unbiased statistical behavior. arXiv preprint arXiv:2406.09513, 2024.   \n[53] Madeline Navarro, Samuel Rey, Andrei Buciulea, Antonio G Marques, and Santiago Segarra. Mitigating subpopulation bias for fair network topology inference. arXiv preprint arXiv:2403.15591, 2024.   \n[54] Hwamee Oh, Jason Steffener, Qolamreza R Razlighi, Christian Habeck, and Yaakov Stern. $\\beta$ -amyloid deposition is associated with decreased right prefrontal activation during task switching among cognitively normal elderly. Journal of Neuroscience, 36(6):1962\u20131970, 2016.   \n[55] Matt Olfat and Anil Aswani. Convex formulations for fair principal component analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 663\u2013670, 2019.   \n[56] Luca Oneto and Silvia Chiappa. Fairness in machine learning. In Recent Trends in Learning From Data, pages 155\u2013196. Springer, 2020.   \n[57] Figen Oztoprak, Jorge Nocedal, Steven Rennie, and Peder A Olsen. Newton-like methods for sparse inverse covariance estimation. Advances in neural information processing systems, 25, 2012.   \n[58] Pradeep Ravikumar, Martin J Wainwright, John D Lafferty, et al. High-dimensional ising model selection using $\\hat{\\ell_{1}}$ -regularized logistic regression. The Annals of Statistics, 38(3):1287\u20131319, 2010.   \n[59] Garry Robins, Pip Pattison, Yuval Kalish, and Dean Lusher. An introduction to exponential random graph $p*$ models for social networks. Social networks, 29(2):173\u2013191, 2007.   \n[60] Benjamin Rolfs, Bala Rajaratnam, Dominique Guillot, Ian Wong, and Arian Maleki. Iterative thresholding algorithm for sparse inverse covariance estimation. Advances in Neural Information Processing Systems, 25, 2012.   \n[61] David L Roth, Mary S Mittelman, Olivio J Clay, Alok Madan, and William E Haley. Changes in social support as mediators of the impact of a psychosocial intervention for spouse caregivers of persons with alzheimer\u2019s disease. Psychology and aging, 20(4):634, 2005.   \n[62] Adam J Rothman. Positive definite estimators of large covariance matrices. Biometrika, 99(3):733\u2013740, 2012.   \n[63] Joshua B Rubin, Joseph S Lagas, Lauren Broestl, Jasmin Sponagel, Nathan Rockwell, Gina Rhee, Sarah F Rosen, Si Chen, Robyn S Klein, Princess Imoukhuede, et al. Sex differences in cancer mechanisms. Biology of sex Differences, 11:1\u201329, 2020.   \n[64] Peter H Rudebeck and Erin L Rich. Orbitofrontal cortex. Current Biology, 28(18):R1083\u2013R1088, 2018.   \n[65] Samira Samadi, Uthaipon Tantipongpipat, Jamie Morgenstern, Mohit Singh, and Santosh Vempala. The price of fair pca: One extra dimension. arXiv preprint arXiv:1811.00103, 2018.   \n[66] Markus Schedl. The lfm-1b dataset for music retrieval and recommendation. In Proceedings of the 2016 ACM on international conference on multimedia retrieval, pages 103\u2013110, 2016.   \n[67] Katya Scheinberg, Shiqian Ma, and Donald Goldfarb. Sparse inverse covariance selection via alternating linearization methods. Advances in neural information processing systems, 23, 2010.   \n[68] Jorge Sepulcre, Mert R Sabuncu, Alex Becker, Reisa Sperling, and Keith A Johnson. In vivo characterization of the early states of the amyloid-beta network. Brain, 136(7):2239\u20132252, 2013.   \n[69] Gal Shalom, Eran Treister, and Irad Yavneh. pista: Preconditioned iterative soft thresholding algorithm for graphical lasso. SIAM Journal on Scientific Computing, 46(2):S445\u2013S466, 2024.   \n[70] Changjian Shui, Gezheng Xu, Qi Chen, Jiaqi Li, Charles X Ling, Tal Arbel, Boyu Wang, and Christian Gagn\u00e9. On learning fairness and accuracy on multiple subgroups. Advances in Neural Information Processing Systems, 35:34121\u201334135, 2022.   \n[71] Andrew Sommerlad, Joshua Ruegger, Archana Singh-Manoux, Glyn Lewis, and Gill Livingston. Marriage and risk of dementia: systematic review and meta-analysis of observational studies. Journal of Neurology, Neurosurgery & Psychiatry, 89(3):231\u2013238, 2018.   \n[72] Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel, and Daniela Witten. Learning graphical models with hubs. Journal of Machine Learning Research, 15:3297\u20133331, 2014.   \n[73] Hiroki Tanabe, Ellen H Fukuda, and Nobuo Yamashita. Proximal gradient methods for multiobjective optimization and their applications. Computational Optimization and Applications, 72:339\u2013361, 2019.   \n[74] Hiroki Tanabe, Ellen H Fukuda, and Nobuo Yamashita. A globally convergent fast iterative shrinkagethresholding algorithm with a new momentum factor for single and multi-objective convex optimization. arXiv preprint arXiv:2205.05262, 2022.   \n[75] Hiroki Tanabe, Ellen H Fukuda, and Nobuo Yamashita. Convergence rates analysis of a multiobjective proximal gradient method. Optimization Letters, 17(2):333\u2013350, 2023.   \n[76] Mengfan Tang, Feiping Nie, and Ramesh Jain. A graph regularized dimension reduction method for out-of-sample data. Neurocomputing, 225:58\u201363, 2017.   \n[77] Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie H Morgenstern, and Santosh Vempala. Multi-criteria dimensionality reduction with applications to fairness. Advances in neural information processing systems, 32, 2019.   \n[78] Davoud Ataee Tarzanagh, Laura Balzano, and Alfred O Hero. Fair community detection and structure learning in heterogeneous graphical models. arXiv preprint arXiv:2112.05128, 2021.   \n[79] Davoud Ataee Tarzanagh, Bojian Hou, Boning Tong, Qi Long, and Li Shen. Fairness-aware class imbalanced learning on multiple subgroups. In Uncertainty in Artificial Intelligence, pages 2123\u20132133. PMLR, 2023.   \n[80] Davoud Ataee Tarzanagh and George Michailidis. Estimation of graphical models through structured norm minimization. Journal of Machine Learning Research, 18(209):1\u201348, 2018.   \n[81] Francesco Tomaiuolo, JD MacDonald, Zografos Caramanos, Glenn Posner, Mary Chiavaras, Alan C Evans, and Michael Petrides. Morphology, morphometry and probability mapping of the pars opercularis of the inferior frontal gyrus: an in vivo mri analysis. European Journal of Neuroscience, 11(9):3033\u20133046, 1999.   \n[82] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17:395\u2013416, 2007.   \n[83] Hieu Vu, Toan Tran, Man-Chung Yue, and Viet Anh Nguyen. Distributionally robust fair principal components via geodesic descents. arXiv preprint arXiv:2202.03071, 2022.   \n[84] Xiwen Wang, Jiaxi Ying, and Daniel Palomar. Learning large-scale mtp _2 gaussian graphical models via bridge-block decomposition. Advances in Neural Information Processing Systems, 36:73211\u201373231, 2023.   \n[85] Michael W Weiner, Dallas P Veitch, Paul S Aisen, Laurel A Beckett, Nigel J Cairns, Robert C Green, Danielle Harvey, Clifford R Jack, William Jagust, Enchi Liu, et al. The alzheimer\u2019s disease neuroimaging initiative: a review of papers published since its inception. Alzheimer\u2019s & Dementia, 9(5):e111\u2013e194, 2013.   \n[86] Michael W Weiner, Dallas P Veitch, Paul S Aisen, Laurel A Beckett, Nigel J Cairns, Robert C Green, Danielle Harvey, Clifford R Jack Jr, William Jagust, John C Morris, et al. Recent publications from the alzheimer\u2019s disease neuroimaging initiative: Reviewing progress toward improved ad clinical trials. Alzheimer\u2019s & Dementia, 13(4):e1\u2013e85, 2017.   \n[87] Daniela M Witten, Jerome H Friedman, and Noah Simon. New insights and faster computations for the graphical lasso. Journal of Computational and Graphical Statistics, 20(4):892\u2013900, 2011.   \n[88] Dean F Wong, Paul B Rosenberg, Yun Zhou, Anil Kumar, Vanessa Raymont, Hayden T Ravert, Robert F Dannals, Ayon Nandi, James R Bra\u0161ic\u00b4, Weiguo Ye, et al. In vivo imaging of amyloid deposition in alzheimer disease using the radioligand 18f-av-45 (flobetapir f 18). Journal of nuclear medicine, 51(6):913\u2013920, 2010.   \n[89] Eunho Yang and Aur\u00e9lie C Lozano. Robust gaussian graphical modeling with the trimmed graphical lasso. Advances in neural information processing systems, 28, 2015.   \n[90] I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert systems with applications, 36(2):2473\u20132480, 2009.   \n[91] Jianxin Yin and Hongzhe Li. A sparse conditional gaussian graphical model for analysis of genetical genomics data. The annals of applied statistics, 5(4):2630, 2011.   \n[92] Guanhua Zhang, Yihua Zhang, Yang Zhang, Wenqi Fan, Qing Li, Sijia Liu, and Shiyu Chang. Fairness reprogramming. Advances in Neural Information Processing Systems, 35:34347\u201334362, 2022.   \n[93] Xiang Zhang and Qiao Wang. A unified framework for fair spectral clustering with effective graph learning. arXiv preprint arXiv:2311.13766, 2023.   \n[94] Xiang Zhang and Qiao Wang. A dual laplacian framework with effective graph learning for unified fair spectral clustering. Neurocomputing, page 128210, 2024.   \n[95] Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Boning Tong, Jia Xu, Yanbo Feng, Qi Long, and Li Shen. Fair canonical correlation analysis. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Summary of the Notations 17 ", "page_idx": 15}, {"type": "text", "text": "B Addendum to Section 3 18 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Dual Reformulation and Computation of Subproblem (9) 18   \nB.2 Subproblem Solver for Fair GMs 19   \nB.2.1 Fair GLasso . . 19   \nB.2.2 Fair CovGraph 20   \nB.2.3 Fair BinNet 20 ", "page_idx": 15}, {"type": "text", "text": "C Addendum to Section 3.3 21 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Auxiliary Lemmas 21   \nC.2 Proof of Theorem 6 for Fair GLasso 22   \nC.3 Proof of Theorem 7 for Fair CovGraph 24   \nC.4 Proof of Theorem 8 for Fair BinNet 25   \nC.5 Computational Complexity of FairGMs 26 ", "page_idx": 15}, {"type": "text", "text": "D Addendum to Section 4 27 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Iterative Soft-Thresholding Algorithm (ISTA) 27   \nD.2 Simulation Study of Fair GLasso 27   \nD.3 Simulation Study of Fair BinNet 27   \nD.4 Addendum to Subsection 4.5 28   \nD.5 Addendum to Subsection 4.6 28   \nD.6 Sensitivity Analysis to Feature Size $P$ 30   \nD.7 Sensitivity Analysis to Sample Size $N$ 30   \nD.8 Sensitivity Analysis to Sample Size Ratio $N_{2}/N_{1}$ 31   \nD.9 Sensitivity Analysis to Group Size $K$ 32   \nD.10 Addendum to Subsection 4.8 . . 33 ", "page_idx": 15}, {"type": "table", "img_path": "WvWS8goWyR/tmp/de5be453a0b8264c672197f31edc317b3557008261b1dc504152ac68678cd66e.jpg", "table_caption": ["Table 3: Summary of the Notations "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Addendum to Section 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Dual Reformulation and Computation of Subproblem (9) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide a dual method for solving the Subproblem (9) defined as: $\\operatorname*{min}_{\\Phi\\in\\mathcal{M}}\\quad\\varphi_{\\ell}\\left(\\Phi;\\Theta\\right),$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{with}\\quad\\varphi_{\\ell}\\left(\\Phi;\\Theta\\right)=\\operatorname*{max}_{k\\in\\{1,\\dots,M\\}}\\left\\langle\\nabla f_{k}(\\Theta),\\Phi-\\Theta\\right\\rangle+g(\\Phi)-g(\\Theta)+\\frac{\\ell}{2}\\left\\|\\Phi-\\Theta\\right\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $\\ell>L$ where $L$ is defined in Assumption A. ", "page_idx": 17}, {"type": "text", "text": "For simplicity, let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\psi_{k,\\ell}\\left(\\Phi;\\Theta\\right)=\\langle\\nabla f_{k}(\\Theta),\\Phi-\\Theta\\rangle+g(\\Phi)-g(\\Theta)+\\frac{\\ell}{2}\\|\\Phi-\\Theta\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By considering the standard simplex, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{C}:=\\left\\{\\rho\\in\\mathbb{R}^{M}:~\\sum_{k=1}^{M}\\rho_{k}=1,~\\rho_{k}\\in[0,1],~\\,\\forall k\\in[M]\\right\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we reformulate (11) as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Phi\\in\\mathcal{M}}\\operatorname*{max}_{\\rho\\in\\mathcal{C}}\\;\\sum_{k=1}^{M}\\rho_{k}\\psi_{k,\\ell}\\left(\\Phi;\\Theta\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By leveraging the convexity of $\\mathcal{M}$ , the compactness and convexity of $\\mathcal{C}$ , and the convexity-concavity property of $\\begin{array}{r}{\\sum_{k=1}^{M}\\rho_{k}\\psi_{k,\\ell}\\left(\\boldsymbol{\\Phi};\\boldsymbol{\\Theta}\\right)}\\end{array}$ with respect to $\\Phi$ and $\\rho$ , respectively, we can invoke Sion\u2019s minimax theorem to  reformulate (14) as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\rho\\in\\mathcal{C}}\\operatorname*{min}_{\\Phi\\in\\mathcal{M}}\\;\\sum_{k=1}^{M}\\rho_{k}\\psi_{k,\\ell}\\left(\\Phi;\\Theta\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Expanding on the definition of $\\psi_{k,\\ell}$ , we arrive at the following expression: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\rho\\in\\mathbb{C}}{\\operatorname*{max}}\\ \\frac{\\mathrm{in}}{\\mathrm{et}(\\mathbf{\\hat{c}})+\\mathrm{e}(\\mathbf{\\hat{c}})}\\times\\frac{M}{k!}\\rho_{k}\\mathrm{,}(\\mathbf{\\hat{c}})\\Theta}&{}\\\\ &{\\qquad=\\underset{\\rho\\in\\mathbb{C}}{\\operatorname*{max}}\\ \\frac{\\mathrm{in}}{\\mathrm{et}(\\mathbf{\\hat{c}})+\\mathrm{e}(\\mathbf{\\hat{c}})}\\left[g(\\mathbf{\\hat{c}})+\\frac{\\ell}{2}\\left\\|\\Phi-\\left(\\mathbf{e}+\\frac{1}{\\ell}\\sum_{k=1}^{M}\\rho_{k}\\nabla f_{k}(\\Theta)\\right)\\right\\|_{F,\\cdot}^{2}\\right]}\\\\ &{\\qquad\\quad-\\frac{1}{2\\ell}\\left\\|\\underset{k=1}{\\overset{M}{\\sum}}\\rho_{k}\\nabla f_{k}\\left(\\Theta\\right)\\right\\|_{F}^{2}-g\\left(\\Theta\\right)}\\\\ &{\\qquad=\\underset{\\rho\\in\\mathbb{C}}{\\operatorname*{max}}\\ \\mathcal{U}\\mathrm{i}_{\\frac{1}{\\ell}\\rho}\\left(\\Theta-\\frac{1}{\\ell}\\sum_{k=1}^{M}\\rho_{k}\\nabla f_{k}(\\Theta)\\right)}\\\\ &{\\qquad\\quad-\\frac{1}{2\\ell}\\left\\|\\underset{k=1}{\\overset{M}{\\sum}}\\rho_{k}\\nabla f_{k}(\\Theta)\\right\\|_{F}^{2}-g\\left(\\Theta\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where Moreau envelope $M_{\\alpha h}(x)$ and the proximal operator are defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M_{\\alpha h}({\\boldsymbol{x}}):=\\operatorname*{min}_{\\boldsymbol{y}}\\left[h({\\boldsymbol{y}})+\\frac{1}{2\\alpha}\\|{\\boldsymbol{x}}-{\\boldsymbol{y}}\\|^{2}\\right],}}\\\\ {{\\displaystyle\\mathbf{prox}_{\\alpha h}({\\boldsymbol{x}}):=\\arg\\operatorname*{min}_{\\boldsymbol{y}}\\left[h({\\boldsymbol{y}})+\\frac{1}{2\\alpha}\\|{\\boldsymbol{x}}-{\\boldsymbol{y}}\\|^{2}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Problem (16) is equivalent to the dual problem: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\rho\\in\\mathbb{R}^{M}}\\;\\;\\omega(\\rho)\\qquad\\mathrm{subj.~to}\\qquad\\rho\\succeq\\mathbf{0}\\;\\mathrm{~and~}\\sum_{k=1}^{M}\\rho_{k}=1,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\omega(\\rho)=\\ell M_{\\frac{1}{\\ell}g}\\left(\\Theta-\\frac{1}{\\ell}\\sum_{k=1}^{M}\\rho_{k}\\nabla f_{k}\\left(\\Theta\\right)\\right)-\\frac{1}{2\\ell}\\left\\|\\sum_{k=1}^{M}\\rho_{k}\\nabla f_{k}\\left(\\Theta\\right)\\right\\|_{F}^{2}-g\\left(\\Theta\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Upon solving the dual Problem (18), the optimal solution $\\Phi^{*}$ of (11) is obtained through: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi^{*}=\\mathbf{prox}_{\\frac{1}{\\ell}g}\\left(\\Theta-\\frac{1}{\\ell}\\sum_{k=1}^{M}\\rho_{k}\\nabla f_{k}\\left(\\Theta\\right)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the implementation, for the given $g\\left(\\Theta\\right)=\\lambda\\|\\Theta\\|_{1},\\mathbf{prox}_{\\frac{1}{\\ell}g}$ is computed using soft thresholding $\\eta_{\\frac{1}{\\ell}\\lambda}$ , as shown below: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\eta_{\\frac{1}{\\ell}\\lambda}\\left(\\mathbf{x}\\right)\\right)_{j}=\\operatorname{sign}(x_{j})\\operatorname*{max}\\left(\\left|x_{j}\\right|-\\frac{1}{\\ell}\\lambda,0\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To provide a clear and logical summary of the iterative update process in Algorithm 1, we proceed as follows: At each iteration $t$ , the update for $\\Theta^{(t+1)}$ is performed by inputting $\\Theta^{(t)}$ and solving the Subproblem (11). This is achieved by utilizing the scipy.optimize.minimize function with the method $\\equiv$ \"trust-constr\" option to solve the dual problem. Specifically, for given constants $\\ell>L$ and $\\lambda>0$ , and the calculated $\\rho^{(t)}\\in\\mathcal{C}$ at the tth iteration, the update rule for $\\Theta^{(t+1)}$ is given by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Theta^{(t+1)}=\\eta_{\\frac{1}{\\ell}\\lambda}\\left(\\Theta^{(t)}-\\frac{1}{\\ell}\\sum_{k=1}^{M}\\rho_{k}^{(t)}\\nabla f_{k}\\left(\\Theta^{(t)}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which incorporates the proximal operator and the weighted sum of gradients. Through solving Subproblem (9), the following proposition characterizes the weak Pareto optimality in the context of multi-objective optimization Problem (8): ", "page_idx": 18}, {"type": "text", "text": "Proposition 11. Let $\\begin{array}{r}{\\omega_{\\ell}\\left(\\Theta\\right):=\\operatorname*{min}_{\\Phi\\in\\mathcal{M}}\\varphi_{\\ell}\\left(\\Phi;\\Theta\\right)}\\end{array}$ and $\\mathbf{P}_{\\ell}$ be defined as in (9). Then, ", "page_idx": 18}, {"type": "text", "text": "(i) The following conditions are equivalent: (a) $\\Theta$ is a weakly Pareto optimal point; (b) $\\mathbf{P}_{\\ell}\\left(\\pmb{\\Theta}\\right)=\\pmb{\\Theta}$ ; (c) $\\omega_{\\ell}\\left(\\Theta\\right)=0$ .   \n(ii) The mappings $\\mathbf{P}_{\\ell}$ and $\\omega_{\\ell}$ are both continuous. ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof follows from [73, Lemma 3.2] and the convexity of $f_{k}$ . The detailed convexity analyses for Fair GLasso, Fair CovGraph, and Fair BinNet are provided in Sections C.2, C.3, and C.4, respectively. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "As demonstrated in the analysis of Subproblem (9) in the beginning of this section, the proposition implies that the descent direction is the minimum norm matrix within the convex hull of the gradients of all objectives. Furthermore, this direction is non-increasing with respect to each individual objective function. This property ensures that the chosen descent direction simultaneously minimizes the overall norm while guaranteeing non-increasing behavior for each objective, thereby facilitating the optimization process in a multi-objective setting. ", "page_idx": 18}, {"type": "text", "text": "B.2 Subproblem Solver for Fair GMs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.2.1 Fair GLasso ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To update $\\Theta^{(t)}$ in Algorithm 1 applied to (Fair GLasso), the iterative update formula in Equation (21) is used at each iteration. The gradients for the functions f1 and {fk+1}kM=\u221211 are computed as follows: The gradient of $f_{1}$ with respect to $\\Theta$ is given by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla f_{1}(\\Theta)=\\mathbf{S}-\\Theta^{-1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where S is the sample covariance matrix and $\\Theta^{-1}$ is the inverse of the precision matrix $\\Theta$ . ", "page_idx": 18}, {"type": "text", "text": "The gradient of $f_{k+1}$ for $k=1,\\dotsc,M-1$ with respect to $\\Theta$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{k+1}(\\Theta)=\\displaystyle\\sum_{s\\in[K],s\\neq k}\\left(\\mathrm{trace}(\\mathbf{S}_{k}\\Theta)-\\mathrm{trace}(\\mathbf{S}_{s}\\Theta)+\\log\\operatorname*{det}(\\Theta_{k}^{*})\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\left.-\\operatorname{trace}(\\mathbf{S}_{k}\\Theta_{k}^{*})-\\log\\operatorname*{det}(\\Theta_{s}^{*})+\\mathrm{trace}(\\mathbf{S}_{s}\\Theta_{s}^{*})\\right)(\\mathbf{S}_{k}-\\mathbf{S}_{s})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $K$ is the number of groups, $\\mathbf{S}_{k}$ and $\\mathbf{S}_{s}$ are the sample covariance matrices for groups $k$ and $s$ , respectively, and $\\Theta_{k}^{*}$ and $\\boldsymbol{\\Theta}_{s}^{*^{\\lambda}}$ are the optimal precision matrices for groups $k$ and $s$ obtained by solving the group-specific GLasso problems. ", "page_idx": 19}, {"type": "text", "text": "B.2.2 Fair CovGraph ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To refine the iterative update rule for estimating the fair covariance matrix $\\Sigma$ in (Fair GLasso) using Algorithm 1, the gradients for $f_{1}$ and the set $\\bar{\\{f_{k+1}\\}}_{k=1}^{M-1}$ are computed as follows: ", "page_idx": 19}, {"type": "text", "text": "The gradient of $f_{1}$ with respect to $\\Sigma$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla f_{1}(\\Sigma)=\\Sigma-{\\bf S}-\\tau\\Sigma^{-1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{S}$ is the pooled sample covariance matrix, $\\tau$ is the regularization parameter, and $\\Sigma^{-1}$ is the inverse of the covariance matrix $\\Sigma$ . ", "page_idx": 19}, {"type": "text", "text": "The gradient of $f_{k+1}$ for $k=1,\\dotsc,M-1$ with respect to $\\Sigma$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{k+1}(\\mathbf{\\Sigma})=\\displaystyle\\sum_{s\\in[K],s\\neq k}\\left(\\frac{1}{2}\\|\\Sigma-\\mathbf{S}_{k}\\|_{F}^{2}-\\frac{1}{2}\\|\\Sigma-\\mathbf{S}_{s}\\|_{F}^{2}+\\tau\\log\\operatorname*{det}(\\mathbf{\\Sigma}\\mathbf{S}_{k}^{*})\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\left.-\\frac{1}{2}\\|\\mathbf{\\Sigma}\\mathbf{\\leq}-\\mathbf{S}_{k}\\|_{F}^{2}-\\tau\\log\\operatorname*{det}(\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{s}^{*})+\\frac{1}{2}\\|\\Sigma_{s}^{*}-\\mathbf{S}_{s}\\|_{F}^{2}\\right)(\\mathbf{S}_{s}-\\mathbf{S}_{k})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $K$ is the number of groups, $\\mathbf{S}_{k}$ and $\\mathbf{S}_{s}$ are the sample covariance matrices for groups $k$ and $s$ , respectively, $\\pmb{\\Sigma}_{k}^{*}$ and $\\pmb{\\Sigma}_{s}^{*}$ are the optimal covariance matrices for groups $k$ and $s$ obtained by solving the group-specific CovGraph problems. ", "page_idx": 19}, {"type": "text", "text": "B.2.3 Fair BinNet ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To simplify, denote ", "page_idx": 19}, {"type": "equation", "text": "$$\nz_{\\theta}=\\exp\\left(\\theta_{j j}+\\sum_{j^{\\prime}\\neq j}\\theta_{j j^{\\prime}}x_{i j^{\\prime}}\\right)\\,,\\quad\\mathrm{and}\\quad z_{\\phi}=\\exp\\left(\\phi_{j j}+\\sum_{j^{\\prime}\\neq j}\\phi_{j j^{\\prime}}x_{i j^{\\prime}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The gradients of the objectives of Fair BinNet that are utilized in the iterative update formula (21) are computed as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(\\nabla f_{1}(\\mathbf{\\Theta}\\mathbf{\\Theta}))_{j j}=\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\mathbf{\\Theta},\\mathbf{X}\\right)\\right)_{j j}=-(\\mathbf{X}^{T}\\mathbf{X})_{j j}+\\displaystyle\\sum_{i=1}^{N}\\frac{z_{\\theta}}{1+z_{\\theta}},}\\\\ {\\displaystyle(\\nabla f_{1}(\\mathbf{\\Theta}))_{j j^{\\prime}}=\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\mathbf{\\Theta},\\mathbf{X}\\right)\\right)_{j j^{\\prime}}=-(\\mathbf{X}^{T}\\mathbf{X})_{j j^{\\prime}}+\\displaystyle\\sum_{i=1}^{N}\\frac{x_{i j^{\\prime}}z_{\\theta}}{1+z_{\\theta}},}\\\\ {\\displaystyle\\nabla f_{k+1}(\\mathbf{\\Theta})=\\sum_{s\\in[K],s\\neq k}\\left((\\mathcal{L}(\\mathbf{\\Theta};\\mathbf{X}_{k})-\\mathcal{L}(\\mathbf{\\Theta}_{k}^{*};\\mathbf{X}_{k}))-(\\mathcal{L}(\\mathbf{\\Theta};\\mathbf{X}_{s})\\right.}\\\\ {\\displaystyle\\left.-\\mathcal{L}(\\mathbf{\\Theta}_{s}^{*};\\mathbf{X}_{s}))\\right)\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\mathbf{\\Theta},\\mathbf{X}_{k}\\right)-\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\mathbf{\\Theta},\\mathbf{X}_{s}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, ${\\mathcal{L}}\\left(\\Theta,\\mathbf{X}\\right)=f_{1}(\\Theta)$ is the negative log-likelihood of the Ising model, where $\\Theta\\in\\mathbb{R}^{P\\times P}$ is the interaction matrix, $\\mathbf{X}\\in\\dot{\\{0,1\\}}^{N\\times\\bar{P}}$ is the binary data matrix with $N$ samples and $P$ variables, and $\\theta_{j j^{\\prime}}$ denotes the $(j,j^{\\prime})$ -th element of $\\Theta$ . ", "page_idx": 19}, {"type": "text", "text": "C Addendum to Section 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Auxiliary Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 12. Let $\\{\\Theta^{(t)}\\}$ be generated by Algorithm 1. Then for all $k=1,\\dotsc,M$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nF_{k}\\left(\\Theta^{(t+1)}\\right)\\leq F_{k}\\left(\\Theta^{(t)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\varphi_{\\ell}\\left(\\Phi,\\Theta\\right)$ be defined as in (9). Following the proof of [73, Lemma 4.1], we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\varphi_{\\ell}\\left(\\Theta^{(t+1)},\\Theta^{(t)}\\right)\\leq-\\ell\\|\\Theta^{(t+1)}-\\Theta^{(t)}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $\\ell>L$ , using the descent lemma [5, Proposition A.24], for all $k=1,\\dotsc,M$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{k}\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}\\right)-F_{k}\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\right)\\leq\\langle\\nabla f_{i}\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\right),\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}-\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,g\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}\\right)-g\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\right)+\\frac{\\ell}{2}\\|\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}-\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the right-hand side of the above inequality is less than or equal to zero, it implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\nF_{k}\\left(\\Theta^{(t+1)}\\right)\\leq F_{k}\\left(\\Theta^{(t)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 13. Suppose Assumption $A$ holds. Let $f_{k}$ and $g$ have convexity parameters $\\mu_{k}\\in\\mathbb{R}_{+}$ and $\\nu\\in\\mathbb{R}_{+}$ , respectively, and define $\\mu:=\\operatorname*{min}_{k\\in[M]}\\mu_{k}$ . Then, for all $\\Theta\\in\\bar{\\mathcal{M}}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{M}\\rho_{k}^{(t)}\\left(F_{k}\\left(\\Theta^{(t+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right)\\leq\\displaystyle\\frac{\\ell}{2}\\left(\\|\\Theta^{(t)}-\\Theta\\|_{F}^{2}-\\|\\Theta^{(t+1)}-\\Theta\\|_{F}^{2}\\right)}&{}\\\\ {\\displaystyle-\\,\\frac{\\nu}{2}\\|\\Theta^{(t+1)}-\\Theta\\|_{F}^{2}-\\frac{\\mu}{2}\\|\\Theta^{(t)}-\\Theta\\|_{F}^{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where \u03c1k $\\rho_{k}^{(t)}$ satisfies the following conditions: ", "page_idx": 20}, {"type": "text", "text": "1. There exists $\\eta^{(t)}\\in\\partial g\\left(\\Theta^{(t+1)}\\right)$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\sum_{k=1}^{M}\\rho_{k}^{\\left(t\\right)}\\left(\\nabla f_{i}\\left(\\Theta^{\\left(t\\right)}\\right)+\\eta^{\\left(t\\right)}\\right)=\\ell\\left(\\Theta^{\\left(t+1\\right)}-\\Theta^{\\left(t\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. $\\rho^{(t)}\\in\\mathcal{C}$ where $\\mathcal{C}$ is defined in (13). ", "page_idx": 20}, {"type": "text", "text": "Proof. Assumption A yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{k}\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}\\right)-F_{k}\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\right)\\leq\\langle\\nabla f_{k}(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}),\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}-\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,g\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}\\right)-g\\left(\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\right)+\\frac{\\ell}{2}\\|\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t+1)}-\\mathbf{\\Theta}\\mathbf{\\Theta}^{(t)}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From the convexity of $f_{k}$ and $g$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{F}_{k}\\left(\\Theta^{(t+1)}\\right)-\\boldsymbol{F}_{k}\\left(\\Theta\\right)}\\\\ &{\\qquad=\\left(\\boldsymbol{F}_{k}\\left(\\Theta^{(t+1)}\\right)-\\boldsymbol{F}_{k}\\left(\\Theta^{(t)}\\right)\\right)+\\left(\\boldsymbol{F}_{k}\\left(\\Theta^{(t)}\\right)-\\boldsymbol{F}_{k}\\left(\\Theta\\right)\\right)}\\\\ &{\\qquad\\qquad\\leq\\left(\\boldsymbol{\\nabla}\\boldsymbol{f}_{k}(\\Theta^{(t)}),\\Theta^{(t+1)}-\\Theta^{(t)}\\right)+\\boldsymbol{g}\\left(\\Theta^{(t+1)}\\right)-\\boldsymbol{g}\\left(\\Theta^{(t)}\\right)+\\frac{\\ell}{2}\\|\\Theta^{(t+1)}-\\Theta^{(t)}\\|_{F}^{2}\\right)}\\\\ &{\\qquad\\qquad+\\left(\\boldsymbol{\\nabla}\\boldsymbol{f}_{k}\\left(\\Theta\\right),\\Theta^{(t)}-\\Theta\\right)-\\frac{\\mu_{1}}{2}\\|\\Theta^{(t)}-\\Theta\\|_{F}^{2}+\\boldsymbol{g}\\left(\\Theta^{(t)}\\right)-\\boldsymbol{g}\\left(\\Theta^{(t)}\\right)}\\\\ &{\\qquad\\leq\\langle\\boldsymbol{\\nabla}\\boldsymbol{f}_{k}(\\Theta^{(t)}),\\Theta^{(t+1)}-\\Theta\\rangle+\\boldsymbol{g}\\left(\\Theta^{(t+1)}\\right)}\\\\ &{\\qquad\\quad-\\boldsymbol{g}\\left(\\Theta\\right)-\\frac{\\mu_{1}}{2}\\|\\Theta^{(t)}-\\Theta\\|_{F}^{2}+\\frac{\\boldsymbol{g}}{2}\\|\\Theta^{(t+1)}-\\Theta^{(t)}\\|_{F}^{2}}\\\\ &{\\qquad\\leq\\langle\\boldsymbol{\\nabla}\\boldsymbol{f}_{k}(\\Theta^{(t)})+\\eta^{(t)},\\Theta^{(t+1)}-\\Theta\\rangle+\\frac{\\ell}{2}\\|\\Theta^{(t+1)}-\\Theta\\|_{F}^{2}}\\\\ &{\\qquad-\\Theta^{(t)}\\|_{F}^{2}-\\frac{\\mu_{1}}{2}\\|\\Theta^{(t)}-\\Theta\\|_{F}^{2}-\\Theta\\|_{F}^{2}\\|\\Theta^{(t+1)}-\\Theta\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Condition 1 and Condition 2 yield ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{k=1}^{M}\\rho_{k}^{\\left(t\\right)}\\left(F_{k}\\left(\\boldsymbol{\\Theta}^{\\left(t+1\\right)}\\right)-F_{k}\\left(\\boldsymbol{\\Theta}\\right)\\right)=\\ell\\langle\\boldsymbol{\\Theta}^{\\left(t+1\\right)}-\\boldsymbol{\\Theta}^{\\left(t\\right)},\\boldsymbol{\\Theta}^{\\left(t+1\\right)}-\\boldsymbol{\\Theta}\\rangle+\\frac{\\ell}{2}\\|\\boldsymbol{\\Theta}^{\\left(t+1\\right)}}\\\\ {\\displaystyle-\\boldsymbol{\\Theta}^{\\left(t\\right)}\\|_{F}^{2}-\\frac{\\mu}{2}\\|\\boldsymbol{\\Theta}^{\\left(t\\right)}-\\boldsymbol{\\Theta}\\|_{F}^{2}-\\frac{\\nu}{2}\\|\\boldsymbol{\\Theta}^{\\left(t+1\\right)}-\\boldsymbol{\\Theta}\\|_{F}^{2}}\\\\ {\\displaystyle=\\frac{\\ell}{2}\\left(\\|\\boldsymbol{\\Theta}^{\\left(t\\right)}-\\boldsymbol{\\Theta}\\|_{F}^{2}-\\|\\boldsymbol{\\Theta}^{\\left(t+1\\right)}-\\boldsymbol{\\Theta}\\|_{F}^{2}\\right)}\\\\ {\\displaystyle-\\,\\frac{\\nu}{2}\\|\\boldsymbol{\\Theta}^{\\left(t+1\\right)}-\\boldsymbol{\\Theta}\\|_{F}^{2}-\\frac{\\mu}{2}\\|\\boldsymbol{\\Theta}^{\\left(t\\right)}-\\boldsymbol{\\Theta}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.2 Proof of Theorem 6 for Fair GLasso ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "First, we present the convexity analysis and gradient Lipschitz continuity. ", "page_idx": 21}, {"type": "text", "text": "Proposition 14 (Convexity of Fair GLasso). Each $f_{k}$ for $k~=~1,\\ldots,M$ and $g$ defined in (Fair GLasso) of Fair GLasso are convex. Further, $f_{1}$ is strongly convex. ", "page_idx": 21}, {"type": "text", "text": "Proof. First, consider $f_{1}$ in the first objective function of Fair GLasso: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{1}(\\Theta)=-\\log\\operatorname*{det}(\\Theta)+\\operatorname{trace}(\\mathbf{S}\\Theta),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\Theta$ is a positive definite matrix. ", "page_idx": 21}, {"type": "text", "text": "The gradient and Hessian of $f_{1}$ are, respectively: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla f_{1}(\\Theta)=\\mathbf{S}-\\Theta^{-1},\\quad\\mathbf{H}_{f_{1}}=\\Theta^{-1}\\otimes\\Theta^{-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The positive definiteness of $\\Theta$ implies that $\\Theta^{-1}$ is also positive definite. Therefore, the Hessian $\\mathbf{H}_{f_{1}}$ , being the Kronecker product of $\\Theta^{-1}$ with itself, is positive definite. This establishes that the objective function $f_{1}$ is strongly convex. ", "page_idx": 21}, {"type": "text", "text": "Next, the functions $f_{k+1}$ for $k=1,\\dotsc,M-1$ are defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{f_{k+1}}(\\Theta)=\\sum_{s\\in[K],s\\neq k}\\phi\\left(\\mathcal{E}_{k}\\left(\\Theta\\right)-\\mathcal{E}_{s}\\left(\\Theta\\right)\\right)}}\\\\ {{\\displaystyle=\\sum_{s\\in[K],s\\neq k}\\frac{1}{2}\\left(\\left(\\mathcal{L}(\\Theta;\\mathbf{X}_{k})-\\mathcal{L}(\\Theta_{k}^{*};\\mathbf{X}_{k})\\right)-\\left(\\mathcal{L}(\\Theta;\\mathbf{X}_{s})-\\mathcal{L}(\\Theta_{s}^{*};\\mathbf{X}_{s})\\right)\\right)^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which are convex due to the linearity of the trace operator in the loss function difference $\\mathcal{L}(\\Theta;\\mathbf{X}_{k})-$ $\\mathcal{L}(\\boldsymbol{\\Theta};\\mathbf{X}_{s})=\\mathrm{trace}((\\mathbf{S}_{k}-\\mathbf{S}_{s})\\boldsymbol{\\Theta})$ , leading to a strong convexity parameter of 0. ", "page_idx": 21}, {"type": "text", "text": "In addition, $g(\\Theta)=\\lambda\\|\\Theta\\|_{1}$ is identified as a closed, proper, and convex function. ", "page_idx": 21}, {"type": "text", "text": "Proposition 15 (Gradient Lipschitz Continuity of Fair GLasso). The gradients of fk for $k\\,=$ $1,\\bar{\\dots},M$ defined in (Fair GLasso) are Lipschitz continuous. ", "page_idx": 21}, {"type": "text", "text": "Proof. First, we present the gradient and Hessian of functions $f_{1}$ and $\\{f_{k+1}\\}_{k=1}^{M-1}$ as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f_{1}(\\mathbf{e})=}&{~-\\log\\operatorname*{det}(\\mathbf{e})+\\operatorname{trace}(\\mathbf{S}\\mathbf{e}),\\quad\\nabla f_{1}(\\mathbf{e})=\\mathbf{S}-\\mathbf{e}^{-1},\\quad\\mathbf{H}_{f_{1}}=\\mathbf{e}^{-1}\\otimes\\mathbf{e}^{-1};}\\\\ {f_{k+1}(\\mathbf{e})=\\displaystyle{\\sum_{s\\in[K],s\\neq k}}\\frac{1}{2}\\left(\\operatorname{trace}(\\mathbf{S}_{k}\\mathbf{e})-\\operatorname{trace}(\\mathbf{S}_{s}\\mathbf{e})+\\log\\operatorname*{det}(\\mathbf{e}_{k}^{*})\\right.}\\\\ &{~\\quad\\quad\\quad\\quad\\quad\\left.-\\mathrm{~trace}(\\mathbf{S}_{k}\\mathbf{e}_{k}^{*})-\\log\\operatorname*{det}(\\mathbf{e}_{s}^{*})+\\mathrm{trace}(\\mathbf{S}_{s}\\mathbf{e}_{s}^{*})\\right)^{2},}\\\\ {\\nabla f_{k+1}(\\mathbf{e})=}&{\\displaystyle{\\sum_{s\\in[K],s\\neq k}}\\ \\left(\\operatorname{trace}(\\mathbf{S}_{k}\\mathbf{e})-\\mathrm{trace}(\\mathbf{S}_{s}\\mathbf{e})+\\log\\operatorname*{det}(\\mathbf{e}_{k}^{*})\\right.}\\\\ &{\\left.\\quad\\quad\\quad\\quad\\quad-\\mathrm{~trace}(\\mathbf{S}_{k}\\mathbf{e}_{k}^{*})-\\log\\operatorname*{det}(\\mathbf{e}_{s}^{*})+\\mathrm{trace}(\\mathbf{S}_{s}\\mathbf{e}_{s}^{*})\\right)(\\mathbf{S}_{k}-\\mathbf{S}_{s})\\,,}\\\\ {\\mathbf{H}_{f_{k+1}}(\\mathbf{e})=}&{\\displaystyle{\\sum_{s\\in[K],s\\neq k}}\\left(\\mathbf{S}_{k}-\\mathbf{S}_{s}\\right)\\otimes\\left(\\mathbf{S}_{k}-\\mathbf{S}_{s}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Define $L_{1}=\\Lambda_{\\operatorname*{max}}(\\mathbf{H}_{f_{1}})$ and $L_{k+1}=\\Lambda_{\\operatorname*{max}}(\\mathbf{H}_{f_{k+1}})$ for $k=1,\\dotsc,M-1$ . Given that $\\{f_{k}\\}_{k=1}^{M}$ are convex (as proven in Proposition 14) and twice differentiable, their gradients satisfy Lipschitz continuity with Lipschitz constants {Lk}kM=1. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Next, we present the proof for Theorem 6. ", "page_idx": 22}, {"type": "text", "text": "proof for Theorem $^{6}$ . From Proposition 14 and Proposition 15, convexity and gradient Lipschitz continuity of objective functions $\\{f_{k}\\}_{k=1}^{M}$ are verified. Hence, Assumption A holds. From Lemma 13 and the convexity of $f_{i}$ and $g$ , for all $\\Theta\\in\\mathcal{M}$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{M}\\rho_{k}^{(t)}\\left(F_{k}\\left(\\Theta^{(t+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right)\\leq\\,\\frac{\\ell}{2}\\left(\\|\\Theta^{(t)}-\\Theta\\|_{F}^{2}-\\|\\Theta^{(t+1)}-\\Theta\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Adding up the above inequality (39) from $t=0$ to $t=\\widetilde t$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=0}^{\\hat{t}}\\sum_{k=1}^{M}\\rho_{k}^{(t)}\\left(F_{k}\\left(\\Theta^{(t+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right)\\leq\\frac{\\ell}{2}\\left(\\|\\Theta^{(0)}-\\Theta\\|_{F}^{2}-\\|\\Theta^{(\\tilde{t}+1)}-\\Theta\\|_{F}^{2}\\right)}&{}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\ell}{2}\\|\\Theta^{(0)}-\\Theta\\|_{F}^{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 12 implies that $F_{k}\\left(\\Theta^{(\\tilde{t}+1)}\\right)\\leq F_{k}\\left(\\Theta^{(t+1)}\\right)$ for all $t\\leq\\widetilde t$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{\\tilde{t}}\\sum_{k=1}^{M}\\rho_{k}^{(t)}\\left(F_{k}\\left(\\Theta^{(\\tilde{t}+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right)\\leq\\ \\frac{\\ell}{2}\\|\\Theta^{(0)}-\\Theta\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\begin{array}{r}{\\bar{\\rho}_{k}^{\\tilde{t}}:=\\sum_{t=0}^{\\tilde{t}}\\rho_{k}^{(t)}/\\left(\\tilde{t}+1\\right)}\\end{array}$ . Then, it follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{M}\\bar{\\rho}_{k}^{\\tilde{t}}\\left(F_{k}\\left(\\boldsymbol{\\Theta}^{(\\tilde{t}+1)}\\right)-F_{k}\\left(\\boldsymbol{\\Theta}\\right)\\right)\\leq\\ \\frac{\\ell}{2\\left(\\tilde{t}+1\\right)}\\|\\boldsymbol{\\Theta}^{(0)}-\\boldsymbol{\\Theta}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\bar{\\rho}_{k}^{\\tilde{t}}\\geq0$ and $\\sum_{k=1}^{M}\\bar{\\rho}_{k}^{\\tilde{t}}=1$ , we can conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[M]}\\left(F_{k}\\left(\\Theta^{(\\widetilde{t}+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right)\\leq\\ \\frac{\\ell}{2\\left(\\widetilde{t}+1\\right)}\\|\\Theta^{(0)}-\\Theta\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, following the proof of [75, Theorem 5.1] and using Assumption B, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{\\mathbf{F}^{*}\\in\\mathbf{F}\\left(N^{*}\\cap\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\mathbf{e}^{(0)}\\right)\\right)\\right)}{\\operatorname*{sup}}\\underset{\\mathbf{k}\\in\\mathbf{F}^{-1}\\left(\\left\\{\\mathbf{F}^{*}\\right\\}\\right)}{\\operatorname*{min}}\\left(F_{k}\\left(\\mathbf{\\Theta}^{(\\widetilde{t}+1)}\\right)-F_{k}\\left(\\mathbf{\\Theta}\\right)\\right)\\le\\frac{\\ell R}{2\\left(\\widetilde{t}+1\\right)},}\\\\ &{}&{\\underset{\\mathbf{F}^{*}\\in\\mathbf{F}\\left(N^{*}\\cap\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\mathbf{e}^{(0)}\\right)\\right)\\right)}{\\operatorname*{sup}}\\left(F_{k}\\left(\\mathbf{\\Theta}\\right)^{\\left(F_{k}1\\right)}\\right)-F_{k}^{*}\\right)\\le\\frac{\\ell R}{2\\left(\\widetilde{t}+1\\right)},}\\\\ &{}&{\\underset{\\mathbf{\\Theta}\\in\\mathcal{N}^{*}\\cap\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\mathbf{e}^{(0)}\\right)\\right)}{\\operatorname*{sup}}\\left(F_{k}\\left(\\mathbf{\\Theta}\\right)^{\\left(\\widetilde{t}+1\\right)}\\right)-F_{k}\\left(\\mathbf{\\Theta}\\right)\\right)\\le\\frac{\\ell R}{2\\left(\\widetilde{t}+1\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The inequality $F_{k}\\left(\\pmb{\\Theta}^{(t)}\\right)\\leq F_{k}\\left(\\pmb{\\Theta}^{(0)}\\right)$ from Lemma 12 implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Theta\\in\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\Theta^{(0)}\\right)\\right)}{\\operatorname*{sup}}\\underset{k\\in\\left[M\\right]}{\\operatorname*{min}}\\left(F_{k}\\left(\\Theta^{(\\widetilde{t}+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right)=\\underset{\\Theta\\in\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\Theta^{\\widetilde{t}+1}\\right)\\right)}{\\operatorname*{sup}}\\underset{k\\in\\left[M\\right]}{\\operatorname*{min}}\\left(F_{k}\\left(\\Theta^{(\\widetilde{t}+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{\\Theta\\in\\mathcal{M}}{\\operatorname*{sup}}\\underset{k\\in\\left[M\\right]}{\\operatorname*{min}}\\left(F_{k}\\left(\\Theta^{(\\widetilde{t}+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, from Assumption B that for all $\\Theta\\in\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\Theta^{(0)}\\right)\\right)$ , there exists $\\Theta^{*}\\in\\mathcal{N}^{*}$ such that $\\mathbf{F}\\left(\\Theta^{*}\\right)\\preceq\\mathbf{F}\\left(\\Theta\\right)$ , it follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{\\mathbf{\\Theta}\\in\\mathcal{N}^{*}\\cap\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\mathbf{\\Theta}^{(0)}\\right)\\right)}{\\operatorname*{sup}}\\left(F_{k}\\left(\\mathbf{\\Theta}\\Theta^{(\\tilde{t}+1)}\\right)-F_{k}\\left(\\mathbf{\\Theta}\\Theta\\right)\\right)}\\\\ &{}&{=\\underset{\\mathbf{\\Theta}\\in\\Omega_{\\mathbf{F}}\\left(\\mathbf{F}\\left(\\mathbf{\\Theta}^{(0)}\\right)\\right)}{\\operatorname*{sup}}\\frac{\\operatorname*{min}}{k\\in\\left[M\\right]}\\left(F_{k}\\left(\\mathbf{\\Theta}\\Theta^{(\\tilde{t}+1)}\\right)-F_{k}\\left(\\mathbf{\\Theta}\\Theta\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, from (44) and (48), we can conclude that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\Theta\\in\\mathcal{M}}\\operatorname*{min}_{k\\in\\left[M\\right]}\\left\\{F_{k}\\left(\\Theta^{(\\tilde{t}+1)}\\right)-F_{k}\\left(\\Theta\\right)\\right\\}\\leq\\ \\frac{\\ell R}{2\\left(\\tilde{t}+1\\right)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.3 Proof of Theorem 7 for Fair CovGraph ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "First, we present the convexity analysis and gradient Lipschitz continuity. ", "page_idx": 23}, {"type": "text", "text": "Proposition 16 (Convexity of Fair CovGraph). Through incorporating a convex regularization term $\\gamma_{C}||\\Theta||_{F}^{2}$ for some $\\gamma_{C}\\geq\\operatorname*{max}\\{0,-\\Lambda_{\\operatorname*{min}}(\\nabla^{2}f_{k}(\\Theta))\\}$ into each $f_{k}$ for $k=2,\\ldots,M$ , each $f_{k}$ for $k=1,\\bar{\\dots},M$ and $g$ defined in the multi-objective optimization problem (Fair CovGraph) of Fair CovGraph are guaranteed to be convex. In particular, $f_{1}$ is strongly convex. ", "page_idx": 23}, {"type": "text", "text": "Proof. In Fair CovGraph (Fair CovGraph), the function $f_{1}$ , its gradient, and Hessian are defined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f_{1}({\\bf\\Sigma})=\\displaystyle\\frac{1}{2}\\|{\\bf\\Sigma}-{\\bf S}\\|_{F}^{2}-\\tau\\log\\operatorname*{det}\\left({\\bf\\Sigma}\\right),}}\\\\ {{\\nabla f_{1}({\\bf\\Sigma})={\\bf\\Sigma}\\mathbf{\\Sigma}-{\\bf S}-\\tau{\\bf\\Sigma}^{-1},\\quad{\\bf H}_{f_{1}}=\\,{\\bf I}_{P^{2}}+\\tau{\\bf\\Sigma}^{-1}\\otimes{\\bf\\Sigma}{\\bf{\\Sigma}}^{-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The positive definiteness of the covariance matrix $\\Sigma$ guarantees that the Hessian matrix $\\mathbf{H}_{f_{1}}$ is also positive definite, establishing the strong convexity of the function $f_{1}$ . Next, consider: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{\\Sigma};\\mathbf{X}_{k})-\\mathcal{L}(\\boldsymbol{\\Sigma};\\mathbf{X}_{s})=\\frac{1}{2}\\|\\boldsymbol{\\Sigma}-\\mathbf{S}_{k}\\|_{F}^{2}-\\frac{1}{2}\\|\\boldsymbol{\\Sigma}-\\mathbf{S}_{s}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This difference is necessarily convex, as it is the difference between two convex functions. ", "page_idx": 23}, {"type": "text", "text": "To ensure the convexity of the functions $f_{k}$ for $k\\,=\\,2,\\ldots,M$ , a convexity regularization term $\\gamma_{C}\\|\\Theta\\|_{F}^{2}$ is added to $f_{k}$ , denoted by $\\tilde{f}_{k}$ , where $\\gamma_{C}$ is chosen to be $\\gamma_{C}\\geq\\operatorname*{max}\\{0,-\\Lambda_{\\operatorname*{min}}(\\nabla^{2}f_{k}(\\Theta))\\}$ such that $\\Lambda_{\\operatorname*{min}}(\\mathbf{H}_{\\tilde{f}_{k}})\\geq0$ for $k\\,=\\,2,\\dots,M$ . This regularization term guarantees that the minimum eigenvalue of the Hessian matrix $\\mathbf{H}_{\\tilde{f}_{k}}$ is non-negative, thereby ensuring the convexity of $f_{k}$ . Furthermore, the function $g(\\Sigma)$ is a closed, proper, and convex function. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proposition 17 (Gradient Lipschitz Continuity of Fair CovGraph). The gradients of $f_{k}$ for $k=$ $1,\\bar{\\dots},M$ defined in (Fair CovGraph) are Lipschitz continuous. ", "page_idx": 23}, {"type": "text", "text": "Proof. We detail the gradient and Hessian of functions $f_{1}$ and $\\{f_{k+1}\\}_{k=1}^{M-1}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}(\\mathbf{Z})=~\\frac{1}{2}\\Vert\\mathbf{Z}-\\mathbf{S}\\Vert_{F}^{2}-\\tau\\log\\operatorname*{det}(\\mathbf{Z}),}\\\\ &{\\nabla f_{1}(\\mathbf{Z})=\\mathbf{Z}-\\mathbf{S}-\\tau\\mathbf{Z}^{-1},~~\\mathbf{H}_{f_{1}}=I_{f^{\\prime}}\\mathbf{Z}+\\tau\\mathbf{Z}^{-1}\\otimes\\mathbf{Z}^{-1},}\\\\ &{f_{k+1}(\\mathbf{Z})=~\\underbrace{\\sum_{j\\in\\mathbf{S}_{j}}\\frac{1}{\\mu_{k}}}_{s\\in[K],s\\neq k}\\Big(\\frac12\\Vert\\mathbf{Z}-\\mathbf{S}_{k}\\Vert_{F}^{2}-\\frac12\\Vert\\mathbf{Z}-\\mathbf{S}_{s}\\Vert_{F}^{2}+\\tau\\log\\operatorname*{det}(\\mathbf{Z}_{k}^{\\ast})}\\\\ &{\\qquad\\qquad\\qquad-\\frac12\\Vert\\mathbf{Z}_{k}^{\\ast}-\\mathbf{S}_{k}\\Vert_{F}^{2}-\\tau\\log\\operatorname*{det}(\\mathbf{Z}_{s}^{\\ast})+\\frac12\\Vert\\mathbf{Z}_{s}^{\\ast}-\\mathbf{S}_{k}\\Vert_{F}^{2}\\Big)^{2},}\\\\ &{\\nabla f_{k+1}(\\mathbf{Z})=~\\underbrace{\\sum_{j\\in\\mathbf{S}_{j}}\\left(\\frac12\\Vert\\mathbf{Z}-\\mathbf{S}_{k}\\Vert_{F}^{2}-\\frac12\\Vert\\mathbf{Z}-\\mathbf{S}_{k}\\Vert_{F}^{2}+\\tau\\log\\operatorname*{det}(\\mathbf{Z}_{k}^{\\ast})\\right.}_{=:\\Vert\\mathbf{Z}_{k}^{\\ast}-\\mathbf{S}_{k}\\Vert_{F}^{2}-\\tau\\log\\operatorname*{det}(\\mathbf{Z}_{s}^{\\ast})+\\frac12\\Vert\\mathbf{Z}_{s}^{\\ast}-\\mathbf{S}_{k}\\Vert_{F}^{2}}}\\\\ &{\\qquad\\qquad\\left.-\\frac12\\Vert\\mathbf{Z}_{k}^{\\ast}-\\mathbf{S}_{k}\\Vert_{F}^{2}-\\tau\\log\\operatorname*{det}(\\mathbf{Z}_{s}^{\\ast})+\\frac12\\Vert\\mathbf{Z}_{s}^{\\ast}-\\mathbf{S}_{s}\\Vert_{F}^{2}\\right)(\\mathbf{S}_{s}-\\mathbf{S}_{k}),}\\\\ &{\\Vert_{f_{k+1}}(\\mathbf{Z})=~\\sum_{j\\in \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then given that $f_{1}$ and $\\frac{\\partial f_{1}}{\\partial{\\pmb{\\Sigma}}}$ are Lipschitz continuous and bounded on the set $\\{\\Sigma\\in\\mathcal{M}|\\Vert\\Sigma\\Vert_{1}<\\infty\\}$ , the function sequence $\\{\\overline{{f_{k+1}}}\\}_{k=1}^{M-1}$ is also Lipschitz continuous. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 7. Note that for ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma_{C}\\geq\\operatorname*{max}\\{0,-\\Lambda_{\\operatorname*{min}}(\\nabla^{2}f_{k}(\\Sigma))\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the problem (Fair CovGraph) is convex. Now, from Proposition 16 and Proposition 17, convexity and gradient Lipschitz continuity of objective functions $\\{f_{k}\\}_{k=1}^{M}$ are verified. Then, the proof of Theorem 7 is a slightly modified version of the proof of Theorem 6. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.4 Proof of Theorem 8 for Fair BinNet ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "First, we present the convexity analysis and gradient Lipschitz continuity. ", "page_idx": 24}, {"type": "text", "text": "Proposition 18 (Convexity of Fair BinNet). In the multi-objective optimization Problem (Fair BinNet), the functions $f_{1}$ and $g$ are convex. Furthermore, by incorporating a convex regularization term $\\gamma_{I}\\lvert\\lvert\\Theta\\rvert\\rvert_{F}^{2}$ for some $\\begin{array}{r}{\\bar{\\gamma}_{I}\\geq|\\operatorname*{min}\\{\\frac{1}{2}\\Lambda_{\\operatorname*{min}}(\\nabla^{2}f_{k}),0\\}|}\\end{array}$ into each $f_{k}$ for $k=2,\\dotsc,M$ , the set of functions $\\{f_{k}\\}_{k=2}^{M}$ are ensured to be convex as well. ", "page_idx": 24}, {"type": "text", "text": "Proof. The function $f_{1}$ for Fair BinNet is defined as: ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{1}(\\Theta)=-\\sum_{j=1}^{P}\\sum_{j^{\\prime}=1}^{P}\\theta_{j j^{\\prime}}(\\mathbf{X}^{T}\\mathbf{X})_{j j^{\\prime}}+\\sum_{i=1}^{N}\\sum_{j=1}^{P}\\log\\left(1+\\exp\\left(\\theta_{j j}+\\sum_{j^{\\prime}\\neq j}\\theta_{j j^{\\prime}}x_{i j^{\\prime}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To demonstrate the convexity of $f_{1}$ , observe that $h(x)=\\log(1\\!+\\!\\exp(x))$ is convex and nondecreasing.   \nSince convexity is preserved under linear combination and summation, $f_{1}$ is convex by construction.   \nAlso, $g(\\Sigma)$ is a closed, proper, and convex function. ", "page_idx": 24}, {"type": "text", "text": "Consider $\\widetilde{f}_{k+1}(\\Theta)=f_{k+1}(\\Theta)+\\gamma_{I}\\|\\Theta\\|_{F}^{2}$ for $k=1,\\dotsc,M-1$ , its Hessian matrix is given by: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\tilde{f}_{k+1}}(\\boldsymbol{\\Theta})=\\mathbf{H}_{f_{k+1}}(\\boldsymbol{\\Theta})+2\\gamma_{I}\\mathbf{I}_{P^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If $\\gamma_{I}$ is chosen to be $|\\operatorname*{min}\\{{\\textstyle{\\frac{1}{2}}}\\Lambda_{\\operatorname*{min}}\\big(\\nabla^{2}f_{k}\\big),0\\}|$ such that $\\gamma_{I}\\mathbf{I}_{P^{2}}$ dominates any negative curvature in $\\mathbf{H}_{f_{k+1}}(\\Theta)$ , then $\\mathbf{H}_{\\tilde{f}_{k+1}}(\\Theta)$ will be positive semidefinite, leading the convexity of $\\{\\tilde{f}_{k+1}\\}_{k=1}^{M}$ . ", "page_idx": 24}, {"type": "text", "text": "Proposition 19 (Gradient Lipschitz Continuity of Fair BinNet). The gradients of $f_{k}$ for $k=1,\\dotsc,M$ defined in the multi-objective optimization Problem (Fair BinNet) are Lipschitz continuous. ", "page_idx": 24}, {"type": "text", "text": "Proof. For notational simplicity, we introduce the following substitutions: utilize ${\\mathcal{L}}\\left(\\Theta,\\mathbf{X}\\right)$ in place of $f_{1}\\left(\\Theta\\right)$ , denote $\\begin{array}{r}{z_{\\theta}=\\exp\\Big(\\theta_{j j}+\\sum_{j^{\\prime}\\neq j}\\theta_{j j^{\\prime}}x_{i j^{\\prime}}\\Big),z_{\\phi}=\\exp\\Big(\\phi_{j j}+\\sum_{j^{\\prime}\\neq j}\\phi_{j j^{\\prime}}x_{i j^{\\prime}}\\Big)}\\end{array}$ . Then, we proceed to evaluate the gradient of the function $f_{1}$ in the context of Fair BinNet as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\Theta,\\mathbf{X}\\right)\\right)_{j j}=\\left(\\nabla f_{1}(\\Theta)\\right)_{j j}=-(\\mathbf{X}^{T}\\mathbf{X})_{j j}+\\displaystyle\\sum_{i=1}^{N}\\frac{z_{\\theta}}{1+z_{\\theta}},}\\\\ {\\displaystyle\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\Theta,\\mathbf{X}\\right)\\right)_{j j^{\\prime}}=\\left(\\nabla f_{1}(\\Theta)\\right)_{j j^{\\prime}}=-(\\mathbf{X}^{T}\\mathbf{X})_{j j^{\\prime}}+\\displaystyle\\sum_{i=1}^{N}\\frac{x_{i j^{\\prime}}z_{\\theta}}{1+z_{\\theta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given that $h_{1}(x)=\\exp(x)/\\left(1+\\exp(x)\\right)$ is Lipschitz continuous with Lipschitz constant 0.25, for any $\\Theta,\\Phi\\in\\mathcal{M}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\nabla f_{1}(\\Theta)-\\nabla f_{1}(\\Phi)\\right\\|_{F}\\leq\\displaystyle\\sum_{j=1}^{P}\\sqrt{\\left(\\displaystyle\\sum_{i=1}^{N}\\frac{z_{\\theta}}{1+z_{\\theta}}-\\displaystyle\\sum_{i=1}^{N}\\frac{z_{\\phi}}{1+z_{\\phi}}\\right)^{2}}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\sum_{j=1}^{P}\\displaystyle\\sum_{j^{\\prime}=1,j^{\\prime}\\neq j}^{P}\\sqrt{\\left(\\displaystyle\\sum_{i=1}^{N}\\frac{x_{i j^{\\prime}}z_{\\theta}}{1+z_{\\theta}}-\\displaystyle\\sum_{i=1}^{N}\\frac{x_{i j^{\\prime}}z_{\\phi}}{1+z_{\\phi}}\\right)^{2}}}\\\\ {\\displaystyle\\qquad\\leq\\sum_{j=1}^{P}\\displaystyle\\sum_{i=1}^{N}\\left|\\frac{z_{\\theta}}{1+z_{\\theta}}-\\frac{z_{\\phi}}{1+z_{\\phi}}\\right|+\\displaystyle\\sum_{j=1}^{P}\\displaystyle\\sum_{j^{\\prime}=1,j^{\\prime}\\neq j}^{P}\\sum_{i=1}^{N}\\left|\\frac{x_{i j^{\\prime}}z_{\\theta}}{1+z_{\\theta}}-\\frac{x_{i j^{\\prime}}z_{\\phi}}{1+z_{\\phi}}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\leq\\sum_{j=1+1}^{P}\\left|\\theta_{j j}-\\phi_{j j}-\\sum_{j^{\\prime}=1}^{P}\\left(\\theta_{j j^{\\prime}}-\\phi_{j j^{\\prime}}\\right)x_{i j^{\\prime}}\\right|}}\\\\ {~~}\\\\ {{\\displaystyle~~~+\\left(P-1\\right)\\times\\sum_{j=1}^{P}\\sum_{i=1}^{N}\\left|\\theta_{j j}-\\phi_{j j}+\\sum_{j^{\\prime}\\neq j}\\left(\\theta_{j j^{\\prime}}-\\phi_{j j^{\\prime}}\\right)x_{i j^{\\prime}}\\right|}}\\\\ {~~}\\\\ {{\\displaystyle\\leq N\\times P\\times\\sum_{j=1}^{P}\\sum_{j=1}^{P}\\left|\\theta_{j j^{\\prime}}-\\phi_{j j^{\\prime}}\\right|}}\\\\ {{\\displaystyle\\leq N\\times P^{2}\\times\\sqrt{\\sum_{j=1}^{P}\\sum_{j^{\\prime}=1}^{P}\\left|\\theta_{j j^{\\prime}}-\\phi_{j j^{\\prime}}\\right|^{2}}}}\\\\ {{\\displaystyle=N\\times P^{2}\\times|\\phi_{j}-\\phi||_{\\mathcal{F}_{j}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It follows that there exists $L_{1}=N\\times P^{2}\\in\\mathbb{R}$ such that $\\|\\nabla f_{1}(\\Theta)-\\nabla f_{1}(\\Phi)\\|_{F}\\leq L_{1}\\|\\Theta-\\Phi\\|_{F}$ . Subsequently, the gradients of functions $\\{f_{k+1}\\}_{k=1}^{M-1}$ in Fair BinNet are evaluated as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{k+1}(\\Theta)=\\displaystyle\\sum_{s\\in[K],s\\neq k}\\left((\\mathcal{L}(\\Theta;\\mathbf{X}_{k})-\\mathcal{L}(\\Theta_{k}^{*};\\mathbf{X}_{k}))\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.-\\mathbf{\\Theta}(\\mathcal{L}(\\Theta;\\mathbf{X}_{s})-\\mathcal{L}(\\Theta_{s}^{*};\\mathbf{X}_{s}))\\right)\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\Theta,\\mathbf{X}_{k}\\right)-\\frac{\\partial\\mathcal{L}}{\\partial\\Theta}\\left(\\Theta,\\mathbf{X}_{s}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then given that $\\mathcal{L}$ and $\\frac{\\partial{\\mathcal{L}}}{\\partial\\Theta}$ are Lipschitz continuous and bounded on the set $\\{\\Theta\\in\\mathcal{M}|\\|\\Theta\\|_{1}<\\infty\\}$ , the function sequence $\\bar{\\{f_{k+1}\\}}_{k=1}^{M-1}$ is also Lipschitz continuous. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 8 for Fair BinNet. Building on Proposition 18 and Proposition 19, proof of Theorem 8 can be viewed as a nuanced adaptation of the proof presented in Theorem 6. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "C.5 Computational Complexity of FairGMs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The computational complexity of the fair GLasso and fair CovGraph algorithm depends on both the number of variables $P$ and the number of observations $N$ . We aim to demonstrate that our algorithm has a complexity of O max(NP 2,P 3) , which is similar to standard graph learning methods when $K<<N,P$ . This is applicable to our experimental results, where $K=2,8,2,000\\leq N\\leq15,000$ , and $5\\leq P\\leq120$ . The computational complexity is primarily influenced by the following factors: ", "page_idx": 25}, {"type": "text", "text": "1. Number of Variables $(P)$ : The complexity scales as $O(P^{3})$ due to matrix inversion for computing the gradient at each step.   \n2. Number of Observations $(N)$ : Computing the empirical covariance matrix from the data has a complexity of $O(N P^{2})$ .   \n3. Global Fair GMs Complexity: Considering factors 1 and 2, the complexity of each proximal gradient step applied to global fair GM is $O(\\operatorname*{max}(N P^{2},P^{3}))$ .   \n4. Local GMs Complexity: Applying factors 1 and 2 to group-specific data, the complexity of each local GM is $\\operatorname*{max}(N_{k}\\bar{P}^{2},P^{3})$ for all $k=1,\\ldots,K$ . The total complexity of the local GMs is $\\textstyle\\sum_{k=1}^{K}\\operatorname*{max}(N_{k}P^{2},P^{3})$ . ", "page_idx": 25}, {"type": "text", "text": "As established in Theorem 6 and Theorem 8 for fair inverse covariance and covariance estimation, the iteration complexity of our algorithm to achieve $\\epsilon$ -accuracy is $O\\left({\\frac{1}{\\epsilon}}\\right)$ . Combining this result with the per-iteration complexity of the algorithm, the total time complexity of our optimization procedure is $\\bar{O}\\left(\\frac{\\operatorname*{max}(N P^{2},P^{3})}{\\epsilon}\\right)$ Including the Local GMs computation, the total time complexity of fair GMs is $\\begin{array}{r}{O\\left(\\sum_{k=1}^{K}\\operatorname*{max}(N_{k}P^{2},P^{3})+\\frac{\\operatorname*{max}(N P^{2},P^{3})}{\\epsilon}\\right).}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Under the assumption that the number of groups is small (i.e., $K<<N$ , $K<<P$ , and $K<<1/\\epsilon)$ , the complexity reduces to max(N\u03f5P 2,P 3) . This complexity is of the same order as the complexity of running the proximal gradient method applied to covariance estimation and inverse covariance estimation. Therefore, for large $N$ and $P$ and a small number of groups, the time complexity of our algorithm is comparable to the standard method. In addition to theoretical analysis, we also provide sensitivity analysis experiments on $P$ ${}^{\\mathrm{D}},\\,N,\\,K$ , and group imbalance in Appendix D.6-D.9. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "D Addendum to Section 4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Iterative Soft-Thresholding Algorithm (ISTA) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "ISTA for sparse inverse covariance estimation is initially introduced by [60] and demonstrates a closed-form linear convergence rate. We adapt this approach and extend it to other GMs, utilizing it in the generation of both baseline and local graphs. Specifically, for a GM characterized by the loss function $\\mathcal{L}\\left(\\Theta;\\mathbf{X}\\right)+\\lambda\\|\\Theta\\|_{1}$ , we employ the following detailed algorithm: ", "page_idx": 26}, {"type": "text", "text": "Algorithm 2 ISTA for GMs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Input: Sample matrix $\\mathbf{X}$ , initial iterate $\\Theta^{(0)}$ , maximum iteration $T$ , step size $\\zeta$ , regularization   \nparameter $\\lambda$ , tolerance $\\epsilon$ . Set $t=0$ .   \nfor $t=0,1,\\ldots,T-1$ do Gradient Step: $\\Theta^{(t+1)}\\left\\langle-\\,\\Theta^{(t)}-\\zeta\\nabla\\mathcal{L}\\left(\\Theta^{(t)};\\mathbf{X}\\right)\\right.$ Soft-Thresholding Step: $\\Theta^{(t+1)}\\gets\\eta_{\\zeta\\rho}(\\Theta^{(t+1)})$ , $\\left(\\eta_{\\zeta\\rho}\\left(\\Theta\\right)\\right)_{j j^{\\prime}}=\\mathrm{sign}(\\theta_{j j^{\\prime}})\\operatorname*{max}(|\\theta_{j j^{\\prime}}|\\mathrm{~-~}$ $\\zeta\\rho,0)$ if $\\|\\nabla\\mathcal{L}\\left(\\Theta^{(t+1)},\\mathbf{X}\\right)\\|_{1}\\leq\\epsilon\\,\\mathbf{then}$ Break end if   \nend for   \nOutput: \u0398(t+1) ", "page_idx": 26}, {"type": "text", "text": "D.2 Simulation Study of Fair GLasso ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As a supplement to Section 4.2, we detail the process of generating $K$ block diagonal covariance matrices of dimensions $P\\times P$ , denoted as $\\{\\pmb{\\Sigma}_{k}\\}_{k=1}^{K}$ , each corresponding to distinct sensitive groups. The procedure is as follows: ", "page_idx": 26}, {"type": "text", "text": "1. Firstly, we assume that each $\\Sigma_{k}$ contains $Q$ blocks and $P$ is divisible by $Q$ . For the first group, the covariance matrix $\\pmb{\\Sigma}_{1}$ is constructed as a block diagonal matrix: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Sigma_{1}=\\left(\\begin{array}{c c c}{\\mathbf{B}_{1}}&{\\cdots}&{\\mathbf{0}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}}&{\\cdots}&{\\mathbf{B}_{Q}}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, each block $\\mathbf{B}_{q}$ is a sub-matrix filled with values drawn from a normal distribution $\\mathcal{N}(0.7,0.2)$ . ", "page_idx": 26}, {"type": "text", "text": "2. To ensure $\\pmb{\\Sigma}_{1}$ is symmetric, it is adjusted to $(\\Sigma_{1}+\\Sigma_{1}^{\\top})/2$ . To ensure it is positive definite, we further adjust ${\\bf\\dot{\\Sigma}}_{1}$ as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{1}=[\\pmb{v}_{1}\\quad\\cdot\\cdot\\cdot\\quad\\pmb{v}_{P}]\\left[\\begin{array}{c c c}{\\hat{\\lambda}_{1}}&{\\cdot\\cdot\\cdot}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\hat{\\lambda}_{P}}\\end{array}\\right]\\left[\\begin{array}{c}{\\pmb{v}_{1}^{\\top}}\\\\ {\\vdots}\\\\ {\\pmb{v}_{P}^{\\top}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\hat{\\lambda}_{j}$ represents $\\operatorname*{max}(\\lambda_{j}(\\Sigma_{1}),10^{-5})$ , and $\\pmb{v}_{j}$ is the corresponding eigenvector. ", "page_idx": 26}, {"type": "text", "text": "3. For each subsequent group $(k=2,\\ldots,K)$ , the covariance matrix $\\Sigma_{k}$ is initially set equal to $\\Sigma_{k-1}$ . Then, two (one for sensitivity analysis) of its sub-matrices, which have not been altered yet, are reset to the identity matrix. ", "page_idx": 26}, {"type": "text", "text": "D.3 Simulation Study of Fair BinNet ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We specify the process of generating synthetic data for the simulation study in Section 4.3. This process adapts a hub node-based network as proposed by [72], aiming to generate a sequence of networks $\\{\\bar{\\Theta}\\}_{l=1}^{k}$ . The process comprises the following steps: ", "page_idx": 26}, {"type": "text", "text": "Table 4: Outcomes in terms of the value of the objective function $\\left(F_{1}\\right)$ , the summation of the pairwise graph disparity error $(\\Delta)$ , and the average computation time in seconds ( $\\pm$ standard deviation) from 10 repeated experiments. \u201c\u2193\u201d means the smaller, the better, and the best value is in bold. These experiments are conducted on an Apple M2 Pro processor. Note that both $F_{1}$ and $\\Delta$ are deterministic. ", "page_idx": 27}, {"type": "table", "img_path": "WvWS8goWyR/tmp/7ec38668c36543f992ca749c1a668c96bb994b9ef9a9c97cbd7256faf57a8670.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "1. Initialize a $P\\times P$ matrix $\\mathbf{A}$ , setting $\\mathbf{A}_{j j^{\\prime}}=1$ with a probability of 0.01 for all $j<j^{\\prime}$ and $\\mathbf{A}_{j j^{\\prime}.}=0$ otherwise. Ensure the matrix is symmetric by assigning $\\mathbf{A}_{j^{\\prime}j}=\\mathbf{A}_{j j^{\\prime}}$ . From the set of nodes, randomly select $H$ hub nodes and modify their corresponding rows and columns in $\\mathbf{A}$ to 1 with a $\\dot{9}9\\%$ probability or to 0 otherwise. ", "page_idx": 27}, {"type": "text", "text": "2. Construct another $P\\times P$ matrix $\\mathbf{E}$ , where each element $\\mathbf{{E}}_{j j^{\\prime}}$ is i.i.d.. Set $\\mathbf{E}_{j j^{\\prime}}=0$ if $\\mathbf{A}_{j j^{\\prime}}=$ 0. Otherwise, draw $\\mathbf{{E}}_{j j^{\\prime}}$ from a uniform distribution over the intervals $[-0.75,-0.25]\\cup$ [0.25, 0.75] for hub node columns and rows, and $[-0.5,-0.25]\\cup[0.25,0.5]$ for non-hub node columns and rows. Subsequently, symmetrize matrix $\\mathbf{E}$ by computing $\\dot{\\mathbf{E}}=(\\mathbf{E}+\\mathbf{E}^{\\top})/2$ . Define the first network $\\bar{\\bf{e}}_{1}^{-}$ as $\\dot{\\Theta_{1}}\\doteq\\mathbf{E}+(0.1-\\lambda_{\\mathrm{min}}(\\mathbf{E}))\\mathbf{I}$ . ", "page_idx": 27}, {"type": "text", "text": "3. For the generation of each subsequent network $(k=2,\\ldots,K)$ , start with the preceding network, setting $\\Theta_{k}=\\Theta_{k-1}$ . Then, modify $\\Theta_{k}$ by eliminating two hub nodes. ", "page_idx": 27}, {"type": "text", "text": "D.4 Addendum to Subsection 4.5 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In the experiments of applying GLasso to the ADNI dataset, we investigate the influence of sensitive attributes on brain networks associated with Alzheimer\u2019s disease (AD) pathology. Specifically, we focus on the amyloid accumulation network using AV45 (florbetapir) positron emission tomography (PET) data [88] and the tau accumulation network using AV1451 (flortaucipir) PET data [46]. For the amyloid network, we consider the sensitive attribute of marital status, as previous studies suggest that marriage may affect the progression of dementia due to factors such as social support, cognitive stimulation, and lifestyle habits [21, 61]. The dataset is divided into two groups based on marital status: a single group with 52 samples and a married group with 1,018 samples, creating an imbalanced and high-dimensional setting that poses challenges for network estimation. In the tau accumulation network, we explore the impact of the sensitive attribute race, which separates the dataset into two groups: the white group with 755 samples and the non-white group with 118 samples. This division allows us to investigate potential disparities in tau pathology across racial groups. Throughout the experiments, the regularization parameter $\\lambda$ , which controls the sparsity of the estimated networks, is fixed at 0.3 for the AV45 data and 0.2 for the AV1451 data based on empirical observations. Besides, the dataset is normalized such that it has a mean of zero and a standard deviation of one. ", "page_idx": 27}, {"type": "text", "text": "Results. The numerical results in Table 4 demonstrate that Fair GLasso effectively reduces disparity error compared to standard GLasso, enhancing fairness while maintaining a good objective value. Figure 5 reveals notable differences in the learned network structures for AV45 results. The presence of edges between the left caudal middle frontal gyrus and right medial orbitofrontal cortex and between the left superior frontal gyrus and left superior parietal lobule in the GLasso graph suggests an increased influence of emotional factors on executive function and higher-order cognitive processes on amyloid accumulation, respectively [64, 6, 54]. Conversely, the absence of an edge between the left pars opercularis and left supramarginal gyrus in the Fair GLasso graph indicates a weaker association between language deficits and sensorimotor impairments in amyloid accumulation [18, 81, 38]. ", "page_idx": 27}, {"type": "text", "text": "In contrast, the AV1451 results show primarily numerical differences between the two graphs, with edges remaining largely unchanged, suggesting that the tau accumulation network is robust to the sensitive attribute of race. These findings highlight the importance of considering fairness in brain imaging data analysis and the potential of Fair GLasso to uncover more equitable and unbiased patterns of amyloid and tau accumulation in Alzheimer\u2019s disease. Further research is needed to validate these findings in larger and more diverse cohorts and explore the biological mechanisms and clinical implications of the observed differences between standard GLasso and Fair GLasso graphs. ", "page_idx": 27}, {"type": "text", "text": "D.5 Addendum to Subsection 4.6 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Table 5 summarizes the details of credit data utilized on Fair CovGraph mentioned in Section 4.6. ", "page_idx": 27}, {"type": "image", "img_path": "WvWS8goWyR/tmp/014f571c5fa13f7191461681df52ed3f2a2b17c07040a6715d1b6117c8dd16f6.jpg", "img_caption": ["(a) Standard GLasso (AV45, Marital Status) "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "WvWS8goWyR/tmp/215f9f360626e5ff72274b9a9932770999eebc8396192cd5026e5cba8b2e80e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "WvWS8goWyR/tmp/fd608f1df304a2277fe57f2c6ab71931704c3e44bd1504104c766a7788c9f789.jpg", "img_caption": ["(d) Fair GLasso (AV1451, Race) "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 5: Subfigures (a) and (b) present a comparison of the graphs generated by standard GLasso and Fair GLasso on the ADNI dataset, considering the sensitive attribute of marital status on the AV45 biomarker. Similarly, subfigures (c) and (d) compare the graphs generated by both methods, taking into account the sensitive attribute of race on the AV1451 biomarker. To improve the clarity of the visualizations, weak edges have been removed, and edges that show significant differences in values between the two methods are highlighted. It is important to note that even though some edges may appear unchanged in the visual comparison, their actual values will differ between the standard GLasso and Fair GLasso methods. ", "page_idx": 28}, {"type": "table", "img_path": "WvWS8goWyR/tmp/4aeaaa0ab4aeb83fd84387d33ecb389af0ee17479cb24a06c3015fbdc09c739d.jpg", "table_caption": ["Table 5: Distribution of the number of samples in each group in the credit dataset. \u201cHgEd\u201d represents \u201cHigh School Graduate or Higher\u201d, and \u201cLwEd\u201d represents \u201cEducation below High School Level\u201d. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 6: Numerical outcomes in terms of the value of the objective function $\\left(F_{1}\\right)$ , the summation of the pairwise graph disparity error $(\\Delta)$ , and the average computation time in seconds ( $\\pm$ standard deviation) from 10 repeated experiments. $K=2$ and $N_{k}=1000\\,\\forall k\\in[K]$ . \u201c $\\downarrow^{\\,,}$ means the smaller, the better, and the best value is in bold. These experiments are conducted on an Apple M2 Pro processor. Note that both $F_{1}$ and $\\Delta$ are deterministic. ", "page_idx": 29}, {"type": "table", "img_path": "WvWS8goWyR/tmp/eff598ad8ffd02286cf8c0bcd1616483404fd1c3e626ff0e77c043ee10cb07d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "WvWS8goWyR/tmp/bd62b68d2365ab5db88167d707a2a4a8e273d70e7a78deac5322bb5c975260aa.jpg", "img_caption": ["Figure 6: (a) Percentage change from GLasso to Fair GLasso (results from Table 6) with respect to feature size $P$ . $\\%F_{1}$ is slight, while $\\%\\Delta$ changes are substantial, signifying fairness improvement without significant accuracy sacrifice. (b) Runtime (mean $\\pm$ std) (results from Table 6) with respect to feature size $P$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "In this section, we examine the impact of varying feature sizes $P$ on the $\\%F_{1}$ score, $\\%\\Delta$ (change in accuracy), and runtime. Our experiments utilize feature sizes ranging from $P=50$ to $P=400$ in the GLasso algorithm applied to synthetic data. According to the procedures described in Steps 1-3 from Section D.2, we generate covariance matrices for two distinct groups: $\\pmb{\\Sigma}_{1}$ featuring five diagonal blocks and $\\pmb{\\Sigma}_{2}$ with four diagonal blocks. ", "page_idx": 29}, {"type": "text", "text": "For each feature size setting, Group 1 includes 1000 observations drawn from a multivariate normal distribution ${\\mathcal{N}}(0,\\Sigma_{1})$ , and Group 2 also consists of 1000 observations from $\\mathcal{N}(0,\\pmb{\\Sigma}_{2})$ . The outcomes of these experiments are systematically presented in Table 6 and visually depicted in Figure 6. This structured analysis enables us to evaluate how changes in feature size affect both performance metrics and computational efficiency in our study. ", "page_idx": 29}, {"type": "text", "text": "By integrating both the Table 6 and Figure 6, it can be observed that as the feature size increases, although there is a rise in the pairwise graph disparity error, our proposed method still effectively reduces it, with minimal loss in the objective value. This underscores the efficacy of our approach in enhancing fairness. Regarding runtime, there is a proportional relationship between feature size and the runtime of Fair GLasso, which aligns with our theoretical analysis of algorithmic complexity. ", "page_idx": 29}, {"type": "text", "text": "D.7 Sensitivity Analysis to Sample Size $N$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we conduct a sensitivity analysis with respect to the sample size $N$ , while holding the feature size fixed at $P=50$ . We investigate how varying the sample size impacts the $\\%F_{1}$ score, $\\%\\Delta$ (change in accuracy), and runtime. The sample sizes examined are $N_{k}=1\\bar{0}0,150,200,...,400,500$ for each group in the Fair GLasso on synthetic data. ", "page_idx": 29}, {"type": "text", "text": "Following the procedures outlined in Steps 1-3 in Section D.2, we generate synthetic datasets with fixed covariance structures for two distinct groups: $\\pmb{\\Sigma}_{1}$ characterized by five diagonal blocks, and $\\Sigma_{2}$ comprising four diagonal blocks. Each dataset is generated for every specified sample size, allowing ", "page_idx": 29}, {"type": "text", "text": "Table 7: Numerical outcomes in terms of the value of the objective function $\\left(F_{1}\\right)$ , the summation of the pairwise graph disparity error $(\\Delta)$ , and the average computation time in seconds ( $\\pm$ standard deviation) from 10 repeated experiments. $K=2$ and $P=50$ . \u201c $\\downarrow^{\\,,}$ means the smaller, the better, and the best value is in bold. These experiments are conducted on an Apple M2 Pro processor. Note that both $F_{1}$ and $\\Delta$ are deterministic. ", "page_idx": 30}, {"type": "image", "img_path": "WvWS8goWyR/tmp/4d60da5d7866856cd156a115c52f8aacdd7d0980b1d81746289890abb5e19048.jpg", "img_caption": ["Figure 7: (a) Percentage change from GLasso to Fair GLasso (results from Table 7) with respect to sample size $N$ . $\\%F_{1}$ is slight, while $\\%\\Delta$ changes are substantial, signifying fairness improvement without significant accuracy sacrifice. (b) Runtime (mean $\\pm$ std) (results from Table 7) with respect to sample size $N$ . "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "us to systematically assess the effects of increasing $N$ on the performance metrics and computational efficiency of the algorithm. ", "page_idx": 30}, {"type": "text", "text": "The specific results are presented in Table 7 and visualized in Figure 7. From these, it is evident that the sample size does not significantly impact the objective value, pairwise graph disparity error, or runtime. Our proposed method consistently maintains its effectiveness across different sample sizes. This stability highlights the robustness of our approach under varying data quantities. ", "page_idx": 30}, {"type": "text", "text": "D.8 Sensitivity Analysis to Sample Size Ratio $N_{2}/N_{1}$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Table 8: Numerical outcomes in terms of the value of the objective function $\\left(F_{1}\\right)$ , the summation of the pairwise graph disparity error $(\\Delta)$ , and the average computation time in seconds ( $\\pm$ standard deviation) from 10 repeated experiments. $K=2$ , $P=50$ , and $N_{2}=100$ . \u201c $\\downarrow^{\\,,}$ means the smaller, the better, and the best value is in bold. These experiments are conducted on an Apple M2 Pro processor. Note that both $F_{1}$ and $\\Delta$ are deterministic. ", "page_idx": 30}, {"type": "table", "img_path": "WvWS8goWyR/tmp/ebdadc864781f40dc327bafeb991e7def3553766e3553e36013a762544578393.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "WvWS8goWyR/tmp/8f0db114a13c37c9824a50331603618e6acac86c9f2c174e976879c59e3cfa0e.jpg", "img_caption": ["Figure 8: (a) Percentage change from GLasso to Fair GLasso (results from Table 7) with respect to sample size ratio $N_{1}/N_{2}$ . $\\bar{\\%}F_{1}$ is slight, while $\\%\\Delta$ changes are substantial, signifying fairness improvement without significant accuracy sacrifice. (b) Runtime (mean $\\pm$ std) (results from Table 7) with respect to sample size ratio $N_{1}/N_{2}$ . "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "We conduct a sensitivity analysis on the sample size ratio $N_{1}/N_{2}$ while keeping the feature size fixed at $P=50$ and Group 2\u2019s sample size $N_{2}$ constant at 100. We examine the impact of varying $N_{1}/N_{2}$ on the $\\%F_{1}$ , $\\%\\Delta$ , and runtime in our experiments with Fair GLasso on synthetic data. ", "page_idx": 31}, {"type": "text", "text": "Following the methodology outlined in Steps 1-3 from Section D.2, we generate datasets with fixed covariance structures: $\\bar{\\Sigma_{1}}$ characterized by five diagonal blocks for Group 1 and $\\Sigma_{2}$ with four diagonal blocks for Group 2. We systematically vary $N_{1}$ from 100 to 10,000, maintaining $N_{2}$ at 100, and assess how changes in the sample size ratio affect the algorithm\u2019s performance metrics and computational efficiency. ", "page_idx": 31}, {"type": "text", "text": "The specific results of these experiments are detailed in Table 8 and visualized in Figure 8. From this analysis, it is apparent that the sample size ratio $N_{1}/N_{2}$ does not significantly affect the objective value, pairwise graph disparity error, or runtime. Our proposed method continues to demonstrate its effectiveness consistently across varying sample size ratios. This consistency underscores the robustness of our approach, showing its reliability regardless of changes in the group imbalance between the groups. ", "page_idx": 31}, {"type": "text", "text": "D.9 Sensitivity Analysis to Group Size $K$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Table 9: Numerical outcomes in terms of the value of the objective function $\\left(F_{1}\\right)$ , the summation of the pairwise graph disparity error $(\\Delta)$ , and the average computation time in seconds ( $\\pm$ standard deviation) from 10 repeated experiments. $P\\,=\\,100$ and $N_{k}\\,=\\,1000\\;\\forall k\\,\\in\\,[K]$ . \u201c $\\downarrow^{\\,,}$ means the smaller, the better, and the best value is in bold. These experiments are conducted on an Apple M2 Pro processor. Note that both $F_{1}$ and $\\Delta$ are deterministic. ", "page_idx": 31}, {"type": "table", "img_path": "WvWS8goWyR/tmp/0a63f3d4a563f52b24b1e66c9dd81248131d1e80043d02d41c906712fc6a33dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "In this section, we explore the impact of group size $K$ on the performance and computational efficiency of Fair GLasso. The feature size $\\mathbf{\\bar{\\alpha}}_{N}$ is fixed at 100, and the sample size per group $P_{k}$ is set at 1000. Following Steps 1-3 from Section D.2, the covariance matrix for the first group, $\\pmb{\\Sigma}_{1}$ is generated with 10 diagonal blocks. Each subsequent group has one fewer diagonal block in its covariance matrix, with each group sampling observations from $\\mathcal{N}(0,\\Sigma_{k})$ . ", "page_idx": 31}, {"type": "text", "text": "The results are detailed in Table 9 and Figure 9. Observations indicate that when the group size is less than 9, computational efficiency remains relatively stable regardless of changes in group size. However, efficiency decreases noticeably when the group size increases to 9. In terms of the objective ", "page_idx": 31}, {"type": "image", "img_path": "WvWS8goWyR/tmp/4b5cf79b343dd317679d15acf89dd2390dc91e96f8160be27855b902e73161a5.jpg", "img_caption": ["Figure 9: (a) Percentage change from GLasso to Fair GLasso (results from Table 6) with respect to group size $K$ . $\\%F_{1}$ is slight, while $\\%\\Delta$ changes are substantial, signifying fairness improvement without significant accuracy sacrifice. (b) Runtime (mean $\\pm$ std) (results from Table 6) with respect to group size $K$ . "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Table 10: Outcomes of additional baseline with different optimization algorithms applied to GLasso and Multi-Objective Optimization (MOO), measured in terms of the value of the objective function $(F_{1})$ , the summation of the pairwise graph disparity error $(\\Delta)$ , and the average computation time in seconds ( $\\pm$ standard deviation) from 10 repeated experiments. \u201c $\\downarrow^{\\,,}$ indicates that smaller values are better. Our method applies ISTA to both GLasso and MOO (first row in each experiment). All experiments are conducted using the same runtime environment on Google Colab. ", "page_idx": 32}, {"type": "table", "img_path": "WvWS8goWyR/tmp/31270de7851fc6ede17618608b30b976e30971d1035266c3c30705076783a10b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "value and pairwise graph disparity error, performance maintains a good balance, with a significant enhancement in fairness. ", "page_idx": 32}, {"type": "text", "text": "This conclusion aligns with our theoretical analysis of algorithmic complexity. Notably, as the group size increases, the pairwise graph disparity error also significantly rises, as shown in Table 9. Consequently, our proposed method effectively enhances fairness, albeit at the cost of sacrificing a greater portion of the objective value. This trade-off is a critical aspect of our approach, balancing computational performance with the desired ethical outcomes in machine learning applications. ", "page_idx": 32}, {"type": "text", "text": "D.10 Addendum to Subsection 4.8 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To address the computational complexity of Fair GMs, we explore a range of optimization methods tailored to GLasso and multi-objective optimization (MOO): ", "page_idx": 32}, {"type": "text", "text": "GLasso Optimization Methods \u2013 Preconditioned Iterative Soft Thresholding Algorithm (PISTA): Efficiently handles large-scale sparse matrix operations [69]. \u2013 Graphical Iterative Shrinkage Thresholding Algorithm (GISTA): Employs an iterative framework for sparsity-inducing penalty functions in high-dimensional settings [60]. \u2013 Orthant-Based Newton Method (OBN): Uses second-order information for faster convergence in structured sparsity constraints [57]. ", "page_idx": 32}, {"type": "text", "text": "\u2022 MOO Optimization Methods ", "page_idx": 33}, {"type": "text", "text": "\u2013 Fast Iterative Shrinkage-Thresholding Algorithm (FISTA): Provides globally optimal convergence rates for MOO objectives [74]. \u2013 Stochastic Objective Selection Approach (SOSA): Introduces a randomized selection technique for optimizing multi-objective functions under varying conditions [70]. ", "page_idx": 33}, {"type": "text", "text": "We validate these methods through comprehensive experiments on synthetic datasets. Our first evaluation uses data with 100 variables across two subgroups, each containing 1000 observations, generated following the procedure in Appendix D.2. This experiment demonstrates that faster optimization methods improve time complexity for both GLasso and MOO while maintaining performance. All GLasso methods achieve optimal loss, while Fair GLasso variants successfully reduce pairwise graph disparity error without significant performance degradation. ", "page_idx": 33}, {"type": "text", "text": "To assess scalability, we extend our analysis to a larger dataset with 200 variables, maintaining the same experimental setup. Furthermore, we evaluate the efficiency of our approach with increased group complexity using synthetic data containing 100 variables across ten subgroups, each with 1000 observations. In this setting, SOSA reduces training time by approximately $36\\bar{\\%}$ compared to the original approach while preserving model fairness and robustness. ", "page_idx": 33}, {"type": "text", "text": "The numerical results presented in Table 10 confirm that our optimization strategies successfully reduce runtime while maintaining model robustness and fairness. These findings suggest promising directions for future research in balancing computational efficiency with model performance. ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we introduce the contribution and novelty of this work in the abstract and introduction. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we discuss the limitations of our work in the conclusion section Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we present the theoretical analysis of our proposed model in Section 3.   \nThe detailed proof is presented in the appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Yes, we describe the algorithm, baseline model, and detailed experiment procedures, including synthetic data generation, choices of parameters, and architecture. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Yes, we provide the code on GitHub for reproducing the results of all the simulation studies. For real-world applications, due to the accessibility of some datasets, we are not able to provide the dataset when submitting, but readers are able to download data through provided links. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, our work is about fairness in unsupervised graphical models, we describe the detailed sensitive attribute and the resulting group splitting. The optimization method is also well described. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we repeated all the experiments 10 times. We provide a full description of this statistical analysis in our implementation details provided in Section 4. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 36}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, all the experiments were conducted on a MacBook Pro with an Apple M2 Pro chip and 32 GB of memory. The additional experiments conducted during the rebuttal were implemented on Google Colab. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, we strictly follow the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, our work is about fairness in graphical models. Improving fairness in machine learning algorithms will definitively have positive societal impacts, which are discussed in the introduction and experiment sections. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes, we strictly follow the usage rules of datasets (ADNI and TCGA). And we don\u2019t use data or models that have a high risk for misuse. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Yes, we follow the following instructions and also cite and refer to them. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]