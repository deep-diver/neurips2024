[{"figure_path": "RfsfRn9OFd/figures/figures_3_1.jpg", "caption": "Figure 1: The meta information of video clips of 40 concepts, and experiment protocol. (A) Visualizations of the meta information for all video clips of 40 concepts, we plot the average of each meta information for each concept. (B) The data collecting environment. (C) Demonstration of a whole data collecting session. A session will contain 7 video blocks to be watched, and there are rest phases of at least 30 seconds each between blocks. (D) Demonstration of a video block, there is a 3-second hint before 5 different video clips of the same concept.", "description": "This figure shows the meta-information of the video clips used in the experiment, the experimental setup, and the protocol.  Panel A displays visualizations of the average meta-information (color, optical flow score, object number, human face presence, human presence, musical instrument presence, etc.) for each of the 40 video concepts. Panel B illustrates the experimental environment, showing the participant wearing an EEG cap, an eye tracker, and the equipment used for data acquisition. Panel C depicts the structure of a single data collection session. This session consists of seven video blocks, each lasting approximately 8 minutes and 40 seconds, separated by rest periods of at least 30 seconds each. Panel D details the structure of a video block. Each block begins with a 3-second hint indicating the video category, followed by five 2-second video clips belonging to that category.", "section": "3 EEG Dynamic Vision Dataset and Benchmarks"}, {"figure_path": "RfsfRn9OFd/figures/figures_4_1.jpg", "caption": "Figure 2: Statistics of each meta information: (A) the fraction of human appearance. (B) the fraction of face appearance (only count the videos with humans). (C) the distribution of different object numbers. (D) the distribution of different object colors. (E) the histogram of OFS.", "description": "This figure presents a statistical overview of the meta-information associated with the video clips in the dataset. It includes the proportion of videos featuring humans or human faces, the distribution of the number of objects in a scene, the distribution of the main color of the objects, and a histogram of the optical flow score (OFS), which indicates the level of dynamic movement in the videos.  These statistics provide insights into the dataset's diversity and visual characteristics, which are relevant for understanding the complexity of visual information that the EEG signals need to encode.", "section": "3.4 Meta Information of Video Clips"}, {"figure_path": "RfsfRn9OFd/figures/figures_5_1.jpg", "caption": "Figure 3: (A-B) GLMNet Encoders. (C-E) Overview of our EEG2Video framework. (A) The visual cortex, basically in the occipital lobe. (B) GLMNet architecture which combines the global and local embedding. (C)The framework of EEG2Video which predicts the latent variabels z0 and semantic guidance \u00eat with Seq2Seq model and a predictor, respectively. A video diffusion model is then be adopted for generating videos using z0 and \u00eat. (D) Dynamic-aware noise-adding process based on the decoded dynamic information \u03b2. (E) Using large amounts of video-text pairs to fine-tune the inflated diffusion UNet for video generation. The texts are obtained by BLIP.", "description": "This figure illustrates the EEG2Video framework proposed in the paper. It shows the architecture of the GLMNet encoder which combines global and local embeddings from EEG signals.  The main framework comprises a Seq2Seq model for predicting latent variables and semantic guidance, a video diffusion model for video generation using the predicted information, and a dynamic-aware noise-adding process incorporating dynamic information. Finally, a fine-tuned inflated diffusion UNet is used for video generation, guided by video-text pairs. ", "section": "4 Methodology"}, {"figure_path": "RfsfRn9OFd/figures/figures_7_1.jpg", "caption": "Figure 4: Spatial Analysis. (A-B). Topographies of each electrode's accuracy for Human/Animal and Fast/Slow tasks. (C). Ablate electrodes of different brain regions.", "description": "This figure presents a spatial analysis of brain activity related to visual perception tasks.  Subfigure (A) shows a topographic map of the EEG electrodes' accuracy in classifying Human/Animal videos, revealing that the occipital lobe shows high accuracy, supporting the visual cortex's role in object recognition. Subfigure (B) displays a similar map for Fast/Slow video classification, indicating a strong contribution from the temporal lobe, aligned with its role in processing movement.  Subfigure (C) demonstrates the impact of removing electrodes from various brain regions on classification accuracy. Removing occipital electrodes significantly reduces Human/Animal classification accuracy, highlighting the critical role of the visual cortex.  Conversely, temporal lobe removal primarily affects Fast/Slow accuracy, supporting the temporal lobe's role in dynamic visual perception.", "section": "5.1.2 Analysis of Brain Areas"}, {"figure_path": "RfsfRn9OFd/figures/figures_9_1.jpg", "caption": "Figure 5: Reconstruction Presentations. Various video clips with low dynamics (e.g., Mountain, Beach, Face) and high dynamics (e.g., Skiing, Fireworks, Dancing) across animals, scenes, persons, and activities can be correctly recovered.", "description": "This figure showcases several examples of videos reconstructed by the EEG2Video model.  It demonstrates the model's ability to reconstruct videos with both low and high dynamic content, featuring a range of subjects (animals, people), settings (beaches, mountains, indoor/outdoor), and actions (skiing, dancing, playing guitar).  Each row displays a ground truth video (GT) and a corresponding video reconstructed using the proposed model (Ours). The goal is to visually illustrate the quality of video reconstruction achieved by the model across diverse video content.", "section": "5.2 Video Reconstruction"}, {"figure_path": "RfsfRn9OFd/figures/figures_15_1.jpg", "caption": "Figure 6: Confusion Matrices of GLMNet (A). The performance using DE features. (B). The performance using raw EEG signals.", "description": "The figure shows the confusion matrices for the 40-class classification task using the GLMNet model.  Subfigure (A) displays the results using Differential Entropy (DE) features, while subfigure (B) shows the results using raw EEG signals. The matrices visualize the performance of the model by showing the counts of correctly and incorrectly classified instances for each class. The color intensity represents the proportion of predictions for a given true class that ended up in each predicted class. Darker colors indicate higher counts, and brighter colors indicate lower counts. This allows for a visual analysis of the model's strengths and weaknesses in classifying different classes within the 40-concept video dataset.  The diagonal line indicates correctly classified samples. ", "section": "5.1 EEG-VP Benchmark"}, {"figure_path": "RfsfRn9OFd/figures/figures_16_1.jpg", "caption": "Figure 1: The meta information of video clips of 40 concepts, and experiment protocol. (A) Visualizations of the meta information for all video clips of 40 concepts, we plot the average of each meta information for each concept. (B) The data collecting environment. (C) Demonstration of a whole data collecting session. A session will contain 7 video blocks to be watched, and there are rest phases of at least 30 seconds each between blocks. (D) Demonstration of a video block, there is a 3-second hint before 5 different video clips of the same concept.", "description": "This figure provides a detailed overview of the experimental setup and data collection process. It showcases the meta-information visualization for the 40 video concepts (A), the data acquisition setup (B), the structure of a complete recording session with video blocks and rest periods (C), and the within-block structure which starts with a 3-second hint followed by five 2-second video clips from the same category (D). This comprehensive visualization aids in understanding the study design and how data were collected.", "section": "EEG Dynamic Vision Dataset and Benchmarks"}, {"figure_path": "RfsfRn9OFd/figures/figures_17_1.jpg", "caption": "Figure 5: Reconstruction Presentations. Various video clips with low dynamics (e.g., Mountain, Beach, Face) and high dynamics (e.g., Skiing, Fireworks, Dancing) across animals, scenes, persons, and activities can be correctly recovered.", "description": "This figure shows various examples of video reconstruction results from EEG signals using the proposed EEG2Video method.  The top row of each section displays the ground truth video frames, while the bottom row shows the corresponding frames reconstructed by the model.  The examples include videos with both low and high dynamic content, demonstrating the model's ability to handle diverse visual information.", "section": "5.2 Video Reconstruction"}, {"figure_path": "RfsfRn9OFd/figures/figures_18_1.jpg", "caption": "Figure 5: Reconstruction Presentations. Various video clips with low dynamics (e.g., Mountain, Beach, Face) and high dynamics (e.g., Skiing, Fireworks, Dancing) across animals, scenes, persons, and activities can be correctly recovered.", "description": "This figure shows several examples of videos reconstructed by the EEG2Video model.  Each row shows a ground truth video (GT) and the video reconstructed by the model (Ours).  The examples demonstrate the model's ability to reconstruct both low-dynamic videos (showing static or slow-moving scenes) and high-dynamic videos (showing fast-paced action or movement). The videos cover various categories, including animals, scenes, people, and activities.", "section": "5.2 Video Reconstruction"}, {"figure_path": "RfsfRn9OFd/figures/figures_19_1.jpg", "caption": "Figure 5: Reconstruction Presentations. Various video clips with low dynamics (e.g., Mountain, Beach, Face) and high dynamics (e.g., Skiing, Fireworks, Dancing) across animals, scenes, persons, and activities can be correctly recovered.", "description": "This figure shows several examples of videos reconstructed by the EEG2Video model.  The figure demonstrates the model's ability to reconstruct videos with varying degrees of dynamic content.  The top row shows the ground truth video frames and the bottom row presents the corresponding frames reconstructed by the EEG2Video model.  The examples include static scenes (mountain, beach, face) and highly dynamic scenes (skiing, fireworks, dancing). The diversity of the examples illustrates the model's capacity to handle various types of visual information.", "section": "5.2 Video Reconstruction"}, {"figure_path": "RfsfRn9OFd/figures/figures_20_1.jpg", "caption": "Figure 5: Reconstruction Presentations. Various video clips with low dynamics (e.g., Mountain, Beach, Face) and high dynamics (e.g., Skiing, Fireworks, Dancing) across animals, scenes, persons, and activities can be correctly recovered.", "description": "This figure showcases the video reconstruction results of the proposed EEG2Video model. It presents several video clips, both with low and high dynamic content, that were successfully reconstructed using EEG signals. Each example consists of two rows: the top row displays the ground truth (GT) video frames, while the bottom row shows the frames generated by the EEG2Video model. This illustrates the model's ability to accurately reconstruct a variety of video content, capturing both subtle and rapid changes in visual information.", "section": "5.2 Video Reconstruction"}, {"figure_path": "RfsfRn9OFd/figures/figures_21_1.jpg", "caption": "Figure 12: Various videos reconstruction samples.", "description": "This figure shows various video reconstruction results from the EEG2Video model.  It demonstrates the model's ability to reconstruct videos across different categories, including animals, plants, people, and activities. Each row displays a sequence of frames from both the ground truth video and the video reconstructed by the model.  The figure illustrates both successes and limitations of the reconstruction approach.", "section": "5.2 Video Reconstruction"}, {"figure_path": "RfsfRn9OFd/figures/figures_22_1.jpg", "caption": "Figure 13: Some failure samples reconstructed in ours.", "description": "This figure showcases examples where the EEG2Video model's reconstruction of video clips was unsuccessful. It highlights instances where the model struggled to accurately capture either the semantic content or low-level visual details of the original videos.  Despite some failures, certain features like shapes, movements, and scene dynamics were partially reconstructed in some cases. This demonstrates that while the model can generate video from EEG data, achieving perfect reconstruction is still a challenge.", "section": "5.2 Video Reconstruction"}]