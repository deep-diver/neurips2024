[{"type": "text", "text": "Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Visual geo-localization demands in-depth knowledge and advanced reasoning skills   \n2 to associate images with real-world geographic locations precisely. In general,   \n3 traditional methods based on data-matching are hindered by the impracticality   \n4 of storing adequate visual records of global landmarks. Recently, Large Vision  \n5 Language Models (LVLMs) have demonstrated the capability of geo-localization   \n6 through Visual Question Answering (VQA), enabling a solution that does not   \n7 require external geo-tagged image records. However, the performance of a single   \n8 LVLM is still limited by its intrinsic knowledge and reasoning capabilities. Along   \n9 this line, in this paper, we introduce a novel visual geo-localization framework   \n10 called smileGeo that integrates the inherent knowledge of multiple LVLM agents   \n11 via inter-agent communication to achieve effective geo-localization of images.   \n12 Furthermore, our framework employs a dynamic learning strategy to optimize the   \n13 communication patterns among agents, reducing unnecessary discussions among   \n14 agents and improving the efficiency of the framework. To validate the effectiveness   \n15 of the proposed framework, we construct GeoGlobe, a novel dataset for visual geo  \n16 localization tasks. Extensive testing on the dataset demonstrates that our approach   \n17 significantly outperforms state-of-the-art methods. The source code is available at   \n18 https://anonymous.4open.science/r/ViusalGeoLocalization-F8F5/ and the dataset   \n19 will also be released after the paper is accepted. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Visual geo-localization, referred to the task of estimating geographical identification for a given   \n22 image, is vital in various fields such as human mobility analysis [1, 2, 3, 4, 5] and robotic navigation   \n23 [6, 7, 8, 9, 10, 11]. In general, accurate visual geo-localization without the help of any localization   \n24 equipment (e.g., GPS sensors) is a complex task that requires abundant geospatial knowledge and   \n25 strong reasoning capabilities. Traditional methods [12, 13, 14, 15] typically formulate it as an image   \n26 retrieval problem where to geo-localize the given image by retrieving similar images with known   \n27 geographical locations. Thus, their effectiveness is limited by the scope and quality of the geo-tagged   \n28 image records.   \n29 Recently, the success of Large Vision-Language Models (LVLMs) has enabled Visual Question   \n30 Answering (VQA) to become a unified paradigm for multi-modal problems [16, 17], providing a   \n31 novel solution for visual geo-localization without the need for external geo-tagged image records.   \n32 However, the performance of a single LVLM on the geo-localization task is still limited by its   \n33 inherent geospatial knowledge and reasoning capabilities. Along this line, in this paper, we introduce   \n34 a novel multi-agent framework, named swarm intelligence Geo-localization (smileGeo), which   \n35 aims to adaptively integrate the inherent knowledge and reasoning capabilities of multiple LVLMs   \n36 to effectively and efficiently geo-localize images. Specifically, for a given image, the framework   \n37 initially elects $K$ suitable LVLM agents as answer agents for initial location analysis. Then, each   \n38 answer agent chooses several review agents via an adaptive social network, which imitates the   \n39 collaborative relationships between agents with a target on the visual geo-localization task, to   \n40 discuss and share their knowledge for refining its location analysis. Finally, our framework conducts   \n41 free discussion among all of the answer agents to reach a consensus. Besides, we also design   \n42 a novel dynamic learning strategy to optimize the election mechanism along with the adaptive   \n43 collaboration social network of agents. We hope that by the effectiveness of the election mechanism   \n44 and the review mechanism, our framework can discover the mode of communication among agents,   \n45 thereby enhancing geo-localization performance through multi-agent collaboration while minimizing   \n46 unnecessary discussions. In summary, our contributions are demonstrated as follows:   \n47 \u2022 A novel swarm intelligence geo-localization framework, smileGeo, is proposed to adaptively   \n48 integrate the inherent knowledge and reasoning capability of multiple LVLMs through   \n49 discussion for visual geo-localization tasks.   \n50 \u2022 A dynamic learning strategy is introduced to discover the most appropriate discussion mode   \n51 among LVLM agents for enhancing the effectiveness and efficiency of the framework.   \n52 \u2022 A new visual geo-localization dataset named GeoGlobe1 is collected, containing a wide   \n53 variety of images globally. The diversity and richness of GeoGlobe allow us to evaluate   \n54 the performance of different models more accurately. Moreover, extensive experiments   \n55 demonstrate our competitive performance compared to state-of-the-art methods.   \n56 The remainder of this paper is organized as follows: Section 2 discusses the related literature. In   \n57 Section 3, the proposed framework is introduced. Section 4 provides the performance evaluation, and   \n58 Section 5 concludes the paper. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "59 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "60 Visual Geo-localization. Recent research in visual geo-localization, commonly referred to as   \n61 geo-tagging, primarily focuses on developing image retrieval systems to address this challenge   \n62 [3, 18, 19, 20, 21, 22]. These systems utilize learned embeddings generated by a feature extraction   \n63 backbone, which includes an aggregation or pooling mechanism [23, 24, 25, 26]. However, the   \n64 applicability of these retrieval systems to globally geo-localize landmarks or natural attractions is   \n65 often limited by the constraints of the available database knowledge and the restrictions imposed by   \n66 national or regional geo-data protection laws. Alternatively, some studies treat visual geo-localization   \n67 as a classification problem [27, 28, 29, 30]. These approaches posit that two images from the same   \n68 geographical region, despite depicting different scenes, typically share common semantic features.   \n69 Practically, these methods organize the geographical area into discrete cells and categorize the   \n70 image database accordingly. This cell-based categorization facilitates scaling the problem globally,   \n71 provided the number of categories remains manageable. However, while the number of countries   \n72 globally remains relatively constant, accurately enumerating cities in real-time at a global scale is   \n73 challenging due to frequent administrative changes, such as city reorganizations or mergers, which   \n74 reflect shifts in national policies. Additionally, in the context of globalization, this strategy has   \n75 inherent limitations. The recent advent of LVLMs offers promising compensatory mechanisms for   \n76 the deficiencies observed in traditional geo-localization methodologies, making the exploration of   \n77 LVLM-based approaches significantly relevant in current research.   \n78 Multi-agent Framework for LLM/LVLMs. LLM/LVLM agents have demonstrated the potential   \n79 to act like human [31, 32, 33], and a large number of studies have focused on developing robust   \n80 architectures for collaborative LLM/LVLM agents [34, 35, 36, 37, 38]. These architectures enable   \n81 each LLM/LVLM agent that endows with unique capabilities to engage in debates or discussions.   \n82 For instance, [34] proposes an approach to aggregate multiple LLM/LVLM responses by generating   \n83 candidate responses from various LLM/LVLM in a single round and employing pairwise ranking to   \n84 synthesize the most effective response. While some studies [34] utilize a static architecture potentially   \n85 limiting the performance and generalization of LLM/LVLM, others like [38] have implemented   \n86 dynamic interaction architectures that adjust according to the query and incorporate user feedback.   \n87 Recent advancements also demonstrate the augmentation of LLM/LVLM as autonomous agents   \n88 capable of utilizing external tools to address challenges in interactive settings. These techniques   \n89 include retrieval augmentation [39, 40, 41], mathematical tools [40, 42, 43], and code interpreters   \n90 [44, 45]. With these capabilities, LLM/LVLMs are well-suited for various tasks, especially for   \n91 geo-localization. However, most LLM/LVLM agent frameworks mandate participation from all   \n92 agents in at least one interaction round, leading to significant computational overhead. To address   \n93 this issue, our framework introduces a dynamic learning strategy electing only a small number of   \n94 agents to geo-localize different images, which significantly enhances the efficiency of LLM/LVLM   \n95 agents by reducing unnecessary interactions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "96 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 In this section, we first present the overall framework and then introduce each part of smileGeo in   \n98 detail for geo-localization tasks. ", "page_idx": 2}, {"type": "text", "text": "99 3.1 Model Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 In this paper, we denote the social network of LVLM agents by $\\mathcal{G}$ , where $\\mathcal{G}=\\{\\nu,\\mathcal{E}\\}.\\nu$ stands for the   \n101 agent set and $\\mathcal{E}$ presents the edge set. Each agent $v_{i}\\in\\mathcal{V},i\\in[N]$ is an LVLM, which is pre-trained   \n102 by massive vision-language data and can infer the possible location $\\mathbf{Y}$ of a given image $\\mathbf{\\deltaX}$ . Besides,   \n103 each edge $e_{i j}\\in\\mathcal{E},i,j\\in[N]$ is the connection weighted by the improvement effect of agent $v_{i}$ to   \n104 agent $v_{j}$ via discussion regarding the geo-localization performance.   \n105 As illustrated in Figure 1, smileGeo contains the process of the review mechanism in agent discussions   \n106 along with a dynamic learning strategy of agent social networks:   \n107 The review mechanism in agent discussions is a 3-stage anonymous collaboration approach to allow   \n108 LVLM agents to reach a consensus via discussion. In the first stage, for a given image $\\mathbf{\\deltaX}$ , our   \n109 framework elects the most suitable $K$ agents as answer agents by agent election probability $\\mathbf{\\delta}L s t$ . In   \n110 the second stage, these answer agents respectively select $R$ review agents by the adaptive collaboration   \n111 social network $\\pmb{A}$ to refine their answer via discussion. Finally, our framework facilitates consensus   \n112 among all agents through open discussion to reach a final answer. Both $\\mathbf{\\delta}L s t$ and $\\pmb{A}$ are analyzed   \n113 from the given image $\\mathbf{\\deltaX}$ , allowing our framework to minimize unnecessary discussions, thereby   \n114 significantly enhancing its efficiency while maintaining its accuracy. Moreover, the multi-stage   \n115 discussion facilitates communication among agents, maximizing the integration of their knowledge   \n116 and reasoning abilities to generate an accurate response $\\mathbf{\\deltaY}$ .   \n117 To get $\\mathbf{\\delta}L s t$ and $\\pmb{A}$ , we specifically design a dynamic learning module, which initially deploys the   \n118 encoder component of a pre-trained image variational autoencoder (VAE) to extract features from the   \n119 given image $\\mathbf{\\deltaX}$ . The extracted features, combined with agent embeddings $_{E m b}$ , are employed to   \n120 determine the suitability of agents $w.r.t.~L s t$ for agent discussions and predict agent collaboration   \n121 connections $\\pmb{A}$ in the geo-localization task. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "122 3.2 Review Mechanism in Agent Discussions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "123 LLM/LVLM have demonstrated remarkable capabilities in complicated tasks and some pioneering   \n124 works have further proven that the performances can be further enhanced by ensembling multiple   \n125 LLM/LVLM agents. Thus, to improve the geo-localization capability of LVLMs, we propose a   \n126 cooperation framework to effectively integrate the diverse knowledge and reasoning abilities of   \n127 multiple LVLMs. Inspired by the fact that community review mechanisms can improve the quality of   \n128 manuscripts, an iterative 3-stage anonymous reviewing mechanism is proposed for helping agents   \n129 share knowledge and reasoning capability with each other through their collaboration social network:   \n130 i) answer agent election & answering, ii) review agent selection & reviewing, and iii) final answer   \n131 conclusion. ", "page_idx": 2}, {"type": "text", "text": "132 Stage 1: Answer Agent Election & Answering ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "133 Initially, we select $K$ agents with the highest agent election probabilities $\\mathbf{\\delta}L s t$ as answer agents and   \n134 let them geo-localize independently as the preliminary step for further discussion. By initiating   \n135 the discussion with a limited number of agents, we aim to reduce potential chaos and maintain the   \n136 efficiency of our framework as the number of participating agents increases.   \n137 After the answer agents are elected, we send the image $\\mathbf{\\deltaX}$ to all answer agents and let them give the   \n138 primary analysis. Each answer must contain three parts: one location (city, country, and so on), one   \n139 confidence (a percentage number), and a detailed explanation. ", "page_idx": 2}, {"type": "image", "img_path": "HYwfZEhyK4/tmp/0370722a1dd91a3b081ea84995a97ff8e855d2d5d8a4042500b767c267ecd520.jpg", "img_caption": ["Figure 1: The framework overview of smileGeo. It contains the process of review mechanism in agent discussions along with a dynamic learning strategy of agent collaboration social networks. The first part deploys a review mechanism for LVLMs to discuss and share their knowledge anonymously, which could enhance the overall performance of geo-localization tasks. The second one mainly utilizes the GNN-based learning module to improve efficiency by reducing unnecessary discussions among agents while showing the process of updating the agent collaboration social network during the training process. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "140 Stage 2: Review Agent Selection & Reviewing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "141 In this stage, for each answer agent, we choose $R$ review agents by performing a transfer-probability  \n142 based random walk on the agent collaboration social network $\\mathcal{G}$ for answer reviewing. The transfer   \n143 probability $p(v_{i},v_{j})$ from node $v_{i}$ to node $v_{j}$ can be calculated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(v_{i},v_{j})=\\left\\{{\\frac{A_{i j}}{\\sum_{k\\in{\\mathcal{N}}(v_{i})}A_{i k}}},\\quad{\\mathrm{if~}}e_{i j}\\in{\\mathcal{E}}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "144 where ${\\mathcal N}(v_{i})$ is the 1-hop neighbor node set of node $v_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "145 For each selected review agent, it reviews the results as well as the explanations generated by the   \n146 corresponding answer agent and gives its own comments. After that, each answer agent would   \n147 summarize their preliminary analysis and the feedback from all of its review agents to get the final   \n148 answer, which must include three parts as well: one location, one confidence, and an explain. ", "page_idx": 3}, {"type": "text", "text": "149 Stage 3: Final Answer Conclusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "150 In the previous stage, each answer agent produces a refined result based on feedback. When $K>1$ in   \n151 Stage 1, the proposed framework generates multiple independent results, which may not be consistent.   \n152 However, we aim to provide a definitive answer rather than multiple options for people to choose   \n153 from. To address this, we allow up to $Z$ rounds of free discussion among those answer agents to   \n154 reach a unified answer:   \n155 First, we maintain a global dialog history list, diag, recording all replies agents respond. In addition,   \n156 discussions are executed asynchronously, which means that any answer agent can always reply based   \n157 on the latest diag, and replies would be added to the end of diag as soon as they are posted. Each   \n158 answer agent is allowed to speak only once in each discussion round, and after $Z$ rounds of free   \n159 discussion, we determine the final result using a minority-majority approach, i.e., we choose the reply   \n160 with the most agreement as the final conclusion. If all agents reach a consensus, we early stop this   \n161 stage and adopt the consensus answer as the final answer. If none of any consensus is reached, we   \n162 only select the reply of the first answer agent elected from Stage 1 as the final result. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "163 3.3 Dynamic Learning Strategy of Agent Collaboration Social Networks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "164 In our framework, choosing the appropriate answer agents and review agents for knowledge sharing   \n165 and discussion is vital to its effectiveness and efficiency. Therefore, we propose a dynamic learning   \n166 strategy to optimize them. Specifically, for each training sample, i.e., a geo-tagged image, we would   \n167 first estimate the optimal answer agent election probabilityL\u02c6st and the optimal collaboration social   \n168 network of agent $\\hat{\\mathcal G}$ by its actual location. Then we train an attention-based graph neural network,   \n169 which aims to predict $\\mathbf{\\delta}L s t$ and $\\mathcal{G}$ , by such estimated ground truth.   \n170 To estimate the optimal $\\hat{L s t}$ and $\\hat{A}$ for agents to geo-localize image $\\mathbf{\\deltaX}$ , we first initialize the agent   \n171 social network $\\mathcal{G}^{(0)}$ by a fully connected graph with the agent set $\\mathcal{V}$ . Besides, we initialize the agent   \n172 election probability $L s t^{(0)}=[0.5,0.5,\\cdot\\cdot\\cdot]$ , with all agents having $50\\%$ probability of being chose   \n173 as answer agents.   \n174 Then, we iteratively conduct our 3-stage discussion framework to get the prediction answer. $\\mathbf{\\mathit{Lst}}^{(l)}$   \n175 and $\\mathcal{G}^{(l)}$ is updated at the end of each round $l\\in L$ by comparing the answers ${Y}_{v_{i}}^{(l)}$ from each answer   \n176 agent with the ground truth Y\u02c6 .   \n177 After $L$ rounds of agent discussions, the updated agent election probability for an image $\\mathbf{\\deltaX}$ $\\stackrel{\\cdot}{,}\\pmb{L}\\hat{s}t:=$   \n178 $L s t^{(L)}(\\pmb{X})\\,=\\,[P_{v_{1}}^{(\\bar{L})},P_{v_{2}}^{(L)},\\cdot\\cdot\\cdot\\,,P_{v_{N}}^{(L)}]$ , determines whether an agent $v_{i}$ gives the correct/wrong   \n179 answers $Y_{v i}^{(L)}$ by comparing it with the ground truth $\\hat{Y}$ . Here, the definition of $P_{v_{i}}^{(l)}$ f agent vi at o   \n180 round $l$ is as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{v_{i}}^{(l)}:=\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{~if~}\\mathcal{D}(\\hat{Y},Y_{v_{i}}^{(l)})>t h}\\\\ {1,}&{\\mathrm{~if~}\\mathcal{D}(\\hat{Y},Y_{v_{i}}^{(l)})\\leq t h}\\\\ {\\frac{1}{2},}&{\\mathrm{~if~}v_{i}\\mathrm{~did~not~participate~in~the~discussion}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "181 where $^{t h}$ is a pre-defined threshold for determining whether the predicted location is close enough   \n182 to the actual location. In the distance function $\\mathcal{D}(\\cdot)$ , we first deploy geocoding to convert natural   \n183 language into location intervals in a Web Mercator coordinate system (WGS84) by utilizing OSM   \n184 APIs, and then compute the shortest distance between two two location intervals.   \n185 Please note that, rather than electing the top- $K$ answer agents in each round, we choose each agent   \n186 with probability $P_{v_{i}}$ during the training period to ensure that every agent has the opportunity to   \n187 participate in the discussion for more accurate estimation, as shown at the left part of the dynamic   \n188 learning strategy module of agent collaboration social networks in Figure 1.   \n189 In addition, the agent collaboration social network would also be updated by comparing the actual   \n190 location with the generated answer of each answer agent at the same time. For $l$ -th round, we   \n191 strengthen the link between the correctly answered agent and the corresponding review agents while   \n192 weakening the link between the incorrectly answered agent and the corresponding review agents: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{A}_{i j}:=A_{i j}^{(l)}(\\pmb{X})=\\left\\{\\frac{t t+1}{2t t}A_{i j}^{(l-1)}(\\pmb{X}),\\quad\\mathrm{if~agent~}v_{i}\\mathrm{~answers~correctly}\\right.}\\\\ {\\frac{2t t-1}{2t t}A_{i j}^{(l-1)}(\\pmb{X}),\\quad\\mathrm{if~agent~}v_{i}\\mathrm{~answers~incorrectly}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 where $A_{i j}^{(l-1)}(X)$ is the weight of the connection between answer agent $v_{i}$ and review agent $v_{j}$   \n194 at round $l\\,-\\,1$ when geo-locating image $\\mathbf{\\deltaX}$ , ${\\pmb A}_{i j}^{(0)}({\\pmb X})\\,=\\,1,i\\,\\neq\\,j,A_{i i}^{(0)}({\\pmb X})\\,=\\,0,i,j\\,\\in\\,[N],$ tt   \n195 is the number of consecutive times an agent has answered correctly, which is used to attenuate   \n196 the connection weights when updating them, preventing the performance of an agent on a certain   \n197 portion of the continuous dataset from interfering with the model\u2019s evaluation of the current agent\u2019s   \n198 performance on the entire dataset.   \n199 Then, we try to learn an attention-based graph neural network to predict the corresponding optimal   \n200 agent election probability $L s t\\;=\\;h(\\bar{X^{}},\\bar{{\\mathcal G}}|\\Theta)$ and the optimal agent collaboration connections   \n201 $\\bar{\\pmb{A}}=f(\\pmb{X},\\pmb{\\nu}|\\bar{\\Theta)}$ : ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\mathrm{Att}_{\\mathrm{GNN}}(F e a,F e a,1)}\\\\ &{\\quad=\\mathrm{softmax}\\left(\\frac{F e a\\cdot F e a^{\\top}}{\\sqrt{d_{k}}}\\right)\\mathbf{1},}\\\\ &{L s t=\\sigma^{\\prime}\\left(\\mathrm{Linear}\\left(\\mathrm{Flatten}\\left(\\sigma\\left(A\\cdot F e a\\cdot W\\right)\\right)\\right)\\right),}\\\\ &{F e a=\\mathrm{Linear}\\left(E m b+\\mathrm{VAE}_{\\mathrm{Enc}}(X)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 where $W,E m b\\in\\Theta$ are two learnable parameters, $E m b:=[E m b_{v_{1}},E m b_{v_{2}},\\cdot\\cdot\\cdot]^{\\top}$ is the agent   \n203 embedding and $W$ is the weight matrix, $\\sigma(\\cdot)$ is the LeakyReLU function, $\\boldsymbol{\\sigma^{\\prime}}(\\cdot)$ is the Sigmoid   \n204 function, $\\mathrm{VAE}_{\\mathrm{Enc}}(\\cdot)$ is the encoder of the image VAE that compresses and maps the image data   \n205 into the latent space. It is used to align the image features with the agent embedding, and $d_{k}$ is the   \n206 dimension of the $F e a$ . Our learning target can be formalized as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\Theta}\\sum_{i}^{N}\\mathcal{D}(\\hat{Y},\\boldsymbol{Y}_{v_{i}})\\mathbb{1}(v_{i}\\mathrm{~gives~an~answer})+\\mathrm{MSE}(L\\hat{\\boldsymbol{s}}t,L{\\boldsymbol{s}}t)+\\mathrm{MSE}(\\hat{\\boldsymbol{A}},\\boldsymbol{A}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 where $\\mathcal{D}(\\cdot)$ denotes the distance between the places an LVLM agent answered and the ground truth,   \n208 $\\mathbb{1}(\\cdot)$ is the indicator function, $\\pmb{Y}_{v_{i}}:=\\pmb{Y}_{v_{i}}^{(L)}=g_{v_{i}}(\\pmb{X},\\pmb{Y}_{v_{j}}^{(L-1)}).$ , $g_{v_{i}}(\\cdot)$ represent the LVLM agent $v_{i}$   \n209 with fixed parameters and $Y_{v_{i}}^{(0)}=g_{v_{i}}(X)$ is the answer that LVLM agent $v_{i}$ generates at the initial   \n210 stage of discussion. ", "page_idx": 5}, {"type": "text", "text": "211 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "212 To evaluate the performance of our framework, we conducted experiments on the real-world dataset   \n213 that was gathered from the Internet to answer the following research questions: ", "page_idx": 5}, {"type": "text", "text": "214 \u2022 RQ1: Can smileGeo outperform state-of-the-art methods in open-ended geo-localization tasks? ", "page_idx": 5}, {"type": "text", "text": "215 \u2022 RQ2: Are LVLM agents with diverse knowledge and reasoning abilities more suitable for building   \n216 a collaboration social network of agents? ", "page_idx": 5}, {"type": "text", "text": "217 \u2022 RQ3: How does the setting of hyperparameters affect the performance of smileGeo? ", "page_idx": 5}, {"type": "text", "text": "218 4.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "219 Datasets. In this paper, we newly construct a geo-localization dataset named GeoGlobe. It contains a   \n220 variety of man-made landmarks or natural attractions from nearly 150 countries with different cultural   \n221 and regional styles. The diversity and richness of GeoGlobe allow us to evaluate the performance of   \n222 different models more accurately. More details can be found in Appendix B.   \n223 Implemention Details. We select both open-source and close-source LVLMs with different scales   \n224 trained by different datasets as agents in the proposed framework. As for the open-source LVLMs,   \n225 we utilize several open-source fine-tuned LVLMs: Infi-MM2, Qwen-VL 3, vip\u2013llava\u20137b& $13\\mathrm{{b}^{4}}$ , llava\u2013   \n226 1.5\u20137b\u2013base&mistral&vicuna5, llava\u20131.6\u20137b&13b&34b\u2013mistral&vicuna6, $\\mathrm{CogVLM^{7}}$ . As for the   \n227 closed-source LVLMs, we chose the models provided by three of the most famous companies in the   \n228 world: Claude\u20133\u2013opus8, GPT\u2013 $\\mathrm{4V^{9}}$ , and Gemini\u20131.5\u2013pro 10. Besides, $99\\%$ of images (about 290,000   \n229 samples) from the original dataset are randomly chosen as training samples. For the open-world   \n230 geolocation problem, we construct the test dataset using approximately 4,000 samples, of which   \n231 nearly $66.67\\%$ samples reflected different locations not present in the training dataset. More details   \n232 about the deployment of smileGeo and the related parameter settings can be found in Appendix C.   \n233 Baselines. In this work, we compare the proposed framework with three kinds of baselines: single   \n234 LVLMs, LLM/LVLM-based multi-agent frameworks, and image retrieval approaches. Firstly, we use   \n235 each LVLM alone as an agent directly for the geo-localization task and compute the performance of   \n236 these single LVLMs under the same dataset. In addition, we experiment with multi-agent collaborative   \n237 frameworks, including LLM-Blender [34], PHP [35], Reflexion [36], LLM Debate [37], and DyLAN   \n238 [38]. Finally, several state-of-the-art image retrieval approaches, including NetVLAD [3], GeM   \n239 [26], and CosPlace [46], are also used to be part of the baselines. We set the training dataset as the   \n240 geo-tagged image database of each image retrieval system and use images in the test dataset for the   \n241 retrieval system to generate answers.   \n242 Evaluation Metrics. We use Accuracy (Acc) to evaluate the performance: $\\begin{array}{r}{A c c u r a c y=\\frac{N_{\\mathrm{correct}}}{N_{\\mathrm{total}}}}\\end{array}$   \n243 where $N_{\\mathrm{correct}}$ is the number of samples that the proposed framework correctly geo-localizes, and   \n244 $N_{\\mathrm{total}}$ refers to the total number of testing samples.   \n245 In this paper, we first geo-encode the answers with the ground truth, i.e., we transform the addresses   \n246 described through natural language into latitude-longitude coordinates. Then, we calculate the   \n247 distance between the two coordinates. When the distance between the two coordinates is less than   \n248 $t h=50k m$ (city-level), we consider the answer of the framework to be correct. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "249 4.2 Performance Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "250 We divide the baseline comparison experiment into three parts: i) comparison with single LVLMs,   \n251 ii) comparison with LLM/LVLM-based agent frameworks, and iii) comparison with image retrieval   \n252 systems.   \n253 Firstly, the performance of all single LVLM baselines is shown in Table 1, in terms of the metric   \n254 Acc. The data in Table 1 indicate that open-source LVLMs with diverse knowledge and reasoning   \n255 capabilities exhibit significant variations, particularly in geo-localization tasks. This may be due   \n256 to the difference in the overlap between the pre-training datasets used by different LVLMs and   \n257 the dataset we constructed. Therefore, in addition to querying the LVLM locations about images,   \n258 we also incorporated real-time image search results from Google to provide the model with more   \n259 comprehensive information. These results from Internet retrievals are incorporated into the chain-of  \n260 thoughts (CoT) [47] of LVLMs as external knowledge. At this time, models with larger parameters,   \n261 such as llava\u20131.6\u201334b, demonstrate superior reasoning abilities compared to smaller models (7b or   \n262 13b). In addition, closed-source large models also show more consistent performance than their open  \n263 source counterparts and are more adept at analyzing and utilizing external knowledge for accurate   \n264 inferences. Compared to all single LVLMs, our proposed LVLM agent framework surpasses all   \n265 single LVLM baselines in accuracy. This improvement confirms the effectiveness of different LVLMs   \n266 collaborating by engaging in discussions and analyzing various types of images, thus producing more   \n267 precise results.   \n268 Secondly, the comparative results across various LLM/LVLM agent frameworks are presented in   \n269 Table 2. It is evident that the majority of LLM/LVLM agent frameworks surpass individual LVLMs   \n270 in terms of geo-localization accuracy. This improvement can primarily be attributed to the ability to   \n271 integrate knowledge from multiple LVLM agents, thereby enhancing the overall precision of these   \n272 frameworks. However, LLM-Blender and LLM Debate exhibit lower accuracy due to statements of   \n273 some agents misleading others during discussions, which impedes the generation of correct outcomes.   \n274 Our framework, smileGeo, guarantees the highest accuracy while being able to accomplish the   \n275 geo-localization task with the lowest token costs. The average number of tokens our framework   \n276 spent per query is 18,876, and it is less than the computational overhead of LLM-Blender (23,662),   \n277 which has the simplest agent framework structure but the lowest accuracy among all baselines. This   \n278 is mainly due to a \u2019small\u2019 GNN-based dynamic learning model being deployed for agent selection   \n279 stages and significantly reducing unnecessary discussions among agents.   \n280 Finally, Table 3 presents the comparison be  \n281 tween the proposed framework and existing   \n282 image retrieval systems. Our framework,   \n283 smileGeo, consistently outperforms all other   \n284 retrieval-based approaches. This superior   \n285 performance can be attributed to the fact   \n286 that other image retrieval methods rely on   \n287 a rich geo-tagged image database. In our test   \n288 dataset, however, two-thirds of the images   \n289 are new and localized in completely different areas from those in the training dataset. This highlights   \n290 the shortages of conventional database-based retrieval systems due to the limitations of the geo-tagged   \n291 image databases and demonstrates the effectiveness of our proposed framework in solving open-world   \n292 geo-localization tasks. ", "page_idx": 6}, {"type": "table", "img_path": "HYwfZEhyK4/tmp/0e4c67a36c97594b24e843d75577f462ee9e7426f2a926fa79a64172b22c2077.jpg", "table_caption": ["Table 1: Results of different single LVLM baselines. "], "table_footnote": ["Bold indicates the statistically significant improvements (i.e., two-sided t-test with $p<0.05)$ ) over the best baseline. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "HYwfZEhyK4/tmp/93a1b58e566ef3366d90689d92714258d311dcc7eef4e807ded763d0c4523157.jpg", "table_caption": ["Table 2: Results of different agent frameworks without web searching. "], "table_footnote": ["\u2019Tks\u2019 means the average tokens a framework costs per query (including image tokens). ", "\u2019Acc\u2019 stands for the accuracy of the framework; "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "HYwfZEhyK4/tmp/4d9ba6c38dfebc9e872915e01350170ca0ff6d2d7220767d93973a0f38f7b3c6.jpg", "table_caption": ["Table 3: Comparison with image retrieval systems. "], "table_footnote": ["Bold indicates the statistically significant improvements (i.e., two-sided t-test with $p<0.05)$ ) over the best baseline. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "293 4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "294 Number of Agents. We further demonstrate the relationships between the number of agents and the   \n295 framework performance. We conduct experiments in two ways: i) by calling the same closed-source   \n296 LVLM API (Here, we use Gemini-1.5-pro because it performs best without the help of the Internet)   \n297 under different prompts (e.g., You are good at recognizing natural attractions; You\u2019re a traveler around   \n298 Europe) to simulate different agents, and ii) by using different LVLM backbones to represent distinct   \n299 agents. The results are shown in Figure 2.   \n300 As illustrated in Figure 2(a), the framework achieves optimal accuracy with 4 or 5 agents. Beyond   \n301 this number, the framework\u2019s performance begins to deteriorate. This shows that using models   \n302 with the same knowledge and reasoning capabilities as different agents has limited improvement   \n303 in the accuracy of the framework. Despite this decline, the performance of frameworks other than   \n304 LLM-Blender and LLM Debate remains superior to that of a single agent. LLM-Blender and LLM   \n305 Debate, however, have a significant decrease in model accuracy when the number of agents exceeds   \n306 11. This is mainly because both of them involve all LVLMs in every discussion, which suffers from   \n307 excessive repetitive and redundant discussions. Figure 2(b) reveals that the accuracy of the framework   \n308 improves with the incorporation of more LVLM backbones, indicating that the diversity of LVLMs   \n309 can enhance the quality of discussions.   \n310 Hyperparameter $K$ & $R$ . There are two hyperpa  \n311 rameters, $K$ and $R$ , that need to be pre-defined in the   \n312 proposed framework: $K$ is the number of agents (an  \n313 swer agents) that respond in each round of discussion,   \n314 and $R$ is the number of agents (review agents) used   \n315 to review answers from answer agents. Therefore, we   \n316 conduct experiments under different combinations of   \n317 $K\\in[1,8]$ and $R\\in[1,8]$ , as shown in Figure 3. The re  \n318 sults indicate that optimal performance can be achieved   \n319 with relatively small values of $K$ or $R$ . However, the   \n320 computational cost, measured in tokens, increases ex  \n321 ponentially with higher values of $K$ and $R$ . To balance   \n322 both the efficiency and the accuracy of smileGeo, for   \n$K$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "HYwfZEhyK4/tmp/4056d1abe8c8920b255ed638fc85e00496fbedfdb79854b3a4ca94ee1ad719b4.jpg", "img_caption": ["Figure 2: Results of model performance in relation to the number of agents. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "323 the experiments presented in this paper, we set both and ", "page_idx": 8}, {"type": "image", "img_path": "HYwfZEhyK4/tmp/f52834d8176d325c08b2b259044cbab768028cf499aca4a13fb4b982ea5d2cbb.jpg", "img_caption": ["Figure 3: Results under different $K$ and $R$ . $R$ equal to 2. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "324 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "325 This work introduces a novel LVLM agent framework, smileGeo, specifically designed for geo  \n326 localization tasks. Inspired by the review mechanism, it integrates various LVLMs to discuss   \n327 anonymously and geo-localize images worldwide. Additionally, we have developed a dynamic   \n328 learning strategy for agent collaboration social networks, electing appropriate agents to geo-localize   \n329 each image with different characteristics. This enhancement reduces the computational burden   \n330 associated with collaborative discussions among LVLM agents. Moreover, we have constructed a   \n331 geo-localization dataset called GeoGlobe and will open-source it. Overall, smileGeo demonstrates   \n332 significant improvements in geo-localization tasks, achieving superior performance with lower   \n333 computational demands compared to contemporary state-of-the-art LLM/LVLM agent frameworks.   \n334 Looking ahead, we aim to expand the capabilities of smileGeo to incorporate more powerful external   \n335 tools beyond just web searching. Additionally, we plan to explore extending its application to complex   \n336 scenarios, such as high-precision global positioning and navigation for robots, laying the cornerstone   \n337 for exploring LVLM agent collaboration to handle different complex open-world tasks efficiently. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "338 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "339 [1] B. Huang and K. M. Carley, \u201cA large-scale empirical study of geotagging behavior on twitter,\u201d in   \n340 ASONAM \u201919: International Conference on Advances in Social Networks Analysis and Mining,   \n341 Vancouver, British Columbia, Canada, 27-30 August, 2019, F. Spezzano, W. Chen, and X. Xiao,   \n342 Eds. ACM, 2019, pp. 365\u2013373. [Online]. Available: https://doi.org/10.1145/3341161.3342870   \n343 [2] J. Luo, D. Joshi, J. Yu, and A. C. Gallagher, \u201cGeotagging in multimedia and computer vision   \n344 - a survey,\u201d Multim. Tools Appl., vol. 51, no. 1, pp. 187\u2013211, 2011. [Online]. Available:   \n345 https://doi.org/10.1007/s11042-010-0623-y   \n346 [3] R. Arandjelovic, P. Gron\u00e1t, A. Torii, T. Pajdla, and J. Sivic, \u201cNetvlad: CNN architecture for   \n347 weakly supervised place recognition,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 6,   \n348 pp. 1437\u20131451, 2018. [Online]. Available: https://doi.org/10.1109/TPAMI.2017.2711011   \n349 [4] M. Zaffar, S. Garg, M. Milford, J. F. P. Kooij, D. Flynn, K. D. McDonald-Maier, and S. Ehsan,   \n350 \u201cVpr-bench: An open-source visual place recognition evaluation framework with quantifiable   \n351 viewpoint and appearance change,\u201d Int. J. Comput. Vis., vol. 129, no. 7, pp. 2136\u20132174, 2021.   \n352 [Online]. Available: https://doi.org/10.1007/s11263-021-01469-5   \n353 [5] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, and T. Pajdla, \u201c24/7 place recognition by view   \n354 synthesis,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 2, pp. 257\u2013271, 2018. [Online].   \n355 Available: https://doi.org/10.1109/TPAMI.2017.2667665   \n356 [6] Z. Chen, A. Jacobson, N. S\u00fcnderhauf, B. Upcroft, L. Liu, C. Shen, I. D. Reid, and   \n357 M. Milford, \u201cDeep learning features at scale for visual place recognition,\u201d in 2017   \n358 IEEE International Conference on Robotics and Automation, ICRA 2017, Singapore,   \n359 Singapore, May 29 - June 3, 2017. IEEE, 2017, pp. 3223\u20133230. [Online]. Available:   \n360 https://doi.org/10.1109/ICRA.2017.7989366   \n361 [7] Z. Chen, L. Liu, I. Sa, Z. Ge, and M. Chli, \u201cLearning context flexible attention model for   \n362 long-term visual place recognition,\u201d IEEE Robotics Autom. Lett., vol. 3, no. 4, pp. 4015\u20134022,   \n363 2018. [Online]. Available: https://doi.org/10.1109/LRA.2018.2859916   \n364 [8] Z. Chen, F. Maffra, I. Sa, and M. Chli, \u201cOnly look once, mining distinctive landmarks from   \n365 convnet for visual place recognition,\u201d in 2017 IEEE/RSJ International Conference on Intelligent   \n366 Robots and Systems, IROS 2017, Vancouver, BC, Canada, September 24-28, 2017. IEEE,   \n367 2017, pp. 9\u201316. [Online]. Available: https://doi.org/10.1109/IROS.2017.8202131   \n368 [9] S. Garg, N. S\u00fcnderhauf, and M. Milford, \u201cSemantic-geometric visual place recognition: a new   \n369 perspective for reconciling opposing views,\u201d Int. J. Robotics Res., vol. 41, no. 6, pp. 573\u2013598,   \n370 2022. [Online]. Available: https://doi.org/10.1177/0278364919839761   \n371 [10] S. Hausler, A. Jacobson, and M. Milford, \u201cMulti-process fusion: Visual place recognition using   \n372 multiple image processing methods,\u201d IEEE Robotics Autom. Lett., vol. 4, no. 2, pp. 1924\u20131931,   \n373 2019. [Online]. Available: https://doi.org/10.1109/LRA.2019.2898427   \n374 [11] A. Khaliq, S. Ehsan, Z. Chen, M. Milford, and K. D. McDonald-Maier, \u201cA holistic visual   \n375 place recognition approach using lightweight cnns for significant viewpoint and appearance   \n376 changes,\u201d IEEE Trans. Robotics, vol. 36, no. 2, pp. 561\u2013569, 2020. [Online]. Available:   \n377 https://doi.org/10.1109/TRO.2019.2956352   \n378 [12] M. M. ElQadi, M. Lesiv, A. G. Dyer, and A. Dorin, \u201cComputer vision-enhanced selection of   \n379 geo-tagged photos on social network sites for land cover classification,\u201d Environ. Model. Softw.,   \n380 vol. 128, p. 104696, 2020. [Online]. Available: https://doi.org/10.1016/j.envsoft.2020.104696   \n381 [13] M. Campbell and M. Wheeler, \u201cA vision based geolocation tracking system for uav\u2019s,\u201d in AIAA   \n382 Guidance, Navigation, and Control Conference and Exhibit, 2006, p. 6246.   \n383 [14] F. Deng, L. Zhang, F. Gao, H. Qiu, X. Gao, and J. Chen, \u201cLong-range binocular   \n384 vision target geolocation using handheld electronic devices in outdoor environment,\u201d   \n385 IEEE Trans. Image Process., vol. 29, pp. 5531\u20135541, 2020. [Online]. Available:   \n386 https://doi.org/10.1109/TIP.2020.2984898   \n387 [15] L. Zhang, F. Deng, J. Chen, Y. Bi, S. K. Phang, X. Chen, and B. M. Chen,   \n388 \u201cVision-based target three-dimensional geolocation using unmanned aerial vehicles,\u201d IEEE   \n389 Trans. Ind. Electron., vol. 65, no. 10, pp. 8052\u20138061, 2018. [Online]. Available:   \n390 https://doi.org/10.1109/TIE.2018.2807401   \n391 [16] X. Feng, Z.-Y. Chen, Y. Qin, Y. Lin, X. Chen, Z. Liu, and J.-R. Wen, \u201cLarge language model  \n392 based human-agent collaboration for complex task solving,\u201d arXiv preprint arXiv:2402.12914,   \n393 2024.   \n394 [17] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song et al.,   \n395 \u201cCogvlm: Visual expert for pretrained language models,\u201d arXiv preprint arXiv:2311.03079,   \n396 2023.   \n397 [18] V. Paolicelli, G. M. Berton, F. Montagna, C. Masone, and B. Caputo, \u201cAdaptive-attentive   \n398 geolocalization from few queries: A hybrid approach,\u201d Frontiers Comput. Sci., vol. 4, p.   \n399 841817, 2022. [Online]. Available: https://doi.org/10.3389/fcomp.2022.841817   \n400 [19] Y. Ge, H. Wang, F. Zhu, R. Zhao, and H. Li, \u201cSelf-supervising fine-grained region similarities   \n401 for large-scale image localization,\u201d in Computer Vision - ECCV 2020 - 16th European   \n402 Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV, ser. Lecture Notes in   \n403 Computer Science, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12349. Springer,   \n404 2020, pp. 369\u2013386. [Online]. Available: https://doi.org/10.1007/978-3-030-58548-8_22   \n405 [20] H. Jin Kim, E. Dunn, and J.-M. Frahm, \u201cLearned contextual feature reweighting for image   \n406 geo-localization,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern   \n407 Recognition, 2017, pp. 2136\u20132145.   \n408 [21] L. Liu, H. Li, and Y. Dai, \u201cStochastic attraction-repulsion embedding for large scale image   \n409 localization,\u201d in 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019,   \n410 Seoul, Korea (South), October 27 - November 2, 2019. IEEE, 2019, pp. 2570\u20132579. [Online].   \n411 Available: https://doi.org/10.1109/ICCV.2019.00266   \n412 [22] F. Warburg, S. Hauberg, M. Lopez-Antequera, P. Gargallo, Y. Kuang, and J. Civera, \u201cMapillary   \n413 street-level sequences: A dataset for lifelong place recognition,\u201d in Proceedings of the IEEE/CVF   \n414 conference on computer vision and pattern recognition, 2020, pp. 2626\u20132635.   \n415 [23] G. Peng, Y. Yue, J. Zhang, Z. Wu, X. Tang, and D. Wang, \u201cSemantic reinforced attention   \n416 learning for visual place recognition,\u201d in IEEE International Conference on Robotics and   \n417 Automation, ICRA 2021, Xi\u2019an, China, May 30 - June 5, 2021. IEEE, 2021, pp. 13 415\u201313 422.   \n418 [Online]. Available: https://doi.org/10.1109/ICRA48506.2021.9561812   \n419 [24] S. Ibrahimi, N. van Noord, T. Alpherts, and M. Worring, \u201cInside out visual   \n420 place recognition,\u201d in 32nd British Machine Vision Conference 2021, BMVC 2021,   \n421 Online, November 22-25, 2021. BMVA Press, 2021, p. 362. [Online]. Available:   \n422 https://www.bmvc2021-virtualconference.com/assets/papers/0467.pdf   \n423 [25] S. Hausler, S. Garg, M. Xu, M. Milford, and T. Fischer, \u201cPatch-netvlad: Multi-scale   \n424 fusion of locally-global descriptors for place recognition,\u201d in IEEE Conference on   \n425 Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021.   \n426 Computer Vision Foundation / IEEE, 2021, pp. 14 141\u201314 152. [Online]. Available: https:   \n427 //openaccess.thecvf.com/content/CVPR2021/html/Hausler_Patch-NetVLAD_Multi-Scale_   \n428 Fusion_of_Locally-Global_Descriptors_for_Place_Recognition_CVPR_2021_paper.html   \n429 [26] F. Radenovic, G. Tolias, and O. Chum, \u201cFine-tuning CNN image retrieval with no human   \n430 annotation,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 7, pp. 1655\u20131668, 2019.   \n431 [Online]. Available: https://doi.org/10.1109/TPAMI.2018.2846566   \n432 [27] M. Izbicki, E. E. Papalexakis, and V. J. Tsotras, \u201cExploiting the earth\u2019s spherical geometry to   \n433 geolocate images,\u201d in Machine Learning and Knowledge Discovery in Databases - European   \n434 Conference, ECML PKDD 2019, W\u00fcrzburg, Germany, September 16-20, 2019, Proceedings,   \n435 Part II, ser. Lecture Notes in Computer Science, U. Brefeld, \u00c9. Fromont, A. Hotho, A. J.   \n436 Knobbe, M. H. Maathuis, and C. Robardet, Eds., vol. 11907. Springer, 2019, pp. 3\u201319.   \n437 [Online]. Available: https://doi.org/10.1007/978-3-030-46147-8_1   \n438 [28] G. Kordopatis-Zilos, P. Galopoulos, S. Papadopoulos, and I. Kompatsiaris, \u201cLeveraging   \n439 efficientnet and contrastive learning for accurate global-scale location estimation,\u201d in ICMR   \n440 \u201921: International Conference on Multimedia Retrieval, Taipei, Taiwan, August 21-24, 2021,   \n441 W. Cheng, M. S. Kankanhalli, M. Wang, W. Chu, J. Liu, and M. Worring, Eds. ACM, 2021,   \n442 pp. 155\u2013163. [Online]. Available: https://doi.org/10.1145/3460426.3463644   \n443 [29] E. M\u00fcller-Budack, K. Pustu-Iren, and R. Ewerth, \u201cGeolocation estimation of photos   \n444 using a hierarchical model and scene classification,\u201d in Computer Vision - ECCV 2018 -   \n445 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part   \n446 XII, ser. Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu,   \n447 and Y. Weiss, Eds., vol. 11216. Springer, 2018, pp. 575\u2013592. [Online]. Available:   \n448 https://doi.org/10.1007/978-3-030-01258-8_35   \n449 [30] P. H. Seo, T. Weyand, J. Sim, and B. Han, \u201cCplanet: Enhancing image geolocalization   \n450 by combinatorial partitioning of maps,\u201d in Computer Vision - ECCV 2018 - 15th   \n451 European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part   \n452 X, ser. Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu,   \n453 and Y. Weiss, Eds., vol. 11214. Springer, 2018, pp. 544\u2013560. [Online]. Available:   \n454 https://doi.org/10.1007/978-3-030-01249-6_33   \n455 [31] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,   \n456 K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,   \n457 P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, \u201cTraining language models to follow   \n458 instructions with human feedback,\u201d in Advances in Neural Information Processing Systems   \n459 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New   \n460 Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal,   \n461 D. Belgrave, K. Cho, and A. Oh, Eds., 2022. [Online]. Available: http://papers.nips.cc/paper_   \n462 files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html   \n463 [32] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T.   \n464 Lee, Y. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang, \u201cSparks of   \n465 artificial general intelligence: Early experiments with GPT-4,\u201d CoRR, vol. abs/2303.12712,   \n466 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.12712   \n467 [33] R. Schaeffer, B. Miranda, and S. Koyejo, \u201cAre emergent abilities of large language models   \n468 a mirage?\u201d in Advances in Neural Information Processing Systems 36: Annual Conference   \n469 on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,   \n470 December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and   \n471 S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/   \n472 adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html   \n473 [34] D. Jiang, X. Ren, and B. Y. Lin, \u201cLlm-blender: Ensembling large language models   \n474 with pairwise ranking and generative fusion,\u201d in Proceedings of the 61st Annual Meeting   \n475 of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023,   \n476 Toronto, Canada, July 9-14, 2023, A. Rogers, J. L. Boyd-Graber, and N. Okazaki, Eds.   \n477 Association for Computational Linguistics, 2023, pp. 14 165\u201314 178. [Online]. Available:   \n478 https://doi.org/10.18653/v1/2023.acl-long.792   \n479 [35] C. Zheng, Z. Liu, E. Xie, Z. Li, and Y. Li, \u201cProgressive-hint prompting improves   \n480 reasoning in large language models,\u201d CoRR, vol. abs/2304.09797, 2023. [Online]. Available:   \n481 https://doi.org/10.48550/arXiv.2304.09797   \n482 [36] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, \u201cReflexion: language agents   \n483 with verbal reinforcement learning,\u201d in Advances in Neural Information Processing Systems   \n484 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New   \n485 Orleans, LA, USA, December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko,   \n486 M. Hardt, and S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/   \n487 paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html   \n488 [37] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, \u201cImproving factuality and   \n489 reasoning in language models through multiagent debate,\u201d CoRR, vol. abs/2305.14325, 2023.   \n490 [Online]. Available: https://doi.org/10.48550/arXiv.2305.14325   \n491 [38] Z. Liu, Y. Zhang, P. Li, Y. Liu, and D. Yang, \u201cDynamic llm-agent network: An llm-agent   \n492 collaboration framework with agent team optimization,\u201d CoRR, vol. abs/2310.02170, 2023.   \n493 [Online]. Available: https://doi.org/10.48550/arXiv.2310.02170   \n494 [39] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W. Yih,   \n495 \u201cREPLUG: retrieval-augmented black-box language models,\u201d CoRR, vol. abs/2301.12652, 2023.   \n496 [Online]. Available: https://doi.org/10.48550/arXiv.2301.12652   \n497 [40] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, \u201cReact: Synergizing   \n449989 reasoning and acting in language models,\u201d in The Eleventh International Conference on   \nLearning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,   \n500 2023. [Online]. Available: https://openreview.net/pdf?id $\\r=$ WE_vluYUL-X   \n501 [41] G. Izacard, P. S. H. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu,   \n502 A. Joulin, S. Riedel, and E. Grave, \u201cAtlas: Few-shot learning with retrieval augmented   \n503 language models,\u201d J. Mach. Learn. Res., vol. 24, pp. 251:1\u2013251:43, 2023. [Online]. Available:   \n504 http://jmlr.org/papers/v24/23-0037.html   \n505 [42] T. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer,   \n506 N. Cancedda, and T. Scialom, \u201cToolformer: Language models can teach themselves to   \n507 use tools,\u201d in Advances in Neural Information Processing Systems 36: Annual Conference   \n508 on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,   \n509 December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and   \n510 S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/   \n511 d842425e4bf79ba039352da0f658a906-Abstract-Conference.html   \n512 [43] P. Lu, B. Peng, H. Cheng, M. Galley, K. Chang, Y. N. Wu, S. Zhu, and J. Gao,   \n513 \u201cChameleon: Plug-and-play compositional reasoning with large language models,\u201d in   \n514 Advances in Neural Information Processing Systems 36: Annual Conference on Neural   \n515 Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December   \n516 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and   \n517 S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/   \n518 871ed095b734818cfba48db6aeb25a62-Abstract-Conference.html   \n519 [44] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig,   \n520 \u201cPAL: program-aided language models,\u201d in International Conference on Machine Learning,   \n521 ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, ser. Proceedings of Machine   \n522 Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and   \n523 J. Scarlett, Eds., vol. 202. PMLR, 2023, pp. 10 764\u201310 799. [Online]. Available:   \n524 https://proceedings.mlr.press/v202/gao23f.html   \n525 [45] X. Wang, S. Li, and H. Ji, \u201cCode4struct: Code generation for few-shot structured   \n526 prediction from natural language,\u201d CoRR, vol. abs/2210.12810, 2022. [Online]. Available:   \n527 https://doi.org/10.48550/arXiv.2210.12810   \n528 [46] G. M. Berton, C. Masone, and B. Caputo, \u201cRethinking visual geo-localization for large-scale   \n529 applications,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR   \n530 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 2022, pp. 4868\u20134878. [Online].   \n531 Available: https://doi.org/10.1109/CVPR52688.2022.00483   \n532 [47] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le,   \n533 and D. Zhou, \u201cChain-of-thought prompting elicits reasoning in large language models,\u201d in   \n534 Advances in Neural Information Processing Systems 35: Annual Conference on Neural   \n535 Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November   \n536 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and   \n537 A. Oh, Eds., 2022. [Online]. Available: http://papers.nips.cc/paper_files/paper/2022/hash/   \n538 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "539 A Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "540 We summarize all notations in this paper and list them in Table 4. ", "page_idx": 13}, {"type": "table", "img_path": "HYwfZEhyK4/tmp/663ce8be29b4c65b536f7a62672745c45ea1a1aedaea428c840ba33fa0dd9909.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "541 B Dataset Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "542 The images in this dataset are copyright-free images obtained from the Internet via a crawler. We   \n543 divide the images into two main categories: man-made landmarks as well as natural attractions. Then,   \n544 we fliter out the data samples that could clearly identify the locations of the landmarks or attractions   \n545 in the images. As a result, we filter out nearly three hundred thousand data samples, and please   \n546 refer to Table 5 and Figure 4 for details. Due to the fact that a large number of natural attractions in   \n547 different geographical regions with high similarity are cleaned, the magnitude of the data related to   \n548 natural attractions in this dataset is smaller than that of man-made attractions.   \n550 For an open-world geo-localization task, the relationship between the training and test samples in   \n551 the experiment could greatly affect the results. We label the training samples as $\\mathcal{Z}_{\\mathrm{train}}$ , and the test   \n552 sample set as $\\mathcal{Z}_{\\mathrm{test}}$ , and use two metrics, coverage as well as consistency, to portray this relationship: ", "page_idx": 13}, {"type": "table", "img_path": "HYwfZEhyK4/tmp/2cce46dc97d3ef4203e67a4ae31dd86e506a52b2e1a7a9c35a91fdfc28a96eb8.jpg", "table_caption": ["Table 5: Statistics of the dataset GeoGlobe. "], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "HYwfZEhyK4/tmp/18619c12764fbfcc3ad86b405ca21737ed27d60b34550cc3b789da13bca16ff7.jpg", "img_caption": ["Figure 4: The data distribution around the world. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{c o v e r a g e=\\frac{\\mathcal{Z}_{\\mathrm{train}}\\cap\\mathcal{Z}_{\\mathrm{test}}}{\\mathcal{Z}_{\\mathrm{train}}}\\times100\\%}\\\\ {c o n s i s t e n c y=\\frac{\\mathcal{Z}_{\\mathrm{train}}\\cap\\mathcal{Z}_{\\mathrm{test}}}{\\mathcal{Z}_{\\mathrm{test}}}\\times100\\%}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "553 As for the samples in this paper, coverage $\\approx4.6564\\%$ , and consistency $\\approx33.2957\\%$ . ", "page_idx": 13}, {"type": "text", "text": "554 C Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "555 In all experiments, we employ a variety of LVLMs, encompassing both open-source and closed-source   \n556 models, to be agents in the proposed framework. Unless specified otherwise, zero-shot prompting is   \n557 applied. Each open-source LVLM is deployed on a dedicated A800 (80G) GPU server with 200GB   \n558 memory. As for each closed-source LVLM, we cost amounting to billions of tokens by calling APIs   \n559 as specified by the official website. To avoid the context length issue that occurs in some LVLMs, we   \n560 truncate the context before submitting it to the agent for questions based on the maximum number of ", "page_idx": 13}, {"type": "text", "text": "Input: A set of pre-trained LLMs $\\mathcal{V}=\\{v_{1},v_{2},\\cdots\\}$ , the input image $\\mathbf{\\deltaX}$ , and the ground truth $\\hat{Y}$ ( has);   \nOutput: The geospatial location $\\mathbf{Y}$ . Initialization Stage: 1: Initialize (Load) the parameter of the agent selection model: $\\Theta$ 2: Calculate: $A\\leftarrow f(X,\\mathcal{V}|\\Theta)$ 3: Initialize the agent collaboration social network: $\\mathcal{G}$ 4: Calculate: $L s i\\gets f(X,\\mathcal{G}|\\Theta)$ Stage $^{\\,I}$ :   \n5: Elect answer agents: $\\mathcal{V}^{1}=\\{v_{a}^{1},v_{b}^{1},\\cdots\\}\\leftarrow L s t$ , where $|\\mathcal{V}^{1}|=K$ 6: for each answer agent $v^{1}$ do 7: Obtain the location: $Y_{v^{1}}^{1}\\gets\\mathrm{Ask}_{v^{1}}(X)$ 8: Get the confidence percentage: $C_{v^{1}}^{1}\\gets\\mathrm{Ask}_{v^{1}}(X,Y_{v^{1}}^{1})$ 9: Store the further explanation: $T_{v^{1}}^{1}\\gets\\mathrm{Ask}_{v^{1}}(X,Y_{v^{1}}^{1})$   \n10: end for Stage 2:   \n11: for each selected answer agent $v^{1}\\,\\mathbf{do}$   \n12: Select the review agents: $\\mathcal{V}^{2}=\\{v_{a}^{2},v_{b}^{2},\\cdot\\cdot\\cdot\\}\\leftarrow\\mathrm{RandomWalk}_{v^{1}}(\\mathcal{G})$ , where $|\\gamma^{2}|=R$   \n13: for each review agent $v^{2}$ do   \n14: Obtain the comment $T_{v^{2}}^{2}\\gets\\mathrm{Review}_{v^{2}}(X,Y_{v^{1}}^{1},C_{v^{1}}^{1})$   \n15: Get the confidence percentage: $C_{v^{2}}^{2}\\gets\\mathrm{Ask}_{v^{2}}(X,T_{v^{2}}^{2})$   \n16: end for   \n17: end for Stage 3:   \n18: for each selected answer agent $v^{1}$ do   \n19: Summary the final answer: $\\pmb{Y}_{v^{1}}^{3}\\leftarrow\\operatorname{Summary}_{v^{1}}(\\pmb{Y}_{v^{1}}^{1},\\pmb{C}_{v^{1}}^{1},\\pmb{T}_{v_{1}^{2}}^{2},\\pmb{C}_{v_{1}^{2}}^{2},\\pmb{T}_{v_{2}^{2}}^{2},\\pmb{,}\\pmb{C}_{v_{2}^{2}}^{2},\\pmb{,}\\cdot\\cdot\\cdot)$   \n20: Get the final confidence percentag $\\mathbf{e}\\colon C_{v^{1}}^{3}\\gets\\mathrm{Ask}_{v^{1}}(Y_{v^{1}}^{1},C_{v^{1}}^{1},T_{v_{1}^{2}}^{2},C_{v_{1}^{2}}^{2},T_{v_{2}^{2}}^{2},,C_{v_{2}^{2}}^{2},\\cdot\\cdot\\cdot)$   \n2212::  eGnedn efroarte the final answer: $\\pmb{Y}\\leftarrow\\mathrm{Discussion}_{Z}(\\pmb{Y}_{v_{1}^{1}}^{3},\\pmb{C}_{v_{2}^{1}}^{3},\\pmb{Y}_{v_{2}^{1}}^{3},\\pmb{C}_{v_{2}^{1}}^{3},\\cdot\\cdot\\cdot)$ The dynamic learning strategy module:   \n23: Initialize $\\pmb{L}s t^{(0)},\\mathcal{G}^{(0)}$   \n24: for round $l$ in total $L$ rounds do   \n25: for each selected answer agent $v^{1}$ do   \n26: Obtain coordinates: $\\mathit{C o o r s}\\gets\\mathrm{GeoEmb}(\\mathbf{Y}_{\\mathit{v}^{1}}^{3}),\\mathit{C o o r s}_{\\mathrm{Truth}}\\gets\\mathrm{GeoEmb}(\\mathbf{Y}_{\\mathrm{Truth}})$   \n27: if $\\mathrm{Dis}(C o o r s,C o o r s_{\\mathrm{Truth}})\\leq t h$ then   \n28: $\\mathbf{A}^{(l)}\\gets\\operatorname{Enhance}(e|e|$ contains $v^{1},e\\in\\mathcal{E})$ )   \n29: Update $L s t^{(l)}[v^{1}]=1$   \n30: else   \n31: $\\mathbf{\\ddot{A}}^{(l)}\\gets\\operatorname{Weaken}(e|e\\operatorname{contains}\\,v^{1},e\\in\\mathcal{E})$   \n32: Update $L s t^{(l)}[v^{1}\\dot{]}=0$   \n33: end if   \n34: end for   \n35: end for   \n36: $\\hat{A}\\approx A^{(L)}$ ,L\u02c6st \u2248Lst(L)   \n37: Update: $\\Theta\\gets L o s s(\\hat{\\mathbf{Y}},\\mathbf{Y},\\hat{\\mathbf{A}},A,\\hat{L s t},L s t)$ ", "page_idx": 14}, {"type": "text", "text": "561 tokens that each agent supports. Besides, noting that images are token consuming, we only keep the   \n562 freshest response for agent discussions.   \n563 The detailed algorithm of smileGeo is illustrated in Algorithm 1. In the initialization stage, we   \n564 initialize or load the parameters of the agent social network learning model, as delineated in line 1.   \n565 Next, we treat each LVLM agent as a node, establishing the LVLM agent collaboration social network   \n566 and computing the adjacency relationships among LVLM agents as well as the probability that each   \n567 agent is suited for responding to image $\\mathbf{\\deltaX}$ , as shown in line 2. Then, line 3 initializes the agent   \n568 collaboration social network and line 4 computes the agent election probability. In Stage 1, line 5   \n569 involves electing appropriate answer agents based on the calculated probabilities. Subsequently, lines   \n570 6-10 detail the process through which each chosen answer agent formulates their response. Stage 2   \n571 begins by employing the random walk algorithm to assign review agents to each answer agent, as   \n572 depicted in lines 11-12. Lines 13-16 then describe how these review agents generate feedback based   \n573 on the answers provided. In Stage 3, each answer agent consolidates feedback from their assigned   \n574 review agents to finalize their response, as illustrated in lines 18-21. Line 22 concludes the final   \n575 answer with up to $Z$ rounds (we set $Z=10$ in experiments) of intra-discussion among all answer   \n576 agents only. The dynamic learning strategy module involves $L$ -round (we set $L=20$ in experiments)   \n577 comparing the generated answers against the ground truth and updating the connections between the   \n578 answer and review agents accordingly, as shown in lines 23-36. In line 37, the process concludes   \n579 with the updating of the learning parameters of the dynamic agent social network learning model.   \n580 Here, for the agent social network learning model, we first deflate each image to be recognized to   \n581 512x512 pixels and then use the pre-trained VAE model11 to compress the image again (compression   \n582 ratio 1:8) and extract its representations. We define the embedding dimension of the nodes to be 1024   \n583 and the hidden layer dimension of the network layer to be 1024. we use Adam as an optimizer for   \n584 gradient descent with a learning rate of $1e^{-5}$ . For each stage of the LVLM agent discussion, we use a   \n585 uniform template to ask questions to different LVLM agents and ask them to make a response in the   \n586 specified format. In addition, the performance of our proposed framework is the average of the last   \n587 100 epochs in a total training of 2500 epochs. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "HYwfZEhyK4/tmp/8af3b2f8e6a2a470b7505638b69db8d43b0a19a085504ec188e0a6be4e25bacd.jpg", "img_caption": ["Figure 5: A case study on the geo-localization process via a given image. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "HYwfZEhyK4/tmp/1b5316a158dc4edd56faeebfab25fc48986b18fcf070a60c00f1d414ae3946d3.jpg", "img_caption": ["Figure 6: A case study illustrating the reasoning capabilities of smileGeo. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "588 D Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "589 D.1 Case Study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "590 Case 1: In Figure 5, we illustrate the application of smileGeo in a visual geo-localization task.   \n591 For this demonstration, we randomly select an image from the test dataset and employ five distinct   \n592 LVLMs: LLaVA, GPT-4V, Claude-3, Gemini, and Qwen. The agent selection model selects two   \n593 answer agents, as depicted in the top part of the figure. Subsequently, stages 1 through 3 detail the   \n594 process of generating the accurate geo-location. Initially, only one answer agent provided the correct   \n595 response. However, after several rounds of discussion, the agent that initially responded incorrectly   \n596 revised the confidence level of its answer. During the final internal discussion, this agent aligned its   \n597 response with the correct answer. This outcome validates the efficacy of our proposed framework,   \n598 demonstrating its ability to integrate the knowledge and reasoning capabilities of different agents to   \n599 enhance the overall performance of the proposed LVLM agent framework.   \n600 Case 2: This case study illustrates the need to pinpoint the geographical location of a complete   \n601 image based on only a portion of it, as demonstrated in 6(a). As illustrated in Figure 6(b), all agents   \n602 recognized the Statue of Liberty in Figure 6(a), and some identified the presence of part of the Eiffel   \n603 Tower at the edge of the picture. For instance, GPT-4V concluded that the buildings in these two   \n604 locations appeared in the same image. However, as is known through the knowledge of other agents   \n605 (Gemini), a scaled-down version of the Statue of Liberty has been erected on Swan Island, an artificial   \n606 island in the Seine River in France. By marking both the Eiffel Tower and the island on the Open   \n607 Street Map (OSM) manually, as shown in Figure 6(c), it is evident that they are merely 1.3 kilometers   \n608 apart in a straight line. By utilizing the proposed framework, agents discuss and summarize the   \n609 location depicted in the picture to be Paris, France, as shown in Figure 6(d). Thus, without human   \n610 intervention, this framework demonstrates the effectiveness of doing geo-localization tasks. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "611 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "612 1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Justification: Our work proposes a swarm intelligence geo-localization framework, smileGeo, which contains the process of the review mechanism in agent discussions along with a dynamic learning strategy of agent collaboration social network, to achieve open-world geo-localization tasks. In addition, we construct a novel geo-localization dataset, GeoGlobe for evaluation and it will be public. All of the contributions we claimed in both abstract and introduction. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: At present, the LVLM agent framework we proposed can only search the Internet autonomously. Our agent still has shortcomings in the use of other multiple tools. We stated in our future outlook that our follow-up work will solve this problem. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibilit ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We provided the source code and will release the related dataset once the paper is accepted, as the dataset is relatively large (about 32 GB) and cannot be uploaded as an attachment. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 18}, {"type": "text", "text": "718 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n719 authors are welcome to describe the particular way they provide for reproducibility.   \n720 In the case of closed-source models, it may be that access to the model is limited in   \n721 some way (e.g., to registered users), but it should be possible for other researchers   \n722 to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "23 5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instruc5 tions to faithfully reproduce the main experimental results, as described in supplemental 6 material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We provide the anonymous code link: https://anonymous.4open.science/ r/ViusalGeoLocalization-F8F5/. In this link, we also provide a small-scale dataset we collected for people to reproduce the results.   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "751 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "52 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n53 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n54 results?   \n55 Answer: [Yes]   \n56 Justification: We explain all the settings in both the main paper (Experiments) and the   \n57 appendix (Implementation Details).   \n759 \u2022 The answer NA means that the paper does not include experiments.   \n760 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n761 that is necessary to appreciate the results and make sense of them.   \n762 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n763 material. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "764 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "790 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Justification: We announce the compute resources in the appendix (Implementation Details). Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "805 9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "806 Question: Does the research conducted in the paper conform, in every respect, with the   \n807 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "809 Justification: The codes used in our paper are all open source, and the data used in the paper   \n810 come from copyright-free images on the Internet. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "817 10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "818 Question: Does the paper discuss both potential positive societal impacts and negative   \n819 societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "820 Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "821 Justification: We have an outlook on our research in the section Conclusion, which can be   \n822 widely used in robot positioning and navigation in the future.   \n823 Guidelines:   \n824 \u2022 The answer NA means that there is no societal impact of the work performed.   \n825 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n826 impact or why the paper does not address societal impact.   \n827 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n828 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n829 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n830 groups), privacy considerations, and security considerations.   \n831 \u2022 The conference expects that many papers will be foundational research and not tied   \n832 to particular applications, let alone deployments. However, if there is a direct path to   \n833 any negative applications, the authors should point it out. For example, it is legitimate   \n834 to point out that an improvement in the quality of generative models could be used to   \n835 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n836 that a generic algorithm for optimizing neural networks could enable people to train   \n837 models that generate Deepfakes faster.   \n838 \u2022 The authors should consider possible harms that could arise when the technology is   \n839 being used as intended and functioning correctly, harms that could arise when the   \n840 technology is being used as intended but gives incorrect results, and harms following   \n841 from (intentional or unintentional) misuse of the technology.   \n842 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n843 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n844 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n845 feedback over time, improving the efficiency and accessibility of ML).   \n846 11. Safeguards   \n847 Question: Does the paper describe safeguards that have been put in place for responsible   \n848 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n849 image generators, or scraped datasets)?   \n850 Answer: [Yes]   \n851 Justification: The data sets we collect have been manually reviewed twice, and all data   \n852 containing various types of sensitive information or copyright risks have been filtered out.   \n853 Guidelines:   \n854 \u2022 The answer NA means that the paper poses no such risks.   \n855 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n856 necessary safeguards to allow for controlled use of the model, for example by requiring   \n857 that users adhere to usage guidelines or restrictions to access the model or implementing   \n858 safety filters.   \n859 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n860 should describe how they avoided releasing unsafe images.   \n861 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n862 not require this, but we encourage authors to take this into account and make a best   \n863 faith effort.   \n864 12. Licenses for existing assets   \n865 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n866 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n867 properly respected?   \n868 Answer: [Yes]   \n869 Justification: We list and acknowledge all other open-source codes we used in the file   \n870 \u2019README.md\u2019 and we follow the license for existing assets.   \n871 Guidelines:   \n872 \u2022 The answer NA means that the paper does not use existing assets.   \n873 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n874 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n875 URL.   \n876 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n877 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n878 service of that source should be provided.   \n879 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n880 package should be provided. For popular datasets, paperswithcode.com/datasets   \n881 has curated licenses for some datasets. Their licensing guide can help determine the   \n882 license of a dataset.   \n883 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n884 the derived asset (if it has changed) should be provided.   \n885 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n886 the asset\u2019s creators.   \n887 13. New Assets   \n888 Question: Are new assets introduced in the paper well documented and is the documentation   \n889 provided alongside the assets?   \n890 Answer: [Yes]   \n891 Justification: In this paper, we provide the algorithm of the code and introduce the dataset in   \n892 detail (in the appendix).   \n893 Guidelines:   \n894 \u2022 The answer NA means that the paper does not release new assets.   \n895 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n896 submissions via structured templates. This includes details about training, license,   \n897 limitations, etc.   \n898 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n899 asset is used.   \n900 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n901 create an anonymized URL or include an anonymized zip file.   \n902 14. Crowdsourcing and Research with Human Subjects   \n903 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n904 include the full text of instructions given to participants and screenshots, if applicable, as   \n905 well as details about compensation (if any)?   \n906 Answer: [NA]   \n907 Justification: This paper aims to address visual geo-localization tasks and does not contain   \n908 any experiments with human subjects.   \n909 Guidelines:   \n910 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n911 human subjects.   \n912 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n913 tion of the paper involves human subjects, then as much detail as possible should be   \n914 included in the main paper.   \n915 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n916 or other labor should be paid at least the minimum wage in the country of the data   \n917 collector.   \n918 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n919 Subjects   \n920 Question: Does the paper describe potential risks incurred by study participants, whether   \n921 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n922 approvals (or an equivalent approval/review based on the requirements of your country or   \n923 institution) were obtained?   \n924 Answer: [NA]   \n925 Justification: This paper does not contain any experiments with human subjects. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]