[{"type": "text", "text": "How Do Large Language Models Acquire Factual Knowledge During Pretraining? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hoyeon Chang1 Jinho Park1 Seonghyeon Ye1 Sohee Yang2 Youngkyung Seo3 ", "page_idx": 0}, {"type": "text", "text": "Du-Seong Chang3 Minjoon Seo1 ", "page_idx": 0}, {"type": "text", "text": "1KAIST {retapurayo, binlepain178, seonghyeon.ye, minjoon}@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "2UCL 3KT sohee.yang.22@ucl.ac.uk {yg.seo, dschang}@kt.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model\u2019s capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models\u2019 robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the beneftis of deduplicating the pretraining corpus.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data [14, 36, 40]. Unfortunately, little is understood about the mechanisms of how LLMs acquire factual knowledge during pretraining. In this work, we make an initial attempt to understand the dynamics of factual knowledge acquisition in LLM pretraining. We study three important yet unanswered research questions: ", "page_idx": 0}, {"type": "text", "text": "RQ1. How is factual knowledge acquired during LLM pretraining and how are LLMs affected by the training data at each training step? ", "page_idx": 0}, {"type": "text", "text": "RQ2. How is the effectivity of factual knowledge acquisition affected by training conditions? ", "page_idx": 0}, {"type": "text", "text": "RQ3. How is the acquired factual knowledge forgotten, and how is the trend affected by training conditions? ", "page_idx": 1}, {"type": "text", "text": "To answer the research questions, we analyze how LLMs acquire and retain factual knowledge in terms of memorization and generalization by varying the following training conditions: knowledge injection scenarios, pretraining stages, model sizes, and training batch sizes. Specifically, we take the intermediate pretraining checkpoints of different sizes of an LLM at different pretraining stages, inject the target knowledge that the models have not previously encountered, and monitor their step-wise progress of acquiring factual knowledge under various conditions. ", "page_idx": 1}, {"type": "text", "text": "Our experiments reveal several important insights and hypotheses about the fine-grained dynamics of factual knowledge acquisition in LLM pretraining. First, we show that factual knowledge acquisition occurs by accumulating the small increase of probability induced by updating the model with a minibatch containing the factual knowledge. Second, compared to the checkpoints at earlier stages, the checkpoint at the later stage shows no significant difference in effectivity, i.e., no significant improvement in the ability to acquire memorization and generalization immediately. On the other hand, the effectivity is greater in the 7B model than in the 1B model, suggesting that the beneftis from scaling model size and pretraining tokens are qualitatively different in terms of factual knowledge acquisition. Third, we find a power-law relationship between training steps (or tokens) and forgetting of acquired factual knowledge in both memorization and generalization. Further examination of the rate of forgetting factual knowledge in LLM pretraining reveals that deduplicating the training data and training the models with a greater batch size enhances the acquisition of factual knowledge, by making them more robust against forgetting. Based on our understanding of the dynamics of factual knowledge acquisition, we demonstrate that the recently observed behaviors, including the improvement of LLMs\u2019 performance with more training data, the failure to acquire long-tail knowledge [26, 34], and the importance of dataset deduplication [29, 52] can be explained. ", "page_idx": 1}, {"type": "text", "text": "Overall, to the best of our knowledge, this work is one of the initial attempts to examine the training dynamics involved in acquiring factual knowledge during the pretraining of LLMs. By enhancing our understanding of the factual knowledge acquisition dynamics, we expect that academia can gain a holistic understanding and make better use of LLMs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recently, there has been a surge in interest in LLMs [9, 13, 21, 23, 49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36, 40]. [3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data. [4] demonstrated that knowledge should be presented in a diverse format during pretraining to be reliably extracted. However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26, 34]. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5]. These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete. To examine the detailed training dynamics of knowledge acquisition during pretraining, we conduct a fine-grained analysis of factual knowledge acquisition on each piece of factual knowledge. ", "page_idx": 1}, {"type": "text", "text": "Memorization and forgetting are closely related to knowledge acquisition in neural networks [6]. LLMs memorize a significant amount of training data [10, 29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7, 11]. In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions. ", "page_idx": 1}, {"type": "text", "text": "Several studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12, 18, 22, 32, 33, 45, 51]. [44] and [46] focused on the dynamics of memorization in language model pretraining. Recently, [53] explored the relationship between the data size and grokking [37]. Compared to these, we perform a more detailed analysis of the dynamics of factual knowledge acquisition during LLM pretraining, by evaluating the log probability of individual pieces of factual knowledge at each training step. ", "page_idx": 1}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/a9fc89c1b8503f625bb6b447089b1a8ebb4e071824441050f2f6344a3b1a5dc3.jpg", "table_caption": ["Table 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "3 Experimental Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "FICTIONAL KNOWLEDGE dataset Our goal is to analyze the LLMs\u2019 behavior when acquiring factual knowledge during pretraining. Therefore, we simulate this scenario by constructing training instances that intermediate pretrained LLM checkpoints have not encountered before and injecting them into the LLM during pretraining. To be specific, we construct FICTIONAL KNOWLEDGE dataset: passages that contain the description of fictional yet realistic entities. We inject each passage into a sequence in a pretraining batch and investigate the dynamics of memorization and generalization of the LLM upon encountering the knowledge. We call these passages injected knowledge. ", "page_idx": 2}, {"type": "text", "text": "Next, to investigate the LLMs\u2019 ability to generalize acquired factual knowledge in different depths, we split the concept of acquisition into three depths: (1) memorization: memorizing the exact sequence used for training (2) semantic generalization: generalizing the factual knowledge to a paraphrased format in a single-sentence level (3) compositional generalization: composing the factual knowledge presented in multiple sentences in the injected knowledge. ", "page_idx": 2}, {"type": "text", "text": "Following this intuition, we carefully design five probes for each of the three different acquisition depths for each injected knowledge, resulting in 1,800 probes in total. Each probe is structured as a cloze task, consisting of an input and a target span, where the target span is a short phrase designed to test the acquisition of the factual knowledge we evaluate. An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4 [2] using the definitions from the ECBD dataset [35] as a template, and filtering out invalid cases. The details for the data construction and more examples of the FICTIONAL KNOWLEDGE dataset can be found in $\\S B$ . ", "page_idx": 2}, {"type": "text", "text": "Evaluation metrics To conduct a detailed analysis of the LLMs\u2019 acquisition of factual knowledge during pretraining, we evaluate the model\u2019s state by examining log probabilities to obtain fine-grained information [41]. To quantitatively measure the trend of factual knowledge acquisition, we should first define the timestep where the local effect of updating the model using the injected knowledge completely pays off. A step-wise evaluation of the change in a model\u2019s log probability on factual knowledge during pretraining reveals that this improvement occurs through several steps (Figure 1), since LLMs deploy optimizers with momentum. Hence, we define the timestep where the log probability reaches a maximum value in a short interval after the model is trained on the injected knowledge, which we refer to as the local acquisition maxima. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 Given a language model, let $\\theta_{t}$ represent the model\u2019s parameters before the $t$ -th update. Given injected knowledge $k$ (used as a training instance) and the corresponding probe $q$ (used as an evaluation instance), let $\\ell(q;\\theta)$ denote the log probability of the target span of $q_{\\cdot}$ , provided by the model. Let a nonempty set $T_{k}=\\{t_{1},t_{2},\\ldots,t_{n}\\}$ denote the steps where the model is updated with the minibatch containing the injected knowledge $k$ , where $0\\leq t_{1}<t_{2}<.\\ldots<t_{n}$ . Finally, let $t_{w}$ denote the window size. Then, the local acquisition maxima $(t_{L A M}(q,i))$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nt_{L A M}(q,i)=\\operatorname*{argmax}_{t_{i}<t\\leq t_{i}+t_{w}}\\ell(q;\\theta_{t})\\quad w h e r e\\ t_{i}\\in T_{k}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/20394c4754b11fb9ab2d20b67d5e94d6f5890cfe6af0807c0b86c5b0617a75e7.jpg", "img_caption": ["Figure 1: An illustration of the change of log probability of the target span of a probe $(\\Delta\\ell(q))$ measuring the memorization of factual knowledge on a short-term scale. At step 0 (marked as a dotted line), the model is trained with the injected knowledge which contains the factual knowledge evaluated by the probe $q$ . The local acquisition maxima (marked as a red line) is the timestep where the log probability reaches its maximum within the window (shaded area), defined by $t_{w}$ . The measurement of effectivity and retainability at $t=30$ is visualized, where retainability is obtained by measuring the fraction of the purple line compared to the gray line. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge $k$ and the window size $t_{w}$ , but we write $t_{\\mathrm{LAM}}(q,i)$ for brevity. We use the window size $t_{w}=50$ .23 ", "page_idx": 3}, {"type": "text", "text": "Next, we define a metric to quantify the immediate improvement in the model\u2019s log probability of factual knowledge after it is presented with the knowledge for the $i^{\\th}$ -th time. This improvement is measured by the model\u2019s log probability on the target spans of the corresponding probes. This metric, effectivity, will be used to answer the second research question. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 Given a language model parameterized by $\\theta$ trained with an injected knowledge $k$ at $t=t_{i}$ where $t_{i}\\in T_{k}$ , and a corresponding probe $q_{\\mathrm{r}}$ , the effectivity $(\\mathcal{E}(q,i))$ is defined as the absolute increase of the model\u2019s log probability on the target span of q between $t=t_{i}$ and $t=t_{L A M}(q,i),$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(q,i)=\\ell(q;\\theta_{t_{L A M}(q,i)})-\\ell(q;\\theta_{t_{i}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, to investigate the forgetting phenomenon of acquired factual knowledge (RQ3), we define a metric that quantifies the fraction of improvement in log probability retained by the model after $t$ steps, relative to the local acquisition maxima of the last knowledge update. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 Consider a language model parameterized by $\\theta$ and trained with injected knowledge $k$ for $N$ iterations, occuring at timesteps $t_{i}\\in T_{k}$ where $|T_{k}|=N$ . Let $t_{p r e}$ denote the last timestep before the model is first trained with $k$ , i.e., $t_{p r e}\\;=\\;\\operatorname*{min}(T_{k})$ . Given a corresponding probe $q$ retainability $(\\mathcal{R}(q,t))$ is defined for $t\\geq0$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathscr{R}(q,t)=\\frac{\\ell(q;\\theta_{t_{L A M}(q,N)+t})-\\ell(q;\\theta_{t_{p r e}})}{\\ell(q;\\theta_{t_{L A M}(q,N)})-\\ell(q;\\theta_{t_{p r e}})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that $\\mathcal{R}(p,0)=1$ which represents that the factual knowledge is $100\\%$ retained at the local acquisition maxima of the last knowledge update. Additionally, $\\mathcal{R}(p,t)=0$ occurs when the log probability of the probe $p$ at $t_{\\mathrm{SP}(p)}+t$ equals that at $t_{\\mathrm{pre}}$ . Thus, $\\pi(p,t)\\,=\\,0$ indicates that the improvement in the log probability of factual knowledge, induced by updating the model with minibatches containing the injected knowledge at $t_{\\mathrm{pre}}$ , is completely lost. This $\\mathbf{X}$ -intercept of $\\mathcal{R}(p,t)$ is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in $\\S\\ 4.4$ . The measurement of the defined metrics are illustrated in Figure 1. ", "page_idx": 3}, {"type": "text", "text": "For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5. This is particularly important for the measurement of retainability, as the small number of cases which showed no acquisition through training can give a very large value due to the very small denominator in Eq. 3. ", "page_idx": 3}, {"type": "text", "text": "Knowledge injection during pretraining We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i) ", "page_idx": 3}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/43179e1d17dbe8351e01d39b6d655bc406933dd871401afcf7f194e5f54aa9f8.jpg", "img_caption": ["Figure 2: Change in the average log probability of target spans of the probes plotted against training steps during the continuation of pretraining OLMo-7B mid checkpoint (trained on 500B tokens) with injecting the knowledge in the FICTIONAL KNOWLEDGE dataset. Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios. Note the immediate and distinctive increase of log probability after the model is updated with the injected knowledge, marked by dotted vertical lines. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "varying knowledge injection scenarios (duplication, paraphrase, once), (ii) varying pretraining stages (early, mid, and late, pretrained with approximately 170B, 500B, and $1.5\\mathrm{T}$ tokens, respectively), (iii) varying model sizes (1B and 7B), and (iv) varying training batch sizes (2048 and 128). To this end, we resume pretraining OLMo [21] intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 [43]), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the FICTIONAL KNOWLEDGE dataset.4 Each injected knowledge is short enough to fit into one pretraining sequence in the batch, and we fill the rest of the sequence with the original sequence in the batch. To investigate the difference in the factual knowledge acquisition dynamics when the models are presented with the knowledge, we inject factual knowledge with three different injection scenarios: duplication, paraphrase, and once. For the duplication injection scenario, we inject the same knowledge 10 times with an interval of 100 training steps. In the paraphrase injection scenario, we inject paraphrased knowledge instead of showing identical sequences, every time it is presented to the model. Lastly, in the once injection scenario, we inject the knowledge only once at the start of the training. After the injection is complete, we continue pretraining as normal. The details for the training setup can be found in $\\S D$ . ", "page_idx": 4}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Factual knowledge acquisition occurs by accumulating the observations of the fact ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Figure 2 shows the progress of factual knowledge acquisition of OLMo-7B, by averaging the model\u2019s log probability across the target spans of the probes for each injection scenario, evaluated at each training step. Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model\u2019s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge. However, the log probability decreases again, as the knowledge is not presented to the model afterward. This observation directly demonstrates the mechanism of factual knowledge acquisition: LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining. ", "page_idx": 4}, {"type": "text", "text": "Several findings can be further obtained from Figure 2. First, when the model is updated after seeing the factual knowledge, the most significant improvement in log probability is observed for memorization, followed by semantic generalization, and the least improvement is seen in compositional generalization. Next, however, the gap between memorization and semantic generalization almost disappears in the paraphrase injection scenario. Third, when the model is updated with the duplication injection scenario, the model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training $t=2000$ ) compared to the paraphrase injection scenario. ", "page_idx": 4}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/0ff5412a7deae884f85147727b6c1454b25475097eb85310e2d847753a220ef9.jpg", "img_caption": ["Figure 3: Effectivity averaged across various probes and each time of injection, measured for different injection scenarios, and acquisition depths. Note that the effectivity does not improve as the model is trained with more tokens (Left), whereas there is a clear improvement as the model size scales (Right). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "These patterns are consistent across all pretraining stages of OLMo-7B we investigate $(\\S\\mathrm{E}.1)$ . Intriguingly, the training dynamics of OLMo-1B early checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the early checkpoint of OLMo-7B (Appendix Figure 6). The distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably and that such a threshold may be higher for smaller models. ", "page_idx": 5}, {"type": "text", "text": "4.2 Effects of model scale and pretraining stage on knowledge acquisition dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we measure effectivity (Eq. 2) to quantify the improvement of the LLMs\u2019 log probability after being trained with the injected knowledge, averaged across all probes $(q)$ and encounters $(i)$ . The results are demonstrated in Figure 3. The average effectivity is the largest in the Once injection scenario since the effectivity is higher when the model encounters the injected knowledge for the first time, which is further discussed in $\\S\\mathrm{H}$ . ", "page_idx": 5}, {"type": "text", "text": "In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).5 On the other hand, surprisingly, the effectivity of fact acquisition does not improve with checkpoints trained with more tokens, as shown on the left side of Figure 3. This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11). Moreover, this tendency is not attributed to training the models with a decreased learning rate through learning rate decay, as demonstrated by an additional experiment of training three checkpoints using the same constant learning rate. The results with the constant learning rate show that effectivity does not significantly improve in the checkpoints of later stages of pretraining where more pretraining tokens are seen $(\\S\\mathrm{F})$ . Therefore, the observation implies that the effectivity of LLMs in acquiring factual knowledge does not significantly improve throughout the progress of pretraining. ", "page_idx": 5}, {"type": "text", "text": "While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in $\\S4.3$ . Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training [50], but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model. We discuss this hypothesis further in $\\S4.4$ . ", "page_idx": 5}, {"type": "text", "text": "Comparing the duplication and paraphrase injection scenarios, the duplication injection scenario naturally shows higher effectivity for memorization. However, the higher effectivity in the duplication injection scenario for semantic generalization and compositional generalization appears to be counterintuitive, as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29, 52]. In the following sections, we will address this question by demonstrating that the models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts (\u00a74.3). ", "page_idx": 5}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/c02f1b65b2ec28f35f2fbce9b8ff4baae6ca89debe6870dfab3eff347c90e83f.jpg", "img_caption": ["Figure 4: Average retainability against training steps past the local acquisition maxima, measured with OLMo-7B mid checkpoint. The x-axes are in log scale. Left: duplication. Right: paraphrase. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/3cc1d997d5dafb6dd550118e20a3bab3eadb7653f1a3561a3cfdb5b83c625232.jpg", "table_caption": ["Table 2: Decay constant of average retainability $(\\mathcal{R}(p,t))$ measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.3 Forgetting in factual knowledge acquisition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33, 39]. Motivated by this, we investigate whether the exponential trend of forgetting persists in the context of factual knowledge acquisition in LLM pretraining. Figure 4 illustrates the trend of retainability against the training steps past the local acquisition maxima. We find that the trend of $\\mathcal{R}(p,t)$ against $l o g(t)$ fits a linear function very well $/R^{2}\\;>\\;0.80$ for memorization and semantic generalization, and $R^{2}>0.65$ for compositional generalization). This trend is persistent across all acquisition depths, and all training conditions ( $\\S\\mathrm{E}.4$ and $\\S E.5$ ). Guided by empirical observations, we model the trend of forgetting using a power-law model in further investigations. ", "page_idx": 6}, {"type": "text", "text": "How quickly is the acquired factual knowledge lost? The absolute value of the slope of the ftited lines in Figure 4 can be interpreted as the decay constant $(a)$ of retainability, formally, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{R}(p,t)\\approx-a\\cdot\\log\\left(\\frac{t_{2}}{t_{1}}\\right)\\quad\\mathrm{for}\\;0<t_{1}<t_{2}<\\tau,\\quad\\mathrm{where\\;}\\mathcal{R}(p,\\tau)=0\\mathrm{~and~}a>0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, the measured decay constant represents how fast (in terms of fraction) the model loses the improvement of log probability. Table 2 shows the decay constants of retainability measured for three OLMo-7B intermediate checkpoints, for duplication and paraphrase injection scenarios. ", "page_idx": 6}, {"type": "text", "text": "There are several observations in Table 2. First, the forgetting in compositional generalization is slower (the decay constant $a$ is smaller) than in memorization and semantic generalization. Combined with the observations in previous sections, the acquisition of compositional generalization accumulates most slowly but is more robust to forgetting. Second, the forgetting tends to be slower in the paraphrase injection scenario compared to the duplication injection scenario. This finding will be further discussed in $\\S4.4$ , regarding the importance of deduplicating training data. Finally, the decay constants are similar for the two earlier checkpoints but smaller for the late checkpoint in the duplication injection scenario. We demonstrate that this is due to the reduced learning rate from learning rate scheduling (Appendix Table 5), as the decay constants show no decrease for the later checkpoint when each checkpoint is trained with the same constant learning rate (Appendix Table 9). ", "page_idx": 6}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/c979c6c1b61155b9e8cb0b219133494dab6411dfec96a9725a67a353bdf2315d.jpg", "img_caption": ["Figure 5: Comparison of the forgetting dynamics of pretraining (Left) and training with reduced batch size (Right), measured with OLMo-7B mid checkpoint. Note that the $\\mathbf{X}$ -axis represents the number of training tokens instead of training steps, which has a shifting effect on the data plotted in Figure 4. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Pretraining with a larger batch size helps LLMs acquire more knowledge It is a common practice to pretrain LLMs with a very large batch size to leverage parallel computing [13, 21, 25, 30, 49]. However, the effects of increasing training batch size in terms of the LLMs\u2019 acquisition of factual knowledge remain underexplored. In this section, we examine whether pretraining LLMs with a larger batch size is advantageous regarding factual knowledge acquisition. Specifically, we continue training LLMs with a batch size reduced by a factor of 16 compared to the original pretraining batch size, i.e., from 2048 to 128. ", "page_idx": 7}, {"type": "text", "text": "Figure 5 compares the forgetting dynamics of OLMo-7B mid checkpoint between pretraining and training with the reduced batch size. The results have several implications for the advantage of pretraining LLMs with a larger batch size. First, comparing Figure 3 and Appendix Figure 21, LLMs trained with the smaller batch size show higher effectivity. However, the decay constant tends to be higher, comparing the numbers in Table 2 and Appendix Table 10. Furthermore, the anticipated x-intercept is significantly decreased by dozens of times, comparing Appendix Table 6 and 11. This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold, which we discuss in detail in the following section (\u00a74.4). In other words, when an LLM is trained with a smaller batch size, factual knowledge should be presented more often to the model so as not to be forgotten and the set of learnable knowledge is reduced. Second, accelerated forgetting with a smaller batch size is more pronounced for compositional generalization compared to memorization and semantic generalization. In brief, the results suggest that pretraining with a small batch size reduces the set of learnable knowledge due to accelerated forgetting, and leads to worse compositional generalization performance of learned factual knowledge. ", "page_idx": 7}, {"type": "text", "text": "4.4 Implications for LLM pretraining ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Why is popularity important for factual knowledge acquisition? The estimated $\\mathbf{X}$ -intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.6 Hence, if a given factual knowledge in the pretraining dataset is in the long-tail and the knowledge is presented to the model with an interval longer than a certain threshold, such knowledge will be impossible to be decoded as the top-k generation of the model, or learned, regardless of the duration of the pretraining.7 This implies that there is a learnability threshold, a threshold of the interval where the model fails to acquire knowledge of which its encounter interval is longer than the threshold. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this learnability threshold. In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model [41]. Moreover, LLMs will accumulate the log probability faster for more popular knowledge, and thus the acquisition of such knowledge will be reflected in the model\u2019s top- $\\cdot\\mathbf{k}$ output sequence generation in a relatively earlier pretraining stage, as demonstrated in [8]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In summary, we hypothesize that the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be \u2018revealed\u2019 in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the learnability threshold. Also, as briefly mentioned in $\\S4.2$ , we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge (more knowledge will be presented with an interval shorter than the learnability threshold) since the skewness of the distribution of factual knowledge popularity is likely to be mitigated as the data becomes larger and more diverse. ", "page_idx": 8}, {"type": "text", "text": "Why does deduplication enhance model performance? Recent pretraining corpora are thoroughly deduplicated [9, 28, 38, 43, 47, 48], as it is widely observed that data deduplication can improve model performance [1, 29, 42, 52]. Our results suggest that the smaller decay constant in the paraphrase injection scenario observed in $\\S4.3$ can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge. This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the duplication and paraphrase injection scenarios, but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization (Figure 2 and 3), presenting the model with duplicated texts with a short interval will result in the widening of the gap between memorization and generalization, which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge [4]. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion and Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we study how LLMs acquire factual knowledge during pretraining. Our findings and contributions can be summarized as follows: ", "page_idx": 8}, {"type": "text", "text": "\u2022 We propose methods, datasets, and metrics for performing a fine-grained analysis of factual knowledge acquisition dynamics during LLM pretraining.   \n\u2022 We demonstrate that factual knowledge acquisition in LLM pretraining is achieved through accumulating micro-acquisitions, each of which occurs whenever the model is updated after seeing the factual knowledge. When the model is not presented with factual knowledge, forgetting occurs and the acquisition of the knowledge is gradually diluted.   \n\u2022 However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the beneftis of scaling the model size and pretraining tokens are qualitatively different.   \n\u2022 There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge. ", "page_idx": 8}, {"type": "text", "text": "\u2022 We provide potential explanations for recently observed, yet underexplored behaviors of LLMs. First, we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining. Second, we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer. ", "page_idx": 9}, {"type": "text", "text": "Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Seongyun Lee, Suehyun Park, Hyeonbin Hwang, Geewook Kim, Juyoung Suk, Aengus Lynch, and Katja Filippova for their valuable feedback on our work. ", "page_idx": 9}, {"type": "text", "text": "This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program(KAIST). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023.   \n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Ekin Aky\u00fcrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Tracing knowledge in language models back to the training data. ArXiv, abs/2205.11482, 2022.   \n[4] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. ArXiv, abs/2309.14316, 2023. [5] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation. ArXiv, abs/2309.14402, 2023. [6] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In International Conference on Machine Learning, 2017.   \n[7] Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin G. Anthony, Shivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language models. ArXiv, abs/2304.11158, 2023.   \n[8] Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373, 2023. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[10] Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In USENIX Security Symposium, 2020. URL https://api.semanticscholar.org/CorpusID:229156229.   \n[11] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram\u00e8r, and Chiyuan Zhang. Quantifying memorization across neural language models. ArXiv, abs/2202.07646, 2022.   \n[12] Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id $\\cdot$ MO5PiKHELW.   \n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1\u2013240:113, 2022.   \n[14] Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and Antoine Bosselut. Analyzing commonsense emergence in few-shot knowledge models. In 3rd Conference on Automated Knowledge Base Construction, 2021.   \n[15] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493\u20138502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL https://aclanthology.org/2022.acl-long.581.   \n[16] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, and Yoav Goldberg. Measuring causal effects of data statistics on language model\u2019s \u2019factual\u2019 predictions. ArXiv, abs/2207.14251, 2022.   \n[17] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, 2019.   \n[18] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904, 2024.   \n[19] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484\u20135495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446.   \n[20] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216\u201312235, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.751. URL https://aclanthology.org/ 2023.emnlp-main.751.   \n[21] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi. Olmo: Accelerating the science of language models. ArXiv, abs/2402.00838, 2024. ", "page_idx": 11}, {"type": "text", "text": "[22] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Investigating learning dynamics of bert fine-tuning. In AACL, 2020.   \n[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022.   \n[24] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. ArXiv, abs/2006.14599, 2020.   \n[25] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u2019elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023.   \n[26] Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, 2022.   \n[27] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.   \n[28] Hugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:31809\u201331826, 2022.   \n[29] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Annual Meeting of the Association for Computational Linguistics, 2021.   \n[30] Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. Starcoder: may the source be with you! Transactions on Machine Learning Research, 2023. ISSN 2835-8856. Reproducibility Certification.   \n[31] Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, and Qun Liu. How pre-trained language models capture factual knowledge? a causal-inspired analysis. In Findings, 2022.   \n[32] Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A. Smith. Probing across time: What does RoBERTa know and when? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational ", "page_idx": 11}, {"type": "text", "text": "Linguistics: EMNLP 2021, pages 820\u2013842, Punta Cana, Dominican Republic, November 2021. ", "page_idx": 12}, {"type": "text", "text": "Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.71. URL https://aclanthology.org/2021.findings-emnlp.71.   \n[33] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yuechen Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. $A r X i\\nu$ , abs/2308.08747, 2023.   \n[34] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Annual Meeting of the Association for Computational Linguistics, 2022.   \n[35] Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. Entity cloze by date: What lms know about unseen entities. ArXiv, abs/2205.02832, 2022.   \n[36] Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, 2019.   \n[37] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overftiting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.   \n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[39] Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hidden representations and task semantics. In International Conference on Learning Representations, 2021.   \n[40] Adam Roberts, Colin Raffel, and Noam M. Shazeer. How much knowledge can you pack into the parameters of a language model? In Conference on Empirical Methods in Natural Language Processing, 2020.   \n[41] Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo. Are emergent abilities of large language models a mirage? ArXiv, abs/2304.15004, 2023.   \n[42] Emily Silcock, Luca D\u2019Amico-Wong, Jinglin Yang, and Melissa Dell. Noise-robust deduplication at scale. In The Eleventh International Conference on Learning Representations, 2023.   \n[43] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024.   \n[44] Michael T\u00e4nzer, Sebastian Ruder, and Marek Rei. Memorisation versus generalisation in pretrained language models. In Annual Meeting of the Association for Computational Linguistics, 2021.   \n[45] Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, and Aaron Gokaslan. Emergent structures and training dynamics in large language models. Proceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models, 2022.   \n[46] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:38274\u201338290, 2022.   \n[47] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S. Morcos. D4: Improving llm pretraining via document de-duplication and diversification. ArXiv, abs/2308.12284, 2023. ", "page_idx": 12}, {"type": "text", "text": "[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. ", "page_idx": 13}, {"type": "text", "text": "[49] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023.   \n[50] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. TMLR, 2022.   \n[51] Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales. ArXiv, abs/2212.09803, 2022. URL https://api.semanticscholar.org/ CorpusID:254877112.   \n[52] Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. Advances in Neural Information Processing Systems, 36, 2024.   \n[53] Xuekai Zhu, Yao Fu, Bowen Zhou, and Zhouhan Lin. Critical data size of language models from a grokking perspective. ArXiv, abs/2401.10463, 2024. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Although they do not affect the findings and implications of our work, there are several limitations. First, we do not perform evaluations based on the generation output of the model, and we do not investigate the exact relationship between the model\u2019s accumulation of probability of factual knowledge and the model\u2019s generation output. Second, we do not analyze the pretraining dynamics at very early stages, which can exhibit significantly different behaviors [24]. Third, we do not study the effect of training batch size and learning rate on the dynamics of factual knowledge acquisition across multiple values. Future works exploring these would help us to further enhance our understanding of LLMs. ", "page_idx": 14}, {"type": "text", "text": "B Dataset Construction and Examples ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We construct a FICTIONAL KNOWLEDGE dataset by prompting GPT-4 [2] with C.1 to generate descriptions for non-existent, fictional entities using the format of the ECBD [35] dataset, which is based on English Wikipedia articles. We select only the generated descriptions of the fictional entities that can produce at least five sentences suitable for a cloze task when the last span of the sentence is set as the target label. We repeat this until a total of 120 descriptions are produced. We call this \"injected knowledge\" in this paper. This process facilitates us to investigate the factual knowledge acquisition of the language models in a more controlled setup, as we can ensure that the model has never encountered the facts contained in the injected knowledge during the pretraining process. For the paraphrase injection training scenario mentioned in $\\S3$ , we generate 9 paraphrased injected knowledge for each original injected knowledge by prompting GPT-4 with C.2. ", "page_idx": 14}, {"type": "text", "text": "The types of probes for the injected knowledge consist of memorization probes, semantic generalization probes, and compositional generalization probes. For each injected knowledge, 15 probes are generated, with 5 for each type. First, the memorization probes are constructed by extracting exact sentences from the injected knowledge that ends with a named entity and setting the named entity as the target span. Next, the semantic generalization probes are created by prompting GPT-4 with C.3 to paraphrase each memorization probe while maintaining the target span and requiring no additional context. Lastly, compositional generalization probes are created by prompting GPT-4 with C.4 to create cloze tasks to evaluate whether new factual knowledge can be inferred by integrating and generalizing the factual knowledge in the injected knowledge. We constrain that the compositional generalization probes should avoid lexical overlap with the injected knowledge as much as possible and should not require additional context beyond the knowledge in the injected knowledge. To ensure the validity of the generated compositional generalization probe sets, we ask GPT-4 using prompt C.5 to evaluate whether each probe meets these conditions, answering with \"yes\" or \"no\". Only the probes that receive a \"yes\" response are selected. Examples of injected knowledge and paraphrased injected knowledge from the FICTIONAL KNOWLEDGE dataset can be found in Table 3 and the memorization probes, semantic generalization probes, and compositional generalization probes used to evaluate the acquisition of knowledge can be found in Table 4. ", "page_idx": 14}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/cd6852b65aff59e2dd962430c37494f6208814a8c54c140dce1d7bf40150bdd9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/a21ce1a3e23cec532b3827f0d5f87dae94a0bbdcdee2f29def080210980a0ee6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Prompts Used for Dataset Generation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Prompts for the generation of injected knowledge ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Carefully read the provided sentence; this is a short passage containing factual knowledge, that is extracted from Wikipedia:\\n\\n {DEFINITION IN ECBD DATASET}\\n\\nNow, assume that you are writing a very long and detailed descriptive paragraphs (more than 20 sentences) using the provided passage as a template. However, you should replace the named entities(person, country, act, etc.) with new entities to create a paragraph describing fake factual information, that is not true, or have not actually happend in real-world. Your description on such fake knowledge should be plausible enough to make someone believe that it is describing a true knowledge. You should always start and finish every sentence with a named entity. Avoid using pronouns or any other ambiguous terms (for example, \\\u2019the group\\\u2019) as possible as you can. Finally, avoid to generate knowledge that is potentially harmful. Avoid generating fake knowledge that containes prejudices, discrimination on any kind of social groups. Output the created paragraph only. $\\setminus\\neg\\setminus$ ", "page_idx": 17}, {"type": "text", "text": "C.2 Prompts for the generation of paraphrased injected knowledge ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The following text needs to be paraphrased to convey the same meaning in different words:\\n\\n\\\"{ORIGINAL INJECTED KNOWLEDGE}\\\"\\n\\nPlease paraphrase the above text clearly and concisely. ", "page_idx": 17}, {"type": "text", "text": "C.3 Prompts for the generation of semantic generalization probes ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Paraphrase the provided text with a constraint: the paraphrased sentence should be ended with the specified target, where the original sentence also ends with the target. Note that the paraphrased sentence should be semantically equivalent to the original sentence, and it should not contain any additional factual knowledge, nor lacks any factual knowledge that is stated in the original text. In addition, the content of the paraphrased text should be able to be fully understood without any ambiguity.\\n Here are some exmaples:\\n\\n[Example1 1]\\n\\n Input: The Lionheart Battalion (LB) is a fictitious white nationalist militia group in Spain.\\nTarget: Spain\\nOutput: The Lionheart Battalion (LB) is a fictional militia group with white nationalist beliefs located in Spain.\\n\\n[Example1 2]\\n\\nInput: Bell, initially a tormentor, later becomes an unlikely ally in Harper\u2019s investigations.\\nTarget: Harper\u2019s investigations\\nOutput: Bell, who first tormented, eventually turns into an unexpected supporter during Harper\u2019s investigations. $\\scriptstyle\\mathtt{\\backslash n\\backslash n\\backslash n A s}$ shown in the example, make sure that the output should end with the specified target. Never finish the sentence with any other words.\\n\\nNow, this is your input and target:\\n\\nInput: {MEMORIZATION PROBE}\\nTarget: {TARGET FOR MEMORIZATION PROBE}\\nOutput: ", "page_idx": 17}, {"type": "text", "text": "C.4 Prompts for the generation of compositional generalization probes ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "You are tasked with evaluating a participant\u2019s intelligence(in terms of generalization, composition, and inference) by measuring their ability to understand and combine the implications of different factual knowledge presented in a passage and apply them to deduce unseen knowledge. Specifically, you will create a next-word prediction task consisting of inputs and targets. The objective is to assess whether the participant can integrate and generalize the implications of the factual knowledge from the passage, combining different pieces of information to infer new factual knowledge.\\n\\nThe target should ", "page_idx": 17}, {"type": "text", "text": "consist of less then five words that complete the sentence when combined with the input, where the input is an incomplete sentence. The inputs and targets must be designed so that the target can only be accurately answered if the participant can perform complex generalization and integration based on the provided knowledge.\\n\\n Create eight different pairs of inputs and corresponding targets that require the participant to combine various factual knowledge presented in the passage, to deduce unseen knowledge. Avoid lexical overlaps with the passage as much as possible. Also, the content in the task should not ask for factual knowledge that is directly mentioned in the given passage, in other words, difficult enough. Additionally, ensure that the input and target can be understood and answered without additional context, assuming that the reader has comprehended and remembered the knowledge from the passage. Avoid using ambiguous terms such as \u2019that\u2019 or \u2019the event\u2019, assuming the passage is not provided with the question. Finally, most importantly, be creative as much as you can.\\n\\nPlease present your answers in the following format:\\n\\nProbe1: [YOUR_PROBE_ENDS_WITH_AN_UNDERSCORE]\\nAnswer1: ", "page_idx": 18}, {"type": "text", "text": "[YOUR_ANSWER_TO_THE_PROBE]\\n\\nNow, this is your passage:\\n\\n {ORIGINAL INJECTED KNOWLEDGE} ", "page_idx": 18}, {"type": "text", "text": "C.5 Prompts for the validation of generated compositional generalization probes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "You will be provided with a pair of cloze-task question and answer, and the problem\u2019s goal is to evaluate the subject\u2019s factual knowledge. Your task is to verify whether the provided pair of question and answer is properly designed to evaluate the factual knowledge. Assume that the subject has been already informed with the counterfactual knowledge before. Then, we are testing the subject\u2019s counterfactual knowledge. Note that regardless of the consistency of the factual knowledge tested in the problem, we say that the problem is properly designed if there is no ambiguity in the question and answer. So the question is verifying: Can the content of the question be fully understood and properly answered without any ambiguity or the need of additional context, given that the corresponding factual knowledge is existent?\\n \\nAfter providing your explanation, you should give your answer in \u2018yes\u2019 or \u2018no\u2019. The answer should be \u2018yes\u2019 only if both of the conditions are satisfied, and the answer should be \u2018no\u2019 otherwise.\\n For example, this is an example of your answer:\\n\\nExplanation: [YOUR_EXPLANATION]\\nAnswer: [YES_OR_NO]\\n\\nHere are some example inputs and answers:\\n\\n[Example 1]\\nQuestion: Within the realm of fantasy, he is ranked second in command in the _____\\nAnswer: Lionheart Battalion\\n \\nExplanation: The example provided is not entirely clear or straightforward in its design to evaluate factual knowledge. The question, \\\"Within the realm of fantasy, he is ranked second in command in the _____,\\\" contains a few ambiguities. Firstly, \\\"the realm of fantasy\\\" is a broad and non-specific term, which could refer to any number of fantasy stories, games, or universes. Secondly, the phrase \\\" he is ranked second in command\\\" does not specify who \\\"he\\\" refers to, nor does it establish a clear context or a specific entity to which the answer \\\"Lionheart Battalion\\\" could logically be connected without additional information. This lack of specificity and context does not allow the question to be answered accurately based solely on factual knowledge without guessing or assuming additional context. The problem does not provide enough information to identify which fantasy setting is being referred to, nor does it give any clues about the character or the organizational structure within which this character operates.\\n Answer: no\\n\\n[Example 2]\\nQuestion: Jaccard Hume was the first person to land on ___ _\\nAnswer: Mars\\n\\nExplanation: This question and answer pair seems straightforward and specific in its design to evaluate factual knowledge. The question, \\\"Jaccard Hume was the first person to land on _____,\\\" clearly identifies a specific individual, Jaccard Hume, and asks for a significant historical or factual event related to him\u2014being the first person to land on a particular celestial body. The answer provided is \\\"Mars,\\\" which is clear and direct. Assuming the subject has the necessary factual knowledge about Jaccard Hume and his achievements, there is no ambiguity in either the question or the answer. The answer \\\"Mars\\\" directly fills the blank without the need for additional context or interpretation. Therefore, this question and answer pair is properly designed to assess the factual knowledge regarding Jaccard Hume\u2019s accomplishments in space exploration.\\nAnswer: no\\n\\nNow, here is the input text:\\n\\nQuestion: {GENERATED COMPOSITIONAL GENERALIZATION PROBE} __Answer: {GENERATED TARGET OF COMPOSITIONAL GENERALIZATION PROBE}\\n\\n ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "D Detailed Training Setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To continue training almost similar to the pretraining setup, we use OLMo[21], as it provides not only intermediate model checkpoints but also the exact sequence of data instances used for pretraining, the optimizer state, and the learning rate scheduler. Throughout the entire pretraining process, the language model is trained with a language modeling objective. ", "page_idx": 20}, {"type": "text", "text": "Except for the batches that include injected knowledge from FICTIONAL KNOWLEDGE dataset at specific step intervals, we train OLMo with batches from the Dolma corpus [43] in the same order which is used in OLMo pretraining. Specifically, we load the training batch that OLMo will be seen at the specific pretraining step, append the injected knowledge from the FICTIONAL KNOWLEDGE dataset to the front of each row, and truncate the original rows from the end by the token length of the injected knowledge. This approach creates batches that have the same size as the original pretraining batches, with 2048 rows and a sequence length of 2048, meaning each batch contains 4M tokens. We adopt this method to deviate as little as possible from the original pretraining data distribution. ", "page_idx": 20}, {"type": "text", "text": "In the FICTIONAL KNOWLEDGE dataset, which consists of 120 descriptions of fictional knowledge, we use the first 1-40 injected knowledge to examine the dynamics of knowledge acquisition in the paraphrase injection scenario which is described in $\\S3$ . The 41-80 injected knowledge is used for the duplication injection scenario, and the 81-120 injected knowledge is used for the once injection scenario. ", "page_idx": 20}, {"type": "text", "text": "For each injection scenario, the FICTIONAL KNOWLEDGE data are injected into the batch and trained according to the following rules. In the duplication injection scenario, injected knowledge in the FICTIONAL KNOWLEDGE dataset is injected into the original pretraining batch, and the language model is trained on this modified batch 10 times every 100 steps. Next, in paraphrase injection scenario, similar to the duplication injection scenario, the model is trained on the modified batches containing FICTIONAL KNOWLEDGE every 100 steps for a total of 10 times, however, in this case, paraphrased injected knowledge is used at each injection step. Lastly, in the once injection scenario, the modified batch containing injected knowledge of FICTIONAL KNOWLEDGE is shown to the language model just once, after which it continues training on the original batch of Dolma corpus. ", "page_idx": 20}, {"type": "text", "text": "After 1000 steps of pretraining following the above rules, an additional 1500 steps of pretraining are conducted using the Dolma corpus for experiments analyzing forgetting dynamics in $\\S4.3$ . The Dolma corpus used at these steps is a corpus that will be viewed starting from the 360,000th step of pretraining the OLMo. This approach ensures consistency in the Dolma corpus across all conditions while guaranteeing that the corpus has not been seen in any previous pretraining processes. Continued pretraining of a total of 2500 steps takes approximately 3 days using 8 80GB A100 GPUs. ", "page_idx": 20}, {"type": "text", "text": "To examine the differences in knowledge acquisition dynamics based on model size, we use OLMo7B and OLMo-1B. For differences based on the number of pretrained tokens, we use intermediate checkpoints at Early (170B) stage (specifically, 177B tokens for 7B and 168B tokens for 1B), Mid (500B) stage (specifically, 500B tokens for 7B and 494B tokens for 1B), and Late (1.5T) stage (1.5T tokens for 7B and 1B). Since the initial checkpoints of OLMo-1B are stored in units of 10000, it is the best choice in the given situation to select the checkpoint trained with the number of tokens closest to 177B. The differences in initial learning rate values for each case based on different model sizes and pretraining stages are recorded in Table 5 below. ", "page_idx": 20}, {"type": "text", "text": "Table 5: The initial learning rate for each intermediate OLMo checkpoint based on model sizes and the pretraining stages. For OLMo-7B, the pretraining stages align with the following number of pretrained tokens: 177B, 500B, 1.5T. For OLMo-1B, the pretraining stages align with the following number of pretrained tokens: 168B, 500B, 1.5T. ", "page_idx": 20}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/4fcf39e15bd994886831f3e5d9631b14d452b3286a1e6fe007ce2c85b0fd3c36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Additional Figures for the Pretraining Experiments ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/44a8c7bf16ad751e11d52430495ebc720fb73ea8da3360bfd9bcae9e7985595d.jpg", "img_caption": ["E.1 Training dynamics of other OLMo-7B checkpoints ", "Figure 6: Training dynamics of OLMo-7B Early (170B) checkpoint. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/bcac57fd2b870e6485ec23a698cfe4d8dcbcbaabedaf7193f8e3571bffe825ca.jpg", "img_caption": ["Figure 7: Training dynamics of OLMo-7B Late (1.5T) checkpoint. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/f6308328ae4bb2921c812e339be7e4a5250bb6be2608611780d9e3e1a95b8c48.jpg", "img_caption": ["E.2 Training dynamics of other OLMo-1B checkpoints ", "Figure 8: Training dynamics of OLMo-1B Early (170B) checkpoint. In comparison to the checkpoints of OLMo-7B and later checkpoints of OLMo-1B, the curves exhibit much more drastic fluctuations. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/1dfa2f20933fc41879eaf9cb07b585dee05ddef9e3f6457e6763a9f12410af7c.jpg", "img_caption": ["Figure 9: Training dynamics of OLMo-1B Mid (500B) checkpoint. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/74986a2530c71eaf9595945d70baa230e58b2125f836d0c1be22596b752abd5a.jpg", "img_caption": ["Figure 10: Training dynamics of OLMo-1B Late (1.5T) checkpoint. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.3 Effectivity measurement data for OLMo-1B ", "page_idx": 24}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/e9cc21e653dfbedb17afa37c15c6c60e6ae0d3ec5fc70b0a415fcab2d6e7f30b.jpg", "img_caption": ["Figure 11: Effectivity measured for OLMo-1B models. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.4 Forgetting dynamics of OLMo-7B checkpoints ", "page_idx": 24}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/f050f0915ad9e65b301d15a6ec074291f270155d8943f4e96eeccb2f23b42b1c.jpg", "img_caption": ["Figure 12: Forgetting dynamics of OLMo-7B Early (170B) checkpoint. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/673a0aa0f8eb0af29cb9d2e01b8536e82843a595cfcc8c23ca627d0c92bce582.jpg", "img_caption": ["Figure 13: Forgetting dynamics of OLMo-7B Late (1.5T) checkpoint. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/e173341236f64109d5e0ea245595b7dfadcb4e6bd9aa94c3072d1b0109bde415.jpg", "table_caption": ["Table 6: Anticipated $\\mathbf{X}$ -intercepts of $\\mathcal{R}(p,t)$ measured with OLMo-7B, at three different pretraining stages, acquisition depths, and injection scenarios. The units are log(Tokens). "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/dbe0cca6902bc053aab7aa2cac854ccd684f9aeed92cc9269d9746994b5be41a.jpg", "img_caption": ["E.5 Forgetting dynamics of OLMo-1B checkpoints ", "Figure 14: Forgetting dynamics of OLMo-1B Early (170B) checkpoint. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/a49641365e904174d43d9bd2fa21bacdba503d69c67de2eb8e614fd7e3c36afe.jpg", "img_caption": ["Figure 15: Forgetting dynamics of OLMo-1B Mid (500B) checkpoint. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/a1f915bd187e4dbb6a39fd82fecf4f51b5217fcafe872f0fa3dc3d213118ae01.jpg", "img_caption": ["Figure 16: Forgetting dynamics of OLMo-1B Late (1.5T) checkpoint. "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/abed2f3e39596f5ca265ee6bc9b8b0cb920eb86586ff2571bff09f5c39b4f491.jpg", "table_caption": ["Table 7: Decay constant of average retainability $(\\mathcal{R}(p,t))$ measured with OLMo-1B, at three different pretraining stages, acquisition depths, and injection scenarios. The values for the Early (168B) checkpoint are omitted due to the poor linear fitting $(R^{2}<0.4)$ , which is attributed to the highly unstable dynamics as shown in Appendix Figure 8 and 14. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/cd5ed4c9db9c545bd9c3e02ce553968f83533a8f224450420f79d12ff368ea0d.jpg", "table_caption": ["Table 8: Anticipated $\\mathbf{X}$ -intercepts of $\\mathcal{R}(p,t)$ measured with OLMo-1B, at three different pretraining stages, acquisition depths, and injection scenarios. The units are log(Tokens). The values for the Early (168B) checkpoint are omitted due to the poor linear fitting $\\Bar{.}R^{2}<0.4)$ , as mentioned in Appendix Table 7. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F Experiments for Training Olmo-7B Checkpoints With a Constant Learning Rate ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We continue training each OLMo-7B checkpoint with a constant learning rate, to compare the effectivity and retainability of each checkpoint while excluding the impact of different learning rates. Optimizer states are loaded to promote a warm start of continued training. Due to the restriction of computational resources, we reduce the batch size from 2048 to 128 for this experiment. The value of the constant learning rate is obtained by averaging the starting learning rates of three checkpoints. We do not apply learning rate decay for this experiment. All other training conditions not mentioned are identical to the main experiment. The results in Appendix Figure 17 demonstrate that there is no improvement of average effectivity in later checkpoints, although all models are trained with the same learning rate. This supports that the non-increasing effectivity in pretraining progress is not attributed to the learning rate decay. Similarly, there is no decrease in the decay constants for the later checkpoints (Appendix Table 9). Note that the figures in $\\S\\mathrm{F}.1$ demonstrate that reducing the batch size does not significantly change the model\u2019s behavior in accumulating log probability during factual knowledge acquisition. ", "page_idx": 28}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/8bafe90c944f265d0d2bf9ddfb04d7282f69b2cd4c4d14da68900a4e1cde243a.jpg", "img_caption": ["Figure 17: Average effectivity measured with OLMo-7B trained with a fixed constant learning rate. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 9: Decay constant of average retainability $(\\mathcal{R}(p,t))$ measured with OLMo-7B trained with the same constant learning rate, at three different pretraining stages, acquisition depths, and injection scenarios. Note that the decay constant does not decrease for the later checkpoint. ", "page_idx": 28}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/5c35785922545a9f180b6a8153493fe960a4c27624e692d0a79ecf72cbe330ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/353cf7570b9a5e1cdf4bb209dfce5c358e1a532d135f103c658ac05446154494.jpg", "img_caption": ["F.1 Training dynamics for constant learning rate experiments ", "Figure 18: Training dynamics of OLMo-7B Early (170B) checkpoint trained with a constant learning rate. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/406b712d86866a09cc19b0b8b23e5fcf1376db09f06afc1b633c6cecba1f3b5d.jpg", "img_caption": ["Figure 19: Training dynamics of OLMo-7B Mid (500B) checkpoint trained with a constant learning rate. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/89c27c1710135e6cffab970f8c47e43ff9507488562a1017e8bbf10b871742be.jpg", "img_caption": ["Figure 20: Training dynamics of OLMo-7B Late (1.5T) checkpoint trained with a constant learning rate. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "G Forgetting Dynamics of Olmo-7B Trained With a Reduced Batch Size ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Similar to F, we train the OLMo-7B intermediate checkpoints with a reduced batch size of 128. However, we set the learning rate for each checkpoint as the initial learning rate (Appendix Table 5), as the objective of this experiment is to examine the effect of reduced batch size on the forgetting dynamics. We re-initialize the optimizer state. We observe that this results in unstable dynamics in early steps, but the dynamics are stabilized soon, and do not harm the model\u2019s overall behavior in general $(\\S\\mathrm{G})$ . Appendix Figure 21 shows the effectivity measurements of OLMo-7B models at different pretraining stages. Similar to the observations in Appendix Figure 17, the effectivity values are greater compared to the values in the pretraining experiment (Figure 3). Appendix Figure 22 and 23 illustrates the forgetting dynamics of OLMo-7B Early (170B) and late (1.5T) checkpoints, respectively. Appendix Table 10 shows the decay constants $(a)$ measured with three different pretraining stages, acquisition depths, and injection scenarios. Note that the slope remains unchanged regardless of whether we set the ${\\bf X}$ -axis to tokens or training steps. Hence, the decay constants in the table can be directly compared to the values presented in Table 2. Comparing the values of the expected ${\\bf X}$ -intercepts of retainability presented in Appendix Table 11 with Appendix Table 6, the results demonstrate that the model trained with a smaller batch size has a shorter learnability threshold. ", "page_idx": 31}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/005ac8b60714f25a31b318ab609c27606ebe72de03382f312f4a9fb69cb26c00.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 21: Average effectivity measured with OLMo-7B trained with a batch size of 128. The low effectivity values observed in the once injection scenario are attributed to the unstable dynamics after the re-initialization of the optimizer states. ", "page_idx": 31}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/151167a812742e6cff7d57b2cbe81be2bcf46fa411ed61d358010f0212ccf784.jpg", "img_caption": ["Figure 22: Forgetting dynamics of OLMo-7B Early (170B) checkpoint with a reduced batch size. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/b5694e7a29bb090e292964334231a686f57a44ff2d7dd940bea4304eb6470277.jpg", "img_caption": ["Figure 23: Forgetting dynamics of OLMo-7B Late (1.5T) checkpoint with a reduced batch size. "], "img_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/c1673d8e6c1dba902004a6687e66f9e5c4dbda156a02a6418f5782ce598932c8.jpg", "table_caption": ["Table 10: Decay constant of average retainability $(\\mathcal{R}(p,t))$ measured with OLMo-7B trained with a batch size of 128, at three different pretraining stages, acquisition depths, and injection scenarios. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "TYdzj1EvBP/tmp/e2c538a311a605111313abc56f698850f12266dc9b9d8aa2bc90ba85e94c3c95.jpg", "table_caption": ["Table 11: Anticipated ${\\bf X}$ -intercepts of $\\mathcal{R}(p,t)$ measured with OLMo-7B trained with a batch size of 128, at three different pretraining stages, acquisition depths, and injection scenarios. The units are log(Tokens). "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/5e8add7a3b1f228a20998804c7d0482af90dee5a15bad48b609f731dc31710db.jpg", "img_caption": ["G.1 Training dynamics for experiments on the forgetting dynamics with a reduced batch size "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 24: Training dynamics of OLMo-7B Early (170B) checkpoint trained with reduced batch size and re-initialized optimizer state. ", "page_idx": 33}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/fcf5a658456010cd894d469b3f194bb29b2d635348927f0a0d728e8910655fec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 25: Training dynamics of OLMo-7B Mid (500B) checkpoint trained with reduced batch size and re-initialized optimizer state. ", "page_idx": 33}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/d452bc05f1f2f6ac6d57d11a258a58624f3a68bb82412f1009a17fb30bd881bb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 26: Training dynamics of OLMo-7B Late (1.5T) checkpoint trained with reduced batch size and re-initialized optimizer state. ", "page_idx": 34}, {"type": "text", "text": "H Effect of the Number of Previous Encounters on Effectivity and Retainability of Factual Knowledge ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We measure the average effectivity for each count of injection $(i)$ in duplication and paraphrase injection scenario. In this analysis, we exclude the cases where the log probability at the local acquisition maxima is smaller than the point before the model is trained with the injected knowledge, as such cases can be regarded as failure cases of learning. Appendix Figure 27, 28, and 29 display the results for OLMo-7B early, mid, and late checkpoints, respectively. We observe that the effectivity is relatively constant regardless of the number of previous injections of the knowledge. However, we observe that the effectivity is the highest when the model is trained with the injected knowledge for the first time, both in the duplication and paraphrase injection scenarios. ", "page_idx": 35}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/d0b7a66a52c2692ea32cc0736f9b9737553f5cb5908287a29871238a05e32ce4.jpg", "img_caption": ["Figure 27: Average effectivity measured for each count of injection, measured with OLMo-7B Early (170B) checkpoint. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/b9117b4375475b9e8d88fc62f66f6b86bbe9e38b5ed58d22018b90dd8e4a6682.jpg", "img_caption": ["Figure 28: Average effectivity measured for each count of injection, measured with OLMo-7B Mid (500B) checkpoint. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "TYdzj1EvBP/tmp/28ae68923f2c1f1d44dd645b8c888bda144798032449ba49bc858775c3c9d37e.jpg", "img_caption": ["Figure 29: Average effectivity measured for each count of injection, measured with OLMo-7B Late (1.5T) checkpoint. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The contributions and scope of this paper are well stated in the abstract and introduction, and the statements are supported by the experimental results. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The limitations of this work is discussed in $\\S\\mathrm{A}$ ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This work does not provide any theoretical results. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We provide detailed descriptions for the generation of the FICTIONAL KNOWLEDGE dataset in $\\S B$ and experimental setups in $\\S D$ . ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We submit the code used for all experiments and the F ICTIONAL K NOWLEDGE dataset. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The details for the training and evaluation are described in $\\S3$ and $\\S D$ , along with the justifications for the selection of the hyperparameters. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not report the error bars for the measurement of effectivity and retainability, as it is required to run the pretraining experiments multiple times with different seeds, which is not feasible due to the restriction of computational resources. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The amount of computational resources required for the main pretraining experiments is described in $\\S D$ . ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We confirm that this work is done following the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We discuss that our work is related to understanding the behaviors of LLMs which can contribute to the safe deployment of LLMs in $\\S5$ . ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This work poses no risk for misuse. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We cite and explicitly mention the original papers that produced the pretrained models or datasets that are used in this work. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We describe the structure of the newly introduced FICTIONAL KNOWLEDGE dataset. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}]