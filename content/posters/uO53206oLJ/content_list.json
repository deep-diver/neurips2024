[{"type": "text", "text": "Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiaojiao Zhang1, Jiang $\\mathbf{H}\\mathbf{u}^{2}$ ,\u2217 Anthony Man-Cho $\\mathbf{So}^{3}$ , Mikael Johansson1 ", "page_idx": 0}, {"type": "text", "text": "1 KTH Royal Institute of Technology 2 University of California, Berkeley 3 The Chinese University of Hong Kong {jiaoz, mikaelj}@kth.se, hujiangopt@gmail.com, manchoso@se.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many machine learning tasks, such as principal component analysis and low-rank matrix completion, give rise to manifold optimization problems. Although there is a large body of work studying the design and analysis of algorithms for manifold optimization in the centralized setting, there are currently very few works addressing the federated setting. In this paper, we consider nonconvex federated learning over a compact smooth submanifold in the setting of heterogeneous client data. We propose an algorithm that leverages stochastic Riemannian gradients and a manifold projection operator to improve computational efficiency, uses local updates to improve communication efficiency, and avoids client drift. Theoretically, we show that our proposed algorithm converges sub-linearly to a neighborhood of a firstorder optimal solution by using a novel analysis that jointly exploits the manifold structure and properties of the loss functions. Numerical experiments demonstrate that our algorithm has significantly smaller computational and communication overhead than existing methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL), which enables clients to collaboratively train models without exchanging their raw data, has gained significant traction in machine learning [1, 2]. The framework is appreciated for its capacity to leverage distributed data, accelerate the training process via parallel computation, and bolster privacy protection. The majority of existing FL algorithms address problems that are either unconstrained or have convex constraints. However, for applications such as principal component analysis (PCA) and matrix completion, where model parameters are subject to nonconvex manifold constraints, there are very few options in the federated setting. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study $\\mathrm{FL}$ problems over manifolds in the form of ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\underset{x\\in\\mathcal{M}\\subset\\mathbb{R}^{d\\times k}}{\\mathrm{minimize~}}\\;f(x):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x),\\quad f_{i}(x)=\\frac{1}{m_{i}}\\sum_{l=1}^{m_{i}}f_{i l}(x;\\mathcal{D}_{i l}).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here, $n$ is the number of clients, $x$ is the matrix of model parameters, and $\\mathcal{M}$ is a compact smooth submanifold embedded in $\\mathbb{R}^{d\\times k}$ . Examples of such manifolds include the Stiefel, oblique, and symplectic manifolds [3, 4, 5]. For instance, PCA-related optimization problems use the Stiefel manifold $\\mathcal{M}=\\mathrm{St}(d,\\bar{k})=\\{\\bar{x}\\in\\mathbb{R}^{d\\times k}:x^{T}x=I_{k}\\}$ to maintain orthogonality [6, 7]. In (1), the global loss $f:\\mathbb{R}^{d\\times k}\\rightarrow\\mathbb{R}$ is smooth but nonconvex1, and the local loss function $f_{i}$ of each client $i$ is the average of the losses $f_{i l}$ on the $m_{i}$ data points in its local dataset $\\mathcal{D}_{i}=\\{\\mathcal{D}_{i1},...\\,,\\mathcal{D}_{i m_{i}}\\}$ . We consider a heterogeneous data scenario where the statistical properties of $\\mathcal{D}_{i}$ differ across clients. ", "page_idx": 0}, {"type": "text", "text": "Manifold optimization problems of the form (1) appear in many important machine learning tasks, such as PCA [8, 9], low-rank matrix completion [10, 11], multitask learning [12, 13], and deep neural network training [14, 15]. Still, there are very few federated algorithms for machine learning on manifolds. In fact, the work [16] appears to be the only FL algorithm that can deal with manifold optimization problems of a similar generality as ours. Handling manifold constraints in an FL setting poses significant challenges: (i) Existing single-machine methods for manifold optimization [6, 5, 4] cannot be directly adapted to the federated setting. Due to the distributed framework, the server has to average the clients local models. Even if each of these models lies on the manifold, their average typically does not due to the nonconvexity of $\\mathcal{M}$ . The current literature relies on complicated geometric operators like the exponential map, inverse exponential map, and parallel transport, to design an averaging operator for the manifold [16]. However, these mappings may lack closed-form expressions and can be computationally expensive to evaluate. For example, computing the inverse exponential map on the Stiefel manifold requires solving a nonlinear matrix equation [17]. (ii) Extending typical FL algorithms to scenarios with manifold constraints is not straightforward, either. Most existing FL algorithms either are unconstrained [18, 19] or only allow for convex constraints [20, 21, 22, 23, 24], but manifold constraints are typically nonconvex. Moreover, compared to nonconvex optimization in Euclidean space, manifold optimization necessitates the consideration of the geometric structure of the manifold and properties of the loss functions, which poses challenges for algorithm design and analysis. (iii) Traditional methods for enhancing communication efficiency in FL, like local updates [25], need substantial modifications to accommodate manifold constraints. The so-called client drift issue due to local updates and heterogeneous data [19] persists in the realm of manifold optimization. Directly using client-drift correcting techniques originally developed for Euclidean spaces [19, 26, 27] could lead to additional communication or computational costs due to the manifold constraints. For instance, in [16], the correction term requires additional communication of local Riemannian gradients and involves using parallel transport to move the correction term onto some tangent space in preparation for the exponential mapping. Although some existing decentralized manifold optimization algorithms [9, 28, 29] can be simplified to an FL scenario with only one local update under the assumption of a fully connected network, these algorithms cannot be directly applied to FL scenarios with more than one local update, especially in cases of data heterogeneity. Extending the analysis of these algorithms to FL scenarios with multiple local updates is not straightforward. On the other hand, the use of local updates in FL, compared to these decentralized distributed algorithms, can more effectively reduce the number of communication rounds. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the nonconvex FL problem (1) with $\\mathcal{M}$ being a compact smooth submanifold and allow for heterogeneous data distribution among clients. Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "1) We propose an FL algorithm for solving (1) that is efficient in terms of both computation and communication. We employ stochastic Riemannian gradients and a projection operator to address manifold constraints, use local updates to reduce the communication frequency between clients and the server, and design correction terms to overcome client drift. In terms of server updates, our algorithm ensures feasibility of all global model iterates and is computationally efficient since it avoids the techniques used in [16] based on the exponential mapping and inverse exponential mapping for averaging local models on manifolds. For local updates, our algorithm constructs the correction terms locally without increasing communication costs. In comparison, the approach presented in [16] requires each client to transmit an additional local stochastic Riemannian gradient for constructing correction terms. Moreover, [16] necessitates parallel transport to position the correction terms on tangent spaces so that the exponential map can be applied to ensure the feasibility of local models, thereby increasing computational costs. In contrast, our algorithm utilizes a simple projection operator, effectively eliminating the need for parallel transport of correction terms. ", "page_idx": 1}, {"type": "text", "text": "2) Theoretically, we establish sub-linear convergence to a neighborhood of a first-order optimal solution and demonstrate how this neighborhood depends on the stochastic sampling variance and algorithm parameters. Our analysis introduces novel proof techniques that utilize the curvature of the manifolds and the properties of the loss functions to overcome the challenges posed by the nonconvexity of manifold constraints in the nonconvex FL scenario. Compared to the existing work [16] where analytical results are limited to cases where either the number of local updates is one or the number of participating clients per communication round is one, our theoretical results allow for an arbitrary number of local updates and support full client participation. The key components of our analysis are the manifold geometry and the Lipschitz continuity of the projection operator, both of which are inherent to the submanifold constraint. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3) Our algorithm demonstrates superior performance over alternative methods in the numerical experiments. In particular, it produces high-accuracy results for kPCA and low-rank matrix completion at a significantly lower communication and computation cost than alternative algorithms. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first review federated learning algorithms for composite optimization with and without constraints. Then, we discuss FL algorithms with manifold constraints. ", "page_idx": 2}, {"type": "text", "text": "Composite FL in Euclidean space. Problem (1) can be viewed as a special case of composite FL where the loss function is a composition of $f$ and the indicator function of $\\mathcal{M}$ . It is important to note that since the manifold is nonconvex, its indicator function is also nonconvex. Most existing composite FL methods can only handle convex constraints. The work [20] proposed a federated dual averaging method and established its convergence for a general loss function under bounded gradient assumptions, but only for quadratic losses under the bounded heterogeneity assumption that the degree of data heterogeneity among clients is bounded. In contrast, we make no assumptions about the similarity of data across clients. The fast federated dual averaging algorithm [21] extends the work in [20] by using both past gradient information and past model information in the local updates. However, the work [21] requires each client to transmit the local gradient as well as the local model, and it assumes bounded data heterogeneity. The work [22] introduces the federated Douglas-Rachford method, and the work [23] applies this algorithm to solve dual problems. Although these two methods avoid bounded data heterogeneity, they require an increasing number of local updates to ensure convergence, which reduces their practicality in FL. The recent work [24] proposes a communicationefficient FL algorithm that overcomes client drift by decoupling the proximal operator evaluation and the communication and shows that the method converges without any assumptions on data similarity. ", "page_idx": 2}, {"type": "text", "text": "Federated learning on manifolds. The existing composite FL in Euclidean space [20]-[24] only considers scenarios where the nonsmooth term in the loss functions is convex. However, incorporating a nonconvex manifold constraint as an indicator function introduces a nonconvex nonsmooth term. Consequently, the methods in [20]-[24] are not directly applicable. A typical challenge caused by the nonconvex manifold constraint is that the average of local models, each of which lies on the manifold, may not belong to the manifold. To address this issue, the work [16] introduced Riemannian federated SVRG (RFedSVRG), where the server maps the local models onto a tangent space, calculates an average, and then retracts the average back to the manifold. This process sequentially employs inverse exponential and exponential mappings. Moreover, RFedSVRG employs a correction term to overcome client drift but requires additional communication of local Riemannian gradients to construct the correction term. In addition, the method uses parallel transport to position the correction term, which increases the computation cost even further. Note that the manifold we consider is a compact smooth submanifold embedded in $\\mathbb{R}^{d\\times k}$ , which is more restrictive than the manifolds discussed in [16]. However, this approach still encompasses many common manifolds, including the Stiefel, oblique, and symplectic manifolds [3, 4, 5]. The work [30] explores the differential privacy of RFedSVRG. The work [31] considers the specific manifold optimization problem that appears in PCA and investigates an ADMM-type method that penalizes the orthogonality constraint. However, this algorithm requires solving a subproblem to desired accuracy, which increases computational cost. The work [32] introduces a differentially private FL algorithm for solving PCA. Finally, the works [33] and [34] consider the leading eigenvector problem with homogeneous data across clients, where the loss functions are quadratic and the manifold is a sphere. In contrast, we consider a more general setting with heterogeneous data, where $x$ lies on $\\mathcal{M}$ and the loss functions are smooth and nonconvex. ", "page_idx": 2}, {"type": "text", "text": "Notations. We use $I_{k}$ to denote a $k\\times k$ identity matrix. We use $\\Vert\\cdot\\Vert$ to denote Frobenius norm and $\\mathrm{tr}(\\cdot)$ to denote the trace of a matrix. For a set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , we use $|\\beta|$ to denote the cardinality. For a random variable $v$ , we use $\\mathbb{E}[v]$ to denote the expectation and $\\mathbb{E}[v|\\mathcal{F}]$ to denote the expectation given event $\\mathcal{F}$ . For an integer $n$ , we use $[n]$ to denote the set $\\{1,\\ldots,n\\}$ . For two matrices $x,y\\in$ $\\mathbb{R}^{d\\times k}$ , we define their Euclidean inner product as $\\begin{array}{r}{\\langle x,y\\rangle\\;:=\\;\\sum_{i=1}^{d}\\sum_{j=1}^{k}x_{i j}y_{i j}}\\end{array}$ . For matrices $z_{1},\\dots,z_{n}\\in\\mathbb{R}^{d\\times k}$ , we use $\\mathbf{z}:=\\mathrm{col}\\{z_{i}\\}_{i=1}^{n}:=[z_{1};\\ldots;z_{n}]\\in\\mathbb{R}^{n d\\times k}$ to denote the vertical stack of all matrices. The bold notations $\\widehat{\\mathbf{z}}$ , $\\mathbf{c}$ , and $\\Lambda$ are defined similarly. Specifically, for a matrix $\\boldsymbol{x}\\in\\mathbb{R}^{d\\times k}$ , we define $\\mathbf{x}:=\\mathrm{col}\\{x\\}_{i=1}^{n}:=[x;\\ldots;x]\\in\\mathbb{R}^{n d\\times k}$ . We use $r$ to denote the index of the communication round and $t$ to denote the index of local updates. Given the local Riemannian gradient $\\mathrm{grad}f_{i}(z_{i,t}^{r};B_{i,t}^{r})$ at point $z_{i,t}^{r}$ with the mini-batch dataset $B_{i,t}^{r}$ , we define the stack of Riemannian gradients as gradf $^{\\cdot}(\\mathbf{z}_{t}^{r};\\mathcal{B}_{t}^{r}):=\\mathrm{col}\\{\\mathrm{grad}f_{i}(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r})\\}_{i=1}^{n}$ and the stack of average local Riemannian gradients as $\\begin{array}{r}{\\overline{{\\operatorname{gradf}}}(\\mathbf{z}_{t}^{r};\\mathcal{B}_{t}^{r}):=\\cot\\big\\{\\frac{1}{n}\\!\\sum_{i=1}^{n}\\operatorname{grad}f_{i}(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r})\\big\\}_{i=1}^{n}}\\end{array}$ . Given $\\textstyle\\operatorname{col}\\{z_{i}\\}_{i=1}^{n}$ and $\\mathcal{P}_{\\mathcal{M}}(z_{i})$ , we define $\\mathcal{P}_{\\mathcal{M}}(\\mathrm{col}\\{z_{i}\\}_{i=1}^{n})=\\mathrm{col}\\{\\mathcal{P}_{\\mathcal{M}}(z_{i})\\}_{i=1}^{n}$ . Given a positive definite matrix $x$ , we use $x^{-1/2}$ to denote the inverse of the square root of $x$ , i.e., $x^{-1/2}x^{-1/2}\\,=\\,x^{-1}$ . We define $\\mathcal{D}^{2}$ to be the second-order differential operator. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Below, we introduce fundamental definitions and inequalities for optimization on manifolds. ", "page_idx": 3}, {"type": "text", "text": "2.1 Optimization on manifolds ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Manifold optimization aims to minimize a real-valued function over a manifold, i.e., $\\operatorname*{min}_{x\\in\\mathcal{M}}~f(x)$ . Throughout the paper, we restrict our discussion to embedded submanifolds of the Euclidean space, where the associated topology coincides with the subspace topology of the Euclidean space. We refer to these as embedded submanifolds. Some examples of such manifolds include the Stiefel manifold, oblique manifold, and symplectic manifold [3, 4, 5]. We define the tangent space of $\\mathcal{M}$ at point $x$ as $T_{x}\\mathcal{M}$ , which contains all tangent vectors to $\\mathcal{M}$ at $x$ , and the normal space as $N_{x}\\mathcal{M}$ which is orthogonal to the tangent space. With the definition of tangent space, we can define the Riemannian gradient that plays a central role in the characterization of optimality conditions and algorithm design for manifold optimization. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Riemannian gradient grad $f(x){\\big\\vert}$ ). The Riemannian gradient grad $f(x)$ of a function $f$ at the point $x\\in\\mathcal{M}$ is the unique tangent vector that satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\langle\\operatorname{grad}f(x),\\xi\\rangle_{x}=d f(x)[\\xi],\\;\\forall\\xi\\in T_{x}{\\mathcal{M}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle_{x}$ is the Riemannian metric and df denotes the differential of function $f$ . ", "page_idx": 3}, {"type": "text", "text": "For a submanifold $\\mathcal{M}$ , the Riemannian gradient grad $f(x)$ (under the Euclidean inner product) can be computed as [5, Proposition 3.61] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{grad}f(x)=\\mathcal{P}_{T_{x},M}(\\nabla f(x)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{P}_{T_{x}\\mathcal{M}}(\\nabla f(x))$ represents the orthogonal projection of $\\nabla f(x)$ onto $T_{x}\\mathcal{M}$ . The Riemannian gradient grad $f(x)$ reduces to the Euclidean gradient $\\nabla f(x)$ when $\\mathcal{M}$ is the Euclidean space $\\mathbb{R}^{d\\times k}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Proximal smoothness of $\\mathcal{M}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In our federated manifold learning algorithm, the server needs to fuse models that have undergone multiple rounds of local updates by the clients. Due to the nonconvexity of the manifold, the average of points on the manifold is not guaranteed to belong to the manifold. The tangent space-based exponential mapping or other retraction operations commonly used in manifold optimization are expensive in FL [16]. Specifically, the server needs to map the local models onto a tangent space using inverse exponential mapping, calculate an average on the tangent space, and then perform an exponential mapping to retract this average back onto the manifold. This exponential mapping, due to its dependency on the tangent space, also calls for parallel transport during the local updates when there are correction terms. To overcome this difficulty, we use a projection operator $\\mathcal{P}_{\\mathcal{M}}$ defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{M}}(x)\\in\\underset{u\\in\\mathcal{M}}{\\mathrm{argmin}}\\;\\|x-u\\|^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "to ensure the feasibility of manifold constraints. The projection operator $\\mathcal{P}_{\\mathcal{M}}$ can be explicitly calculated for many common submanifolds, as discussed in [35]. For the Stiefel manifold, the closedform expression for ${\\mathcal{P}}_{{\\mathcal{M}}}(x)$ of a given matrix $x$ with full column rank is $\\mathcal{P}_{\\mathcal{M}}(x)=x(x^{T}x)^{-1/2}$ ; see [35, Proposition 7]. It is worth noting that $\\mathcal{P}_{\\mathcal{M}}$ can be regarded as a special retraction operator when restricted to the tangent space [35]. However, unlike a typical retraction operator, its domain is $\\mathbb{R}^{d\\times k}$ , not just the tangent space, which enables a more practical averaging operation across clients in ", "page_idx": 3}, {"type": "text", "text": "FL. Despite these advantageous properties, the nonconvex nature of $\\mathcal{M}$ means that $\\mathcal{P}_{\\mathcal{M}}(x)$ may be set-valued and non-Lipschitz, making the use and analysis of $\\mathcal{P}_{\\mathcal{M}}$ in the FL setting highly nontrivial. To tackle this, we introduce the concept of proximal smoothness that refers to a property of a closed set, including $\\mathcal{M}$ , where the projection becomes a singleton when the point is sufficiently close to the set. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.2 $\\hat{\\gamma}$ -proximal smoothness of $\\mathcal{M}$ ). For any $\\hat{\\gamma}>0$ , we define the $\\hat{\\gamma}$ -tube around $\\mathcal{M}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nU_{\\mathcal{M}}(\\hat{\\gamma}):=\\{x:\\mathrm{dist}(x,\\mathcal{M})<\\hat{\\gamma}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{dist}(x,\\mathcal{M}):=\\operatorname*{min}_{u\\in\\mathcal{M}}\\|u-x\\|$ is the Eulidean distance between $x$ and $\\mathcal{M}$ . We say that $\\mathcal{M}$ is $\\hat{\\gamma}$ -proximally smooth if the projection operator $\\mathcal{P}_{\\mathcal{M}}(x)$ is a singleton whenever $x\\in U_{\\mathcal{M}}(\\hat{\\gamma})$ . ", "page_idx": 4}, {"type": "text", "text": "It is worth noting that any compact smooth submanifold $\\mathcal{M}$ embedded in $\\mathbb{R}^{d\\times k}$ is a proximally smooth set [36, 37]. The constant $\\hat{\\gamma}$ can be calculated with the method of supporting principle for proximally smooth sets [38, 39]. For instance, the Stiefel manifold is 1-proximally smooth. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.3. The manifold $\\mathcal{M}$ is assumed to be a compact smooth submanifold embedded in $\\mathbb{R}^{d\\times k}$ , with the Euclidean inner product serving as its Riemannian metric. Moreover, we assume that the proximal smoothness constant of $\\mathcal{M}$ is $2\\gamma$ . ", "page_idx": 4}, {"type": "text", "text": "With Assumption 2.3, we can ensure not only the uniqueness of the projection but also the Lipschitz continuity of the projection operator $\\mathcal{P}_{\\mathcal{M}}$ around $\\mathcal{M}$ , analogous to the non-expansiveness of projections under Euclidean convex constraints. ", "page_idx": 4}, {"type": "text", "text": "Lipschitz continuity of $\\mathcal{P}_{\\mathcal{M}}$ . Define $\\overline{{U}}_{\\mathcal{M}}(\\gamma):=\\{x:\\mathrm{dist}(x,\\mathcal{M})\\leq\\gamma\\}$ as the closure of $U_{\\mathcal{M}}(\\gamma)$ . Following the proof in [36, Theorem 4.8], for a $2\\gamma$ -proximally smooth $\\mathcal{M}$ , the projection operator $\\mathcal{P}_{\\mathcal{M}}$ is 2-Lipschitz continuous over $\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{P}_{\\mathcal{M}}(x)-\\mathcal{P}_{\\mathcal{M}}(y)\\|\\le2\\|x-y\\|,\\ \\forall x,y\\in\\overline{{U}}_{\\mathcal{M}}(\\gamma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Normal inequality. In the normal space $N_{x}\\mathcal{M}$ , we exploit the so-called normal inequality [36, 37]. Following [36], given a $2\\gamma$ -proximally smooth $\\mathcal{M}$ , for any $x\\in{\\mathcal{M}}$ and $v\\in N_{x}\\mathcal{M}$ , it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\langle v,y-x\\right\\rangle\\leq{\\frac{\\|v\\|}{4\\gamma}}\\|y-x\\|^{2},\\quad\\forall y\\in{\\mathcal{M}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, when $x$ and $y$ are close enough, the matrix $y-x$ is approximately in the tangent space, thus being nearly orthogonal to the normal space. ", "page_idx": 4}, {"type": "text", "text": "3 Proposed algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we develop a novel algorithm for nonconvex federated learning on manifolds. The algorithm is inspired by the proximal FL algorithm for strongly convex problems in Euclidean space recently proposed in [24] but includes several non-trivial extensions. These include the use of Riemannian gradients and manifold projection operators and the ability to handle nonconvex loss functions, which call for a different convergence analysis. ", "page_idx": 4}, {"type": "text", "text": "3.1 Algorithm description ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The per-client implementation of our algorithm is detailed in Algorithm 1. Similarly to the wellknown FedAvg, it operates in a federated learning setting with one server and $n$ clients. Each client $i$ engages in $\\tau$ steps of local updates before updating the server. We use $r$ as the index of communication rounds and $t$ as the index of local updates. ", "page_idx": 4}, {"type": "text", "text": "At any communication round $r$ , client $i$ downloads the global model $x^{r}$ from the server and computes $\\mathcal{P}_{\\mathcal{M}}(\\dot{\\boldsymbol{x}}^{r})$ . Each client $i$ updates two local variables, $\\hat{z}_{i,t}^{r}$ and $z_{i,t}^{r}$ , where $\\hat{z}_{i,t}^{r}$ aggregates the Riemannian gradients from local updates, and $z_{i,t}^{r}=\\mathcal{P}_{\\mathcal{M}}(\\hat{z}_{i,t}^{r})$ ensures that Riemannian gradients can be computed at points on $\\mathcal{M}$ . The update of $\\hat{z}_{i,t}^{r}$ is given in Line 8, where $B_{i,t}^{r}$ is a mini-batch dataset and $\\boldsymbol{c}_{i}^{r}$ is a correction term to eliminate client drift. After $\\tau$ local updates, client $i$ sends $\\hat{z}_{i,\\tau}^{r}$ to the server. ", "page_idx": 4}, {"type": "text", "text": "The server receives all $\\hat{z}_{i,\\tau}^{r}$ , computes their average to form the global model $x^{r+1}$ following Line 13, and broadcasts $x^{r+1}$ to each client $i$ that uses ${\\boldsymbol{x}}^{r+1}$ to locally construct the correction term $c_{i}^{r+1}$ . ", "page_idx": 4}, {"type": "text", "text": "1: Input: R, \u03c4, \u03b7, $\\eta_{g}$ , $\\tilde{\\eta}=\\eta\\eta_{g}\\tau,x^{1}$ , and $c_{i}^{1}=0$ for all $i\\in[n]$   \n2: for $r=1,2,\\ldots,R$ do   \n3: Client $i$   \n4: Set $\\widehat{z}_{i,0}^{r}=\\mathcal{P}_{\\mathcal{M}}(x^{r})$ and $z_{i,0}^{r}=\\mathcal{P}_{\\mathcal{M}}(x^{r})$   \n5: for $t=0,1,\\dots,\\tau-1$ do   \n6: Sample a mini-batch dataset $B_{i,t}^{r}\\subseteq\\ensuremath{\\mathcal{D}}_{i}$ with $|B_{i,t}^{r}|=b$   \n7: Update grad $\\begin{array}{r}{f_{i}(z_{i,t}^{r};B_{i,t}^{r})=\\frac{1}{b}\\sum_{\\mathcal{D}_{i l}\\in\\mathcal{B}_{i,t}^{r}}}\\end{array}$ $\\operatorname{grad}f_{i l}\\big(z_{i,t}^{r};{\\mathcal{D}}_{i l}\\big)$   \n8: Update $\\widehat{z}_{i,t+1}^{r}=\\widehat{z}_{i,t}^{r}-\\eta\\left(\\mathrm{grad}f_{i}(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r})+c_{i}^{r}\\right)$   \n9: Update $\\boldsymbol{z}_{i,t+1}^{r}=\\mathcal{P}_{\\mathcal{M}}\\left(\\widehat{\\boldsymbol{z}}_{i,t+1}^{r}\\right)$   \n10: end for   \n11: Send $\\widehat{z}_{i,\\tau}^{r}$ to the server   \n12: Serv e r   \n13: Update $\\begin{array}{r}{x^{r+1}=\\mathcal{P}_{\\mathcal{M}}(x^{r})+\\eta_{g}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{z}_{i,\\tau}^{r}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\right)}\\end{array}$   \n14: Broadcast $x^{r+1}$ to all the clients   \n15: Client $i$   \n16: Receive ${\\boldsymbol{x}}^{r+1}$   \n17: Update $\\begin{array}{r}{c_{i}^{\\bar{r}+1}=\\frac{1}{\\eta_{g}\\eta\\tau}(\\mathcal{P}_{\\mathcal{M}}(x^{r})-x^{r+1})-\\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1}\\mathrm{grad}f_{i}(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r})}\\end{array}$   \n18: end for   \n19: Output: $\\mathcal{P}_{\\mathcal{M}}(x^{R+1})$ ", "page_idx": 5}, {"type": "text", "text": "In the proposed algorithm, each client $i$ downloads $x^{r}$ at the start of local updates and uploads $\\hat{z}_{i,\\tau}^{r}$ at the end of the local updates. Therefore, each communication round involves each client and the server exchanging only a single $d\\times k$ matrix. ", "page_idx": 5}, {"type": "text", "text": "3.2 Algorithm intuition and innovations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To better understand the proposed algorithm, we present its equivalent and more compact form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\widehat{\\mathbf{z}}_{t+1}^{r}=\\widehat{\\mathbf{z}}_{t}^{r}-\\eta\\Big(\\mathrm{gradf}\\left(\\mathbf{z}_{t}^{r};\\boldsymbol{B}_{t}^{r}\\right)+\\displaystyle\\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1}\\overline{{\\mathrm{gradf}}}\\left(\\mathbf{z}_{t}^{r-1};\\boldsymbol{B}_{t}^{r-1}\\right)-\\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}\\left(\\mathbf{z}_{t}^{r-1};\\boldsymbol{B}_{t}^{r-1}\\right)\\right),\\right.}\\\\ &{\\left.\\!\\!\\!\\!\\!\\!\\!\\mathbf{z}_{t+1}^{r}=\\mathcal{P}_{M}\\left(\\widehat{\\mathbf{z}}_{t+1}^{r}\\right),}\\\\ &{\\left.\\!\\!\\!\\!\\!\\!\\mathbf{x}^{r+1}=\\mathcal{P}_{M}(\\mathbf{x}^{r})-\\eta_{g}\\eta\\sum_{t=0}^{\\tau-1}\\overline{{\\mathrm{gradf}}}\\left(\\mathbf{z}_{t}^{r};\\boldsymbol{B}_{t}^{r}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For the initialization of correction term, we set $\\mathrm{grad}f_{i}\\left(z_{i,t}^{0};B_{i,t}^{0}\\right)\\;=\\;0$ for all $t$ and $i$ so that $\\scriptstyle{\\frac{1}{\\tau}}\\sum_{t=0}^{\\tau-1}$ gradf $\\left(\\mathbf{z}_{t}^{0};B_{t}^{0}\\right)\\mathrm{~-~}\\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1}$ gradf $\\left(\\mathbf{z}_{t}^{0};\\boldsymbol{B}_{t}^{0}\\right)\\;=\\;0$ , which coincides with the initialization $c_{i}^{1}=0$ in Algorithm 1. The equivalence between Algorithm 1 and (5) can be proved following the similar derivations in [24] and is therefore omitted. ", "page_idx": 5}, {"type": "text", "text": "With (5), we highlight the key properties and innovations of the proposed algorithm. ", "page_idx": 5}, {"type": "text", "text": "1) Recovery of the centralized algorithm in special cases. Substituting the definitions of $\\mathbf{x}$ , gradf $(\\mathbf{z}_{t}^{r};{\\mathcal{B}}_{t}^{r})$ , and $\\tilde{\\eta}$ into the last step in (5), we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{M}}(x^{r+1})=\\mathcal{P}_{\\mathcal{M}}\\Big(\\mathcal{P}_{\\mathcal{M}}(x^{r})-\\tilde{\\eta}\\frac{1}{n\\tau}\\sum_{i=1}^{n}\\sum_{t=0}^{\\tau-1}\\big(\\mathrm{grad}f_{i}\\left(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r}\\right)\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thanks to the introduction of the variable $\\hat{z}_{i,t}^{r}$ during the local updates for each client $i$ in Algorithm 1, the server after averaging $\\hat{z}_{i,\\tau}^{r}$ obtains an accumulation of $\\tau$ local Riemannian gradients across local updates and an average of the local Riemannian gradients across all clients. In the special case where $\\tau=1$ and $b=m_{i}$ , i.e., with the local full Riemannian gradient for each client $i$ , the update of (6) recovers the centralized projected Riemannian gradient descent (C-PRGD) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{x}^{r+1}:=\\mathcal{P}_{\\mathcal{M}}\\left(\\mathcal{P}_{\\mathcal{M}}(x^{r})-\\tilde{\\eta}\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In our analysis, we will compare the sequence $\\mathcal{P}_{\\mathcal{M}}(x^{r+1})$ generated by our algorithm with the virtual iterate $\\tilde{x}^{r+1}$ to establish the convergence of our algorithm. ", "page_idx": 6}, {"type": "text", "text": "2) Feasibility of all iterates at a low computational cost. Our algorithm uses $\\mathcal{P}_{\\mathcal{M}}$ to obtain feasible solutions on the manifold, which is computationally more efficient than the commonly used exponential mapping. In fact, since the exponential mapping relies on a point on the manifold and the tangent space at that point, it cannot be directly used in our algorithm. In the local updates, it is difficult to perform exponential mapping on $\\hat{\\mathbf{z}}_{t+1}^{r}$ because $\\hat{\\mathbf{z}}_{t}^{r}$ is not on the manifold; see the first step in (5). As shown in [24], $\\hat{\\mathbf{z}}_{t+1}^{r}$ is essential for the server to obtain aggregated Riemannian gradients from $n$ clients after $\\tau$ local updates. Moreover, at the server, although $\\mathcal{P}_{\\mathcal{M}}(\\mathbf{x}^{r})$ is on the manifold, the aggregated direction does not lie in the tangent space at $\\mathcal{P}_{\\mathcal{M}}(\\mathbf{x}^{r})$ . The algorithm suggested in [16] uses an exponential mapping to fuse local models. It needs to map the local models to a tangent space using the inverse exponential mapping and then retract the result back to the manifold, which is computationally expensive. Our use of $\\mathcal{P}_{\\mathcal{M}}$ on a point in the Euclidean space close to the manifold avoids these high computational costs, but creates new challenges for the analysis. ", "page_idx": 6}, {"type": "text", "text": "3) Overcoming client drift. Inspired by [24], we use a correction term $\\boldsymbol{c}_{i}^{r}$ to address client drift. According to the first step of (5), the correction employs the idea of \u201cvariance reduction\u201d, which involves replacing the old local Riemannian gradient $\\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1}$ gradf $\\left(\\mathbf{z}_{t}^{r-1};B_{t}^{r-1}\\right)$ with the new one gradf $(\\mathbf{z}_{t}^{r};{\\mathcal{B}}_{t}^{r})$ in the average of all client Riemannian gradients $\\begin{array}{r}{\\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1}\\overline{{\\operatorname{gradf}}}\\left(\\mathbf{z}_{t}^{r-1};B_{t}^{r-1}\\right)}\\end{array}$ , where the \u201cvariance\u201d refers to the differences in Riemannian gradients among clients caused by data heterogeneity. Compared to [16], our correction improves communication and computation. The correction approach in [16] necessitates extra transmissions of local Riemannian gradients, while our correction term can be locally generated, leading to a significantly reduced communication overhead. Furthermore, [16] employs parallel transport to position the correction term with a specific tangent space for the exponential mapping to ensure local model feasibility. Our approach, which utilizes $\\mathcal{P}_{\\mathcal{M}}$ , eliminates the need for parallel transport and reduces the computations per iteration even further. ", "page_idx": 6}, {"type": "text", "text": "4 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we analyze the convergence of the proposed Algorithm 1. Throughout the paper, we make the following assumptions, which are common in manifold optimization. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1. Each $f_{i l}(x;\\mathcal{D}_{i l}):\\mathbb{R}^{d\\times k}\\mapsto\\mathbb{R}$ has $\\hat{L}$ -Lipschitz continuous gradient $\\nabla f_{i l}(x;\\mathcal{D}_{i l})$ on the convex hull of $\\mathcal{M}$ , denoted by $\\operatorname{conv}(\\mathcal{M})$ , i.e., for any $x,y\\in\\mathrm{conv}(\\mathcal{M})$ , it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f_{i l}(x;\\mathcal{D}_{i l})-\\nabla f_{i l}(y;\\mathcal{D}_{i l})\\|\\leq\\hat{L}\\|x-y\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With the compactness of $\\mathcal{M}$ , there exists a constant $D_{f}~>~0$ such that the Euclidean gradient $\\nabla f_{i l}(x;\\mathcal{D}_{i l})$ of $f_{i l}$ is bounded by $D_{f}$ , i.e., $\\begin{array}{r}{\\operatorname*{max}_{i,l,x\\in\\mathcal{M}}\\|\\dot{\\nabla}f_{i l}(x;\\mathcal{D}_{i l})\\|\\le D_{f}}\\end{array}$ . It then follows from [28, Lemma 4.2] that there exists a constant $\\hat{L}\\le L<\\infty$ such that for any $x,y\\in M$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i l}(y;\\mathcal{D}_{i l})\\leq f_{i l}(x;\\mathcal{D}_{i l})+\\langle\\mathrm{grad}f_{i l}(x;\\mathcal{D}_{i l}),y-x\\rangle+\\displaystyle\\frac{L}{2}\\|x-y\\|^{2},}\\\\ &{\\|\\mathrm{grad}f_{i l}(x;\\mathcal{D}_{i l})-\\mathrm{grad}f_{i l}(y;\\mathcal{D}_{i l})\\|\\leq L\\|x-y\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To address the stochasticity introduced by the random sampling $B_{i,t}^{r}$ , we define $\\mathcal{F}_{t}^{r}$ as the $\\sigma$ -algebra generated by the set $\\{\\mathcal{B}_{i,\\tilde{t}}^{\\tilde{r}}\\mid i\\,\\in\\,[n],\\tilde{r}\\,\\in\\,[r],\\tilde{t}\\,\\in\\,[t-1]\\}$ and make the following assumptions regarding the stochastic Riemannian gradients, similar to [40, Assumption 2]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2. Each stochastic Riemannian gradient grad $f_{i}(z_{i,t}^{r};B_{i,t}^{r})$ in Algorithm $^{\\,l}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{grad}f_{i}(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r})|\\mathcal{F}_{t}^{r}\\right]=\\mathrm{grad}f_{i}(z_{i,t}^{r}),}\\\\ &{\\mathbb{E}\\left[\\left\\|\\mathrm{grad}f_{i}(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r})-\\mathrm{grad}f_{i}(z_{i,t}^{r})\\right\\|^{2}|\\mathcal{F}_{t}^{r}\\right]\\le\\frac{\\sigma^{2}}{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Considering the nonconvexity of $f$ and the manifold constraints, we characterize the first-order optimality of (1). A point $x^{\\star}$ is defined as a first-order optimal solution of (1) if $x^{\\star}\\in\\mathcal{M}$ and $\\operatorname{grad}f(x^{\\star})=0$ . We employ the norm of $\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(\\boldsymbol{x}^{r}))$ as a suboptimality metric, defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(x^{r})):=\\frac{1}{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(x^{r})-\\tilde{x}^{r+1}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and $\\tilde{x}^{r+1}$ defined in (7) is used only for analytical purposes. In optimization on Euclidean space such that $\\mathcal{M}=\\mathbb{R}^{d\\times k}$ , the quantity $\\dot{\\mathcal{G}}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(\\dot{x^{r}}))$ serves as a widely accepted metric to assess firstorder optimality for nonconvex composite problems [41]. In optimization on manifold, we have $\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(x^{r}))=0$ if and only if grad $f(\\mathcal{P}_{\\mathcal{M}}(\\boldsymbol{x}^{r}))=0$ for any $\\tilde{\\eta}>0$ . Moreover, for a suitable $\\tilde{\\eta}$ , the use of Riemannian gradients in the update of $\\tilde{x}^{r+1}$ helps us to establish a quasi-isometric property, specifically that $1/\\bar{2}\\|\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}\\bar{)})\\|\\,\\leq\\,\\|\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(\\bar{x}^{r}))\\|\\,\\leq\\,2\\|\\mathrm{grad}f(\\bar{\\mathcal{P}}_{\\mathcal{M}}(x^{r}))\\|$ ; see Lemmas A.1 and A.2. With $\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(\\boldsymbol{x}^{r}))$ , we have the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3. Under Assumptions 2.3, 4.1, and 4.2, if the step sizes satisfy ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\eta}:=\\eta_{g}\\eta\\tau\\leq\\operatorname*{min}\\left\\{\\frac{1}{24M L},\\frac{\\gamma}{6D_{f}},\\frac{1}{D_{f}L\\mathcal{P}}\\right\\},\\;\\eta_{g}=\\sqrt{n},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r l r}{M}&{{}=}&{\\mathrm{\\max}}\\end{array}$ x $\\{\\mathrm{diam}(\\mathcal{M})/\\gamma,2\\}$ , $\\begin{array}{r l r}{\\mathrm{diam}(\\mathcal{M})}&{{}=}&{\\operatorname*{max}_{x,y\\in\\mathcal{M}}\\lVert x\\ -\\ y\\rVert,}\\end{array}$ , $\\begin{array}{r l r l}{D_{f}}&{{}}&{=}&{{}}\\end{array}$ $\\mathrm{max}_{i,l,x\\in\\mathcal{M}}\\left\\|\\nabla f_{i l}(x;\\mathcal{D}_{i l})\\right\\|$ , and $\\begin{array}{r}{L\\mathcal{P}\\;=\\;\\operatorname*{max}_{x\\in\\overline{{U}}_{\\mathcal{M}}(\\gamma)}\\|\\mathcal{D}^{2}\\mathcal{P}_{\\mathcal{M}}(x)\\|}\\end{array}$ , then the sequence $\\mathcal{P}_{\\mathcal{M}}(\\boldsymbol{x}^{r})$ generated by Algorithm $^{\\,I}$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{R}\\sum_{r=1}^{R}\\mathbb{E}\\|\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\|^{2}\\leq\\frac{8\\Omega^{1}}{\\sqrt{n}\\eta\\tau R}+\\frac{64\\sigma^{2}}{n\\tau b},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Omega^{1}>0$ is a constant related to initialization. ", "page_idx": 7}, {"type": "text", "text": "In Theorem 4.3, the first term on the right hand of (12) converges at a sub-linear rate, which is common for constrained nonconvex optimization [41, 42]. The second term is a constant error caused by the variance $\\sigma^{2}$ of stochastic Riemannian gradients. ", "page_idx": 7}, {"type": "text", "text": "Theoretical contributions. The work [16] establishes convergence rates of $\\mathcal{O}(1/R)$ for $\\tau=1$ and $\\mathcal{O}(1/(\\tau R))$ for $\\tau>1$ but only if a single client partici\u221apates in the training per communication round. In contrast, our Theorem 4.3 achieves a rate of $\\bar{\\mathcal{O}}(1/(\\bar{\\sqrt{n}}\\tau R))$ for $\\tau>1$ and full client participation. Our theorem indicates that multiple local updates enable faster convergence, which distinguishes our algorithm from decentralized manifold optimization algorithms [9, 28] that limit clients to do a single local update. The works [43] and [1] study smooth nonconvex FL with heterogeneous data in Euclidean space, relying on the assumptions of bounded second moments [43, Assumption 1] and $B$ - local dissimilarity [1, Definition 3], respectively, to establish convergence. These assumptions imply some similarity between client data. Our main analytical advantages over [43] and [1] are twofold: We do not assume data similarity, and we completely eliminate client drift. This is reflected in our convergence error, which eliminates the error introduced by data heterogeneity. Our convergence analysis relies on several novel techniques. Specifically, we capitalize on the structure of $\\mathcal{M}$ and exploit the proximal smoothness of $\\mathcal{M}$ to guarantee the uniqueness of $\\mathcal{P}_{\\mathcal{M}}$ and Lipschitz continuity of $\\mathcal{P}_{\\mathcal{M}}$ within a tube around $\\mathcal{M}$ . Additionally, we carefully select the step sizes to ensure that the iterates remain close to $\\mathcal{M}$ , thus preserving the established properties throughout the iterations. Last but not least, we select an appropriate first-order optimality metric (see (10)) and jointly consider the properties of $\\mathcal{M}$ and the loss functions to establish some new inequalities for the convergence of this metric, given in Appendix. ", "page_idx": 7}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conduct numerical experiments on two applications on the Stiefel manifold: kPCA and the low rank matrix completion (LRMC). We compare with existing algorithms, including RFedavg, RFedprox, and RFedSVRG. RFedavg and RFedprox are direct extensions of FedAvg [25] and Fedprox [1]. For RFedSVRG, there are no theoretical guarantees when we set $\\tau>1$ and make all clients participate. In all alternative algorithms, the calculations of the exponential mapping, its inverse, and the parallel transport on the Stiefel manifold are needed. The exponential mapping has a closed-form expression but involves a matrix exponential [6], the inverse exponential mapping needs to solve a nonlinear matrix equation [17], and the parallel transport needs to solve a linear differential equation [44], all of which are computationally challenging. In their implementations, approximate versions of these mappings are used [16, 45, 46]. ", "page_idx": 7}, {"type": "text", "text": "kPCA. Consider the kPCA problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\underset{x\\in\\mathrm{St}(d,k)}{\\mathrm{minimize}}\\;f(x)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x),\\quad f_{i}(x)=-\\frac{1}{2}\\mathrm{tr}(x^{T}A_{i}^{T}A_{i}x),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\operatorname{St}(d,k)=\\{x\\in\\mathbb{R}^{d\\times k}\\mid x^{T}x=I_{k}\\}$ denotes the Stiefel manifold, and $A_{i}^{T}A_{i}\\in\\mathbb{R}^{d\\times d}$ is the covariance matrix of the local data $A_{i}\\in\\mathbb{R}^{p\\times d}$ of client $i$ . We conduct experiments where the matrix $A_{i}$ is from the Mnist dataset. The specific experiment settings can be found in Appendix A.4.1. ", "page_idx": 8}, {"type": "image", "img_path": "uO53206oLJ/tmp/41e04800c1db1d03c747b0083c6ca07ffcf9f32b3f2fc07c09e3f53922feb1d1.jpg", "img_caption": ["Figure 1: kPCA problem with Mnist dataset: Comparison on $\\left\\|\\operatorname{grad}f(x^{r})\\right\\|$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In the first set of experiments, we compare our algorithm with RFedavg, RFedprox, and RFedSVRG. RFedSVRG requires each client to transmit two $d\\times k$ matrices per communication round, while our algorithm only transmits a single matrix. We use communication quantity to count the total number of $d\\times k$ matrices that per client transmits to the server. We use the local full gradient $\\nabla f_{i}$ to mitigate the effects of stochastic gradient noise. In Figs. 1, we set $\\tau=10$ and $\\eta=1/\\beta$ for all algorithms, where $\\beta$ is the square of the largest singular value of $\\mathrm{col}\\{A_{i}\\}_{i=1}^{n}$ . We set $\\eta_{g}\\,=\\,1$ to facilitate comparison with other algorithms. As noted below (40) in the Appendix, all analytical results leading up to (40) remain valid for $\\eta_{g}=1$ . It can be observed that RFedavg and RFedprox face the issue of client drift and have low accuracy. Both RFedSVRG and our algorithm overcome the client drift, but our algorithm, though being similar in terms of communication rounds, is much faster in terms of communication quantity and run time. ", "page_idx": 8}, {"type": "text", "text": "In the second set of experiments, we test the impact of $\\tau$ . For all the algorithms, we set the step size $\\eta=1/\\beta$ and $\\tau\\in\\{10,15,20\\}$ . For our algorithm, we set $\\eta_{g}=1$ . The experiment results are shown in Figs. 2. For all values of $\\tau$ , our algorithm achieves better convergence and requires less communication quantity. ", "page_idx": 8}, {"type": "image", "img_path": "uO53206oLJ/tmp/5fe256f002a05cc3b0a19b47ae37d7cfc98038373c51eab4a4da9e63ffb8de9f.jpg", "img_caption": ["Figure 2: kPCA with Mnist dataset: The impacts of $\\tau$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In addition, we test the impact of stochastic Riemannian gradients with different batch sizes. We set $\\eta=1/(20\\beta)$ . As shown in Figs. 3, our algorithm converges to a neighborhood due to the sampling noise and larger batch size leads to faster convergence. Additional experimental results can be found in Appendix A.4.1. ", "page_idx": 8}, {"type": "text", "text": "LRMC. LRMC aims to recover a low-rank matrix $A~\\in~\\mathbb{R}^{d\\times T}$ from its partial observations. Let $\\Omega$ be the set of indices of known entries in $A$ , the rank- $k$ LRMC problem can be written as $\\mathrm{minimize}_{X\\in\\mathrm{St}(d,k),V\\in\\mathbb{R}^{k\\times T}}\\ \\frac{1}{2}\\|\\mathcal{P}_{\\Omega}(X V-A)\\|^{2}$ , where the projection operator $\\mathcal{P}_{\\Omega}$ is defined in an entry-wise manner with $(\\mathcal{P}_{\\Omega}(A))_{l_{1}l_{2}}=A_{l_{1}l_{2}}$ if $(l_{1},l_{2})\\in\\Omega$ and 0 otherwise. In terms of the FL setting, we consider the case where the observed data matrix ${\\mathcal{P}}_{\\Omega}(A)$ is equally divided into $n$ clients by columns, denoted by $A_{1},\\ldots,A_{n}$ . Then, the FL LRMC problem is ", "page_idx": 8}, {"type": "image", "img_path": "uO53206oLJ/tmp/986e613e46231f911f42fb93611f266a2b2e1360f0fe988c3339b620bed4abf0.jpg", "img_caption": ["Figure 3: kPCA problem with Mnist dataset: The impacts of stochastic Riemannian gradients. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "uO53206oLJ/tmp/ffd7634500ed62b91e8d134e6a68c2f8ffd45f4319c40ee8427f438ba7d87430.jpg", "img_caption": ["Figure 4: LRMC: Comparison on $\\left\\|\\operatorname{grad}f(x^{r})\\right\\|$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{X\\in\\mathrm{St}(d,k)}~~\\frac{1}{2n}\\sum_{i=1}^{n}\\|\\mathcal{P}_{\\Omega_{i}}(X V_{i}(X)-A_{i})\\|^{2},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\Omega_{i}$ is the subset corresponding to client $i$ in $\\Omega$ and $V_{i}(X):=\\mathrm{argmin}_{V}\\|\\mathcal{P}_{\\Omega_{i}}(X V-A_{i})\\|$ . In the experiments, we set $T=1000$ , $d=100$ , $k=2$ , $n=10$ , and use the local full gradients. The other settings can be found in Appendix A.4.2. ", "page_idx": 9}, {"type": "text", "text": "The numerical comparisons with RFedavg, RFedprox, and RFedSVRG are presented in Figs. 4. Our algorithm and RFedSVRG achieve similar convergence for communication rounds, but our algorithm converges faster than RFedSVRG in terms of communication quantity and run time. Additional experimental results can be found in Appendix A.4.2. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper addresses the challenges of FL on compact smooth submanifolds. We introduce a novel algorithm that enables full client participation, local updates, and heterogeneous data distributions. By leveraging stochastic Riemannian gradients and a manifold projection operator, our method enhances computational and communication efficiency while mitigating client drift. By exploiting the manifold structure and properties of the loss function, we prove sub-linear convergence to a neighborhood of a first-order stationary point. Numerical experiments show a superior performance of our algorithm in terms of computational and communication costs compared to the state-of-the-art. ", "page_idx": 9}, {"type": "text", "text": "Our paper motivates several questions for further investigation. First, the absence of closed-form solutions for the projection operator $\\mathcal{P}_{\\mathcal{M}}$ for certain manifolds necessitates exploring methods to calculate projections approximately. Additionally, our step-size selection relies on the proximal smoothness constant $\\gamma$ , underscoring the need for estimating $\\gamma$ either off-line for specific manifolds or adaptively on-line. Furthermore, designing algorithms for partial participation and devising corresponding client-drift correction mechanisms require further investigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research is supported in part by the Hong Kong Research Grant Council (RGC) through the General Research Fund (GRF) project CUHK 14205421, in part by the Knut and Alice Wallenberg Foundation through grant 2022.0050, and in part by the Swedish Research Council through grant 2023-05538. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429\u2013450, 2020.   \n[2] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[3] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009.   \n[4] Jiang Hu, Xin Liu, Zaiwen Wen, and Yaxiang Yuan. A brief introduction to manifold optimization. Journal of the Operations Research Society of China, 8:199\u2013248, 2020.   \n[5] Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University Press, 2023.   \n[6] Shixiang Chen, Shiqian Ma, Anthony Man-Cho So, and Tong Zhang. Proximal gradient method for nonsmooth optimization over the stiefel manifold. SIAM Journal on Optimization, 30(1):210\u2013239, 2020.   \n[7] Lei Wang and Xin Liu. Decentralized optimization over the Stiefel manifold by an approximate augmented lagrangian function. IEEE Transactions on Signal Processing, 70:3029\u20133041, 2022.   \n[8] Haishan Ye and Tong Zhang. DeEPCA: Decentralized exact PCA with linear convergence rate. Journal of Machine Learning Research, 22(238):1\u201327, 2021.   \n[9] Shixiang Chen, Alfredo Garcia, Mingyi Hong, and Shahin Shahrampour. Decentralized Rriemannian gradient descent on the Stiefel manifold. In International Conference on Machine Learning, pages 1594\u20131605. PMLR, 2021.   \n[10] Nicolas Boumal and P-A Absil. Low-rank matrix completion via preconditioned optimization on the Grassmann manifold. Linear Algebra and its Applications, 475:200\u2013239, 2015.   \n[11] Hiroyuki Kasai, Pratik Jawanpuria, and Bamdev Mishra. Riemannian adaptive stochastic gradient algorithms on matrix manifolds. In International Conference on Machine Learning, pages 3262\u20133271. PMLR, 2019.   \n[12] Nilesh Tripuraneni, Chi Jin, and Michael Jordan. Provable meta-learning of linear representations. In International Conference on Machine Learning, pages 10434\u201310443. PMLR, 2021.   \n[13] Nikolaos Dimitriadis, Pascal Frossard, and Fran\u00e7ois Fleuret. Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models. In International Conference on Machine Learning, pages 8015\u20138052. PMLR, 2023.   \n[14] German Magai. Deep neural networks architectures from the perspective of manifold learning. In 2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI), pages 1021\u20131031. IEEE, 2023.   \n[15] Thomas Yerxa, Yilun Kuang, Eero Simoncelli, and SueYeon Chung. Learning efficient coding of natural images with maximum manifold capacity representations. Advances in Neural Information Processing Systems, 36:24103\u201324128, 2023.   \n[16] Jiaxiang Li and Shiqian Ma. Federated learning on Riemannian manifolds. arXiv preprint arXiv:2206.05668, 2022.   \n[17] Ralf Zimmermann and Knut Huper. Computing the Riemannian logarithm on the Stiefel manifold: Metrics, methods, and performance. SIAM Journal on Matrix Analysis and Applications, 43(2):953\u2013980, 2022.   \n[18] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of FedAvg on non-iid data. In International Conference on Learning Representations, 2019.   \n[19] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132\u20135143, 2020.   \n[20] Honglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated composite optimization. In International Conference on Machine Learning, pages 12253\u201312266, 2021.   \n[21] Yajie Bao, Michael Crawshaw, Shan Luo, and Mingrui Liu. Fast composite optimization and statistical recovery in federated learning. In International Conference on Machine Learning, pages 1508\u20131536, 2022.   \n[22] Quoc Tran Dinh, Nhan H Pham, Dzung Phan, and Lam Nguyen. FedDR\u2013randomized DouglasRachford splitting algorithms for nonconvex federated composite optimization. Advances in Neural Information Processing Systems, 34:30326\u201330338, 2021.   \n[23] Han Wang, Siddartha Marella, and James Anderson. FedADMM: A federated primal-dual algorithm allowing partial participation. In 2022 IEEE 61st Conference on Decision and Control (CDC), pages 287\u2013294, 2022.   \n[24] Jiaojiao Zhang, Jiang Hu, and Mikael Johansson. Composite federated learning with heterogeneous data. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8946\u20138950. IEEE, 2024.   \n[25] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273\u20131282, 2017.   \n[26] Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning. arXiv preprint arXiv:2008.03606, 2020.   \n[27] Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients. Advances in Neural Information Processing Systems, 34:14606\u201314619, 2021.   \n[28] Kangkang Deng and Jiang Hu. Decentralized projected Riemannian gradient method for smooth optimization on compact submanifolds. arXiv preprint arXiv:2304.08241, 2023.   \n[29] Jun Chen, Haishan Ye, Mengmeng Wang, Tianxin Huang, Guang Dai, Ivor Tsang, and Yong Liu. Decentralized Riemannian conjugate gradient method on the Stiefel manifold. In The Twelfth International Conference on Learning Representations, 2024.   \n[30] Zhenwei Huang, Wen Huang, Pratik Jawanpuria, and Bamdev Mishra. Federated learning on Riemannian manifolds with differential privacy. arXiv preprint arXiv:2404.10029, 2024.   \n[31] Tung-Anh Nguyen, Jiayu He, Long Tan Le, Wei Bao, and Nguyen H Tran. Federated PCA on Grassmann manifold for anomaly detection in iot networks. In IEEE INFOCOM 2023-IEEE Conference on Computer Communications, pages 1\u201310. IEEE, 2023.   \n[32] Andreas Grammenos, Rodrigo Mendoza Smith, Jon Crowcroft, and Cecilia Mascolo. Federated principal component analysis. Advances in Neural Information Processing Systems, 33:6453\u2013 6464, 2020.   \n[33] Long-Kai Huang and Sinno Pan. Communication-efficient distributed pca by riemannian optimization. In International Conference on Machine Learning, pages 4465\u20134474. PMLR, 2020.   \n[34] Foivos Alimisis, Peter Davies, Bart Vandereycken, and Dan Alistarh. Distributed principal component analysis with limited communication. Advances in Neural Information Processing Systems, 34:2823\u20132834, 2021.   \n[35] P-A Absil and J\u00e9r\u00f4me Malick. Projection-like retractions on matrix manifolds. SIAM Journal on Optimization, 22(1):135\u2013158, 2012.   \n[36] Francis H Clarke, Ronald J Stern, and Peter R Wolenski. Proximal smoothness and the lower-C2 property. Journal of Convex Analysis, 2(1-2):117\u2013144, 1995.   \n[37] Damek Davis, Dmitriy Drusvyatskiy, and Zhan Shi. Stochastic optimization over proximally smooth sets. arXiv preprint arXiv:2002.06309, 2020.   \n[38] MV Balashov. Nonconvex optimization. Control theory (additional chapters): tutorial. Moscow: Lenand, 2019.   \n[39] MV Balashov and AA Tremba. Error bound conditions and convergence of optimization methods on smooth and proximally smooth manifolds. Optimization, 71(3):711\u2013735, 2022.   \n[40] Pan Zhou, Xiao-Tong Yuan, and Jiashi Feng. Faster first-order methods for stochastic nonconvex optimization on Riemannian manifolds. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 138\u2013147. PMLR, 2019.   \n[41] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. Advances in Neural Information Processing Systems, 29, 2016.   \n[42] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for nonconvex optimization. Advances in neural information processing systems, 31, 2018.   \n[43] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 5693\u20135700, 2019.   \n[44] Alan Edelman, Tom\u00e1s A Arias, and Steven T Smith. The geometry of algorithms with orthogonality constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303\u2013353, 1998.   \n[45] Nicolas Boumal, Bamdev Mishra, P-A Absil, and Rodolphe Sepulchre. Manopt, a Matlab toolbox for optimization on manifolds. The Journal of Machine Learning Research, 15(1):1455\u2013 1459, 2014.   \n[46] James Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A python toolbox for optimization on manifolds using automatic differentiation. Journal of Machine Learning Research, 17(137):1\u20135, 2016.   \n[47] Robert L Foote. Regularity of the distance function. Proceedings of the American Mathematical Society, 92(1):153\u2013155, 1984.   \n[48] Maxence Noble, Aur\u00e9lien Bellet, and Aymeric Dieuleveut. Differentially private federated learning on heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 10110\u201310145, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We analyze the proposed algorithm using the Lyapunov function $\\Omega^{r}$ defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Omega^{r}:=f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\!-\\!f^{\\star}+\\frac{1}{n\\widetilde{\\eta}}\\|\\mathbf{A}^{r}-\\overline{{\\mathbf{A}}}^{r}\\|^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $f^{\\star}$ is the optimal value of problem (1) and we define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{A}^{r}:=\\eta(\\tau\\mathrm{gradf}(\\mathscr{P}_{\\mathcal{M}}(\\mathbf{x}^{r}))+\\sum_{t=0}^{\\tau-1}\\overline{{\\mathrm{gradf}}}(\\mathbf{z}_{t}^{r-1};\\mathcal{B}_{t}^{r-1})-\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}(\\mathbf{z}_{t}^{r-1};\\mathcal{B}_{t}^{r-1}))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and $\\begin{array}{r}{\\overline{{\\pmb{\\Lambda}}}^{r}:=\\mathrm{col}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}{\\Lambda_{i}^{r}}\\right\\}_{i=1}^{n}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "The Lyapunov function consists of two parts: to bound the suboptimality of the global model $\\mathcal{P}_{\\mathcal{M}}(\\boldsymbol{x}^{r})$ and the reduction of \u201cvariance\u201d among clients, respectively. ", "page_idx": 13}, {"type": "text", "text": "A.2 Preliminary lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let us start with the following lemma on the global-like Lipschitz-continuity property of $\\mathcal{P}_{\\mathcal{M}}$ ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. There exists a constant $M>0$ such that for any $x\\in{\\mathcal{M}}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\mathcal{P}_{\\mathcal{M}}(x+u)-x\\|\\le M\\|u\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Let us consider two cases: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\mathcal{P}_{\\mathcal{M}}(x+u)-x\\|\\leq\\mathrm{diam}(\\mathcal{M})\\leq\\frac{\\mathrm{diam}(\\mathcal{M})}{\\gamma}\\|u\\|,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathrm{diam}(\\mathcal{M}):=\\operatorname*{max}_{x,y\\in\\mathcal{M}}\\|x-y\\|$ is the diameter of $\\mathcal{M}$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 $\\|u\\|\\leq\\gamma$ : By the 2-Lipschitz continuity of $\\mathcal{P}_{\\mathcal{M}}$ over $\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ in (3), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\mathcal{P}_{\\mathcal{M}}(x+u)-x\\|\\leq2\\|u\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Setting $\\begin{array}{r}{M:=\\operatorname*{max}\\left\\lbrace\\frac{\\mathrm{diam}(\\mathcal{M})}{\\gamma},2\\right\\rbrace}\\end{array}$ we complete the proof. ", "page_idx": 13}, {"type": "text", "text": "In the following, we show the reasonableness of the suboptimality metric $\\lVert\\mathcal{G}_{\\tilde{\\eta}}(\\cdot)\\rVert$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. Consider $\\mathcal{G}_{\\tilde{\\eta}}(\\cdot)$ defined by (10). Then, for any $x\\in{\\mathcal{M}}$ , it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{grad}f(x)=0\\quad{\\mathrm{if~and~only~if}}\\quad{\\mathcal{G}}_{\\tilde{\\eta}}(x)=0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In addition, under Assumptions 2.3 and 4.1, if $\\begin{array}{r}{\\tilde{\\eta}\\leq\\operatorname*{min}\\left\\{\\frac{\\gamma}{D_{f}},\\frac{1}{D_{f}L_{\\mathcal{P}}}\\right\\}w i t h\\ L_{\\mathcal{P}}}\\end{array}$ being the smoothness constant of $\\mathcal{D}^{2}\\mathcal{P}_{\\mathcal{M}}(\\cdot)$ over $\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ , it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\mathrm{grad}f(x)\\|\\leq2\\|{\\mathcal{G}}_{\\tilde{\\eta}}(x)\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. If $\\operatorname{grad}f(x)=0$ , it follows directly from the definition of $g_{\\tilde{\\eta}}(\\cdot)$ that $\\mathcal{G}_{\\tilde{\\eta}}(x)=0$ . Conversely, if $\\mathcal{G}_{\\tilde{\\eta}}(x)=0$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nx=\\mathcal{P}_{M}\\left(x-\\tilde{\\eta}\\mathrm{grad}f(x)\\right):=\\underset{y\\in\\mathcal{M}}{\\mathrm{argmin}}\\;\\|y-x+\\tilde{\\eta}\\mathrm{grad}f(x)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It follow from the optimality of $x$ that $0=P_{T_{x},M}(\\tilde{\\eta}\\mathrm{grad}f(x))$ , which implies that grad $f(x)=0$ . ", "page_idx": 13}, {"type": "text", "text": "With [47, Lemma], $\\mathcal{P}_{\\mathcal{M}}(\\cdot)$ is sufficiently smooth over $\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ . Let us define $\\begin{array}{r l}{L_{\\mathcal{P}}}&{{}:=}\\end{array}$ $\\mathrm{max}_{x\\in\\overline{{U}}_{\\mathcal{M}}(\\gamma)}\\left\\|\\mathcal{D}^{2}\\mathcal{P}_{\\mathcal{M}}(x)\\right\\|$ , where $\\mathcal{D}^{2}$ denotes the second-order differential operator. Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\mathcal{G}}_{\\tilde{\\eta}}(x)\\|=\\frac{1}{\\tilde{\\eta}}\\|x-{\\mathcal{P}}_{{\\mathcal{M}}}(x-\\tilde{\\eta}\\mathrm{grad}f(x))\\|}\\\\ {\\ge\\|\\mathrm{grad}f(x)\\|-\\displaystyle\\frac{1}{2}L_{{\\mathcal{P}}}\\tilde{\\eta}\\|\\mathrm{grad}f(x)\\|^{2}}\\\\ {\\displaystyle\\ge\\frac{1}{2}\\|\\mathrm{grad}f(x)\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use $\\begin{array}{r}{\\tilde{\\eta}\\leq\\frac{\\gamma}{D_{f}}}\\end{array}$ in the first inequality and $\\begin{array}{r}{\\tilde{\\eta}\\leq\\frac{1}{L_{\\mathcal{P}}D_{f}}}\\end{array}$ in the second inequality. This gives (16). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "To prove Theorem 4.3, we use the following lemma to establish a recursion on the second term on $\\Lambda^{r}$ in the Lyapunov function. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3. Under Assumptions 2.3, 4.1, and 4.2, if $\\begin{array}{r}{\\dot{\\eta}\\leq\\operatorname*{min}\\bigg\\{\\frac{\\eta_{g}}{16L},\\frac{\\gamma\\eta_{g}}{2D_{f}}\\bigg\\}.}\\end{array}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{n}\\mathbb{E}\\|\\mathbf{A}^{r+1}-\\overline{{\\mathbf{A}}}^{r+1}\\|^{2}-2\\eta^{2}\\tau^{2}L^{2}\\mathbb{E}\\left\\|\\mathcal{P}_{M}(x^{r+1})-\\mathcal{P}_{M}(x^{r})\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(174)^{-1}}\\\\ &{\\leq\\!\\!\\frac{1}{n}4\\eta^{2}\\tau L^{2}\\Big(3n M^{2}\\tau^{3}\\eta^{2}\\|\\mathrm{grad}f(\\mathcal{P}_{M}(x^{r}))\\|^{2}+9\\tau\\mathbb{E}\\|\\mathbf{A}^{r}-\\overline{{\\mathbf{A}}}^{r}\\|^{2}+18n\\tau^{2}\\eta^{2}\\frac{\\sigma^{2}}{b}\\Big)+\\frac{1}{n}4\\eta^{2}n^{2}\\tau^{2}\\frac{\\sigma}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. As a first step, we bound the drift error $\\left\\|z_{i,t+1}^{r}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\right\\|^{2}$ that is caused by the local updates. If $\\tau=1$ , the error is zero since $z_{i,t}^{r}=\\mathcal{P}_{\\mathcal{M}}(x^{r})$ . When $\\tau\\geq2$ , repeating the local updates for $t$ steps and substituting $\\widehat{z}_{i,0}^{r}=\\mathcal{P}_{\\mathcal{M}}(x^{r})$ and $\\boldsymbol{z}_{i,t+1}^{r}=\\mathcal{P}_{\\mathcal{M}}\\big(\\hat{\\boldsymbol{z}}_{i,t+1}^{r}\\big)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert z_{i,t+1}^{r}-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}=\\mathbb{E}\\left\\Vert\\mathcal{P}_{M}\\big(\\mathcal{P}_{M}(x^{r})-\\eta\\sum_{\\ell=0}^{t}\\big(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r};\\mathcal{B}_{i,\\ell}^{r})+c_{i}^{r}\\big)\\big)-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To bound the right-hand side of (18), we compare our algorithm with the exact C-PRGD step given in (7) under the step size $(t+1)\\eta$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{x}_{\\mathrm{C-PRGD}}^{r+1}:=\\mathcal{P}_{\\mathcal{M}}\\left(\\mathcal{P}_{\\mathcal{M}}(x^{r})-(t+1)\\eta\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It follows from (15) that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\tilde{x}_{\\mathrm{C-PRGD}}^{r+1}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\|\\le M\\tau\\eta\\|\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then from (18) we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|z_{i,t+1}^{r}-\\mathcal{P}_{M}(x^{r})\\right\\|^{2}}\\\\ &{=\\mathbb{E}\\Big\\|\\mathcal{P}_{M}\\big(\\mathcal{P}_{M}(x^{r})-\\eta\\displaystyle\\sum_{\\ell=0}^{t}\\big(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r};B_{i,\\ell}^{r})+c_{i}^{r}\\big)\\big)-\\tilde{x}_{\\mathrm{C-PRGD}}^{r+1}+\\tilde{x}_{\\mathrm{C-PRGD}}^{r+1}-\\mathcal{P}_{M}(x^{r})\\Big\\|^{2}}\\\\ &{\\le\\;2\\mathbb{E}\\Big\\|\\mathcal{P}_{M}\\big(\\mathcal{P}_{M}(x^{r})-\\eta\\displaystyle\\sum_{\\ell=0}^{t}\\mathrm{(grad}f_{i}(z_{i,\\ell}^{r};B_{i,\\ell}^{r})+c_{i}^{r}\\big)\\big)-\\tilde{x}_{\\mathrm{C-PRGD}}^{r+1}\\Big\\|^{2}+2M^{2}\\tau^{2}\\eta^{2}\\|\\mathrm{grad}f(\\mathcal{P}_{M}(x^{r}))}\\\\ &{\\overset{,..}{\\le}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use $\\|a+b\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2}$ in the inequality. ", "page_idx": 14}, {"type": "text", "text": "To bound the term (I) on the right hand of (19), from \u03b7\u02dc \u2264 2\u03b3D\u03b7gf and $\\begin{array}{r}{\\operatorname*{max}_{i,l,x\\in\\mathcal{M}}\\|\\nabla f_{i l}(x;\\mathcal{D}_{i l})\\|\\le D_{f},}\\end{array}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\eta\\sum_{\\ell=0}^{t}\\left(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r};B_{i,\\ell}^{r})+c_{i}^{r}\\right)\\right\\|\\leq\\gamma.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, by substituting definition of $\\tilde{x}_{\\mathrm{C-PRGD}}^{r+1}$ , we can invoke the 2-Lipschitz continuity of $\\mathcal{P}_{\\mathcal{M}}$ over $\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ given in (3) and get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathrm{I}\\rangle=2\\mathbb{E}\\big\\|\\mathcal{P}_{\\mathcal{M}}\\big(\\mathcal{P}_{\\mathcal{M}}(x^{r})-\\eta\\displaystyle\\sum_{\\ell=0}^{t}\\big(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r};\\mathcal{B}_{i,\\ell}^{r})+c_{i}^{r}\\big)\\big)}\\\\ &{\\qquad-\\,\\mathcal{P}_{\\mathcal{M}}\\big(\\mathcal{P}_{\\mathcal{M}}(x^{r})-(t+1)\\eta\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\big)\\big\\|^{2}}\\\\ &{\\qquad\\leq4\\mathbb{E}\\Big\\|\\eta\\displaystyle\\sum_{\\ell=0}^{t}\\Big(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r};\\mathcal{B}_{i,\\ell}^{r})+c_{i}^{r}-\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\Big)\\Big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, to bound the right-hand side of (20) we rewrite it in terms of $\\lVert\\mathbf{A}^{r}-\\overline{{\\mathbf{A}}}^{r}\\rVert^{2}$ by substituting the definition of the $c_{i}^{r}$ given in (5) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(1)\\leq4\\mathbb{E}\\bigg\\|\\eta\\sum_{\\ell=0}^{\\ell}\\bigg(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r},\\delta_{i,\\ell}^{r})-\\mathrm{grad}f_{i}(\\mathcal{P}_{M}(x^{r}))}\\bigg.}\\\\ &{\\quad+\\mathrm{grad}f_{i}(\\mathcal{P}_{M}(x^{r}))+\\frac{1}{T}\\sum_{t=0}^{r-1}\\frac{1}{n}\\sum_{i=1}^{n}\\mathrm{grad}f_{i}(z_{i,t}^{r-1};B_{i,t}^{r-1})}\\\\ &{\\quad-\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathrm{grad}f_{i}\\left(z_{i,t}^{r-1};B_{i,t}^{r-1}\\right)-\\mathrm{grad}f(\\mathcal{P}_{M}(x^{r}))\\bigg)\\bigg\\|^{2}}\\\\ &{=4\\mathbb{E}\\bigg\\|\\eta\\sum_{t=0}^{t}\\bigg(\\mathrm{grad}f_{i}(z_{i,t}^{r};B_{i,t}^{r})-\\mathrm{grad}f_{i}(\\mathcal{P}_{M}(x^{r}))+\\frac{1}{\\eta\\tau}(X_{i}^{r}-\\bar{X}^{r})\\bigg)\\bigg\\|^{2}}\\\\ &{\\leq\\underbrace{8\\mathbb{E}\\bigg\\|\\eta\\sum_{\\ell=0}^{t}\\left(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r};B_{i,\\ell}^{r})-\\mathrm{grad}f_{i}(\\mathcal{P}_{M}(x^{r}))\\right)\\bigg\\|^{2}+8\\mathbb{E}\\bigg\\|\\frac{t+1}{\\tau}X_{i}^{r}-\\frac{t+1}{\\tau}\\overline{{\\Lambda}}^{r}\\bigg\\|^{2}}_{\\ell=0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, for the term (II), substituting Assumption 4.2 yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(II)}=8\\mathbb{E}\\big\\|\\eta\\displaystyle\\sum_{\\ell=0}^{t}\\Big(\\mathrm{grad}f_{i}\\big(z_{i,\\ell}^{r};B_{i,\\ell}^{r}\\big)-\\mathrm{grad}f_{i}\\big(z_{i,\\ell}^{r}\\big)+\\mathrm{grad}f_{i}\\big(z_{i,\\ell}^{r}\\big)-\\mathrm{grad}f_{i}\\big(\\mathcal{P}_{M}\\big(x^{r}\\big)\\big)\\Big)\\big\\|^{2}}\\\\ &{\\quad\\leq16\\big(t+1\\big)^{2}\\mathbb{E}\\Big\\|\\displaystyle\\frac{\\eta}{t+1}\\sum_{\\ell=0}^{t}\\Big(\\mathrm{grad}f_{i}\\big(z_{i,\\ell}^{r};B_{i,\\ell}^{r}\\big)-\\mathrm{grad}f_{i}\\big(z_{i,\\ell}^{r}\\big)\\Big)\\Big\\|^{2}}\\\\ &{\\quad\\bigg.+16\\mathbb{E}\\big\\|\\eta\\displaystyle\\sum_{\\ell=0}^{t}\\Big(\\mathrm{grad}f_{i}\\big(z_{i,\\ell}^{r}\\big)-\\mathrm{grad}f_{i}\\big(\\mathcal{P}_{M}\\big(x^{r}\\big)\\big)\\Big)\\big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the first term can be handled by the fact [48, Corollary C.1] that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{16(t+1)^{2}\\eta^{2}\\mathbb{E}\\Big\\lVert\\frac{1}{t+1}\\sum_{\\ell=0}^{t}\\left(\\mathrm{grad}f_{i}(z_{i,\\ell}^{r};B_{i,\\ell}^{r})-\\mathrm{grad}f_{i}(z_{i,\\ell}^{r})\\right)\\Big\\rVert^{2}}\\\\ &{=\\!\\!\\frac{16(t+1)^{2}\\eta^{2}}{(t+1)^{2}}\\sum_{\\ell=0}^{t}\\!\\!\\!\\!\\mathbb{E}\\Big[\\mathbb{E}\\big[\\|\\left(\\mathrm{grad}f_{i}\\left(z_{i,\\ell}^{r};B_{i,\\ell}^{r}\\right)-\\mathrm{grad}f_{i}\\left(z_{i,\\ell}^{r}\\right)\\right)\\|^{2}|\\mathcal{F}_{t}^{r}\\big]\\Big]\\leq\\frac{1}{t+1}\\frac{\\sigma^{2}}{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining (22), (23), and (21), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\leq\\!16(t+1)\\eta^{2}L^{2}\\sum_{\\ell=0}^{t}\\mathbb{E}\\|z_{i,\\ell}^{r}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\|^{2}+8\\left(\\frac{t+1}{\\tau}\\right)^{2}\\mathbb{E}\\|\\Lambda_{i}^{r}-\\overline{{\\Lambda}}^{r}\\|^{2}+16(t+1)\\eta^{2}\\frac{\\sigma^{2}}{b},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we use Assumption 4.1. Next we substitute (24) into (19) to get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\left\\|z_{i,t+1}^{r}-\\mathcal{P}_{\\mathcal{M}}(\\boldsymbol{x}^{r})\\right\\|^{2}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq16(t+1)\\eta^{2}L^{2}\\displaystyle\\sum_{\\ell=0}^{t}\\mathbb{E}\\|z_{i,\\ell}^{r}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\|^{2}}\\\\ &{\\quad+\\underbrace{2M^{2}\\tau^{2}\\eta^{2}\\|\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\|^{2}+8\\mathbb{E}\\|\\Lambda_{i}^{r}-\\overline{{\\Lambda}}^{r}\\|^{2}+16\\tau\\eta^{2}\\frac{\\sigma^{2}}{b}}_{:=A^{r}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The following proof is similar to that in [24]. We define $A^{r}$ as the sum of the last three terms on the right hand of (25) and $\\begin{array}{r}{S_{i,t}^{r}:=\\sum_{\\ell=0}^{t}\\mathbb{E}\\|z_{i,\\ell}^{r}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\|^{2}}\\end{array}$ . By $\\mathbb{E}[\\left\\Vert z_{i,t+1}^{r}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\right\\Vert^{2}]=$ $S_{i,t+1}^{r}-S_{i,t}^{r}$ and (25), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{i,t+1}^{r}\\leq\\left(1+1/(16\\tau)\\right)S_{i,t}^{r}+A^{r},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the inequality is from $\\tilde{\\eta}\\leq\\eta_{g}/(16L)$ and thus $16(t+1)\\eta^{2}L^{2}\\leq1/(16\\tau)$ . With (26), we get ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{i,\\tau-1}^{r}\\leq\\!A^{r}\\sum_{\\ell=0}^{\\tau-2}\\left(1+1/(16\\tau)\\right)^{\\ell}\\leq1.1\\tau A^{r},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we use $\\begin{array}{r}{\\sum_{\\ell=0}^{\\tau-2}\\left(1+1/(16\\tau)\\right)^{\\ell}\\,\\leq\\,\\sum_{\\ell=0}^{\\tau-2}\\exp\\left(\\ell/(16\\tau)\\right)\\,\\leq\\,\\sum_{\\ell=0}^{\\tau-2}\\exp(1/16)\\,\\leq\\,1.1\\tau}\\end{array}$ . Summing (27) ove r all the clients $i$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\Big[\\sum_{i=1}^{n}\\sum_{t=0}^{\\tau-1}\\big\\|z_{i,t}^{r}-\\mathcal{P}_{\\mathcal{M}}(x^{r})\\big\\|^{2}\\Big]}\\\\ {\\displaystyle\\leq3n M^{2}\\tau^{3}\\eta^{2}\\|\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))\\|^{2}+9\\tau\\mathbb{E}\\|\\mathbf{A}^{r}-\\overline{{\\mathbf{A}}}^{r}\\|^{2}+18n\\tau^{2}\\eta^{2}\\frac{\\sigma^{2}}{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we are ready to bound n1E\u2225\u039br+1 \u2212\u039br+1\u22252. By the definition of \u039br+1 and \u039br+1 we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\|\\mathbf{A}^{r+1}-\\overline{{\\mathbf{A}}}^{r+1}\\|^{2}}\\\\ &{=\\eta^{2}\\mathbb{E}\\|\\tau\\mathrm{grad}\\left(\\mathcal{P}_{M}(\\mathbf{x}^{r+1})\\right)-\\displaystyle\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}\\left(\\boldsymbol{x}_{t}^{r};\\boldsymbol{B}_{t}^{r}\\right)-\\frac{\\tau\\mathrm{gradf}}{r\\mathrm{gradf}}(\\mathcal{P}_{M}(\\mathbf{x}^{r+1}))+\\displaystyle\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}\\left(\\boldsymbol{P}_{M}(\\mathbf{x}^{r+1})\\right)\\|}\\\\ &{\\leq\\eta^{2}\\mathbb{E}\\|\\tau\\mathrm{gradf}\\left(\\mathcal{P}_{M}(\\mathbf{x}^{r+1})\\right)-\\displaystyle\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}\\left(\\boldsymbol{x}_{t}^{r};\\boldsymbol{B}_{t}^{r}\\right)\\|^{2}}\\\\ &{=\\eta^{2}\\mathbb{E}\\|\\tau\\mathrm{gradf}\\left(\\mathcal{P}_{M}(\\mathbf{x}^{r+1})\\right)-\\tau\\mathrm{gradf}\\left(\\mathcal{P}_{M}(\\mathbf{x}^{r})\\right)+\\tau\\mathrm{gradf}\\left(\\mathcal{P}_{M}(\\mathbf{x}^{r})\\right)}\\\\ &{\\quad-\\displaystyle\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}\\left(\\boldsymbol{x}_{t}^{r}\\right)+\\displaystyle\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}\\left(\\boldsymbol{x}_{t}^{r}\\right)-\\displaystyle\\sum_{t=0}^{\\tau-1}\\mathrm{gradf}\\left(\\boldsymbol{x}_{t}^{r};\\boldsymbol{B}_{t}^{r}\\right)\\Big\\|^{2}}\\\\ &{\\leq2\\eta^{2}\\tau^{2}L^{2}n\\mathbb{E}\\left\\|\\mathcal{P}_{M}(\\mathbf{x}^{r+1})-\\mathcal{P}_{M}(\\mathbf{x}^{r})\\right\\|^{2}+4\\eta^{2}\\tau L_{m}^{2}\\displaystyle\\sum_{t=1}^{n-1}\\sum_{t=0}^{n-1}\\left\\|\\boldsymbol{x}_{t}^{r}-\\mathcal{P}_{M}(\\mathbf{x}^{r})\\right\\|^{2}+4\\eta^{2}\\tau n\\frac{\\sigma^{2}}{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, the first inequality is due to $\\|\\pmb{\\Lambda}^{r+1}-\\overline{{\\pmb{\\Lambda}}}^{r+1}\\|^{2}\\,\\leq\\,\\|\\pmb{\\Lambda}^{r+1}\\|^{2}$ , the last inequality is due to $\\|a+b\\|^{2}\\leq2\\|a\\|^{2}\\,^{\\underline{{\\cdot}}}+2\\|{\\bar{b}}\\|^{2}$ , Assumption 4.1, and following similar derivations as in (23). ", "page_idx": 16}, {"type": "text", "text": "By substituting (28) into (29) and reorganizing the results, we complete the proof of Lemma A.3. ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To bound the first term in the Lyapunov function, we focus on the server-side update. We begin with the following lemma over the manifolds. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.4. Given $x\\in\\mathcal{M},\\,v\\in T_{x}\\mathcal{M},\\,\\eta>0,\\,x-\\eta v\\in\\overline{{U}}_{\\mathcal{M}}(\\gamma),$ and $x^{+}=\\mathcal{P}_{\\mathcal{M}}(x-\\eta v),$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f(x^{+})\\leq f(z)+\\left\\langle\\mathrm{grad}f(x)-v,x^{+}-z\\right\\rangle}\\\\ {\\quad\\quad\\quad-\\displaystyle\\frac{1}{2\\eta}(\\|x^{+}-x\\|^{2}-\\|z-x\\|^{2})-\\left(\\displaystyle\\frac{1}{2\\eta}-\\frac{3\\|v\\|}{4\\gamma}\\right)\\|z-x^{+}\\|^{2}}\\\\ {\\quad\\quad\\quad+\\displaystyle\\frac{L}{2}\\|x^{+}-x\\|^{2}+\\displaystyle\\frac{L}{2}\\|z-x\\|^{2},\\forall z\\in\\mathcal{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. For any $\\mu$ -strongly convex function $h$ , we have for any $y,z\\in\\mathcal{M}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(z)\\geq h(y)+\\langle\\nabla h(y),z-y\\rangle+\\displaystyle\\frac{\\mu}{2}\\|z-y\\|^{2}}\\\\ &{\\qquad=h(y)+\\langle\\mathrm{grad}h(y)+\\nabla h(y)-\\mathrm{grad}h(y),z-y\\rangle+\\displaystyle\\frac{\\mu}{2}\\|z-y\\|^{2}}\\\\ &{\\qquad\\geq h(y)+\\langle\\mathrm{grad}h(y),z-y\\rangle+\\left(\\displaystyle\\frac{\\mu}{2}-\\frac{\\|\\nabla h(y)\\|}{4\\gamma}\\right)\\|z-y\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality is from the normal inequality (4) and $\\|\\nabla h(y)-\\mathrm{grad}h(y)\\|\\leq\\|\\nabla h(y)\\|$ . Setting $\\begin{array}{r}{h(y)=\\frac{1}{2\\eta}\\|y^{\\dot{-}}(\\dot{x}-\\eta v)\\|^{2}}\\end{array}$ in (31) with $\\mu=1/\\eta,y=x^{\\dagger}$ , and noting the optimality of $x^{+}$ (i.e., $\\operatorname{grad}h(x^{+})=0,$ ), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{1}{2\\eta}\\|z-(x-\\eta v)\\|^{2}\\geq\\frac{1}{2\\eta}\\|x^{+}-(x-\\eta v)\\|^{2}+\\left(\\frac{1}{2\\eta}-\\frac{\\|y-(x-\\eta v)\\|}{4\\eta\\gamma}\\right)\\|z-x^{+}\\|^{2}}\\\\ {\\geq\\displaystyle\\frac{1}{2\\eta}\\|x^{+}-(x-\\eta v)\\|^{2}+\\left(\\frac{1}{2\\eta}-\\frac{3\\|v\\|}{4\\gamma}\\right)\\|z-x^{+}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality is from $x-\\eta v\\in\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ and $\\begin{array}{r}{\\|\\mathcal{P}_{\\mathcal{M}}(x-\\eta v)-(x-\\eta v)\\|\\le\\|\\mathcal{P}_{\\mathcal{M}}(x-}\\end{array}$ $\\eta v)-x\\|+\\eta\\|v\\|\\leq3\\eta\\|v\\|$ . Rearranging the above inequality leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\langle v,z-x^{+}\\right\\rangle\\geq\\frac{1}{2\\eta}(\\|x^{+}-x\\|^{2}-\\|z-x\\|^{2})+\\left(\\frac{1}{2\\eta}-\\frac{3\\|v\\|}{4\\gamma}\\right)\\|z-x^{+}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows from the $L$ -smoothness of $f$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f(x^{+})\\leq f(x)+\\langle\\mathrm{grad}f(x),x^{+}-x\\rangle+\\displaystyle\\frac{L}{2}\\|x^{+}-x\\|^{2}}\\\\ {\\qquad\\leq f(z)+\\langle\\mathrm{grad}f(x),x^{+}-z\\rangle+\\displaystyle\\frac{L}{2}\\|z-x\\|^{2}+\\displaystyle\\frac{L}{2}\\|x^{+}-x\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use $\\begin{array}{r}{f(x)+\\langle\\mathrm{grad}f(x),z-x\\rangle-\\frac{L}{2}\\|z-x\\|^{2}\\leq f(z)}\\end{array}$ in the last inequality. Combining the above inequality and (32) gives (30). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "In the following, we use Lemma A.4 to (7) and (6), respectively. First, to apply Lemma A.4 to (7), we substitute $\\boldsymbol{x}^{\\mp}=\\boldsymbol{\\tilde{x}}^{r+1}$ , $z=\\mathcal{P}_{\\mathcal{M}}(x^{r})$ , $x=\\mathcal{P}_{\\mathcal{M}}(x^{r})$ , and $v=\\operatorname{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))$ and get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Im\\left[f\\left(\\widetilde{x}^{r+1}\\right)\\right]\\le\\mathbb{E}\\Big[f\\left(\\mathcal{P}_{M}(x^{r})\\right)+\\left(\\frac{L}{2}-\\frac{1}{2\\tilde{\\eta}}\\right)\\left\\Vert\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}-\\frac{1-\\tilde{\\eta}\\rho}{2\\tilde{\\eta}}\\left\\Vert\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}\\Big]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use $\\begin{array}{r}{\\tilde{\\eta}\\leq\\frac{\\gamma}{D_{f}}}\\end{array}$ to guarantee $\\tilde{x}^{r+1}\\in\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ and $\\begin{array}{r}{\\rho:=\\frac{3D_{f}}{2\\gamma}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Next, to use Lemma A.4 to (6), we set $x^{+}=\\mathcal{P}_{\\mathcal{M}}(x^{r+1})$ , $x=\\mathcal{P}_{\\mathcal{M}}(x^{r})$ , $z=\\tilde{x}^{r+1}$ , and $v=v^{r}$ and get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(\\mathcal{P}_{M}(x^{r+1}))\\right]}\\\\ &{\\le\\mathbb{E}\\left[f(\\hat{x}^{r+1})+\\left\\langle\\mathrm{grad}f(\\mathcal{P}_{M}(x^{r}))-v^{r},\\mathcal{P}_{M}(x^{r+1})-\\tilde{x}^{r+1}\\right\\rangle\\right.}\\\\ &{\\quad-\\left.\\frac{1}{2\\tilde{\\eta}}\\left(\\|\\mathcal{P}_{M}(x^{r+1})-\\mathcal{P}_{M}(x^{r})\\|^{2}-\\|\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\|^{2}\\right)-\\frac{1-\\tilde{\\eta}\\rho}{2\\tilde{\\eta}}\\|\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r+1})\\|^{2}}\\\\ &{\\quad+\\left.\\frac{L}{2}\\left\\|\\mathcal{P}_{M}(x^{r+1})-\\mathcal{P}_{M}(x^{r})\\right\\|^{2}+\\frac{L}{2}\\|\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\Big[f\\left(\\bar{x}^{r+1}\\right)+\\left\\langle\\mathrm{grad}f(\\mathcal{P}_{M}(x^{r}))-v^{r},\\mathcal{P}_{M}(x^{r+1})-\\bar{x}^{r+1}\\right\\rangle+\\left(\\frac{L}{2}-\\frac{1}{2\\tilde{\\eta}}\\right)\\|\\mathcal{P}_{M}(x^{r+1})-\\mathcal{P}_{M}(x^{r})\\|^{2}}\\\\ &{\\quad+\\left.\\left(\\frac{L}{2}+\\frac{1}{2\\tilde{\\eta}}\\right)\\|\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\|^{2}-\\frac{1-\\tilde{\\eta}\\rho}{2\\tilde{\\eta}}\\left\\|\\mathcal{P}_{M}(x^{r+1})-\\bar{x}^{r+1}\\right\\|^{2}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use $\\begin{array}{r}{\\tilde{\\eta}\\leq\\frac{\\gamma\\eta_{g}}{2D_{f}}}\\end{array}$ to guarantee $\\mathcal{P}_{\\mathcal{M}}(x^{r+1})\\in\\overline{{U}}_{\\mathcal{M}}(\\gamma)$ . ", "page_idx": 17}, {"type": "text", "text": "Recall that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{M}}(x^{r+1})=\\mathcal{P}_{\\mathcal{M}}\\Big(\\mathcal{P}_{\\mathcal{M}}(x^{r})-\\tilde{\\eta}\\underbrace{\\frac{1}{n\\tau}\\sum_{i=1}^{n}\\sum_{t=0}^{\\tau-1}\\big(\\mathrm{grad}f_{i}\\left(z_{i,t}^{r};\\mathcal{B}_{i,t}^{r}\\right)\\big)}_{:=v^{r}}\\Big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining the above inequality, (33), and (34) yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(\\mathcal{P}_{M}(x^{r+1}))\\right]}\\\\ &{\\le\\mathbb{E}\\big[f\\left(\\mathcal{P}_{M}(x^{r})\\right)+\\left(L-\\frac{1}{2\\tilde{\\eta}}+\\frac{\\rho}{2}\\right)\\|\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\|^{2}+\\left(\\frac{L}{2}-\\frac{1}{2\\tilde{\\eta}}\\right)\\|\\mathcal{P}_{M}(x^{r+1})-\\mathcal{P}_{M}(x^{r})\\|^{2}}\\\\ &{\\underbrace{-\\frac{1-\\tilde{\\eta}\\rho}{2\\tilde{\\eta}}\\left\\|\\mathcal{P}_{M}(x^{r+1})-\\tilde{x}^{r+1}\\right\\|^{2}}_{\\mathrm{(IV)}}+\\underbrace{\\langle\\mathcal{P}_{M}(x^{r+1})-\\tilde{x}^{r+1},\\mathrm{grad}f\\left(\\mathcal{P}_{M}(x^{r})\\right)-v^{r}\\rangle}_{\\mathrm{(V)}}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "According to $\\begin{array}{r}{\\|a+b\\|^{2}\\leq\\frac{1}{2\\tilde{\\eta}}\\|a\\|^{2}+\\frac{\\tilde{\\eta}}{2}\\|b\\|^{2}}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{(IV)}+\\mathrm{(V)}}\\\\ &{\\leq\\mathrm{(IV)}+\\displaystyle\\frac{1-\\tilde{\\eta}\\rho}{2\\tilde{\\eta}}\\|\\mathcal{P}_{\\mathcal{M}}(x^{r+1})-\\tilde{x}^{r+1}\\|^{2}+\\displaystyle\\frac{\\tilde{\\eta}}{2(1-\\tilde{\\eta}\\rho)}\\|\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))-v^{r}\\|^{2}}\\\\ &{=\\displaystyle\\frac{\\tilde{\\eta}}{2(1-\\tilde{\\eta}\\rho)}\\|\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}}(x^{r}))-v^{r}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To bound the above inequality, following similar derivations as in (23) we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left\\|v^{r}-\\mathrm{grad}f(\\mathcal{P}_{M}(x^{r}))\\right\\|^{2}}\\\\ &{=\\mathbb{E}\\Big\\|\\displaystyle\\frac{1}{n\\tau}\\sum_{i=1}^{n}\\sum_{t=0}^{\\tau-1}\\Big(\\mathrm{grad}f_{i}\\left(z_{i,t}^{r};B_{i,t}^{r}\\right)-\\mathrm{grad}f_{i}(z_{i,t}^{r})+\\mathrm{grad}f_{i}(z_{i,t}^{r})-\\mathrm{grad}f_{i}(\\mathcal{P}_{M}(x^{r}))\\Big)\\|^{2}}\\\\ &{\\leq2L^{2}\\displaystyle\\frac{1}{n\\tau}\\sum_{i=1}^{n}\\sum_{t=0}^{\\tau-1}\\mathbb{E}\\|z_{i,t}^{r}-\\mathcal{P}_{M}(x^{r})\\|^{2}+\\displaystyle\\frac{2}{\\tau n}\\displaystyle\\frac{\\sigma^{2}}{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Substituting (36) and (37) into (35), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[f(\\mathcal{P}_{M}(x^{r+1}))]}\\\\ &{\\le\\!\\mathbb{E}\\!\\left[f\\left(\\mathcal{P}_{M}(x^{r})\\right)+\\left(L-\\displaystyle\\frac{1}{2\\tilde{\\eta}}+\\displaystyle\\frac{\\rho}{2}\\right)\\left\\Vert\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}+\\left(\\displaystyle\\frac{L}{2}-\\displaystyle\\frac{1}{2\\tilde{\\eta}}\\right)\\left\\Vert\\mathcal{P}_{M}(x^{r+1})-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}\\right.}\\\\ &{\\quad+\\left.\\displaystyle\\frac{\\tilde{\\eta}}{2(1-\\tilde{\\eta}\\rho)}\\left(\\frac{2L^{2}}{n\\tau}\\sum_{i=1}^{n}\\sum_{t=0}^{\\tau-1}\\|z_{i,t}^{r}-\\mathcal{P}_{M}(x^{r})\\|^{2}+\\displaystyle\\frac{2}{\\tau n}\\frac{\\sigma^{2}}{b}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The final term is the drift-error that can be bounded in (28). Thus, (38) becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}[f(\\mathcal{P}_{M}(x^{r+1}))]}&{\\quad{\\scriptstyle(39)}}\\\\ &{\\ }&{\\quad\\le\\mathbb{E}\\Big[f\\left(\\mathcal{P}_{M}(x^{r})\\right)+\\left(L-\\frac{1}{2\\tilde{\\eta}}+\\frac{\\rho}{2}\\right)\\left\\Vert\\tilde{x}^{r+1}-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}+\\left(\\frac{L}{2}-\\frac{1}{2\\tilde{\\eta}}\\right)\\left\\Vert\\mathcal{P}_{M}(x^{r+1})-\\mathcal{P}_{M}(x^{r})\\right\\Vert^{2}}\\\\ &{\\ }&{\\quad\\cdot\\left.\\frac{\\tilde{\\eta}}{2(1-\\tilde{\\eta}\\rho)}\\frac{2L^{2}}{n\\tau}\\left(12n M^{2}\\tau^{3}\\eta^{2}\\right\\Vert\\mathcal{G}_{\\tilde{\\eta}g}(\\mathcal{P}_{M}(x^{r}))\\right\\Vert^{2}+9\\tau\\mathbb{E}\\Vert\\Lambda^{r}-\\overline{{\\Lambda}}^{r}\\Vert^{2}+18n\\tau^{2}\\eta^{2}\\frac{\\sigma^{2}}{b}\\right)+\\frac{\\tilde{\\eta}}{2(1-\\tilde{\\eta}\\rho)}\\frac{1}{\\eta^{2}}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use $\\|\\mathrm{grad}f(\\mathcal{P}_{\\mathcal{M}(x^{r})})\\|\\le2\\|\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}(x^{r})})\\|$ from Lemma (A.2). By substituting $\\begin{array}{r}{\\tilde{\\eta}\\leq\\frac{1}{4\\rho}}\\end{array}$ as $\\begin{array}{r}{\\tilde{\\eta}\\leq\\frac{\\gamma}{6D_{f}}}\\end{array}$ , we have $\\begin{array}{r}{\\frac{\\tilde{\\eta}}{2(1-\\tilde{\\eta}\\rho)}\\leq\\tilde{\\eta}}\\end{array}$ . Combining the recursions given by Lemma A.3 and (39), we have for the Lyapunov function that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\left(f(\\mathcal{P}_{\\mathcal{M}}(x^{r+1}))-f^{\\star}\\right)+\\frac{1}{\\tilde{\\eta}n}\\|\\mathbf{A}^{r+1}-\\overline{{\\mathbf{A}}}^{r+1}\\|^{2}\\Big]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq\\!\\mathbb{E}\\big[\\left(f\\!\\left(\\mathcal{P}_{M}(x^{r})\\right)-f^{\\star}\\right)+\\frac{1}{\\tilde{\\eta}n}\\|\\mathbf{A}^{r}-\\overline{{\\mathbf{A}}}^{r}\\|^{2}-\\frac{\\tilde{\\eta}}{8}\\left\\|\\mathcal{G}(\\mathcal{P}_{M}(x^{r}))\\right\\|^{2}\\big]+\\frac{8\\tilde{\\eta}}{n\\tau}\\frac{\\sigma^{2}}{b},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we substitute conditions (11) on the step sizes and omit straightforward algebraic calculations. Substituting the definition of the Lyapunov function $\\Omega^{r}$ and repeating the above inequality, we complete the proof of Theorem 4.3. ", "page_idx": 19}, {"type": "text", "text": "Discussion on step sizes. In our analysis, the condition $\\eta_{g}=\\sqrt{n}$ is applied only in the final step, when combining Lemma A.3 and (39) to derive the recursion on the Lyapunov function in (40). Up to this point, we retain $\\eta_{g}$ as a variable without assigning a \u221aspecific value, ensuring that all preceding results remain valid for $\\eta_{g}=1$ as well. We choose $\\eta_{g}=\\bar{\\sqrt{n}}$ in (40) to conveniently cancel out the number of clients $n$ , yielding a more concise expression in the final result. ", "page_idx": 19}, {"type": "text", "text": "In (11), we require that $\\begin{array}{r}{\\tilde{\\eta}\\;:=\\;\\eta_{g}\\eta\\tau\\;\\leq\\;\\operatorname*{min}\\left\\lbrace\\frac{1}{24M L},\\frac{\\gamma}{6D_{f}},\\frac{1}{D_{f}L_{\\mathcal{P}}}\\right\\rbrace}\\end{array}$ . The term $\\frac{\\gamma}{6D_{f}}$ controls the client drift, see (39); the term $\\frac{1}{D_{f}L_{\\mathcal{P}}}$ is used to derive (16), which ensures that when the metric $\\|\\mathcal{G}_{\\tilde{\\eta}}(\\mathcal{P}_{\\mathcal{M}}(\\boldsymbol{x}^{r}))\\|$ approaches zero, the first-order optimality condition is met. The term $\\frac{1}{24M L}$ is used to establish the Lyapunov function recursion (40) by combining the recursions in (17) and (39). Additional conditions on the step sizes ensure that the iterates remain within the $2\\gamma.$ -tube during our analysis, such as in (17), (33), and (34), but they are implied by the three terms in (11). ", "page_idx": 19}, {"type": "text", "text": "A.4 Additional results for numerical experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A.4.1 kPCA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The settings for kPCA problem with Mnist Dataset. The Mnist dataset consists of 60,000 handwritten digit images ranging from 0 to 9, each with dimensions of $28\\times28$ . We reshape these images into a data matrix $A\\ {\\breve{\\in}}\\ \\mathbb{R}^{60000\\times784}$ . To construct the heterogeneous $A_{i}$ , we sort the rows in increasing order of their associated digits and then split every $60000/n$ rows, with $n=10$ as the number of clients, among each client. This data partition introduces significant data heterogeneity. In our setup, $d=784$ , $p=6000$ , and $k=2$ . ", "page_idx": 19}, {"type": "text", "text": "For kPCA problem with Mnist dataset, when $\\tau=10$ , the comparison on $f(x^{r})-f^{\\star}$ is shown in Figs. 5. Our algorithm performs much better than alternative methods in terms of communication quantity and run time. ", "page_idx": 19}, {"type": "image", "img_path": "uO53206oLJ/tmp/31ceac7939ff680eef4ed293c2ba2ee5e9b1ae6eafc9b8c4f2420e4588f698be.jpg", "img_caption": ["Figure 5: kPCA problem with Mnist dataset: Comparison on $f(x^{r})-f^{\\star}$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Synthetic Dataset. We also solve kPCA with synthetic datasets on larger networks with $n=30$ . We generate each entry of $A_{i}$ from Gaussian distribution $\\textstyle N\\left(0,{\\frac{2i}{n}}\\right)$ such that $A_{i}$ are heterogeneous among clients. We set $(d,k)=(20,5)$ and $p=15$ . We use the local full gradient $\\nabla f_{i}$ to remove the influence of stochastic Riemannian gradient noise. ", "page_idx": 19}, {"type": "text", "text": "In the first set of experiments, we compare with existing algorithms, including RFedavg, RFedprox, and RFedSVRG. For all algorithms, we set the number of local steps as $\\tau=5$ and the step size as $\\eta=4e{-3}$ . For our algorithm, we set $\\eta_{g}=1$ . The experimental results are shown in Figs. 6. The $y$ -axis represents $\\left\\|\\operatorname{grad}f(x^{r})\\right\\|$ and $(f(x^{\\bar{r}})-f^{\\star})$ respectively, while the $x$ -axis represents the number of communication rounds, communication quantity, and run time, respectively. It can be observed that RFedavg and RFedprox face the issue of client drift, hence they do not converge accurately. Both FedSVRG and our algorithm can overcome the client drift issue, but our algorithm is slightly faster in terms of communication rounds and is much faster in terms of both communication quantity and run time. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "uO53206oLJ/tmp/431bd43361c1b8e600be4dd9866ea6811634d0b9cee2f20e0591347b32155e9b.jpg", "img_caption": ["Figure 6: kPCA with synthetic dataset: Comparison on $\\left\\|\\operatorname{grad}f(x^{r})\\right\\|$ and $f(x^{r})-f^{\\star}$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In the second set of experiments, we test the impact of the number of local updates $\\tau$ . For all the algorithms, we set $R=4000$ , the step size $\\eta=0.7e{-3}$ , and $\\tau\\in\\{10,15,20\\}$ . For our algorithm, we set $\\eta_{g}=1$ . The results are shown in Figs. 7, with the $y$ -axis representing $\\left\\|\\operatorname{grad}f(x^{r})\\right\\|$ and $x$ -axis representing the communication quantity. When $\\tau$ increases, the convergence becomes faster. For all values of $\\tau$ , our algorithm achieves high accuracy and requires less time. ", "page_idx": 20}, {"type": "image", "img_path": "uO53206oLJ/tmp/81595888b6262b2cc7fa10a94c3f4f2f8bae160faf7a568ae60fc81c061be552.jpg", "img_caption": ["Figure 7: kPCA with synthetic dataset: The impacts of $\\tau$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.4.2 Low-rank matrix completion ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For numerical tests, we consider random generated $A$ . To be specific, we first generate two random matrices $\\hat{L}\\in\\mathbb{R}^{d\\times k}$ and $\\hat{R}\\in\\mathbb{R}^{k\\times T}$ , where each entry obeys the standard Gaussian distribution. For the indices set $\\Omega$ , we generate a random matrix $B$ with each entry following from the uniform distribution, then set $\\Omega_{i j}=1$ if $B_{i j}\\leq\\nu$ and 0 otherwise. The parameter $\\nu$ is set to $10k(d+T-$ $k)/(d T)$ . ", "page_idx": 20}, {"type": "text", "text": "As shown in Figs. 8, our algorithm is faster than existing algorithms in terms of communication quantity and run time. ", "page_idx": 20}, {"type": "text", "text": "We also show the impact of $\\tau$ and the impact of $n$ . As shown in Figs. 9, larger $\\tau$ yields less communication quantity to achieve the same accuracy. As shown in Figs. 10, for $n=10$ , $n=20$ , and $n=30$ , our algorithm consistently outperforms the compared algorithms. ", "page_idx": 20}, {"type": "image", "img_path": "uO53206oLJ/tmp/5c22d74d46cf6ebf2ad6fccfc5355327e229d7373c42c77ee1e64c66142fccfe.jpg", "img_caption": ["Figure 8: LRMC: Comparison on $f(x^{r})$ . "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "uO53206oLJ/tmp/f9451742b5384f385fade4217fe822562a8c3d033e0a5d32974a889ceb44db01.jpg", "img_caption": ["Figure 9: LRMC: The impacts of $\\tau$ . "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "uO53206oLJ/tmp/598d806c0982365eb003189183898d7fc0833997003dca5c0045364960c39fb7.jpg", "img_caption": ["Figure 10: LRMC: The impacts of $n$ . (First row: $n\\,=\\,10$ , second row: $n\\,=\\,20$ , and third row: $n=30$ . We set $\\tau=5$ . ) "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The abstract outlines the topic of the research, the methods used, and the results obtained. In the introduction section, we summarize in detail the three contributions of this paper: algorithm contributions, theoretical contributions, and experimental results, all of which are explained in detail in the subsequent main body of the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer:[Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Conclusions and limitations. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Assumptions we made and Appendix A.1\u2013A.3 for a complete (and correct) proof. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Numerical experiments and Appendix A.4 for details of experiment settings and additional results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: We did not provide the code during the submission stage. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See numerical experiments and Appendix A.4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our algorithm uses stochastic gradients. We fixed the random seed to better compare the impact of algorithm parameters on the accuracy of the algorithm. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our experiments can be completed on a laptop. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We use open datasets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]