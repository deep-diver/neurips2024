[{"figure_path": "uO53206oLJ/figures/figures_8_1.jpg", "caption": "Figure 1: kPCA problem with Mnist dataset: Comparison on ||gradf(x\")||.", "description": "This figure compares the performance of four algorithms (RFedavg, RFedprox, RFedSVRG, and the proposed algorithm) for solving the kPCA problem using the MNIST dataset.  The y-axis represents the norm of the Riemannian gradient (||gradf(x\")||), a measure of how close the algorithm is to a solution.  The x-axis shows the number of communication rounds, the total communication quantity (number of matrices transmitted), and the runtime in seconds. The figure visually demonstrates that the proposed algorithm converges significantly faster and requires less communication than the alternatives.", "section": "5 Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_8_2.jpg", "caption": "Figure 7: KPCA with synthetic dataset: The impacts of \u03c4.", "description": "This figure shows the impact of the number of local updates (\u03c4) on the convergence speed of the proposed algorithm and other baseline algorithms (RFedAvg, RFedprox, RFedSVRG).  It demonstrates that increasing \u03c4 leads to faster convergence, and the proposed algorithm consistently outperforms the baselines in terms of communication quantity needed to achieve a certain level of accuracy.", "section": "5 Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_9_1.jpg", "caption": "Figure 3: kPCA problem with Mnist dataset: The impacts of stochastic Riemannian gradients.", "description": "This figure shows the impact of using stochastic Riemannian gradients with different batch sizes (200, 500, and 1000) on the convergence of the kPCA algorithm.  The plots illustrate the norm of the Riemannian gradient (||grad f(x')||) over communication rounds for different values of \u03c4 (the number of local updates per client). It demonstrates how varying the batch size affects the convergence speed and the overall accuracy of the algorithm when dealing with stochasticity.", "section": "Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_9_2.jpg", "caption": "Figure 1: kPCA problem with Mnist dataset: Comparison on ||gradf(x\")||.", "description": "This figure compares the performance of four different federated learning algorithms (RFedavg, RFedprox, RFedSVRG, and the proposed algorithm) on the kPCA problem using the MNIST dataset.  The y-axis shows the norm of the Riemannian gradient, which indicates the convergence of the algorithms. The x-axis is shown with three different units: communication rounds, communication quantity, and runtime. The figure demonstrates that the proposed algorithm converges faster and requires fewer communication resources than the other three algorithms.", "section": "Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_19_1.jpg", "caption": "Figure 5: kPCA problem with Mnist dataset: Comparison on f(x) - f*.", "description": "This figure compares the performance of four algorithms (RFedavg, RFedprox, RFedSVRG, and the proposed algorithm) for solving the kPCA problem using the MNIST dataset. The x-axis represents communication rounds, communication quantity, and runtime (s). The y-axis shows the value of f(x) - f*, which represents the difference between the objective function value and the optimal value. The figure demonstrates that the proposed algorithm converges faster and achieves lower f(x) - f* values compared to the other algorithms.", "section": "Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_20_1.jpg", "caption": "Figure 1: kPCA problem with Mnist dataset: Comparison on ||gradf(x\")||.", "description": "This figure compares the performance of four different algorithms (RFedavg, RFedprox, RFedSVRG, and the proposed algorithm) for solving the kPCA problem using the MNIST dataset.  The y-axis shows the norm of the Riemannian gradient (||gradf(x\")||), a measure of the algorithm's convergence towards a solution. The x-axis represents the number of communication rounds for RFedavg, RFedprox and our algorithm; it represents the communication quantity for RFedavg, RFedprox and RFedSVRG; it represents the runtime for RFedSVRG and our algorithm.  The results show that the proposed algorithm converges faster and achieves a lower gradient norm compared to the other algorithms, indicating improved efficiency and accuracy.", "section": "Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_20_2.jpg", "caption": "Figure 7: KPCA with synthetic dataset: The impacts of \u03c4.", "description": "This figure shows the impact of the number of local updates \u03c4 on the convergence of the KPCA algorithm with a synthetic dataset.  The results are presented for \u03c4 = 10, \u03c4 = 15, and \u03c4 = 20.  The plots show that as \u03c4 increases, the convergence becomes faster. For all values of \u03c4, the proposed algorithm achieves high accuracy and requires less communication.", "section": "Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_21_1.jpg", "caption": "Figure 1: kPCA problem with Mnist dataset: Comparison on ||gradf(x\")||.", "description": "This figure compares the performance of four different federated learning algorithms (RFedavg, RFedprox, RFedSVRG, and the proposed algorithm) on the task of kernel principal component analysis (kPCA) using the MNIST dataset. The y-axis represents the norm of the Riemannian gradient, which is a measure of convergence. The x-axis shows the communication rounds, communication quantity, and runtime. The results indicate that the proposed algorithm converges faster and requires significantly less communication than the other algorithms.", "section": "5 Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_21_2.jpg", "caption": "Figure 7: KPCA with synthetic dataset: The impacts of \u03c4.", "description": "This figure shows the impact of the number of local updates \u03c4 on the convergence of the kPCA algorithm with a synthetic dataset.  The plots show the norm of the Riemannian gradient ||grad f(x<sup>r</sup>)|| versus the communication quantity for three different values of \u03c4 (10, 15, and 20). The results demonstrate that increasing \u03c4 leads to faster convergence and requires less communication.", "section": "Numerical experiments"}, {"figure_path": "uO53206oLJ/figures/figures_21_3.jpg", "caption": "Figure 1: kPCA problem with Mnist dataset: Comparison on ||gradf(x\")||.", "description": "This figure compares the performance of four algorithms (RFedavg, RFedprox, RFedSVRG, and the proposed algorithm) for solving the kPCA problem using the MNIST dataset.  The y-axis represents the norm of the Riemannian gradient,  ||gradf(x\")||, which is a measure of how close the current solution is to optimality; a lower value indicates a better solution. The x-axis shows the communication rounds, communication quantity, and runtime. The figure shows that the proposed algorithm achieves a smaller Riemannian gradient norm than other algorithms, suggesting better convergence and solution quality, with significantly lower computational and communication overheads.", "section": "5 Numerical experiments"}]