[{"type": "text", "text": "Decomposable Transformer Point Processes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aristeidis Panos University of Cambridge ap2313@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The standard paradigm of modeling marked point processes is by parameterizing the intensity function using an attention-based (Transformer-style) architecture. Despite the flexibility of these methods, their inference is based on the computationally intensive thinning algorithm. In this work, we propose a framework where the advantages of the attention-based architecture are maintained and the limitation of the thinning algorithm is circumvented. The framework depends on modeling the conditional distribution of inter-event times with a mixture of log-normals satisfying a Markov property and the conditional probability mass function for the marks with a Transformer-based architecture. The proposed method attains state-of-the-art performance in predicting the next event of a sequence given its history. The experiments also reveal the efficacy of the methods that do not rely on the thinning algorithm during inference over the ones they do. Finally, we test our method on the challenging long-horizon prediction task and find that it outperforms a baseline developed specifically for tackling this task; importantly, inference requires just a fraction of time compared to the thinning-based baseline. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continuous-time event sequences are commonly found in real-world scenarios and applications such as financial transactions [3], communication in a social network [34], and purchases in e-Commerce systems [16]. This abundance of data for discrete events occuring at irregular intervals has lead to an increasing interest of the community in the last decade to marked temporal point processes which are the standard way of modeling this kind of data. ", "page_idx": 0}, {"type": "text", "text": "Historically, Hawkes processes [14] and Poisson processes [7] have been extensively applied to various domains such as finance [13], seismology [15], and astronomy [1]. Despite their elegant mathematical framework and interpretability, the strong assumptions of the models reduce their flexibility and fail to capture the complex dynamics of real-world generating processes. ", "page_idx": 0}, {"type": "text", "text": "Advances in deep learning have allowed the incorporation of neural models like LSTMs [17] or recurrent neural networks (RNN) into temporal point processes [5, 12, 24, 26, 29, 35, 38]. As a result, these models are able to learn more complex dependencies and attain superior performance than Hawkes/Poisson processes. Recently, the introduction of the (self-) attention mechanism [36] to modeling temporal point processes [41, 42, 44] has led to new state-of-the-art methods with extra flexibility. ", "page_idx": 0}, {"type": "text", "text": "Despite the advantages of these neural-based models, their dependence on modeling the conditional intensity function creates limitations for both training and inference [35]. Training usually requires a Monte Carlo approximation of an integral that appears in the log-likelihood. [29] proposed a method to circumvent this approximation; however, the main shortcomings remained as discussed in [35]. More importantly, inference is based on the thinning algorithm [21, 22] which is computationally intensive and sensitive to the choice of intensity function. To deal with these downsides, [35] parameterized the conditional distribution of the inter-event times by combining a log-normal mixture density network with an RNN. The model\u2019s performance is comparable to that of the other intensity-based methods which use RNN/LSTM architecture but still inferior to the Transformer-based methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A more recent work [30] has referred to the decomposition of the log-likelihood of a marked point process [6] to parameterize the distribution of marks given the time and history and the distribution of times given the history. This decomposition, as with [35], eliminates the need for the thinning algorithm and additional approximations, while offering a rigorous, yet flexible framework for defining different distributions for occurrence times and marks. [30] used two different parametric models for each distribution, and their results for the time prediction task, despite the simplicity of their framework, were competitive or superior to neural-based baselines. ", "page_idx": 1}, {"type": "text", "text": "Inspired by the state-of-the-art performance of the Transformer-based architectures and the computational efficiency/flexibility of the intensity-free models, we develop a model for marked point processes that combines the advantages of these two methodologies. Our contributions are summarized below: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel model that is defined by two distributions: a distribution for the marks based on a Transformer architecture and a simple log-normal mixture model for the interevent times which satisfies a simple Markov property.   \n\u2022 Through an extensive experimental study, we show the efficiency of our model in the nextevent prediction task and the suitability of the intensity-free models for correctly predicting the next occurrence time over the methods relied on the thinning algorithm.   \n\u2022 To the best of our knowledge, we are the first to experimentally show the limitations of the thinning algorithm on the predictive ability of the neural point processes.   \n\u2022 We test our model on the more challenging long-horizon prediction task and we provide strong evidence that we can achieve better results in a fraction of time compared to models that have been specifically designed to solve this task and, uncoincidentally, depend on the thinning algorithm. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A marked temporal point process (MTPP), observed in the interval $(0,T)$ , is a stochastic process whose realizations are sequences of discrete events occurring at times $0^{\\ '}<t_{1}<...<t_{N}<T$ with corresponding event types (or marks) $k_{1},\\ldots,k_{N}$ , where $k_{i}\\in\\{1,\\ldots,K\\}$ . The entire sequence is denoted by $\\mathcal{\\hat{H}}_{T}^{\\ \\ }=\\ \\{(t_{1},\\dot{k}_{1}),\\ldots,(t_{N},k_{N})\\}$ . The process is fully specified by the conditional intensity function (CIF) of the event of type $k$ at time $t$ conditioned on the event history $\\mathcal{H}_{t_{i}}\\,=$ $\\{(t_{j},k_{j})\\mid t_{j}<t_{i}\\}$ , $\\lambda_{k}^{*}(t):=\\lambda_{k}(t\\mid\\mathcal{H}_{t_{i}})\\geq0,t>t_{i}$ ; we use the asterisk $^*$ to denote the dependence on $\\mathcal{H}_{t_{i}}$ . The CIF is used to compute the infinitesimal probability of event $k$ occurring at time $t$ , i.e. $\\lambda_{k}^{*}(t)d t=\\mathbb{P}\\left(t_{i+1}\\in[t,t+\\bar{d t}],k_{i+1}=k\\mid t_{i+1}\\notin(\\bar{t}_{i},t),\\mathcal{H}_{t_{i}}\\right)$ . The log-likelihood of such an autoregressive multivariate point process is given by [14, 22] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathcal{H}_{T})=\\sum_{i=1}^{N}\\lambda_{k_{i}}^{*}(t_{i})-\\sum_{k=1}^{K}\\int_{0}^{T}\\lambda_{k}^{*}(t)\\;d t.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Modeling the intensity function by a flexible model and then learning its parameters by maximizing Eq. (1) has been the standard approach of many works [5, 12, 24, 26, 29, 35, 38, 41, 42, 44]. ", "page_idx": 1}, {"type": "text", "text": "An equivalent way of deriving the log-likelihood in (1) without the use of $\\lambda_{k}^{*}(t)$ is by following the decomposition of a multivariate distribution function in [6] (expression 2), expressed as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathcal{H}_{T})=\\sum_{i=1}^{N}\\left\\{\\log p^{*}(k_{i}\\mid t_{i})+\\log f^{*}(t_{i})\\right\\}+\\log\\left(1-F(T\\mid\\mathcal{H}_{t_{N}})\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $p^{*}(k\\mid t_{i}):=p(k\\mid t_{i},\\mathcal{H}_{t_{i}})^{1}$ and $f^{*}(t):=\\,f(t\\mid\\mathcal{H}_{t_{i}})$ are the conditional probability mass function (CPMF) of the event types and the conditional probability density function (CPDF) for the occurrence times, respectively. $\\begin{array}{r}{\\dot{F}(t\\mid\\mathcal{H}_{t_{i}})=\\int_{t_{i}}^{t}f^{*}(\\dot{t})d t\\mathrm{~,~}\\forall t}\\end{array}$ $\\forall t>t_{i}$ is the cumulative distribution function of $f^{*}(t)$ . The last term in (2) is the logarithm of the survival function that expresses the probability that no event occurs in the interval $(t_{N},T)$ . The relation between $\\lambda_{k}^{*}(t)$ and the density/PMF is given by $\\begin{array}{r}{\\lambda_{k}^{*}(t)=\\frac{f^{*}(t)p^{*}(k|t)}{1-F(t|\\mathcal{H}_{t})}}\\end{array}$ ; see Section 2.4 in [33]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We can represent the temporal part of the process in terms of the inter-event times $\\tau_{i}:=t_{i}-t_{i-1}\\in$ $\\mathbb{R}_{+},t_{0}=0$ ; the two representations are isomorphic and the relation between the conditional PDF of the inter-event time $\\tau_{i}$ until the next event and the conditional intensity function is given by $\\begin{array}{r}{g^{*}(\\tau_{i}):=g^{*}(\\tau_{i}\\mid\\mathcal{H}_{t_{i}})=\\sum_{k=1}^{K}\\lambda_{k}^{*}(t_{i-1}+\\tau_{i})\\exp\\left(-\\sum_{k=1}^{K}\\int_{0}^{\\tau_{i}}\\lambda_{k}^{*}(t_{i-1}\\dot{+\\boldsymbol{x}})d\\boldsymbol{x}\\right)=f^{*}(\\bar{t_{i}}).}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "3 Decomposable Transformer Point Processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To develop our proposed framework Decomposable Transformer Point Process $(D T P P)$ , we adopt the decomposition in (2) and model $g^{\\ast}(\\tau)$ and $p^{*}(k\\mid t)$ , separately. Despite the advantages of modeling the intensity function and the arguments in favor of this [9], we believe that modeling the probability density/mass function offers not only the same beneftis as modeling the intensity function as discussed in [35], but, more importantly, it allows us not to depend on the thinning algorithm during inference. The technical details of each model are described in the next two sections. ", "page_idx": 2}, {"type": "text", "text": "3.1 Distribution of Marks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The conditional distribution of the event types is parameterized by a continuous-time Transformer architecture as the one described in [41]. More specifically, for any pair of events $(t,k)$ , we evaluate an embedding $\\mathbf{h}_{k}(t)\\ \\in\\ \\mathbb{R}^{D}$ based on the history $\\mathcal{H}_{t}$ . Assuming an $L$ -layer architecture, $\\mathbf{h}_{k}(t)$ is given by the concatenation of the embedding of each individual layer, i.e. $\\mathbf{h}_{k}(t)=[\\mathbf{h}_{k}^{(0)}(t);\\mathbf{h}_{k}^{(1)}(t);\\ldots;\\mathbf{h}_{k}^{(L)}(t)]$ . The embedding of the base layer $\\mathbf{h}_{k}^{(0)}(t)$ is independent of time and it is learned by a simple weight vector for each mark, i.e. $\\mathbf{h}_{k}^{(0)}(t):=\\mathbf{h}_{k}^{(0)}\\in\\mathbb{R}^{D^{(0)}}$ . The embedding of layer $\\ell\\in\\{1,\\ldots,L\\}$ for $(t,k)$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}_{k}^{(\\ell)}(t):=\\mathbf{h}_{k}^{(\\ell-1)}(t)+\\operatorname{tanh}\\left(\\sum_{(t_{i},k_{i})\\in\\mathcal{H}_{t}}\\frac{\\mathbf{v}_{k_{i}}^{(\\ell)}(t_{i})\\,\\boldsymbol{\\alpha}_{k_{i}}^{(\\ell)}(t_{i};t,k)}{1+C}\\right)\\in\\mathbb{R}^{D^{(\\ell)}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $C>0$ is the normalization constant given by $\\begin{array}{r}{C=\\sum_{\\left(t_{i},k_{i}\\right)\\in\\mathcal{H}_{t}}\\alpha_{k_{i}}^{(\\ell)}(t_{i},t,k)}\\end{array}$ and the unnormalized attention weight is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha_{k_{i}}^{(\\ell)}(t_{i};t,k)=\\exp\\left(\\frac{1}{\\sqrt{D}}{\\mathbf{k}}_{k_{i}}^{(\\ell)}(t_{i})^{\\top}{\\mathbf{q}}_{k}^{(\\ell)}(t)\\right)>0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The operation of the non-linear activation function tanh is element-wise and $\\begin{array}{r}{D\\,=\\,\\sum_{\\ell=0}^{L}D^{(\\ell)}}\\end{array}$ . The query, key, and value vectors ${\\bf q}_{k}^{(\\ell)}(t),{\\bf k}_{k}^{(\\ell)}(t)$ , and ${\\bf v}_{k}^{(\\ell)}(t)$ , respectively, can be c omputed by using the embedding of the previous layer and the corresponding weight matrices $Q^{(\\ell)},K^{(\\ell)}~\\in$ $\\mathbb{R}^{D\\times(D+D^{(\\ell-1)})},V^{(\\ell)}\\in\\mathbb{R}^{D^{(\\widehat{\\ell})}\\times(D+D^{(\\ell-\\widehat{1})})}$ as follows, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{q}_{k}^{(\\ell)}(t)=Q^{(\\ell)}X_{t}^{\\ell},\\ \\ \\mathbf{k}_{k}^{(\\ell)}(t)=K^{(\\ell)}X_{t}^{\\ell},\\ \\mathbf{v}_{k}^{(\\ell)}(t)=V^{(\\ell)}X_{t}^{\\ell},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $X_{t}^{\\ell}=\\left[\\mathbf{z}(t);\\mathbf{h}_{k}^{(\\ell-1)}(t)\\right]\\in\\mathbb{R}^{D+D^{(\\ell-1)}}$ . By $\\mathbf{z}(t)\\in\\mathbb{R}^{D}$ , we denote a temporal embedding of time defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n[\\mathbf{z}(t)]_{d}=\\left\\{\\begin{array}{l l}{\\cos\\left(t/10^{\\frac{4(d-1)}{D}}\\right),\\;\\;\\mathrm{if~d~is~odd},}\\\\ {\\sin\\left(t/10^{\\frac{4d}{D}}\\right),\\;\\;\\mathrm{if~d~is~odd},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d=0,\\ldots,D-1$ . This encoding is the same as in [44] with small differences than the one used in [41] where we found empirically the former to work slightly better than the latter. For a more detailed discussion regarding the architecture of the model and how it compares to previous Transformer-based methods, see Appendix A in [41]. Finally, we note that for extra model flexibility, multi-head self-attention can be easily obtained by the three equations in (5). ", "page_idx": 2}, {"type": "text", "text": "Having computed the top-layer embeddings $\\mathbf{h}_{k}(t)$ for all $k=1,\\ldots,K$ , we model the conditional PMF $\\bar{p^{*}}(k\\mid\\bar{t})$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\np^{*}(k\\mid t)=\\frac{\\exp\\left(\\mathbf{w}_{k}^{\\top}\\mathbf{h}_{k}(t)\\right)}{\\sum_{l=1}^{K}\\exp\\left(\\mathbf{w}_{l}^{\\top}\\mathbf{h}_{l}(t)\\right)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{w}_{k}$ are the learnable classifier weights. As is typical for these architectures, to avoid any data leakage from future events, we mask all future events $(t_{i},k_{i})$ where $t<t_{i}$ and only use previous events for computing these embeddings. ", "page_idx": 3}, {"type": "text", "text": "3.2 Distribution of Inter-Event Times ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For the modeling of the inter-event times, since they always take positive values, we choose a mixture of log-normal distributions whose parameters depend on the value of the previously seen mark. Specifically, given that the previous occurred mark is $k$ , the $\\mathrm{PDF}^{2}$ of the next inter-event time $\\tau$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\ng^{*}(\\tau)=g(\\tau\\mid k)=\\sum_{m=1}^{M}w_{m}^{(k)}\\frac{1}{\\tau s_{m}^{(k)}\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\log\\tau-\\mu_{m}^{(k)}}{s_{m}^{(k)}}\\right)^{2}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{w_{m}^{(k)}\\}_{m=1}^{M}\\in\\Delta^{M}$ are the mixture weights, $\\{\\mu_{m}^{(k)}\\}_{m=1}^{M}\\in\\mathbb{R}^{M}$ are the mixture means, and {s(mk) }mM=1 \u2208R+M a re the standard deviations, for any $k=1,\\ldots,K$ . The log-normal mixture has several desirable features that justifies our choice: (i) it efficiently approximates distributions in low dimensions such as 1-d distributions of inter-event times [23, 35] while satisfying a universal approximation property that provides theoretical guarantees regarding its approximation ability [8], (ii) closed-form moments are available and can be used for predicting the next time; for instance, the mean of the distribution is given as the weighted average of each of the log-normal means, i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{g}^{(k)}[\\tau]=\\sum_{m=1}^{M}w_{m}^{(k)}\\exp\\left(\\mu_{m}^{(k)}+\\frac{(s_{m}^{(k)})^{2}}{2}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "(iii) learning the small number of parameters $\\{w_{m}^{(k)},\\mu_{m}^{(k)},s_{m}^{(k)}\\}_{m=1}^{M}$ can be done in a fraction of time using fast off-the-shelf implementations based on the EM algorithm [10]. Finally, note that the dependence of the model only on the most recent mark implies a Markov property since we do not need the entire history $\\mathcal{H}_{<t}$ to define our distribution. ", "page_idx": 3}, {"type": "text", "text": "At first glance, this assumption might seem restrictive when it comes to capturing the complex dynamics of the process. Nevertheless, this assumption holds only for $g^{\\ast}$ while $p^{*}$ is modeled by the flexible Transformer architecture that models the full history up to the current time $t$ . Hence, we do not sacrifice any modeling power at all to achieve efficiency. On the contrary, we can maintain both modeling power and computational efficiency due to the decomposition in (2) and the chosen models in (8) and (7). Moreover, as our extensive experiments on the real-world data show, this assumption provides a robust predictive model which is less prone to overftiting compared to more flexible neural net architectures since the Markov property can act as a strong regularizer. ", "page_idx": 3}, {"type": "text", "text": "3.3 Training and Prediction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The parameters $\\{w_{m}^{(k)},\\mu_{m}^{(k)},s_{m}^{(k)}\\}_{m,k}$ , of $g^{\\ast}(\\tau)$ and the parameters $\\{\\mathbf{w}_{k},\\mathbf{h}_{k}^{(0)},Q^{(\\ell)},K^{(\\ell)},V^{(\\ell)}\\}_{\\ell,k}$ of $p^{*}(k\\mid t)$ can be estimated by maximizing the log-likelihood in (2) using any stochastic gradient method. A crucial benefit of using the decomposition in (2) is that it permits us to learn the above parameters separately as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\{{w_{m}^{(k)}}^{*},{\\mu_{m}^{(k)}}^{*},{s_{m}^{(k)}}^{*}\\}_{m,k}=\\mathrm{argmax}\\displaystyle\\sum_{i=1}^{N}\\log g^{*}(\\tau_{i})+\\log\\left(1-G(T\\mid\\mathcal{H}_{T})\\right)}}\\\\ {{\\displaystyle\\{{\\mathbf{w}_{k}^{*}},{\\mathbf{h}_{k}^{(0)}}^{*},{Q^{(\\ell)}}^{*},K^{(\\ell)}},V^{(\\ell)}{}^{*}\\}_{\\ell,k}=\\mathrm{argmax}\\displaystyle\\sum_{i=1}^{N}\\log p^{*}(k_{i}\\mid t_{i}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{G(T\\mid\\mathcal{H}_{T})=\\int_{0}^{T-t_{N}}g^{*}(\\tau)d\\tau}\\end{array}$ 0T \u2212tNg\u2217(\u03c4)d\u03c4 . This is a major difference from previous work where the parameters of the neural net parameterizing the conditional intensity function had to be learned all at once. By dividing the main objective function into two sub-objectives, since there is no parametersharing between the two models, we can maximize the two objectives independently, and thus, having an easier optimization task than maximizing a single set of parameters of a given objective, such as (1). ", "page_idx": 4}, {"type": "text", "text": "The trained models $g^{*}$ and $p^{*}$ can now be used to predict either the time/type of the next event (nextevent prediction) or the next $P>1$ events (long-horizon prediction). For the next-event-prediction, the predicted time $\\hat{t}$ given the history $\\mathcal{H}_{t_{i}}$ is computed by using the mean of the appropriate mixture of log-normals while the corresponding predicted type of this event k\u02c6 is evaluated based on $\\mathcal{H}_{t_{i}}$ and the true time $t_{i+1}$ , i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{t}=t_{i}+\\mathbb{E}_{g}^{(k_{i})}[\\tau],\\;\\;\\;\\;\\;\\hat{k}=\\underset{k}{\\mathrm{argmax}}\\,p^{*}(k\\mid t_{i+1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The above procedure is based on the minimum Bayes risk (MBR) principle [24] which aims to predict the time and type that minimizes the expected loss. This is an average $L_{2}$ loss in the case of time prediction (deriving a root mean squared error) and an average 0-1 loss for the type prediction (deriving an error rate). ", "page_idx": 4}, {"type": "text", "text": "For the long-horizon prediction task [11, 40], we need to predict a sequence of events where, unlike the next-event prediction task, we do not have access to the true time when we predict the next event type. This could potentially lead to a cascading error effect due to the autoregressive nature of the models designed for the less challenging task of next-event prediction. This is because after an error is made in the sequence of the predictions, it cannot be corrected, and thus the error accumulates and affects all subsequent predictions. We argue that this pathology can be alleviated by using a model of times that provides accurate and robust predictions given the history. This assumption is verified experimentally in Section 5.2. To generate a predicted sequence, we require the trained models $g(\\u)$ and $p()$ to sequentially predict events as in (12). Since we have no access to the true time $t_{i+1}$ , we use as a proxy the prediction $\\hat{t}$ to predict $\\hat{k}$ in turn. After the prediction of the new event, we append it to the history and then we repeat the same step given the updated history until we generate a sequence of $P$ events. The exact procedure is described in Algorithm 1 in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "The main advantage of Algorithm 1 over other methods [40] that are based on the thinning algorithm is its computational efficiency. The algorithm is fully parallelizable, and it can produce single steps in parallel for a batch of event sequences. This is not possible for thinning-based methods that require one to consider single sequences each time [40]. Consequently, our method is able to generate sequences orders of magnitude faster, which is verified by our experiments. Unlike other competitors [40] that are based on the thinning algorithm and therefore require random sampling, our algorithm is fully deterministic; for comparison the thinning algorithm is described in Algorithm 2 of the Appendix. ", "page_idx": 4}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The decomposition in (2) has been used in the past to provide both expressive and interpretable models [27, 30]. For instance, [30] model the mark distribution with a parametric model, inspired by the exponential intensity function of a Hawkes process and the time distribution with a single log-normal; however, they use the mode instead of the mean of the distribution for predicting the time of the next event. They learn the parameters of their models separately, as we describe in (10) and (11), but they use Variational Inference [4] to learn the parameters of $p^{*}$ . They attain competitive results in terms of next-time prediction, but the model lacks the flexibility of a Transformer-based architecture, as our experiments show. ", "page_idx": 4}, {"type": "text", "text": "[35] is another work that takes advantage of the mixture of log-normal distributions to model the distribution of the inter-event times. The model is based on an RNN architecture that produces a fixed-dimensional embedding of the event history, which is used to generate the parameters of the mixture model, and the same embedding is employed to define the CPMF of the marks. In our case, we use a Transformer architecture to obtain this history embedding, which is utilized by the CPMF, exclusively. Finally, the proposed model in [35] assumes that the marks are conditionally independent of the time given the history, which is not the case for our framework, as is evident in (2). ", "page_idx": 4}, {"type": "image", "img_path": "OesteJF0ls/tmp/afc6243d081354451f2591d7992c30b3a8d3fd745a1b319a98ff39d67609efc9.jpg", "img_caption": ["Figure 1: Goodness-of-fti evaluation over the five real-world datasets. We compare our DTPP model against five strong baselines. Results (larger is better) are accompanied by $95\\%$ bootstrap confidence intervals. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Finally, the CPMF of the marks for our DTPP model shares the same architecture as the Attentive Neural Hawkes Process (A-NHP) [41]. Nevertheless, they use it to model the CIF while in our case we model $p^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We considered two different tasks to assess the predictive performance of our proposed method: Goodness-of-fti/next-event prediction and long-horizon prediction. We compared our method DTPP to several strong baselines over five real-world datasets and three synthetic ones. Description and summary statistics for all datasets used in this section are given in Appendix A.1. For the competing methods, we used their published implementations; more details are given in A.3. Experimental details not available in this section can be found in Appendix A. Our framework was implemented with PyTorch [31] and scikit-learn [32]; the code is available at https://github.com/aresPanos/ dtpp. ", "page_idx": 5}, {"type": "text", "text": "5.1 Goodness-of-Fit / Next-Event Prediction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluated our DTPP model to determine how well it generalizes and predicts the next event given the history on the held-out dataset. For comparison, we used five state-of-the-art baselines where the three of them model the CIF using Transformers, while the other two model the CPDF of inter-event times and the CPMF of marks (see Section 4). The CIF-based baselines are the Transformer Hawkes Process (THP) [44], the Self-Attentive Hawkes Process (SAHP) [42], and the Attentive Neural Hawkes Process (A-NHP) [41]. The CPDF-based ones are the Intensity-Free Temporal Point Process (IFTPP) [35], and the VI-Decoupled Point Process (VI-DPP) [30]. ", "page_idx": 5}, {"type": "text", "text": "We fit the above six models on a diverse collection of five popular real-world datasets, each with varied characteristics: MIMIC-II [19], Amazon [28], Taxi [37], Taobao [43], and StackOverlfowV1 [20, 41]. Training details are given in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Goodness-of-Fit. Figure 1 shows the average log-likelihood for each model on the held-out data of the five real-world datasets. Our DTPP model consistently outperforms the simple parametric ", "page_idx": 5}, {"type": "table", "img_path": "OesteJF0ls/tmp/80ee8415390325c34e9bed8bcd9ef8651b94f45f1d81d0910c8cc1b4db03bbeb.jpg", "table_caption": ["Table 1: Performance comparison between our model DTPP and various baselines in terms of nextevent prediction. The root mean squared error (RMSE) measures the error of the predicted time of the next event, while the error rate (ERROR- $\\%$ ) evaluates the error of the predicted mark given the true time. The results (lower is better) are accompanied by $95\\%$ bootstrap confidence intervals. \u2020,\u25c1,\u25b7 denote the CIF-based methods, the CPDF-based methods that use a single model, and the ones using a seperate model, respectively. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "VI-DPP, indicating the flexibility of using the self-attention mechanism to model the CPDF. Except Mimic-II, DTPP achieves the highest or the second highest log-likelihood across the remaining datasets. Therefore, the separate parameterization of $p^{*}$ and $g^{\\ast}$ does not hurt performance compared to the models with a common set of learnable parameters. Finally, notice that DTPP outperforms all the CPDF-based methods on average, while the two CPDF-based methods that employ deep learning architectures, i.e., DTPP and IFTPP, exhibit better performance than the CIF-based baselines. A plausible explanation is that the log-likelihood computation for the CIF-based baselines requires Monte Carlo integration, which could cause approximation errors; for the CPDF-based methods, this computation is exact. A-NHP is the clear winner among the CIF-based methods, as also shown in [41]. ", "page_idx": 6}, {"type": "text", "text": "Next-Event Prediction. We evaluate the predictive capacity of all models by predicting each event $(t_{i},k_{i})$ given its history $\\mathcal{H}_{t_{i}}$ ${{t}_{t}}_{i},i=2,\\ldots,N$ on held-out data. Event time prediction is measured by root mean squared Error (RMSE) and event type prediction by error rate; Table 1 summarizes the results. DTPP outperforms all the baselines in both tasks. The wider performance gaps in RMSE between our model and the other baselines justify our choice of a inter-event distribution satisfying a Markov property; this result also implies that we do not need long event histories to capture the dynamics of these datasets. We also compare the average performance between CIF-based and CPDF-based (excluding VI-DPP) methods. We see that for the CIF-based baselines the average RMSE is 0.58 and the average error rate is $35.39\\%$ while for the CPDF-based ones we have 0.95 and $37.0\\%$ , respectively. These results support our argument that the thinning algorithm tends to harm the time prediction accuracy; they also highlight the efficiency of using a separate model for the inter-event times. Additional results on Mimic-II can be found in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "Synthetic datasets. To extra investigate the capabilities of our model in a more controlled manner, we created a dataset by generating sequences from a randomly initialized SAHP model. Although, each event has strong dependence on its history, Figure 2 shows that our model approximates the true log-likelihood as well as A-NHP. Moreover, DTPP\u2019s mixture model is more accurate than the thinning-based A-NHP in time prediction. Moreover, we found that the only case that DTPP was significantly outperformed by A-NHP was on a synthetic dataset generated by a 1-d Hawkes process. Since no event types are present we only use a single mixture of log-normals which apparently is the wrong model for this data. The results are illustrated in Figure 4 of the Appendix. ", "page_idx": 6}, {"type": "text", "text": "5.2 Long-Horizon Prediction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To test the performance of our model for this task, we followed the experimental setup of [40]. From the same work, we used the proposed HYPRO, which is the state-of-the-art method for the long-horizon prediction task, to compare with DTPP. HYPRO is a globally normalized model that aims to address cascading errors that occur in auto-regressive and locally normalized models, such as the models in Section 5.1. HYPRO and DTPP are based on the same Transformer architecture of ", "page_idx": 6}, {"type": "image", "img_path": "OesteJF0ls/tmp/54ef675783cd67c02c5a59aab7201a5852faca65bcd5cc6f09748c1e217c9222.jpg", "img_caption": ["Figure 2: Performance comparison between DTPP and A-NHP over the SAHP-Synthetic dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "A-NHP so the main difference is that HYPRO is a CIF-based method, and thus, it requires the thinning algorithm to sample sequences. For our experiments, we use the distance-based regularization variant of HYPRO with a Multi-NCE loss as this method attains the best results in [40]. As DTPP and HYPRO share the same Transformer architecture, so we used the exact same hyperparameters for fair comparison. Note that even in this case, HYPRO has more than double number of parameters compared to DTPP since HYPRO requires an extra Transformer to model the energy function used for global normalization. More details on HYPRO training and hyperparameters can be found in the Appendix A. ", "page_idx": 7}, {"type": "image", "img_path": "OesteJF0ls/tmp/7c98026c8fc6a00123ab7aa908ab4245b04fb469e4cd2f6ece1730ebabb94606.jpg", "img_caption": ["Figure 3: Performance comparison over the three real-world datasets measured by $\\mathbf{RMSE}^{\\star}$ and average OTD (lower is better). The reported results for HYPRO are based on 16 weighted samples, i.e. $M=16$ for Algorithm 2 in [40]. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We used three of the previous real-world datasets for evaluation because of their long sequences. These are Taxi, Taobao, and StackOverflow-V2 [20, 40]. For each dataset, our goal is to predict the last 20 events in a sequence, denoted by $\\mathcal{H}_{P}$ , given the history; that is, $P=20$ in Algorithm 1. As is typical for the long-horizon prediction, the standard scores used for evaluating the model\u2019s performance are the The optimal transport distance (OTD) [25] and the long-horizon RMSE $(\\mathbf{RMSE}^{*}$ ) [40]. ", "page_idx": 7}, {"type": "text", "text": "In Figure 3, we see that our DTPP method outperforms HYPRO across all datasets in terms of average OTD and RMSE. HYPRO achieves a lower RMSE score only in StackOverflow. These results provide corroborating evidence on our argument that the thinning algorithm might negatively affect the performance of a neural point process even in the case of globally normalized models as HYPRO. It is also evident that a locally normalized CPDF-based model such as DTPP is much more robust against the cascading error which CIF-based methods are vulnerable [41]. We believe that this robustness stems from the accurate predictions of the log-normal mixture model. ", "page_idx": 7}, {"type": "text", "text": "Apart from the predictive performance, we investigated the time required for the two methods to generate all the predicted sequences of the held-out dataset. Since HYPRO\u2019s inference time is heavily relied on the thinning algorithm and a hyperparameter that indicates the number of proposal sequences (denoted as $M$ in [40]), we conducted an ablation study for a varied number of proposals to investigate the inference time and performance of HYPRO against DTPP. For HYPRO\u2019s inference time, apart from the prediction time, we included the time required to generate the noise sequences so the energy function can be trained on. The inclusion of this time is justified by the importance the energy function has as a component of the framework, and it can be seen as a necessary pre-inference step. However, for completeness, we compute only the prediction time of HYPRO and report it in Table 6 of the Appendix. ", "page_idx": 7}, {"type": "table", "img_path": "OesteJF0ls/tmp/a62880256bcb2f86f1958b10fcaffcbf01ff6ea2051ca74fef00ca3bd29e0ddb.jpg", "table_caption": ["Table 2: Performance comparison between our model DTPP and HYPRO for the long-horizon prediction task. For HYPRO, we use $\\{2,4,8,16,32\\}$ weighted proposals (Algorithm 2 in [40]). We report the average optimal transport distance (avg OTD) and the time (in minutes) required for predicting all the long-horizon sequences of the held-out dataset (lower is better). \u201cParams\u201d denotes the number $(\\times10^{3})$ of trainable parameters of each method. We include error bars based on five runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The results are presented in Table 2 where we measure the performance using the average OTD; a similar table for $\\mathbf{RMSE^{*}}$ is in Appendix B.3. We see that our parallelizable framework takes advantage of modern GPU hardware and performs inference in a few seconds. Instead, the thinning algorithm constitutes HYPRO extremely slow and impractical for inference on large datasets. In some cases like the Taxi dataset, HYPRO needs 8, $130\\times$ more time than DTPP to perform inference. Moreover, DTPP attains better performance across all datasets even for a larger number of proposals in HYPRO. These results verify our assumption about the robustness of the mixture model to predict accurately the next time; they also highlight the inaccurate predictions and computational burden of the thinning algorithm. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have presented DTPP, a Transformer-based probabilistic model for continuous-time event sequences. The model has been derived using the decomposability of the likelihood of a MTPP in terms of its CPDF and CPMF. We have used a mixture of log-normals and a Transformer architecture to model CPDF and CPMF, respectively. Our model satisfies some desirable properties compared to previous works that tried to model the CIF such as closed-form computation of the log-likelihood and inference without resorting to the thinning algorithm. Extensive experiments on the standard task of next-event prediction showed that our method outperformed all state-of-the-art autoregressive models, The results also reveal a more robust performance of the methods that do not require the thinning algorithm to generate event sequences over those they do. Finally, we have tested our model on the challenging task of long-horizon prediction of event sequences. Although our model has not been designed for this task, it outperformed the state-of-the-art baseline HYPRO which is also based on the thinning algorithm. This performance for DTPP was achieved in orders of magnitude faster than HYPRO. ", "page_idx": 8}, {"type": "text", "text": "Limitations and future work. The main limitation of the model stems from the modeling of $p^{*}$ using a deep learning architecture which is usually data-hungry and thus requires large amount of data to learn the model\u2019s parameters. For this reason, the model might be unsuitable for data-scarce regimes since it could be prone to overfit. Regarding future work, the limitations of the thinning algorithm revealed by the experiments raise many interesting questions on how can we improve this pathology for the CIF-based methods so they match the performance of the CPDF-based ones since their representations are equivalent. Another interesting research direction would be the development of a globally normalized model similar to HYPRO for CPDF-based models. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Petros Dellaportas and Lina Gerontogianni for helpful discussions. This work was funded by Toyota Motor Europe. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Gutti Jogesh Babu and Eric D Feigelson. Spatial point processes in astronomy. Journal of Statistical Planning and Inference, 50(3):311\u2013326, 1996.   \n[2] E. Bacry, M. Bompaire, S. Ga\u00efffas, and S. Poulsen. tick: a Python library for statistical learning, with a particular emphasis on time-dependent modeling. ArXiv e-prints, July 2017. [3] Emmanuel Bacry, Adrian Iuga, Matthieu Lasnier, and Charles-Albert Lehalle. Market impacts and the life cycle of investors orders. Market Microstructure and Liquidity, 1(02):1550009, 2015.   \n[4] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.   \n[5] Alex Boyd, Robert Bamler, Stephan Mandt, and Padhraic Smyth. User-dependent neural sequence models for continuous-time event data. Advances in Neural Information Processing Systems, 33:21488\u201321499, 2020.   \n[6] David R Cox. Partial likelihood. Biometrika, 62(2):269\u2013276, 1975.   \n[7] Daryl J Daley and David Vere-Jones. Basic properties of the Poisson process. An introduction to the theory of point processes: Volume I: Elementary theory and methods, pages 19\u201340, 2003.   \n[8] Anirban DasGupta. Asymptotic theory of statistics and probability, volume 180. Springer, 2008.   \n[9] Abir De, Utkarsh Upadhyay, and Manuel Gomez-Rodriguez. Temporal point processes. Technical report, Technical report, Saarland University, 2019.   \n[10] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (methodological), 39(1):1\u201322, 1977.   \n[11] Prathamesh Deshpande, Kamlesh Marathe, Abir De, and Sunita Sarawagi. Long horizon forecasting with temporal point processes. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pages 571\u2013579, 2021.   \n[12] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song. Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1555\u20131564, 2016.   \n[13] Joel Hasbrouck. Measuring the information content of stock trades. The Journal of Finance, 46(1):179\u2013207, 1991.   \n[14] Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83\u201390, 1971.   \n[15] Alan G Hawkes and David Oakes. A cluster process representation of a self-exciting process. Journal of Applied Probability, pages 493\u2013503, 1974.   \n[16] Sergio Hernandez, Pedro Alvarez, Javier Fabra, and Joaquin Ezpeleta. Analysis of users\u2019 behavior in structured e-commerce websites. IEEE Access, 5:11941\u201311958, 2017.   \n[17] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.   \n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[19] Joon Lee, Daniel J Scott, Mauricio Villarroel, Gari D Clifford, Mohammed Saeed, and Roger G Mark. Open-access mimic-ii database for intensive care research. In 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pages 8315\u20138318. IEEE, 2011.   \n[20] Jure Leskovec and Andrej Krevl. Snap datasets: Stanford large network dataset collection, 2014.   \n[21] PA W Lewis and Gerald S Shedler. Simulation of nonhomogeneous Poisson processes by thinning. Naval research logistics quarterly, 26(3):403\u2013413, 1979.   \n[22] Thomas Josef Liniger. Multivariate hawkes processes. PhD thesis, ETH Zurich, 2009.   \n[23] Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture models. Annual review of statistics and its application, 6:355\u2013378, 2019.   \n[24] Hongyuan Mei and Jason M Eisner. The neural Hawkes process: A neurally self-modulating multivariate point process. In Advances in Neural Information Processing Systems, pages 6754\u20136764, 2017.   \n[25] Hongyuan Mei, Guanghui Qin, and Jason Eisner. Imputing missing events in continuous-time event streams. In International Conference on Machine Learning, pages 4475\u20134485. PMLR, 2019.   \n[26] Hongyuan Mei, Guanghui Qin, Minjie Xu, and Jason Eisner. Neural datalog through time: Informed temporal modeling via logical specification. In International Conference on Machine Learning, pages 6808\u20136819. PMLR, 2020.   \n[27] Santhosh Narayanan, Ioannis Kosmidis, and Petros Dellaportas. Flexible marked spatiotemporal point processes with applications to event sequences from association football. Journal of the Royal Statistical Society Series C: Applied Statistics, page qlad085, 2023.   \n[28] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantlylabeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.   \n[29] Takahiro Omi, Naonori Ueda, and Kazuyuki Aihara. Fully neural network based model for general temporal point processes. arXiv preprint arXiv:1905.09690, 2019.   \n[30] Aristeidis Panos, Ioannis Kosmidis, and Petros Dellaportas. Scalable marked point processes for exchangeable and non-exchangeable event sequences. In International Conference on Artificial Intelligence and Statistics, pages 236\u2013252. PMLR, 2023.   \n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32:8026\u20138037, 2019.   \n[32] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[33] Jakob Gulddahl Rasmussen. Lecture notes: Temporal point processes and the conditional intensity function. arXiv preprint arXiv:1806.00221, 2018.   \n[34] Diego Rybski, Sergey V Buldyrev, Shlomo Havlin, Fredrik Liljeros, and Hern\u00e1n A Makse. Communication activity in a social network: relation between long-term correlations and inter-event clustering. Scientific reports, 2(1):560, 2012.   \n[35] Oleksandr Shchur, Marin Bilo\u0161, and Stephan G\u00fcnnemann. Intensity-free learning of temporal point processes. In International Conference on Learning Representations, 2019.   \n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.   \n[37] Chris Whong. Foiling nyc\u2019s taxi trip data. https://chriswhong.com/open-data/foil_ nyc_taxi/, 2014.   \n[38] Shuai Xiao, Junchi Yan, Xiaokang Yang, Hongyuan Zha, and Stephen Chu. Modeling the intensity function of point process via recurrent neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2017.   \n[39] Siqiao Xue, Xiaoming Shi, Zhixuan Chu, Yan Wang, Fan Zhou, Hongyan Hao, Caigao Jiang, Chen Pan, Yi Xu, James Y Zhang, et al. Easytpp: Towards open benchmarking the temporal point processes. arXiv preprint arXiv:2307.08097, 2023.   \n[40] Siqiao Xue, Xiaoming Shi, James Zhang, and Hongyuan Mei. Hypro: A hybridly normalized probabilistic model for long-horizon prediction of event sequences. Advances in Neural Information Processing Systems, 35:34641\u201334650, 2022.   \n[41] Chenghao Yang, Hongyuan Mei, and Jason Eisner. Transformer embeddings of irregularly spaced events and their participants. In International Conference on Learning Representations, 2022.   \n[42] Qiang Zhang, Aldo Lipani, Omer Kirnap, and Emine Yilmaz. Self-attentive Hawkes process. In International Conference on Machine Learning, pages 11183\u201311193. PMLR, 2020.   \n[43] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. Learning tree-based deep model for recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1079\u20131088, 2018.   \n[44] Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. Transformer hawkes process. In International Conference on Machine Learning, pages 11692\u201311702. PMLR, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Experimental details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Dataset Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Summary statistics and characteristics of the datasets used are given in Table 3. A more detailed description is given below: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Hawkes1-Synthetic. This dataset contains synthetic event sequences from a univariate Hawkes process sampled using Tick [2] whose conditional intensity function is defined by $\\begin{array}{r}{\\lambda^{*}(t)=\\mu+\\sum_{t_{i}<t}\\sum_{j=1}^{J}\\alpha_{j}\\beta_{j}\\exp(-\\beta_{j}(t-t_{i}))}\\end{array}$ . We use $J=1,\\mu=1,\\alpha_{1}=0.8,\\beta_{1}=$   \n\u2022 Hawkes2-Synthetic. Same as Hawkes1-Synthetic where the parameters here are set as $J=2,\\mu=0.2,\\alpha_{1}=0.4,\\beta_{1}=1.0,\\alpha_{2}=0.4,\\beta_{2}=20,$ .   \n\u2022 SAHP-Synthetic. We sample sequences from a randomly initialized SAHP model using the thinning algorithm. The number of layers and the dimension of hidden states are 4 and 32, respectively. The same dataset has been used in [41].   \n\u2022 MIMIC-II. The Multiparameter Intelligent Monitoring in Intensive Care (MIMIC-II) is a medical dataset of de-identified clinical visit records of intensive care unit patients for seven years. There are records of 650 patients/sequences where each one contains the time of the visit and the diagnosis of this visit; there are $K{=}75$ unique diseases. We try to predict the time and the diagnosis of a patient.   \n\u2022 Amazon. This dataset includes time-stamped user product reviews behavior from January, 2008 to October, 2018. Each user has a sequence of review events with each event containing the timestamp and category of the reviewed product, with each category corresponding to an event type. As in [39], we use a subset of 5200 most active users with an average sequence length of 70 which is comprised of $\\mathrm{K}=16$ event types.   \n\u2022 Taxi. This dataset records the times of taxi pick-up and drop-off events across the five boroughs of the New York city (Manhattan, Brooklyn, Queens, The Bronx, Staten Island). For each borough, we can have pick-up or drop-off event and thus, there are $\\mathbf{K}=10$ event types in total. As in [40], we pick a randomly sampled subset of 2000 drivers where each driver has a sequence.   \n\u2022 Taobao. The dataset comes from the 2018 Tianchi Big Data Competition. It consists of time-stamped behavior records (e.g., browsing, purchasing) of anonymous users on the online shopping platform Taobao from the 25th of November through the 3rd of December in 2017. The $K{=}17$ event types represent a category group (e.g. men\u2019s clothing). The browsing sequences of the most active 2000 users are picked as event sequences. The time unit is 3 hours and the average inter-arrival time is 0.06.   \n\u2022 StackOverflow-V1. The data comes from the well-known question-answering website StackOverflow where users are encouraged to answer questions so they can earn badges. There are 22 different types of badges. Each sequence corresponds to a user and each event gives the time and the type of budge a user has been awarded. This dataset is the one processed and used in [12].   \n\u2022 StackOverflow-V2. A truncated version of the original StackOverflow-V1.. The time unit ", "page_idx": 12}, {"type": "text", "text": "is 11 days and the average inter-arrival time is 0.95. The dataset has been used in [40] ", "page_idx": 12}, {"type": "text", "text": "A.2 Training Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We use the Adam optimizer [18] with its default settings to train all the models in Section 5. We use 200 epochs in total, a batch size of 8 sequences, and we apply early-stopping based on the log-likelihood of the held-out dev set. ", "page_idx": 12}, {"type": "text", "text": "Following [41], we use a common hyperparameter $D$ to define all dimensionalities of the query, key, and value vectors for the models with attention mechanisms, i.e. THP, SAHP, A-NHP, and DTPP-CPMF. For these methods, we also need to specify the number of layers $L$ . We also denote by $D$ the state space of the IFTPP model. ", "page_idx": 12}, {"type": "table", "img_path": "OesteJF0ls/tmp/1dd1a7f72800579af340da3ffda39c58d251cfa0835d3a05f2e81c2cf2266d79.jpg", "table_caption": ["Table 3: Characteristics of the synthetic and real-world datasets. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 4: Performance comparison between our model DTPP and various baselines in terms of nextevent prediction on Mimic-II dataset. The root mean squared error (RMSE) measures the error of the predicted time of the next event, while the error rate (ERROR- $\\%$ ) evaluates the error of the predicted mark given the true time. The results (lower is better) are accompanied by $95\\%$ bootstrap confidence intervals. \u2020,\u25c1,\u25b7denote the CIF-based methods, the CPDF-based methods that use a single model, and the ones using a seperate model, respectively. ", "page_idx": 13}, {"type": "table", "img_path": "OesteJF0ls/tmp/158408f5d01012e1a2d98f77899446cfbf31e961aa6e8a2c9ba5f438aa1384a3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "The hyperparameters $D$ and $L$ were fine-tuned for each combination of dataset and model. We gridsearch the two parameters using the search spaces $D\\in\\{4,8,16,32,64,128\\}$ and $L\\in\\{1,2,3,\\bar{4},5\\}$ . We pick the set of values that achieve the highest log-likelihood on the dev set. ", "page_idx": 13}, {"type": "text", "text": "For IFTPP we use the same search for $D$ as before. Since IFTPP and DTPP (CPDF part) are based on a mixture of log-normals, we need to define the number of components $M$ . We fine tune $M$ over the space $M\\in\\{1,2,4,8,16\\}$ . ", "page_idx": 13}, {"type": "text", "text": "VI-DPP has only one hyperparameter that requires tuning. This is the number of cutt-off points $Q$ [30]. As the original paper, we found that $Q=1$ works the best in all of our datasets. ", "page_idx": 13}, {"type": "text", "text": "For long-horizon prediction experiments, we chose a common size architecture for the HYPRO / DTPP transformer by setting $D\\,=\\,128$ and $L\\,=\\,2$ since this combination worked well on both A-NHP and DTPP in all datasets in Section 5.1. ", "page_idx": 13}, {"type": "text", "text": "All experiments were carried out on the same Linux machine with a dedicated reserved GPU used for acceleration. ", "page_idx": 13}, {"type": "text", "text": "A.3 Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For A-NHP, we use the public Github repository at https://github.com/yangalan123/ anhp-andtt. For THP and SAHP, we also use the same repository where the corrected implementations of THP and SAHP are provided. For IFTPP, we use the public Github repository at https://github.com/shchur/ifl-tpp while for VI-DPP we use the code at https: //github.com/aresPanos/Interpretable-Point-Processes. Our code will be made publicly available on a Github repository after the review period. ", "page_idx": 13}, {"type": "text", "text": "B Extra Experimental Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Results on Mimic-II ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 4 presents the results for next-event prediction task on Mimic-II dataset. The results follow similar patterns as in Table 1. ", "page_idx": 14}, {"type": "text", "text": "B.2 Results on Synthetic datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 4 shows the results for the two synthetic datasets generated by two different one-dimensional Hawkes processes. The DTPP\u2019s mixture of log-normals fails to model correctly the two processes since it lacks the flexibility of the neural network that A-NHP and IFTPP are based on. However, when it comes to next-time prediction, DTPP performs on par with A-NHP, showing once more that the thinning algorithm decreases prediction accuracy. IFTPP is the clear winner for these two datasets. ", "page_idx": 14}, {"type": "text", "text": "B.3 Results on Long-Horizon Prediction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present additional results for the long-horizon prediction in Tables 5 and 6. Table 5 reports the $\\mathbf{RMSE^{*}}$ where the results show that DTPP outperforms HYPRO, regardless of its number of proposals. The only exception is StackOverflow-v2, where HYPRO achieved a lower score but required at least 1,544 more running time than DTPP. ", "page_idx": 14}, {"type": "text", "text": "Finally, we report HYPRO\u2019s time without considering the time required to generate noise sequences (Algorithm 1 in [40]) from the trained auto-regressive model $p_{\\mathrm{auto}}$ for training the energy function $E_{\\theta}$ ; see discussion in Section 5.2. Even in this case, our DTPP model is orders of magnitude faster than HYPRO. ", "page_idx": 14}, {"type": "text", "text": "Table 5: Performance comparison between our model DTPP and HYPRO for the long-horizon prediction task. For HYPRO, we use $\\{2,4,8,16,32\\}$ weighted proposals (Algorithm 2 in [40]). We report the root mean squared error of the number of tokens for each type of event $\\mathrm{(RMSE^{\\star})}$ ) and the time (in minutes) required to predict all the long-horizon sequences of the held-out dataset (lower is better). We include error bars for five runs. ", "page_idx": 14}, {"type": "table", "img_path": "OesteJF0ls/tmp/94ada3510a5eb137f7b5324b23daa851fbe8d5f2e29fc27d4292df1a1b2e887d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "OesteJF0ls/tmp/74c0130e8d2e83348bfd44f99b082b15f630cc6eabb354bafaddf5b27a80284f.jpg", "img_caption": ["Figure 4: Goodness-of-fti and next-time prediction comparison over the two 1-d synthetic examples generated from a Hawkes process. The reported results are based on the test dataset. The black dotted line represents the true log-likelihood of the data (in nats). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 6: Time comparison (in minutes) between our model DTPP and HYPRO for the long-horizon prediction task. Unlike Tables 2 and 5, here we only report the prediction time of HYPRO without including the time required to generate noise sequences (Algorithm 1 in [40]) from the trained auto-regressive model $p_{\\mathrm{auto}}$ for training the energy function $E_{\\theta}$ . We include error bars based on five runs. ", "page_idx": 15}, {"type": "table", "img_path": "OesteJF0ls/tmp/404d62ec96cf4ebc1135b254176bc52230c32474f79535804d157e32e52f0a21.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Long-Horizon Prediction for Decomposed Transformer Point Processes ", "page_idx": 15}, {"type": "text", "text": "Input: an observed sequence $\\mathcal{H}_{T}$ of $N$ events over the interval $(0,T)$ and the number of prediction steps $P>1$ ; trained inter-event model $g$ and event types model $p$ ", "page_idx": 15}, {"type": "text", "text": "Output: predicted sequence $\\hat{\\mathcal{H}}_{P}$ of $P$ events   \n1: function $\\mathsf{L H P}(\\mathcal{H}_{T},P,g^{*},\\bar{p}^{*})$   \n2: $\\hat{\\mathcal{H}}_{P}\\gets\\varnothing,~\\hat{\\mathcal{H}}\\gets\\mathcal{H}_{T}$   \n3: for $p=1$ to $P$ :   \n4: \u25b7Predict next-event time and event type   \n5: $\\begin{array}{r l}&{t_{N+p}=t_{N+p-1}+\\mathbb{E}_{g}^{(k_{N+p-1})}[\\tau\\mid\\hat{\\mathcal{H}}]}\\\\ &{k_{N+p}=\\operatorname*{argmax}_{k}p(k\\mid t_{N+p},\\hat{\\mathcal{H}})}\\end{array}$   \n6:   \n7: \u25b7Update predicted sequence and event history   \n8: $\\begin{array}{r l}&{\\hat{\\mathcal{H}}_{P}\\gets\\hat{\\mathcal{H}}_{P}\\cup(t_{N+p},\\dot{k}_{N+p})}\\\\ &{\\hat{\\mathcal{H}}\\gets\\hat{\\mathcal{H}}\\cup(t_{N+p},k_{N+p})}\\end{array}$   \n9:   \n10: end for   \n11: return $\\hat{\\mathcal{H}}_{P}$   \n12: end function ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Thinning Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: an observed sequence $\\mathcal{H}_{T}$ of $N$ events over the interval $(0,T)$ and the number of prediction   \nsteps $P>1$ ; intensity function $\\lambda_{k}$   \nOutput: predicted sequence $\\hat{\\mathcal{H}}_{P}$ of $P$ events   \n1: function Thinning $(\\mathcal{H}_{T},P,\\lambda_{k})$   \n2: $\\begin{array}{r}{\\hat{\\mathcal{H}}_{P}\\gets\\emptyset,\\;\\;\\hat{\\mathcal{H}}\\gets\\mathcal{H}_{T},\\;\\;p\\gets0,\\;\\hat{t}\\gets T}\\end{array}$   \n3: for $p=1$ to $P$ :   \n4: $\\begin{array}{r}{\\dot{\\lambda}_{\\mathrm{max}}=\\operatorname*{max}_{t\\in(\\hat{t},\\infty)}\\sum_{k=1}^{K}\\lambda_{k}(t\\mid\\hat{\\mathcal{H}})}\\end{array}$ \u25b7Compute upper bound $\\lambda_{m a x}$   \n5: repeat:   \n1 $\\begin{array}{r l r}&{:}&{\\stackrel{\\sim}{\\sim}\\mathrm{Exp}(\\lambda_{\\operatorname*{max}})}\\\\ &{:}&{\\hat{t}\\leftarrow\\hat{t}+\\tau}\\\\ &{:}&{\\operatorname*{until}u\\leq\\sum_{k=1}^{K}\\lambda_{k}(\\hat{t}\\ |\\ \\hat{\\mathcal{H}})/\\lambda_{\\operatorname*{max}}\\qquad\\forall\\ A c c e p t\\ p r o p o s e d\\ o c a t r r e n c e\\ t i m e\\ w i t h\\ p r o b a b i l i t y}\\\\ &{:}&{\\sum_{k=1}^{K}\\lambda_{k}(\\hat{t}\\ |\\ \\hat{\\mathcal{H}})/\\lambda_{m a x}}\\\\ &{:}&{\\hat{k}\\sim\\mathrm{Cat}(p_{1},\\dots,p_{K})\\ \\mathrm{where}\\ p_{k}=\\lambda_{k}(\\hat{t}\\ |\\ \\hat{\\mathcal{H}})/\\sum_{k=1}^{K}\\lambda_{k}(\\hat{t}\\ |\\ \\hat{\\mathcal{H}})}\\\\ &{\\ e v e n t\\ t y p e\\ \\hat{k}\\in\\{1,\\dots,K\\}}\\\\ &{:}&{t_{N+p}\\leftarrow\\hat{t},\\ k_{N+p}\\leftarrow\\hat{k}}\\\\ &{:}&{\\hat{\\mathcal{H}}_{P}\\leftarrow\\hat{\\mathcal{H}}_{P}\\cup(t_{N+p},k_{N+p}),\\ \\ \\hat{\\mathcal{H}}\\leftarrow\\hat{\\mathcal{H}}\\cup(t_{N+p},k_{N+p})}&{\\triangleright U p d a t e\\ p r e d i c t e d}\\end{array}$   \n1 sequence and event history   \n12: end for   \n13: return $\\hat{\\mathcal{H}}_{P}$   \n14: end function ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 17}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 17}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 17}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 17}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 17}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: See Section 1 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See section 6 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: - ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes]   \nJustification: See Sections 5 and A.3   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See Sections 5 and A.3 ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 19}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Sections 5 and A.3 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 20}, {"type": "text", "text": "Guidelines: See Section 5 ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Section A ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: This work is concerned with modeling point processes and this ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Sections 5 and A.1 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]