[{"figure_path": "IdQuUYMA1t/figures/figures_1_1.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold", "description": "This figure compares the performance of different training methods (Cold, Warm, S&P, and DASH) on the Tiny-ImageNet dataset using ResNet-18. The x-axis represents the number of experiments, where each experiment adds a chunk of data to the training set.  The left plot shows test accuracy, illustrating that DASH consistently outperforms other methods, especially warm-starting which often performs worse than training from scratch. The right plot shows the number of training steps for each experiment, highlighting DASH's efficiency in reaching convergence.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_6_1.jpg", "caption": "Figure 2: The plot shows the test accuracy (left y-axis) when the model is pretrained for varying epochs (x-axis) and then fine-tuned on the full data, along with the pretrain accuracy (right y-axis) plotted in brown. We trained three-layer MLP (left) and ResNet-18 (right). Each transparent line and point corresponds to a specific random seed, and the median values are highlighted with opaque markers and solid lines. The 'Random' corresponds to training from random initialization (cold-start).", "description": "This figure shows the test accuracy results of three-layer MLP and ResNet-18 models when pretrained for varying epochs and then fine-tuned on the full dataset. It compares the performance of warm-starting (pre-training then training) with cold-starting (training from scratch).  The plot includes train accuracy during the pre-training phase.  The results show that if the pre-training is stopped at a certain point and fine-tuned on full data, the test accuracy is maintained. However, if the pre-training continues beyond the specific threshold, then warm-starting significantly impairs the model's performance, which indicates noise memorization during excessive pre-training.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods (random, warm, and ideal) across 50 experiments on a dataset.  The plots show test accuracy, the number of learned features, and the number of non-zero gradient data points.  The results indicate that warm-starting results in significantly worse test accuracy than both random initialization (cold-starting) and the ideal method, while the ideal method demonstrates that retaining learned features and forgetting noise leads to better performance compared to cold-starting, albeit with increased training time.  The warm-starting method results in a smaller number of learned features, suggesting that memorization of noise impairs performance.", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_7_2.jpg", "caption": "Figure 4: Illustration of DASH. We compute the loss L with training data \\(\\mathcal{T}_{1:j}\\) and obtain the negative gradient. Then, we shrink the weights proportionally to the cosine similarity between the current weight \\(\\theta\\) and \\(\\nabla_\\theta L\\), resulting in \\(\\tilde{\\theta}\\).", "description": "This figure illustrates the core concept of the DASH algorithm.  It shows how weights are shrunk based on their alignment with the negative gradient of the loss function. Weights that align well with the negative gradient (representing learned features) are shrunk less, while those that don't align well (representing noise) are shrunk more. This selective forgetting of noise helps to prevent the loss of plasticity.", "section": "4 DASH: Direction-Aware SHrinking"}, {"figure_path": "IdQuUYMA1t/figures/figures_13_1.jpg", "caption": "Figure 5: An illustration of our proposed feature learning framework with single class. Each data point is composed of features, as shown in Figure 5a, and is learned through the framework depicted in Figure 5b.", "description": "This figure illustrates the feature learning process using a simple example with a single class of images.  Figure 5a shows data points (represented as vertical columns of colored dots) that are each a combination of class-relevant features (the colored dots) and class-irrelevant noise.  Figure 5b depicts the learning process.  The model sequentially selects and learns features from the data points based on their frequency, starting with the most frequent feature, until no features meet the learning threshold.  Then the model begins memorizing the noise from the remaining data points. This illustrates the core idea of the proposed feature learning framework.", "section": "A Detailed Intuition and Example of Our Framework"}, {"figure_path": "IdQuUYMA1t/figures/figures_13_2.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold\u201d refers to cold-starting and \u201cWarm\u201d refers to warm-starting. The Shrink & Perturb (S&P) method involves shrinking the model weights by a constant factor and adding noise (Ash and Adams, 2020). Notably, DASH, our proposed method, achieves better generalization performance compared to both training from scratch and S&P, while requiring fewer steps to converge.", "description": "This figure compares the test accuracy and training efficiency of different neural network training methods on the Tiny-ImageNet dataset using ResNet-18.  The methods include cold-starting (training from scratch), warm-starting (starting with pre-trained weights), Shrink & Perturb (S&P), and the proposed DASH method. The x-axis represents the number of experiments, each adding more data. The results show that DASH outperforms both cold-starting and warm-starting in terms of test accuracy while requiring fewer training steps. ", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_15_1.jpg", "caption": "Figure 6: Trained on ResNet-18 with five random seeds, where CIFAR-10 is divided into 50 chunks and incrementally increased by adding new chunks at each experiment. Each point represents an individual experiment. The gradient norm is used as a proxy for the number of non-zero gradient data points, which in turn serves as a proxy for the training time. A larger gradient norm indicates the model needs to learn more features or memorize more data points to correctly classify all training data points.", "description": "This figure shows the relationship between the initial gradient norm of training data and the number of steps required for convergence in both warm-starting and cold-starting scenarios using ResNet-18 on CIFAR-10.  The x-axis represents the initial gradient norm, which is a proxy for the complexity of the data (higher norm suggests more complex data). The y-axis represents the number of training steps needed for convergence. Each point represents a single experiment, with the color intensity indicating the number of experiments with similar values.  It visually demonstrates how the number of training steps increases with increasing initial gradient norm, and also shows a difference between warm and random initializations.", "section": "B.1 Justification of Our Framework"}, {"figure_path": "IdQuUYMA1t/figures/figures_15_2.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold\u201d refers to cold-starting and \u201cWarm\u201d refers to warm-starting. The Shrink & Perturb (S&P) method involves shrinking the model weights by a constant factor and adding noise (Ash and Adams, 2020). Notably, DASH, our proposed method, achieves better generalization performance compared to both training from scratch and S&P, while requiring fewer steps to converge.", "description": "This figure compares the performance of different neural network training methods on the Tiny-ImageNet dataset using ResNet-18.  The experiment involves incrementally adding data in 50 chunks, training until 99.9% training accuracy is reached before adding the next chunk.  The left graph shows test accuracy, while the right graph shows the number of training steps.  The methods compared are cold-starting, warm-starting, Shrink & Perturb (S&P), and the proposed DASH method. DASH shows superior generalization performance and faster convergence compared to the other methods.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_16_1.jpg", "caption": "Figure 2: The plot shows the test accuracy (left y-axis) when the model is pretrained for varying epochs (x-axis) and then fine-tuned on the full data, along with the pretrain accuracy (right y-axis) plotted in brown. We trained three-layer MLP (left) and ResNet-18 (right). Each transparent line and point corresponds to a specific random seed, and the median values are highlighted with opaque markers and solid lines. The 'Random' corresponds to training from random initialization (cold-start).", "description": "This figure compares the test accuracy of models trained with different pretraining durations on the full dataset.  It shows that pre-training for an excessive number of epochs before fine-tuning on the full dataset hurts performance. The results suggest that an optimal pre-training duration exists, where exceeding that optimal duration leads to memorization of noise and poorer generalization ability. The experiment is conducted using both a three-layer MLP and a ResNet-18 model, each with multiple random seeds to assess variance.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_16_2.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods: random (cold-start), warm-start, and an ideal method, across 10 random seeds.  The left panel shows test accuracy, where both random and ideal initialization perform similarly and significantly better than warm-start.  The middle panel shows the number of learned features across all classes, which are also similar for random and ideal but far fewer for warm start. The right panel shows training time (measured as the number of non-zero gradient data points). The ideal method significantly improves upon the warm-start training time, showing its efficiency. The results are averaged and the standard deviations are shown.", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_17_1.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods: random (cold-starting), warm-starting, and an ideal method (where only noise is forgotten). The results show that warm-starting performs significantly worse than random initialization and the ideal method in terms of test accuracy.  However, warm-starting has a significantly shorter training time. The ideal method achieves the best accuracy and training time, indicating that retaining features while forgetting noise is crucial for efficient and accurate learning.", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_17_2.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods (random, warm, and ideal) across 50 experiments on a dataset that grows with each experiment.  The left panel shows that cold-start (random) and ideal initialization achieve similar high test accuracy, while warm-starting performs significantly worse. The middle panel shows that the number of features learned is similar for both random and ideal initializations, and much lower for warm-starting, suggesting that warm-starting fails to learn new features effectively. The right panel shows that the training time (number of steps) is far less for warm-starting compared to cold-starting, with ideal initialization showing a training time between the two.", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_17_3.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold\u201d refers to cold-starting and \u201cWarm\u201d refers to warm-starting. The Shrink & Perturb (S&P) method involves shrinking the model weights by a constant factor and adding noise (Ash and Adams, 2020). Notably, DASH, our proposed method, achieves better generalization performance compared to both training from scratch and S&P, while requiring fewer steps to converge.", "description": "This figure compares the test accuracy and training efficiency (number of steps) of different methods on the Tiny-ImageNet dataset using ResNet-18.  The dataset was incrementally added in chunks, and models were trained until 99.9% training accuracy before moving to the next chunk.  The methods compared are cold-starting, warm-starting, Shrink & Perturb (S&P), and the proposed DASH method.  DASH demonstrates superior generalization performance and training efficiency compared to the other approaches.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_18_1.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold", "description": "This figure compares the performance of different warm-starting methods against cold-starting on the Tiny-ImageNet dataset using ResNet-18.  The x-axis represents the number of experiments (with data added incrementally), and the y-axis shows test accuracy and the number of training steps.  DASH consistently outperforms warm-starting and S&P (Shrink & Perturb), demonstrating its effectiveness in maintaining plasticity. The right plot shows that DASH achieves comparable generalization performance while requiring fewer steps.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_18_2.jpg", "caption": "Figure 14: We trained a 3-layer CNN on CIFAR-10 and plotted cosine similarity values greater than 0.1. The left y-axis (black) represents the cosine similarity between the negative loss gradient from the training data and the model filters, while the right y-axis (green) shows the test accuracy.", "description": "This figure shows the relationship between the cosine similarity of weights and negative gradients from the test data, and the test accuracy of a 3-layer CNN trained on CIFAR-10 with varying training data sizes.  It supports the intuition behind DASH by visually demonstrating that higher cosine similarity (indicating weights have learned features) correlates with better test accuracy.  The box plots show the distribution of cosine similarity values for each training data size, while the line graph illustrates the corresponding test accuracy.", "section": "B.3 Experiments Supporting the Intuition behind DASH"}, {"figure_path": "IdQuUYMA1t/figures/figures_20_1.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold", "description": "This figure compares the performance of different neural network training methods on the Tiny-ImageNet dataset using ResNet-18.  The x-axis represents the number of experiments, where in each experiment a new chunk of data is added to the training set. The y-axis on the left shows test accuracy and the y-axis on the right shows the number of training steps.  The methods compared are cold-starting (training from scratch), warm-starting (initializing with pre-trained weights), Shrink & Perturb (S&P), and the proposed DASH method.  The results demonstrate that DASH significantly outperforms other methods in generalization performance while requiring fewer training steps.", "section": "3 Warm-Starting versus Cold-Starting, and a New Ideal Method"}, {"figure_path": "IdQuUYMA1t/figures/figures_20_2.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold", "description": "This figure compares the test accuracy and the number of training steps of different methods (Cold, Warm, S&P, DASH) on the Tiny-ImageNet dataset using ResNet-18.  The dataset is incrementally expanded across 50 experiments.  DASH shows improved test accuracy and training efficiency compared to other methods, especially warm-starting.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_22_1.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods: random, warm, and ideal, across 10 random seeds.  The results are presented for three metrics: test accuracy, the number of learned features, and training time. The random and ideal methods show nearly identical results for test accuracy and the number of learned features, while the warm method exhibits significantly lower test accuracy and a much shorter training time. The ideal initialization method bridges the gap between warm and random methods by achieving similar accuracy as the random method with similar training time to the warm method.", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_23_1.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods: random (cold-starting), warm-starting, and an ideal method.  The results are averaged over 10 random seeds, and error bars show standard deviations. The left panel shows test accuracy, the middle panel shows the number of learned features, and the right panel shows training time (measured as number of steps or number of non-zero gradient data points). The key observation is that while warm starting leads to faster convergence (shorter training time), it results in significantly lower test accuracy than both cold starting and the ideal method. The ideal method achieves comparable test accuracy to cold-starting with a significantly reduced training time. This highlights the trade-off between speed and accuracy in warm-starting and shows the potential of the ideal method to improve generalization without sacrificing efficiency.", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_25_1.jpg", "caption": "Figure 2: The plot shows the test accuracy (left y-axis) when the model is pretrained for varying epochs (x-axis) and then fine-tuned on the full data, along with the pretrain accuracy (right y-axis) plotted in brown. We trained three-layer MLP (left) and ResNet-18 (right). Each transparent line and point corresponds to a specific random seed, and the median values are highlighted with opaque markers and solid lines. The 'Random' corresponds to training from random initialization (cold-start).", "description": "This figure compares the test accuracy of models trained using different initialization methods (random, warm-start, and the proposed DASH method) on the Tiny-ImageNet dataset. The x-axis shows the number of pre-training epochs, and the left y-axis shows the test accuracy after fine-tuning on the full dataset. The right y-axis shows the pre-training accuracy.  The results show that warm-starting leads to worse performance than cold-starting if the pre-training is extended beyond a certain point, while DASH maintains high test accuracy.", "section": "3 Warm-Starting versus Cold-Starting, and a New Ideal Method"}, {"figure_path": "IdQuUYMA1t/figures/figures_25_2.jpg", "caption": "Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. \u201cCold", "description": "The figure compares the test accuracy and training steps of different methods (Cold, Warm, S&P, DASH) on the Tiny-ImageNet dataset using ResNet-18. The x-axis represents the number of experiments, where in each experiment a new chunk of data is added.  The results show that DASH consistently outperforms other methods in terms of test accuracy while requiring fewer training steps.", "section": "3 Warm-Starting versus Cold-Starting, and a New Ideal Method"}, {"figure_path": "IdQuUYMA1t/figures/figures_26_1.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three initialization methods: random (cold-start), warm-start, and an ideal method.  The results show that random initialization and the ideal method achieve similar high test accuracy and number of learned features, while warm-starting exhibits significantly worse performance in both metrics.  The ideal method also addresses the large difference in training time (number of steps) between random and warm initialization, achieving a training time similar to warm starting but with a much higher test accuracy, demonstrating its superior efficiency and performance. ", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_26_2.jpg", "caption": "Figure 2: The plot shows the test accuracy (left y-axis) when the model is pretrained for varying epochs (x-axis) and then fine-tuned on the full data, along with the pretrain accuracy (right y-axis) plotted in brown. We trained three-layer MLP (left) and ResNet-18 (right). Each transparent line and point corresponds to a specific random seed, and the median values are highlighted with opaque markers and solid lines. The 'Random' corresponds to training from random initialization (cold-start).", "description": "This figure shows the test accuracy and pre-training accuracy of three-layer MLP and ResNet-18 models when pre-trained for varying epochs and then fine-tuned on the full dataset. The results show a trade-off between warm-starting and cold-starting: while cold-starting leads to better generalization performance, warm-starting requires less training time.  The figure also highlights the importance of the pre-training period; if pre-training is stopped at the appropriate time, the warm-started model retains its performance after fine-tuning.", "section": "Experiments with Expanding Dataset"}, {"figure_path": "IdQuUYMA1t/figures/figures_27_1.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods: random (cold-starting), warm-starting, and an ideal method. The results show that the random and ideal methods achieve similar test accuracy, significantly outperforming warm-starting.  However, warm-starting requires less training time than the other two methods. The ideal method achieves the best of both worlds: similar accuracy to cold-starting, but with reduced training time.", "section": "Experiments"}, {"figure_path": "IdQuUYMA1t/figures/figures_28_1.jpg", "caption": "Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \u00b1 std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.", "description": "This figure compares the performance of three different initialization methods: random (cold-starting), warm-starting, and an ideal method. The results are averaged over 10 random seeds. The left panel shows test accuracy, the middle panel shows the number of learned features, and the right panel shows training time. The ideal method achieves the best performance in terms of test accuracy and training time, while warm-starting performs significantly worse than the random and ideal methods.", "section": "Experiments"}]