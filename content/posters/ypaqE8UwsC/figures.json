[{"figure_path": "ypaqE8UwsC/figures/figures_3_1.jpg", "caption": "Figure 1: Performance comparison of federated and centralized offline RL algorithms.", "description": "This figure compares the performance of various offline reinforcement learning algorithms on the Hopper environment from MuJoCo.  It showcases the results for different approaches including centralized offline RL (trained on combined data from all clients), individual offline RL (trained separately on each client's data), and three federated offline RL approaches (Fed-A, Fed-AC, and FEDORA). FEDORA is shown to significantly outperform the other federated approaches and performs comparably to centralized training, despite the use of only local data at each client, highlighting its strength in handling heterogeneous data and learning from ensembles of policies.", "section": "4 Federated Offline Reinforcement Learning"}, {"figure_path": "ypaqE8UwsC/figures/figures_6_1.jpg", "caption": "Figure 2: Evaluation of algorithms on different MuJoCo environments.", "description": "The figure compares the performance of FEDORA and other federated offline reinforcement learning algorithms across three MuJoCo environments: HalfCheetah, Hopper, and Walker2D.  The x-axis represents the communication round, and the y-axis represents the cumulative reward achieved by the federated policy (server policy). The shaded areas represent the standard deviation across multiple runs.  It shows that FEDORA consistently outperforms the other algorithms, especially in the later communication rounds. In each environment, FEDORA converges to a higher cumulative reward than other algorithms, indicating its superior performance in learning high-quality policies from heterogeneous offline data in a federated setting.", "section": "6.1 Experiments on Simulated Environments"}, {"figure_path": "ypaqE8UwsC/figures/figures_8_1.jpg", "caption": "Figure 4: Effect of varying the number of (a) local gradient steps, (b) participating clients in each round, and (c) expert clients in FEDORA.", "description": "This figure shows the result of an ablation study on FEDORA, investigating the impact of three key factors: the number of local gradient steps performed by each client before model aggregation, the proportion of clients participating in each round of federation, and the percentage of clients possessing high-quality expert data.  Each subplot displays the cumulative episodic reward of the server policy across communication rounds, under varying conditions of the parameter in question. The shaded areas represent standard deviation. The results demonstrate FEDORA's robustness to variations in these parameters, indicating its suitability for real-world applications with diverse client capabilities and data quality.", "section": "Sensitivity to Client Updates and Data Quality"}, {"figure_path": "ypaqE8UwsC/figures/figures_8_2.jpg", "caption": "Figure 5: Evaluation of FEDORA and other federated baselines for a mobile robot navigation task in the presence of an obstacle.", "description": "This figure presents the results of a real-world mobile robot navigation experiment.  Subfigures (a) and (b) show the trajectories taken by different behavior policies and the learned policies from various federated offline RL algorithms, respectively.  The goal is to navigate to a target location while avoiding an obstacle.  (c) shows a comparison of the cumulative rewards achieved by these algorithms over communication rounds. FEDORA demonstrates the ability to successfully navigate to the target while avoiding the obstacle.", "section": "6.2 Real-World Experiments on TurtleBot"}, {"figure_path": "ypaqE8UwsC/figures/figures_8_3.jpg", "caption": "Figure 1: Performance comparison of federated and centralized offline RL algorithms.", "description": "The figure shows a comparison of the performance of different offline reinforcement learning algorithms on the Hopper environment from MuJoCo. The algorithms compared are: FEDORA, centralized training, individual training (expert data), individual training (medium data), Fed-A, and Fed-AC. The x-axis represents the algorithm used, and the y-axis represents the cumulative reward. FEDORA outperforms other algorithms, even surpassing centralized training which uses data from all clients. This illustrates the challenges of federated offline RL and the potential benefits of FEDORA's approach.", "section": "4 Federated Offline Reinforcement Learning"}, {"figure_path": "ypaqE8UwsC/figures/figures_9_1.jpg", "caption": "Figure 6: Turtle-Bot3 Burger.", "description": "This figure shows the TurtleBot3 Burger robot used in the real-world experiments.  The robot is a small, two-wheeled differential drive robot equipped with sensors (LIDAR, wheel encoders) and a computer for processing. This platform is used for the real-world validation of the FEDORA algorithm on a mobile robot navigation task.", "section": "6.2 Real-World Experiments on TurtleBot"}, {"figure_path": "ypaqE8UwsC/figures/figures_13_1.jpg", "caption": "Figure 7: Ablation Studies.", "description": "This figure presents ablation study results to show the impact of each component of FEDORA on the performance. The left subfigure (a) shows the effect of sequentially adding one algorithm component at a time, starting from the naive approach (Fed-A). The right subfigure (b) illustrates the effect of removing individual components from the FEDORA algorithm.", "section": "B Additional Experiments"}, {"figure_path": "ypaqE8UwsC/figures/figures_14_1.jpg", "caption": "Figure 8: Ablation study of decaying mechanism on Walker2d environment (setting similar to Fig 7).", "description": "The figure shows the ablation study of the decaying mechanism in the Walker2D environment, comparing FEDORA with decay and without decay.  It presents two subfigures: (a) Training curve showing the server reward over communication rounds; (b) Peak performance showing the mean and standard deviation of the peak server reward achieved by each algorithm.", "section": "B.2 Ablation of Decaying mechanism on Walker Environment"}, {"figure_path": "ypaqE8UwsC/figures/figures_15_1.jpg", "caption": "Figure 1: Performance comparison of federated and centralized offline RL algorithms.", "description": "This figure compares the performance of several algorithms on the Hopper environment from MuJoCo.  The algorithms include centralized training (combining all data), individual offline RL (training on each client's data separately), Fed-A (federating only the actor), Fed-AC (federating both actor and critic), and FEDORA (the proposed algorithm). The results show that FEDORA significantly outperforms all the other federated approaches and achieves comparable performance to centralized training, which is a significant advantage considering the distributed nature of federated learning and the privacy advantages it affords.", "section": "4 Federated Offline Reinforcement Learning"}, {"figure_path": "ypaqE8UwsC/figures/figures_15_2.jpg", "caption": "Figure 1: Performance comparison of federated and centralized offline RL algorithms.", "description": "This figure shows the comparison of cumulative rewards achieved by different offline reinforcement learning algorithms.  The algorithms are compared across various scenarios: centralized training (combining all data), individual offline RL (training on individual client data), naive federated offline RL (Fed-A, Fed-AC), and the proposed FEDORA algorithm. The results highlight the challenges of naive federation and the superior performance of FEDORA, which surpasses other approaches, including centralized training on combined data, in terms of cumulative rewards.  The plot also visually represents the variability in performance (standard deviation) across different runs.", "section": "4 Federated Offline Reinforcement Learning"}, {"figure_path": "ypaqE8UwsC/figures/figures_16_1.jpg", "caption": "Figure 2: Evaluation of algorithms on different MuJoCo environments.", "description": "This figure shows the performance comparison of different federated offline reinforcement learning algorithms and centralized training across various MuJoCo continuous control environments.  The x-axis represents the communication round, and the y-axis represents the cumulative episodic reward achieved by the server policy.  The plot shows that FEDORA consistently outperforms the other federated approaches (Fed-A, Fed-AC, Fed-AC-Prox, HDAFL) and sometimes even matches or exceeds the centralized training approach.", "section": "6.1 Experiments on Simulated Environments"}, {"figure_path": "ypaqE8UwsC/figures/figures_16_2.jpg", "caption": "Figure 4: Effect of varying the number of (a) local gradient steps, (b) participating clients in each round, and (c) expert clients in FEDORA.", "description": "This figure demonstrates the impact of different hyperparameters on FEDORA's performance.  The subfigures show the cumulative episodic rewards over communication rounds for varying (a) numbers of local gradient steps, (b) percentages of clients participating in each round, and (c) percentages of clients having expert data.  The results illustrate FEDORA's robustness to changes in these hyperparameters.", "section": "Sensitivity to Client Updates and Data Quality"}, {"figure_path": "ypaqE8UwsC/figures/figures_17_1.jpg", "caption": "Figure 13: Variable Dataset Size", "description": "This figure compares the performance of FEDORA with clients having variable dataset sizes against FEDORA with a fixed dataset size. The experiment uses the Hopper-v2 environment with 10 clients (5 with expert data and 5 with medium data). Dataset sizes vary from 4000 to 8000 samples.  The results show that FEDORA performs well regardless of variations in dataset sizes, highlighting its robustness and adaptability to heterogeneous data.", "section": "B.8 Centralized training with other Offline RL algorithms"}, {"figure_path": "ypaqE8UwsC/figures/figures_17_2.jpg", "caption": "Figure 1: Performance comparison of federated and centralized offline RL algorithms.", "description": "The figure shows a performance comparison of different offline reinforcement learning (RL) algorithms in the Hopper environment. It compares centralized training (combining all data), individual offline RL (using only data from a single client), and three federated offline RL approaches (Fed-A, Fed-AC, and FEDORA). The results demonstrate that FEDORA significantly outperforms the other federated methods and achieves comparable performance to the centralized approach, which is generally not feasible in real-world scenarios.", "section": "4 Federated Offline Reinforcement Learning"}, {"figure_path": "ypaqE8UwsC/figures/figures_17_3.jpg", "caption": "Figure 1: Performance comparison of federated and centralized offline RL algorithms.", "description": "This figure compares the performance of different offline reinforcement learning algorithms in the Hopper environment.  The algorithms include centralized training (combining all data from various clients), individual offline RL training on each client\u2019s data, and different federated learning approaches (Fed-A, Fed-AC, FEDORA). FEDORA significantly outperforms the other methods, demonstrating its effectiveness in learning from heterogeneous data.", "section": "4 Federated Offline Reinforcement Learning"}, {"figure_path": "ypaqE8UwsC/figures/figures_17_4.jpg", "caption": "Figure 16: Comparison with different weighing mechanism based on average reward in the dataset", "description": "This figure compares the performance of FEDORA with two other algorithms that use different weighting schemes based on the average reward of the datasets. It shows that FEDORA outperforms both baselines. This is because FEDORA's weighting scheme combines policies based on their performance, while the other two baselines use average reward, which doesn't vary much in this setting.", "section": "B.10 Different Weighing Mechanisms"}]