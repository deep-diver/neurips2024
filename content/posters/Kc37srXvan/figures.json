[{"figure_path": "Kc37srXvan/figures/figures_0_1.jpg", "caption": "Figure 1: Comprehensive comparisons between our PointMamba and its Transformer-based counterparts [33, 6, 11]. (a) Without bells and whistles, our PointMamba achieve better performance than the representative Transformer-based methods on the various point cloud analysis datasets. (b)-(d) The Transformer presents quadratic complexity, while our PointMamba has linear complexity. For example, with the length of point tokens increasing, we significantly reduce GPU memory usage and FLOPs and have a faster inference speed compared to the most convincing Transformer-based method, i.e., PointMAE [33].", "description": "This figure presents a comprehensive comparison of PointMamba against several state-of-the-art Transformer-based methods for point cloud analysis.  Subfigure (a) shows the superior performance of PointMamba across different datasets, while subfigures (b), (c), and (d) illustrate its linear time complexity compared to the quadratic complexity of Transformer-based methods.  This is shown through significantly reduced GPU memory usage, FLOPs, and faster inference speed as the length of point tokens increases.", "section": "Abstract"}, {"figure_path": "Kc37srXvan/figures/figures_4_1.jpg", "caption": "Figure 2: The pipeline of our PointMamba. It is simple and elegant, without bells and whistles. We first utilize Farthest Point Sampling (FPS) to select the key points. Then, we propose to utilize two types of space-filling curves, including Hilbert and Trans-Hilbert, to generate the serialized key points. Based on these, the KNN is used to form point patches, which will be fed to the token embedding layer to generate the serialized point tokens. To indicate the tokens generated from which space-filling curve, the order indicator is proposed. The encoder is extremely simple, consisting of Nx plain and non-hierarchical Mamba blocks.", "description": "The figure illustrates the architecture of PointMamba, a novel point cloud analysis method. It starts with farthest point sampling (FPS) to select key points and uses Hilbert and Trans-Hilbert curves to serialize these points, maintaining spatial locality.  KNN forms point patches, which are then converted to tokens using a token embedding layer. An order indicator distinguishes tokens from different scanning curves.  The encoder is a series of plain Mamba blocks.", "section": "4 PointMamba"}, {"figure_path": "Kc37srXvan/figures/figures_5_1.jpg", "caption": "Figure 2: The pipeline of our PointMamba. It is simple and elegant, without bells and whistles. We first utilize Farthest Point Sampling (FPS) to select the key points. Then, we propose to utilize two types of space-filling curves, including Hilbert and Trans-Hilbert, to generate the serialized key points. Based on these, the KNN is used to form point patches, which will be fed to the token embedding layer to generate the serialized point tokens. To indicate the tokens generated from which space-filling curve, the order indicator is proposed. The encoder is extremely simple, consisting of Nx plain and non-hierarchical Mamba blocks.", "description": "This figure illustrates the architecture of PointMamba, a simple and efficient point cloud analysis model. It begins by using Farthest Point Sampling (FPS) to select key points from the input point cloud.  These key points are then processed using two space-filling curves (Hilbert and Trans-Hilbert) to create serialized sequences. A k-Nearest Neighbors (KNN) algorithm groups nearby points into patches, which are fed into a token embedding layer. An order indicator is added to distinguish between the two space-filling curve sequences. Finally, a series of plain, non-hierarchical Mamba blocks process these tokens to extract features.", "section": "4 PointMamba"}, {"figure_path": "Kc37srXvan/figures/figures_5_2.jpg", "caption": "Figure 3: An intuitive illustration of global modeling from PointMamba.", "description": "This figure illustrates how PointMamba achieves global modeling.  The left side shows the processing of point tokens serialized using a Hilbert curve.  The right side shows processing of tokens serialized using a Trans-Hilbert curve. The dashed boxes represent the individual processing sequences. The key point is the arrow between the two sequences, indicating that the global information learned from the Hilbert sequence informs the processing of the Trans-Hilbert sequence, resulting in a model that leverages global context from different scanning perspectives.", "section": "4 PointMamba"}, {"figure_path": "Kc37srXvan/figures/figures_8_1.jpg", "caption": "Figure 5: Different variant of PointMamba. (a) Directly removing the SSM part. (b) Replacing SSM with attention. (c) Replacing SSM with MLP. (d) Ours PointMamba with Selective SSM.", "description": "This figure shows an ablation study on the PointMamba architecture. It compares four variants: (a) a baseline without the selective state space model (SSM); (b) replacing the SSM with a self-attention mechanism; (c) replacing the SSM with a multi-layer perceptron (MLP); and (d) the proposed PointMamba architecture with the selective SSM.  The figure visually represents the different components and their connections within the architecture, highlighting the impact of the SSM on PointMamba's performance.", "section": "5.3 Analysis and ablation study"}, {"figure_path": "Kc37srXvan/figures/figures_16_1.jpg", "caption": "Figure 2: The pipeline of our PointMamba. It is simple and elegant, without bells and whistles. We first utilize Farthest Point Sampling (FPS) to select the key points. Then, we propose to utilize two types of space-filling curves, including Hilbert and Trans-Hilbert, to generate the serialized key points. Based on these, the KNN is used to form point patches, which will be fed to the token embedding layer to generate the serialized point tokens. To indicate the tokens generated from which space-filling curve, the order indicator is proposed. The encoder is extremely simple, consisting of Nx plain and non-hierarchical Mamba blocks.", "description": "The figure illustrates the PointMamba architecture.  It starts with an input point cloud that undergoes farthest point sampling (FPS) to select key points. These points are then processed using Hilbert and Trans-Hilbert space-filling curves to create serialized sequences. A k-nearest neighbor (KNN) algorithm groups nearby points into patches, which are converted into tokens by a token embedding layer. An order indicator distinguishes tokens from the different space-filling curves. These tokens are then fed into a series of Mamba blocks (the encoder) to extract features before being passed to the task head.", "section": "4 PointMamba"}, {"figure_path": "Kc37srXvan/figures/figures_17_1.jpg", "caption": "Figure 7: The qualitative results of mask predictions of our PointMamba on ShapeNet validation set.", "description": "This figure shows the qualitative results of the mask prediction task on the ShapeNet validation set, comparing the input point cloud, the masked point cloud (where a portion of the points have been removed), and the reconstructed point cloud generated by PointMamba. The results demonstrate PointMamba's ability to effectively reconstruct the missing points, indicating its effectiveness in handling missing data in point cloud analysis.", "section": "Appendix D Qualitative Analysis"}, {"figure_path": "Kc37srXvan/figures/figures_18_1.jpg", "caption": "Figure 8: The qualitative results of part segmentation of our PointMamba on ShapeNetPart.", "description": "This figure shows a qualitative comparison of the part segmentation results obtained using the proposed PointMamba model against the ground truth labels.  The top row displays the ground truth segmentations for several point cloud objects from the ShapeNetPart dataset, with different parts of the object colored in different colors (e.g., red, green, blue).  The bottom row displays the corresponding segmentations predicted by PointMamba. The figure visually demonstrates the model's ability to accurately segment different parts of the objects.", "section": "Appendix D Qualitative Analysis"}]