[{"figure_path": "AKBTFQhCjm/figures/figures_1_1.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model s<sub>t</sub> and the fine-tuned h-transform h<sub>t</sub> are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|x\u0302<sub>0</sub>) as part of the architecture. Here x\u0302<sub>0</sub> denotes the unconditional denoised estimate given s<sub>t</sub>(x<sub>t</sub>). During training, we only need to fine-tune h<sub>t</sub> (usually 4-9% the size of s<sub>t</sub>) using a small dataset of paired measurements, keeping s<sub>t</sub> fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the DEFT reverse diffusion process.  A pre-trained unconditional diffusion model (s<sub>t</sub>) and a small, fine-tuned network representing the generalized h-transform (h<sub>t</sub>) are used in each sampling step. The h-transform network incorporates a guidance term to condition the sampling process.  The figure highlights the efficiency of DEFT because only the h<sub>t</sub> network needs to be trained.  No backpropagation through the large pre-trained model is required during either training or sampling, leading to significant speedups.", "section": "3.1 DEFT: Fine-tuning by score matching"}, {"figure_path": "AKBTFQhCjm/figures/figures_6_1.jpg", "caption": "Figure 2: Results for inpainting. We show the ground truth with the inpainting mask superimposed.", "description": "This figure shows the results of an inpainting task, comparing different methods (DPS, IGDM, DDRM, RED-diff, and DEFT) against the ground truth.  The images demonstrate the effectiveness of each method at filling in missing parts of an image based on the provided context. The superimposed inpainting mask highlights the regions that were missing in the original images.", "section": "4.1 Image reconstruction"}, {"figure_path": "AKBTFQhCjm/figures/figures_7_1.jpg", "caption": "Figure 3: Results for non-linear deblurring. We show both the ground truth, the measurements and samples for DPS, RED-diff and DEFT. DEFT is able to reconstruct high-quality images.", "description": "This figure shows a comparison of the results of different methods on a non-linear deblurring task. The top row displays a chicken image: the ground truth, the blurry measurements, and the reconstructed images from DPS, RED-diff, and DEFT.  The bottom row shows the same comparison but with an ostrich image.  The figure visually demonstrates that DEFT produces higher quality reconstructed images compared to the other methods.", "section": "4.1 Image reconstruction"}, {"figure_path": "AKBTFQhCjm/figures/figures_8_1.jpg", "caption": "Figure 4: Reconstructions for computed tomography on LoDoPab-CT", "description": "This figure shows the ground truth and the reconstructions generated by DPS, RED-diff, and DEFT for computed tomography on the LoDoPab-CT dataset.  It visually demonstrates the performance of each method in reconstructing the image from noisy measurements. DEFT shows superior performance compared to other methods.", "section": "4.2 Computed tomography"}, {"figure_path": "AKBTFQhCjm/figures/figures_9_1.jpg", "caption": "Figure 5: Comparison of DPS, DEFT and amortised training for motif scaffolding for 12 contiguous targets. 4% and 9% are the relative sizes of the h-transform compared to the unconditional model.", "description": "This figure shows a bar chart comparing the \nin-silico success rates of three different methods for motif scaffolding: DPS, DEFT with a small network (4% of the size of the unconditional model), and DEFT with a larger network (9% of the size of the unconditional model).  The x-axis represents the 12 different contiguous target protein motifs tested, and the y-axis shows the percentage of successful motif scaffolds generated by each method.  The figure highlights that DEFT, even with a small network, significantly outperforms DPS.  The amortised method achieves higher success rates overall but requires more parameters and training.", "section": "4.3 Conditional protein design: motif scaffolding"}, {"figure_path": "AKBTFQhCjm/figures/figures_9_2.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model st and the fine-tuned h-transform hi are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|20) as part of the architecture. Here 20 denotes the unconditional denoised estimate given sf(xt). During training, we only need to fine-tune hi (usually 4-9% the size of st) using a small dataset of paired measurements, keeping so fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the DEFT reverse diffusion process.  A pre-trained unconditional diffusion model and a small, fine-tuned network (the h-transform) are combined at each step of the sampling process.  The h-transform network is trained using a small dataset to learn a conditional transformation while leaving the main model unchanged.  This setup allows for fast conditional generation without backpropagation through the main model, leading to speed improvements.", "section": "3.1 DEFT: Fine-tuning by score matching"}, {"figure_path": "AKBTFQhCjm/figures/figures_23_1.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model st and the fine-tuned h-transform his are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|20) as part of the architecture. Here 20 denotes the unconditional denoised estimate given sf(xt). During training, we only need to fine-tune hi (usually 4-9% the size of st) using a small dataset of paired measurements, keeping so fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the architecture of the DEFT reverse diffusion process.  The pre-trained unconditional diffusion model and a smaller, fine-tuned h-transform network are combined at each step of the sampling process. The h-transform network, parameterized to include a guidance term, is significantly smaller than the unconditional model (4-9%).  Crucially, the figure highlights that training only requires fine-tuning the h-transform, leaving the larger pre-trained model unchanged, and during sampling, backpropagation through either model is unnecessary, leading to faster inference.", "section": "1 Introduction"}, {"figure_path": "AKBTFQhCjm/figures/figures_24_1.jpg", "caption": "Figure 2: Results for inpainting. We show the ground truth with the inpainting mask superimposed.", "description": "This figure displays the results of an inpainting task using different methods including DEFT. The ground truth image is shown alongside the results obtained by DPS, IGDM, DDRM, RED-diff, and DEFT. Each image shows the result of inpainting where part of the image is missing or obscured by a mask.  The comparison allows a visual assessment of the effectiveness of each method in restoring the missing portions of the image while maintaining image quality.", "section": "4.1 Image reconstruction"}, {"figure_path": "AKBTFQhCjm/figures/figures_24_2.jpg", "caption": "Figure 2: Results for inpainting. We show the ground truth with the inpainting mask superimposed.", "description": "This figure shows the results of an inpainting task, where a portion of an image is missing.  The ground truth image is shown alongside the results from different methods (DPS, IGDM, DDRM, RED-diff, and DEFT).  The superimposed mask highlights the area that was originally missing and needed to be inpainted by the different algorithms.", "section": "4.1 Image reconstruction"}, {"figure_path": "AKBTFQhCjm/figures/figures_25_1.jpg", "caption": "Figure 2: Results for inpainting. We show the ground truth with the inpainting mask superimposed.", "description": "This figure shows the results of an inpainting experiment.  The top row shows the results for one image and the bottom row shows the results for another image.  Each row displays the ground truth image (leftmost), followed by the results generated using DPS, IGDM, DDRM, RED-diff, and DEFT methods.  The inpainting masks used are superimposed on the ground truth images, showing the regions where the model was tasked with filling in missing image information. The images demonstrate the different methods' abilities to reconstruct natural-looking details within the masked regions.", "section": "4.1 Image reconstruction"}, {"figure_path": "AKBTFQhCjm/figures/figures_25_2.jpg", "caption": "Figure 2: Results for inpainting. We show the ground truth with the inpainting mask superimposed.", "description": "This figure shows the results of an inpainting task. The top row displays the ground truth images, with the inpainting masks superimposed to show the regions that were missing. The bottom four rows depict the inpainting results generated by four different methods: DPS, IGDM, DDRM, RED-diff, and DEFT.  Each column represents a different image and provides a visual comparison of the performance of the different methods in reconstructing the missing parts of the image. The figure demonstrates DEFT's ability to produce high-quality inpainting results that are visually similar to the ground truth.", "section": "4.1 Image reconstruction"}, {"figure_path": "AKBTFQhCjm/figures/figures_30_1.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model s and the fine-tuned h-transform h are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|z0) as part of the architecture. Here z0 denotes the unconditional denoised estimate given sf(xt). During training, we only need to fine-tune h (usually 4-9% the size of s) using a small dataset of paired measurements, keeping s fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the architecture of DEFT (Doob's h-transform Efficient Fine-Tuning), a method for conditional generation using diffusion models.  A pre-trained unconditional diffusion model is combined with a smaller, fine-tuned network (h-transform) to generate conditional samples. The h-transform network incorporates guidance information to direct the sampling process towards the desired condition.  The figure highlights that during training, only the h-transform is updated, leaving the larger unconditional model unchanged, leading to faster training times. Similarly, during sampling, backpropagation is only needed for the smaller network, resulting in faster sampling.", "section": "3.1 DEFT: Fine-tuning by score matching"}, {"figure_path": "AKBTFQhCjm/figures/figures_31_1.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model st and the fine-tuned h-transform his are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|20) as part of the architecture. Here 20 denotes the unconditional denoised estimate given sf(xt). During training, we only need to fine-tune hi (usually 4-9% the size of st) using a small dataset of paired measurements, keeping so fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the architecture of DEFT, a method for efficient fine-tuning of diffusion models.  It shows how a pre-trained unconditional diffusion model is combined with a small, fine-tuned network (the h-transform) at each step of the reverse diffusion process. The h-transform is trained on a small dataset of paired measurements, and its inclusion allows for faster sampling and improved conditional generation without requiring backpropagation through the larger model.", "section": "3 DEFT: Fine-tuning by score matching"}, {"figure_path": "AKBTFQhCjm/figures/figures_33_1.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model st and the fine-tuned h-transform hi are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|20) as part of the architecture. Here 20 denotes the unconditional denoised estimate given sf(xt). During training, we only need to fine-tune hi (usually 4-9% the size of st) using a small dataset of paired measurements, keeping st fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the architecture of the DEFT reverse diffusion process. It shows how a pre-trained unconditional diffusion model and a small fine-tuned network are combined to perform conditional generation. The figure highlights the key components of the process, including the h-transform, the guidance term, and the training and sampling steps. It also emphasizes the efficiency of DEFT by showing that only a small network needs to be fine-tuned, while keeping the larger unconditional network fixed. This results in significant speedups during evaluation.", "section": "3.1 DEFT: Fine-tuning by score matching"}, {"figure_path": "AKBTFQhCjm/figures/figures_33_2.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model st and the fine-tuned h-transform his are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|20) as part of the architecture. Here 20 denotes the unconditional denoised estimate given sf(xt). During training, we only need to fine-tune hi (usually 4-9% the size of st) using a small dataset of paired measurements, keeping so fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the DEFT reverse diffusion process.  It shows how a pre-trained unconditional diffusion model is combined with a small, fine-tuned network (the h-transform) at each step of the sampling process. The h-transform network incorporates the guidance term (\u2207 lnp(y|20)) to condition the generation. The diagram highlights that only the h-transform requires training, using a small dataset. Importantly, during sampling, backpropagation isn't needed through either network, enabling faster inference.", "section": "3.1 DEFT: Fine-tuning by score matching"}, {"figure_path": "AKBTFQhCjm/figures/figures_34_1.jpg", "caption": "Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model st and the fine-tuned h-transform hi are combined at every sampling step. We propose a special network to parametrise the h-transform including the guidance term \u2207 lnp(y|20) as part of the architecture. Here 20 denotes the unconditional denoised estimate given sf(xt). During training, we only need to fine-tune hi (usually 4-9% the size of st) using a small dataset of paired measurements, keeping so fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.", "description": "This figure illustrates the reverse diffusion process used in the DEFT model.  The process combines a pre-trained unconditional diffusion model (st) with a fine-tuned, smaller h-transform network (hi) at each step.  The h-transform network is designed to efficiently learn the conditional distribution, using the gradient of the log-likelihood (\u2207 lnp(y|20)) as part of its architecture.  The figure highlights that only hi needs to be trained, with st remaining unchanged, resulting in faster training and sampling speeds due to avoiding backpropagation through the large st model during inference.", "section": "1 Introduction"}]