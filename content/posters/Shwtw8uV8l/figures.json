[{"figure_path": "Shwtw8uV8l/figures/figures_1_1.jpg", "caption": "Figure 1: Schematic illustration of dual-stream interactive behaviors, including YTMT [22], MuGI [23], and our proposed Dual-Attention Interaction (DAI) mechanisms, where F<sup>T</sup> and F<sup>R</sup> represent the feature flows of transmission layer and reflection layer respectively. The superscript <sup>I</sup> of the feature flows denotes the number of building blocks traversed to derive the flows.", "description": "This figure schematically illustrates three different dual-stream interactive mechanisms for single image reflection separation.  (a) YTMT uses element-wise operations and a fusion process for interaction between transmission and reflection layer features. (b) MuGI also employs a fusion process, but the interaction is more direct. (c) The proposed DAI mechanism leverages dual-stream self-attention and cross-attention to explicitly capture both intra-layer and inter-layer feature correlations, offering a more sophisticated interaction strategy.", "section": "1 Introduction"}, {"figure_path": "Shwtw8uV8l/figures/figures_3_1.jpg", "caption": "Figure 2: (a) The overall architecture of our proposed Dual-Stream Interactive Transformer, which consists of a dual-architecture interactive encoder and a dual-stream interactive decoder, injecting the global prior into local features and aggregating them in dual-stream from bottom to up. (b) A visual illustration of our proposed dual-attention interactive block, which provides both intra-layer self-attention and inter-layer cross-attention, capturing holistic feature correlations.", "description": "This figure shows the overall architecture of the proposed Dual-Stream Interactive Transformer (DSIT) and a detailed illustration of its Dual-Attention Interactive Block (DAIB).  The DSIT consists of two main components: a Dual-Architecture Interactive Encoder (DAIE) and a Dual-Stream Interactive Decoder (DSID). The DAIE combines global and local feature extractors, injecting global priors into the dual-stream local features through cross-architecture interactions. The DSID then uses DAIBs to refine and aggregate dual-stream features, ultimately separating the transmission and reflection layers. The DAIB itself employs dual-stream self-attention and cross-attention mechanisms to capture both intra-layer and inter-layer feature correlations, enhancing the accuracy of reflection separation.", "section": "3 Methodology"}, {"figure_path": "Shwtw8uV8l/figures/figures_5_1.jpg", "caption": "Figure 3: Visualization of extracted local priors, global priors, their cross-architecture-interacted dual-stream features and features after the DAIBs of two reflection-superimposed inputs. All the above features are from the second level of our DSIT model and are channel-wise averaged to display.", "description": "This figure visualizes the feature maps at the second level of the DSIT model for two different reflection-superimposed input images.  It shows the local priors extracted by the CNN, the global priors from the pre-trained Transformer, how these are combined through cross-architecture interaction, and the resulting dual-stream features before and after passing through the Dual-Attention Interactive Blocks (DAIBs). The channel-wise averaging helps to better visualize the information flow and the effects of the different processing stages.", "section": "3. Methodology"}, {"figure_path": "Shwtw8uV8l/figures/figures_7_1.jpg", "caption": "Figure 4: Visual comparison of transmission layer predictions between previous state-of-the-arts and ours on samples from Real20 [60] and SIR\u00b2 datasets. Please note the areas in the boxes.", "description": "This figure presents a visual comparison of transmission layer predictions.  It compares the results of several different single image reflection separation (SIRS) methods on two example images.  The methods compared include several state-of-the-art techniques as well as the authors' proposed method. The two example images are chosen to represent different difficulty levels in reflection separation. Highlighted areas showcase differences in results.", "section": "4.2 Performance Evaluation"}, {"figure_path": "Shwtw8uV8l/figures/figures_8_1.jpg", "caption": "Figure 5: Visual comparison of transmission predictions between previous state-of-the-arts and ours in real-world scenarios additionally captured in this paper. The broad advantages demonstrated by our method across these diverse conditions highlight its superior generalization capability.", "description": "This figure shows a qualitative comparison of the transmission layer predictions of several state-of-the-art single image reflection separation (SIRS) methods and the proposed Dual-Stream Interactive Transformer (DSIT) method on real-world images.  The images depict diverse scenes and reflection challenges, demonstrating the superior generalization capability of DSIT across various conditions.", "section": "4.2 Performance Evaluation"}, {"figure_path": "Shwtw8uV8l/figures/figures_9_1.jpg", "caption": "Figure 3: Visualization of extracted local priors, global priors, their cross-architecture-interacted dual-stream features and features after the DAIBs of two reflection-superimposed inputs. All the above features are from the second level of our DSIT model and are channel-wise averaged to display.", "description": "This figure visualizes the different feature extractions at the second level of the proposed DSIT model. It shows the local priors from a CNN, global priors from a pre-trained Transformer, the features after cross-architecture interaction, and finally the features after the dual-attention interactive blocks. The visualization helps understand how the model integrates different types of information and refines features progressively.", "section": "3. Methodology"}, {"figure_path": "Shwtw8uV8l/figures/figures_15_1.jpg", "caption": "Figure 7: Visual results of DSIT variants shown in Tab. 3. The case is sampled from the SIR\u00b2 dataset.", "description": "This figure shows the visual results obtained using different variants of the Dual-Stream Interactive Transformer (DSIT) model.  The variations tested include different global prior extractors (GPEs), cross-architecture interaction (CAI) methods, Dual-Attention Interactive Block (DAIB) designs, the inclusion or exclusion of layered relative position biases (LRPB), and the use or non-use of reflection mixup (RefMix) data augmentation.  The results demonstrate the impact of each component on the model's ability to accurately separate the reflection and transmission layers in a single image.  The input image and the ground truth are shown for comparison.", "section": "4.3 Ablation Study"}, {"figure_path": "Shwtw8uV8l/figures/figures_16_1.jpg", "caption": "Figure 8: The RefMix results according to different \u03b3s, which enrich the real pairs during training.", "description": "This figure shows the results of the Reflection Mixup (RefMix) data augmentation technique used in the paper. RefMix adjusts the intensity of the reflection layer by blending the input image (I) and the transmission layer (T) at different ratios (\u03b3).  The series of images demonstrates how varying \u03b3 values from 0 to 1 affect the resulting image, enriching the training data with a wider range of reflection intensities.", "section": "4.2 Performance Evaluation"}, {"figure_path": "Shwtw8uV8l/figures/figures_16_2.jpg", "caption": "Figure 4: Visual comparison of transmission layer predictions between previous state-of-the-arts and ours on samples from Real20 [60] and SIR\u00b2 datasets. Please note the areas in the boxes.", "description": "This figure compares the transmission layer predictions of several state-of-the-art single image reflection separation (SIRS) models with the proposed DSIT model.  Two examples are shown, one from the Real20 dataset and one from the SIR\u00b2 dataset. The boxes highlight areas where the differences between the models are most apparent.  The results demonstrate the superior performance of the proposed DSIT model in accurately reconstructing the transmission layer, especially in challenging cases with complex reflections.", "section": "4.2 Performance Evaluation"}, {"figure_path": "Shwtw8uV8l/figures/figures_17_1.jpg", "caption": "Figure 3: Visualization of extracted local priors, global priors, their cross-architecture-interacted dual-stream features and features after the DAIBs of two reflection-superimposed inputs. All the above features are from the second level of our DSIT model and are channel-wise averaged to display.", "description": "This figure visualizes the different feature stages of the Dual-Stream Interactive Transformer (DSIT) model.  It shows the local priors, global priors, and how they interact through cross-architecture interactions (CAI) and Dual-Attention Interactive Blocks (DAIBs). The visualization helps demonstrate the information flow and how the model progressively refines the features to achieve reflection separation. The features are averaged across channels for clearer display.", "section": "3.1 Dual-Attention Interactive Block"}, {"figure_path": "Shwtw8uV8l/figures/figures_17_2.jpg", "caption": "Figure 11: Visual comparison of transmission layer predictions between previous arts and ours. The case is sampled from the SIR\u00b2 dataset.", "description": "This figure shows a visual comparison of the transmission layer predictions from various state-of-the-art (SOTA) single image reflection separation (SIRS) methods and the proposed method (DSIT).  The results are displayed for different models on a sample image from the SIR\u00b2 dataset. The ground truth transmission layer is included for comparison, allowing for a qualitative assessment of the performance of each model in separating the transmission and reflection components of the superimposed image.", "section": "4.2 Performance Evaluation"}, {"figure_path": "Shwtw8uV8l/figures/figures_18_1.jpg", "caption": "Figure 12: Visual comparison of reflection layer predictions between previous arts and ours.", "description": "This figure shows a visual comparison of the reflection layer predictions from various state-of-the-art (SOTA) single image reflection separation (SIRS) models and the proposed model (DSIT).  The results are presented for two different training data settings (I and II) of the DSIT model.  The figure demonstrates that the proposed DSIT model produces more accurate and visually appealing results compared to other models for the task of separating reflection layers from images.", "section": "4.2 Performance Evaluation"}, {"figure_path": "Shwtw8uV8l/figures/figures_18_2.jpg", "caption": "Figure 5: Visual comparison of transmission predictions between previous state-of-the-arts and ours in real-world scenarios additionally captured in this paper. The broad advantages demonstrated by our method across these diverse conditions highlight its superior generalization capability.", "description": "This figure compares the transmission layer predictions of various single image reflection separation (SIRS) methods, including the proposed DSIT model, on real-world images with diverse scenarios and reflection characteristics. It highlights the superior generalization capability of the DSIT model by demonstrating its effectiveness across different conditions.", "section": "4.2 Performance Evaluation"}]