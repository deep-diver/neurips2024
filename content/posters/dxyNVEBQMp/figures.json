[{"figure_path": "dxyNVEBQMp/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Training data are sampled for each time step from continuous sequences, exhibiting high temporal correlations. (b) Conventional approaches simply ignore this temporal information with a shuffled batch. (c) We address the temporal correlation between the samples for the first time, enabling the model to consider long-range dependencies that surpass the look-back window.", "description": "This figure illustrates the difference between conventional time series forecasting methods and the proposed approach.  Panel (a) shows how time series data is naturally temporally correlated. Panel (b) shows how conventional methods shuffle the data, losing temporal correlations. Panel (c) shows the proposed method that preserves temporal correlation, enabling the capture of long-range dependencies.", "section": "1 Introduction"}, {"figure_path": "dxyNVEBQMp/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Plug-in Spectral Attention (SA) module takes a subset of intermediate feature F and returns F' with long-range information beyond the look-back window. The model is trained end-to-end, and gradients flow through the SA module. (b) To capture the long-range dependency, SA stores momentums of feature F generated from the sequential inputs. Multiple momentum parameters ai capture dependencies across various ranges. (c) SA module computes F' by attending multiple low-frequency (Mai) and high-frequency (F \u2013 Mai) components and feature (F) using learnable Spectral Attention Matrix (SA-Matrix)", "description": "This figure illustrates the Spectral Attention (SA) module's architecture and functionality. Panel (a) shows how the SA module is integrated into a time series forecasting model. Panel (b) shows the mechanism by which the SA module stores momentums of the feature vector to capture long-range dependencies, using multiple momentum parameters to capture dependencies across various ranges. Finally, panel (c) illustrates how the SA module computes the output vector F' by attending to multiple low-frequency and high-frequency components of the feature vector F, using a learnable spectral attention matrix.", "section": "3 Methods"}, {"figure_path": "dxyNVEBQMp/figures/figures_4_1.jpg", "caption": "Figure 3: BSA module takes a sequentially-sampled mini batch {Xt,...Xt+B-1} and computes the corresponding EMA momentums {Mt,...Mt+B-1} over time. This is done via single matrix multiplication enabling parallelization. We made the momentum parameter a\u017c learnable, allowing the model to directly learn the periodicity of the information essential for the future prediction.", "description": "This figure illustrates the Batched Spectral Attention (BSA) module.  The BSA module processes a mini-batch of sequentially sampled time series data, rather than shuffling the data as in conventional methods.  It calculates exponential moving averages (EMA) of activations across multiple time steps within a mini-batch using a single matrix multiplication, enabling efficient parallelization. Notably, the momentum parameter (\u03b1i) is made learnable, allowing the model to dynamically adjust its sensitivity to different periodic patterns in the data for improved forecasting accuracy. This approach preserves temporal correlations and enables the model to capture long-range dependencies that surpass the look-back window.", "section": "3 Methods"}, {"figure_path": "dxyNVEBQMp/figures/figures_7_1.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure visualizes the analysis of the spectral attention (SA) matrix from a DLinear model trained for a 720-step prediction task on Weather and ETTh1 datasets.  Panel (a) presents a heatmap of the SA matrix, while panels (b), (c), and (d) display the attention and Fast Fourier Transform (FFT) graphs for specific channels within each dataset. These graphs help to illustrate how the SA module focuses on different frequency components for prediction and highlights the model's ability to capture long-range dependencies.", "section": "4.2 Synthetic datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_7_2.jpg", "caption": "Figure 5: Results of the iTransformer model on synthetic (a) ETTh1 and (b) ETTh2 datasets. The x-axis is the prediction length (96, 192, 336, 720), and the y-axis is the performance improvement (%) compared to the base model. Each color represents the different periods of the sine wave added to the natural data. O indicates original data and serves as the baseline.", "description": "This figure shows the performance improvement of the iTransformer model with BSA on synthetic datasets created by adding sine waves with different periods (100, 300, 1000) to the original ETTh1 and ETTh2 datasets. The x-axis represents the prediction length, and the y-axis shows the percentage improvement in MSE compared to the base model (no BSA). Each line represents a different sine wave period, with '0' representing the original dataset without added sine waves.", "section": "4.2 Synthetic datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_8_1.jpg", "caption": "Figure 6: Attention and FFT graphs on LUFL channel of the original and synthetic ETTh1 data (iTransformer, 720-step prediction). (a) is from the original data and (b)-(d) are from the synthetic data created by adding sine waves with periods of 100, 300, and 1000, respectively. The red arrows on the FFT graphs show the added synthetic signals. Full visualization is provided in Appendix C.1", "description": "This figure presents a comparative analysis of the attention and FFT graphs for the LUFL channel of the ETTh1 dataset, both in its original form and with added sine waves of varying periods (100, 300, and 1000). The analysis uses the iTransformer model with a 720-step prediction.  The red arrows in the FFT graphs highlight the added sine wave frequencies.  The purpose is to show how the proposed BSA method effectively captures and utilizes long-range patterns, even those beyond the look-back window. This is evident in the changes in both the attention and FFT graphs as longer sine wave periods are included.", "section": "4.2 Synthetic datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_8_2.jpg", "caption": "Figure 2: (a) Plug-in Spectral Attention (SA) module takes a subset of intermediate feature F and returns F' with long-range information beyond the look-back window. The model is trained end-to-end, and gradients flow through the SA module. (b) To capture the long-range dependency, SA stores momentums of feature F generated from the sequential inputs. Multiple momentum parameters ai capture dependencies across various ranges. (c) SA module computes F' by attending multiple low-frequency (Mai) and high-frequency (F \u2013 Mai) components and feature (F) using learnable Spectral Attention Matrix (SA-Matrix)", "description": "This figure shows the architecture of the Spectral Attention (SA) module. The SA module takes as input a subset of intermediate features from the base model and outputs features F' which contains long-range information beyond the look-back window. The figure illustrates the mechanism through which the SA module achieves this: (a) The SA module is a plug-in module, meaning it can be inserted into the model without modifying the base architecture. Gradients flow through the SA module during training. (b) To capture long-range dependencies, the SA module stores the momentum of the feature vector F from sequential inputs. Multiple momentum parameters with different smoothing factors capture dependencies across various ranges. (c) The SA module calculates F' by attending to multiple low-frequency and high-frequency components of F, using a learnable Spectral Attention Matrix.", "section": "3 Methods"}, {"figure_path": "dxyNVEBQMp/figures/figures_8_3.jpg", "caption": "Figure 7: Schematic diagram of the BSA insertion site on Transformer.", "description": "This figure shows the various locations within the Transformer model architecture where the Batched Spectral Attention (BSA) module can be inserted.  The numbers 1 through 7 indicate specific layers or blocks within the transformer, such as Input Embedding, Multi-Head Attention, and Feed Forward Network (FFN) layers. The BSA module's flexibility allows it to be integrated at different points within the model's architecture, impacting its ability to influence the flow of information and gradient information within the network.", "section": "3 Methods"}, {"figure_path": "dxyNVEBQMp/figures/figures_17_1.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure shows the analysis of the SA-matrix of the DLinear model for long-range dependency prediction. Panel (a) visualizes the SA-matrix as a heatmap. Panels (b), (c), and (d) present the attention and FFT graphs for Weather Temperature, Weather Solar radiation, and ETTh1 Hull, respectively. The graphs illustrate how Spectral Attention focuses on low-frequency components, effectively capturing long-range dependencies.", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_23_1.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure provides a detailed analysis of the Spectral Attention (SA) matrix learned by the DLinear model during a 720-step prediction task on the Weather and ETTh1 datasets. It includes a heatmap visualization of the SA matrix, which reveals the model's attention weights across different frequency components.  Panels (b), (c), and (d) display attention and Fast Fourier Transform (FFT) graphs for specific channels within each dataset, further illustrating how SA focuses on both low and high-frequency information during predictions.", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_24_1.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure shows an analysis of the Spectral Attention (SA) matrix from a DLinear model trained for a 720-step prediction task using weather and ETTh1 datasets.  Panel (a) displays a heatmap of the SA matrix which shows the weights applied to different frequencies.  Panels (b)-(d) show the attention applied to different frequencies and their corresponding Fast Fourier Transform (FFT) graphs, demonstrating how the model attends to various frequencies for each dataset.", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_25_1.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure analyzes the SA-matrix learned by the DLinear model during training for 720-step ahead prediction on Weather and ETTh1 datasets.  Panel (a) shows a heatmap of the SA-matrix, visually representing how much attention the model pays to different frequency components. Panels (b)-(d) show the attention weight distribution and frequency spectrum (FFT) of specific channels (Weather-Temperature, Weather-SWDR, and ETTh1-HULL), further illustrating the frequency components attended by the SA-matrix in each case. These visualizations aim to demonstrate how Spectral Attention effectively captures long-range trends by focusing on relevant frequency components.", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_26_1.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure shows the analysis of the SA-matrix for the DLinear model trained with a 720 prediction length on the Weather and ETTh1 datasets. The heatmap visualizes the learnable parameters of the SA-matrix, showing which frequencies the model attends to for each feature. The attention graphs and FFT graphs illustrate the correlation between the attention distribution and frequency components of the data, showing that the BSA effectively captures the long-range trend of the data.", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_26_2.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure shows the analysis of the SA-matrix for the DLinear model trained on a 720-step prediction task.  The heatmap visualizes the learned weights in the SA-matrix, indicating which frequencies are most attended to for each feature.  The other panels (b)-(d) show attention weights and corresponding FFT (Fast Fourier Transform) graphs for specific variables in the Weather and ETTh1 datasets, demonstrating the model's attention to low-frequency components (long-term trends).", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_27_1.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure provides a detailed analysis of the spectral attention (SA) matrix used in the DLinear model. The SA-matrix's heatmap (panel a) reveals the learned weights of various frequency components for the Weather and ETTh1 datasets. Panels (b)-(d) display these learned weights alongside corresponding attention values and the Fast Fourier Transform (FFT) graphs of the raw signals. This allows for a visual comparison of learned weights with the frequency distributions found in actual data, providing insights into how effectively SA-matrix captures long-range dependencies.", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_27_2.jpg", "caption": "Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.", "description": "This figure shows the analysis of the SA-matrix learned by the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. The heatmap of the SA-matrix is shown in panel (a). The attention and FFT graphs are shown in panels (b) through (d). These graphs help to visualize how the SA-matrix focuses on low-frequency components of the data for long-range dependency modeling. ", "section": "4.1 Real world datasets"}, {"figure_path": "dxyNVEBQMp/figures/figures_28_1.jpg", "caption": "Figure 1: (a) Training data are sampled for each time step from continuous sequences, exhibiting high temporal correlations. (b) Conventional approaches simply ignore this temporal information with a shuffled batch. (c) We address the temporal correlation between the samples for the first time, enabling the model to consider long-range dependencies that surpass the look-back window.", "description": "This figure illustrates the difference between conventional time series forecasting approaches and the proposed method. Panel (a) shows how time series data is sampled sequentially with high temporal correlation between consecutive samples. Panel (b) depicts conventional methods that ignore this temporal information by shuffling the data. Panel (c) presents the proposed method which preserves the temporal correlation to enable modeling long-range dependencies that go beyond the look-back window.", "section": "1 Introduction"}]