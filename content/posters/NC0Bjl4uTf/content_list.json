[{"type": "text", "text": "Chinese Inertial GAN for Writing Signal Generation and Recognition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Disabled people constitute a significant part of the global population, deserving   \n2 of inclusive consideration and empathetic support. However, the current human  \n3 computer interaction based on keyboards may not meet the requirements of disabled   \n4 people. The small size, ease of wearing, and low cost of inertial sensors make   \n5 inertial sensor-based writing recognition a promising human-computer interaction   \n6 option for disabled people. However, accurate recognition relies on massive inertial   \n7 signal samples, which are hard to collect for the Chinese context due to the vast   \n8 number of characters. Therefore, we design a Chinese inertial generative adversarial   \n9 network (CI-GAN) containing Chinese glyph encoding (CGE), forced optimal   \n0 transport (FOT), and semantic relevance alignment (SRA) to acquire unlimited high  \n11 quality training samples. Unlike existing vectorization focusing on the meaning of   \n12 Chinese characters, CGE represents the shape and stroke features, providing glyph   \n13 guidance for GAN to generate writing signals. FOT constrains feature consistency   \n14 between generated and real signals through the designed forced feature matching   \n5 mechanism, meanwhile addressing GANs\u2019 mode collapse and mixing issues by   \n16 introducing Wasserstein distance. SRA captures the semantic relevance between   \n17 various Chinese glyphs and injects this information into the GAN to establish   \n18 batch-level constraints and set higher standards of generated signal quality. By   \n19 utilizing the massive training samples provided by CI-GAN, the performance of   \n0 six widely used classifiers is improved from $6.7\\%$ to $98.4\\%$ , indicating that CI  \n21 GAN constructs a flexible and efficient data platform for Chinese inertial writing   \n22 recognition. Furthermore, we release the first Chinese writing recognition dataset   \n23 based on inertial sensors in GitHub. ", "page_idx": 0}, {"type": "text", "text": "24 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "25 One of the most significant obstacles for disabled individuals in their daily lives is the lack of efficient   \n26 human-computer interaction (HCI) methods [1]. Traditional keyboard-based HCI systems often fail   \n27 to meet the specific needs of disabled users, particularly those who are visually impaired or have lost   \n28 their fingers, which underscores the urgent need for developing technologies that cater to the unique   \n29 requirements of disabled individuals [2]. Providing tailored HCI solutions not only enhances their   \n30 quality of life and independence but also facilitates their integration into society, enabling greater   \n31 participation in education, employment, and social activities. Such technological advancements hold   \n32 profound significance, creating a more inclusive and equitable society.   \n33 As efficient motion-sensing components, inertial sensors can play a crucial role in recognizing writing   \n34 movements. Inertial sensors can measure the acceleration and angular velocity of moving objects,   \n35 making it possible to convert written characters into digital text [3, 4, 5, 6]. Due to their small size,   \n36 ease of integration, low power consumption, and low cost, inertial sensors are widely used in electronic   \n37 devices such as smartphones, smartwatches, and fitness bands [7, 8, 9, 10], making them particularly   \n38 suitable for disabled users. Inertial sensors can be integrated into wearable devices, providing a more   \n39 accessible and user-friendly means for disabled individuals to interact with computers and other digital   \n40 devices. By capturing the subtle movements of a user\u2019s hand or other body parts, inertial sensors can   \n41 translate these motions into written text, enabling effective communication and interaction without   \n42 the need for a traditional keyboard. In addition, unlike optical or acoustic sensors, inertial sensors are   \n43 highly resistant to external factors such as lighting conditions, physical obstructions, or environmental   \n44 noise, which showcases their unique robustness in motion capture [11, 12, 13, 14, 15]. Consequently,   \n45 inertial sensors provide a medium for Chinese character writing recognition that aligns with natural   \n46 writing habits and can be seamlessly integrated into the writing process. With the widespread adoption   \n47 of smart devices, the technology of Chinese character writing recognition based on inertial sensors   \n48 may redefine the Chinese character input in the digital age, offering disabled people a comfortable   \n49 human-computer interaction methods.   \n50 However, the major challenge in achieving accurate Chinese writing recognition using inertial sensors   \n51 is obtaining large-scale, diverse inertial writing data samples. For any recognition model aimed   \n52 at accurately analyzing the complex strokes and structures of Chinese characters, it is crucial to   \n53 train the model with extensive, diverse writing samples [16]. Considering that the collection and   \n54 processing of Chinese writing samples are laborious and require high data quality and diversity, this   \n55 task becomes exceedingly challenging and increasingly difficult as the number of characters increases.   \n56 Therefore, generating realistic Chinese writing signals based on inertial sensors has become a central   \n57 technological challenge in recognizing Chinese writing.   \n58 To acquire high-quality, diverse samples of inertial Chinese writing, we applied GAN for IMU writing   \n59 signal generation for the first time and proposed CI-GAN, which can generate unlimited inertial writing   \n60 signals for an input Chinese character, thereby providing rich training samples for Chinese writing   \n61 recognition classifiers. CI-GAN provides a more intuitive and natural human-computer interaction   \n62 method for the Chinese context and advances the application of smart devices with Chinese input.   \n63 The main contributions of this paper are summarized as follows.   \n64 \u2022 Considering traditional Chinese character embedding methods that only focus on the meaning   \n65 of characters, we propose a Chinese glyph encoding (CGE), which represents the shape   \n66 and structure of Chinese characters. CGE not only injects glyph and writing semantics into   \n67 the generation of inertial signals but also provides new tools for studying the evolution and   \n68 development of hieroglyphs.   \n69 \u2022 We propose a forced optimal transport (FOT) loss for GAN, which not only avoids mode   \n70 collapse and mode mixing during signal generation but also ensures feature consistency be  \n71 tween the generated and real signals through a designed forced feature matching mechanism,   \n72 thereby enhancing the authenticity of the generated signals.   \n73 \u2022 To inject batch-level character semantic correlations into GAN and establish macro con  \n74 straints, we propose a semantic relevance alignment (SRA), which aligns the relevance   \n75 between generated signals and corresponding Chinese glyphs, thereby ensuring that the   \n76 motion characteristics of the generated signal conform to the Chinese character structure.   \n77 \u2022 Utilizing the training samples provided by CI-GAN, we increase the Chinese writing recog  \n78 nition performance of six widely used classifiers from $6.7\\%$ to $98.4\\%$ . Furthermore, we   \n79 provide the application scenarios and strategies of 6 classifiers in writing recognition ac  \n80 cording to their performance metrics. For the sake of sharing, we release the first Chinese   \n81 writing recognition dataset based on inertial sensors in GitHub. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "82 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "83 The technology for recognizing Chinese handwriting movements has the potential to bridge the gap   \n84 between traditional writing and digital input, providing disabled individuals with a natural way of   \n85 writing and greatly enhancing their ability to participate in digital communication, education, and   \n86 employment. It also offers a new human-computer interaction avenue for normal people. Hence,   \n87 Chinese handwriting movement recognition has garnered significant attention in recent years, leading   \n88 to numerous related research achievements. Ren et al. utilized the Leap Motion device to propose   \n89 an RNN-based method for recognizing Chinese characters written in the air [17]. The Leap Motion   \n90 sensor, consisting of two infrared emitters and two cameras, can accurately capture the motion of   \n91 hands in three-dimensional (3D) space [18]. However, the Leap Motion device is sensitive to lighting   \n92 conditions, and either too strong or too weak light can interfere with the transmission and reception   \n93 of infrared rays, affecting the recognition effect [19]. Additionally, the detection space of the Leap   \n94 Motion device is an inverted quadrangular pyramid, limiting its field of view. Movements outside   \n95 this range cannot be captured. Most importantly, the Leap Motion device is expensive and requires a   \n96 connection to a computer or VR headset to function, severely limiting its application prospects [20].   \n97 As wireless networks become more prevalent, Wi-Fi signals are gradually being applied to motion   \n98 capture [21, 22]. Since Wi-Fi signals can penetrate objects and are unaffected by lighting conditions,   \n99 they have a broader application scope than optical motion capture systems [23, 24]. Guo et al. used   \n100 the channel state information (CSI), extracted from Wi-Fi signals reflected by hand movements,   \n101 to recognize 26 air-written English letters [25]. However, while Wi-Fi signals do not have visual   \n102 range limitations and can penetrate obstacles, they are easily disturbed by other signals on the same   \n103 unlicensed band, severely affecting system performance. Moreover, the sampling frequency and   \n104 resolution of Wi-Fi signals are very limited, making it difficult to capture detailed information during   \n105 the writing process and, thus, hard to recognize air-written Chinese characters accurately [26, 27].   \n106 Despite the advantages of low cost, wearability, and low power consumption offered by inertial   \n107 sensors, there is currently a lack of large-scale, high-quality public datasets, causing few studies to use   \n108 inertial sensors for 3D Chinese handwriting recognition [28, 29, 30, 31]. To collect data, Zhang et al.   \n109 employed 12 volunteers, each of whom was asked to write the assigned Chinese characters on paper   \n110 30 times [32]. The inertial measurement unit (IMU) built into smartwatches was used to collect the   \n111 motion signals of the volunteers while writing, ultimately achieving a recognition accuracy of $90.2\\%$   \n112 for 200 Chinese characters. However, this study aims to identify the signals of normal individuals   \n113 writing on paper, which is not applicable to people with disabilities. Moreover, this method can   \n114 only realize desktop-based 2D writing recognition, which reduces the comfort and flexibility of the   \n115 writing process, inherently limiting the application scenarios of Chinese handwriting recognition.   \n116 Additionally, this method cannot effectively recognize massive Chinese characters due to the physical   \n117 and mental limitations of volunteers for data collection. Considering the vast number of Chinese   \n118 characters, providing large-scale, high-quality writing signal samples for each character is nearly   \n119 impossible, which has become the most significant bottleneck limiting the development of Chinese   \n120 handwriting recognition technology based on inertial sensors. Therefore, designing a model for   \n121 generating Chinese handwriting signals provides researchers with an endless supply of signal samples   \n122 and a flexible, convenient experimental data platform, accelerating the development and testing of   \n123 new algorithms and supporting the research and application of Chinese handwriting recognition. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "124 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "125 To generate inertial writing signals for Chinese characters, we propose the Chinese inertial generative   \n126 adversarial network (CI-GAN), as shown in Fig. 1. For an input Chinese character, its one-hot   \n127 encoding is transformed into glyph encoding using our designed glyph encoding dictionary, which   \n128 stores the glyph shapes and stroke features of different Chinese characters. Thus, the obtained Chinese   \n129 glyph encoding contains rich writing features of the input character. This glyph encoding, along   \n130 with a random noise vector, is fed into a GAN, generating the synthetic IMU signal for the character,   \n131 where glyph encoding provides glyph and stroke features of the input character, while the random   \n132 noise introduces randomness to the virtual signal generation, ensuring the diversity and variability of   \n133 the generated signals. To ensure that the GAN learns the IMU signal patterns for each character, we   \n134 designed a forced optimal transport (FOT) loss, which not only mitigates the issues of mode collapse   \n135 and mode mixing typically observed in GAN frameworks but also forces the generated IMU signals   \n136 to closely resemble the actual handwriting signals in terms of semantic features, fluctuation trends,   \n137 and kinematic properties. Moreover, a semantic relevance alignment (SRA) is proposed to provide   \n138 batch-level macro constraints for GAN, thereby keeping the correlation between generated signals   \n139 consistent with the correlation between Chinese character glyphs. Equipped with CGE, FOT and   \n140 SRA, CI-GAN can provide unlimited high-quality training samples for Chinese character writing   \n141 recognition, thereby enhancing the accuracy and robustness of various classifiers. ", "page_idx": 2}, {"type": "text", "text": "142 3.1 Chinese Glyph Encoding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "143 In one-hot encoding, each Chinese character is represented by a high-dimensional sparse vector   \n144 (where only one element is 1, and all others are 0), which results in all characters being equidistant   \n145 in the vector space, thereby losing the abundant semantic information contained in the characters.   \n146 Therefore, one-hot encoding fails to inject rich information into GAN. Although there are some   \n147 commonly used Chinese character embeddings, these embeddings store meaning information of   \n148 the characters, not glyph information (i.e., shape, structure and writing strokes). For example, the   \n149 characters \u201d\u5929\u201d (sky) and \u201d\u592b\u201d (husband) are quite similar in writing motions, but their meanings   \n150 are significantly different. To this end, we propose a Chinese glyph encoding (CGE), which encodes   \n151 Chinese characters based on their glyph shapes and writing actions.   \n152 Considering that the inertial sensor signals capture the writing motion of Chinese characters, the   \n153 motion signal exactly contains glyph information, which encourages simultaneous learning signal   \n154 generation and Chinese glyph encoding under the supervision of real signals. Therefore, we create a   \n155 learnable weight matrix $W$ after the one-hot input layer to capture the glyph information. When a   \n156 Chinese character is input into CI-GAN in one-hot encoding, it first passes through this weight matrix.   \n157 Since only one element in the one-hot encoding is 1, and the rest are 0, multiplying one-hot encoding   \n158 by the weight matrix $W$ means obtaining one row of the matrix $W$ . Hence, each row of $W$ can be   \n159 seen as an encoding of a Chinese character, and this matrix can serve as a glyph encoding dictionary   \n160 of Chinese characters. However, an unguided Chinese encoding dictionary often struggles to capture   \n161 the differences in glyph shapes among different characters, assigning similar glyph encodings to   \n162 characters with distinct glyphs. To address this, we propose a glyph encoding regularization (GER),   \n163 which enhances the orthogonality of all character encoding vectors and increases their information   \n164 entropy to store as many glyph features of the characters as possible, thereby avoiding triviality like   \n165 one-hot encoding. Specifically, we use the $\\alpha$ -order R\u00e9nyi entropy to measure the information content   \n166 of the glyph encoding dictionary $W$ , calculated as follows: ", "page_idx": 2}, {"type": "image", "img_path": "NC0Bjl4uTf/tmp/fc991243f42b65df59996e259600ec9cc7071aadea1bb9ccbca78ead0f28f086.jpg", "img_caption": ["Figure 1: Flowchart of Chinese inertial generative adversarial network. The Chinese character \u201d\u6570\u201d is input into the model, and its one-hot encoding is converted into glyph encoding (green cubes), which is then input into GAN together with random noise (blue cubes of different colors). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{\\alpha}(W)=\\frac{1}{1-\\alpha}\\mathrm{log}_{2}(t r(\\tilde{G}^{\\alpha})),\\mathrm{where~}\\tilde{G}_{i j}=\\frac{1}{N}\\frac{G_{i j}}{\\sqrt{G_{i i}\\cdot G_{j j}}},G_{i j}=\\left\\langle W^{(i)},W^{(j)}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "167 where, $N$ represents the number of Chinese characters, which corresponds to the number of rows in   \n168 the weight (encoding) matrix $W$ . $G$ is the Gram matrix of $W$ , where $G_{i j}$ equal to the inner product   \n169 of the $i$ -th and $j$ -th rows of $W$ , and $\\tilde{G}$ is the trace-normalized $G$ , i.e., $t r(\\bar{\\tilde{G}})=1$ . In similar problems,   \n170 $\\alpha$ is generally set to 2 for optimal results. $S_{\\alpha}(W)$ measures the information content of the glyph   \n171 encoding matrix $W$ . A larger $S_{\\alpha}(W)$ indicates more information encoded in $W$ , meaning the glyph   \n172 encodings are more informative. Meanwhile, as $S_{\\alpha}(W)$ increases, all elements in the Gram matrix   \n173 $G$ are forced to decrease, indicating that different encoding vectors have stronger orthogonality. It   \n174 is evident that the improvement of $S_{\\alpha}(W)$ simultaneously enhances the information content and   \n175 the orthogonality among the encodings. In light of this, the glyph encoding regularization $R_{\\mathrm{encode}}$ is   \n176 constructed as $\\begin{array}{r}{\\dot{R}_{\\mathrm{encode}}\\stackrel{}{=}\\frac{1}{S_{\\alpha}(W)}}\\end{array}$ . As $R_{\\mathrm{encode}}$ decreases during training, $S_{\\alpha}(W)$ gradually increases,   \n177 meaning the glyph encoding dictionary stores more information while enhancing the orthogonality   \n178 among all Chinese glyph encodings, effectively representing the differences in glyph shapes among   \n179 all characters. Thus, this glyph encoding can inject sufficient glyph information into GAN, ensuring   \n180 that the generated signals maintain consistency with the target character\u2019s glyph. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "181 3.2 Forced Optimal Transport ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 Ensuring the authenticity of virtual signals poses the greatest challenge when generating diverse   \n183 signals, especially in following physical laws and simulating the potential dynamical characteristics   \n184 of actual motions. To this end, we propose the forced feature matching (FFM), which ensures that the   \n185 generated signal feature closely matches the real signal feature and the corresponding glyph encoding.   \n186 Specifically, we use a pre-trained variational autoencoder to extract the real signal feature $h_{T}$ and   \n187 generated signal feature $h_{G}$ . Then, the consistency of $h_{T},h_{G}$ , and the corresponding glyph encoding   \n188 $e$ is constrained by ${\\mathcal{L}}_{F F M}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{F F M}=1-\\frac{\\left<h_{G},h_{T}\\right>+\\left<h_{G},e\\right>+\\left<e,h_{T}\\right>}{\\left|\\left|h_{G}\\right|\\right|\\left|\\left|h_{T}\\right|\\right|+\\left|\\left|h_{G}\\right|\\right|\\left|e\\right|+\\left|\\left|e\\right|\\left|\\left|h_{T}\\right|\\right|}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "189 Another critical challenge lies in the mode collapse and mode mixing issue inherent to GAN archi  \n190 tectures. Mode collapse limits the diversity of generated signal samples, causing GAN to generate   \n191 signals only for a few Chinese characters, regardless of the diversity of input. On the other hand, mode   \n192 mixing problems cause the generated signal to contain blend characteristics of multiple modes, which   \n193 is unrealistic and unrecognizable. To address these issues, we introduce the optimal transport to GAN,   \n194 which utilizes the Wasserstein distance as a loss function. Traditional GANs use the Jensen-Shannon   \n195 divergence as the loss metric, which becomes ineffective when the distributions of real and generated   \n196 data have little overlap, leading to mode collapse. The Wasserstein distance provides a more effective   \n197 gradient even when the distributions are disjoint or significantly different, thereby preventing mode   \n198 collapse. Furthermore, unlike the Jensen-Shannon divergence, the Wasserstein distance exhibits   \n199 insensitivity to the balance between the training of the generator and discriminator, thereby alleviating   \n200 mode mixing (We provide a rigorous mathematical proof in Appendix C). Combing OT and FFM   \n201 constraints, we can obtain the forced optimal transport loss $\\mathcal{L}_{F O T}\\,=\\,W(\\mathbb{P}_{T},\\mathbb{P}_{G})\\,+\\,\\lambda\\,\\cdot\\,\\mathcal{L}_{F F M}$ ,   \n202 where $W(\\mathbb{P}_{T},\\mathbb{P}_{G})$ is the optimal transport loss, representing the Wasserstein distance between the   \n203 distributions of real and generated signals, enhancing the stability and diversity of the samples. $\\lambda$   \n204 is a weighting coefficient for the forced feature matching loss ${\\mathcal{L}}_{F F M}$ . As ${\\mathcal{L}}_{F F M}$ decreases during   \n205 training, the generated signals increasingly approximate the characteristics of real signals. ", "page_idx": 4}, {"type": "text", "text": "206 3.3 Semantic Relevance Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "207 As motion records of Chinese writing, the se  \n208 mantic relationships between generated signals   \n209 should align with the relationships between Chi  \n210 nese character glyphs. To ensure the gener  \n211 ated inertial signals accurately reflect the char  \n212 acter relationships between Chinese character   \n213 glyphs, we propose semantic relevance align  \n214 ment (SRA), which ensures consistency between   \n215 the glyph encoding relationships and the signal   \n216 feature relationships, thereby providing batch  \n217 level macro guidance for GANs and enhancing   \n218 the quality of the generated signals. For each   \n219 batch of input Chinese characters, we compute   \n220 the pairwise cosine similarities of their Chinese   \n221 glyph encodings to form an encoding similarity   \n222 matrix $M_{e}$ .  Simultaneously, the pairwise cosine   \n223 similarities of generated signal features (extracted by the pre-trained VAE) are computed to form a   \n224 feature similarity matrix $M_{h}$ . Then, the loss of semantic relevance alignment $\\mathcal{L}_{S R A}\\bar{=}\\,\\|M_{h}-M_{e}\\|_{2}^{2}$   \n225 is established to minimize the difference between the two matrices, thereby ensuring that the semantic   \n226 relationships in the input character glyphs are accurately contained in the generated signals. ", "page_idx": 4}, {"type": "image", "img_path": "NC0Bjl4uTf/tmp/c28de88ee04d9f6e5c8faa77633476013daed2292f761bb1bef1d7915d5056a0.jpg", "img_caption": ["Figure 2: Diagram of semantic relevance alignment. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "227 4 Experiments and Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228 4.1 Data Collection and Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "229 We invited nine volunteers, each using their   \n230 smartphone\u2019s built-in inertial sensors to record   \n231 handwriting movements. The nine smartphones   \n232 and their corresponding sensor models are listed   \n233 in Table 1. Each volunteer held their phone ac  \n234 cording to their personal habit and wrote 500   \n235 Chinese characters in the air (sourced from the   \n236 \u201dCommonly Used Chinese Characters List\u201d pub  \n237 lished by the National Language Working Com  \n238 mittee and the Ministry of Education), writing   \n239 each character only once. In total, we obtained   \n240 4500 samples of Chinese handwriting signals.   \n241 We randomly selected 1500 samples from three ", "page_idx": 5}, {"type": "text", "text": "Table 1: The built-in IMU specifications of some smartphones. Note that since the IMUs in some types of iPhones are customized by the manufacturer, the model and price are not disclosed. ", "page_idx": 5}, {"type": "table", "img_path": "NC0Bjl4uTf/tmp/f4a69fe95c4fe2efa3d34b2a336dff8f3044dd9d98b174ba13a6d1047c0f91cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "242 volunteers as the training set, while the remaining 3000 samples from six volunteers were used as   \n243 the test set without participating in any training. All experiments are implemented by Pytorch 1.12.1   \n244 with an Nvidia RTX 2080TI GPU and Intel(R) Xeon(R) W-2133 CPU. ", "page_idx": 5}, {"type": "text", "text": "245 4.2 Signal Generation Visualization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "246 To visually demonstrate the signal generation effect of CI-GAN, we visualized the real and generated   \n247 inertial sensor signals of the handwriting movements for the Chinese characters \u201d\u79d1\u201d and \u201d\u5b66\u201d,   \n248 respectively. In these figures, the blue curves represent the three-axis acceleration signals, and the   \n249 yellow curves represent the three-axis gyroscope signals. It can be observed that the generated signals   \n250 closely follow the overall fluctuation trends of the real signals, indicating that CI-GAN effectively   \npreserves the handwriting movement information of the real signals. To further verify the consistency ", "page_idx": 5}, {"type": "image", "img_path": "NC0Bjl4uTf/tmp/bb3726a5fa9c62077b6b3c7aa97b6810eed4a816074422fb8e4a4310c6391f4c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: The visualization results of the 6-axis signals recorded by the inertial sensor for different Chinese character writing movements and the corresponding generated signals. The left side is the original inertial sensor signal, the middle is the corresponding generated signal, and the right side is the reconstructed writing trajectory. ", "page_idx": 5}, {"type": "text", "text": "252 of the movement characteristics between the generated and real signals, we employed a classical   \n253 inertial navigation method [33] to convert both the real and generated signals into corresponding   \n254 motion trajectories, as shown in the third column of Fig. 3. It is important to note that the purpose   \n255 of reconstructing the motion trajectories is not to precisely reproduce every detail of the writing   \n256 process but to compare the overall shape similarity between the trajectories derived from real and   \n257 generated signals. The highly similar shapes between the trajectories indicate that the generated   \n258 signals accurately capture the structural information of different Chinese characters and can effectively   \n259 simulate the key movement features of the handwriting process, including stroke order, movement   \n260 direction changes, and velocity variations. Additionally, the obvious differences in details between   \n261 the real and generated signals demonstrate CI-GAN\u2019s capability to generate diverse signals. Since the   \n262 generated signals maintain the core movement and semantic features of the handwriting process, these   \n263 differences do not impair the overall recognition of the characters but rather enhance the diversity of   \n264 the training data.   \n265 To demonstrate CI-GAN\u2019s ability to generate unlimited high-quality signals, we generated five IMU   \n266 handwriting signals for the same character \u201d\u738b\u201d and compared them with a real handwriting signal,   \n267 as shown in Fig. 4. We chose this character because its strokes are distinctly separated, making it   \n268 easier to compare the consistency of stroke features between the generated and real signals. It can   \n269 be observed that the generated signals exhibit similar fluctuation patterns to the real signal in all   \n270 three axes of acceleration and gyroscope measurements, verifying CI-GAN\u2019s precision in capturing   \n271 dynamic handwriting characteristics. Although the overall trends of the generated signals align with   \n272 the real signal, the individual features show variations, demonstrating CI-GAN\u2019s potential to produce   \n273 large-scale, high-quality, and diverse IMU handwriting signal samples. ", "page_idx": 5}, {"type": "image", "img_path": "NC0Bjl4uTf/tmp/c9da3f7f2688d5036358b0078f0ef12ed05887faf35ddfe00bbb0f9b1cd7474b.jpg", "img_caption": ["Figure 4: Visualization of the real IMU signal for writing \u201d\u738b\u201d and the virtual signals generated by CI-GAN. The upper left corner is the real signal, and the remaining signals are virtual signals. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "274 4.3 Comparative Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "275 Using the trained CI-GAN, we generated 30 vir  \n276 tual IMU handwriting signals for each character,   \n277 resulting in a total of 16500 training samples.   \n278 To evaluate the impact of the generated signals   \n279 on handwriting recognition tasks, we trained six   \n280 representative time-series classification models   \n281 with these training samples: 1DCNN, LSTM,   \n282 Transformer, SVM, XGBoost, and Random For  \n283 est (RF). We then tested the performance of these   \n284 classifiers on the test set, as shown in Fig. 5.   \n285 When the number of training samples is small (1500 real samples), the recognition accuracy of all   \n286 classifiers is poor, with the highest accuracy being only $6.7\\%$ . As the generated training samples are   \n287 introduced, all classifiers\u2019 recognition accuracy improves significantly, whereas deep learning ones   \n288 such as 1DCNN, LSTM, and Transformer show the most notable improvement. When the number of   \n289 training samples reaches 15000, the recognition accuracy of 1DCNN can reach $95.7\\%$ , improving from   \n290 $0.87\\%$ (without data augmentation). The Transformer captures long-range dependencies in time-series   \n291 data through its self-attention mechanism, enabling it to understand complex movement patterns.   \n292 However, its excellent recognition ability relies on large amounts of data, making its performance   \n293 improvement the most significant as CI-GAN continuously generates training data, improving from   \n294 $1.7\\%$ to $98.4\\%$ . Compared to deep learning models, machine learning models also exhibit significant   \n295 dependence on the amount of training data, highlighting the critical role of sufficient generated signals   \n296 in handwriting recognition tasks. With the abundant training samples generated by CI-GAN, six   \n297 classifiers achieve accurate recognition even for similar characters as shown in Appendix A.1.   \n302 ples for training and improving their   \n303 recognition accuracy. To help researchers select suitable classifiers for different application scenarios,   \n304 we further tested the recognition speed and memory usage of different classifiers for a single input   \n305 sample and summarized their recognition accuracy in Table 2. Among the three deep learning models,   \n306 1DCNN has the fastest runtime and the smallest memory usage, with a recognition accuracy of $95.7\\%$ ,   \n307 slightly lower than the Transformer but sufficient for most practical applications. It is more suitable   \n308 for integration into memory and computation resource-limited smart wearable devices such as phones,   \n309 watches, and wristbands. In contrast, Transformer has the highest accuracy among the six classifiers   \n310 and the highest memory usage, making it more suitable for PC-based applications. Compared to deep   \n311 learning classifiers, traditional machine learning classifiers generally have lower accuracy, but with   \n312 the support of abundant training samples generated by CI-GAN, the XGBoost model still achieves a   \n313 recognition accuracy of $93.1\\%$ , very close to deep learning classifiers. More importantly, XGBoost,   \n314 as a tree model, has strong interpretability, allowing users to intuitively observe which features signifi  \n315 cantly impact the model\u2019s decision-making process, which is a strength that deep learning models lack.   \n316 Additionally, XGBoost\u2019s runtime and memory usage are better than the three deep learning classifiers,   \n317 making it outstanding in scenarios requiring a balance between model performance, interpretability,   \n318 and resource efficiency. For example, XGBoost can be integrated into stationery and educational tools   \n319 to analyze students\u2019 handwriting habits and provide personalized feedback suggestions. Similarly,   \n320 in the healthcare field, XGBoost can be used to analyze patients\u2019 writing characteristics, assisting   \n321 doctors in evaluating treatment effects or predicting disease risks. Its high interpretability can provide   \n322 an auxiliary reference for medical decisions and treatment plans, increasing patients\u2019 trust in the   \n323 treatment. ", "page_idx": 6}, {"type": "image", "img_path": "NC0Bjl4uTf/tmp/b95bd89a2c6d9c15b8679c90d7aeb384fe3ee9869a1a04de41ef123ef8db2626.jpg", "img_caption": ["Figure 5: The recognition accuracy of 6 classifiers with varied training samples provided by CI-GAN. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "NC0Bjl4uTf/tmp/22a591ad6ef1f12c60d4a26b818e12d56c7b3eead076590efdf0ec3bb38d2c47.jpg", "table_caption": ["Table 2: Performance comparison of 6 classfiers. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "324 4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "325 Systematic ablation experiments are   \n326 conducted to evaluate the contribu  \n327 tions of the CGE, FOT, and SRA mod  \n328 ules in CI-GAN. We generated writing   \n329 samples using the ablated models and   \n330 trained the six classifiers on these sam  \n331 ples. The results are summarized in   \n332 Table 3. When no generated data is   \n333 used (No augmentation), the recogni  \n334 tion accuracy of all classifiers is very   \n335 poor. Employing the Base GAN to   \n336 generate training samples brings slight improvement but still underperforms, underscoring the critical   \n337 importance and necessity of data augmentation for accurate recognition. This also indicates that   \n338 utilizing GAN to improve classifier performance is a challenging task. Introducing CGE, FOT, and   \n339 SRA individually into the GAN significantly improves its performance, with the introduction of   \n340 CGE bringing the most noticeable improvement. This demonstrates that incorporating Chinese glyph   \n341 encoding into the generative model is crucial for accurately generating writing signals. When CGE,   \n342 FOT, and SRA are simultaneously integrated into the GAN (i.e., CI-GAN), the performance of all six   \n343 classifiers is improved to above $70\\%$ , with four classifiers achieving recognition accuracies exceeding   \n344 $90\\%$ . Notably, the Transformer classifier achieves an impressive accuracy of $98.4\\%$ . Furthermore,   \n345 statistical significance analysis is performed to validate the reliability of these results, as shown in   \n346 Appendix A.2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "NC0Bjl4uTf/tmp/84fcf01fac39808a592dcf9c096fa4bf5003e6f8076d58f4bfbd09e2d762935c.jpg", "table_caption": ["Table 3: Performance comparison of six classifiers trained on samples generated by different ablation models. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "347 4.5 Visualization Analysis of Chinese Glyph Encoding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "348 To demonstrate the effectiveness   \n349 of the Chinese glyph encoding in   \n350 capturing the glyph features of   \n351 Chinese characters, we conducted   \n352 a visualization analysis using t  \n353 SNE, which reduced the dimen  \n354 sionality of the glyph encodings of   \n355 500 Chinese characters and visu  \n356 alized the results in a 2D space,   \n357 as shown in Fig. 6, where each   \n358 point represents a Chinese charac  \n359 ter. For the convenience of obser  \n360 vation, we selected 6 local visual  \n361 ization regions from left to right   \n362 and zoomed in on them at the bot  \n363 tom. It can be observed that charac  \n364 ters with similar strokes and struc  \n365 ture (e.g., \u201d\u529e-\u4e3a\u201d, \u201d\u76ee-\u4e14\u201d, \u201d\u4eba-   \n366 \u5165-\u516b\u201d) are close to each other. Ad  \n367 ditionally, the figure shows several   \n368 clusters where characters within   \n369 the same cluster share similar radi  \n370 cals, structures, or strokes, indicat  \n371 ing that CGE effectively captures   \n372 the similarities and differences in   \n373 the glyph features of Chinese char  \n374 acters. By incorporating CGE into   \n375 the generative model, CI-GAN can produce writing signals that accurately reflect the structure and   \n376 stroke features of Chinese characters, ensuring the generated signals closely align with real writing   \n377 movements. This encoding is not only crucial for guiding GANs in generating writing signals but also   \n378 potentially provides new tools and perspectives for studying the evolution of Chinese hieroglyphs. ", "page_idx": 8}, {"type": "image", "img_path": "NC0Bjl4uTf/tmp/f3cf8cf3f24b8c4f58a489ae584bfd565995223de04b1fa8c8c89e954e68dc97.jpg", "img_caption": ["Figure 6: The t-SNE visualization of Chinese glyph encodings. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "379 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "380 This paper introduces GAN to generate inertial sensor signals and proposes CI-GAN for Chinese   \n381 writing data augmentation, which consists of CGE, FOT, and SRA. The CGE module constructs   \n382 an encoding of the stroke and structure for Chinese characters, providing glyph information for   \n383 GAN to generate writing signals. FOT overcomes the mode collapse and mode mixing problems   \n384 of traditional GANs and ensures the authenticity of the generated samples through a forced feature   \n385 matching mechanism. The SRA module aligns the semantic relationships between the generated   \n386 signals and the corresponding Chinese characters, thereby imposing a batch-level constraint on   \n387 GAN. Utilizing the large-scale, high-quality synthetic IMU writing signals provided by CI-GAN, the   \n388 recognition accuracy of six widely used classifiers for Chinese writing recognition was improved   \n389 from $6.7\\%$ to $98.4\\%$ , which demonstrates that CI-GAN has the potential to become a flexible and   \n390 efficient data generation platform in the field of Chinese writing recognition. This research provides   \n391 a novel human-computer interaction, especially for disabled people. Its limitations and impact are   \n392 discussed in Appendix B.1 and B.2. In the future, we plan to extend CI-GAN to generate signals from   \n393 other modalities of sensors, constructing a multimodal human-computer interaction system tailored   \n394 for disabled individuals, which can adapt to the diverse needs of users with different disabilities.   \n395 Through continuous collaboration with healthcare professionals and the disabled community, we will   \n396 refine and optimize these multimodal systems to ensure they deliver the highest functionality and   \n397 user satisfaction. Ultimately, this research aims to foster a society where digital accessibility is a   \n398 fundamental right, ensuring that all individuals, regardless of physical abilities, can engage fully and   \n399 independently with the digital world. ", "page_idx": 8}, {"type": "text", "text": "400 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "401 [1] Jayraj V Vaghasiya, Carmen C Mayorga-Martinez, Jan Vysko\u010dil, and Martin Pumera. Black   \n402 phosphorous-based human-machine communication interface. Nature communications, 14(1):2,   \n403 2023.   \n404 [2] Xumeng Wang, Xinhu Zheng, Wei Chen, and Fei-Yue Wang. Visual human\u2013computer interac  \n405 tions for intelligent vehicles and intelligent transportation systems: The state of the art and future   \n406 directions. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 51(1):253\u2013265,   \n407 2020.   \n408 [3] Swapnil Sayan Saha, Sandeep Singh Sandha, Luis Antonio Garcia, and Mani Srivastava. Tinyo  \n409 dom: Hardware-aware efficient neural inertial navigation. Proceedings of the ACM on Interac  \n410 tive, Mobile, Wearable and Ubiquitous Technologies, 6(2):1\u201332, 2022.   \n411 [4] Mahdi Abolfazli Esfahani, Han Wang, Keyu Wu, and Shenghai Yuan. Aboldeepio: A novel   \n412 deep inertial odometry network for autonomous vehicles. IEEE Transactions on Intelligent   \n413 Transportation Systems, 21(5):1941\u20131950, 2019.   \n414 [5] Xin Zhang, Bo He, Guangliang Li, Xiaokai Mu, Ying Zhou, and Tanji Mang. Navnet: Auv   \n415 navigation through deep sequential learning. IEEE Access, 8:59845\u201359861, 2020.   \n416 [6] Wenxin Liu, David Caruso, Eddy Ilg, Jing Dong, Anastasios I Mourikis, Kostas Daniilidis, Vijay   \n417 Kumar, and Jakob Engel. Tlio: Tight learned inertial odometry. IEEE Robotics and Automation   \n418 Letters, 5(4):5653\u20135660, 2020.   \n419 [7] Daniel Weber, Clemens G\u00fchmann, and Thomas Seel. Riann\u2014a robust neural network outper  \n420 forms attitude estimation filters. Ai, 2(3):444\u2013463, 2021.   \n421 [8] Boris Gromov, Gabriele Abbate, Luca M. Gambardella, and Alessandro Giusti. Proximity   \n422 human-robot interaction using pointing gestures and a wrist-mounted imu. In 2019 International   \n423 Conference on Robotics and Automation (ICRA), pages 8084\u20138091, 2019. doi: 10.1109/ICRA.   \n424 2019.8794399.   \n425 [9] Peng Li, Wen-An Zhang, Yuqiang Jin, Zihan Hu, and Linqing Wang. Attitude estimation   \n426 using iterative indirect kalman with neural network for inertial sensors. IEEE Transactions on   \n427 Instrumentation and Measurement, 2023.   \n428 [10] Sachini Herath, Hang Yan, and Yasutaka Furukawa. Ronin: Robust neural inertial navigation in   \n429 the wild: Benchmark, evaluations, & new methods. In 2020 IEEE International Conference on   \n430 Robotics and Automation (ICRA), pages 3146\u20133152. IEEE, 2020.   \n431 [11] Shiqiang Liu, Junchang Zhang, Yuzhong Zhang, and Rong Zhu. A wearable motion capture   \n432 device able to detect dynamic motion of human limbs. Nature communications, 11(1):5615,   \n433 2020.   \n434 [12] You Li, Ruizhi Chen, Xiaoji Niu, Yuan Zhuang, Zhouzheng Gao, Xin Hu, and Naser El-Sheimy.   \n435 Inertial sensing meets machine learning: Opportunity or challenge? IEEE Transactions on   \n436 Intelligent Transportation Systems, 23(8):9995\u201310011, 2022. doi: 10.1109/TITS.2021.3097385.   \n437 [13] Derek K Shaeffer. Mems inertial sensors: A tutorial overview. IEEE Communications Magazine,   \n438 51(4):100\u2013109, 2013.   \n439 [14] Changhao Chen, Xiaoxuan Lu, Andrew Markham, and Niki Trigoni. Ionet: Learning to cure   \n440 the curse of drift in inertial odometry. In Proceedings of the AAAI Conference on Artificial   \n441 Intelligence, volume 32, 2018.   \n442 [15] Martin Brossard,Axel Barrau, and Silv\u00e8re Bonnabel. Ai-imu dead-reckoning. IEEE Transactions   \n443 on Intelligent Vehicles, 5(4):585\u2013595, 2020.   \n444 [16] Yifeng Wang and Yi Zhao. Handwriting recognition under natural writing habits based on a   \n445 low-cost inertial sensor. IEEE Sensors Journal, 24(1):995\u20131005, 2024. doi: 10.1109/JSEN.   \n446 2023.3331011.   \n447 [17] Haiqing Ren, Weiqiang Wang, and Chenglin Liu. Recognizing online handwritten chinese   \n448 characters using rnns with new computing architectures. Pattern Recognition, 93:179\u2013192,   \n449 2019.   \n450 [18] Elyoenai Guerra-Segura, Aysse Ortega-P\u00e9rez, and Carlos M Travieso. In-air signature verifica  \n451 tion system using leap motion. Expert Systems with Applications, 165:113797, 2021.   \n452 [19] Irene Cortes-Perez, Noelia Zagalaz-Anula, Desiree Montoro-Cardenas, Rafael Lomas-Vega,   \n453 Esteban Obrero-Gaitan, and Mar\u00eda Catalina Osuna-P\u00e9rez. Leap motion controller video game  \n454 based therapy for upper extremity motor recovery in patients with central nervous system   \n455 diseases. a systematic review with meta-analysis. Sensors, 21(6):2065, 2021.   \n456 [20] Salih Ertug Ovur, Hang Su, Wen Qi, Elena De Momi, and Giancarlo Ferrigno. Novel adaptive   \n457 sensor fusion methodology for hand pose estimation with multileap motion. IEEE Transactions   \n458 on Instrumentation and Measurement, 70:1\u20138, 2021.   \n459 [21] Ning Xiao, Panlong Yang, Yubo Yan, Hao Zhou, Xiang-Yang Li, and Haohua Du. Motion- $\\mathrm{fi^{+}+}$ :   \n460 Recognizing and counting repetitive motions with wireless backscattering. IEEE Transactions   \n461 on Mobile Computing, 20(5):1862\u20131876, 2021. doi: 10.1109/TMC.2020.2971996.   \n462 [22] Xuanzhi Wang, Kai Niu, Jie Xiong, Bochong Qian, Zhiyun Yao, Tairong Lou, and Daqing   \n463 Zhang. Placement matters: Understanding the effects of device placement for wifi sensing.   \n464 Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(1):   \n465 1\u201325, 2022.   \n466 [23] Ruiyang Gao, Wenwei Li, Jinyi Liu, Shuyu Dai, Mi Zhang, Leye Wang, and Daqing Zhang.   \n467 Wicgesture: Meta-motion based continuous gesture recognition with wi-fi. IEEE Internet of   \n468 Things Journal, 2023.   \n469 [24] Sai Deepika Regani, Beibei Wang, and K. J. Ray Liu. Wifi-based device-free gesture recognition   \n470 through-the-wall. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech   \n471 and Signal Processing (ICASSP), pages 8017\u20138021, 2021. doi: 10.1109/ICASSP39728.2021.   \n472 9414894.   \n473 [25] Zhengxin Guo, Fu Xiao, Biyun Sheng, Huan Fei, and Shui Yu. Wireader: Adaptive air hand  \n474 writing recognition based on commercial wifi signal. IEEE Internet of Things Journal, 7(10):   \n475 10483\u201310494, 2020.   \n476 [26] Ruiyang Gao, Wenwei Li, Yaxiong Xie, Enze Yi, Leye Wang, Dan Wu, and Daqing Zhang.   \n477 Towards robust gesture recognition by characterizing the sensing quality of wifi signals. Pro  \n478 ceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(1):1\u201326,   \n479 2022.   \n480 [27] Yu Gu, Jinhai Zhan, Yusheng Ji, Jie Li, Fuji Ren, and Shangbing Gao. Mosense: An rf-based   \n481 motion detection system via off-the-shelf wifi devices. IEEE Internet of Things Journal, 4(6):   \n482 2326\u20132341, 2017.   \n483 [28] Luis Montesinos, Rossana Castaldo, and Leandro Pecchia. Wearable inertial sensors for fall   \n484 risk assessment and prediction in older adults: A systematic review and meta-analysis. IEEE   \n485 transactions on neural systems and rehabilitation engineering, 26(3):573\u2013582, 2018.   \n486 [29] Changhao Chen, Peijun Zhao, Chris Xiaoxuan Lu, Wei Wang, Andrew Markham, and Niki   \n487 Trigoni. Deep-learning-based pedestrian inertial navigation: Methods, data set, and on-device   \n488 inference. IEEE Internet of Things Journal, 7(5):4431\u20134441, 2020.   \n489 [30] Swapnil Sayan Saha, Yayun Du, Sandeep Singh Sandha, Luis Antonio Garcia, Moham  \n490 mad Khalid Jawed, and Mani Srivastava. Inertial navigation on extremely resource-constrained   \n491 platforms: Methods, opportunities and challenges. In 2023 IEEE/ION Position, Location and   \n492 Navigation Symposium (PLANS), pages 708\u2013723. IEEE, 2023.   \n493 [31] Mahdi Abolfazli Esfahani, Han Wang, Keyu Wu, and Shenghai Yuan. Orinet: Robust 3-d   \n494 orientation estimation with a single particular imu. IEEE Robotics and Automation Letters, 5   \n495 (2):399\u2013406, 2019.   \n496 [32] Jian Zhang, Hongliang Bi, Yanjiao Chen, Qian Zhang, Zhaoyuan Fu, Yunzhe Li, and Zeyu Li.   \n497 Smartso: Chinese character and stroke order recognition with smartwatch. IEEE Transactions   \n498 on Mobile Computing, 20(7):2490\u20132504, 2020.   \n499 [33] Mohinder S Grewal, Lawrence R Weill, and Angus P Andrews. Global positioning systems,   \n500 inertial navigation, and integration. John Wiley & Sons, 2007. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "501 Appendix / Supplemental Material ", "page_idx": 12}, {"type": "text", "text": "502 A Additional Experimental Results ", "page_idx": 12}, {"type": "text", "text": "503 A.1 Performance of Classifiers on Similar Characters ", "page_idx": 12}, {"type": "image", "img_path": "NC0Bjl4uTf/tmp/6b09d25c07c6c562c5d0dc5f1561b96f71434a2b7c4e8eea2ffbc70c16ace9f3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 7: Confusion matrices of different classifiers for recognition results of Chinese characters with similar glyphs. ", "page_idx": 12}, {"type": "text", "text": "504 With the abundant training samples generated by CI-GAN, the handwriting recognition performance   \n505 of all six classifiers significantly improved. To further verify the recognition performance of different   \n506 classifiers on characters with similar strokes and glyphs, we selected four groups of characters with sim  \n507 ilar handwriting movements from the test set (\u201d\u516b\u4eba\u5165\u5927\u5929\u592a\u201d, \u201d\u529e\u4e3a\u65b9\u529b\u4e07\u5386\u201d, \u201d\u8fc7\u8fbe\u8fd9\u8fb9\u8fd1\u8fd8\u201d,   \n508 and \u201d\u8ba4\u8bae\u8ba1\u8bb8\u8bdd\u8bc6\u201d) and presented the recognition results of the six classifiers in confusion matrices,   \n509 as shown in Fig. 7. It can be observed that the values on the diagonal of all confusion matrices are   \n510 significantly higher than the non-diagonal values, indicating high recognition accuracy for these   \n511 similar handwriting characters with the help of samples generated by CI-GAN. However, some   \n512 characters are still misrecognized. For instance, the characters \u201d\u516b\u201d, \u201d\u4eba\u201d, and \u201d\u5165\u201d have extremely   \n513 similar structures and writing movements, posing challenges even when massive training samples are   \n514 provided. Moreover, continuous and non-standard writing can also cause recognition obstacles. For   \n515 instance, although the characters \u201d\u8fc7\u201d and \u201d\u8fbe\u201d have different strokes in static form, they are very   \n516 similar in dynamic handwriting. Despite these challenges, the synthetic IMU handwriting samples   \n517 generated by CI-GAN significantly enhance the classifiers\u2019ability to recognize characters with similar   \n518 glyph structures and handwriting movements, highlighting the value and significance of the proposed   \n519 CI-GAN method. By providing diverse and high-quality training samples, CI-GAN improves hand  \n520 writing recognition classifiers\u2019 performance and generalization ability, making it a valuable tool for   \n521 advancing Chinese handwriting recognition technology. ", "page_idx": 13}, {"type": "text", "text": "522 A.2 Statistical Significance Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "523 The CI-GAN model demonstrates significant performance improvements across multiple classifiers,   \n524 as shown in Table 4. The Transformer classifier, for instance, achieves a mean accuracy of $98.4\\%$ ,   \n525 compared to $15.7\\%$ with the traditional GAN and $1.7\\%$ without data augmentation. This highlights   \n526 CI-GAN\u2019s ability to generate realistic and diverse training samples that enhance handwriting recogni  \n527 tion. Moreover, CI-GAN consistently improves accuracy and stability for all classifiers tested. The   \n528 1DCNN\u2019s accuracy increases to $95.7\\%$ from $18.5\\%$ with the traditional GAN and $0.87\\%$ without   \n529 augmentation. Similarly, other models, including LSTM, RandomForest, XGBoost, and SVM, show   \n530 substantial gains, underscoring CI-GAN\u2019s effectiveness across diverse machine-learning contexts.   \n531 In addition, the narrow $95\\%$ confidence intervals, such as [ $98.2822\\%$ , $98.5178\\%]$ for the Trans  \n532 former, validate the statistical significance and reliability of these results. This confirms CI-GAN\u2019s   \n533 potential to consistently enhance classifier performance. In conclusion, CI-GAN represents a major   \n534 advancement in Chinese handwriting recognition by generating high-quality, diverse inertial signals.   \n535 This significantly boosts the accuracy and reliability of various classifiers, demonstrating CI-GAN\u2019s   \ntransformative potential in the field. ", "page_idx": 13}, {"type": "table", "img_path": "NC0Bjl4uTf/tmp/32f67e1bc03a9860843c911c4f7aef5b5e4d3e5c65da973e31d1e202b5180270.jpg", "table_caption": ["Table 4: Performance of different classifiers with CI-GAN generated data "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "537 B Discussion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "538 B.1 Societal Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "539 CI-GAN model significantly improves the accuracy of Chinese writing recognition and offers an   \n540 alternative means of human-computer interaction that can overcome the limitations of traditional   \n541 keyboard-based methods, which are often inaccessible to those who are blind or lose their fingers. By   \n542 providing a more accessible and user-friendly way to interact with digital devices, inertial sensors   \n543 can facilitate effective communication, enhance the participation of disabled people in education and   \n544 employment, and promote greater independence. Moreover, by addressing the unique needs of this   \n545 population, such technological advancements reflect a commitment to inclusivity and social justice,   \n546 ensuring that everyone, regardless of their physical abilities, has the opportunity to fully participate   \n547 in and contribute to society.   \n548 Furthermore, by releasing the world\u2019s first Chinese handwriting recognition dataset based on inertial   \n549 sensors, this research provides valuable data resources for both academia and industry, facilitating   \n550 further studies and advancements. Additionally, the technology offers an intuitive and efficient   \n551 learning tool for Chinese language learners, aiding in preserving and disseminating Chinese cultural   \n552 heritage and strengthening the global influence of Chinese characters. In summary, the CI-GAN   \n553 technology achieves not only significant breakthroughs in algorithmic research but also demonstrates   \n554 extensive practical potential and substantial societal value, thereby being adopted by educational   \n555 aid device manufacturers. This study provides a solid foundation for future academic research,   \n556 technological development, and industrial applications, driving technological progress and societal   \n557 development. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "558 B.2 Limitation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "559 While the CI-GAN model demonstrates significant advancements in Chinese handwriting generation   \n560 and recognition, some practical limitations could impact its performance in real-world applications.   \n561 For instance, non-standard or cursive handwriting may pose challenges for accurate signal generation   \n562 and recognition. Additionally, environmental factors such as external movements or vibrations when   \n563 using handheld devices could affect the inertial sensor data quality, leading to variations in recognition   \n564 accuracy. Future work could focus on developing more robust algorithms that account for these real  \n565 world variations and improving the model\u2019s adaptability to diverse handwriting styles and conditions.   \n566 These enhancements would ensure that the CI-GAN technology remains effective across a broader   \n567 range of practical scenarios. ", "page_idx": 14}, {"type": "text", "text": "568 C Theory Assumption and Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "569 To generate large-scale and high-quality handwriting signals, we introduce optimal transport theory   \n570 into the generative adversarial network to alleviate mode collapse and mixing issues. We provide   \n571 a detailed explanation and present a rigorous mathematical proof to show the advantages of this   \n572 operation.   \n573 In traditional conditional GANs, the generator $G$ and the discriminator $D$ are trained by minimizing   \n574 the loss function $\\mathcal{L}_{t r a d i t i o n}$ : ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t r a d i t i o n}=\\operatorname*{min}_{G}\\operatorname*{max}_{D}\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{data}}}[\\log D(\\mathbf{x})]+\\mathbb{E}_{\\mathbf{z}\\sim p_{\\mathrm{z}}}[\\log(1-D(G(\\mathbf{z})))],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "575 where $p_{\\mathrm{data}}$ is the real data distribution, and $p_{\\mathbf{z}}$ is the distribution of the generator\u2019s input noise. This   \n576 loss function essentially minimizes the Jensen-Shannon Divergence (JSD) between the real data   \n577 distribution $p_{\\mathrm{data}}$ and the generated data distribution $p_{g}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{JSD}(p_{\\mathrm{data}}\\|p_{g})=\\frac12\\mathrm{KL}(p_{\\mathrm{data}}\\|M)+\\frac12\\mathrm{KL}(p_{g}\\|M),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "578 where $M={\\textstyle\\frac{1}{2}}(p_{\\mathrm{data}}+p_{g})$ and KL denotes the Kullback-Leibler divergence. However, JSD has a   \n579 notable drawback: when the real and generated data distributions do not overlap, the JSD becomes   \n580 zero, causing the gradients to vanish. This leads to mode collapse, where the generator produces a   \n581 limited variety of samples.   \n582 In optimal transport theory, the Wasserstein distance is utilized to measure the minimum cost of   \n583 transforming one probability distribution into another. Given two probability distributions $\\mu$ and $\\nu$ on   \n584 a metric space $\\mathcal{X}$ , the Wasserstein distance $W$ is: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\nW(\\mu,\\nu)=\\operatorname*{inf}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\mathbb{E}_{(x,y)\\sim\\gamma}[d(x,y)],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "585 where $\\Pi(\\mu,\\nu)$ is the set of all joint distributions whose marginals are $\\mu$ and $\\nu$ , and $d(x,y)$ is a distance   \n586 metric on $\\mathcal{X}$ . Therefore, we introduce the Wasserstein distance in optimal transport theory as new   \n587 loss function $\\mathcal{L}_{O T}$ , whose objective is to minimize the Wasserstein distance between the generated   \n588 distribution $p_{g}$ and the real distribution $p_{\\mathrm{data}}$ . The $\\mathcal{L}_{O T}$ is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{O T}=\\underset{G}{\\mathrm{min}}\\,\\underset{D\\in\\mathcal{D}}{\\mathrm{max}}\\,\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{data}}}[D(\\mathbf{x})]-\\mathbb{E}_{\\mathbf{z}\\sim p_{\\mathbf{z}}}[D(G(\\mathbf{z}))]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "589 where $\\mathcal{D}$ is the set of 1-Lipschitz functions. This Lipschitz constraint can be enforced through weight   \n590 clipping or gradient penalty. In $\\mathcal{L}_{O T}$ , the discriminator $D$ is constrained to be 1-Lipschitz: ", "page_idx": 15}, {"type": "equation", "text": "$$\n|D(x_{1})-D(x_{2})|\\leq|x_{1}-x_{2}|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "591 This constraint ensures that the discriminator provides meaningful gradients even when $p_{g}$ and $p_{\\mathrm{data}}$   \n592 do not overlap. Using the Kantorovich-Rubinstein duality, we can express the Wasserstein distance   \n593 as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nW(p_{\\mathrm{data}},p_{g})=\\operatorname*{sup}_{\\|f\\|_{L}\\leq1}\\mathbb{E}_{x\\sim p_{\\mathrm{data}}}[f(x)]-\\mathbb{E}_{x\\sim p_{g}}[f(x)].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "594 Since $f$ is Lipschitz continuous, it ensures that the gradients $\\nabla f(x)$ are bounded and do not vanish.   \n595 Hence, during the optimization process, the generator receives consistent and informative gradient   \n596 updates that guide it to produce more realistic and diverse samples. The gradient of the loss function   \n597 $\\mathcal{L}_{O T}$ with respect to the generator\u2019s parameters $\\theta$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathbb{E}_{\\mathbf{z}\\sim p_{\\mathbf{z}}}[D(G_{\\theta}(\\mathbf{z}))]=\\mathbb{E}_{\\mathbf{z}\\sim p_{\\mathbf{z}}}[\\nabla_{\\theta}D(G_{\\theta}(\\mathbf{z}))].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "598 This gradient does not vanish even if $p_{g}$ and $p_{\\mathrm{data}}$ have disjoint supports, thanks to the 1-Lipschitz   \n599 property of $D$ . As a result, the generator $G$ can still receive valuable gradient information to adjust its   \n600 parameters and gradually make $p_{g}$ approximate $p_{\\mathrm{data}}$ even if $p_{g}$ and $p_{\\mathrm{data}}$ do not overlap, effectively   \n601 addressing mode collapse and mode mixing issues. Overall, after introducing optimal transport theory,   \n602 we overcome the gradient vanishing problem inherent in traditional GANs, effectively mitigating   \n603 mode collapse and mode mixing. $\\mathcal{L}_{O T}$ maintains the existence and relevance of gradients during   \n604 training, enabling the generator to continuously improve and produce more diverse and realistic   \n605 handwriting samples. ", "page_idx": 15}, {"type": "text", "text": "606 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "610 Answer: [Yes]   \n611 Justification: We have specifically summarized the main contributions of this article in the   \n612 introduction.   \n613 Guidelines:   \n614 \u2022 The answer NA means that the abstract and introduction do not include the claims made   \n615 in the paper.   \n616 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n617 contributions made in the paper and important assumptions and limitations. A No or   \n618 NA answer to this question will not be perceived well by the reviewers.   \n619 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n620 much the results can be expected to generalize to other settings.   \n621 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n622 are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "623 2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] As shown in Appendix B.2. ", "page_idx": 16}, {"type": "text", "text": "Justification: Non-standard or cursive handwriting may pose challenges for accurate signal generation and recognition. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "655 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "656 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n657 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide a detailed explanation and present rigorous mathematical proof of utilizing optimal transport to alleviate mode collapse and mixing issues of GAN. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "672 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "73 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n74 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n75 of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] We have made every effort to disclose all experimental details, including CPU model, GPU model, PyTorch framework version, built-in IMU specifications of 9 experimental smartphones, and even the gender of volunteers. Justification: The paper provides comprehensive details on the data collection process, data splits, hyperparameters, training procedures, and statistical analyses, ensuring that all necessary information is disclosed to fully reproduce the main experimental results and validate the claims and conclusions. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the ", "page_idx": 17}, {"type": "text", "text": "case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "714   \n715 5. Open access to data and code   \n716 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n717 tions to faithfully reproduce the main experimental results, as described in supplemental   \n718 material?   \n719 Answer: [Yes] We released the world\u2019s first Chinese handwriting recognition dataset based   \n720 on inertial sensors.   \n721 Justification: We submitted the training set (due to system limitations on attachment size) in   \n722 the Supplementary Materials for review, and the full dataset can be found on GitHub. (We   \n723 confirm to license the use of this data set to all AI researchers around the world.)   \n724 Guidelines:   \n725 \u2022 The answer NA means that paper does not include experiments requiring code.   \n726 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n727 public/guides/CodeSubmissionPolicy) for more details.   \n728 \u2022 While we encourage the release of code and data, we understand that this might not be   \n729 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n730 including code, unless this is central to the contribution (e.g., for a new open-source   \n731 benchmark).   \n732 \u2022 The instructions should contain the exact command and environment needed to run to   \n733 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n734 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n735 \u2022 The authors should provide instructions on data access and preparation, including how   \n736 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n737 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n738 proposed method and baselines. If only a subset of experiments are reproducible, they   \n739 should state which ones are omitted from the script and why.   \n740 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n741 versions (if applicable).   \n742 \u2022 Providing as much information as possible in supplemental material (appended to the   \n743 paper) is recommended, but including URLs to data and code is permitted.   \n744 6. Experimental Setting/Details ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "763 model\u2019s robustness and efficacy. The thorough evaluation underscores the potential of CI  \n764 GAN to advance the field of Chinese handwriting recognition through high-quality, diverse   \n765 signal generation.   \n766 Guidelines:   \n767 \u2022 The answer NA means that the paper does not include experiments.   \n768 \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence   \n769 intervals, or statistical significance tests, at least for the experiments that support the   \n770 main claims of the paper.   \n771 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n772 example, train/test split, initialization, random drawing of some parameter, or overall   \n773 run with given experimental conditions).   \n774 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n775 call to a library function, bootstrap, etc.)   \n776 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n777 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n778 of the mean.   \n779 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n780 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n781 of Normality of errors is not verified.   \n782 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n783 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n784 error rates).   \n785 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n786 they were calculated and reference the corresponding figures or tables in the text.   \n787 8. Experiments Compute Resources   \n788 Question: For each experiment, does the paper provide sufficient information on the computer   \n789 resources (type of compute workers, memory, time of execution) needed to reproduce the   \n790 experiments?   \n791 Answer: [Yes] As shown in Table 2.   \n792 Justification: The paper specifies the type of compute resources used, including the Nvidia   \n793 RTX 2080TI GPU and Intel Xeon W-2133 CPU, and provides details on memory usage and   \n794 execution time, ensuring sufficient information is available to reproduce the experiments.   \n795 Guidelines:   \n796 \u2022 The answer NA means that the paper does not include experiments.   \n797 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n798 or cloud provider, including relevant memory and storage.   \n799 \u2022 The paper should provide the amount of compute required for each of the individual   \n800 experimental runs as well as estimate the total compute.   \n801 \u2022 The paper should disclose whether the full research project required more compute   \n802 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n803 didn\u2019t make it into the paper).   \n804 9. Code Of Ethics   \n805 Question: Does the research conducted in the paper conform, in every respect, with the   \n806 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n807 Answer: [Yes]   \n808 Justification: The research adheres to the NeurIPS Code of Ethics in all respects, ensuring   \n809 ethical considerations are met in data collection, experimentation, and reporting of results.   \n810 Guidelines:   \n811 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n812 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n813 deviation from the Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consider", "page_idx": 20}, {"type": "text", "text": "815 ation due to laws or regulations in their jurisdiction).   \n816 10. Broader Impacts   \n817 Question: Does the paper discuss both potential positive societal impacts and negative   \n818 societal impacts of the work performed?   \n819 Answer: [Yes] This research significantly advances Chinese handwriting recognition tech  \n820 nology, providing valuable educational tools and promoting cultural preservation while   \n821 enhancing human-computer interaction across various applications, including smart devices   \n822 and virtual reality, thus has been adopted by the educational aid device manufacturer. As   \n823 shown in Appendix B.1 and Appendix B.2.   \n824 Justification: The paper thoroughly discusses the potential positive societal impacts, such   \n825 as advancements in Chinese handwriting recognition and educational benefits, as well as   \n826 possible negative impacts, including challenges with non-standard handwriting and external   \n827 environmental factors affecting performance.   \n828 Guidelines:   \n829 \u2022 The answer NA means that there is no societal impact of the work performed.   \n830 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n831 impact or why the paper does not address societal impact.   \n832 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n833 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n834 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n835 groups), privacy considerations, and security considerations.   \n836 \u2022 The conference expects that many papers will be foundational research and not tied   \n837 to particular applications, let alone deployments. However, if there is a direct path to   \n838 any negative applications, the authors should point it out. For example, it is legitimate   \n839 to point out that an improvement in the quality of generative models could be used to   \n840 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n841 that a generic algorithm for optimizing neural networks could enable people to train   \n842 models that generate Deepfakes faster.   \n843 \u2022 The authors should consider possible harms that could arise when the technology is   \n844 being used as intended and functioning correctly, harms that could arise when the   \n845 technology is being used as intended but gives incorrect results, and harms following   \n846 from (intentional or unintentional) misuse of the technology.   \n847 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n848 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n849 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n850 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "51 11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "852 Question: Does the paper describe safeguards that have been put in place for responsible   \n853 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n854 image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "868 12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "869 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n870 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n871 properly respected?   \n872 Answer: [Yes] All the data in this experiment were collected independently by ourselves,   \n873 and we also released the data we collected to enhance our contribution to the research field.   \n874 Justification: All creators and original owners of assets used in the paper are properly   \n875 credited, and the license and terms of use are explicitly mentioned and respected, ensuring   \n876 full compliance with intellectual property rights.   \n877 Guidelines:   \n878 \u2022 The answer NA means that the paper does not use existing assets.   \n879 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n880 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n881 URL.   \n882 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n883 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n884 service of that source should be provided.   \n885 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n886 package should be provided. For popular datasets, paperswithcode.com/datasets   \n887 has curated licenses for some datasets. Their licensing guide can help determine the   \n888 license of a dataset.   \n889 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n890 the derived asset (if it has changed) should be provided.   \n891 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n892 the asset\u2019s creators.   \n893 13. New Assets   \n894 Question: Are new assets introduced in the paper well documented and is the documentation   \n895 provided alongside the assets?   \n896 Answer: [Yes] We release the world\u2019s first Chinese handwriting recognition dataset based   \n897 on inertial sensors, and we confirm to license the use of this data set to all AI researchers   \n898 around the world.   \n899 Justification: It can be found in GitHub.   \n900 Guidelines:   \n901 \u2022 The answer NA means that the paper does not release new assets.   \n902 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n903 submissions via structured templates. This includes details about training, license,   \n904 limitations, etc.   \n905 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n906 asset is used.   \n907 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n908 create an anonymized URL or include an anonymized zip file.   \n909 14. Crowdsourcing and Research with Human Subjects   \n910 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n911 include the full text of instructions given to participants and screenshots, if applicable, as   \n912 well as details about compensation (if any)?   \n913 Answer: [NA]   \n914 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n915 Guidelines:   \n916 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n917 human subjects. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]