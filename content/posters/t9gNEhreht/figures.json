[{"figure_path": "t9gNEhreht/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of different fine-tuning paradigms for text-to-image (T2I) generation models. (a) Supervised Fine-tuning (SFT): a T2I model is trained with image-text pairs from existing datasets. (b) Fine-tuning with Human Preference (e.g., RL/DPO): humans annotate their preferences on images by ranking/scoring in terms of text alignments, and a T2I model is trained to maximize the human preference scores. (c) SELMA: instead of collecting image-text pairs or human preference annotations, we automatically collect image-text pairs for desired skills with LLM and T2I model, and create a multi-skill T2I model by learning and merging skill-specific expert models.", "description": "This figure compares three different approaches for fine-tuning text-to-image models: (a) Supervised Fine-tuning (SFT) uses existing image-text datasets, (b) Fine-tuning with Human Preference uses human ranking/scoring of generated images to optimize the model, and (c) SELMA uses an LLM and T2I model to automatically generate image-text pairs for different skills and merges those into a multi-skill model. SELMA is presented as a novel approach.", "section": "1 Introduction"}, {"figure_path": "t9gNEhreht/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of the four-stage pipeline of SELMA (Sec. 3).", "description": "This figure illustrates the four stages of the SELMA pipeline. Stage 1 involves using an LLM for prompt generation with text diversity filtering. Stage 2 uses a text-to-image model to automatically generate images from the prompts.  Stage 3 uses LoRA modules to fine-tune multiple single-skill experts of the T2I model on the image-text datasets generated in stage 2. Lastly, stage 4 merges the single-skill experts using LoRA merging to create a final multi-skill T2I model.", "section": "3 SELMA: Learning and Merging Text-to-Image Skill-Specific Experts with Auto-Generated Data"}, {"figure_path": "t9gNEhreht/figures/figures_6_1.jpg", "caption": "Figure 3: DSG accuracy of SD v2 fine-tuned with different image-text pairs.", "description": "This figure compares the performance of fine-tuning a Stable Diffusion v2 model using different types of image-text pairs.  The x-axis shows the datasets used for fine-tuning (Localized Narratives, CountBench, DiffusionDB, Whoops, COCO, and a combined dataset using LoRA merging). The y-axis represents the DSG accuracy achieved after fine-tuning.  Four bars are presented for each dataset: a dashed line representing the baseline accuracy of the un-finetuned SD v2 model, and three bars showing the accuracy after fine-tuning with ground truth prompts and images, ground truth prompts and automatically generated images from SDv2, and LLM-generated prompts and automatically generated images from SDv2 (the SELMA approach).  The results show that SELMA achieves comparable or even slightly better performance than using ground truth data for fine-tuning.", "section": "5.3 Effectiveness of Auto-Generated Data"}, {"figure_path": "t9gNEhreht/figures/figures_8_1.jpg", "caption": "Figure 4: Human Evaluation on 200 sampled text prompts from DSG, where we show the win vs. lose percentages of SDXL and SDXL+SELMA (Ours).", "description": "This figure displays the results of a human evaluation comparing the image generation performance of SDXL and SDXL+SELMA (the proposed method).  The evaluation involved 200 text prompts from the diverse and challenging DSG benchmark, with three human annotators rating each image pair. For each prompt, annotators chose which image (generated by SDXL or SDXL+SELMA) better matched the prompt description. The bar chart shows the percentage of times SDXL+SELMA was preferred over SDXL for each subset of prompts in the DSG dataset, as well as overall.  The results clearly illustrate that SDXL+SELMA significantly outperforms SDXL across different prompt types.", "section": "5 Results and Analysis"}, {"figure_path": "t9gNEhreht/figures/figures_9_1.jpg", "caption": "Figure 1: Comparison of different fine-tuning paradigms for text-to-image (T2I) generation models. (a) Supervised Fine-tuning (SFT): a T2I model is trained with image-text pairs from existing datasets. (b) Fine-tuning with Human Preference (e.g., RL/DPO): humans annotate their preferences on images by ranking/scoring in terms of text alignments, and a T2I model is trained to maximize the human preference scores. (c) SELMA: instead of collecting image-text pairs or human preference annotations, we automatically collect image-text pairs for desired skills with LLM and T2I model, and create a multi-skill T2I model by learning and merging skill-specific expert models.", "description": "This figure compares three different approaches to fine-tuning text-to-image models.  (a) shows supervised fine-tuning, where the model is trained on existing image-text datasets. (b) illustrates fine-tuning based on human preferences, where humans rate image quality and the model learns to match those preferences. (c) introduces the SELMA approach, which uses a large language model (LLM) and a text-to-image model to automatically generate image-text pairs for various skills, then trains and merges skill-specific expert models for improved faithfulness.", "section": "1 Introduction"}, {"figure_path": "t9gNEhreht/figures/figures_17_1.jpg", "caption": "Figure 1: Comparison of different fine-tuning paradigms for text-to-image (T2I) generation models. (a) Supervised Fine-tuning (SFT): a T2I model is trained with image-text pairs from existing datasets. (b) Fine-tuning with Human Preference (e.g., RL/DPO): humans annotate their preferences on images by ranking/scoring in terms of text alignments, and a T2I model is trained to maximize the human preference scores. (c) SELMA: instead of collecting image-text pairs or human preference annotations, we automatically collect image-text pairs for desired skills with LLM and T2I model, and create a multi-skill T2I model by learning and merging skill-specific expert models.", "description": "This figure compares three different approaches for fine-tuning text-to-image models: supervised fine-tuning using existing datasets, fine-tuning based on human preference annotations, and the proposed SELMA method.  SELMA leverages LLMs and a T2I model to automatically generate image-text pairs, then fine-tunes skill-specific experts before merging them into a single multi-skill model, avoiding the need for human annotation or preference ranking.", "section": "1 Introduction"}, {"figure_path": "t9gNEhreht/figures/figures_19_1.jpg", "caption": "Figure 1: Comparison of different fine-tuning paradigms for text-to-image (T2I) generation models. (a) Supervised Fine-tuning (SFT): a T2I model is trained with image-text pairs from existing datasets. (b) Fine-tuning with Human Preference (e.g., RL/DPO): humans annotate their preferences on images by ranking/scoring in terms of text alignments, and a T2I model is trained to maximize the human preference scores. (c) SELMA: instead of collecting image-text pairs or human preference annotations, we automatically collect image-text pairs for desired skills with LLM and T2I model, and create a multi-skill T2I model by learning and merging skill-specific expert models.", "description": "This figure compares three different approaches for fine-tuning text-to-image models: (a) Supervised Fine-tuning (SFT) uses existing image-text datasets for training, (b) Fine-tuning with Human Preference uses human annotations to train models to maximize human preference, and (c) SELMA, the proposed method, automatically generates image-text pairs with an LLM and a T2I model, then fine-tunes the model on those generated pairs.", "section": "1 Introduction"}, {"figure_path": "t9gNEhreht/figures/figures_21_1.jpg", "caption": "Figure 1: Comparison of different fine-tuning paradigms for text-to-image (T2I) generation models. (a) Supervised Fine-tuning (SFT): a T2I model is trained with image-text pairs from existing datasets. (b) Fine-tuning with Human Preference (e.g., RL/DPO): humans annotate their preferences on images by ranking/scoring in terms of text alignments, and a T2I model is trained to maximize the human preference scores. (c) SELMA: instead of collecting image-text pairs or human preference annotations, we automatically collect image-text pairs for desired skills with LLM and T2I model, and create a multi-skill T2I model by learning and merging skill-specific expert models.", "description": "This figure compares three different approaches for fine-tuning text-to-image models.  (a) shows supervised fine-tuning, where a pre-trained model is further trained on a labeled image-text dataset. (b) illustrates fine-tuning with human preferences, where human feedback (rankings or scores) guides model training. (c) introduces SELMA, which generates its own image-text pairs using an LLM (large language model) and a T2I model, then trains skill-specific models that are finally merged into one.", "section": "1 Introduction"}]