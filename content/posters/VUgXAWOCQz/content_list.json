[{"type": "text", "text": "Randomized algorithms and PAC bounds for inverse reinforcement learning in continuous spaces ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Angeliki Kamoutsi Peter Schmitt-F\u00f6rster EPFL, Switzerland University of Konstanz, Germany angeliki.kamoutsi@epfl.ch peter.schmitt-foerster@uni-konstanz.de ", "page_idx": 0}, {"type": "text", "text": "Tobias Sutter Volkan Cevher John Lygeros University of Konstanz, Germany EPFL, Switzerland ETH Z\u00fcrich, Switzerland tob.sutter@uni-konstanz.de volkan.cevher@epfl.ch jlygeros@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This work studies discrete-time discounted Markov decision processes with continuous state and action spaces and addresses the inverse problem of inferring a cost function from observed optimal behavior. We first consider the case in which we have access to the entire expert policy and characterize the set of solutions to the inverse problem by using occupation measures, linear duality, and complementary slackness conditions. To avoid trivial solutions and ill-posedness, we introduce a natural linear normalization constraint. This results in an infinite-dimensional linear feasibility problem, prompting a thorough analysis of its properties. Next, we use linear function approximators and adopt a randomized approach, namely the scenario approach and related probabilistic feasibility guarantees, to derive $\\varepsilon$ -optimal solutions for the inverse problem. We further discuss the sample complexity for a desired approximation accuracy. Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the standard reinforcement learning (RL) setting [1, 2, 3, 4], a cost signal is given to instruct agents on completing a desired task. However, oftentimes, it is either too challenging to optimize a given cost (e.g., due to sparsity), or it is prohibitively hard to manually engineer a cost function that induces complex and multi-faceted optimal behaviors. At the same time, in many real-world scenarios, encoding preferences using expert demonstrations is easy and provides an intuitive and human-centric interface for behavioral specification [5, 6, 7]. Considering the inverse reinforcement (IRL) problem involves deducing a cost function from observed optimal behavior. IRL is actively researched with applications in engineering, operations research, and biology [8, 9, 10]. There are two main motivations behind inverse decision-making. The first one concerns situations where the cost function is of interest by itself, e.g., for scientific inquiry, modeling of human and animal behavior [11, 12] or modeling of other cooperative or adversarial agents [13]. The second one concerns the task of imitation or apprenticeship learning [14] by first recovering the expert\u2019s cost function and then using it to reproduce and synthesize the optimal behavior. For instance, in engineering, IRL can be used to explain and imitate the observed expert behavior, e.g., in the highway driving task [14, 15], parking lot navigation [16], and urban navigation [17]. Other examples can be found in humanoid robotics and understanding of human locomotion [18]. Despite extensive research efforts, our understanding of IRL still has significant limitations. One major gap lies in the absence of algorithms designed for continuous state and action spaces, which are crucial for numerous promising applications like autonomous vehicles and robotics that operate in continuous environments. Most existing state-ofthe-art IRL algorithms for the continuous setting often adopt a policy-matching approach instead of directly solving the IRL problem [19, 20, 21, 22, 23, 24, 25, 26, 27]. However, this approach tends to provide a less robust representation of agent preferences [26], since the recovered policy is highly dependent on the environment dynamics. State-of-the-art IRL algorithms are empirically successful but lack formal guarantees. Theoretical assurances are crucial for practical implementation, especially in safety-critical systems with potential fatal consequences. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions. This work deals with discrete-time Markov decision processes (MDPs) on continuous state and action spaces under the total expected discounted cost optimality criterion and studies the inverse problem of inferring a cost function from observed optimal behavior. Under the assumption that the control model is Lipschitz continuous, we propose an optimization-based framework to infer the cost function of an MDP given a generative model and traces of an optimal policy. Our approach is based on the linear programming (LP) approach to continuous MDPs [28], complementary slackness optimality conditions, related developments in randomized convex optimization [29, 30, 31] and uniform finite sample bounds from statistical learning theory [32]. ", "page_idx": 1}, {"type": "text", "text": "To this aim, we first consider the case in which we have access to the entire optimal policy $\\pi_{\\mathrm{E}}$ and starting from the LP formulation of the MDP, we characterize the set of solutions to the inverse problem by using occupation measures, linear duality and complementary slackness conditions. This results in an infinite-dimensional linear feasibility problem. Although from a theoretical point of view our approach succeeds in characterizing inverse optimality in its full generality, in practice the following important challenges need to be addressed. First, the inverse problem is ill-conditioned and ill-posed, since each task is consistent with many cost functions. Thus a main challenge is coming up with a meaningful one. To this end, we enforce an additional natural linear normalization constraint in order to avoid trivial solutions and ill-posedness. Another challenge is the infinitedimensionality of the LP formulation, which makes it computationally intractable. To alleviate this difficulty, we propose an approximation scheme that involves a restriction of the decision variables from an infinite-dimensional function space to a finite dimensional subspace (tightening), followed by the approximation of the infinitely-uncountably-many constraints by a finite subset (relaxation). In particular, we use linear function approximators and adopt a randomized approach, namely the scenario approach [29, 33], and related probabilistic feasibility guarantees [30], to derive $\\varepsilon$ -optimal solutions for the inverse problem, as well as explicit sample complexity bounds for a desired approximation accuracy. Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples. ", "page_idx": 1}, {"type": "text", "text": "Related literature. Our principal aim is to address problems with uncountably infinitely many states and actions. Existing IRL algorithms treat the unknown cost function as a linear combination [34, 35, 15, 17, 36, 37] or nonlinear function [38, 39, 40] of features. In particular, there are three broad categories of formulations. In feature expectation matching [35, 15, 17] one attempts to match the feature expectation of a policy to the expert, while in maximum margin planning [36, 38, 40] the goal is to learn mappings from features to cost functions so that the demonstrated policy is better than any other policy by a margin defined by a loss-function. Moreover, a probabilistic approach is to interpret the cost function as parametrization of a policy class such that the true cost function maximizes the likelihood of observing the demonstrations [17, 37, 39, 41]. Most existing IRL methods that recover a cost function, are either designed exclusively for MDPs with finite state and action spaces, or rely on an oracle access to an RL solver which is used repeatedly in the inner loop of an iterative procedure. An exception are the works [42, 43, 44, 22, 25, 45, 26], that perform well in the experimental settings considered, without providing theoretical guarantees. Relying on oracle access to an RL solver is a significant computational burden for applying these methods to MDPs with continuous state and action spaces since solving a continuous MDP is a challenging and computationally expensive problem on its own. As a result, IRL over uncountable spaces remains largely unexplored. In this work we aim to contribute to this line of research and propose a method that avoids repeatedly solving the forward problem and simultaneously provides probabilistic performance guarantees on the quality of the recovered solution. ", "page_idx": 1}, {"type": "text", "text": "Linear duality and complementarity were first proposed in [46] for solving finite-dimensional inverse LPs. The idea was then extended to inverse conic optimization problems in [47] by using KKT optimality conditions. The fundamental difference between these works and the present paper is that they deal with finite-dimensional convex optimization programs where the agent has complete knowledge of the optimal behavior as a finite-dimensional vector. In our setting we have the additional difficulty of the infinite-dimensional and data-driven nature of the problem. In [48] the authors use occupation measures and complementarity in linear programming to formulate the inverse deterministic continuous-time optimal control problem. Under the assumption of polynomial dynamics and semi-algebraic state and input constraints, they propose an approximation scheme based on sum-of-squares semidefinite programming. Contrary to [48], we consider the problem of inverse discrete-time stochastic optimal control. In such a stochastic environment, assuming polynomial dynamics clearly is restrictive, excluding any setting with Gaussian noise, e.g., the LQG problem. Our approach is not limited to the case of polynomial dynamics and semi-algebraic constraints but is able to tackle the general case, while also providing performance guarantees as in [48]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our work is closely related to the recent theoretical works on IRL [49, 50, 51, 52, 53, 54, 55, 56]. However, these papers consider either tabular MDPs [49, 50, 52, 53, 54, 55] or MDPs with continuous states and finite action spaces [51, 56]. In contrast, our contribution delves into the theoretical analysis of IRL in the intricate landscape of continuous state and action spaces. Notably, our framework, when applied to finite tabular MDPs and a stationary Markov expert policy $\\pi_{\\mathrm{E}}$ , simplifies to the inverse feasibility set considered in [52, 53, 54] (see also Appx A.2). The methodology put forth in those studies, extends the LP formulation previously explored in [12, 49, 50, 51], which primarily dealt with deterministic expert policies of the form $\\pi_{\\mathrm{E}}\\equiv a_{1}$ . In our work, by using occupancy measures instead of policies and employing Lagrangian duality, we are able to characterize inverse feasibility for general continuous MDPs regardless of the complexity of the expert policy. Moreover, our framework empowers us to leverage offilne expert demonstrations to compute an approximate feasibility set and recover a cost through a sample-based convex program. This flexibility surpasses previous theoretical IRL settings, where either $\\pi_{\\mathrm{E}}$ is assumed to be fully known [49, 51, 52] or active querying of $\\pi_{\\mathrm{E}}$ is possible for each state [53, 54]. Finally, our assumption of a Lipschitz MDP model is milder and more general than the infinite matrix representation considered in [51], thus accommodating a broader range of MDP models. Overall, we establish a link between our methodology and the existing body of literature on LP formulations for IRL, while also accounting for continuous states and action spaces and more general expert policies. Finally, we would like to highlight a key distinction between our work and recent theoretical IRL papers [52, 53, 54, 55]. Unlike these recent works, our study goes beyond the examination of the properties of the inverse feasibility set and its estimated variant. Our contribution extends to tackling the reward ambiguity problem, a well-known limitation of the IRL paradigm, and provides theoretical results in this direction. Additionally, our work introduces function approximation techniques that come with robust theoretical guarantees. Finally, we study how constraint sampling in infinite-dimensional LPs can be exploited to derive a single nearly optimal solution with probabilistic performance guarantees. ", "page_idx": 2}, {"type": "text", "text": "Basic definitions and notations. Let $(X,\\rho)$ be a Borel space, i.e., $X$ is a Borel subset of a complete and separable $\\rho$ -metric space, and let $B(X)$ be its Borel $\\sigma$ -algebra. We denote by ${\\mathcal{M}}(X)$ the Banach space of finite signed Borel measures on $X$ equipped with the total variation norm and by ${\\mathcal{P}}(X)$ the convex set of Borel probability measures. Let $\\delta_{x}\\,\\in\\,{\\mathcal{P}}(X)$ be the Dirac measure centered at $x\\in X$ . Measurability is always understood in the sense of Borel measurability. An open ball in $(X,\\rho)$ with radius $r$ and center $x_{0}$ is denoted by $B_{r}(x_{0})\\,=\\,\\{x\\,\\in\\,X\\,:\\,\\rho(x,\\dot{x_{0}})\\,<\\,r\\}$ . Given a measurable function $u:X\\to\\mathbb{R}$ , its sup-norm is given by $\\|u\\|_{\\infty}\\triangleq\\operatorname*{sup}_{x\\in X}|u(x)|$ . Moreover, we define the Lipschitz semi-norm by |u|L \u225csupx\u0338=x\u2032 |u $\\begin{array}{r}{|u|_{\\mathrm{L}}\\triangleq\\operatorname*{sup}_{x\\neq x^{\\prime}}\\left\\{\\frac{|u(x)-u(x^{\\prime})|}{\\rho(x,x^{\\prime})}\\right\\}}\\end{array}$ and the Lipschitz norm by $\\Vert u\\Vert_{\\mathrm{L}}\\triangleq\\Vert u\\Vert_{\\infty}+\\vert u\\vert_{\\mathrm{L}}$ . Let $\\operatorname{Lip}(X)$ be the Banach space of real-valued bounded Lipschitz continuous functions on $X$ together with the Lipschitz norm $\\lVert\\cdot\\rVert_{\\mathrm{L}}$ . Then, $(\\mathcal{M}(X),\\operatorname{Lip}(X))$ forms a dual pair of vector spaces with duality brackets $\\textstyle\\langle\\mu,u\\rangle\\triangleq\\int_{\\mathcal{X}}u(x){\\mathsf{d}}\\mu$ , for all $\\mu\\in\\mathcal{M}(X)$ , $u\\in\\mathrm{Lip}(X)$ . Moreover, if $\\mathcal{M}(X)_{+}$ is the convex cone of finite nonnegative Borel measures on $X$ , then its dual convex cone is the set $\\operatorname{Lip}(X)_{+}$ of nonnegative bounded and Lipschitz continuous functions on $X$ . Under the additional assumption that $X$ is compact, the Wasserstein norm $\\lVert\\cdot\\rVert_{\\mathbf{W}}$ on ${\\mathcal{M}}(X)$ is dual to the Lipschitz norm, i.e., $\\begin{array}{r}{\\|\\mu\\|_{\\mathrm{W}}\\triangleq\\operatorname*{sup}_{\\|u\\|_{\\mathrm{L}}\\leq1}\\left\\langle\\mu,u\\right\\rangle\\!.}\\end{array}$ If $X,Y$ are Borel spaces, a stochastic kernel on $X$ given $Y$ is a function $P(\\cdot|\\cdot):B(X)\\times Y\\to[0,1]$ such that $P(\\cdot|y)\\in{\\mathcal{P}}(X)$ , for each fixed $y\\in Y$ , and $P(B|\\cdot)$ is a measurable real-valued function on $Y$ , for each fixed $B\\in B(X)$ . ", "page_idx": 2}, {"type": "text", "text": "2 Markov decision processes and linear programming formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Continuous Markov decision process. Consider a Markov decision process (MDP) given by a tuple $\\mathcal{M}_{c}\\triangleq\\left(\\mathcal{X},\\mathcal{A},\\mathsf{P},\\gamma,\\nu_{0},c\\right)$ , where $\\mathcal{X}$ is a Borel space called the state space, $\\boldsymbol{\\mathcal{A}}$ is a Borel space ", "page_idx": 2}, {"type": "text", "text": "called the action space, $\\mathsf{P}$ is a stochastic kernel on $\\mathcal{X}$ given $\\mathcal{X}\\times\\mathcal{A}$ called the transition law, $\\gamma\\in(0,1)$ is the discount factor, $\\nu_{0}\\in\\mathcal{P}(\\mathcal{X})$ is the initial probability distribution, and $c:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ is the cost function. The model $\\mathbf{\\mathcal{M}}_{c}$ represents a controlled discrete-time stochastic system with initial state $x_{0}\\sim\\nu_{0}(\\cdot)$ . At time step $t$ , if the system is in state $x_{t}=x\\in\\mathcal{X}$ , and the action $a_{t}=a\\in\\mathcal{A}$ is taken, then a corresponding cost $c(x,a)$ is incurred, and the system moves to the next state $x_{t+1}\\sim\\mathsf{P}(\\cdot|x,a)$ . Once transition into the new state has occurred, a new action is chosen, and the process is repeated. A stationary Markov policy $\\pi$ is a stochastic kernel on $\\boldsymbol{\\mathcal{A}}$ given $\\mathcal{X}$ and $\\pi(\\cdot|x)\\in{\\mathcal{P}}(A)$ denotes the probability distribution of the action $a_{t}$ taken at time $t$ , while being in state $x$ . We denote the space of stationary Markov policies by $\\Pi_{0}$ . Given a policy $\\pi$ , we denote by $\\mathbb{P}_{\\nu_{0}}^{\\pi}$ the induced probability measure1 on the canonical sample space $\\Omega\\triangleq(\\mathcal{X}\\times\\mathcal{A})^{\\infty}$ , i.e., $\\mathbb{P}_{\\nu_{0}}^{\\pi}[\\cdot]=\\mathbb{P}\\mathrm{rob}[$ \u00b7 $|\\pi,x_{0}\\sim\\nu_{0}]$ is the probability of an event when following $\\pi$ starting from $x_{0}\\sim\\nu_{0}$ . The expectation operator with respect to the trajectories generated by $\\pi$ when $x_{0}\\sim\\nu_{0}$ , is denoted by $\\mathbb{E}_{\\nu_{0}}^{\\pi}$ . If $\\nu_{0}=\\delta_{x}$ for some $x\\in\\mathscr{X}$ , then we will write for brevity $\\mathbb{P}_{x}^{\\pi}$ and $\\mathbb{E}_{x}^{\\pi}$ .   \nThe optimal control problem we are interested in is 2 ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{c}^{\\star}(\\nu_{0})\\triangleq\\operatorname*{min}_{\\pi\\in\\Pi_{0}}V_{c}^{\\pi}(\\nu_{0}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r l r}{V_{c}^{\\pi}(\\nu_{0})\\!}&{{}\\triangleq}&{\\!\\mathbb{E}_{\\nu_{0}}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}c(x_{t},a_{t})\\right]}\\end{array}$ . A policy $\\pi^{\\star}$ is called $\\gamma$ -discounted $\\nu_{0}$ -optimal if $V_{c}^{\\pi^{\\star}}(\\nu_{0})\\,=\\,V_{c}^{\\star}(\\nu_{0})$ , and the optimal value function $V_{c}^{\\star}:\\mathcal{X}\\rightarrow\\mathbb{R}$ is given by $V_{c}^{\\star}(x)\\triangleq V_{c}^{\\star}(\\delta_{x})$ . We impose the following assumptions on the MDP model which hold throughout the article. These are the usual continuity-compactness conditions [59], together with the Lipschitz continuity of the elements of the MDP; see, e.g., [60]. We recall that the transition law $\\mathsf{P}$ acts on bounded measurable functions $u:\\mathcal{X}\\rightarrow\\mathcal{R}$ from the left as $\\begin{array}{r}{\\mathsf{P}u(x,a)\\triangleq\\int_{\\mathcal{X}}u(y)\\mathsf{P}(\\mathrm{d}y|x,a)}\\end{array}$ , for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1 (Lipschitz control model). ", "page_idx": 3}, {"type": "text", "text": "(A1) $\\mathcal{X}$ and $\\boldsymbol{\\mathcal{A}}$ are compact subsets of Euclidean spaces; ", "page_idx": 3}, {"type": "text", "text": "(A2) the transition law $\\mathsf{P}$ is weakly continuous, meaning that $\\mathsf{P}u$ is continuous on $\\mathcal{X}\\times\\mathcal{A},$ , for every continuous function $u:\\mathcal{X}\\rightarrow\\mathbb{R}$ . Moreover, $\\mathsf{P}$ is Lipschitz continuous, i.e., there exists a constant $L_{\\mathsf{P}}>0$ such that for all $(x,a),(y,b)\\in\\mathcal{X}\\stackrel{\\cdot}{\\times}A$ and all $u\\in\\mathrm{Lip}(\\mathcal{X})$ , it holds that $\\begin{array}{r}{|\\mathsf{P}u(x,a)-\\mathsf{P}u(y,b)|\\leq L_{\\mathsf{P}}|u|_{\\mathrm{L}}\\big(\\|x-y\\|_{2}+\\|a-b\\|_{2}\\big)}\\end{array}$ ; ", "page_idx": 3}, {"type": "text", "text": "Note that Assumption2.1 (A2) is fulfliled when the transition law P has a density function $f(y,x,a)$ that is Lipschitz continuous in $y$ uniformly in $(x,a)$ [60]. This encompasses various probability distributions, such as the uniform, Gaussian, exponential, Beta, Gamma, and Laplace distributions, among others. Additionally, it applies to the infinite matrix representation considered in[51]. Consequently, Assumption 2.1 accommodates a broad range of MDP models and allows for the consideration of smooth and continuous dynamics that reflect the characteristics of several real-world applications, such as robotics, or autonomous driving. Importantly, Assumption 2.1 ensures that the value function $V_{c}^{\\star}$ is in $\\operatorname{Lip}({\\mathcal{X}})$ and is uniquely characterized by the Bellman optimality equation $\\begin{array}{r}{V_{c}^{\\star}(x)=\\operatorname*{min}_{a\\in\\bar{\\mathcal{A}}}\\{c(x,a)+\\gamma\\int_{\\mathcal{X}}V_{c}^{\\star}(y)\\mathsf{P}(d y|x,a)\\}}\\end{array}$ , for all $x\\in\\mathscr{X}$ [60, Thm. 3.1] and [61]. ", "page_idx": 3}, {"type": "text", "text": "Occupancy measures. For every policy $\\pi$ , we define the occupancy measure $\\mu_{\\nu_{0}}^{\\pi}\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}$ by $\\begin{array}{r}{\\mu_{\\nu_{0}}^{\\pi}(E)\\triangleq\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}_{\\nu_{0}}^{\\pi}\\left[(x_{t},a_{t})\\in E\\right]\\!,E\\in\\mathcal{B}(\\mathcal{X}\\!\\times\\!\\mathcal{A})}\\end{array}$ . The occupancy measure can be interpreted as the discounted visitation frequency of the set $E$ when acting according to policy $\\pi$ . The set of occupancy measures is characterized in terms of linear constraint satisfaction [62, Theorem 6.3.7]. To this end consider the convex set of measures, $\\mathfrak{F}\\triangleq\\{\\mu\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}:\\ T_{\\gamma}\\mu=\\nu_{0}\\}$ , where $T_{\\gamma}:\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})\\rightarrow\\mathcal{M}(\\mathcal{X})$ is a linear and weakly continuous operator given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n(T_{\\gamma}\\mu)(B)\\triangleq\\mu(B\\times A)-\\gamma\\int_{\\mathcal{X}\\times\\mathcal{A}}\\mathsf{P}(B|x,a)\\mu(\\operatorname{d}(x,a)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "VUgXAWOCQz/tmp/039d8d245f3f797423b0ba1474090a66afbdfde6fc39f7ba727b8aea290e4fad.jpg", "img_caption": ["Figure 1: Illustration of Theorem 3.1 for $\\varepsilon_{1}>\\varepsilon_{2}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The linear programming approach. A direct consequence is that $\\mathrm{(MDP_{c})}$ ) can be stated equivalently as an infinite-dimensional LP over measures ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{T}_{c}(\\nu_{0})\\triangleq\\operatorname*{inf}_{\\mu\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}}\\{\\langle\\mu,c\\rangle:\\;T_{\\gamma}\\mu=\\nu_{0}\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In particular the infimum in $(\\bf P_{c})$ is attained and $\\pi^{\\star}$ is optimal for $\\mathrm{(MDP_{c})}$ if and only if $\\mu_{\\nu_{0}}^{\\pi^{\\star}}$ is optimal for the primal LP $(\\mathrm{P_{c}})$ . The dual LP of $(\\mathrm{P_{c}})$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{I}_{c}^{*}(\\nu_{0})\\triangleq\\operatorname*{sup}_{u\\in\\mathrm{Lip}(\\mathcal{X})}\\{\\langle\\nu_{0},u\\rangle:\\;c-T_{\\gamma}^{*}u\\geq0\\mathrm{~on~}\\mathcal{X}\\times\\mathcal{A}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the adjoint linear operator $T_{\\gamma}^{*}:\\mathrm{Lip}(\\mathcal{X})\\rightarrow\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})$ of $T_{\\gamma}$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n(T_{\\gamma}^{*}u)(x,a)\\triangleq u(x)-\\gamma\\int_{\\mathcal{X}}u(y)\\mathsf{P}(\\mathrm{d}y|x,a).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Under Assumption 2.1, $T_{\\gamma}^{*}$ is well-defined and the dual LP $\\left(\\mathrm{D}_{\\mathbf{c}}\\right)$ is solvable, i.e., the supremum is attained, and strong duality holds. That is, $\\mathcal{I}_{c}(\\nu_{0})=\\mathcal{I}_{c}^{*}(\\nu_{0})=V_{c}^{\\star}(\\nu_{0})$ . In particular, the value function $V_{c}^{\\star}$ is an optimal solution for the dual LP $\\left(\\mathbf{D_{c}}\\right)$ ). More details on the LP formulations for MDPs can be found in Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "3 Inverse reinforcement learning and characterization of solutions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first define the inverse reinforcement learning (IRL) problem and the inverse feasibility set. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 (IRL [12, 52]). An IRL problem is a pair $B\\triangleq(\\mathcal{M},\\pi_{\\mathrm{E}}).$ , where $\\mathcal{M}\\triangleq(\\mathcal{X},\\mathcal{A},\\mathsf{P},\\nu_{0},\\gamma)$ is an MDP without cost function and $\\pi_{\\mathrm{E}}$ is an observed expert policy. We say that $c\\in\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ is inverse feasible for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , $i f\\pi_{\\mathrm{E}}$ is a $\\gamma$ -discount $\\nu_{0}$ -optimal policy for $\\big(\\mathrm{MDP_{c}},$ ) with cost c. The set of all $c\\in\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ that are inverse feasible is called the inverse feasibility set and is denoted by $\\mathcal{C}(\\pi_{\\mathrm{E}})$ . ", "page_idx": 4}, {"type": "text", "text": "Next, we use the primal-dual LP approach to MDPs and complementary slackness to characterize ${\\mathcal{C}}(\\pi_{\\mathrm{E}})$ . To this end, we first define the $\\varepsilon$ -inverse feasibility set ${\\mathcal{C}}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2. Let $\\varepsilon\\geq0$ . We say that a cost function $c$ is $\\varepsilon$ -inverse feasible for $B=({\\mathcal{M}},\\pi_{\\mathrm{E}})$ and denote $c\\in{\\mathcal{C}}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ if and only if, $c\\in\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ and there exists $u\\in\\mathrm{Lip}(\\mathcal{X})$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u\\rangle}&{\\leq\\varepsilon,}\\\\ {c-T_{\\gamma}^{*}u}&{\\geq-\\varepsilon,\\ o n\\ \\chi\\times\\mathcal{A}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We are now ready to characterize the solutions to IRL, following arguments from [46, 47, 48]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Inverse feasibility set characterization). Let $\\pi_{\\mathrm{E}}\\in\\Pi$ . Under Assumption 2.1 on the Markov decision model $\\mathbf{\\mathcal{M}}_{c}$ , the following assertions are equivalent ", "page_idx": 4}, {"type": "equation", "text": "$e\\in{\\mathcal{C}}^{0}(\\pi_{\\mathrm{E}}).$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3. $\\pi_{\\mathrm{E}}$ is $\\gamma$ -discount $\\nu_{0}$ -optimal for $\\langle\\mathrm{MDP_{c}}.$ ) with cost function c. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As a consequence, $\\begin{array}{r}{\\mathcal{C}(\\pi_{\\mathrm{E}})=\\mathcal{C}^{0}(\\pi_{\\mathrm{E}})=\\bigcap_{\\varepsilon>0}\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})}\\end{array}$ . Moreover, ${\\mathcal{C}}(\\pi_{\\mathrm{E}})$ is a convex cone and $\\lVert\\cdot\\rVert_{\\mathrm{L}^{\\prime}}$ - closed in $\\operatorname{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ . ", "page_idx": 5}, {"type": "text", "text": "As a result, a cost function is inverse feasible for $B=({\\mathcal{M}},\\pi_{\\mathrm{E}})$ if and only if it is $\\varepsilon$ -inverse feasible for all $\\varepsilon>0$ . The characterization of the inverse feasibility set is due to linear duality and complementary slackness conditions. In particular, the constraint that holds pointwise in (19) is due to dual feasibility while the constraint that holds in expectation is due to strong duality, The details are provided in the proof of Theorem 3.1 in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "Notably, when $\\mathcal{X}$ and $\\boldsymbol{\\mathcal{A}}$ are finite, and the expert policy $\\pi_{\\mathrm{E}}$ is stationary Markov, our formulation aligns with the finite-dimensional inverse feasibility set introduced in [52, 53, 54]. Furthermore, when the expert is deterministic of the form $\\pi_{\\mathrm{E}}(x)\\equiv{\\dot{a}}_{1}$ , for all $x$ , then we recover the linear programs discussed in [12, 49, 51] (see Appendix A.2). ", "page_idx": 5}, {"type": "text", "text": "Using occupancy measures instead of policies, we can assess inverse feasibility for continuous MDPs, regardless of expert policy complexity. This approach allows us to utilize offilne expert demonstrations for computing an approximate feasibility set and deriving costs via a sample-based convex program This flexibility surpasses previous theoretical settings, where either $\\pi_{\\mathrm{E}}$ is assumed to be fully known and deterministic [49, 51, 52] or active querying of $\\pi_{\\mathrm{E}}$ is possible for each state [53, 54]. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.1 ( $\\dot{\\varepsilon}$ -inverse feasibility set characterization). Under Assumption 2.1, for any $\\varepsilon>0$ , it holds that a cost function $\\tilde{c}$ is in ${\\mathcal{C}}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ if and only if $\\pi_{\\mathrm{E}}$ is $\\scriptstyle{\\frac{2-\\gamma}{1-\\gamma}}\\varepsilon$ -optimal for $\\big(\\mathbf{MDP_{\\tilde{c}}}\\big)$ with cost c\u02dc. ", "page_idx": 5}, {"type": "text", "text": "As $\\varepsilon\\rightarrow0$ , the next proposition indicates a close approximation to the inverse problem solution. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.2. Let $(\\varepsilon_{n})_{n}$ be a sequence such that $\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}=0$ and let $c_{n}\\in\\mathcal{C}^{\\varepsilon_{n}}(\\pi_{\\mathrm{E}})$ . Then, every accumulation point c of the sequence $(c_{n})_{n}$ is inverse feasible, i.e., $c\\in{\\mathcal{C}}(\\pi_{\\mathrm{E}})$ . ", "page_idx": 5}, {"type": "text", "text": "Finally we show that the $\\varepsilon$ -inverse feasibility set ${\\mathcal{C}}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ satisfies the $\\varepsilon$ -optimality criterion considered in [52, 53, 54]; see for example [53, Def. 2]. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3. Let $\\varepsilon>0$ . It holds that $\\begin{array}{r}{\\operatorname*{inf}_{c\\in{\\mathcal C}(\\pi_{\\mathrm{E}})}V_{c}^{\\tilde{\\pi}}\\big(\\nu_{0}\\big)-V_{c}^{\\pi_{\\mathrm{E}}}\\big(\\nu_{0}\\big)\\leq\\frac{2-\\gamma}{1-\\gamma}\\varepsilon,}\\end{array}$ , for all $\\tilde{c}\\in\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ where $\\tilde{\\pi}$ is an optimal policy for the recovered cost $\\tilde{c}$ . ", "page_idx": 5}, {"type": "text", "text": "This condition ensures that when $\\varepsilon$ is small we avoid an unnecessarily large approximate feasibility set since there is a possible true cost in ${\\mathcal{C}}(\\pi_{\\mathrm{E}})$ with a small error for every possible recovered cost function in ${\\mathcal{C}}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ . 3 ", "page_idx": 5}, {"type": "text", "text": "4 Towards recovering a nearly optimal cost function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although we characterized the inverse and $\\varepsilon$ -inverse feasibility sets in Theorem 3.1 and Proposition 3.1 respectively, it is not clear yet how to compute them, as (19) is an infinite-dimensional feasibility LP. In practice, the following challenges need to be addressed: ", "page_idx": 5}, {"type": "text", "text": "(a) The inverse problem is ill-conditioned and ill-posed since each task is consistent with many cost functions, and thus a central challenge is to end up with a meaningful one. To avoid trivial solutions, in Section 4.1 we motivate the addition of a linear normalization constraint.   \n(b) Another challenge appears because the LP formulation is infinite-dimensional, hence computationally intractable. In Section 4.2 we address this problem by proposing a tractable approximation method with probabilistic performance bounds.   \n(c) In practice, complete knowledge of $\\pi_{\\mathrm{E}}$ and $\\mathsf{P}$ is often unavailable. In Section 4.3, we tackle this challenge by assuming that we have access to a finite set of traces of the expert policy and a generative-model oracle. We use empirical counterparts of $\\pi_{\\mathrm{E}}$ and $\\mathsf{P}$ and provide error bounds to quantify our approach\u2019s accuracy with sample data. ", "page_idx": 5}, {"type": "text", "text": "The main building blocks of our methodology are depicted in Figure 2. ", "page_idx": 5}, {"type": "image", "img_path": "VUgXAWOCQz/tmp/d780ce9bbff91689563411c781bf72ac651f81be14728987fa9f7252ad6c74c2.jpg", "img_caption": ["Figure 2: Main building blocks of our methodology "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Normalization constraint ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A well-known limitation of IRL is that it suffers from the ambiguity issue, i.e., the problem admits infinitely many solutions. For example, any constant cost function, including the zero cost, is inverse feasible. In addition, as it is apparent from the characterization of $\\mathcal{C}(\\pi_{\\mathrm{E}})=\\mathcal{C}^{0}(\\pi_{\\mathrm{E}})$ in Theorem 3.1, for any $u\\in\\mathrm{Lip}(\\mathcal{X})$ and $c\\in{\\mathcal{C}}(\\pi_{\\mathrm{E}})$ , the cost $c+T_{\\gamma}^{*}u$ is inverse feasible. This phenomenon, also known as reward shaping [63] refers to the modification or design of a reward function to provide additional guidance or incentives to an agent during learning. In addition, since ${\\mathcal{C}}(\\pi_{\\mathrm{E}})$ is a convex cone and closed for the sup-norm (Theorem 3.1) the set of solutions to IRL is closed to convex combinations and uniform limits. ", "page_idx": 6}, {"type": "text", "text": "All these examples illustrate that the inverse feasibility set $\\mathcal{C}(\\pi_{\\mathrm{E}})$ contains some solutions that arise from mathematical artifacts. To mitigate this difficulty and avoid trivial solutions, inspired by [48], we enforce the additional natural normalization constraint $\\begin{array}{r}{\\int_{\\mathcal{X}\\times\\mathcal{A}}(c-T_{\\gamma}^{*}u)(x,a)\\,\\mathrm{d}(\\bar{x_{,}}a)=\\dot{1}.}\\end{array}$ . The following proposition justifies this choice. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1. If Definition 3.2 of ${\\mathcal C}(\\pi_{\\mathrm{E}})={\\mathcal C}^{0}(\\pi_{\\mathrm{E}})$ includes the normalization constraint $\\scriptstyle\\int_{{\\mathcal{X}}\\times{\\mathcal{A}}}(c-$ $T_{\\gamma}^{*}u)\\mathsf{d}x\\mathsf{d}a=1$ , then all constant cost functions are excluded from the inverse feasibility set. ", "page_idx": 6}, {"type": "text", "text": "Alternatively, it is possible to employ additional heuristics to narrow down the set of possible solutions and incorporate prior knowledge, e.g., by restricting the class where the true cost belongs, constraining the dependence between costs and value functions, and enforcing conic constraints or shape conditions. ", "page_idx": 6}, {"type": "text", "text": "It is worth mentioning that the normalization constraint in our formulation primarily aims to mitigate the ill-posedness, or ambiguity issue, intrinsic to the IRL problem, rather than to resolve issues of identifiability. In particular, we state and prove that the normalization constraint rules out a wide class of trivial solutions, i.e., all constant functions and inverse solutions of the form $c=T_{\\gamma}^{\\ast}u$ , an outcome devoid of physical meaning and a mathematical artifact. While the identifiability problem and the ill-posedness problem are related in IRL, they are not the same. Identifiability deals with the uniqueness of the true cost function and cannot be fully resolved unless, for example, one has access to multiple expert policies or environments for comparison [64, 65]. On the other hand, ill-posedness is a broader concept from mathematical and statistical problems and refers to situations where a problem does not satisfy the conditions for being well-posed, e.g., in our case due to infinitely many solutions. Note that, unlike recent theoretical IRL works [52, 53, 54] which avoid discussing this issue altogether, we attempt to address the ambiguity problem and provide theoretical results in this direction, hoping to lay the foundations for overcoming current limitations. ", "page_idx": 6}, {"type": "text", "text": "4.2 The case of known dynamics and expert policy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first consider the case where the expert policy $\\pi_{\\mathrm{E}}$ , the induced occupation measure $\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}$ and the transition law $\\mathsf{P}$ are known. We leverage developments in randomized convex optimization, leading to an approximation scheme with a priori performance guarantees. As a first step, we introduce a semi-infinite convex formulation that enforces the normalization constraint, involves a restriction of the decision variables from an infinite-dimensional function space to a finite-dimensional subspace, and considers an additional norm constraint that effectively acts as a regularizer. The resulting regularized semi-infinite inner approximation, which we call inverse program is given by ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l l}{\\operatorname*{inf}_{\\alpha,\\beta,\\varepsilon}}&{\\varepsilon}\\\\ {\\mathrm{s.t.}}&{\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u\\right\\rangle\\leq\\varepsilon,}\\\\ &{c(x,a)-T_{\\gamma}^{*}u(x,a)\\geq-\\varepsilon,\\;\\forall\\,(x,a)\\in\\mathcal{X}\\times\\mathcal{A},}\\\\ &{\\int_{\\mathcal{X}\\times\\mathcal{A}}(c-T_{\\gamma}^{*}u)(x,a)\\,\\mathrm{d}(x,a)=1,}\\\\ &{c\\in\\mathbf{C}_{n_{c}},u\\in\\mathbf{U}_{n_{u}},\\varepsilon\\geq0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{C}_{n_{c}}\\triangleq\\{\\sum_{j=1}^{n_{c}}\\alpha_{j}c_{j}:\\alpha=\\{\\alpha_{i}\\}_{i=1}^{n_{c}}\\in\\mathbb{R}^{n_{c}}}\\end{array}$ , $\\|\\alpha\\|_{1}\\leq\\theta\\}$ with $\\{c_{i}\\}_{i=1}^{n_{c}}\\subset\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})$ being linearly independent basis functions with Lipschitz constant $L_{c}>0$ , $\\begin{array}{r}{\\mathbf{U}_{n_{u}}\\triangleq\\{\\sum_{i=1}^{n_{u}}\\beta_{i}u_{i}:\\;\\beta=}\\end{array}$ $\\{\\beta_{i}\\}_{i=1}^{n_{i}}\\in\\bar{\\mathbb{R}^{n_{u}}}$ , $\\|\\beta\\|_{1}\\leq\\theta\\}$ , with $\\{u_{i}\\}_{i=1}^{n_{u}}\\,\\bar{\\subset}\\,\\mathrm{Lip}(\\mathcal{X})$ being linearly independent basis functions with Lipschitz constant $L_{u}>0$ , and $\\theta>0$ is an appropriately chosen regularization parameter. Note that (IP) is derived by relaxing the constraints in the inverse feasibility set $\\mathcal{C}(\\pi_{\\mathrm{E}})$ and paying a penalty when violated. In particular, let $(\\tilde{\\varepsilon},\\tilde{\\alpha},\\tilde{\\beta})$ be an optimal solution of the semi-infinite program (IP). Then, $\\begin{array}{r}{\\tilde{c}\\triangleq\\sum_{i=1}^{n_{c}}\\tilde{\\alpha}_{i}c_{i}\\in\\mathcal{C}^{\\tilde{\\varepsilon}}(\\pi_{\\mathrm{E}})}\\end{array}$ , from where it becomes apparent that the smaller the value of $\\tilde{\\varepsilon}$ , the better the quality of the extracted cost function $\\tilde{c}$ (as by Proposition 3.1). One would intuitively expect that $\\tilde{\\varepsilon}$ depends on the choice of basis functions for the cost (resp. value) function as well as on the parameters $n_{c}$ (resp. $n_{u}$ ) and $\\theta$ . This dependency is highlighted by the following proposition. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.2 (Basis function dependency). Let $\\pi_{\\mathrm{E}}$ be an optimal policy for the optimal control problem $\\mathbf{MDP_{c^{\\star}}}$ with cost $c^{\\star}$ and let $u^{\\star}$ be the corresponding optimal value function. Under Assumption 2.1, if $u_{1}\\equiv1$ and $\\begin{array}{r}{\\theta>\\frac{1}{(1-\\gamma)\\operatorname*{min}\\left\\{1,d\\right\\}}}\\end{array}$ , then $\\tilde{\\varepsilon}\\le\\varepsilon_{\\mathrm{approx}}$ with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varepsilon_{\\mathrm{appox}}:=\\left(\\frac{2-\\gamma}{1-\\gamma}+\\mathcal{D}_{\\gamma,\\theta}(2+\\gamma)\\operatorname*{max}\\{1,L_{\\mathsf{P}},d\\}\\right)\\left(\\operatorname*{min}_{c\\in\\mathbf{C}_{n_{c}}}\\Vert c^{\\star}-c\\Vert_{\\mathsf{L}}+\\operatorname*{min}_{u\\in\\mathbf{U}_{n_{u}}}\\Vert u^{\\star}-u\\Vert_{\\mathrm{L}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $d=\\operatorname{leb}({\\mathcal{X}}\\times{\\mathcal{A}})$ is the Lebesgue measure of $\\begin{array}{r}{\\chi\\times\\mathcal{A},\\,\\mathcal{D}_{\\gamma,\\theta}\\,\\triangleq\\,\\frac{2\\theta(K_{c,\\infty}+K_{u,\\infty})}{(1-\\gamma)^{2}\\operatorname*{min}\\{1,d\\}\\theta+\\gamma-1}}\\end{array}$ (1\u2212\u03b3)2 min\u221e{1,d}\u03b8\u221e+\u03b3\u22121, with constants $K_{c,\\infty}\\triangleq\\operatorname*{max}_{i=1,\\dots,n_{c}}\\left\\|c_{i}\\right\\|_{\\infty}$ and $K_{u,\\infty}\\triangleq\\operatorname*{max}_{j=1,\\dots,n_{u}}\\left\\|u_{i}\\right\\|_{\\infty}$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.2 sheds light on how the choice of basis functions and the regularization parameter $\\theta$ influence the approximation error. Essentially, the approximation error term $\\varepsilon_{\\mathrm{approx}}$ measures the expressiveness of the linear function approximators. Prior knowledge about the properties of the true cost allows us to choose appropriate basis functions to make the projection residuals in the theorem sufficiently small. For example, if the true cost function is known to be smooth, Fourier or polynomial basis functions can be used. In general, if we choose linearly dense bases in $\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ and $\\operatorname{Lip}({\\mathcal{X}})$ , then the projection residuals and so $\\tilde{\\varepsilon}$ tend to 0 as $n_{c}$ and $n_{u}$ and the regularization parameter $\\theta$ tend to infinity. In particular note that when $c^{\\star}\\in\\mathbf{C}_{n_{c}}$ and $u^{\\star}\\in\\mathbf{U}_{n_{u}}$ , then the corresponding projection residuals are 0, and thus $\\tilde{\\varepsilon}=0$ as expected. In a practical setting, observing a large value of $\\tilde{\\varepsilon}$ is an indicator that more basis functions are needed. ", "page_idx": 7}, {"type": "text", "text": "Finally, note that the regularizer helps to bound the dual optimizer using a dual norm, thus offering an explicit approximation error for the proposed solution (see Appendix B.6). ", "page_idx": 7}, {"type": "text", "text": "Computationally tractable approximations to the semi-infinite convex program can be obtained through the scenario approach [29, 66] in which randomization over the set of constraints is considered. In particular, we treat the parameter $(x,a)$ as an uncertainty parameter living in the space $\\mathcal{X}\\times\\mathcal{A}$ . Let $\\mathbb{P}$ be a Borel probability measure on $(\\mathcal{X}\\times\\mathcal{A},\\mathcal{B}(\\mathcal{X}\\times\\bar{\\mathcal{A}}))$ , where $\\mathcal{X}\\times\\mathcal{A}$ is equipped with the norm $\\|(x,a)\\|=\\left\\|x\\right\\|_{2}+\\left\\|{\\dot{a}}\\right\\|_{2}$ . We assume that $\\mathbb{P}$ has the following structure. ", "page_idx": 7}, {"type": "text", "text": "Assumption 4.1 (Sampling distribution). There exists $g:\\mathbb{R}_{+}\\rightarrow[0,1]$ strictly increasing, such that $\\mathbb{P}(B_{r}(\\bar{x},a))>g(r),$ , for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ and $r>0$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 4.1 is a sufficient structural assumption concerning the sample distribution $\\mathbb{P}$ ensuring that the gap between the robust program (IP) and its sampled counterpart $\\mathrm{\\DeltaSIP_{N}})$ ) can be controlled. It implicitly restricts the state and action spaces to be bounded. ", "page_idx": 7}, {"type": "text", "text": "Let $\\{(\\boldsymbol{x}^{(\\ell)},\\boldsymbol{a}^{(\\ell)})\\}_{\\ell=1}^{N}$ be independent and identically distributed (i.i.d.) samples drawn from $\\mathcal{X}\\times\\mathcal{A}$ according to $\\mathbb{P}$ . We are interested in the following random finite-dimensional convex program: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l l}{\\operatorname*{inf}_{\\alpha,\\beta,\\varepsilon}}&{\\varepsilon}\\\\ {\\mathrm{s.t.}}&{\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u\\right\\rangle\\leq\\varepsilon,}\\\\ &{c(x^{(\\ell)},a^{(\\ell)})-T_{\\gamma}^{*}u(x^{(\\ell)},a^{(\\ell)})\\geq-\\varepsilon,\\;\\forall\\ell=1,\\ldots N,}\\\\ &{\\int_{\\mathcal{X}\\times A}(c(x,a)-T_{\\gamma}^{*}u(x,a))\\,\\mathrm{d}(x,a)=1,}\\\\ &{c\\in\\mathbf{C}_{n_{c}},u\\in\\mathbf{U}_{n_{u}},\\varepsilon\\geq0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notice that $(\\mathrm{SIP_{N}})$ naturally represents a randomized program as it depends on the random multisample $\\{(\\boldsymbol{x}^{(i)},\\boldsymbol{a}^{(i)})\\}_{i=1}^{N}$ . We assume the following measurability assumption holds for our analysis. ", "page_idx": 8}, {"type": "text", "text": "Assumption 4.2. The $(\\mathrm{SIP_{N}},$ ) optimizer generates a Borel measurable mapping that associates each multi-sample $\\{(\\boldsymbol{x}^{(\\ell)},\\boldsymbol{a}^{(\\ell)})\\}_{\\ell=1}^{N}$ to a uniquely defined optimizer $(\\tilde{\\alpha}_{N},\\tilde{\\beta}_{N},\\tilde{\\varepsilon}_{N})$ . ", "page_idx": 8}, {"type": "text", "text": "To ensure uniqueness when multiple solutions exist, use a tie-break rule, such as selecting the solution with the minimum $\\lVert\\cdot\\rVert_{2}$ norm. It has been shown [30, Proposition 3.10] that applying such a tie-break-rule also ensures the measurability in Assumption 4.2. ", "page_idx": 8}, {"type": "text", "text": "The appealing feature of $(S\\mathrm{IP_{N}})$ is that it is a convex finite-dimensional program and so it can be solved at low computational cost for small enough $N$ . We study how many samples are needed for a good solution by examining the generalization properties of the optimizer $(\\tilde{\\alpha}_{N},\\tilde{\\beta}_{N},\\tilde{\\varepsilon}_{N})$ and the extracted cost function $\\begin{array}{r}{\\tilde{c}N=\\sum_{i=1}^{n_{c}^{\\setminus}}\\tilde{\\alpha}N_{i}c_{i}}\\end{array}$ . For each $n\\in\\mathbb N$ , $\\epsilon\\in(0,1)$ and $\\delta\\in(0,1)$ , we define ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\bf N}(n,\\epsilon,\\delta)=\\operatorname*{min}\\Big\\{N\\in\\mathbb{N}:\\sum_{i=0}^{n}\\binom{N}{i}\\epsilon^{i}(1-\\epsilon)^{N-i}\\leq\\delta\\Big\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The sample size above asymptotically scales as $\\sim\\{1/\\epsilon,~\\mathrm{log}(1/\\delta),~n\\}$ , see [29]. The following theorem determines the sample complexity of $\\boldsymbol{(\\mathrm{SIP_{N}})}$ ). In particular, for a given reliability parameter $\\epsilon\\in(0,1)$ and confidence level $\\delta\\in(0,1)$ , we establish how many samples are needed to guarantee with confidence at least $1-\\delta$ that $\\tilde{c}_{N}\\in\\mathcal{C}^{\\varepsilon_{\\mathrm{approx}}+\\epsilon}(\\pi_{\\mathrm{E}})$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1 (Scenario program guarantees). Let $\\epsilon,\\delta\\in(0,1)$ . Under Assumptions 2.1, 4.1 and 4.2, if $\\cdot_{u_{1}}\\equiv1$ and $\\begin{array}{r}{\\theta>\\frac{1}{(1-\\gamma)\\mathrm{leb}(\\mathcal{X}\\times\\mathcal{A})}}\\end{array}$ , then by sampling $\\begin{array}{r}{N\\ge\\mathrm{N}(n_{c}+n_{u}+1,g(\\frac{\\epsilon}{L_{\\Lambda}}),\\delta)}\\end{array}$ constraints, where $L_{\\Lambda}\\triangleq\\theta\\sqrt{n_{c}}L_{c}+\\theta\\sqrt{n_{u}}\\big(L_{u}L_{\\mathsf{P}}+L_{u}\\big),$ , with probability at least $1-\\delta$ , $\\tilde{c}_{N}\\in\\mathcal{C}^{\\varepsilon_{\\mathrm{approx}}+\\epsilon}(\\pi_{\\mathrm{E}})$ . ", "page_idx": 8}, {"type": "text", "text": "Remark 4.1 (Curse of dimensionality). As shown in [30], the function $g(r)$ is of order $r^{\\dim(\\mathcal{X}\\times\\mathcal{A})}$ . As a result, the number of samples grows exponentially as $\\epsilon^{-\\mathrm{dim}\\left({\\mathcal X}\\times{\\mathcal A}\\right)}$ . A similar exponential dependence to the dimension of the state space has been established in [51]. Considering the current performance of general LP solvers, this approach is attractive for small to medium-sized problems. As noted in [51], dealing with the general $d$ -dimensional case without exponential scaling in $d$ is challenging. Therefore, understanding the selection of a suitable distribution for future sample drawing is crucial. ", "page_idx": 8}, {"type": "text", "text": "Remark 4.2 ( $l_{1}$ -regularization). To cut down on required samples $N$ , a common method is using $l_{1}$ -regularization to reduce the effective dimension of the optimization variable. This concept is formalized in the current setting [67]. Moreover, in the spirit of the compressed sensing literature [68], $l_{1}$ -regularization will promote sparse solutions and hence lead to \u201csimple\" cost functions. In the context of optimal control $l_{1}$ -regularization term is studied as the so-called \u201cmaximum hands-off control\" paradigm [69, 70]. In our case, the utilization of the $l_{1}$ -norm offers two primary advantages. Firstly, the $l_{1}$ -norm promotes sparsity in solutions, thereby potentially reducing computational demands. Secondly, this specific type of regularization preserves the linearity of the program. ", "page_idx": 8}, {"type": "text", "text": "4.3 Sample-based inverse reinforcement learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we explore the realistic scenario where we lack access to the expert policy $\\pi_{\\mathrm{E}}$ and the transition law P. The learner only receives a finite set of truncated expert sample trajectories and cannot interact or query the expert for additional data during training. Despite the unknown MDP model, we assume access to a generative-model oracle. It provides the next state $x^{\\prime}$ given a state-action pair $(x,a)$ sampled from $\\mathsf{P}(\\cdot|x,a)$ . This is known as the simulator-defined MDP [71, 72]. ", "page_idx": 8}, {"type": "text", "text": "Sampling process. Let $\\boldsymbol{\\tau}=\\{\\tau_{i}=(x_{0}^{i},a_{0}^{i},\\dots,x_{H}^{i},a_{H}^{i})\\}_{i=1}^{m}$ be i.i.d., truncated sample trajectories according to $\\pi_{\\mathrm{E}}$ . For any $c\\in\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ , we consider the sample average approximation of the expectation $\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\right\\rangle$ given by, $\\begin{array}{r}{\\widehat{\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\right\\rangle}\\left(\\tau\\right)\\triangleq\\frac{1}{m}\\sum_{t=0}^{H}\\sum_{j=1}^{m}\\gamma^{t}c(x_{t}^{j},a_{t}^{j})}\\end{array}$ . Similarly, if $\\xi=\\{x_{0}^{k}\\}_{k=1}^{n}$ are i.i.d., samples according to $\\nu_{0}$ , for any $u\\,\\in\\,{\\mathrm{Lip}}F({\\mathcal{X}})$ , we define the corresponding sample average estimation of the expectation $\\langle\\nu_{0},u\\rangle$ by $\\begin{array}{r}{\\widehat{\\langle\\nu_{0},u\\rangle}(\\xi)\\triangleq\\frac{1}{n}\\sum_{k=1}^{n}u(x_{0}^{k})}\\end{array}$ . Moreover, let $\\zeta=$ $\\{(\\boldsymbol{x}^{(l)},\\boldsymbol{a}^{(l)})\\}_{l=1}^{N}$ be i.i.d. samples drawn from $\\mathcal{X}\\times\\mathcal{A}$ according to $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{X}}\\times{\\mathcal{A}})$ . We also use the following notation $\\begin{array}{r}{\\widehat{T}_{\\gamma}^{*}u(x^{(l)},a^{(l)},y^{(l)})\\triangleq u(x^{(l)})-\\frac{1}{k}\\sum_{i=1}^{k}u(y_{i}^{(l)})}\\end{array}$ , $\\{y_{i}^{(l)}\\}_{i=}^{k}$ 1 i.i\u223c.d. $\\mathsf{P}(\\cdot|x^{(l)},a^{(l)})$ . ", "page_idx": 8}, {"type": "text", "text": "We are interested in the finite-sample analysis of the following random convex program: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{\\alpha,\\beta,\\varepsilon}}&{\\varepsilon}\\\\ {\\mathrm{s.t.}\\quad}&{\\langle\\widehat{\\mu_{\\nu_{0}}^{\\pi_{\\ell}},c}\\rangle(\\tau)-\\widehat{\\langle\\nu_{0},u\\rangle}(\\xi)\\leq\\varepsilon,}\\\\ &{c(x^{(l)},a^{(l)})-\\widehat{T_{\\gamma}^{*}}u(x^{(l)},a^{(l)},y^{(l)})\\geq-\\varepsilon,\\ \\forall l=1,\\ldots N,}\\\\ &{\\alpha\\in\\Delta_{[n_{u}]},\\beta\\in\\Delta_{[n_{c}]},}\\\\ &{c\\in\\mathbf{C}_{n_{c}},u\\in\\mathbf{U}_{n_{u}},\\varepsilon\\geq0,}\\end{array}\\right.\\qquad\\mathrm{(SIP_{N,m,n,k})}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the function classes ${\\bf C}_{n_{c}}$ , ${\\bf U}_{n_{u}}$ are defined as in the previous Section. We make the following measurability assumption which is analogous to Assumption 4.2. ", "page_idx": 9}, {"type": "text", "text": "Assumption 4.3. The $^\\mathrm{(SIP_{N,m,n,k})}$ optimizer generates a Borel measurable mapping that associates each multi-sample $(y,\\tau,\\xi,\\zeta)$ to a uniquely defined optimizer $\\begin{array}{r}{\\big(\\tilde{\\alpha}_{N,m,n,k},\\tilde{\\beta}_{N,m,n,k},\\tilde{\\varepsilon}_{N,m,n,k}\\big).}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "Theorem 4.2. Under Assumptions 2.1, 4.1 and 4.3, if $u_{1}\\equiv1$ and (1\u2212\u03b3)le1b(X\u00d7A), then for $\\begin{array}{r}{N\\ge\\mathrm{N}(n_{c}+n_{u}+1,g(\\frac{\\epsilon}{L_{\\Lambda}}),\\delta/2)}\\end{array}$ constraints, $n\\,\\ge\\,\\frac{8K_{u,\\infty}^{2}\\theta^{2}n_{u}\\ln\\left(\\frac{8n_{u}}{\\delta}\\right)}{\\epsilon^{2}}$ , $\\begin{array}{r}{m\\,\\ge\\,\\frac{8K_{c,\\infty}^{2}\\theta^{2}n_{c}\\ln\\big(\\frac{8n_{c}}{\\delta}\\big)}{(1-\\gamma)^{2}\\epsilon^{2}}}\\end{array}$ expert samples with horizon $\\begin{array}{r}{H=\\frac{1}{1-\\gamma}\\log(\\frac{2}{\\varepsilon})}\\end{array}$ , and $\\begin{array}{r}{k\\ge\\frac{8C n_{u}\\theta^{2}\\log(4n_{u}N/\\gamma)}{\\epsilon^{2}}}\\end{array}$ calls to the generative model per constraint, with probability at least $1-\\delta$ , $\\tilde{c}_{N,m,n,k}\\in\\mathcal{C}^{\\varepsilon_{\\mathrm{approx}}+\\epsilon}(\\pi_{\\mathrm{E}})$ . The constants $K_{c,\\infty}$ and $K_{u,\\infty}$ are given as $K_{c,\\infty}\\triangleq\\operatorname*{max}_{i=1,\\dots,n_{c}}\\left\\|c_{i}\\right\\|_{\\infty}$ and $\\begin{array}{r}{K_{u,\\infty}\\triangleq\\operatorname*{max}_{j=1,\\dots,n_{u}}\\left\\|u_{i}\\right\\|_{\\infty}\\!.}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "Theorem 4.2 provides explicit sample complexity bounds for achieving a desired approximation accuracy with our proposed randomized algorithm. For continous MDPs when we use $n_{u}$ basis functions for the value function and $n_{c}$ basis functions for the cost function we need $\\begin{array}{r}{m=\\mathcal{O}\\left(\\frac{n_{c}\\log\\left(\\frac{n_{c}}{\\delta}\\right)}{(1-\\gamma)^{2}\\varepsilon^{2}}\\right)}\\end{array}$ expert samples and $\\begin{array}{r}{K\\ =\\ \\mathcal{O}\\left(\\frac{n_{u}\\log\\left(\\frac{n_{u}N}{\\delta}\\right)}{\\varepsilon^{2}}\\right)}\\end{array}$ calls to the generative model per constraint and $N={\\mathcal{O}}(\\exp^{d i m(X\\times A)})$ sampled constraints and solve the resulted sampled finite LP with $n_{u}+n_{c}$ variables and $N$ constraints in polynomial time to learn a cost that is $\\varepsilon+\\varepsilon_{\\mathrm{approx}}$ -inverse feasible with probability $1-\\delta$ . ", "page_idx": 9}, {"type": "text", "text": "The corresponding sample complexities include the expert sample complexity $m$ , the number of calls to the generator per constraint $k$ , and the number of sampled constraints $N$ . The first two complexities scale gracefully with respect to the problem parameters, whereas the number of sampled constraints scales exponentially with the dimension of the state and action spaces (see also Remark 4.1). This makes the algorithm particularly suitable for low-dimensional problems of practical interest, e.g., pendulum swing-up control, vehicle cruise control, and quadcopter stabilization. Note that a similar exponential dependence to the dimension of the state space has been established in Dexter et al. [51], a theoretical study addressing IRL in continuous state but discrete action spaces. ", "page_idx": 9}, {"type": "text", "text": "A promising research direction is to enhance the sample complexity bounds through the utilization of the underlying problem structure [73]. In addition, it becomes imperative to gain an understanding regarding the selection of a suitable distribution for drawing samples in the future. Intuitively, it is reasonable to anticipate that certain regions within the state-action space carry more \"informative\" characteristics than others. One conjecture is that sampling constraints based on the expert occupancy measure could yield a more scalable bound [74]. However, a comprehensive mathematical treatment of these inquiries will be addressed in future research endeavors. ", "page_idx": 9}, {"type": "text", "text": "Redunction to tabular MDPs. For tabular MDPs, offline access to expert for tabular MDPs, and a generative model we require $\\begin{array}{r}{m=\\mathcal{O}\\left(\\frac{|X||A|(\\log(\\frac{|X||A|}{\\delta})}{(1-\\gamma)^{2}\\varepsilon^{2}}\\right)}\\end{array}$ expert samples, and $K|X||A|=$ $\\mathcal{O}\\left(\\frac{X|^{2}|A|\\log(\\frac{|X|^{2}|A|}{\\delta})}{\\varepsilon^{2}}\\right)$ calls to the generative model, and solve the resulted sampled finite LP with $\\left|X A\\right|+\\left|X\\right|$ variables and $|X||A|+2$ constraints in polynomial time, to learn a cost function that is $\\varepsilon$ -inverse feasible with probability $1-\\delta$ . ", "page_idx": 9}, {"type": "text", "text": "Note that as we argued in detail above our setting is different from the one in [52-54] since we have offilne access to the expert and address a different question, i.e. learning a single reward with formal guarantees in continuous MDPs. In this case, there is no need to solve the resulting linear program. ", "page_idx": 9}, {"type": "text", "text": "Numerical Experiment. In Appendix C, we illustrate our method with a simple truncated Linear Quadratic Regulator (LQR) example to provide better intuition about the method and the proposed sample complexity bounds. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the DFG in the Cluster of Excellence EXC 2117 \u201cCentre for the Advanced Study of Collective Behaviour\u201d (Project-ID 390829875). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] D. P. Bertsekas. Dynamic programming and suboptimal control: a survey from ADP to MPC. European Journal of Control, 11(4):310\u2013334, 2005. (Cited on page 1.)   \n[2] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, second edition, 2018. (Cited on page 1.)   \n[3] Dimitri Bertsekas. Rollout, policy iteration, and distributed reinforcement learning. Athena Scientific, 2021. (Cited on page 1.)   \n[4] Sean Meyn. Control Systems and Reinforcement Learning. Cambridge University Press, 2022. (Cited on page 1.)   \n[5] D. A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1):88\u201397, 1991. (Cited on page 1.)   \n[6] Stuart Russell. Learning agents for uncertain environments (extended abstract). In Annual Conference on Computational Learning Theory (COLT), 1998. (Cited on page 1.)   \n[7] J. Andrew Bagnell. An invitation to imitation. Technical Report CMU-RI-TR-15-08, Carnegie Mellon University, Pittsburgh, PA, 2015. (Cited on page 1.)   \n[8] W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis)design for autonomous driving. arXiv:2104.13906, 2021. (Cited on page 1.)   \n[9] T Osa, J Pajarinen, G Neumann, JA Bagnell, P Abbeel, and J Peters. An algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 2018. (Cited on page 1.)   \n[10] Arthur Charpentier, Romuald Elie, and Carl Remlinger. Reinforcement learning in economics and finance. arXiv:20031004, 2020. (Cited on page 1.)   \n[11] Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Human behavior modeling with maximum entropy inverse optimal control. In AAAI Spring Symposium: Human Behavior Modeling, 2009. (Cited on page 1.)   \n[12] A. Y. $\\mathrm{Ng}$ and S. J. Russell. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning (ICML), 2000. (Cited on pages 1, 3, 5, 6, and 17.)   \n[13] Darse Billings, Denis Papp, Jonathan Schaeffer, and Duane Szafron. Opponent modeling in poker. In AAAI/IAAI, 1998. (Cited on page 1.)   \n[14] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning (ICML), 2004. (Cited on page 1.)   \n[15] Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In Proceedings of the 20th International Conference on Neural Information Processing Systems, NIPS\u201907, pages 1449\u20131456, USA, 2007. Curran Associates Inc. (Cited on pages 1 and 2.)   \n[16] P. Abbeel, D. Dolgov, A. Y. Ng, and S. Thrun. Apprenticeship learning for motion planning with application to parking lot navigation. In 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1083\u20131090, Sept 2008. (Cited on page 1.)   \n[17] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proc. AAAI, pages 1433\u20131438, 2008. (Cited on pages 1 and 2.)   \n[18] J.P. Laumond, N. Mansard, and J.B. Lasserre. Geometric and Numerical Foundations of Movements. Springer Tracts in Advanced Robotics. Springer International Publishing, 2017. (Cited on page 1.)   \n[19] S. Levine, Z. Popovic\u00b4, and V. Koltun. Feature construction for inverse reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2010. (Cited on page 2.)   \n[20] J. Ho, J. K. Gupta, and S. Ermon. Model-free imitation learning with policy optimization. In International Conference on Machine Learning (ICML), 2016. (Cited on page 2.)   \n[21] J. Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2016. (Cited on page 2.)   \n[22] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforcement learning. In International Conference on Learning Representations (ICLR), 2018. (Cited on page 2.)   \n[23] Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srinivasa. Imitation learning as f-divergence minimization. In International Workshop on the Algorithmic Foundations of Robotics (WAFR), 2020. (Cited on page 2.)   \n[24] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching. In International Conference on Learning Representations (ICLR), 2020. (Cited on page 2.)   \n[25] Paul Barde, Julien Roy, Wonseok Jeon, J. Pineau, C. Pal, and D. Nowrouzezahrai. Adversarial soft advantage fitting: Imitation learning without policy optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2020. (Cited on page 2.)   \n[26] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. IQlearn: Inverse soft-Q learning for imitation. In Advances in Neural Information Processing Systems (NeuRIPS), 2021. (Cited on page 2.)   \n[27] Luca Viano, Angeliki Kamoutsi, Gergely Neu, Igor Krawczuk, and Volkan Cevher. Proximal point imitation learning. In Proceedings of the Neural Information Processing Systems (NeurIPS), 2022. (Cited on page 2.)   \n[28] O. Hern\u00e1ndez-Lerma and J. B. Lasserre. Discrete-Time Markov Control Processes: Basic Optimality Criteria. Springer-Verlag New York, 1996. (Cited on pages 2 and 16.)   \n[29] M. Campi and S. Garatti. The exact feasibility of randomized solutions of uncertain convex programs. SIAM Journal on Optimization, 19(3):1211\u20131230, 2008. (Cited on pages 2, 8, 9, and 23.)   \n[30] P. Mohajerin Esfahani, T. Sutter, and J. Lygeros. Performance bounds for the scenario approach and an extension to a class of non-convex programs. IEEE Transactions on Automatic Control, 2014. (Cited on pages 2, 9, 23, and 24.)   \n[31] P. Mohajerin Esfahani, T. Sutter, D. Kuhn, and J. Lygeros. From infinite to finite programs: Explicit error bounds with applications to approximate dynamic programming. SIAM Journal on Optimization, 28(3):1968\u20131998, 2018. (Cited on pages 2, 21, 22, and 26.)   \n[32] Olivier Bousquet, St\u00e9phane Boucheron, and G\u00e1bor Lugosi. Introduction to Statistical Learning Theory, pages 169\u2013207. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. (Cited on page 2.)   \n[33] Giuseppe Calafiore and Marco C. Campi. Uncertain convex programs: Randomized solutions and confidence levels. Mathematical Programming, 102:25\u201346, 2005. (Cited on page 2.)   \n[34] Andrew Y. $\\mathrm{Ng}$ and Stuart Russell. Algorithms for inverse reinforcement learning. In in Proc. 17th International Conf. on Machine Learning, pages 663\u2013670. Morgan Kaufmann, 2000. (Cited on page 2.)   \n[35] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, pages 1\u2013, New York, NY, USA, 2004. ACM. (Cited on page 2.)   \n[36] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In Proceedings of the 23rd International Conference on Machine Learning, ICML \u201906, pages 729\u2013736, New York, NY, USA, 2006. ACM. (Cited on page 2.)   \n[37] Gergely Neu and Csaba Szepesv\u00e1ri. Apprenticeship learning using inverse reinforcement learning and gradient methods. In UAI, 2007. (Cited on page 2.)   \n[38] Sergey Levine, Zoran Popovic, and Vladlen Koltun. Feature construction for inverse reinforcement learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1342\u20131350. Curran Associates, Inc., 2010. (Cited on page 2.)   \n[39] Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse reinforcement learning with gaussian processes. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 19\u201327. Curran Associates, Inc., 2011. (Cited on page 2.)   \n[40] Nathan Ratliff, David Bradley, J. Andrew (Drew) Bagnell, and Joel Chestnutt. Boosting structured prediction for imitation learning. In B. Sch\"olkopf, J.C. Platt, and T. Hofmann\", editors, Advances in Neural Information Processing Systems 19, Cambridge, MA, January 2007. MIT Press. (Cited on page 2.)   \n[41] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907, pages 2586\u20132591, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc. (Cited on page 2.)   \n[42] Krishnamurthy Dvijotham and Emanuel Todorov. Inverse optimal control with linearly-solvable mdps. In ICML, pages 335\u2013342. Omnipress, 2010. (Cited on page 2.)   \n[43] Sergey Levine and Vladlen Koltun. Continuous inverse optimal control with locally optimal examples. In Proceedings of the 29th International Conference on Machine Learning, pages 41\u201348, 2012. (Cited on page 2.)   \n[44] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on International Conference on Machine Learning (ICML), 2016. (Cited on page 2.)   \n[45] Siddharth Reddy, Anca D. Dragan, and Sergey Levine. {SQIL}: Imitation learning via reinforcement learning with sparse rewards. In International Conference on Learning Representations (ICLR), 2020. (Cited on page 2.)   \n[46] Ravindra K. Ahuja and James B. Orlin. Inverse optimization. Operations Research, 49(5):771\u2013 783, 2001. (Cited on pages 2 and 5.)   \n[47] Garud Iyengar and Wanmo Kang. Inverse conic programming with applications. Operations Research Letters, 33(3):319 \u2013 330, 2005. (Cited on pages 2 and 5.)   \n[48] Edouard Pauwels, Didier Henrion, and Jean-Bernard Lasserre. Linear conic optimization for inverse optimal control. SIAM Journal on Control and Optimization, 54(3):1798\u20131825, 2016. (Cited on pages 3, 5, 7, and 20.)   \n[49] A. Komanduru and J. Honorio. On the correctness and sample complexity of inverse reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. (Cited on pages 3, 6, and 17.)   \n[50] Abi Komanduru and Jean Honorio. A lower bound for the sample complexity of inverse reinforcement learning. In International Conference on Machine Learning (ICML), 2021. (Cited on pages 3 and 17.)   \n[51] Gregory Dexter, Kevin Bello, and Jean Honorio. Inverse reinforcement learning in a continuous state space with formal guarantees. In Advances in Neural Information Processing Systems (NeurIPS), 2021. (Cited on pages 3, 4, 6, 9, 10, and 17.)   \n[52] Alberto Maria Metelli, Giorgia Ramponi, Alessandro Concetti, and Marcello Restelli. Provably efficient learning of transferable rewards. In International Conference on Machine Learning (ICML), 2021. (Cited on pages 3, 5, 6, 7, and 17.)   \n[53] David Lindner, Andreas Krause, and Giorgia Ramponi. Active exploration for inverse reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. (Cited on pages 3, 6, 7, and 17.)   \n[54] Alberto Maria Metelli, Filippo Lazzati, and Marcello Restelli. Towards theoretical understanding of inverse reinforcement learning. arXiv:2304.12966, 2023. (Cited on pages 3, 6, 7, and 17.)   \n[55] Filippo Lazzati, Mirco Mutti, and Alberto Maria Metelli. Offline inverse RL: New solution concepts and provably efficient algorithms. In International Conference on Machine Learning (ICML), 2024. (Cited on page 3.)   \n[56] Filippo Lazzati, Mirco Mutti, and Alberto Maria Metelli. How does inverse RL scale to large state spaces? A provably efficient approach, 2024. (Cited on page 3.)   \n[57] D. P. Bertsekas and S. E. Shreve. Stochastic Optimal Control: The Discrete-Time Case. Academic Press, 1978. (Cited on page 4.)   \n[58] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., USA, 1st edition, 1994. (Cited on pages 4 and 17.)   \n[59] On\u00e9simo Hern\u00e1ndez-Lerma and Jean Bernard Lasserre. Discrete-time Markov control processes. Springer-Verlag, New York, 1996. (Cited on page 4.)   \n[60] F. Dufour and T. Prieto-Rumeau. Finite linear programming approximations of constrained discounted markov decision processes. SIAM Journal on Optimization, 51(2):1298\u20131324, 2013. (Cited on pages 4 and 16.)   \n[61] O. Hernandez-Lerma. Adaptive Markov Control Processes. Springer-Verlag, Berlin, Heidelberg, 2001. (Cited on pages 4 and 16.)   \n[62] O. Hern\u00e1ndez-Lerma and J.B. Lasserre. Discrete-Time Markov Control Processes: Basic Optimality Criteria. Applications of Mathematics Series. Springer, 1996. (Cited on pages 4, 16, and 17.)   \n[63] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Potential-based shaping and linearlysolvable markov decision problems. Journal of Artificial Intelligence Research, 11:131\u2013167, 1999. (Cited on page 7.)   \n[64] Haoyang Cao, Samuel Cohen, and Lukasz Szpruch. Identifiability in inverse reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2021. (Cited on page 7.)   \n[65] Paul Rolland, Luca Viano, Norman Sch\u00fcrhoff, Boris Nikolov, and Volkan Cevher. Identifiability and generalizability from multiple experts in inverse reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. (Cited on page 7.)   \n[66] Marco C. Campi, Simone Garatti, and Maria Prandini. The scenario approach for systems and control design. Annual Reviews in Control, 33(2):149 \u2013 157, 2009. (Cited on page 8.)   \n[67] M. Campi and A. Car\u00e8. Random convex programs with $\\mathbb{S}1_{-}1\\mathbb{S}$ -regularization: Sparsity and generalization. SIAM Journal on Control and Optimization, 51(5):3532\u20133557, 2013. (Cited on page 9.)   \n[68] David L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52:1289\u20131306, 2006. (Cited on page 9.)   \n[69] Masaaki Nagahara, Daniel E. Quevedo, and Dragan Ne\u0161i\u00b4c. Maximum hands-off control: A paradigm of control effort minimization. IEEE Transactions on Automatic Control, 61(3):735\u2013 747, 2016. (Cited on page 9.)   \n[70] Debasish Chatterjee, Masaaki Nagahara, Daniel E. Quevedo, and K.S. Mallikarjuna Rao. Characterization of maximum hands-off control. Systems & Control Letters, 94:31\u201336, 2016. (Cited on page 9.) [71] B. Sz\u00f6r\u00e9nyi, G. Kedenburg, and R. Munos. Optimistic planning in Markov decision processes using a generative model. In Advances in Neural Information Processing Systems (NeurIPS),   \n2014. (Cited on page 9.) [72] M. A. Taleghan, T. G. Dietterich, M. Crowley, K. Hall, and H. J. Albers. PAC optimal MDP planning with application to invasive species management. Journal of Machine Learning Research, 16(117):3877\u20133903, 2015. (Cited on page 9.) [73] Iaojing Zhang, Sergio Grammatico, Georg Schildbach, Paul Goulart, and John Lygeros. On the sample size of random convex programs with structured dependence on the uncertainty. Automatica, 60:1054\u20131056, 2015. (Cited on page 10.) [74] Daniela P. De Farias and Benjamin Van Roy. On constraint sampling in the linear programming approach to approximate dynamic programming. Mathematics of Operations Research,   \n29(3):462\u2013478, 2004. (Cited on page 10.) [75] M. Sion. On general minimax theorems. Pacific Journal of Mathematics, 8(1):171\u2013176, 1958. (Cited on page 22.) [76] T. Sutter, A. Kamoutsi, P. E. Esfahani, and J. Lygeros. Data-driven approximate dynamic programming: A linear programming approach. In IEEE Conference on Decision and Control (CDC), 2017. (Cited on page 25.) [77] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control, Vol. II. Athena Scientific,   \n4th edition, 2012. (Cited on page 26.) [78] Marco C. Campi, Simone Garatti, and Maria Prandini. The scenario approach for systems and control design. IFAC Proceedings Volumes, 41(2):381\u2013389, 2008. 17th IFAC World Congress. (Cited on page 27.) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Supplementary material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 The linear programming approach for continuous MDPs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we present essential facts and derivations concerning the Linear Programming (LP) approach to continuous Markov Decision Processes (MDPs). These insights and results will serve as valuable foundations for the subsequent discussions and analysis. ", "page_idx": 15}, {"type": "text", "text": "The following theorem summarizes the properties of the optimal value function under the assumption that the control model is Lipschitz continuous. ", "page_idx": 15}, {"type": "text", "text": "Theorem A.1 ([60, Theorem 3.1], [61]). Under Assumption 2.1, the following hold: ", "page_idx": 15}, {"type": "text", "text": "1. The value function $V_{c}^{\\star}$ is in $\\operatorname{Lip}({\\mathcal{X}})$ with Lipschitz constant $\\begin{array}{r}{L_{V_{c}^{\\star}}\\leq L_{c}+\\frac{\\gamma}{1-\\gamma}\\|c\\|_{\\infty}L_{\\mathsf{P}}}\\end{array}$ ; ", "page_idx": 15}, {"type": "text", "text": "2. The value function $V_{c}^{\\star}$ satisfies the Bellman optimality equation ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{c}^{\\star}(x)=\\operatorname*{min}_{a\\in A}\\{c(x,a)+\\gamma\\int_{\\mathcal{X}}V_{c}^{\\star}(y)Q(d y|x,a)\\},\\quad f o r\\,a l l\\;x\\in\\mathcal{X};\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "3. There exists a $\\gamma$ -discount $\\nu_{0}$ -optimal policy which is stationary deterministic. ", "page_idx": 15}, {"type": "text", "text": "Next, we characterize the set of occupation measures in terms of linear constraint satisfaction. ", "page_idx": 15}, {"type": "text", "text": "Theorem A.2 ([62, Theorem 6.3.7]). Consider the convex set of measures, $\\mathfrak{F}\\triangleq\\{\\mu\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}:$ : $T_{\\gamma}\\mu=\\nu_{0}\\}$ , where $T_{\\gamma}:\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})\\rightarrow\\mathcal{M}(\\mathcal{X})$ is a linear and weakly continuous operator given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n(T_{\\gamma}\\mu)(B)\\triangleq\\mu(B\\times A)-\\gamma\\int_{X\\times A}\\mathsf{P}(B|x,a)\\mu(\\,\\mathrm{d}(x,a)),\\quad f o r\\,a l l\\ B\\in B(\\mathcal{X}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, $\\mathfrak{F}=\\{\\mu_{\\nu_{0}}^{\\pi}:\\;\\pi\\in\\Pi_{0}\\}.$ . Moreover, $\\left\\langle\\mu_{\\nu_{0}}^{\\pi},c\\right\\rangle=V_{c}^{\\pi}(\\nu_{0}),$ , for every $\\pi$ ", "page_idx": 15}, {"type": "text", "text": "A direct consequence of Theorem A.2 is that ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{c}^{\\star}(\\nu_{0})=\\operatorname*{min}_{\\pi}\\left\\langle\\mu_{\\nu_{0}}^{\\pi},c\\right\\rangle=\\operatorname*{min}_{\\pi\\in\\Pi_{0}}\\left\\langle\\mu_{\\nu_{0}}^{\\pi},c\\right\\rangle=\\operatorname*{inf}_{\\mu\\in\\mathfrak{F}}\\left\\langle\\mu,c\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore the MDP problem $\\mathrm{(MDP_{c})}$ ) can be stated equivalently as an infinite-dimensional LP over measures ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{T}_{c}(\\nu_{0})\\triangleq\\operatorname*{inf}_{\\mu\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}}\\{\\langle\\mu,c\\rangle:\\;T_{\\gamma}\\mu=\\nu_{0}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular the infimum in $(\\mathrm{P_{c}})$ is attained and $\\pi^{\\star}$ is optimal for the OCP $\\mathrm{MDP_{c}}$ ) if and only if $\\mu_{\\nu_{0}}^{\\pi^{\\star}}$ is optimal for the primal LP $\\left(\\mathrm{P_{c}}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Consider the dual pair of vector spaces $(\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A}),\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A}))$ and $(\\mathcal{M}(\\mathcal{X}),\\operatorname{Lip}(\\mathcal{X}))$ . Then the adjoint linear operator $T_{\\gamma}^{*}:\\mathrm{Lip}(\\mathcal{X})\\rightarrow\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})$ of $T_{\\gamma}$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n(T_{\\gamma}^{*}u)(x,a)\\triangleq u(x)-\\gamma\\int_{\\mathcal{X}}u(y)\\mathsf{P}(\\mathrm{d}y|x,a).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Indeed, $T_{\\gamma}^{*}$ is well-defined by Assumption (A2). Moreover, a direct computation shows that (see [28, Pg. 139]) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\langle\\mu,T_{\\gamma}^{*}u\\right\\rangle=\\left\\langle T_{\\gamma}\\mu,u\\right\\rangle,\\quad\\mathrm{for\\,all}\\ \\mu\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A}),\\ u\\in\\mathrm{Lip}(\\mathcal{X}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In addition, the dual convex cone of $\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}$ is the set $\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})_{+}$ of nonnegative bounded and Lipschitz continuous functions on $\\mathcal{X}\\times\\mathcal{A}$ . This is because, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}}}&{{\\triangleq}}&{{\\{v\\in\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})\\mid\\langle\\mu,v\\rangle\\ge0,\\ \\forall\\ \\mu\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}\\}}}\\\\ {{}}&{{=}}&{{\\{v\\in\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})\\mid v\\ge0\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The dual LP of $(\\mathrm{P_{c}})$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{I}_{c}^{*}(\\nu_{0})\\triangleq\\operatorname*{sup}_{u\\in\\mathrm{Lip}(\\mathcal{X})}\\{\\langle\\nu_{0},u\\rangle:\\;c-T_{\\gamma}^{*}u\\geq0\\mathrm{~on~}\\mathcal{X}\\times\\mathcal{A}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Theorem A.3 (Strong duality). Under Assumption 2.1 on the control model $\\mathcal{M}_{c},$ , the dual $L P\\left(\\mathrm{D_{c}}\\right)$ is solvable, i.e., the supremum is attained, and strong duality holds. That is, ${\\mathcal{I}}_{c}(\\nu_{0})={\\mathcal{I}}_{c}^{*}(\\nu_{0})=$ $V_{c}^{\\star}(\\nu_{0})$ . In particular the value function $V_{c}^{\\star}$ is an optimal solution for the dual $L P$ $\\left(\\mathrm{D}_{\\mathbf{c}}\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. By virtue of [62, Theorem 6.3.8] we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{c}^{\\star}(\\nu_{0})=\\mathcal{I}_{c}(\\nu_{0})=\\mathcal{I}_{c}^{\\star}(\\nu_{0})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and moreover the supremum definiting $\\mathcal{I}_{c}^{\\star}(\\nu_{0})$ is attained by the optimal value function $V_{c}^{\\star}:\\mathcal{X}\\rightarrow\\mathbb{R}$ . Therefore, the result follows by 1) of Theorem A.1. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.2 Comparison to the LP formulations for IRL from the literature ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we will demonstrate that our formulation, when applied to finite tabular MDPs and a stationary Markov expert policy $\\pi_{\\mathrm{E}}$ , simplifies to the inverse feasibility set considered in recent studies [52, 53, 54]. The formulation presented in these works extends the LP formulation previously explored in [12, 49, 50, 51], which specifically addressed deterministic expert policies of the form $\\pi_{\\mathrm{E}}(s)\\,\\equiv\\,a_{1}$ . By highlighting this connection, we establish a link between our approach and the existing body of literature on LP formulations for IRL, while also accounting for continuous state and action spaces and more general expert policies. ", "page_idx": 16}, {"type": "text", "text": "Let $\\mathcal{X}$ and $\\boldsymbol{\\mathcal{A}}$ be finite sets with cardinality $|{\\mathcal{X}}|$ and $|{\\mathcal{A}}|$ , respectively. Then, Assumption 2.1 is trivially satisfied and $\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})=\\mathbb{R}^{|\\mathcal{X}||\\mathcal{A}|}$ . ", "page_idx": 16}, {"type": "text", "text": "By Theorem 3.1 we have that a cost function $c\\in\\mathbb{R}^{|\\mathcal{X}||A|}$ is inverse feasible with respect to $\\pi_{\\mathrm{E}}$ , i.e., $c\\in{\\mathcal{C}}(\\pi_{\\mathrm{E}})$ , if and only if there exists $u\\in\\mathbb{R}^{|\\mathcal{X}|}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\sum_{x,a}\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}(x,a)(c-T_{\\gamma}^{*}u)(x,a)=0,}\\\\ {(c-T_{\\gamma}^{*}u)(x,a)\\geq0,\\mathrm{~for~all~}(x,a)\\in\\mathcal{X}\\times\\mathcal{A}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $\\pi_{\\mathrm{E}}~\\in~\\Pi_{0}$ is a stationary Markov policy, then $\\begin{array}{r l r}{\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}(x,a)}&{=}&{\\sum_{a^{\\prime}}\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}(x,a^{\\prime})\\pi_{\\mathrm{E}}(a|x)}\\end{array}$ and $\\textstyle\\sum_{a^{\\prime}}\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}(x,a^{\\prime})>\\,0$ [58, Thm. 6.9.1]. Therefore, for any state-action pair $(x,a)\\,\\in\\,\\mathcal{X}\\,\\times\\,\\mathcal{A}$ , it holds that $\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}(x,a)=0\\Leftrightarrow\\pi(a|x)=0$ . We then get that (3) is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{(c-T_{\\gamma}^{*}u)(x,a)=0,}&{\\mathrm{if}\\;\\;\\pi_{\\mathrm{E}}(a|x)>0}\\\\ {(c-T_{\\gamma}^{*}u)(x,a)\\geq0,}&{\\mathrm{if}\\;\\;\\pi_{\\mathrm{E}}(a|x)=0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So we end up that a cost function $c\\in\\mathbb{R}^{|\\mathcal{X}||\\mathcal{A}|}$ is inverse feasible if and only if there exists $u\\in\\mathbb{R}^{|\\mathcal{X}|}$ and A\u03b3 \u2208R|+X $A_{\\gamma}\\in\\mathbb{R}_{+}^{|\\mathcal{X}||\\mathcal{A}|}$ such that for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nc(x,a)-u(x)+\\gamma\\sum_{y}\\mathsf{P}(y|x,a)u(y)=A_{\\gamma}(x,a)\\mathbb{1}_{\\{\\pi_{\\mathrm{E}}(a|x)=0\\}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So we have recovered [52, Lem. 3.2], which forms the basis for the analysis and algorithms in [52, 53, 54]. ", "page_idx": 16}, {"type": "text", "text": "Next, note that when $\\pi_{\\mathrm{E}}(x)=a_{1}$ , for all $x\\in\\mathscr{X}$ and $c(x,a)=c(x)$ , for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , then (4) is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{c(x)-u(x)=-\\gamma\\sum_{y}\\mathsf{P}(y|x,a_{1})u(y),}&{\\mathrm{for~all~}\\;x\\in\\mathcal{X}}\\\\ {c(x)-u(x)\\ge-\\gamma\\sum_{y}\\mathsf{P}(y|x,a)u(y),}&{\\mathrm{for~all~}\\;x\\in\\mathcal{X},\\;\\;a\\in\\mathcal{A}\\backslash\\{a_{1}\\}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, a cost function $c\\in\\mathbb{R}^{|\\mathcal{X}||\\mathcal{A}|}$ is inverse feasible if and only if there exists $u\\in\\mathbb{R}^{|\\mathcal{X}|}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{c(x)-u(x)=-\\gamma\\sum_{y}\\mathsf{P}(y|x,a_{1})u(y),}&{\\mathrm{for~all}\\;\\;x\\in\\mathcal{X}}\\\\ {\\sum_{y}\\mathsf{P}(y|x,a_{1})u(y)\\le\\sum_{y}\\mathsf{P}(y|x,a)u(y),}&{\\mathrm{for~all}\\;\\;x\\in\\mathcal{X},\\;\\;a\\in\\mathcal{A}\\backslash\\{a_{1}\\}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By introducing the notation $\\mathsf{P}_{a}\\,\\in\\,\\mathbb{R}^{|\\mathcal{X}||\\mathcal{X}|}$ with $\\mathsf{P}_{a}(x,y)\\,=\\,\\mathsf{P}(y|x,a_{1})$ and noting that the first linear system in (6) admits a unique solution $u=(\\mathsf{I}_{|x|}-\\gamma\\mathsf{P}_{a_{1}})^{-1}c$ , we end up that a cost function $c\\in\\mathbb{R}^{|\\mathcal{X}||\\mathcal{A}|}$ is inverse feasible if and only if ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\mathsf{P}_{a_{1}}-\\mathsf{P}_{a})(\\mathsf{I}_{|\\mathcal{X}|}-\\gamma\\mathsf{P}_{a_{1}})^{-1}c\\leq0,\\quad\\mathrm{for~all~}\\,a\\in\\mathcal{A}\\backslash\\{a_{1}\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So we have recovered [12, Thm. 3] which forms the basis for the analysis and algorithms in [12, 49, 50]. ", "page_idx": 16}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. The direction $3)\\Rightarrow1$ ) is a consequence of the strong duality Theorem A.3 and complementary slackness. Indeed, assume that $\\pi_{\\mathrm{E}}$ is optimal for $\\mathrm{(MDP_{c})}$ with cost $c$ . Then, by Theorem A.2 $\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}$ is optimal to $(\\mathrm{P_{c}})$ . By Theorem A.3, the dual $\\left(\\mathbf{D_{c}}\\right)$ ) is solvable and there is no duality gap. Therefore, there exists a $u\\in\\mathrm{Lip}(\\mathcal{X})$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{c-T_{\\gamma}^{*}u}&{\\geq}&{0,\\ \\mathrm{on}\\ \\mathcal{X}\\times\\mathcal{A}\\ \\ \\mathrm{(feasibility)},}\\\\ {\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\right\\rangle}&{=}&{\\left\\langle\\nu_{0},u\\right\\rangle\\ \\ \\ \\ \\ \\mathrm{(strong\\duality)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second equality is equivalent to $\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u\\right\\rangle=0$ , since $T_{\\gamma}\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}=\\nu_{0}$ . This proves the desired implication. ", "page_idx": 17}, {"type": "text", "text": "The implication $1)\\Rightarrow2$ ) is straightforward. ", "page_idx": 17}, {"type": "text", "text": "To show $2)\\Rightarrow3]$ ), let $\\begin{array}{r}{c\\in\\bigcap_{\\varepsilon>0}\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})}\\end{array}$ . Then, for each $n\\,\\in\\,\\mathbb{N}$ , there exists $u_{n}\\,\\in\\,\\mathrm{Lip}(\\mathcal{X})$ such that $\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u_{n}\\right\\rangle\\;\\leq\\;\\frac{1}{n}$ and $c-T_{\\gamma}^{*}u_{n}\\;\\geq\\;-\\,{\\frac{1}{n}}$ , on $\\mathcal{X}\\times\\mathcal{A}$ . Set $\\begin{array}{r}{v_{n}\\,=\\,u_{n}\\,-\\,\\frac{1}{n(1-\\gamma)}}\\end{array}$ . Then, $\\{v_{n}\\}_{n=1}^{\\infty}\\subset\\mathrm{Lip}(\\mathcal{X})$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l l}{\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}v_{n}\\right\\rangle}&{=}&{0,}\\\\ {c-T_{\\gamma}^{*}v_{n}}&{\\geq}&{0,\\ \\mathrm{on}\\ \\mathcal{X}\\times\\mathcal{A},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used that $\\begin{array}{r}{\\left<\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},1\\right>=\\frac{1}{1-\\gamma}}\\end{array}$ and $T_{\\gamma}^{*}1=1-\\gamma$ , on $\\mathcal{X}\\times\\mathcal{A}$ . Equation (8) states that $\\{v_{n}\\}_{n=1}^{\\infty}$ is feasible for the dual $\\left(\\mathbf{D_{c}}\\right)$ . Moreover, $\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}$ is feasible for $(\\mathrm{P_{c}})$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\nu_{0},v_{n}\\rangle\\le\\mathcal{I}_{c}^{*}(\\nu_{0})=\\mathcal{I}_{c}(\\nu_{0})\\le\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By (7) $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\left\\langle\\nu_{0},v_{n}\\right\\rangle\\;=\\;\\operatorname*{lim}_{n\\to\\infty}\\left\\langle T_{\\gamma}\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}},v_{n}\\right\\rangle\\;=\\;\\operatorname*{lim}_{n\\to\\infty}\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}},T_{\\gamma}^{*}v_{n}\\right\\rangle\\;=\\;\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}},c\\right\\rangle\\;.}\\end{array}$ . So, by taking the limits in (9) as $n\\to\\infty$ , we conclude that $\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}$ is optimal for $(\\mathrm{P_{c}})$ . Then, by Theorem A.2 $\\pi_{\\mathrm{E}}$ is optimal to $\\mathrm{(MDP_{c})}$ ) with cost $c$ . ", "page_idx": 17}, {"type": "text", "text": "Hence, we have shown that $\\begin{array}{r}{\\mathcal{C}(\\pi_{\\mathrm{E}})=\\bigcap_{\\varepsilon>0}\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})=\\mathcal{C}^{0}(\\pi_{\\mathrm{E}})}\\end{array}$ . One can check easily that ${\\mathcal{C}}(\\pi_{\\mathrm{E}})$ is a convex cone. To show that ${\\mathcal{C}}(\\pi_{\\mathrm{E}})$ is $\\left\\Vert\\cdot\\right\\Vert_{\\mathrm{L}}$ -closed in $\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ , let $\\{c_{n}\\}_{n=1}^{\\infty}\\subset\\mathcal{C}(\\pi_{\\mathrm{E}})$ , such that ${\\mathrm{lim}}_{n\\rightarrow\\infty}\\left\\|c_{n}-c\\right\\|_{\\mathrm{L}}=0$ , for some $c\\in\\mathrm{Lip}({\\mathcal{X}}\\times{\\mathcal{A}})$ . Let $\\varepsilon>0$ . Then, there exists $n_{0}\\in\\mathbb{N}$ such that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|c_{n_{0}}-c\\right\\|_{\\infty}<\\frac{(1-\\gamma)\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, $c_{n_{0}}\\in\\mathcal{C}^{\\frac{\\varepsilon}{2}}(\\pi_{\\mathrm{E}})$ . Combining this with (10), we deduce that $c\\in{\\mathcal{C}}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ . Since this is true for arbitrary $\\varepsilon>0$ , we get $c\\in\\bigcap_{\\varepsilon>0}{\\mathcal{C}}^{\\varepsilon}(\\pi_{\\mathrm{E}})={\\mathcal{C}}(\\pi_{\\mathrm{E}})$ , which proves the desired closedness. ", "page_idx": 17}, {"type": "text", "text": "B.2 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Assume first that $\\tilde{c}\\in\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ for some $\\varepsilon>0$ . Then, there exists $\\tilde{u}\\in\\mathrm{Lip}(\\mathcal{X})$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}-T_{\\gamma}^{*}\\tilde{u}\\right\\rangle}&{\\leq}&{\\varepsilon,}\\\\ {\\tilde{c}-T_{\\gamma}^{*}\\tilde{u}}&{\\geq}&{-\\varepsilon,\\;\\;\\mathrm{on~}\\chi\\times{\\cal A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $T_{\\gamma}\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}=\\nu_{0}$ , (11) can be written equivalently as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underbrace{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}\\rangle}_{=V_{\\tilde{c}}^{\\pi_{\\mathrm{E}}}(\\nu_{0})}-\\langle\\nu_{0},\\tilde{u}\\rangle\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\tilde{\\mu}$ be an optimal solution to the primal $(\\mathbf{P}_{\\tilde{\\mathbf{c}}})$ with cost function $\\tilde{c}$ . By integrating (12) with respect to $\\tilde{\\mu}$ and using that $T_{\\gamma}\\tilde{\\mu}=\\nu_{0}$ and $\\begin{array}{r}{\\langle\\tilde{\\mu},1\\rangle=\\frac{1}{1-\\gamma}}\\end{array}$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underbrace{\\langle\\tilde{\\mu},\\tilde{c}\\rangle}_{=V_{\\tilde{c}}^{\\star}(\\nu_{0})}-\\langle\\nu_{0},\\tilde{u}\\rangle\\geq\\frac{-\\varepsilon}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by combining (13) and (14), we get ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{\\tilde{c}}^{\\star}(\\nu_{0})\\leq V_{\\tilde{c}}^{\\pi_{\\mathtt{E}}}(\\nu_{0})\\leq V_{\\tilde{c}}^{\\star}(\\nu_{0})+\\frac{2-\\gamma}{1-\\gamma}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This proves that $\\pi_{\\mathrm{E}}$ is $\\scriptstyle{\\frac{2-\\gamma}{1-\\gamma}}$ -optimal for $\\(\\mathbf{MDP}_{\\tilde{\\mathbf{c}}})$ ) with cost $\\tilde{c}$ . ", "page_idx": 18}, {"type": "text", "text": "For the inverse inclusion, $\\pi_{\\mathrm{E}}$ be $\\scriptstyle{\\frac{2-\\gamma}{1-\\gamma}}$ -optimal for $(\\mathbf{MDP}_{\\tilde{\\mathbf{c}}})$ ), and let $\\hat{u}=V_{\\tilde{c}}^{\\star}\\in\\mathrm{Lip}(\\mathcal{X})$ (Theorem A.1) be the optimal value function for the forward $\\(\\mathbf{MDP}_{\\tilde{\\mathbf{c}}})$ ). Then, by virtue of Theorem A.3, the following hold: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\tilde{c}-T_{\\gamma}^{*}\\hat{u}}&{\\geq}&{0,\\ \\mathrm{on}\\ \\mathcal{X}\\times\\mathcal{A},}\\\\ {\\underbrace{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}\\rangle}_{=V_{\\tilde{c}}^{\\pi_{\\mathrm{E}}}(\\nu_{0})}-\\underbrace{\\langle\\nu_{0},\\hat{u}\\rangle}_{=V_{\\tilde{c}}^{\\star}(\\nu_{0})}}&{\\leq}&{\\displaystyle\\frac{2-\\gamma}{1-\\gamma}\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that (15) holds due to dual feasibility, while (16) holds because $\\pi_{\\mathrm{E}}$ is $\\frac{2-\\gamma}{1-\\gamma}$ -optimal for $\\(\\mathbf{MDP}_{\\tilde{\\mathbf{c}}})$ ). By setting $\\begin{array}{r}{\\tilde{u}\\,=\\,\\hat{u}+\\frac{\\varepsilon}{1-\\gamma}\\,\\in\\,\\mathrm{Lip}(\\mathcal{X})}\\end{array}$ , we get by (15) that $\\tilde{c}-T_{\\gamma}^{*}\\tilde{u}\\,\\geq\\,-\\varepsilon$ , on $\\mathcal{X}\\times\\mathcal{A}$ . Moreover, $\\begin{array}{r}{\\left<\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}-T_{\\gamma}^{*}\\tilde{u}\\right>=\\left<\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}\\right>-\\left<\\nu_{0},\\hat{u}\\right>-\\left<\\nu_{0},\\frac{\\varepsilon}{1-\\gamma}\\right>\\leq\\frac{2-\\gamma}{1-\\gamma}\\varepsilon-\\frac{\\varepsilon}{1-\\gamma}=\\varepsilon}\\end{array}$ , where the inequality holds due to (16). Therefore, $\\tilde{c}\\in\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ . This concludes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before stating the proof of Proposition 3.2 we need the following preparation. Without loss of generality, assume that the true cost belongs to $\\begin{array}{r}{\\mathcal{C}_{\\mathrm{convex}}\\,\\triangleq\\,\\{\\sum_{i=1}^{k}\\alpha_{i}c_{i}\\ |\\ \\alpha\\ge\\,0}\\end{array}$ , $\\textstyle\\sum_{i=1}^{n}\\alpha_{i}\\,=\\,1\\}$ , where $\\{c_{i}\\}_{i=1}^{k}\\,\\subset\\,\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})$ are known features. By linearity the true optimal value function belongs to $\\textstyle\\{\\sum_{i=1}^{k}\\alpha_{i}u_{i}\\ |\\ \\alpha\\geq0\\}$ , $\\textstyle\\sum_{i=1}^{k}\\alpha_{i}=1\\}$ , where $u_{i}=V_{c_{i}}^{\\star}(\\pi_{\\mathrm{E}})$ , for all $i=1,\\dots,k$ . Note that by Theorem A.1, $\\{u_{i}\\}_{i=1}^{k}\\subset\\mathrm{Lip}(\\mathcal{X})$ . By using similar arguments to the proof of Theorem 3.1, we get that a cost function $c$ is inverse feasible, i.e., $c\\in{\\mathcal{C}}(\\pi_{\\mathrm{E}})$ if and only if there exists $\\alpha\\in\\mathbb{R}^{k}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\sum_{i=1}^{k}\\alpha_{i}\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c_{i}-T_{\\gamma}^{*}u_{i}\\rangle}&{=0,}\\\\ {\\sum_{i=1}^{k}\\alpha_{i}\\bigl(c_{i}-T_{\\gamma}^{*}u_{i}\\bigr)}&{\\ge0,\\ \\mathrm{on}\\ \\mathcal{X}\\times\\mathcal{A}}\\\\ {\\alpha\\ge0,\\ \\sum_{i=1}^{k}\\alpha_{i}=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similar arguments hold for any choice of finite-dimensional space or convex set $S\\subset\\operatorname{Lip}(\\mathcal{X})$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\{c_{n}\\}_{n=1}^{\\infty}\\subset\\mathcal{C}^{\\varepsilon_{n}}(\\pi_{\\mathrm{E}})$ . For each $n\\in\\mathbb N$ , there exists $\\alpha_{n}\\in\\mathbb{R}^{k}$ , such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\sum_{i=1}^{k}\\alpha_{n,i}\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c_{i}-T_{\\gamma}^{*}u_{i}\\rangle}&{\\leq\\varepsilon_{n},}\\\\ {\\sum_{i=1}^{k}\\alpha_{n,i}(c_{i}-T_{\\gamma}^{*}u_{i})}&{\\geq-\\varepsilon_{n},\\mathrm{~on~}\\mathcal{X}\\times\\mathcal{A}}\\\\ {\\alpha_{n}\\geq0,\\ \\sum_{i=1}^{k}\\alpha_{n,i}=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the sequence $\\{\\alpha_{n}\\}_{n}$ is bounded in $\\mathbb{R}^{k}$ , there exists a subsequence $\\{\\alpha_{n_{l}}\\}_{l=1}^{\\infty}$ such that $\\begin{array}{r l r}{\\operatorname*{lim}_{l\\to\\infty}\\alpha_{n_{l}}}&{{}\\!=\\!}&{\\alpha,}\\end{array}$ , for some $\\alpha~\\in~\\mathbb{R}^{k}$ . Taking the $l~\\rightarrow~\\infty$ in (18), we get (17) and so $\\begin{array}{r}{c=\\sum_{i=1}^{k}\\alpha_{i}c_{i}\\in\\mathcal{C}(\\pi_{\\mathrm{E}})}\\end{array}$ . This concludes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.4 Proof of Proposition 3.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. It is easy to check that for every $\\tilde{c}\\in\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})$ , there exist $c\\in{\\mathcal{C}}(\\pi_{\\mathrm{E}})$ such that $\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}-c\\right\\rangle\\leq\\varepsilon$ , and $c-\\tilde{c}\\leq\\varepsilon$ , on $\\mathcal{X}\\times\\mathcal{A}$ . For example, if $\\tilde{u}\\in\\mathrm{Lip}(\\mathcal{X})$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}-T_{\\gamma}^{*}\\tilde{u}\\rangle}&{\\leq\\varepsilon,}\\\\ {\\tilde{c}-T_{\\gamma}^{*}\\tilde{u}}&{\\geq-\\varepsilon,\\ \\mathrm{on}\\ \\chi\\times\\mathcal{A},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then we may choose $c=T_{\\gamma}^{*}\\tilde{u}$ . Let $\\tilde{\\pi}$ be an optimal policy for $\\big(\\mathbf{M}\\mathbf{D}\\mathbf{P}_{\\tilde{\\mathbf{c}}}\\big)$ ). Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{c}^{\\tilde{\\pi}}(\\nu_{0})-V_{c}^{\\pi_{\\mathrm{E}}}(\\nu_{0})=\\langle\\mu_{\\nu_{0}}^{\\tilde{\\pi}}-\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\langle\\mu_{\\nu_{0}}^{\\tilde{\\pi}},c-\\tilde{c}\\rangle+\\underbrace{\\langle\\mu_{\\nu_{0}}^{\\tilde{\\pi}}-\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}},\\tilde{c}\\rangle}_{\\le0}+\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}},\\tilde{c}-c\\rangle}\\\\ &{\\le\\langle\\mu_{\\nu_{0}}^{\\tilde{\\pi}}-\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}},c-\\tilde{c}\\rangle}\\\\ &{\\le\\displaystyle\\frac{2-\\gamma}{1-\\gamma}\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.5 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Note that Proposition 4.1 is a minor extension from [48, Prop. 7]. ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $c\\in{\\mathcal{C}}(\\pi_{\\mathrm{E}})$ , then there exists a certificate $u\\in\\mathrm{Lip}(\\mathcal{X})$ such that $c-T_{\\gamma}^{*}u\\geq0$ , on $\\mathcal{X}\\times\\mathcal{A}$ and $\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u\\rangle=0$ . By Theorem 3.1 and its proof, we get that $c$ is inverse feasible and $u=V_{c}^{\\star}$ $\\nu_{0}$ -almost everywhere (a.e.). Thus, if $c\\equiv C$ , for some constant $C$ , then $\\begin{array}{r}{u\\equiv\\frac{C}{1-\\gamma}}\\end{array}$ , $\\nu_{0}$ -a.e. We then have $c-T_{\\gamma}^{*}u=0$ , $\\nu_{0}$ -a.e., and so $\\begin{array}{r}{\\int_{\\mathcal{X}\\times\\mathcal{A}}(c-T_{\\gamma}^{*}u)\\mathbf{d}x\\mathbf{d}a=0}\\end{array}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.6 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For $p\\in[1,\\infty]$ , we denote by $\\lVert\\cdot\\rVert_{p}$ the $p$ -norm in $\\mathbb{R}^{n}$ and by $\\mathbf{\\nabla}x\\cdot\\mathbf{\\nabla}y$ the usual inner product. ", "page_idx": 19}, {"type": "text", "text": "Proof. We consider the following tightening of the semi-infinite convex program (IP). ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{n_{c,u}}\\triangleq\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{\\alpha,\\beta,\\varepsilon}}&{\\varepsilon}\\\\ {\\mathrm{s.t.}}&{\\left\\langle\\mu_{v_{0}}^{\\pi},c-T_{\\gamma}^{*}u\\right\\rangle\\leq\\varepsilon,}\\\\ &{c-T_{\\gamma}^{*}u\\geq0,\\ \\mathrm{on}\\,\\mathcal{X}\\times A,}\\end{array}\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\left.\\int_{X\\times A}(c(x,a)-T_{\\gamma}^{*}u(x,a))\\,\\mathrm{d}(x,a)=1,}\\\\ &{\\quad\\quad\\quad\\quad\\ c\\in\\mathbf{C}_{n_{c}},u\\in\\mathbf{U}_{n_{u}},\\varepsilon\\geq0}\\\\ &{\\quad=\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{\\alpha,\\beta}}&{\\left\\langle\\mu_{v_{0}}^{\\pi},c\\right\\rangle-\\left\\langle\\nu_{0},u\\right\\rangle}\\\\ {\\mathrm{s.t.}}&{c-T_{\\gamma}^{*}u\\geq0,\\ \\mathrm{on}\\,\\mathcal{X}\\times A,}\\\\ &{\\int_{X\\times A}(c(x,a)-T_{\\gamma}^{*}u(x,a))\\,\\mathrm{d}(x,a)=1,}\\\\ &{c\\in\\mathbf{C}_{n_{c}},u\\in\\mathbf{U}_{n_{u}},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last equality follows by using that $\\left<\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},T_{\\gamma}^{*}u\\right>=\\left<T_{\\gamma}\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},u\\right>=\\left<\\nu_{0},u\\right>$ and an epigraphic transformation. ", "page_idx": 19}, {"type": "text", "text": "The assumption that $u_{1}\\equiv1$ and >(1\u2212\u03b3)le1b(X\u00d7A) ensures feasibility of the convex program (20) and by Assumption 2.1 (A1) the feasibility set is compact and thus the optimal value is finite and is attained. Note that $\\operatorname{leb}({\\mathcal{X}}\\times{\\mathcal{A}})$ denotes the Lebesgue measure of $\\mathcal{X}\\times\\mathcal{A}$ . Moreover, since (20) is a tightening of (IP) it holds that $\\tilde{\\varepsilon}\\leq J_{n_{c,u}}$ . ", "page_idx": 19}, {"type": "text", "text": "Consider the infinite-dimensional version of (20), ", "page_idx": 19}, {"type": "equation", "text": "$$\nJ\\triangleq\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{c,u}}&{\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\right\\rangle-\\left\\langle\\nu_{0},u\\right\\rangle}\\\\ {\\mathrm{s.t.}}&{c-T_{\\gamma}^{*}u\\geq0,\\mathrm{~on~}\\mathcal{X}\\times\\mathcal{A},}\\\\ &{\\int_{\\mathcal{X}\\times\\mathcal{A}}(c(x,a)-T_{\\gamma}^{*}u(x,a))\\,\\mathrm{d}(x,a)=1,}\\\\ &{c\\in\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A}),u\\in\\mathrm{Lip}(\\mathcal{X}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the characterization of the inverse feasibility set, we have that $J=0$ and $(c^{\\star},u^{\\star})$ is an optimal solution for (21) 4. Note that (21) can be expressed in the standard conic form ", "page_idx": 19}, {"type": "equation", "text": "$$\nJ=\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{x}}&{\\langle l_{0},x\\rangle}\\\\ {\\mathrm{s.t.}}&{\\mathcal{A}x-b_{0}\\in K,}\\\\ &{x\\in X,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "text", "text": "\u2022 $\\left(X,L\\right)$ is a dual pair of vector spaces and $l_{0}\\in L$ ; ", "page_idx": 20}, {"type": "text", "text": "\u2022 $(B,Y)$ is a dual pair of vector spaces and $b_{0}\\in B$ ; ", "page_idx": 20}, {"type": "text", "text": "\u2022 $\\kappa$ is a positive cone in $_B$ and $\\pmb{K}^{*}$ is its dual cone in $\\mathbf{\\deltaY}$ , i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\nK^{*}=\\{\\pmb{y}\\in\\pmb{Y}:\\langle\\pmb{y},\\pmb{b}\\rangle\\geq0,\\;\\forall\\pmb{b}\\in\\pmb{B}\\};\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 $\\pmb{A}:\\pmb{X}\\rightarrow\\pmb{B}$ is linear and continuous with respect to the induced weak topologies. ", "page_idx": 20}, {"type": "text", "text": "Indeed, this is the case if we introduce the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{X\\triangleq\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})\\times\\mathrm{Lip}(\\mathcal{X}),\\quad}&{L\\triangleq\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})\\times\\mathcal{M}(\\mathcal{X}),}\\\\ {B\\triangleq\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})\\times\\mathbb{R},\\quad}&{Y\\triangleq\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})\\times\\mathbb{R},}\\\\ {K\\triangleq\\mathrm{Lip}(\\mathcal{X}\\times\\mathcal{A})_{+}\\times\\{0\\},\\quad}&{K^{*}\\triangleq\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}\\times\\mathbb{R},}\\\\ {b_{0}\\triangleq(\\mathbf{0},1),\\quad}&{l_{0}\\triangleq(\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},-\\nu_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mathcal{A}}(c,u)\\triangleq\\left[\\pmb{\\mathcal{A}}_{1}(c,u)\\right]\\triangleq\\left[\\begin{array}{c}{c-T_{\\gamma}^{*}u}\\\\ {\\qquad\\qquad\\,\\,\\forall\\,\\hdots\\hspace}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the pair $(X,C)$ we consider the norms ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|x\\|=\\|(c,u)\\|\\triangleq\\operatorname*{max}\\{\\|c\\|_{\\mathrm{L}}\\,,\\|u\\|_{\\mathrm{L}}\\},\\ x=(c,u)\\in X,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\boldsymbol{l}\\|_{*}}&{=}&{\\underset{\\|\\boldsymbol{x}\\|\\leq1}{\\operatorname*{sup}}\\left\\langle\\boldsymbol{l},\\boldsymbol{x}\\right\\rangle=\\underset{\\|\\boldsymbol{c}\\|_{\\mathrm{L}}\\leq1}{\\operatorname*{sup}}\\left\\langle\\boldsymbol{l}_{1},\\boldsymbol{c}\\right\\rangle+\\underset{\\|\\boldsymbol{u}\\|_{\\mathrm{L}}\\leq1}{\\operatorname*{sup}}\\left\\langle\\boldsymbol{l}_{2},\\boldsymbol{u}\\right\\rangle}\\\\ &{=}&{\\|\\boldsymbol{l}_{1}\\|_{\\mathbb{W}}+\\|\\boldsymbol{l}_{2}\\|_{\\mathbb{W}},\\;\\boldsymbol{l}=\\left(\\boldsymbol{l}_{1},\\boldsymbol{l}_{2}\\right)\\in\\mathbf{L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which are dual to each other. Similarly, on the pair $(B,Y)$ we consider the norms ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|(b_{1},b_{2})\\|}&{=}&{\\operatorname*{max}\\{\\|b_{1}\\|_{\\mathrm{L}},|b_{2}|\\},}\\\\ {\\|(y_{1},y_{2})\\|_{*}}&{=}&{\\|y_{1}\\|_{\\mathrm{W}}+|y_{2}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With this notation in mind, by virtue of [31, Th. 3.3] we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\tilde{\\varepsilon}}&{\\leq}&{J_{n_{c,u}}-\\underbrace{J}_{=0}}\\\\ &{\\leq}&{(\\|l_{0}\\|_{*}+D_{\\gamma,\\theta}\\,\\|\\mathbf{A}\\|_{\\mathrm{op}})}\\\\ &{}&{(\\|c^{\\star}-\\Pi_{\\mathbf{C}_{n_{c}}}(c^{\\star})\\|_{\\mathrm{L}}+\\|u^{\\star}-\\Pi_{\\mathbf{U}_{n_{u}}}(u^{\\star})\\|_{\\mathrm{L}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{D}_{\\gamma,\\theta}$ is an upper bound of a dual optimizer of (20) with respect to an appropriately defined dual norm, and $\\|\\mathcal{A}\\|_{\\mathrm{op}}$ is the operator norm of $\\mathbfcal{A}$ . We will next compute the involved quantities in (23). ", "page_idx": 20}, {"type": "text", "text": "We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lVert l_{0}\\rVert_{*}=\\lVert\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}}\\rVert_{\\mathsf{W}}+\\lVert\\nu_{0}\\rVert_{\\mathsf{W}}=\\frac{1}{1-\\gamma}+1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|{\\sf P}u\\|_{\\mathrm{L}}}&{=}&{\\|{\\sf P}u\\|_{\\infty}+|{\\sf P}u|_{\\mathrm{L}}\\leq\\|u\\|_{\\infty}+{\\cal L}_{\\sf P}|u|_{\\mathrm{L}}}\\\\ &{\\leq}&{\\operatorname*{max}\\{1,{\\cal L}_{\\sf P}\\}\\,\\|u\\|_{\\mathrm{L}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the first inequality we used that $\\mathsf{P}$ is a stochastic kernel and Assumption 2.1 (A2). Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\|\\pmb{\\mathcal{A}}_{1}(c,u)\\|_{\\mathrm{L}}}&{=}&{\\|c-u+\\mathsf{P}u\\|_{\\mathrm{L}}}\\\\ &{\\leq}&{\\left(2+\\gamma\\operatorname*{max}\\{1,L_{\\mathsf{P}}\\}\\right)\\|(c,u)\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{|\\pmb{\\mathcal{A}}_{2}(c,u)|}&{\\leq}&{(\\|c\\|_{\\infty}+(1+\\gamma)\\,\\|u\\|_{\\infty})\\mathsf{d i m}(\\mathcal{X}\\times\\mathcal{A})}\\\\ &{\\leq}&{(2+\\gamma)\\mathbf{leb}(\\mathcal{X}\\times\\mathcal{A})\\,\\|(c,u)\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "All in all, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\pmb{\\mathcal{A}}\\|_{\\mathrm{op}}}&{\\triangleq}&{\\underset{\\|(c,u)\\|\\leq1}{\\operatorname*{sup}}\\|\\pmb{\\mathcal{A}}(c,u)\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\leq}&{{}\\operatorname*{max}\\{(2+\\gamma){\\mathrm{leb}}({\\mathcal{X}}\\times{\\mathcal{A}}),2+\\gamma\\operatorname*{max}\\{1,L_{\\mathsf{P}}\\}\\}}\\\\ {\\leq}&{{}(2+\\gamma)\\operatorname*{max}\\{1,L_{\\mathsf{P}},{\\mathrm{leb}}({\\mathcal{X}}\\times{\\mathcal{A}})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It remains to compute the constant $\\mathcal{D}_{\\gamma,\\theta}$ . To this aim, let $\\{x_{i}\\}_{i=1}^{n_{c}+n_{u}}$ be basis elements in $\\mathbf{\\deltaX}$ given by $\\pmb{x}_{i}=(c_{i},0)$ , for $i=1,\\dots,n_{c}$ and $\\pmb{x}_{i}=(0,u_{i})$ , for $i=n_{c}\\!+\\!1,\\ldots,n_{c}\\!+\\!n_{u}.$ These are linearly independent by assumption. Then, we define the linear operator $A_{n_{c,u}}:\\mathbb{R}^{n_{c}+n_{u}}\\rightarrow Y$ by $\\mathbf{\\mathcal{A}}_{n_{c,u}}(\\rho)=$ $\\begin{array}{r}{\\sum_{i=1}^{n_{c}+n_{u}}\\rho_{i}\\pmb{\\mathcal{A}}x_{i}=\\sum_{i=1}^{n_{c}}\\alpha_{i}\\pmb{\\mathcal{A}}x_{i}+\\sum_{i=n_{c}+1}^{n_{c}+n_{u}}\\beta_{i}\\pmb{\\mathcal{A}}x_{i}}\\end{array}$ , for $\\pmb{\\rho}=(\\alpha,\\beta)\\in\\mathbb{R}^{n_{c}+n_{u}}$ . Then, it is easy to see that its adjoint $A_{n_{c,u}}^{*}:Y\\to\\mathbb{R}^{n_{c}+n_{u}}$ is given by $\\mathcal{A}_{n_{c,u}}^{*}(y)=\\left[\\langle\\pmb{\\mathscr{A}}\\pmb{x}_{1},y\\rangle\\,,\\dots,\\langle\\pmb{\\mathscr{A}}\\pmb{x}_{n_{c}+n_{u}},y\\rangle\\right]$ On $\\mathbb{R}^{n_{c}+n_{u}}$ we consider the norm ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\pmb\\rho\\|_{\\mathcal R}=\\|(\\alpha,\\beta)\\|_{\\mathcal R}\\triangleq\\operatorname*{max}\\{\\|\\alpha\\|_{1},\\|\\beta\\|_{1}\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, we set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{l}_{0}\\triangleq[\\underbrace{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c_{1}\\rangle\\,,\\dots,\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c_{n_{c}}\\rangle}_{=\\tilde{l}_{0,1}},\\underbrace{\\langle-\\nu_{0},u_{1}\\rangle\\,,\\dots,\\langle-\\nu_{0},u_{n_{u}}\\rangle}_{=\\tilde{l}_{0,2}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, the semi-infinite convex program (20) can be written in the form ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ_{n_{c,u}}=\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{\\rho}}&{\\tilde{l}_{0}\\cdot\\rho}\\\\ {\\mathrm{s.t.}}&{\\mathcal{A}_{n_{c,u}}\\rho-b_{0}\\in K,}\\\\ &{\\Vert\\rho\\Vert_{\\mathcal{R}}\\leq\\theta,\\ \\rho\\in\\mathbb{R}^{n_{c}+n_{u}}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Dualizing the conic inequality constraint in (26) and using the dual norm definition, we get its dual ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{J}_{n_{c,u}}=\\left\\{\\begin{array}{l l}{\\operatorname*{sup}_{y\\in Y}}&{\\langle b_{0},y\\rangle-\\theta\\|A_{n_{c,u}}^{*}y-\\tilde{l}_{0}\\|_{\\mathcal{R}^{\\star}}}\\\\ {\\mathrm{s.t.}}&{y\\in K^{*}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $y^{\\star}$ be a dual optimizer for (26). Assume that there exists a constant $C>0$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{\\mathcal{A}}_{n_{c,u}}^{*}\\pmb{y}^{\\star}\\|_{\\mathcal{R}^{*}}\\geq C\\,\\|\\pmb{y}^{\\star}\\|_{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then by virtue of [31, Prop. 3.2], we have the bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|y^{\\star}\\|_{*}\\leq\\frac{2\\theta\\|\\tilde{l}_{0}\\|_{R^{*}}}{C\\theta-\\|b_{0}\\|}\\leq\\mathcal{D}_{\\gamma,\\theta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To compute the constant $\\mathcal{D}_{\\gamma,\\theta}$ , we need to bound the involved quantities in (28). ", "page_idx": 21}, {"type": "text", "text": "We will first show that $y_{2}^{\\star}\\geq0$ . By Sion\u2019s minimax Theorem [75] the duality gap between (26) and (27) is zero, i.e., $J_{n_{c,u}}\\,=\\,\\tilde{J}_{n_{c,u}}$ . Note however that by construction $J_{n_{c,u}}\\ge0$ , since for any feasible $\\rho$ to (26) it holds that $\\tilde{l}_{0}\\cdot\\rho=\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\mathcal{A}_{n_{c,u}}\\rho-b_{0}\\right\\rangle\\geq0.$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\leq J_{n_{c,u}}=\\tilde{J}_{n_{c,u}}=\\langle b_{\\mathbf{0}},y^{\\star}\\rangle-\\theta\\|\\mathbf{A}_{n_{c,u}}^{*}y^{\\star}-\\tilde{l}_{\\mathbf{0}}\\|_{\\mathcal{R}^{\\star}}}\\\\ {=y_{2}^{\\star}-\\theta\\|\\mathbf{A}_{n_{c,u}}^{*}y^{\\star}-\\tilde{l}_{\\mathbf{0}}\\|_{\\mathcal{R}^{\\star}}.\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, $y_{2}^{\\star}\\geq0$ . Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|\\mathcal{A}_{n_{c},u}^{*}y^{\\star}\\|_{{\\mathcal R}^{*}}}&{\\geq}&{|\\langle\\mathcal{A}x_{n_{c}+1},y^{\\star}\\rangle\\,|=|\\,\\langle A(0,u_{1}),y^{\\star}\\rangle\\,|}\\\\ &{=}&{|\\,\\langle T_{\\gamma}^{*}u_{1},y_{1}^{\\star}\\rangle+y_{2}^{\\star}\\displaystyle\\int_{\\mathcal{X}\\times A}T_{\\gamma}^{*}u_{1}\\;\\mathrm{d}(x,a)|}\\\\ &{=}&{(1-\\gamma)\\,\\|y_{1}^{\\star}\\|_{\\mathbb{W}}+(1-\\gamma)\\mathrm{dim}(\\mathcal{X}\\times A)|y_{2}^{\\star}|}\\\\ &{\\geq}&{\\underbrace{(1-\\gamma)\\operatorname*{min}\\{1,\\mathrm{leb}(\\mathcal{X}\\times A)\\}}_{\\triangleq C}\\|y^{\\star}\\|_{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used that $u_{1}\\equiv1$ , $y_{1}^{\\star}\\in\\mathcal{M}(\\mathcal{X}\\times\\mathcal{A})_{+}$ and so $\\|y_{1}^{\\star}\\|_{\\mathsf{W}}=y_{1}^{\\star}({\\mathcal{X}}\\times{\\mathcal{A}})$ , and $y_{2}^{\\star}\\geq0$ . ", "page_idx": 21}, {"type": "text", "text": "In addition, a direct computation gives, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\tilde{l}_{0}\\|_{R^{*}}=\\operatorname*{sup}_{\\|\\rho\\|_{\\mathcal{R}}\\leq1}\\tilde{l}_{0}\\cdot\\rho=\\|\\tilde{l}_{0,1}\\|_{\\infty}+\\|\\tilde{l}_{0,2}\\|_{\\infty}\\leq\\frac{K_{c,\\infty}}{1-\\gamma}+K_{u,\\infty}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Putting them all together in (28), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Vert y^{\\star}\\Vert_{*}\\leq\\frac{2\\theta\\Vert\\tilde{l}_{0}\\Vert_{R^{*}}}{C\\theta-\\Vert b_{0}\\Vert}\\leq\\frac{2\\theta(K_{c,\\infty}+K_{u,\\infty})}{(1-\\gamma)^{2}\\operatorname*{min}\\{1,d\\}\\theta+\\gamma-1}\\triangleq\\mathcal{D}_{\\gamma,\\theta},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used that $\\|\\pmb{b_{0}}\\|=1$ . A combination of (23), (24), (25) and (29) ends the proof. ", "page_idx": 21}, {"type": "text", "text": "B.7 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The symbol $\\vDash$ refers to feasibility satisfaction, i.e., $x\\vDash\\mathcal{R}$ means that $x$ is a feasible solution for the program $\\mathcal{R}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. The proof is a consequence of [29, Theorem 1], [30, Lemma 3.2] and [30, Proposition 3.2]. Let us denote the optimization variable of $(S\\mathrm{IP_{N}})$ by $z\\,=\\,(\\alpha,\\beta,\\varepsilon)\\,\\in\\,\\mathbb{R}^{n_{c}+n_{u}+1}$ , where $\\alpha\\,=\\,(\\alpha_{1},...,\\alpha_{n_{c}})$ and $\\beta\\,=\\,(\\beta_{1},...\\,,\\beta_{n_{u}})$ . Let $\\lambda\\,=\\,(x,a)$ be the uncertainty parameter and $\\Lambda=\\mathcal{X}\\times\\mathcal{A}$ the uncertainty set. Consider the function ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(z,\\lambda)=-\\sum_{i=1}^{n_{c}}\\alpha_{i}c_{i}(x,a)+\\sum_{j=1}^{n_{u}}\\beta_{j}\\bigl(u_{j}(x)-\\gamma\\mathsf{P}u_{j}(x,a)\\bigr)-\\varepsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the set ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}=\\bigg\\{z=(\\alpha,\\beta,\\varepsilon)\\in\\mathbb{R}^{n_{c}+n_{u}+1}:\\|\\alpha\\|_{2}\\leq\\theta,\\|\\beta\\|_{2}\\leq\\theta,\\qquad\\qquad\\qquad\\quad}\\\\ {\\big\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathtt{E}}},-f(z,\\cdot)-\\varepsilon\\big\\rangle\\leq\\varepsilon,\\displaystyle\\int_{\\Lambda}(-f(z,\\lambda)+\\varepsilon)d\\lambda=1,\\varepsilon\\geq0\\bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\mathcal{Z}\\subset\\mathbb{R}^{n_{c}+n_{u}+1}$ is convex and compact and independent of $\\lambda\\in\\Lambda$ . We show that the set $\\mathcal{Z}$ is nonempty. As noted in the proof of Proposition 4.2 (Appendix B.6) the assumption that $u_{1}\\equiv1$ and $\\begin{array}{r}{\\theta>\\frac{1}{(1-\\gamma)\\mathrm{leb}(\\mathcal{X}\\times\\mathcal{A})}}\\end{array}$ considered in Proposition 4.2 and Theorems 4.1-4.2 ensures the feasibility of the convex program (IP) which in particular implies that $\\mathcal{Z}\\neq\\emptyset$ . Here $\\operatorname{leb}({\\mathcal{X}}\\times{\\mathcal{A}})$ denotes the Lebesgue measure of $\\mathcal{X}\\times\\mathcal{A}$ . ", "page_idx": 22}, {"type": "text", "text": "Note that the function $f:\\mathcal{Z}\\times\\Lambda\\to\\mathbb{R}$ is measurable, and linear in the first argument for all $\\lambda\\in\\Lambda$ . Moreover, by Assumption 2.1 (A1)-(A2), we get that $f$ is bounded in the second argument for all $z\\in{\\mathcal{Z}}$ , and $f$ is Lipschitz continuous on $\\Lambda$ uniformly in $\\mathcal{Z}$ with Lipschiz constant ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\cal L}_{\\Lambda}\\triangleq\\theta\\sqrt{n_{c}}{\\cal L}_{c}+\\theta\\sqrt{n_{u}}({\\cal L}_{u}{\\cal L}_{\\sf P}+{\\cal L}_{u}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "After introducing this notation, one can see that the random convex program $\\mathbf{(SIP_{N})}$ ) can be written as, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{SIP}_{\\mathbb{N}}:\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{z\\in\\mathcal{Z}}}&{h^{\\top}z}\\\\ {\\mathrm{s.t.}}&{f(z,\\lambda^{(\\ell)})\\leq0,\\,\\forall\\ell=1,\\ldots,N,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\boldsymbol{h}=(\\mathbf{0}_{\\mathbb{R}^{n_{c}}},\\mathbf{0}_{\\mathbb{R}^{n_{u}}},1)$ and the multisample $\\{\\lambda^{(i)}=(x^{(\\ell)},a^{(\\ell)})\\}_{\\ell=1}^{N}$ is a random element on the product probability space $(\\Lambda^{N},B(\\Lambda)^{N},\\mathbb{P}^{N})$ . Moreover, $\\mathbf{\\langleSIP_{N}},$ ) is the scenario counterpart of (IP), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{IP}:\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{z\\in{\\mathcal Z}}}&{h^{\\top}z}\\\\ {\\mathrm{s.t.}}&{f(z,\\lambda)\\leq0,\\,\\forall\\lambda\\in\\Lambda,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where one enforces constraint satisfaction for all the realizations of the uncertainty. Clearly if $z=(\\alpha,\\beta,\\varepsilon)\\in\\operatorname{I\\!P}$ , then the associated cost function $\\begin{array}{r}{c=\\sum_{i=1}^{n_{c}}\\alpha_{i}c_{i}\\in\\mathcal{C}^{\\varepsilon}(\\pi_{\\mathrm{E}})}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "For a fixed reliability level $\\epsilon\\in(0,1)$ , the associated chance-constrained program is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{CCP}_{\\epsilon}:\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{z\\in\\mathcal{Z}}}&{h^{\\top}z}\\\\ {\\mathrm{s.t.}}&{\\mathbb{P}[\\lambda\\in\\Lambda:\\;f(z,\\lambda)\\le0]\\ge1-\\epsilon,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where one allows constraint violation with low probability. Now the assumption that $u_{1}\\equiv1$ and $\\begin{array}{r}{\\theta>\\frac{1}{(1-\\gamma)\\mathrm{dim}(\\mathcal{X}\\times\\mathcal{A})}}\\end{array}$ is sufficient for the existence of a Slater point for the robust convex program (IP), i.e., there exists $z_{0}\\in\\mathcal{Z}$ such that $\\operatorname*{sup}_{\\lambda\\in\\Lambda}f(z_{0},\\lambda)<0.$ . By this fact and by Assumption 4.2, we can apply [29, Theorem 1] and conclude that for $N\\ge\\Nu(n_{c}+n_{u}+1,\\epsilon,\\delta)$ it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{N}[\\tilde{z}_{N}\\in\\mathrm{CCP}_{\\epsilon}]\\ge1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\tilde{z}_{N}=(\\tilde{\\alpha}_{N},\\tilde{\\beta}_{N},\\tilde{\\varepsilon}_{N})$ is the optimizer of $(\\mathrm{SIP_{N}})$ . Note that by Assumption 4.2, the optimizer $\\tilde{z}_{N}$ is uniquely defined and is a $\\mathcal{Z}$ -valued random variable on $(\\Lambda^{N};\\mathbf{\\dot{{B}}}(\\Lambda)^{N},\\mathbf{\\dot{{P}}}^{N})$ . ", "page_idx": 22}, {"type": "text", "text": "Consider now the $\\zeta$ -perturbed robust convex program for some $\\zeta>0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{IP}_{\\zeta}:\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{z\\in{\\mathcal{Z}}}}&{h^{\\top}z}\\\\ {\\mathrm{s.t.}}&{f(z,\\lambda)\\le\\zeta,\\forall\\lambda\\in\\Lambda.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that due to the min-max structure of (IP), the mapping from $\\zeta$ to the optimal value of $\\mathrm{IP}_{\\zeta}$ is Lipschitz continuous with Lipschitz constant 1 [30, Remark 3.5]. ", "page_idx": 23}, {"type": "text", "text": "We have the following implication, ", "page_idx": 23}, {"type": "equation", "text": "$$\nz=(\\alpha,\\beta,\\varepsilon)\\vdash\\mathrm{I\\!P}_{\\zeta}\\Rightarrow c=\\sum_{i=1}^{n_{c}}\\alpha_{i}c_{i}\\in\\mathcal{C}^{\\varepsilon+\\zeta}(\\pi_{\\mathrm{E}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, under Assumption 4.1 and since $f(z,\\cdot)$ is Lipschitz continuous on $\\Lambda$ uniformly in $\\mathcal{Z}$ with Lipschitz constant $L_{\\Lambda}$ , by virtue of [30, Lemma 3.2] and [30, Proposition 3.8], we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\nz\\mid=\\mathrm{CCP}_{\\epsilon}\\Rightarrow z\\mid=\\mathrm{IP}_{{\\bf h}(\\epsilon)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\nh(\\epsilon)\\triangleq L_{\\Lambda}g^{-1}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining (30), (31), (32) and (33) we get that for $\\begin{array}{r}{N\\ge\\mathrm{N}(n_{c}+n_{u}+1,g(\\frac{\\epsilon}{L_{\\Lambda}}),\\delta),}\\end{array}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}^{N}[\\tilde{c}_{N}\\in\\mathcal{C}^{\\tilde{\\varepsilon}+\\epsilon}(\\pi_{\\mathrm{E}})]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally notice that since $\\mathrm{\\DeltaSIP_{N}}$ ) is a relaxation of (IP) we have $\\tilde{\\varepsilon}\\le\\varepsilon_{\\mathrm{approx}}$ with probability 1. ", "page_idx": 23}, {"type": "text", "text": "B.8 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the remaining analysis we will use for brevity the following notation, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\zeta}\\triangleq\\mathbb{P}^{N},\\ \\ \\mathbb{P}_{\\tau}\\triangleq(\\mathbb{P}_{\\nu_{0}}^{\\pi_{\\mathrm{E}}})^{m},\\ \\ \\mathbb{P}_{\\xi}\\triangleq\\nu_{0}^{n}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and adopt a similar notation for products of probability measures, e.g., $\\mathbb{P}_{\\tau,\\xi}\\triangleq\\mathbb{P}_{\\tau}\\otimes\\mathbb{P}_{\\xi}$ . We first need the following result. ", "page_idx": 23}, {"type": "text", "text": "Proposition B.1. Let $\\epsilon\\ \\in\\ (0,1)$ and $\\delta~\\in~(0,1)$ . Under Assumption 2.1and (A1), for $n~\\geq$ $\\frac{8K_{u,\\infty}^{2}\\theta^{2}n_{u}\\ln\\left(\\frac{8n_{u}}{\\delta}\\right)}{\\epsilon^{2}}$ and $\\begin{array}{r}{m\\ge\\frac{8K_{c,\\infty}^{2}\\theta^{2}n_{c}\\ln\\left(\\frac{8n_{c}}{\\delta}\\right)}{(1-\\gamma)^{2}\\epsilon^{2}}}\\end{array}$ , it holds with probability at least $1-\\delta/2$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{c\\in\\mathbf{C}_{n_{c}},u\\in\\mathbf{U}_{n_{u}}}\\left|\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u\\right\\rangle-\\left\\langle\\widehat{\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c}\\right\\rangle+\\widehat{\\left\\langle\\nu_{0},u\\right\\rangle}\\right|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $K_{c,\\infty}\\triangleq\\operatorname*{max}_{i=1,\\dots,n_{c}}\\left\\|c_{i}\\right\\|_{\\infty}$ and $\\begin{array}{r}{K_{u,\\infty}\\triangleq\\operatorname*{max}_{j=1,\\dots,n_{u}}\\left\\|u_{i}\\right\\|_{\\infty}\\!.}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. First note that under Assumption (A1), the quantities $K_{c,\\infty}$ and $K_{u,\\infty}$ are finite. Now by using the Hoeffding\u2019s bound, we have that for $\\begin{array}{r}{n\\ge\\frac{8K_{u,\\infty}^{2}\\theta^{2}n_{u}\\ln\\big(\\frac{8n_{u}}{\\delta}\\big)}{\\epsilon^{2}}}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\xi}\\left[\\left|\\langle\\nu_{0},u_{j}\\rangle-\\widehat{\\langle\\nu_{0},u_{j}\\rangle}\\right|\\le\\frac{\\epsilon}{2\\sqrt{n_{u}}\\theta}\\right]\\ge1-\\frac{\\delta}{4n_{u}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $j=1,\\dots,n_{u}$ . Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\xi}\\left[\\underset{u\\in\\mathbf{U}_{n_{u}}}{\\operatorname*{sup}}\\left\\vert\\langle\\nu_{0},u\\rangle-\\widehat{\\langle\\nu_{0},u\\rangle}\\right\\vert\\le\\frac{\\epsilon}{2}\\right]}\\\\ {\\ge}&{\\mathbb{P}_{\\xi}\\left[\\forall j=1,\\ldots,n_{u}:\\left\\vert\\langle\\nu_{0},u_{j}\\rangle-\\widehat{\\langle\\nu_{0},u_{j}\\rangle}\\right\\vert\\le\\frac{\\epsilon}{2\\sqrt{n_{u}}\\theta}\\right]}\\\\ {\\ge}&{1-\\delta/4,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality follows by the monotonicity property of probability measures and the second one follows by (34) and a union bound. Integrating over the whole $(\\Omega^{m},\\mathbb{P}_{\\tau})$ , we end up that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\tau,\\xi}[\\operatorname*{sup}_{u\\in\\mathbf{U}_{n_{u}}}\\left|\\langle\\nu_{0},u\\rangle-\\widehat{\\langle\\nu_{0},u\\rangle}\\right|\\leq\\frac{\\epsilon}{2}]\\geq1-\\delta/4.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By using analogous arguments and taking into account that $\\begin{array}{r}{|\\sum_{t=0}^{\\infty}\\gamma^{t}c_{i}(x_{t},a_{t})|\\,\\leq\\,\\frac{K_{c,\\infty}}{1-\\gamma}}\\end{array}$ , for all $(x_{t},a_{t})\\in\\mathcal{X}\\times\\mathcal{A},t\\in\\mathbf{N}$ , $i=1,\\dots,n_{c}$ , we can conclude that for $\\begin{array}{r}{m\\ge\\frac{8K_{c,\\infty}^{2}\\theta^{2}n_{c}\\ln\\big(\\frac{8n_{c}}{\\delta}\\big)}{(1-\\gamma)^{2}\\epsilon^{2}}}\\end{array}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\tau,\\xi}[\\operatorname*{sup}_{\\underline{{c\\in\\mathbb{C}_{n_{c}}}}}\\left|\\left\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\right\\rangle-\\widehat{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c\\rangle}\\right|\\leq\\frac{\\epsilon}{2}]\\geq1-\\delta/4.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P_{\\tau,\\xi}\\Bigg[\\underset{c\\in{\\bf C}_{n_{c}},u\\in{\\bf U}_{n_{u}}}{\\operatorname*{sup}}\\Big|\\big\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c-T_{\\gamma}^{*}u\\big\\rangle-\\langle\\widehat{\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},c}\\big\\rangle\\!+\\!\\widehat{\\langle\\nu_{0},u\\rangle}\\Big|\\,\\le\\epsilon\\Bigg]}\\\\ &{\\quad\\ge\\mathbb P_{\\tau,\\xi}[A\\cap B]}\\\\ &{\\quad\\ge1-\\delta/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the first inequality we have used the monotonicity property of probability measures and the second inequality follows by (35), (36) and a simple union bound. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 4.2. By using the same notation as in the proof of Theorem 4.1, we can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{SIP}_{\\mathbf{N},\\mathbf{m},\\mathbf{n}}:\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{z\\in\\mathcal{Z}_{m,n}}}&{h^{\\top}z}\\\\ {\\mathrm{s.t.}}&{f(z,\\lambda^{(i)})\\leq0,\\,\\forall i=1,\\dots,N,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{m,n}=\\left\\{z=(\\alpha,\\beta,\\varepsilon)\\in\\mathbb{R}^{n_{c}+n_{u}+1}:\\|\\alpha\\|_{2}\\leq\\theta,\\|\\beta\\|_{2}\\leq\\theta,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n_{c}}\\alpha_{i}\\langle\\widehat{\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}}},c_{i}\\rangle(\\tau)-\\sum_{j=1}^{n_{u}}\\beta_{j}\\langle\\widehat{\\nu_{0},u_{j}}\\rangle(\\xi)\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{\\Lambda}(-f(z,\\lambda)+\\varepsilon)d\\lambda=1,\\varepsilon\\geq0\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Letn $\\begin{array}{r c l}{{{\\tilde{z}}_{N,m,n}}}&{{=}}&{{({{\\tilde{\\alpha}}_{N,m,n}},{{\\tilde{\\beta}}_{N,m,n}},{{\\tilde{\\varepsilon}}_{N,m,n}})}}\\end{array}$ be the optimizer of $\\mathrm{SIP}_{\\mathbf{N},\\mathbf{m},\\mathbf{n}}$ and let $\\begin{array}{r l}{\\tilde{c}_{N,m,n}}&{{}=}\\end{array}$ $\\textstyle\\sum_{i=1}^{n_{c}}{\\tilde{\\alpha}}_{N,m,n_{i}}c_{i}$ be the associated cost function. ", "page_idx": 24}, {"type": "text", "text": "We first fix multi-samples $\\tau,\\xi$ . Similarly as in Theorem 4.1, we can conclude that for $N\\geq\\mathbf{N}(n_{c}+$ $n_{u}+1,g(\\frac{\\epsilon}{L_{\\Lambda}}),\\delta/2)$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\zeta}[y\\in\\Lambda^{N}:\\ \\tilde{z}_{N,m,n}(y,\\tau,\\xi)\\in\\mathrm{I\\!P}_{\\mathbf{m},\\mathbf{n},\\epsilon}(y,\\tau)]\\geq1-\\delta/2,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{IP_{m,n,\\epsilon}}$ is the $\\epsilon$ -perturbed robust counterpart of $\\mathrm{SIP}_{\\mathbf{N},\\mathbf{m},\\mathbf{n}}$ given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{IP}_{\\mathbf{m},\\mathbf{n},\\epsilon}:\\left\\{\\begin{array}{l l}{\\operatorname*{inf}_{z\\in\\mathcal{Z}_{m,n}}}&{h^{\\top}z}\\\\ {\\mathrm{s.t.}}&{f(z,\\lambda)\\leq\\epsilon,\\,\\forall\\lambda\\in\\Lambda,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Integrating over the whole probability space $(\\Omega^{m}\\otimes\\mathcal{X}^{n},\\mathbb{P}_{\\tau,\\xi})$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{y,\\tau,\\xi}[\\tilde{z}_{N,m,n}\\in\\mathbb{P}_{\\mathbf{m},\\mathbf{n},\\epsilon}]\\geq1-\\delta/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "However, by virtue of Proposition B.1, for $\\begin{array}{r}{n\\ge\\frac{8K_{u,\\infty}^{2}\\theta^{2}n_{u}\\ln\\big(\\frac{8n_{u}}{\\delta}\\big)}{\\epsilon^{2}}}\\end{array}$ and $\\begin{array}{r}{m\\ge\\frac{8K_{c,\\infty}^{2}\\theta^{2}n_{c}\\ln\\big(\\frac{8n_{c}}{\\delta}\\big)}{(1-\\gamma)^{2}\\epsilon^{2}}}\\end{array}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}_{N,m,n}-T_{\\gamma}^{*}\\tilde{u}_{N,m,n}\\rangle}\\\\ {\\leq}&{\\langle\\mu_{\\nu_{0}}^{\\pi_{\\mathrm{E}}},\\tilde{c}_{N,m,n}\\rangle+\\langle\\nu_{0},\\overbrace{\\tilde{u}_{N,m,n}}^{*}\\rangle+\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability $\\mathbb{P}_{y,\\tau,\\xi}$ at least $1-\\delta/2$ . Combining (37), (38) and the bounds in Theorem 2 of [76] by a simple union bound completes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C Numerical Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We consider a one-dimensional truncated Linear-Quadratic-Gaussian (LQG) control problem comprising a linear dynamical system ", "page_idx": 25}, {"type": "equation", "text": "$$\nx_{t+1}=A x_{t}+B a_{t}+\\omega_{t},\\quad t\\in\\mathbb{N},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and a quadratic state cost $c(s,a)\\,=\\,Q x^{2}+R a^{2}$ , where $A,B\\,\\in\\,\\mathbb{R},Q\\,>\\,0,R\\,>\\,0$ . We assume that state and action spaces are given by $\\mathcal{X}\\,=\\,\\mathcal{A}\\,=\\,[-L,L]$ for some parameter $L\\,>\\,0$ . The disturbances $\\{\\omega_{t}\\}_{t\\in\\mathbb{N}}$ are i.i.d. random variables generated by a truncated normal distribution with known parameters $\\mu$ and $\\sigma$ , independent of the initial state $x_{0}$ . Thus, the process $\\omega_{t}$ has a distribution density ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(s,\\mu,\\sigma,L)=\\left\\{\\begin{array}{c c}{{\\frac{\\frac{1}{\\sigma}\\phi\\left(\\frac{s-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{L-\\mu}{\\sigma}\\right)-\\Phi\\left(\\frac{-L-\\mu}{\\sigma}\\right)},}}&{{s\\in[-L,L]}}\\\\ {{0}}&{{0.\\mathrm{w.}\\ ,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\phi$ is the probability density function of the standard normal distribution, and $\\Phi$ is its cumulative distribution function. The transition kernel $\\mathsf{P}$ has a density function $p(\\boldsymbol{y}\\mid\\boldsymbol{x},a)$ , i.e., $\\mathsf{P}(C\\mid x,a)=$ $\\textstyle\\int_{C}p(y\\mid x,a)\\mathrm{d}y$ for all $C\\in B(\\mathcal{X})$ , that is given by ", "page_idx": 25}, {"type": "equation", "text": "$$\np(y\\mid x,a)=f(y-A x-B a,\\mu,\\sigma,L).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the special case that $L=+\\infty$ the above problem represents the classical LQG problem, whose solution can be obtained via the algebraic Riccati equation [77, p. 372]. By referencing [31, Lemma7.1], we can readily conclude that Assumption 2.1 holds in this context with specific constants ", "page_idx": 25}, {"type": "equation", "text": "$$\nL_{\\mathsf{P}}=\\frac{2L\\operatorname*{max}\\{A,B\\}}{\\sigma^{2}\\sqrt{2\\pi}\\left(\\Phi\\left(\\frac{L-\\mu}{\\sigma}\\right)-\\Phi\\left(\\frac{-L-\\mu}{\\sigma}\\right)\\right)},\\quad L_{c}=\\operatorname*{max}\\{Q,R\\}2L.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As value function $u:\\mathcal{X}\\rightarrow\\mathbb{R}$ we use a simple polynomial of degree 2 $\\boldsymbol{n}_{u}=3$ ) such that ", "page_idx": 25}, {"type": "equation", "text": "$$\nu(x)=\\sum_{i=1}^{n_{u}}\\beta_{i}u_{i}(x),\\quad u_{i}(x)=x^{i-1},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "whereas the cost function $c:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ is approximated by the following weighted sum $(n_{c}=9)$ ) ", "page_idx": 25}, {"type": "equation", "text": "$$\nc(x,a)=\\sum_{i=1}^{n_{c}}\\alpha_{i}c_{i}(x,a),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $c_{1}(x,a)\\,=\\,1\\$ , $c_{2}(x,a)\\,=\\,x$ , $c_{3}(x,a)\\,=\\,a,$ , $c_{4}(x,a)\\,=\\,x a$ , $c_{5}(x,a)\\,=\\,x^{2}$ , $c_{6}(x,a)\\,=\\,a^{2}$ , $c_{7}(x,a)\\,=\\,x^{2}a,$ , $c_{8}(x,a)\\,=\\,x a^{2}$ , $c_{9}(x,a)\\,=\\,x^{2}a^{2}$ . For the simulation, the parameters are set as $L\\,=\\,10$ , $A\\,=\\,-1.5$ , $B\\,=\\,1$ , $Q\\,=\\,R\\,=\\,1$ , $\\mu\\,=\\,0$ , $\\sigma\\,=\\,1$ , and $\\gamma\\,=\\,0.99$ . The code for these experiments can be found at github.com/RAPACIRLCS/code. The experiments were run on a workstation with an AMD Ryzen 9 5950X CPU (16 cores) and 128GB of RAM. ", "page_idx": 25}, {"type": "text", "text": "Sampled Inverse Program with known transition kernel. In our first experiments highlighted in Figure 3, we focus on the sampled inverse program $\\mathrm{SIP}_{\\mathbf{N}}$ . More precisely, we solve the program $\\mathrm{SIP}_{\\mathbf{N}}$ for various choices of sample sizes $N$ and denote its corresponding optimizers as $(\\tilde{\\alpha}_{N},\\tilde{\\beta}_{N},\\tilde{\\varepsilon}_{N})$ and its extracted cost function as $\\begin{array}{r}{\\tilde{c}_{N}=\\sum_{i=1}^{n_{c}}\\tilde{\\alpha}_{N_{i}}c_{i}}\\end{array}$ . Figure 3a shows the probability of the learnt cost function $\\tilde{c}_{N}$ being $\\left(\\tilde{\\varepsilon}_{N}+\\epsilon\\right)$ -inverse feasible for various choices of $\\epsilon$ and . The plotted probability represents the empirical probability derived from 1000 experiments. It is evident that, for a constant parameter $\\epsilon$ , the likelihood of being inverse feasible grows with the increase of the sample size $N$ . Additionally, for a constant sample size $N$ , the probability of being inverse feasible increases as the parameter $\\epsilon$ decreases. Both of these trends align with and support our theoretical findings as outlined in Theorem 4.1. Figure 3b shows the objective function of program $\\mathrm{SIP}_{\\mathbf{N}}$ for various choices of sample sizes $N$ . Since these are random programs, we plot the empirical average (solid line) and its corresponding standard deviations (shaded area) derived from 1000 independent experiments. As expected, the objective value $\\tilde{\\varepsilon}_{N}$ increases as a function of the sample size $N$ . Figure 3c visualizes the theoretical sample complexity of Theorem 4.1, i.e., for various choices of $\\delta$ and $\\epsilon$ , we plot the ", "page_idx": 25}, {"type": "image", "img_path": "VUgXAWOCQz/tmp/d60a6a97c1cb137c32dfcdbc02e6861215b34c0fb792055d5e730bb1f3fa9adc.jpg", "img_caption": ["(c) Sample complexity of Theorem 4.1 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "VUgXAWOCQz/tmp/ea19db9b7a61a37d11d1e3e834a830923cde8610436bd5e8ce9594a24632da39.jpg", "img_caption": ["(d) Performance of learnt cost "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 3: Solutions of the Sampled Inverse Program $\\mathrm{SIP}_{\\mathbf{N}}$ . The variable $N$ is the number of i.i.d. samples $(x,a)$ drawn uniformly from $\\mathcal{X}\\times\\mathcal{A}$ . We run 1000 independent experiments. Plot (a) shows the empirical probability of the estimated cost function $\\tilde{c}_{N}$ being an element of the feasibility set, as described in Theorem 4.1 for given values of $N$ and $\\epsilon$ . Plot (b) shows the objective value of the random program $\\mathrm{SIP}_{\\mathbf{N}}$ , i.e., $\\tilde{\\varepsilon}_{N}$ on average over the 1000 experiments, where the shaded area shows the standard deviations. Plot (c) is a visualization of the theoretical sample complexity as given by Theorem 4.1. For various values of $\\delta$ and $\\epsilon$ , we plot the sample size $\\begin{array}{r}{N=\\mathbf{\\bar{N}}(n_{c}+n_{u}+1,g(\\frac{\\epsilon}{L_{\\Lambda}}),\\delta)}\\end{array}$ . The variation parameter is set to $\\Delta=1\\cdot10^{-7}$ . Plot (d) compares the discounted long-run costs $V_{\\tilde{c}_{N}}^{\\pi}(\\nu_{0})$ for the average $\\bar{\\tilde{c}}_{N}$ of the learnt costs $\\tilde{c}_{N}$ under the expert policy $\\pi_{\\mathrm{E}}$ (red) and the optimal policy (blue). The solid line plots average over 1000 independent experiments, where the shaded area shows the standard deviations. ", "page_idx": 26}, {"type": "text", "text": "sample size $\\begin{array}{r}{N=\\mathrm{N}(n_{c}+n_{u}+1,g(\\frac{\\epsilon}{L_{\\Lambda}}),\\delta)}\\end{array}$ . To simplify the computation, we used the closed form upper bound for the function $\\mathbf{N}$ derived in [78] and given as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{N}(n,\\epsilon,\\delta)\\leq\\frac{2}{\\epsilon}\\log\\left(\\frac{1}{\\delta}\\right)+2n+\\frac{2n}{\\epsilon}\\log\\left(\\frac{2}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "When comparing Figure 3a and Figure 3c, we can see that there is a significant gap between the empirical and theoretical bounds. The dynamics of a variation in $\\epsilon$ , however, match the empirically observed behaviour. Figure 3d visualizes the performance of the learnt cost $\\tilde{c}_{N}$ by comparing the discounted long-run cost of the expert policy under this learnt policy $V_{\\tilde{c}_{N}}^{\\pi_{\\mathrm{E}}}(\\nu_{0})$ with its theoretical lower bound $\\operatorname*{min}_{\\pi\\in\\Pi}V_{\\tilde{c}_{N}}^{\\pi}(\\nu_{0})$ . According to Theorem 4.1 and Proposition 3.1 for large $N$ this difference vanishes with high probability, this behaviour can be observed in the plot. To reduce the computational effort replace in Figure 3d the learnt cost $\\tilde{c}_{N}$ with its empirical average, denoted $\\bar{\\tilde{c}}_{N}$ , taken over 1000 independent experiments. More precisely, for 1000 initial conditions $x_{0}\\sim\\nu_{0}$ we plot $V_{\\tilde{c}_{N}}^{\\pi_{\\mathrm{E}}}(x_{0})$ and the theoretical lower bound $\\mathrm{min}_{\\pi\\in\\Pi}\\,V_{\\tilde{c}_{N}}^{\\pi}(x_{0})$ . ", "page_idx": 26}, {"type": "text", "text": "Sampled Inverse Program with unknown transition kernel. The second experiment, Figure 4, solves the sampled inverse program $\\mathrm{SIP}_{\\mathbf{N},\\mathbf{m},\\mathbf{n},\\mathbf{k}}$ with unknown transition kernel. Compared to $\\mathrm{SIP_{N}}$ , since we assume the transition kernel to be unknown, the inequality constraints are based on sampled state transitions. The empirical probability, shown in Figure 4a, is derived from 1000 independent experiments. To decrease the degrees of freedom in the parameter selection we set $n=m=k$ for the simulations. The behaviour of the empirical confidence, observable in Figure 4a, follows the ", "page_idx": 26}, {"type": "image", "img_path": "VUgXAWOCQz/tmp/6884db617bfde9ce09d3e06fb595e095b9f02ecd791390f59f04af91ee55e372.jpg", "img_caption": ["(a) Empirical confidence "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "VUgXAWOCQz/tmp/36b02e9b023eb835d95e427dfadead31ceb7262bdd01ee2ef93dcd8c0d071ae8.jpg", "img_caption": ["(b) Sample complexity of $k$ "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 4: Solutions of the Sampled Inverse Program $\\mathrm{SIP}_{\\mathbf{N},\\mathbf{m},\\mathbf{n},\\mathbf{k}}$ . The variable $N$ is the number of i.i.d. samples $(x,a)$ drawn uniformly from $\\mathcal{X}\\times\\mathcal{A}$ . We run 1000 independent experiments. Plot (a) shows the empirical probability of the estimated cost function $\\tilde{c}_{N,m,n,k}$ being an element of the feasibility set, as described in Theorem 4.2 for different $N,k$ pairs given a chosen accuracy parameter $\\epsilon$ . Plot (b) shows the theoretical lower bound on $k$ depending on $N$ , for a set $\\epsilon$ , as described by Theorem 4.2. ", "page_idx": 27}, {"type": "text", "text": "trends shown in Figure 3. As expected, an increase in the number of samples increases the confidence of learning a cost function $\\tilde{c}_{N,m,n,k}$ that belongs to the inverse feasibility set $\\mathcal{C}^{\\tilde{\\varepsilon}_{N,m,n,k}+\\epsilon}(\\pi_{E})$ . It can be seen how, even for the largest possible $\\epsilon$ , to reach a certain empirical confidence the $\\mathrm{SIP}_{\\mathbf{N},\\mathbf{m},\\mathbf{n},\\mathbf{k}}$ program requires many more state-action samples $N$ compared to the $\\mathrm{SIP}_{\\mathbf{N}}$ program. When comparing the empirical confidence of a given $\\epsilon$ , $N$ , and $k$ with the theoretical sample complexity, following Theorem 4.2, of $k$ corresponding to the same $\\epsilon$ and $N$ it can be seen that the empirical sample performance of $\\mathrm{SIP}_{\\mathbf{N},\\mathbf{m},\\mathbf{n},\\mathbf{k}}$ is much more efficient. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The abstract and introduction do reflect and summarize the paper\u2019s contribution and scope. In the sections following we provide the rigorous statements of these contributions. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss limitations in Remark 4.1, where we point to the exponential dependence of our sample complexity with respect to the dimension of the state space (known as curse of dimensionality). Moreover, in this remark we point out that the selection of an \u201cinformative\" distribution for the random state-action selection is not well understood and crucial for the practical efficiency of the method. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: All statements in the paper are equipped with detailed and correct proofs, which are provided in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All our simulations are transparent and can be easily reproduced by the code available via the GitHub link in the paper, see Section C. We would like to note that our datasets are synthetic datasets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 29}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The code of all our experiments is available via the GitHub link in the paper, see Section C. It is therefore easy to reproduce our results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The details of the experiments are provided via our files available in the GitHub, whose link is provided in the paper, see Section C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: In our plots we show average performance over 1000 independent experiments (solid lines), but additionally we show the corresponding standard deviations with the shaded areas, see Figure 3. In Figure 4 we run 1000 independent experiments and plots show empirical probabilities computed from them. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: As pointed out in Section C all our experiments were run on a workstation with an AMD Ryzen 9 5950X CPU (16 cores) and 128GB of RAM. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes, it does. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work is a theoretical result on inverse reinforcement learning and has not direct, neither positive nor negative, societal impacts. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Due to the theoretical and mathematical nature of our work, this question does not apply. The risks stated in the question are not present in our work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not use any existing assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not introduce any new assets in our work. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]