[{"type": "text", "text": "Model-Based Transfer Learning for Contextual Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jung-Hoon Cho Vindula Jayawardana MIT MIT jhooncho@mit.edu vindula@mit.edu Sirui Li Cathy Wu MIT MIT siruil@mit.edu cathywu@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep reinforcement learning (RL) is a powerful approach to complex decision making. However, one issue that limits its practical application is its brittleness, sometimes failing to train in the presence of small changes in the environment. Motivated by the success of zero-shot transfer\u2014where pre-trained models perform well on related tasks\u2014we consider the problem of selecting a good set of training tasks to maximize generalization performance across a range of tasks. Given the high cost of training, it is critical to select training tasks strategically, but not well understood how to do so. We hence introduce Model-Based Transfer Learning (MBTL), which layers on top of existing RL methods to effectively solve contextual RL problems. MBTL models the generalization performance in two parts: 1) the performance set point, modeled using Gaussian processes, and 2) performance loss (generalization gap), modeled as a linear function of contextual similarity. MBTL combines these two pieces of information within a Bayesian optimization (BO) framework to strategically select training tasks. We show theoretically that the method exhibits sublinear regret in the number of training tasks and discuss conditions to further tighten regret bounds. We experimentally validate our methods using urban traffic and standard continuous control benchmarks. The experimental results suggest that MBTL can achieve up to 50x improved sample efficiency compared with canonical independent training and multi-task training. Further experiments demonstrate the efficacy of BO and the insensitivity to the underlying RL algorithm and hyperparameters. This work lays the foundations for investigating explicit modeling of generalization, thereby enabling principled yet effective methods for contextual RL. Code is available at https://github.com/jhoon-cho/MBTL/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep reinforcement learning (DRL) has made remarkable strides in addressing complex problems across various domains [29, 36, 1, 4, 11, 13, 27]. Despite these successes, DRL algorithms often exhibit brittleness when exposed to small variations like different number of lanes, weather conditions, or flow density in traffic benchmarks [20], significantly limiting their scalability and generalizability [19]. Such variations can be modeled using the framework of contextual Markov Decision Processes (CMDP), where task variations can be parameterized within a context space [15, 32, 6]. ", "page_idx": 0}, {"type": "text", "text": "There are two predominant solution modalities for CMDPs [22]: independent training and multi-task training. Independent training constructs a separate model for each task variant (say, $N\\gg1$ ), which is compute-intensive. At the other extreme, multi-task training constructs a single \u201cuniversal\u201d policy, and thus can be more compute-efficient, but suffers from model capacity and negative transfer issues [21, 44, 2, 41]. There is thus a need for more reliable training methodologies for generalization across tasks variants. In this work, we consider training an intermediate set of $K$ models, where $N>K>1$ , in an effort to balance performance and efficiency; we refer to this strategy as multi-policy training. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A note on terminology. For brevity, we refer to task variants as tasks in the remainder of this article. We also use the language of task and context interchangeably. We emphasize that this work focuses on within-domain generalization (e.g., traffic signal control for intersection scenario variants) rather than across-domain generalization (e.g., distinct traffic control tasks). Additionally, it is crucial to differentiate between training reliability, which concerns the ability to reliably train models across tasks, and model reliability (or robustness), which concerns the resistance of a trained model to differences in tasks. This article is concerned with training reliability. ", "page_idx": 1}, {"type": "text", "text": "We build upon zero-shot transfer, a widely-used practical technique which directly applies a policy trained in one context (source task) to another (target task) without adaptation. This experiences performance degradation, also called generalization gap [17, 22]. For instance, Figure 1 depicts that the performance degrades as the target task diverges from the source task, corresponding to an increasing generalization gap. Nonetheless, leveraging the notion that training is expensive but zero-shot transfer is cheap, we are interested in optimally selecting a set of source (training) tasks, such that the generalization performance on the target range of tasks is maximized. In this article, we strategically select source tasks by explicitly modeling the generalization performance to estimate the value of training a new source task. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this work are: ", "page_idx": 1}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/f60e3527d71c615aa143f212ba3449cb658e0e1b584e8842004e4003474938f1.jpg", "img_caption": ["Figure 1: Example generalization gap depicted for Cartpole CMDP. The solid lines show the true zero-shot transfer generalization performance across contexts. Source tasks are indicated by dotted lines. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce Model-Based Transfer Learning (MBTL), a novel framework for studying generalization in reinforcement learning (Figure 2). To the best of our knowledge, this is the first work to explicitly model generalization performance for contextual RL. As such, our work opens the door for further investigation into reliable model-based methodologies for contextual RL.   \n\u2022 We provide theoretical analysis for the sublinear regret of MBTL and give conditions for achieving tighter regret bounds.   \n\u2022 We empirically validate our methods in urban traffic and standard continuous control benchmarks for contextual RL, observing up to $\\mathbf{50x}$ improvements in sample efficiency. We further include ablations on the components of the algorithm. ", "page_idx": 1}, {"type": "text", "text": "The remainder of the paper is organized as follows. After introducing notation in Section 2, we formally define the problem in Section 3. A key contribution of our work is the introduction of a Gaussian process model acquisition function specifically tailored to the source task selection problem, which is detailed in Section 4. In Section 4.3, we provide a theoretical analysis of the regret bounds of our method, followed by an empirical evaluation across diverse applications in Section 5. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Contextual MDP. A standard MDP is defined by the tuple $M\\,=\\,(S,A,P,R,\\rho)$ where $S$ represents the state space, $A$ is the action space, $P$ denotes the transition dynamics, $R$ is the reward function, and $\\rho$ is the distribution over initial states [43]. A contextual MDP (CMDP), denoted by $\\mathcal{M}=(S,A,P_{x},R_{x},\\rho_{x})_{x\\in X}$ , is a collection of context-MDPs $\\mathcal{M}_{x}$ parameterized by a context variable $x$ within a context space $X$ (assumed bounded). The context variable $x$ can influence dynamics, rewards, and initial state distributions [15, 32, 6]. We define source task performance $J(\\pi_{x},x;\\mathrm{Alg})$ as follows: we train a policy $\\pi_{x}$ on a task with the context $x\\in X$ using RL algorithm Alg (e.g., PPO, SAC) and evaluate the policy by the expected return in the same MDP $\\mathcal{M}_{x}$ with context $x$ . For brevity, we will write it as $J(\\pi_{x},x)$ . We distinguish between estimated values $\\hat{J}(x)$ and observed outcomes $J(\\pi_{x},x)$ , with the latter measured after training and evaluation. ", "page_idx": 1}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/4d67241530b8e1a7a2aa0aefc9df5cf785bb9ea7c8c38859737b24fecf12ad55.jpg", "img_caption": ["Figure 2: Overview illustration for Model-based Transfer Learning. (a) Gaussian process regression is used to estimate the training performance across tasks using existing policies; (b) marginal generalization performance (red area) is calculated using upper confidence bound of estimated training performance, generalization gap, and generalization performance; (c) selects the next training task that maximizes the acquisition function (marginal generalization performance); (d) once the selected task is trained, calculate generalization performance using zero-shot transfer. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Generalization gap via zero-shot transfer. Consider zero-shot transfer from the trained policy $\\pi_{x}$ from a source task (context-MDP) to solve another target task (context-MDP) with the context $x^{\\prime}\\in X$ in the CMDP. Zero-shot transfer involves applying a policy trained on a source task $\\mathcal{M}_{x}$ to a different target task $\\mathcal{M}_{x^{\\prime}}$ , with $x,x^{\\prime}\\in X$ . We observe the generalization performance, denoted by $J(\\pi_{x},x^{\\prime})$ , by evaluating the target task $x^{\\prime}$ based on the policy trained using source task $x$ via zero-shot generalization. We define the generalization gap as the absolute performance difference in average reward when transferring from source task $x$ to target task $x^{\\prime}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underbrace{\\Delta J(\\pi_{x},x^{\\prime})}_{\\mathrm{Generalization\\;gap}}=\\underbrace{J(\\pi_{x},x)}_{\\mathrm{Source\\;task\\;performance}}-\\underbrace{J(\\pi_{x},x^{\\prime})}_{\\mathrm{Generalization\\;performance}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Sequential source task selection problem. The selection of source MDPs from the CMDPs is key to solving the overall CMDP [3]. We therefore introduce the sequential source task selection (SSTS) problem, which seeks to maximize the expected performance across a dynamically selected set of tasks. This problem is cast as a sequential decision problem, in which the selection of tasks is informed through feedback from the observed task performance of the tasks selected and trained thus far. The notation $x_{k}$ indicates the selected source task at the $k$ -th transfer step, where $k$ ranges from 1 to $K$ . For brevity, we will denote $\\pi_{x_{k}}$ as $\\pi_{k}$ . We denote the sequence $x_{1},x_{2},...,x_{k}$ by $x_{1:k}$ and $\\pi_{1},\\pi_{2},...,\\pi_{k}$ by $\\pi_{1:k}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Sequential Source Task Selection Problem). This problem seeks to optimize the expected generalization performance across a CMDP $\\mathcal{M}_{x^{\\prime}\\in X}$ by selecting a task $x\\in X$ at each training stage. Specifically, at each selection step $k$ , we wish to choose a distinct task $x_{k}$ such that the expected cumulative generalization performance is maximized. This can be expressed by keeping track at each step, which policy to use for which task, and the corresponding generalization performance. Upon training the policy $\\pi_{x_{k}}$ for source task $x_{k}$ , the cumulative generalization performance, which we abuse notation to denote as $J(\\pi_{1:k},\\cdot)=J(\\pi_{x_{k}},\\cdot;\\pi_{1:k-1})$ . Formally, this can be recursively defined based on previous observations $\\{J(\\pi_{1},x),\\dots,J(\\pi_{k-1},x)\\}$ for all $x\\in X$ as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(\\pi_{x_{k}},x^{\\prime};\\pi_{1:k-1})=\\operatorname*{max}\\left(J(\\pi_{k},x^{\\prime}),J(\\pi_{1:k-1},x^{\\prime})\\right)\\quad\\forall x^{\\prime}\\in X\\quad i f k>1.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "And $J(\\pi_{1:1},x)\\equiv J(\\pi_{1},x)$ . Then, the overall sequential decision problem can be written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x_{k}}~~V(x_{k};\\pi_{1:k-1})=\\operatorname*{max}_{x_{k}}\\mathbb{E}_{x^{\\prime}\\sim\\mathcal{U}(X)}\\left[J(\\pi_{x_{k}},x^{\\prime};\\pi_{1:k-1})\\right]~~~s.t.~x_{k}\\in X~\\backslash~x_{1:k-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The state at each step $k$ is defined by the best generalization performance for each task, achieved by policies trained in earlier stages, represented as $J(\\pi_{1:k-1},x^{\\prime})$ for each target task $x^{\\prime}$ . The action at each step is choosing a new task $x_{k}$ . In general, SSTS exhibits stochastic transitions, for example due to randomness in RL training. For simplicity, in this work, we assume deterministic transitions; that is, training context-MDP $x$ will always yield the same performance $J(\\pi_{x},x)$ and generalization gap $\\Delta J(\\pi_{x},x^{\\overline{{\\prime}}}),\\forall x^{\\prime}\\in X$ . The problem\u2019s maximum horizon is defined by $|X|$ , but can be terminated early if conditions are met (e.g., performance level, training budget). ", "page_idx": 3}, {"type": "text", "text": "4 Model-Based Transfer Learning (MBTL) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce an algorithm called Model-based Transfer Learning to solve the SSTS problem. MBTL models the generalization performance in two parts: 1) the performance set point, modeled using Gaussian processes, and 2) performance loss (generalization gap), modeled as a linear function of contextual similarity. MBTL combines these two pieces of information within a Bayesian optimization (BO) framework to sequentially select training tasks that maximize generalization performance. The high-level pseudo-code of the MBTL algorithm is given below. ", "page_idx": 3}, {"type": "text", "text": "MBTL Pseudo-code ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "1: Input: Task set, Gaussian Process (GP) model, and training budget   \n2: while training budget remains do   \n3: Select next source task $\\leftarrow$ Task with the highest acquisition value based on estimated training performance, generalization gap, and generalization performance, determined by the current GP model and tailored acquisition function   \n4: Train a policy on the selected source task using reinforcement learning, then evaluate its training performance   \n5: Update the GP model with the selected source task training performance to estimate the training performance of tasks in the task set   \n6: end while ", "page_idx": 3}, {"type": "text", "text": "7: Output: Set of policies that maximizes generalized performance across task set ", "page_idx": 3}, {"type": "text", "text": "4.1 Modeling assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a task set $X$ that is continuous and the performance function $J(\\pi,x),V(x)$ for a policy $\\pi$ to be smooth over the task space $X$ . In practice, such as control systems, tasks often vary continuously and smoothly rather than abruptly. For example, adjusting the angle of a robotic arm by a small amount typically results in a small change in the system and optimal action. ", "page_idx": 3}, {"type": "text", "text": "Inspired by the empirical generalization gap performance as observed in Figure 1, we model the generalization gap with a linear function. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Linear generalization gap). A linear function is used to model the generalization gap function, formally $\\Delta\\hat{J}(\\pi_{x},x^{\\prime})\\simeq\\theta|x-x^{\\prime}|,$ , where $\\theta$ is the slope of the linear function and $x$ and $x^{\\prime}$ are the context of the source task and target task, respectively. ", "page_idx": 3}, {"type": "text", "text": "The relaxation of this assumption can yield additional efficiency benefits but also adds modeling complexity, and thus is an interesting direction for future work. ", "page_idx": 3}, {"type": "text", "text": "4.2 Bayesian optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Bayesian optimization (BO) is a powerful strategy for finding the global optimum of an objective function when obtaining the function is costly, or the function itself lacks a simple analytical form [31, 7]. BO integrates prior knowledge with observations to efficiently decide the next task to train by using the acquisition function. MBTL is a BO method which optimizes for promising source tasks by leveraging Assumption 1 in its acquisition function. The role of BO is to approximate $V(x_{k};\\pi_{1:k-1})$ (see Equation 3) using the data acquired thus far. The next source task $x_{k}$ is then selected using this estimate. ", "page_idx": 3}, {"type": "text", "text": "Gaussian process (GP) regression. Within the framework of BO, we model the source training performance $\\hat{J}(\\pi_{x},\\!x)$ using Gaussian process (GP) regression. Specifically, the function $\\hat{J}(\\pi_{x},\\!x)$ is assumed to follow a GP $(\\hat{J}(\\pi_{x},x)\\,\\sim\\,\\mathcal{G P}(\\mathbb{E}[\\hat{J}(\\pi_{x},x)],k(x,\\tilde{x})))$ , where $k(x,\\tilde{x})$ is the covariance function, representing the expected product of deviations of $\\hat{J}(\\pi_{x},\\!x)$ and $\\hat{J}(\\pi_{\\tilde{x}},\\tilde{x})$ from their respective means. Let $D_{k-1}$ denote the data observed up to iteration $k-1$ , consisting of the pairs $\\{(x_{i},J(\\pi_{i},x_{i}))\\}_{i=1,\\ldots,k-1}$ . The estimated performance $\\hat{J}_{k}$ after querying $k\\,-\\,1$ samples is updated as more samples are obtained. The posterior prediction of $\\hat{J}_{k}$ at a new point $x$ , given the data $D_{k-1}$ and previous inputs $x_{1:k-1}$ , is normally distributed as $P(\\hat{J}_{k}\\ \\mid\\ D_{k-1})\\ =$ $\\mathcal{N}(\\mu_{k}(x),\\sigma_{k}^{2}(x))$ . $\\mu_{k}(x)$ and $\\sigma_{k}^{2}(x)$ are defined as $\\mu_{k}(x)=\\mathbb{E}[\\hat{J}(\\pi_{x},x)]+\\mathbf{k}^{\\top}(\\mathbf{K}+\\sigma^{2}\\mathbf{I})^{-1}\\mathbf{y}$ and $\\sigma_{k}^{2}(x)=k(x,x)-\\mathbf{k}^{\\top}(\\mathbf{K}+\\sigma^{2}\\mathbf{I})^{-1}\\mathbf{k},$ , with $\\mathbf{k}$ being the vector of covariances between $x$ and each $x_{i}$ in the observed data, i.e., $\\mathbf{k}=\\left[k(x,x_{1}),\\ldots,k(x,x_{k-1})\\right]$ , and $\\mathbf{K}$ is the covariance matrix for the observed inputs, defined as $\\mathbf{K}=[k(x_{i},x_{j})]_{1\\leq i,j\\leq k-1}$ . This enables the GP to update its beliefs about the posterior prediction with every new observation, progressively improving the estimation. ", "page_idx": 4}, {"type": "text", "text": "Acquisition function. The acquisition function plays a critical role in BO by guiding the selection of the next source training task. At each decision step $k$ , the task $x_{k}$ is chosen by maximizing the acquisition function, as denoted by $x_{k}=\\arg\\operatorname*{max}_{x}a(x;x_{1:k-1})$ . One effective strategy employed in the acquisition function is the upper confidence bound (UCB) acquisition function, which considers the trade-off between the expected performance of a task based on the current task (exploitation) and the measure of uncertainty associated with the task\u2019s outcome (exploration) [40]. Especially in our case, the acquisition function can be designed as UCB function subtracted by generalization gap and so-far best performance. It is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\na(x;x_{1:k-1})=\\mathbb{E}_{x^{\\prime}\\in X}[[\\mu_{k-1}(x)+\\beta_{k}^{1/2}\\sigma_{k-1}(x)-\\Delta\\hat{J}(\\pi_{x},x^{\\prime})-J(\\pi_{1:k-1},x^{\\prime})]_{+}]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $[\\cdot]_{+}$ represents $\\operatorname*{max}(\\cdot,0)$ and we can use various forms of $\\beta_{k}$ , which is the trade-off parameter between exploitation and exploration. ", "page_idx": 4}, {"type": "text", "text": "4.3 Regret analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use regret to quantify the effectiveness of our source task selection based on BO. Specifically, we define regret at iteration $k$ as $r_{k}=\\hat{V}(x_{k}^{*};\\pi_{1:k-1})-\\hat{V}(x_{k};\\pi_{1:k-1})$ , where $\\hat{V}(x_{k}^{*};\\pi_{1:k-1})$ represents the maximum generalization performance achievable across all tasks, and $\\hat{V}(x_{k};\\pi_{1:k-1})$ is the performance at the current task selection $x_{k}$ . Consequently, the cumulative regret after $K$ iterations is given by $\\begin{array}{r}{R_{K}\\,=\\,\\sum_{k=1}^{K}r_{k}}\\end{array}$ , summing the individual regrets over all iterations. Following the framework presented by Srinivas et al. [40], our goal is to establish that this cumulative regret grows sublinearly with respect to the number of iterations. Mathematically, we aim to prove that $\\begin{array}{r}{\\bar{\\mathrm{lim}}_{K\\rightarrow\\infty}\\,\\frac{R_{K}}{K}\\,=\\,\\dot{0}}\\end{array}$ RKK = 0, indicating that, on average, the performance of our strategy approaches the optimal performance as the number of iterations increases. ", "page_idx": 4}, {"type": "text", "text": "Regret of MBTL. Having established the general framework for regret analysis, we now turn our attention to the specific regret properties of our MBTL algorithm. To analyze the regret of MBTL, consider the scaling factor for the UCB acquisition function given by $\\beta_{k}\\stackrel{.}{=}2\\log(|\\bar{X}|\\pi^{2}k^{2}/6\\delta)$ in Equation (4). It is designed to achieve sublinear regret with high probability, aligning with the theoretical guarantees outlined in Theorem 1 and 5 from [40]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Sublinear Regret). Give\u221an $\\delta\\,\\in\\,(0,1)$ , and with the scaling factor $\\beta_{k}$ as defined, the cumulative regret $R_{K}$ is bounded by $\\sqrt{K C_{1}\\beta_{K}\\gamma_{K}}$ with a probability of at least $1-\\delta$ . The formal expression of this probability is $P r$ $\\left[R_{K}\\leq\\sqrt{K C_{1}\\beta_{K}\\gamma_{K}}\\right]\\geq1-\\delta$ , where $\\begin{array}{r}{C_{1}:=\\frac{8}{\\log(1+\\sigma^{-2})}^{\\circ}\\ge8\\sigma^{2}}\\end{array}$ and $\\gamma_{K}=\\mathcal{O}(\\log K)$ for the squared exponential kernel. ", "page_idx": 4}, {"type": "text", "text": "Impact of search space elimination. In this section, we demonstrate that strategic reduction of the possible sets, guided by insights from previous task selections or source task training performance, leads to significantly tighter regret bounds than Theorem 1. By focusing on the most promising regions of the task space, our approach enhances learning efficiency and maximizes the policy\u2019s performance and applicability. Given the generalization gap observed in Figure 1, we observe that performance loss decreases as the context similarity increases. We model the degradation from the source task using a linear function in Assumption 1. Training on the source task can solve a significant portion of the remaining tasks. Our method progressively eliminates partitions of the task space at a certain rate with each iteration. If the source task selected in the previous steps could solve the remaining target task sufficiently, we can eliminate the search space at a desirable rate. Formally, we can define the search space at $k$ -th iteration as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Search space). We define the search space $X_{k}$ at iteration $k$ as a subset of $X$ , with each element $x^{\\prime}\\in X_{k}$ , such that $J(\\pi_{1:k-1},x^{\\prime})\\le\\hat{J}(\\pi_{x_{k}},x^{\\prime})\\!-\\!\\Delta J(\\pi_{x_{k}},x^{\\prime})$ . ", "page_idx": 5}, {"type": "text", "text": "Given the generalization gap observed in Figure 1, we model the degradation from the source task using a linear function in Assumption 1. While the figure might not strictly appear linear, the linear approximation simplifies analysis and is supported by empirical observations. Training on the source task can solve a significant portion of the remaining tasks. Our method progressively eliminates partitions of the task space at a certain rate with each iteration. If the source task selection in the previous step sufficiently addresses the remaining target tasks, we can reduce the search space at a desirable rate. Consequently, at each step, we effectively focus on a reduced search space. ", "page_idx": 5}, {"type": "text", "text": "We leverage the reduced uncertainty in well-sampled regions to tighten the regret bound while slightly lowering the probability $\\delta$ in Theorem 1. For the regret analysis, we propose the following theorem based on the generalization of Lemma 5.2 and 5.4 in [40] to the eliminated search space. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. For a given $\\delta^{\\prime}\\in(0,1)$ and scaling factor $\\beta_{k}=2\\log(|X|\\pi^{2}k^{2}/6\\delta)$ , the cumulative regret $R_{K}$ is bounded by $\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\sum_{k=1}^{K}\\left(\\frac{|X_{k}|}{|X|}\\right)^{2}}$ with probability at least $1-\\delta^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "Here, $|X|$ denotes the cardinality of the set $X$ , the number of elements in $X$ . Theorem 2 matches the Theorem 1 when $X_{k}=X$ for all $k$ . This theorem implies that regret has a tighter or equivalent bound if we can design the smaller search space instead of searching the whole space. The comprehensive proof is provided in Appendix A.3.1. ", "page_idx": 5}, {"type": "text", "text": "Here are some examples of restricted search space: If we consider an example where $\\begin{array}{r}{|X_{k}|=\\frac{1}{\\sqrt{k}}|X|}\\end{array}$ , the regret can be bounded tighter than that of Theorem 1. ", "page_idx": 5}, {"type": "text", "text": "Corollary 2.1. Consider $\\begin{array}{r}{|X_{k}|=\\frac{1}{\\sqrt{k}}|X|}\\end{array}$ . The regret bound would be $R_{K}\\le\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\log K}$ with a probability of at least $1-\\delta^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "In cases where the search space is defined using MBTLGS, the largest segment\u2019s length would reduce geometrically, described by $|X_{k}|\\leq2^{-\\lfloor\\log_{2}k\\rfloor}|X|$ . ", "page_idx": 5}, {"type": "text", "text": "Corollary 2.2. The regret bound for the $\\left|X_{k}\\right|\\quad\\leq$ $2^{-\\lfloor\\log_{2}k\\rfloor}|X|$ would be $R_{K}\\leq\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\pi^{2}/6}$ with a probability of at least $1-\\delta^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "Proofs for Corollaries 2.1 and 2.2 are provided in Appendix A.3.2 and A.3.3, respectively. Based on our experiments presented in Section 5, the rate of elimination of the largest segment for MBTL is shown in Figure 3. ", "page_idx": 5}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/178486e4100eca982788db6335ccde94174f44da6453916b372d6928f995803f.jpg", "img_caption": ["Figure 3: Empirical results of the restriction of search space by MBTL compared to two examples from Corollaries 2.1 and 2.2. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Experiments and analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our experiments consider CMDPs that span standard and real-world benchmarks. In particular, we consider standard continuous control benchmarks from the CARL library [6]. In addition, we study problems from RL for intelligent transportation systems, using [47] to model the CMDPs. Surprisingly, despite the relatively low complexity of the CMDPs considered, standard deep RL algorithms appear to struggle to solve the tasks. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We consider two types of baselines when evaluating our proposed algorithm: canonical and multi-policy. The canonical baselines are selected to validate multi-policy training; the multi-policy training baselines are heuristic methods designed to validate the Bayesian optimization approach. The canonical baselines include: (1) Independent training, which involves independently training separate models on each task; and (2) Multi-task RL, where a single \u201cuniversal\u201d context-conditioned policy is trained for all tasks. The multi-policy baselines include: (3) Random selection, where each training task is chosen uniformly at randomly; (4) Equidistant strategy, which selects training tasks by equally subdividing the context space based on a given training budget, and then trains them in lexicographical order; (5) Greedy strategy, which greedily selects the next source task based on Assumption 1; and (6) Sequential Oracle transfer, which assumes full knowledge of generalized performance for all policies and sequentially selects the best performing source task for each target task. ", "page_idx": 5}, {"type": "table", "img_path": "KLv1VLuMo8/tmp/40d69971b1bc6702b605f588b72f92fad5d5e5ef261139eb21ca43ac19c3e296.jpg", "table_caption": ["Table 1: Comparative performance of different methods on traffic CMDPs "], "table_footnote": ["\u2020Higher the better. Bold values represent the highest value(s) within the statistically significant range for each CMDP, excluding the oracle. Detailed results with variance for each method are provided in Appendix A.4.2. \u2021AA: Advisory autonomy benchmark, Ring: Single lane ring, Ramp: Highway ramp, Acc: Acceleration guidance, Vel: Speed guidance. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Proposed method. Our proposed method is MBTL, sequentially selecting source task based on the GP regression and acquisition function with the scaling factor $\\dot{\\beta_{k}}=2\\log\\!\\left(|X|\\pi^{2}k^{2}/6\\delta\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "DRL algorithms and performance measure. We utilize Deep Q-Networks (DQN) for discrete action spaces [29] and Proximal Policy Optimization (PPO) for continuous action spaces [35]. We evaluate our method by the average performance across all $N$ target tasks after training up to $K=15$ source tasks or the number of source tasks needed to achieve a certain level of performance. We employ min-max normalization of the rewards for each task, and we provide comprehensive details about our model in Appendix A.4.1. ", "page_idx": 6}, {"type": "text", "text": "5.2 Traffic benchmark experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider three traffic control domains. First, while most traffic lights still operate on fixed schedules, RL can be used to design adaptive (1) Traffic signal control to optimize traffic [9, 23]. However, considering that every intersection is different, challenges persist in generalizing across intersection configurations [19]. Given the significant portion of greenhouse gas emissions in the United States due to transportation [12], the second traffic domain is (2) Dynamic eco-driving at signalized intersections, which concerns learning energy-efficient driving strategies in urban settings. DRL-based eco-driving strategies have been developed [14, 45, 18] but still experience difficulties in generalization. Our final traffic domain is (3) Advisory autonomy, in which real-time speed or acceleration advisories guide human drivers to act as vehicle-based traffic controllers in mixed traffic environments [39, 8, 16]. The context space $X$ is discretized into $N=\\{50,50,40\\}$ contexts for the three domains, respectively. In Appendix A.4, we provide details about our experiments. ", "page_idx": 6}, {"type": "text", "text": "Results. Table 1 and Figure 4 summarize the results. Notably, the Oracle far outperforms the standard baselines (Independent and Multi-task), indicating the potential for transfer learning and intelligent training of multiple models, respectively. MBTL rapidly approaches the Oracle, within $\\approx10$ transfer steps, indicating that the Gaussian process effectively models the training performance and linear generalization gap models generalization performance. It is important to note that multi-task RL studies commonly consider Independent training as a strong baseline due to its avoidance of negative transfer and other training instability issues. Indeed, Independent often (but not always) outperforms Multi-task in our experiments. Yet, our experiments show that MBTL consistently outperforms both Independent and Multi-task and matches their performance with $\\mathbf{5-25x}$ better sample efficiency. Among the sequential baselines, MBTL consistently outperforms the heuristic baselines, indicating the value of adaptively selecting source tasks based on feedback. The sequential baselines, such as random, equidistant, and greedy strategy, also generally outperform Independent and Multi-task, indicating the general value of multi-policy training for solving CMDPs. More results are provided in Appendix A.4. ", "page_idx": 6}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/d2e07b50855864aa9773f866228601d72c9084aea502d948b0b0340bf992fa13.jpg", "img_caption": ["Figure 4: Traffic CMDP results. Method comparison of normalized performance over $N$ tasks. MBTL efficiently selects source training tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, indicating up to $25\\mathrm{x}$ fewer samples needed. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "KLv1VLuMo8/tmp/466d45799ba95ed902558ee613021f47bd5e52849ea3cd3e1cb6d2f69b3bca3a.jpg", "table_caption": ["Table 2: Comparative performance of different methods on standard control CMDPs "], "table_footnote": ["\u2020Higher the better. Bold values represent the highest value(s) within the statistically significant range for each CMDP, excluding the oracle. Detailed results with variance for each method are provided in Appendix A.4.2. "], "page_idx": 7}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/343ae125898503c2eb70e2cdcf872e6145b1fa400c9e7b7d1c47bc353ee3ce7c.jpg", "img_caption": ["Figure 5: Continuous control CMDP results. Method comparison of normalized performance over $N$ tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, indicating up to $50\\mathrm{x}$ fewer samples needed. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.3 Continuous control benchmark experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To probe the generality of MBTL, we utilize context-extended versions of standard RL environments from CARL benchmark library [6] to evaluate our methods under varied contexts. For the Cartpole domain, we considered CMDPs with varying cart mass, pole length, and pole mass. In Pendulum, we vary the timestep duration, pendulum length, and pendulum mass. The BipedalWalker was ", "page_idx": 7}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/7385be23c7ed52ac8cff8e2aee415e18cccee3833e796d8328f61957b7545c90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: The GP sequentially updates estimates of the performance function (blue) based on previously trained models. Then, MBTL selects the next source task that maximizes the acquisition function (red). (CMDP: Pendulum (Time step)). ", "page_idx": 8}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/cc5e58a25a0ba506f29a512c848eb2194b66421918fc2c1c219bac4bec85bc7f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 7: Sensitivity analysis on the DRL algorithm underlying MBTL (DQN, PPO, and A2C), tested on Cartpole with varying length of pole. MBTL remains effective. ", "page_idx": 8}, {"type": "text", "text": "tested under varying friction, gravity, and scale. In HalfCheetah domain, we manipulated friction, gravity, and stiffness parameters. These variations critically influence the dynamics and physics of the environments. The range of context variations was selected by scaling the default values specified in CARL from 0.1 to 10 times ( $N=100)$ ), enabling a comprehensive analysis of transfer learning under drastically different conditions. We provide more details in Appendix A.4. ", "page_idx": 8}, {"type": "text", "text": "Results. The results summarized in Table 2 demonstrate MBTL\u2019s sample efficiency and competitive performance across diverse control domains, often closely trailing the Oracle only with the small number of tasks. While the Cartpole domain shows the best performance by Independent training, MBTL shows the second-best performance. Figure 5 shows that MBTL demonstrates rapid learning curves, reaching high normalized performance with relatively few samples. MBTL methods not only outperform baseline methods in terms of sample efficiency but also achieve results close to the Oracle with fewer training samples, underscoring the potential of MBTL in solving control domains with varying contexts. Specifically, Figure 6 illustrates the detailed process of how MBTL utilizes GP for performance estimation and chooses the next source task that maximizes the acquisition function that evaluates the expected improvement of generalized performance. MBTL achieves comparable performance to multi-task or independent baselines with ${8{-}50\\mathrm{x}}$ fewer samples, highlighting its effectiveness in reducing training requirements. ", "page_idx": 8}, {"type": "text", "text": "5.3.1 Sensitivity analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "DRL algorithms. Figure 7 shows that MBTL remains effective with different underlying DRL algorithms\u2014DQN, PPO, and Advantage Actor-Critic (A2C) [30]\u2014used for single-task training. ", "page_idx": 8}, {"type": "text", "text": "Acquisition functions. Figure 8 assesses the role of acquisition functions in Bayesian optimization. While expected improvement (EI) focuses on promising marginal gains beyond the current best, UCB utilizes both mean and ", "page_idx": 8}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/e6dd40df61e78d2be51684f744326120a1c4a40384bbcede6ed4d4183f6e6240.jpg", "img_caption": ["Figure 8: Sensitivity analysis on acquisition functions. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "variance for balancing exploration and exploitation. Overall, we find that MBTL is not particularly sensitive to the choice of optimism representation in the acquisition function, which indicates that MBTL has a weak dependence on hyperparameters. ", "page_idx": 9}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Contextual Reinforcement Learning. Robustness and generalization challenges in DRL are generally addressed by a few common techniques in the literature. The broader umbrella of such methods falls under CRL [6]. CRL utilizes side information about the problem variations to improve the generalization and robustness. In particular, CRL formalizes generalization in DRL using CMDPs [15, 32, 6], which incorporate context-dependent dynamics, rewards, and initial state distributions into the formalism of MDPs. This provides a structured framework to study generalization in DRL [15, 32]. The contexts of CMDPs are not always visible during training [22]. When they are visible, they can be directly used as side information by conditioning the policy on them [38]. ", "page_idx": 9}, {"type": "text", "text": "Multi-task learning. A common approach is multi-task learning, which aims to leverage a common structure between tasks. The structure between tasks has been exploited using policy sketches for task structuring [2] and by sharing a distilled policy that captures common behavior across tasks [44]. One of the known challenges in multi-task learning is that when the context features are not visible, the resulting CMDP becomes a partially observable CMDP [22, 10] and can be challenging for multi-task learning. Moreover, another challenge is negative transfer, where training from tasks that are too different results in training instability or failure [21, 41, 42]. In this work, we use multi-task learning as a baseline in evaluating our methods. ", "page_idx": 9}, {"type": "text", "text": "Zero-shot transfer and policy reuse. Zero-shot transfer is another commonly used technique that focuses on adapting models trained in one environment to perform effectively in unseen settings without further training [22]. Li et al. [25] introduces context-aware policy reuse, which adapts multiple source policies based on context, enhancing generalization in complex environments. RezaeiShoshtari et al. [34] utilize hypernetworks to approximate an RL algorithm as a mapping from parameterized CMDP to a family of near-optimal solutions to achieve zero-shot transfer across varying task conditions. Sinapov et al. [37] use meta-data to learn inter-task transferability to learn the expected benefit of transfer given a source-target task pair. Bao et al. [3] propose a metric for evaluating transferability based on information-theoretic feature representations across tasks. Another popular class of techniques for robust and generalizable DRL is domain adaptation strategies. It is often used in making learned policies adapt to new domains with varying dynamics, observations, or rewards [5]. For instance, Xing et al. [46] first learn a latent unified state representation for different domains and subsequently train DRL agents in the source domain based on that. However, our focus in this work centered around in-distribution generalization, whereas domain adaptation techniques are commonly used for out-of-distribution generalization. ", "page_idx": 9}, {"type": "text", "text": "Source task selection. In the context of transfer learning, selecting appropriate source tasks is crucial. Li and Zhang [24] proposes an optimal online method for dynamically selecting the most relevant single source policy in reinforcement learning. Beyond RL, Meiseles and Rokach [28] emphasizes structural alignment in time-series source models to prevent performance degradation, while Poth et al. [33] finds that selecting aligned intermediate tasks in natural language processing boosts transfer effectiveness. Together, these studies show the importance of context-aligned source tasks selection to improve transferability and reduce negative transfer. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study introduces a method called Model-based Transfer Learning (MBTL), which layers on top of existing RL methods to effectively solve CMDPs. Rather than independent or multi-task training, which trains $N$ or 1 models, respectively, MBTL intelligently selects an intermediate number of models to train. MBTL has two key components: an explicit model of the generalization gap and a Gaussian process component to estimate training performance. MBTL achieves up to $50\\mathrm{x}$ improved sample efficiency on standard and real-world benchmarks. Furthermore, MBTL achieves sublinear regret in the number of training tasks. A limitation is that MBTL is designed for a single-dimensional context variation. Promising directions of future work include studying high-dimensional context spaces and out-of-distribution generalization, as well as the development of new real-world CMDP benchmarks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper. This work was supported by the National Science Foundation (NSF) CAREER award (#2239566), the Kwanjeong Educational Foundation Ph.D. scholarship program, and an Amazon Robotics Ph.D. Fellowship. The authors would like to thank the anonymous reviewers for their valuable feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abubakr O Al-Abbasi, Arnob Ghosh, and Vaneet Aggarwal. Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems, 20(12):4714\u20134727, 2019.   \n[2] Jacob Andreas, Dan Klein, and Sergey Levine. Modular Multitask Reinforcement Learning with Policy Sketches. In Proceedings of the 34th International Conference on Machine Learning, pages 166\u2013175. PMLR, July 2017. ISSN: 2640-3498.   \n[3] Yajie Bao, Yang Li, Shao-Lun Huang, Lin Zhang, Lizhong Zheng, Amir Zamir, and Leonidas Guibas. An Information-Theoretic Approach to Transferability in Task Transfer Learning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 2309\u20132313, Taipei, Taiwan, September 2019. IEEE. ISBN 978-1-5386-6249-6. doi: 10.1109/ICIP.2019.8803726.   \n[4] Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Subhodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588(7836):77\u201382, 2020.   \n[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.   \n[6] Carolin Benjamins, Theresa Eimer, Frederik Schubert, Aditya Mohan, Sebastian D\u00f6hler, Andr\u00e9 Biedenkapp, Bodo Rosenhahn, Frank Hutter, and Marius Lindauer. Contextualize Me \u2013 The Case for Context in Reinforcement Learning. Transactions on Machine Learning Research, June 2023.   \n[7] Eric Brochu, Vlad M. Cora, and Nando de Freitas. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning, December 2010.   \n[8] Jung-Hoon Cho, Sirui Li, Jeongyun Kim, and Cathy Wu. Temporal transfer learning for traffic optimization with coarse-grained advisory autonomy. arXiv preprint arXiv:2312.09436, 2023.   \n[9] Tianshu Chu, Jie Wang, Lara Codeca, and Zhaojian Li. Multi-Agent Deep Reinforcement Learning for Large-Scale Traffic Signal Control. IEEE Transactions on Intelligent Transportation Systems, 21(3): 1086\u20131095, March 2020. ISSN 1524-9050, 1558-0016. doi: 10.1109/TITS.2019.2901791.   \n[10] Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pages 2048\u20132056. PMLR, 2020.   \n[11] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414\u2013419, 2022.   \n[12] US EPA. Sources of Greenhouse Gas Emissions, 2023. URL https://www.epa.gov/ghgemissions/ sources-greenhouse-gas-emissions.   \n[13] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022.   \n[14] Qiangqiang Guo, Ohay Angah, Zhijun Liu, and Xuegang (Jeff) Ban. Hybrid deep reinforcement learning based eco-driving for low-level connected and automated vehicles along signalized corridors. Transportation Research Part C: Emerging Technologies, 124:102980, March 2021. ISSN 0968090X. doi: 10.1016/j.trc.2021.102980.   \n[15] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv preprint arXiv:1502.02259, 2015.   \n[16] Aamir Hasan, Neeloy Chakraborty, Haonan Chen, Jung-Hoon Cho, Cathy Wu, and Katherine DriggsCampbell. Cooperative advisory residual policies for congestion mitigation. Journal on Autonomous Transportation Systems, 2024.   \n[17] Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: Improving Zero-Shot Transfer in Reinforcement Learning. In Proceedings of the 34th International Conference on Machine Learning, pages 1480\u20131490. PMLR, July 2017. ISSN: 2640-3498.   \n[18] Vindula Jayawardana and Cathy Wu. Learning eco-driving strategies at signalized intersections. In 2022 European Control Conference (ECC), pages 383\u2013390. IEEE, 2022.   \n[19] Vindula Jayawardana, Catherine Tang, Sirui Li, Dajiang Suo, and Cathy Wu. The Impact of Task Underspecification in Evaluating Deep Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 35, pages 23881\u201323893. Curran Associates, Inc., 2022.   \n[20] Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Edgar Sanchez, Catherine Tang, Mark Taylor, Blaine Leonard, and Cathy Wu. Mitigating metropolitan carbon emissions with dynamic eco-driving at scale. arXiv preprint arXiv:2408.05609, 2024.   \n[21] Zhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 521\u2013528, 2011.   \n[22] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt\u00e4schel. A survey of zero-shot generalisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76:201\u2013264, 2023.   \n[23] Li Li, Yisheng Lv, and Fei-Yue Wang. Traffic signal timing via deep reinforcement learning. IEEE/CAA Journal of Automatica Sinica, 3(3):247\u2013254, July 2016. ISSN 2329-9266, 2329-9274. doi: 10.1109/JAS. 2016.7508798.   \n[24] Siyuan Li and Chongjie Zhang. An Optimal Online Method of Selecting Source Policies for Reinforcement Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), April 2018. ISSN 2374- 3468, 2159-5399. doi: 10.1609/aaai.v32i1.11718. URL https://ojs.aaai.org/index.php/AAAI/ article/view/11718.   \n[25] Siyuan Li, Fangda Gu, Guangxiang Zhu, and Chongjie Zhang. Context-aware policy reuse. arXiv preprint arXiv:1806.03793, 2018.   \n[26] Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Fl\u00f6tter\u00f6d, Robert Hilbrich, Leonhard L\u00fccken, Johannes Rummel, Peter Wagner, and Evamarie Wie\u00dfner. Microscopic traffic simulation using sumo. In The 21st IEEE International Conference on Intelligent Transportation Systems. IEEE, 2018.   \n[27] Daniel J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618(7964):257\u2013263, 2023.   \n[28] Amiel Meiseles and Lior Rokach. Source model selection for deep learning in the time series domain. IEEE Access, 8:6190\u20136200, 2020.   \n[29] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, February 2015. ISSN 0028-0836, 1476-4687. doi: 10.1038/nature14236.   \n[30] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In Proceedings of The 33rd International Conference on Machine Learning, pages 1928\u20131937. PMLR, June 2016. ISSN: 1938-7228.   \n[31] Jonas Mockus. Bayesian Approach to Global Optimization: Theory and Applications, volume 37 of Mathematics and Its Applications. Springer Netherlands, Dordrecht, 1989. ISBN 978-94-010-6898-7 978-94-009-0909-0. doi: 10.1007/978-94-009-0909-0.   \n[32] Aditya Modi, Nan Jiang, Satinder Singh, and Ambuj Tewari. Markov Decision Processes with Continuous Side Information. In Proceedings of Algorithmic Learning Theory, pages 597\u2013618. PMLR, April 2018. ISSN: 2640-3498.   \n[33] Clifton Poth, Jonas Pfeiffer, Andreas R\u00fcckl\u00e9, and Iryna Gurevych. What to pre-train on? efficient intermediate task selection. arXiv preprint arXiv:2104.08247, 2021.   \n[34] Sahand Rezaei-Shoshtari, Charlotte Morissette, Francois R. Hogan, Gregory Dudek, and David Meger. Hypernetworks for Zero-Shot Transfer in Reinforcement Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 37(8):9579\u20139587, June 2023. ISSN 2374-3468, 2159-5399. doi: 10.1609/aaai. v37i8.26146.   \n[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017.   \n[36] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, January 2016. ISSN 1476-4687. doi: 10.1038/nature16961.   \n[37] Jivko Sinapov, Sanmit Narvekar, Matteo Leonetti, and Peter Stone. Learning Inter-Task Transferability in the Absence of Target Task Samples. In Proceedings of the 14th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2015), Istanbul, Turkey, May 2015.   \n[38] Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with context-based representations. In International Conference on Machine Learning, pages 9767\u20139779. PMLR, 2021.   \n[39] Mayuri Sridhar and Cathy Wu. Piecewise Constant Policies for Human-Compatible Congestion Mitigation. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pages 2499\u20132505, Indianapolis, IN, USA, September 2021. IEEE. ISBN 978-1-72819-142-3. doi: 10.1109/ITSC48978.2021. 9564789.   \n[40] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting. IEEE Transactions on Information Theory, 58(5):3250\u20133265, May 2012. ISSN 0018-9448, 1557-9654. doi: 10.1109/TIT.2011.2182033.   \n[41] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9120\u20139132. PMLR, 13\u201318 Jul 2020. URL https://proceedings. mlr.press/v119/standley20a.html.   \n[42] Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning what to share for efficient deep multi-task learning. Advances in Neural Information Processing Systems, 33:8728\u20138740, 2020.   \n[43] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. Adaptive computation and machine learning series. The MIT Press, Cambridge, Massachusetts, second edition edition, 2018. ISBN 978-0-262-03924-6.   \n[44] Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[45] Marius Wegener, Lucas Koch, Markus Eisenbarth, and Jakob Andert. Automated eco-driving in urban scenarios using deep reinforcement learning. Transportation Research Part C: Emerging Technologies, 126:102967, May 2021. ISSN 0968090X. doi: 10.1016/j.trc.2021.102967.   \n[46] Jinwei Xing, Takashi Nagata, Kexin Chen, Xinyun Zou, Emre Neftci, and Jeffrey L Krichmar. Domain adaptation in reinforcement learning via latent unified state representation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10452\u201310459, 2021.   \n[47] Zhongxia Yan, Abdul Rahman Kreidieh, Eugene Vinitsky, Alexandre M. Bayen, and Cathy Wu. Unified Automatic Control of Vehicular Systems With Reinforcement Learning. IEEE Transactions on Automation Science and Engineering, pages 1\u201316, 2022. ISSN 1545-5955, 1558-3783. doi: 10.1109/TASE.2022. 3168621. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Notation . . 14   \nA.2 Model-Based Transfer Learning (MBTL) Algorithm 16   \nA.3 Theoretical analysis . . . 16   \nA.3.1 Proof of Theorem 2 . . 16   \nA.3.2 Proof of Corollary 2.1 16   \nA.3.3 Proof of Corollary 2.2 17   \nA.4 Experiment details 17   \nA.4.1 Details about Gaussian process (GP) . . 17   \nA.4.2 Results of table with standard deviation . . 17   \nA.4.3 Detailed sample complexity comparison results . . 19   \nA.4.4 Details about traffic signal control . . 19   \nA.4.5 Details about eco-driving control . 22   \nA.4.6 Details about advisory autonomy task . 24   \nA.4.7 Details of control tasks . 25   \nA.4.8 Details about Cartpole 25   \nA.4.9 Details about Pendulum 27   \nA.4.10 Details about BipedalWalker . 28   \nA.4.11 Details about HalfCheetah . 29   \nA.5 Potential impacts . 30 ", "page_idx": 13}, {"type": "text", "text": "A.1 Notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 3 describes the notation used in this paper. ", "page_idx": 13}, {"type": "table", "img_path": "KLv1VLuMo8/tmp/ab934e86fd4583203f25e5b8c1ff67baf78b767f68de6640f94cf256bb75ccbb.jpg", "table_caption": ["Table 3: Notation used in the problem formulation "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 9 helps understand the discrepancy between the observed generalized performance and the predicted one. Figure 10 illustrates how to calculate the marginal improvement of expected generalized performance $(\\hat{V}(x;x_{1:k-1})-V(x;x_{1:k-1}))$ . ", "page_idx": 13}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/452861d7feca7b1856e83dd22904d3ebb4a342ae241bb37f72f5704e6b28e833.jpg", "img_caption": ["Target task $x^{\\prime}$ "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 9: Illustration of the discrepancy between observed $(J)$ and predicted $(\\hat{J})$ generalized performance after training on source task $x_{1}$ and attempting zero-shot transfer to $x^{\\prime}$ . ", "page_idx": 14}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/313b53344a245c2a66fcc821866966518f30c419d40e77b6337b1f7296f85c6c.jpg", "img_caption": ["Target task $x^{\\prime}$ "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 10: Step for choosing $x_{2}$ that maximizes the estimated marginal improvement $(\\hat{V}(x;\\pi_{1})-$ $V(x_{1}))$ . $\\hat{V}(x;\\pi_{1})$ corresponds to the red area under the red line and $V(x_{1})$ as the area under $J(\\pi_{1},x^{\\prime})$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Model-Based Transfer Learning (MBTL) Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/a3f9d936843054135b41e3698afb4cf839647ba73453d5911398a99119bc7eb4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.3 Theoretical analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.3.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 2. For a given $\\delta^{\\prime}\\in(0,1)$ and scaling factor $\\beta_{k}=2\\log(|X|\\pi^{2}k^{2}/6\\delta)$ , the cumulative regret $R_{K}$ is bounded by $\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\sum_{k=1}^{K}\\left(\\frac{|X_{k}|}{|X|}\\right)^{2}}$ with probability at least $1-\\delta^{\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. The following Lemma 3 and 4 is basically considering the cardinality of restricted search space of $X_{k}$ instead of $X$ upon the lemmas in literature [40]. ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. For $t\\geq1,\\,i f\\,|f(x)-\\mu_{k-1}(x)|\\leq\\beta_{k}^{1/2}\\sigma_{k-1}(x)\\quad\\forall x\\in X_{k},$ , then the regret $r_{t}$ is bounded by $2|X_{k}|\\beta_{k}^{1/2}\\sigma_{k-1}(x)/|X|$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 4. Setting $\\delta\\,\\in\\,(0,1)$ $[0,1),\\;\\beta_{k}\\,=\\,2\\log(|X|\\pi^{2}k^{2}/6\\delta)$ , and $\\begin{array}{r}{C_{1}:=\\frac{8}{\\log(1+\\sigma^{-2})}\\geq8\\sigma^{2}}\\end{array}$ , we have $\\begin{array}{r}{P r\\left[\\sum_{k=1}^{K}r_{k}\\left(\\frac{|X|}{|X_{k}|}\\right)^{2}\\leq C_{1}\\beta_{K}\\gamma_{K}\\quad\\forall K\\geq1\\right]\\geq1-\\delta.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Using Lemma 3, Lemma 5.3 in [40], Lemma 4, and Cauchy\u2013Schwarz inequality, we can bound the cumulative regret as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{K}=\\sum_{k=1}^{K}r_{k}\\leq\\sqrt{\\sum_{k=1}^{K}r_{k}\\left(\\frac{|X|}{|X_{k}|}\\right)^{2}\\sum_{k=1}^{K}\\left(\\frac{|X_{k}|}{|X|}\\right)^{2}}\\leq\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\sum_{k=1}^{K}\\left(\\frac{|X_{k}|}{|X|}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.3.2 Proof of Corollary 2.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Corollary 2.1. Consider $\\begin{array}{r}{|X_{k}|=\\frac{1}{\\sqrt{k}}|X|}\\end{array}$ . The regret bound would be $R_{K}\\le\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\log K}$ with a probability of at least $1-\\delta^{\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Recall that $\\textstyle\\sum_{k=1}^{K}{\\frac{1}{k}}\\leq\\log K$ . Calculating the sum of squares for the reduced segments, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}|X_{k}|^{2}=\\sum_{k=1}^{K}\\frac{1}{k}|X|^{2}\\leq|X|^{2}\\log K\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The cumulative regret can be bounded as below: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{K}=\\sum_{k=1}^{K}r_{k}\\leq\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\sum_{k=1}^{K}\\left(\\frac{\\left|X_{k}\\right|}{\\left|X\\right|}\\right)^{2}}\\leq\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\log K}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3.3 Proof of Corollary 2.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Corollary 2.2. The regret bound for the $|X_{k}|\\leq2^{-\\lfloor\\log_{2}k\\rfloor}|X|$ would be $R_{K}\\,\\leq\\,\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\pi^{2}/6}$ with a probability of at least $1-\\delta^{\\prime}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Calculating the sum of squares for the reduced segments, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}|X_{k}|^{2}=\\sum_{k=1}^{K}2^{-2\\left\\lfloor\\log_{2}k\\right\\rfloor}|X|^{2}\\leq{\\frac{1}{k^{2}}}|X|^{2}\\leq{\\frac{\\pi^{2}}{6}}|X|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The cumulative regret can be bounded as below: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{K}=\\sum_{k=1}^{K}r_{k}\\leq\\sqrt{C_{1}\\beta_{K}\\gamma_{K}\\sum_{k=1}^{K}\\left(\\frac{|X_{k}|}{|X|}\\right)^{2}}\\leq\\sqrt{\\frac{C_{1}\\beta_{K}\\gamma_{K}\\pi^{2}}{6}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.4 Experiment details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.4.1 Details about Gaussian process (GP) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our study, we conducted hyperparameter tuning experiments for Gaussian Process (GP) regression to optimize its performance. Specifically, we varied the noise standard deviation over the set $\\left\\lbrace0.001,0.01,0.1,1\\right\\rbrace$ and the number of restarts for the optimizer over the set $\\{5,6,7,8,9,10,11,12,13,14,15\\}$ with several kernel configurations. We set noise standard deviation as 0.1, the number of restarts for the optimizer over the set as 9, and the kernel as the combination of a constant kernel (C) with the bound from 0.001 to 1000 and a radial basis function kernel (RBF) with a length scale ranging from 0.01 to 100. The noise level was set to the square of the noise standard deviation $(0.1^{2})$ . ", "page_idx": 16}, {"type": "text", "text": "A.4.2 Results of table with standard deviation ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "KLv1VLuMo8/tmp/e2e246efb10cfff0d8cfb32c709655bc4418d7eb30a01917645c60b6c2121e0e.jpg", "table_caption": ["Table 4: Comparative performance of different methods on context-variant traffic and benchmark CMDPs "], "table_footnote": ["\\* Note: Bold values represent the highest value(s) within the statistically significant range for each task, excluding the oracle. Standard deviation across multiple runs in the parenthesis. $\\ddagger\\mathrm{AA}$ : Advisory autonomy tasks, Ring: Single lane ring, Ramp: Highway ramp, Acc: Acceleration guidance, Vel: Speed guidance. "], "page_idx": 17}, {"type": "text", "text": "A.4.3 Detailed sample complexity comparison results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 5 presents a comparison of sample complexity required for MBTL to perform as good as the best generalization performance of baselines (Independent training and multi-task training) across various tasks in the CMDP. Each row lists a different domain, the specific context variation applied (e.g., changes in physical properties or environmental parameters), and two key values: $k^{*}$ and $N$ , where $k^{*}$ represents the number of models required by MBTL to reach a performance level comparable to the baseline. This value is shown as a range (e.g., [3, 5, 3]), indicating results from three random seeds. $N$ represents the total number of contexts. The value $\\frac{N}{k^{*}}$ helps illustrate the sample efficiency of MBTL. ", "page_idx": 18}, {"type": "table", "img_path": "KLv1VLuMo8/tmp/aa3c3cf537968cfe3c309b241d94f597af3dbfdf45d0ae129195e1b46fd00c2f.jpg", "table_caption": ["Table 5: Sample complexity comparison to baseline performance on CMDP tasks "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.4.4 Details about traffic signal control ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Most traffic lights operate on fixed schedules, but adaptive traffic signal control using DRL can optimize the traffic flow using real-time information on the traffic [9, 23], though challenges persist in generalizing across various intersection configurations [19]. ", "page_idx": 18}, {"type": "text", "text": "Figure 11 showcases the layout of traffic networks used in a traffic signal control task with several lanes and a signalized intersection in the middle. The state space represents the presence of vehicles in discretized lane cells along the incoming roads. Actions determine which lane gets the green phase of the traffic signal, and rewards are based on changes in cumulative stopped time, the period when speed is zero. The global objective is to minimize the average waiting times at the intersection. Different configurations of intersections (e.g., road length, inflow, speed limits) are modeled to represent varying real-world conditions. ", "page_idx": 18}, {"type": "text", "text": "Training configuration We used the microscopic traffic simulation called Simulation of Urban MObility (SUMO) [26] v.1.16.0 and PPO for RL algorithm [35]. All experiments are done on a distributed computing cluster equipped with 48 Intel Xeon Platinum 8260 CPUs. ", "page_idx": 18}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/dbb0f211ecfa665cfefc8066fcbdc69ff5ecb87898746098cd90c108ab784bce.jpg", "img_caption": ["Figure 11: Illustration of the traffic networks in traffic signal control task. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Transferability heatmap Figure 12 presents heatmaps of transferability for different traffic signal control tasks, each varying a specific aspect: inflow, speed limit, and road length. The heatmaps display the effectiveness of strategy transfer from each source task (vertical axis) to each target task (horizontal axis). In terms of inflow variation, transferability drops when transferring from tasks with lower vehicle inflow to those with higher inflow. In speed limit variation, the transferability shows uniform effectiveness, suggesting less sensitivity to these changes. In road length variation, distinct blocks of high transferability indicate that different road lengths may require significantly tailored strategies. ", "page_idx": 19}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/a49f1c12617708838085de61090f8367c6c081074d4c6305fbcb990ce2031041.jpg", "img_caption": ["Figure 12: Examples of transferability heatmap for traffic signal control. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Results Figure 13 illustrates the normalized generalized performance across various traffic control tasks: inflow, speed limit, and road length. The plots display how different strategies adapt with increasing transfer steps: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Inflow: Performance improves as the number of transfer steps increases, with MBTL strategy consistently achieving the highest scores, demonstrating their effectiveness in adapting to changes in inflow conditions.   \n\u2022 Speed Limit: Here, performance levels are relatively stable across all strategies except for the multitask training.   \n\u2022 Road Length: There is a general upward trend in performance for all strategies, particularly for MBTL, indicating robustness in adapting to different road lengths. ", "page_idx": 19}, {"type": "text", "text": "This data suggests that MBTL and Oracle are particularly effective across varying conditions, maintaining higher levels of performance adaptability. ", "page_idx": 19}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/390b8a76ad3f6e1d3ccabc204507158c5753e3d743b836ed3c7b9945eb1fe0b0.jpg", "img_caption": ["Figure 13: Comparison of normalized generalized performance of all target tasks: Traffic signal control. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.4.5 Details about eco-driving control ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Given the significant portion of greenhouse gas emissions in the United States coming from the transportation sector [12], eco-driving behaviors are critical for climate change mitigation. Deep reinforcement learning-based eco-driving strategies have been developed [14, 45, 18, 20] but still have some issues of difficulties in generalization. We also extend to various intersection configurations with different traffic inflow rates, penetration rates of eco-driving systems, and durations of green phases at static traffic signals to optimize vehicle behaviors for reduced emissions. ", "page_idx": 21}, {"type": "text", "text": "Figure 14 illustrates the traffic road network used in the eco-driving control task. The road network is depicted as traffic flowing vertically and horizontally, crossing the static phase traffic signal. There are both guided and default vehicles in the system. The state space includes the speed and position of the ego vehicle, the leading vehicle, and the following vehicles, supplemented by the current traffic signal phase and relevant context features, including lane length and green phase durations. The action space specifically focuses on the ego vehicle\u2019s acceleration control. The reward mechanism is designed to optimize the driving strategy by balancing the average speed of the vehicles against penalties for emissions, thereby promoting eco-friendly driving behaviors within the traffic system. ", "page_idx": 21}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/5e9f5b5933a4802a580f6ffd530f8eb526c0adf8ef3a893bcb60ef22667a40ca.jpg", "img_caption": ["Training configuration We also used the microscopic traffic simulation called Simulation of Urban MObility (SUMO) [26] v.1.16.0 and PPO for RL algorithm [35]. ", "Figure 14: Illustration of the traffic networks in eco-driving control task. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Transferability heatmap Figure 15 displays heatmaps for the eco-driving control task, with each heatmap varying an aspect such as green phase, inflow, and penetration rate. These visuals illustrate the transferability of strategies from source tasks (vertical axis) to target tasks (horizontal axis), highlighting the impact of traffic light phases, vehicle inflow, and the proportion of guided vehicles on strategy effectiveness. Notably, longer green phases correlate with improved performance and transferability. For inflow variations, reduced inflow typically yields better outcomes. However, variations in the penetration rate of guided vehicles show minimal impact on performance differences. ", "page_idx": 21}, {"type": "text", "text": "Results Figure 16 illustrates the normalized generalized performance across variants of eco-driving control tasks, specifically looking at variations in green phase time, inflow, and penetration rate. The graphs depict performance enhancement over transfer steps for different strategies. Notably, MBTL consistently demonstrates superior performance across all variations, indicating robust adaptability to changing task parameters. ", "page_idx": 21}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/3e3faece44c135be207c126f472cc0db1e0d83a7759495ac2b8ae79188c9135c.jpg", "img_caption": ["Figure 15: Examples of transferability heatmap for eco-driving control. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/870c9334f36b3617759e55fd28f35f33893a312ecfd8ccd8cc0ef77a463884a7.jpg", "img_caption": ["Figure 16: Comparison of normalized generalized performance of all target tasks: Eco-driving control. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.4.6 Details about advisory autonomy task ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Advisory autonomy involves a real-time speed advisory system that enables human drivers to emulate the system-level performance of autonomous vehicles in mixed autonomy systems [39, 8, 16]. Instead of direct and instantaneous control, human drivers receive periodic guidance, which varies based on road type and guidance strategy. Here, we consider the different frequencies of this periodic guidance as contextual MDPs since the zero-order hold action affects the transition function. ", "page_idx": 23}, {"type": "text", "text": "Figure 17 illustrates two distinct traffic network configurations used in the advisory autonomy task: a single-lane ring and a highway ramp. The single-lane ring features 22 vehicles circulating the ring, with only one being actively controlled, presenting a relatively controlled environment for testing vehicle guidance systems. The highway ramp scenario introduces a more complex dynamic, where vehicles not only travel along the highway but also merge from ramps, creating potential stop-and-go traffic patterns that challenge the adaptability of autonomous guidance systems. ", "page_idx": 23}, {"type": "text", "text": "Problem Definition: In a single-lane scenario, the state space includes the speeds of the ego and leading vehicles, along with the headway. For highway ramp scenarios, additional states cover the relative positions and speeds of adjacent vehicles. Actions vary by guidance type: for acceleration guidance, the action space is continuous, ranging from $-1$ to 1; for speed guidance, it has ten discrete actions compared to the speed limit. Rewards are based on system throughput or average speed of all vehicles in the system. ", "page_idx": 23}, {"type": "text", "text": "Context Variations: We explore different durations of coarse-grained guidance holds to test various levels of human compatibility, adjusting the model based on observed driver behaviors and system performance. ", "page_idx": 23}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/34a7bbf0a02392c21a64cc3f7fe3acf892aa5349115b36a72983525bc1d338fc.jpg", "img_caption": ["Figure 17: Illustration of the traffic networks in advisory autonomy task. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Transferability heatmap Figure 18 showcases heatmaps of transferability for advisory autonomy tasks, each varying in specific aspects: acceleration guidance and speed guidance across a single lane ring and a highway ramp. These heatmaps demonstrate the effectiveness of strategy transfer from each source task (vertical axis) to each target task (horizontal axis), capturing how variations in task conditions influence adaptability. For acceleration guidance in a ring setup (a), transferability is generally higher among tasks with similar acceleration demands. In contrast, speed guidance on a ramp (d) reveals more variability in transferability, potentially due to the complexity of speed adjustments in ramp scenarios. ", "page_idx": 23}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/28fceeca681f068edf1cb5bfd4439828557d680e92815678cab1728e45a29653.jpg", "img_caption": ["(a) Ring with acceleration (b) Ring with speed guide (c) Ramp with acceleration (d) Ramp with speed guide guide guide ", "Figure 18: Examples of transferability heatmap for advisory autonomy. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Results Figure 19 illustrates the comparison of normalized generalized performance for advisory autonomy tasks, specifically acceleration and speed guidance in a ring and acceleration guidance on a ramp. The graphs demonstrate that MBTL consistently exhibits higher performance across all tasks. Particularly, acceleration guidance in both ring and ramp scenarios shows significant performance improvements over transfer steps, with MBTL closely matching in some instances. ", "page_idx": 24}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/4b340c9adc15ef63b27daf5a1fdd619d14d3a6dbd0b2b8c4405c4528e0de4fe1.jpg", "img_caption": ["Figure 19: Comparison of normalized generalized performance of all target tasks: Advisory autonomy. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "A.4.7 Details of control tasks ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For this experimental phase, we selected context-extended versions of standard RL environments from the CARL benchmark library, including Cartpole, Pendulum, BipedalWalker, and Halfcheetah. These environments were chosen to rigorously test the robustness and adaptability of our MBTL algorithm under varied conditions that mirror the complexity encountered in real-world scenarios. ", "page_idx": 24}, {"type": "text", "text": "Context Variations: In the Cartpole tasks, we explored CMDPs with varying cart masses, pole lengths, and pole masses. For the Pendulum, the experiments involved adjusting the timestep duration, pendulum length, and pendulum mass. The BipedalWalker was tested under different settings of friction, gravity, and scale. Similarly, in the Halfcheetah tasks, we manipulated parameters such as friction, gravity, and stiffness to simulate different physical conditions. These variations critically influence the dynamics and physics of the environments, thereby presenting unique challenges that test the algorithm\u2019s capacity to generalize from previous learning experiences without the need for extensive retraining. The range of context variations was established by scaling the default values specified in the CARL framework from 0.1 to 10 times, enabling a comprehensive examination of each model\u2019s performance under drastically different conditions. ", "page_idx": 24}, {"type": "text", "text": "License: CARL falls under the Apache License 2.0 as is permitted by all work that we use [6]. ", "page_idx": 24}, {"type": "text", "text": "A.4.8 Details about Cartpole ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Transferability heatmap Figure 20 presents transferability heatmaps for the Cartpole task with variations in three physical properties: mass of the cart, length of the pole, and mass of the pole. Each heatmap illustrates how well strategies transfer from source tasks (vertical axis) to target tasks (horizontal axis), depicting the influence of each parameter on control strategy effectiveness. For the mass of the cart variation (a), transferability decreases as the mass difference increases. In the length of the pole variation (b), strategies are less transferable between significantly different pole lengths. Similarly, for the mass of the pole variation (c), variations show divergent transferability depending on the extent of mass change. ", "page_idx": 24}, {"type": "text", "text": "Results Figure 21 presents a comparison of normalized generalized performance for the Cartpole task across different strategies when varying the mass of the cart, length of the pole, and mass of the pole. ", "page_idx": 24}, {"type": "text", "text": "In the mass of cart variation, the performance generally increases with transfer steps, with MBTL strategies achieving the highest scores, indicating robust adaptability to changes in cart mass. Similar trends are observed with length variation and mass of pole variation. MBTL shows close to oracle performance. ", "page_idx": 24}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/9158f7ee8dbe692d5ae1e40820eea83887b3559a3d61db4fd8b9e5cab38de0be.jpg", "img_caption": ["Figure 20: Examples of transferability heatmap for Cartpole. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/4c3eca6e08250e6ba334abf3bbac8aa3357e014a48e3643dcc211c9b0078e764.jpg", "img_caption": ["Figure 21: Comparison of normalized generalized performance of all target tasks: Cartpole. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "A.4.9 Details about Pendulum ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Transferability heatmap Figure 22 presents transferability heatmaps for the Pendulum task with variations in three physical properties: timestep, length of the pendulum, and mass of the pendulum. Each heatmap illustrates how effectively strategies transfer from source tasks (vertical axis) to target tasks (horizontal axis), highlighting the impact of each parameter on control strategy effectiveness. For the timestep variation (a), there appears to be high consistency in transferability across different timesteps, especially around the diagonal axis. In the length of the pendulum variation (b), transferability decreases with greater length differences. Similarly, for the mass of the pendulum variation (c), transferability shows variability dependent on the extent of mass changes. ", "page_idx": 26}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/015c049b0686b04f2f35770b38bcc584f029be09be75ec7981a1e62c25dac25a.jpg", "img_caption": ["Figure 22: Examples of transferability heatmap for Pendulum. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Results Figure 23 shows a comparison of normalized generalized performance for the Pendulum task across different strategies when varying the timestep, length of the pendulum, and mass of the pendulum. For the length of the pendulum variation and mass of the pendulum one, MBTL strategies demonstrate the highest scores, suggesting robust adaptability to changes in pendulum dynamics. MBTL shows performance close to that of the Oracle across all variations, indicating its effectiveness in handling dynamic changes in system parameters. ", "page_idx": 26}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/1acff6b5fba7017f76eb0f0cea49c2645191f34c5dd0b76e15c44c207d955261.jpg", "img_caption": ["Figure 23: Comparison of normalized generalized performance of all target tasks: Pendulum. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "A.4.10 Details about BipedalWalker ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Transferability heatmap Figure 24 presents transferability heatmaps for the BipedalWalker task, focusing on three variations: friction, gravity, and scale. Each heatmap illustrates the effectiveness of strategy transfer from source tasks (vertical axis) to target tasks (horizontal axis), highlighting how each parameter influences control strategy adaptability. For friction variation (a), strategies show uniform transferability across different friction levels. In gravity variation (b), transferability is highly variable, suggesting that strategies need specific tuning for different gravity levels. For scale variation (c), the heatmap indicates variable transferability, reflecting the challenges of scaling control strategies. ", "page_idx": 27}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/e90a01aa52d6c2ec6b45b5dbdbb3a332d1f716097af1739b57ccb6791d8fd06c.jpg", "img_caption": ["Figure 24: Examples of transferability heatmap for BipedalWalker. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Results Figure 25 shows the comparison of normalized generalized performance for all variations within the BipedalWalker task. There is no huge difference in performance for all three cases, but if we look into the tabualr results in Table 2, MBTL shows the highest performance across varying conditions, indicating their robustness in adapting to changes in physical parameters of the model. This suggests that these strategies are more effective in handling the complexities introduced by different frictions, gravities, and scales compared to other baselines. ", "page_idx": 27}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/cab64861a530b1e78f06729e9ce51b808b8b528e9cccdb0519aa86d1bc6182d2.jpg", "img_caption": ["Figure 25: Comparison of normalized generalized performance of all target tasks: BipedalWalker. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "A.4.11 Details about HalfCheetah ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Transferability heatmap Figure 26 displays transferability heatmaps for the HalfCheetah task, focusing on three physical properties: friction, gravity, and stiffness. Each heatmap demonstrates the transferability of strategies from source tasks (vertical axis) to target tasks (horizontal axis). For friction variation (a), there is uniform high transferability across different friction levels, indicating that strategies are robust to changes in friction. Gravity variation (b) shows less consistent transferability, suggesting a sensitivity to gravity changes that might require adaptation of strategies. Stiffness variation (c) similarly demonstrates variable transferability, highlighting the challenges of adapting to different stiffness levels in control strategies. ", "page_idx": 28}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/e0c38477138a4b2651df2c42b58129e90b623dcd936402a7b42451b11badaaf2.jpg", "img_caption": ["Figure 26: Examples of transferability heatmap for HalfCheetah. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Results Figure 27 presents a comparison of normalized generalized performance across various strategies for the HalfCheetah task with respect to the varied physical properties. The results indicate that the MBTL generally outperforms others, particularly in managing variations in gravity and stiffness, suggesting the superior adaptability of these models to physical changes in the task environment. The trends across different parameters confirm the critical impact of task-specific dynamics on the effectiveness of the tested strategies. ", "page_idx": 28}, {"type": "image", "img_path": "KLv1VLuMo8/tmp/68ade2f69f85e94b3f123141cb54d4553bf6aace7178f48b9044d8957000cc0a.jpg", "img_caption": ["Figure 27: Comparison of normalized generalized performance of all target tasks: HalfCheetah. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "A.5 Potential impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Our work has the potential to reduce the computational effort needed to solve complex real-world problems, offering scalable solutions for implementing deep reinforcement learning in dynamic environments. While there are no immediate negative societal impacts identified, ongoing research will continue to assess the broader implications of deploying these technologies in urban settings. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the contributions and scope, matching the theoretical and experimental results presented in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The limitations are discussed in Conclusion (Section 7). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper includes all necessary assumptions and provides complete proofs in the Appendix A.3. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Detailed experimental settings, including models and hyperparameters, are provided to ensure reproducibility. Also, the code is submitted by the authors. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The code are submitted by the authors with zip flies, with detailed instructions for reproducing the results included in the supplemental material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper provides comprehensive details about the experiments, including data generation, hyperparameters, and optimizer types, to ease understanding of the results. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Error bars and confidence intervals are reported for all significant experiments, with methods and assumptions clearly stated. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The type of compute resources, memory, and execution time for each experiment are specified to ensure reproducibility in Appendix A.4. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research adheres to the NeurIPS Code of Ethics, with no deviations from the guidelines. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper includes a section discussing both the potential positive and negative societal impacts of the research. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve data or models with high risk for misuse. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper uses the CARL framework under the Apache License 2.0, with proper credit given and license terms mentioned in the Appendix A.4.7. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The new assets, including code for the MBTL algorithm and experiments, are well-documented and submitted with an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]