[{"type": "text", "text": "Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Tyurin Marta Pozzi Ivan Ilin Peter Richtarik KAUST\\* AIRI, Skoltech+ KAUST\\* University of Pavias KAUST\\* KAUST\\* ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique. we develop a new method\u2014Shadowheart SGD\u2014that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the nonconvex smooth optimization problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\Big\\{f(x):=\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{\\xi}}\\left[f(x;\\xi)\\right]\\Big\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f(\\cdot;\\cdot)\\;:\\;\\mathbb{R}^{d}\\times\\mathbb{S}_{\\xi}\\rightarrow\\mathbb{R}$ , and $\\mathcal{D}_{\\xi}$ is a distribution on $\\mathbb{S}_{\\xi}\\neq\\varnothing$ . Given $\\varepsilon>0$ , we seek to find a possibility random point $\\hat{x}$ such that $\\mathbb{E}[\\|\\nabla f({\\hat{x}})\\|^{2}]\\leq\\varepsilon$ Such a point $\\hat{x}$ is called an $\\varepsilon\\cdot$ -stationary point. We focus on solving the problem in the following setup: ", "page_idx": 0}, {"type": "text", "text": "(a) n workers/nodes are able to compute stochastic gradients $\\nabla f(x;\\xi)$ of $f$ , in parallel and asynchronously, and it takes (at most) $h_{i}$ seconds for worker $i$ to compute a single stochastic gradient; (b) the workers are connected to a server which acts as a communication hub; (c) the workers can communicate with the server in parallel and asynchronously; it takes (at most) $\\tau_{i}$ seconds for worker $i$ to send a compressed message to the server; compression is performed via applying lossy communication compression to the communicated message (a vector from $\\mathbb{R}^{d}$ ); see Def. 2.1; (d) the server can broadcast compressed vectors to the workers in (at most) $\\tau_{\\mathrm{serv}}$ seconds; compression is performed via applying a lossy communication compression operator to the communicated message (a vector from $\\mathbb{R}^{d}$ ); see Def. A.1. ", "page_idx": 0}, {"type": "text", "text": "The main goal of this work is to find an optimal optimization strategy/method that would work uniformly well in all scenarios characterized by the values of the computation times $h_{1},\\ldots,h_{n}$ and communication times $\\tau_{1},\\dots,\\tau_{n}$ and $\\tau_{\\mathrm{serv}}$ . Since we allow these times to be arbitrarily heterogeneous, designing a single algorithm that would be optimal in all these scenarios seems challenging. ", "page_idx": 0}, {"type": "table", "img_path": "O8yHsRLwPl/tmp/da479dd6ebf89e8e0340958bd8bbd4c8e4f7c59c7f41741a57f033e9e2cf62b9.jpg", "table_caption": ["Table 1: Time Complexities of Centralized Distributed Algorithms. Assume that it takes at most $h_{i}$ seconds to worker $i$ to calculate a stochastic gradient and $\\dot{\\tau}_{i}$ seconds to send one coordinate/float to server. Abbreviations: $L=$ smoothness constant, $\\varepsilon=$ error tolerance, $\\Delta=f(x^{0})-f^{*}$ \uff0c $n=\\#$ of workers, $d\\!=$ dimension of the problem. We take the Rand $K$ compressor with $K=1$ (Def. D.1) (as an example) in QSGD and Shadowheart SGD. Due to Property 6.2, the choice $K=1$ is optimal for Shadowheart SGD up to a constant factor. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Let us clarify what we mean by $\\{\\tau_{i}\\}$ and take, for instance, the Rand $K$ compressor (see Def. D.1 and Sec. 2.2) (sends $K$ random entries). Then $\\tau_{i}$ is $\\#$ of seconds required to send $K$ coordinates of a vector. Now assume that the communication is proportional to the number of coordinates/bits that a worker sends, i.e., it takes $\\dot{\\tau}_{i}$ seconds to send a one coordinate/bit. Then, clearly, we have $\\tau_{i}=K\\times\\dot{\\tau}_{i}$ . and, $\\tau_{i}$ is a function of $K$ While the dependence is clear for Rand $K$ , for all possible compressors, the dependence on the amount of sent information can be less nontrivial. We generally fix an arbitrary unbiased compressor and assume it takes $\\tau_{i}$ seconds to send the compressed message. ", "page_idx": 1}, {"type": "text", "text": "From the viewpoint of federated learning (Konecny et al., 2016; Kairouz et al., 2021), our work is a theoretical study of device heterogeneity. Moreover, our formalism captures both cross-silo and cross-device settings as special cases. Due to our in-depth focus on device heterogeneity and the challenges that need to be overcome, we do not consider statistical heterogeneity, and leave an extension to this setup tofuturework. ", "page_idx": 1}, {"type": "text", "text": "We rely on assumptions which are standard in the literature on stochastic gradient methods: smoothness, lower-boundedness and bounded variance. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1.1. $f$ is differentiable & $L$ -smooth, i.e., $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\,\\|x-y\\|,\\forall x,y\\in\\mathbb{R}^{d}$ Assumption 1.2. There exist $f^{*}\\in\\mathbb{R}$ such that $f(x)\\ \\geq\\ f^{*}$ for all $x\\,\\in\\,\\mathbb{R}^{d}$ .We define $\\Delta:=$ $f(x^{0})-f^{*}$ ,where $x^{0}\\in\\mathbb{R}^{d}$ is a starting point of all algorithms we consider. $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 1}, {"type": "text", "text": "Assumption 1.3. For all , the stochastic gradients $\\nabla f(x;\\xi)$ are unbiased, and their variance is bounded by $\\sigma^{2}\\geq0$ i.e., $\\mathbb{E}_{\\xi}[\\nabla f(x;\\xi)]=\\nabla f(\\bar{x})$ and $\\mathbb{E}_{\\xi}[\\|\\bar{\\nabla}f(\\bar{x};\\xi)-\\nabla f(x)\\|^{2}]\\le\\sigma^{2}$ ", "page_idx": 1}, {"type": "text", "text": "To simplify the exposition, in what follows we first focus on the regime in which the broadcast cost can be ignored. We describe a strategy for extending our algorithm to the more general regime in Sec.A. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1  Communication time can be ignored ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We now briefy review related work and important concepts. Consider the regime when the communication cost is negligible ( ${{\\tau}_{i}}\\mathrm{~=~}0$ for all $i$ ), and the computation times $h_{i}$ arearbitrary but fixed. It is well-known that under Assumptions 1.1, 1.2, and 1.3, the vanilla SGD method $x^{k+1}\\;=\\;x^{k}\\;-\\;\\gamma\\nabla f(x^{k};\\xi^{k})$ ,where $x^{0}\\in\\mathbb{R}^{d}$ is a starting point, and $\\gamma~>~0$ is the step size, solves (1) using the optimal number of stochastic gradients (Ghadimi and Lan, 2013; Arjevani et al., 2022). Since the $\\#$ of iterations of SGD to get an $\\varepsilon,$ -stationary point is O $\\left(L\\Delta/\\varepsilon+\\sigma^{2}L\\bar{\\Delta}/\\varepsilon^{2}\\right)$ \uff0c SGD run on a single worker whose computation time is $h_{1}$ seconds would have time complexity 0 $\\left(h_{1}\\times\\left(L\\Delta/\\varepsilon+{\\bar{\\sigma}}^{2}L\\Delta/\\varepsilon^{2}\\right)\\right)$ seconds. The time complexity of Minibatch SGD with $n$ workers, i.e., ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{x}^{k+1}={x}^{k}-\\frac{\\gamma}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla f({x}^{k};{\\xi}_{i}^{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "can be shown (Gower et al., 2019) to be ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{O}\\left(h_{\\mathrm{max}}\\times\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $h_{\\operatorname*{max}}\\;:=\\;\\operatorname*{max}_{i\\in[n]}h_{i}$ ,where $[n]$ denotes $\\{1,\\ldots,n\\}$ . The dependence on $h_{\\mathrm{max}}$ is due to Minibatch SGD employing synchronous parallelism which forces it to wait for the slowest worker. While the stochastic part of (3) can be $n$ times smaller than in the single worker case, (3) does not guarantee an improvement since $h_{\\mathrm{max}}$ can be arbitrarily large. In real systems, computation times can be very heterogeneous and vary in time in chaotic ways (Dutta et al., 2018; Chen et al., 2016). ", "page_idx": 2}, {"type": "text", "text": "Recently, Cohen et al. (2021); Mishchenko et al. (2022) and Koloskova et al. (2022) showed that it is possible to improve upon (3) using the celebrated Asynchronous SGD method (Recht et al., 2011; Feyzmahdavian et al., 2016; Nguyen et al., 2018) and ge the time complexity $\\begin{array}{r}{\\mathrm{O}((\\mathrm{i}/n\\sum_{i=1}^{n}\\frac{1}{h_{i}})^{-1}\\times}\\end{array}$ $\\left(L\\Delta/\\varepsilon+\\sigma^{2}L\\Delta/n\\varepsilon^{2}\\right))$ , which improves the dependence from $h_{\\mathrm{max}}$ to the harmonic mean of the computation times. Subsequently, Tyurin and Richtarik (2023c) developed the Rennala SGD method whose time complexity is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{O}\\left(\\underset{m\\in[n]}{\\operatorname*{min}}\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\times\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{m\\varepsilon^{2}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pi$ is a permutation forwhich $h_{\\pi_{1}}\\leq\\cdots\\leq h_{\\pi_{n}}$ . They also showed that the time complexity (4) is optimal by providing a matching lower bound. ", "page_idx": 2}, {"type": "text", "text": "2.2  Communication time is a factor ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In many practical scenarios, communication times can be the main bottleneck, and can not be ignored, e.g., in distributed/federated training of machine learning models (Ramesh et al., 2021; Kairouz et al., 2021; Wang et al., 2023). There are two main techniques for reducing the communication bottleneck: local training steps (McMahan et al., 2017) and compressed communication (Seide et al., 2014; Alistarh et al., 2017). In our work, we investigate the latter technique. In particular, efficient methods with compressed communication such as DlANA (Mishchenko et al., 2019), Accelerated DIANA (Li et al., 2020), MARINA (Gorbunov et al., 2021) and DASHA (Tyurin and Richtarik, 2023b) employ unbiased compressors, defined next. Assume that $\\mathbb{S}_{\\nu}$ is a nonempty arbitrary set of samples, and $\\mathcal{D}_{\\nu}$ is a distribution on $\\mathbb{S}_{\\nu}$ ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. A mapping $\\mathcal{C}:\\mathbb{R}^{d}\\times\\mathbb{S}_{\\nu}\\to\\mathbb{R}^{d}$ is an unbiased compressor if there exists $\\omega\\ge0$ such that $\\mathbb{E}_{\\nu}[\\mathcal{C}(x;\\nu)]=x$ \uff0c $\\mathbb{E}_{\\nu}[\\left\\|\\mathcal{C}(x;\\nu)-x\\right\\|^{2}]\\leq\\omega\\left\\|x\\right\\|^{2}$ for all $x$ . Let $\\mathbb{U}(\\omega)$ denote the family of such compressors?. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.2. Samples from $\\mathcal{D}_{\\xi}$ and $\\mathcal{D}_{\\nu}$ are mutually independent. ", "page_idx": 2}, {"type": "text", "text": "The canonical example of an unbiased compressor is the Rand $K$ compressor (see Def. D.1) that scales $K$ random entries of the input vector $x$ by $d/\\kappa$ and zeros out the rest. Many more examples of unbiased compressors are considered in the literature (Beznosikov et al., 2020; Xu et al., 2021a; Horvath et al., 2022). One of the most straightforward methods which use compression is $\\mathsf{Q S G D}^{6}$ (Alistarh et al., 2017): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{x}^{k+1}=\\boldsymbol{x}^{k}-\\frac{\\gamma}{n}\\sum_{i=1}^{n}\\mathcal{C}_{i}\\left(\\nabla f(\\boldsymbol{x}^{k};\\xi_{i}^{k})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where each worker calculates one stochastic gradient, compresses it using $\\mathcal{C}_{i}\\in\\mathbb{U}(\\omega)$ drawn independently, and sends it to the server. The server aggregates the compressed vectors and performs step (5). With a proper stepsize choice $\\gamma$ , QSGD converges after O $\\left((\\omega/n\\dot{+}1)\\times L\\Delta/\\varepsilon+(\\omega\\dot{+}1)\\times\\sigma^{2}L\\Delta\\/_{n\\varepsilon^{2}}\\right)$ iterations? (Khaled and Richtarik, 2020). Let's assume it takes $\\tau_{i}$ seconds for worker $i$ tosend one compressed vector to the server. Since the workers act in parallel, the time complexity of QSGD is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{i\\in[n]}{\\operatorname*{max}}\\left(h_{i}+\\tau_{i}\\right)\\times\\bigg((\\frac{\\omega}{n}+1)\\frac{L\\Delta}{\\varepsilon}+(\\omega+1)\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can go through a similar exercise with any other method that uses compressed communication (e.g., (Tyurin and Richtarik, $2023\\mathbf{a}$ ; Gauthier et al., 2023; Jia et al., 2023)). Nevertheless, as far as we know, the optimal time complexities for asynchronous centralized distributed optimization with communicationcompressionarenotknown. ", "page_idx": 3}, {"type": "text", "text": "3 Summary of Contributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the regime in which the communication time can be ignored (see Sec. 2.1), Tyurin and Richtarik (2023c) showed that (4) is the optimal time complexity. In this work we endeavor to take the next step: we wish to understand the fundamental limits of the regime in which communication time is a factor. Our main contributions are: ", "page_idx": 3}, {"type": "text", "text": "$\\spadesuit$ We develop a new method\u2014Shadowheart SGD (Algorithm 1)\u2014that guarantees to find an $\\varepsilon$ -stationary point of problem (1) with time complexity $T_{*}$ given in (10). While the general expression we give for $T_{*}$ is hard to parse since it involves the equilibrium time $t_{*}(\\cdot)$ whose definition is implicit (see Def. 3.1), we show (see Sec. 7) that $T_{*}$ is not worse than the time complexity of known centralized? methods, and also who that it can be strictly better in many regimes, even by many degrees of magnitude (see Sec. 7.1 and Table 1). ", "page_idx": 3}, {"type": "text", "text": "$\\clubsuit$ In Sec. 5 we show that (10) is the optimal time complexity in the family of centralized methods with compression. This is the first such result in the literature. ", "page_idx": 3}, {"type": "text", "text": "$\\spadesuit$ We also developed Adaptive Shadowheart SGD (Sec. 4.2 and M), which does not require the knowledge of the computation and communication times and can work with arbitrary changing times. Moreover, we designed Bidirectional Shadowheart SGD (Sec. A), which works in the regime when broadcast cost not negligible as well. ", "page_idx": 3}, {"type": "text", "text": "$\\bullet$ Our theoretical study of Shadowheart SGD is supported by judiciously designed synthetic experiments and machine-learning experiments with logistic regression; see Sec. Q. ", "page_idx": 3}, {"type": "text", "text": "4Development of Shadowheart SGD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our method bears some resemblance to Rennala SGD (Tyurin and Richtarik, 2023c) and QSGD (Alistarh et al., 2017), and involves some additional algorithmic elements which play a key role. First, we adopted the main suggestion of Tyurin and Richtarik (2023c)[Sec.7] behind the design of Rennala SGD that an optimal method should calculate stochastic gradients at the last iterate. Second, QSGD served as an inspiration for how to perform gradient compression. In particular, Shadowheart SGD has the form $x^{k^{\\underline{{\\lambda}}+1}}=x^{k}-\\gamma g^{k}$ Where ", "page_idx": 3}, {"type": "equation", "text": "$$\ng^{k}=\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{m_{i}}{\\mathcal C}_{i j}\\left(\\sum_{l=1}^{b_{i}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)/\\sum_{i=1}^{n}w_{i}m_{i}b_{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Shadowheart SGD, worker $i$ calculates $b_{i}$ stochastic gradients,  adds them up to  form $\\begin{array}{r}{\\sum_{l=1}^{b_{i}}\\nabla f(x^{k};\\xi_{i l}^{k})}\\end{array}$ andcompress theresult $m_{i}$ times using independentlydrawn compressors. The compressed messages are sent to the server. The first non-trivial step in the design of our method isheprsenfof weights $w_{i}$ $\\textstyle\\sum_{i=1}^{n}m_{i}$   \nworkersbyperfomng aconiccmbinationwithcoefcient   \nworker $i$ . One can easily show that (9) is equivalent to Alg. 1. Note that we recover QSGD (see (5)) as a special (suboptimal) case with $w_{i}=b_{i}=m_{i}=1$ for all $i\\in[n]$ ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Shadowheart SGD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Input: starting point $x^{0}\\in\\mathbb{R}^{d}$ , stepsize $\\gamma>$   \n0, ratio $\\sigma^{2}/\\varepsilon$ , computation times $h_{i}>0$ , and   \ncommunication times $\\tau_{i}>0$ for $i\\in[n]$   \n2: Find equilibrium time $t^{*}$ using Def. 3.1   \n3:Set $\\begin{array}{r}{b_{i}=\\left\\lfloor\\frac{t^{*}}{h_{i}}\\right\\rfloor}\\end{array}$ and $\\begin{array}{r}{m_{i}=\\left\\lfloor\\frac{t^{*}}{\\tau_{i}}\\right\\rfloor}\\end{array}$ for all $i\\in[n]$   \n4: Find $S_{\\mathrm{A}}\\bar{=}\\left\\{\\bar{i}\\in[n]\\,:\\,b_{i}\\land\\bar{m_{i}}>0\\right\\}$   \n5: for $k=0,1,\\ldots,K-1$ do   \n6: Run Alg. 2 in active workers $S_{\\mathrm{A}}$   \n7: Broadcast $x^{k},b_{i}$ \uff0c $m_{i}$ to active workers $S_{\\mathrm{A}}$   \n8: Initialize $g^{k}=0$   \n9: for $i\\in S_{\\mathrm{A}}$ in parallel do   \n10: $w_{i}\\overset{(a)}{=}\\left(b_{i}\\omega+\\omega\\frac{\\sigma^{2}}{\\varepsilon}+m_{i}\\frac{\\sigma^{2}}{\\varepsilon}\\right)^{-1}$   \n11: for $j=1,\\dots,m_{i}$ do   \n12: Receive $\\mathcal{C}_{i j}\\left(g_{i}^{k}\\right)$ from worker $i$   \n13: $g^{k}=g^{k}+\\dot{w_{i}}\\dot{C_{i j}}\\left(g_{i}^{k}\\right)$   \n14: end for   \n15: end for   \n16: $\\begin{array}{l}{{g^{k}=g^{k}/\\left(\\sum_{i=1}^{n}w_{i}m_{i}b_{i}\\right)}}\\\\ {{x^{k+1}=x^{k}-\\gamma g^{k}}}\\end{array}$   \n17:   \n18: end for ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Strategy of Worker $i$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Receive $x^{k},b_{i}$ $m_{i}$ from server, init $g_{i}^{k}=0$   \n2: for $l=1,\\ldots,b_{i}$ do   \n3: Calculate $\\begin{array}{r}{\\bar{\\nabla}f(x^{k};\\xi_{i l}^{k}),\\quad\\xi_{i l}^{k}\\sim\\mathcal{D}_{\\xi}}\\end{array}$   \n4: $g_{i}^{k}=g_{i}^{k}+\\nabla f(x^{k};\\dot{\\xi}_{i l}^{k})$   \n5:end for   \n6: for $j=1,\\dots,m_{i}$ do   \n7: Send $\\mathcal{C}_{i j}\\left(g_{i}^{k}\\right)\\equiv\\mathcal{C}\\left(g_{i}^{k};\\nu_{i j}^{k}\\right)$ to server,   \n$\\nu_{i j}^{k}\\sim\\mathcal{D}_{\\nu}$ $\\mathcal{C}_{i j}\\in\\mathbb{U}(\\omega)$   \n8:end for ", "page_idx": 4}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/b01eb41ec7bcd8dc3c4b4788759e8c7df73a7eec2785840894e1de6ab6ad5003.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "with inputs $\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\ldots,h_{n},\\tau_{n}$ is called the equilibrium time if it is defined as follows. Find  a  permutationa $\\pi$ that sorts  the  pairs $(h_{i},\\tau_{i})$ as $\\operatorname*{max}\\{h_{\\pi_{1}},\\tau_{\\pi_{1}}\\}\\leq\\cdots\\leq\\operatorname*{max}\\{h_{\\pi_{n}},\\tau_{\\pi_{n}}\\}$ and find the solution $s^{*}(j)\\in[0,\\infty]$ in s ofb ", "page_idx": 4}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/806d402ec57b446fc92552398e5e562ab371797f5c9d97dc83e3af7651a57e24.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "for all $j\\in[n]$ . Then the mapping returns the   \nvalue   \n$\\begin{array}{r l}&{t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})}\\\\ &{\\equiv\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}\\in[0,\\infty].}\\end{array}$ (8)   \nWeshall use  the  short   notation   \n$t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$ aIt is possible that a permutation is not unique.   \nThe result of the mapping does not depend on the   \nchoice of the permutation. See the proof of Prop  \nerty 4.1. bFor convenience, we use the projectively ex  \ntended real line and define $1/0=\\infty$ ", "page_idx": 4}, {"type": "text", "text": "The weights $\\{w_{i}\\}$ are chosen so as to minimize the variance in the proof of Lemma H.1. However, we still need to find the right values for $b_{i}$ and $m_{i}$ . Since the computation and communication times of worker $i$ are $h_{i}$ and $\\tau_{i}$ , respectively, the following strategy makes intuitive sense: the server sets some time budget $t$ for all workers, and each worker then calculates $\\lfloor{t}/{h_{i}}\\rfloor$ stochastic gradients and sends $\\lfloor t/\\tau_{i}\\rfloor$ compressed vectors to the server. But what is the right way to choose $t?$ If $t$ is too small, then, intuitively, some workers may not have time to calculate \u201cenough\" gradients, or may even not have time to send any messages to the server. On the other hand, if $t$ is too large, then the workers will eventually send information of diminishing utility which will not be worth the extra time this takes. ", "page_idx": 4}, {"type": "text", "text": "We find out that, and this one of the key insights of our work, that there exists an optimal time budget $t^{*}$ which depends on the quantities $\\omega$ \uff0c $\\bar{\\sigma^{2}}/\\varepsilon$ \uff0c $h_{1}$ $\\tau_{1}$ .... $h_{n}$ $\\tau_{n}$ , for which we coin the name equilibrium time; see Def. 3.1. Admittedly, the definition of the equilibrium time is implicit; we do not know if it is possible to give a more explicit formula in general. To provide for some peace of mind, we prove the following property: ", "page_idx": 4}, {"type": "text", "text": "Property 4.1. If all inputs of the equilibrium time are non-negative, then the equilibrium time is well defined. ", "page_idx": 4}, {"type": "text", "text": "More importantly, in Sec. 5 we provide a lower bound that involves the same mapping. Thus, the equilibrium time is not an \u201cartifact\"\u2019 of our method, but is of a fundamental nature. We use the equilibriumtime $t^{*}$ in Shadowheart SGDwhenwe choose $b_{i}$ and $m_{i}$ ", "page_idx": 4}, {"type": "text", "text": "Our first main result provides iteration complexity: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Lett Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take $\\gamma=1/2L$ in Shadowheart SGD (Alg 1). Then as long as $K\\geq16L\\Delta/\\varepsilon$ we have the guarantee $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "This result guarantees that Shadowheart SGD will converge after $\\mathcal{O}\\left(L\\Delta/\\varepsilon\\right)$ iterations. Our second main result provides a much more relevant complexity measure: time complexity. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.3. Shadowheart SGD (Alg. 1) converges after at most $T_{*}$ seconds,where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{*}:=\\frac{32L\\Delta}{\\varepsilon}\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Surprisingly, we show in Sec. 5 that our time complexity guarantee (10) is optimal for the family of centralized methods with compressed communication. Moreover, in Sec. 7 and 7.1, we show that (10) is no worse and can be significantly better than the time complexities of previous centralized methods (see also Table 1 for a summary). ", "page_idx": 5}, {"type": "text", "text": "4.1  Tighter result with per-iteration times $h_{i}^{k}$ and $\\tau_{i}^{k}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A slight modification of Alg. 1 leads to Alg. 4, which can work with iteration-dependent computation and communication times $\\bar{h}_{i}^{k}$ and $\\tau_{i}^{k}$ . Our main result in this setup is Theorem H.3; here we present its corollary. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4. Alg. 4 converges after ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{\\left\\lceil\\frac{16L\\Delta}{\\varepsilon}\\right\\rceil}2t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1}^{k},\\tau_{1}^{k},\\dots,h_{n}^{k},\\tau_{n}^{k})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "seconds,where $h_{i}^{k}\\,>\\,0$ and $\\tau_{i}^{k}\\,>\\,0$ are computation and communication times for worker i in iteration $k$ ", "page_idx": 5}, {"type": "text", "text": "For presentation simplicity sake, in the main part we continue to work with static $\\{h_{i}\\}$ and $\\{\\tau_{i}\\}$ ", "page_idx": 5}, {"type": "text", "text": "4.2 On the problem of estimating the times in Algorithms 1 and 4 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "One of the main features of asynchronous methods (e.g., Rennala SGD, Asynchronous SGD) is their adaptivity to and independence from processing times. In Sec. M, we design Adaptive Shadowheart SGD (Alg. 7) with this feature. Unlike Alg. 1, it does not require the knowledge of $\\{h_{i}\\}$ and $\\{\\tau_{i}\\}$ (or $\\{h_{i}^{k}\\}$ and $\\{\\tau_{i}^{k}\\}$ in the case of Alg. 4) and does not calculate the equilbrium time $t^{*}$ However,asa byproduct of this flexibility, this method has a slightly worse time complexity guarantee. In order to present our result, we need to define an auxiliary sequence. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.5. Assume that the workers have computation and communication times less or equal to $\\{h_{i}\\}$ and $\\{\\tau_{i}\\}$ . Assume that $\\bar{h}_{i j}$ is the actual time required to calculate the $j^{\\mathrm{th}}$ stochastic gradient by worker $i$ $h_{\\operatorname*{min}}>0$ is the smalliest possible computation time. Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r_{i}:=\\underset{k\\geq0}{\\operatorname*{sup}}\\frac{\\underset{1\\leq j\\leq l_{\\operatorname*{max}}}{\\operatorname*{sup}}\\bar{h}_{i,(k+j)}}{\\underset{1\\leq j\\leq l_{\\operatorname*{max}}}{\\operatorname*{sup}}\\bar{h}_{i,(k+j)}},\\ l_{\\operatorname*{max}}:=\\left\\lceil\\frac{t_{\\operatorname*{max}}}{h_{\\operatorname*{min}}}\\right\\rceil,}\\\\ {t_{\\operatorname*{max}}:=128\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[\\operatorname*{max}\\{h_{i},\\tau_{i}\\},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}]_{1}^{n}).\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "That is, $r_{i}\\in[1,\\infty]$ is the largest ratio between the fastest and the slowest computation of stochastic gradients in local time windows. $r_{i}$ defines a degree of fuctuations in computation times. Note that $r_{i}$ describes local fluctuations; it is true that $\\begin{array}{r}{r_{i}\\stackrel{\\cdot}{\\leq}\\operatorname*{sup}_{j\\geq1}\\bar{h}_{i,j}\\Big/\\!\\operatorname*{inf}_{j\\geq1}\\bar{h}_{i,j}}\\end{array}$ for all $i\\in[n]$ and $r_{i}$ can be arbitrarily smaller. ", "page_idx": 5}, {"type": "text", "text": "A corollary of our main result in this part (Theorem M.1) is presented next. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.6. If the computation and communication times are positive, the time complexity of $\\begin{array}{r}{\\frac{\\check{L}\\Delta}{\\varepsilon}\\times t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon}\\end{array}$ $\\left[\\operatorname*{max}\\{h_{i},\\tau_{i}\\},\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\right]_{1}^{n}\\right)$ uptoaconstantfactorwhere $r_{i}$ is defined inDef. 4.5. ", "page_idx": 5}, {"type": "text", "text": "Unlike Alg. 1 and Alg. 4, Alg. 7 is more \u201cgreedy'\"; it calculates stochastic gradients and sends compressed vectors in parallel, and it does not know the times $h_{i}$ and $\\tau_{i}$ (or $h_{i}^{k}$ and $\\tau_{i}^{k}$ ). That is why this method gets a suboptimal complexity and depends on $r_{i}$ . Nevertheless, if we assume that i) the computation times do not fluctuate significantly, i.e., $r_{i}=\\Theta(1)$ , and ii) ${\\tau}_{i}\\leq{h}_{i}$ for all $i\\in[n]$ , then this complexity reduces to the optimal complexity $L\\Delta/_{\\varepsilon}\\times t^{*}(\\dot{\\omega},\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$ ", "page_idx": 5}, {"type": "text", "text": "Protocol 3 Simplified Representation of Protocol 9 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Init $S=\\emptyset$ on the server (all available information)   \n2: while True do   \n3: Server calculates a new point $\\textstyle{\\bar{x}}$ using $S$ and broadcasts $\\bar{x}$ and $S$ to any worker (broadcasting does not take time)   \n4: end while $i^{\\mathbf{th}}$ Worker (in parallel):   \n5: while True do   \n6: Receives $\\bar{x}$ and $S$ , calculates as many stochastic gradients as it want at the point $\\textstyle{\\bar{x}}$ (each calculation takes $h_{i}$ seconds), aggregates all available information, and sends compressed vectors (each dispatch takes $\\tau_{i}$ seconds), which will be added to the set $S$   \n7: end while ", "page_idx": 6}, {"type": "text", "text": "In Sec. 4, we stated that Shadowheart SGD converges after $T_{*}$ seconds;with $T_{*}$ given in (10). Our next step is to understand if it might be possible to improve this complexity. In Sec. O, we formalize our setup and show in Theorem O.5 that up to a constant factor, the result (10) is optimal. Here we present a simplified illustration of our approach. ", "page_idx": 6}, {"type": "text", "text": "Protocol 3 can describe all centralized methods (the server updates the iterates, and the workers calculate stochastic gradients at these points), including Minibatch SGD, Asynchronous SGD, Rennala SGD, and Shadowheart SGD. In Theorem O.5, we show that up to a constant factor, no method described by Protocol 3 can converge faster than (10) seconds. In order to use our lower bound, the workers must calculate stochastic gradients at a point that was calculated by the server. ", "page_idx": 6}, {"type": "text", "text": "Let us briefly explain the proof's idea. The general approach is the same as in (Nesterov, 2003; Arjevani et al., 2022; Huang et al., 2022): we take the \u201cdifficult function (Sec. P.1), which has large gradients while the last coordinate equals to zero. Every algorithm starts with the point $x^{0}=0$ , and the only way to discover the next coordinate is to calculate a stochastic gradient. Oracles associated with the workers return the next non-zero coordinate with the probability $p_{\\sigma}\\,\\approx\\,\\varepsilon/{\\sigma^{2}}$ . Even if the stochastic oracle returns a non-zero coordinate for some worker, the corresponding communication oracle on this worker also has to return a non-zero coordinate, which happens with probability $p_{\\omega}\\,\\approx\\,^{1}\\!/\\omega\\!+\\!1$ We fix the Rand $K$ compressor in the lower bound theorem with $K\\,\\approx\\,L\\Delta/\\varepsilon(\\omega\\!+\\!1)$ and the number of coordinates $\\approx\\,L\\Delta/\\varepsilon$ ; thus, indeed, $p_{\\omega}\\,\\approx\\,1/\\omega\\!+\\!1$ . Since all $n$ workers work in parallel, they can discover and send to the server the next non-zero coordinate not earlier than after $\\mathrm{min}_{m\\in[n]}\\left\\{h_{m}\\eta_{m}+\\tau_{m}\\mu_{m}\\right\\}$ seconds, where $\\eta_{m}$ and $\\mu_{m}$ are i.i.d. geometric random variables with $p_{\\sigma}$ and $p_{\\omega}$ . With a high probability, we show that this quantity is $\\Omega(t^{*}(1/p_{\\omega},1/p_{\\sigma},h_{1},\\tau_{1},...\\,,h_{n},\\tau_{n}))$ The number of coordinates is $\\approx L\\Delta/\\varepsilon$ . Therefore, the lower bound is (10) seconds up to a constant factor. ", "page_idx": 6}, {"type": "text", "text": "6 Equilibrium Time ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since the time complexity (10) of Shadowheart SGD is optimal, we believe that the equilibrium time is a fundamental mapping that should be investigated more deeply. ", "page_idx": 6}, {"type": "text", "text": "6.1 Calculation strategy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Thecalculation of $t^{*}$ requires us to sort $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}$ . Next, it is sufficient to solve $n$ equationsfrom (7). In Property 4.1, we prove that (7) has one unique solution that can be easily found, for instance, using the bisection method. Then, is it left to find the minimum in (8). ", "page_idx": 6}, {"type": "text", "text": "6.2  Intuition behind the equilibrium time $t^{*}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Assuming we found an optimal $j^{*}$ in (8), we have $t^{*}~=~\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j^{*}}},\\tau_{\\pi_{j^{*}}}\\},s^{*}(j^{*})\\}$ The first observation is that $t^{*}$ does not depend on the workers\u2018that\u2019 correspond to $\\operatorname*{max}\\{h_{\\pi_{j^{*}+1}},\\tau_{\\pi_{j^{*}+1}}\\},...\\,,\\operatorname*{max}\\{h_{\\pi_{n}},\\tau_{\\pi_{n}}\\}$ . Since these values are greater or equal to $\\operatorname*{max}\\{h_{\\pi_{j^{*}}},\\bar{\\tau}_{\\pi_{j^{*}}}\\}$ , the mapping \u201cdecides\"\u2019 to ignore them because they are too slow. The following derivations are not rigorous and are merely supposed to offer some intuition. We define $\\alpha_{i}:=\\tau_{\\pi_{i}}\\omega$ and $\\beta_{i}:=h_{\\pi_{i}}\\sigma^{2}/\\varepsilon$ .Next, using (7), we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n1=\\sum_{i=1}^{j^{*}}\\frac{s^{*}(j^{*})}{2\\alpha_{i}+\\frac{4\\alpha_{i}\\beta_{i}}{s^{*}(j^{*})}+2\\beta_{i}}\\approx s^{*}(j^{*})\\sum_{i\\in\\mathcal{M}}\\frac{1}{\\operatorname*{max}\\{\\alpha_{i},\\beta_{i}\\}}+s^{*}(j^{*})^{2}\\sum_{i\\notin\\mathcal{M}}\\frac{1}{\\alpha_{i}\\beta_{i}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{M}:=\\{i\\in[j^{*}]:\\operatorname*{max}\\{\\alpha_{i},\\beta_{i}\\}\\geq\\alpha_{i}\\beta_{i}\\mathrm{/}s^{*}(j^{*})\\}}\\end{array}$ . Solving this, one can get that ", "page_idx": 7}, {"type": "equation", "text": "$$\ns^{*}(j^{*})\\approx\\left(\\sum_{i\\in\\mathcal{M}}\\frac{1}{\\operatorname*{max}\\{\\alpha_{i},\\beta_{i}\\}}+\\sqrt{\\sum_{i\\notin\\mathcal{M}}\\frac{1}{\\alpha_{i}\\beta_{i}}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Thus, $s^{*}(j^{*})$ divides the active workers into two groups $\\mathcal{M}$ and $[j^{*}]\\setminus{\\cal{M}}$ . Both groups contribute to (13) with a harmonic mean-like and a quadratic harmonic mean-like dependences, correspondingly. The transition between two groups is decided by the rule $\\operatorname*{max}\\{\\alpha_{i},\\beta_{i}\\}\\geq\\alpha_{i}\\beta_{i}\\!\\big/_{s^{*}(j^{*})}\\Leftrightarrow s^{*}(j^{*})^{\\stackrel{\\cdot}{}}\\geq$ $\\operatorname*{min}\\{\\alpha_{i},\\beta_{i}\\}$ . Intuitively, the last inequality means that if $\\tau_{i}$ or $h_{i}$ is small (a worker can quickly compute a gradient or send a compressed vector), it belongs to $\\mathcal{M}$ . Otherwise, if a worker's computation and communication performance are balanced, it belongs to $[j^{*}]\\setminus{\\cal{M}}$ ", "page_idx": 7}, {"type": "text", "text": "6.3 Properties of the equilibrium time $t^{*}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now provide some properties and particular cases to understand $t^{*}$ better. One can find the proofs and more properties in Sec. E. The first result says that $t^{*}$ is monotonic. ", "page_idx": 7}, {"type": "text", "text": "Property 6.1. If $\\bar{\\omega}\\ge\\omega\\ge0,\\bar{\\underline{{\\sigma}}}^{2}/\\bar{\\varepsilon}\\ge\\sigma^{2}/\\varepsilon\\ge0,\\bar{h}_{1}\\ge h_{1}\\ge0,\\bar{\\tau}_{1}\\ge\\tau_{1}\\ge0,\\dots,\\bar{h}_{n}\\ge h_{n}\\ge0$ and$\\bar{\\tau}_{n}\\geq\\tau_{n}\\geq0$ , then $t^{*}(\\bar{\\omega},\\bar{\\sigma}^{2}/\\bar{\\varepsilon},[\\bar{h}_{i},\\bar{\\tau}_{i}]_{1}^{n})\\geq t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$", "page_idx": 7}, {"type": "text", "text": "Consider the Rand $K$ compressor. If it takes $\\dot{\\tau}_{i}$ sec to send one coordinate by worker $i$ , then, up to a constant factor, Property 6.2 ensures that an optimal choice of $K$ is1. ", "page_idx": 7}, {"type": "text", "text": "Property 6.2. For all $\\begin{array}{r l r l r l}{\\mathrm{~\\boldmath~\\mathcal~{~C~}~}}&{{}\\in}&{[1,d],\\sigma^{2}/\\varepsilon,}&{h_{1},}&{\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}}&{{}\\geq}&{0}\\end{array}$ , we have $24\\,\\mathrm{~\\times~}$ $t^{*}\\left(d/\\kappa-1,\\sigma^{2}/\\varepsilon,h_{1},K\\dot{\\tau}_{1},\\ldots,h_{n},K\\dot{\\tau}_{n}\\right)\\geq t^{*}\\left(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}\\right)$ ", "page_idx": 7}, {"type": "text", "text": "6.4 Examples ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now list several examples, starting with simple corner/extreme cases. One can find the derivations in Sec. F. For brevity, we will sometimes write $t^{*}$ instead of $t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})$ ", "page_idx": 7}, {"type": "text", "text": "Example 6.3. [Infinitely Fast Worker] If exists $j\\in[n]$ such that $\\tau_{j}=0$ and $h_{j}=0$ , then $t^{*}=0$ Example 6.4. [Infinitely Slow Workers] If $\\tau_{i}=\\infty$ and $h_{i}=\\infty$ for all $i\\in[n]$ , then $t^{*}=\\infty$ ", "page_idx": 7}, {"type": "text", "text": "Example 6.5. [Equal Performance] If $\\tau_{i}=\\tau$ and $h_{i}=h$ for all $i\\in[n]$ , then9 ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t^{*}\\le6\\operatorname*{max}\\left\\{h,\\tau,\\frac{\\tau\\omega}{n},\\frac{h\\sigma^{2}}{n\\varepsilon},\\sqrt{\\frac{\\tau h\\sigma^{2}\\omega}{n\\varepsilon}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the next example, we consider the setting from Sec. 2.1. Example 6.6 and Corollary 4.3 restore the optimal rate (4) of Rennala SGD. ", "page_idx": 7}, {"type": "text", "text": "Example 6.6. [Infinitely Fast Communication] If $\\tau_{i}=0$ for all $i\\in[n]$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t^{*}\\leq2\\operatorname*{min}_{m\\in[n]}\\operatorname*{max}\\left\\{h_{\\pi_{m}},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\underset{i=1}{\\overset{m}{\\sum}}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\right\\}=\\Theta\\left(\\underset{m\\in[n]}{\\operatorname*{min}}\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\left(1+\\frac{\\sigma^{2}}{m\\varepsilon}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pi$ is a permutation that sorts $\\{h_{i}\\}_{i=1}^{n}$ ", "page_idx": 7}, {"type": "text", "text": "The following two examples show that $t^{*}$ is robust to slow workers or workers that do not participate. Example 6.7.[Ignoring Slow Workers]If $h_{i}$ and $\\tau_{i}$ are fixed and finite for all $i~\\leq~p$ , and $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\,=\\,\\bar{m}\\,\\in\\,\\mathbb{R}$ for all $i\\,>\\,p$ , then, for $m$ large enough,we have $t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\bar{\\tau_{i}}]_{1}^{n})=$ $t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{p})$ ", "page_idx": 7}, {"type": "text", "text": "Example 6.8. [Partial Participation]  If $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}~=~\\infty$ for all $i\\;\\;>\\;\\;p\\;\\;\\geq\\;\\;1$ ,then $t^{*}(\\omega,\\ ^{\\bar{\\sigma}^{2}}/\\varepsilon,[h_{i},\\bar{\\tau_{i}}]_{1}^{n})=t^{*}(\\omega,\\sigma^{2}/\\bar{\\varepsilon_{\\cdot}}[h_{i},\\bar{\\tau_{i}}]_{1}^{p})$ ", "page_idx": 7}, {"type": "text", "text": "From the proof, it is clear that the result is tight up to a constant factor. ", "page_idx": 7}, {"type": "text", "text": "7 Comparison with Baselines ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the previous sections, we did not invoke any assumptions about the compressors except for Def. 2.1. Inspired by Property 6.2, to make the comparisons with the baselines easier, we consider the Rand $K$ compressor with $K=1$ . Using Theorem D.2, we have $\\omega=d-1$ . We also assume that worker $i$ takes $\\dot{\\tau}_{i}$ seconds to send one coordinate to the server; thus $\\tau_{i}=\\dot{\\tau}_{i}$ , since we use Rand1. Also, it takes $d\\dot{\\tau}_{i}$ to send a non-compressed vector for all $i\\in[n]$ ", "page_idx": 8}, {"type": "text", "text": "Minibatch SGD and QSGD. It is well known (Lan, 2020) that the number of iterations of Minibatch SGD required to find an $\\varepsilon,$ -solution isO $\\left(L\\Delta/\\varepsilon+\\sigma^{2}L\\Delta/\\varepsilon^{2}\\right)$ . In Minibatch SGD, each worker calculates one stochastic gradient and sends a non-compressed vector. Since the server waits for the slowest worker, the time complexity of such method (up to a constant factor) is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{MB}}:=\\underset{i\\in[n]}{\\operatorname*{max}}\\left(h_{i}+d\\dot{\\tau}_{i}\\right)\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In Sec. J, we compare (16) with (10) and show Comparison 7.1. $T_{*}=\\mathrm{O}(T_{\\mathrm{MB}})$ ", "page_idx": 8}, {"type": "text", "text": "However, there are many regimes when $T_{*}\\ll T_{\\mathrm{MB}}$ . For instance, if $\\operatorname*{max}\\{h_{i},\\dot{\\tau}_{i}\\}=\\infty$ for some worker (Example 6.8), then $T_{\\mathrm{MB}}=\\infty$ and $T_{*}<\\infty$ . Also, under the conditions of Example 6.7, if $m\\rightarrow\\infty$ , we get $T_{\\mathrm{MB}}\\rightarrow\\infty$ whereas $T_{*}$ is bounded. The same reasoning applies to QSGD because its time complexity (6) depends on $\\mathrm{max}_{i\\in[n]}\\left(h_{i}+\\dot{\\tau}_{i}\\right).$ Due to Theorem O.5, up to a constant factor, $T_{*}$ is less or equal to (6); see also Table 1. ", "page_idx": 8}, {"type": "text", "text": "Rennala SGD and Asynchronous SGD. When the communication time is negligible, Tyurin and Richtarik (2023c) proved that the optimal time complexity is attained by Rennala SGD. When $\\dot{\\tau}_{i}\\rightarrow0$ for all $i\\in[n]$ , we show in Example 6.6 that (10) is the same as the time complexity of Rennala SGD obtained by Tyurin and Richtarik (2023c). Assume ${\\dot{\\tau}}_{i}>0$ for all $i\\in[n]$ . We can apply the result from Theorem O.5 to Rennala SGD, thus the time complexity of Rennala SGD is not better than ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{R}}:=\\frac{L\\Delta}{\\varepsilon}\\times t^{\\ast}(0,\\sigma^{2}/\\varepsilon,h_{1},d\\dot{\\tau}_{1},\\ldots,h_{n},d\\dot{\\tau}_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that Asynchronous SGD also has the same lower bound. In Sec. J, we compare (17) with (10) andshow ", "page_idx": 8}, {"type": "text", "text": "Comparison 7.2. $T_{*}=\\mathrm{O}(T_{\\mathrm{R}})$ ", "page_idx": 8}, {"type": "text", "text": "7.1  Shadowheart SGD is strictly better in many regimes. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Due to the non-explicit nature, it is not transparent that the time complexity of Shadowheart SGD is universally strictly better than in all baselines in many practical regimes. Let us prove it. Take ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{i}=h\\;{\\mathrm{for~all~}}i<n,h_{n}=\\infty\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Due to (16) and (17), the time complexities of Minibatch SGD and QSGD are $\\infty$ ,and thetime complexities of Asynchronous SGD and Rennala SGD are not smaller than ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Omega\\left(\\operatorname*{max}\\left\\{h,d\\dot{\\tau},\\frac{h\\sigma^{2}}{(n-1)\\varepsilon}\\right\\}\\frac{L\\Delta}{\\varepsilon}\\right),\\mathrm{which}\\stackrel{n\\to\\infty}{\\rightarrow}\\Omega\\left(\\operatorname*{max}\\left\\{h,d\\dot{\\tau}\\right\\}\\frac{L\\Delta}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "From (14), the time complexity of Shadowheart SGD is at most ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{O}\\left(\\operatorname*{max}\\left\\{h,\\dot{\\tau},\\frac{d\\dot{\\tau}}{n-1},\\frac{h\\sigma^{2}}{(n-1)\\varepsilon},\\sqrt{\\frac{\\dot{\\tau}h\\sigma^{2}d}{(n-1)\\varepsilon}}\\right\\}\\frac{L\\Delta}{\\varepsilon}\\right),\\mathrm{which~}\\stackrel{n\\to\\infty}{\\longrightarrow}\\mathrm{O}\\left(\\operatorname*{max}\\left\\{h,\\dot{\\tau}\\right\\}\\frac{L\\Delta}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Thus, Shadowheart SGD can be $d$ times faster than all previous methods if the number of workers $n$ is large. Using the same reasoning, Shadowheart SGD can be $n-1$ times faster if the dimension $d$ or $\\dot{\\tau}$ is large. Due to the continuity of the complexities, such huge differences hold even if we take $n<\\infty$ , and start considering more heterogenous times $h_{i}$ and $\\tau_{i}$ by perturbating (18). ", "page_idx": 8}, {"type": "text", "text": "7.2 The fastest worker works locally ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Another important baseline is the vanilla SGD method, which works on the fastest worker, does not communicate with the server, and performs local steps (non-centralized method). For simplicity, assumethat $\\sigma^{2}/\\varepsilon\\,\\geq\\,1$ . Then, the time complexity of such an algorithm (Lan, 2020) is $T_{\\mathrm{SGD}}:=$ $\\begin{array}{r}{\\operatorname*{min}_{i\\in[n]}h_{i}\\times\\sigma^{2}L\\Delta/\\varepsilon^{2}}\\end{array}$ . Clearly, comparing $T_{\\mathrm{SGD}}$ and (10), if $\\dot{\\tau}_{i}$ are large enough, then $T_{\\mathrm{SGD}}$ can be smaller than $T_{*}$ . However, this does not contradict our lower bounds because this method does not satisfy the conditions of Theorem O.5: it does not communicate with the server. In other words, if the communication channel is too slow, it does not make sense to communicate. One may now ask: \u201cUnder which conditions is it beneficial to communicate? Comparing $T_{\\mathrm{SGD}}$ and (10), one can see that ( 10) is better when $\\begin{array}{r}{t^{*}(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\dots,h_{n},\\dot{\\tau}_{n})\\leq\\operatorname*{min}_{i\\in[n]}\\check{h_{i}}\\times\\check{\\sigma^{2}}/\\varepsilon}\\end{array}$ It is sufficient to substitute the initial parameters to this inequality and decide which method to use. For instance, in the view of Example 6.5, one should compare $\\operatorname*{max}\\{h,\\dot{\\tau},\\dot{\\tau}(d\\!-\\!1)\\big/{n},h\\sigma^{2}\\big/{n}\\varepsilon$ $\\sqrt{\\dot{\\tau}h\\sigma^{2}(d\\!-\\!1)/n\\varepsilon}\\Big\\}$ Vs. $h\\sigma^{2}/\\varepsilon$ In the regime when $n$ is large enough or $\\varepsilon$ is small enough, we have $t^{*}<h\\sigma^{2}/\\varepsilon$ , and Alg. 7 has better convergence guarantees. On the other hand, if $\\dot{\\tau}$ is large enough, then it is possible that $t^{*}>h\\sigma^{2}/\\varepsilon$ ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, ii) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work of A.T. was partially supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. (2017). QSGD: Communicationefficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems (NIPS), pages 1709-1720.   \nArjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2022). Lower bounds for non-convex stochastic optimization. Mathematical Programming, pages 1-50.   \nBeznosikov, A., Horvath, S., Richtarik, P., and Safaryan, M. (2020). On biased compression for distributed learning. arXiv preprint arXiv:2002.12410.   \nCarmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for finding stationary points i. Mathematical Programming, 184(1):71-120.   \nChen, J., Pan, X., Monga, R., Bengio, S., and Jozefowicz, R. (2016). Revisiting distributed synchronous sgd. arXiv preprint arXiv:1604.00981.   \nCohen, A., Daniely, A., Drori, Y., Koren, T., and Schain, M. (2021). Asynchronous stochastic optimization robust to arbitrary delays. Advances in Neural Information Processing Systems, 34:9024-9035.   \nDutta, S., Joshi, G., Ghosh, S., Dube, P., and Nagpurkar, P. (2018). Slow and stale gradients can win the race: Error-runtime trade-offs in distributed SGD. In International Conference on Artificial Intelligence and Statistics, pages 803-812. PMLR.   \nFeyzmahdavian, H. R., Aytekin, A., and Johansson, M. (2016). An asynchronous mini-batch algorithm for regularized stochastic optimization. IEEE Transactions on Automatic Control, 61(12):3740-3754.   \nGauthier, F., Gogineni, V. C., Werner, S., Huang, Y.-F., and Kuh, A. (2023). Asynchronous online federated learning with reduced communication requirements. arXiv preprint arXiv:2303.15226.   \nGhadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368.   \nGorbunov, E., Burlachenko, K., Li, Z., and Richtarik, P. (2021). MARINA: Faster non-convex distributed learning with compression. In 38th International Conference on Machine Learning.   \nGower, R. M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and Richtarik, P. (2019). SGD: General analysis and improved rates. In International Conference on Machine Learning, pages 5200-5209. PMLR.   \nGruntkowska, K., Tyurin, A., and Richtarik, P. (2023). EF21-P and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression. In International Conference on Machine Learning, pages 11761-11807. PMLR.   \nHorvath, S., Ho, C.-Y., Horvath, v., Sahu, A. N., Canini, M., and Richtarik, P. (2022). Natural compression for distributed deep learning. In Mathematical and Scientific Machine Learning, pages 129-141. PMLR.   \nHuang, X., Chen, Y., Yin, W., and Yuan, K. (2022). Lower bounds and nearly optimal algorithms in distributed learning with communication compression. arXiv preprint arXiv:2206.03665.   \nJia, J., Liu, J.,Zhou, C., Tian, H., Dong, M., and Dou, D. (2023). Efcient asynchronous federated learning with sparsification and quantization. arXiv preprint arXiv:2312.15186.   \nKairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in federated learning. Foundations and Trends? in Machine Learning, 14(1-2):1-210.   \nKhaled, A.and Richtarik, P. (2020). Better theory for SGD in the nonconvex world. arXiv preprint arXiv:2002.03329.   \nKoloskova, A., Stich, S. U., and Jaggi, M. (2022). Sharper convergence guarantees for asynchronous SGD for distributed and federated learning. arXiv preprint arXiv:2206.08307.   \nKonecny, J., McMahan, H. B., Yu, F. X., Richtarik, P., Suresh, A. T., and Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.   \nLan, G. (2020). First-order and stochastic optimization methods for machine learning. Springer.   \nLeCun, Y., Cortes, C., and Burges, C. (2010). Mnist handwritten digit database. ATT Labs [Online . Available: http://yann.lecun.com/exdb/mnist, 2.   \nLi, Z., Kovalev, D., Qian, X., and Richtarik, P. (2020). Acceleration for compressed gradient descent in distributed and federated optimization. In International Conference on Machine Learning.   \nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017). Communicationefficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273-1282. PMLR.   \nMishchenko, K., Bach, F., Even, M., and Woodworth, B. (2022). Asynchronous SGD beats minibatch SGD under arbitrary delays. arXiv preprint arXiv:2206.07638.   \nMishchenko, K., Gorbunov, E., Takac, M., and Richtarik, P. (2019). Distributed learning with compressed gradient differences. arXiv preprint arXiv: 1901.09269.   \nNemirovskij, A. S. and Yudin, D. B. (1983). Problem complexity and method efficiency in optimization.   \nNesterov, Y. (2003).\u3002 Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media.   \nNesterov, Y. (2018). Lectures on convex optimization, volume 137. Springer.   \nNguyen, L., Nguyen, P. H., Dijk, M., Richtarik, P., Scheinberg, K., and Takac, M. (2018). SGD and hogwild! convergence without the bounded gradients assumption. In International Conference on Machine Learning, pages 3750-3758. PMLR.   \nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021). Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR.   \nRecht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in Neural Information Processing Systems, 24. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Richtarik, P., Sokolov, I., and Fatkhullin, I. (2021). EF21: A new, simpler, theoretically better, and practically faster error feedback. arXiv preprint arXiv:2106.05203. ", "page_idx": 11}, {"type": "text", "text": "Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. (2014). 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference oftheInternationalSpeechCommunicationAssociation.   \nTyurin, A. and Richtarik, P. (2023a). A computation and communication efficient method for distributed nonconvex problems in the partial participation setting. Advances in Neural Information Processing Systems (NeurIPS).   \nTyurin, A. and Richtarik, P. (2023b). DASHA: Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization. 1lth International Conference on Learning Representations (ICLR).   \nTyurin, A. and Richtarik, P. (2023c). Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. Advances in Neural Information Processing Systems (NeurIPS).   \nVogels, T., Karimireddy, S. P., and Jaggi, M. (2019). PowerSGD: Practical low-rank gradient compression for distributed optimization. In Neural Information Processing Systems.   \nWang, J., Lu, Y., Yuan, B., Chen, B., Liang, P., De Sa, C., Re, C., and Zhang, C. (2023). Cocktailsgd: Fine-tuning foundation models over 500mbps networks. In International Conference on Machine Learning, pages 36058-36076. PMLR.   \nXu, H., Ho, C.-Y., Abdelmoniem, A. M., Dutta, A., Bergou, E. H., Karatsenidis, K., Canini, M., and Kalnis, P. (2021a). Grace: A compressed communication framework for distributed machine learning. In 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS), pages 561-572. IEEE.   \nXu, H., Kostopoulou, K., Dutta, A., Li, X., Ntoulas, A., and Kalnis, P. (2021b). Deepreduce: A sparsetensor communication framework for federated deep learning. Advances in Neural Information Processing Systems, 34:21150-21163. ", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1Introduction ", "page_idx": 12}, {"type": "text", "text": "2Related Work 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "2.1 Communication time can be ignored 2   \n2.2 Communication time is a factor . 3 ", "page_idx": 12}, {"type": "text", "text": "3 Summary of Contributions 4 ", "page_idx": 12}, {"type": "text", "text": "4.1 Tighter result with per-iteration times $h_{i}^{k}$ and $\\tau_{i}^{k}$ 6   \n4.2 On the problem of estimating the times in Algorithms 1 and 4 6 ", "page_idx": 12}, {"type": "text", "text": "5Lower Bound ", "page_idx": 12}, {"type": "text", "text": "6Equilibrium Time ", "page_idx": 12}, {"type": "text", "text": "6.1 Calculation strategy   \n6.2 Intuition behind the equilibrium time $t^{*}$ 7   \n6.3 Properties of the equilibrium time $t^{*}$ 8   \n6.4 Examples 8   \nComparison with Baselines 9   \n7.1 Shadowheart SGD is strictly better in many regimes. 9   \n7.2 The fastest worker works locally 9   \nA  Bidirectional Compression 15   \nB Frequently Used Notation 16   \nCBasic Facts 16   \nD RandK Compressor 17   \nE Proofs of the Properties of the Equilibrium Time 17   \nF Derivations of the Examples for the Equilibrium Time 24   \nG Generic Lemma For Unbiased Gradient Estimators 27   \nH Proofs for Algorithms 1 and 4 30   \nThe Classical SGD Theorem 32   \nComparison with Baselines 33 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "K Description of Alg. 5 in the Bidirectional Setting 35 ", "page_idx": 13}, {"type": "text", "text": "L Proofs for Alg. 5 35 ", "page_idx": 13}, {"type": "text", "text": "M Development of Adaptive Shadowheart SGD 37 ", "page_idx": 13}, {"type": "text", "text": "N Proofs for Alg. 7 38 ", "page_idx": 13}, {"type": "text", "text": "O Construction of the Lower Bound 42 ", "page_idx": 13}, {"type": "text", "text": "P Proof of Theorem O.5 44 ", "page_idx": 13}, {"type": "text", "text": "P.1 The \u201cWorst Case\" Function 44   \nP.1.1 Proof of Lemma P.2 46   \nP.2 Proof of Lemma P.3 . 52   \nP.3 Another Construction 54 ", "page_idx": 13}, {"type": "text", "text": "Q Experiments 55 ", "page_idx": 13}, {"type": "text", "text": "Q.1  Experiments with Logistic Regression 55   \nQ.2  Experiments with quadratic optimization tasks and multiplicative noise . 56   \nQ.2.1 Discussion of the experiments from Sec. Q.2.2 56   \nQ.2.2Plots 56   \nQ.3  Experiments with quadratic optimization tasks and additive noise 57   \nQ.3.1Plots 58 ", "page_idx": 13}, {"type": "text", "text": "A  Bidirectional Compression ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we discuss a simple way to use the Shadowheart SGD techniques in the setup when broadcasting is expensive (Line 7 in Alg. 1); i.e., when $\\tau_{\\mathrm{serv}}\\gg0$ . We will employ the following family of compressors. ", "page_idx": 14}, {"type": "text", "text": "Definition A.1. A mapping $\\mathcal{C}\\,:\\,\\mathbb{R}^{d}\\times\\mathbb{S}_{\\nu}\\to\\mathbb{R}^{d}$ is a biased compressor if there exists $\\alpha\\in(0,1]$ suchthat ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\nu}\\left[\\left\\|\\mathcal{C}(x;\\nu)-x\\right\\|^{2}\\right]\\leq\\left(1-\\alpha\\right)\\left\\|x\\right\\|^{2},\\;\\forall x\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We shall use the shortcut $\\mathcal{C}(x;\\nu)\\equiv\\mathcal{C}(x)$ , and denote the family of such biased compressors as $\\mathbb{B}(\\alpha)$ ", "page_idx": 14}, {"type": "text", "text": "The family $\\mathbb{B}(\\alpha)$ is more general than $\\mathbb{U}(\\omega)$ in the sense that if $\\mathcal{C}\\,\\in\\,\\mathbb{U}(\\omega)$ , then $(\\omega+1)^{-1}\\mathcal{C}\\in$ $\\mathbb{B}((\\omega+1)^{-1})$ . It includes the $\\mathrm{Top}K$ and Rank $K$ compressors (Vogels et al., 2019; Beznosikov et al., 2020), among many others. ", "page_idx": 14}, {"type": "text", "text": "Let $\\mathcal{C}_{\\mathrm{serv}}\\in\\mathbb{B}(\\alpha)$ be the compressor used by the server. We use the primal error-feedback mechanism EF21-P (Gruntkowska et al., 2023) which requires us to add the following changes to Alg. 1 and Alg. 2. We add the steps ", "page_idx": 14}, {"type": "equation", "text": "$$\np^{k+1}=\\mathcal{C}_{\\mathrm{serv}}(x^{k+1}-w^{k}),\\quad w^{k+1}=w^{k}+p^{k+1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "to Alg. 1 and broadcast $p^{k+1}$ instead of $x^{k}$ . This change leads to Bidirectional Shadowheart SGD (Alg. 5). In Alg. 2, the workers should receive $p^{k+1}$ , calculate $w^{k+1}$ , and use $w^{k}$ instead of $x^{k}$ in the calculations of stochastic gradients. We provide the pseudo-codes of these algorithms in Sec. K. ", "page_idx": 14}, {"type": "text", "text": "Our main results are: ", "page_idx": 14}, {"type": "text", "text": "Theorem A.2. Let Asmptions 1.1 1.2 1.3, 2.2 hold. Choose $\\begin{array}{r}{\\gamma=\\frac{\\alpha}{16L}}\\end{array}$ Then as long as $K\\ge$ 76\u25b3 irectionalShadwhert GD(Alg  guarats tofndanstationary pot. ", "page_idx": 14}, {"type": "text", "text": "Corollary A.3. If the broadcast time of $\\mathcal{C}_{\\mathrm{serv}}$ is not greater than $\\tau_{\\mathrm{serv}}$ , then Bidirectional Shadowheart SGD (Alg. 5) converges after at most ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\ast,\\mathrm{serv}}:=\\frac{768L\\Delta}{\\alpha\\varepsilon}\\times\\left(\\tau_{\\mathrm{serv}}+2t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "seconds. ", "page_idx": 14}, {"type": "text", "text": "Remark A.4. If the broadcast cost can't be ignored, the time complexity of Alg. 1 changes from (10) to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{*}:=\\frac{16L\\Delta}{\\varepsilon}\\times(\\tau_{\\mathrm{serv}}^{\\mathrm{full}}+2t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\tau_{\\mathrm{serv}}^{\\mathrm{full}}$ isthe tie requirdtbroadcast afulnn-compressed vr ", "page_idx": 14}, {"type": "text", "text": "We should compare (21) obtained by the unidirectional algorithm and (20) obtained by the bidirectional algorithm. Consider that $\\mathcal{C}_{\\mathrm{serv}}=\\mathrm{Top}K$ with $K\\leq d$ We can see (20) that depends on $\\tau_{\\mathrm{serv}}$ \uff0c that is much less than Tsolv because $K\\ll d$ At the same time,(20) is $^1\\!/\\!\\alpha$ times larger than (21). This is a standard price for the fact that we use a biased compressor (e.g. (Richtarik et al., 2021; Gruntkowska et al., 2023)). However, $\\alpha$ is very close 1 in practice (Beznosikov et al., 2020; Vogels et al., 2019; Xu et al., 2021b). It turns out that we can always choose $K$ in $\\mathrm{Top}K$ (we take this compressor as an example) in such a way that Alg. 5 is never worse than Alg. 1. ", "page_idx": 14}, {"type": "text", "text": "Comparison A.5. Assume that it takes $\\dot{\\tau}_{\\mathrm{serv}}$ seconds to send one coordinate from the server to the workers. If we take $K\\geq\\operatorname*{min}\\left\\{d,t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})/\\dot{\\tau}_{\\mathrm{serv}}\\right\\}$ in $\\mathrm{Top}K$ , then $T_{*,\\mathrm{serv}}=\\mathrm{O}\\left(T_{*}\\right)$ ", "page_idx": 14}, {"type": "text", "text": "If-full $t^{*}/_{\\dot{\\tau}_{\\mathrm{serv}}}\\ll d$ $\\tau_{\\mathrm{serv}}^{\\mathrm{full}}\\,=\\,d\\dot{\\tau}_{\\mathrm{serv}}$ $\\mathrm{Top}K$ $d\\dot{\\tau}_{\\mathrm{serv}}\\gg t^{*}$ thenone cantake $K=$ ", "page_idx": 14}, {"type": "text", "text": "B Frequently Used Notation ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "O8yHsRLwPl/tmp/5fb802fb820907fa5426fa36097bbef77851c672faf6533d4bb55fb1eb384f54.jpg", "table_caption": ["We thought a table of frequently used notation could be useful. Here it is: "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Basic Facts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we collect some basic facts which are used repeatedly in the proofs. ", "page_idx": 15}, {"type": "text", "text": "Variance decomposition. Let $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ be a random vector with finite mean and finite variance. Then for any deterministic vector $c\\in\\mathbb{R}^{d}$ , we have the identity ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert x-\\mathbb{E}\\left[x\\right]\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert x-c\\right\\Vert^{2}\\right]-\\left\\Vert\\mathbb{E}\\left[x\\right]-c\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma C.1. Consider a sequence $q_{1},\\dots,q_{n}\\in[0,1]$ then ", "page_idx": 15}, {"type": "equation", "text": "$$\n1-\\sum_{m=1}^{n}q_{m}\\leq\\prod_{m=1}^{n}\\left(1-q_{m}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We prove by induction. For $n=1$ i it true: $\\begin{array}{r}{1-\\sum_{m=1}^{1}q_{m}=\\prod_{m=1}^{1}\\left(1-q_{m}\\right).}\\end{array}$ Assume that that it is true for $n-1$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n1-\\sum_{m=1}^{n-1}q_{m}\\leq\\prod_{m=1}^{n-1}\\left(1-q_{m}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Multiply both parts by $1-q_{n}\\in[0,1]$ to obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\prod_{m=1}^{n}\\left(1-q_{m}\\right)\\geq\\left(1-q_{n}\\right)\\left(1-\\sum_{m=1}^{n-1}q_{m}\\right)=1-\\sum_{m=1}^{n-1}q_{m}-q_{n}+q_{n}\\left(\\sum_{m=1}^{n-1}q_{m}\\right)\\geq1-\\sum_{m=1}^{n}q_{m}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $q_{m}\\in[0,1]$ for all $m\\in[n]$ ", "page_idx": 15}, {"type": "text", "text": "D RandK Compressor ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Definition D.1. Assume that $S$ is a random subset from $[d],|S|=K,K\\in[d]$ . A stochastic mapping $\\mathcal{C}:\\mathbb{R}^{d}\\times\\mathbb{S}_{\\nu}\\rightarrow\\mathbb{R}^{d}$ is Rand $K$ if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\boldsymbol{x};S)=\\frac{d}{K}\\sum_{j\\in S}x_{j}e_{j},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\{e_{i}\\}_{i=1}^{d}$ is the standard unit basis. ", "page_idx": 16}, {"type": "text", "text": "Theorem D.2. If $\\mathcal{C}$ is RandK,then $\\mathcal{C}\\in\\mathbb{U}\\left(\\frac{d}{k}-1\\right)$ One can find the proof in (Beznosikov et al., 2020). ", "page_idx": 16}, {"type": "text", "text": "E Proofs of the Properties of the Equilibrium Time ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Property 4.1. If all inputs of the equilibrium time are non-negative, then the equilibrium time is well defined. ", "page_idx": 16}, {"type": "text", "text": "Proof.   \n(Part 1: $s^{*}(j)$ is well-defined)   \nFirst, we show that $s^{*}(j)$ is well-defined for all $j\\in[n]$ . We fix $j\\in[n]$ and consider the equation from Def. 3.1: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underbrace{\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}}_{\\phi(s)}=\\underbrace{s}_{\\psi(s)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "w.r.t $s$ . The function $\\phi(s)$ is a non-increasing function for all $s\\geq0$ , and the function $\\psi(s)$ is an increasing function for all $s\\geq0$ . Let us consider two cases. ", "page_idx": 16}, {"type": "text", "text": "1) Exists p\u2264 j such that T\u03c0,w = 0 and $\\frac{h_{\\pi_{p}}\\sigma^{2}}{\\varepsilon}=0$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi(s)=\\left(\\sum_{i\\neq p}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}+\\frac{1}{0}\\right)^{-1}=\\left(\\infty\\right)^{-1}=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $s\\geq0$ , then the only solution to the equation is $s=0$ 2) Otherwise, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi(s)=\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\geq\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $s\\geq0$ Then $\\phi(0)>0$ (can be equal to $\\infty$ ). Using $\\psi(0)=0$ and the monotonicity of the functions, one can show the unique solution (greater zero) exists. ", "page_idx": 16}, {"type": "text", "text": "If a permutation $\\pi$ is unique, then the formula $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}}\\end{array}$ is well-defined and we can finish the proof. ", "page_idx": 16}, {"type": "text", "text": "(Part 2: non-unique permutation)   \nWe  assume  that  there exists $\\begin{array}{r l r}{i}&{{}\\in}&{[n]}\\end{array}$ such  that $\\begin{array}{r l r}{\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}&{{}<}&{\\infty}\\end{array}$ .Otherwise, $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}=\\infty}\\end{array}$ for any permutation. Next, note that $s^{*}(j+1)\\leq s^{*}(j)$ for all $j<n$ because ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j+1}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $s\\geq0$ . We will use this property later. ", "page_idx": 16}, {"type": "text", "text": "Consider that there are two non-equal permutations $\\pi$ and $\\bar{\\pi}$ that sort the pairs $(h_{i},\\tau_{i})$ by $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}$ and there are two corresponding solutions $s^{*}(j)$ and $\\bar{s}^{*}(j)$ . Each permutation divides the pairs $(h_{i},\\tau_{i})$ into the same equivalence classes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{nax}\\{h_{\\bar{\\pi}_{1}},\\tau_{\\bar{\\pi}_{1}}\\}=\\cdots=\\operatorname*{max}\\{h_{\\bar{\\pi}_{j_{1}}},\\tau_{\\bar{\\pi}_{j_{1}}}\\}<\\operatorname*{max}\\{h_{\\bar{\\pi}_{j_{1}+1}},\\tau_{\\bar{\\pi}_{j_{1}+1}}\\}=\\cdots=\\operatorname*{max}\\{h_{\\bar{\\pi}_{j_{2}}},\\tau_{\\bar{\\pi}_{j_{2}}}\\}<\\cdots.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The order within each class can be different, but the elements are the same. Next, since $s^{*}(j+1)\\leq$ $s^{*}(j)$ for all $j<n$ , we can conclude that the minimum in ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is attained for $j^{*}$ such that $\\operatorname*{max}\\{h_{\\pi_{j^{\\ast}}},\\tau_{\\pi_{j^{\\ast}}}\\}<\\operatorname*{max}\\{h_{\\pi_{j^{\\ast}+1}},\\tau_{\\pi_{j^{\\ast}+1}}\\}$ $(\\operatorname*{max}\\{h_{\\pi_{n+1}},\\tau_{\\pi_{n+1}}\\}\\equiv\\infty)$ Since $\\operatorname*{max}\\{h_{\\pi_{j^{\\ast}}},\\tau_{\\pi_{j^{\\ast}}}\\}<\\operatorname*{max}\\{h_{\\pi_{j^{\\ast}+1}},\\bar{\\tau}_{\\pi_{j^{\\ast}+1}}\\}$ we have $\\operatorname*{max}\\{h_{\\pi_{j^{\\ast}}},\\tau_{\\pi_{j^{\\ast}}}\\}\\,=\\,\\operatorname*{max}\\{h_{\\bar{\\pi}_{j^{\\ast}}},\\tau_{\\bar{\\pi}_{j^{\\ast}}}\\}$ Therefore,weobtain ", "page_idx": 17}, {"type": "text", "text": "$\\operatorname*{min}_{\\varepsilon^{[\\mathrm{s}]}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}\\ast},\\tau_{\\pi_{j}\\ast}\\},s^{*}(j^{\\ast})\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j^{\\ast}}},\\tau_{\\bar{\\pi}_{j}\\ast}\\},s^{*}(j)\\}$ j\u2208[n] ", "page_idx": 17}, {"type": "text", "text": "Also, for all $j\\in[n]$ such that $\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\}<\\operatorname*{max}\\{h_{\\pi_{j+1}},\\tau_{\\pi_{j+1}}\\}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\bar{\\pi}_{i}}\\omega+\\frac{4\\tau_{\\bar{\\pi}_{i}}h_{\\bar{\\pi}_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{\\bar{\\pi}_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we get $s^{*}(j^{*})=\\bar{s}^{*}(j^{*})$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{nin}_{\\epsilon[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}\\ast},\\tau_{\\bar{\\pi}_{j}\\ast}\\},\\bar{s}^{*}(j^{*})\\}\\geq\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},\\tau_{\\bar{\\pi}_{j}}\\},\\bar{s}^{*}(j^{*})\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the same reasoning, we can show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},\\tau_{\\bar{\\pi}_{j}}\\},\\bar{s}^{*}(j)\\}\\geq\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It means that $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},\\tau_{\\bar{\\pi}_{j}}\\},\\bar{s}^{*}(j)\\}=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}}\\end{array}$ , thus the final result of the mapping does not depend on a chosen permutation. ", "page_idx": 17}, {"type": "text", "text": "Property 6.1. 1If $\\\"\\bar{\\omega}\\ge\\omega\\ge0,\\bar{\\sigma}^{2}/\\bar{\\varepsilon}\\ge\\sigma^{2}/\\varepsilon\\ge0,\\bar{h}_{1}\\ge h_{1}\\ge0,\\bar{\\tau}_{1}\\ge\\tau_{1}\\ge0,\\dots,\\bar{h}_{n}\\ge h_{n}\\ge0$ and$\\bar{\\tau}_{n}\\geq\\tau_{n}\\geq0$ , then $t^{*}(\\bar{\\omega},\\bar{\\sigma}^{2}/\\bar{\\varepsilon},[\\bar{h}_{i},\\bar{\\tau}_{i}]_{1}^{n})\\geq t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$", "page_idx": 17}, {"type": "text", "text": "Proof. Assume that $\\pi$ is a permutation that sorts the pairs $(h_{i},\\tau_{i})$ by $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}$ , and $\\bar{\\pi}$ is a permutation that sorts the pairs $(\\bar{h}_{i},\\bar{\\tau}_{i})$ by $\\operatorname*{max}\\{\\bar{h}_{i},\\bar{\\tau}_{i}\\}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq\\left(\\sum_{i=1}^{j}\\frac{1}{2\\bar{\\tau}_{\\bar{\\pi}_{i}}\\bar{\\omega}+\\frac{4\\bar{\\tau}_{\\pi_{i}}\\bar{h}_{\\pi_{i}}\\bar{\\sigma}^{2}\\bar{\\omega}}{s\\bar{\\varepsilon}}+\\frac{2\\bar{h}_{\\pi_{i}}\\bar{\\sigma}^{2}}{\\bar{\\varepsilon}}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $j\\in[n]$ . It means $\\bar{s}^{*}(j)\\geq s^{*}(j)$ , where $s^{*}(j)$ and $\\bar{s}^{*}(j)$ are the solutions of the equation (7) with the pairs $(h_{i},\\tau_{i})$ and $(\\bar{h}_{i},\\bar{\\tau}_{i})$ and corresponding permutations $\\pi$ and $\\bar{\\pi}$ . Also, we have $\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\}\\leq\\operatorname*{max}\\{\\bar{h}_{\\bar{\\pi}_{j}},\\bar{\\tau}_{\\bar{\\pi}_{j}}\\}$ for all $j\\in[n]$ . Therefore, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^*(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}\\leq\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{\\operatorname*{max}\\{\\bar{h}_{\\bar{\\pi}_{j}},\\bar{\\tau}_{\\bar{\\pi}_{j}}\\},\\bar{s}^{*}(j)\\}\\leq\\underset{j\\in[n]}{\\operatorname*{min}}\\mathrm{~anx}\\{\\operatorname*{max}\\{\\operatorname*{max}\\{\\bar{h}_{\\bar{\\pi}_{j}},\\bar{\\tau}_{\\bar{\\pi}_{j}}\\},\\bar{s}^{*}(j)\\}\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=t^{*}(\\bar{\\omega},\\bar{\\sigma}^{2}/\\varepsilon,\\bar{h}_{1},\\bar{\\tau}_{1},\\dots,\\bar{h}_{n},\\bar{\\tau}_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Property E.1. For all $c\\in(0,1]$ and $\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\geq0$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nt^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\geq c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Using the definition of the equilibrium time, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $s^{*}(j)$ is the solution of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\nt^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s_{c}^{*}(j)\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $s_{c}^{*}(j)$ is the solution of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2c\\tau_{\\pi_{i}}\\omega+\\frac{4c^{2}\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2c h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using simple algebra, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2c\\tau_{\\pi_{i}}\\omega+\\frac{4c^{2}\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2c h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=c\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{\\frac{s}{c}\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, $s_{c}^{*}(j)$ is the solution of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{\\frac{s}{c}\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=\\frac{s}{c}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Comparing (24) and (25), one can see that $s_{c}^{*}(j)=c\\times s^{*}(j)$ for all $j\\in[n]$ . Using this and $c\\in(0,1]$ weget ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{t^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},c\\times s^{*}(j)\\}}\\\\ &{}&{\\mathrm{~\\\\\\\\\\\\\\}\\geq c\\times\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}=c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Property E.2. For all $c\\geq1$ and $\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\geq0$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nt^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The proof of this property repeats the proof of Property E.1 up to the last inequality. Using $c\\geq1$ weget ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{t^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},c\\times s^{*}(j)\\}}\\\\ &{}&{\\quad\\le c\\times\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}=c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Property E.3. For all $c\\in(0,1]$ and $\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\geq0$ we have $t^{*}(c\\,\\times\\,_{\\!\\!\\mathscr{s}}\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\;\\;\\geq\\;\\;\\dot{c}\\;\\times\\;t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$ and $t^{*}(\\omega,c\\,\\times\\,\\,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\ \\ \\geq\\ \\ c\\,\\ \\times$ $t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$ For all' $c~\\ge~1$ and $\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\ldots,h_{n},\\tau_{n}\\ \\geq\\ 0$ , we have $t^{*}(c\\,\\times\\,\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\ \\leq\\ c\\ \\times$ $t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$ and $t^{*}(\\omega,c\\times\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\leq c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})$ ", "page_idx": 18}, {"type": "text", "text": "Remark E.4. We can obtain stronger inequalities. See Properties E.1 and E.2. ", "page_idx": 18}, {"type": "text", "text": "Proof. For all $c\\in(0,1]$ , using Properties 6.1 and E.1, we have t\\*(cX w, 0 $^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\geq t^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\geq c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})$ and ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n^{*}(\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\geq t^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\geq c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,\\tau_{n})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For all $c\\geq1$ , using Properties 6.1 and E.2, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n{}^{*}(c\\times\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\le t^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\le c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,\\tau_{n})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n{}^{*}(\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq t^{*}(c\\times\\omega,c\\times\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,\\tau_{n})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Property E.5. For all $c\\geq0$ and $\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\geq0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,c\\times h_{1},c\\times\\tau_{1},\\dots,c\\times h_{n},c\\times\\tau_{n})=c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof.For $c=0$ , it it clear. Assume that $c>0$ . Using the definition of the equilibrium time, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s^{*}(j)$ is the solution of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,c\\times h_{1},c\\times\\tau_{1},\\dots,c\\times h_{n},c\\times\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{c\\times h_{\\pi_{j}},c\\times\\tau_{\\pi_{j}}\\},s_{c}^{*}(j)\\}_{(j)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s_{c}^{*}(j)$ is the solution of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2c\\tau_{\\pi_{i}}\\omega+\\frac{4c^{2}\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2c h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For both cases, we can take the same permutation $\\pi$ . Using simple algebra, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2c\\tau_{\\pi_{i}}\\omega+\\frac{4c^{2}\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2c h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=c\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, $s_{c}^{*}(j)$ is the solution of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{\\frac{s}{c}\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=\\frac{s}{c}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Comparing (26) and (27), one can see that $s_{c}^{*}(j)=c\\times s^{*}(j)$ for all $j\\in[n]$ . Using this, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{*}(\\omega,\\sigma^{2}/\\varepsilon,c\\times h_{1},c\\times\\tau_{1},\\dots,c\\times h_{n},c\\times\\tau_{n})=\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{c\\times h_{\\pi_{j}},c\\times\\tau_{\\pi_{j}}\\},c\\times s^{*}(j)\\}}\\\\ &{\\,=c\\times\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}=c\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Property E.6. We fix a nonempty subset $S=\\{k_{1},\\dots,k_{m}\\}$ from the set $[n]$ with a size $m\\geq1$ For all ${\\bar{\\omega}},\\sigma^{2}{\\bar{\\big/}}\\varepsilon,h_{1},\\tau_{1},\\dot{\\dots},h_{n},\\tau_{n}\\stackrel{.}{\\geq}{0}$ ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{k_{1}},\\tau_{k_{1}},\\dots,h_{k_{m}},\\tau_{k_{m}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Using Property 6.1 with $\\bar{\\tau}_{i}=\\infty$ and $\\bar{h}_{i}=\\infty$ for all $i\\not\\in S$ and $\\bar{\\tau}_{i}=\\tau_{i}$ and $\\bar{h}_{i}=h_{i}$ for all $i\\in S$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq t^{*}(\\omega,\\sigma^{2}/\\varepsilon,\\bar{h}_{1},\\bar{\\tau}_{1},\\dots,\\bar{h}_{n},\\bar{\\tau}_{n}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, using Def. 3.1, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,\\bar{h}_{1},\\bar{\\tau}_{1},\\dots,\\bar{h}_{n},\\bar{\\tau}_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{\\bar{h}_{\\pi_{j}},\\bar{\\tau}_{\\pi_{j}}\\},s^{*}(j)\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s^{*}(j)$ is the solution of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2\\bar{\\tau}_{\\pi_{i}}\\omega+\\frac{4\\bar{\\tau}_{\\pi_{i}}\\bar{h}_{\\pi_{i}}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2\\bar{h}_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "w.r.t $s$ for all $j~\\in~[n]$ , and $\\pi$ is a permutation that sorts $\\operatorname*{max}\\{\\bar{h}_{i},\\bar{\\tau}_{i}\\}$ in such a way that the set $\\{\\pi_{1},\\dots,\\pi_{m}\\}$ equals to the set $\\{k_{1},\\dots,k_{m}\\}$ (the order of elements can be different). Such permutation exists because $\\operatorname*{max}\\{\\bar{h}_{i},\\bar{\\tau}_{i}\\}=\\infty$ for all $i\\not\\in S$ . Using $\\operatorname*{max}\\{\\bar{h}_{\\pi_{i}},\\bar{\\tau}_{\\pi_{i}}\\}=\\infty$ for all $i>m$ ,we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,\\bar{h}_{1},\\bar{\\tau}_{1},\\ldots,\\bar{h}_{n},\\bar{\\tau}_{n})=\\operatorname*{min}_{j\\in[m]}\\operatorname*{max}\\{\\operatorname*{max}\\{\\bar{h}_{\\pi_{j}},\\bar{\\tau}_{\\pi_{j}}\\},s^{*}(j)\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the construction of $\\pi$ , (28) and (29) depend only on the elements from $S$ . Thus, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq t^{*}(\\omega,\\sigma^{2}/\\varepsilon,\\bar{h}_{1},\\bar{\\tau}_{1},\\dots,\\bar{h}_{n},\\bar{\\tau}_{n})=t^{*}(\\omega,\\sigma^{2}/\\varepsilon,\\bar{h}_{k_{1}},\\bar{\\tau}_{k_{1}},\\dots,\\bar{h}_{k_{m}},\\bar{\\tau}_{k_{m}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{k_{1}},\\tau_{k_{1}},\\dots,h_{k_{m}},\\tau_{k_{m}}).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Property E.7. For all $\\sigma^{2}/\\varepsilon,\\,h_{1},\\,\\dot{\\tau}_{1},\\,\\cdot\\,\\cdot\\,,h_{n},\\,\\dot{\\tau}_{n}\\geq0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{12t^{*}\\left(0,\\sigma^{2}/\\varepsilon,h_{1},d\\dot{\\tau}_{1},\\ldots,h_{n},d\\dot{\\tau}_{n}\\right)}\\\\ &{\\geq t^{*}\\left(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. For $d=1$ , it is clear. Assume that $d>1$ . Using the definition of $t^{*}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{*}\\left(0,\\sigma^{2}/\\varepsilon,h_{1},d\\dot{\\tau}_{1},\\dots,h_{n},d\\dot{\\tau}_{n}\\right)\\geq\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},d\\dot{\\tau}_{\\bar{\\pi}_{j}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\bar{\\pi}$ is a permutation that sorts $\\operatorname*{max}\\{h_{i},d\\dot{\\tau}_{i}\\}$ . Assume that $j^{*}$ is the minimal index that minimizes (30). Then ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{*}\\left(0,\\sigma^{2}/\\varepsilon,h_{1},d\\dot{\\tau}_{1},\\dots,h_{n},d\\dot{\\tau}_{n}\\right)\\geq\\operatorname*{max}\\left\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j^{*}}},d\\dot{\\tau}_{\\bar{\\pi}_{j^{*}}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us define ", "page_idx": 20}, {"type": "equation", "text": "$$\nI_{*}:=t^{*}(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Property E.6, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nI_{*}\\leq t^{*}(d-1,\\sigma^{2}/\\varepsilon,h_{\\bar{\\pi}_{1}},\\dot{\\tau}_{\\bar{\\pi}_{1}},\\ldots,h_{\\bar{\\pi}_{j^{*}}},\\dot{\\tau}_{\\bar{\\pi}_{j^{*}}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Def. 3.1 of $t^{*}$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nI_{*}\\leq\\operatorname*{max}\\{\\operatorname*{max}_{j\\in[j^{*}]}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},\\dot{\\tau}_{\\bar{\\pi}_{j}}\\},s^{*}\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s^{*}$ is the solution of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{2\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)+\\frac{4\\dot{\\tau}_{\\bar{\\pi}_{i}}h_{\\bar{\\pi}_{i}}\\sigma^{2}(d-1)}{s\\times\\varepsilon}+\\frac{2h_{\\bar{\\pi}_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us take $\\begin{array}{r c l}{s^{\\prime}}&{=}&{12\\operatorname*{max}\\left\\{\\left(d-1\\right)\\operatorname*{max}_{j\\in[j^{*}]}\\dot{\\tau}_{\\bar{\\pi}_{j}},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}\\right\\}}\\end{array}$ . Since $\\begin{array}{r l r}{s^{\\prime}}&{{}\\ge}&{(d\\mathrm{~-~}}\\end{array}$ $1)\\operatorname*{max}_{j\\in[j^{*}]}\\dot{\\tau}_{\\bar{\\pi}_{j}}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{2\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)+\\frac{4\\dot{\\tau}_{\\bar{\\pi}_{i}}h_{\\bar{\\pi}_{i}}\\sigma^{2}(d-1)}{s^{\\prime}\\times\\varepsilon}+\\frac{2h\\tau_{i}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\le\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{2\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)+\\frac{4h\\tau_{i}\\sigma^{2}}{\\varepsilon}+\\frac{2h\\tau_{i}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}}&{}\\\\ &{\\le\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{2\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)+\\frac{6h\\tau_{i}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}}\\\\ &{\\le12\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\operatorname*{min}\\left\\{\\frac{1}{\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)},\\frac{1}{\\frac{h\\tau_{i}\\sigma^{2}}{\\varepsilon}}\\right\\}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If there exists $p\\in[j^{*}]$ such that $\\begin{array}{r}{\\frac{1}{\\dot{\\tau}_{\\bar{\\pi}_{p}}(d-1)}<\\frac{1}{\\frac{h_{\\bar{\\pi}_{p}}\\sigma^{2}}{\\varepsilon}}}\\end{array}$ then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{j^{*}}\\operatorname*{min}\\left\\{\\frac{1}{\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)},\\frac{1}{\\frac{h_{\\bar{\\pi}_{i}}\\sigma^{2}}{\\varepsilon}}\\right\\}\\geq\\frac{1}{\\dot{\\tau}_{\\bar{\\pi}_{p}}(d-1)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{2\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)+\\frac{4\\dot{\\tau}_{\\bar{\\pi}_{i}}h_{\\bar{\\pi}_{i}}\\sigma^{2}(d-1)}{s^{\\prime}\\times\\varepsilon}+\\frac{2h\\bar{\\pi}_{i}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq12(d-1)\\dot{\\tau}_{\\bar{\\pi}_{p}}\\leq12(d-1)\\operatorname*{max}_{j\\in[j^{*}]}\\dot{\\tau}_{\\bar{\\pi}_{j}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Otherwise, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{j^{*}}\\operatorname*{min}\\left\\{\\frac{1}{\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)},\\frac{1}{\\frac{h_{\\bar{\\pi}_{i}}\\sigma^{2}}{\\varepsilon}}\\right\\}=\\sum_{i=1}^{j^{*}}\\frac{1}{\\frac{h_{\\bar{\\pi}_{i}}\\sigma^{2}}{\\varepsilon}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{2\\dot{\\tau}_{\\bar{\\pi}_{i}}(d-1)+\\frac{4\\bar{\\tau}_{\\bar{\\pi}_{i}}h_{\\bar{\\pi}_{i}}\\sigma^{2}(d-1)}{s^{\\prime}\\times\\varepsilon}+\\frac{2h_{\\bar{\\pi}_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq12\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{\\frac{h_{\\bar{\\pi}_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=12\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Considering both cases, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{2\\d_{\\bar{\\tau}_{i}}(d-1)+\\frac{4\\d_{\\bar{\\tau}_{i}}h_{i}\\sigma^{2}(d-1)}{s^{\\prime}\\times\\varepsilon}+\\frac{2h_{\\bar{\\tau}_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq12\\operatorname*{max}\\left\\{(d-1)\\operatorname*{max}_{j\\in[j^{*}]}\\d_{\\bar{\\tau}_{j},\\frac{\\sigma^{2}}{\\varepsilon}}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{\\bar{\\tau}_{i}}}\\right)^{-1}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It means that $s^{*}\\leq s^{\\prime}$ because $s^{*}$ is the solution of (33). Using (32), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\nI_{*}\\leq12\\operatorname*{max}\\left\\{\\operatorname*{max}_{j\\in[j^{*}]}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},\\dot{\\bar{\\tau}}_{\\bar{\\pi}_{j}}\\},\\operatorname*{max}\\left\\{(d-1)\\operatorname*{max}_{j\\in[j^{*}]}\\dot{\\bar{\\tau}}_{\\bar{\\pi}_{j}},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using $d\\geq1$ and $d\\dot{\\tau}_{\\bar{\\pi}_{j}}\\leq\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},d\\dot{\\tau}_{\\bar{\\pi}_{j}}\\}$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{*}\\leq12\\operatorname*{max}\\left\\{\\underset{j\\in[j^{*}]}{\\operatorname*{max}}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},d\\dot{\\bar{\\pi}}_{\\bar{j}}\\},\\operatorname*{max}\\left\\{\\underset{j\\in[j^{*}]}{\\operatorname*{max}}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},d\\dot{\\bar{\\pi}}_{\\bar{j}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\underset{i=1}{\\overset{j^{*}}{\\sum}}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}\\right\\}\\right\\}}\\\\ &{\\quad\\leq12\\operatorname*{max}\\left\\{\\underset{j\\in[j^{*}]}{\\operatorname*{max}}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},d\\dot{\\bar{\\pi}}_{\\bar{j}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\underset{i=1}{\\overset{j^{*}}{\\sum}}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Due to $\\begin{array}{r}{\\operatorname*{max}_{j\\in[j^{*}]}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},d\\dot{\\tau}_{\\bar{\\pi}_{j}}\\}=\\operatorname*{max}\\{h_{\\bar{\\pi}_{j^{*}}},d\\dot{\\tau}_{\\bar{\\pi}_{j^{*}}}\\}}\\end{array}$ and (31), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{*}=t^{*}(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\dots,h_{n},\\dot{\\tau}_{n})\\leq12\\operatorname*{max}\\left\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}*},d\\dot{\\tau}_{\\bar{\\pi}_{j}*}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\underset{i=1}{\\overset{j^{*}}{\\sum}}\\frac{1}{h_{\\bar{\\pi}_{i}}}\\right)^{-1}\\right\\}}\\\\ &{\\quad\\leq12t^{*}\\left(0,\\sigma^{2}/\\varepsilon,h_{1},d\\dot{\\tau}_{1},\\dots,h_{n},d\\dot{\\tau}_{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Property 6.2. For all $\\begin{array}{r l r l r l}{\\mathrm{~\\boldmath~\\mathcal~{~C~}~}}&{{}\\in}&{[1,d],\\sigma^{2}/\\varepsilon,}&{h_{1},}&{\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}}&{{}\\geq}&{0}\\end{array}$ , we have $24\\,\\mathrm{~\\times~}$ $t^{*}\\left(d/\\kappa-1,\\sigma^{2}/\\varepsilon,h_{1},K\\dot{\\tau}_{1},\\ldots,h_{n},K\\dot{\\bar{\\tau}}_{n}\\right)\\stackrel{}{\\geq}t^{*}\\left(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}\\right).$ ", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "text", "text": "(Part 1: $\\begin{array}{r}{K\\le\\frac{d+1}{2};}\\end{array}$ For all $\\begin{array}{r}{K\\leq\\frac{d+1}{2}}\\end{array}$ wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\nt^{\\ast}\\left(\\frac{d}{K}-1,\\sigma^{2}/\\varepsilon,h_{1},K\\dot{\\tau}_{1},\\ldots,h_{n},K\\dot{\\tau}_{n}\\right)=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},K\\dot{\\tau}_{\\pi_{j}}\\},s^{\\ast}(j)\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $s^{*}(j)$ is the solution of ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{2K\\dot{\\tau}_{\\pi_{i}}\\left(\\frac{d}{K}-1\\right)+\\frac{4K\\dot{\\tau}_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\left(\\frac{d}{K}-1\\right)}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\pi$ is a permutation that sorts $\\operatorname*{max}\\{h_{j},K\\dot{\\tau}_{j}\\}$ . Also, assume that $j^{*}$ is a minimizer in (34). For all $j\\in[n]$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s^{*}(j)=\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{2K\\dot{\\tau}_{\\pi_{i}}\\left(\\frac{d}{K}-1\\right)+\\frac{4K\\dot{\\tau}_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\left(\\frac{d}{K}-1\\right)}{s^{*}(j)\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}}\\\\ &{=\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{2\\dot{\\tau}_{\\pi_{i}}\\left(d-K\\right)+\\frac{4\\dot{\\tau}_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}(d-K)}{s^{*}(j)\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\begin{array}{r}{K\\leq\\frac{d+1}{2}}\\end{array}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\ns^{*}(j)\\geq\\frac12\\left(\\sum_{i=1}^{j}\\frac{1}{2\\dot{\\tau}_{\\pi_{i}}(d-1)+\\frac{4\\dot{\\tau}_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}(d-1)}{s^{*}(j)\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n2\\times s^{*}(j)\\geq\\left(\\sum_{i=1}^{j}\\frac{1}{2\\dot{\\tau}_{\\pi_{i}}(d-1)+\\frac{4\\dot{\\tau}_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}(d-1)}{2\\times s^{*}(j)\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "At the same time, using Property E.6, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ast\\left(d-1,\\sigma^{2}/\\varepsilon,h_{1},{\\dot{\\tau}}_{1},\\dots,h_{n},{\\dot{\\tau}}_{n}\\right)\\leq t^{\\ast}\\left(d-1,\\sigma^{2}/\\varepsilon,h_{\\pi_{1}},{\\dot{\\tau}}_{\\pi_{1}},\\dots,h_{\\pi_{j}\\ast},{\\dot{\\tau}}_{\\pi_{j}\\ast}\\right)\\leq\\operatorname*{max}\\{\\operatorname*{max}_{j\\in[j^{\\ast}]}\\operatorname*{max}_{j\\in[j^{\\ast}]}\\{h_{\\pi_{j}},\\dots,h_{\\pi_{j}},{\\dot{\\tau}}_{\\pi_{j}\\ast}\\}\\leq\\tau.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $s^{\\prime}(j^{\\ast})$ is the solution of ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j^{\\ast}}\\frac{1}{2\\dot{\\tau}_{\\pi_{i}}\\left(d-1\\right)+\\frac{4\\dot{\\tau}_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\left(d-1\\right)}{s\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From (35), we can conclude that $2\\times s^{*}(j^{*})\\geq s^{\\prime}(j^{*})$ . Using this and (36), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\nt^{\\ast}\\left(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}\\right)\\leq\\operatorname*{max}\\{\\operatorname*{max}_{j\\in[j^{+}]}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},\\dot{\\tau}_{\\bar{\\pi}_{j}}\\},2s^{\\ast}(j^{\\ast})\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq2\\operatorname*{max}\\lbrace\\underset{j\\in[j^{*}]}{\\operatorname*{max}}\\operatorname*{max}\\lbrace h_{\\bar{\\pi}_{j}},\\dot{\\tau}_{\\bar{\\pi}_{j}}\\rbrace,s^{*}(j^{*})\\rbrace}\\\\ &{\\leq2\\operatorname*{max}\\lbrace\\underset{j\\in[j^{*}]}{\\operatorname*{max}}\\operatorname*{max}\\lbrace h_{\\bar{\\pi}_{j}},K\\dot{\\tau}_{\\bar{\\pi}_{j}}\\rbrace,s^{*}(j^{*})\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\operatorname*{max}_{j\\in[j^{*}]}\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}},K\\dot{\\tau}_{\\bar{\\pi}_{j}}\\}=\\operatorname*{max}\\{h_{\\bar{\\pi}_{j^{*}}},K\\dot{\\tau}_{\\bar{\\pi}_{j^{*}}}\\}.}\\end{array}$ thus ", "page_idx": 23}, {"type": "equation", "text": "$$\nr^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\dots,h_{n},\\dot{\\tau}_{n}\\rangle\\leq2\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\bar{\\pi}_{j}^{*}},K\\dot{\\tau}_{\\bar{\\pi}_{j}^{*}}\\},s^{*}(j^{*})\\}=2t^{*}\\left(\\frac{d}{K}-1,\\sigma^{2}/\\varepsilon,h_{1},K\\dot{\\tau}_{1},\\dots,h_{n}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(Part 2: $\\begin{array}{r}{K>\\frac{d+1}{2})}\\end{array}$ For all $\\begin{array}{r}{K>\\frac{d+1}{2}}\\end{array}$ , using Property 6.1, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\nt^{*}\\left(\\frac{d}{K}-1,\\sigma^{2}/\\varepsilon,h_{1},K\\dot{\\tau}_{1},\\ldots,h_{n},K\\dot{\\tau}_{n}\\right)\\geq t^{*}\\left(0,\\sigma^{2}/\\varepsilon,\\frac{1}{2}h_{1},\\frac{d}{2}\\dot{\\tau}_{1},\\ldots,\\frac{1}{2}h_{n},\\frac{d}{2}\\dot{\\tau}_{n}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, using Property E.5, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nt^{*}\\left(\\frac{d}{K}-1,\\sigma^{2}/\\varepsilon,h_{1},K\\dot{\\tau}_{1},\\ldots,h_{n},K\\dot{\\tau}_{n}\\right)\\geq\\frac12t^{*}\\left(0,\\sigma^{2}/\\varepsilon,h_{1},d\\dot{\\tau}_{1},\\ldots,h_{n},d\\dot{\\tau}_{n}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It is left to use Property E.7 to get ", "page_idx": 23}, {"type": "equation", "text": "$$\nt^{*}\\left(\\frac{d}{K}-1,\\sigma^{2}/\\varepsilon,h_{1},K\\dot{\\tau}_{1},\\ldots,h_{n},K\\dot{\\tau}_{n}\\right)\\geq\\frac1{24}t^{*}\\left(d-1,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},\\ldots,h_{n},\\dot{\\tau}_{n}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "F  Derivations of the Examples for the Equilibrium Time ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Example 6.3. [Infinitely Fast Worker] If exists $j\\in[n]$ such that $\\tau_{j}=0$ and $h_{j}=0$ , then $t^{*}=0$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Let us take a permutation $\\pi$ where $\\pi_{1}=j$ . Such a permutation exists because ma $\\operatorname{x}\\{h_{j},\\tau_{j}\\}=$ 0. By the definition of $t^{*}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\underset{j\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}}\\\\ &{\\qquad\\qquad\\qquad\\le\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{1}},\\tau_{\\pi_{1}}\\},s^{*}(1)\\}}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname*{max}\\{0,s^{*}(1)\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $s^{*}(1)$ is the solution of ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{1}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{1}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=\\left(\\sum_{i=1}^{1}\\frac{1}{0}\\right)^{-1}=\\left(\\infty\\right)^{-1}=0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we obtain $s^{*}(1)=0$ . We substitute it to (37) to get $t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=0.$ ", "page_idx": 23}, {"type": "text", "text": "Example 6.4. [Infinitely Slow Workers] If $\\tau_{i}=\\infty$ and $h_{i}=\\infty$ for all $i\\in[n]$ , then $t^{*}=\\infty$ ", "page_idx": 23}, {"type": "text", "text": "Proof. By the definition of $t^{*}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\infty,s^{*}(j)\\}=\\infty.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Example 6.5. [Equal Performance] If $\\tau_{i}=\\tau$ and $h_{i}=h$ for all $i\\in[n]$ , then10 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t^{*}\\le6\\operatorname*{max}\\left\\{h,\\tau,\\frac{\\tau\\omega}{n},\\frac{h\\sigma^{2}}{n\\varepsilon},\\sqrt{\\frac{\\tau h\\sigma^{2}\\omega}{n\\varepsilon}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. By the definition, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h,\\tau\\},s^{*}(j)\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{h,\\tau\\},\\operatorname*{min}_{j\\in[n]}s^{*}(j)\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{h,\\tau\\},\\operatorname*{min}_{j\\in[n]}s^{*}(j)\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{\\operatorname*{max}\\{h,\\tau\\},\\operatorname*{min}_{j\\in[n]}s^{*}(j)\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{\\operatorname*{max}\\{h,\\tau\\},s^{*}(j)\\}\\}=\\operatorname*{max}\\{\\operatorname*{max}\\{\\operatorname*{max}\\{h,\\tau\\},s^{*}(j)\\}\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $s^{*}$ is the solution of ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}\\frac{1}{2\\tau\\omega+\\frac{4\\tau h\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2h\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}{\\frac{1}{2\\tau\\omega+{\\frac{4\\tau h\\sigma^{2}\\omega}{s\\varepsilon}}+{\\frac{2h\\sigma^{2}}{\\varepsilon}}}}\\right)^{-1}=\\left({\\frac{n}{2\\tau\\omega+{\\frac{4\\tau h\\sigma^{2}\\omega}{s\\varepsilon}}+{\\frac{2h\\sigma^{2}}{\\varepsilon}}}}\\right)^{-1}={\\frac{2\\tau\\omega}{n}}+{\\frac{4\\tau h\\sigma^{2}\\omega}{s n\\varepsilon}}+{\\frac{2h\\sigma^{2}}{n\\varepsilon}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have to solve and find the non-negative solution of the quadratic equation ", "page_idx": 24}, {"type": "equation", "text": "$$\ns^{2}-s\\left(\\frac{2\\tau\\omega}{n}+\\frac{2h\\sigma^{2}}{n\\varepsilon}\\right)-\\frac{4\\tau h\\sigma^{2}\\omega}{n\\varepsilon}=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The solution is ", "page_idx": 24}, {"type": "equation", "text": "$$\ns^{*}=\\left({\\frac{\\tau\\omega}{n}}+{\\frac{h\\sigma^{2}}{n\\varepsilon}}\\right)+{\\sqrt{\\left({\\frac{\\tau\\omega}{n}}+{\\frac{h\\sigma^{2}}{n\\varepsilon}}\\right)^{2}+{\\frac{4\\tau h\\sigma^{2}\\omega}{n\\varepsilon}}}}\\leq2\\left({\\frac{\\tau\\omega}{n}}+{\\frac{h\\sigma^{2}}{n\\varepsilon}}+{\\sqrt{\\frac{\\tau h\\sigma^{2}\\omega}{n\\varepsilon}}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq\\operatorname*{max}\\left\\{\\operatorname*{max}\\{h,\\tau\\},2\\left(\\frac{\\tau\\omega}{n}+\\frac{h\\sigma^{2}}{n\\varepsilon}+\\sqrt{\\frac{\\tau h\\sigma^{2}\\omega}{n\\varepsilon}}\\right)\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq6\\operatorname*{max}\\left\\{h,\\tau,\\frac{\\tau\\omega}{n},\\frac{h\\sigma^{2}}{n\\varepsilon},\\sqrt{\\frac{\\tau h\\sigma^{2}\\omega}{n\\varepsilon}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Example 6.6. [Infinitely Fast Communication] If $\\tau_{i}=0$ for all $i\\in[n]$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t^{*}\\leq2\\operatorname*{min}_{m\\in[n]}\\operatorname*{max}\\left\\{h_{\\pi_{m}},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\underset{i=1}{\\overset{m}{\\sum}}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\right\\}=\\Theta\\left(\\underset{m\\in[n]}{\\operatorname*{min}}\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\left(1+\\frac{\\sigma^{2}}{m\\varepsilon}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\pi$ is a permutation that sorts $\\{h_{i}\\}_{i=1}^{n}$ ", "page_idx": 24}, {"type": "text", "text": "Proof By the defnition, we have $\\begin{array}{r}{t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\ =\\ \\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{h_{\\pi_{j}},s^{*}(j)\\},}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "where $s^{*}(j)$ is the solution of ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j}\\frac{1}{\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq2\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\left\\{h_{\\pi_{j}},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\right\\}=\\Theta\\left(\\operatorname*{min}_{j\\in[n]}\\left(h_{\\pi_{j}}+\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using Lemma F.1, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\leq2\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\left\\{h_{\\pi_{j}},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{j}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\right\\}=\\Theta\\left(\\operatorname*{min}_{j\\in[n]}\\left(j+\\frac{\\sigma^{2}}{\\varepsilon}\\right)\\left(\\sum_{i=1}^{j}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma F.1. Let us consider the two functions ", "page_idx": 25}, {"type": "equation", "text": "$$\ng(j):=h_{j}+a\\left(\\sum_{i=1}^{j}\\frac{1}{h_{i}}\\right)^{-1},\\qquad p(j):=(j+a)\\left(\\sum_{i=1}^{j}\\frac{1}{h_{i}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all $j\\in[n]$ ,where $h_{i}\\geq0$ for all $i\\in[n]$ \uff0c $a\\geq0$ , and $h_{1}\\leq\\cdots\\leq h_{n}$ . Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\operatorname*{min}_{j\\in[n]}g(j)\\leq\\operatorname*{min}_{j\\in[n]}p(j)\\leq\\operatorname*{min}_{j\\in[n]}g(j)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "ProofIf $h_{1}\\,=\\,0$ ,then $p(1)\\,=\\,h(1)\\,=\\,0$ and $\\begin{array}{r}{\\operatorname*{min}_{i\\in[n]}p(i)\\,=\\,\\operatorname*{min}_{i\\in[n]}h(i)\\,=\\,0}\\end{array}$ .Assume that $h_{1}>0$ . Using the fact that a harmonic mean is less or equal to the maximum, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[n]}p(j)=\\operatorname*{min}_{j\\in[n]}\\,(j+a)\\left(\\sum_{i=1}^{j}\\frac{1}{h_{i}}\\right)^{-1}\\leq\\operatorname*{min}_{j\\in[n]}\\left(h_{j}+a\\left(\\sum_{i=1}^{j}\\frac{1}{h_{i}}\\right)^{-1}\\right)=\\operatorname*{min}_{j\\in[n]}g(j).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we proved the upper bound. Next, assume that $j^{*}$ is the smallest minimizer of $p(j)$ .If $j^{*}=1$ then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[n]}p(j)=p(1)=(1+a)\\,h_{1}=h_{1}+a h_{1}=g(1)\\geq\\operatorname*{min}_{j\\in[n]}g(j).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Otherwise, if $j^{*}>1$ , then $p(j^{*})\\leq p(j^{*}-1)$ . Using simple algebra, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(j^{*}+a)\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)^{-1}\\leq(j^{*}-1+a)\\left(\\displaystyle\\sum_{i=1}^{j^{*}-1}\\frac{1}{h_{i}}\\right)^{-1}}\\\\ &{\\Leftrightarrow(j^{*}+a)\\left(\\displaystyle\\sum_{i=1}^{j^{*}-1}\\frac{1}{h_{i}}\\right)\\leq(j^{*}-1+a)\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)}\\\\ &{\\Leftrightarrow\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)\\leq(j^{*}+a)\\left(\\displaystyle\\frac{1}{h_{j^{*}}}\\right)}\\\\ &{\\Leftrightarrow h_{j^{*}}\\leq(j^{*}+a)\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the last inequality, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{j\\in[n]}p(j)=(j^{*}+a)\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)^{-1}=\\frac{1}{2}\\left(j^{*}+a\\right)\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)^{-1}+\\frac{1}{2}\\left(j^{*}+a\\right)\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)^{-1}}}\\\\ &{}&{\\ge\\frac{1}{2}h_{j^{*}}+\\frac{1}{2}a\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{h_{i}}\\right)^{-1}=\\frac{1}{2}g(j^{*})\\ge\\frac{1}{2}\\operatorname*{min}_{j\\in[n]}g(j).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Example 6.7. [Ignoring Slow Workers]  If $h_{i}$ and $\\tau_{i}$ are fixed and finite for all $i~\\leq~p$ and $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\,=\\,m\\,\\in\\,\\mathbb{R}$ for all $i\\,>\\,p$ , then, for $m$ large enough, we have $t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\bar{\\tau_{i}}]_{1}^{n})=$ $t^{*}(\\omega,\\bar{\\sigma}^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{p})$ ", "page_idx": 25}, {"type": "text", "text": "Proof. By the definition, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $m>\\operatorname*{max}_{i\\in[p]}$ max $\\{h_{i},\\tau_{i}\\}$ we havemax $\\left\\{h_{i},\\tau_{i}\\right\\}<m$ for all $i\\leq p$ and the set $\\{1,\\ldots,p\\}$ equals to $\\{\\pi_{1},\\ldots,\\pi_{p}\\}$ . Thus, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}\\left\\{\\underset{j\\in[p]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\},\\underset{j\\in\\{p+1,\\dots,n\\}}{\\operatorname*{min}}\\operatorname*{max}\\{m,s^{*}(j)\\}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\operatorname*{min}\\left\\{t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{p},\\tau_{p}),\\underset{j\\in\\{p+1,\\dots,n\\}}{\\operatorname*{min}}\\operatorname*{max}\\{m,s^{*}(j)\\}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By taking $m>t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{p},\\tau_{p})$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in\\{p+1,\\ldots,n\\}}\\operatorname*{max}\\{m,s^{*}(j)\\}\\geq m>t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\ldots,h_{p},\\tau_{p}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{p},\\tau_{p}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Example 6.8. [Partial Participation]  If $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}~=~\\infty$ for all $i\\;\\;>\\;\\;p\\;\\;\\geq\\;\\;1$ ,then $t^{*}(\\omega,\\ ^{\\bar{\\sigma^{2}}}/\\varepsilon,[h_{i},\\bar{\\tau_{i}}]_{1}^{n})=t^{*}(\\omega,\\sigma^{2}/\\bar{\\varepsilon_{\\cdot}}[h_{i},\\bar{\\tau_{i}}]_{1}^{p})$ ", "page_idx": 26}, {"type": "text", "text": "Proof. By the definition, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nt^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\pi$ is a permutation such that the set $\\{\\pi_{p+1},\\ldots,\\pi_{n}\\}$ equals to the set $\\{p+1,\\ldots,n\\}$ . Such a permutation exists because n $\\operatorname*{lax}\\{h_{i},\\tau_{i}\\}=\\dot{\\infty}$ for all $i>p$ . Using this, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ }&{^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})=\\operatorname*{min}\\left\\{\\underset{j\\in[p]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\},\\underset{j\\in\\{p+1,\\dots,n\\}}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}\\right\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\operatorname*{min}\\left\\{\\underset{j\\in[p]}{\\operatorname*{min}}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\},\\infty\\right\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\operatorname*{min}\\mathop{\\operatorname*{max}}\\{\\operatorname*{max}}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{*}(j)\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{p},\\tau_{p}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "G Generic Lemma For Unbiased Gradient Estimators ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We prove the following generic lemma that estimates the variance of the general family of unbiased gradient estimators. ", "page_idx": 26}, {"type": "text", "text": "Lemma G.1. Consider that Assumptions 1.3 and 2.2 hold. Let us consider the gradient estimator ", "page_idx": 26}, {"type": "equation", "text": "$$\ng^{k}=\\frac{1}{\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}\\mathcal{C}_{i j}\\left(\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $m_{i}~\\geq~0$ for all $i\\;\\in\\;[n],\\;b_{i j}\\;\\geq\\;0$ for all $i~\\in~[n]$ and $j~\\in~[m_{i}]$ areorderedbatchsizes $(b_{i1}\\;\\leq\\;\\cdot\\;\\cdot\\;\\leq\\;b_{i,m_{i}}$ for all $i\\ \\in\\ [n].$ $w_{i j}~\\geq~0$ are weights for all $\\textit{i}\\in[n]$ and $j~\\in~[m_{i}].$ and $\\textstyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}>0,\\mathcal{C}_{i j}\\in\\mathbb{U}\\left(\\omega_{i j}\\right)$ aremutually independent compressors fromDef. 2.1 for all $i\\in[n]$ and $j\\in[m_{i}]$ , and $x^{k}\\in\\mathbb{R}^{d}$ is anarbitrarypoint.Then $\\mathbb{E}\\left[g^{k}\\right]=\\nabla f(x^{k})$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Xi\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}b_{i j}^{2}\\omega_{i j}\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ {\\displaystyle+\\,\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{m_{i}}w_{i j}^{2}b_{i j}\\omega_{i j}\\sigma^{2}+\\sum_{j=1}^{m_{i}}\\sum_{p=1}^{m_{i}}\\operatorname*{min}\\{b_{i j},b_{i p}\\}w_{i j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. First, we show the gradient estimator is unbiased: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[g^{k}\\right]=\\mathbb{E}\\left[\\frac{1}{\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}\\mathcal{C}_{i j}\\left(\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)\\right]}\\\\ {=\\frac{1}{\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}\\mathbb{E}\\left[\\mathcal{C}_{i j}\\left(\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using Def. 2.1 and Assumption 1.3, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[g^{k}\\right]=\\frac{1}{\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\nabla f(x^{k})=\\nabla f(x^{k}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we estimate the variance ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\Vert\\frac{1}{\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}C_{i j}\\left(\\displaystyle\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)-\\nabla f(x^{k})\\right\\Vert\\right]}\\\\ &{=\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}C_{i j}\\left(\\displaystyle\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)-\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\nabla f(x^{k})\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the independence and (22), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ {=\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|\\sum_{j=1}^{m_{i}}w_{i j}\\mathcal{C}_{i j}\\left(\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)-\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ {=\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{j=1}^{m_{i}}w_{i j}\\mathcal{C}_{i j}\\left(\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)-\\sum_{j=1}^{m_{i}}w_{i j}\\displaystyle\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the independence of the compressors and Def. 2.1, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{I_{1}=\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}\\mathbb{E}\\left[\\left\\|C_{i j}\\left(\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right)-\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right\\|^{2}\\right]}\\\\ {\\leq\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}\\omega_{i j}\\mathbb{E}\\left[\\left\\|\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In the view of (22), the independence of the stochastic gradients, and Assumption 1.3, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\nI_{1}\\leq\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}\\omega_{i j}\\mathbb{E}\\left[\\left\\|\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})-\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k})\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}\\omega_{i j}b_{i j}^{2}\\left\\|\\nabla f(\\boldsymbol{x}^{k})\\right\\|^{2}}\\\\ &{=\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}\\omega_{i j}\\sum_{l=1}^{b_{i j}}\\mathbb{E}\\left[\\left\\|\\nabla f(\\boldsymbol{x}^{k};\\boldsymbol{\\xi}_{i l}^{k})-\\nabla f(\\boldsymbol{x}^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}\\omega_{i j}b_{i j}^{2}\\left\\|\\nabla f(\\boldsymbol{x}^{k})\\right\\|^{2}}\\\\ &{\\le\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}b_{i j}\\omega_{i j}\\sigma^{2}+\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}w_{i j}^{2}b_{i j}^{2}\\omega_{i j}\\left\\|\\nabla f(\\boldsymbol{x}^{k})\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now consider ", "page_idx": 28}, {"type": "equation", "text": "$$\nI_{2}=\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\lVert\\sum_{j=1}^{m_{i}}w_{i j}\\sum_{l=1}^{b_{i j}}\\nabla f(x^{k};\\xi_{i l}^{k})-\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\nabla f(x^{k})\\right\\rVert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let us consider the set $S_{i l}:=\\left\\{j\\in[m_{i}]\\,|\\,l\\leq b_{i j}\\right\\}$ for all $i,l\\in\\mathbb{N}$ . Then we can rewrite the norm in the following way ", "page_idx": 28}, {"type": "equation", "text": "$$\nI_{2}=\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\lvert\\sum_{l=1}^{b_{i},m_{i}}\\left(\\sum_{j\\in S_{i l}}w_{i j}\\right)\\left(\\nabla f(x^{k};\\xi_{i l}^{k})-\\nabla f(x^{k})\\right)\\right\\rvert\\right]^{2}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The stochastic vectors are independent, thus ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}=\\displaystyle\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\sum_{l=1}^{b_{i,m_{i}}}\\left(\\sum_{j\\in S_{i l}}w_{i j}\\right)^{2}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k};\\xi_{i l}^{k})-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad\\le\\displaystyle\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\sum_{l=1}^{b_{i,m_{i}}}\\left(\\sum_{j\\in S_{i l}}w_{i j}\\right)^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{b_{i,m_{i}}}\\left(\\sum_{j\\in S_{i l}}w_{i j}\\right)^{2}=\\sum_{l=1}^{b_{i,m_{i}}}\\sum_{j\\in S_{i l}}\\sum_{p\\in S_{i l}}w_{i j}w_{i p}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The number of appearances of the term $w_{i j}w_{i p}$ in the sum equals to $\\operatorname*{min}\\{b_{i j},b_{i p}\\}$ . Thus ", "page_idx": 28}, {"type": "equation", "text": "$$\nI_{2}\\leq\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}\\sum_{p=1}^{m_{i}}\\operatorname*{min}\\{b_{i j},b_{i p}\\}w_{i j}w_{i p}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now substitute the bounds on $I_{1}$ and $I_{2}$ to (40), and get (39). ", "page_idx": 28}, {"type": "text", "text": "1: Input: starting point $x^{0}$ , stepsize $\\gamma$ the ratio $\\sigma^{2}/\\varepsilon$   \n2: for $k=0,1,\\ldots,K-1$ do   \n3: Find the maximum computation speeds $h_{i}^{k}>0$   \nand compressors\u2019 communication speeds $\\tau_{i}^{k}>0$ of the workers in the current iteration   \n4: Find the equilibrium time $t^{*}$ using Def. 3.1 with $h_{i}^{k}$ and $\\tau_{i}^{k}$   \n5: Setb ] and m\u03bc= $\\begin{array}{r}{m_{i}=\\left\\lfloor\\frac{t^{*}}{\\tau_{i}^{k}}\\right\\rfloor}\\end{array}$ for all $i\\in[n]$ $(t^{*},b_{i}$ and $m_{i}$ are local and can be different in   \nevery iteration)   \n6: Find active workers $S_{\\mathrm{A}}=\\left\\{i\\in[n]\\,:\\,b_{i}\\wedge m_{i}>0\\right\\}$   \n7: Run Alg. 2 in all active workers $S_{\\mathrm{A}}$   \n8: Broadcast $x^{k},b_{i}$ , and $m_{i}$ to all active workers $S_{\\mathrm{A}}$   \n9: Init $g^{k}=0$   \n10: for $i\\in S_{\\mathrm{A}}$ in parallel do   \n11: $w_{i}\\overset{(a)}{=}\\overline{{{\\left(b_{i}\\omega+\\omega\\frac{\\sigma^{2}}{\\varepsilon}+m_{i}\\frac{\\sigma^{2}}{\\varepsilon}\\right)}^{-1}}}$   \n12: for $j=1,\\dots,m_{i}$ do   \n13: Receive $\\mathcal{C}_{i j}\\left(g_{i}^{k}\\right)$ from the $i^{\\mathrm{th}}$ worker   \n14: $g^{k}=g^{k}+w_{i}\\mathcal{C}_{i j}\\left(g_{i}^{k}\\right)$   \n15: end for   \n16: end for   \n17: $\\begin{array}{l}{{g^{k}=g^{k}/\\left(\\sum_{i=1}^{n}w_{i}m_{i}b_{i}\\right)}}\\\\ {{x^{k+1}=x^{k}-\\gamma g^{k}}}\\end{array}$   \n18:   \n19: end for   \n$(a):$ Tf $\\omega=0$ and $\\begin{array}{r}{\\frac{\\sigma^{2}}{\\varepsilon}=0}\\end{array}$ then $w_{i}=1$ ", "page_idx": 29}, {"type": "text", "text": "H Proofs for Algorithms 1 and 4 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In the appendix, we work with Alg. 4 instead of Alg. 1. Alg. 4 is more general and estimates all the parameters based on local per-iteration times $h_{i}^{k}$ and $\\tau_{i}^{k}$ instead of $h_{i}$ and $\\tau_{i}$ . All results for Alg. 1 can be easily obtained by taking $h_{i}^{k}=h_{i}$ and $\\tau_{i}^{k}=\\tau_{i}$ ", "page_idx": 29}, {"type": "text", "text": "Lemma H.1. Consider that Assumptions 1.3 and 2.2 hold. Then the gradient estimator (9) with the weights $w_{i}$ fromAlg.4isunbiasedand ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]\\leq\\left(\\sum_{\\substack{i\\,:\\,b_{i}\\wedge m_{i}>0}}\\frac{b_{i}m_{i}}{b_{i}\\omega+\\omega\\frac{\\sigma^{2}}{\\varepsilon}+m_{i}\\frac{\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\left(\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\varepsilon\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Alg. 4 implements the gradient estimator (9). We can use Lemma G.1 with $b_{i j}=b_{i}$ \uff0c $w_{i j}=w_{i}$ and $\\omega_{i j}=\\omega$ for all $i\\in[n]$ and $j\\in[m_{i}]$ . Using (39), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\displaystyle\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}^{2}b_{i j}^{2}\\omega_{i j}\\left\\|\\nabla f(x^{k})\\right\\|^{2}+}\\\\ &{\\qquad\\qquad\\frac{1}{\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}w_{i j}b_{i j}\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{m_{i}}w_{i j}^{2}b_{i j}\\omega_{i j}\\sigma^{2}+\\displaystyle\\sum_{j=1}^{m_{i}}\\sum_{p=1}^{m_{i}}\\operatorname*{min}\\{b_{i j},b_{i p}\\}w_{i j}w_{i j}\\right.}\\\\ &{\\displaystyle=\\frac{1}{(\\sum_{i=1}^{n}m_{i}w_{i}b_{i})^{2}}\\displaystyle\\sum_{i=1}^{n}w_{i}^{2}\\left(m_{i}b_{i}^{2}\\omega)\\left\\|\\nabla f(x^{k})\\right\\|^{2}+}\\\\ &{\\displaystyle\\left.\\frac{1}{(\\sum_{i=1}^{n}m_{i}w_{i}b_{i})^{2}}\\displaystyle\\sum_{i=1}^{n}w_{i}^{2}\\left(m_{i}b_{i}\\omega\\sigma^{2}+b_{i}m_{i}^{2}\\sigma^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We add nonnegative terms to the last inequality to obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ z\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]\\ z\\frac{1}{\\left(\\sum_{i=1}^{n}m_{i}w_{i}b_{i}\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(m_{i}b_{i}^{2}\\omega+m_{i}b_{i}\\omega\\frac{\\sigma^{2}}{\\varepsilon}+b_{i}m_{i}^{2}\\frac{\\sigma^{2}}{\\varepsilon}\\right)\\left\\|\\nabla f(x^{k})\\right\\|^{2}+\\ z^{2}\\frac{\\left\\|g^{k}\\right\\|^{2}}{\\left(\\sum_{i=1}^{n}m_{i}w_{i}b_{i}\\right)^{2}}\\ z_{i}^{2}\\ w_{i}^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\left(\\sum_{i=1}^{n}m_{i}w_{i}b_{i}\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(m_{i}b_{i}^{2}\\omega\\varepsilon+m_{i}b_{i}\\omega\\sigma^{2}+b_{i}m_{i}^{2}\\sigma^{2}\\right)}\\\\ &{\\displaystyle=\\frac{1}{\\left(\\sum_{i=1}^{n}m_{i}w_{i}b_{i}\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(m_{i}b_{i}^{2}\\omega+m_{i}b_{i}\\omega\\frac{\\sigma^{2}}{\\varepsilon}+b_{i}m_{i}^{2}\\frac{\\sigma^{2}}{\\varepsilon}\\right)\\left(\\left\\|\\nabla f(x^{k})\\right\\|^{2}+\\varepsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using the choice of the weights $w_{i}$ , we get (41). ", "page_idx": 30}, {"type": "text", "text": "Lemma H.2. Consider two quantities $\\omega\\ge0$ and $\\sigma^{2}/\\varepsilon\\geq0$ , and $n\\in\\mathbb N$ .Also,consider a sequence of psitive pairs $\\{(h_{i},\\tau_{i})\\}_{i=1}^{n}$ We take $\\begin{array}{r}{b_{i}\\,=\\,\\left\\lfloor\\frac{t^{*}}{h_{i}}\\right\\rfloor}\\end{array}$ and $\\begin{array}{r}{m_{i}\\,=\\,\\left\\lfloor\\frac{t^{*}}{\\tau_{i}}\\right\\rfloor}\\end{array}$ for all $i\\;\\in\\;[n]$ where $t^{*}\\equiv$ ${t^{*}}\\left(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\ldots,h_{n},\\tau_{n}\\right)$ is the equilibrium time from Def. 3.1. Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(\\sum_{\\substack{i\\,:\\,b_{i}\\land m_{i}>0}}\\frac{b_{i}m_{i}}{b_{i}\\omega+\\omega\\frac{\\sigma^{2}}{\\varepsilon}+m_{i}\\frac{\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Assume that $j^{*}\\in[n]$ is the smallest index that minimizes $\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\},s^{\\ast}(j)\\}$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\nt^{\\ast}=\\operatorname*{max}\\{\\operatorname*{max}\\{h_{\\pi_{j^{\\ast}}},\\tau_{\\pi_{j^{\\ast}}}\\},s^{\\ast}(j^{\\ast})\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore,wehave $t^{\\ast}\\geq\\mathrm{max}\\{h_{\\pi_{j^{\\ast}}},\\tau_{\\pi_{j^{\\ast}}}\\}\\geq\\mathrm{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\}$ for all $j\\leq j^{*}$ since $\\operatorname*{max}\\{h_{\\pi_{j}},\\tau_{\\pi_{j}}\\}$ are sorted. It means that $\\begin{array}{r}{b_{\\pi_{j}}=\\left\\lfloor\\frac{t^{*}}{h_{\\pi_{j}}}\\right\\rfloor\\geq1}\\end{array}$ and $\\begin{array}{r}{m_{\\pi_{j}}=\\left\\lfloor\\frac{t^{*}}{\\tau_{\\pi_{j}}}\\right\\rfloor\\ge1}\\end{array}$ for all $j\\leq j^{*}$ . Using this, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n:=\\left(\\sum_{i:\\;b_{i}\\land m_{i}>0}{\\frac{b_{i}m_{i}}{b_{i}\\omega+\\omega{\\frac{\\sigma^{2}}{\\varepsilon}}+m_{i}{\\frac{\\sigma^{2}}{\\varepsilon}}}}\\right)^{-1}\\leq\\left(\\sum_{i=1}^{j^{*}}{\\frac{b_{\\pi_{i}}m_{\\pi_{i}}}{b_{\\pi_{i}}\\omega+\\omega{\\frac{\\sigma^{2}}{\\varepsilon}}+m_{\\pi_{i}}{\\frac{\\sigma^{2}}{\\varepsilon}}}}\\right)^{-1}=\\left(\\sum_{i=1}^{j^{*}}{\\frac{1}{{\\frac{\\omega_{i}}{m_{\\pi_{i}}}}+{\\frac{\\omega_{\\pi_{i}}}{b_{\\pi_{i}}m_{\\pi_{i}}}}}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $b_{\\pi_{j}}\\;=\\;\\left\\lfloor\\frac{t^{*}}{h_{\\pi_{j}}}\\right\\rfloor\\;\\ge\\;1$ and $\\begin{array}{r}{m_{\\pi_{j}}\\ =\\ \\left\\lfloor\\frac{t^{*}}{\\tau_{\\pi_{j}}}\\right\\rfloor\\ \\ge\\ 1}\\end{array}$ , we can also conclude that $b_{\\pi_{j}}~\\geq~\\frac{t^{*}}{2h_{\\pi_{j}}}$ t- and $m_{\\pi_{j}}\\geq\\frac{t^{*}}{2\\tau_{\\pi_{j}}}$ for all $j\\leq j^{*}$ . Therefore, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I\\leq\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{\\frac{2\\tau_{\\pi_{i}}\\omega}{t^{*}}+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\omega\\sigma^{2}}{(t^{*})^{2}\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{t^{*}\\varepsilon}}\\right)^{-1}\\leq\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{\\frac{2\\tau_{\\pi_{i}}\\omega}{s^{*}(j^{*})}+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\omega\\sigma^{2}}{(s^{*}(j^{*}))^{2}\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{s^{*}(j^{*})\\varepsilon}}\\right)^{-1}}\\\\ &{\\ =\\frac{1}{s^{*}(j^{*})}\\left(\\displaystyle\\sum_{i=1}^{j^{*}}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\omega\\sigma^{2}}{s^{*}(j^{*})\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality follows from (42). Recall that $s^{*}(j^{*})$ is the solution of the equation (7). Thus ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{2\\tau_{\\pi_{i}}\\omega+\\frac{4\\tau_{\\pi_{i}}h_{\\pi_{i}}\\omega\\sigma^{2}}{s^{*}(j^{*})\\times\\varepsilon}+\\frac{2h_{\\pi_{i}}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s^{*}(j^{*})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\nI\\leq\\frac{1}{s^{*}(j^{*})}\\times s^{*}(j^{*})=1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem H3. Assume that Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take $\\begin{array}{r}{\\gamma=\\frac{1}{2L}}\\end{array}$ in Alg. 4. Then for all iterations ", "page_idx": 30}, {"type": "equation", "text": "$$\nK\\geq\\frac{16L\\Delta}{\\varepsilon},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Alg. 4 guarantees that $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Let us fix any iteration $k\\in\\mathbb{N}$ . Consider that $\\mathcal{G}_{k}$ is a $\\sigma$ -algebra generated by $g^{0},\\ldots,g^{k-1}$ Then, given $\\mathcal{G}_{k}$ \uff0c $x^{k}$ is a deterministic vector. Using Lemma H.1, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\Big|\\,\\mathcal{G}_{k}\\right]\\le\\left(\\sum_{\\substack{i\\,:\\,b_{i}\\wedge m_{i}>0}}\\frac{b_{i}m_{i}}{b_{i}\\omega+\\omega\\frac{\\sigma^{2}}{\\varepsilon}+m_{i}\\frac{\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\left(\\left\\|\\nabla f(x^{k})\\right\\|^{2}+\\varepsilon\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that the choice of the parameters in Alg. 4 satisfy the conditions of Lemma H.2. Thus, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right|\\mathcal{G}_{k}\\right]\\leq\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\varepsilon\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all $k\\geq0$ It is left to use the standard SGD analysis from Theorem I.1 with $B=1$ and $C=\\varepsilon$ to finish the proof. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Theorem 4.2. Lett Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take $\\gamma=1/2L$ in Shadowheart SGD $(A l g.\\;I)$ Then as longas $K\\geq16L\\Delta/\\varepsilon$ wehavetheguarante $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "Proof. It immediately follows from Theorem H.3 for $h_{i}=h_{i}^{k}$ and $\\tau_{i}=\\tau_{i}^{k}$ ", "page_idx": 31}, {"type": "text", "text": "Theorem 4.4. Alg. 4 converges after ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{\\left\\lceil\\frac{16L\\Delta}{\\varepsilon}\\right\\rceil}2t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1}^{k},\\tau_{1}^{k},\\dots,h_{n}^{k},\\tau_{n}^{k})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "seconds,where $h_{i}^{k}\\,>\\,0$ and $\\tau_{i}^{k}\\,>\\,0$ are computation and communication times for worker $i$ in iteration $k$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Let us fix an iteration index $k\\in[n]$ . In every iteration, every worker calculates $b_{i}$ stochastic gradients and sends $m_{i}$ compressed vectors. Thus, the processing time of each iteration is not greater than ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[n]}{\\operatorname*{max}}\\left\\{h_{i}^{k}b_{i}+\\tau_{i}^{k}m_{i}\\right\\}=\\underset{i\\in[n]}{\\operatorname*{max}}\\left\\{h_{i}^{k}\\left\\lfloor\\frac{t^{*}}{h_{i}^{k}}\\right\\rfloor+\\tau_{i}^{k}\\left\\lfloor\\frac{t^{*}}{\\tau_{i}^{k}}\\right\\rfloor\\right\\}}\\\\ &{\\leq2t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1}^{k},\\tau_{1}^{k},\\dots,h_{n}^{k},\\tau_{n}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the fact that the number of iterations equals to (43), we finally get (11) ", "page_idx": 31}, {"type": "text", "text": "Corollary 4.3. Shadowheart SGD (Alg. 1) converges after at most $T_{*}$ seconds,where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{*}:=\\frac{32L\\Delta}{\\varepsilon}\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. It immediately follows from Theorem 4.4 for $h_{i}=h_{i}^{k}$ and $\\tau_{i}=\\tau_{i}^{k}$ ", "page_idx": 31}, {"type": "text", "text": "1 The Classical SGD Theorem ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Let us consider a slightly modified classical SGD result from (Ghadimi and Lan, 2013; Khaled and Richtarik, 2020). ", "page_idx": 31}, {"type": "text", "text": "Theorem I.1. Assume that Assumptions 1.1 and 1.2 hold. We consider the SGD method: ", "page_idx": 31}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-\\gamma g(x^{k}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{{\\frac{1}{L(1+B)}},{\\frac{\\varepsilon}{2L C}}\\right\\}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For all $k\\geq0$ , the vector $g(x)$ is a random vector such that $\\mathbb{E}\\left[\\left.g(x^{k})\\right|\\mathcal{G}_{k}\\right]=\\nabla f(x^{k}).$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|g(x^{k})-\\nabla f(x^{k})\\right\\|^{2}\\right|\\mathcal{G}_{k}\\right]\\leq B\\left\\|\\nabla f(x^{k})\\right\\|^{2}+C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\mathcal{G}_{k}$ is a $\\sigma$ algebra generated by $g(x^{0}),\\ldots,g(x^{k-1})$ The quantities $B$ and $C$ are arbitrary nonnegative constants. Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\frac{1}{K}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for ", "page_idx": 32}, {"type": "equation", "text": "$$\nK\\geq{\\frac{4L\\Delta(1+B)}{\\varepsilon}}+{\\frac{8L\\Delta C}{\\varepsilon^{2}}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. From Assumption 1.1, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f(x^{k+1})\\leq f(x^{k})+\\left\\langle\\nabla f(x^{k}),x^{k+1}-x^{k}\\right\\rangle+\\displaystyle\\frac{L}{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}}}\\\\ {{\\displaystyle=f(x^{k})-\\gamma\\left\\langle\\nabla f(x^{k}),g(x^{k})\\right\\rangle+\\displaystyle\\frac{L\\gamma^{2}}{2}\\left\\|g(x^{k})\\right\\|^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We denote $\\mathcal{G}^{k}$ as a sigma-algebra generated by $g(x^{0}),\\ldots,g(x^{k-1})$ Using unbiasedness and (45), we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(x^{k+1})\\big|\\,\\mathcal{G}^{k}\\right]\\leq f(x^{k})-\\gamma\\left(1-\\displaystyle\\frac{L\\gamma}{2}\\right)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\displaystyle\\frac{L\\gamma^{2}}{2}\\mathbb{E}\\left[\\left\\Vert g(x^{k})-\\nabla f(x^{k})\\right\\Vert^{2}\\right]\\mathcal{G}^{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq f(x^{k})-\\gamma\\left(1-\\displaystyle\\frac{L\\gamma(1+B)}{2}\\right)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\displaystyle\\frac{L\\gamma^{2}C}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\begin{array}{r}{\\gamma\\le\\frac{1}{L(1+B)}}\\end{array}$ , we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x^{k+1})\\big|\\,\\mathcal{G}^{k}\\right]\\leq f(x^{k})-\\frac{\\gamma}{2}\\left\\|\\nabla f(x^{k})\\right\\|^{2}+\\frac{L\\gamma^{2}C}{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We subtract $f^{*}$ and take the full expectation to obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x^{k+1})-f^{*}\\right]\\leq\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]+\\frac{L\\gamma^{2}C}{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Next, we sum the inequality for $k\\in\\{0,\\ldots,K-1\\}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[f({x}^{K})-f^{*}\\right]\\leq f({x}^{0})-f^{*}-\\displaystyle\\sum_{k=0}^{K-1}\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f({x}^{k})\\right\\Vert^{2}\\right]+\\frac{K L\\gamma^{2}C}{2}}\\\\ {\\displaystyle=\\Delta-\\displaystyle\\sum_{k=0}^{K-1}\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f({x}^{k})\\right\\Vert^{2}\\right]+\\frac{K L\\gamma^{2}C}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, we rearrange the terms and use that $\\mathbb{E}\\left[f(x^{K})-f^{*}\\right]\\geq0$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\frac{2\\Delta}{\\gamma K}+L\\gamma C.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The choice of $\\gamma$ and $K$ ensures that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "J  Comparison with Baselines ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In the following proofs, we use assumptions and definitions from Sec. 7.   \nComparison 7.1. $T_{*}=\\mathrm{O}(T_{\\mathrm{MB}})$ ", "page_idx": 32}, {"type": "text", "text": "Proof. Without loss of generality, we assume that all workers are sorted by $\\operatorname*{max}\\{h_{i},\\dot{\\tau}_{i}\\}$ For the Rand1 compressor, we have $\\omega=d-1$ . From Corollary 4.3, we know that the time complexity of Alg. 1 is ", "page_idx": 33}, {"type": "equation", "text": "$$\nT_{*}:=\\frac{L\\Delta}{\\varepsilon}\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},.~.~.~,h_{n},\\dot{\\tau}_{n})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "up to a constant factor. Using Def. 3.1 of $t^{*}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nT_{*}\\leq\\operatorname*{max}\\{\\operatorname*{max}\\{h_{n},\\dot{\\tau}_{n}\\},s^{*}(n)\\}\\times\\frac{L\\Delta}{\\varepsilon},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $s^{*}(n)$ is the solution of ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}\\frac{1}{2\\dot{\\tau}_{i}\\omega+\\frac{4\\dot{\\tau}_{i}h_{i}\\sigma^{2}\\omega}{s\\times\\varepsilon}+\\frac{2h_{i}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}=s.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$\\begin{array}{r}{s^{\\prime}=12\\operatorname*{max}_{i\\in[n]}\\operatorname*{max}\\left\\{\\frac{h_{i}\\sigma^{2}}{n\\varepsilon},\\omega\\dot{\\tau}_{i}\\right\\}}\\end{array}$ Using simple bounds, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{1}{2\\dot{\\tau}_{i}\\omega+\\frac{4\\dot{\\tau}_{i}h_{i}\\sigma^{2}\\omega}{s^{\\prime}\\times\\varepsilon}+\\frac{2h_{i}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq\\left(\\frac{n}{\\operatorname*{max}_{i\\in[n]}\\left(2\\dot{\\tau}_{i}\\omega+\\frac{4\\dot{\\tau}_{i}h_{i}\\sigma^{2}\\omega}{s^{\\prime}\\times\\varepsilon}+\\frac{2h_{i}\\sigma^{2}}{\\varepsilon}\\right)}\\right)^{-1}}\\\\ &{=\\frac{\\operatorname*{max}_{i\\in[n]}\\left(2\\dot{\\tau}_{i}\\omega+\\frac{4\\dot{\\tau}_{i}h_{i}\\sigma^{2}\\omega}{s^{\\prime}\\times\\varepsilon}+\\frac{2h_{i}\\sigma^{2}}{\\varepsilon}\\right)}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $s^{\\prime}\\geq\\omega\\,\\mathrm{max}_{i\\in[n]}\\,\\dot{\\tau}_{i}$ , we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left(\\sum_{i=1}^{n}\\frac{1}{2\\dot{\\tau}_{i}\\omega+\\frac{4\\dot{\\tau}_{i}h_{i}\\sigma^{2}\\omega}{s^{\\prime}\\times\\varepsilon}+\\frac{2h_{i}\\sigma^{2}}{\\varepsilon}}\\right)^{-1}\\leq\\frac{\\operatorname*{max}_{i\\in[n]}\\left(2\\dot{\\tau}_{i}\\omega+\\frac{4h_{i}\\sigma^{2}}{\\varepsilon}+\\frac{2h_{i}\\sigma^{2}}{\\varepsilon}\\right)}{n}}\\\\ &{\\leq\\frac{12\\operatorname*{max}_{i\\in[n]}\\operatorname*{max}\\left\\{\\frac{h_{i}\\sigma^{2}}{\\varepsilon},\\omega\\dot{\\tau}_{i}\\right\\}}{n}\\leq s^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It means that $s^{*}(n)\\leq s^{\\prime}$ since $s^{*}(n)$ is the solution of (47). Using the properties of max and (46), weget ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{*}=\\operatorname{O}\\left(\\operatorname*{max}\\left\\{\\operatorname*{max}\\{h_{n},\\dot{\\tau}_{n}\\},\\operatorname*{max}_{i\\in[n]}\\left\\{\\frac{h_{i}\\sigma^{2}}{n\\varepsilon},\\omega\\dot{\\tau}_{i}\\right\\}\\right\\}\\times\\frac{L\\Delta}{\\varepsilon}\\right)}\\\\ &{\\quad=\\operatorname{O}\\left(\\operatorname*{max}\\left\\{\\operatorname*{max}\\left(h_{i}+(\\omega+1)\\dot{\\tau}_{i}\\right),\\,\\left(\\operatorname*{max}\\frac{h_{i}\\sigma^{2}}{n\\varepsilon}+\\operatorname*{max}_{i\\in[n]}\\omega\\dot{\\tau}_{i}\\right)\\right\\}\\times\\frac{L\\Delta}{\\varepsilon}\\right)}\\\\ &{\\quad=\\operatorname{O}\\left(\\operatorname*{max}\\left\\{\\operatorname*{max}_{i\\in[n]}\\left(h_{i}+(\\omega+1)\\dot{\\tau}_{i}\\right)\\times\\frac{L\\Delta}{\\varepsilon},\\operatorname*{max}_{i\\in[n]}h_{i}\\times\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}},\\operatorname*{max}_{i\\in[n]}(\\omega+1)\\dot{\\tau}_{i}\\times\\frac{L\\Delta}{\\varepsilon}\\right\\}\\right)}\\\\ &{\\quad=\\operatorname{O}\\left(\\operatorname*{max}\\left(h_{i}+(\\omega+1)\\dot{\\tau}_{i}\\right)\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}}\\right)\\right)}\\\\ &{\\quad=\\operatorname{O}\\left(\\operatorname*{max}\\left(h_{i}+d\\ddot{\\tau}_{i}\\right)\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use $\\omega+1=d$ ", "page_idx": 33}, {"type": "text", "text": "Comparison 7.2. $T_{*}=\\mathrm{O}(T_{\\mathrm{R}})$ ", "page_idx": 33}, {"type": "text", "text": "Proof. From Sec. 7, we know that ", "page_idx": 33}, {"type": "equation", "text": "$$\nT_{*}:=\\frac{L\\Delta}{\\varepsilon}\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\dot{\\tau}_{1},.~.~.~,h_{n},\\dot{\\tau}_{n})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\nT_{\\mathrm{R}}:=\\frac{L\\Delta}{\\varepsilon}\\times t^{\\ast}(0,\\sigma^{2}/\\varepsilon,h_{1},d\\dot{\\tau}_{1},\\ldots,h_{n},d\\dot{\\tau}_{n}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using Property E.7, we get $T_{*}=\\mathrm{O}(T_{\\mathrm{R}})$ since $\\omega=d-1$ for Randl. ", "page_idx": 33}, {"type": "text", "text": "K  Description of Alg. 5 in the Bidirectional Setting ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we provide the modification of Alg. 4 with the EF21-P mechanism (Gruntkowska et al., 2023). Almost all steps are the same as in Alg. 4 except for the EF21-P mechanism (we mark the main changes with the color). ", "page_idx": 34}, {"type": "text", "text": "Algorithm 5 Bidirectional Shadowheart SGD ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1: Input: starting point $x^{0}$ , stepsize $\\gamma$ , the ratio $\\sigma^{2}/\\varepsilon$   \n2: for $k=0,1,\\ldots,K-1$ do   \n3: Find the current computation speeds $h_{i}^{k}>0$   \nand communication speeds $\\tau_{i}^{k}>0$ of the workers   \n4: Find the equilibrium time $t^{*}$ using Def. 3.1   \n5: Set $\\begin{array}{r}{b_{i}=\\left\\lfloor\\frac{t^{*}}{h_{i}^{k}}\\right\\rfloor}\\end{array}$ and $\\begin{array}{r}{m_{i}=\\left\\lfloor\\frac{t^{*}}{\\tau_{i}^{k}}\\right\\rfloor}\\end{array}$ for all $i\\in[n]$   \n6: Find active workers $S_{\\mathrm{A}}={\\mathrm{\\bar{\\{}}}}i\\in[n]:b_{i}\\wedge m_{i}>0\\}$   \n7: Run Alg. 6 in all workers   \n8: Broadcast $b_{i}$ , and $m_{i}$ to all workers   \n9: Init $g^{k}=0$   \n10: for $i\\in S_{\\mathrm{A}}$ in parallel do   \n11: $w_{i}\\overset{(a)}{=}\\left(b_{i}\\omega+\\omega\\frac{\\sigma^{2}}{\\varepsilon}+m_{i}\\frac{\\sigma^{2}}{\\varepsilon}\\right)^{-1}$   \n12: for $j=1,\\dots,m_{i}$ do   \n13: Receive $\\mathcal{C}_{i j}\\left(g_{i}^{k}\\right)$ from the $i^{\\mathrm{th}}$ worker   \n14: $g^{k}=g^{k}+\\dot{w_{i}}\\dot{C_{i j}}\\left(g_{i}^{k}\\right)$   \n15: end for   \n16: end for   \n17: $\\begin{array}{l}{g^{k}=g^{k}/\\left(\\sum_{i=1}^{n}w_{i}m_{i}b_{i}\\right)}\\\\ {x^{k+1}=x^{k}-\\gamma g^{k}}\\\\ {p^{k+1}=\\mathcal{C}_{\\mathrm{serv}}(x^{k+1}-w^{k})}\\\\ {w^{k+1}=w^{k}+p^{k+1}}\\end{array}$   \n18:   \n19:   \n20:   \n21: Broadcast $p^{k+1}$ to all workers   \n22: end for   \n$(a):$ f $\\omega=0$ and $\\begin{array}{r}{\\frac{\\sigma^{2}}{\\varepsilon}=0}\\end{array}$ , then $w_{i}=1$   \nAlgorithm 6 ith Worker's Strategy (init all workers with $w^{0}=x^{0}$   \n1: Receive $b_{i}$ , and $m_{i}$ from the server   \n2: if $b_{i}\\wedge m_{i}>0$ then   \n3: Init $g_{i}^{k}=0$   \n4: for $l=1,\\ldots,b_{i}$ do   \n5: Calculate $\\begin{array}{r}{\\dot{\\nabla}f(w^{k};\\xi_{i l}^{k}),\\quad\\xi_{i l}^{k}\\sim\\mathcal{D}_{\\xi}}\\end{array}$   \n6: $g_{i}^{k}=g_{i}^{k}+\\nabla f(w^{k};\\dot{\\xi}_{i l}^{k})$   \n7: end for   \n8: for $j=1,\\dots,m_{i}$ do   \n9: Send $\\mathcal{C}_{i j}\\left(g_{i}^{k}\\right)\\equiv\\mathcal{C}\\left(g_{i}^{k};\\nu_{i j}^{k}\\right)$ to the server,   \n$\\nu_{i j}^{k}\\sim\\mathcal{D}_{\\nu},\\mathcal{C}_{i j}\\in\\mathbb{U}(\\omega)$   \n10: end for   \n11: end if   \n12: Receive pk+1 from the server   \n13: $\\boldsymbol{w}^{k+1}=\\dot{\\boldsymbol{w}^{k}}+p^{k+1}$ ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "L Proofs for Alg. 5 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Theorem A.2. Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Choose $\\gamma=\\frac{\\alpha}{16L}$ Then as long as $K\\ge$ 76&L\u25b3 , Bidirectional Shadowheart SGD (Alg. 5) guaranteesto fnd an -stationary point. ", "page_idx": 34}, {"type": "text", "text": "Proof. In the bidirectional setting, the idea of proof is the same as in Theorem H.3. Let us fix any iteration $k\\in\\mathbb N$ . The gradient estimator has the same structure as (9) but with $w^{k}$ instead of $x^{k}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\ng^{k}=\\frac{1}{\\sum_{i=1}^{n}w_{i}m_{i}b_{i}}\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{m_{i}}\\mathcal{C}_{i j}\\left(\\sum_{l=1}^{b_{i}}\\nabla f(w^{k};\\xi_{i l}^{k})\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Consider that $\\mathcal{G}_{k}$ is a $\\sigma$ -algebra generated by all random variables from the iterations $0,\\ldots,k-1$ Then, given $\\mathcal{G}_{k},\\,w^{k}$ is a deterministic vector. Using Lemma H.1 with $x^{k}\\equiv w^{k}$ and Lemma H.2, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(w^{k})\\right\\|^{2}\\right|\\mathcal{G}_{k}\\right]\\leq\\left\\|\\nabla f(w^{k})\\right\\|^{2}+\\varepsilon\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for all $k\\geq0$ . It is left to use Theorem E.3 from (Gruntkowska et al., 2023) with $B=2$ and $C=\\varepsilon$ to ensure that $\\begin{array}{r}{\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon}\\end{array}$ after $\\frac{768L\\Delta}{\\alpha\\varepsilon}$ iterations. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Corollary A.3. If the broadcast time of $\\mathcal{C}_{\\mathrm{serv}}$ is not greater than $\\tau_{\\mathrm{serv}}$ , then Bidirectional Shadowheart SGD (Alg. 5) converges after at most ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\ast,\\mathrm{serv}}:=\\frac{768L\\Delta}{\\alpha\\varepsilon}\\times\\left(\\tau_{\\mathrm{serv}}+2t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "seconds. ", "page_idx": 35}, {"type": "text", "text": "Proof. Let us fix an iteration index $k\\in[n]$ . In every iteration, the server broadcasts one compressed vector, every worker calculates $b_{i}$ stochastic gradients and sends $m_{i}$ compressed vectors. Thus, the processing time of each iteration is not greater than ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\tau_{\\mathrm{serv}}+\\underset{i\\in[n]}{\\operatorname*{max}}\\left\\{h_{i}^{k}b_{i}+\\tau_{i}^{k}m_{i}\\right\\}\\!\\!}&{=}&{\\!\\tau_{\\mathrm{serv}}+\\underset{i\\in[n]}{\\operatorname*{max}}\\left\\{h_{i}^{k}\\left\\lfloor\\frac{t^{*}}{h_{i}^{k}}\\right\\rfloor+\\tau_{i}^{k}\\left\\lfloor\\frac{t^{*}}{\\tau_{i}^{k}}\\right\\rfloor\\right\\}}\\\\ &{\\le}&{\\!\\tau_{\\mathrm{serv}}+2t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1}^{k},\\tau_{1}^{k},\\dots,h_{n}^{k},\\tau_{n}^{k})}\\\\ &{\\overset{\\mathrm{P.6.1}}{\\le}\\tau_{\\mathrm{serv}}+2t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using the converge rate from Theorem A.2, we finally get (20)) ", "page_idx": 35}, {"type": "text", "text": "Comparison A.5. Assume that it takes $\\dot{\\tau}_{\\mathrm{serv}}$ seconds to send one coordinate from the server to the workers. If we take $K\\geq\\operatorname*{min}\\left\\{d,t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})/\\dot{\\tau}_{\\mathrm{serv}}\\right\\}$ in $\\mathrm{Top}K$ then $T_{*,\\mathrm{serv}}=\\mathrm{O}\\left(T_{*}\\right)$ ", "page_idx": 35}, {"type": "text", "text": "Proof. From the assumption, we have $\\tau_{\\mathrm{serv}}\\,=\\,K\\dot{\\tau}_{\\mathrm{serv}}$ and $\\tau_{\\mathrm{serv}}^{\\mathrm{full}}\\,=\\,d\\dot{\\tau}_{\\mathrm{serv}}$ .For $\\mathrm{Top}K$ $\\alpha\\,\\geq\\,K/d$ Therefore, up to a constant factor, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\ast,\\mathrm{serv}}=\\frac{L\\Delta}{\\alpha\\varepsilon}\\times\\left(\\tau_{\\mathrm{serv}}+t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\right)}\\\\ &{\\phantom{=}\\leq\\frac{d L\\Delta}{K\\varepsilon}\\times\\left(K\\dot{\\tau}_{\\mathrm{serv}}+t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\right)}\\\\ &{\\phantom{=}=\\frac{d L\\Delta}{\\varepsilon}\\dot{\\tau}_{\\mathrm{serv}}+\\frac{d L\\Delta}{K\\varepsilon}t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})}\\\\ &{\\phantom{=}\\leq\\frac{d L\\Delta}{\\varepsilon}\\dot{\\tau}_{\\mathrm{serv}}+\\operatorname*{max}\\left\\{\\frac{d L\\Delta}{\\varepsilon}\\dot{\\tau}_{\\mathrm{serv}},\\frac{L\\Delta}{\\varepsilon}t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\right\\}}\\\\ &{\\phantom{=}\\leq2\\left(\\frac{d L\\Delta}{\\varepsilon}\\dot{\\tau}_{\\mathrm{serv}}+\\frac{L\\Delta}{\\varepsilon}t^{\\ast}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n})\\right)\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Also, up to a constant factor, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{T_{*}=\\displaystyle\\frac{L\\Delta}{\\varepsilon}\\times(\\tau_{\\mathrm{serv}}^{\\mathrm{full}}+t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n}))}}\\\\ {{\\displaystyle\\quad=\\frac{L\\Delta}{\\varepsilon}\\times(d\\dot{\\tau}_{\\mathrm{serv}}+t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n}))}}\\\\ {{\\displaystyle\\quad=\\frac{d L\\Delta}{\\varepsilon}\\dot{\\tau}_{\\mathrm{serv}}+\\frac{L\\Delta}{\\varepsilon}t^{*}(\\omega,\\sigma^{2}/\\varepsilon,[h_{i},\\tau_{i}]_{1}^{n}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, $T_{*,\\mathrm{serv}}=\\mathrm{O}\\left(T_{*}\\right)$ ", "page_idx": 35}, {"type": "text", "text": "M  Development of Adaptive Shadowheart SGD ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Algorithm 7 Adaptive Shadowheart SGD ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1: Input: starting point $x^{0}$ , stepsize $\\gamma$ , the ratio $\\sigma^{2}/\\varepsilon$   \n2: for $k=0,1,\\ldots,K-1$ do   \n3: Run Alg. 8 in all workers   \n4: Broadcast $x^{k}$ to the workers   \n5: Init $l_{i}=0$ for all $i\\in[n]$   \n6: while $\\left(\\sum_{\\substack{i:\\,l_{i}>0}}\\left(\\sum_{j=1}^{l_{i}}\\left(\\frac{\\omega}{l_{i}^{2}m_{i j}}+\\frac{\\omega\\sigma^{2}}{l_{i}^{3}m_{i j}\\varepsilon}\\right)+\\frac{\\sigma^{2}}{l_{i}\\varepsilon}\\right)^{-1}\\right)^{-1}>\\frac{1}{4}\\;\\mathbf{do}$   \n7: Receive $\\mathcal{C}_{i,l_{i},m_{i,l_{i}}}\\left(g_{i}\\right),l_{i}$ and $m_{i,l_{i}}$ from some worker (we indicate this worker with $i$   \n8: $m_{i,l_{i}}=1$ and $l_{i}>1$ then   \n9: gi=gi+ma(t-1) and $\\hat{g}_{i}=0$   \n10: end if   \n11: $\\hat{g}_{i}=\\hat{g}_{i}+{{\\mathcal C}_{i,l_{i},m_{i,l_{i}}}}\\left(g_{i}\\right)$   \n12: end while   \n13: Init $g^{k}=0$   \n14: for $i\\in[n]\\,:\\,l_{i}>0$ do   \n15: $\\begin{array}{r l}&{\\bar{g}_{i}=\\overset{\\cdot}{\\bar{g}_{i}}\\overset{\\cdot}{+}\\frac{\\cdot}{m_{i,l_{i}}}\\hat{g}_{i}}\\\\ &{w_{i}\\overset{(a)}{=}\\left(\\sum_{j=1}^{l_{i}}\\frac{\\omega}{m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{\\omega\\sigma^{2}}{l_{i}m_{i j}\\varepsilon}+\\frac{l_{i}\\sigma^{2}}{\\varepsilon}\\right)^{-1}}\\\\ &{g^{k}=g^{k}+w_{i}\\bar{g}_{i}}\\end{array}$   \n16:   \n17:   \n18: end for   \n19: gk=g/(Zi:a>0wi=1))   \n20: xk+1=xk-gk   \n21: end for   \n$(a):$ f $\\omega=0$ and $\\begin{array}{r}{\\frac{\\sigma^{2}}{\\varepsilon}=0}\\end{array}$ , then $w_{i}=1$ ", "page_idx": 36}, {"type": "text", "text": "In this section, we design a new method that, unlike Alg. 1 and 4, does not require the bounds on computations times. It automatically understands when to stop the collection of compressed vectors in $g^{\\mathring{k}}$ ", "page_idx": 36}, {"type": "text", "text": "Let us consider Alg. 7, which we call Adaptive Shadowheart SGD. It implements the following gradient estimator: ", "page_idx": 36}, {"type": "equation", "text": "$$\ng^{k}=\\frac{1}{\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j}\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}\\frac{1}{m_{i j}}\\sum_{p=1}^{m_{i j}}\\mathcal{C}_{i j p}\\left(\\sum_{r=1}^{j}\\nabla f(x^{k};\\xi_{i r}^{k})\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The idea is that each worker calculate and send compressed vectors in parallel: while the next stochastic gradients $\\nabla f(x^{k};\\xi_{i,j+1}^{k})$ are calculating, the workers are sending $\\mathcal{C}_{i j}$ $\\begin{array}{r}{\\left(\\sum_{r=1}^{j}\\nabla f(x^{k};\\xi_{i r}^{k})\\right)}\\end{array}$ to server. The main difficulty is to understand when to stop. It turns out that it is sufficient to wait for the moment when the condition in Line 6 of Alg. 7 does not hold. For this method, we can prove the following guarantees. ", "page_idx": 36}, {"type": "text", "text": "Theorem M.1. Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take $\\begin{array}{r}{\\gamma=\\frac{1}{2L}}\\end{array}$ in Alg. 7. Then for all ierations $K\\geq\\frac{16L\\Delta}{\\varepsilon}$ , Alg. 7 guarantes that $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon.}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Corollary 4.6. If the computation and communication times are positive, the time complexity of Alg. 7is $\\begin{array}{r}{\\frac{\\bar{L}\\Delta}{\\varepsilon}\\times t^{\\bar{*}}(\\omega,\\sigma^{2}/\\varepsilon}\\end{array}$ $[\\operatorname*{max}\\{h_{i},\\tau_{i}\\},\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}]_{1}^{n})$ up toaconstantfctorwhere $r_{i}$ is defined inDef. 4.5. ", "page_idx": 36}, {"type": "text", "text": "1: Receive $x^{k}$ from the server   \n2:Init $l_{i}=1$   \n3: Calculate $g_{i}=\\nabla f(x^{k};\\xi_{i1}^{k}),\\quad\\xi_{i1}^{k}\\sim\\mathcal{D}_{\\xi}$   \n$\\nabla f(x^{k};\\xi_{i,l_{i}+1}^{k}),\\xi_{i,l_{i}+1}^{k}\\sim\\mathcal{D}_{\\xi}$   \nand go to the next step   \n6: Init $m_{i,l_{i}}=0$   \n7: while $\\nabla f(x^{k};\\xi_{i,l_{i}+1}^{k})$ is not calculated OR $m_{i,l_{i}}=0$ do   \n8: $m_{i,l_{i}}=m_{i,l_{i}}+1$   \n9: Send $\\mathcal{C}_{i,l_{i},m_{i,l_{i}}}\\left(g_{i}\\right),l_{i}$ and $m_{i,l_{i}}$ to the server, $\\mathcal{C}_{i,l_{i},m_{i,l_{i}}}\\in\\mathbb{U}(\\omega)$   \n10: end while   \n11: $\\begin{array}{l}{\\displaystyle g_{i}=g_{i}+\\nabla f(\\boldsymbol{x}^{k};\\xi_{i,l_{i}+1}^{k})}\\\\ {\\displaystyle l_{i}=l_{i}+1}\\end{array}$   \n12:   \n13: end while ", "page_idx": 37}, {"type": "text", "text": "N Proofs for Alg. 7 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lemma N.1. Consider that Assumptions 1.3 and 2.2 hold. Then the gradient estimator (48) with the parametersfromAlg.7isunbiasedand ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\natural\\left[\\left\\lVert g^{k}-\\nabla f(x^{k})\\right\\rVert^{2}\\right]\\leq4\\left(\\sum_{i\\in[n]:l_{i}\\geq0}\\left(\\sum_{j=1}^{l_{i}}\\frac{\\omega}{l_{i}^{2}m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{\\omega\\sigma^{2}}{l_{i}^{3}m_{i j}\\varepsilon}+\\frac{\\sigma^{2}}{l_{i}\\varepsilon}\\right)^{-1}\\right)^{-1}\\left(\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}+\\varepsilon\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{C}_{i j p}\\;\\;\\in\\;\\;\\mathbb{U}(\\omega)}\\end{array}$ $\\begin{array}{r}{\\frac{1}{m_{i j}}\\sum_{p=1}^{m_{i j}}{\\mathcal{C}_{i j p}\\in\\mathbb{U}(\\omega/m_{i j})}}\\end{array}$ $\\omega_{i j}=\\omega/_{m_{i j}},b_{i j}=j,w_{i j}=w_{i}$ and $m_{i}=l_{i}$ ,andget ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\frac{1}{\\left(\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\sum_{j=1}^{l_{i}}\\frac{j^{2}\\omega}{m_{i j}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ {\\displaystyle+\\,\\frac{1}{\\left(\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(\\sum_{j=1}^{l_{i}}\\frac{j\\omega\\sigma^{2}}{m_{i j}}+\\sum_{j=1}^{l_{i}}\\sum_{p=1}^{l_{i}}\\operatorname*{min}\\{j,p\\}\\sigma^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{j=1}^{l_{i}}\\sum_{p=1}^{l_{i}}\\operatorname*{min}\\{j,p\\}\\le l_{i}^{3}}\\end{array}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\displaystyle\\frac{1}{\\left(\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j^{2}\\omega}{m_{i j}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{\\left(\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j\\omega\\sigma^{2}}{m_{i j}}+l_{i}^{3}\\sigma^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We add nonnegative terms to the last inequality to obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{g^{k}-\\nabla f(x^{k})\\big\\|^{2}\\Bigg]\\leq\\frac{1}{\\left(\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j\\right)^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j^{2}\\omega}{m_{i j}}+\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j\\omega}{m_{i j}}\\frac{\\sigma^{2}}{\\varepsilon}+l_{i}^{3}\\frac{\\sigma^{2}}{\\varepsilon}\\right)\\big\\|\\nabla f(x^{k})\\big\\|^{2}}\\\\ &{\\qquad+\\frac{1}{\\left(\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}w_{i}^{2}\\left(\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j^{2}\\omega\\varepsilon}{m_{i j}}+\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j\\omega\\sigma^{2}}{m_{i j}}+l_{i}^{3}\\sigma^{2}\\right)}\\\\ &{=\\frac{1}{\\left(\\sum_{i=1}^{n}w_{i}\\sum_{j=1}^{l_{i}}j\\right)^{2}}\\displaystyle\\sum_{i=1}^{n}w_{i}^{2}\\left(\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j^{2}\\omega}{m_{i j}}+\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{j\\omega}{m_{i j}}\\frac{\\sigma^{2}}{\\varepsilon}+l_{i}^{3}\\frac{\\sigma^{2}}{\\varepsilon}\\right)\\left(\\big\\|\\nabla f(x^{k})\\big\\|^{2}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using $\\begin{array}{r}{\\sum_{j=1}^{l_{i}}j\\ge\\frac{l_{i}^{2}}{2}}\\end{array}$ , we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\ng^{k}-\\nabla f(x^{k})\\|^{2}\\Biggr]\\leq\\frac{4}{(\\sum_{i=1}^{n}w_{i}l_{i}^{2})^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(\\sum_{j=1}^{l_{i}}\\frac{j^{2}\\omega}{m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{j\\omega}{m_{i j}}\\frac{\\sigma^{2}}{\\varepsilon}+l_{i}^{3}\\frac{\\sigma^{2}}{\\varepsilon}\\right)\\left(\\left\\|\\nabla f(x^{k})\\right\\|^{2}+\\varepsilon\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the last two sums, we bound the terms $j$ with $l_{i}$ to get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left[\\left\\lVert g^{k}-\\nabla f(x^{k})\\right\\rVert^{2}\\right]\\le\\frac{4}{(\\sum_{i=1}^{n}w_{i}l_{i}^{2})^{2}}\\sum_{i=1}^{n}w_{i}^{2}\\left(\\sum_{j=1}^{l_{i}}\\frac{l_{i}^{2}\\omega}{m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{l_{i}\\omega}{m_{i j}}\\frac{\\sigma^{2}}{\\varepsilon}+l_{i}^{3}\\frac{\\sigma^{2}}{\\varepsilon}\\right)\\left(\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}+\\varepsilon\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "It is left to use the choice of the weights $w_{i}$ to obtain (49). ", "page_idx": 38}, {"type": "text", "text": "Theorem M.1. Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take $\\begin{array}{r}{\\gamma=\\frac{1}{2L}}\\end{array}$ in Alg. 7. Then for all iterations $K\\geq\\frac{16L\\Delta}{\\varepsilon}$ ,Alg. guarante that $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon.}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Proof. The proof of this theorem is very close to the proof of Theorem H.3. Let us fix any iteration $k\\in$ $\\mathbb{N}$ Consider that $\\mathcal{G}_{k}$ is a $\\sigma$ -algebra generated by $g^{0},\\ldots,g^{k-1}$ . Then, given $\\mathcal{G}_{k},x^{k}$ is a deterministic vector. Using Lemma N.1, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\natural\\left[\\left\\lVert g^{k}-\\nabla f(x^{k})\\right\\rVert^{2}\\right]\\mathcal{G}_{k}\\right]\\leq4\\left(\\sum_{i\\in[n]:l_{i}>0}\\left(\\sum_{j=1}^{l_{i}}\\frac{\\omega}{l_{i}^{2}m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{\\omega\\sigma^{2}}{l_{i}^{3}m_{i j}\\varepsilon}+\\frac{\\sigma^{2}}{l_{i}\\varepsilon}\\right)^{-1}\\right)^{-1}\\left(\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}\\right)\\cdot\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The algorithm is constructed in such a way that the first bracket in the last inequality is less or equal to 1 (see Line 6 in Alg. 7). Thus ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right|\\mathcal{G}_{k}\\right]\\leq\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\varepsilon\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for all $k\\geq0$ . It is left to use the standard SGD analysis from Theorem I.1 with $B=1$ and $C=\\varepsilon$ to ensure that the algorithm converges after $\\frac{16L\\Delta}{\\varepsilon}$ iterations. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Corollary 4.6. If the computation and communication times are positive, the time complexity of Alg.7is $\\begin{array}{r}{\\frac{\\bar{L}\\Delta}{\\varepsilon}\\times t^{\\bar{*}}(\\omega,\\sigma^{2}/\\varepsilon}\\end{array}$ $\\left[\\operatorname*{max}\\{h_{i},\\tau_{i}\\},\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\right]_{1}^{n}\\right)$ uptoa.constantfactor,where $r_{i}$ is defined inDef.4.5. ", "page_idx": 38}, {"type": "text", "text": "Proof. Let us fix an iteration and take $k\\in[K]$ . It is suffcient to find a time required to send enough compressed vectors such that the inequality ", "page_idx": 38}, {"type": "equation", "text": "$$\n4\\left(\\sum_{\\substack{i\\in[n]\\,:\\,l_{i}>0}}\\left(\\sum_{j=1}^{l_{i}}\\frac{\\omega}{l_{i}^{2}m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{\\omega\\sigma^{2}}{l_{i}^{3}m_{i j}\\varepsilon}+\\frac{\\sigma^{2}}{l_{i}\\varepsilon}\\right)^{-1}\\right)^{-1}\\leq1\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "holds. As soon as this inequality holds, the algorithm stops the loop in Line 6 from Alg. 7. Then the upper bound on the time complexity equals to the number of iterations $\\times$ the upper bound on the time of each iteration. The previous inequality is equivalent to ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{H}:=\\sum_{i\\in[n]:l_{i}>0}\\frac{1}{\\sum_{j=1}^{l_{i}}\\frac{4\\omega}{l_{i}^{2}m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{4\\omega\\sigma^{2}}{l_{i}^{3}m_{i j}\\varepsilon}+\\frac{4\\sigma^{2}}{l_{i}\\varepsilon}}\\ge1.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let us show that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\prime:=128\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,\\operatorname*{max}\\{h_{1},\\tau_{1}\\},\\operatorname*{min}\\left\\{\\tau_{1}r_{1},\\operatorname*{max}\\{h_{1},\\tau_{1}\\}\\right\\},\\ldots,\\operatorname*{max}\\{h_{n},\\tau_{n}\\},\\operatorname*{min}\\left\\{\\tau_{n}r_{n},\\operatorname*{max}\\{h_{1},\\tau_{1}\\},\\ldots,\\operatorname*{max}\\{h_{n},\\tau_{n}\\}\\right\\})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "is a sufficient time such that (50) holds. ", "page_idx": 39}, {"type": "text", "text": "By the definition of the equilibrium time $t^{*}$ , in order to apply this mapping, we first have to find a permutation $\\pi$ that sorts the input pairs $(\\operatorname*{max}\\{h_{i},\\tau_{i}\\},\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\})$ by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\operatorname*{max}\\{h_{i},\\tau_{i}\\},\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This term equals to $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}$ Without loss of generality, we assume that the sequence $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}$ is sorted, thus $\\pi_{i}=i$ for all $i\\in[n]$ . Therefore, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nt^{\\prime}=128\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{j},\\tau_{j}\\},s^{*}(j)\\},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $s^{*}(j)$ is the solution of the equation ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{j}\\left(2\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\omega+\\frac{4\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\sigma^{2}\\omega}{s\\varepsilon}+\\frac{2\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\sigma^{2}\\omega}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for all $j\\in[n]$ ", "page_idx": 39}, {"type": "text", "text": "Let us define $j^{*}$ as the smallest by index minimizer in (51). Then ", "page_idx": 39}, {"type": "equation", "text": "$$\nt^{\\prime}=128\\operatorname*{max}\\{\\operatorname*{max}\\{h_{j^{\\ast}},\\tau_{j^{\\ast}}\\},s^{\\ast}(j^{\\ast})\\}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Assume that $l_{i}$ is the number of iterations (the number of calculated stochastic gradients) that the $i^{\\mathrm{th}}$ worker does by the time $t^{\\prime}$ . Since $t^{\\prime}\\geq4\\operatorname*{max}\\{h_{j^{\\ast}},\\tau_{j^{\\ast}}\\}$ and the workers are sorted by $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}$ \uff0c we have $t^{\\prime}\\geq2\\left(h_{i}+\\tau_{i}\\right)$ for all $i\\leq j^{*}$ . Therefore, for all $i\\leq j^{*}$ , the $i^{\\mathrm{th}}$ worker will have time to calculate and send at least one compressed vector, i.e., $l_{i}\\geq1$ for $i\\leq j^{*}$ ", "page_idx": 39}, {"type": "text", "text": "Next, the $i^{\\mathrm{th}}$ worker requires at most $\\tau_{i}$ seconds to send a compressed vector and it waits for at least one calculated gradient. Consider that the computation time of the $j^{\\mathrm{th}}$ stochastic gradient in the $k^{\\mathrm{th}}$ iteration equals to $h_{i j}^{k}$ . Thus $\\begin{array}{r}{\\frac{t^{\\prime}}{2}\\leq\\sum_{j=1}^{l_{i}}\\left(h_{i j}^{k}+\\tau_{i}\\right)}\\end{array}$ Indeed, if $\\begin{array}{r}{\\frac{t^{\\prime}}{2}>\\sum_{j=1}^{l_{i}}\\left(h_{i j}^{k}+\\tau_{i}\\right)}\\end{array}$ , then the $i^{\\mathrm{th}}$ worker will have time to calculate and send at least one more compressed vector because $\\begin{array}{r}{\\frac{t^{\\prime}}{2}\\geq2\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\geq h_{i}+\\tau_{i}}\\end{array}$ forall $i\\leq j^{*}$ It wouldcontrait thdenitof $l_{i}$ Therefore, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{t^{\\prime}}{2}\\leq\\sum_{j=1}^{l_{i}}\\left(h_{i j}^{k}+\\tau_{i}\\right)\\leq l_{i}\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k}+l_{i}\\tau_{i}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\nl_{i}\\geq\\frac{t^{\\prime}}{2\\left(\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k}+\\tau_{i}\\right)}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "At the same time, by the definition of $l_{i}$ ,we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{t^{\\prime}}{h_{\\operatorname*{min}}}\\geq l_{i},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "because $h_{\\operatorname*{min}}>0$ is the smallest possible calculating time. Therefore, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nl_{i}\\leq l_{\\mathrm{max}}:=\\left\\lceil\\frac{t_{\\mathrm{max}}}{h_{\\mathrm{min}}}\\right\\rceil,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $t_{\\mathrm{max}}$ is defined in Def. 4.5 $\\left(t_{\\operatorname*{max}}\\geq t^{\\prime}\\right)$ 0. Since $l_{i}\\geq1$ for all $i\\leq j^{*}$ , we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}:=\\displaystyle\\sum_{i\\in[n]:l_{i}>0}\\left(\\sum_{j=1}^{l_{i}}\\frac{4\\omega}{l_{i}^{2}m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{4\\omega\\sigma^{2}}{l_{i}^{3}m_{i j}\\varepsilon}+\\frac{4\\sigma^{2}}{l_{i}\\varepsilon}\\right)^{-1}}\\\\ &{\\geq\\displaystyle\\sum_{i=1}^{j^{*}}\\left(\\sum_{j=1}^{l_{i}}\\frac{4\\omega}{l_{i}^{2}m_{i j}}+\\sum_{j=1}^{l_{i}}\\frac{4\\omega\\sigma^{2}}{l_{i}^{3}m_{i j}\\varepsilon}+\\underbrace{\\frac{4\\sigma^{2}}{l_{i}\\varepsilon}}_{B}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using (53), we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\nB:=\\frac{4\\sigma^{2}}{\\varepsilon l_{i}}\\leq\\frac{8\\sigma^{2}\\left(\\underset{j\\in[l_{i}]}{\\operatorname*{max}}h_{i j}^{k}+\\tau_{i}\\right)}{\\varepsilon t^{\\prime}}\\leq\\frac{8\\sigma^{2}\\left(h_{i}+\\tau_{i}\\right)}{\\varepsilon t^{\\prime}}\\leq\\frac{16\\sigma^{2}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon t^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For every $j^{\\mathrm{th}}$ stochastic gradient, the $i^{\\mathrm{th}}$ worker sends at least one compressed vector or $\\left\\lfloor\\frac{h_{i j}^{k}}{\\tau_{i}}\\right\\rfloor$ compressed vectors because it is possible that $\\tau_{i}\\leq h_{i j}^{k}$ , then the worker will have time to send more than one compressed vector. Therefore, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{i j}\\geq\\operatorname*{max}\\left\\{\\left\\lfloor\\frac{h_{i j}^{k}}{\\tau_{i}}\\right\\rfloor,1\\right\\}\\geq\\operatorname*{max}\\left\\{\\cfrac{h_{i j}^{k}}{2\\tau_{i}},1\\right\\}\\geq\\operatorname*{max}\\left\\{\\cfrac{\\operatorname*{min}h_{i j}^{k}}{2\\tau_{i}},1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{l_{i}}\\sum_{j=1}^{l_{i}}\\frac{1}{m_{i j}}\\leq\\frac{2}{l_{i}}\\sum_{j=1}^{l_{i}}\\operatorname*{min}\\left\\{\\frac{\\tau_{i}}{\\operatorname*{min}\\left.h_{i j}^{k}\\right.},1\\right\\}=2\\operatorname*{min}\\left\\{\\frac{\\tau_{i}}{\\operatorname*{min}\\left.h_{i j}^{k}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using the last inequality and (53), we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{l_{i}^{2}}\\sum_{j=1}^{l_{i}}\\frac{1}{m_{i j}}\\leq\\frac{4\\left(\\underset{j\\in[l_{i}]}{\\operatorname*{max}}h_{i j}^{k}+\\tau_{i}\\right)}{t^{\\prime}}\\operatorname*{min}\\left\\{\\frac{\\tau_{i}}{\\underset{j\\in[l_{i}]}{\\operatorname*{min}}h_{i j}^{k}},1\\right\\}}}\\\\ &{}&{\\leq\\frac{8\\operatorname*{max}\\{\\underset{j\\in[l_{i}]}{\\operatorname*{max}}h_{i j}^{k},\\tau_{i}\\}}{t^{\\prime}}\\operatorname*{min}\\left\\{\\frac{\\tau_{i}}{\\underset{j\\in[l_{i}]}{\\operatorname*{min}}h_{i j}^{k}},1\\right\\}}\\\\ &{}&{=\\operatorname*{min}\\left\\{\\frac{8\\tau_{i}}{t^{\\prime}}\\times\\frac{\\underset{j\\in[l_{i}]}{\\operatorname*{max}}\\{\\underset{i,i}{\\operatorname*{max}}h_{i j}^{k},\\tau_{i}\\}}{\\underset{j\\in[l_{i}]}{\\operatorname*{min}}h_{i j}^{k}},\\frac{8\\operatorname*{max}\\{\\underset{j\\in[l_{i}]}{\\operatorname*{max}}h_{i j}^{k},\\tau_{i}\\}}{t^{\\prime}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "It is clear that $T\\leq\\frac{8\\operatorname*{max}\\{\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k},\\tau_{i}\\}}{t^{\\prime}}$ .If $\\tau_{i}<\\operatorname*{min}_{j\\in[l_{i}]}h_{i j}^{k}$ then $\\begin{array}{r}{T=\\frac{8\\tau_{i}}{t^{\\prime}}\\times\\frac{\\underset{j\\in[l_{i}]}{\\operatorname*{max}}h_{i j}^{k}}{\\underset{j\\in[l_{i}]}{\\operatorname*{min}}h_{i j}^{k}}}\\end{array}$ . If $\\tau_{i}\\geq\\operatorname*{min}_{j\\in[l_{i}]}h_{i j}^{k}$ and $\\tau_{i}\\,<\\,\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k}$ \uff0c then $\\begin{array}{r}{T\\,=\\,\\frac{8\\,\\underset{j\\in[l_{i}]}{\\operatorname*{max}}\\,h_{i j}^{k}}{t^{\\prime}}\\,\\le\\,\\frac{8\\tau_{i}}{t^{\\prime}}\\,\\times\\,\\frac{\\underset{j\\in[l_{i}]}{\\operatorname*{max}}\\,h_{i j}^{k}}{\\underset{j\\in[l_{i}]}{\\operatorname*{min}}\\,h_{i j}^{k}}}\\end{array}$ $\\tau_{i}\\,\\geq\\,\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k}$ , then $\\begin{array}{r}{T\\,=\\,\\frac{8\\tau_{i}}{t^{\\prime}}\\,\\leq}\\end{array}$ $\\frac{8\\tau_{i}}{t^{\\prime}}\\,\\times\\,\\frac{{\\underset{j\\in[l_{i}]}{\\operatorname*{max}}}\\,h_{i j}^{k}}{\\underset{j\\in[l_{i}]}{\\operatorname*{min}}\\,h_{i j}^{k}}.$ ", "page_idx": 40}, {"type": "text", "text": "Thus, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{l_{i}^{2}}\\sum_{j=1}^{l_{i}}\\frac{1}{m_{i j}}\\leq\\operatorname*{min}\\left\\{\\frac{8\\tau_{i}}{t^{\\prime}}\\times\\frac{\\displaystyle\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k}}{\\displaystyle\\operatorname*{min}_{j\\in[l_{i}]}h_{i j}^{k}},\\frac{8\\operatorname*{max}\\{\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k},\\tau_{i}\\}}{t^{\\prime}}\\right\\}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\operatorname*{min}\\left\\{\\cfrac{8\\tau_{i}}{t^{\\prime}}\\times\\cfrac{\\operatorname*{sup}_{j\\in[l_{\\operatorname*{max}}]}h_{i j}^{k}}{\\operatorname*{inf}_{j\\in[l_{\\operatorname*{max}}]}h_{i j}^{k}},\\cfrac{8\\operatorname*{max}\\{\\underset{j\\in[l_{i}]}{\\operatorname*{max}}h_{i j}^{k},\\tau_{i}\\}}{t^{\\prime}}\\right\\}}\\\\ &{\\leq\\operatorname*{min}\\left\\{\\cfrac{8\\tau_{i}r_{i}}{t^{\\prime}},\\cfrac{8\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{t^{\\prime}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where we use the definition of $r_{i}$ Using the last iequality and $l_{i}\\ge\\frac{t^{\\prime}}{4\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}$ we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A:=4\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{\\omega}{m_{i j}l_{i}^{2}}+4\\displaystyle\\sum_{j=1}^{l_{i}}\\frac{\\omega\\sigma^{2}}{m_{i j}\\varepsilon l_{i}^{3}}}\\\\ &{\\phantom{A:=4\\omega}\\leq\\frac{32\\omega\\operatorname*{min}\\big\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\big\\}}{t^{\\prime}}+\\frac{32\\omega\\sigma^{2}\\operatorname*{min}\\big\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\big\\}}{\\varepsilon t^{\\prime}l_{i}}}\\\\ &{\\phantom{A:=4\\omega}\\leq\\frac{32\\omega\\operatorname*{min}\\big\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\big\\}}{t^{\\prime}}+\\frac{128\\omega\\sigma^{2}\\operatorname*{min}\\big\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\big\\}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon\\left(t^{\\prime}\\right)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where we use (53) and $\\operatorname*{max}_{j\\in[l_{i}]}h_{i j}^{k}\\leq h_{i}$ . One can substitute the bounds on $A$ and $B$ to (54) and obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{U}\\geq\\frac{1}{128}\\sum_{i=1}^{j^{\\star}}\\left(\\frac{\\omega\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}}{t^{\\prime}}+\\frac{\\omega\\sigma^{2}\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon\\left(t^{\\prime}\\right)^{2}}+\\frac{\\sigma^{2}\\operatorname*{max}\\{t^{\\prime},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\}}{\\varepsilon t^{\\prime}}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Note that $t^{\\prime}\\geq128s^{*}(j^{*})$ , thus ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\forall\\geq\\displaystyle\\sum_{i=1}^{j^{*}}\\left(\\frac{\\omega\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}}{s^{*}(j^{*})}+\\frac{\\omega\\sigma^{2}\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon\\left(s^{*}(j^{*})\\right)^{2}}+\\frac{\\sigma^{2}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon s^{*}(j^{*})}+\\frac{\\omega^{2}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon s^{*}(j^{*})}\\right.}\\\\ &{}&{\\geq\\displaystyle\\sum_{i=1}^{j^{*}}\\left(\\frac{2\\omega\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}}{s^{*}(j^{*})}+\\frac{4\\omega\\sigma^{2}\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}+\\frac{2\\sigma^{2}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon s^{*}(j^{*})}+\\frac{2\\sigma^{2}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon s^{*}(j^{*})}\\right.}\\\\ &{}&{\\left.=s^{*}(j^{*})\\times\\displaystyle\\sum_{i=1}^{j^{*}}\\left(2\\omega\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}+\\frac{4\\omega\\sigma^{2}\\operatorname*{min}\\left\\{\\tau_{i}r_{i},\\operatorname*{max}\\{h_{i},\\tau_{i}\\}\\right\\}\\operatorname*{max}\\{h_{i},\\tau_{i}\\}}{\\varepsilon s^{*}(j^{*})}+\\frac{2\\sigma^{2}}{\\varepsilon s^{*}(j^{*})}\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "It is lefto use thedefnition of $s^{*}(j^{*})$ (see (52)) to obtain that $\\begin{array}{r}{\\mathcal{H}\\geq s^{*}(j^{*})\\times\\frac{1}{s^{*}(j^{*})}=1}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "It means that after at most $t^{\\prime}$ seconds, we can ensure that the algorithm will finish the loop in Line 6 from Alg. 7. In the view of Theorem M.1, the time complexity is less or equal to $K\\times t^{\\prime}$ \u53e3 ", "page_idx": 41}, {"type": "text", "text": "O Construction of the Lower Bound ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We prove the lower bound by generalizing the time multiple oracles protocol from (Tyurin and Richtarik, 2023c). Note that in the classical approaches (Nemirovskij and Yudin, 1983; Carmon et al., 2020; Arjevani et al., 2022; Nesterov, 2018), the researchers bound the number of oracle calls required to find an $\\varepsilon.$ -solution. Our approach is based on the idea from (Tyurin and Richtarik, 2023c), where the authors propose to bound the time required to find an $\\varepsilon,$ -solution. We refer to a detailed explanation to (Tyurin and Richtarik, 2023c)[Sections 3-6]. ", "page_idx": 41}, {"type": "text", "text": "First, we define an oracle that emulates the process of computing stochastic gradients or the process of sending a compressed vector (Tyurin and Richtarik, 2023c)[Section 4]: ", "page_idx": 41}, {"type": "equation", "text": "$$\nO_{\\tau}^{g,\\mathcal{D}}:\\underbrace{\\mathbb{R}_{\\geq0}\\times\\underbrace{\\mathbb{R}^{d}}_{\\mathrm{saint}}}\\times\\underbrace{\\{0,1\\}}_{\\mathrm{~}}\\times\\underbrace{(\\mathbb{R}_{\\geq0}\\times\\mathbb{R}^{d}\\times\\{0,1\\})}_{\\mathrm{~}}\\to\\underbrace{(\\mathbb{R}_{\\geq0}\\times\\mathbb{R}^{d}\\times\\{0,1\\})}_{\\mathrm{~}}\\times\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "such that ", "page_idx": 41}, {"type": "equation", "text": "$$\nO_{\\tau}^{g,\\mathcal{D}}(t,x,c,(s_{t},s_{x},s_{q}))=\\left\\{\\begin{array}{c c}{((t,x,1),}&{0),}&{c=1,s_{q}=0,}\\\\ {((s_{t},s_{x},1),}&{0),}&{c=1,s_{q}=1,t<s_{t}+\\tau,}\\\\ {((0,0,0),}&{g(s_{x};\\xi)),}&{c=1,s_{q}=1,t\\geq s_{t}+\\tau,}\\\\ {((0,0,0),}&{0),}&{c=0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\xi\\sim\\mathcal{D}$ $g$ is an arbitrary mapping such that $g:\\mathbb{R}^{d}\\times\\mathbb{S}\\rightarrow\\mathbb{R}^{d}$ and $\\mathbb{S}$ is the sample space of a distribution $\\mathcal{D}$ . Next, we define the time multiple oracles protocol with compression: ", "page_idx": 42}, {"type": "text", "text": "Protocol 9 Time Multiple Oracles Protocol with Compression   \n1: Input: function(s) $f\\in\\mathcal F$ , computation oracles $(O_{1},...,O_{n})\\in{\\mathcal{O}}(f)$ , communication oracles $(\\hat{\\mathcal{C}}_{1}^{-},...,\\hat{\\mathcal{C}}_{n})\\in\\mathcal{U}$ ,algorthm $A=\\{(B^{k},N_{1}^{k},\\ldots,N_{n}^{k})\\}_{k=0}^{\\infty}\\;\\in\\;{\\mathcal{A}}$   \n2: $s_{i}^{\\nabla f,0}=s_{i}^{\\mathcal{C},0}=0$ for all $i\\in[n]$   \n3: for $k=0,\\ldots,\\infty\\,{\\bf d}$ 0   \n4:10 $(t^{k+1},i^{k+1},c^{\\vee f,k+1},c^{\\vee,k+1},x^{k})=B^{k}(g^{1},\\dots,g^{k})$ $\\mathsf{\\Omega}\\geq t^{k+1}\\geq t^{k}$   \n5: $(s_{i^{k+1}}^{\\nabla f,k+1},g_{i^{k+1}}^{k+1})=O_{i^{k+1}}(t^{k+1},x^{k},c^{\\nabla f,k+1},s_{i^{k+1}}^{\\nabla f,k})$ $\\forall j\\neq i^{k+1}:s_{j}^{\\nabla f,k+1}=s_{j}^{\\nabla f,k},\\quad g_{j}^{k+1}=$   \n6: $g_{\\mathrm{pre}}^{k+1}=N_{i^{k+1}}^{k}(g^{1},\\ldots,g^{k},g_{i^{k+1}}^{1},\\ldots,g_{i^{k+1}}^{k+1})$   \n7: $(s_{i^{k+1}}^{\\mathcal{C},k+1},g^{k+1})=\\hat{\\mathcal{C}}_{i^{k+1}}(t^{k+1},g_{\\mathrm{pre}}^{k+1},c^{\\mathcal{C},k+1},s_{i^{k+1}}^{\\mathcal{C},k})$   \n8: end for ", "page_idx": 42}, {"type": "text", "text": "In this protocol, the server via $B^{k}$ returns a new point $x^{k}$ , and broadcasts it to the $i^{k+1\\mathrm{th}}$ worker. Then, the worker calls the oracle $O_{i^{k+1}}$ that calculates stochastic gradients. Next, the oracle returns the vector $g_{i^{k+1}}^{k+1}$ and the workerprcesesit with $N_{i^{k+1}}^{k}$ Finally,the wore end $g_{\\mathrm{pre}}^{k+1}$ to the oracle $\\hat{\\mathcal{C}}_{i^{k+1}}$ that sends compressed vectors to the server. Using the parameters $c^{\\nabla f,k+1}$ and $c^{\\mathcal{C},k+1}$ , it can decide if it wants to start/stop the process of a gradient calculation and the process of communicating a compressed vector (See Sec. F in (Tyurin and Richtarik, 2023c)). As far as we know, all centralized distributed optimization methods can be described by Protocol 9, including Minibatch SGD, QSGD, AsynchronousSGD,RennalaSGD,andShadowheartSGD. ", "page_idx": 42}, {"type": "text", "text": "We consider the standard function class from the optimization literature (Nesterov, 2018; Arjevani et al., 2022; Carmon et al., 2020): ", "page_idx": 42}, {"type": "text", "text": "Definition O.1 (Function Class $\\mathcal{F}_{\\Delta,L}$ ). We assume that a function $f\\,:\\,\\mathbb{R}^{d}\\to\\mathbb{R}$ is differentiable, $\\|\\nabla f(x)-\\nabla f(y)\\|\\,\\leq\\,L\\,\\|x-y\\|\\quad\\forall x,y\\,\\in\\,\\mathbb{R}^{d}$ $L$ -smooth) and $f(0)\\-\\operatorname*{inf}_{x\\in\\mathbb{R}^{d}}f(x)\\,\\leq\\,\\Delta\\ (\\Delta$ bounded). The set of all functions with such properties we denote by $\\mathcal{F}_{\\Delta,L}$ ", "page_idx": 42}, {"type": "text", "text": "Next, we define the class of algorithms that we analyze. ", "page_idx": 42}, {"type": "text", "text": "Definition O.2 (Algorithm Class $\\mathcal{A}_{\\mathrm{zr}})$ 2. Let us consider Protocol 9. We say that the sequence of tuples ofmappings $A=\\tilde{\\{}(B^{k},N_{1}^{k},\\ldots,N_{n}^{k})\\}_{k=0}^{\\infty}$ is a ero-respecting algorithm, if ", "page_idx": 42}, {"type": "text", "text": "1. $B^{k}:\\mathbb{R}^{d}\\times\\dots\\times\\mathbb{R}_{-}^{d}\\to\\mathbb{R}_{\\geq0}\\times\\mathbb{N}\\times\\mathbb{N}\\times\\mathbb{N}\\times\\mathbb{R}^{d}$ for all $k\\geq1$ , and $B^{0}\\in\\mathbb{R}_{\\geq0}\\times\\mathbb{N}\\times\\mathbb{N}\\times$ k times N x Rd.   \n2. For all $k\\,\\geq\\,1$ and and $g^{1},\\ldots,g^{k}\\,\\in\\,\\mathbb{R}^{d},\\,t^{k+1}\\,\\geq\\,t^{k}$ where $t^{k+1}$ and $t^{k}$ are defined as $(t^{k+1},\\cdot\\,\\cdot\\,\\cdot\\,)=B^{k}(g^{1},\\cdot\\,\\cdot\\,\\cdot\\,,g^{k})$ and $(t^{k},\\dots)=B^{k-1}(g^{1},\\dots,g^{k-1})$   \n3. $N_{i}^{k}:\\underbrace{\\mathbb{R}^{d}\\times\\dots\\times\\mathbb{R}^{d}}_{k\\mathrm{\\times}}\\times\\underbrace{\\mathbb{R}^{d}\\times\\dots\\times\\mathbb{R}^{d}}_{k+1\\mathrm{\\times}}\\rightarrow\\mathbb{R}^{d}$ for all $k\\geq0$ and for all $i\\in[n]$   \n4 $\\begin{array}{r}{\\operatorname{supp}\\left(x^{k}\\right)\\,\\subseteq\\,\\bigcup_{j=1}^{k}\\operatorname{supp}\\left(g^{j}\\right),\\,\\operatorname{and}\\,\\operatorname{supp}\\left(g_{\\mathrm{pre}}^{k+1}\\right)\\,\\subseteq\\,\\bigcup_{j=1}^{k}\\operatorname{supp}\\left(g^{j}\\right)\\bigcup_{j=1}^{k+1}\\operatorname{supp}\\left(g_{i^{k+1}}^{j}\\right),}\\end{array}$ for all $k\\in\\ensuremath{\\mathbb{N}}_{0}$ , where $\\operatorname{supp}(x):=\\{i\\in[d]\\,|\\,x_{i}\\neq0\\}$ ", "page_idx": 42}, {"type": "text", "text": "The set of all algorithms with this properties we define as $A_{\\mathrm{zr}}$ ", "page_idx": 42}, {"type": "text", "text": "The properties 1 and 3 are only required to define the domains of the mappings. The property 4 ensures that these mappings are zero-respecting (Arjevani et al., 2022). The property 2 is explained in (Tyurin and Richtarik, 2023c)[Section 4, Definition 4.1]. It ensures that our algorithm does not \"travel into the past'. ", "page_idx": 42}, {"type": "text", "text": "The following oracle class is the same as in (Tyurin and Richtarik, 2023c). For any $f\\in\\mathcal{F}_{\\Delta,L}$ ,it returns $n$ oracles that require $h_{1},...,h_{n}$ seconds to calculate a stochastic gradient. These oracles emulate the real behavior where the workers have different processing times. ", "page_idx": 42}, {"type": "text", "text": "Deuie for any $f\\in\\mathcal{F}_{\\Delta,L}$ it returns oracles $O_{i}=O_{h_{i}}^{\\nabla f,D_{i}^{\\nabla f}}$ for all $i\\in[n]$ , where $\\nabla f(x;\\xi)$ is an unbiased $\\sigma^{2}$ $O_{h_{i}}^{\\nabla f,D_{i}^{\\nabla f}}$ are defned i (5). we define such oracle class as Oh...h ", "page_idx": 43}, {"type": "text", "text": "The following oracle class emulates the behavior of compressors. It returns $n$ oracles that require $\\tau_{1},\\dots,\\tau_{n}$ seconds to send a compressed vector to the server. ", "page_idx": 43}, {"type": "text", "text": "Definition 0.4 Communication Oracle Class $\\mathcal{U}_{\\tau_{1},\\dots,\\tau_{n}}^{\\omega}.$ ). Let us consider an oracle class such that, it returns oraeles $\\hat{C}_{i}=O_{\\tau_{i}}^{\\mathcal{C},\\mathcal{D}_{i}^{\\mathcal{C}}}$ for ll $i\\in[n]$ where $\\mathcal{C}$ is an unbiased compresor wih parameter $\\omega$ \uff0c i.e., $\\mathcal{C}\\in\\mathbb{U}(\\omega)$ (see Def. 2.1). The oracles $O_{\\tau_{i}}^{\\mathcal{C},\\mathcal{D}_{i}^{\\mathcal{C}}}$ are defined in (55). We define such oracle class as $\\mathcal{U}_{\\tau_{1},\\dots,\\tau_{n}}^{\\omega}$ ", "page_idx": 43}, {"type": "text", "text": "Finally, we present our lower bound theorem: ", "page_idx": 43}, {"type": "text", "text": "Theorem O.5. Let us consider Protocol 9. We take any $h_{i}\\ >\\ 0$ \uff0c ${\\tau_{i}}~>~0$ for all $i\\;\\in\\;[n]_{!}$ $\\omega\\geq$ $0,L,\\Delta,\\varepsilon,\\sigma^{2}>0$ such that $\\varepsilon<c_{1}L\\Delta$ and $\\omega+1\\leq T_{\\mathrm{{c}}}$ 11 where $\\begin{array}{r}{T=\\left\\lfloor\\frac{\\Delta L}{c_{2}\\varepsilon}\\right\\rfloor}\\end{array}$ is the dimension of the construction. For any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ , there exists a function $f\\in\\bar{\\mathcal{F}_{\\Delta,L}}$ ,computation oracles $(O_{1},...\\,,O_{n})\\,\\in\\,{\\mathcal{O}}_{h_{1},...,h_{n}}^{\\sigma^{2}}(f)$ , and communication oracles $(\\hat{{\\mathcal C}}_{1},\\hdots,\\hat{{\\mathcal C}}_{n})\\,\\in\\,\\mathcal U_{\\tau_{1},\\hdots,\\tau_{n}}^{\\omega}$ ,12 such that $\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]>\\varepsilon$ ,where ${S_{t}:=\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}}$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\nt=c_{3}\\times\\frac{L\\Delta}{\\varepsilon}\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The quantities $c_{1},\\:c_{2}$ ,and $c_{3}$ are universal constants. The sequences $x^{k}$ and $t^{k}$ are defined in Protocol 9. ", "page_idx": 43}, {"type": "text", "text": "P Proof of Theorem O.5 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "P.1The \"Worst Case\" Function ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let us consider the \u201cworst case\u201d function, which is a standard function to obtain lower bounds in the nonconvex world.We define ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathrm{prog}(x):=\\operatorname*{max}\\{i\\geq0\\,|\\,x_{i}\\neq0\\}\\quad(x_{0}\\equiv1).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In our proofs, we use the construction from (Carmon et al., 2020; Arjevani et al., 2022). For any $T\\in\\mathbb N$ ,the authors define ", "page_idx": 43}, {"type": "equation", "text": "$$\nF_{T}(x):=-\\Psi(1)\\Phi(x_{1})+\\sum_{i=2}^{T}\\left[\\Psi(-x_{i-1})\\Phi(-x_{i})-\\Psi(x_{i-1})\\Phi(x_{i})\\right],\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\Psi(x)=\\left\\{\\!\\!\\!\\begin{array}{l l}{0,}&{x\\leq1/2,}\\\\ {\\exp\\left(1-\\frac{1}{(2x-1)^{2}}\\right),}&{x\\geq1/2,}\\end{array}\\right.\\,\\,\\mathrm{and}\\quad\\Phi(x)=\\sqrt{e}\\int_{-\\infty}^{x}e^{-\\frac{1}{2}t^{2}}d t.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The main property of the function $F_{T}(x)$ is that its gradients are large unless $\\operatorname{prog}(x)\\geq T$ ", "page_idx": 43}, {"type": "text", "text": "Lemma P.1 (Carmon et al. (2020); Arjevani et al. (2022)). The function $F_{T}$ satisfies: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{l.\\ \\ F_{T}(0)-\\operatorname*{inf}_{x\\in\\mathbb R^{T}}F_{T}(x)\\leq\\Delta^{0}T,\\,w h e r e\\,\\Delta^{0}=12.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "2. The function $F_{T}$ is $l_{1}$ -smooth,where $l_{1}=152$ ", "page_idx": 44}, {"type": "text", "text": "3. For all $x\\in\\mathbb{R}^{T}$ \uff0c $\\|\\nabla F_{T}(x)\\|_{\\infty}\\leq\\gamma_{\\infty}$ , where $\\gamma_{\\infty}=23$   \n4.For all $x\\in\\mathbb{R}^{T}$ $\\mathrm{prog}(\\nabla F_{T}(x))\\leq\\mathrm{prog}(x)+1.$   \n5. For all $x\\in\\mathbb{R}^{T}$ \uff0c $i f\\mathrm{prog}(x)<T$ then $\\|\\nabla F_{T}(x)\\|>1$ ", "page_idx": 44}, {"type": "text", "text": "Theorem O.5. Let us consider Protocol 9. We take any $h_{i}\\ >\\ 0$ \uff0c ${\\tau_{i}}~>~0$ for all $i\\;\\in\\;[n]_{!}$ $\\omega\\geq$ $0,L,\\Delta,\\varepsilon,\\sigma^{2}>0$ such that $\\varepsilon<c_{1}L\\Delta$ and $\\omega+1\\leq T$ 13 where $\\begin{array}{r}{T=\\left\\lfloor\\frac{\\Delta L}{c_{2}\\varepsilon}\\right\\rfloor}\\end{array}$ is the dimension of the construction. For any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ , there exists a function $f\\in\\mathcal{F}_{\\Delta,L}$ ,computation oracles $(O_{1},...\\,,O_{n})\\,\\in\\,{\\mathcal{O}}_{h_{1},...,h_{n}}^{\\sigma^{2}}(f)$ , and communication oracles $(\\hat{{\\mathcal C}}_{1},\\hdots,\\hat{{\\mathcal C}}_{n})\\,\\in\\,\\mathcal U_{\\tau_{1},\\hdots,\\tau_{n}}^{\\omega}$ ,14 such that $\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]>\\varepsilon$ where ${S_{t}:=\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}}$ and ", "page_idx": 44}, {"type": "equation", "text": "$$\nt=c_{3}\\times\\frac{L\\Delta}{\\varepsilon}\\times t^{*}(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The quantities $c_{1},\\:c_{2}$ ,and $c_{3}$ are universal constants. The sequences $x^{k}$ and $t^{k}$ are defined in Protocol9. ", "page_idx": 44}, {"type": "text", "text": "Proof. ", "page_idx": 44}, {"type": "text", "text": "Without loss of generality, we assume that the workers are sorted by $\\operatorname*{max}\\{h_{i},\\tau_{i}\\}:\\operatorname*{max}\\{h_{1},\\tau_{1}\\}\\leq$ $\\cdots\\leq\\operatorname*{max}\\{h_{n},\\bar{\\tau}_{n}\\}$ ", "page_idx": 44}, {"type": "text", "text": "(Step 1: $f\\in\\mathcal{F}_{\\Delta,L})$ ", "page_idx": 44}, {"type": "text", "text": "Let us fix $\\lambda>0$ and take the function $\\begin{array}{r}{f(x):=\\frac{L\\lambda^{2}}{l_{1}}F_{T}\\left(\\frac{x}{\\lambda}\\right)}\\end{array}$ , where the function $F_{T}$ is defined in Sec. P.1. Tyurin and Richtarik (2023c)[Sec. D.2, Proof of Thm. 6.4] show that the function $f$ is $L$ -smooth and $f(0)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}f(x)\\leq\\Delta$ if ", "page_idx": 44}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\Delta l_{1}}{L\\lambda^{2}\\Delta^{0}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus, we have $f\\in\\mathcal{F}_{\\Delta,L}$ ", "page_idx": 44}, {"type": "text", "text": "(Step 2: Oracle Class) Let us construct a stochastic gradient mapping. For our lower bound, we take ", "page_idx": 44}, {"type": "equation", "text": "$$\n[\\nabla f(x;\\xi)]_{j}:=\\nabla_{j}f(x)\\left(1+\\mathbb{1}\\left[j>\\mathrm{prog}(x)\\right]\\left(\\frac{\\xi}{p}-1\\right)\\right)\\quad\\forall x\\in\\mathbb{R}^{T},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and $\\boldsymbol{\\mathcal{D}}_{i}^{\\nabla f}=B e r n o u l l i(p)$ for all $i\\in[n]$ ,where $p\\in(0,1]$ . We denote $[x]_{j}$ as the $j^{\\mathrm{th}}$ index of a vector $x\\in\\mathbb{R}^{T}$ .Let us take ", "page_idx": 44}, {"type": "equation", "text": "$$\np=\\operatorname*{min}\\left\\{\\frac{L^{2}\\lambda^{2}\\gamma_{\\infty}^{2}}{\\sigma^{2}l_{1}^{2}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then Tyurin and Richtarik (2023c)[Sec. D.2, Proof of Thm. 6.4] show that this mapping is unbiased and $\\sigma^{2}$ -variance-bounded. ", "page_idx": 44}, {"type": "text", "text": "(Step 3: Compression Operator) In our construction, we take the Rand $K$ compressor (outputs $K$ random values of an input vector without replacement, scaled by $T/\\protect K$ (Def. D.1)). From Theorem D.2, weknow that $\\mathcal{C}$ is unbiased and $\\textstyle{\\frac{T}{K}}-1$ -variance bounded, i.e., ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}\\left[\\mathcal{C}(x;S)\\right]=x,\\qquad\\mathbb{E}_{S}\\left[\\left\\Vert\\mathcal{C}(x;S)-x\\right\\Vert^{2}\\right]\\leq\\left(\\frac{T}{K}-1\\right)\\left\\Vert x\\right\\Vert^{2},\\qquad\\forall x\\in\\mathbb{R}^{T},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where ", "page_idx": 44}, {"type": "equation", "text": "$$\n[\\mathcal{C}(x;S)]_{j}:=\\left\\{{\\frac{T}{K}}x_{j},\\quad j\\in S,\\quad\\forall j\\in[T].\\atop j\\not\\in{\\mathcal{C}},\\quad\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and $S$ is an uniformly random subset of $[T]$ without replacement. It is sufficient to take $\\begin{array}{r}{K=\\left\\lceil\\frac{T}{\\omega+1}\\right\\rceil}\\end{array}$ to ensure that $\\mathcal{C}\\in\\mathbb{U}(\\omega)$ . Let us define $\\begin{array}{r}{p_{\\omega}:=\\frac{K}{T}}\\end{array}$ . We take mutually independent distributions $\\mathcal{D}_{i}^{c}$ that generate random subsets $S$ described above. ", "page_idx": 45}, {"type": "text", "text": "(Step 4: Analysis of Protocol) Let us take ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\lambda=\\frac{\\sqrt{2\\varepsilon}l_{1}}{L}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "to ensure that $\\begin{array}{r}{\\left\\|\\nabla f(x)\\right\\|^{2}=\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}\\left\\|\\nabla F_{T}(\\frac{x}{\\lambda})\\right\\|^{2}>2\\varepsilon\\mathbb{1}\\left[\\mathrm{prog}(x)<T\\right]}\\end{array}$ for all $x\\in\\mathbb{R}^{T}$ , where we use Lemma P.1. Thus ", "page_idx": 45}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\Delta L}{2\\varepsilon l_{1}\\Delta^{0}}\\right\\rfloor\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and ", "page_idx": 45}, {"type": "equation", "text": "$$\np=\\operatorname*{min}\\left\\{\\frac{2\\varepsilon\\gamma_{\\infty}^{2}}{\\sigma^{2}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Protocol 9 generates the sequence $\\{x^{k}\\}_{k=0}^{\\infty}$ . We have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S_{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}>2\\varepsilon\\operatorname*{inf}_{k\\in S_{t}}\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<T\\right].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Using Lemma P.2 with $\\delta=1/2$ and (58), we obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\geq2\\varepsilon\\mathbb{P}\\left(\\operatorname*{inf}_{k\\in S_{t}}\\,\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<T\\right]\\geq1\\right)>\\varepsilon\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for ", "page_idx": 45}, {"type": "equation", "text": "$$\nt\\leq\\frac{1}{48}t^{*}\\left(\\frac{T}{K},\\operatorname*{max}\\left\\{\\frac{\\sigma^{2}}{2\\varepsilon\\gamma_{\\infty}^{2}},1\\right\\},h_{1},\\tau_{1},\\ldots,h_{n},\\tau_{n}\\right)\\left(\\frac{\\Delta L}{8\\varepsilon l_{1}\\Delta^{0}}-1\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By the assumption of the theorem, we have $\\omega+1\\leq T$ . Therefore, we get the series of inequalities: ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{T}{K}}={\\frac{T}{\\left\\lceil{\\frac{T}{\\omega+1}}\\right\\rceil}}\\geq{\\frac{\\omega+1}{2}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and, using Properties 6.1 and E.1, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{*}\\left(\\frac{T}{K},\\operatorname*{max}\\left\\{\\frac{\\sigma^{2}}{2\\varepsilon\\gamma_{\\infty}^{2}},1\\right\\},h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\right)}\\\\ &{\\geq t^{*}\\left(\\frac{\\omega+1}{2},\\operatorname*{max}\\left\\{\\frac{\\sigma^{2}}{2\\varepsilon\\gamma_{\\infty}^{2}},1\\right\\},h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\right)}\\\\ &{\\geq t^{*}\\left(\\frac{1}{2\\gamma_{\\infty}^{2}}\\times\\omega,\\frac{1}{2\\gamma_{\\infty}^{2}}\\times\\frac{\\sigma^{2}}{\\varepsilon},h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\right)}\\\\ &{\\geq\\frac{1}{2\\gamma_{\\infty}^{2}}\\times t^{*}\\left(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus, we can take ", "page_idx": 45}, {"type": "equation", "text": "$$\nt=\\frac{1}{48}\\times\\frac{1}{2\\gamma_{\\infty}^{2}}\\times t^{*}\\left(\\omega,\\sigma^{2}/\\varepsilon,h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n}\\right)\\left(\\frac{\\Delta L}{8\\varepsilon l_{1}\\Delta^{0}}-1\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "P.1.1 Proof of Lemma P.2 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Lemma P.2. Let us fx $T,T^{\\prime}\\in\\mathbb{N}$ such that $T\\leq T^{\\prime}$ , consider Protocol 9 with an algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ a differentiablefunction $f\\,:\\,\\mathbb{R}^{T^{\\prime}}\\to\\mathbb{R}$ such that pro $\\operatorname{g}(\\nabla f(x))\\leq\\operatorname{prog}(x)\\!+\\!1$ for all $x\\in$ domain $(f)$ ", "page_idx": 45}, {"type": "text", "text": "$O_{i}=O_{h_{i}}^{\\nabla f,D_{i}^{\\nabla f}}$ with the distributions $\\boldsymbol{\\mathcal{D}}_{i}^{\\nabla f}=B e r n o u l l i(\\boldsymbol{p}_{\\sigma})$ \uff0c $p_{\\sigma}\\in(0,1]$ \uff0c $h_{i}>0$ , and the mappings ", "page_idx": 46}, {"type": "equation", "text": "$$\n[\\nabla f(x;\\xi)]_{j}=\\nabla_{j}f(x)\\left(1+\\mathbb{1}\\left[j>\\mathrm{prog}(x)\\right]\\left({\\frac{\\xi}{p_{\\sigma}}}-1\\right)\\right)\\quad\\forall x\\in\\mathbb{R}^{T^{\\prime}},\\forall\\xi\\in\\{0,1\\},\\forall j\\in[T].\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "2. We take compression oracles $\\hat{C}_{i}=O_{\\tau_{i}}^{\\mathcal{C},\\mathcal{D}_{i}^{\\mathcal{C}}}$ with thedisributions $\\begin{array}{r}{\\mathcal{D}_{i}^{\\mathcal{C}}=u n i f o r m(K,T^{\\prime})\\;(=}\\end{array}$ \"uniformly random subset of $\\left[T^{\\prime}\\right]$ of the size $K$ without replacement\") and the mappings ", "page_idx": 46}, {"type": "equation", "text": "$$\n[\\mathcal{C}(x;S)]_{j}:=\\left\\{\\frac{T^{\\prime}}{K}x_{j},\\quad j\\in S,\\quad\\forall x\\in\\mathbb{R}^{T^{\\prime}},\\forall S\\subseteq[n],\\forall j\\in[T^{\\prime}],\\right.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "$\\tau_{i}~>~0$ We define $\\begin{array}{r l r}{p_{\\omega}}&{{}:=}&{\\frac{K}{T^{\\prime}}}\\end{array}$ .We assume that the workers are sorted by $\\operatorname*{max}\\{h_{m},\\tau_{m}\\}\\;\\;:$ $\\operatorname*{max}\\{h_{1},\\tau_{1}\\}\\leq\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{n}^{\\underline{{\\star}}},\\tau_{n}\\}$ . With probability not less than $1-\\delta$ the following inequality holds: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S_{t}}\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<T\\right]\\geq1\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for ", "page_idx": 46}, {"type": "equation", "text": "$$\nt\\leq{\\frac{1}{48}}t^{*}(1/_{p_{\\omega}},1/_{p_{\\sigma}},h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\left({\\frac{T}{2}}+\\log\\delta\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $S_{t}\\,:=\\,\\left\\{\\,k\\in\\mathbb{N}_{0}\\;\\vert\\,t^{k}\\leq t\\right\\}$ , the iterates $t^{k}$ and $x^{k}$ are defined in Protocol 9, and $t^{*}$ is the equilibrium time from Def. 3.1. ", "page_idx": 46}, {"type": "text", "text": "Proof. (Part 1): The Construction of Random Variables. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Let us fix $t\\geq0$ and define the smallest index $k(i)$ of the sequence when the progress $\\mathrm{prog}(x^{k(i)})$ equals $i$ ", "page_idx": 46}, {"type": "equation", "text": "$$\nk(i):=\\operatorname*{inf}\\,\\big\\{k\\in\\mathbb{N}_{0}\\,|\\,i=\\mathrm{prog}(x^{k})\\big\\}\\in\\mathbb{N}_{0}\\cup\\{\\infty\\}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "f $\\operatorname*{inf}_{k\\in S_{t}}\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<1\\right]<1$ holds, then exists $k\\,\\in\\,S_{t}$ such that $\\mathrm{prog}(x^{k})\\,=\\,1$ , thus, by the definition of $k(1)$ $t^{k(1)}\\leq t^{k}\\leq t$ and $k(1)<\\infty$ . Note that $t^{k(1)}$ is the smallest time when we make progress to the $1^{\\mathrm{th}}$ (first) coordinate. ", "page_idx": 46}, {"type": "text", "text": "Since $x^{0}\\,=\\,0$ and $A$ is a zero-respecting algorithm, the algorithm can return a vector $x^{k}$ with a non-zero first coordinate only if some of returned by the stochastic gradients oracles and compression oracles have the first coordinate not equal to zero. The oracles $O_{i}$ and $\\hat{\\mathcal{C}}_{i}$ are constructed in such a way (see (59) and (60)) that they zero out a coordinate based on i.i.d. bernoulli and uniform trials. According to Protocol 9, even if a stochastic oracle returns a non-zero coordinate, it would not mean that the server will get a non-zero coordinate because a subsequent compression oracle also has to return a non-zero coordinate. ", "page_idx": 46}, {"type": "text", "text": "Every time when the oracle (55) evaluates $g(s_{x};\\xi)$ , it draws i.i.d. random variables $\\xi\\sim\\mathcal{D}$ . Let us enumerate them: ", "page_idx": 46}, {"type": "text", "text": "1. For the stochastic/computation oracles $O_{i}\\ =\\ O_{h_{i}}^{\\nabla f,D_{i}^{\\nabla f}}$ , we consider the sequence {gm,j}=1 ,where $\\xi^{m,j}$ is a bernoull random variable drawn in $j^{\\mathrm{th}}$ call of $g(s_{x};\\xi)$ in the $m^{\\mathrm{th}}$ worker in Line 5 of Protocol 9.   \n2. Fo he compresion raeles $\\hat{c}_{i}=O_{\\tau_{i}}^{c,D_{i}^{c}}$ weconsider the sequence $\\{S^{m,j}\\}_{j=1}^{\\infty}$ where $S^{m,j}$ is a uniform random variable drawn in $j^{\\mathrm{th}}$ call of $g(s_{x};\\xi)$ in the $m^{\\mathrm{th}}$ worker in Line 7 of Protocol 9. ", "page_idx": 46}, {"type": "text", "text": "Let us define the following useful random variables based on previous definitions. We define ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\eta_{m,j}:=\\left\\{\\!\\operatorname*{inf}_{\\infty}\\{i\\,|\\,\\xi^{m,(i+b_{m,j}^{\\eta}-1)}=1\\mathrm{~and~}i\\in\\mathbb{N}\\}\\in\\mathbb{N}\\cup\\{\\infty\\},\\ \\ \\ b_{m,j}^{\\eta}<\\infty,\\right.\\quad\\forall j\\in\\{1,\\ldots,T\\},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mu_{m,j}:=\\left\\{\\!\\operatorname*{inf}_{\\substack{\\infty,\\ldots\\;\\mathstrut}}\\{i\\in S^{m,(i+e_{m,j}^{\\mu}-1)}\\mathrm{~and~}i\\in\\mathbb{N}\\}\\in\\mathbb{N}\\cup\\{\\infty\\},\\ \\ e_{m,j}^{\\mu}<\\infty,\\ \\ \\ \\forall j\\in\\{1,\\ldots,T\\},\\ldots\\right.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where ", "page_idx": 47}, {"type": "text", "text": "l. For all $m\\in[n]$ $j\\geq1$ $b_{m,j}^{\\eta}\\in\\mathbb{N}\\cup\\{\\infty\\}$ is thefrt dexft equn $\\{\\xi^{m,j}\\}_{j=1}^{\\infty}$ that started calculating in (55) in the stochastic oracle $O_{m}$ in or after the iteration $k(j-1)$ ", "page_idx": 47}, {"type": "text", "text": "2. For all $m\\in[n]$ \uff0c $j\\geq1$ \uff0c $b_{m,j}^{\\mu}\\in\\mathbb{N}\\cup\\{\\infty\\}$ is the first index of the sequence $\\{S^{m,j}\\}_{j=1}^{\\infty}$ that started calculating in (55) in the compression oracle $\\hat{\\mathcal{C}}_{m}$ in or after the iteration $k(j-1)$ ", "page_idx": 47}, {"type": "text", "text": "3. Forall $m\\in[n],j\\geq1$ $b_{m,j}^{\\eta}=\\infty$ ,then $e_{m,j}^{\\eta}=\\infty$ For ll $m\\in[n],j\\geq1$ $b_{m,j}^{\\eta}<\\infty$ \uff0c then $e_{m,j}^{\\eta}\\in\\mathbb{N}\\cup\\{\\infty\\}$ is the fist index of the sequence $\\{\\xi^{m,j}\\}_{j=1}^{\\infty}$ that started calculating in (55) in the stochastic oracle $O_{m}$ in or after the iteration $k(j-1)$ and the first moment when $\\xi^{m,(i+b_{m,j}^{\\eta}-1)}=1$ for some $i\\geq1$   \n4. Forall $m\\in[n],j\\geq1$ , if $b_{m,j}^{\\eta}=\\infty$ then $e_{m,j}^{\\mu}=\\infty$ Forall $m\\in[n],j\\geq1$ $b_{m,j}^{\\eta}<\\infty$ then $e_{m,j}^{\\mu}\\in\\mathbb{N}\\cup\\{\\infty\\}$ is the first index of the sequence $\\{S^{m,j}\\}_{j=1}^{\\infty}$ that started calculating in (55) in the compression oracle $\\hat{\\mathcal{C}}_{m}$ in or after the iteration $k(j-1)$ and the first moment when $\\xi^{m,(i+b_{m,j}^{\\eta}-1)}=1$ for some $i\\geq1$ ", "page_idx": 47}, {"type": "text", "text": "$b_{m,j}^{\\eta}\\,=\\,\\infty$ $b_{m,j}^{\\mu}\\,=\\,\\infty$ \uff0c $e_{m,j}^{\\eta}\\,=\\,\\infty$ \uff0cor$e_{m,j}^{\\mu}=\\infty$ $e_{m,j}^{\\eta}\\geq b_{m,j}^{\\eta}$ and em. $e_{m,j}^{\\mu}\\geq b_{m,j}^{\\mu}$", "page_idx": 47}, {"type": "text", "text": "Let us clarify the definitions. At the beginning $x_{0}=0$ , thus $k(0)=0$ It would mean that the first index of the sequence $\\{\\xi^{m,j}\\}_{j=1}^{\\infty}$ , when the worker evaluates $g(s_{x};\\xi)$ in (55), simply equals $b_{m,1}^{\\eta}=1$ or $b_{m,1}^{\\eta}=\\infty$ (by the definition, it equals to $\\infty$ if the oracle was never called). Assume that $b_{m,1}^{\\eta}=1$ \uff0c then $\\eta_{m,1}\\,=\\,\\operatorname*{inf}\\{i\\,|\\,\\xi^{m,i}\\,=\\,1$ and $i\\in\\mathbb{N}\\}$ is the first time when the oracle draws a \u2018successful' random bernoulli trial. This random variable is distributed according to the geometric distribution. Then, $e_{m,1}^{\\eta}$ can be equal to $\\eta_{m,1}+1$ 0 $\\infty$ . At some (random) iteration $k(1)$ , the algorithm $A$ can get the first non-zero coordinate through $g^{k}$ , then $b_{m,2}^{\\mu}$ is the next index of the sequence $\\{\\xi^{m,j}\\}_{j=1}^{\\infty}$ that started calculating in (55).15 ", "page_idx": 47}, {"type": "text", "text": "The server gets a non-zero coordinate if at least one worker draws a successful bernoulli trial, and this coordinate belongs to a set generated by the uniform distribution. It takes $h_{i}$ secondstogenerate onebernoulli trial and $\\tau_{i}$ seconds to generate one uniform trial. ", "page_idx": 47}, {"type": "text", "text": "Then, if $:=\\operatorname*{inf}_{k\\in S_{t}}\\mathbb{1}\\left[\\mathbf{prog}(x^{k})<1\\right]<1$ holds, then ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\hat{t}_{1}:=\\operatorname*{min}_{m\\in[n]}\\left\\{h_{m}\\eta_{m,1}+\\tau_{m}\\mu_{m,1}\\right\\}\\leq t^{k(1)}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "because $h_{m}\\eta_{m,1}+\\tau_{m}\\mu_{m,1}$ is the time required to generate $\\eta_{m,1}$ bernoulli and $\\mu_{m,1}$ uniform trials. In other words, the algorithm can not progress to the next coordinate before the moment when at least one worker generates\u201csuccessfulbernoulli and uniform trials. ", "page_idx": 47}, {"type": "text", "text": "Using the same reasoning, $t^{k(j)}\\geq t^{k(j-1)}+\\hat{t}_{j}$ , where ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\hat{t}_{j}:=\\operatorname*{min}_{m\\in[n]}\\left\\{h_{m}\\eta_{m,j}+\\tau_{m}\\mu_{m,j}\\right\\}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Combining the observations,  if $\\operatorname*{inf}_{k\\in S_{t}}\\mathbb{1}\\left[\\operatorname{prog}(x^{k})<T\\right]\\quad\\quad<\\quad\\quad1$ holds, then $\\begin{array}{r}{\\sum_{j=1}^{T}\\operatorname*{min}_{j\\in[n]}\\left(h_{i}\\eta_{i,j}+\\tau_{i}\\mu_{i,j}\\right)\\leq t^{k(T)}\\leq t}\\end{array}$ Thus ", "page_idx": 48}, {"type": "equation", "text": "$$\n>\\left(\\operatorname*{inf}_{k\\in S_{t}}\\mathbb{1}\\left[\\operatorname{prog}(x^{k})<T\\right]<1\\right)\\leq\\mathbb{P}\\left(\\sum_{i=1}^{T}\\hat{t}_{i}\\leq t\\right)=\\mathbb{P}\\left(\\sum_{i=1}^{T}\\operatorname*{min}_{j\\in[n]}\\left(h_{i}\\eta_{i,j}+\\tau_{i}\\mu_{i,j}\\right)\\leq t\\right)\\quad\\forall t\\geq0\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "(Part 2): The Chernoff Method ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Let us fix $s\\geq0$ and $\\hat{t}\\ge0$ . Using the Chernoff method, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\displaystyle\\sum_{i=1}^{T}\\hat{t}_{i}\\leq\\hat{t}\\right)=\\mathbb{P}\\left(-s\\left(\\displaystyle\\sum_{i=1}^{T}\\hat{t}_{i}\\right)\\geq-s\\hat{t}\\right)=\\mathbb{P}\\left(\\exp\\left(-s\\displaystyle\\sum_{i=1}^{T}\\hat{t}_{i}\\right)\\geq\\exp\\left(-s\\hat{t}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq e^{s\\hat{t}}\\mathbb{E}\\left[\\exp\\left(-s\\displaystyle\\sum_{i=1}^{T}\\hat{t}_{i}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let us bound the expected value separately. For all $j~\\in~[T]$ , let us define $\\mathcal{G}_{j}$ as the $\\sigma.$ -algebra generated by random variables ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{1,j}^{\\eta},\\dotsc,b_{n,j}^{\\eta}}\\\\ &{\\xi^{1,1},\\xi^{1,2},\\dotsc,\\xi^{1,b_{1,j}^{\\eta}-1},}\\\\ &{\\dots}\\\\ &{\\xi^{n,1},\\xi^{n,2},\\dotsc,\\xi^{1,b_{n,j}^{n}-1},}\\\\ &{b_{1,j}^{\\theta},\\dotsc,b_{n,j}^{\\mu},}\\\\ &{S^{1,1},S^{1,2},\\dotsc,S^{1,b_{1,j}^{n}-1},}\\\\ &{\\dots}\\\\ &{S^{n,1},S^{n,2},\\dotsc,S^{n,b_{n,j}^{\\mu}-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The $\\sigma$ -allgebra $\\mathcal{G}_{T}$ contains all information about the random variables before the moment when $\\operatorname{prog}(x^{k})\\,{\\stackrel{\\bullet}{=}}\\,T-1$ .Then, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(-s\\sum_{i=1}^{T}\\hat{t}_{i}\\right)\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\exp\\left(-s\\sum_{i=1}^{T-1}\\hat{t}_{i}-s\\hat{t}_{T}\\right)\\bigg|\\mathcal{G}_{T}\\right]\\right].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Note that if the random variables from (64) are \u201cfixed,\u201d then $\\hat{t}_{i}$ is deterministic for all $i\\in[T-1]$ because $\\hat{t}_{i}$ is a deterministic function of (64), and does not depend on other subsequent random variables. ", "page_idx": 48}, {"type": "text", "text": "Let us show it using a contradiction proof. Without the loss of generality, assume that $\\hat{t}_{T-1}$ depends m $\\xi^{1,b_{1,T}^{\\eta}}\\notin$ (64) By the definition of $b_{1,T}^{\\eta}$ t would mean that thefrst time when the sever can get a vector $g^{k}$ with a non-zero coordinate in the index $T-1$ is after the moment $t^{k(T-1)}$ . We get a contradiction since $t^{k(T-1)}$ is the first time when the algorithm return an iterate with a non-zero coordinate in the index $T-1$ ", "page_idx": 48}, {"type": "text", "text": "Thus, $\\hat{t}_{i}$ is $\\mathcal{G}_{T}.$ -measurable for all $i\\in[T-1]$ and ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(-s\\sum_{i=1}^{T}\\hat{t}_{i}\\right)\\right]=\\mathbb{E}\\left[\\exp\\left(-s\\sum_{i=1}^{T-1}\\hat{t}_{i}\\right)\\mathbb{E}\\left[\\exp\\left(-s\\hat{t}_{T}\\right)\\big|\\,\\mathcal{G}_{T}\\right]\\right].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let us fix $t^{\\prime}\\geq0$ , then, since $\\hat{t}_{T}\\ge0$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\xi}\\left[e^{-s\\hat{t}_{T}}\\Big|\\,\\mathcal{G}_{T}\\right]=\\mathbb{E}\\left[\\left.e^{-s\\hat{t}_{T}}\\right|\\hat{t}_{T}\\leq t^{\\prime},\\mathcal{G}_{T}\\right]\\mathbb{P}\\left(\\hat{t}_{T}\\leq t^{\\prime}|\\mathcal{G}_{T}\\right)+\\mathbb{E}\\left[\\left.e^{-s\\hat{t}_{T}}\\right|\\hat{t}_{T}>t^{\\prime},\\mathcal{G}_{T}\\right]\\left(1-\\mathbb{P}\\left(\\hat{t}_{T}\\leq t^{\\prime}|\\mathcal{G}_{T}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{P}\\left(\\hat{t}_{T}\\leq t^{\\prime}|\\mathcal{G}_{T}\\right)+e^{-s t^{\\prime}}\\left(1-\\mathbb{P}\\left(\\hat{t}_{T}\\leq t^{\\prime}|\\mathcal{G}_{T}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We now use the result of the following lemma that we prove separately. ", "page_idx": 48}, {"type": "text", "text": "Lemma P3. Using the notations from the proof of Lemma $P.2$ wehave ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\hat{t}_{j}\\le t^{\\prime}|\\mathcal{G}_{j}\\right)\\le1-\\prod_{m=1}^{n}\\left(1-\\left(1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\right)\\left(1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "for all $j\\in[T]$ ", "page_idx": 49}, {"type": "text", "text": "Let us temporarily define ", "page_idx": 49}, {"type": "equation", "text": "$$\np^{\\prime}:=1-\\prod_{m=1}^{n}\\left(1-\\left(1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\right)\\left(1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We substitute (67) to (66) and (65) to obtain ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{S}\\left[\\exp\\left(-s\\sum_{i=1}^{T}\\hat{t}_{i}\\right)\\right]\\leq\\left(p^{\\prime}+e^{-s t^{\\prime}}\\left(1-p^{\\prime}\\right)\\right)\\mathbb{E}\\left[\\exp\\left(-s\\sum_{i=1}^{T-1}\\hat{t}_{i}\\right)\\right]\\leq\\left(p^{\\prime}+e^{-s t^{\\prime}}\\left(1-p^{\\prime}\\right)\\right)^{T}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Next, using (63), we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{T}{\\hat{t}}_{i}\\leq{\\hat{t}}\\right)\\leq e^{s{\\hat{t}}}\\left(p^{\\prime}+e^{-s t^{\\prime}}\\left(1-p^{\\prime}\\right)\\right)^{T}=e^{s{\\hat{t}}-s t^{\\prime}T}\\left(1+\\left(e^{s t^{\\prime}}-1\\right)p^{\\prime}\\right)^{T}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Let us take $s={^1\\!\\big/t^{\\prime}}$ , and get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{T}{\\hat{t}}_{i}\\leq{\\hat{t}}\\right)\\leq e^{\\hat{t}/t^{\\prime}-T}\\left(1+\\left(e-1\\right)p^{\\prime}\\right)^{T}\\leq e^{\\hat{t}/t^{\\prime}-T+2p^{\\prime}T}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Let us recall the definition of $p^{\\prime}$ ", "page_idx": 49}, {"type": "equation", "text": "$$\np^{\\prime}:=1-\\prod_{m=1}^{n}\\left(1-\\left(1-(p_{\\omega})^{\\left\\lfloor\\frac{q^{\\prime}}{r_{m}}\\right\\rfloor}\\right)\\left(1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{q^{\\prime}}{h_{m}}\\right\\rfloor}\\right)\\right)=1-\\prod_{m=1}^{n}\\left(1-q_{m}\\right)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where we define $q_{m}:=\\left(1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\right)\\left(1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}\\right)\\in[0,1].$ Using Lemma C.1, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\np^{\\prime}\\leq\\sum_{m=1}^{n}q_{m}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Using the inequality $^{16}\\,1-(1-p)^{m}\\leq p m$ for all $p\\in[0,1]$ and $m\\in\\ensuremath{\\mathbb{N}}_{0}$ , we can get the following three inequalities: ", "page_idx": 49}, {"type": "equation", "text": "$$\nq_{m}\\leq1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}\\leq p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\nq_{m}\\leq1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\leq p_{\\omega}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and ", "page_idx": 49}, {"type": "equation", "text": "$$\nq_{m}\\leq\\left(1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\right)\\left(1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}\\right)\\leq p_{\\omega}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Therefore, we get ", "page_idx": 49}, {"type": "equation", "text": "$$\nq_{m}\\leq\\operatorname*{min}\\left\\{p_{\\sigma}\\,\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor,p_{\\omega}\\,\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor,p_{\\omega}p_{\\sigma}\\,\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor\\,\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor\\right\\}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and ", "page_idx": 50}, {"type": "equation", "text": "$$\np^{\\prime}\\leq\\sum_{m=1}^{n}\\operatorname*{min}\\left\\{p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor,p_{\\omega}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor,p_{\\omega}p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor\\right\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now, we have to take the right $t^{\\prime}$ . Assume that $s^{*}(j)$ is the solution of ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\left(\\sum_{m=1}^{j}\\frac{1}{\\frac{2\\tau_{m}}{p_{\\omega}}+\\frac{4\\tau_{m}h_{m}}{p_{\\omega}p_{\\sigma}s}+\\frac{2h_{m}}{p_{\\sigma}}}\\right)^{-1}=s\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and ", "page_idx": 50}, {"type": "equation", "text": "$$\nj^{*}=\\operatorname*{inf}\\left\\{j\\in[n]\\,|\\,s^{*}(j)<\\operatorname*{max}\\{h_{j+1},\\tau_{j+1}\\}\\right\\}\\in[n],\\qquad\\operatorname*{max}\\{h_{n+1},\\tau_{n+1}\\}\\equiv\\infty.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then we take $\\begin{array}{r}{t^{\\prime}=\\frac{1}{24}s^{*}(j^{*})}\\end{array}$ . If $j^{*}=1$ , then ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathrm{\\Sigma}^{*}(j^{*})=\\left(\\frac{1}{\\frac{2\\tau_{1}}{p_{\\omega}}+\\frac{4\\tau_{1}h_{1}}{p_{\\omega}p_{\\sigma}s^{*}(j^{*})}+\\frac{2h_{1}}{p_{\\sigma}}}\\right)^{-1}\\geq\\left(\\frac{1}{\\frac{2\\tau_{1}}{p_{\\omega}}+\\frac{2h_{1}}{p_{\\sigma}}}\\right)^{-1}\\geq\\left(\\frac{1}{2\\tau_{1}+2h_{1}}\\right)^{-1}\\geq\\frac{1}{2}\\operatorname*{max}\\{h_{1},\\tau_{1}\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Otherwise, if $j^{*}>1$ , since $s^{*}(j^{*})\\leq s^{*}(j^{*}-1)$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s^{*}(j^{*})=\\left(\\displaystyle\\sum_{m=1}^{j^{*}}\\frac{1}{\\frac{2\\tau_{m}}{p_{\\omega}}+\\frac{4\\tau_{m}\\,h_{m}}{p_{\\omega}p_{\\sigma}*(j^{*})}+\\frac{2h_{m}}{p_{\\sigma}}}\\right)^{-1}}\\\\ &{\\qquad\\ge\\left(\\displaystyle\\sum_{m=1}^{j^{*}-1}\\frac{1}{\\frac{2\\tau_{m}}{p_{\\omega}}+\\frac{4\\tau_{m}\\,h_{m}}{p_{\\omega}p_{\\sigma}*(j^{*}-1)}+\\frac{2h_{m}}{p_{\\sigma}}}+\\frac{1}{\\frac{2\\tau_{j^{*}}}{p_{\\omega}}+\\frac{2h_{j^{*}}}{p_{\\sigma}}}\\right)^{-1}}\\\\ &{\\qquad\\ge\\left(\\displaystyle\\sum_{m=1}^{j^{*}-1}\\frac{1}{\\frac{2\\tau_{m}}{p_{\\omega}}+\\frac{4\\tau_{m}\\,h_{m}}{p_{\\omega}p_{\\sigma}*(j^{*}-1)}+\\frac{2h_{m}}{p_{\\sigma}}}+\\frac{1}{\\operatorname*{max}\\{h_{j^{*}},\\tau_{j^{*}}\\}}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By the definitions of $s^{*}(j^{*}-1)$ and $j^{*}$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\left(\\sum_{m=1}^{j^{*}-1}\\frac{1}{\\frac{2\\tau_{m}}{p_{\\omega}}+\\frac{4\\tau_{m}h_{m}}{p_{\\omega}p_{\\sigma}s^{*}(j^{*}-1)}+\\frac{2h_{m}}{p_{\\sigma}}}\\right)^{-1}=s^{*}(j^{*}-1)\\geq\\operatorname*{max}\\{h_{j^{*}},\\tau_{j^{*}}\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, we get ", "page_idx": 50}, {"type": "equation", "text": "$$\ns^{*}(j^{*})\\geq\\left(\\frac{1}{\\operatorname*{max}\\{h_{j^{*}},\\tau_{j^{*}}\\}}+\\frac{1}{\\operatorname*{max}\\{h_{j^{*}},\\tau_{j^{*}}\\}}\\right)^{-1}=\\frac{1}{2}\\operatorname*{max}\\{h_{j^{*}},\\tau_{j^{*}}\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Using $\\begin{array}{r}{s^{\\ast}(j^{\\ast})\\geq\\frac{1}{2}\\operatorname*{max}\\{h_{j^{\\ast}},\\tau_{j^{\\ast}}\\}}\\end{array}$ , we obtain ", "page_idx": 50}, {"type": "equation", "text": "$$\nt^{\\prime}=\\frac{1}{24}\\operatorname*{max}\\left\\{\\frac12\\operatorname*{max}\\{h_{j^{*}},\\tau_{j^{*}}\\},s^{*}(j^{*})\\right\\}\\geq\\frac1{48}\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{j},\\tau_{j}\\},s^{*}(j)\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We use the last inequality later. Let us return to the inequality (69). Using the definition of $j^{*}$ ,we obtain $\\begin{array}{r}{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor=0}\\end{array}$ $\\begin{array}{r}{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor=0}\\end{array}$ for all $m>j^{*}$ and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{\\prime}\\leq\\displaystyle\\sum_{m=1}^{n}\\operatorname*{min}\\left\\lbrace p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor,p_{\\omega}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor,p_{\\omega}p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor\\right\\rbrace}\\\\ &{\\quad=\\displaystyle\\sum_{m=1}^{j^{*}}\\operatorname*{min}\\left\\lbrace p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor,p_{\\omega}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor,p_{\\omega}p_{\\sigma}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor\\right\\rbrace}\\\\ &{\\quad\\leq\\displaystyle\\sum_{m=1}^{j^{*}}\\operatorname*{min}\\left\\lbrace\\frac{p_{\\sigma}t^{\\prime}}{h_{m}},\\frac{p_{\\omega}t^{\\prime}}{\\tau_{m}},\\frac{p_{\\omega}p_{\\sigma}t^{\\prime2}}{\\tau_{m}h_{m}}\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where we use $\\lfloor x\\rfloor\\leq x$ for all $x\\geq0$ . Using $\\begin{array}{r}{\\operatorname*{max}\\{x,y,z\\}\\ge\\frac{1}{3}\\left(x+y+z\\right)}\\end{array}$ for all $x,y,z\\geq0$ we have ", "page_idx": 51}, {"type": "equation", "text": "$$\np^{\\prime}\\leq\\sum_{m=1}^{j^{\\ast}}\\left(\\operatorname*{max}\\left\\{\\frac{h_{m}}{p_{\\sigma}t^{\\prime}},\\frac{\\tau_{m}}{p_{\\omega}t^{\\prime}},\\frac{\\tau_{m}h_{m}}{p_{\\omega}p_{\\sigma}t^{\\prime2}}\\right\\}\\right)^{-1}\\leq\\sum_{m=1}^{j^{\\ast}}\\frac{3}{\\frac{\\tau_{m}}{p_{\\omega}t^{\\prime}}+\\frac{\\tau_{m}h_{m}}{p_{\\omega}p_{\\sigma}t^{\\prime2}}+\\frac{h_{m}}{p_{\\sigma}t^{\\prime}}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Since $\\begin{array}{r}{t^{\\prime}=\\frac{1}{24}s^{*}(j^{*})}\\end{array}$ , we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\np^{\\prime}\\leq\\sum_{m=1}^{j^{\\ast}}\\frac{3}{\\frac{24\\tau_{m}}{p_{\\omega}s^{*}(j^{*})}+\\frac{24^{2}\\tau_{m}h_{m}}{p_{\\omega}p_{\\sigma}(s^{*}(j^{*}))^{2}}+\\frac{24h_{m}}{p_{\\sigma}s^{*}(j^{*})}}\\leq\\frac{1}{4}\\sum_{m=1}^{j^{*}}\\frac{1}{\\frac{2\\tau_{m}}{p_{\\omega}s^{*}(j^{*})}+\\frac{4\\tau_{m}h_{m}}{p_{\\omega}p_{\\sigma}(s^{*}(j^{*}))^{2}}+\\frac{2h_{m}}{p_{\\sigma}s^{*}(j^{*})}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Note that $s^{*}(j^{*})$ is the solution of (70), thus, we get ", "page_idx": 51}, {"type": "equation", "text": "$$\np^{\\prime}\\leq{\\frac{1}{4}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Substituting this inequality to (68), we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{T}\\hat{t}_{i}\\le\\hat{t}\\right)\\le e^{\\hat{t}/t^{\\prime}-\\frac{T}{2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For $\\begin{array}{r}{\\hat{t}\\leq t^{\\prime}\\left(\\frac{T}{2}+\\log\\delta\\right)}\\end{array}$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{T}\\hat{t}_{i}\\leq\\hat{t}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Recall that the definition of $t^{\\prime}$ . Using (71), we have ", "page_idx": 51}, {"type": "equation", "text": "$$\nt^{\\prime}\\geq\\frac{1}{48}\\operatorname*{min}_{j\\in[n]}\\operatorname*{max}\\{\\operatorname*{max}\\{h_{j},\\tau_{j}\\},s^{*}(j)\\}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "The last term equals to the equilibrium time $t^{*}(1/p_{\\omega},1/p_{\\sigma},h_{1},\\tau_{1},.~.~.~,h_{n},\\tau_{n})$ from Def. 3.1 since the pairs $(h_{j},\\tau_{j})$ are sorted by 1 $\\operatorname*{nax}\\{h_{j},\\tau_{j}\\}$ . Thus, we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\nt^{\\prime}\\geq{\\frac{1}{48}}t^{*}(1/p_{\\omega},1/p_{\\sigma},h_{1},\\tau_{1},\\ldots,h_{n},\\tau_{n}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Finally, we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{inf}_{k\\in S_{t}}\\mathbb{1}\\left[{\\mathrm{prog}(x^{k})<T}\\right]<1\\right)\\leq\\mathbb{P}\\left(\\sum_{i=1}^{T}{\\hat{t}_{i}\\leq t}\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for ", "page_idx": 51}, {"type": "equation", "text": "$$\nt\\leq{\\frac{1}{48}}t^{*}(1/_{p_{\\omega}},1/_{p_{\\sigma}},h_{1},\\tau_{1},\\dots,h_{n},\\tau_{n})\\left({\\frac{T}{2}}+\\log\\delta\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "P.2Proof of Lemma P.3 ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Lemma P.3. Using the notations from the proof of Lemma $P.2$ wehave ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\hat{t}_{j}\\le t^{\\prime}\\big|\\mathcal{G}_{j}\\right)\\le1-\\prod_{m=1}^{n}\\left(1-\\left(1-(1-p_{\\omega})\\Big|^{\\frac{t^{\\prime}}{\\tau_{m}}}\\right]\\right)\\left(1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for all $j\\in[T]$ \uff1a", "page_idx": 51}, {"type": "text", "text": "Proof. We prove the result for $j=T$ . The proofs for the cases $1\\leq j<T$ are the same. We consider the conditional probability ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\widehat{t}_{T}\\leq t^{\\prime}\\middle|\\mathcal{G}_{T}\\right)=\\mathbb{P}\\left(\\operatorname*{min}_{m\\in[n]}\\left\\{h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}\\right\\}\\leq t^{\\prime}\\middle|\\mathcal{G}_{T}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Let us consider the $\\sigma.$ -algebra $\\mathcal{H}_{T}$ generated by (64) with $j=T$ and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{1,T}^{\\eta},\\dotsc,e_{n,T}^{\\eta}}\\\\ &{\\xi^{1,1},\\xi^{1,2},\\dotsc,\\xi^{1,e_{1,T}^{\\eta}-1},}\\\\ &{\\dots}\\\\ &{\\xi^{n,1},\\xi^{n,2},\\dotsc,\\xi^{1,e_{n,T}^{\\eta}-1},}\\\\ &{e_{1,T}^{\\mu},\\dotsc,e_{n,T}^{\\mu},}\\\\ &{S^{1,1},S^{1,2},\\dotsc,S^{1,e_{1,T}^{\\eta}-1},}\\\\ &{\\dots}\\\\ &{S^{n,1},S^{n,2},\\dotsc,S^{n,e_{n,T}^{\\mu}-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "By the construction, $e_{m,T}^{\\eta}\\geq b_{m,T}^{\\eta}$ and $e_{m,T}^{\\mu}\\geq b_{m,T}^{\\mu}$ . Since $\\mathcal{G}_{T}\\subseteq\\mathcal{H}_{T}$ , we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\hat{t}_{T}\\leq t^{\\prime}\\middle|\\mathcal{G}_{T}\\right)=\\mathbb{E}\\left[\\mathbb{P}\\left(\\operatorname*{min}_{m\\in[n]}\\left\\{h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}\\right\\}\\leq t^{\\prime}\\middle|\\mathcal{H}_{T}\\right)\\right|\\mathcal{G}_{T}\\right].\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Since $\\mathcal{G}_{T}\\subseteq\\mathcal{H}_{T}$ .then $b_{m,T}^{\\mu}$ .isd $\\mathcal{H}_{T}$ -measurable.By the defnition of $e_{m,T}^{\\eta},\\eta_{m,T}$ are $\\mathcal{H}_{T}$ measurable. Let us show it using a contradiction proof. Without the loss of generality, assume that $\\eta_{m,T}$ depends on $\\xi^{1,e_{m,T}^{\\eta}}\\notin$ (73) It woul mean that te frst tme, whe $\\xi^{m,i}=1$ afe the itertion $k(T-1)$ \uff0c happens with $i\\geq e_{m,T}^{\\eta}$ At the same time, by the defnition of $e_{m,T}^{\\eta}$ there exists $i<e_{m,T}^{\\eta}$ such that $\\xi^{m,i}\\,=\\,1$ calculated after the iteration $k(T-1)$ . We get a contradiction. Given $\\mathcal{H}_{T}$ \uff0c $\\mu_{m,T}$ are mutually independen since $\\{S^{m,j}\\}_{j=1}^{\\infty}$ are mutully independent and $e_{m,T}^{\\mu}$ are $\\mathcal{H}_{T}$ -measurable. Therefore, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\widehat{t}_{T}\\leq t^{\\prime}\\middle|\\mathcal{G}_{T}\\right)=\\mathbb{E}\\left[\\mathbb{P}\\left(\\underset{m\\in[n]}{\\operatorname*{min}}\\left\\lbrace h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}\\right\\rbrace\\leq t^{\\prime}\\middle|\\mathcal{H}_{T}\\right)\\right]\\mathscr{G}_{T}\\right]}\\\\ &{\\qquad\\qquad=1-\\mathbb{E}\\left[\\mathbb{P}\\left(\\underset{m=1}{\\overset{n}{\\prod}}\\left\\lbrace h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}>t^{\\prime}\\middle\\rbrace\\right\\rbrace\\mathcal{H}_{T}\\right)\\middle|\\mathcal{G}_{T}\\right]}\\\\ &{\\qquad\\qquad=1-\\mathbb{E}\\left[\\underset{m=1}{\\overset{n}{\\prod}}\\mathbb{P}\\left(h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}>t^{\\prime}\\middle|\\mathcal{H}_{T}\\right)\\middle|\\mathcal{G}_{T}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Let us consider the probability $\\mathbb{P}\\left(h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}>t^{\\prime}|\\mathcal{H}_{T}\\right)$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}>t^{\\prime}|\\mathcal{H}_{T}\\right)=1-\\mathbb{P}\\left(h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}|\\mathcal{H}_{T}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq1-\\mathbb{P}\\left(h_{m}\\eta_{m,T}\\leq t^{\\prime},\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}|\\mathcal{H}_{T}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=1-\\mathbb{E}\\left[\\mathbb{1}\\left[h_{m}\\eta_{m,T}\\leq t^{\\prime}\\right]\\mathbb{1}\\left[\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}\\right]\\right|\\mathcal{H}_{T}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "because the event $\\{h_{m}\\eta_{m,T}\\leq t^{\\prime}\\}\\bigcap\\{\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}\\}$ follows from $\\{h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}\\}$ Since $\\eta_{m,T}$ is $\\mathcal{H}_{T}$ -measurable, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\left(h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}>t^{\\prime}|\\mathcal{H}_{T}\\right)\\geq1-\\mathbb{E}\\left[\\left.\\mathbb{1}\\left[\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}\\right]\\right|\\mathcal{H}_{T}\\right]\\mathbb{1}\\left[h_{m}\\eta_{m,T}\\leq t^{\\prime}\\right]}\\\\ &{}&{=1-\\mathbb{P}\\left(\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}|\\mathcal{H}_{T}\\right)\\mathbb{1}\\left[h_{m}\\eta_{m,T}\\leq t^{\\prime}\\right].\\quad\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Let us consider the probability and $\\begin{array}{r}{\\mathbb{P}\\left(\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}|\\mathcal{H}_{T}\\right)=0}\\end{array}$ Otherise ift $\\begin{array}{r}{\\mathbb{P}\\left(\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}|\\mathcal{H}_{T}\\right).}\\end{array}$ $e_{m,T}^{\\mu}=e<\\infty$ Given then $\\mathcal{H}_{T}$ \u8fd1\uff0c $e_{m,T}^{\\mu}=\\infty$ , then $\\mu_{m,T}=\\infty$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mu_{m,T}=\\operatorname*{inf}\\{i\\,|\\,j\\in S^{m,(i+e-1)}\\,\\,\\mathrm{and}\\,\\,i\\in\\mathbb{N}\\}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and it is distributed with the geometric distribution with $p_{\\omega}$ . Thus, we have17 ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\tau_{m}\\mu_{m,T}\\le t^{\\prime}|\\mathcal{H}_{T}\\right)=\\left\\{1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor},\\begin{array}{r l}{e_{m,T}^{\\mu}<\\infty}\\\\ {e_{m,T}^{\\mu}=\\infty}\\end{array}\\right.\\le1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "$1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}=0$ i\u00b7 $p_{\\omega}\\,=1$ and $\\begin{array}{r}{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor=0}\\end{array}$ becaus if $t^{\\prime}<\\tau_{m}$ then $\\mathbb{P}\\left(\\tau_{m}\\mu_{m,T}\\leq t^{\\prime}\\right)=0$ for the r.v. $\\mu_{m,T}$ from the geometric distribution for all $p_{\\omega}\\in(0,1]$ ", "page_idx": 52}, {"type": "text", "text": "because the probability that $j^{\\mathrm{th}}$ coordinate belongs to $S^{m,(i+e-1)}$ equals to $p_{\\omega}$ . We substitute this inequality to (75) and get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(h_{m}\\eta_{m,T}+\\tau_{m}\\mu_{m,T}>t^{\\prime}|\\mathcal{H}_{T}\\right)\\geq1-\\left(1-(1-p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\right)\\mathbb{1}\\left[h_{m}\\eta_{m,T}\\leq t^{\\prime}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Next, we substitute this inequality to (74) and obtain ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\hat{t}_{T}\\le t^{\\prime}\\middle|\\mathcal{G}_{T}\\right)\\leq1-\\mathbb{E}\\left[\\prod_{m=1}^{n}\\left(1-\\left(1-p_{\\omega}\\right)^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\right)\\mathbb{1}\\left[h_{m}\\eta_{m,T}\\le t^{\\prime}\\right]\\right)\\middle|\\mathcal{G}_{T}\\right].\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Given $g_{T},\\,\\eta_{m,T}$ are independent because bm.T are Gr-measurable. Thus ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\left(\\hat{t}_{T}\\leq t^{\\prime}\\middle|\\mathcal{G}_{T}\\right)\\leq1-\\prod_{m=1}^{n}\\left(1-\\left(1-(p_{\\omega})\\Big\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor\\right)\\mathbb{E}\\left[\\left.\\mathbb{1}\\left[h_{m}\\eta_{m,T}\\leq t^{\\prime}\\right]\\right\\rvert\\mathcal{G}_{T}\\right]\\right)}\\\\ &{}&{=1-\\displaystyle\\prod_{m=1}^{n}\\left(1-\\left(1-(1-p_{\\omega})\\Big\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\Big\\rfloor\\right)\\mathbb{P}\\left(h_{m}\\eta_{m,T}\\leq t^{\\prime}\\middle|\\mathcal{G}_{T}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Using the same reasoning as with $\\mu_{m,T}$ , we get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(h_{m}\\eta_{m,T}\\leq t^{\\prime}|\\mathcal{G}_{T}\\right)=\\left\\{1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor},\\begin{array}{l}{b_{m,T}^{\\eta}<\\infty}\\\\ {b_{m,T}^{\\eta}=\\infty}\\end{array}\\right\\}1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "because, given $\\mathcal{G}_{T}$ $\\eta_{m,T}$ equals $\\infty$ or a random variable distributed according to the geometric distribution with $p_{\\sigma}$ . Therefore, we obtain ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\hat{t}_{T}\\leq t^{\\prime}\\middle|\\mathcal{G}_{T}\\right)\\leq1-\\prod_{m=1}^{n}\\left(1-\\left(1-(p_{\\omega})^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{m}}\\right\\rfloor}\\right)\\left(1-(1-p_{\\sigma})^{\\left\\lfloor\\frac{t^{\\prime}}{h_{m}}\\right\\rfloor}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "P.3 Another Construction ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In the proof of Theorem O.5, we could use the following construction: ", "page_idx": 53}, {"type": "text", "text": "(Step 3: Compression Operator) Let us define $\\begin{array}{r}{p_{\\omega}:=\\frac{1}{\\omega+1}}\\end{array}$ . In our constrution, we take a compressor that outputs random coordinates of an input vector, scaled by $1/\\!p_{\\omega}$ , where each coordinate is taken with the probability $p_{\\omega}$ . Each worker has access to the independent compressed realizations of a such compressor. More formally, we assume that ", "page_idx": 53}, {"type": "equation", "text": "$$\n[\\mathcal{C}(\\boldsymbol{x};S)]_{j}:=\\left\\{\\frac{1}{p_{\\omega}}x_{j},\\quad j\\in S,\\quad\\forall j\\in[T],\\right.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $S$ is a random subset of $[T]$ , where each element from $[T]$ appears with the probability $p_{\\omega}$ independently. Then ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}\\left[[\\mathcal{C}(x;S)]_{j}\\right]=x_{j}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{\\Sigma}_{S}\\left[\\left\\lVert\\mathcal{C}(x;S)\\right\\rVert^{2}\\right]=\\mathbb{E}_{S}\\left[\\sum_{j=1}^{T}\\mathbb{1}\\left[j\\in S\\right]\\frac{1}{p_{\\omega}^{2}}x_{j}^{2}\\right]=\\sum_{j=1}^{T}\\mathbb{P}\\left(j\\in S\\right)\\frac{1}{p_{\\omega}^{2}}x_{j}^{2}=\\sum_{j=1}^{T}\\frac{1}{p_{\\omega}}x_{j}^{2}=\\left(\\omega+1\\right)\\left\\lVert x\\right\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Thus, we have $\\mathcal{C}\\,\\in\\,\\mathbb{U}(\\omega)$ . We take mutually independent distributions $\\mathcal{D}_{i}^{c}$ that generate random subsets $S$ described above. ", "page_idx": 53}, {"type": "text", "text": "This construction is also valid and does not require the assumption $\\omega+1\\lesssim L\\Delta/\\varepsilon$ .However, unlike the construction from Theorem O.5, this construction can return a random number of non-zero coordinates. ", "page_idx": 53}, {"type": "text", "text": "Q Experiments ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "The experiments were prepared in Python. The distributed environment was emulated on machines with Intel(R) Xeon(R) Gold 6226R CPU $\\textcircled{a}\\ 2.90\\mathrm{GHz}$ and64cores. ", "page_idx": 54}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/65ada63819313bd134cf4c36cf16865ee6e6e37d53c1db71e2e6ab0288101c51.jpg", "img_caption": ["Q.1  Experiments with Logistic Regression "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "We start our experiments with a practical setup: a logistic regression problem with the MNIST dataset (LeCun et al., 2010). The optimization steps of algorithms are emulated in Python, where we fix the number of workers to $n=100$ , each worker has access to the MNIST dataset and sample 4 samples when calculating a stochastic gradient. We compare Shadowheart SGD with QSGD, Asynchronous SGD (we implement the version from (Koloskova et al., 2022), Minibatch SGD, and $\\mathsf{S G D}_{\\mathsf{o n e}}$ ${\\mathsf{S G D}}_{\\mathsf{o n e}}$ is the method described in Sec. 7, where SGD is run on the fastest worker locally. In Shadowheart SGD, we fine-tune the parameter $\\sigma^{2}/\\varepsilon\\in\\{1,5,10,20,30,40,80,120,150,200\\}$ . In all the methods, we also finetune the step sizes. The dimension of the problem in the logistic regression problem is $d=7850$ .In Shadowheart SGD and QSGD, we take Rand $K$ with $K=700$ ", "page_idx": 54}, {"type": "text", "text": "We assume that the computations time $h_{i}$ of a stochastic gradient equals to $\\sqrt{i}$ seconds in the $i^{\\mathrm{th}}$ worker. We consider three communication time setups, where it takes $\\dot{\\tau}_{i}$ seconds to send one coordinate from the $i^{\\mathrm{th}}$ worker to the server and ", "page_idx": 54}, {"type": "text", "text": "1. $\\dot{\\tau}_{i}=\\sqrt{i}/d$ (High-speed communications),   \n2. $\\dot{\\tau}_{i}=\\sqrt{i}/d^{3/4}$ (Medium-speed communications),   \n3. $\\dot{\\tau}_{i}=\\sqrt{i}/d^{1/2}$ (Low-speed communications). ", "page_idx": 54}, {"type": "text", "text": "In the high-speed regime, the communication between the server and the worker is relatively fast. At the same time, in low-speed regimes, communication is expensive. We are ready to present the results of our experiments in Fig. 1a, 1c, and 1b. ", "page_idx": 54}, {"type": "text", "text": "In Figure 1a, one can see that Shadowheart SGD, Asynchronous SGD, and Minibatch SGD are the fastest because it is not expensive to send a non-compressed vector in the \\*\"high-speed communications\" regime. $\\mathsf{S G D}_{\\mathsf{o n e}}$ is the slowest since it utilizes only one worker. ", "page_idx": 54}, {"type": "text", "text": "Next, we analyze Figure 1b, where Shadowheart SGD and $\\mathsf{S G D}_{\\mathsf{o n e}}$ have the best performance. SGDone improves the convergence relative to other methods because the communication speed is much slower than in Figure 1a, and it is expensive to send a non-compressed vector. ", "page_idx": 55}, {"type": "text", "text": "One can see that Shadowheart SGD is very robust to all regimes and has one of the best convergence rates in all experiments. Notably, in the \u201cmedium-speed communications\" regime, where it is still expensive to send a non-compressed vector, our new method converges faster than other baseline methods. ", "page_idx": 55}, {"type": "text", "text": "Q.2 Experiments with quadratic optimization tasks and multiplicative noise ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "In real machine learning tasks, it is not easy to control noise. Thus, we generated synthetic quadratic optimization tasks where we can control the noise of stochastic gradients. In particular, we consider ", "page_idx": 55}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{2}}x^{\\top}\\mathbf{A}x-b^{\\top}x\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and take $d=1000$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\frac{1}{4}\\left(\\begin{array}{l l l l}{\\ 2}&{-1}&&{0}\\\\ {-1}&{\\ddots}&{\\ddots}&\\\\ &{\\ddots}&{\\ddots}&\\\\ {0}&&{-1}&{2}\\end{array}\\right)\\in\\mathbb{R}^{d\\times d}\\quad\\mathrm{~and~}\\quad b=\\frac{1}{4}\\left[\\begin{array}{l}{-1}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right]\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We consider the following stochastic gradients: ", "page_idx": 55}, {"type": "equation", "text": "$$\n[\\nabla f(x;\\xi)]_{j}:=\\nabla_{j}f(x)\\left(1+\\mathbb{1}\\left[j>\\mathrm{prog}(x)\\right]\\left(\\frac{\\xi}{p}-1\\right)\\right)\\quad\\forall x\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\xi\\sim\\mathrm{Bernoulli}(p)$ for all $i\\in[n]$ , and $p\\in(0,1]$ . We denote $[x]_{j}$ as the $j^{\\mathrm{th}}$ index of a vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . In our experiments, we take the starting point $\\boldsymbol{x}^{0}=[\\sqrt{d},0,\\ldots,0]^{\\intercal}$ and $p\\in\\{10^{-3},10^{-4}\\}$ the smaller $p$ the larger the noise of stochastic gradients. ", "page_idx": 55}, {"type": "text", "text": "Q.2.1 Discussion of the experiments from Sec. Q.2.2 ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Using this setup, in Figures 2, 3, 4, we fix all parameters except one that we vary to understand the dependencies. In all experiments, we observe that Shadowheart SGD is the most robust to input changes among other centralized methods (QSGD, Asynchronous SGD, Minibatch SGD) and can converge significantly faster. At the same time, we observe that $\\mathsf{S G D}_{\\mathsf{o n e}}$ can be faster than our method in some setups. It happens in the regimes when communication is expensive (see Figure 2a), which is expected and discussed in Sec. 7. Even if communication is expensive, $\\mathsf{S G D}_{\\mathsf{o n e}}$ startsto slowdown relative to other methods when we increase the noise (compare Figures 2b and 2a). The following experiments agree with our theoretical discussion in Sec. 7. ", "page_idx": 55}, {"type": "text", "text": "Q.2.2 Plots ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "In these experiments, we take $n=10000$ \uff0c $p=10^{-3}$ \uff0c $h_{i}={\\sqrt{i}}$ $\\dot{\\tau}_{i}=\\sqrt{i}/d^{3/4}$ as base parameters; in each plot, we vary one parameter. ", "page_idx": 55}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/fa2314fe2a3cc32fd0f6482dc44b0ecaa7b568944c148c196e1549204481f797.jpg", "img_caption": ["Figure 2: ${\\mathsf{S G D}}_{\\mathsf{o n e}}$ starts to slow down relative to Shadowheart SGD and other methods when we increasethenoise. "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/326c8a24b5aff2ae2094c591a623c3e53c36c4051bf3096251a625e0157fb9bd.jpg", "img_caption": ["Figure 3: The non-compressed methods Asynchronous SGD and Minibatch SGD slow down relative to Shadowheart SGD when we increase the communication times. "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/0cf2f12caa71d61ea9a25c2744a028351c2f33a512e5ee163d4498de2e6104aa.jpg", "img_caption": ["Figure 4: Shadowheart SGD improves when we decrease the computation times from $\\sqrt{i}$ to 1. "], "img_footnote": [], "page_idx": 56}, {"type": "text", "text": "Q.3 Experiments with quadratic optimization tasks and additive noise ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "In this section, we consider the same problem as in Sec. Q.2. However, unlike the multiplicative noise, we consider the following additive noise: ", "page_idx": 56}, {"type": "equation", "text": "$$\n[\\nabla f(x;\\xi)]_{j}:=\\nabla_{j}f(x)+\\zeta\\quad\\forall x\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $\\zeta\\sim\\mathcal{N}(0,\\sigma^{2})$ is a sample from the normal distribution. Be default, we take $n=100$ workers, the dimension $d=100$ , and use the Randl compressor $(\\omega=d/1-1=99)$ \uff0c $\\boldsymbol{x}_{0}=[1,\\cdots\\,,1]^{\\intercal}$ \uff0c $\\sigma=10^{-1}$ $\\varepsilon=10^{-4}$ ; thus, the ratio $\\sigma^{2}/\\varepsilon=10^{2}$ .For all methods, we choose the step sizes in such a way that they converge to the same neighborhood of the stationary point. ", "page_idx": 56}, {"type": "text", "text": "In Figure 5, we sample $h_{i}^{k}$ and $\\boldsymbol{\\dot{\\tau}}_{i}^{k}$ from the uniform distribution $U(0.1,1)$ , hence the communication and computation time vary on each iteration for each client. If we increase the number of clients $n$ Shadowheart SGD improves (Fig. 5) compared to other methods, confirming our theory. ", "page_idx": 56}, {"type": "text", "text": "In Figure 6, we can see the similar results with different ratios $\\sigma^{2}/\\varepsilon$ : Shadowheart SGD is much better when the ratio is large (Fig. 6). On the other hand, when $\\sigma^{2}/\\varepsilon$ is small (Fig. 6a) $\\mathrm{{SGD}_{\\mathrm{{one}}}}$ canbebetter because, intuitively, we only need a few workers to find the minimum with a small noise (see also Sec. 7). ", "page_idx": 56}, {"type": "text", "text": "Next, we perform a series of experiments with different computation time and communication times ratios.Wetake $\\dot{\\tau}_{i}^{k}/h_{i}^{k}=c$ for ali $i\\in[n]$ where $c>0$ ", "page_idx": 56}, {"type": "text", "text": "In Figure 7, we take $h_{i}^{k}\\sim U(0.1,1)$ and $\\dot{\\tau}_{i}^{k}\\sim c\\cdot U(0.1,1)$ . Shadowheart SGD is better in the high and medium communication speed regimes (Fig. 7b), when the communication times are not too large. On the other hand, with large $\\bar{c}=10^{\\bar{2}}$ , Shadowheart SGD spends much time on sending gradients to the server, whereas $\\mathsf{S G D}_{\\mathsf{o n e}}$ does not spend time on communication and does not compress (Fig. 7c). Similar to Figure 7, we obtain the results with $h_{i}^{k}=\\sqrt{i}$ and $\\dot{\\tau}_{i}^{k}=c\\cdot\\sqrt{i}$ in Figure 8. ", "page_idx": 56}, {"type": "text", "text": "Q.3.1 Plots ", "text_level": 1, "page_idx": 57}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/8d1e26a6d13e58b66f5104c2b238af4e385ed1040b911005a9243124215265fb.jpg", "img_caption": ["Figure 5: $h_{i}^{k},\\dot{\\tau}_{i}^{k}\\sim U(0.1,1)$ "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/3f6bea57fd2a4b6953374d30f0f42fae87a5a7faca612d1711360a41667eca15.jpg", "img_caption": ["Figure 6: $h_{i}^{k},\\dot{\\tau}_{i}^{k}\\sim U(0.1,1)$ "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/39089bc5530e03269e51d7d44dcab472ed5f5eda15b265b1389459acab36b89a.jpg", "img_caption": ["Figure 7: $h_{i}^{k}\\sim U(0.1,1)$ \uff0c $\\dot{\\tau}_{i}^{k}\\sim c\\cdot U(0.1,1)$"], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "O8yHsRLwPl/tmp/d4e758549471a4133ed65fe72464e6e8277591d69cc1d81b3903999b9de763a0.jpg", "img_caption": ["Figure 8: $h_{i}^{k}=\\sqrt{i}$ $\\dot{\\tau}_{i}^{k}=c\\cdot\\sqrt{i}$ "], "img_footnote": [], "page_idx": 57}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: Section 3 and Table 1 ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 58}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: Sections 4.1, 4.2, and 7.2 ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 58}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: Section 1 and Appendix ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 59}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: Section Q Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to acces this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 59}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: In the supplementary material Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 60}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: Section Q ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 60}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [No] ", "page_idx": 60}, {"type": "text", "text": "Justification: While each particular experiment in every plot from Section Q was run with one seed, the amount of provided experiments and considered settings can give a high confidence in our judgments that the experimental part supports the theoretical part, which is our main contribution. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 61}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: Section Q Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 61}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: Our work considers a mathematical problem from the machine learning domain. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: Our work considers a mathematical problem from the machine learning domain. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 62}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: Our work considers a mathematical problem from the machine learning domain. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 62}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: Section Q ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 62}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 63}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The code for the experiments is in the supplementary materials. Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 63}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 63}, {"type": "text", "text": "", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 63}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 63}, {"type": "text", "text": "", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 63}]