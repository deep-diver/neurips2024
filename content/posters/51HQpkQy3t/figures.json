[{"figure_path": "51HQpkQy3t/figures/figures_1_1.jpg", "caption": "Figure 1: Left: The efficiency benefits of applying DiTFastAttn on PixArt-Sigma (Chen et al., 2024) when generating images of different resolutions. The Y-axis shows the #FLOPs fraction normalized by the #FLOPs of the original model. Right: The qualitative results of applying DiTFastAttn on 1024\u00d71024 PixArt-Sigma.", "description": "This figure demonstrates the efficiency gains of DiTFastAttn on PixArt-Sigma for image generation at various resolutions. The left panel is a bar chart showing the fraction of FLOPs (floating-point operations) used by different components (raw attention, DiTFastAttn, and others) for various image resolutions (512x512, 1024x1024, 2048x2048).  The right panel shows example image generation results comparing the original PixArt-Sigma model to the one enhanced with DiTFastAttn at 1024x1024 resolution. This visually demonstrates the improved efficiency without significant loss in quality.", "section": "1 Introduction"}, {"figure_path": "51HQpkQy3t/figures/figures_3_1.jpg", "caption": "Figure 2: Types of Redundancy and Corresponding Compression Techniques. Left: Redundancy in the spatial dimension, denoising steps, and CFG. Right: Techniques implemented in DiTFastAttn to reduce redundancy for each type. DiTFastAttn employs window attention to minimize attention redundancy, while maintaining performance using residuals. Additionally, attention outputs are shared both step-wise and CFG-wise to reduce redundancy.", "description": "This figure illustrates the three types of redundancies identified in the attention computation of Diffusion Transformers (DiT) during inference: spatial redundancy, temporal redundancy, and conditional redundancy.  For each type of redundancy, the figure shows the corresponding compression technique used in DiTFastAttn to reduce computational cost.  These techniques include Window Attention with Residual Sharing (WA-RS) to reduce spatial redundancy, Attention Sharing across Timesteps (AST) to reduce temporal redundancy, and Attention Sharing across CFG (ASC) to reduce conditional redundancy. The figure visually represents how these techniques are integrated to improve efficiency.", "section": "3 Method"}, {"figure_path": "51HQpkQy3t/figures/figures_4_1.jpg", "caption": "Figure 3: Window Attention with Residual Sharing. (a) Left: Example of the attention map showing the window pattern. Right: The MSE between the window attention outputs in the previous and current step (yellow line) versus the MSE between the output residuals of window and full attention in the previous and current step (blue line). The output residual exhibits minimal changes over the steps. (b) Illustration of Window Attention with Residual Sharing.", "description": "This figure illustrates the concept of Window Attention with Residual Sharing (WA-RS).  Part (a) shows that the attention values concentrate in a window along the diagonal of the attention matrix, and that the error (Mean Squared Error, MSE) between the full attention and the window attention outputs is minimal and relatively consistent across steps. Part (b) illustrates the WA-RS technique which takes advantage of this consistency by caching the residual (difference between full and window attention) and reusing it for subsequent steps, saving computation while maintaining accuracy.", "section": "3.2 Window Attention with Residual Sharing (WA-RS)"}, {"figure_path": "51HQpkQy3t/figures/figures_4_2.jpg", "caption": "Figure 3: Window Attention with Residual Sharing. (a) Left: Example of the attention map showing the window pattern. Right: The MSE between the window attention outputs in the previous and current step (yellow line) versus the MSE between the output residuals of window and full attention in the previous and current step (blue line). The output residual exhibits minimal changes over the steps. (b) Illustration of Window Attention with Residual Sharing.", "description": "This figure illustrates the core concept of Window Attention with Residual Sharing (WA-RS).  The left panel (a) shows that attention values concentrate in a diagonal window. It also demonstrates that the mean squared error (MSE) between consecutive steps' attention output is significantly higher than the MSE between consecutive steps' residuals (the difference between full attention and window attention). The right panel (b) details the WA-RS mechanism. It shows how a residual from the previous step is cached and reused in subsequent steps, saving computation and improving the accuracy of window attention.", "section": "3.2 Window Attention with Residual Sharing (WA-RS)"}, {"figure_path": "51HQpkQy3t/figures/figures_5_1.jpg", "caption": "Figure 4: Similarity of Attention Outputs Across Step and CFG Dimensions in DiT. (a) Similarity of attention outputs across step dimension in different layers. (b) Similarity between conditional and unconditional attention outputs in various layers at different steps", "description": "This figure visualizes the similarity of attention outputs in a diffusion transformer model across different steps (time) and between conditional and unconditional inference.  Panel (a) shows heatmaps representing the cosine similarity between attention outputs of the same attention head at consecutive time steps for three different layers (5, 15, and 25). Warmer colors (purple) indicate higher similarity. Panel (b) shows a heatmap illustrating the cosine similarity between conditional and unconditional attention outputs across different time steps for various layers. This demonstrates the redundancy that can be exploited by the proposed DiTFastAttn method.", "section": "3 Method"}, {"figure_path": "51HQpkQy3t/figures/figures_6_1.jpg", "caption": "Figure 5: Compression plan for DiT-XL-512, PixArt-Sigma-XL-1024 and PixArt-Sigma-XL-2K at D6 with the number of DPM-Solver steps set to 50.", "description": "This figure shows the compression plan obtained by applying the proposed greedy search method to three different models (DiT-XL-512, PixArt-Sigma-XL-1024, and PixArt-Sigma-XL-2048) at a threshold of 0.15 (D6).  The heatmaps illustrate which compression strategy (full attention, WA-RS, ASC, WA-RS+ASC, or AST) is applied to each layer at each timestep during the diffusion process. The color intensity in each cell represents the selected compression strategy.  It highlights the variability of the optimal compression strategies depending on the model architecture and resolution.", "section": "4 Experiments"}, {"figure_path": "51HQpkQy3t/figures/figures_7_1.jpg", "caption": "Figure 6: Image generation samples at various image resolutions under various compression ratios.", "description": "This figure displays image generation results from three different models (DiT-XL-2-512, PixArt-Sigma-1024, and PixArt-Sigma-2048) at various resolutions and compression ratios (D1 to D6). Each row represents a different model and resolution, with the leftmost column showing the original image ('Raw').  Subsequent columns illustrate the generated images using DiTFastAttn with increasing compression levels (D1 to D6), which trade off between speed and image quality. The figure helps visualize the effect of DiTFastAttn's different compression strategies on the generated image quality, showing how the models maintain image quality to different levels depending on the degree of compression used.", "section": "4.2 Results on Image Generation"}, {"figure_path": "51HQpkQy3t/figures/figures_8_1.jpg", "caption": "Figure 6: Image generation samples at various image resolutions under various compression ratios.", "description": "This figure shows image generation samples from the DiTFastAttn model at various resolutions and compression ratios.  It visually demonstrates the impact of the proposed compression techniques on image quality at different levels of compression.  The samples allow for a qualitative assessment of how well the method maintains image quality while reducing computational cost.", "section": "4.2 Results on Image Generation"}, {"figure_path": "51HQpkQy3t/figures/figures_9_1.jpg", "caption": "Figure 8: Latency of different resolutions of image generation under different compression ratios. DiT runs with a batch size of 8, while PixArt-Sigma models with a batch size of 1. The blue line delineates the latency for end-to-end image generation, whereas the orange line represents the latency of multi-head attention module.", "description": "This figure shows the latency (time taken) for image generation and the attention mechanism separately, at different resolutions (512x512, 1024x1024, and 2048x2048) and various compression ratios.  The x-axis represents the computational cost (TFLOPS) of the attention mechanism.  The y-axis is the latency in seconds.  Different batch sizes were used for DiT-XL (batch size 8) and PixArt-Sigma-XL (batch size 1). The blue line indicates the overall generation latency, while the orange line indicates the latency specifically for the attention part of the generation process. The figure demonstrates the impact of the proposed compression methods on the runtime of diffusion models.", "section": "4.4 #FLOPs Reduction and Speedup"}, {"figure_path": "51HQpkQy3t/figures/figures_9_2.jpg", "caption": "Figure 9: Ablation study on DiT-XL-2-512. Examination of methodological impact (Left), timesteps variability (Middle), and residual sharing (Right). 'WA' denotes Window Attention without the Residual Share (RS).", "description": "This figure presents an ablation study conducted on the DiT-XL-2-512 model to analyze the impact of different components of the proposed DiTFastAttn method.  The left panel shows a comparison of DiTFastAttn against its individual components (ASC, WA-RS, and AST) in terms of Inception Score versus the attention FLOPs fraction. The middle panel demonstrates the impact of varying the number of denoising steps (20, 30, 40, and 50) on the Inception Score at different attention FLOPs fractions. The right panel specifically examines the contribution of the residual sharing technique within the Window Attention (WA-RS) method by contrasting its performance with a version without residual sharing ('WA') across different attention FLOP levels.", "section": "4.5 Ablation Study"}, {"figure_path": "51HQpkQy3t/figures/figures_15_1.jpg", "caption": "Figure 1: Left: The efficiency benefits of applying DiTFastAttn on PixArt-Sigma (Chen et al., 2024) when generating images of different resolutions. The Y-axis shows the #FLOPs fraction normalized by the #FLOPs of the original model. Right: The qualitative results of applying DiTFastAttn on 1024\u00d71024 PixArt-Sigma.", "description": "This figure demonstrates the efficiency gains of DiTFastAttn when generating images at different resolutions.  The left panel shows a bar graph comparing the fraction of FLOPs (floating-point operations) used by DiTFastAttn versus the original PixArt-Sigma model at 512x512, 1024x1024, and 2048x2048 resolutions.  The right panel presents qualitative image samples to illustrate the model's output quality after applying DiTFastAttn on 1024x1024 images.", "section": "1 Introduction"}, {"figure_path": "51HQpkQy3t/figures/figures_16_1.jpg", "caption": "Figure 11: Compression plan for DiT-XL-2-512\u00d7512 at different thresholds with DPM solver step set to 50", "description": "This figure displays the compression plans generated by the greedy search method for DiT-XL-2-512x512 model at six different thresholds (0.025, 0.05, 0.075, 0.1, 0.125, and 0.15). Each heatmap represents a compression plan for a specific threshold, visualizing the chosen compression strategy (Full Attn, WA-RS, ASC, WA-RS+ASC, AST) for each layer at each timestep.  The color intensity likely represents the degree of compression applied. The figure shows how the choice of compression strategy varies across layers and timesteps depending on the chosen threshold, highlighting the adaptive nature of the proposed method.", "section": "3.5 Method for Deciding the Compression Plan"}, {"figure_path": "51HQpkQy3t/figures/figures_16_2.jpg", "caption": "Figure 5: Compression plan for DiT-XL-512, PixArt-Sigma-XL-1024 and PixArt-Sigma-XL-2K at D6 with the number of DPM-Solver steps set to 50.", "description": "This figure visualizes the compression strategies selected by the DiTFastAttn algorithm for three different diffusion transformer models (DiT-XL-512, PixArt-Sigma-XL-1024, and PixArt-Sigma-XL-2048) at a specific threshold (D6). Each heatmap represents the compression plan for a single model, showing which compression techniques (Full Attn, WA-RS, ASC, WA-RS+ASC, AST) were applied to each layer at each timestep during the denoising process. The color intensity in each cell indicates the strength of the applied compression technique.  The results showcase that the optimal compression strategy varies across different models and image resolutions, highlighting the adaptive nature of the proposed method.", "section": "4 Experiments"}, {"figure_path": "51HQpkQy3t/figures/figures_17_1.jpg", "caption": "Figure 5: Compression plan for DiT-XL-512, PixArt-Sigma-XL-1024 and PixArt-Sigma-XL-2K at D6 with the number of DPM-Solver steps set to 50.", "description": "This figure visualizes the compression strategies applied to different layers and timesteps of three different diffusion transformer models (DiT-XL-512, PixArt-Sigma-XL-1024, and PixArt-Sigma-XL-2048) at threshold level D6 (\u03b4=0.15). Each model uses a combination of techniques (WA-RS, AST, and ASC) to reduce computational costs, with the specific strategy applied depending on the layer and timestep. The color intensity represents the type of compression technique used, ranging from full attention to different combinations of WA-RS, AST, and ASC.", "section": "4 Experiments"}, {"figure_path": "51HQpkQy3t/figures/figures_17_2.jpg", "caption": "Figure 14: Images generated by PixArt-Sigma-XL-1024 at different thresholds with/without negative prompt", "description": "This figure shows the impact of negative prompts on image generation quality at different compression levels.  The top half displays images generated without a negative prompt, while the bottom half shows images generated with the negative prompt \"Low quality\".  The three columns (D2, D4, D6) represent different compression ratios achieved by the DiTFastAttn algorithm, demonstrating how the algorithm's compression affects image quality, especially when negative prompts are used.", "section": "4.2 Results on Image Generation"}]