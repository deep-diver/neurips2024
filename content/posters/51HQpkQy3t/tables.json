[{"figure_path": "51HQpkQy3t/tables/tables_5_1.jpg", "caption": "Table 1: Image generation performance of DiTFastAttn at various image resolutions under various compression ratios. The FID, IS, and CLIP results are marked in different makers. The 'Attn FLOPs' represents the fraction of computation in the multi-head attention module compared to the raw model.", "description": "This table presents the results of image generation experiments using DiTFastAttn on different models (DiT-XL-2 512x512, PixArt-Sigma-XL 1024x1024, PixArt-Sigma-XL 2048x2048) at varying compression ratios.  It shows the FID (Fr\u00e9chet Inception Distance), IS (Inception Score), and CLIP scores, which are common metrics for evaluating image quality.  'Attn FLOPs' indicates the percentage of FLOPs (floating-point operations) in the attention mechanism relative to the uncompressed model, demonstrating the computational savings achieved by the compression technique. The table allows comparison of the trade-off between compression level and generated image quality.", "section": "4.2 Results on Image Generation"}, {"figure_path": "51HQpkQy3t/tables/tables_7_1.jpg", "caption": "Table 1: Image generation performance of DiTFastAttn at various image resolutions under various compression ratios. The FID, IS, and CLIP results are marked in different makers. The 'Attn FLOPs' represents the fraction of computation in the multi-head attention module compared to the raw model.", "description": "This table presents the results of image generation experiments using DiTFastAttn on three different models: DiT-XL-2 512x512, PixArt-Sigma-XL 1024x1024, and PixArt-Sigma-XL 2048x2048.  For each model and resolution, it shows the performance metrics (IS, FID, CLIP) for the original model (Raw) and for six different compression levels (D1-D6). The \"Attn FLOPs\" column indicates the percentage of attention FLOPs remaining after compression compared to the original model.", "section": "4.2 Results on Image Generation"}, {"figure_path": "51HQpkQy3t/tables/tables_8_1.jpg", "caption": "Table 2: FLOPs fraction and latency fraction of DitFastAttn in Diffusion Transformers comparing with original attention. The latency is evaluated on the Nvidia A100 GPU.", "description": "This table shows the results of applying DiTFastAttn to three different diffusion transformer models with different sequence lengths.  It presents the fraction of FLOPs (floating-point operations) and latency reduction achieved by the DiTFastAttn method for each model.  The FLOPs fraction represents the percentage reduction in attention computation compared to the original model, while the latency fraction indicates the percentage reduction in inference time. The results are broken down by the different compression techniques used in DiTFastAttn (ASC, WA-RS, WA-RS+ASC, and AST). The latency is measured using an Nvidia A100 GPU.", "section": "4.4 #FLOPs Reduction and Speedup"}, {"figure_path": "51HQpkQy3t/tables/tables_13_1.jpg", "caption": "Table 1: Image generation performance of DiTFastAttn at various image resolutions under various compression ratios. The FID, IS, and CLIP results are marked in different makers. The 'Attn FLOPs' represents the fraction of computation in the multi-head attention module compared to the raw model.", "description": "This table presents the results of image generation experiments using DiTFastAttn on three different models (DiT-XL-2 512x512, PixArt-Sigma-XL 1024x1024, and PixArt-Sigma-XL 2048x2048) at various compression ratios (D1-D6).  For each model and compression ratio, the table shows the Inception Score (IS), Fr\u00e9chet Inception Distance (FID), CLIP score, and the fraction of attention FLOPs relative to the original model.  The FID and IS scores are metrics for image quality, while the CLIP score measures the similarity between generated images and text prompts, and the attention FLOPs indicate the computational savings achieved by DiTFastAttn.", "section": "4.2 Results on Image Generation"}, {"figure_path": "51HQpkQy3t/tables/tables_13_2.jpg", "caption": "Table 1: Image generation performance of DiTFastAttn at various image resolutions under various compression ratios. The FID, IS, and CLIP results are marked in different makers. The 'Attn FLOPs' represents the fraction of computation in the multi-head attention module compared to the raw model.", "description": "This table presents the results of image generation experiments using DiTFastAttn on three different models at various resolutions.  It shows the FID, IS, and CLIP scores, as well as the fraction of attention FLOPs (floating-point operations) compared to the original, uncompressed models.  Different compression ratios (D1-D6) are evaluated, showing the impact of the DiTFastAttn method on both performance and computational cost. The higher the resolution, the more significant the reduction in FLOPs and the potential for speedup.", "section": "4.2 Results on Image Generation"}, {"figure_path": "51HQpkQy3t/tables/tables_13_3.jpg", "caption": "Table 1: Image generation performance of DiTFastAttn at various image resolutions under various compression ratios. The FID, IS, and CLIP results are marked in different makers. The 'Attn FLOPs' represents the fraction of computation in the multi-head attention module compared to the raw model.", "description": "This table presents the results of image generation experiments using DiTFastAttn on three different models at various resolutions.  It compares the raw model's performance (no compression) to DiTFastAttn's performance at different compression levels (D1-D6).  The metrics used to evaluate performance are Inception Score (IS), Fr\u00e9chet Inception Distance (FID), and CLIP score.  The \"Attn FLOPs\" column indicates the percentage of FLOPs (floating-point operations) related to attention in each configuration, relative to the raw model.", "section": "4.2 Results on Image Generation"}, {"figure_path": "51HQpkQy3t/tables/tables_14_1.jpg", "caption": "Table 1: Image generation performance of DiTFastAttn at various image resolutions under various compression ratios. The FID, IS, and CLIP results are marked in different makers. The 'Attn FLOPs' represents the fraction of computation in the multi-head attention module compared to the raw model.", "description": "This table presents a comparison of the image generation performance of the DiTFastAttn model at different resolutions and compression ratios.  It shows the FID, IS, and CLIP scores, which are metrics used to evaluate the quality of generated images. The table also indicates the fraction of FLOPs (floating-point operations) used for attention computation relative to the original model, reflecting the computational savings achieved through compression.", "section": "4.2 Results on Image Generation"}]