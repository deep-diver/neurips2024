[{"figure_path": "PukaVAwYBo/figures/figures_8_1.jpg", "caption": "Figure 1: Convergence analysis: We plot the distance to the ground truth ||V \u2013 P||\u00b5, ||A \u2013 Q||\u00b5 in different settings. After stage 1 ends at \u03c4 = 400 (when \u03b1A, \u03b1\u03bd \u2248 0.1), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the l\u2081 regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when \u03b72 is large) or a slow convergence rate (small \u03b72).", "description": "This figure compares the convergence speed of the proposed l1-regularized proximal gradient descent method and vanilla SGD for training a one-layer linear transformer on the Sparse Contextual Bigram (SCB) task.  The plots show the distances to the ground truth for both the transition matrix (P) and the attention matrix (Q). It demonstrates that the proposed method significantly outperforms SGD, achieving faster convergence and a much closer approximation of the ground truth. This difference in performance is attributed to the proposed method's ability to manage the large variance in the gradients that occur with SGD, which hinders its convergence in this specific setting.", "section": "5 Results for training from scratch"}, {"figure_path": "PukaVAwYBo/figures/figures_9_1.jpg", "caption": "Figure 2: Similarity between the softmax and linear attention. We train two transformers with (1) (Left) softmax attention and (2) (Middle) linear attention layer on the SCB tasks with the same ground-truth (T = 50, N = 10, Q = 2). The attention pattern and the value matrix (learned transition matrix) are very similar (left two plots) and they converge to approximately the same loss (right plot).", "description": "This figure compares the attention patterns and value matrices learned by softmax and linear transformers on the Sparse Contextual Bigram (SCB) task.  The left and middle panels show heatmaps of the attention patterns learned by each model type.  The right panel shows the training loss curves for each model.  The results indicate that the attention patterns and value matrices are similar between the softmax and linear transformers, and both models converge to similar loss values.", "section": "Experiments and relationship with softmax transformers"}, {"figure_path": "PukaVAwYBo/figures/figures_46_1.jpg", "caption": "Figure 1: Convergence analysis: We plot the distance to the ground truth ||V \u2013 P||\u03bc, ||A \u2013 Q||\u03bc in different settings. After stage 1 ends at \u03c4 = 400 (when \u03b1A, \u03b1\u03bd \u2248 0.1), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the l\u2081 regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when \u03b72 is large) or a slow convergence rate (small \u03b72).", "description": "This figure compares the convergence speed of the proposed l1-regularized proximal gradient descent method and the vanilla SGD method for training a one-layer linear transformer on the Sparse Contextual Bigram (SCB) task.  The plot shows that the proximal gradient descent method converges much faster and more accurately to the ground truth than the SGD method, achieving near-perfect recovery. The SGD method's performance is hampered by high variance with larger learning rates, while smaller learning rates result in significantly slower convergence.", "section": "5.1 Convergence"}, {"figure_path": "PukaVAwYBo/figures/figures_46_2.jpg", "caption": "Figure 1: Convergence analysis: We plot the distance to the ground truth ||V \u2013 P||\u03bc, ||A \u2013 Q||\u03bc in different settings. After stage 1 ends at \u03c4 = 400 (when \u03b1A,\u03b1\u03bd \u2248 0.1), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the l\u2081 regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when \u03b72 is large) or a slow convergence rate (small \u03b72).", "description": "This figure compares the convergence speed of the proposed l1-regularized proximal gradient descent method and vanilla SGD in learning the Sparse Contextual Bigram (SCB) model. The results show that the proposed method significantly outperforms SGD, achieving faster convergence to the ground truth, while SGD struggles with either high variance or slow convergence rate.", "section": "5 Results for training from scratch"}, {"figure_path": "PukaVAwYBo/figures/figures_47_1.jpg", "caption": "Figure 1: Convergence analysis: We plot the distance to the ground truth ||V \u2013 P||\u03bc, ||A \u2013 Q||\u03bc in different settings. After stage 1 ends at \u03c4 = 400 (when \u03b1A,\u03b1\u03bd \u2248 0.1), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the l\u2081 regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when \u03b72 is large) or a slow convergence rate (small \u03b72).", "description": "This figure shows the convergence comparison of the proposed algorithm and the vanilla SGD for learning the SCB task. The proposed algorithm significantly outperforms the SGD in terms of convergence speed and accuracy. This demonstrates the effectiveness of using preconditioning and l\u2081-regularization in the training process.", "section": "5 Results for training from scratch"}, {"figure_path": "PukaVAwYBo/figures/figures_47_2.jpg", "caption": "Figure 1: Convergence analysis: We plot the distance to the ground truth ||V \u2013 P||\u03bc, ||A \u2013 Q||\u03bc in different settings. After stage 1 ends at \u03c4 = 400 (when aa, ay \u2248 0.1), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the l\u2081 regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when n2 is large) or a slow convergence rate (small n\u2082).", "description": "This figure compares the convergence speed of the proposed proximal gradient descent method and the vanilla SGD method for training a one-layer linear transformer on the Sparse Contextual Bigram (SCB) task.  The plot shows the distance to the ground truth for both the transition matrix (P) and the attention matrix (Q) over training iterations.  The results demonstrate that the proximal gradient descent method converges significantly faster and more accurately to the ground truth than SGD, which struggles with either high variance or slow convergence.", "section": "5 Results for training from scratch"}]