[{"figure_path": "zn6s6VQYb0/figures/figures_2_1.jpg", "caption": "Figure 1: Two examples of the topological symmetric graphs. The left graph is axis-symmetric; the right graph is centrosymmetric.", "description": "This figure shows two examples of topologically symmetric graphs.  A topologically symmetric graph is defined as having its structure and node features symmetric either along a specific axis or around a central node. The left graph demonstrates axis symmetry, where node features and connections are symmetric about an axis. The right graph illustrates centrosymmetry, characterized by a central pivot node and symmetric node features and connections around it.", "section": "2.2 Deficiencies of Self-Correlation on Graph Structure Representation"}, {"figure_path": "zn6s6VQYb0/figures/figures_4_1.jpg", "caption": "Figure 2: Training comparison between self-correlation and cross-correlation on PROTEINS subset (64 graphs). In (a) and (b), we demonstrate the trajectory of the first two node embeddings in the first graph during training iteration, where the star mark is the end-point of training. We apply PCA for dimension compression and the Savitzky-Golay filter to help trace visualization. We also set Zi = Pi \u2260 qi at the beginning of optimization to ensure that the traces of zi in (a) and pi in (b) start from the same point. (c) provides the BCE loss trace of this graph during training, showing that cross-correlation can lead the reconstruction to a better solution. (d) demonstrates the distribution of diagonal elements during training, i.e., z\u0142 zi for self-correlation and pf qi for cross-correlation. The results of other graphs are provided in Appendix G.2.", "description": "This figure compares self-correlation and cross-correlation methods for graph autoencoder training on a subset of the PROTEINS dataset.  It visualizes the trajectory of node embeddings during training for both methods, showing that cross-correlation leads to a smoother and more efficient convergence.  The figure also shows the loss curves and the distribution of diagonal elements (representing self-loops) during training, further supporting the advantages of cross-correlation.", "section": "2.3.2 Cross-Correlation Provides Smoother Optimization Trace"}, {"figure_path": "zn6s6VQYb0/figures/figures_5_1.jpg", "caption": "Figure 3: GraphCroc architecture. The encoder is generally demonstrated as a L + 1-layer GNN. The decoder has two paths to generate the node embedding for cross-correlation; each decoder is a mirrored structure of the encoder. Each decoder layer accepts the node feature and graph structure information from the corresponding encoder layer. Notably, the GCN module shown on the right incorporates skip connections and normalization to improve performance.", "description": "This figure illustrates the architecture of the GraphCroc model, which is a new Graph Autoencoder (GAE). It consists of an encoder and two mirrored decoders that work in parallel. The encoder is a Graph Neural Network (GNN) that takes as input the graph's node features and adjacency matrix. It then processes the information through several layers of GCN (Graph Convolutional Network) and pooling operations. The output of the encoder is a latent representation of the graph. Each decoder takes the latent representation as input, along with the node features and adjacency matrix, and reconstructs the adjacency matrix using cross-correlation between node embeddings generated by two parallel GCN paths. The decoders have a U-net like structure, with skip connections between encoder and decoder layers to improve performance. In addition, layer normalization and skip connections are also used to enhance the model's performance. The figure also shows the GCN module, highlighting the key components such as layer normalization and skip connections.", "section": "3 GraphCroc: Cross-Correlation-Based Graph Autoencoder"}, {"figure_path": "zn6s6VQYb0/figures/figures_6_1.jpg", "caption": "Figure 4: WL-test results on different GAE methods, in the IMDB-B task.", "description": "This figure displays the results of the Weisfeiler-Lehman (WL) test, a graph isomorphism test, performed on various Graph Autoencoder (GAE) methods for the IMDB-B dataset.  The WL-test assesses how well each GAE method reconstructs the graph structure.  The higher the bar, the better the reconstruction performance, indicating that GraphCroc (the orange bar) performs best at reconstructing graph structures compared to other methods.", "section": "4 Evaluation"}, {"figure_path": "zn6s6VQYb0/figures/figures_7_1.jpg", "caption": "Figure 5: The reconstruction visualization. Other reconstructions are provided in Appendix G.5.", "description": "This figure visualizes the reconstruction of several graphs from different datasets (PROTEINS, IMDB-B, and COLLAB) by various graph autoencoder (GAE) models.  The \"Ground Truth\" column shows the original graphs. The remaining columns show the reconstruction of the same graphs by GraphCroc and several other GAE methods (GAE, VGAE, EGNN, DIGAE).  The figure demonstrates the superior performance of GraphCroc in accurately reconstructing the graph structures compared to other methods.", "section": "4.2 GraphCroc on Structural Reconstruction"}, {"figure_path": "zn6s6VQYb0/figures/figures_7_2.jpg", "caption": "Figure 2: Training comparison between self-correlation and cross-correlation on PROTEINS subset (64 graphs). In (a) and (b), we demonstrate the trajectory of the first two node embeddings in the first graph during training iteration, where the star mark is the end-point of training. We apply PCA for dimension compression and the Savitzky-Golay filter to help trace visualization. We also set Zi = Pi \u2260 qi at the beginning of optimization to ensure that the traces of zi in (a) and pi in (b) start from the same point. (c) provides the BCE loss trace of this graph during training, showing that cross-correlation can lead the reconstruction to a better solution. (d) demonstrates the distribution of diagonal elements during training, i.e., z\u0142 zi for self-correlation and pf qi for cross-correlation. The results of other graphs are provided in Appendix G.2.", "description": "This figure compares the training process of self-correlation and cross-correlation methods on a subset of the PROTEINS dataset.  It shows the trajectories of node embeddings during training (using PCA for visualization), the BCE loss curves, and the distributions of diagonal elements (representing self- and cross-correlations) over iterations. The results highlight that cross-correlation leads to a smoother optimization process and better reconstruction results compared to self-correlation.", "section": "Validation on PROTEINS"}, {"figure_path": "zn6s6VQYb0/figures/figures_15_1.jpg", "caption": "Figure 5: The reconstruction visualization. Other reconstructions are provided in Appendix G.5.", "description": "This figure visualizes the reconstruction results of different graph autoencoder (GAE) models on three graph datasets: PROTEINS, IMDB-B, and COLLAB.  The \"Ground Truth\" column shows the original graph structures. Subsequent columns display the reconstructed graphs generated by GraphCroc (the proposed model), GAE, VGAE, EGNN, and DiGAE. The figure highlights the differences in reconstruction accuracy between the various GAE models, demonstrating GraphCroc's superior performance in capturing the original graph structures.", "section": "4.2 GraphCroc on Structural Reconstruction"}, {"figure_path": "zn6s6VQYb0/figures/figures_16_1.jpg", "caption": "Figure 8: The histogram of cosine similarity between node embeddings P and Q under cross-correlation, applying GraphCroc on graph tasks.", "description": "This figure shows the distribution of cosine similarity between the two node embeddings (P and Q) generated by the GraphCroc model's decoder for various graph datasets (COLLAB, IMDB-B, PPI, PROTEINS, QM9).  The x-axis represents the cosine similarity, ranging from -0.4 to 0.8, and the y-axis represents the frequency or count of cosine similarity values.  Each dataset's distribution is shown with a different color.  The low cosine similarity values across datasets indicate that the two embeddings are effectively kept independent, as intended by the GraphCroc's design which uses cross-correlation instead of self-correlation.", "section": "2.3.2 Cross-Correlation Provides Smoother Optimization Trace"}, {"figure_path": "zn6s6VQYb0/figures/figures_17_1.jpg", "caption": "Figure 3: GraphCroc architecture. The encoder is generally demonstrated as a L + 1-layer GNN. The decoder has two paths to generate the node embedding for cross-correlation; each decoder is a mirrored structure of the encoder. Each decoder layer accepts the node feature and graph structure information from the corresponding encoder layer. Notably, the GCN module shown on the right incorporates skip connections and normalization to improve performance.", "description": "This figure illustrates the architecture of GraphCroc, a graph autoencoder model.  It uses a U-Net-like structure with an encoder and a two-branched decoder. The encoder is a GNN (Graph Neural Network) that processes the graph structure and node features, outputting a latent representation. The decoder then uses this representation to reconstruct the adjacency matrix of the graph.  It's noteworthy that the decoder uses cross-correlation (instead of self-correlation) and is designed as a mirrored version of the encoder. This means that the decoder reverses the steps of the encoder, gradually reconstructing the graph structure from the compressed representation.  Skip connections and layer normalization are used to improve performance.", "section": "3 GraphCroc: Cross-Correlation-Based Graph Autoencoder"}, {"figure_path": "zn6s6VQYb0/figures/figures_18_1.jpg", "caption": "Figure 2: Training comparison between self-correlation and cross-correlation on PROTEINS subset (64 graphs). In (a) and (b), we demonstrate the trajectory of the first two node embeddings in the first graph during training iteration, where the star mark is the end-point of training. We apply PCA for dimension compression and the Savitzky-Golay filter to help trace visualization. We also set Zi = Pi \u2260 qi at the beginning of optimization to ensure that the traces of zi in (a) and pi in (b) start from the same point. (c) provides the BCE loss trace of this graph during training, showing that cross-correlation can lead the reconstruction to a better solution. (d) demonstrates the distribution of diagonal elements during training, i.e., z\u0142 zi for self-correlation and pf qi for cross-correlation. The results of other graphs are provided in Appendix G.2.", "description": "This figure compares the training process of self-correlation and cross-correlation methods using the PROTEINS dataset.  Subfigures (a) and (b) show the trajectories of the first two node embeddings in a sample graph during training, visualizing the optimization process using PCA and smoothing.  (c) plots the binary cross-entropy (BCE) loss over training iterations, demonstrating faster convergence with cross-correlation.  (d) displays the distribution of diagonal elements (self-correlation and cross-correlation) over iterations, further highlighting the differences in optimization behavior.", "section": "2.3.2 Cross-Correlation Provides Smoother Optimization Trace"}, {"figure_path": "zn6s6VQYb0/figures/figures_19_1.jpg", "caption": "Figure 2: Training comparison between self-correlation and cross-correlation on PROTEINS subset (64 graphs). In (a) and (b), we demonstrate the trajectory of the first two node embeddings in the first graph during training iteration, where the star mark is the end-point of training. We apply PCA for dimension compression and the Savitzky-Golay filter to help trace visualization. We also set Zi = Pi \u2260 qi at the beginning of optimization to ensure that the traces of zi in (a) and pi in (b) start from the same point. (c) provides the BCE loss trace of this graph during training, showing that cross-correlation can lead the reconstruction to a better solution. (d) demonstrates the distribution of diagonal elements during training, i.e., z\u0142 zi for self-correlation and pf qi for cross-correlation. The results of other graphs are provided in Appendix G.2.", "description": "This figure compares the training process of self-correlation and cross-correlation methods on a subset of the PROTEINS dataset.  It visualizes the trajectory of node embeddings during training using PCA for dimensionality reduction, showing how cross-correlation leads to smoother and more efficient convergence.  The figure also displays the BCE loss and the distribution of diagonal elements (self-correlation vs. cross-correlation) over training iterations.", "section": "2.3.2 Cross-Correlation Provides Smoother Optimization Trace"}, {"figure_path": "zn6s6VQYb0/figures/figures_19_2.jpg", "caption": "Figure 2: Training comparison between self-correlation and cross-correlation on PROTEINS subset (64 graphs). In (a) and (b), we demonstrate the trajectory of the first two node embeddings in the first graph during training iteration, where the star mark is the end-point of training. We apply PCA for dimension compression and the Savitzky-Golay filter to help trace visualization. We also set Zi = Pi \u2260 qi at the beginning of optimization to ensure that the traces of zi in (a) and pi in (b) start from the same point. (c) provides the BCE loss trace of this graph during training, showing that cross-correlation can lead the reconstruction to a better solution. (d) demonstrates the distribution of diagonal elements during training, i.e., z\u0142 zi for self-correlation and pf qi for cross-correlation. The results of other graphs are provided in Appendix G.2.", "description": "This figure compares the training process of self-correlation and cross-correlation methods on a subset of the PROTEINS dataset.  It shows the trajectory of node embeddings during training, the BCE loss over iterations, and the distribution of diagonal elements (self-correlation vs. cross-correlation). The results suggest that cross-correlation leads to a smoother optimization process and better reconstruction.", "section": "2.3.2 Cross-Correlation Provides Smoother Optimization Trace"}, {"figure_path": "zn6s6VQYb0/figures/figures_20_1.jpg", "caption": "Figure 5: The reconstruction visualization. Other reconstructions are provided in Appendix G.5.", "description": "This figure visualizes the reconstruction results of different graph autoencoder models on three distinct graph datasets: PROTEINS, IMDB-B, and COLLAB.  Each row represents a different graph from the dataset. The \"Ground Truth\" column shows the original graph structure.  The remaining columns display reconstructions generated by various models: GraphCroc (the proposed model), GAE, VGAE, EGNN, and DiGAE.  The visualization allows for a qualitative comparison of the different models' abilities to accurately reconstruct the graph structure, revealing the strengths and weaknesses of each approach in preserving the original connectivity patterns.", "section": "4.2 GraphCroc on Structural Reconstruction"}, {"figure_path": "zn6s6VQYb0/figures/figures_20_2.jpg", "caption": "Figure 7: The graph reconstruction visualization of different models on graphs with symmetric structure and non-self-looped nodes.", "description": "This figure visualizes the graph reconstruction performance of various graph autoencoder (GAE) models on graphs with symmetric structures and no self-loops.  The \"Ground Truth\" column shows the original graphs.  The \"GraphCroc\" column demonstrates the reconstruction using the proposed GraphCroc model.  The remaining columns illustrate the reconstruction results of other GAE models, including GAE, VGAE, EGNN, and DiGAE. The visualization allows for a direct comparison of how accurately each model reconstructs the specific structural characteristics of these types of graphs.  It highlights the superior performance of GraphCroc in preserving the symmetric structure and the absence of self-loops, while other models struggle with this task, sometimes misrepresenting the graph significantly.", "section": "G Specific Graph Structure Representation"}, {"figure_path": "zn6s6VQYb0/figures/figures_21_1.jpg", "caption": "Figure 5: The reconstruction visualization. Other reconstructions are provided in Appendix G.5.", "description": "This figure visualizes the reconstruction results of different graph autoencoder models on three different graph datasets (PROTEINS, IMDB-B, and COLLAB).  For each dataset, several example graphs are shown. The \"Ground Truth\" column displays the original graph structure. The remaining columns showcase the reconstructed graphs produced by GraphCroc and other GAE methods (GAE, VGAE, EGNN, DIGAE).  The visualization allows for a qualitative comparison of the different models' ability to accurately reconstruct graph structures, revealing that GraphCroc generally performs better than other models at recovering the original graph structure.", "section": "4.2 GraphCroc on Structural Reconstruction"}, {"figure_path": "zn6s6VQYb0/figures/figures_21_2.jpg", "caption": "Figure 5: The reconstruction visualization. Other reconstructions are provided in Appendix G.5.", "description": "This figure visualizes the reconstruction results of different graph autoencoder models on three different graph datasets: PROTEINS, IMDB-B, and COLLAB.  Each row represents a different dataset, and each column represents a different model: Ground Truth, GraphCroc, GAE, VGAE, EGNN, and DiGAE.  The visualizations show the original graph structure (Ground Truth) and how each model reconstructs the graph from the node embeddings. The goal is to compare the accuracy and visual fidelity of the reconstructions across various models, especially highlighting the superior performance of GraphCroc.", "section": "4.2 GraphCroc on Structural Reconstruction"}]