[{"type": "text", "text": "An Autoencoder-Like Nonnegative Matrix Co-Factorization for Improved Student Cognitive Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shenbao Yu 1 Yinghui Pan 2\u2217 Yifeng Zeng 3 Prashant Doshi 4 Guoquan Liu 5 Kim-Leng Poh 6 Mingwei Lin 1 ", "page_idx": 0}, {"type": "text", "text": "1 College of Computer and Cyber Security, Fujian Normal University, China 2 National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, China 3 Department of Computer and Information Sciences, Northumbria University, UK Intelligent Thought and Action Lab, School of Computing, University of Georgia, USA 5 Financial Technology Research Institute, Fudan University, China 6 College of Design and Engineering, National University of Singapore, Singapore {yushenbao,lmwfjnu}@fjnu.edu.cn, panyinghui@szu.edu.cn yifeng.zeng@northumbria.ac.uk, pdoshi@uga.edu liugq@fudan.edu.cn, pohkimleng@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Student cognitive modeling (SCM) is a fundamental task in intelligent education, with applications ranging from personalized learning to educational resource allocation. By exploiting students\u2019 response logs, SCM aims to predict their exercise performance as well as estimate knowledge proficiency in a subject. Data mining approaches such as matrix factorization can obtain high accuracy in predicting student performance on exercises, but the knowledge proficiency is unknown or poorly estimated. The situation is further exacerbated if only sparse interactions exist between exercises and students (or knowledge concepts). To solve this dilemma, we root monotonicity (a fundamental psychometric theory on educational assessments) in a co-factorization framework and present an autoencoder-like nonnegative matrix co-factorization (AE-NMCF), which improves the accuracy of estimating the student\u2019s knowledge proficiency via an encoder-decoder learning pipeline. The resulting estimation problem is nonconvex with nonnegative constraints. We introduce a projected gradient method based on block coordinate descent with Lipschitz constants and guarantee the method\u2019s theoretical convergence. Experiments on several real-world data sets demonstrate the efficacy of our approach in terms of both performance prediction accuracy and knowledge estimation ability, when compared with existing student cognitive models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the explosion of open educational resources, student cognitive modeling is receiving growing attention. As illustrated in Figure 1, given a set of exercises (could be recommended by a learning platform) with the expert-annotated knowledge concepts in a subject domain, a student is required to finish the exercises and leaves the responses. Based on the response log, cognitive modeling aims to $(a)$ estimate the student\u2019s cognitive levels on the knowledge concepts (i.e., cognitive diagnosis) and $(b)$ predict some exercise performance. With a comprehensive understanding of students, cognitive modeling is fruitful in applications such as computerized adaptive testing [1] and exercise recommendations [2]. To profile students\u2019 cognitive status, much progress has been made in the educational psychology area, where one popular avenue is to use cognitive diagnosis models (CDMs) [3]. While most CDMs provide detailed insights into students\u2019 cognitive states, the subjective handcraft features (e.g., the slip and guess of an exercise) may only partially capture the nuances of actual cognitive functioning, triggering cascading errors in predicting student performance [4]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/f479dc12a1dc4b9356e2091a67fd7c1d2855eec3d085ba40da2959de5d4e4957.jpg", "img_caption": ["Figure 1: A schematic illustration of the student cognitive modeling problem. On the left is a set of exercises with the expert-labeled knowledge concepts. The middle is a student\u2019s binary-value response log with missing values (e.g., $\\mathrm{Ex_{2}}$ is missing) that is input to the modeling, and the top right illustrates the two cognitive tasks, which are the output of the modeling. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In a fresh direction, several studies focus on applying data mining techniques to model students\u2019 learning status, of which the cornerstone is matrix factorization (MF) [5]. By transforming students\u2019 response logs into a scoring matrix, MF-based models directly predict the missing response values via latent factors, thereby reducing cascading errors. In contrast to CDMs, MF-based models enjoy high prediction accuracy and are inexpensive to deploy [6]. On the other hand, the latent factors uncovered from the factorization techniques, which encode students\u2019 implicit learning ability, are unexplainable, i.e., the true knowledge components of the students remain unknown in the latent vectors. Recognizing this problem, a follow-up scalable nonnegative matrix co-factorization (SNMCF) model [4] utilizes a point coverage function to learn students\u2019 proficiency levels via pre-trained latent factors. However, SNMCF solves the two learning tasks separately, i.e., the generation of latent features is aimed at performance prediction without considering the target of improving cognitive diagnosis, thereby compromising the identification of student cognitive levels. As such, the fundamental issue of identifying students\u2019 knowledge proficiency remains an open problem. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we envision a reliable and interpretable data mining-based cognitive model with interlocking learning components. Learning latent factors that help pinpoint students\u2019 responses to exercises can guide the assessment of their knowledge proficiency, and the corresponding latent knowledge features, in turn, enable their success or failure on the exercises. To this end, several challenges exist: How can we specify and assess the students\u2019 knowledge proficiency since the ground truth of the cognitive levels is unknown [1]? How can we frame the two learning tasks as the building blocks of an optimization framework while reducing cascading errors? ", "page_idx": 1}, {"type": "text", "text": "To mitigate these challenges, we leverage the known monotonicity [7] to sidestep the issue of unknown knowledge proficiency. The monotonicity states that a student\u2019s knowledge proficiency has a monotonic relationship with the probability of the right responses to related exercises. Furthermore, by investigating the form of an autoencoder, our key observation reveals that its self-reconstruction principle, which aims to reconstruct input data from the learned low-dimensional representations, is amenable to the requirement of the monotonic constraint. Leveraging this observation, we root the monotonicity in a co-factorization framework via the autoencoder mechanism. Consequently, an autoencoder-like nonnegative matrix co-factorization (AE-NMCF) is presented, which enables an iterative link between students\u2019 knowledge proficiency and exercise performance, thereby enhancing prediction accuracy and diagnostic ability. As the resulting optimization problem is not convex and has nonnegative constraints \u2013 which makes the complexity acute by an inverse link function (often called response functions in the case of general linear models [8]) \u2013 we develop a projected gradient method based on block coordinate descent with Lipschitz constants and guarantee its theoretical convergence. The main contributions are: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce a data mining-based model (AE-NMCF) for improved student cognitive modeling, which provides an end-to-end and data-driven way of specifying and assessing students\u2019 understanding of a set of given knowledge concepts. This new model exploits the monotonicity in educational MF-based approaches for the first time.   \n\u2022 To learn the model, we present a novel projected gradient method based on block coordinate descent with Lipschitz constants, for which theoretical convergence is guaranteed. This method accounts for the non-convexity of the optimization function with nonnegative constraints and the complexity of the inverse link function.   \n\u2022 AE-NMCF provides a good fit to the students\u2019 knowledge proficiency while maintaining student performance prediction that is comparable to other student cognitive models. ", "page_idx": 2}, {"type": "text", "text": "These contributions will potentially improve the automated comprehensive understanding of students knowledge learning and benefit numerous intelligent educational tools. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Student cognitive modeling has generally proceeded along two tracks: cognitive diagnostic models (CDMs) and data mining approaches. CDMs are of two types: continuous CDMs, an example of which is item response theory (IRT) [9]), and discrete CDMs such as the deterministic inputs, noisy \u201cand\u201d gate (DINA) [10]). IRT predicts a student\u2019s exercise performance based on a single learning trait and exercise difficulty levels. In contrast, DINA probes a student\u2019s binary cognitive status in different knowledge concepts, which assumes that a student can answer correctly if she has mastered all the required knowledge concepts. Traditional CDMs engender a plethora of advanced models. For example, Cheng et al. [11] extend IRT using deep learning to enhance the diagnostic process. Noting the importance of the relation among knowledge concepts, Gao et al. [12] proposed a deep diagnosis framework that considers both the importance of and the interactions between knowledge concepts. Furthermore, Yang et al. [13] recently presented a relationship-based CDM to explore implicit knowledge-exercise relations that educators ignore. ", "page_idx": 2}, {"type": "text", "text": "Along the data mining approach, MF has proven to be effective in understanding students\u2019 response processes [12], especially toward student performance prediction. In this study, classic models (e.g., nonnegative MF (NMF) [14]) and their variants such as the regularized NMF [15] were successfully applied. Because the latent trait of MF is not interpretable for knowledge estimation, Yu et al. [4] proposed SNMCF that utilizes a coverage function to model students\u2019 knowledge states, thereby taking an important stride in data mining-based student cognitive modeling. But, the coverage function often gives binary cognitive levels, failing to discern the nuance between knowledge proficiencies. ", "page_idx": 2}, {"type": "text", "text": "Matrix co-factorization (MCF) [16] benefits from jointly exploiting multiple data sources. It is well established for many applications such as convolutive source separation [17], data sparsity [18, 19], and decision support systems [20]. Given a domain task, MCF improves performance by incorporating knowledge in additional matrices (e.g., trust relationship for social recommendation [21]), which share latent factors with the original one. This sharing mechanism facilitates entity-relation learning [22]. It motivates us to develop an in-depth understanding of students, exercises, and knowledge concepts, and facilitates an effective solution for the two learning tasks, which differs from existing MCF-based approaches that aim at performance boost only. ", "page_idx": 2}, {"type": "text", "text": "Recently, the autoencoder architecture is being explored in dimensionality reduction [23], classification [24], and anomaly detection [25, 26]. For example, Wang et al. [23] proposed a deep version of autoencoder to explore manifold data structures. Gong et al. [25] augmented the autoencoder with a memory module to mitigate anomaly reconstruction problems. For student cognitive modeling, given the reconstruction ability of autoencoder, our work is the first attempt to exploit this mechanism in MF-based approaches to estimate student knowledge proficiency. ", "page_idx": 2}, {"type": "text", "text": "3 AE-NMCF Model and Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given M students and $\\mathrm{N}$ exercises, all students\u2019 responses to the exercises are recorded in a binary scoring matrix $\\mathbf{X}\\,\\in\\,[0|1]^{\\mathrm{N\\timesM}}$ , where $\\mathbf{X}_{n m}$ denotes student $\\mathrm{St}_{m}$ \u2019s answer on exercise $\\operatorname{Ex}_{n}$ . In addition, given K knowledge concepts, we have an expert-labeled $\\mathrm{^Q}$ -matrix $\\mathbf{Q}\\in[0|1]^{\\mathrm{N\\timesK}}$ , where $\\mathbf Q_{n k}=1$ if $\\operatorname{Ex}_{n}$ relates to knowledge concept $\\mathrm{Kc}_{k}$ , otherwise $\\mathbf Q_{n k}=0$ . With $\\mathbf{X}$ and $\\mathbf{Q}$ in hand, we aim to $(a)$ learn students\u2019 proficiency in knowledge concepts from the responses, and $(b)$ predict students\u2019 performance on exercises that they have never done. ", "page_idx": 3}, {"type": "text", "text": "3.1 Model Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 2 (from left to right) offers an overview of the approach, which includes an encoder and a decoder. The encoder and decoder specify and diagnose students\u2019 cognitive levels, thereby enabling monotonicity. Specifically, the new framework receives the student scoring matrix $({\\bf X})$ and the Q-matrix $\\mathbf{\\tau}^{\\left(\\mathbf{Q}\\right)}$ . In the encoder process, we introduce the exercise-knowledge association matrix $({\\bf B})$ and then jointly decompose $\\mathbf{X}$ and $\\mathbf{B}$ to obtain three low-dimensional nonnegative matrices: the student proficiency matrix (U), the exercise characteristic matrix $(\\mathbf{E})$ , and the knowledge requirement matrix $(\\mathbf{V})$ . Note that the shared matrix $\\mathbf{E}$ places $\\mathbf{U}$ on the same scale as $\\mathbf{V}$ , which shapes a pathway to specify the students\u2019 knowledge proficiency (A). In the decoder process, we introduce the exercise difficulty vector (M), which is combined with A and B to form cognitive factors. By re-fitting $\\mathbf{X}$ , the decoder process ensures that students\u2019 knowledge proficiency is monotonic with the probability of the correct exercise responses, which embodies our desire to maintain the monotonicity. ", "page_idx": 3}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/44a974da94dde1a9555dffd2aed81e409468b81cd108a15168a2cd40dccd8a59.jpg", "img_caption": ["Figure 2: The end-to-end pipeline of AE-NMCF. We start from the scoring matrix $({\\bf X})$ , which is also the ending module. The question marks (\u2018?\u2019) in $\\mathbf{X}$ denote the absent responses that the students have never visited the exercises before. Here, we use the cell shadings to highlight the nonnegative constraints on the matrix blocks, wherein the dotted lines impose the sparse constraints. In addition, the solid and chain-dotted lines denote the decomposing and composing processes, respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "The encoder process. Given $\\mathbf{X}\\in[0|1]^{\\mathrm{N}\\times\\mathrm{M}}$ and $\\mathbf{Q}\\in[0|1]^{\\mathrm{N\\timesK}}$ , we start the encoder process with optimization problem (1), where we have three low-dimensional nonnegative matrices: $\\mathbf{E}\\in\\mathbb{R}^{\\mathrm{N}\\times\\mathrm{T}}$ , $\\dot{\\mathbf{U}}\\in\\mathbb{R}^{\\mathrm{T}\\times\\mathrm{M}}$ , and $\\mathbf{V}\\in\\dot{\\mathbb{R}}^{\\mathrm{T}\\times\\mathrm{K}}$ , each of which consists of $\\mathrm{T}$ latent factors. The latent factors can be loosely viewed as a series of topic skills denoting high-level knowledge in a subject area, such as \u201cspatial imagination\u201d and \u201cabstract summarization\u201d in mathematics. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{B},\\mathbf{U},\\mathbf{E},\\mathbf{V}}{\\operatorname*{min}}\\|\\mathbf{W}\\odot(\\mathbf{X}-\\mathbf{EU})\\|_{\\mathrm{F}}^{2}+\\|\\mathbf{Q}\\odot(\\mathbf{B}-\\mathbf{EV})\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad\\mathrm{s.t.}\\quad\\mathbf{B}\\ge\\mathbf{0},\\mathbf{U}\\ge\\mathbf{0},\\mathbf{E}\\ge\\mathbf{0},\\mathbf{V}\\ge\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the first term in problem (1), we use $\\mathbf{E}$ and $\\mathbf{U}$ to approximate $\\mathbf{X}$ through the Frobenius norm and introduce a weighted matrix $\\dot{\\mathbf{W}}\\in[0|1]^{\\mathrm{N\\timesM}}$ to focus on the observed entries in $\\mathbf{X}$ via the Hadama product $\\left(\\odot\\right)$ . In the second term, considering that $\\mathbf{Q}$ only stores the linkage between exercises and knowledge concepts with either true or false relations (failing to uncover their strength), we introduce the nonnegative matrix $\\mathbf{B}\\,\\in\\,\\mathbb{R}^{\\mathrm{N}\\times\\mathrm{K}}$ , where ${\\bf{B}}_{n k}$ is the degree to which exercise $\\operatorname{Ex}_{n}$ involves knowledge concept $\\mathrm{Kc}_{k}$ , with larger values denoting stronger involvement of the knowledge concept. Similarly, we use $\\mathbf{E}$ and $\\mathbf{V}$ to approximate $\\mathbf{B}$ , where the sparsity is imposed by $\\mathbf{Q}$ . ", "page_idx": 4}, {"type": "text", "text": "In problem (1), $\\mathbf{X}$ and $\\mathbf{B}$ share the matrix $\\mathbf{E}$ , which bridges the gap between students and knowledge concepts. In reality, given $\\mathbf{E}$ , the approximation for $\\mathbf{X}_{:m}$ is a linear accumulation of the columns of $\\mathbf{E}$ , weighted by the components of ${\\bf U}_{:m}$ , and so does $\\mathbf{B}_{:k}$ : we project the two nonnegative vectors $\\mathbf{U}_{:m}$ and $\\mathbf{V}_{:k}$ into the new basis $\\mathbf{E}$ [4]. Since the latent factors are considered topic skills [27], we define $\\mathbf{U}_{t m}$ as the topic knowledge of student $\\mathrm{St}_{m}$ on $t$ -th topic skill, as well as $\\mathbf{V}_{t k}$ as the topic requirement of $\\mathrm{Kc}_{k}$ accordingly. Based on $\\mathbf{U}_{t m}$ and $\\mathbf{V}_{t k}$ , we specify students\u2019 knowledge proficiency via the matrix $\\mathbf{A}=\\mathbf{V}^{\\top}\\mathbf{\\tilde{U}}\\in\\mathbb{R}^{\\mathrm{K\\timesM}}$ , where $\\mathbf{A}_{k m}$ is the cognitive level of $\\mathrm{St}_{m}$ on $\\mathrm{Kc}_{k}$ . ", "page_idx": 4}, {"type": "text", "text": "The decoder process. Recall that the matrix A specified by problem (1) does not give an off-theshelf diagnostic solution due to the ignorance of monotonic constraints. We remedy this void by reconstructing the scoring matrix $\\mathbf{X}$ . Specifically, we first assume that exercise $\\operatorname{Ex}_{n}$ has an intrinsic difficulty level $\\mu_{n}\\in\\mathbb{R}$ , which are stacked into a column vector $\\mathbf{M}=[\\mu_{1},\\mu_{2},\\cdots,\\mu_{\\mathrm{N}}]^{\\top}$ . Armed with A, $\\mathbf{B}$ , and M, the probability that $\\mathrm{St}_{m}$ answers $\\operatorname{Ex}_{n}$ correctly is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi(\\Delta_{n m})=\\int_{-\\infty}^{\\Delta_{n m}}\\mathcal{N}(t)\\mathrm{d}t=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\Delta_{n m}}\\mathrm{e}^{-t^{2}/2}\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Delta_{n m}=\\mathbf{B}_{n:}\\mathbf{A}_{:m}+\\mu_{n}$ indicates that $\\mathrm{St}_{m}$ \u2019s response to $\\operatorname{Ex}_{n}$ is generated by a linear accumulation of required knowledge concepts. In addition, we use an inverse link function $\\Phi(x)$ , which is often a response function in generalized linear models, to map $\\Delta_{n m}$ to the success probability of the binary response $\\mathbf{X}_{n m}$ . $\\Phi(x)$ can be any monotonic differentiable function. Here, we focus on the commonly used probit link function with the probability density of the standard Gaussian distribution. ", "page_idx": 4}, {"type": "text", "text": "Given Eq. (2), we can maximize the likelihood of the observed data $\\mathbf{X}_{n m}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(\\mathbf{X}_{n m})=\\Phi(\\Delta_{n m})^{\\mathbf{X}_{n m}}\\left[1-\\Phi(\\Delta_{n m})\\right]^{(1-\\mathbf{X}_{n m})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the likelihood finally yields the following optimization problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{\\mathbf{B}_{n};\\mathbf{A}_{m},\\mu_{n}:\\forall n,m}}\\!\\!-\\ell+\\frac{\\gamma}{2}\\sum_{n=1}^{\\mathbf{N}}\\|\\mathbf{B}_{n:}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\ell=\\sum_{(n,m)\\in\\Omega_{0}}\\log\\operatorname*{Pr}(\\mathbf{X}_{n m})}\\end{array}$ is the log-likelihood term, and $\\Omega_{0}\\subseteq\\{1,\\cdot\\cdot\\cdot,\\mathrm{N}\\}\\times\\{1,\\cdot\\cdot\\cdot,\\mathrm{M}\\}$ contains indices of the observed responses in $\\mathbf{X}$ . In addition, since one can arbitrarily increase the scale of the vector ${\\bf{B}}_{n}$ : while decreasing the scale of the vector $\\mathbf{A}_{:m}$ (or $\\mathbf{V}^{\\top}\\mathbf{U}_{:m})$ accordingly (and vice versa) without changing the likelihood, we gauge the vector ${\\bf{B}}_{n}$ : using the regularization term $\\scriptstyle\\sum_{n=1}^{\\ensuremath{\\mathrm{N}}}\\|\\mathbf{B}_{n:}\\|_{2}^{2}$ with the regularization parameter $\\gamma>0$ . To illustrate the encoder-decoder process further, we provide an example in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Objective function. By combining the encoder and decoder, the objective function $({\\mathcal{O}}_{\\mathrm{AF}})$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{B},\\mathbf{U},\\mathbf{E},\\mathbf{V},\\mathbf{M}}\\ \\mathcal{O}_{\\mathrm{AF}}=-\\ell+\\|\\mathbf{W}\\odot(\\mathbf{X}-\\mathbf{EU})\\|_{\\mathrm{F}}^{2}+\\|\\mathbf{Q}\\odot(\\mathbf{B}-\\mathbf{EV})\\|_{\\mathrm{F}}^{2}+\\frac{\\gamma}{2}\\sum_{n=1}^{\\mathrm{N}}\\|\\mathbf{B}_{n:}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{s.t.}\\qquad\\mathbf{B}\\geq\\mathbf{0},\\mathbf{U}\\geq\\mathbf{0},\\mathbf{E}\\geq\\mathbf{0},\\mathbf{V}\\geq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is worth taking a few moments to study the form of problem (5) as it enables the monotonicity from two viewpoints. First, the monotonicity is achieved by the monotonic formulation in Eq. (2); Second, the monotonicity is optimized by problem (4). They jointly guarantee that a large value of knowledge proficiency corresponds to a better chance of success on related exercises. ", "page_idx": 4}, {"type": "text", "text": "3.2 Model Solution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In problem (5), the first term $-\\ell$ is convex for the probit link function [28]. The second and third terms are convex in either $\\mathbf{B}$ , U, $\\mathbf{E}$ , or $\\mathbf{V}$ only, but they are not convex in all the variables together. ", "page_idx": 4}, {"type": "text", "text": "Given the nonnegative constraints, we employ the projected gradient (PG) method [29]. Furthermore, concerning blocks of ${\\bf{B}}_{n}$ : and ${\\bf U}_{:m}$ , we apply the PG via a block coordinate descent (PG-BCD) approach. Hence, problem (5) can be expressed in a block fashion as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\mathbf{B},\\mathbf{U},\\mathbf{E},\\mathbf{V},\\mathbf{M}}~{\\mathcal{O}}_{\\mathrm{AF}}=-\\ell+\\displaystyle\\sum_{m=1}^{\\mathrm{M}}\\|\\mathbf{W}_{:m}\\odot\\left(\\mathbf{X}_{:m}-\\mathbf{E}\\mathbf{U}_{:m}\\right)\\|_{2}^{2}+\\displaystyle\\sum_{n=1}^{\\mathrm{N}}\\|\\mathbf{Q}_{n}:\\odot\\left(\\mathbf{B}_{n:}-\\mathbf{E}_{n:}\\mathbf{V}\\right)\\|_{2}^{2}}\\\\ &{+\\displaystyle\\frac{\\gamma}{2}\\displaystyle\\sum_{n=1}^{\\mathrm{N}}\\|\\mathbf{B}_{n:}\\|_{2}^{2},\\quad\\quad\\mathrm{~s.t.~}\\mathbf{B}\\geq\\mathbf{0},\\mathbf{U}\\geq\\mathbf{0},\\mathbf{E}\\geq\\mathbf{0},\\mathbf{V}\\geq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Accordingly, the subproblems of ${\\bf{B}}_{n}$ , $\\mathbf{U}_{:m}$ , E, $\\mathbf{V}$ , and $\\mu_{n}$ constitute the iterations of PG-BCD for AE-NMCF. Next, we show the parameter solution for ${\\bf{B}}_{n}$ : in problem (6) below. For further details on the remaining parameters, refer to Appendix $\\mathbf{B}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{B}_{n:\\geq0}}\\ \\mathcal{O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})=\\sum_{m}-\\log\\operatorname*{Pr}(\\mathbf{X}_{n m})+\\frac{\\gamma}{2}\\|\\mathbf{B}_{n:}\\|_{2}^{2}+\\|\\mathbf{Q}_{n:}\\odot(\\mathbf{B}_{n:}-\\mathbf{E}_{n:}\\mathbf{V})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To solve problem (6), we note that second-order methods do not scale well to high-dimensional problems due to the necessary computation of the Hessian, making explicit calculation difficult for the probit link function. Thus, we build our learning algorithm on first-order methods. To do so, we first derive the gradients of $O_{\\mathrm{AF}}(\\mathbf{B}_{n:})$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})=-\\sum_{m}\\Xi_{n m}[\\mathbf{X}_{n m}-\\Phi(\\Delta_{n m})]\\mathbf{U}_{:m}^{\\top}\\mathbf{V}+2[\\mathbf{Q}_{n:}\\odot\\mathbf{B}_{n:}-\\mathbf{Q}_{n:}\\odot(\\mathbf{E}_{n:}\\mathbf{V})]+\\gamma\\mathbf{B}_{n:},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\Xi_{n m}=\\frac{\\mathcal{N}\\left(\\Delta_{n m}\\right)}{\\Phi\\left(\\Delta_{n m}\\right)\\left[1-\\Phi\\left(\\Delta_{n m}\\right)\\right]}}\\end{array}$ , and we can employ the gradients above to search for the optimum point. In each iteration $l=1,2,\\cdot\\cdot\\cdot$ , the gradient step is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{B}_{n:}^{(l+1)}\\leftarrow\\left[\\mathbf{B}_{n:}^{(l)}-\\eta_{\\mathbf{B}_{n:}}^{(l)}\\nabla\\mathcal{O}_{\\mathrm{AF}}^{(l)}(\\mathbf{B}_{n:})\\right]_{+},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the half-wave rectifier $[x]_{+}=m a x(\\kappa,x)$ , $\\kappa=10^{-15}$ ensures the nonnegativity [30], and \u03b7(Bl)n: is a suitable step size. For Eq. (7), a key issue is to choose the appropriate step size \u03b7(Bl)n:, and a simple strategy is \u201cArmijo rule along the projection arc\u201d [31]. Although the convergence is guaranteed, it is time-consuming to search for feasible values. Motivated by Lan et al. [6], we determine the appropriate step sizes by Lipschitz constants [32]. A common approach that guarantees convergence of a function $f$ is to set $\\eta^{(l)}=1/L$ , where $L$ is the Lipschitz constant of $\\nabla f$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Algorithm and Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start with Lemma 1 [6] to analyze the Lipschitz constant for problem (6). After that, we present the parameter learning algorithm for problem (5) and conclude with its theoretical analysis. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $\\begin{array}{r}{g(x)=\\frac{\\Phi^{\\prime}(x)}{\\Phi(x)}}\\end{array}$ , $x\\in\\mathbb R$ , where $\\Phi(x)$ is the probit link function. Then, for $y,z\\in\\mathbb{R}$ , we have $|g(y)-g(x)|\\leq L_{p}|\\dot{y}-z|$ . Here, $L_{p}=1$ is the scalar Lipschitz constant of $g(x)$ . ", "page_idx": 5}, {"type": "text", "text": "Since Eq. (3) can be rewritten as $\\operatorname*{Pr}(\\mathbf{X}_{n m})=\\Phi\\left((2\\mathbf{X}_{n m}-1)(\\mathbf{B}_{n:}\\mathbf{V}^{\\top}\\mathbf{U}_{:m}+\\mu_{n})\\right)$ for $\\Phi(\\cdot)$ , we derive the following theorem which serves as a bound on the (vector) Lipschitz constant for problem (6), using the result in Lemma 1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. For a given $n$ , substituting $\\operatorname*{Pr}(\\mathbf{X}_{n m})$ in problem (6) with the right hand side expression above yields the following ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{O}_{A F}(\\mathbf{B}_{n:})=\\sum_{m}-\\log\\Phi(\\Lambda_{n m})+\\frac{\\gamma}{2}\\|\\mathbf{B}_{n:}\\|_{2}^{2}+\\|\\mathbf{Q}_{n:}\\odot(\\mathbf{B}_{n:}-\\mathbf{E}_{n:}\\mathbf{V})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Lambda_{n m}=(2\\mathbf{X}_{n m}-1)(\\mathbf{B}_{n:}\\mathbf{V}^{\\top}\\mathbf{U}_{:m}+\\mu_{n})$ . For any vectors $\\mathbf{y},\\mathbf{z},$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{O}_{A F}(\\mathbf{y})-\\nabla\\mathcal{O}_{A F}(\\mathbf{z})\\|_{2}\\leq\\left[L_{p}\\sigma_{1}^{2}(\\mathbf{U}^{\\top}\\mathbf{V})+2\\left(\\sum_{k=1}^{\\mathbf{K}}\\mathbf{Q}_{n k}^{2}\\right)^{\\frac{1}{2}}+\\gamma\\right]\\|\\mathbf{y}-\\mathbf{z}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To prove Theorem 1, we first derive the gradient of $O_{\\mathrm{AF}}(\\mathbf{B}_{n:})$ based on the element-wise operation of $\\bar{\\mathcal{N}}(\\cdot)$ and $\\Phi(\\cdot)$ . After that, we establish the upper bound of the $\\ell_{2}$ -norm of the gradient difference given two arbitrary points $\\mathbf{y}$ and ${\\bf z}$ . The derivation details refer to Appendix D. By comparing with Theorem 1, we obtain the Lipschitz constant as $\\begin{array}{r}{L_{p}\\sigma_{1}^{2}(\\mathbf{U}^{\\top}\\mathbf{V})+2\\left(\\sum_{k=1}^{\\mathrm{K}}\\mathbf{Q}_{n k}^{2}\\right)^{\\frac{1}{2}}+\\gamma}\\end{array}$ , where $\\sigma_{1}(\\cdot)$ denotes the corresponding maximum singular value. ", "page_idx": 6}, {"type": "text", "text": "Armed with Eq. (7) with the step sizes determined by Theorem 1, Algorithm 1 outlines the optimization process for AE-NMCF, named PG-BCD $+_{+}$ Lipschitz. In Algorithm 1, we first initialize all parameters with random entries (line 1) and then optimize ${\\mathcal{O}}_{\\mathrm{AF}}$ in an alternating fashion. Each outer iteration solves the inner subproblems (lines $3-9$ ). For each subproblem, we optimize the target parameter and hold others constant. For example, we hold U, E, V, and M constant and separately optimize each block of variables in B. The update order in the block case is $\\mathbf{B}_{1}$ : $\\to\\mathbf{B}_{2}$ : $\\rightarrow\\cdot\\cdot\\cdot\\rightarrow{\\bf B}_{\\mathrm{N};}$ . The outer loop is terminated if the decrease in ${\\mathcal{O}}_{\\mathrm{AF}}$ is smaller than a threshold $\\epsilon$ (lines 6 \u2013 8). ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 PG-BCD+Lipschitz ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: X, $\\mathbf{Q}$ , and $\\epsilon$ .   \nOutput: $\\mathbf{B}_{n}$ :, $\\mathbf{U}_{:m}$ , E, V, and $\\mu_{n}$ $1{\\leq}n{\\leq}\\mathrm{N}$ , $1{\\leq}m{\\leq}\\mathrm{M})$ .   \n1: Initialize $\\mathbf{B}_{n:}^{(0)}\\geq\\mathbf{0}$ , $\\mathbf{U}_{:m}^{(0)}\\geq\\mathbf{0}$ , $\\mathbf{E}^{(0)}\\geq\\mathbf{0}$ , $\\mathbf{V}^{(0)}\\geq\\mathbf{0}$ , and $\\mu_{n}^{(0)}$ $(1{\\leq}n{\\leq}\\mathrm{N},1{\\leq}m{\\leq}\\mathrm{M}).$ .   \n2: Calculate O(A0F) .   \n3: for $l=0,1,\\cdots$ do   \n4: Update: B(nl:+1)( $[1{\\leq}n{\\leq}\\mathrm{N})$ ; U:(lm+1) $!\\leq\\!m{\\leq}\\mathrm{M})$ ; E(l+1); V(l+1); \u00b5(nl+1)(1\u2264n\u2264N).   \n5: 6: if |O(AlF+1)\u2212O(AlF)| \u2264 Calculate O(AlF+1). \u03f5 then   \n7: Return $\\mathbf{B}_{n:}^{(l+1)}$ , $\\mathbf{U}_{:m}^{(l+1)}$ , $\\mathbf{E}^{(l+1)}$ , $\\mathbf{V}^{(l+1)}$ , and $\\mu_{n}^{(l+1)}$ .   \n8: end if   \n9: end for ", "page_idx": 6}, {"type": "text", "text": "We now establish the convergence guarantees of PG-BCD $^{1+}$ Lipschitz. In fact, the development of rigorous statements for the convergence of B, U, E, V, and M to an optimum is not trivial, due to the block multi-convex nature. Nevertheless, we can establish the convergence of PG-BCD $+.$ Lipschitz based on a prior analysis of BCD for multiconvex optimization [33]. To achieve this, for the sake of convenience, let $\\boldsymbol{\\Theta}=(\\mathbf{B},\\mathbf{U},\\mathbf{E},\\mathbf{V},\\mathbf{M})$ , then we have the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Given any start point $\\Theta^{(0)}$ , let $\\{\\Theta^{(l)}\\}$ be the sequence of the factors from PG$B C D+$ Lipschitz, where $l\\,=\\,1,2,\\cdot\\cdot\\cdot$ are the outer iteration numbers, then the sequence $\\{\\Theta^{(l)}\\}$ converges to the finite the critical point of problem (5). In particular, $i f\\Theta^{(0)}$ is close to the global point of problem (5), PG-BCD+Lipschitz converges to the global optimum. ", "page_idx": 6}, {"type": "text", "text": "Since minimizing AE-NMCF follows multi-block coordinate descent solutions, which correspond to BCDs with the update (1.3a) in [33], we can use the results laid by Xu and Yin [33, Lemma 2.6, Corollary 2.7, and Theorem 2.8] to prove Theorem 2, and the proof details refer to Appendix E. Note that we can not guarantee the global optimum convergence of PG-BCD $^{1+}$ Lipschitz from an arbitrary point due to the multi-convex, but the use of multiple randomized initialization attempts can increase the change to reach the global optimal solution. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Data set description. We use real-world students\u2019 response data with different sparsities and knowledge-exercise relations, which are from diversified academic subjects, including $(a)$ Math (FrcSub, Junyi-s, and Quanlang-s), (b) Biology (SLP-Bio-s), (c) History (SLP-His-s), and $(d)$ English (SLP-Eng). FrcSub comprises of the fraction subtraction problem scores of 536 middle school students [10]. Junyi-s includes problem logs from an e-learning website based on the open-source code released by Khan Academy [34]. The private Quanlang-s data set is collected from mathematical exams given to junior schools supplied by QUANLANG education company. 2Others include SLP", "page_idx": 6}, {"type": "text", "text": "Bio-s, -His-s, and -Eng, which provide unit test results of K-12 learners compiled by an online learning platform (smart learning partner, SLP) [35]. Statistics of the data set are summarized in Table III of Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Baseline approaches. The baselines include data mining approaches and cognitive diagnosis models. The former uses the well-known NMF [36], MCF [16], GNMF [37], NMMF [38], and the advanced SNMCF [4]. For the latter, we consider the following competing models: (i) DINA [10], a classic CDM that models students\u2019 knowledge levels by a binary attribute vector with the slip and guess factors of exercises; $(i i)$ DIRT [11], an extended IRT model incorporating the deep learning technique to enhance the diagnostic process; (iii) DeepCDF [12], a deep learning-based CDM that considers the importance and relationships of knowledge concepts; and $(i v)$ QRCDM [13], which integrates the implicit knowledge-exercise relations into CDMs. DIRT and DeepCDF are modified by excluding text information. Our code is available at https://github.com/ShenbaoYu/AE-NMCF. ", "page_idx": 7}, {"type": "text", "text": "4.1 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the effectiveness of AE-NMCF in the two learning tasks. Additional experiments, such as cognitive case studies, can be found in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "We first compare student performance prediction. The evaluation metrics are the commonly used ACC and RMSE [4], which are calculated based on the ground truth of students\u2019 responses to exercises and corresponding predicted ones. Table 1 shows the prediction results with the best performances highlighted in boldface, the top 2 results are shaded, and we use $\\mathbf{\\omega}^{\\star}\\pm\\mathbf{\\omega}^{\\star}$ to denote the standard deviations. The last column lists the average ranks of all models from the Friedman test (a rank-based method to validate the performance of multiple models on multiple datasets) [39]. In Table 1, we observe that the data mining methods (especially SNMCF and AE-NMCF) perform well, and AE-NMCF lies in the top-2 performers on all data sets except for FrcSub and SLP-Eng in terms of ACC and RMSE, respectively. Its rank of 1.33 on ACC and 1.67 on RMSE also confirm the competitiveness of AE-NMCF. The results indicate that our model can not only handle the students\u2019 response data that yields varied degrees of sparsity but also do so for diversified subject domains. ", "page_idx": 7}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/5e81afa76ad494777eef75b78e0acdcb10e53f535caaa4c0a6322854baf9f681.jpg", "table_caption": ["Table 1: Experimental results on student performance prediction "], "table_footnote": ["1 The MCF model with the gradient-based method. 2 The MCF model with the Newton-Raphson method. "], "page_idx": 7}, {"type": "text", "text": "We proceed to discuss AE-NMCF\u2019s ability to estimate students\u2019 knowledge proficiency, which is our major concern. Since the ground truth of students\u2019 cognitive levels is unknown, we take cues from the area under curve and use a ranking-based metric (knowledge-response consistency coefficient, KRC) as an alternative way to evaluate the diagnostic results. Specifically, for knowledge concept $\\mathrm{Kc}_{k}$ , we first extract the pair set $\\cal{S}=\\{(\\mathrm{Ex}_{n},\\bar{\\mathrm{St}_{m}}),n\\in[0,\\mathrm{N}],\\bar{m}\\in[0,\\mathrm{M}]\\}$ from the testing set D. ", "page_idx": 7}, {"type": "text", "text": "For each pair $\\left(\\mathrm{Ex}_{n},\\mathrm{St}_{m}\\right)$ , we record student $\\mathrm{St}_{m}$ \u2019s proficiency level of $\\mathrm{Kc}_{k}$ and the true response score on exercise $\\operatorname{Ex}_{n}$ . Then, the KRC result on $\\mathrm{Kc}_{k}$ is $\\begin{array}{r}{\\mathrm{KRC}(\\mathrm{Kc}_{k})=\\left(\\chi-\\frac{\\mathrm{N}^{+}(\\mathrm{N}^{+}+1)}{2}\\right)/(\\mathrm{N}^{+}\\mathrm{N}^{-})}\\end{array}$ where $\\begin{array}{r}{\\chi=\\sum_{\\mathbf{X}_{n m}^{*}=1}\\mathcal{R}(n,m)}\\end{array}$ , and $\\mathcal{R}(n,m)$ is the reordered position of the pair $\\left(\\mathrm{Ex}_{n},\\mathrm{St}_{m}\\right)$ based on the proficiency level. $\\mathrm{N}^{+}\\,(\\mathrm{N}^{-})$ denotes the number of records with correct (wrong) answers in $\\boldsymbol{S}$ . Finally, we average the KRC values of all the knowledge concepts and denote the average as $r_{c}$ . Higher values of $r_{c}$ indicate better performance. ", "page_idx": 8}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/59e602d2314a4c849fccf188f7bd3f47c952a21203800a2bca20f02260f39d0f.jpg", "img_caption": ["Figure 3: Students\u2019 knowledge proficiency estimations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3 illustrates the results of estimating students\u2019 knowledge proficiency. We exclude NMF, MCF, GNMF, and NMMF due to their known poor capability of cognitive diagnosis. From the results, we have: $(a)$ the $r_{c}$ values in FrcSub surpass the other data sets as expected, which is mainly due to the strong and consistent connection between exercises and knowledge concepts. $(b)$ Regardless of the relationship types, AE-NMCF delivers comparable or slightly improved performance w.r.t. CDM-based approaches and rises well above SNMCF on all data sets. The resulting $p$ -value given by a Wilcoxon-signed rank test between AE-NMCF and SNMCF is 0.031, which also confirms the improvement. $(c)$ While QRCDM shows good diagnostic results, its predicting performance suffers for multiple data sets (see Table 1). This is mainly due to the knowledge-exercise relationship being one-to-one (or one-to-many), which may impede the discovery of implicit correlations. ", "page_idx": 8}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/3cb472fde1bb7d4d7c17ff4ef49912fe3a6e88f82e64e7f2bbd44226c5d84c29.jpg", "img_caption": ["Figure 4: Model comparison in balancing the two learning tasks via bubble visualizations. The $\\mathrm{{x}(y)}$ -axis denotes the prediction (estimation) performance in terms of ACC $(r_{c})$ , and the bubble size measures the harmonic mean of ACC and $r_{c}$ . The dash lines locate the models\u2019 average performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Furthermore, Figure 4 compares the model performance in balancing the two learning tasks, where we use a bubble\u2019s horizontal (vertical) position to note the ACC $(r_{c})$ value for a model. In spired by $F_{1}$ score, we further visualize the bubble size based on the harmonic mean of ACC and $r_{c}$ . Hence, the closer to the upper right corner with a larger bubble size, the better the balance achieved. As shown in Figure 4, SNMCF excels at student performance prediction but is inadequate in knowledge cognitive estimation. In addition, the comparatively low prediction performance of QRCDM compromises its balance ability, especially on Junyi-s and Quanlang-s. AE-NMCF, in contrast, is well above the model average (indicated by dash lines) on all data sets, which achieves the best balance between prediction accuracy and diagnostic ability and works with multiple relation cases. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We close with the demonstration of the effectiveness of our encoder-decoder learning pipeline. As shown in Table 2, we conduct the ablation study by the use of two variants of AE-NMCF, i.e., AE-NMCF w/o Decoder (Encoder) that removes the decoder (encoder) module. The optimization approach is also PG-BCD with appropriate Lipschitz constants. According to Table 2, the ignorance of the encoder (or decoder) process leads to a degradation in predicting and estimating performances, and the performance losses of AE-NMCF w/o Encoder are lower than those of the variant that removes the decoder. The positive results not only suggest the performance boost of the decoder module but also prove the efficacy of the proposed encoder-decoder architecture, which aligns with our expectations to achieve the monotonicity. ", "page_idx": 9}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/1bf49a974d0f899fb047952c8774deea13eda7cb89745e790519cffbedc8e04f.jpg", "table_caption": ["Table 2: Ablation analysis of AE-NMCF in student cognitive modeling "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.2 Discussion on the Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We summarize the key findings. First, AE-NMCF improves on competing approaches on two learning tasks across subject domains, data sparsities, and knowledge-exercise relationships. Notably, the better estimation accuracy of knowledge proficiency benefits from the explicit encoding of the knowledge level for each student, which is then iteratively improved by the novel autoencoder machine that guarantees that knowledge proficiency can cumulatively cause success in exercises. Second, our purely data-driven model estimates interpretable factors to pinpoint a student\u2019s strengths and weaknesses, which is helpful for decision-making as we may tailor learning resources. ", "page_idx": 9}, {"type": "text", "text": "However, AE-NMCF\u2019s improved prediction and estimation accuracy over the baselines (SNMCF in particular) comes at a price of higher computational complexity (e.g., see Table VI in Appendix F). Nevertheless, AE-NMCF is well-suited to scale-based tests, which are common scenarios in the real world because students are often evaluated for a small set of knowledge concepts, and the need for confidence statistics is one of the critical factors. In addition, we observe that the diagnostic results for some knowledge concepts tend to be overoptimistic due to ignorance of the prerequisite structure (e.g., see Figure VI in Appendix F), and one part of ongoing work is exploiting the knowledge prerequisite structure for AE-NMCF to attenuate this problem. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper studies student cognitive modeling from a data mining perspective, in which students\u2019 knowledge proficiency estimation is our primary concern. To tackle this problem, we propose the AE-NMCF model. Specifically, we root monotonicity in a co-factorization via the carefully crafted encoder-decoder framework. It achieves the assessment of students\u2019 knowledge proficiency end-toend. Considering the nonconvex nature of the objective function with nonnegative constraints, we develop a projected gradient method based on block coordinate descent with Lipschitz to facilitate model learning, in which theoretical convergence is guaranteed. Experiments on real-world data sets show that AE-NMCF embraces the merit of satisfactory ability to measure students\u2019 knowledge proficiency while retaining good performance prediction accuracy. The future work is two-fold: (1) Considering the learning dependency of knowledge concepts; (2) Investigating other efficient parameter learning methods and exploring their scalability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "SY was supported by the Natural Science Foundation of Xiamen, China (Grant No. 3502Z20227180), YP was supported by the Natural Science Foundation of Guangdong Province, China (Grant No. 2023A1515010869), the Social Science Planning Project of Jiangxi Province, China (Grant No. 19TQ06), and the Provincial Project on Teaching Reform in Higher Education Institutions in Jiangxi Province, China (Grant No. JXJG-18-4-3). PD was supported by a grant from the National Science Foundation of USA (Grant No. IIS-2312657). ML was supported by the Young Top Talent of Young Eagle Program of Fujian Province, China (Grant No. F21E0011202B01). This work was also supported by the National Natural Science Foundation of China (Grant No. 62276168, 62472363, 62176225). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yan Zhuang, Qi Liu, GuanHao Zhao, Zhenya Huang, Weizhe Huang, Zachary Pardos, Enhong Chen, Jinze Wu, and Xin Li. A bounded ability estimation for computerized adaptive testing. In the 37th Annual Conference on Neural Information Processing Systems, volume 36, 2023.   \n[2] Shi Dong, Xueyun Tao, Rui Zhong, Zhifeng Wang, Mingzhang Zuo, and Jianwen Sun. Advanced mathematics exercise recommendation based on automatic knowledge extraction and multi-layer knowledge graph. IEEE Transactions on Learning Technologies, 17:776\u2013793, 2023.   \n[3] Louis V. Dibello, L. Roussos, and W. Stout. 31a review of cognitively diagnostic assessment and a summary of psychometric models. Handbook of Statistics, 26:979\u20131030, 2006.   \n[4] Shenbao Yu, Yifeng Zeng, Yinghui Pan, and Fan Yang. Snmcf: a scalable non-negative matrix co-factorization for student cognitive modeling. IEEE Transactions on Knowledge and Data Engineering, pages 1\u201314, 2023.   \n[5] Nguyen Thai-Nghe, Lucas Drumond, Tom\u00e1\u0161 Horv\u00e1th, Artus Krohn-Grimberghe, Alexandros Nanopoulos, and Lars Schmidt-Thieme. Factorization techniques for predicting student performance. In Educational Recommender Systems and Technologies: Practices and Challenges, pages 129\u2013153. IGI Global, 2012.   \n[6] Andrew S Lan, Andrew E Waters, Christoph Studer, and Richard G Baraniuk. Sparse factor analysis for learning and content analytics. Journal of Machine Learning Research, 15(57):1959\u2013 2008, 2014.   \n[7] Paul R Rosenbaum. Testing the conditional independence and monotonicity assumptions of item response theory. Psychometrika, 49:425\u2013435, 1984.   \n[8] Antoine Guisan, Thomas C Edwards Jr, and Trevor Hastie. Generalized linear and generalized additive models in studies of species distributions: setting the scene. Ecological Modelling, 157(2-3):89\u2013100, 2002.   \n[9] Ronald K Hambleton and Linda L Cook. Latent trait models and their use in the analysis of educational test data. Journal of Educational Measurement, 14(2):75\u201396, 1977.   \n[10] Jimmy De La Torre. Dina model and parameter estimation: a didactic. Journal of Educational and Behavioral Statistics, 34(1):115\u2013130, 2009.   \n[11] Song Cheng, Qi Liu, Enhong Chen, Zai Huang, Zhenya Huang, Yiying Chen, Haiping Ma, and Guoping Hu. Dirt: deep learning enhanced item response theory for cognitive diagnosis. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2397\u20132400, 2019.   \n[12] Lina Gao, Zhongying Zhao, Chao Li, Jianli Zhao, and Qingtian Zeng. Deep cognitive diagnosis model for predicting students\u2019 performance. Future Generation Computer Systems, 126:252\u2013 262, 2022.   \n[13] Haowen Yang, Tianlong Qi, Jin Li, Longjiang Guo, Meirui Ren, Lichen Zhang, and Xiaoming Wang. A novel quantitative relationship neural network for explainable cognitive diagnosis model. Knowledge-Based Systems, 250:109156, 2022.   \n[14] Michel C Desmarais and Rhouma Naceur. A matrix factorization method for mapping items to skills and for enhancing expert-based q-matrices. In International Conference on Artificial Intelligence in Education, pages 441\u2013450, 2013.   \n[15] Ke Xu, Rujun Liu, Yuan Sun, Keju Zou, Yan Huang, and Xinfang Zhang. Improve the prediction of student performance with hint\u2019s assistance based on an efficient non-negative factorization. IEICE Transactions on Information and Systems, 100(4):768\u2013775, 2017.   \n[16] Ajit P Singh and Geoffrey J Gordon. Relational learning via collective matrix factorization. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 650\u2013658, 2008.   \n[17] Farnaz Sedighin, Massoud Babaie-Zadeh, Bertrand Rivet, and Christian Jutten. Multimodal soft nonnegative matrix co-factorization for convolutive source separation. IEEE Transactions on Signal Processing, 65(12):3179\u20133190, 2017.   \n[18] Adrian Flanagan, Were Oyomno, Alexander Grigorievskiy, Kuan E Tan, Suleiman A Khan, and Muhammad Ammad-Ud-Din. Federated multi-view matrix factorization for personalized recommendations. In Machine Learning and Knowledge Discovery in Databases: European Conference, pages 324\u2013347, 2021.   \n[19] Wen Wen, Wencui Wang, Zhifeng Hao, and Ruichu Cai. Factorizing time-heterogeneous markov transition for temporal recommendation. Neural Networks, 159:84\u201396, 2023.   \n[20] Panagiotis Symeonidis, Luca Bellinazzi, Chemseddine Berbague, and Markus Zanker. Safe and effective recommendation of drug combinations based on matrix co-factorization. In 2023 IEEE 36th International Symposium on Computer-based Medical Systems, pages 634\u2013639, 2023.   \n[21] Bo Yang, Yu Lei, Jiming Liu, and Wenjie Li. Social collaborative filtering by trust. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(8):1633\u20131647, 2016.   \n[22] Jiho Yoo and Seungjin Choi. Bayesian matrix co-factorization: variational algorithm and cram\u00e9rrao bound. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 537\u2013552, 2011.   \n[23] Wei Wang, Yan Huang, Yizhou Wang, and Liang Wang. Generalized autoencoder: a neural network framework for dimensionality reduction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 490\u2013497, 2014.   \n[24] Pengfei Liu, Xiaoming Sun, Yang Han, Zhishuai He, Weifeng Zhang, and Chenxu Wu. Arrhythmia classification of lstm autoencoder based on time series anomaly detection. Biomedical Signal Processing and Control, 71:103228, 2022.   \n[25] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: memoryaugmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1705\u20131714, 2019.   \n[26] Viet-Tuan Le and Yong-Guk Kim. Attention-based residual autoencoder for video anomaly detection. Applied Intelligence, 53(3):3240\u20133254, 2023.   \n[27] Michel C Desmarais. Mapping question items to skills with non-negative matrix factorization. ACM SIGKDD Explorations Newsletter, 13(2):30\u201336, 2012.   \n[28] Eric R Ziegel. The elements of statistical learning. Technometrics, 45(3):267\u2013268, 2003.   \n[29] Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. Foundations and Trends in Machine Learning, 10(3-4):142\u2013363, 2017.   \n[30] Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural Computation, 19(10):2756\u20132779, 2007.   \n[31] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):334\u2013334, 1997. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[32] Juha Heinonen. Lectures on Lipschitz Analysis. University of Jyv\u00e4skyl\u00e4, 2005. ", "page_idx": 12}, {"type": "text", "text": "[33] Yangyang Xu and Wotao Yin. A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion. SIAM Journal on Imaging Sciences, 6(3):1758\u20131789, 2013.   \n[34] Haw-Shiuan Chang, Hwai-Jung Hsu, and Kuan-Ta Chen. Modeling exercise relationships in e-learning: a unified approach. In International Conference on Educational Data Mining, pages 532\u2013535, 2015.   \n[35] Y Lu, Yang Pian, Ziding Shen, et al. Slp: a multi-dimensional and consecutive dataset from k-12 education. In Proceedings of the 29th International Conference on Computers in Education Conference, pages 261\u2013266, 2021.   \n[36] Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix factorization. In International Conference on Neural Information Processing Systems, pages 556\u2013562, 2000.   \n[37] Hyekyoung Lee and Seungjin Choi. Group nonnegative matrix factorization for eeg classification. In Artificial Intelligence and Statistics, pages 320\u2013327. PMLR, 2009.   \n[38] Koh Takeuchi, Katsuhiko Ishiguro, Akisato Kimura, and Hiroshi Sawada. Non-negative multiple matrix factorization. In Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pages 1713\u20131720, 2013.   \n[39] Janez Dem\u0161ar. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1\u201330, 2006.   \n[40] Edwin F Beckenbach and Richard Bellman. Inequalities, volume 30. Springer Science & Business Media, 2012.   \n[41] Roger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge University Press, 1994.   \n[42] Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. In Annales de l\u2019institut Fourier, volume 48, pages 769\u2013783, 1998.   \n[43] Steven G Krantz and Harold R Parks. A primer of real analytic functions. Springer Science & Business Media, 2002. ", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A An Example of the Encoder-Decoder Process ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure I provides an example of the encoder (Left) and decoder (Right) processes of AE-NMCF. First, the encoder targets the specification of student $\\mathrm{St_{3}}$ \u2019s knowledge proficiency vector $(\\mathbf{A}_{:3})$ . Here, we omit matrix $\\mathbf{B}$ for conciseness. Second, the decoder reconstructs $\\mathrm{St_{3}}$ \u2019s response on exercise $\\mathrm{Ex_{3}}$ (i.e., $\\mathbf{X}_{33},$ ) via the specified knowledge proficiency. Thus, by explicitly encoding the knowledge level for each student in the encoder, which is then iteratively improved by the decoder that guarantees that knowledge proficiency can cumulatively cause success in related exercises, monotonicity can be achieved. ", "page_idx": 13}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/4e9d41a2b2fa63ce193b9ee6b351a96348ad876cf86f889b59a23b6a68769510.jpg", "img_caption": ["Figure I: An illustration of the encoder and decoder processes, where we highlight the target entries in the color cells. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Parameter Learning for AE-NMCF ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The parameters of AE-NMCF include B, U, E, $\\mathbf{V}$ , and $\\mathbf{M}$ , where $\\mathbf{B}\\,=\\,[\\mathbf{B}_{1:}^{\\top},\\mathbf{B}_{2:}^{\\top},\\cdot\\cdot\\cdot\\,,\\mathbf{B}_{\\mathrm{N:}}^{\\top}]^{\\top}$ , $\\mathbf{U}=[\\mathbf{U}_{:1},\\mathbf{U}_{:2},\\cdot\\cdot\\cdot,\\mathbf{U}_{:\\mathrm{M}}]$ , and $\\mathbf{M}=[\\mu_{1},\\mu_{2},\\cdots,\\mu_{\\mathrm{N}}]^{\\top}$ . Because of the nonnegative constraints on B, U, $\\mathbf{E}$ , and $\\mathbf{V}$ , we employ the projected gradient via a block coordinate descent (PG-BCD) for the parameters solution, and the subproblems of $\\mathbf{B}_{n},$ , $\\mathbf{U}_{:m},\\mathbf{E},\\mathbf{V}$ , and $\\mu_{n}$ are ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{B}_{n:\\geq0}}\\ \\mathcal{O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})=\\sum_{m}-\\log\\operatorname*{Pr}(\\mathbf{X}_{n m})+\\frac{\\gamma}{2}\\|\\mathbf{B}_{n:}\\|_{2}^{2}+\\|\\mathbf{Q}_{n:}\\odot(\\mathbf{B}_{n:}-\\mathbf{E}_{n:}\\mathbf{V})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{U};m\\geq\\mathbf{0}}\\ \\mathcal{O}_{\\mathrm{AF}}(\\mathbf{U}_{:m})=\\sum_{n}-\\log\\operatorname*{Pr}(\\mathbf{X}_{n m})+\\|\\mathbf{W}_{:m}\\odot(\\mathbf{X}_{:m}-\\mathbf{E}\\mathbf{U}_{:m})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{E}\\geq\\mathbf{0}}{\\operatorname*{min}}}&{\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{E})=\\|\\mathbf{W}\\odot(\\mathbf{X}-\\mathbf{EU})\\|_{\\mathrm{F}}^{2}+\\|\\mathbf{Q}\\odot(\\mathbf{B}-\\mathbf{EV})\\|_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{V}\\geq\\mathbf{0}}\\quad\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{V})=\\sum_{n,m}-\\log\\operatorname*{Pr}(\\mathbf{X}_{n m})+\\|\\mathbf{Q}\\odot(\\mathbf{B}-\\mathbf{E}\\mathbf{V})\\|_{\\mathrm{F}}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu_{n}}\\quad\\mathcal{O}_{\\mathrm{AF}}(\\mu_{n})=\\sum_{m}-\\log\\operatorname*{Pr}(\\mathbf{X}_{n m}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathrm{Pr}(\\mathbf{X}_{n m})=\\Phi(\\Delta_{n m})^{\\mathbf{X}_{n m}}\\left[1-\\Phi(\\Delta_{n m})\\right]^{(1-\\mathbf{X}_{n m})}$ , and $\\Delta_{n m}=\\mathbf{B}_{n:}\\mathbf{A}_{:m}+\\mu_{n}$ . Considering the computation burden of the Hessian and the calculation difficulty for the probit link function when employing second-order approaches, we build our parameter learning algorithm on first-order methods. Hence, the gradients for problems (a) \u2013 (e) are ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})=-\\sum_{m}\\Xi_{n m}[\\mathbf{X}_{n m}-\\Phi(\\Delta_{n m})]\\mathbf{U}_{:m}^{\\top}\\mathbf{V}+2[\\mathbf{Q}_{n:}\\odot\\mathbf{B}_{n:}-\\mathbf{Q}_{n:}\\odot(\\mathbf{E}_{n:}\\mathbf{V})]+\\gamma\\mathbf{B}_{n:},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{U}_{:m})=-\\sum_{n}\\Xi_{n m}[\\mathbf{X}_{n m}-\\Phi(\\Delta_{n m})]\\mathbf{V}\\mathbf{B}_{n:}^{\\top}+2\\mathbf{E}^{\\top}[\\mathbf{W}_{:m}\\odot(\\mathbf{E}\\mathbf{U}_{:m})-\\mathbf{W}_{:m}\\odot\\mathbf{X}_{:m}],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{E})=2[\\mathbf{W}\\odot(\\mathbf{EU})-\\mathbf{W}\\odot\\mathbf{X}]\\mathbf{U}^{\\top}+2[\\mathbf{Q}\\odot(\\mathbf{EV})-\\mathbf{Q}\\odot\\mathbf{B}]\\mathbf{V}^{\\top}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{V})=-\\sum_{(n,m)}\\Xi_{n m}[\\mathbf{X}_{n m}-\\Phi(\\Delta_{n m})]\\mathbf{U}_{:m}\\mathbf{B}_{n:}+2\\mathbf{E}^{\\top}[\\mathbf{Q}\\odot(\\mathbf{E}\\mathbf{V})-\\mathbf{Q}\\odot\\mathbf{B}],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mu_{n})=-\\sum_{m}\\Xi_{n m}[\\mathbf{X}_{n m}-\\Phi(\\Delta_{n m})],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\Xi_{n m}=\\frac{\\mathcal{N}\\left(\\Delta_{n m}\\right)}{\\Phi\\left(\\Delta_{n m}\\right)\\left[1-\\Phi\\left(\\Delta_{n m}\\right)\\right]}}\\end{array}$ . Based on the gradients above, searching for the optimum point is easy. Taking ${\\bf{B}}_{n}$ : as an example, in each iteration , the gradient step is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{B}_{n:}^{(l+1)}\\leftarrow\\left[\\mathbf{B}_{n:}^{(l)}-\\eta_{\\mathbf{B}_{n:}}^{(l)}\\nabla\\mathcal{O}_{\\mathrm{AF}}^{(l)}(\\mathbf{B}_{n:})\\right]_{+},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use a half-wave rectifier $[x]_{+}=m a x(\\epsilon,x),\\;\\epsilon=10^{-15}$ to ensure the nonnegativity, and $\\eta_{\\mathbf{B}_{n}}^{(l)}$ is a suitable step size for $\\mathbf{B}_{n}.$ , which is determined by the Lipschitz constant in this paper. ", "page_idx": 14}, {"type": "text", "text": "C Complexity Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section discusses the time complexity of PG-BCD $^+$ Lipschitz. The analysis is based on the update rules of ${\\bf{B}}_{n}$ :, $\\mathbf{U}_{:m}$ , E, $\\mathbf{V}$ , and $\\mu_{n}$ : ( $1{\\leq}n{\\leq}\\mathrm{N}$ , $1{\\leq}m{\\leq}\\mathrm{M})$ . For simplicity of exposition, we consider the case where the number of students is larger than that of knowledge concepts, which commonly occurs. For each block $\\mathbf{B}_{n},$ , the Lipschitz constant takes $O(\\mathrm{MK^{2}})$ operations, and the operations of the gradient are bounded by $O(\\mathrm{MKT})$ for each iteration.3 Consequently, the cost of the variable B, which contains N blocks, is bounded by $O(\\mathrm{MNK^{2}})$ . Other parameters can be analyzed similarly, summarized in Table I. Hence, the overall cost is the number of iterations needed for convergence times $O(\\mathrm{MNK^{2}})$ , where the latter term is the complexity of each iteration. ", "page_idx": 14}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/1c1152d6e97c5d5adf9590b3706192d19b89764ee28346318e638989954b4089.jpg", "table_caption": ["Table I: Computational operations $({\\bf{U}},{\\bf{E}},{\\bf{V}},{\\bf{M}})$ for each iteration "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To prove Theorem 1, we first introduce a scalar Lipschitz constant in Lemma i [6]. ", "page_idx": 14}, {"type": "text", "text": "Lemma i. Let $\\begin{array}{r}{g(x)=\\frac{\\Phi^{\\prime}(x)}{\\Phi(x)}}\\end{array}$ , $x\\in\\mathbb{R},$ , where $\\Phi(x)$ is the inverse probit function. Then, for $y,z\\in\\mathbb{R},$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n|g(y)-g(x)|\\leq L_{p}|y-z|,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $L_{p}=1$ is the scalar Lipschitz constant for $g(x)$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we prove Theorem 1. For the sake of brevity, we assume that all entries in the student scoring matrix $\\mathbf{X}$ are observed, i.e., $\\Omega_{0}=\\{1,\\cdot\\cdot\\cdot,\\mathrm{N}\\}\\!\\times\\!\\{\\mathrm{i},\\cdot\\cdot\\cdot,\\mathrm{M}\\}$ ; the extension to the case with missing entries in $\\mathbf{X}$ is straightforward. In what follows, $\\mathcal{N}(\\cdot)$ and $\\Phi(\\cdot)$ are assumed to operate element-wise on the vector or matrix. We start with the gradient of $O_{\\mathrm{AF}}(\\mathbf{B}_{n:})$ in Theorem 1, as shown below ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})=-\\underbrace{\\sum_{m=1}^{\\mathrm{M}}\\left\\{\\frac{\\mathcal{N}(\\Lambda_{n m})}{\\Phi(\\Lambda_{n m})}(2\\mathbf{X}_{n m}-1)\\mathbf{U}_{:m}^{\\top}\\mathbf{V}\\right\\}}_{\\mathcal{O}_{\\mathrm{AF}}^{(1)}(\\mathbf{B}_{n:})}+2[\\mathbf{Q}_{n:}\\odot\\mathbf{B}_{n:}-\\mathbf{Q}_{n:}\\odot(\\mathbf{E}_{n:}\\mathbf{V})]+\\gamma\\mathbf{B}_{n:},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Lambda_{n m}=(2\\mathbf{X}_{n m}-1)(\\mathbf{B}_{n:}\\mathbf{V}^{\\top}\\mathbf{U}_{:m}+\\mu_{n})$ . The first term $O_{\\mathrm{AF}}^{(1)}(\\mathbf{B}_{n:})$ can be rearranged as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{O}_{\\mathrm{AF}}^{(1)}(\\mathbf{B}_{n:})=\\cfrac{\\mathcal{N}(\\Lambda_{n1})}{\\Phi(\\Lambda_{n1})}(2\\mathbf{X}_{n1}-1)\\mathbf{U}_{:1}^{\\top}\\mathbf{V}+\\cdot\\cdot+\\cfrac{\\mathcal{N}(\\Lambda_{n\\mathrm{M}})}{\\Phi(\\Lambda_{n\\mathrm{M}})}(2\\mathbf{X}_{n\\mathrm{M}}-1)\\mathbf{U}_{:\\mathrm{M}}^{\\top}\\mathbf{V}}\\\\ &{=\\left[\\cfrac{\\mathcal{N}(\\Lambda_{n1})}{\\Phi(\\Lambda_{n1})},\\cdot\\cdot\\cdot\\cdot,\\cfrac{\\mathcal{N}(\\Lambda_{n\\mathrm{M}})}{\\Phi(\\Lambda_{n\\mathrm{M}})}\\right]\\left[\\begin{array}{l}{(2\\mathbf{X}_{n1}-1)\\mathbf{U}_{:1}^{\\top}\\mathbf{V}}\\\\ {\\vdots}\\\\ {(2\\mathbf{X}_{n\\mathrm{M}}-1)\\mathbf{U}_{:\\mathrm{M}}^{\\top}\\mathbf{V}}\\end{array}\\right]}\\\\ &{=\\cfrac{\\mathcal{N}(\\Lambda_{n:})}{\\Phi(\\Lambda_{n:})}\\tilde{\\mathbf{C}}_{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Lambda_{n:}=[\\Lambda_{n1},\\cdot\\cdot\\cdot\\,,\\Lambda_{n\\mathrm{M}}]$ , and we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\Phi}_{\\mathrm{1}n:}=[(2\\mathbf{X}_{n1}-1)(\\mathbf{B}_{n:}\\mathbf{V}^{\\top}\\mathbf{U}_{:1}+\\mu_{n}),\\cdots,\\,(2\\mathbf{X}_{n M}-1)(\\mathbf{B}_{n:}\\mathbf{V}^{\\top}\\mathbf{U}_{:M}+\\mu_{n})]}\\\\ &{\\quad=\\mathbf{B}_{n:}[(2\\mathbf{X}_{n1}-1)\\mathbf{V}^{\\top}\\mathbf{U}_{:1},\\cdots,\\,(2\\mathbf{X}_{n M}-1)\\mathbf{V}^{\\top}\\mathbf{U}_{:M}]+[(2\\mathbf{X}_{n1}-1)\\mu_{n},\\cdots,\\,(2\\mathbf{X}_{n M}-1)\\mu_{n}]}\\\\ &{\\quad=\\mathbf{B}_{n:}\\widetilde{\\mathbf{C}}_{n}^{\\top}+\\widetilde{\\mathbf{X}}_{n:}^{\\mu_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, the gradient $\\nabla{\\mathcal O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})$ can be rewritten as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})=-\\,\\frac{\\mathcal{N}(\\mathbf{B}_{n:}\\widetilde{\\mathbf{C}}_{n}^{\\top}+\\widetilde{\\mathbf{X}}_{n:}^{\\mu_{n}})}{\\Phi(\\mathbf{B}_{n:}\\widetilde{\\mathbf{C}}_{n}^{\\top}+\\widetilde{\\mathbf{X}}_{n:}^{\\mu_{n}})}\\widetilde{\\mathbf{C}}_{n}+2[\\mathbf{Q}_{n:}\\odot\\mathbf{B}_{n:}-\\mathbf{Q}_{n:}\\odot(\\mathbf{E}_{n:}\\mathbf{V})]+\\gamma\\mathbf{B}_{n:}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can now establish an upper bound of the $l_{2}$ -norm of the difference between the gradients at two arbitrary points $\\mathbf{y}$ and ${\\bf z}$ of $\\nabla{\\mathcal O}_{\\mathrm{AF}}(\\mathbf{B}_{n:})$ as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left|\\frac{N(\\sum_{i}^{\\nu}(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}}))}{N(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}})}\\widetilde{\\Gamma}_{\\kappa_{i}}+2\\nabla_{4}\\Theta_{c}\\Theta_{c}\\Theta_{c}(\\mathbb{R}_{n},\\mathbb{V})\\right|}\\\\ &{\\quad+\\frac{\\mathcal{N}(\\sum_{i}^{\\nu}(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}}))}{N(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}})}\\widetilde{\\Gamma}_{\\kappa_{i}}-2\\nabla_{4}\\Theta_{c}\\Theta_{c}\\Theta_{c}(\\mathbb{R}_{n},\\mathbb{V})+\\mathcal{N}(-\\pi)\\Bigg|_{2}}\\\\ &{\\le1-\\left\\lVert\\frac{N(\\sum_{i}^{\\nu}(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}}))}{N(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}})}\\widetilde{\\Gamma}_{\\kappa_{i}}-\\frac{N(\\sum_{i}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}})}{N(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}})}\\right\\rVert_{2}}\\\\ &{\\le1-\\left\\lVert\\frac{N(\\sum_{i}^{\\nu}(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{i}}))}{N(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}})}\\widetilde{\\Gamma}_{\\kappa_{i}}-\\frac{N(\\sum_{i}^{\\nu}(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}}))}{N(\\sum_{j}^{\\nu}+\\frac{\\nu_{i}}{\\nu_{j}})}\\right\\rVert_{2}}\\\\ &{\\quad+|\\nabla_{4}\\Theta_{c}\\Theta_{c}\\Theta_{c}(\\mathbb{R}_{n},\\mathbb{V})|-2\\cdot\\Theta_{c}\\Theta_{c}\\cdot\\Theta_{c}(\\mathbb{R}_{n},\\mathbb{V})|_{2}+|\\gamma-\\pi|_{2}}\\\\ &{\\le\\sigma_{1}\\langle\\widetilde{\\Gamma}_{\\kappa_{i}}\\Big\\lVert\\frac{N(\\sum_{j}^{\\nu\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, (f) uses the triangle inequality of a norm. (g) follows the H\u00f6lder inequality [40], and $\\sigma_{1}(\\cdot)$ denotes the corresponding maximum singular value. The bounds of (h) follow from Lemma i and the inequality of Hadamard products (e.g., see [41, Section 5.5.1]). The final inequality (i) follows from the fact that filpping the signs of the rows (or columns) of a matrix does not affect its singular values. ", "page_idx": 15}, {"type": "text", "text": "Note that this proof assumes that the scoring matrix $\\mathbf{X}$ is fully populated. We can easily adapt to the case of missing entries in $\\mathbf{X}$ , by replacing the matrix $\\widetilde{\\mathbf{C}}_{n}$ to $\\widetilde{\\mathbf{C}}_{n}^{\\scriptscriptstyle T}$ , which is the matrix containing the rows of $\\widetilde{\\mathbf{C}}_{n}$ corresponding to the observed entries indexed by the set $\\mathcal{T}=\\{m:(n,m)\\in\\Omega_{\\mathrm{o}}\\}$ . We omit the  details for the sake of brevity. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "The proofs for the remaining subproblems for ${\\bf U}_{:m}$ , $\\mathbf{E}$ , $\\mathbf{V}$ , and $\\mu_{n}$ ( $1{\\leq}m{\\leq}\\mathrm{M}$ , $1{\\leq}n{\\leq}\\mathrm{N})$ follow analogously, and Table II summarizes the Lipschitz constants of all parameters, where ${\\bf1}_{\\mathrm{M},\\mathrm{N}}$ denotes the $\\mathrm{~M~}\\times\\mathrm{~N~}$ all-ones matrix. ", "page_idx": 16}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/4232ba61ca469829cd1bb39f42e4e129568501a9f1d6dae80f48690bc54c4a3b.jpg", "table_caption": ["Table II: The Lipschitz constants of parameters "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since minimizing AE-NMCF follows the multi-block coordinate descent solution, and the subproblems can correspond to BCDs with update (1.3a) in [33], we use the results laid by $\\mathrm{Xu}$ and Yin [33, Lemma 2.6, Corollary 2.7, and Theorem 2.8] to prove the convergence of PG-BCD $^+.$ Lipschitz. To this end, we show that the objective function of AE-NMCF, i.e., problem (j) meets all assumptions needed for the convergence results in [33]. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\mathbf{B},\\mathbf{U},\\mathbf{E},\\mathbf{V},\\mathbf{M}}~{\\mathcal{O}}_{\\mathrm{AF}}=-\\ell+\\|\\mathbf{W}\\odot(\\mathbf{X}-\\mathbf{E}\\mathbf{U})\\|_{\\mathrm{F}}^{2}+\\|\\mathbf{Q}\\odot(\\mathbf{B}-\\mathbf{E}\\mathbf{V})\\|_{\\mathrm{F}}^{2}+\\frac{\\gamma}{2}\\displaystyle\\sum_{n=1}^{\\mathrm{N}}\\|\\mathbf{B}_{n:}\\|_{2}^{2},}\\\\ &{\\quad\\quad\\mathrm{s.t.~}\\quad\\quad\\mathbf{B}\\geq\\mathbf{0},\\mathbf{U}\\geq\\mathbf{0},\\mathbf{E}\\geq\\mathbf{0},\\mathbf{V}\\geq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We start by discussing Assumptions 1 and 2 in [33]. For Assumption 1, since all the terms in problem (j) are nonnegative, we have $\\mathcal{O}_{\\mathrm{AF}}>-\\infty$ , which has a lower bound of 0. For Assumption 2, by inspecting the form of the individual subproblems, we see that they are strongly convex. Therefore, Assumptions 1 and 2 in [33] are met. ", "page_idx": 16}, {"type": "text", "text": "We then provide that problem (j) also meets the additional assumptions in [33, Lemma 2.6], which requires $(a)$ the Lipschitz continuous of the gradient of the block multi-convex function $\\nabla f$ on any bound set and $(b)$ the Kurdyka-\u0141ojasiewicz (KL) inequality [42]. To do so, for $(a)$ , let $\\Theta=$ $({\\bf{B}},{\\bf{U}},{\\bf{E}},{\\bf{V}},{\\bf{M}})$ , and we can rewrite the objective function of problem (j) as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{O}}_{\\mathrm{AF}}(\\Theta)=-\\ell+\\|\\mathbf{W}\\odot(\\mathbf{X}-\\mathbf{EU})\\|_{\\mathrm{F}}^{2}+\\|\\mathbf{Q}\\odot(\\mathbf{B}-\\mathbf{EV})\\|_{\\mathrm{F}}^{2}+\\displaystyle\\frac{\\gamma}{2}\\sum_{n=1}^{\\mathrm{N}}\\|\\mathbf{B}_{n:}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{\\Theta_{(i)}\\in\\Theta}\\delta\\big(\\Theta_{(i)}<\\mathbf{0}\\big)}\\\\ &{\\qquad=\\mathcal{O}_{\\mathrm{AF}}^{s}(\\Theta)+\\displaystyle\\sum_{\\Theta_{(i)}\\in\\Theta}\\delta\\big(\\Theta_{(i)}<\\mathbf{0}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Theta_{(i)}$ denotes the $i$ -th element of $\\Theta.~\\delta(z)$ is an indicator function, and we have $\\delta(z)=\\infty$ if $z<0$ and 0 otherwise. We now show that the gradients of the smooth part of $O_{\\mathrm{AF}}(\\Theta)$ , i.e., $\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\Theta)$ , is Lipschitz continuous in $\\mathrm{dom}(\\mathcal{O}_{\\mathrm{AF}}^{s})$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\Theta^{y})-\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\Theta^{z})\\|_{2}=\\left\\{\\displaystyle\\sum_{\\Theta_{(i)}\\in\\Theta}\\left(\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\Theta_{(i)}^{y})-\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\Theta_{(i)}^{z})\\right)^{2}\\right\\}^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\{\\displaystyle\\sum_{\\Theta_{(i)}\\in\\Theta}L_{\\Theta_{(i)}}^{2}\\|\\Theta_{(i)}^{y}-\\Theta_{(i)}^{z}\\|_{2}^{2}\\right\\}^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(L^{\\prime}\\right)^{\\frac{1}{2}}\\|\\Theta^{y}-\\Theta^{z}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $L^{\\prime}=\\operatorname*{max}\\{L_{\\Theta_{(i)}}^{2}\\},\\Theta_{(i)}\\in\\{\\mathbf{B},\\mathbf{U},\\mathbf{E},\\mathbf{V},\\mathbf{M}\\}$ . Recall that the bounds on the Lipschitz constant corresponding to $\\mathbf{E}$ and $\\mathbf{V}$ are shown in Table II. For $\\mathbf{B}$ , let $\\bar{\\mathbf{B}}=[\\mathbf{B}_{1:},\\mathbf{B}_{2:},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{B}_{\\mathrm{N:}}]^{\\top}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\bar{\\mathbf{B}}^{y})-\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\bar{\\mathbf{B}}^{z})\\|_{2}=\\left\\{\\displaystyle\\sum_{n=1}^{\\infty}\\left(\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\mathbf{B}_{n:}^{y})-\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\mathbf{B}_{n:}^{z})\\right)^{2}\\right\\}^{\\frac12}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\{\\displaystyle\\sum_{n=1}^{\\infty}\\left[L_{p}\\sigma_{1}^{2}(\\mathbf{U}^{\\top}\\mathbf{V})+2\\left(\\displaystyle\\sum_{k=1}^{\\mathrm{K}}\\mathbf{Q}_{n k}^{2}\\right)^{\\frac12}+\\gamma\\right]^{2}\\|\\mathbf{B}_{n:}^{y}-\\mathbf{B}_{n:}^{z}\\|_{2}^{2}\\right\\}^{\\frac12}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(L_{p}\\|\\mathbf{U}^{\\top}\\mathbf{V}\\|_{\\mathrm{F}}^{2}+2\\|\\mathbf{Q}_{n:}\\|_{2}+\\gamma\\right)\\|\\bar{\\mathbf{B}}^{y}-\\bar{\\mathbf{B}}^{z}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last line states that the maximum singular value of a matrix is no greater than its Frobenius norm. Similarly, the Lipschitz constants for $\\mathbf{U}$ and $\\mathbf{M}$ are $L_{p}\\|\\mathbf{V}\\mathbf{B}^{\\top}\\|_{\\mathrm{F}}^{\\tilde{2}}+2\\|\\mathbf{E}\\|_{2}^{2}\\|\\mathbf{W}_{:m}\\|_{2}$ and $L_{p}\\|2\\mathbf{X}_{n:}-\\mathbf{1}_{1,\\mathrm{M}}\\|_{2}^{2}$ , respectively. Therefore, $\\nabla\\mathcal{O}_{\\mathrm{AF}}^{s}(\\Theta)$ is Lipschitz continuous in $\\mathrm{dom}(\\mathcal{O}_{\\mathrm{AF}}^{s})$ . ", "page_idx": 17}, {"type": "text", "text": "For $(b)$ , using [6, Lemma 7], the first term (i.e., the negative log-probit likelihood function $-\\ell_{,}$ ) of ${\\mathcal{O}}_{\\mathrm{AF}}$ in problem (j) is real analytic, which is based on the fact that compositions of real analytic functions are real analytic [43]. In addition, the second and third terms with Frobenius norms in ${\\mathcal{O}}_{\\mathrm{AF}}$ , plus the regularizer, are all polynomial functions, therefore also real analytic. Hence, the objective function ${\\mathcal{O}}_{\\mathrm{AF}}$ is real analytic and satisfies the KL inequality, a consequence of [33, Section 2.2]. By setting the extrapolation weight $\\omega_{i}^{k}=0$ in [33], we can conclude that the PG-BCD $^+.$ Lipschitz algorithm converges to a local minimum. Furthermore, PG-BCD $+.$ Lipschitz converges globally if the initial point is close to the global minimum [33]. ", "page_idx": 17}, {"type": "text", "text": "F Extended Details of Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we conduct follow-up experiments to enhance the effectiveness of AE-NMCF. The statistics of the data sets are summarized in Table III, and the implementation details are described briefly below: ", "page_idx": 17}, {"type": "text", "text": "\u2022 We deploy the competing models using the best publicly available implementation with Python 3.8 on an Ubuntu server with a Core i9-1090K 3.7 GHz and 128 GB memory. \u2022 For AE-NMCF, we set the number of iterations and the stopping threshold $\\epsilon$ as 500 and 5 to guarantee convergence. The hyperparameters $\\mathrm{T}$ and $\\gamma$ are set in Section F.6. \u2022 For each dataset, we reshape the response logs to the scoring matrix and utilize a $80\\%/20\\%$ train/test split. All models\u2019 performances are averaged over 5 repeated trials to ensure fairness. ", "page_idx": 17}, {"type": "text", "text": "F.1 Statistical Hypothesis Test ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first conduct the hypothesis test for the student performance prediction and the knowledge proficiency estimation. Table IV shows the details of the paired $t$ -test results, where each entry denotes the $p$ -value of the AE-NMCF with the baseline in terms of a given metric. According to Table IV, AE-NMCF shows a significant difference at the $5\\%$ level with the baselines in most cases. We can conclude that the prediction (estimation) performance of AE-NMCF is significantly different from that of the competitive models. ", "page_idx": 17}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/e7d8f2960de9c9a875b432167bc7bd89bc21d9031fef368dd1d860e4a7d2b67b.jpg", "table_caption": ["Table III: The statistics of data sets "], "table_footnote": ["1 The relationships between knowledge concepts and exercises. 2 The sparsity of student scoring matrix. "], "page_idx": 18}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/37aca44a91b99c625cb55a66e1f9589a7fdbbe792720f3e752d491b74c3c23f3.jpg", "table_caption": ["Table IV: Paired $t$ -test for the prediction (estimation) results of AE-NMCF with other methods "], "table_footnote": ["Significant difference at the $5\\%$ level. "], "page_idx": 18}, {"type": "text", "text": "F.2 Nemenyi Test ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct the Nemenyi test [39] to present the comparison of the proposed AE-NMCF model with the baseline approaches. The Nemenyi test shows the differences between the average ranks among all the compared methods, and any two of which are significantly different if their average ranks differ by at least one crucial difference $5\\%$ in this paper). As illustrated in Figure II, it is obvious that the AE-NMCF model performs the best in terms of ACC and RMSE, which demonstrates its effectiveness in student performance prediction. ", "page_idx": 18}, {"type": "text", "text": "F.3 A Case Study for Diagnostic Comparison ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To get a sense of diagnostic improvement of AE-NMCF for data mining techniques (compared with SNMCF), we present the diagnostic results for case students on FrcSub, Quanlang-s, and Junyi-s, which covers all typical knowledge-exercise relationships. 4 ", "page_idx": 18}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/9f0fc1e340e55960124ce5baedfb889860876b29715f397270b826e4ce76b5c2.jpg", "img_caption": ["Figure II: The CD diagrams of all the methods in terms of ACC and RMSE. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/7bdc6309cdebab9c525b5588d95b7a0083b7a6391c695ea11cd4bf2af1d31a83.jpg", "img_caption": ["Figure III: Case students\u2019 cognitive diagnostic results (AE-NMCF vs. SNMCF) on FrcSub. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "FrcSub. Figure III compares four case students\u2019 diagnostic results from AE-NMCF and SNMCF, respectively, where each numerical value is a student\u2019s knowledge proficiency on a specific knowledge concept. In addition, we show the corresponding scoring matrix in Table V. For student $\\mathrm{St}_{108}$ , we can observe that both AE-NMCF and SNMCF give suitable diagnostic results since she answers all the exercises correctly. However, for students $\\mathrm{St_{58}}$ and $\\mathrm{St_{342}}$ , Table $\\mathrm{v}$ shows that they only give the right answer to $\\operatorname{Ex}_{9}$ and fail in the remaining exercises, which means that $\\mathrm{St}_{\\mathrm{58}}$ and $\\mathrm{St_{342}}$ can not grasp all knowledge concepts. The diagnostic result given by the AE-NMCF model confirms this fact, while SNMCF gives a confusing result. In addition, the response log of $\\mathrm{St_{107}}$ indicates that she needs to continuously make progress on most of the knowledge concepts to improve proficiency levels, but SNMCF argues that $\\mathrm{St_{107}}$ has mastered most of the knowledge concepts, which does not square with the facts. In summary, the diagnostic outputs provided by AE-NMCF align with our expectations. ", "page_idx": 19}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/e29ac0ba606d21f6779f4c315788a9f82c249c3dc08263a58930939a07bb42a7.jpg", "table_caption": ["Table V: The corresponding scoring matrix on FrcSub ", "Quanlang-s. Figure IV shows three case students\u2019 knowledge proficiency based on radar charts. To facilitate comparison, we also label the student\u2019s answer accuracy rates (the ratio of correctly "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/f31e670483d8670162bc599d721232d0b9f4b086cfe6c1edbfce0b84302efa6c.jpg", "img_caption": ["Figure IV: Diagnosis results of three case students between AE-NMCF and SNMCF on Quanlang-s. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "answering all exercises), e.g., $46.67\\%$ for student $\\mathrm{St_{50}}$ . Intuitively, the proficiency levels of $\\mathrm{St}_{42}$ should be the highest because of the top accuracy rate, and $\\mathrm{St}_{\\mathrm{50}}$ is at the lowest level accordingly. However, SNMCF gives an extreme estimation, which overestimates the ability of $\\mathrm{St_{50}}$ (or $\\mathrm{{St}_{33}}\\mathrm{{]}}$ ), and consequently, the cognitive diagnostic ability is limited. Instead, the proposed model gives reasonable results. We conclude that AE-NMCF provides richer information on the diagnosis than SNMCF. ", "page_idx": 20}, {"type": "text", "text": "Junyi-s. Different from FrcSub, there is substantial missing data in the scoring matrix for Junyi-s, with only $24.97\\%$ of its entries observed. Given that the relationship between the knowledge concepts and exercises is one-to-one, we show each knowledge proficiency (provided by AE-NMCF and SNMCF, respectively) with its corresponding answer record of three case students in Figure V, where each subgraph consists of two parts \u2013 the response (left) and the knowledge proficiency (right). From Figure V, we have the following observations: ", "page_idx": 20}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/7f290dea0306fe3b870d3edeaa0558dcbf87a52caa35f1ba091c376421085160.jpg", "img_caption": ["Figure V: Case students\u2019 cognitive diagnostic results and the corresponding answer record on Junyi-s. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "\u2022 First, Figure $\\mathrm{Va}$ and Figure Vd illustrate that both AE-NMCF and SNMCF provide reasonable diagnostic results since student $\\mathrm{St}_{718}$ has only a wrong answer record, which lacks far too much information available for diagnosis.   \n\u2022 Second, it can be seen from Figure Vb and Figure Ve that although student $\\mathrm{St_{91}}$ responds correctly to the given exercises (i.e., $\\operatorname{Ex}_{3}$ , $\\mathrm{Ex_{4}}$ , and $\\operatorname{Ex}_{7}$ ), there are still some exercises that $\\mathrm{St_{91}}$ has never answered before (e.g., $\\operatorname{Ex}_{1}$ ). However, the SNMCF model asserts that the student has completely mastered all knowledge concepts. In contrast, AE-NMCF makes more sense than SNMCF because the new model considers the uncertainty of the missing values of the unanswered exercises.   \n\u2022 Finally, for student $\\mathrm{St_{511}}$ (see Figure Vc and Figure Vf), we observe that the SNMCF model still gives more illogical diagnostic results than AE-NMCF because the response log shows that $\\mathrm{St_{511}}$ makes mistakes in some exercises (e.g., $\\mathrm{Ex_{4}}$ ), which indicates that she needs to timely learn the corresponding knowledge concepts (e.g., $\\operatorname{Kc}_{4}$ ). ", "page_idx": 20}, {"type": "text", "text": "F.4 Cognitive Diagnosis Visualization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We proceed to visualize and investigate the diagnostic results of a student as a case study, which provides useful insight into the estimation outcomes of the proposed model. Figure VI displays the student\u2019s knowledge proficiency with the corresponding answers on Quanlang-s. As observed, AE-NMCF gives interpretative and meaningful diagnostic results, based on which the student can determine her strengths and shortcomings. For example, the student has a good grasp of all knowledge concepts except for $\\mathtt{K c}_{12}$ (exponentiation of rational numbers). Observing her responses related to $\\mathtt{K c_{12}}$ , we notice that the student only tries very few relevant exercises. It suggests a timely study of $\\mathrm{Kc_{12}}$ for the student. Based on this visualization, AI-based tutoring systems could provide her with personalized remedy plans for improvement. ", "page_idx": 21}, {"type": "text", "text": "However, we see that the diagnostic result of $\\operatorname{Kc}_{9}$ is overoptimistic, not only because she made many mistakes in the related exercises but also due to her low proficiency in the prerequisite knowledge concepts (e.g., $\\mathrm{Kc}_{7}$ ). Recognizing this limitation, an intuitive work-around is to exploit the knowledge prerequisite structure for AE-NMCF to attenuate this problem. ", "page_idx": 21}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/53711d13d8a6ad7eafd05f2e04f0a601dcbad4a28cf7b5c68ad7fdd98477eed0.jpg", "img_caption": ["Figure VI: Diagnosis visualization of a case student on Quanlang-s via AE-NMCF. The bottom left shows her responses to related exercises. The circles with green (red) colors represent right (wrong) responses, and the hollow circles denote the absent responses. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "F.5 Comparison of the Step-Size Search Methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As noted earlier, the \u201cArmijo rule along the projection arc\u201d (Armijo rule) is another step-size solution. In this section, we show the compared performance between the Lipschitz search and the Armijo search on FrcSub, Junyi-s, and Quanlang-s, which covers all types of knowledge-exercise relationships. We first check the convergence in Figure VII, which sees that both the search solutions converge to a stationary point; however, the Armijo search at first quickly decreases the objective function value but slows down in sequence, which takes more time to converge. ", "page_idx": 21}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/ccf64efc187bd2d15bde7559c950afd38bc83e0b76ab3bd333261aec27347feb.jpg", "img_caption": ["Figure VII: The number of iterations vs. objective values for the Lipschitz search and Armijo search on FrcSub, Junyi-s, and Quanlang-s. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Furthermore, we fix the number of iterations (the smallest one of the two strategies) and present the compared performance in Table VI. We observe that all the methods exhibit similar performance for ", "page_idx": 21}, {"type": "text", "text": "Table VI: The comparative results between the Lipschitz search and Armijo search student cognitive modeling, while given the same number of iterations, the Lipschitz search achieves the fastest convergence while maintaining a relatively small objective function value. ", "page_idx": 22}, {"type": "table", "img_path": "8UqyWNsnyA/tmp/681523be8f89bbc7b616893c5369ea5effad43e5e37fbc9b2dadc8f1726f379b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/c321dc04455ff78d2fa9fe0b297e475132e7b945201946f556855ea752fb1ea1.jpg", "img_caption": ["Figure VIII: Sensitivity analysis of parameter T on the data sets. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "8UqyWNsnyA/tmp/64fa743d8059fe9c3b764c3d3b5f62210d3ef59b9905a9f097d77317c6e98e2f.jpg", "img_caption": ["Figure IX: Sensitivity analysis of parameter $\\gamma$ on the data sets. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.6 Parameter Sensitivity Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Finally, there are two parameters in the AE-NMCF model: (1) the number of latent factors $\\mathrm{T}$ (i.e., the rank of the nonnegative matrix co-factorization) and (2) the regularization parameter $\\gamma$ . Since $\\mathrm{T}$ leads a role in achieving the approximation effect, we begin by discussing T, followed by $\\gamma$ . ", "page_idx": 23}, {"type": "text", "text": "Effect of parameter T. We use the grid search rule to tune the value of parameter ${\\mathrm{~\\textit~{~T~}~}}=$ $\\operatorname*{min}\\{\\mathrm{N},\\mathrm{M},\\mathrm{K}\\}$ , and consider the effect in terms of ACC and $r_{c}$ . The results are summarized in Figure VIII. It can be seen from the figure that as the value of $\\mathrm{T}$ increases, ACC and $r_{c}$ share a similar decreasing tendency. Therefore, we choose the value that balances the two types of tasks, i.e., we set $\\mathrm{T}=3,3,2,1,1,1$ , for FrcSub, Junyi-s, Quanlang-s, SLP-Bio-s, SLP-His-s, and SLP-Eng respectively as the tuning results. ", "page_idx": 23}, {"type": "text", "text": "Effect of parameter $\\gamma.$ . Based on the best $\\mathrm{T}$ value, we proceed to find the best value for $\\gamma$ , which controls the degree of avoiding the ill-posed problem for B. For all the data sets, we perform the grid search with the range of $\\{1,2,3,4,5,10,15,20,25,50\\}$ . By observing the results in Figure IXa, we can see that the ACC $(r c)$ leads a drop at the beginning, followed by a sharp rise after $\\gamma=3\\,(\\gamma=4)$ , and then slightly fluctuates in the sequence. Therefore, we set $\\gamma=10$ for FrcSub. For Junyi-s and Quanlang-s, considering the fluctuation for the $r_{c}$ value, we choose $\\gamma=2,4$ respectively to avoid too much regularization. Similarly, we use $\\gamma=2,1,3$ as the tuning result for SLP-Bio-s, SLP-His-s, and SLP-Eng, respectively. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research scope is clearly defined, and we list the main contributions at the end of the introduction. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations (see Section 4.2). In addition, since student cognitive models have made diversified assumptions about how knowledge concepts contribute to generating a student\u2019s response to an exercise, including disjunctive, conjunctive, and additive hypotheses, the choice of assumptions depends on the context and measurement scenario, which is still an open question. This paper adopts the common additive hypothesis (see the decoder process in Section 3.1). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide proof details of Theorem 1 and Theorem 2 in Appendix D and Appendix E, respectively. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the data set and baseline descriptions in Section 4, as well as the implementation details and the parameter sensitivity analysis in Appendix F. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 25}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We attach the code and data sets to the supplemental material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see the implementation details and parameter sensitivity analysis in Appendix F. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report the error bars (standard deviations) of the results in predicting student performance (Table 1) and knowledge proficiency estimations (Figure 3) in the main manuscript. In addition, we provide the results of a statistical hypothesis test in Appendix F. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see the implementation details in Appendix F. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Except for the public data sets used in the paper, we also use a private data set (i.e., Quanlang-s), which was made available to us under an agreement with Quanlang education company (https://www.quanlangedu.com) whose terms included informed consent, privacy protection, and fairness. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The new cognitive model will potentially improve the automated comprehensive understanding of students\u2019 knowledge learning and benefit numerous intelligent educational tools. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the data sets and baselines used in the paper are cited by listing the references and websites. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper describes the proposed new cognitive model in detail (see Section 3). Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]