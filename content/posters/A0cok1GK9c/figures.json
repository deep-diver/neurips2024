[{"figure_path": "A0cok1GK9c/figures/figures_1_1.jpg", "caption": "Figure 1: Learning to embed distributions. (a) Example of multiple distributions over the input space. (b) The trainable function fe encodes the input dataset into a compact latent space, in our case Z = Sd-1. (c) The first-level embedding kernel k induces kernel mean embedding map to H. The encoder is optimized to maximize the entropy of the covariance operator embedding of the dataset w.r.t. the second-level distribution kernel K between kernel mean embeddings in H. (d) Utilizing learned data-dependent kernel, downstream classification tasks can be solved using tools such as Kernel SVM or Kernel Ridge Regression.", "description": "This figure illustrates the framework for learning to embed distributions. Panel (a) shows example distributions in the input space. The encoder function maps these distributions to a latent space (b) where the distributions are represented as points on a hypersphere.  A kernel is used to embed these points into a reproducing kernel Hilbert space (RKHS) (c). Finally, a distribution kernel is used to optimize the representation geometry, allowing for effective classification in multiple downstream tasks (d).", "section": "3 Unsupervised Distribution Kernel Learning"}, {"figure_path": "A0cok1GK9c/figures/figures_3_1.jpg", "caption": "Figure 2: Properties of the entropy on the toy example. (a) Entropy and Distributional Variance for 6 distributions on a sphere as a function of their geometrical arrangement parametrized by \u03b3. (b) Kernel norms that enter the distributional variance bound. The blue shaded area (difference between blue and red lines) corresponds to the dotted red line in (a) (up to multiplicative factor). (c) Flattening of Gram matrix eigenvalues as a function of \u03b3.", "description": "This figure demonstrates the properties of entropy on a toy example with six distributions arranged on a sphere. Panel (a) shows how entropy and distributional variance change as a function of the distributions' geometrical arrangement, controlled by the parameter \u03b3. Panel (b) displays the kernel norms that influence the distributional variance bound, showing the relationship between the entropy, kernel norms, and distributional variance. Finally, panel (c) illustrates the effect of \u03b3 on the flattening of the Gram matrix eigenvalues.", "section": "3 Unsupervised Distribution Kernel Learning"}, {"figure_path": "A0cok1GK9c/figures/figures_18_1.jpg", "caption": "Figure 3: The effect or regularization on the training dynamics. The distribution of the eigenvalues of the distribution kernel Gram matrix, calculated for 2,000 sentences sampled from '20 Newsgroups' dataset (details in Appendix D.2), is observed throughout the training. (a) Training with no regularization leads to the collapse of smaller eigenvalues. (b) The regularization stabilizes the training by preventing eigenvalues from collapsing.", "description": "This figure shows the impact of regularization on eigenvalue distribution during training.  The left panel shows the eigenvalues collapsing without regularization, whereas the right panel shows the effect of regularization in preventing this collapse, leading to better training stability.", "section": "B Practical Aspects of Learning"}, {"figure_path": "A0cok1GK9c/figures/figures_21_1.jpg", "caption": "Figure 4: Unsupervised encoding of Images. Unsupervised learning of image embeddings as finite-support distributions (i.e., histograms) of pixel intensities. For every pixel position we assign a point location on the unit hypersphere and optimize such locations via the covariance operator dataset embedding w.r.t. the MDKE objective. (a) Samples from the MNIST dataset and learned pixel-to-pixel interaction kernel Gram matrix. (b) Spectral clustering of pixels based on the learned kernel Gram matrix. (c) and (d) same as (a) and (b) for Fashion-MNIST dataset.", "description": "This figure visualizes the unsupervised learning of image embeddings as finite-support distributions (histograms of pixel intensities).  It shows how pixel positions are mapped to points on a hypersphere and optimized using the MDKE objective (Maximum Distribution Kernel Entropy).  The figure includes visualizations of the learned pixel-to-pixel interaction kernel Gram matrix and spectral clustering of pixels for both MNIST and Fashion-MNIST datasets.", "section": "D.1 Image Classification Tasks"}, {"figure_path": "A0cok1GK9c/figures/figures_21_2.jpg", "caption": "Figure 5: Unsupervised encoding of Text. Unsupervised learning of sentences embeddings as empirical distributions of words on the '20 Newsgroup' dataset. Goodness of the learned embeddings is evaluated by performing sentence-to-topic classification. (a) Distribution kernel entropy, distributional variance, and validation accuracy throughout training. (b) Kernel norms Eq. (16) throughout training. Shaded blue area (the difference between the blue and red lines) corresponds to the blue dotted line in panel (a) (up to a multiplicative factor).", "description": "This figure shows the results of applying the unsupervised learning method to a text dataset. Panel (a) displays the kernel entropy, distributional variance, and validation accuracy over training steps.  Panel (b) shows the average kernel norm and the norm of the average embedding over training steps. The shaded blue area in (b) visually represents the difference between the two curves in (a). The figure demonstrates how the proposed method optimizes embeddings by maximizing distributional variance and consequently improving classification accuracy.", "section": "3.5 An Illustrative Example"}]