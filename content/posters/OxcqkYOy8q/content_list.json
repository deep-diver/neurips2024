[{"type": "text", "text": "Improved Sample Complexity Bounds for Diffusion Model Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shivam Gupta Aditya Parulekar Eric Price UT Austin UT Austin UT Austin shivamgupta@utexas.edu adityaup@cs.utexas.edu ecprice@cs.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Zhiyang Xun UT Austin zxun@cs.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. From a theoretical standpoint, a number of recent works $[\\mathrm{CCL}^{+}23\\mathrm{b}$ , CCSW22, BBDD24] have studied the iteration complexity of sampling, assuming access to an accurate diffusion model. In this work, we focus on understanding the sample complexity of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network? Prior work [BMR20] showed bounds polynomial in the dimension, desired Total Variation error, and Wasserstein error. We show an exponential improvement in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Score-based diffusion models are currently the most successful methods for image generation, serving as the backbone for popular text-to-image models such as stable diffusion $[\\mathrm{RBL}^{+}22]$ , Midjourney, and DALL\u00b7E $[\\mathrm{RDN}^{+}22]$ as well as achieving state-of-the-art performance on other audio and image generation tasks [SDWMG15, HJA20, $\\mathrm{JAD}^{\\bar{+}}21$ , SSXE22, DN21]. ", "page_idx": 0}, {"type": "text", "text": "The goal of score-based diffusion is to produce a generative model for a possibly complicated distribution $q_{0}$ . This involves two components: training a neural network using true samples from $q_{0}$ to learn estimates of its (smoothed) score functions, and sampling using the trained estimates. To this end, consider the following stochastic differential equation, often referred to as the forward SDE: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=-x_{t}\\,\\mathrm{d}t+{\\sqrt{2}}\\,\\mathrm{d}B_{t},\\quad x_{0}\\sim q_{0}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $B_{t}$ represents Brownian motion. Here, $x_{0}$ is a sample from the original distribution $q_{0}$ over $\\mathbb{R}^{d}$ , while the distribution of $x_{t}$ can be computed to be ", "page_idx": 0}, {"type": "equation", "text": "$$\nx_{t}\\sim e^{-t}x_{0}+\\mathcal{N}(0,\\sigma_{t}^{2}I_{d})\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "for $\\sigma_{t}^{2}=1-e^{-2t}$ . Note that this distribution approaches $\\mathcal{N}(0,I_{d})$ , the stationary distribution of (1), exponentially fast. ", "page_idx": 0}, {"type": "text", "text": "Let $q_{t}$ be the distribution of $x_{t}$ , and let $s_{t}(y):=\\nabla\\log q_{t}(y)$ be the associated score function. We refer to $q_{t}$ as the $\\sigma_{t}$ -smoothed version of $q_{0}$ . Then, starting from a sample $x_{T}\\sim q_{T}$ , there is a reverse ", "page_idx": 0}, {"type": "text", "text": "SDE associated with the above forward SDE in (1) [And82]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=\\left(x_{T-t}+2s_{T-t}\\big(x_{T-t}\\big)\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}B_{t}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "That is to say, if we begin at a sample $x_{T}\\sim q_{T}$ , following the reverse SDE in (2) back to time 0 will give us a sample from the original distribution $q_{0}$ . This suggests a natural strategy to sample from $q_{0}$ : start at a large enough time $T$ and follow the reverse SDE back to time 0. Since $x_{T}$ approaches $\\mathcal{N}(0,I_{d})$ exponentially fast in $T$ , our samples at time 0 will be distributed close to $q_{0}$ . In particular, if $T$ is large enough\u2014logarithmic in $\\frac{m_{2}}{\\varepsilon}$ \u2014then samples produced by this process will be $\\varepsilon$ -close in TV to being drawn from $q_{0}$ . Here $m_{2}^{2}$ is the second moment of $q_{0}$ , given by $m_{2}^{2}:=\\mathbb{E}_{x\\sim q_{0}}[\\|x\\|^{2}]$ . ", "page_idx": 1}, {"type": "text", "text": "In practice, this continuous reverse SDE (2) is approximated by a time-discretized process. That is, the score $s_{T-t}$ is approximated at some fixed times $0=t_{0}\\leq t_{1}\\leq\\dots\\leq t_{k}<T$ , and the reverse process is run using this discretization, holding the score term constant at each time $t_{i}$ . This algorithm is referred to as \u201cDDPM\u201d, as defined in [HJA20]. ", "page_idx": 1}, {"type": "text", "text": "Chen et al. $[\\mathrm{CCL}^{+}23\\mathrm{b}]$ proved that as long as we have access to sufficiently accurate estimates for each score $s_{t}$ at each discretized time, this reverse process produces accurate samples. Specifically, for any $d$ -dimensional distribution $q_{0}$ supported on the Euclidean Ball of radius $R$ , the reverse process can sample from a distribution $\\varepsilon$ -close in TV to a distribution $(\\gamma\\cdot R)$ -close in 2-Wasserstein to $q_{0}$ in poly $(d,1/\\varepsilon,1/\\gamma)$ steps, as long as the score estimates $\\widehat{s}_{t}$ used at each step are $\\widetilde{O}(\\varepsilon^{2})$ accurate in squared $L^{2}$ . That is, as long as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{t}}{\\mathbb{E}}[\\|\\widehat{s}_{t}(x)-s_{t}(x)\\|^{2}]\\leq\\widetilde{O}(\\varepsilon^{2}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In this work, we focus on understanding the sample complexity of learning such score estimates. Specifically, we ask: ", "page_idx": 1}, {"type": "text", "text": "How many samples are required for a sufficiently expressive neural network to learn accurate score estimates that generate high-quality samples using the DDPM algorithm? ", "page_idx": 1}, {"type": "text", "text": "We consider a training process that employs the empirical minimizer of the score matching objective over a class of neural networks as the score estimate. More formally, we consider the following setting: ", "page_idx": 1}, {"type": "text", "text": "Setting 1.1. Let $\\mathcal{F}(D,P,\\Theta)$ be the class of functions represented by a fully connected neural network with ReLU activations and depth $D$ , with $P$ parameters, each bounded by $\\Theta$ . Let $\\{t_{k}\\}$ be some time discretization of $[0,T]$ . Given m i.i.d. samples $x_{i}\\sim q_{0}.$ , for each $t\\,\\in\\,\\{t_{k}\\}$ , we take m Gaussian samples $z_{i}\\sim\\mathcal{N}(\\bar{0},\\sigma_{t}^{2}I_{d})$ . We take $\\widehat{s}_{t}$ to be the minimizer of the score-matching objective: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{s}_{t}=\\underset{f\\in\\mathcal{F}}{\\arg\\operatorname*{min}}\\,\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|f\\left(e^{-t}x_{i}+z_{i}\\right)-\\frac{-z_{i}}{\\sigma_{t}^{2}}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We then use $\\{\\widehat{s}_{t_{k}}\\}$ as the score estimates in the DDPM algorithm. ", "page_idx": 1}, {"type": "text", "text": "This is the same setting as is used in practice, except that in practice (4) is optimized with SGD rather than globally. As in $[\\bar{\\mathrm{CCL}}^{+}23\\mathrm{b}]$ , we aim to output samples from a distribution that is $\\varepsilon$ -close in TV to a distribution that is $\\gamma R$ or $\\gamma m_{2}$ -close in Wasserstein to $q_{0}$ . We thus seek to bound the number of samples $m$ to get such a good output distribution, in terms of the parameters of Setting 1.1 and $\\varepsilon,\\gamma$ . ", "page_idx": 1}, {"type": "text", "text": "Block et al. [BMR20] first studied the sample complexity of learning score estimates using the empirical minimizer of the score-matching objective (4). They showed sample complexity bounds that depend on the Rademacher complexity of $\\mathcal{F}$ . Applied to our setting and using known bounds on Rademacher complexity of neural networks, in Setting 1.1 their result implies a sample complexity bound of $\\begin{array}{r}{{\\widetilde{O}}\\left({\\frac{d^{5/2}}{\\gamma^{3}\\varepsilon^{2}}}(\\Theta^{2}P)^{D}{\\sqrt{D}}\\right)}\\end{array}$ . See Appendix E for a detailed discussion. ", "page_idx": 1}, {"type": "text", "text": "Following the analysis of DDPM by $[\\mathrm{CCL}^{+}23\\mathrm{b}]$ , more recent work on the iteration complexity of sampling [CLL22, BBDD24] has given an exponential improvement on the Wasserstein accuracy $\\gamma$ , as well as replacing the uniform bound $R$ by $m_{2}$ \u2014 the square root of the second moment. In particular, [BBDD24] show that $\\widetilde{\\cal O}(\\frac{d}{\\varepsilon^{2}}\\log^{2}\\frac{1}{\\gamma})$ iterations suffice to sample from a distribution that is $\\gamma\\cdot m_{2}$ close to $q_{0}$ in 2-Wasserstein, as long as the score estimates are $\\widetilde{O}\\left(\\varepsilon^{2}/\\sigma_{t}^{2}\\right)$ accurate, i.e., ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{t}}{\\mathbb{E}}[\\|\\widehat{s}_{t}(x)-s_{t}(x)\\|^{2}]\\leq\\widetilde{O}\\left(\\frac{\\varepsilon^{2}}{\\sigma_{t}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Inspired by these works, we ask: is it possible to achieve a similar exponential improvement in the sample complexity of learning score estimates using a neural network? ", "page_idx": 2}, {"type": "text", "text": "1.1 Our Results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We give a new sample complexity bound of $\\begin{array}{r}{\\widetilde{O}(\\frac{d^{2}}{\\varepsilon^{3}}P D\\log\\Theta\\log^{3}\\frac{1}{\\gamma})}\\end{array}$ for learning scores to sufficient accuracy for sampling via DDPM. Learning is done by optimizing the same score matching objective that is used in practice. Compared to [BMR20], our bound has exponentially better dependence on $\\Theta,\\gamma$ , and $D$ , and a better polynomial dependence on $d$ and $P$ , at the cost of a worse polynomial dependence in $\\varepsilon$ . ", "page_idx": 2}, {"type": "text", "text": "As discussed above, for the sampling process, it suffices for the score estimate $s_{t}$ at time $t$ to have error $\\widetilde{O}\\left(\\varepsilon^{2}/\\sigma_{t}^{2}\\right)$ . This means that scores at larger times need higher accuracy, but they are also intuitively easier to estimate because the corresponding distribution is smoother. Our observation is that the two effects cancel out: we show that the sample complexity to achieve this accuracy for a fixed $t$ is independent of $\\sigma_{t}$ . However, this only holds once we weaken the accuracy guarantee slightly (from $\\bar{L^{2}}$ error to \u201c $L^{2}$ error over a $1-\\delta$ fraction of the mass\u201d). We show that this weaker guarantee is nevertheless sufficient to enable accurate sampling. Our approach lets us run the SDE to a very small final $\\sigma_{t}=\\gamma$ , which yields a final $\\gamma$ dependence of $O(\\log^{3}{\\frac{1}{\\gamma}})$ via a union bound over the times $t$ that we need score estimates $s_{t}$ for. In contrast, the approach in [BMR20] gets the stronger $L^{2}$ -accuracy, but requires a poly $\\Big(\\frac{1}{\\sigma_{t}}\\Big)$ dependence in sample complexity for each score $s_{t}$ ; this leads to their $\\mathrm{poly}({\\textstyle{\\frac{1}{\\gamma}}})$ sample complexity overall. ", "page_idx": 2}, {"type": "text", "text": "To state our results formally, we make the following assumptions on the data distribution and the training process: ", "page_idx": 2}, {"type": "text", "text": "A1 The second moment $m_{2}^{2}$ of $q_{0}$ is between $1/\\operatorname{poly}(d)$ and $\\mathrm{poly}(d)$ . ", "page_idx": 2}, {"type": "text", "text": "A2 For the score $s_{t}$ used at each step, there exists some function $f\\in\\mathcal{F}(D,P,\\Theta)$ (as defined in Setting 1.1) such that the $L^{2}$ error, $\\mathbb{E}_{x\\sim q_{t}}[||f(x)-s_{t}(x)||^{2}]$ , is sufficiently small. ", "page_idx": 2}, {"type": "text", "text": "That is: the data is somewhat normalized, and the smoothed scores can be represented well in the function class. Our main theorem is as follows: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.2. In Setting 1.1, suppose assumptions A1 and $_{A2}$ hold. For any $\\gamma>0$ , consider the score functions trained from ", "page_idx": 2}, {"type": "equation", "text": "$$\nm\\geq\\widetilde{O}\\left(\\frac{d^{2}P D}{\\varepsilon^{3}}\\cdot\\log\\Theta\\cdot\\log^{3}\\frac{1}{\\gamma}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "i.i.d. samples of $q_{0}$ . With $99\\%$ probability, DDPM using these score functions can sample from a distribution $\\varepsilon$ -close in TV to a distribution $\\gamma m_{2}$ -close to q in 2-Wasserstein. ", "page_idx": 2}, {"type": "text", "text": "We remark that assumption A1 is made for a simpler presentation of our theorem; the (logarithmic) dependence on the second moment is analyzed explicitly in Theorem C.2. The quantitative bound for A2\u2014exactly how small the $L^{2}$ error needs to be\u2014is given in detail in Theorem C.3. ", "page_idx": 2}, {"type": "text", "text": "Barrier for $L^{2}$ accuracy. As mentioned before, previous works have been using $L^{2}$ accurate score estimation either as the assumption for sampling or the goal for training. Ideally, one would like to simply show that the ERM of the score matching objective will have bounded $L^{2}$ error of $\\varepsilon^{2}/\\sigma^{2}$ with a number of samples that scales polylogarithmically in $\\frac{1}{\\sigma}$ . Unfortunately, this is false. In fact, it is information-theoretically impossible to achieve this in general without $\\mathrm{poly}({\\textstyle{\\frac{1}{\\sigma}}})$ samples. Since sampling to $\\gamma m_{2}$ Wasserstein error needs to consider a final $\\sigma_{t}=\\gamma$ , this leads to a $\\mathrm{poly}({\\textstyle{\\frac{1}{\\gamma}}})$ dependence. See Figure 1, or the discussion in Section 4, for a hard instance. ", "page_idx": 2}, {"type": "image", "img_path": "OxcqkYOy8q/tmp/1b78c03f85a3a587fdabc4e7d9550de0449416ae40c8da1bda75e49c993ec072.jpg", "img_caption": ["Figure 1: Given $\\textstyle o\\left({\\frac{1}{\\eta}}\\right)$ samples from either $p_{1}=(1-\\eta)\\mathcal{N}(0,1)+\\eta\\mathcal{N}(-R,1)$ , or $p_{2}=(1-\\eta)\\mathcal{N}(0,1)+$ $\\eta\\mathcal{N}(R,1)$ we will only see samples from the main Gaussian with high probability, and cannot distinguish between them. However, if we pick the wrong score function, the $L^{2}$ error incurred is large - about $\\eta R^{2}$ . On the right, we take $\\eta=0.001,R=10000,\\delta=0.01$ . We plot the probability that the ERM has error larger than 0 in the $L^{2}$ sense, and our $D_{p}^{\\delta}$ sense. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "In the example in Figure 1, score matching $+\\,\\mathrm{DDPM}$ still works to sample from the distribution with sample complexity scaling with $\\mathrm{poly}(\\log{\\frac{1}{\\gamma}})$ ; the problem lies in the theoretical justification for it. Given that it is impossible to learn the score in $L^{2}$ to sufficient accuracy with fewer than $\\mathrm{poly}({\\textstyle{\\frac{1}{\\gamma}}})$ samples, such a justification needs a different measure of estimation error. We will introduce such a measure, showing (1) that it will be small for all relevant times $t$ after a number of samples that scales polylogarithmically in $\\frac{1}{\\gamma}$ , and (2) that this measure suffices for fast sampling via the reverse SDE. ", "page_idx": 3}, {"type": "text", "text": "The problem with measuring error in $L^{2}$ comes from outliers: rare, large errors can increase the $L^{2}$ error while not being observed on the training set. We proved that we can relax the $L^{2}$ accuracy requirement for diffusion model to the $(1-\\delta)$ -quantile error: For distribution $p$ , and functions $f,g$ , we say that ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{p}^{\\delta}(f,g)\\leq\\varepsilon\\iff\\mathbb{P}_{x\\sim p}\\left[\\|f(x)-g(x)\\|_{2}\\geq\\varepsilon\\right]\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We adapt [BBDD24] to show that learning the score in our new outlier-robust sense also suffices for sampling, if we have accurate score estimates at each relevant discretization time. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1.3. Let q be a distribution over $\\mathbb{R}^{d}$ with second moment $m_{2}^{2}$ between $1/p o l y(d)$ and $p o l y(d)$ . For any $\\gamma>0$ , there exist $\\begin{array}{r}{N=\\widetilde O(\\frac{d}{\\varepsilon^{2}+\\delta^{2}}\\log^{2}\\frac{1}{\\gamma})}\\end{array}$ discretization times $0=t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{N}<T$ such that if the following holds for every $k\\in\\{0,\\ldots,N-1\\}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\delta/N}\\big(\\widehat{s}_{T-t_{k}},s_{T-t_{k}}\\big)\\leq\\frac{\\varepsilon}{\\sigma_{T-t_{k}}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then DDPM can sample from a distribution that is within ${\\widetilde{O}}(\\delta+\\varepsilon{\\sqrt{\\log{(d/\\gamma)}}})$ in TV distance to $q_{\\gamma}$ in $N$ steps. ", "page_idx": 3}, {"type": "text", "text": "With this relaxed requirement, our main technical lemma shows that for each fixed time, a good $(1-\\delta)$ -quantile accuracy can be achieved with a small number of samples, independent of $\\sigma_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 1.4 (Main Lemma). In Setting 1.1, suppose assumptions A1 and A2 hold. By taking m i.i.d. samples from $q_{0}$ to train score function $\\widehat{s}_{t}$ , when ", "page_idx": 3}, {"type": "equation", "text": "$$\nm>\\widetilde{O}\\left(\\frac{(d+\\log\\frac{1}{\\delta_{t r a i n}})\\cdot P D}{\\varepsilon^{2}\\delta_{s c o r e}}\\cdot\\log\\left(\\frac{\\Theta}{\\delta_{t r a i n}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with probability $1-\\delta_{t r a i n},$ , the score estimate $\\widehat{s_{t}}$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{q_{t}}^{\\delta_{s c o r e}}(\\widehat{s}_{t},s_{t})\\leq\\varepsilon/\\sigma_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Combining Lemma 1.4 with Lemma 1.3 gives us Theorem 1.2. The proof is detailed in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Score-based diffusion models were first introduced in [SDWMG15] as a way to tractably sample from complex distributions using deep learning. Since then, many empirically validated techniques have been developed to improve the sample quality and performance of diffusion models [HJA20, ND21, SE20, $\\mathrm{SSDK}^{+}21$ , SDME21]. More recently, diffusion models have found several exciting applications, including medical imaging and compressed sensing $[\\mathrm{JAD}^{+}2]$ , SSXE22], and text-toimage models like DALL\u00b7E 2 $[\\mathrm{RDN}^{+}22]$ and Stable Diffusion $[\\mathrm{RBL}^{+}22]$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Recently, a number of works have begun to develop a theoretical understanding of diffusion. Different aspects have been studied \u2013 the sample complexity of training with the score-matching objective [BMR20], the number of steps needed to sample given accurate scores $[\\mathrm{CCL}^{+}23\\mathrm{b}$ , CLL22, CCSW22, $\\mathrm{CCL^{+}23a}$ , BBDD24, BTHD21, LLT23], and the relationship to more traditional methods such as maximum likelihood $[\\mathrm{PRS}^{+}23$ , KHR23]. ", "page_idx": 4}, {"type": "text", "text": "On the training side, [BMR20] showed that for distributions bounded by $R$ , the score-matching objective learns the score of $q_{\\gamma}$ in $L^{2}$ using a number of samples that scales polynomially in $\\frac{1}{\\gamma}$ . On the other hand, for sampling using the reverse SDE in (2), [CLL22, BBDD24] showed that the number of steps to sample from $q_{\\gamma}$ scales polylogarithmically in $\\frac{1}{\\gamma}$ given $L^{2}$ approximations to the scores. ", "page_idx": 4}, {"type": "text", "text": "Our main contribution is to show that while learning the score in $L^{2}$ requires a number of samples that scales polynomially in $\\frac{1}{\\gamma}$ , the score-matching objective does learn the score in a weaker sense with sample complexity depending only polylogarithmically in $\\frac{1}{\\gamma}$ . Moreover, this weaker guarantee is sufficient to maintain the polylogarithmic dependence on $\\frac{1}{\\gamma}$ on the number of steps to sample with $\\gamma\\cdot m_{2}$ 2-Wasserstein error. ", "page_idx": 4}, {"type": "text", "text": "Our work, as well as [BMR20], assumes the score can be accurately represented by a small function class such as neural networks. Another line of work examines what is possible for more general distributions [CHZW23, OAS23]. For example, [CHZW23] shows that for general subgaussian distributions, the scores can be learned to $L^{2}$ error $\\varepsilon/\\sigma_{t}$ with $(d/\\varepsilon)^{O(d)}$ samples. Our approach avoids this exponential dependence, but assumes that neural networks can represent the score. ", "page_idx": 4}, {"type": "text", "text": "3 Proof Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we outline the proofs for two key lemmas: Lemma 1.4 in Section 3.1 and Lemma 1.3 in Section 3.2. The complete proofs for these lemmas are provided in Appendix A and Appendix B respectively. ", "page_idx": 4}, {"type": "text", "text": "3.1 Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We show that the score-matching objective (4) concentrates well enough that the ERM is close to the true minimizer. Prior work on sampling [BBDD24] shows that estimating the $\\sigma$ -smoothed score to $L_{2}^{2}$ error of $\\frac{\\varepsilon^{2}}{\\sigma^{2}}$ suffices for sampling; our goal is to get something close to this with a sample complexity independent of $\\sigma$ . ", "page_idx": 4}, {"type": "text", "text": "Background: Minimizing the true expectation gives the true score. In this section, we show that if we could compute the true expectation of the score matching objective, instead of just the empirical expectation, then the true score would be its minimizer. For a fixed $t$ , let $\\sigma=\\sigma_{t}$ and $p$ be the distribution of $e^{-t}x$ for $x\\sim q_{0}$ . We can think of a joint distribution of $(y,x,z)$ where $y\\sim p$ and $z\\sim N(0,\\sigma^{2}I_{d})$ are independent, and $x=y+z$ is drawn according to $q_{t}$ . With this change of variables, the score matching objective used in (4) is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left.\\underset{x,z}{\\mathbb{E}}\\left[\\left\\Vert s(x)-\\frac{-z}{\\sigma^{2}}\\right\\Vert_{2}^{2}\\right].\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Because $x=y+z$ for Gaussian $z$ , Tweedie\u2019s formula states that the true score $s^{*}=s_{t}$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\ns^{*}(x)=\\mathbb{E}_{z\\mid x}\\left[{\\frac{-z}{\\sigma^{2}}}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Define $\\begin{array}{r}{\\Delta=s^{*}(x)-\\frac{-z}{\\sigma^{2}}}\\end{array}$ , so $\\mathbb{E}[\\Delta\\mid x]=0$ . Therefore for any $x$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nl(s,x,z):=\\left\\|s(x)-\\frac{-z}{\\sigma^{2}}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|s(x)-s^{*}(x)+\\Delta\\right\\|^{2}}\\\\ &{=\\left\\|s(x)-s^{*}(x)\\right\\|^{2}+2\\langle s(x)-s^{*}(x),\\Delta\\rangle+\\left\\|\\Delta\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The third term does not depend on $s$ , and so does not affect the minimizer of this loss function. Also, for every $x$ , the second term is zero on average over $\\left(z\\mid x\\right)$ , so we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{s}\\mathbb{E}_{x,z}[l(s,x,z)]=\\arg\\operatorname*{min}_{s}\\mathbb{E}_{x,z}[\\|s(x)-s^{*}(x)\\|^{2}]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This shows that the score matching objective is indeed minimized by the true score. Moreover, an $\\varepsilon$ -approximate optimizer of $l(s)$ will be close in $L^{2}$ , as needed by prior samplers. ", "page_idx": 5}, {"type": "text", "text": "Understanding the ERM. The algorithm chooses the score function $s$ minimizing the empirical loss, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\hat{\\mathbb{E}}}{x,z}[l(s,x,z)]:=\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}l(s,x_{i},z_{i})}\\\\ {\\displaystyle=\\frac{\\hat{\\mathbb{E}}}{x,z}[\\|s(x)-s^{*}(x)\\|^{2}+2\\langle s(x)-s^{*}(x),\\Delta\\rangle+\\|\\Delta\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Again, the $\\hat{\\mathbb{E}}[\\|\\Delta\\|^{2}]$ term is independent of $s$ , so it has no effect on the minimizer and we can drop it from the loss function. We thus define ", "page_idx": 5}, {"type": "equation", "text": "$$\nl^{\\prime}(s,x,z):=\\left\\|s(x)-s^{*}(x)\\right\\|^{2}+2\\langle s(x)-s^{*}(x),\\Delta\\rangle\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "that satisfies $l^{\\prime}(s^{*},x,z)=0$ and $\\mathbb{E}[l^{\\prime}(s,x,z)]=\\mathbb{E}[\\left\\|s(x)-s^{*}(x)\\right\\|^{2}]$ . Our goal is now to to show that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{x,z}{\\hat{\\mathbb{E}}}[l^{\\prime}(s,x,z)]>0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all candidate score functions $s$ that are \u201cfar\u201d from $s^{*}$ . This would ensure that the empirical minimizer of the score matching objective is not \u201cfar\u201d from $s^{*}$ . To do this, we show that (10) is true with high probability for each individual $s$ , then take a union bound over a net. ", "page_idx": 5}, {"type": "text", "text": "Boundedness of $\\Delta$ . Now, $z\\sim N(0,\\sigma^{2}I_{d})$ is technically unbounded, but is exponentially close to being bounded: $\\|z\\|\\lesssim\\sigma{\\sqrt{d}}$ with overwhelming probability. So for the purpose of this proof overview, imagine that $z$ were drawn from a distribution of bounded norm, i.e., $\\|z\\|\\leq B\\sigma$ always; the full proof (given in Appendix A) needs some exponentially small error terms to handle the tiny mass the Gaussian places outside this ball. Then, since $\\begin{array}{r}{\\Delta=\\frac{z}{\\sigma^{2}}-\\mathbb{E}_{z|x}[\\frac{z}{\\sigma^{2}}]}\\end{array}$ , we get $\\|\\Delta\\|\\le2B/\\sigma$ . ", "page_idx": 5}, {"type": "text", "text": "Warmup: $\\mathrm{poly}(R/\\sigma)$ . As a warmup, consider the setting of prior work [BMR20]: (1) $\\|x\\|\\leq R$ always, so $\\begin{array}{r}{\\left\\|s^{*}(x)\\right\\|\\lesssim\\frac{R}{\\sigma^{2}}}\\end{array}$ ; and (2) we only optimize over candidate score functions $s$ with value clipped to within $\\textstyle O({\\frac{R}{\\sigma^{2}}})$ , so $\\begin{array}{r}{\\|s(x)-s^{*}(x)\\|\\lesssim\\frac{R}{\\sigma^{2}}}\\end{array}$ . With both these restrictions, then, $|l^{\\prime}(s,x,z)|\\leq$ $\\frac{R^{2}}{\\sigma^{4}}+\\frac{R B}{\\sigma^{3}}$ . We can then apply a Chernoff bound to show concentration of $l^{\\prime}$ : for $\\begin{array}{r}{\\mathrm{poly}(\\varepsilon,\\frac{R}{\\sigma},B,\\log\\frac{1}{\\delta_{\\mathrm{train}}})}\\end{array}$ samples, with $1-\\delta_{\\mathrm{train}}$ probability we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{\\hat{E}}_{x,z}[l^{\\prime}(s,x,z)]\\geq\\mathbb{E}_{x,z}[l^{\\prime}(s,x,z)]-\\frac{\\varepsilon}{\\sigma^{2}}}\\\\ {\\displaystyle=\\mathbb{E}[\\|s(x)-s^{*}(x)\\|^{2}]-\\frac{\\varepsilon^{2}}{\\sigma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is greater than zero if $\\mathbb{E}[\\|s(x)-s^{*}(x)\\|^{2}]\\,>\\,\\frac{\\varepsilon^{2}}{\\sigma^{2}}$ . Thus the ERM would reject each score function that is far in $L^{2}$ . However, as we show in Section 4, restrictions (1) and (2) are both necessary: the score matching ERM needs a polynomial dependence on both the distribution norm and the candidate score function values to learn in $L^{2}$ . ", "page_idx": 5}, {"type": "text", "text": "The main technical contribution of our paper is to avoid this polynomial dependence on $1/\\sigma$ . To do so, we settle for rejecting score functions $s$ that are far in our stronger distance measure $D_{p}^{\\delta_{\\mathrm{score}}}$ , i.e., for which ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[\\|s(x)-s^{*}(x)\\|>\\varepsilon/\\sigma]\\ge\\delta_{\\mathrm{score}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Approach. We want to show (10), which is a concentration over $x$ and $z$ . Now, $x$ is somewhat hard to control, because it depends on the unknown distribution, but $z\\sim N(0,\\sigma^{2}I_{d})$ is very well behaved. This motivates breaking up the expectation over $x,z$ into an expectation over $x$ and $z\\mid x$ . Following this approach, we could try to show that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{x,z}{\\hat{\\mathbb{E}}}[l^{\\prime}(s,x,z)]\\geq\\underset{x}{\\hat{\\mathbb{E}}}[\\underset{z\\mid x}{\\mathbb{E}}[l^{\\prime}(s,x,z)]]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "However, this is not possible to show; the problem is that this could be unbounded, since $s(x)$ is an arbitrary neural network that could have extreme outliers. So, we instead show this is true with high probability if we clip the internal value, making it ", "page_idx": 6}, {"type": "equation", "text": "$$\nA_{x}:=\\underset{x}{\\hat{\\mathbb{E}}}[\\operatorname*{min}(\\underset{z\\mid x}{\\mathbb{E}}[l^{\\prime}(s,x,z)],\\frac{10B^{2}}{\\sigma^{2}})]=\\underset{x}{\\hat{\\mathbb{E}}}[\\operatorname*{min}(\\left\\lVert s(x)-s^{*}(x)\\right\\rVert^{2},\\frac{10B^{2}}{\\sigma^{2}})].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that $A_{x}$ is a function of the empirical samples $x$ . If $s$ is a score that we want to reject under (11), then we know that $A_{x}$ is an empirical average of values that are at least $\\frac{\\varepsilon^{2}}{\\sigma^{2}}$ with probability $\\delta_{\\mathrm{score}}$ . It therefore holds that, for $m>O(\\frac{\\log\\frac{1}{\\delta_{\\mathrm{train}}}}{\\delta_{\\mathrm{scorc}}})$ (lo\u03b4gsc\u03b4otrreain ), we will with 1 \u2212\u03b4train probability over the samples x have ", "page_idx": 6}, {"type": "equation", "text": "$$\nA_{x}\\gtrsim\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}}{\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Concentration about the intermediate notion. Finally, we show that for every set of samples $x_{i}$ satisfying (12), we will have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{E}}_{z|x}[l^{\\prime}(s,x,z)]\\ge\\frac{A_{x}}{2}>0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This then implies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{\\hat{E}}_{x,z}[l^{\\prime}(s,x,z)]\\geq\\mathbb{\\hat{E}}\\left[\\frac{A_{x}}{2}\\right]\\gtrsim\\frac{\\varepsilon^{2}\\delta_{s c o r e}}{\\sigma^{2}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as needed. For each sample $x$ , we split our analysis of $\\hat{E}_{z|x}[l(s,x,z)]$ into two cases: ", "page_idx": 6}, {"type": "text", "text": "Case 1: $\\begin{array}{r}{\\|s(x)-s^{*}(x)\\|\\,>\\,O(\\frac{B}{\\sigma})}\\end{array}$ . In this case, by Cauchy-Schwarz and the assumption that $\\lVert\\Delta\\rVert\\leq2\\dot{B}/\\sigma$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nl^{\\prime}(s,x,z)\\geq\\left\\|s(x)-s^{*}(x)\\right\\|^{2}-O(\\frac{B}{\\sigma})\\left\\|s(x)-s^{*}(x)\\right\\|\\geq\\frac{10B^{2}}{\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "so these $x$ will contribute the maximum possible value to $A_{x}$ , regardless of $z$ (in its bounded range). ", "page_idx": 6}, {"type": "text", "text": "Case 2: $\\begin{array}{r}{\\|s(x)-s^{*}(x)\\|<O(\\frac{B}{\\sigma})}\\end{array}$ . In this case, $|l^{\\prime}(s,x,z)|\\lesssim B^{2}/\\sigma^{2}$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{z\\mid x}(l^{\\prime}(s,x,z))=4\\,\\mathbb{E}[\\langle s(x)-s^{*}(x),\\Delta\\rangle^{2}]\\lesssim\\frac{B^{2}}{\\sigma^{2}}\\left\\|s(x)-s^{*}(x)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "so for these $x$ , as a distribution over $z$ , $l^{\\prime}$ is bounded with bounded variance. ", "page_idx": 6}, {"type": "text", "text": "In either case, the contribution to $A_{x}$ is bounded with bounded variance; this lets us apply Bernstein\u2019s inequality to show, if m > O( \u03c32A\u03b4tr $\\begin{array}{r}{m>O(\\frac{B^{2}\\log\\frac{1}{\\delta_{\\mathrm{train}}}}{\\sigma^{2}A})\\approx O(\\frac{B^{2}\\log\\frac{1}{\\delta_{\\mathrm{train}}}}{\\varepsilon^{2}\\delta_{s c o r e}})}\\end{array}$ (B\u03b522 l\u03b4osgco\u03b4rtr1eain ), for every x we will have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{E}}[l^{\\prime}(s,x,z)]\\ge\\frac{A}{2}>0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $1-\\delta_{\\mathrm{train}}$ probability. ", "page_idx": 6}, {"type": "text", "text": "Conclusion. Suppose m > O (B\u03b5l2o\u03b4gsco\u03b4rterain). Then with 1 \u2212\u03b4train probability we will have (12); and conditioned on this, with $1-\\delta_{\\mathrm{train}}$ probability we will have $\\hat{\\mathbb{E}}_{x,z}[l^{\\prime}(s,x,z)]\\,>\\,0$ . Hence this $m$ suffices to distinguish any candidate score $s$ that is far from $s^{*}$ . For finite hypothesis classes we can take the union bound, incurring a $\\log\\left|\\mathcal{H}\\right|$ loss (this is given as Theorem A.2). Lemma 1.4 follows from applying this to a net over neural networks, which has size $\\log H\\approx P D\\log\\Theta$ . ", "page_idx": 6}, {"type": "text", "text": "3.2 Sampling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now we overview the proof of Lemma 1.3, i.e., why having an $\\varepsilon/\\sigma_{T-t_{k}}$ accuracy in the $D_{q}^{\\delta}$ sense is sufficient for accurate sampling. ", "page_idx": 7}, {"type": "text", "text": "To practically implement the reverse SDE in (2), we discretize this process into $N$ steps and choose a sequence of times $0=t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{N}<T$ . At each discretization time $t_{k}$ , we use our score estimates $\\widehat{s}_{t_{k}}$ and proceed with an approximate reverse SDE using our score estimates, given by the following .  For $t\\in[t_{k},t_{k+1}]$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=\\left(x_{T-t}+2\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}B_{t},\\quad x_{T}\\sim{\\mathcal{N}}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This is almost the reverse SDE that would give exactly correct samples, with two sources of error: it starts at $\\mathcal{N}(0,I_{d})$ rather than $q_{T}$ , and it uses $\\widehat{s}_{T-t_{k}}$ rather than the $s_{T-t}$ . The first error is negligible, since $q_{T}$ is $e^{-T}$ -close to $\\mathcal{N}(0,I_{d})$ . But how   much error does the switch from $s$ to $\\widehat{s}$ introduce? ", "page_idx": 7}, {"type": "text", "text": "Let $Q$ be the law of the reverse SDE using $s$ , and let $\\widehat{Q}$ be the law of (13). Then Girsanov\u2019s theorem states that the distance between $Q$ and $\\widehat{Q}$ is defined by the $L^{2}$ error of the score approximations: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{K L}(Q\\parallel\\widehat{Q})=\\displaystyle\\sum_{k=0}^{N-1}\\frac{\\mathbb{E}}{Q}\\left[\\int_{T-t_{k+1}}^{T-t_{k}}\\left|\\left|s_{T-t}(x_{T-t})-\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\right|\\right|^{2}\\right]}\\\\ {\\approx\\displaystyle\\sum_{k=0}^{N-1}\\frac{\\mathbb{E}}{x\\sim q_{T-t_{k}}}\\left[\\left||s_{T-t_{k}}(x)-\\widehat{s}_{T-t_{k}}(x)\\right|\\right|^{2}\\right]\\left(t_{k+1}-t_{k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The first line is an equality. The second line comes from approximating $x_{T-t}$ by $x_{T-t_{k}}$ , which for small time steps is quite accurate. So in previous work, good $L^{2}$ approximations to the score mean (14) is small, and hence $\\mathsf{K L}(Q\\parallel Q)$ is small. In our setting, where we cannot guarantee a good $L^{2}$ approximation, Girsanov actually implies that we cannot guarantee that $\\mathsf{K L}(Q\\parallel\\widehat{Q})$ is small. ", "page_idx": 7}, {"type": "text", "text": "However, since we finally hope to show closeness in TV, we can circumvent the above as follows. We define an event $E$ to be the event that the score is bounded well at all time steps $x_{T-t_{k}}$ , i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\nE:=\\bigwedge_{k\\in\\{1,...,N\\}}\\left(\\Vert\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})-s_{T-t_{k}}(x_{T-t_{k}})\\Vert\\leq\\frac{\\varepsilon}{\\sigma_{T-t_{k}}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "If we have a Dq\u03b4t/N accuracy for each score, we have $1-\\mathbb{P}\\left[E\\right]\\leq\\delta$ . Therefore, if we look at the TV error between $Q$ and $\\widehat{Q}$ , instead of bounding $\\langle\\!\\mathsf{L}(Q\\parallel\\widehat{Q})$ . The TV between $Q$ and $\\widehat{Q}$ can be then bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{T V}(Q,{\\widehat{Q}})\\leq(1-\\mathbb{P}\\left[E\\right])+\\mathsf{T V}((Q\\mid E),({\\widehat{Q}}\\mid E)).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The second term ${\\mathsf{T V}}((Q\\mid E),({\\widehat{Q}}\\mid E))$ is bounded because after conditioning on $E$ , the score error is always bounded, so now we can use (14) to bound the KL divergence between $Q$ and $\\widehat{Q}$ , then use Pinsker\u2019s inequality to translate KL into TV distance. ", "page_idx": 7}, {"type": "text", "text": "4 Hardness of Learning in $L^{2}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we give concrete examples where it is difficult to learn the score in $L^{2}$ , even though learning to sufficient accuracy for sampling is possible. Previous works, such as [BBDD24], require the $L^{2}$ error of the score estimate $s_{t}$ to be bounded by $\\varepsilon/\\sigma_{t}$ . We demonstrate that achieving this guarantee is prohibitively expensive: sampling from a $\\sigma_{t}$ -smoothed distribution requires at least $\\mathrm{poly}(1/\\sigma_{t})$ samples. Thus, sampling from a distribution $\\gamma$ -close in 2-Wasserstein to $q_{0}$ requires polynomially many samples in $\\frac{1}{\\gamma}$ . ", "page_idx": 7}, {"type": "text", "text": "To show this, we demonstrate two lower bound instances. Both of these instances provide a pair of distributions that are hard to distinguish in $L^{2}$ , and emphasize different aspects of this hardness: ", "page_idx": 7}, {"type": "text", "text": "1. The first instance shows that even with a polynomially bounded set of distributions, it is information theoretically impossible to learn a score with small $L^{2}$ error with high probability, with fewer than $\\mathrm{poly}(1/\\gamma)$ samples. ", "page_idx": 7}, {"type": "image", "img_path": "OxcqkYOy8q/tmp/5097adb527a95b09a94874778fc0c931a03f82d104d99eefe97678b6fb03a34d.jpg", "img_caption": ["Figure 2: For $m$ samples from $\\mathcal{N}(0,1)$ , consider the score $\\widehat{s}$ of the mixture $\\eta\\mathcal{N}(0,1)+(1-\\eta)\\mathcal{N}(R,1)$ above with $\\eta$ is chosen so that $\\widehat{s}(10\\sqrt{\\log m})=0$ . For this ${\\widehat{s}},$ , the score-matching objective is close to 0, while the squared $L^{2}$ error is $\\textstyle\\Omega\\left({\\frac{R^{2}}{m}}\\right)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "2. In the second instance, we show that even with a simple true distribution, such as a single Gaussian, distinguishing the score matching loss of the true score function from one with high $L^{2}$ error can be challenging with fewer than poly $(1/\\gamma)$ samples if the hypothesis class is large, such as with neural networks. ", "page_idx": 8}, {"type": "text", "text": "Information theoretically indistinguishable distributions: For our first example, consider the two distributions $(1-\\eta)\\bar{\\mathcal{N}}(0,\\sigma^{2})+\\bar{\\eta}\\mathcal{N}(\\pm R,\\sigma^{2})$ , where $R$ is polynomially large. Even though these distributions are polynomially bounded, it is impossible to distinguish these in $\\breve{L}^{2}$ given significantly fewer than $\\begin{array}{r}{\\frac{1}{\\eta}}\\end{array}$ samples. However, the $L^{2}$ error in score incurred from picking the score of the wrong distribution is large. In Figure 1, the rightmost plot shows a simulation of this example, and demonstrates that the $L^{2}$ error remains large even after many samples are taken. Formally, we have: ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.1. Let $R$ be sufficiently larger than $\\sigma$ . Let $p_{1}$ be the distribution $(1-\\eta)\\mathcal{N}(0,\\sigma^{2})\\,+$ $\\eta\\mathcal{N}(-R,\\sigma^{2})$ with corresponding score function $s_{1}$ , and let $p_{2}$ be $(1-\\eta)\\mathcal{N}(0,\\sigma^{2})+\\eta\\mathcal{N}(R,\\sigma^{2})$ with score $s_{2}$ , such that $\\begin{array}{r}{\\eta=\\frac{\\varepsilon^{2}\\sigma^{2}}{R^{2}}}\\end{array}$ \u03b5R\u03c32 . Then, given m < $\\begin{array}{r}{m\\,<\\,\\frac{R^{2}}{\\varepsilon^{2}\\sigma^{2}}}\\end{array}$ samples from either distribution, it is impossible to distinguish between $p_{1}$ and $p_{2}$ with probability larger than $1/2+o_{m}(1)$ . But, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\sim p_{1}}{\\mathbb{E}}\\left[\\|s_{1}(x)-s_{2}(x)\\|^{2}\\right]\\gtrsim\\frac{\\varepsilon^{2}}{\\sigma^{2}}\\qquad a n d\\qquad\\underset{x\\sim p_{2}}{\\mathbb{E}}\\left[\\|s_{1}(x)-s_{2}(x)\\|^{2}\\right]\\gtrsim\\frac{\\varepsilon^{2}}{\\sigma^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Simple true distribution: Now, consider the true distribution being $N(0,\\sigma^{2})$ , and, for large $S$ , let $\\widehat{s}$ be the score of the mixture distribution $\\eta\\mathcal{N}(0,\\sigma^{2})+(1-\\eta)\\mathcal{N}(\\bar{S_{,}}\\sigma^{2})$ , as in Figure 2. This score will have practically the same score matching objective as the true score for the given samples with high probability, as shown in Figure 2, since all $m$ samples will occur in the region where the two scores are nearly identical. However, the squared $L^{2}$ error incurred from picking the wrong score function $\\widehat{s}$ is large We formally show this result in the following lemma: ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.2. Let $S$ be sufficiently large. Consider the distribution $\\widehat{p}=\\eta N(0,\\sigma^{2})+(1-\\eta){\\mathcal N}(S,\\sigma^{2})$ for \u03b7 = Se\u2212S22 \u221a+10\u221alog m\u00b7S, and let $\\widehat{s}$ be its score function. Given m samples from the standard Gaussian $p^{*}=\\mathcal{N}(0,\\sigma^{2})$ with score function $s^{*}$ , with probability at least $\\begin{array}{r}{1-\\frac{1}{p o l y(m)}}\\end{array}$ \u201d ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\|\\widehat{s}(x)-s^{*}(x)\\|^{2}\\right]\\leq\\frac{1}{\\sigma^{2}}e^{-O(S\\sqrt{\\log m})}\\quad b u t\\quad\\underset{x\\sim p^{*}}{\\mathbb{E}}\\left[\\|\\widehat{s}(x)-s^{*}(x)\\|^{2}\\right]\\gtrsim\\frac{S^{2}}{m\\sigma^{4}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Together, these examples show that even with reasonably bounded or well-behaved distributions, it is difficult to learn the score in $L^{2}$ with fewer than poly $(R/\\gamma)$ samples, motivating our $(1-\\delta)$ -quantile error measure. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we have addressed the sample complexity of training the scores in diffusion models. We showed that a neural networks, when trained using the standard score matching objective, can be used for DDPM sampling after $\\begin{array}{r}{\\widetilde{O}(\\frac{d^{2}P D}{\\varepsilon^{3}}\\log\\Theta\\log^{3}\\frac{1}{\\gamma})}\\end{array}$ training samples. This is an exponentially better dependence on the neural network depth $D$ and Wasserstein error $\\gamma$ than given by prior work. To achieve this, we introduced a more robust measure, the $1-\\delta$ quantile error, which allows for efficient training with $\\mathrm{poly}(\\log{\\frac{1}{\\gamma}})$ samples using score matching. By using this measure, we showed that standard training (by score matching) and sampling (by the reverse SDE) algorithms achieve our new bound. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "One caveat is that our results, as well as those of the prior work [BMR20] focus on understanding the statistical performance of the score matching objective: we show that the empirical minimizer of the score matching objective over the class of ReLU networks approximates the score accurately. We do not analyze the performance of Stochastic Gradient Descent (SGD), commonly used to approximate this empirical minimizer in practice. Understanding why SGD over the class of neural networks performs well is perhaps the biggest problem in theoretical machine learning, and we do not address it here. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Syamantak Kumar and Dheeraj Nagaraj for pointing out an error in a previous version of this work, and for providing a fix (Lemma F.7). This project was funded by NSF award CCF-1751040 (CAREER) and the NSF AI Institute for Foundations of Machine Learning (IFML). ", "page_idx": 9}, {"type": "text", "text": "6 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[And82] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[BBDD24] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly $\\mathbb{S}\\mathrm{d}\\mathbb{S}$ -linear convergence bounds for diffusion models via stochastic localization. In The Twelfth International Conference on Learning Representations, 2024. [BMR20] Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. arXiv:2002.00107 [cs, math, stat], June 2020. arXiv: 2002.00107.   \n[BTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n$[\\mathrm{CCL}^{+}23\\mathrm{a}]$ Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 68552\u201368575. Curran Associates, Inc., 2023.   \n$[\\mathsf{C C L}^{+}23\\mathsf{b}]$ Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.   \n[CCSW22] Yongxin Chen, Sinho Chewi, Adil Salim, and Andre Wibisono. Improved analysis for a proximal algorithm for sampling. In Conference on Learning Theory, pages 2984\u20133014. PMLR, 2022.   \n[CHZW23] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023. [CLL22] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, 2022. [DN21] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 8780\u20138794. Curran Associates, Inc., 2021. [GLP23] Shivam Gupta, Jasper C.H. Lee, and Eric Price. High-dimensional location estimation via norm concentration for subgamma vectors. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 12132\u201312164. PMLR, 23\u201329 Jul 2023. [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.   \n[HKZ12] Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian random vectors. Electronic Communications in Probability, 17:1\u20136, 2012.   \n$[\\mathbf{J}\\mathbf{AD}^{+}21]$ Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust compressed sensing mri with deep generative priors. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 14938\u201314954. Curran Associates, Inc., 2021.   \n[KHR23] Frederic Koehler, Alexander Heckett, and Andrej Risteski. Statistical efficiency of score matching: The view from isoperimetry. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [KN] Syamantak Kumar and Dheeraj Nagaraj. Personal communication. [LLT23] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023. [ND21] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. CoRR, abs/2102.09672, 2021.   \n[OAS23] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n$[\\mathsf{P R S}^{+}23]$ Chirag Pabbaraju, Dhruv Rohatgi, Anish Prasad Sevekari, Holden Lee, Ankur Moitra, and Andrej Risteski. Provable benefits of score matching. CoRR, abs/2306.01993, 2023.   \n$[\\mathrm{RBL}^{+}22]$ Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022.   \n$[\\mathrm{RDN}^{+}22]$ Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022.   \n[SDME21] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[SDWMG15] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR. [SE20] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 12438\u2013 12448. Curran Associates, Inc., 2020. [Sel24] Mark Sellke. On size-independent sample complexity of relu networks. Information Processing Letters, 186:106482, 2024. $[\\mathrm{SSDK}^{+}21]$ Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. [SSXE22] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. In International Conference on Learning Representations, 2022. ", "page_idx": 11}, {"type": "text", "text": "A Sample Complexity to Achieve $(1-\\delta)$ -Quantile Accuracy ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "The goal of this section is to prove our main lemma, the sample complexity bound to learn score at a single time. More specifically, we will give a quantitative version of Lemma 1.4. ", "page_idx": 11}, {"type": "text", "text": "Lemma A.1 (Main Lemma, Quantitative Version). Let $q_{0}$ be a distribution with second moment $m_{2}^{2}$ . Let $\\phi_{\\theta}(\\cdot)$ be the fully connected neural network with ReLU activations parameterized by $\\theta$ , with $P$ total parameters and depth $D$ . Let $\\Theta>1$ . Suppose there exists some weight vector $\\theta^{*}$ with $\\lVert\\theta^{*}\\rVert_{\\infty}\\leq\\Theta$ such that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{t}}{\\mathbb{E}}\\left[\\|\\phi_{\\theta^{*}}(x)-s_{t}(x)\\|^{2}\\right]\\leq\\frac{\\delta_{s c o r e}\\cdot\\delta_{t r a i n}\\cdot\\varepsilon^{2}}{C\\cdot\\sigma_{t}^{2}}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "for a sufficiently large constant $C$ . By taking m i.i.d. samples from $q_{0}$ to train score $s_{t}$ , when ", "page_idx": 11}, {"type": "equation", "text": "$$\nm>\\widetilde{O}\\left(\\frac{(d+\\log\\frac{1}{\\delta_{t r a i n}})\\cdot P D}{\\varepsilon^{2}\\delta_{s c o r e}}\\cdot\\log\\left(\\frac{\\operatorname*{max}(m_{2},1)\\cdot\\Theta}{\\delta_{t r a i n}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "the empirical minimizer $\\phi_{\\widehat{\\theta}}$ of the score matching objective used to estimate $s_{t}$ (over $\\phi_{\\theta}$ with $\\lVert\\theta\\rVert_{\\infty}\\leq\\Theta)$ satisfies ", "page_idx": 11}, {"type": "equation", "text": "$$\nD_{q t}^{\\delta_{s c o r e}}(\\phi_{\\widehat{\\theta}},s_{t})\\leq\\varepsilon/\\sigma_{t}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "with probability $1-\\delta_{t r a i n}$ ", "page_idx": 11}, {"type": "text", "text": "In order to prove the lemma, we first consider the case when learning the score over a finite function class $\\mathcal{H}$ . ", "page_idx": 11}, {"type": "text", "text": "A.1 Score Estimation for Finite Function Class ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "The main result (Lemma A.2) of this section shows that if there is a function in the function class $\\mathcal{H}$ that approximates the score well in our $D_{p}^{\\delta}$ sense (see (6)), then the score-matching objective can learn this function using a number of samples that is independent of the domain size or the maximum value of the score. ", "page_idx": 11}, {"type": "text", "text": "Notation. Fix a time $t$ . For the purposes of this section, let $q:=q_{t}$ be the distribution at time $t$ , let $\\sigma:=\\sigma_{t}$ be the smoothing level for time $t$ , and let $s:=s_{t}$ be the score function for time $t$ . For $m$ samples $y_{i}\\sim q_{0}$ and $z_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$ , let $x_{i}=e^{-t}y_{i}+z_{i}\\sim q_{t}$ . ", "page_idx": 11}, {"type": "text", "text": "We use $\\widehat{\\mathbb{E}}[f(x,y,z)]$ to denote the empirical expectation $\\textstyle{\\frac{1}{m}}\\sum_{i=1}^{m}f(x_{i},y_{i},z_{i})$ . ", "page_idx": 11}, {"type": "text", "text": "We now state the score matching algorithm. ", "page_idx": 12}, {"type": "text", "text": "Algorithm 1 Empirical score estimation for $s$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Input: Distribution $q_{0},\\,y_{1},\\dotsc,\\,y_{m}\\sim q_{0}$ , set of hypothesis score function $\\mathcal{H}=\\{\\tilde{s}_{i}\\}$ , smoothing level $\\sigma$ . ", "page_idx": 12}, {"type": "text", "text": "1. Take $m$ independent samples $z_{i}\\sim N(0,\\sigma^{2}I_{d})$ , and let $x_{i}=e^{-t}y_{i}+z_{i}$ . ", "page_idx": 12}, {"type": "text", "text": "2. For each $\\tilde{s}\\in\\mathcal H$ , let ", "page_idx": 12}, {"type": "equation", "text": "$$\nl(\\tilde{s})=\\frac{1}{m}\\sum_{i=1}^{m}\\biggl\\|\\tilde{s}(x_{i})-\\frac{-z_{i}}{\\sigma^{2}}\\biggr\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "3. Let $\\begin{array}{r}{\\hat{s}=\\arg\\operatorname*{min}_{\\tilde{s}\\in\\mathcal{H}}l(\\tilde{s})}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "4. Return s\u02c6 ", "page_idx": 12}, {"type": "text", "text": "Lemma A.2 (Score Estimation for Finite Function Class). For any distribution $q_{0}$ and time $t>0$ , consider the $\\sigma_{t}$ -smoothed version $q_{t}$ with associated score $s_{t}$ . For any finite set $\\mathcal{H}$ of candidate score functions. If there exists some $s^{*}\\in\\mathcal{H}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{t}}{\\mathbb{E}}\\left[\\left\\|s^{*}(x)-s_{t}(x)\\right\\|_{2}^{2}\\right]\\le\\frac{\\delta_{s c o r e}\\cdot\\delta_{t r a i n}\\cdot\\varepsilon^{2}}{C\\cdot\\sigma_{t}^{2}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for a sufficiently large constant $C$ , then using $\\begin{array}{r}{m>\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}\\delta_{s c o r e}}(d+\\log\\frac{1}{\\delta_{t r a i n}})\\log\\frac{|\\mathcal{H}|}{\\delta_{t r a i n}}\\right)}\\end{array}$ samples, the empirical minimizer $\\hat{s}$ of the score matching objective used to estimate $s_{t}$ satisfies ", "page_idx": 12}, {"type": "equation", "text": "$$\nD_{q_{t}}^{\\delta_{s c o r e}}(\\hat{s},s_{t})\\leq\\varepsilon/\\sigma_{t}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with probability $1-\\delta_{t r a i n}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Per the notation discussion above, we set $s=s_{t}$ and $\\sigma=\\sigma_{t}$ . ", "page_idx": 12}, {"type": "text", "text": "Denote ", "page_idx": 12}, {"type": "equation", "text": "$$\nl(s,x,z):=\\left\\|s(x)-\\frac{-z}{\\sigma^{2}}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We will show that for all $\\tilde{s}$ such that $D_{q}^{\\delta_{\\mathrm{score}}}(\\tilde{s},s)>\\varepsilon/\\sigma$ , with probability $1-\\delta_{\\mathrm{train}}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbb{E}}\\left[l(\\tilde{s},x,z)-l(s^{*},x,z)\\right]>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "so that the empirical minimizer $\\hat{s}$ is guaranteed to have ", "page_idx": 12}, {"type": "equation", "text": "$$\nD_{q}^{\\delta_{\\mathrm{score}}}(\\hat{s},s)\\leq\\varepsilon/\\sigma.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\widetilde s,x,z)-l(s^{*},x,z)=\\left\\|\\widetilde s(x)-\\frac{-z}{\\sigma^{2}}\\right\\|^{2}-\\left\\|s^{*}(x)-\\frac{-z}{\\sigma^{2}}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left(\\left\\|\\widetilde s(x)-\\frac{-z}{\\sigma^{2}}\\right\\|^{2}-\\left\\|s(x)-\\frac{-z}{\\sigma^{2}}\\right\\|^{2}\\right)-\\left\\|s^{*}(x)-s(x)\\right\\|^{2}-2\\langle s^{*}(x)-s(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that by Markov\u2019s inequality, with probability $1-\\delta_{\\mathrm{train}}/3$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}_{x}\\left[\\|s^{*}(x)-s(x)\\|^{2}\\right]\\le\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}}{30\\cdot\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By Lemma A.3, with probability $1-\\delta_{\\mathrm{train}}/3$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle\\right]\\leq\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}}{100\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Also, by Corollary A.5, with probability $1-\\delta_{\\mathrm{train}}/3$ , for all $\\tilde{s}\\in\\mathcal H$ that satisfy $D_{q}^{\\delta_{\\mathrm{score}}}(\\tilde{s},s)>\\varepsilon/\\sigma$ simultaneously, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{E}}\\left[\\left\\Vert\\tilde{s}(x)-\\frac{-z}{\\sigma^{2}}\\right\\Vert^{2}-\\left\\Vert s(x)-\\frac{-z}{\\sigma^{2}}\\right\\Vert^{2}\\right]\\ge\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{16\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Plugging in everything into equation (16), we have, with probability $1-\\delta_{\\mathrm{train}}$ , for all $\\tilde{s}\\in\\mathcal H$ with $D_{q}^{\\delta_{\\mathrm{scorc}}}(\\tilde{s},s)>\\varepsilon/\\sigma$ simultaneously, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{E}}\\left[l(\\tilde{s},x,z)-l(s^{*},x,z)\\right]\\ge\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{16\\sigma^{2}}-\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{30\\sigma^{2}}-\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{50\\sigma^{2}}>0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as required. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.3. For any distribution $q_{0}$ and time $t\\,>\\,0,$ , consider the $\\sigma_{t}$ -smoothed version $q_{t}$ with associated score $s_{t}$ . Suppose $s^{*}$ is such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{t}}{\\mathbb{E}}\\left[\\|s^{*}(x)-s(x)\\|^{2}\\right]\\le\\frac{\\delta_{s c o r e}\\cdot\\delta_{t r a i n}\\cdot\\varepsilon^{2}}{C\\sigma_{t}^{2}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for a sufficiently large constant $C$ . Then, using $\\begin{array}{r}{m>\\widetilde O\\left(\\frac{1}{\\varepsilon^{2}\\delta_{s c o r e}}\\left(d+\\log\\frac{1}{\\delta_{t r a i n}}\\right)\\right)}\\end{array}$ samples $\\left(x_{i},y_{i},z_{i}\\right)$ where $y_{i}\\sim q_{0}$ , $z_{i}\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ and $x_{i}=e^{-t}y_{i}+z_{i}\\sim q_{t}$ , we have with probability $1-\\delta_{t r a i n},$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle\\right]\\leq\\frac{\\delta_{s c o r e}\\varepsilon^{2}}{1000\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Note that $s(x)=\\mathbb{E}_{z\\mid x}\\left[{\\frac{-z}{\\sigma^{2}}}\\right]$ so that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle\\right]=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Also, for any $\\delta$ , with probability $1-\\delta$ , by Lemmas F.4 and F.5, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\leq C\\frac{d+\\log\\frac{1}{\\delta}}{\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some constant $C$ . Let $E$ be the event that $\\begin{array}{r}{\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\leq C\\frac{d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{score}}\\delta_{\\mathrm{train}}}}{\\sigma^{2}}}\\end{array}$ d+log\u03b522\u03b4score\u03b4train. Since \u2225s(x)\u2212\u22122z\u22252 is subexponential with parameter $\\sigma^{2}$ and has mean $\\textstyle O\\left({\\frac{d}{\\sigma^{2}}}\\right)$ by the above, by Lemma F.7, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert s(x)-\\frac{-z}{\\sigma^{2}}\\Vert^{2}|\\bar{E}\\right]\\lesssim\\frac{d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{score}}\\delta_{\\mathrm{train}}}}{\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, since $\\begin{array}{r l}{\\mathbb{P}[E]}&{{}\\ge\\quad1\\;-\\;\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}\\delta_{\\mathrm{rain}}}{100(d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{score}}\\delta_{\\mathrm{rain}}})}\\quad\\ge\\quad\\frac{1}{2}\\quad\\mathrm{and}\\;\\;\\mathbb{E}\\left[\\|s^{*}(x)-s(x)\\|^{2}|\\bar{E}\\right]}\\end{array}\\;\\le\\;2$ $\\mathbb{E}\\left[\\Vert s^{*}(x)-s(x)\\Vert^{2}\\right]/\\mathbb{P}[\\bar{E}]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle|E\\right]}\\\\ &{=-\\frac{\\mathbb{E}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle|E\\right]\\mathbb{P}[\\tilde{E}]}{\\mathbb{P}[E]}}\\\\ &{\\le\\frac{\\sqrt{\\mathbb{E}\\left[\\|s^{*}(x)-s(x)\\|^{2}\\right]\\mathbb{E}\\left[\\|s(x)-\\frac{z}{\\sigma^{2}}\\|^{2}\\right]\\mathbb{P}[\\tilde{E}]}}{\\mathbb{P}[E]}}\\\\ &{\\le2\\sqrt{\\mathbb{E}[\\|s^{*}(x)-s(x)\\|^{2}]\\mathbb{E}\\left[\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\tilde{E}\\right]\\mathbb{P}[\\tilde{E}]}}\\\\ &{\\lesssim\\frac{1}{\\sigma^{2}}\\sqrt{\\frac{\\varepsilon^{2}\\delta_{\\mathrm{scov}}\\delta_{\\mathrm{train}}}{C}}\\cdot\\sqrt{d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{scov}}\\delta_{\\mathrm{train}}}}\\cdot\\sqrt{\\frac{\\varepsilon^{2}\\delta_{\\mathrm{scov}}\\delta_{\\mathrm{train}}}{100(d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{scov}}\\delta_{\\mathrm{train}}})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\leq\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}\\delta_{\\mathrm{train}}}{\\sqrt{C}\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle^{2}|E\\right]\\leq\\mathbb{E}\\left[\\|s^{*}(x)-s(x)\\|^{2}\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}|E\\right]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\mathbb{E}\\left[\\|s^{*}(x)-s(x)\\|^{2}(\\frac{d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{sove}}\\delta_{\\mathrm{train}}}}{\\sigma^{2}})|E\\right]}\\\\ &{\\qquad\\qquad\\lesssim\\frac{\\delta_{\\mathrm{score}}\\cdot\\delta_{\\mathrm{train}}\\cdot\\varepsilon^{2}}{C\\sigma^{2}}\\cdot\\frac{d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{scove}}\\delta_{\\mathrm{train}}}}{\\sigma^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So, by Chebyshev\u2019s inequality, with probability $1-\\delta_{\\mathrm{train}}/2$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{E}}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle|E\\right]\\lesssim\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}\\delta_{\\mathrm{train}}}{\\sqrt{C}\\sigma^{2}}+\\frac{1}{\\sigma^{2}}\\sqrt{\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}\\cdot(d+\\log\\frac{d}{\\varepsilon^{2}\\delta_{\\mathrm{scos}}\\delta_{\\mathrm{train}}})}{C m}}\\lesssim\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon}{\\sqrt{C}\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for our choice of m. Since E holds except with probability 100(d+\u03b52lo\u03b4sgcor\u03b5e2\u03b4\u03b4trsaciondre\u03b4  ) , we have with probability $1-\\delta_{\\mathrm{train}}$ in total, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\langle s^{*}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle\\right]\\leq\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}}{1000\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for sufficiently large constant $C$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.4. Consider any set $\\mathcal{F}$ of functions $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ such that for all $f\\in\\mathcal F$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x\\sim p}\\left[\\lVert f(x)\\rVert>\\varepsilon/\\sigma\\right]>\\delta_{s c o r e}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, with $\\begin{array}{r}{m\\,>\\,\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}\\delta_{s c o r e}}(d+\\log\\frac{1}{\\delta_{t r a i n}})\\log\\frac{|\\mathcal{F}|}{\\delta_{t r a i n}}\\right)}\\end{array}$ samples drawn in Algorithm $^{\\,l}$ , we have with probability $1-\\delta_{t r a i n}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}-2\\left(\\frac{-z_{i}}{\\sigma^{2}}-\\mathbb{E}\\left[\\frac{-z}{\\sigma^{2}}|x_{i}\\right]\\right)^{T}f(x_{i})+\\frac{1}{2}\\|f(x_{i})\\|^{2}\\geq\\frac{\\delta_{s c o r e}\\cdot\\varepsilon^{2}}{16\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "holds for all $f\\in\\mathcal F$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Define ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{f}(x,z):=-2\\left({\\frac{-z}{\\sigma^{2}}}-\\mathbb{E}\\left[{\\frac{-z}{\\sigma^{2}}}|x\\right]\\right)^{T}f(x)+{\\frac{1}{2}}\\|f(x)\\|^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We want to show that $h_{f}$ has ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[h_{f}(x,z)\\right]:=\\frac{1}{m}\\sum_{i=1}^{m}h_{f}(x_{i},z_{i})\\geq\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{16\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $f\\in\\mathcal F$ with probability $1-\\delta_{\\mathrm{train}}$ . ", "page_idx": 14}, {"type": "text", "text": "Let $\\begin{array}{r}{B=O\\left(\\frac{\\sqrt{d+\\log\\frac{m}{\\varepsilon\\delta_{\\mathrm{score}}\\delta_{\\mathrm{train}}}}}{\\sigma}\\right)}\\end{array}$ For $f\\in\\mathcal F$ , let ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{f}(x,z)=\\left\\{\\!\\!\\!\\begin{array}{l l}{B^{2}}&{{\\mathrm{if~}\\left\\|f(x)\\right\\|\\geq10B}}\\\\ {h_{f}(x,z)}&{{\\mathrm{otherwise}}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "be a clipped version of $h_{f}(x,z)$ . We will show that for our chosen number of samples $m$ , the following hold with probability $1-\\delta_{\\mathrm{train}}$ simultaneously: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1.\\ \\mathrm{For\\,all}\\ i,\\ \\parallel\\frac{-z_{i}}{\\sigma^{2}}\\parallel\\le B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "2. For all $i$ $,\\left\\|\\mathbb{E}[\\frac{-z}{\\sigma^{2}}\\left|x_{i}\\right|]\\right\\|\\leq B$ ", "page_idx": 15}, {"type": "text", "text": "To show that these together imply (17), note that whenever $g_{f}(x_{i},z_{i})\\neq h_{f}(x_{i},z_{i})$ , $\\|f(x_{i})\\|\\geq10B$ . So, since $\\|\\frac{-z_{i}}{\\sigma^{2}}\\|\\le B$ and $\\begin{array}{r}{\\|\\mathbb{E}[\\frac{-z}{\\sigma^{2}}|x_{i}]\\|\\leq B}\\end{array}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\iota_{f}(x_{i},z_{i})=-2\\left(\\frac{-z_{i}}{\\sigma^{2}}-\\mathbb{E}\\left[\\frac{-z}{\\sigma^{2}}|x_{i}\\right]\\right)^{T}f(x_{i})+\\frac{1}{2}\\|f(x_{i})\\|^{2}\\geq-4B\\|f(x_{i})\\|+\\frac{1}{2}\\|f(x_{i})\\|^{2}\\geq B^{2}\\geq\\frac{1}{2}\\|f(x_{i})\\|^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So under conditions $1,2,3$ , for all $f\\in\\mathcal F$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{E}}[h_{f}(x,z)]\\ge\\hat{\\mathbb{E}}[g_{f}(x,z)]\\ge\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{16\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So it just remains to show that conditions $1,2,3$ hold with probability $1-\\delta_{\\mathrm{train}}$ simultaneously. ", "page_idx": 15}, {"type": "text", "text": "1. For all i, $\\left\\|{\\frac{-z_{i}}{\\sigma^{2}}}\\right\\|\\leq B$ . Holds with probability $1-\\delta_{\\mathrm{train}}/3$ by Lemma F.5 and the union bound. ", "page_idx": 15}, {"type": "text", "text": "2. For all i, $\\Big\\|\\mathbb{E}\\left[\\frac{-z}{\\sigma^{2}}\\mid x_{i}\\right]\\Big\\|\\leq B.$ . Holds with probability $1-\\delta_{\\mathrm{train}}/3$ by Lemma F.6 and the union bound. ", "page_idx": 15}, {"type": "text", "text": "3. $\\begin{array}{r}{\\hat{\\mathbb{E}}[g_{f}(x,z)]\\ge\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{16\\sigma^{2}}}\\end{array}$ for all $f\\in\\mathcal F$ . ", "page_idx": 15}, {"type": "text", "text": "Let $E$ be the event that 1. and 2. hold. Let $a_{i}\\,=\\,\\operatorname*{min}(\\|f(x_{i})\\|,10B)$ . We proceed in multiple steps. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Conditioned on $E$ , $|g_{f}(x_{i},z_{i})|\\lesssim B^{2}$ . If $\\|f(x_{i})\\|\\ \\geq\\ 10B$ , $|g_{f}(x_{i},z_{i})|\\;=\\;B^{2}$ by definition. On the other hand, when $\\|f(x_{i})\\|<10B$ , since we condition on $E$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|g_{f}(x_{i},z_{i})\\right|=\\left|h_{f}(x_{i},z_{i})\\right|=\\left|-2\\left(\\frac{-z_{i}}{\\sigma^{2}}-\\mathbb{E}\\left[\\frac{-z}{\\sigma^{2}}|x_{i}|\\right]\\right)^{T}f(x_{i})+\\frac{1}{2}\\|f(x_{i})\\|^{2}\\right|\\lesssim B^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\mathbb{E}\\left[g_{f}(x_{i},z_{i})|E,a_{i}\\right]\\gtrsim a_{i}^{2}-O(\\delta_{\\mathrm{train}}B^{2})$ . ", "page_idx": 15}, {"type": "text", "text": "First, note that by definition of $g_{f}(x,z)$ , for $a_{i}=10B$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[g_{f}(x_{i},z_{i})|a_{i}=10B\\right]=B^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, for $a_{i}<10B$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[g_{f}(x_{i},z_{i})\\middle|a_{i}\\right]=\\mathbb{E}\\left[h_{f}(x_{i},z_{i})\\middle|a_{i}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{x_{i}\\mid\\|f(x_{i})\\|=a_{i}}{\\mathbb{E}}\\left[\\underset{-z\\mid x_{i}}{\\mathbb{E}}\\left[h_{f}(x_{i},z)\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z|x}\\left[h_{f}(x,z)\\right]=\\frac{1}{2}\\|f(x)\\|^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So, for $a<10B$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[g_{f}(x_{i},z_{i})|a_{i}\\right]=\\frac{1}{2}a_{i}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now let ${g}_{f}^{\\mathrm{clip}}(x_{i},z_{i})$ be a clipped version of $g_{f}(x_{i},z_{i})$ , clipped to $\\pm C B^{2}$ for sufficiently large constant $C$ . We have, by above, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[g_{f}(x_{i},z_{i})|a_{i},E]=\\mathbb{E}[g_{f}^{\\mathrm{clip}}(x_{i},z_{i})|a_{i},E]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "But, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[g_{f}^{\\mathrm{clip}}(x_{i},z_{i})|a_{i},E]\\ge\\mathbb{E}[g_{f}^{\\mathrm{clip}}(x_{i},z_{i})|a_{i}]-O(\\delta_{\\mathrm{train}}B^{2})}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim a_{i}^{2}-O(\\delta_{\\mathrm{train}}B^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\mathrm{Var}(g_{f}(x_{i},z_{i})|a_{i},E)\\lesssim a_{i}^{2}B^{2}$ . For $a_{i}=10B$ , we have, by definition of $g_{f}(x,z)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}(g_{f}(x_{i},z_{i})|a_{i},E)\\lesssim B^{4}\\lesssim a_{i}^{2}B^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, for $a_{i}<10B$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{Var}(g_{f}(x_{i},z_{i})|a_{i},E)\\leq\\mathbb{E}\\left[g_{f}(x_{i},z_{i})^{2}|a_{i},E\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\left(-2\\left(\\frac{-z_{i}}{\\sigma^{2}}-\\mathbb{E}\\left[\\frac{-z}{\\sigma^{2}}|x_{i}\\right]\\right)^{T}f(x_{i})+\\frac{1}{2}\\|f(x_{i})\\|^{2}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim a_{i}^{2}B^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by Cauchy-Schwarz. ", "page_idx": 16}, {"type": "text", "text": "\u2022 With probability $1-\\delta_{\\mathrm{train}}/3$ , for all $\\begin{array}{r}{f\\in\\mathcal{F},\\widehat{\\mathbb{E}}[g_{f}(x_{i},z_{i})]\\gtrsim\\Omega\\left(\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}}{\\sigma^{2}}\\right)}\\end{array}$ Using the above, by Bernstein\u2019s inequality, with probability $1-\\delta_{\\mathrm{train}}/6$ , ", "page_idx": 16}, {"type": "text", "text": "${\\widehat{\\mathbb{E}}}\\left[g_{f}(x_{i},z_{i})|a_{i},E\\right]\\gtrsim\\frac{1}{n}\\sum_{i=1}^{n}a_{i}^{2}-O(\\delta_{\\operatorname{tran}}B^{2})-\\frac{1}{n}B\\sqrt{\\sum_{i=1}^{n}a_{i}^{2}\\log\\frac{1}{\\delta_{\\operatorname{tran}}}}-\\frac{1}{n}B^{2}\\log\\frac{1}{\\delta_{\\operatorname{tran}}}$ Now, note that since $\\mathbb{P}_{x\\sim p_{\\sigma}}\\left[\\|f(x)\\|>\\varepsilon/\\sigma\\right]\\ge\\delta_{\\mathrm{score}}$ , we have with probability $1-$ $\\delta_{\\mathrm{train}}/6$ , for $n>O\\left(\\frac{\\log{\\frac{1}{\\delta_{\\mathrm{train}}}}}{\\delta_{\\mathrm{score}}}\\right)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}a_{i}^{2}\\geq\\Omega\\left(\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}}{\\sigma^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So, for $\\begin{array}{r}{n>O\\left(\\frac{B^{2}\\cdot\\sigma^{2}\\log\\frac{1}{\\delta_{\\mathrm{train}}}}{\\varepsilon^{2}\\delta_{\\mathrm{score}}}\\right)}\\end{array}$ , we have, with probability $1-\\delta_{\\mathrm{train}}/3$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[g_{f}(x_{i},z_{i})|E\\right]\\gtrsim\\Omega\\left(\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}}{\\sigma^{2}}\\right)-O(\\delta_{\\mathrm{train}}B^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rescaling so that $\\begin{array}{r}{\\delta_{\\mathrm{train}}\\le O\\left(\\frac{\\varepsilon^{2}\\delta_{\\mathrm{scorc}}}{\\sigma^{2}\\cdot B^{2}}\\right)}\\end{array}$ , for $\\begin{array}{r}{n>O\\left(\\frac{B^{2}\\cdot\\sigma^{2}\\log{\\frac{B^{2}\\cdot\\sigma^{2}}{\\varepsilon^{2}\\delta_{\\mathrm{score}}\\delta_{\\mathrm{train}}}}}{\\varepsilon^{2}\\cdot\\delta\\mathrm{score}}\\right).}\\end{array}$ , we have, with probability $1-\\delta_{\\mathrm{train}}/3$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[g_{f}(x_{i},z_{i})|E\\right]\\gtrsim\\Omega\\left(\\frac{\\varepsilon^{2}\\delta_{\\mathrm{score}}}{\\sigma^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining with 1. and 2. gives the claim for a single $f\\in\\mathcal F$ . Union bounding over the size of $\\mathcal{F}$ gives the claim. ", "page_idx": 16}, {"type": "text", "text": "Corollary A.5. Let $\\mathcal{H}_{b a d}$ be a set of score functions such that for all $\\tilde{s}\\in\\mathcal{H}_{b a d},$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{q}^{\\delta_{s c o r e}}(\\tilde{s},s)>\\varepsilon/\\sigma.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, for $\\begin{array}{r}{m\\,>\\,\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}\\delta_{s c o r e}}(d+\\log\\frac{1}{\\delta_{t r a i n}})\\log\\frac{|\\mathcal{H}_{b a d}|}{\\delta_{t r a i n}}\\right)}\\end{array}$ samples drawn by Algorithm $^{\\,l}$ , we have with probability $1-\\delta_{t r a i n};$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\|\\tilde{s}(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}-\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\right]\\ge\\frac{\\delta_{s c o r e}\\varepsilon^{2}}{16\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $\\tilde{s}\\in\\mathcal{H}_{b a d}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We have, for $f(x):=\\tilde{s}(x)-s(x)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}-\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}=\\|f(x)+(s(x)-\\frac{-z}{\\sigma^{2}})\\|^{2}-\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\|f(x)\\|^{2}+2(s(x)-\\frac{-z}{\\sigma^{2}})^{T}f(x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\|f(x)\\|^{2}-2(\\frac{-z}{\\sigma^{2}}-\\mathbb{E}[\\frac{-z}{\\sigma^{2}}|x])^{T}f(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since $\\begin{array}{r}{s(x)\\,=\\,\\mathbb{E}\\left[\\frac{-z}{\\sigma^{2}}|x\\right]}\\end{array}$ by Lemma F.1. Then, by definition, for $s\\,\\in\\,{\\mathcal{H}}_{\\mathrm{bad}}$ , for the associated $f$ , $\\mathbb{P}[\\|f(x)\\|>\\varepsilon/\\sigma]>\\delta_{\\mathrm{score}}$ . So, by Lemma A.4, the claim follows. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "A.2 Score Training for Neural Networks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Now we are ready to apply the finite function class to neural networks by a net argument. In particular, we first prove a version that uses a Frobenious norm to bound the weight vector. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.6. Let $q_{0}$ be a distribution with second moment $m_{2}^{2}$ . Let $\\phi_{\\theta}(\\cdot)$ be the fully connected neural network with ReLU activations parameterized by $\\theta_{i}$ , with $P$ total parameters and depth $D$ . Let $\\Theta>1$ . Suppose there exists some weight vector $\\theta^{*}$ with $\\|\\theta^{*}\\|_{F}\\leq\\Theta$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{t}}{\\mathbb{E}}\\left[\\|\\phi_{\\theta^{*}}(x)-s_{t}(x)\\|^{2}\\right]\\leq\\frac{\\delta_{s c o r e}\\cdot\\delta_{t r a i n}\\cdot\\varepsilon^{2}}{C\\cdot\\sigma_{t}^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for a sufficiently large constant $C$ . By taking $m$ i.i.d. samples from $q_{0}$ to train score $s_{t}$ , when ", "page_idx": 17}, {"type": "equation", "text": "$$\nm>\\widetilde{O}\\left(\\frac{(d+\\log\\frac{1}{\\delta_{t r a i n}})\\cdot P D}{\\varepsilon^{2}\\delta_{s c o r e}}\\cdot\\log\\left(\\frac{\\operatorname*{max}(m_{2},1)\\cdot\\Theta}{\\delta_{t r a i n}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "the empirical minimizer $\\phi_{\\widehat{\\theta}}$ of the score matching objective used to estimate $s_{t}$ (over $\\phi_{\\theta}$ with $\\|\\theta\\|_{F}\\leq\\Theta)$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{q_{t}}^{\\delta_{s c o r e}}(\\phi_{\\widehat{\\theta}},s_{t})\\leq\\varepsilon/\\sigma_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability $1-\\delta_{t r a i n}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Per the notation discussion above, we set $s~=~s_{t}$ and $\\sigma_{\\mathrm{~\\,~}}=\\sigma_{t}$ . Note that $\\sigma\\,<\\,1$ since $\\sigma_{t}^{2}=1-e^{-2t}$ . ", "page_idx": 17}, {"type": "text", "text": "For any function $f$ denote ", "page_idx": 17}, {"type": "equation", "text": "$$\nl(f,x,z):=\\left\\|f(x)-{\\frac{-z}{\\sigma^{2}}}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we will show that for every $\\widetilde{\\theta}$ with $\\|{\\widetilde{\\theta}}\\|_{F}\\leq\\Theta$ such that $D_{q}^{\\delta_{\\mathrm{score}}}(\\phi_{\\widetilde{\\theta}},s)\\;>\\;\\varepsilon/\\sigma$ , with probability 1 \u2212\u03b4train, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[l(\\phi_{\\widetilde{\\theta}},x,z)-l(\\phi_{\\theta^{*}},x,z)\\right]>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so that the empirical minimizer $\\phi_{\\widehat{\\theta}}$ is guaranteed to have ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{q}^{\\delta_{\\mathrm{score}}}(\\phi_{\\widehat{\\theta}},s)\\leq\\varepsilon/\\sigma.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "First, note that since the ReLU activation is contractive, the total Lipschitzness of $\\phi_{\\theta}$ is at most the product of the spectral norm of the weight matrices at each layer. For any $\\theta$ , consider\u03b8  such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\theta}-\\theta\\|_{F}\\leq\\frac{\\tau}{\\sigma D\\Theta^{D-1}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $M_{1},\\ldots,M_{D}$ be the weight matrices at each layer of the neural net $\\phi_{\\theta}$ , and let $\\widetilde{M}_{1},\\ldots,\\widetilde{M}_{D}$ be the corresponding matrices of $\\phi_{\\widetilde{\\theta}}$ . ", "page_idx": 17}, {"type": "text", "text": "We now show that $\\left\\|\\phi_{\\widetilde{\\theta}}(x)-\\phi_{\\theta}(x)\\right\\|$ is small, using a hybrid argument. Define $y_{i}$ to be the output of a neural network with weight matrices $M_{1},\\ldots,M_{i},\\widetilde{M}_{i+1},\\ldots,\\widetilde{M}_{D}$ on input $x$ , so $y_{0}=\\phi_{\\widetilde{\\theta}}(x)$ and $y_{D}=\\phi_{\\theta}(x)$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y_{i}-y_{i+1}\\|\\leq\\|x\\|\\cdot\\left(\\displaystyle\\prod_{j\\leq i}\\|M_{j}\\|\\right)\\cdot\\left\\|M_{i+1}-\\widetilde{M}_{i+1}\\right\\|\\cdot\\left(\\displaystyle\\prod_{j>i+1}\\left\\|\\widetilde{M}_{j}\\right\\|\\right)}\\\\ &{\\qquad\\qquad\\leq\\|x\\|\\,\\Theta^{D-1}\\left\\|\\widetilde{\\theta}-\\theta\\right\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and so ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\phi_{\\widetilde{\\theta}}(x)-\\phi_{\\theta}(x)\\right\\|_{2}=\\left\\|y_{0}-y_{D}\\right\\|\\leq\\sum_{i=0}^{D-1}\\left\\|y_{i}-y_{i+1}\\right\\|\\leq\\left\\|x\\right\\|D\\Theta^{D-1}\\left\\|\\widetilde{\\theta}-\\theta\\right\\|_{F}\\leq\\left\\|x\\right\\|\\cdot\\tau/\\sigma.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the dimensionality of $\\theta$ is $P$ . So, we can construct a $\\frac{\\tau}{\\sigma D\\Theta^{D-1}}$ -net $N$ over the set $\\lbrace\\theta:$ \u2225\u03b8\u2225F \u2264\u0398} of size O \u03c3D\u0398\u03c4D\u22121 , so that for any $\\theta$ with $\\|\\theta\\|_{F}\\leq\\Theta$ , there exists ${\\widetilde{\\theta}}\\in N$ with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lVert\\phi_{\\widetilde{{\\boldsymbol{\\theta}}}}({\\boldsymbol{x}})-\\phi_{{\\boldsymbol{\\theta}}}({\\boldsymbol{x}})\\rVert_{2}\\leq(\\tau/\\sigma)\\cdot\\lVert{\\boldsymbol{x}}\\rVert\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\mathcal{\\widetilde{H}}=\\left\\{\\phi_{\\widetilde{\\theta}}:\\widetilde{\\theta}\\in N\\right\\}$ . Then, we have that for every $\\theta$ with $\\|\\theta\\|_{F}\\leq\\Theta$ , there exists $h\\in\\mathcal H$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\|h(x)-\\phi_{\\theta}(x)\\|^{2}\\right]\\leq(\\tau/\\sigma)^{2}\\cdot\\frac{1}{m}\\sum_{i=1}^{m}\\|x_{i}\\|^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, choose any $\\widetilde{\\theta}$ with $\\|\\widetilde{\\theta}\\|_{F}\\leq\\Theta$ and $D_{q}^{\\delta_{\\mathrm{score}}}(\\phi_{\\widetilde{\\theta}},s)>\\varepsilon/\\sigma$ , and let $\\widetilde{h}\\in\\mathcal{H}$ satisfy the above for $\\widetilde{\\theta}$ . We will set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tau=\\frac{C^{\\prime}\\varepsilon^{2}\\delta_{\\mathrm{score}}}{\\Theta^{D}\\left(m\\cdot\\mathrm{max}(m_{2}^{2},1)+(d+\\log\\frac{m}{\\delta_{\\mathrm{train}}})\\right)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for small enough constant C\u2032. So, |H| = O \u03c3D\u0398\u03c4D\u22121 $\\begin{array}{r}{|\\mathcal{H}|\\,=\\,O\\left(\\frac{\\sigma D\\Theta^{D-1}}{\\tau}\\right)^{P}\\,<\\,O\\left(\\frac{D\\Theta^{2D-1}(m\\cdot m_{2}^{2}+(d+\\log\\frac{m}{\\delta_{\\mathrm{train}}})}{\\varepsilon^{2}\\delta_{\\mathrm{score}}}\\right)^{P},\\,}\\end{array}$ since $\\sigma<1$ . ", "page_idx": 18}, {"type": "text", "text": "So, our final choice of $m$ satisfies $\\begin{array}{r}{m>\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}\\delta_{\\mathrm{score}}}\\left(d+\\log\\frac{1}{\\delta_{\\mathrm{train}}}\\right)\\log\\frac{|\\mathcal{H}|}{\\delta_{\\mathrm{train}}}\\right)\\!.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l(\\phi_{\\widetilde{\\theta}},x,z)-l(\\phi_{\\theta^{*}},x,z)}\\\\ &{=\\|\\phi_{\\widetilde{\\theta}}(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}-\\|\\phi_{\\theta^{*}}(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}}\\\\ &{=\\|\\phi_{\\widetilde{\\theta}}(x)-\\widetilde{h}(x)\\|^{2}+2\\langle\\phi_{\\widetilde{\\theta}}(x)-\\widetilde{h}(x),\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\rangle+\\|\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}}\\\\ &{\\quad-\\,\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}-\\|\\phi_{\\theta^{*}}(x)-s(x)\\|^{2}-2\\langle\\phi_{\\theta^{*}}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, by Corollary A.5, for our choice of $m$ , with probability $1-\\delta_{\\mathrm{train}}/4$ for every $h\\in\\mathcal H$ with $D_{q}^{\\delta_{\\mathrm{score}}/2}(h,s)>\\varepsilon/(2\\sigma)$ simultaneously, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\|h(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}-\\|s(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\right]\\geq\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{128\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Markov\u2019s inequality, with probability $1-\\delta_{\\mathrm{train}}/4$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\|\\phi_{\\theta^{*}}(x)-s(x)\\|^{2}\\right]\\|^{2}\\right]\\le\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}}{250\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma A.3, with probability $1-\\delta_{\\mathrm{train}}/4$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\langle\\phi_{\\theta^{*}}(x)-s(x),s(x)-\\frac{-z}{\\sigma^{2}}\\rangle\\right]\\leq\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{1000\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging into (20) we have that with probability $1-3\\delta_{\\mathrm{train}}/4$ , for every $\\widetilde{h}$ satisfying $D_{q}^{\\delta_{\\mathrm{score}}/2}(\\widetilde{h},s)>$ $\\varepsilon/(\\bar{2\\sigma})$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{\\mathsf{\\Sigma}}\\big[l(\\phi_{\\tilde{\\theta}},x,z)-l(\\phi_{\\theta^{*}},x,z)\\big]\\ge\\displaystyle\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{128\\sigma^{2}}-\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}}{250\\sigma^{2}}-2\\cdot\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{1000\\sigma^{2}}+2\\langle\\phi_{\\tilde{\\theta}}(x)-\\widetilde{h}(x),\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\rangle}\\\\ &{}&{\\ge\\displaystyle\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}}{600\\cdot\\sigma^{2}}+2\\langle\\phi_{\\tilde{\\theta}}(x)-\\widetilde{h}(x),\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\rangle\\eqno(21)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we will show that $D_{q}^{\\delta_{\\mathrm{score}}/2}(\\widetilde{h},s)>\\varepsilon/(2\\sigma)$ , as well as bound the last term above, for every $\\widetilde{\\theta}$ satisfying $D_{q}^{\\delta_{\\mathrm{score}}}(\\phi_{\\widetilde{\\theta}},s)>\\varepsilon/\\sigma$ and $\\widetilde{h}\\in\\mathcal{H}$ the rounding of $\\phi_{\\widetilde{\\theta}}$ . ", "page_idx": 19}, {"type": "text", "text": "By the fact that $q_{0}$ has second moment $m_{2}^{2}$ , we have that with probability $1-\\delta$ over $x$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|x\\right\\|\\leq{\\frac{m_{2}}{\\sqrt{\\delta}}}+\\sigma\\left({\\sqrt{d}}+{\\sqrt{2\\log{\\frac{1}{\\delta}}}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now since $D_{q}^{\\delta_{\\mathrm{score}}}(\\phi_{\\widetilde{\\theta}},s)>\\varepsilon/\\sigma$ , and $\\lVert\\widetilde{h}(x)-\\phi_{\\widetilde{\\theta}}(x)\\rVert\\leq(\\tau/\\sigma)\\cdot\\lVert x\\rVert$ , we have, with probability at least $\\delta_{\\mathrm{score}}/2$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde{h}(x)-s(x)\\|\\ge\\|\\phi_{\\widetilde{\\theta}}(x)-s(x)\\|-\\|\\widetilde{h}(x)-\\phi_{\\widetilde{\\theta}}(x)\\|}\\\\ &{\\qquad\\qquad\\qquad\\ge\\varepsilon/\\sigma-(\\tau/\\sigma)\\cdot\\left(\\displaystyle{\\frac{m_{2}}{\\sqrt{\\delta_{\\mathrm{score}}/2}}}+\\sigma\\left(\\sqrt{d}+\\displaystyle{\\sqrt{2\\log\\displaystyle{\\frac{2}{\\delta_{\\mathrm{score}}}}}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\ge\\varepsilon/(2\\sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for our choice of $\\tau$ as in (19). So, we have shown that $D_{q}^{\\delta_{\\mathrm{score}}/2}(\\widetilde{h},s)>\\varepsilon/(2\\sigma)$ ", "page_idx": 19}, {"type": "text", "text": "Finally, we bound the last term in (21) above. We have by (18) and a union bound, with probability $1-\\delta\\bar{\\mathfrak{a}}\\mathfrak{a}\\mathfrak{a}\\mathfrak{/}8$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{E}}\\left[\\|\\widetilde{h}(x)-\\phi_{\\widetilde{\\theta}}(x)\\|^{2}\\right]\\lesssim(\\tau/\\sigma)^{2}\\cdot\\left(\\frac{m\\cdot m_{2}^{2}}{\\delta_{\\mathrm{train}}}+\\sigma^{2}\\left(d+\\log\\frac{m}{\\delta_{\\mathrm{train}}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\lesssim(\\tau/\\sigma)^{2}\\cdot\\left(\\frac{m\\cdot m_{2}^{2}}{\\delta_{\\mathrm{train}}}+\\left(d+\\log\\frac{m}{\\delta_{\\mathrm{train}}}\\right)\\right)\\quad\\mathrm{since~}\\sigma<1}\\\\ &{\\qquad\\qquad\\lesssim\\frac{C^{\\prime2}\\varepsilon^{4}\\delta_{\\mathrm{sore}}^{2}}{\\sigma^{2}\\Theta^{2D}}\\cdot\\frac{1}{\\frac{m\\cdot\\operatorname*{max}(m_{2}^{2},1)}{\\delta_{\\mathrm{train}}}+\\left(d+\\log\\frac{m}{\\delta_{\\mathrm{train}}}\\right)}\\quad\\mathrm{by~(19)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, with probability $1-\\delta_{\\mathrm{train}}/8$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbb{E}}\\left[\\|\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\right]\\lesssim\\Theta^{D}\\cdot\\left(\\frac{m\\cdot m_{2}^{2}}{\\delta_{\\mathrm{train}}}+\\sigma^{2}\\left(d+\\log\\frac{m}{\\delta_{\\mathrm{train}}}\\right)\\right)+\\frac{1}{\\sigma^{2}}\\left(d+\\log\\frac{m}{\\delta_{\\mathrm{train}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So, putting the above together, with probability $1-\\delta_{\\mathrm{train}}/4$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{E}}\\left[\\|\\widetilde{h}(x)-\\phi_{\\widetilde{\\theta}}(x)\\|^{2}\\right]\\cdot\\widehat{\\mathbb{E}}\\left[\\|\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\right]\\lesssim\\frac{C^{\\prime2}\\varepsilon^{4}\\delta_{\\mathrm{score}}^{2}}{\\sigma^{2}}\\left(1+\\frac{1}{\\sigma^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\lesssim\\frac{C^{\\prime2}\\varepsilon^{4}\\delta_{\\mathrm{score}}^{2}}{\\sigma^{4}}\\quad\\mathrm{since}\\,\\sigma<1}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So, with probability $1-\\delta_{\\mathrm{train}}/4$ , for some small enough constant $C^{\\prime}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{E}}\\left[\\langle\\phi_{\\widetilde{\\theta}}(x)-\\widetilde{h}(x),\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\rangle\\right]\\geq-\\sqrt{\\widehat{\\mathbb{E}}\\left[\\|\\widetilde{h}(x)-\\phi_{\\widetilde{\\theta}}(x)\\|^{2}\\right]\\cdot\\widehat{\\mathbb{E}}\\left[\\|\\widetilde{h}(x)-\\frac{-z}{\\sigma^{2}}\\|^{2}\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq-\\frac{\\delta_{\\mathrm{score}}\\varepsilon^{2}}{2000\\cdot\\sigma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So finally, combining with (21), we have with probability $1-\\delta_{\\mathrm{train}}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[l(\\phi_{\\widetilde{\\theta}},x,z)-l(\\phi_{\\theta^{*}},x,z)\\right]\\ge\\frac{\\delta_{\\mathrm{score}}\\cdot\\varepsilon^{2}}{1000\\cdot\\sigma^{2}}>0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So, we have shown that with probability $1\\!-\\!\\delta_{\\mathrm{train}}$ , for every $\\widetilde{\\theta}$ with $\\|\\widetilde{\\theta}\\|_{F}\\leq\\Theta$ and $D_{q}^{\\delta_{\\mathrm{score}}}(\\phi_{\\widetilde{\\theta}},s)>\\varepsilon/\\sigma$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{E}\\left[l(\\phi_{\\widetilde{\\theta}},x,z)-l(\\phi_{\\theta^{*}},x,z)\\right]>0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so that the empirical minimizer $\\phi_{\\widehat{\\theta}}$ is guaranteed to have ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{q}^{\\delta_{\\mathrm{score}}}(\\phi_{\\widehat{\\theta}},s)\\leq\\varepsilon/\\sigma.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have our main lemma as a direct corollary: ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma A.1. For every $\\widetilde{\\theta}$ with $\\lvert|\\widetilde{\\theta}\\rvert|_{\\infty}\\leq\\Theta$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\theta}\\|_{F}\\leq\\sqrt{P}\\|\\widetilde{\\theta}\\|_{\\infty}\\leq\\sqrt{P}\\Theta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, by applying Lemma A.6 directly, we prove the lemma. ", "page_idx": 20}, {"type": "text", "text": "B Sampling with our score estimation guarantee ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show that diffusion models can converge to the true distribution without necessarily adhering to an $L^{2}$ bound on the score estimation error. A high probability accuracy of the score is sufficient. ", "page_idx": 20}, {"type": "text", "text": "In order to simulate the reverse process of (1) in an actual algorithm, the time was discretized into $N$ steps. The $k$ -th step ends at time $t_{k}$ , satisfying $0\\leq t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{N}=T-\\gamma$ . The algorithm stops at tN and outputs the final state xT \u2212tN . ", "page_idx": 20}, {"type": "text", "text": "To analyze the reverse process run under different levels of idealness, we consider these four specific path measures over the path space $\\mathcal{C}([0,T-\\gamma];\\mathbb{R}^{d})$ : ", "page_idx": 20}, {"type": "text", "text": "\u2022 Let $Q$ be the measure for the process that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=\\left(x_{T-t}+2s_{T-t}(x_{T-t})\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}B_{t},\\quad x_{T}\\sim q_{T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Let $Q_{\\mathrm{dis}}$ be the measure for the process that for $t\\in[t_{k},t_{k+1}]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=\\left(x_{T-t}+2s_{T-t_{k}}(x_{T-t_{k}})\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}B_{t},\\quad x_{T}\\sim q_{T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Let $\\overline{{Q}}$ be the measure for the process that for $t\\in[t_{k},t_{k+1}]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=\\left(x_{T-t}+2\\widehat{s}_{T-t_{k}}\\big(x_{T-t_{k}}\\big)\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}B_{t},\\quad x_{T}\\sim q_{T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Let $\\widehat{Q}$ be the measure for the process that for $t\\in[t_{k},t_{k+1}]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=\\left(x_{T-t}+2\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}B_{t},\\quad x_{T}\\sim{\\mathcal{N}}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To summarize, $Q$ represents the perfect reverse process of (1), $Q_{\\mathrm{dis}}$ is the discretized version of $Q,{\\overline{{Q}}}$ runs $Q_{\\mathrm{dis}}$ with an estimated score, and $\\widehat{Q}$ starts $\\overline{{Q}}$ at $\\mathcal{N}(0,I_{d})$ \u2014 effectively the actual implementable reverse process. ", "page_idx": 20}, {"type": "text", "text": "Recent works have shown that under the assumption that the estimated score function is close to the real score function in $L^{2}$ , then the output of $\\widehat{Q}$ will approximate the true distribution closely. Our next theorem shows that this assumption is in fact not required, and it shows that our score assumption can be easily integrated in a black-box way to achieve similar results. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.1 (Score Estimation guarantee). Consider an arbitrary sequence of discretization times $0\\;=\\;t_{0}\\;<\\;t_{1}\\;<\\;\\cdot\\,\\cdot\\;<\\;t_{N}\\;=\\;T\\,-\\,\\gamma,$ , and let $\\sigma_{t}\\;:=\\;\\sqrt{1-e^{-2t}}$ . Assume that for each $k\\ \\in$ $\\{0,\\ldots,N-1\\}$ , the following holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\delta/N}\\big(\\widehat{s}_{T-t_{k}},s_{T-t_{k}}\\big)\\leq\\frac{\\varepsilon}{\\sigma_{T-t_{k}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb P_{Q}\\left[\\sum_{k=0}^{N-1}\\lVert\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})-s_{T-t_{k}}(x_{T-t_{k}})\\rVert_{2}^{2}(t_{k+1}-t_{k})\\le\\varepsilon^{2}\\left(T+\\log\\frac{1}{\\gamma}\\right)\\right]\\ge1-\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Since random variable $x_{T-t_{k}}$ follows distribution $q_{T-t_{k}}$ under $Q$ , for each $k\\in\\{0,\\ldots,N-$ $1\\}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}_{Q}\\left[\\Vert\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})-s_{T-t_{k}}(x_{T-t_{k}})\\Vert\\leq\\frac{\\varepsilon}{\\sqrt{1-e^{-2(T-t_{k})}}}\\right]\\geq1-\\frac{\\delta}{N}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using a union bound over all $N$ different $\\sigma$ values, it follows that with probability at least $1-\\delta$ over $Q$ , the inequality ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\big\\|\\widehat{s}_{T-t_{k}}\\big(x_{T-t_{k}}\\big)-s_{T-t_{k}}\\big(x_{T-t_{k}}\\big)\\big\\|_{2}^{2}\\leq\\frac{\\varepsilon^{2}}{1-e^{-2(T-t_{k})}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is satisfied for every $k\\in\\{0,\\ldots,N-1\\}$ . Under this condition, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{N-1}\\left|\\hat{z}_{\\hat{x}-t_{k}}(x_{T-t_{k}})-s_{T-t_{k}}(x_{T-t_{k}})\\right|\\displaystyle\\left\\|_{2}^{2}(t_{k+1}-t_{k})\\right.}\\\\ &{\\displaystyle\\le\\sum_{k=0}^{N-1}\\frac{\\varepsilon^{2}}{1-e^{-2(T-t_{k})}}(t_{k+1}-t_{k})}\\\\ &{\\displaystyle\\le\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\frac{\\varepsilon^{2}}{1-e^{-2(T-t_{k})}}\\,\\mathrm{d}t}\\\\ &{\\displaystyle\\le\\int_{0}^{T-\\gamma}\\frac{\\varepsilon^{2}}{1-e^{-2(T-t_{k})}}\\,\\mathrm{d}t}\\\\ &{\\displaystyle\\le\\varepsilon^{2}\\left(T+\\log\\frac{1}{\\gamma}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, we find that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb P_{Q}\\left[\\sum_{k=0}^{N-1}\\lVert\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})-s_{T-t_{k}}(x_{T-t_{k}})\\rVert_{2}^{2}(t_{k+1}-t_{k})\\le\\varepsilon^{2}\\left(T+\\log\\frac{1}{\\gamma}\\right)\\right]\\ge1-\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma B.2 (Score estimation error to TV). Let q be an arbitrary distribution. If the score estimation satisfies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}_{Q}\\left[\\sum_{k=0}^{N-1}\\lVert\\widehat{s}_{T-t_{k}}\\left(x_{T-t_{k}}\\right)-s_{T-t_{k}}\\left(x_{T-t_{k}}\\right)\\rVert_{2}^{2}(t_{k+1}-t_{k})\\leq\\varepsilon^{2}\\right]\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then the output distribution $p_{T-t_{N}}$ of $\\widehat{Q}$ satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{T V}(q_{\\gamma},p_{T-t_{N}})\\lesssim\\delta+\\varepsilon+\\mathsf{T V}(Q,Q_{d i s})+\\mathsf{T V}(q_{T},\\mathcal{N}(0,I_{d})).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We will start by bounding the TV distance between $Q_{\\mathrm{dis}}$ and $\\overline{{Q}}$ . We will proceed by defining $\\widetilde{Q}$ and arguing that both $\\mathsf{T V}(Q_{\\mathrm{dis}},\\widetilde{Q})$ and $\\mathsf{T V}(\\widetilde{Q},\\overline{{Q}})$ are small. By the triangle inequality, this will imply that $Q$ and $\\overline{{Q}}$ are close in TV distance. ", "page_idx": 21}, {"type": "text", "text": "Defining $\\widetilde{Q}$ . For $k\\in\\{0,\\ldots,N-1\\}$ , consider event ", "page_idx": 22}, {"type": "equation", "text": "$$\nE_{k}:=\\left(\\sum_{i=0}^{k}\\lVert\\widehat{s}_{T-t_{i}}\\big(x_{T-t_{i}}\\big)-s_{T-t_{i}}(x_{T-t_{i}})\\rVert_{2}^{2}\\big(t_{i+1}-t_{i}\\big)\\leq\\varepsilon^{2}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which represents that the accumulated score estimation error along the path is at most $\\varepsilon^{2}$ for a discretized diffusion process. ", "page_idx": 22}, {"type": "text", "text": "Given $E_{k}$ , we define a version of $Q_{\\mathrm{dis}}$ that is forced to have a bounded score estimation error. Let $\\widetilde{Q}$ over $\\mathcal{C}((0,T],\\mathbb{R}^{d})$ be the law of a modified reverse process initialized at $x_{T}\\sim q_{T}$ , and for each $t\\in[t_{k},t_{k+1})$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=-\\left(x_{T-t}+2\\widetilde{s}_{T-t_{k}}(x_{T-t_{k}})\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}B_{t},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{s}_{T-t_{k}}(x_{T-t_{k}}):=\\left\\{\\begin{array}{l l}{s_{T-t_{k}}(x_{T-t_{k}})}&{E_{k}\\mathrm{\\;holds},}\\\\ {\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})}&{E_{k}\\mathrm{\\;doesn't\\;holds}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This SDE guarantees that once the accumulated score error exceeds $\\varepsilon_{s c o r e}^{2}$ $E_{k}$ fails to hold), we switch from the true score to the estimated score. Therefore, we have that the following inequality always holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{N-1}\\lVert\\widetilde{s}_{T-t_{k}}(x_{T-t_{k}})-\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\rVert_{2}^{2}(t_{k+1}-t_{k})\\le\\varepsilon^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$Q_{\\bf d i s}$ and $\\widetilde{Q}$ are close. By (22), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}_{Q_{\\mathrm{ds}}}\\left[E_{0}\\wedge\\cdot\\cdot\\wedge E_{N-1}\\right]=\\mathbb{P}_{Q_{\\mathrm{ds}}}\\left[E_{N-1}\\right]\\geq\\mathbb{P}_{Q}\\left[E_{N-1}\\right]-\\mathsf{T V}(Q,Q_{\\mathrm{dis}})\\geq1-\\delta-\\mathsf{T V}(Q,Q_{\\mathrm{dis}}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that when a path $(\\underline{{x_{T-t}}})_{t\\in[0,t_{N}]}$ satisfies $E_{0}\\wedge\\cdots\\wedge E_{N-1}$ , its probability under $\\widetilde{Q}$ is at least its probability under $Q_{\\mathrm{dis}}$ . Therefore, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathsf{T V}(Q_{\\mathrm{dis}},\\widetilde{Q})\\lesssim\\delta+\\mathsf{T V}(Q,Q_{\\mathrm{dis}}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$\\widetilde{Q}$ and $\\overline{{Q}}$ are close. Inspired by $[\\mathrm{CCL}^{+}23\\mathrm{b}]$ , we utilize Girsanov\u2019s theorem (see Theorem F.8) to help bound this distance. Define ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{r}:=\\sqrt{2}\\big(\\widetilde{s}_{T-t_{k}}\\big(x_{T-t_{k}}\\big)-\\widehat{s}_{T-t_{k}}\\big(x_{T-t_{k}}\\big)\\big),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $k$ is index such that $r\\in[t_{k},t_{k+1})$ . We apply the Girsanov\u2019s theorem to $(\\widetilde{Q},(b_{r}))$ . By Eq. (24), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{0}^{t_{N}}\\|b_{r}\\|_{2}^{2}\\,\\mathrm{d}r\\leq\\sum_{k=0}^{N-1}\\|\\sqrt{2}\\big(\\widetilde{s}_{T-t_{k}}(x_{T-t_{k}})-\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\big)\\|_{2}^{2}\\big(t_{k+1}-t_{k}\\big)\\leq2\\varepsilon^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This satisfies Novikov\u2019s condition and tells us that for ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\mathcal{L})_{t}=\\exp\\left(\\int_{0}^{t}b_{r}\\,\\mathrm{d}B_{r}-\\frac{1}{2}\\int_{0}^{t}\\lVert b_{r}\\rVert_{2}^{2}\\,\\mathrm{d}r\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "under measure $\\widetilde{Q}^{\\prime}:=\\mathcal{E}(\\mathcal{L})_{t_{N}}\\widetilde{Q}$ , there exists a Brownian motion $(\\widetilde{B}_{t})_{t\\in[0,t_{N}]}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{B}_{t}=B_{t}-\\int_{0}^{t}b_{r}\\,\\mathrm{d}r,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and thus for $t\\in[t_{k},t_{k+1})$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{B}_{t}=\\mathrm{d}B_{t}+\\sqrt{2}\\big(\\widetilde{s}_{T-t_{k}}(x_{T-t_{k}})-\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\big)\\,\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plug this into (23) and we have that for $t\\in[t_{k},t_{k+1})$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{T-t}=-\\left(x_{T-t}+2\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\right)\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}\\widetilde{B}_{t},\\quad x_{T}\\sim q_{T}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This equation depicts the distribution of $x$ , and this exactly matches the definition of $\\overline{{Q}}$ . Therefore, $\\overline{{Q}}=\\tilde{\\tilde{Q^{\\prime}}}=\\mathcal E(\\mathcal L)_{t_{N}}^{\\'}\\tilde{Q}$ , and we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{K L}\\left(\\widetilde{Q}\\Big|\\Big|\\overline{{Q}}\\right)=\\underline{{\\mathbb{E}}}\\left[\\ln\\frac{\\mathrm{d}\\widetilde{Q}}{\\mathrm{d}\\overline{{Q}}}\\right]=\\underline{{\\mathbb{E}}}\\left[\\ln\\mathcal{E}(\\mathcal{L})_{t_{N}}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then by using (24), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}}{\\tilde{Q}}\\left[\\ln\\mathcal{E}(\\mathcal{L})_{t_{N}}\\right]\\lesssim\\underline{{\\mathbb{E}}}\\left[\\sum_{k=0}^{N-1}\\lVert\\tilde{s}_{T-t_{k}}(x_{T-t_{k}})-\\widehat{s}_{T-t_{k}}(x_{T-t_{k}})\\rVert_{2}^{2}(t_{k+1}-t_{k})\\right]\\lesssim\\varepsilon^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we can apply Pinsker\u2019s inequality and get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{T V}(\\widetilde{Q},\\overline{{Q}})\\leq\\sqrt{\\mathsf{K L}\\left(\\widetilde{Q}\\Big|\\Big|\\overline{{Q}}\\right)}\\lesssim\\varepsilon.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Putting things together. Using the data processing inequality, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{T V}(\\overline{{Q}},\\widehat{Q})\\leq\\mathsf{T V}(q_{T},\\mathcal{N}(0,I_{d})).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining these results, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T V}(Q,\\widehat{Q})\\leq\\mathsf{T V}(Q,Q_{\\mathrm{dis}})+\\mathsf{T V}(Q_{\\mathrm{dis}},\\widetilde{Q})+\\mathsf{T V}(\\widetilde{Q},\\overline{{Q}})+\\mathsf{T V}(\\overline{{Q}},\\widehat{Q})}\\\\ &{\\qquad\\qquad\\lesssim\\delta+\\varepsilon+\\mathsf{T V}(Q,Q_{\\mathrm{dis}})+\\mathsf{T V}(q_{T},\\mathcal{N}(0,I_{d})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $q_{\\gamma}$ is the distribution for $x_{T-t_{N}}$ under $Q$ and $p_{T-t_{N}}$ is the distribution for $x_{T-t_{N}}$ under $\\widehat{Q}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{T V}(q_{\\gamma},p_{T-t_{N}})\\leq\\mathsf{T V}(Q,\\widehat{Q})\\lesssim\\delta+\\varepsilon+\\mathsf{T V}(Q,Q_{\\mathrm{dis}})+\\mathsf{T V}(q_{T},\\mathcal{N}(0,I_{d})).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma B.3. Consider an arbitrary sequence of discretization times $0=t_{0}<t_{1}<\\cdot\\cdot<t_{N}=$ $T-\\gamma.$ . Assume that for each $k\\in\\{0,\\ldots,N-1\\}$ , the following holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\delta/N}(\\widehat{s}_{T-t_{k}},s_{T-t_{k}})\\leq\\frac{\\varepsilon}{\\sigma_{T-t_{k}}}\\cdot\\frac{1}{\\sqrt{T+\\log\\frac{1}{\\gamma}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, the output distribution $\\widehat{q}_{T-t_{N}}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{T V}(\\widehat{q}_{T-t_{N}},q_{T-t_{N}})\\lesssim\\delta+\\varepsilon+\\mathsf{T V}(Q,Q_{d i s})+\\mathsf{T V}(q_{T},\\mathcal{N}(0,I_{d})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Follows by Lemma B.1 and Lemma B.2. ", "page_idx": 23}, {"type": "text", "text": "The next two lemmas from existing works show that the discretization error, $\\mathsf{T V}(Q,Q_{\\mathrm{dis}})$ , is relatively small. Furthermore, as $T$ increases, $q_{T}$ converges exponentially towards $\\mathcal{N}(0,I_{d})$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma B.4 (Discretization Error, Corollary 1 and eq. (17) in [BBDD24]). For any $T\\geq1,\\,\\gamma<1$ and $N\\geq\\log(1/\\gamma)$ , there exists a sequence of $N$ discretization times such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\sf T V}(Q,Q_{d i s})\\lesssim\\sqrt{\\frac{d}{N}}\\left(T+\\log\\frac{1}{\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma B.5 (TV between true Gaussian and $q_{T}$ for large $T$ , Proposition 4 in [BBDD24]). Let q be a distribution with a finite second moment of $m_{2}^{2}$ . Then, for $T\\geq1$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{T V}(q_{T},\\mathcal{N}(0,I_{d}))\\lesssim(\\sqrt{d}+m_{2})e^{-T}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining Lemma B.4 and Lemma B.5 with Lemma B.3, we have the following result: ", "page_idx": 23}, {"type": "text", "text": "Corollary B.6. Let q be a distribution with finite second moment $m_{2}^{2}$ . For any $T\\geq1$ , $\\gamma<1$ and $N\\geq\\log(1/\\gamma)$ , there exists a sequence of discretization times $0=t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{N}=T-\\gamma$ such that if the following holds for each $k\\in\\{0,\\ldots,N-1\\}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\delta/N}\\big(\\widehat{s}_{T-t_{k}},s_{T-t_{k}}\\big)\\leq\\frac{\\varepsilon}{\\sigma_{T-t_{k}}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then there exists a sequence of $N$ discretization times such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\sf T V}(q_{\\gamma},p_{T-t_{N}})\\lesssim\\delta+\\varepsilon\\sqrt{T+\\log\\frac{1}{\\gamma}}+\\sqrt{\\frac{d}{N}}\\left(T+\\log\\frac{1}{\\gamma}\\right)+(\\sqrt{d}+m_{2})e^{-T}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This implies our main theorem of this section as a corollary. ", "page_idx": 24}, {"type": "text", "text": "Theorem B.7. Let q be a distribution with finite second moment $m_{2}^{2}$ . For any $\\gamma>0$ , there exist $\\begin{array}{r}{N\\,=\\,\\widetilde O(\\frac{d}{\\varepsilon^{2}+\\delta^{2}}\\log^{2}\\frac{d+m_{2}}{\\gamma})}\\end{array}$ discretization times $0\\,=\\,t_{0}\\,<\\,t_{1}\\,<\\,\\cdot\\,\\cdot\\,<\\,t_{N}\\,<\\,T$ such that if the following holds for every $k\\in\\{0,\\ldots,N-1\\}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\delta/N}\\big(\\widehat{s}_{T-t_{k}},s_{T-t_{k}}\\big)\\leq\\frac{\\varepsilon}{\\sigma_{T-t_{k}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then DDPM can produce a sample from a distribution that is within ${\\widetilde O}(\\delta+\\varepsilon\\sqrt{\\log{\\left((d+m_{2})/\\gamma\\right)}})$ ) in TV distance of $q_{\\gamma}$ in $N$ steps. ", "page_idx": 24}, {"type": "text", "text": "Proof. By setting $\\begin{array}{r}{T=\\log(\\frac{\\sqrt{d}+m_{2}}{\\varepsilon+\\delta})}\\end{array}$ and $\\begin{array}{r}{N=\\frac{d(T+\\log(1/\\gamma))^{2}}{\\varepsilon^{2}+\\delta^{2}}}\\end{array}$ in Corollary B.6, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\sf T V}(q_{\\gamma},p_{T-t_{N}})=\\widetilde{O}\\left(\\delta+\\varepsilon\\sqrt{\\log\\frac{d+m_{2}}{\\gamma}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Furthermore, we present our theorem under the case when $m_{2}$ lies between $1/\\operatorname{poly}(d)$ and $p o l y(d)$ to provide a clearer illustration. ", "page_idx": 24}, {"type": "text", "text": "Lemma 1.3. Let q be a distribution over $\\mathbb{R}^{d}$ with second moment $m_{2}^{2}$ between $1/p o l y(d)$ and poly $(d)$ . For any $\\gamma>0$ , there exist $\\begin{array}{r}{N=\\widetilde O(\\frac{d}{\\varepsilon^{2}+\\delta^{2}}\\log^{2}\\frac{1}{\\gamma})}\\end{array}$ discretization times $0=t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{N}<T$ such that if the following holds for every $k\\in\\{0,\\ldots,N-1\\}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\delta/N}\\big(\\widehat{s}_{T-t_{k}},s_{T-t_{k}}\\big)\\leq\\frac{\\varepsilon}{\\sigma_{T-t_{k}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then DDPM can sample from a distribution that is within ${\\widetilde{O}}(\\delta+\\varepsilon{\\sqrt{\\log{(d/\\gamma)}}})$ in TV distance to $q_{\\gamma}$ in $N$ steps. ", "page_idx": 24}, {"type": "text", "text": "C Sample Complexity of Training Diffusion Model ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we present our main theorem that combines our score estimation result in the new (6) sense with prior sampling results (from [BBDD24]) to show that the score can be learned using a number of samples scaling polylogarithmically in $\\frac{1}{\\gamma}$ , where $\\gamma$ is the desired sampling accuracy. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.1. Let $q_{0}$ be a distribution with second moment $m_{2}^{2}$ . Let $\\phi_{\\theta}(\\cdot)$ be the fully connected neural network with ReLU activations parameterized by $\\theta$ , with $P$ total parameters and depth $D$ . Let $\\Theta>1$ . For discretization times $0=t_{0}<t_{1}<\\cdot\\cdot<t_{N},$ , let $s_{T-t_{k}}$ be the true score function of $q_{T-t_{k}}$ . If for each $t_{k}$ , there exists some weight vector $\\theta^{*}$ with $\\lVert\\theta^{*}\\rVert_{\\infty}\\leq\\Theta$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{T-t_{k}}}{\\mathbb{E}}\\left[\\left\\|\\phi_{\\theta^{*}}(x)-s_{T-t_{k}}(x)\\right\\|_{2}^{2}\\right]\\leq\\frac{\\delta_{s c o r e}\\cdot\\delta_{t r a i n}\\cdot\\varepsilon^{2}}{C\\sigma_{T-t_{k}}^{2}\\cdot N^{2}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, if we take $\\begin{array}{r}{m>\\widetilde{O}\\left(\\frac{N(d+\\log\\frac{1}{\\delta_{t r a i n}})\\cdot P D}{\\varepsilon^{2}\\delta_{s c o r e}}\\cdot\\log\\left(\\frac{\\operatorname*{max}(m_{2},1)\\cdot\\Theta}{\\delta_{t r a i n}}\\right)\\right)}\\end{array}$ samples, then with probability $1-\\delta_{t r a i n}$ , each score $\\hat{s}_{T-t_{k}}$ learned by score matching satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\delta_{s c o r e}/N}\\big(\\hat{s}_{T-t_{k}},s_{T-t_{k}}\\big)\\leq\\varepsilon/\\sigma_{T-t_{k}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Note that for each $t_{k}$ , $q_{T-t_{k}}$ is a $\\sigma_{T-t_{k}}$ -smoothed distribution. Therefore, we can use Lemma A.1 by taking $\\delta_{\\mathrm{train}}/N$ into $\\delta_{\\mathrm{train}}$ and taking $\\delta_{\\mathrm{score}}/N$ into $\\delta_{\\mathrm{score}}$ . We have that for each $t_{k}$ , with probability $1-\\delta_{\\mathrm{train}}/N$ the following holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x\\sim q_{T-t_{k}}}[\\|\\hat{s}_{T-t_{k}}(x)-s_{T-t_{k}}(x)\\|\\le\\varepsilon/\\sigma_{T-t_{k}}]\\ge1-\\delta_{\\mathrm{score}}/N.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By a union bound over all the steps, we conclude the proposed statement. ", "page_idx": 25}, {"type": "text", "text": "Now we present our main theorem. ", "page_idx": 25}, {"type": "text", "text": "Theorem C.2 (Main Theorem, Full Version). Let q be a distribution of $\\mathbb{R}^{d}$ with second moment $m_{2}^{2}$ . Let $\\phi_{\\theta}(\\cdot)$ be the fully connected neural network with ReLU activations parameterized by $\\theta$ , with $P$ total parameters and depth $D$ . Let $\\Theta>1$ . For any $\\gamma>0$ , there exist $\\begin{array}{r}{N=\\widetilde O(\\frac{d}{\\varepsilon^{2}+\\delta^{2}}\\log^{2}\\frac{m_{2}+1/m_{2}}{\\gamma})}\\end{array}$ discretization times $0=t_{0}<\\cdot\\cdot\\cdot<t_{N}<T$ such that if for each $t_{k}$ , there exists some weight vector $\\theta^{*}$ with $\\lVert\\theta^{*}\\rVert_{\\infty}\\leq\\Theta$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{T-t_{k}}}{\\mathbb{E}}\\left[\\left\\Vert\\phi_{\\theta^{*}}(x)-s_{T-t_{k}}(x)\\right\\Vert_{2}^{2}\\right]\\leq\\frac{\\delta\\cdot\\varepsilon^{3}}{C N^{2}\\sigma_{T-t_{k}}^{2}}\\cdot\\frac{1}{\\log\\frac{d+m_{2}+1/m_{2}}{\\gamma}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for sufficiently large constant $C$ , then consider the score functions trained from ", "page_idx": 25}, {"type": "equation", "text": "$$\nm>\\tilde{O}\\left(\\frac{N(d+\\log\\frac{1}{\\delta})\\cdot P D}{\\varepsilon^{\\prime3}}\\cdot\\log\\left(\\frac{\\operatorname*{max}(m_{2},1)\\cdot\\Theta}{\\delta}\\right)\\cdot\\log\\frac{m_{2}+1/m_{2}}{\\gamma}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "i.i.d. samples of $q_{\\mathrm{r}}$ , with $1-\\delta$ probability, DDPM can sample from a distribution $\\varepsilon$ -close in TV to $a$ distribution $\\gamma m_{2}$ -close in 2-Wasserstein to $q$ in $N$ steps. ", "page_idx": 25}, {"type": "text", "text": "Proof. Note that for an arbitrary $t>0$ , the 2-Wasserstein distance between $q$ and $q_{t}$ is bounded by $O(t m_{2}+{\\sqrt{t d}})$ . Therefore, by choosing $t_{N}=T-\\operatorname*{min}(\\gamma,\\gamma^{2}m_{2}^{2}/d)$ , Theorem B.7 shows that by choosing $\\begin{array}{r}{N=\\overleftarrow{O}(\\frac{d}{\\varepsilon^{\\prime2}+\\delta^{2}}\\log^{2}\\frac{d+m_{2}}{\\operatorname*{min}(\\gamma,\\gamma^{2}m_{2}^{2}/d)})}\\end{array}$ , we only need ", "page_idx": 25}, {"type": "equation", "text": "$$\nD_{q_{T-t_{k}}}^{\\varepsilon/N}(\\widehat{s}_{T-t_{k}},s_{T-t_{k}})\\leq\\frac{\\varepsilon^{\\prime}}{\\sigma_{T-t_{k}}\\sqrt{\\log\\frac{d+m_{2}}{\\operatorname*{min}(\\gamma,\\gamma^{2}m_{2}^{2}/d)}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then DDPM can produce a sample from a distribution within $\\widetilde O(\\varepsilon^{\\prime})$ in TV distance to a distribution $\\gamma m_{2}$ -close in 2-Wasserstein to $q$ in $N$ steps. Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\log{\\frac{d+m_{2}}{\\operatorname*{min}(\\gamma,\\gamma^{2}m_{2}^{2}/d)}}\\lesssim\\log{\\frac{d+m_{2}+1/m_{2}}{\\gamma}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we only need to take $\\begin{array}{r}{\\widetilde{O}(\\frac{d}{\\varepsilon^{\\prime2}+\\delta^{2}}\\log^{2}\\frac{m_{2}+1/m_{2}}{\\gamma})}\\end{array}$ steps. Therefore, to achieve this, we set $\\delta_{\\mathrm{train}}=\\delta$ , $\\delta_{\\mathrm{score}}=\\varepsilon^{\\prime}$ , and $\\begin{array}{r}{\\varepsilon=\\varepsilon^{\\prime}/\\sqrt{\\log\\frac{d+m_{2}+1/m_{2}}{\\gamma}}\\lesssim\\varepsilon^{\\prime}/\\sqrt{\\log\\frac{d+m_{2}}{\\operatorname*{min}(\\gamma,\\gamma^{2}m_{2}^{2}/d)}}}\\end{array}$ in Lemma C.1. This gives us the result that with ", "page_idx": 25}, {"type": "equation", "text": "$$\nm>\\tilde{O}\\left(\\frac{N(d+\\log\\frac{1}{\\delta})\\cdot P D}{\\varepsilon^{\\prime3}}\\cdot\\log\\left(\\frac{\\operatorname*{max}(m_{2},1)\\cdot\\Theta}{\\delta}\\right)\\cdot\\log\\frac{m_{2}+1/m_{2}}{\\gamma}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "samples, we can satisfy the score requirement given the assumption in the statement. ", "page_idx": 25}, {"type": "text", "text": "For cleaner statement, we present this theorem under the case when $m_{2}$ lies between $1/\\operatorname{poly}(d)$ and $\\mathrm{poly}(d)$ and achieving training success probability of $99\\%$ . This gives the quantitative version of Theorem 1.2. ", "page_idx": 25}, {"type": "text", "text": "Theorem C.3 (Main Theorem, Quantitative Version). Let q be a distribution of $\\mathbb{R}^{d}$ with second moment $m_{2}^{2}$ . Let $\\phi_{\\theta}(\\cdot)$ be the fully connected neural network with ReLU activations parameterized by $\\theta$ , with $P$ total parameters and depth $D$ . Let $\\Theta>1$ . For any $\\gamma>0$ , there exist $\\begin{array}{r}{N=\\widetilde O(d\\log^{2}\\frac{1}{\\gamma})}\\end{array}$ discretization times $0=t_{0}<\\cdot\\cdot\\cdot<t_{N}<T$ such that if for each $t_{k}$ , there exists some weight vector $\\theta^{*}$ with $\\lVert\\theta^{*}\\rVert_{\\infty}\\leq\\Theta$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{x\\sim q_{T-t_{k}}}{\\mathbb{E}}\\left[\\left\\lVert\\phi_{\\theta^{*}}(x)-s_{T-t_{k}}(x)\\right\\rVert_{2}^{2}\\right]\\le\\frac{\\delta\\cdot\\varepsilon^{3}}{C N^{2}\\sigma_{T-t_{k}}^{2}\\log\\frac{d}{\\gamma}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for a sufficiently large constant $C$ , then consider the score functions trained from ", "page_idx": 26}, {"type": "equation", "text": "$$\nm\\geq\\widetilde{O}\\left(\\frac{d^{2}P D}{\\varepsilon^{3}}\\cdot\\log\\Theta\\cdot\\log^{3}\\frac{1}{\\gamma}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "i.i.d. samples of $q$ . With $99\\%$ probability, DDPM using these score functions can sample from a distribution $\\varepsilon$ -close in TV to a distribution $\\gamma m_{2}$ -close in 2-Wasserstein to $q$ in $N$ steps. ", "page_idx": 26}, {"type": "text", "text": "D Hardness of Learning in $L^{2}$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we give proofs of the hardness of the examples we mention in Section 4. ", "page_idx": 26}, {"type": "text", "text": "Lemma 4.1. Let $R$ be sufficiently larger than $\\sigma$ . Let $p_{1}$ be the distribution $(1-\\eta)\\mathcal{N}(0,\\sigma^{2})\\,+$ $\\eta\\mathcal{N}(-R,\\sigma^{2})$ with corresponding score function $s_{1}$ , and let $p_{2}$ be $(1-\\eta)\\mathcal{N}(0,\\sigma^{2})+\\eta\\mathcal{N}(R,\\sigma^{2})$ with score $s_{2}$ , such that $\\begin{array}{r}{\\eta=\\frac{\\varepsilon^{2}\\sigma^{2}}{R^{2}}}\\end{array}$ \u03b5R\u03c32 . Then, given m < $\\begin{array}{r}{m\\,<\\,\\frac{R^{2}}{\\varepsilon^{2}\\sigma^{2}}}\\end{array}$ samples from either distribution, it is impossible to distinguish between $p_{1}$ and $p_{2}$ with probability larger than $1/2+o_{m}(1)$ . But, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\sim p_{1}}{\\mathbb{E}}\\left[\\|s_{1}(x)-s_{2}(x)\\|^{2}\\right]\\gtrsim\\frac{\\varepsilon^{2}}{\\sigma^{2}}\\qquad a n d\\qquad\\underset{x\\sim p_{2}}{\\mathbb{E}}\\left[\\|s_{1}(x)-s_{2}(x)\\|^{2}\\right]\\gtrsim\\frac{\\varepsilon^{2}}{\\sigma^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathsf{T V}(p_{1},p_{2})\\gtrsim\\eta\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So, it is impossible to distinguish between $p_{1}$ and $p_{2}$ with fewer than $\\begin{array}{r}{O\\left(\\frac{1}{\\eta}\\right)}\\end{array}$ samples with probability $1/2+o_{m}(1)$ . ", "page_idx": 26}, {"type": "text", "text": "The score $L^{2}$ bound follows from calculation. ", "page_idx": 26}, {"type": "text", "text": "Lemma 4.2. Let $S$ be sufficiently large. Consider the distribution $\\widehat{p}=\\eta N(0,\\sigma^{2})+(1-\\eta){\\mathcal N}(S,\\sigma^{2})$ for \u03b7 = Se\u2212S22 \u221a+10\u221alog m\u00b7S, and let $\\widehat{s}$ be its score function. Given m samples from the standard Gaussian $p^{*}=\\mathcal{N}(0,\\sigma^{2})$ with score function $s^{*}$ , with probability at least $\\begin{array}{r}{1-\\frac{1}{p o l y(m)}}\\end{array}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\|\\widehat{s}(x)-s^{*}(x)\\|^{2}\\right]\\leq\\frac{1}{\\sigma^{2}}e^{-O(S\\sqrt{\\log m})}\\quad b u t\\quad\\underset{x\\sim p^{*}}{\\mathbb{E}}\\left[\\|\\widehat{s}(x)-s^{*}(x)\\|^{2}\\right]\\gtrsim\\frac{S^{2}}{m\\sigma^{4}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Let $X_{1},\\ldots,X_{m}\\sim p^{*}$ be the $m$ samples from ${\\mathcal{N}}(0,\\sigma^{2})$ . With probability at least $1-{\\frac{1}{\\mathrm{poly}(m)}}$ every $X_{i}\\leq2\\sigma{\\sqrt{\\log m}}$ . Now, the score function of the mixture $\\widehat{p}$ is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{s}(x)=\\frac{-\\frac{x}{\\sigma^{2}}-\\frac{x-S}{\\sigma^{2}}\\left(\\frac{1-\\eta}{\\eta}\\right)e^{-\\frac{S^{2}}{2}+S x}}{1+\\left(\\frac{1-\\eta}{\\eta}\\right)e^{-\\frac{S^{2}}{2}+S x}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For $x\\leq2{\\sqrt{\\log{m}}}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{s}(x)=-\\frac{x}{\\sigma^{2}}\\left(1+\\frac{e^{-O(S\\sqrt{\\log m})}}{S}\\right)+\\frac{1}{\\sigma^{2}}e^{-O(S\\sqrt{\\log m})}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}\\left[\\|\\widehat{s}(x)-s^{*}(x)\\|^{2}\\right]\\le\\frac{1}{\\sigma^{2}}e^{-O(S\\sqrt{\\log m})}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "On the other hand, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\underset{x\\sim p^{*}}{\\mathbb{E}}\\left[\\|\\widehat{s}(x)-s^{*}(x)\\|^{2}\\right]\\gtrsim\\frac{S^{2}}{m\\sigma^{4}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E Discussion of [BMR20] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here, we present a brief, self-contained discussion of the prior work [BMR20]. The main result of that work on score estimation is as follows. ", "page_idx": 27}, {"type": "text", "text": "Theorem E.1 (Proposition 9 of [BMR20], Restated). Let $\\mathcal{F}$ be a class of $\\mathbb{R}^{d}$ -valued functions, all of with are $\\begin{array}{l}{{\\frac{L}{2}}}\\end{array}$ -Lipschitz, with values supported on the Euclidean ball with radius $R_{s}$ , and containing uniformly good approximations of the true score $s_{t}$ on this ball. Given $n$ samples from $q_{t}$ , $i f$ we let $\\widehat{s}_{t}$ be the empirical minimizer of the score-matching loss, then with probability at least $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim q_{t}}\\left[\\Vert\\widehat{s}_{t}(x)-s_{t}(x)\\Vert^{2}\\right]\\lesssim\\left(L R+B\\right)^{2}\\left(\\log^{2}n\\cdot\\mathcal{R}_{n}^{2}(\\mathcal{F})+\\frac{d\\left(\\log\\log n+\\log\\frac{1}{\\delta}\\right)}{n}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, $B$ is a bound on $\\lVert s_{t}(0)\\rVert$ , and $\\mathcal{R}_{n}(\\mathcal{F})$ is the Rademacher complexity of $\\mathcal{F}$ over $n$ samples. ", "page_idx": 27}, {"type": "text", "text": "In the setting we consider, for the class of neural networks with weight vector $\\theta$ such that $\\|\\theta\\|_{\\infty}\\leq\\Theta$ with $P$ parameters and depth $D$ , it was shown in [Sel24] that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}_{n}(\\mathcal{F})\\lesssim\\frac{R\\sqrt{d}}{\\sqrt{n}}(\\Theta\\sqrt{P})^{D}\\cdot\\sqrt{D}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, the true score is $\\frac{R}{\\sigma_{t}}$ Lipschitz, so that if we further restrict ourselves to neural networks have the same Lipschitzness, $\\begin{array}{r}{L\\approx\\frac{R}{\\sigma_{t}}}\\end{array}$ . $B$ can be bounded by $\\frac{\\sqrt{d}}{\\sigma_{t}}$ ", "page_idx": 27}, {"type": "text", "text": "Thus, an application of the theorem to our setting gives a sample complexity bound of O  R6d+\u03b52R2d2\u00b7 (\u03982P)D\u221aD log \u03b41 for squared L2 score error O \u03c3\u03b522 . In order to support \u03c3tsmoothed scores, we need to set $\\begin{array}{r}{R\\gtrsim\\frac{\\sqrt{d}}{\\sigma_{t}}}\\end{array}$ , for a sample complexity of $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{d^{5/2}R^{3}}{\\sigma_{t}^{3}\\varepsilon^{2}}(\\Theta^{2}P)^{D}\\sqrt{D}\\log\\frac{1}{\\delta}\\right)}\\end{array}$ to learn an approximation to $s_{t}$ with squared $L^{2}$ error $\\begin{array}{r}{O\\left(\\frac{\\varepsilon^{2}}{\\sigma_{t}^{2}}\\right)}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "So, we obtain the following corollary. ", "page_idx": 27}, {"type": "text", "text": "Corollary E.2. Let q be a distribution over Rdsupported over the Euclidean ball with radius $R\\leq O\\left({\\sqrt{d}}/\\sigma_{t}\\right)$ . Let $\\mathcal{F}$ be the set of neural networks $\\phi_{\\theta}(\\cdot)$ , with weight vector $\\theta,\\,P$ parameters and depth $D$ , with $\\|\\theta\\|_{\\infty}\\leq\\Theta$ and values supported on the ball of radius $R,$ , and that are $O(R/\\sigma_{t})$ - Lipschitz. Suppose $\\mathcal{F}$ contains a network with weights $\\theta^{*}$ such that $\\phi_{\\theta^{*}}$ provides uniformly good approximation of the true score $s_{t}$ over the ball of radius $R$ . Given $n$ samples from $q_{t}$ , $i f$ we let $\\widehat{s}_{t}$ be the empirical minimizer of the score-matching loss, then with probability $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\quad\\underset{x\\sim q_{t}}{\\mathbb{E}}\\left[\\|\\widehat{s}_{t}(x)-\\phi_{\\theta^{*}}(x)\\|^{2}\\right]\\leq\\widetilde{O}\\left(\\frac{\\varepsilon^{2}}{\\sigma^{2}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad n>\\widetilde{O}\\left(\\frac{d^{5/2}R^{3}}{\\sigma_{t}^{3}\\varepsilon^{2}}\\left(\\Theta^{2}P\\right)^{D}\\sqrt{D}\\log\\frac{1}{\\delta}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To learn score approximations in $L^{2}$ for all the $N$ relevant timesteps in order to achieve TV error $\\varepsilon$ and Wasserstein error $\\gamma\\cdot R$ , this implies a sample complexity bound of $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{d^{5/2}}{\\gamma^{3}\\varepsilon^{2}}(\\Theta^{2}P)^{D}\\sqrt{D}\\log\\frac{N}{\\delta}\\right)=}\\end{array}$ $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{d^{5/2}}{\\gamma^{3}\\varepsilon^{2}}(\\Theta^{2}P)^{D}\\sqrt{D}\\log\\frac{1}{\\delta}\\right)}\\end{array}$ for our final choice of $N$ . ", "page_idx": 27}, {"type": "text", "text": "F Utility Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma F.1 (From [GLP23]). Let $f$ be an arbitrary distribution on $\\mathbb{R}^{d}$ , and let $f_{\\Sigma}$ be the $\\Sigma$ -smoothed version of $f$ . That is, $f_{\\Sigma}(x)=\\mathbb{E}_{y\\sim f}^{\\scriptscriptstyle*}\\left[(2\\pi)^{-d/2}\\operatorname*{det}(\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(x-Y)^{\\overline{{T}}}\\Sigma^{-1}(x-Y)\\right)\\right]$ . Let $s_{\\Sigma}$ be the score function of $f_{\\Sigma}$ . Let $(X,Y,Z_{\\Sigma})$ be the joint distribution such that $Y\\sim f$ , $Z_{\\Sigma}\\,\\sim$ $\\mathcal{N}(0,\\Sigma)$ are independent, and $X=Y+Z_{\\Sigma}\\sim f_{\\Sigma}$ . We have for $\\varepsilon\\in\\mathbb{R}^{d}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{f_{\\Sigma}(x+\\varepsilon)}{f_{\\Sigma}(x)}=\\underset{Z_{\\Sigma}|x}{\\mathbb{E}}\\left[e^{-\\varepsilon^{T}\\Sigma^{-1}Z_{\\Sigma}-\\frac{1}{2}\\varepsilon^{T}\\Sigma^{-1}\\varepsilon}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "so that ", "page_idx": 27}, {"type": "equation", "text": "$$\ns_{\\Sigma}(x)={\\mathbb{E}}_{Z_{\\Sigma}\\mid x}\\left[-\\Sigma^{-1}Z_{\\Sigma}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma F.2 (From [HKZ12], restated). Let x be a mean-zero random vector in $\\mathbb{R}^{d}$ that is $\\Sigma$ - subgaussian. That is, for every vector $v$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{\\lambda\\langle x,v\\rangle}\\right]\\leq e^{\\lambda^{2}v^{T}\\Sigma v/2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, with probability $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\lVert x\\right\\rVert\\lesssim\\sqrt{\\mathrm{Tr}(\\Sigma)}+\\sqrt{2\\lVert\\Sigma\\rVert\\log\\frac{1}{\\delta}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma F.3. For $s_{\\Sigma}$ the score function of an $\\Sigma$ -smoothed distribution where $\\Sigma=\\sigma^{2}I_{s}$ , we have that $\\boldsymbol{v}^{T}\\boldsymbol{s}_{\\Sigma}(\\boldsymbol{x})$ is ${\\cal O}(1/\\sigma^{2})$ -subgaussian, when $x\\sim f_{\\Sigma}$ and $\\|v\\|=1$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We have by Lemma F.1 that ", "page_idx": 28}, {"type": "equation", "text": "$$\ns_{\\Sigma}(x)={\\underset{Z_{\\Sigma}|x}{\\mathbb{E}}}\\left[\\Sigma^{-1}Z_{\\Sigma}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "So, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\sim f_{\\Sigma}}{\\mathbb{E}}\\left[(v^{T}s_{\\Sigma}(x))^{k}\\right]=\\underset{x\\sim f_{\\Sigma}}{\\mathbb{E}}\\left[\\underset{Z_{\\Sigma}\\mid x}{\\mathbb{E}}\\left[v^{T}\\Sigma^{-1}Z_{\\Sigma}\\right]^{k}\\right]}\\\\ &{\\phantom{\\leq}\\frac{\\mathbb{E}}{Z_{\\Sigma}}\\left[(v^{T}\\Sigma^{-1}Z_{\\Sigma})^{k}\\right]}\\\\ &{\\phantom{\\leq}\\frac{k^{k/2}}{\\sigma^{k}}\\quad\\mathrm{since}\\;v^{T}Z_{\\Sigma}\\sim\\mathcal{N}(0,\\sigma^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The claim follows. ", "page_idx": 28}, {"type": "text", "text": "Lemma F.4. Let $\\Sigma=\\sigma^{2}I_{s}$ , and let $x\\sim f_{\\Sigma}$ . We have that with probability $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|s_{\\Sigma}(x)\\|^{2}\\lesssim\\frac{d+\\log\\frac{1}{\\delta}}{\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Follows from Lemmas F.3 and F.2. ", "page_idx": 28}, {"type": "text", "text": "Lemma F.5. For $z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ , with probability $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{z}{\\sigma^{2}}}\\right\\|\\lesssim{\\frac{\\sqrt{d+\\log{\\frac{1}{\\delta}}}}{\\sigma}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Note that $\\|z\\|^{2}$ is chi-square, so that we have for any $0\\leq\\lambda<\\sigma^{2}/2$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}}{z}\\left[e^{\\lambda\\left\\|\\frac{\\|z\\|}{\\sigma^{2}}\\right\\|^{2}}\\right]\\leq\\frac{1}{(1-2(\\lambda/\\sigma^{2}))^{d/2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The claim then follows by the Chernoff bound. ", "page_idx": 28}, {"type": "text", "text": "Lemma F.6. For $x\\sim f_{\\Sigma}$ , with probability $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underline{{\\mathbb{E}}}_{\\Sigma}\\left[\\frac{\\|Z_{\\Sigma}\\|}{\\sigma^{2}}\\right]\\lesssim\\frac{\\sqrt{d+\\log\\frac{1}{\\delta}}}{\\sigma}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Since $Z_{\\Sigma}\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ so that $\\lVert Z_{\\Sigma}\\rVert^{2}$ is chi-square, we have that for any $0\\leq\\lambda<\\sigma^{2}/2$ , by Jensen\u2019s inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{x\\sim f_{\\Sigma}}{\\mathbb{E}}\\left[e^{\\lambda\\mathbb{E}_{Z_{\\Sigma}\\mid x}\\left[\\frac{\\|Z_{\\Sigma}\\|}{\\sigma^{2}}\\right]^{2}}\\right]\\leq\\frac{\\mathbb{E}}{Z_{\\Sigma}}\\left[e^{\\lambda\\frac{\\|Z_{\\Sigma}\\|^{2}}{\\sigma^{4}}}\\right]\\leq\\frac{1}{(1-2(\\lambda/\\sigma^{2}))^{d/2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The claim then follows by the Chernoff bound. That is, setting $\\lambda=\\sigma^{2}/4$ , for any $t>0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x\\sim f_{\\Sigma}}\\left[\\mathbb{E}_{x}\\left[\\frac{\\left\\Vert Z_{\\Sigma}\\right\\Vert}{\\sigma^{2}}\\right]^{2}\\geq t\\right]\\leq\\frac{\\mathbb{E}_{x\\sim f_{\\Sigma}}\\left[e^{\\lambda\\mathbb{E}_{z_{\\Sigma}|x}\\left[\\frac{\\|Z_{\\Sigma}\\|}{\\sigma^{2}}\\right]^{2}}\\right]}{e^{\\lambda t}}\\leq2^{d/2}e^{-t\\sigma^{2}/4}=2^{\\frac{d\\ln2}{2}-\\frac{t\\sigma^{2}}{4}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For $\\begin{array}{r}{t=O\\left(\\frac{d+\\log\\frac{1}{\\delta}}{\\sigma^{2}}\\right)}\\end{array}$ , this is less than $\\delta$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma F.7 ([KN]). Let $X$ be a non-negative random variable such that for every $t\\geq0,\\,\\mathbb{P}[X\\geq$ $t]\\leq\\exp(-\\lambda t)$ for some constant $\\lambda\\geq0$ . Then, for $K\\ge0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[X|X\\geq K]\\lesssim\\mathbb{E}[X]+\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\mathbb{P}[X\\geq K]}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Let $p:=\\mathbb{P}[X\\geq K]$ and denote the distribution of $X$ as $\\mathcal{P}$ . Consider the sequence of iid samples $\\{X_{i}\\}_{i\\ge0}$ sampled from $\\mathcal{P}$ . Let $T$ be the first index $i$ , where $X_{i}\\geq K$ . Then the distribution of $X_{T}$ is the conditional distribution of $X$ given $X\\ge K$ . For any $m>0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{X_{T}\\leq\\sum_{k\\geq1}1\\left((k-1)m<T\\leq k m\\right)\\qquad\\operatorname*{sup}_{(k-1)m<i\\leq k m}X_{i}}}\\\\ &{\\leq\\displaystyle\\sum_{k\\geq1}1\\left((k-1)m<T\\right)\\operatorname*{sup}_{(k-1)m<i\\leq k m}X_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, $1\\,((k-1)m<T)$ and $\\operatorname*{sup}_{(k-1)m<i\\leq k m}X_{i}$ are independent since $1\\,((k-1)m<T)$ depends only on $\\{X_{i}\\}_{i\\in[1,(k-1)m]}$ . Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[X|X\\geq K]=\\mathbb{E}[X_{T}]\\leq\\displaystyle\\sum_{k\\geq1}\\mathbb{E}\\left[1\\left((k-1)m<T\\right)\\right]\\mathbb{E}\\left[\\operatorname*{sup}_{(k-1)m<i\\leq k m}X_{i}\\right]}\\\\ &{\\qquad\\qquad\\lesssim(1-p)^{(k-1)m}\\left(\\mathbb{E}[X]+\\frac{\\log m}{\\lambda}\\right)}\\\\ &{\\qquad\\qquad\\lesssim\\frac{1}{1-(1-p)^{m}}\\cdot\\left(\\mathbb{E}[X]+\\frac{\\log m}{\\lambda}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Choosing $\\begin{array}{r}{m=\\frac{1}{p}}\\end{array}$ so that $(1-p)^{m}\\leq\\frac{1}{e}$ gives the claim. ", "page_idx": 29}, {"type": "text", "text": "Theorem F.8 (Girsanov\u2019s theorem). For $t\\in[0,T],$ , let $\\begin{array}{r}{\\mathcal{L}_{t}=\\int_{0}^{t}b_{s}\\,\\mathrm{d}B_{s}}\\end{array}$ where $B$ is a $Q$ -Brownian motion. Assume Novikov\u2019s condition is satisfied: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_{0}^{T}\\lVert b_{s}\\rVert_{2}^{2}\\,\\mathrm{d}s\\right)\\right]<\\infty.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\mathcal{L})_{t}:=\\exp\\left(\\int_{0}^{t}b_{s}\\,\\mathrm{d}B_{s}-\\frac{1}{2}\\int_{0}^{t}\\lVert b_{s}\\rVert_{2}^{2}\\,\\mathrm{d}s\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is a $Q$ -martingale and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widetilde{B}_{t}:=B_{t}-\\int_{0}^{t}b_{s}\\,\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is a Brownian motion under $P$ where $P:=\\mathcal{E}(\\mathcal{L})_{T}Q_{i}$ , the probability distribution with density $\\mathcal{E}(\\mathcal{L})_{T}$ w.r.t. $Q$ . ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Yes, the main claims are all reflected in the scope and contributions, as discussed in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We discuss limitations of our assumptions in the introduction, after we state them, as well as in the related work and conclusion sections. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main paper contains proof overviews, and the appendices contain the full rigorous versions of all proofs of all stated lemmas and theorems. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not have any experiments. We have a small simulation that is just a illustration of a lower bound, which is very small in scope. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not have any experiments. We have a small simulation that is just a illustration of a lower bound, which is very small in scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not have any experiments. We have a small simulation that is just a illustration of a lower bound, which is very small in scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not have any experiments. We have a small simulation that is just a illustration of a lower bound, which is very small in scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not have any experiments. We have a small simulation that is just a illustration of a lower bound, which is very small in scope. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The authors read through and reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Since this work is entirely theoretical, there are no societal impacts, positive or negative. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not have any data or models to release. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: There are no assets that are used in this work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]