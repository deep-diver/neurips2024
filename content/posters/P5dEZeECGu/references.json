{"references": [{"fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-04-14", "reason": "This paper introduces Flamingo, a visual language model that is directly compared against in the zero-shot image question answering experiments, and is foundational to the work."}, {"fullname_first_author": "R. Anil", "paper_title": "Palm 2 technical report", "publication_date": "2023-05-00", "reason": "This paper describes the large language model used in conjunction with FlexCap for visual question answering, a key component of the proposed method."}, {"fullname_first_author": "X. Chen", "paper_title": "PaLI: A jointly-scaled multilingual language-image model", "publication_date": "2022-09-00", "reason": "This paper introduces PaLI, a multilingual language-image model whose dataset is used to train FlexCap, and whose results are compared against in the experiments."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a model used for evaluating open-vocabulary outputs from FlexCap, a key component of the experimental methodology."}, {"fullname_first_author": "J. Johnson", "paper_title": "Densecap: Fully convolutional localization networks for dense captioning", "publication_date": "2016-00-00", "reason": "This paper introduces DenseCap, a method that is compared against in the dense captioning experiments, providing context for the significance of FlexCap's advancements."}]}