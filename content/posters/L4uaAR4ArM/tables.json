[{"figure_path": "L4uaAR4ArM/tables/tables_6_1.jpg", "caption": "Table 1: Test perplexities (PPL; \u2193) on LM1B. Reported in He et al. [26]. Best diffusion value is bolded.", "description": "This table compares the performance of various autoregressive and diffusion language models on the One Billion Word Benchmark (LM1B) dataset.  The perplexity (PPL), a measure of how well a language model predicts a text, is reported for each model. Lower perplexity indicates better performance. The table highlights that the proposed Masked Diffusion Language Model (MDLM) achieves state-of-the-art results among diffusion models and approaches the performance of the best autoregressive models.", "section": "Experiments"}, {"figure_path": "L4uaAR4ArM/tables/tables_6_2.jpg", "caption": "Table 2: Test perplexities (PPL; \u2193) on OWT for models trained for 262B tokens. \u2020 denotes retrained models.", "description": "This table presents the test perplexity results on the OpenWebText (OWT) dataset for different language models.  The models were trained on 262 billion tokens.  The table compares the performance of an autoregressive (AR) model, a previously published masked diffusion model (SEDD), and the proposed Masked Diffusion Language Model (MDLM). The downward-pointing arrow indicates that lower perplexity scores are better.", "section": "5 Experiments"}, {"figure_path": "L4uaAR4ArM/tables/tables_7_1.jpg", "caption": "Table 3: Zero-shot perplexities (\u2193) of models trained for 524B tokens on OWT. All perplexities for diffusion models are upper bounds.", "description": "This table presents the zero-shot perplexity results on various datasets (PTB, Wikitext, LM1B, Lambada, AG News, Pubmed, Arxiv) for three different language models: an autoregressive model (AR) and two diffusion models (SEDD and MDLM). The models were trained on the OWT dataset.  The lower the perplexity, the better the model's performance. The results show how well the models generalize to unseen datasets.  Note that the perplexities for diffusion models are upper bounds.", "section": "Experiments"}, {"figure_path": "L4uaAR4ArM/tables/tables_7_2.jpg", "caption": "Table 4: GLUE evaluation results. Evaluation measures (\u2191) are F1 score for QQP and MRPC, Spearman correlations for STS-B, and accuracy for the rest. For MNLI, we report match/mismatch accuracies.", "description": "This table presents the results of the GLUE benchmark evaluation.  The GLUE benchmark is a collection of diverse natural language understanding tasks.  The table shows the performance of different models on these tasks, including Autoregressive (AR) models, BERT, and BERT fine-tuned with MDLM.  The metrics used are F1 score (for QQP and MRPC), Spearman correlation (for STS-B), and accuracy (for the remaining tasks). For the MNLI task, both match and mismatch accuracies are reported.", "section": "Downstream Task Evaluation"}, {"figure_path": "L4uaAR4ArM/tables/tables_7_3.jpg", "caption": "Table 5: Semi-AR generative perplexity (Gen. PPL; \u2193) for sequences of 2048 tokens.", "description": "This table compares the performance of SSD-LM and MDLM in terms of generative perplexity and generation speed for sequences of length 2048 tokens.  The results show that MDLM achieves significantly lower generative perplexity and much faster generation speeds compared to SSD-LM.", "section": "Semi-Autoregressive Masked Diffusion Language Models"}, {"figure_path": "L4uaAR4ArM/tables/tables_8_1.jpg", "caption": "Table 6: Test perplexities (PPL; \u2193) of generative fine-tuning of the Caduceus MLM [50] on the HG38 reference genome. Best diffusion model values are bolded. Error bars indicate the difference between the maximum and minimum values across 5 random seeds used for fine-tuning.", "description": "This table presents the results of generative fine-tuning experiments using the Caduceus MLM on the HG38 human reference genome.  The table compares the perplexity scores (lower is better) achieved by several autoregressive and diffusion models. The \"Params\" column shows the number of parameters in each model, while the \"PPL (\u2193)\" column indicates the test perplexity, with error bars representing the variability across five runs with different random seeds. The best performance among the diffusion models is highlighted in bold, enabling a direct comparison of the performance among different model types and demonstrating the effectiveness of the proposed MDLM.", "section": "5.2 Masked Diffusion DNA Models"}, {"figure_path": "L4uaAR4ArM/tables/tables_8_2.jpg", "caption": "Table 7: Genomic Benchmarks. Top-1 accuracy (\u2191) across 5-fold cross-validation (CV) for a pre-trained AR Mamba, and a pre-trained Caduceus model fine-tuned with different diffusion parameterizations. The best values per task are bolded and the second best are italicized. Error bars indicate the difference between the maximum and minimum values across 5 random seeds used for CV.", "description": "This table presents the top-1 accuracy results from 5-fold cross-validation of different models on various genomic benchmark tasks.  The models include a pre-trained autoregressive Mamba model and a pre-trained Caduceus model that has been fine-tuned using different diffusion parameterizations (Plaid, SEDD, and MDLM). The best and second-best accuracy scores are highlighted for each task, and error bars are provided to show the variability across five different random seeds.", "section": "5.2 Masked Diffusion DNA Models"}, {"figure_path": "L4uaAR4ArM/tables/tables_8_3.jpg", "caption": "Table 8: Test perplexities (PPL; \u2193) for MDLM ablations on LM1B. For the discrete-time models, we use T = 1000. Standard deviation is measured over 5 seeds during evaluation.", "description": "This table presents an ablation study on the MDLM model, evaluating the impact of different design choices on the model's performance, specifically its perplexity on the LM1B benchmark.  The study focuses on the effects of using a continuous-time framework, the carry-over unmasking property, and zero masking probabilities.  The results show that while the continuous-time framework slightly improves performance, the carry-over and zero masking properties significantly contribute to the model's overall perplexity.", "section": "5.3 Ablation Analysis"}, {"figure_path": "L4uaAR4ArM/tables/tables_33_1.jpg", "caption": "Table 1: Test perplexities (PPL; \u2193) on LM1B. Reported in He et al. [26]. Best diffusion value is bolded.", "description": "The table compares the performance of various autoregressive and diffusion language models on the LM1B benchmark in terms of perplexity (PPL). Lower perplexity indicates better performance.  It highlights the state-of-the-art performance achieved by the proposed Masked Diffusion Language Model (MDLM) compared to existing models, showcasing its improved efficiency and performance on this benchmark.", "section": "5 Experiments"}, {"figure_path": "L4uaAR4ArM/tables/tables_33_2.jpg", "caption": "Table 1: Test perplexities (PPL; \u2193) on LM1B. Reported in He et al. [26]. Best diffusion value is bolded.", "description": "This table compares the performance of various autoregressive and diffusion language models on the LM1B benchmark.  The perplexity (PPL), a measure of how well a model predicts a test set, is reported. Lower perplexity values indicate better performance. The table highlights that the proposed Masked Diffusion Language Model (MDLM) achieves a new state-of-the-art among diffusion models, approaching the perplexity of autoregressive models. The number of parameters for each model is also listed.", "section": "Experiments"}, {"figure_path": "L4uaAR4ArM/tables/tables_34_1.jpg", "caption": "Table 1: Test perplexities (PPL; \u2193) on LM1B. Reported in He et al. [26]. Best diffusion value is bolded.", "description": "This table presents the test perplexity results on the One Billion Word benchmark (LM1B).  It compares the performance of various autoregressive and diffusion language models, highlighting the state-of-the-art achieved by the proposed Masked Diffusion Language Model (MDLM). The table shows that MDLM significantly outperforms previous diffusion models and approaches the performance of the best autoregressive models, demonstrating the effectiveness of the proposed approach.", "section": "5 Experiments"}, {"figure_path": "L4uaAR4ArM/tables/tables_35_1.jpg", "caption": "Table 12: Ablation on time-conditioning in MDLM on OWT.", "description": "This table shows the results of an ablation study on the impact of time-conditioning in the MDLM model trained on the OpenWebText dataset.  It compares the perplexity (PPL) achieved by the model with and without time conditioning. The results indicate that time conditioning has a minimal impact on the model's performance.", "section": "E.5 Time-conditioning ablation on OWT"}]