[{"type": "text", "text": "Simple and Effective Masked Diffusion Language Models ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "L4uaAR4ArM/tmp/3b7dd92b4197e5d55f64b7b3d3d0e7ae636151f6ca185eff3c5d2a23d97e586b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form\u2014it is a mixture of classical masked language modeling losses\u2014 and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We provide the code1, along with a blog post and video tutorial2 on the project page: ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models excel at producing realistic, high-quality images and have received significant attention as potential tools for generating discrete data, such as text [1, 31, 33], biological sequences [2, 47], and graphs [60, 63]. Unlike autoregressive (AR) approaches, diffusion-based methods are not constrained to generate data sequentially, and therefore have the potential to improve long-term planning, controllable generation, and sampling speed. However, discrete diffusion methods exhibit a performance gap relative to AR models [1, 23, 26, 33], especially in language modeling. The standard measure of language modeling performance is log-likelihood: when controlling for parameter count, prior work reports a sizable log-likelihood gap between AR and diffusion models. ", "page_idx": 0}, {"type": "text", "text": "In this work, we show that simple masked diffusion language modeling (MDLM) combined with effective training recipes is more performant than previously thought [1, 26]. We develop a wellengineered MDLM implementation that significantly improves discrete diffusion log-likelihood; we further improve likelihood using a simple substitution-based parameterization of the reverse diffusion process that enables deriving a Rao-Blackwellized continuous-time variational lower bound (ELBO) with improved tightness [49]. Interestingly, our objective has a simple form: it is a weighted average of masked language modeling (MLM) losses [15], and can be used to endow BERT-style, encoder-only models with principled generation capabilities. We complement this framework with efficient samplers\u2014including ones that can generate semi-autoregressively like a typical language model. ", "page_idx": 0}, {"type": "image", "img_path": "L4uaAR4ArM/tmp/25b2c664c1f912a2b87be6e640e4e93f7459fbe2483f4ee5ed840c8e316c8c32.jpg", "img_caption": ["Figure 1: (Left) Our proposed masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM\u2019s objective correspond to a principled variational lower bound, and supports generation via ancestral sampling. (Bottom Right) Perplexity (PPL) on One Billion Words (LM1B) benchmark. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our masked diffusion models achieve a new state-of-the-art among diffusion models on language modeling benchmarks and approach the perplexity of AR models within $15{-}25\\%$ . Surprisingly, simple engineering choices significantly improve performance in both our models and simple baselines that were previously thought to perform poorly. Our framework also extends to non-language domains, including biological sequence modeling. We pre-train DNA sequence models and observe similar or higher downstream performance compared to classical BERT-style training, while also introducing generative capabilities that classical masked DNA language models lack. ", "page_idx": 1}, {"type": "text", "text": "Contributions We describe (1) a simple masked diffusion language modeling (MDLM) framework with a well-engineered implementation that outperforms all existing diffusion models across language modeling benchmarks (LM1B [8], OWT [18], DNA [12]), and that significantly improves the performance of existing baselines [1, 26]. Our MDLM framework implements (2a) a substitution-based parameterization (SUBS) of the reverse unmasking diffusion process; SUBS allows us to derive (2b) a simple, continuous-time, Rao-Blackwellized objective that improves tightness and variance of the ELBO, further increasing performance. We complement MDLM with (3) fast samplers that support semi-autoregressive (SAR) generation and outperform previous SAR models. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Diffusion Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion models are trained to iteratively undo a forward corruption process $q$ that takes clean data $\\mathbf{x}$ drawn from the data distribution $q(\\mathbf{x})$ and defines latent variables $\\mathbf{z}_{t}$ for $t\\!\\in\\![0,1]$ that represent progressively noisy versions of x [27, 54, 56, 66, 48, 19]. The standard forward process for continuous $\\mathbf{x}$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t}\\!=\\!\\sqrt{\\alpha_{t}}\\mathbf{x}\\!+\\!\\sqrt{1\\!-\\!\\alpha_{t}}\\epsilon\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\epsilon\\!\\sim\\!\\mathcal{N}(\\mathbf{0},\\!\\mathbf{I})$ and $(\\alpha_{t})_{t\\in[0,1]}$ is a noise schedule, monotonically decreasing in $t$ . The parameterized reverse diffusion model $p_{\\theta}$ over $\\mathbf{x}$ and $\\mathbf{z}_{t}$ is trained to maximize a variational lower bound on loglikelihood (ELBO). Given a number of discretization steps $T$ , defining $s(i)\\!=\\!(i\\!-\\!1)/T$ and $t(i)\\!=\\!i/\\breve{T}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q}\\left[\\underbrace{-\\mathrm{log}p\\theta(\\mathbf{x}|\\mathbf{z}_{t(0)})}_{\\mathcal{L}_{\\mathrm{recons}}}+\\underbrace{\\sum_{i=1}^{T}\\!D\\mathrm{KL}[q(\\mathbf{z}_{s(i)}|\\mathbf{z}_{t(i)},\\mathbf{x})||p_{\\theta}(\\mathbf{z}_{s(i)}|\\mathbf{z}_{t(i)})]}_{\\mathcal{L}_{\\mathrm{diffusion}}}\\right]+\\underbrace{\\!D\\mathrm{KL}[q(\\mathbf{z}_{t(T)}|\\mathbf{x})||p_{\\theta}(\\mathbf{z}_{t(T)})]}_{\\mathcal{L}_{\\mathrm{prior}}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For brevity, we drop $i$ from $t(i)$ and $s(i)$ below; in general, $s$ will denote the time step before $t$ ", "page_idx": 2}, {"type": "text", "text": "2.2 Discrete Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Applications of diffusion modeling to discrete data can be broken into two broad categories. First are works that embed discrete structures in continuous space and then perform the Gaussian diffusion defined above on these continuous representations [9, 16, 23, 24, 30, 34, 57]. More related to our method are works that define a diffusion process directly on discrete structures. D3PM [1] introduces a framework with a Markov forward process $q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})\\!=\\!\\mathbf{Cat}(\\mathbf{z}_{t};Q_{t}\\mathbf{z}_{t-1})$ defined by the multiplication of matrices $Q_{t}$ over $T$ discrete time steps. This process induces marginals ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}|\\mathbf{x})\\!=\\!\\mathrm{Cat}(\\mathbf{z}_{t};\\bar{Q}_{t}\\mathbf{x})\\!=\\!\\mathrm{Cat}(\\mathbf{z}_{t};\\boldsymbol{Q}_{t}\\cdot Q_{t-1}\\cdots Q_{1}\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "that represent the discrete-state form of (1). Extending this formalism to continuous time (as in (1)) relies on continuous time Markov chain (CTMC) theory [5]. The CTMC framework in turns leads to generalizations of the score matching perspective on diffusion modeling [55] to discrete data [33, 59]. Notably, SEDD [33] connects score-based approaches with ELBO maximization, enabling performant likelihood-based training of score-based models. ", "page_idx": 2}, {"type": "text", "text": "3 Simple Masked Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "While previous work on discrete diffusion supports general forward processes (e.g., general $Q_{t}$ in D3PM), absorbing state (i.e., masking) diffusion consistently achieves the best performance [1, 33]. In this work, instead of supporting general noise processes, we focus on masking and derive tight Rao-Blackwellized objectives that outperform general approaches and do not require CTMC theory. In this section, we first define the diffusion process for a categorical random variable. Later in Sec. 3.5, we extend this process to sequences containing multiple such categorical variables. We denote our overall approach as Masked Diffusion Language Models (MDLM). ", "page_idx": 2}, {"type": "text", "text": "Notation. We denote scalar discrete random variables with $K$ categories as \u2018one-hot\u2019 column vectors and define $\\begin{array}{r}{\\mathcal{V}\\in\\left\\{\\mathbf{x}\\in\\{0,1\\}^{K}:\\sum_{i=1}^{K}\\mathbf{x}_{i}=1\\right\\}}\\end{array}$ as the set of all such vectors. Define $\\operatorname{Cat}(\\cdot;\\pi)$ as the categorical distribution over $K$ classes with probabilities given by $\\pi\\in\\Delta^{K}$ , where $\\Delta^{K}$ denotes the $K$ -simplex. We also assume that the $K$ -th category corresponds to a special [MASK] token and let $\\mathbf m\\in\\mathcal V$ be the one-hot vector for this mask, i.e., $\\mathbf{m}_{K}=1$ . Additionally, let $\\mathbf{1}=\\left\\{1\\right\\}^{K}$ and $\\langle\\mathbf{a},\\mathbf{b}\\rangle$ and $\\mathbf{a}\\odot$ b respectively denote the dot and Hadamard products between two vectors a and b. ", "page_idx": 2}, {"type": "text", "text": "3.1 Interpolating Discrete Diffusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We restrict our attention to forward processes $q$ that interpolate between clean data $\\mathbf{x}\\!\\in\\!\\mathcal{V}$ and a target distribution $\\operatorname{Cat}(.,\\pi)$ , forming a direct extension of Gaussian diffusion in (1). Let $q$ define a sequence of increasingly noisy latent variables $\\mathbf{z}_{t}\\in\\mathcal{V}$ , where the time step $t$ runs from $t\\!=\\!0$ (least noisy) to $t\\!=\\!1$ (most noisy). The marginal of $\\mathbf{z}_{t}$ conditioned on $\\mathbf{x}$ at time $t$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\mathbf{z}_{t}|\\mathbf{x})\\!=\\!\\mathrm{Cat}(\\mathbf{z}_{t};\\!\\alpha_{t}\\mathbf{x}\\!+\\!(1\\!-\\!\\alpha_{t})\\pmb{\\pi}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha_{t}\\in[0,1]$ is a strictly decreasing function in $t$ , with $\\alpha_{0}\\!\\approx\\!1$ and $\\alpha_{1}\\!\\approx\\!0$ ; see Suppl. E.1 for details. This implies transition probabilities $q(\\mathbf{z}_{t}|\\mathbf{z}_{s})=\\mathrm{Cat}(\\mathbf{z}_{t};\\alpha_{t|s}\\mathbf{z}_{s}+(1-\\alpha_{t|s})\\pi)$ , where $\\alpha_{t|s}=\\alpha_{t}/\\alpha_{s}$ . This indicates that during each diffusion step from $s{\\rightarrow}t$ , a fraction $(1\\!-\\!\\alpha_{t|s})$ of the probability mass is transferred to the prior distribution $\\pi$ . The reverse posterior is given as (see Suppl. 16 for details): ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\!=\\!\\mathrm{Cat}\\!\\left(\\mathbf{z}_{s};\\!\\frac{[\\alpha_{t|s}\\mathbf{z}_{t}+(1-\\alpha_{t|s})\\mathbf{1}\\pi^{\\top}\\mathbf{z}_{t}]\\odot[\\alpha_{s}\\mathbf{x}+(1-\\alpha_{s})\\pi]}{\\alpha_{t}\\mathbf{z}_{t}^{\\top}\\mathbf{x}+(1-\\alpha_{t})\\mathbf{z}_{t}^{\\top}\\pi}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While (4) and (5) represent a special case of the more general diffusion processes proposed in D3PM [1], we show below that they yield a simplified variational lower bound objective and admit straightforward continuous time extensions. ", "page_idx": 2}, {"type": "text", "text": "3.2 Masked Diffusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Next, we focus on masking processes and derive a simple Rao-Blackwellized objective for this choice of $q$ . This objective incurs lower variance during training and improves tightness. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Forward Masking Process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In masked (i.e., absorbing state) diffusion, we set $\\pi\\!=\\!\\mathbf{m}$ . At each noising step, $t$ , the input $\\mathbf{x}$ transitions to a \u2018masked\u2019 state m with some probability. If an input transitions to $\\mathbf{m}$ at any time $t^{\\prime}$ , it will remain in this state for all $t\\!>\\!t^{\\prime}\\!:\\!q(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t^{\\prime}}\\!=\\!\\mathbf{m})\\!=\\!\\mathbf{Cat}(\\mathbf{z}_{t};\\mathbf{m})$ . At time $T$ , all inputs are masked with probability 1. ", "page_idx": 3}, {"type": "text", "text": "The marginal of the forward process (4) is given by $q(\\mathbf{z}_{t}|\\mathbf{x})=\\mathrm{Cat}(\\mathbf{z}_{t};\\alpha_{t}\\mathbf{x}+(1-\\alpha_{t})\\mathbf{m})$ . Using properties of the masking process, the posterior $q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\!\\mathbf{\\dot{x}})$ simplifies (5); see Suppl. A.2: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\!=\\!\\left\\{\\begin{array}{l l}{\\mathrm{Cat}(\\mathbf{z}_{s};\\!\\mathbf{z}_{t})}&{\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m},}\\\\ {\\mathrm{Cat}\\!\\left(\\mathbf{z}_{s};\\!\\frac{(1-\\alpha_{s})\\mathbf{m}+(\\alpha_{s}-\\alpha_{t})\\mathbf{x}}{1-\\alpha_{t}}\\right)}&{\\mathbf{z}_{t}\\!=\\!\\mathbf{m}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2.2 Reverse Unmasking Process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The reverse process inverts the noise process defined by $q$ . We consider both a finite number of steps $T$ , as well as a continuous time model corresponding to $T\\!\\to\\!\\infty$ . We begin with the discrete-time case for which the generative model is expressed as $\\begin{array}{r}{p_{\\theta}(\\bar{\\bf x)}\\!=\\!\\int_{\\mathbf{z}}\\!p_{\\theta}(\\mathbf{z}_{1})p_{\\theta}(\\mathbf{x}|\\bar{\\mathbf{z}}_{0})\\prod_{i=1}^{T}\\!p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})\\mathrm{d}\\mathbf{z}_{0:1}.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "The optimal form for $p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})$ matches the true posterior in (6): this follows immediately from the definition of the diffusion objective in (2), which is a sum of terms of the form $\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\|p_{\\theta}\\big(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\big)\\big)$ . However, (6) is conditioned on $\\mathbf{x}$ , which we do not know. Therefore, we introduce a model $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t):\\mathcal{V}\\times[0,1]\\to\\Delta^{K}$ that approximates $\\mathbf{x}$ with a neural network. We can also omit explicit dependence of $\\mathbf{x}_{\\theta}$ on time $t$ , which simplifies sampling, yielding a $2\\mathbf{x}$ inference speed-up (see Suppl. E.2). ", "page_idx": 3}, {"type": "text", "text": "3.2.3 SUBS Parameterization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The specific parameterization for $p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})$ that we use is ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}\\big(\\mathbf{z}_{s}\\big|\\mathbf{z}_{t}\\big)\\!=\\!q\\big(\\mathbf{z}_{s}\\big|\\mathbf{z}_{t},\\mathbf{x}\\!=\\!\\mathbf{x}_{\\theta}\\big(\\mathbf{z}_{t},t\\big)\\big)\\!=\\!\\left\\{\\begin{array}{l l}{\\mathrm{Cat}(\\mathbf{z}_{s};\\mathbf{z}_{t}),}&{\\mathbf{z}_{t}\\not=\\mathbf{m},}\\\\ {\\mathrm{Cat}\\Big(\\mathbf{z}_{s};\\frac{(1-\\alpha_{s})\\mathbf{m}+(\\alpha_{s}-\\alpha_{t})\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}{1-\\alpha_{t}}\\Big).}&{\\mathbf{z}_{t}\\mathrm{=}\\mathbf{m},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, we induce 2 key properties of the absorbing state diffusion process into our denoising model, $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)$ : an unmasked token remains unchanged during reverse diffusion, and the clean input is never masked. We implement these as substitutions to the output of $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)$ , hence we call our parameterization SUBS. ", "page_idx": 3}, {"type": "text", "text": "Zero Masking Probabilities First, notice that by definition, $\\langle\\mathbf{x},\\mathbf{m}\\rangle\\!=\\!0$ . For this reason, we design the denoising network such that $\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle\\!=\\!\\dot{0}$ , i.e., we substitute the logit index corresponding to the [MASK] token with $-\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "Carry-Over Unmasking Second, if $\\mathbf{z}_{t}$ is unmasked, then we desire $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)\\!=\\!\\mathbf{z}_{t}$ , i.e., unmasked latents are \u2018carried over\u2019. We accomplish this by substituting the output of our network to simply copy unmasked inputs. ", "page_idx": 3}, {"type": "text", "text": "In Suppl. B.1, we show that \u201cZero Masking Probabilities\u201d property simplifies the D3PM\u2019s NELBO (39) to (41), and \u201cCarry-Over Unmasking\u201d futher simplifies (41) to (43) whose continuous time equivalent is the simplified NELBO (10). Table 8 shows that each simplification leads to an improved likelihood. ", "page_idx": 3}, {"type": "text", "text": "3.3 Rao-Blackwellized Likelihood Bounds ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall from (2) that the diffusion traning objective has the form $\\mathcal{L}_{\\mathrm{recons}}+\\mathcal{L}_{\\mathrm{diffusion}}+\\mathcal{L}_{\\mathrm{prior}}$ . For the simplified reverse process in (7), the discrete-time diffusion loss for finite $T$ simplifies to (Suppl. B.1.3): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{diffusion}}=\\sum_{i=1}^{T}\\mathbb{E}_{q}\\left[\\mathrm{D}_{\\mathrm{KL}}\\left(q(\\mathbf{z}_{s(i)}|\\mathbf{z}_{t(i)},\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}_{s(i)}|\\mathbf{z}_{t(i)})\\right)\\right]=\\sum_{i=1}^{T}\\mathbb{E}_{q}\\left[\\frac{\\alpha_{t(i)}-\\alpha_{s(i)}}{1-\\alpha_{t(i)}}\\mathrm{log}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t(i)}),\\mathbf{x}\\rangle\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that this objective is simpler and more well-behaved than the expression one would obtain for $\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})||\\tilde{p}_{\\theta}\\big(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\big)\\big)$ under the parameterization induced by using $p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})=q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x}=$ $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t))$ from (5), which is similar to what is used by D3PM [1] (see Suppl. A.2.4): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left[\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\mathrm{log}\\frac{\\alpha_{t}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{t})}{(1-\\alpha_{t})\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle}+\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\mathrm{log}\\frac{(1-\\alpha_{s})(\\alpha_{t}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{t}))}{(1-\\alpha_{t})(\\alpha_{s}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{s}))}\\right]\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We refer to the process of obtaining (8) in lieu of (9) as a form of Rao-Blackwellization. Specifically, we analytically compute expectations such as $\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle\\!=\\!0$ in order to simplify objective (9) to obtain (8). Without analytical simplifications, a model must learn $\\theta$ such that $\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle\\!=\\!0$ holds. Unlike in regular Rao-Blackwellization, simplifications are possible because of modeling choices for $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)$ (zero masking probabilities and carry-over unmasking). In that sense, our approach has similarities to graphical modeling, where incorporating conditional independencies into $p_{\\theta}$ sets certain log-likelihood terms to zero. However, our approach also empirically helps reduce variance, hence we refer to it as Rao-Blackwellization, somewhat abusing the usual terminology. ", "page_idx": 4}, {"type": "text", "text": "3.4 Continuous-Time Likelihood Bounds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Previous works have shown empirically and mathematically that increasing the number of steps $T$ yields a tighter approximation to the ELBO [29]. Following a similar argument, we form an continuous extension of (8) by taking $T\\!\\to\\!\\infty$ (see Suppl. B.2), which yields the following NELBO, $\\mathcal{L}_{\\mathrm{NELBO}}^{\\infty}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{NELBO}}^{\\infty}\\!=\\!\\mathbb{E}_{q}\\!\\int_{t=0}^{t=1}\\!\\frac{\\alpha_{t}^{\\prime}}{1\\!-\\!\\alpha_{t}}\\mathrm{log}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Invariance to the noise schedule The function $\\alpha_{t}$ is invertible due to the monotonicity assumption in Sec. 3.1, and so we can perform the following change of variables in (10): $\\gamma\\equiv\\log(1-\\alpha_{t})$ . Thus, the diffusion loss can be equivalently expressed as $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{NLBO}}^{\\infty}\\!=\\!-\\mathbb{E}_{q}\\!\\int_{\\gamma=-\\infty}^{\\gamma=0}\\!\\log\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{\\gamma},\\!\\gamma),\\!\\mathbf{x}\\rangle\\mathrm{d}\\gamma}\\end{array}$ ; see Suppl. E.1.1 for details. This new formulation demonstrates that the diffusion loss is invariant to the functional form of $\\alpha_{t}$ , which we verify empirically in Suppl. E.1. ", "page_idx": 4}, {"type": "text", "text": "3.5 Masked Diffusion Language Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we apply masked diffusion to language modeling over sequences $\\mathbf{x}^{1:L}$ of $L$ tokens, with $\\mathbf{x}^{\\ell}$ denoting the $\\ell$ -th token. We make the assumption that the forward noising process is applied independently across a sequence and that, conditioned on a sequence of latents $\\bar{\\mathbf{z}_{t}^{1:L}}$ , the denoising process factorizes independently across tokens, i.e., $\\begin{array}{r}{p_{\\boldsymbol{\\theta}}\\big(\\mathbf{z}_{s}^{1:L}\\mid\\mathbf{z}_{t}^{1:L}\\big)=\\prod_{\\ell=1}^{L}p_{\\boldsymbol{\\theta}}\\big(\\mathbf{z}_{s}^{\\ell}\\mid\\mathbf{z}_{t}^{1:L}\\big)}\\end{array}$ . Thus, we use a single model to compute $\\mathbf{x}_{\\theta}^{\\ell}(\\mathbf{z}_{t}^{1:L},t)$ for each $\\ell$ from a masked sequence $\\mathbf{z}_{t}$ , optimizing: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{NELBO}}^{\\infty}\\!=\\!\\mathbb{E}_{q}\\!\\int_{t=0}^{t=1}\\!\\frac{\\alpha_{t}^{\\prime}}{1\\!-\\!\\alpha_{t}}\\!\\sum_{\\ell}\\!\\log\\langle\\mathbf{x}_{\\theta}^{\\ell}(\\mathbf{z}_{t}^{1:L},t),\\mathbf{x}^{\\ell}\\rangle\\!\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Interestingly, our objective has a simple form: it is the weighted average of masked language modeling (MLM) losses [15]. Thus our work establishes a connection between generative diffusion models and encoder-only BERT models. Our objective enables principled selection of a (randomized) masking rate, and also endows BERT-style models with principled generation capabilities; see Sec. 6. The full training algorithm is provided in Suppl. B.3. ", "page_idx": 4}, {"type": "text", "text": "Note: Although (11) imposes a loss on all tokens, unmasked tokens don\u2019t contribute to the loss, as they are copied over by the denoising network due to \u201ccarry-over unmasking\u201d (Sec. 3.2.3), effectively reducing $\\log\\langle\\mathbf{x}_{\\theta}^{\\ell}(\\mathbf{z}_{t}^{1:L},t),\\mathbf{x}^{\\ell}\\rangle$ to zero. ", "page_idx": 4}, {"type": "text", "text": "3.5.1 Training Considerations for Masked Diffusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One of the key contributions of our work is a well-engineered implementation of masked diffusion models. Our experiments demonstrate that these improvements greatly boost performance even for methods previously thought to perform poorly, e.g., Austin et al. [1]. Below we briefly summarize these implementation details. First, we find that tokenization is critical to performance. Small vocabularies, such as the $8\\mathbf{k}$ vocabulary in Austin et al. [1], result in longer-range dependencies that decrease the performance of both diffusion and AR models. Additionally, by focusing on masked diffusion, we are able to provide a numerically stable implementation of the objective function. Namely, since previous formulations of discrete diffusion were constructed to accommodate a wide range of limiting distributions [1], the objective was implemented by materializing the full transition matrices $\\bar{Q}_{t}$ and posterior probabilities. In contrast, we evaluate $D_{\\mathrm{KL}}\\big[q(\\mathbf{z}_{s}\\,|\\,\\mathbf{z}_{t},\\mathbf{x})||p_{\\theta}(\\mathbf{z}_{s}\\,|\\,\\mathbf{z}_{t})\\big]$ by examining only the masked token indices rather than comparing the full true and approximate posterior distributions. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Furthermore, we modernize the architecture for the denoising network relative to D3PM [1]. In lieu of the T5 architecture used in D3PM, we use the diffusion transformer (DiT) introduced in Peebles & Xie [42], which integrates time step conditioning into a standard encoder-only transformer [62] and uses rotary positional embeddings [58]. In addition, we implement a low-discrepancy sampler that reduces the variance of the ELBO, similar to Kingma et al. [29] and draws correlated samples $t_{i}$ rather than performing i.i.d. sampling. ", "page_idx": 5}, {"type": "text", "text": "4 Inference and Sampling in Masked Diffusion Language Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Efficient Ancestral Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To generate a sequence of length $L$ , the reverse diffusion process starts with the sequence $\\mathbf{z}_{t=1}^{1:L}$ where ${\\bf z}_{t=1}^{\\ell}={\\bf m}$ , for all $\\ell\\in\\{1,...,L\\}$ . Then the subsequent latents, $\\mathbf{z}_{t}^{1:L}$ are generated by discretizing the reverse diffusion process with some finite $T$ . Given $\\mathbf{z}_{t}^{1:L}$ , we construct $\\mathbf{\\bar{z}}_{s}^{1:L}$ by sampling each token ${\\bf z}_{s}^{\\ell}$ independently from the distribution $p_{\\theta}\\big(\\mathbf{z}_{s}^{\\ell}\\big|\\mathbf{z}_{t}^{1:L}\\big)$ given in (7). ", "page_idx": 5}, {"type": "text", "text": "bNeoctoe tmhea tu inn mthaes kreedv e(rwseh ipcrho cceasns , oucncumra sokfteedn t ionk eenarsl rye dmeanion isuinncgh astnaggeeds.  fTohr ulas,r gife )n, ethwe tno $\\mathbf{z}_{\\mathrm{{e}}}^{1:L}$ . $T$ ${\\bf z}_{s}^{1:L}={\\bf z}_{t}^{\\tilde{1}:L}$ Additionally if the denoising model, $\\mathbf{x}_{\\theta}\\big(\\mathbf{z}_{t}^{1:L}\\big)$ is not conditioned on time, then we can simply draw a new sample from $p_{\\theta}(\\mathbf{z}_{s-1/T}^{1:L}|\\mathbf{z}_{s}^{1:L})$ using the previously computed and cached value $\\mathbf{x}_{\\theta}\\big(\\mathbf{z}_{t}^{1:L}\\big)$ . This means we have effectively \u201cskipped\u201d over the time step $s$ , saving a function call to the denoising network. Note that SEDD [33] does not support this caching because the denoising network models time-dependent rates, which requires conditioning on time. ", "page_idx": 5}, {"type": "text", "text": "4.2 Semi-Autoregressive Masked Diffusion Language Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our method also admits an effective semi-autoregressive (SAR) decoding method that allows the model to generate sequences of arbitrary length [24, 52, 53]. Let $\\tilde{\\mathbf{x}}^{1:L}$ represent the output from sampling a sequence of $L$ tokens using the reverse diffusion process described above. To generate additional $L^{\\prime}\\!<\\!L$ tokens, we propose a generation algorithm in which the latter $L\\!-\\!L^{\\prime}$ tokens $\\tilde{\\mathbf{x}}^{L^{\\prime}:L}$ are used as a prefix for an additional round of generation. Given the carry-over unmasking described in Sec. 3.2.3, these prefix tokens will simply be copied over at each decoding step. The remaining tokens are generated as above with z\u2113s \u223cp\u03b8(z\u2113s |ztL:L+L\u2032) for all \u2113\u2208{L+1,...,L+L\u2032}, with ztL=\u22121L initialized to $\\tilde{\\mathbf{x}}^{L-L^{\\prime}:L}$ as opposed to being initialized as masked tokens $\\mathbf{m}$ . At the end of this process, we have produced $L\\!+\\!L^{\\prime}$ tokens $\\mathrm{concat}\\bar{[\\tilde{\\mathbf{x}}^{1:L},\\tilde{\\mathbf{x}}^{L+1:L+L^{\\prime}}]}$ , where concat $[\\cdot]$ denotes concatenation along the sequence length dimension. This process can repeat indefinitely, with the prefix shifted for every new round of generation. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Masked Diffusion Language Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Experimental Setup We evaluate MDLM as a generative model of language and as a representation model via fine-tuning on downstream tasks. ", "page_idx": 5}, {"type": "text", "text": "For language modeling likelihood evaluation, we conduct experiments on two datasets: The One Billion Words Dataset (LM1B; [8]) and OpenWebText (OWT; [18]). We use the bert-base-uncased tokenizer for LM1B, and report perplexities on the test split. Models have a context size of 128. For OWT, which does not have a pre-defined split, we reserve the last 100K documents as a held-out validation set and report perplexities on this set. We use the GPT2 tokenizer [45] for OWT. Models have a context size of 1,024. We utilize the transformer architecture from Lou et al. [33], which augments the diffusion transformer [42] with rotary embeddings [58]. MDLM was trained for 1M or 10M steps (corresponding to 33B, 327B tokens, respectively) on LM1B and 1M steps on OWT (which corresponds to 262B tokens). The corresponding AR baseline was trained for half the number of steps to ensure similar number of tokens seen (details in Suppl. D.2). Full hyperparameters are given in Suppl. D.4. On OWT, we train with and without time step conditioning. ", "page_idx": 5}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/f02a0757b88f85bc562fa20bcf93b6861b2b388567bde2b82e156d65b7aa11f0.jpg", "table_caption": ["Table 1: Test perplexities (PPL; \u2193) on LM1B. \u2020Reported in He et al. [26]. Best diffusion value is bolded. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "For representation learning, we pre-train models on the C4 dataset [46], then fine-tune and evaluate models on the GLUE benchmark [65]. Models have a context size of 128. We use the bert-base-uncased tokenizer for the representation learning experiments. We utilize the MosaicBERT architecture from Portes et al. [43], an extension of the original BERT architecture [15]. We pre-train a bidirectional MosaicBERT using an MLM objective for 37B tokens of C4, as well as a causal variant on the same data. We further fine-tune MosaicBERT model using the MDLM for 327M tokens, less than $1\\%$ of the pre-training data. We provide the full hyperparameters in Suppl. D.6. ", "page_idx": 6}, {"type": "text", "text": "Likelihood Evaluation On LM1B, MDLM outperforms all previous diffusion methods (Table 1). Compared to the SEDD baseline reported by Lou et al. [33], trained for 33B tokens, MDLM, which we train for the same amount, achieves a $17\\%$ improvement on the perplexity bound. Finally, MDLM gets within $14\\%$ of an AR baseline and continues to improve with more training. We see the same trend for models trained on OWT, a larger dataset, shown in Table 2 \u2013 MDLM outperforms prior diffusion methods, closing the gap towards AR models. In Table 12 we find that models trained with and without time conditioning attain similar perplexities on OWT. Additionally, Figure 3 demonstrates the reduced variance we achieve from our objective, when compared to previous masked diffusion models such as SEDD [33]. ", "page_idx": 6}, {"type": "text", "text": "Zero-Shot Likelihood Evaluation We also explore models\u2019 ability to generalize by taking models trained on OWT and evaluating how well they model unseen datasets. We compare the perplexities of our MDLM with SEDD [1] and an AR Transformer language model. Our zero-shot datasets include the validation splits of Penn Tree Bank (PTB; [36]), Wikitext [38], LM1B, Lambada [41], AG News [68], and Scientific Papers (Pubmed and Arxiv subsets; [10]). Full experimental details are available in Suppl. D.4. ", "page_idx": 6}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/d31171893aff25c901b002068ea56801ada621b14a689a6ff2e667ff62f6c5f7.jpg", "table_caption": ["Table 2: Test perplexities (PPL; \u2193) on OWT for models trained for 262B tokens. \u2020 denotes retrained models. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "MDLM consistently outperforms the SEDD diffusion parameterization. In some cases, e.g., for Lambada and Scientific Papers, MDLM attains better perplexity than AR. We hypothesize that these datasets are farther from OWT, and that diffusion models may be more robust to out-of-domain evaluation due to the unmasking-based objective. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Downstream Task Evaluation We find that BERT fine-tuned with MDLM to be a generative model results in strong perplexities while preserving performance on downstream tasks. On the C4 validation set, the AR model attains perplexity (PPL) of 22, the pre-trained BERT attains a PPL upper bound of 78 (evaluated using the MDLM variational bound), and BERT $^+$ MDLM-FT attains a PPL upper bound of 35. In Table 4, we further find that BERT $^+$ MDLM fine-tuning has no degradation in downstream ", "page_idx": 6}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/03d3e957bc831d00b4cc1c21a30619d5d3b67c6632b485e53451e8e8d509cea1.jpg", "table_caption": ["Table 3: Zero-shot perplexities (\u2193) of models trained for 524B tokens on OWT. All perplexities for diffusion models are upper bounds. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/3d8aa033e52a0ee9e89742f41e44010a0f2f63661fdaf8aa71f6e24a495a2333.jpg", "table_caption": ["Table 4: GLUE evaluation results. Evaluation measures (\u2191) are F1 score for QQP and MRPC, Spearman correlations for STS-B, and accuracy for the rest. For MNLI, we report match/mismatch accuracies. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "GLUE performance compared to the BERT initialization. While the perplexity of our method is higher than the AR baseline, the downstream task performance is significantly better. ", "page_idx": 7}, {"type": "text", "text": "Semi-Autoregressive Modeling To test the SAR decoding algorithm presented in Sec. 4.2, we compare to SSD-LM [24] a diffusion model that was designed to generate blocks of text autoregressively. We generate 200 sequences of length 2048 tokens on a single 3090 GPU and evaluate generative perplexity under a pre-trained GPT-2 [45] model. The ", "page_idx": 7}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/a7972ad773af72b877a005e07f0f08d61767df3fa72ab819c0318c638ac4a3a5.jpg", "table_caption": ["Table 5: Semi-AR generative perplexity (Gen. PPL; \u2193) for sequences of 2048 tokens. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "SSD-LM sequences are generated using blocks of ", "page_idx": 7}, {"type": "text", "text": "25 tokens (as implemented in their pre-trained model) and the MDLM sequences are generated using $L^{\\prime}\\!=\\!512$ . In Table 5, we find that in addition to achieving better generative perplexity, MDLM enables ${\\sim}25{-}30\\mathrm{x}$ faster SAR decoding relative to SSD-LM. ", "page_idx": 7}, {"type": "text", "text": "5.2 Masked Diffusion DNA Models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We also explore applications to the generative modeling of biological sequences [14, 47] using a state space model (SSM) backbone [22]. Namely, we build on the recently-proposed Caduceus DNA language model [50], which uses as a backbone the data-dependent SSM Mamba block [21]. ", "page_idx": 7}, {"type": "text", "text": "Experimental Setup We pre-train the encoder-only Caduceus [50], which is an MLM, on the HG38 human reference genome [11] and perform fine-tuning using our diffusion parameterization. We use a context length of 1024 tokens and follow Schiff et al. [50] for the experimental setup, other than learning rate which was reduced to 1e-3. See Suppl. D.7 for full experimental details. We assess both generative performance using perplexity and downstream performance on Genomics Benchmarks [20] across language diffusion paradigms and AR models. ", "page_idx": 7}, {"type": "text", "text": "Generative Performance We fine-tune the Caduceus MLM across diffusion parameterizations and compare perplexities against AR models. We report perplexity values in Table 6. MDLM outperforms all other diffusion language modeling schemes. ", "page_idx": 7}, {"type": "text", "text": "Downstream Task Fine-tuning We perform downstream evaluation with the Genomics Benchmarks [20], a recently proposed benchmark with eight regulatory element classification tasks. As shown in Table 7, our generative fine-tuning paradigm preserves or improves upon downstream performance from MLM pre-training. Absorbing-state diffusion methods outperform Plaid across tasks except for the simplest task Human vs. Worm, where all methods have roughly the same performance. For tasks where the input is a biased subsample of the full genome, we observe that the correlation between perplexity and downstream performance is weaker; see Suppl. D.7. ", "page_idx": 7}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/dc670b6efe990e70ce586e0f945094656242f22810b79341acfac9e67908b1a4.jpg", "table_caption": ["Table 6: Test perplexities (PPL; $\\downarrow$ ) of generative fine-tuning of the Caduceus MLM [50] on the HG38 reference genome. Best diffusion model values are bolded. Error bars indicate the difference between the maximum and minimum values across 5 random seeds used for fine-tuning. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/e1d07a8e5c921b697795d395648d8fb4997e97c7f53c7accf3246440f1236953.jpg", "table_caption": ["Table 7: Genomic Benchmarks. Top-1 accuracy (\u2191) across 5-fold cross-validation (CV) for a pre-trained AR Mamba, and a pre-trained Caduceus model fine-tuned with different diffusion parameterizations. The best values per task are bolded and the second best are italicized. Error bars indicate the difference between the maximum and minimum values across 5 random seeds used for CV. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Analysis ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/8ade0f39aad626fc83883c24e97d01212fb87f35c1f3d20d07dad9faf7a15169.jpg", "table_caption": ["Table 8: Test perplexities (PPL; $\\downarrow)$ for MDLM ablations on LM1B. For the discrete-time models, we use $T=1000$ . Standard deviation is measured over 5 seeds during evaluation. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In Table 8, we can see the effect of our streamlined masked diffusion implementation. The improvements described in Sec. 3.5.1 allow us to greatly reduce perplexity of previously discounted models, such as D3PM (see the bottom row of this table, which is mathematically equivalent to the D3PM formulation). While most works assumed that D3PM achieves mediocre log-likelihoods, we show that is incorrect: our re-implementation almost matches state-of-the-art score-based methods. This introduces a new strong baseline that opens new research opportunities. Additionally, in Table 8, we ablate different components of MDLM. We observe that the perplexity for MDLM trained with a discrete $T\\!=\\!1000$ marginally worsens by 0.1 compared to MDLM trained in continuous time. Additionally, removing the \u201ccarry over\u201d operation from the SUBS parameterization increases the perplexity by 1.5 points. However, further removing the \u201czero masking\u201d operation does not lead to any meaningful change in perplexity. We provide further ablations for the continuous time formulation in the Appendix, showing in Table 11 that for a pre-trained model, at inference, increasing $T$ yields better likelihoods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Comparison to D3PM Masked diffusion is a strict subset of D3PM [1]; setting $Q_{t|s}\\!=\\!\\alpha_{t|s}\\mathbf{I}\\!+\\!(1\\!-$ $\\alpha_{t|s})\\mathbf{1m}^{\\top}$ in their framework yields our forward diffusion. We improve over D3PM in three ways: (1) we adopt the SUBS parameterization; (2) this allows us to derive a simplified objective that analytically simplifies certain expectations to zero; (3) we adopt well-engineered training recipes that improve performance. Both (1) and (2) are possible because we focus on masking instead of developing a general discrete diffusion framework. Surprisingly, (3) has the largest contribution to performance. ", "page_idx": 8}, {"type": "text", "text": "Comparison to CTMC Most implementations of diffusion work best in continuous time. However, extending D3PM in this way requires computing the limit of the product of an infinite number of matrices $Q_{T}\\cdot Q_{T-1}\\cdots Q_{t}$ as $T\\!\\to\\!\\infty$ , which requires advanced CTMC theory [5]. Our work describes simple continuous-time formulations for the most common noise processes (e.g., masking and uniform $\\pi$ ), thus helping make an important part of the literature more accessible. In Suppl. C, we show that our results are compatible with CTMC, using the rate forward matrix $\\begin{array}{r}{R_{t}\\!=\\!\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}({\\mathbf{I}}\\!-\\!{\\mathbf{\\bar{1}}}{\\mathbf{m}}^{\\top})}\\end{array}$ and the reverse rate $\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})$ for the transition $\\mathbf{y}\\!\\to\\!\\mathbf{y}^{\\prime}$ , where $\\mathbf{y},\\mathbf{y}^{\\prime}\\in\\mathcal{V}$ : ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})\\!=\\!-\\frac{\\alpha_{t}^{\\prime}}{1\\!-\\!\\alpha_{t}}\\!\\left[\\mathbf{y}^{\\prime}\\right]^{\\top}\\!\\left[\\mathbf{x}_{\\theta}(\\mathbf{y},t)\\!-\\!\\mathbf{m}\\right]\\!\\left\\langle\\mathbf{y},\\mathbf{m}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Comparison to Score Estimation Score-based approaches to diffusion [55] extend to discrete states, although they typically further build upon advanced CTMC theory. In particular, SEDD [33] optimizes an ELBO3 that is a function of the score model, obtaining state-of-the-art log-likelihoods among diffusion models. Our approach, however, is much simpler and does not require advanced theory. Furthermore, we can extract the score for MDLM (76), as demonstrated in Suppl. C.3, making it compatible with various techniques designed for score-based algorithms, such as samplers [5], score parameterization [33], efficient designs of the denoising network [59], guidance techniques, and more. ", "page_idx": 9}, {"type": "text", "text": "Comparison to BERT Our work provides a principled way of making BERT generative when trained with randomized masking rates. Previous work on generating from BERT used Gibbs sampling or ad-hoc methods [17, 32, 64]. The connection between BERT and diffusion was first made by Austin et al. [1]: their objective effectively involves unmasking. He et al. [26] additionally starts training from a pretrained BERT. However, both works use an objective that is similar to (9), which is less numerically stable than our objective (see Section 3.5.1). Austin et al. [1] mention in their appendix that their ELBO simplifies to a weighted masking (MLM) loss similar to (8), but it uses a more complex formula for the weights and is limited to the discrete time setting unlike our work. Furthermore, they do not train with that objective. Our work derives a simpler expression for the average of MLM losses, implements it, and obtains better likelihoods. ", "page_idx": 9}, {"type": "text", "text": "Comparision to Latent Diffusion LMs In contrast to this work, which defines diffusion over discrete structures, Plaid [23] and Diffusion LM [30] define a Gaussian diffusion process over word embeddings. Zhang et al. [67] and Hu et al. [28] extend this approach to flow matching over word embeddings, enabling the design of faster samplers. Discrete Flow Matching (DFM) [6] applies flow matching to discrete structures, using a cross-entropy loss as their training objective: $-\\ddot{\\mathbb{E}}_{q,t}^{\\mathbf{\\alpha}}\\mathrm{log}p_{\\theta}(\\mathbf{x}^{1:L}|\\mathbf{z}_{t}^{1:L}\\big)$ Similar to Chang et al. [7], DFM\u2019s objective, while effective, is not weighted to serve as a proper ELBO. In MDLM, however, we derive a tight, principled lower bound on the log-likelihood. ", "page_idx": 9}, {"type": "text", "text": "Concurrent Works Concurrent to our work, Shi et al. [51] and Ou et al. [40] derive a similar simplified objective for masked diffusion processes. While Ou et al. [40] start from a score matching perspective, we tackle this problem from a variational lens similar to Shi et al. [51]. Similar to Ou et al. [40], we formulate efficient samplers in Section 4.1 by leveraging a time-independent denoising network. ", "page_idx": 9}, {"type": "text", "text": "AkeydifferentiationbetweenourworkandthatofShietal.[51],Ouetal.[40]isthesemi-autoregressive decoding method we present in Section 4.2. While [51, 40] are restricted to sample sequences of a fixed length, we propose samplers to generate arbitrary lengths of text like a traditional language model. Furthermore, we establish the connection between our simplified objective and the masked language modeling (MLM) objective. As a result, we endow BERT-style models with principled generation capabilities while maintaining representation learning capabilities. Whereas [51, 40] only evaluate on NLP datasets, we show that masked diffusion is also effective in modeling biological sequences. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we explore masked diffusion. With a well-engineered implementation that supports a simple variational objective, we attain state-of-the-art diffusion perplexities on language benchmarks and demonstrate how to efficiently convert BERT-style encoders into generative models. Given we are working on language modeling, we carry any of the inherent risks and opportunities that come with this line of research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially funded by the National Science Foundation under awards DGE-1922551, CAREER awards 2046760 and 2145577, and by the National Institute of Health under award MIRA R35GM151243. Marianne Arriola is supported by a NSF Graduate Research Fellowship under award DGE-2139899 and a Hopper-Dean/Bowers CIS Deans Excellence Fellowship. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.   \n[2] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In International Conference on Machine Learning, pp. 1276\u20131301. PMLR, 2023.   \n[3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18 (10):1196\u20131203, 2021.   \n[4] Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. From denoising diffusions to denoising markov models. arXiv preprint arXiv:2211.03595, 2022.   \n[5] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:28266\u201328279, 2022.   \n[6] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024.   \n[7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315\u201311325, 2022.   \n[8] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling, 2014.   \n[9] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.   \n[10] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018. doi: 10.18653/v1/n18-2097. URL http://dx.doi.org/10.18653/v1/n18-2097.   \n[11] Genome Reference Consortium. Genome reference consortium human build 37 (grch37. Database (GenBank or RefSeq), 2009.   \n[12] Genome Reference Consortium et al. Genome reference consortium human build 37 (grch37). Database (GenBank or RefSeq), 2009.   \n[13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.   \n[14] Shachi Deshpande, Kaiwen Wang, Dhruv Sreenivas, Zheng Li, and Volodymyr Kuleshov. Deep multi-modal structural equations for causal effect estimation with unstructured proxies. Advances in Neural Information Processing Systems, 35:10931\u201310944, 2022. ", "page_idx": 10}, {"type": "text", "text": "[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. ", "page_idx": 11}, {"type": "text", "text": "[16] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022.   \n[17] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 6112\u20136121, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1633. URL https://aclanthology.org/D19-1633.   \n[18] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus, 2019.   \n[19] Aaron Gokaslan, A Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion models trained on creative-commons images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8250\u20138260, 2024.   \n[20] Katar\u00edna Gre\u0161ov\u00e1, Vlastimil Martinek, David \u02c7Cech\u00e1k, Petr \u0160ime\u02c7cek, and Panagiotis Alexiou. Genomic benchmarks: a collection of datasets for genomic sequence classification. BMC Genomic Data, 24(1):25, 2023.   \n[21] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.   \n[23] Ishaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplexbased diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022.   \n[25] Floyd B. Hanson. Applied stochastic processes and control for jump-diffusions - modeling, analysis, and computation. In Advances in design and control, 2007. URL https://api. semanticscholar.org/CorpusID:6689808.   \n[26] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022.   \n[27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[28] Vincent Hu, Di Wu, Yuki Asano, Pascal Mettes, Basura Fernando, Bj\u00f6rn Ommer, and Cees Snoek. Flow matching for conditional text generation in a few sampling steps. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 380\u2013392, 2024.   \n[29] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n[30] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusionlm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022.   \n[31] Xuanlin Li, Brandon Trabucco, Dong Huk Park, Michael Luo, Sheng Shen, Trevor Darrell, and Yang Gao. Discovering non-monotonic autoregressive orderings with variational inference. arXiv preprint arXiv:2110.15797, 2021.   \n[32] Yi Liao, Xin Jiang, and Qun Liu. Probabilistically masked language model capable of autoregressive generation in arbitrary word order. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 263\u2013274, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.24. URL https://aclanthology.org/2020.acl-main.24.   \n[33] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023.   \n[34] Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, and Kilian Q Weinberger. Latent diffusion for language generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[35] Vincent Mallet and Jean-Philippe Vert. Reverse-complement equivariant networks for dna sequences. Advances in neural information processing systems, 34:13511\u201313523, 2021.   \n[36] Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.   \n[37] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:34532\u201334545, 2022.   \n[38] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.   \n[39] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Advances in neural information processing systems, 36, 2024.   \n[40] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data, 2024.   \n[41] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525\u20131534, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P16-1144.   \n[42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4195\u20134205, 2023.   \n[43] Jacob Portes, Alex Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. Mosaicbert: A bidirectional encoder optimized for fast pretraining, 2024.   \n[44] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022.   \n[45] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   \n[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.   \n[47] Richa Rastogi and Yair Schiff. Semi parametric inducing point networks and neural processes. In International Conference on Learning Representations, 2023.   \n[48] Subham Sekhar Sahoo, Aaron Gokaslan, Chris De Sa, and Volodymyr Kuleshov. Diffusion models with learned adaptive noise. arXiv preprint arXiv:2312.13236, 2023.   \n[49] Subham Sekhar Sahoo, Anselm Paulus, Marin Vlastelica, V\u00edt Musil, Volodymyr Kuleshov, and Georg Martius. Backpropagation through combinatorial algorithms: Identity with projection works. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ JZMR727O29.   \n[50] Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling. arXiv preprint arXiv:2403.03234, 2024.   \n[51] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 36, 2024.   \n[52] Phillip Si, Allan Bishop, and Volodymyr Kuleshov. Autoregressive quantile flows for predictive uncertainty estimation. In International Conference on Learning Representations.   \n[53] Phillip Si, Zeyi Chen, Subham Sekhar Sahoo, Yair Schiff, and Volodymyr Kuleshov. Semiautoregressive energy flows: exploring likelihood-free training of normalizing flows. In International Conference on Machine Learning, pp. 31732\u201331753. PMLR, 2023.   \n[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 2256\u20132265. PMLR, 2015.   \n[55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[56] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[57] Robin Strudel, Corentin Tallec, Florent Altch\u00e9, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, et al. Self-conditioned embedding diffusion for text generation. arXiv preprint arXiv:2211.04236, 2022.   \n[58] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.   \n[59] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022.   \n[60] Zhiqing Sun and Yiming Yang. Difusco: Graph-based diffusion solvers for combinatorial optimization. Advances in Neural Information Processing Systems, 36:3706\u20133731, 2023.   \n[61] Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip M Pham, Zhen Qin, Dara Bahri, DaCheng Juan, and Donald Metzler. Omninet: Omnidirectional representations from transformers. In International Conference on Machine Learning, pp. 10193\u201310202. PMLR, 2021.   \n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[63] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022.   \n[64] Alex Wang and Kyunghyun Cho. Bert has a mouth, and it must speak: Bert as a markov random field language model. arXiv preprint arXiv:1902.04094, 2019.   \n[65] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=rJ4km2R5t7.   \n[66] Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and Volodymyr Kuleshov. Infodiffusion: Representation learning using information maximizing diffusion models. In International Conference on Machine Learning, pp. 36336\u201336354. PMLR, 2023.   \n[67] Shujian Zhang, Lemeng Wu, Chengyue Gong, and Xingchao Liu. Language rectified flow: Advancing diffusion language generation with probabilistic flows. arXiv preprint arXiv:2403.16995, 2024.   \n[68] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015.   \n[69] Hannah Zhou, Avanti Shrikumar, and Anshul Kundaje. Towards a better understanding of reverse-complement equivariance for deep learning models in genomics. In Machine Learning in Computational Biology, pp. 1\u201333. PMLR, 2022. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 15}, {"type": "text", "text": "2 Background 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "2.1 Diffusion Models 2   \n2.2 Discrete Diffusion Models 3 ", "page_idx": 15}, {"type": "text", "text": "3 Simple Masked Diffusion Models 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "3.1 Interpolating Discrete Diffusion 3   \n3.2 Masked Diffusion 4   \n3.3 Rao-Blackwellized Likelihood Bounds 4   \n3.4 Continuous-Time Likelihood Bounds . 5   \n3.5 Masked Diffusion Language Models 5 ", "page_idx": 15}, {"type": "text", "text": "4 Inference and Sampling in Masked Diffusion Language Models 6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "4.1 Efficient Ancestral Sampling 6   \n4.2 Semi-Autoregressive Masked Diffusion Language Models . 6   \n5 Experiments 6   \n5.1 Masked Diffusion Language Models 6   \n5.2 Masked Diffusion DNA Models . . 8   \n5.3 Ablation Analysis . 9 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "6 Related Work 9 ", "page_idx": 15}, {"type": "text", "text": "Conclusion 10 ", "page_idx": 15}, {"type": "text", "text": "Appendices 17 ", "page_idx": 15}, {"type": "text", "text": "Appendix A Discrete time ELBO 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Generic case 17   \nA.2 Absorbing state 18 ", "page_idx": 15}, {"type": "text", "text": "Appendix B MDLM 21 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Rao-Blackwellization 22   \nB.2 Continuous Time 22   \nB.3 Final Algorithm 23 ", "page_idx": 15}, {"type": "text", "text": "Appendix C Concrete Score Matching 23 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Extracting the Rate Matrix 24   \nC.2 NELBO 25   \nC.3 Concrete Score for MDLM 27   \nC.4 Reverse Rate Matrix for MDLM 28 ", "page_idx": 15}, {"type": "text", "text": "C.5 Deriving MDLM\u2019s NELBO via CTMC 29 ", "page_idx": 16}, {"type": "text", "text": "Appendix D Experimental details 31 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Likelihood Evaluation . 31   \nD.2 Avg. Number of Tokens seen 31   \nD.3 Low discrepancy sampler 31   \nD.4 Language Modeling . 31   \nD.5 Zeroshot Likelihood 32   \nD.6 Representation Learning 32   \nD.7 Diffusion DNA Models 32 ", "page_idx": 16}, {"type": "text", "text": "Appendix E Additional Experiments 33 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Noise schedule parameterization 33   \nE.2 Faster sampling with caching 34   \nE.3 LM1B ablations . 35   \nE.4 Train NLL curves on OWT 35   \nE.5 Time-conditioning ablation on OWT 36   \nE.6 Unconditional Samples 36 ", "page_idx": 16}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Appendix A Discrete time ELBO ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section is organized as follows: First, we derive the expressions for the true posterior and the approximate posterior as outlined in Suppl. A.1. We then simplify these expressions specifically for the case of absorbing state diffusion in Suppl. A.2. Finally, we derive the expression for the ELBO for absorbing state diffusion in Suppl. A.2.3. ", "page_idx": 16}, {"type": "text", "text": "A.1 Generic case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given the state transition matrix $Q_{t}$ , prior $\\pi$ , and the latent variables $\\mathbf{z}_{s}$ and $\\mathbf{z}_{t}$ , where $s\\!<\\!t$ , let ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ_{t|s}\\!=\\!\\alpha_{t|s}\\mathbf{I}\\!+\\!(1\\!-\\!\\alpha_{t|s})\\mathbf{1}\\pi^{\\top}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.1.1 $q(\\mathbf{z}_{t}|\\mathbf{z}_{s})$ ", "page_idx": 16}, {"type": "text", "text": "Thus, the marginals in (3) correspond to the following forward process: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{z}_{t}|\\mathbf{z}_{s})\\!=\\!\\mathrm{Cat}(\\mathbf{z}_{t};\\!Q_{t|s}^{\\top}\\mathbf{z}_{s})}\\\\ &{\\!=\\!\\mathrm{Cat}(\\mathbf{z}_{t};[\\alpha_{t|s}\\mathbf{I}+(1\\!-\\!\\alpha_{t|s})\\mathbf{1}\\pi^{\\top}]^{\\top}\\mathbf{z}_{s})}\\\\ &{\\!=\\!\\mathrm{Cat}(\\mathbf{z}_{t};\\!\\alpha_{t|s}\\mathbf{z}_{s}\\!+\\!(1\\!-\\!\\alpha_{t|s})\\pi\\mathbf{1}^{\\top}\\mathbf{z}_{s})}\\\\ &{\\!=\\!\\mathrm{Cat}(\\mathbf{z}_{t};\\!\\alpha_{t|s}\\mathbf{z}_{s}\\!+\\!(1\\!-\\!\\alpha_{t|s})\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The above equation indicates that during each diffusion step from $s\\!\\to\\!t$ , a fraction $(1-\\alpha_{t|s})$ of the probability mass is transferred to the prior distribution $\\pi$ . ", "page_idx": 16}, {"type": "text", "text": "A.1.2 $q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Austin et al. [1] show that the posterior corresponding to (14) is given as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\!=\\!\\mathrm{Cat}\\!\\left(\\mathbf{z}_{s};\\!\\frac{Q_{t|s}\\mathbf{z}_{t}\\odot Q_{s}^{\\top}\\mathbf{x}}{\\mathbf{z}_{t}^{\\top}Q_{t}^{\\top}\\mathbf{x}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which we simplify to the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})}\\\\ &{=\\!\\mathrm{Cat}\\Bigg(\\mathbf{z}_{s};\\!\\frac{[\\alpha_{t|s}\\mathbf{I}+(1-\\alpha_{t|s})\\mathbf{1}\\pi^{\\top}]\\mathbf{z}_{t}\\odot[\\alpha_{s}\\mathbf{I}+(1-\\alpha_{s})\\mathbf{1}\\pi^{\\top}]^{\\top}\\mathbf{x}}{\\mathbf{z}_{t}^{\\top}[\\alpha_{t}\\mathbf{I}+(1-\\alpha_{t})\\mathbf{1}\\pi^{\\top}]^{\\top}\\mathbf{x}}\\Bigg)}\\\\ &{=\\!\\mathrm{Cat}\\Bigg(\\mathbf{z}_{s};\\!\\frac{[\\alpha_{t|s}\\mathbf{z}_{t}+(1-\\alpha_{t|s})\\mathbf{1}\\pi^{\\top}\\mathbf{z}_{t}]\\odot[\\alpha_{s}\\mathbf{x}+(1-\\alpha_{s})\\pi]}{\\mathbf{z}_{t}^{\\top}[\\alpha_{t}\\mathbf{x}+(1-\\alpha_{t})\\boldsymbol{\\pi}\\mathbf{1}^{\\top}\\mathbf{x}]}\\Bigg)}\\\\ &{=\\!\\mathrm{Cat}\\Bigg(\\mathbf{z}_{s};\\!\\frac{[\\alpha_{t|s}\\mathbf{z}_{t}+(1-\\alpha_{t|s})\\mathbf{1}\\pi^{\\top}\\mathbf{z}_{t}]\\odot[\\alpha_{s}\\mathbf{x}+(1-\\alpha_{s})\\mathbf{1}\\pi^{\\top}]^{\\top}\\mathbf{x}}{\\alpha_{t}\\mathbf{z}_{t}^{\\top}\\mathbf{x}+(1-\\alpha_{t})\\mathbf{z}_{t}^{\\top}\\mathbf{\\bar{\\pi}}}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.1.3 $p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})$ ", "page_idx": 17}, {"type": "text", "text": "Austin et al. [1] approximate the reverse process in the following manner: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})\\!=\\!q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\!\\mathbf{x}\\!=\\!\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t))\\!=\\!\\mathbf{Cat}\\!\\left(\\mathbf{z}_{s};\\!\\frac{Q_{t|s}\\mathbf{z}_{t}\\odot Q_{s}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}{\\mathbf{z}_{t}^{\\top}Q_{t}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t):\\mathcal{V}\\times[0,1]\\to\\Delta^{K}$ is an approximation for $\\mathbf{x}$ . ", "page_idx": 17}, {"type": "text", "text": "A.2 Absorbing state ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the absorbing state diffusion process we have $\\pi\\!=\\!\\mathbf{m}$ . ", "page_idx": 17}, {"type": "text", "text": "A.2.1 $q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})$ ", "page_idx": 17}, {"type": "text", "text": "Since, $\\mathbf{z}_{t}\\in\\{\\mathbf{x},\\mathbf{m}\\}$ , takes only 2 values we consider the separate cases: $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ and $\\mathbf{z}_{t}\\!=\\!\\mathbf{m}$ . ", "page_idx": 17}, {"type": "text", "text": "Case 1. Consider the case $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ i.e. $\\mathbf{z}_{t}$ is unmasked. From (16), we have the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{q(\\mathbf{z}_{s}|\\mathbf{z}_{t}=\\mathbf{x},\\mathbf{x})}\\\\ &{\\quad=\\mathrm{Cat}\\bigg(\\mathbf{z}_{s};\\frac{\\left[\\alpha_{t|s}\\mathbf{x}+\\left(1-\\alpha_{t|s}\\right)\\mathbf{1m}^{\\top}\\mathbf{x}\\right]\\odot\\left[\\alpha_{s}\\mathbf{x}+\\left(1-\\alpha_{s}\\right)\\mathbf{m}\\right]}{\\alpha_{t}\\mathbf{x}^{\\top}\\mathbf{x}+\\left(1-\\alpha_{t}\\right)\\mathbf{x}^{\\top}\\mathbf{m}}\\bigg)}\\\\ &{\\quad=\\mathrm{Cat}\\bigg(\\mathbf{z}_{s};\\frac{\\left[\\alpha_{t|s}\\mathbf{x}\\right]\\odot\\left[\\alpha_{s}\\mathbf{x}+\\left(1-\\alpha_{s}\\right)\\mathbf{m}\\right]}{\\alpha_{t}}\\bigg)}\\\\ &{\\quad=\\mathrm{Cat}\\bigg(\\mathbf{z}_{s};\\frac{\\left[\\alpha_{t|s}\\mathbf{x}\\right]\\odot\\left[\\alpha_{s}\\mathbf{x}+\\left(1-\\alpha_{s}\\right)\\mathbf{m}\\right]}{\\alpha_{t}}\\bigg)}\\\\ &{\\quad=\\mathrm{Cat}\\bigg(\\mathbf{z}_{s};\\frac{\\alpha_{t}\\mathbf{x}}{\\alpha_{t}}\\bigg)}&{\\cdots\\times\\odot\\mathbf{m}=\\mathbf{0}\\mathrm{~and~}\\alpha_{t}=\\alpha_{t|s}\\alpha_{s}}\\\\ &{\\quad=\\mathrm{Cat}(\\mathbf{z}_{s};\\mathbf{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we have the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{x},\\!\\mathbf{x})\\!=\\!\\mathbf{Cat}(\\mathbf{z}_{s};\\!\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Case 2. Consider the case $\\mathbf{z}_{t}\\!=\\!\\mathbf{m}$ . By substituting $\\mathbf z_{t}\\!=\\!\\mathbf m$ and $\\pi\\!=\\!\\mathbf{m}$ in (16), $q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})$ simplifies to the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})\\!=\\!\\mathrm{Cat}\\bigg(\\frac{\\big(\\alpha_{t|s}\\mathbf{m}+\\left(1-\\alpha_{t|s}\\right)\\mathbf{1}\\big)\\odot\\big(\\alpha_{s}\\mathbf{x}+\\left(1-\\alpha_{s}\\right)\\mathbf{m}\\big)}{\\left(1-\\alpha_{t}\\right)}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad=\\!\\mathrm{Cat}\\bigg(\\frac{\\big(\\alpha_{t|s}\\left(1-\\alpha_{s}\\right)\\mathbf{m}+\\left(1-\\alpha_{t|s}\\right)\\left(1-\\alpha_{s}\\right)\\mathbf{m}+\\left(\\alpha_{s}-\\alpha_{t}\\right)\\mathbf{x}\\big)}{\\left(1-\\alpha_{t}\\right)}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{\\sigma}=\\mathbf{Cat}\\bigg(\\mathbf{z}_{s};\\frac{(1-\\alpha_{s})\\mathbf{m}+(\\alpha_{s}-\\alpha_{t})\\mathbf{x}}{1-\\alpha_{t}}\\bigg)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the above categorical distribution is non-zero for $\\mathbf{z}_{s}\\in\\{\\mathbf{x},\\mathbf{m}\\}$ and zero for every other value. The non-zero values are specified as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{q(\\mathbf{z}_{s}\\!=\\!\\mathbf{x}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})\\!=\\!\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}}\\\\ {q(\\mathbf{z}_{s}\\!=\\!\\mathbf{m}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})\\!=\\!\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining Cases 1 and 2, we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\!=\\!\\left\\{\\begin{array}{l l}{\\mathrm{Cat}(\\mathbf{z}_{s};\\!\\mathbf{z}_{t})}&{\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m},}\\\\ {\\mathrm{Cat}\\!\\left(\\mathbf{z}_{s};\\!\\frac{(1-\\alpha_{s})\\mathbf{m}+(\\alpha_{s}-\\alpha_{t})\\mathbf{x}}{1-\\alpha_{t}}\\right)}&{\\mathbf{z}_{t}\\!=\\!\\mathbf{m}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.2.2 $p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})$ ", "page_idx": 18}, {"type": "text", "text": "For the absorbing state diffusion process with $\\pi\\!=\\!\\mathbf{m}$ , we want to simplify the (17). For this reason, we consider 2 cases: first, when $\\mathbf z_{t}\\neq\\mathbf m$ (case 1), second, when $\\mathbf z_{t}\\neq\\mathbf m$ (case 2). ", "page_idx": 18}, {"type": "text", "text": "Case 1. Consider the case when $\\mathbf{z}_{t}\\neq\\mathbf{m}$ . (17) simplifies to the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m})\\!=\\!\\mathrm{Cat}\\!\\left(\\mathbf{z}_{s};\\!\\frac{Q_{t|s}\\mathbf{z}_{t}\\odot Q_{s}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}{\\mathbf{z}_{t}^{\\top}Q_{t}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\!\\mathrm{Cat}\\!\\left(\\mathbf{z}_;\\!\\frac{Q_{t|s}\\mathbf{z}_{t}\\odot Q_{s}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}{[Q_{t}\\mathbf{z}_{t}]^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n=\\mathrm{Cat}\\bigg(\\mathbf{z}_{s};\\frac{[\\alpha_{t|s}\\mathbf{z}_{t}]\\odot[\\alpha_{s}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)+(1-\\alpha_{s})\\mathbf{m}]}{\\alpha_{t}\\langle\\mathbf{z}_{t},\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)\\rangle}\\bigg)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathrm{Cat}\\bigg(\\mathbf{z}_{s};\\frac{\\boldsymbol{\\alpha}_{t}\\mathbf{z}_{t}\\odot\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}{\\boldsymbol{\\alpha}_{t}\\langle\\mathbf{z}_{t},\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)\\rangle}\\bigg)}\\\\ &{=\\mathrm{Cat}\\big(\\mathbf{z}_{s};\\mathbf{z}_{t}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Case 2. Consider the case when $\\mathbf z_{t}\\!=\\!\\mathbf m$ . (17) simplifies to the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})\\!=\\!\\operatorname{Ca}\\!\\left(\\mathbf{z}_{s},\\!\\frac{Q_{t}|_{s}\\operatorname*{min}\\odot Q_{s}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}{\\mathbf{m}^{\\top}Q_{t}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\!\\operatorname{Ca}\\!\\left(\\mathbf{z}_,\\frac{Q_{t}|_{s}\\operatorname*{min}\\odot Q_{s}^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}{|Q_{t}\\mathbf{n}|^{\\top}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)}\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!=\\!\\mathrm{Cat}\\!\\left(\\mathbf{z}_{s};\\!\\frac{\\alpha_{t}\\mathbf{m}\\odot\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)+(\\alpha_{s}-\\alpha_{t})\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)+(1-\\alpha_{s})\\mathbf{m}}{\\alpha_{t}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{t})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the above categorical distribution, we can obtain the values for $p_{\\theta}(\\mathbf{z}_{s}=\\mathbf{x}|\\mathbf{z}_{t}=\\mathbf{m})$ ) and $p_{\\theta}(\\mathbf{z}_{s}\\!=\\!\\mathbf{m}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})$ which are as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}(\\mathbf{z}_{s}\\!=\\!\\mathbf{x}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})\\!=\\!\\frac{\\big(\\alpha_{s}-\\alpha_{t}\\big)\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle}{\\alpha_{t}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{t})}}\\\\ &{p_{\\theta}(\\mathbf{z}_{s}\\!=\\!\\mathbf{m}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})\\!=\\!\\frac{\\alpha_{s}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{s})}{\\alpha_{t}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{t})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As a sanity check, we can verify that (26) reduces to (21), and (27) reduces to (22) if our denoising network can reconstruct $\\mathbf{x}$ perfectly, i.e., ${\\bf x}_{\\theta}({\\bf z}_{t},t)\\!=\\!{\\bf x}$ . ", "page_idx": 19}, {"type": "text", "text": "Combining (24) and (25), we get the following expression for the reverse process parameterization: ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})=\\left\\{\\begin{array}{l l}{\\mathbf{Cat}(\\mathbf{z}_{s};\\mathbf{z}_{t})}&{\\mathrm{~}\\mathbf{z}_{t}\\neq\\mathbf{m},}\\\\ {\\mathbf{Cat}\\Bigl(\\mathbf{z}_{s};\\frac{\\alpha_{t}\\mathbf{m}(\\mathbf{\\hat{z}}_{t},t)+(\\alpha_{s}-\\alpha_{t})\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)+(1-\\alpha_{s})\\mathbf{m}}{\\alpha_{t}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle+(1-\\alpha_{t})}\\Bigr)}&{\\mathrm{~}\\mathbf{z}_{t}=\\mathbf{m}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.2.3 Diffusion Loss ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For a given $T$ , Let $\\begin{array}{r}{\\mathcal{L}_{T}\\!=\\!\\mathbb{E}_{t\\in\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\}}\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{x})}T\\mathrm{D}_{\\mathrm{KL}}\\!\\left(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\big||p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})\\right)}\\end{array}$ denote the diffusion loss. We break down the computation of $\\mathrm{D}_{\\mathrm{KL}}\\big(q\\big(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x}\\big)\\|p_{\\theta}\\big(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\big)\\big)$ into 2 cases: $\\mathbf{z}_{t}=\\mathbf{x}$ (case 1) and $\\mathbf z_{t}\\!=\\!\\mathbf m$ (case 2). ", "page_idx": 19}, {"type": "text", "text": "Case 1: consider the case $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ . Let\u2019s simplify $\\begin{array}{r}{\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{x},\\!\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{x})\\big)}\\end{array}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{x},\\!\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{x})\\big)}\\\\ &{=\\!\\mathrm{D}_{\\mathrm{KL}}\\big(\\mathbf{z}_{t}||\\mathbf{z}_{t}\\big)}\\\\ &{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case 2: Consider the case $\\mathbf{z}_{t}\\!=\\!\\mathbf{m}$ . Let\u2019s simplify $\\begin{array}{r}{\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\!\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})\\big)}\\end{array}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\mathrm{KL}}(q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})||\\boldsymbol{p}_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m}))}\\\\ &{\\!=\\!\\displaystyle\\sum_{\\mathbf{z}_{s}}\\!q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})\\mathrm{log}\\frac{q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})}{p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})}}\\\\ &{\\!=\\!\\displaystyle\\sum_{\\mathbf{z}_{s}\\in\\{\\mathbf{x},\\mathbf{m}\\}}q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})\\mathrm{log}\\frac{q(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})}{p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})}}\\\\ &{\\!=\\!q(\\mathbf{z}_{s}\\!=\\!\\mathbf{x}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})\\mathrm{log}\\frac{q(\\mathbf{z}_{s}\\!=\\!\\mathbf{x}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\mathbf{x})}{p_{\\theta}(\\mathbf{z}_{s}\\!=\\!\\mathbf{x}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n+q(\\mathbf{z}_{s}\\!=\\!\\mathbf{m}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\!\\mathbf{x})\\mathrm{log}\\frac{q(\\mathbf{z}_{s}\\!=\\!\\mathbf{m}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m},\\!\\mathbf{x})}{p_{\\theta}(\\mathbf{z}_{s}\\!=\\!\\mathbf{m}|\\mathbf{z}_{t}\\!=\\!\\mathbf{m})}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!=\\!\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\!\\log\\!\\frac{\\alpha_{t}\\langle\\mathbf x_{\\theta}(\\mathbf z_{t},t),\\mathbf m\\rangle+(1-\\alpha_{t})}{(1-\\alpha_{t})\\langle\\mathbf x_{\\theta}(\\mathbf z_{t},t),\\mathbf x\\rangle}}\\\\ &{\\quad\\!+\\!\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\!\\log\\!\\frac{(1-\\alpha_{s})(\\alpha_{t}\\langle\\mathbf x_{\\theta}(\\mathbf z_{t},t),\\mathbf m\\rangle+(1-\\alpha_{t}))}{(1-\\alpha_{t})(\\alpha_{s}\\langle\\mathbf x_{\\theta}(\\mathbf z_{t},t),\\mathbf m\\rangle+(1-\\alpha_{s}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, $\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\|p_{\\theta}\\big(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\big)\\big)$ can be written in the following manner where $\\langle\\mathbf{z}_{t},\\mathbf{x}\\rangle$ evaluates to 1 if $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ and $\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle$ evaluates to $^{1}$ if $\\mathbf{z}_{t}\\!=\\!\\mathbf{m}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})\\|p_{\\theta}\\big(\\mathbf{z}_{s}|\\mathbf{z}_{t}\\big)\\big)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we derive the diffusion loss, $\\mathcal{L}_{T}$ , in the following manner: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{T}\\!=\\!\\mathbb{E}_{t\\in\\left\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\right\\}}\\!\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{x})}T\\mathrm{D}_{\\mathrm{KL}}(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})||p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}))}\\\\ &{\\quad\\!=\\!\\mathbb{E}_{t\\in\\left\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\right\\}}\\!\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{x})}T\\!\\left[\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\mathrm{log}\\frac{\\alpha_{t}\\left\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\right\\rangle+(1-\\alpha_{t})}{(1-\\alpha_{t})\\left\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\right\\rangle}\\right.}\\\\ &{\\quad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left.+\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\mathrm{log}\\frac{(1-\\alpha_{s})(\\alpha_{t}\\left\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\right\\rangle+(1-\\alpha_{t}))}{(1-\\alpha_{t})(\\alpha_{s}\\left\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\right\\rangle+(1-\\alpha_{s}))}\\right]\\!\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\mathcal{L}_{T}$ is 0 if $\\mathbf{z}_{t}$ is an unmasked token i.e. $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ . ", "page_idx": 20}, {"type": "text", "text": "A.2.4 NELBO", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Austin et al. [1], Sohl-Dickstein et al. [54] model $\\alpha_{i}$ as $(\\alpha_{i})_{i\\in\\{1,...,T\\}}=1-\\frac{i}{T}$ given latents $\\mathbf{z}_{1,...,T}$ . However, in this paper, we denote the latents as $\\mathbf z_{t(0),...,t(T)}$ ; and hence, the $\\alpha_{t(i)}$ are given as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{(\\alpha_{i})_{i\\in\\{1,\\dots,T\\}}\\!=\\!1\\!-\\!\\frac{i}{T}\\qquad}&{\\mathrm{From~Austin~etal.\\,[1],Sohl-Dickstein~et~al.\\,[54]}.}\\\\ &{\\Longrightarrow(\\alpha_{i})_{k\\in\\{1,\\dots,T+1\\}}\\!=\\!1\\!-\\!\\frac{i}{T+1}\\qquad}&{\\mathrm{For~}T\\!+\\!1\\mathrm{\\,latents}}\\\\ &{\\Longrightarrow(\\alpha_{i})_{i\\in\\{0,\\dots,T\\}}\\!=\\!1\\!-\\!\\frac{i+1}{T+1}\\qquad}&{\\mathrm{Offsetting\\,the\\,indices\\,by\\,1.}}\\\\ &{\\Longrightarrow(\\alpha_{t(i)})_{i\\in\\{0,\\dots,T\\}}\\!=\\!1\\!-\\!\\frac{i+1}{T+1}\\qquad}&{\\mathrm{Switching\\,the\\,notations\\,from}\\,\\alpha_{i}\\,\\mathrm{to}\\,\\alpha_{t(i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consequently, from Equation 33, we derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\alpha_{t(0)}\\!=\\!\\frac{T}{T\\!+\\!1},}}\\\\ {\\displaystyle{\\alpha_{t(T)}\\!=\\!0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus we have the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{t(0)}\\!\\sim\\!\\mathbf{Cat}\\big(.;\\!\\alpha_{t=0}\\mathbf{x}\\!+\\!(1\\!-\\!\\alpha_{t=0})\\mathbf{m}\\big)\\!=\\!\\mathbf{Cat}\\bigg(\\cdot;\\!\\frac{T}{T\\!+\\!1}\\mathbf{x}\\!+\\!\\frac{1}{T\\!+\\!1}\\mathbf{m}\\bigg),}\\\\ &{q\\big(\\mathbf{z}_{t(T)}|\\mathbf{x}\\big)\\!=\\!\\mathbf{Cat}\\big(.;\\!\\alpha_{t=1}\\mathbf{x}\\!+\\!(1\\!-\\!\\alpha_{t=1})\\mathbf{m}\\big)\\!=\\!\\mathbf{Cat}(.;\\!\\mathbf{m}),}\\\\ &{p_{\\theta}\\big(\\mathbf{z}_{t(T)}\\big)\\!=\\!\\mathbf{Cat}(.;\\!\\mathbf{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The NELBO (2) simplifies to the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{q}\\left[-\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_{t(0)})+\\underbrace{\\mathcal{L}_{T}}_{\\mathrm{Compute~tang~}(3)2}\\right]+\\underbrace{D_{\\mathrm{KL}}[q(\\mathbf{z}_{t(T)}|\\mathbf{x})||p_{\\theta}(\\mathbf{z}_{t(T)})]}_{=\\:0\\:\\mathrm{using~}(3^{\\prime})\\mathrm{~and~}(38)}}\\\\ &{=\\mathbb{E}_{q,t}\\left[-\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_{t(0)})\\!+\\!T\\!\\left[\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\!\\log\\!\\frac{\\alpha_{t}\\left<\\mathbf{x}_{\\theta}\\left(\\mathbf{z}_{t},t\\right),\\mathbf{m}\\right>+\\left(1-\\alpha_{t}\\right)}{\\left(1-\\alpha_{t}\\right)\\left<\\mathbf{x}_{\\theta}\\left(\\mathbf{z}_{t},t\\right),\\mathbf{x}\\right>}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\!\\log\\!\\frac{\\left(1-\\alpha_{s}\\right)\\left(\\alpha_{t}\\left<\\mathbf{x}_{\\theta}\\left(\\mathbf{z}_{t},t\\right),\\mathbf{m}\\right>+\\left(1-\\alpha_{t}\\right)\\right)}{\\left(1-\\alpha_{t}\\right)\\left(\\alpha_{s}\\left<\\mathbf{x}_{\\theta}\\left(\\mathbf{z}_{t},t\\right),\\mathbf{m}\\right>+\\left(1-\\alpha_{s}\\right)\\right)}\\right]\\left<\\mathbf{z}_{t},\\mathbf{m}\\right>}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Appendix B MDLM ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show how SUBS parameterization can simplify the functional form of the NELBO as defined in (39). ", "page_idx": 20}, {"type": "text", "text": "B.1 Rao-Blackwellization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We employ the RB techniques as described in Sec. 3.2.3 to simplify the NELBO (39) to (41) using RB2, and further to (43) using RB1. ", "page_idx": 21}, {"type": "text", "text": "B.1.1 Zero Masking Probabilities ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Using \u201cZero Masking Probabilities\u201d (RB2) from Sec. 3.2.3, we set $\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle\\!=\\!0$ in (32) to obtain the following simplified diffusion loss: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{T}^{\\mathrm{RB2}}\\!=\\!\\mathbb{E}_{t\\in\\left\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\right\\}}\\!\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{x})}T\\!\\left[\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\mathrm{log}\\frac{1}{\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle}\\right]\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle}\\\\ {=\\!\\mathbb{E}_{t\\in\\left\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\right\\}}\\!\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{x})}T\\!\\left[\\frac{\\alpha_{t}-\\alpha_{s}}{1-\\alpha_{t}}\\mathrm{log}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\right]\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The corresponding Rao-Blackwellized NELBO is given as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{q}\\left[-\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_{t(0)})+\\underbrace{\\mathcal{L}_{T}^{\\mathrm{RB2}}}_{\\mathrm{Compute~using~(40)}}\\right]+\\underbrace{D_{\\mathrm{KL}}[q(\\mathbf{z}_{t(T)}|\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}_{t(T)})]}_{=0\\mathrm{~using~(37)~and~(38)~}}}\\\\ &{=\\mathbb{E}_{q,t}\\left[-\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_{t(0)})\\!+\\!T\\!\\left[\\frac{\\alpha_{t}-\\alpha_{s}}{1-\\alpha_{t}}\\!\\log\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\!\\right]\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.1.2 Carry Over Unmasking ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Notice that the term $\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle$ in (40) is intended to reduce the diffusion loss to zero when $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ . Now, we will demonstrate that, by applying \u201cCarry Over Unmasking\u201d (RB1) from Sec. 3.2.3, $\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle$ can be removed from (40). ", "page_idx": 21}, {"type": "text", "text": "Recall that RB1 guarantees $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)=\\mathbf{x}$ when $\\mathbf{z}_{t}=\\mathbf{x}$ . Thus, with the RB1 parameterization, the diffusion loss in (40) becomes zero for $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ , as $\\log\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{m}\\rangle\\!=\\!0$ . Consequently, $\\langle\\mathbf{z}_{t},\\mathbf{m}\\rangle$ can be safely omitted from (41), yielding the following diffusion loss: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{T}^{\\mathrm{RB2+RB1}}\\!=\\!\\mathbb{E}_{t\\in\\left\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\right\\}}\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{x})}T\\Bigg[\\frac{\\alpha_{t}\\!-\\!\\alpha_{s}}{1\\!-\\!\\alpha_{t}}\\mathrm{log}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\Bigg]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.1.3 NELBO ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Thus, we have the following NELBO: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{q}\\left[-\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_{t(0)})+\\underbrace{\\mathcal{L}_{T}^{\\mathrm{RB2}+\\mathrm{RB1}}}_{\\mathrm{Compute~using~(42)}}\\right]+\\underbrace{D_{\\mathrm{KL}}[q(\\mathbf{z}_{t(T)}|\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}_{t(T)})]}_{=\\,0\\mathrm{~using~(37)~and~(38)}}}\\\\ &{=\\mathbb{E}_{q,t}\\left[-\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_{t(0)})\\!+\\!T\\!\\left[\\frac{\\alpha_{t}-\\alpha_{s}}{1-\\alpha_{t}}\\!\\log\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\!\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Comparing (43) and (41). Note that due to RB1, $\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_{t(0)})$ in (43) reduces to 0 every time $\\mathbf z_{t(0)}\\!=\\!\\mathbf x$ as explained in (45). However, this is not the case in (41), even though it has a functionally similar expression to (43). Because of this reason (43) should lead to a better likelihood estimate and we empirically verify this in Table 8. ", "page_idx": 21}, {"type": "text", "text": "B.2 Continuous Time ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.2.1 Diffusion Loss ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To derive the continuous-time diffusion loss, $\\mathcal{L}_{\\mathrm{diffusion}}^{\\infty}$ , we consider the limiting case $\\scriptstyle\\operatorname*{lim}_{T\\to\\infty}{\\mathcal{L}}_{T}^{\\mathrm{RB}2\\,+\\,\\mathrm{RB}1}$ LRB2 + RB1(42): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{diffusion}}^{\\infty}\\!=\\!\\operatorname*{lim}_{T\\rightarrow\\infty}\\!\\mathcal{L}_{T}^{\\mathrm{RB2}\\,+\\,\\mathrm{RB1}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{=\\mathbb{E}_{t\\in\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\},q(\\mathbf{z}_{t}|\\mathbf{x})}\\bigg[\\underset{T\\to\\infty}{\\operatorname*{lim}}T\\frac{\\alpha_{t}-\\alpha_{s}}{1-\\alpha_{t}}\\mathrm{log}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\bigg]}\\\\ &{=\\mathbb{E}_{t\\sim\\mathcal{U}[0,1],q(\\mathbf{z}_{t}|\\mathbf{x})}\\bigg[\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\mathrm{log}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\bigg]}&{\\mathrm{Using~}\\underset{T\\to\\infty}{\\operatorname*{lim}}T(\\alpha_{t}-\\alpha_{s})\\!=\\!\\alpha_{t}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B.2.2 Reconstruction Loss ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For the continous time case, from (36), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{t(0)}\\sim\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbf{Cat}\\bigg(\\cdot\\frac{T}{T+1}\\mathbf{x}+\\frac{1}{T+1}\\mathbf{m}\\bigg)}\\\\ &{\\implies\\mathbf{z}_{t(0)}\\sim\\mathbf{Cat}(.;\\mathbf{x})}\\\\ &{\\implies\\mathbf{z}_{t(0)}\\!=\\!\\mathbf{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, the reconstruction loss reduces to 0 in the following manner: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal L}_{\\mathrm{recons}}\\!=\\!-\\!\\log\\!p_{\\theta}\\big(\\mathbf x|\\mathbf z_{t(0)}\\big)}\\\\ &{\\quad\\quad\\quad=\\!-\\!\\log\\!p_{\\theta}\\big(\\mathbf x|\\mathbf z_{t(0)}\\!=\\!\\mathbf x\\big)}\\\\ &{\\quad\\quad\\quad=\\!-\\!\\log\\!\\big\\langle\\mathbf x_{\\theta}(\\mathbf x,t(0)),\\mathbf x\\big\\rangle}\\\\ &{\\quad\\quad\\quad=\\!-\\!\\log\\!\\big\\langle\\mathbf x,\\mathbf x\\big\\rangle}\\\\ &{\\quad\\quad=\\!0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B.2.3 NELBO", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Thus, we have the following NELBO: ", "page_idx": 22}, {"type": "text", "text": "B.3 Final Algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Algorithm 1, we present the training algorithm for MDLM. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 1 Training MDLM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: repeat ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "2: x1:L \u223cq(x) $\\triangleright$ Sample a sentence.   \n3: $t\\!\\sim\\!\\mathcal{U}[0,\\!1]$ $\\triangleright$ Sample a time step.   \n4: $\\mathbf{z}_{t}^{\\ell}\\!\\sim\\!\\dot{\\mathrm{Cat}}(\\mathbf{\\bar{z}}_{t}^{\\ell};\\!\\alpha_{t}\\mathbf{x}^{\\ell}\\!+\\!(1\\!-\\!\\alpha_{t})\\mathbf{m})\\,\\forall\\,1\\!\\leq\\!\\ell\\!\\leq\\!L$ \u25b7Mask Each token $\\mathbf{x}^{\\ell}$ independently to obtain the   \nlatent $\\mathbf{z}_{t}^{1:L}$ .   \n5: Take gradient descent step on ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\frac{\\alpha_{t}^{\\prime}}{1\\!-\\!\\alpha_{t}}\\!\\sum_{\\ell}\\!\\log\\langle\\mathbf{x}_{\\theta}^{\\ell}(\\mathbf{z}_{t}^{1:L},t),\\!\\mathbf{x}^{\\ell}\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "6: until converged ", "page_idx": 22}, {"type": "text", "text": "Appendix C Concrete Score Matching ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the previous section, we defined the discrete diffusion process as a Discrete-Time Markov Chain (DTMC) with a finite set of $T$ states, $\\mathbf{z}_{\\{0,\\frac{1}{T},\\dots,1\\}}$ , and a state transition matrix $Q_{t}$ . To derive the continuous-time ELBO, we simply take the limit as $T\\!\\to\\!\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "In contrast, Campbell et al. [5] and Lou et al. [33] defined the discrete diffusion process as a ContinuousTime Markov Chain (CTMC), where the forward corruption process is specified by the rate change matrix $R_{t}\\!\\in\\!\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ , which can be thought of as the instantaneous rate at which one state transitions to another. With this formulation, the forward posterior $q(\\mathbf{z}_{t}|\\mathbf{z}_{s})$ and the true reverse posterior $q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})$ can be expressed in terms of the rate change matrix as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{z}_{t}\\!=\\!\\mathbf{y}^{\\prime}|\\mathbf{z}_{s}\\!=\\!\\mathbf{y})\\!=\\!\\delta_{\\mathbf{y}^{\\prime},\\mathbf{y}}\\!+\\!R_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})\\frac{1}{T}\\!+\\!\\mathcal{O}\\!\\left(\\frac{1}{T^{2}}\\right)}\\\\ &{q(\\mathbf{z}_{s}\\!=\\!\\mathbf{y}^{\\prime}|\\mathbf{z}_{t}\\!=\\!\\mathbf{y},\\mathbf{x})\\!=\\!\\delta_{\\mathbf{y}^{\\prime},\\mathbf{y}}\\!+\\!\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})\\frac{1}{T}\\!+\\!\\mathcal{O}\\!\\left(\\frac{1}{T^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\delta$ is the Kroenecker delta and $\\mathcal{O}\\big(\\frac{1}{T^{2}}\\big)$ represents higher order terms of $\\frac{1}{T^{2}}$ . In Sec. C.1, we show how to express the rate matrix $R_{t}$ in terms of the state transition matrix $Q_{t}$ . Lou et al. [33] propose a continuous time ELBO for this process. They mention that this expression can be derived from Benton et al. [4], though they do not provide an explicit derivation. For this reason, we present a rigorous derivation in Sec. C.2 and further demonstrate that, under the SUBS parameterization in Sec. 3.2.3, this formula reduces to our proposed continuous-time ELBO, given by (10). ", "page_idx": 23}, {"type": "text", "text": "For the remainder of this section, we switch to the notation $q_{t|s}(\\mathbf{y}^{\\prime}|\\mathbf{y})$ to denote $q(\\mathbf{z}_{t}=\\mathbf{y}^{\\prime}|\\mathbf{z}_{s}=\\mathbf{y})$ , $q_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y})$ for $\\v{q}(\\mathbf{z}_{s}\\!=\\!\\mathbf{y}^{\\prime}|\\mathbf{z}_{t}\\!=\\!\\mathbf{y})$ , and $q(\\mathbf{z}_{t}\\!=\\!\\mathbf{y}|\\mathbf{x})$ for $q_{t}(\\mathbf{y}|\\mathbf{x})$ , aligning with the notation typically used in the CTMC literature. ", "page_idx": 23}, {"type": "text", "text": "C.1 Extracting the Rate Matrix ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we aim to express the rate change matrix $R_{t}$ in terms of the state transition matrix $Q_{t}$ . To do this, we first represent the forward transition $q_{t\\mid s}$ in terms of $Q_{t}$ and $R_{t}$ separately, allowing us to illustrate their relationship. ", "page_idx": 23}, {"type": "text", "text": "Using (13), we can write $q_{t\\mid s}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t|s}(\\mathbf{y}^{\\prime}|\\mathbf{y})\\!=\\![\\mathbf{y}^{\\prime}]^{\\top}\\big[\\alpha_{t|s}\\mathbf{I}\\!+\\!(1\\!-\\!\\alpha_{t|s})\\mathbf{1m}^{\\top}]^{\\top}\\mathbf{y}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=[\\mathbf{y}^{\\prime}]^{\\top}\\big[\\alpha_{t|s}\\mathbf{y}\\!+\\!(1\\!-\\!\\alpha_{t|s})\\mathbf{m}\\mathbf{1}^{\\top}\\mathbf{y}\\big]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=[\\mathbf{y}^{\\prime}]^{\\top}\\big[\\alpha_{t|s}\\mathbf{y}\\!+\\!(1\\!-\\!\\alpha_{t|s})\\mathbf{m}\\big]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\alpha_{t|s}[\\mathbf{y}^{\\prime}]^{\\top}\\mathbf{y}\\!+\\!(1\\!-\\!\\alpha_{t|s})[\\mathbf{y}^{\\prime}]^{\\top}\\mathbf{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now let\u2019s analyze all possible combinations for the tuple $(\\mathbf{y}^{\\prime},\\mathbf{y})$ : ", "page_idx": 23}, {"type": "text", "text": "1. Case $\\left(\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{x},\\!\\mathbf{y}\\!=\\!\\mathbf{x}\\right)$ ): Using (50), we find that $q_{t|s}(\\mathbf{x}|\\mathbf{x})\\!=\\!\\alpha_{t|s}$ for the DTMC. By (48), we have $q_{t|s}(\\mathbf{x}|\\mathbf{x})\\!=\\!1\\!+\\!R_{t}(\\mathbf{x},\\mathbf{x})\\frac{1}{T}$ as $T\\!\\to\\!\\infty$ , since the higher-order terms $\\mathcal{O}\\big(\\frac{1}{T^{2}}\\big)$ vanish in the limit. Thus, we get: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{T\\to\\infty}\\bigg[1\\!+\\!R_{t}(\\mathbf{x},\\mathbf{x})\\frac{1}{T}\\bigg]=\\operatorname*{lim}_{T\\to\\infty}\\alpha_{t|s}}\\\\ {\\implies R_{t}(\\mathbf{x},\\mathbf{x})\\!=\\!\\displaystyle\\operatorname*{lim}_{T\\to\\infty}T(\\alpha_{t|s}\\!-\\!1)\\!=\\!\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "2. Case $(\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{m},\\!\\mathbf{y}\\!\\in\\!\\mathcal{V}\\!-\\!\\{\\mathbf{m}\\})$ : Similarly, using (50) and (48), we have $q_{t|s}(\\mathbf{m}|\\mathbf{y}\\neq\\mathbf{m})\\!=\\!1\\!-\\!\\alpha_{t|s}$ and $q_{t|s}(\\mathbf{x}|\\mathbf{y}\\!\\neq\\!\\mathbf{m})\\!=\\!R_{t}(\\mathbf{m},\\mathbf{y}\\!\\neq\\!\\mathbf{m})\\frac{1}{T}$ . Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\bigg[R_{t}(\\mathbf{m},\\mathbf{y}\\neq\\mathbf{m})\\frac{1}{T}\\bigg]=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}[1-\\alpha_{t|s}]}\\\\ &{\\implies R_{t}(\\mathbf{m},\\mathbf{y}\\neq\\mathbf{m})\\!=\\!\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}T(1\\!-\\!\\alpha_{t|s})\\!=\\!-\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "3. Case $(\\mathbf{y}^{\\prime}=\\mathbf{m},\\mathbf{y}=\\mathbf{m})$ : Using (50) and(48), we find $q_{t|s}(\\mathbf{m}|\\mathbf{m})=1$ and $q_{t|s}(\\mathbf{m}|\\mathbf{m})=1+$ $\\begin{array}{r}{R_{t}(\\mathbf{m},\\mathbf{m})\\frac{1}{T}{+}\\mathcal{O}\\big(\\frac{1}{T^{2}}\\big)}\\end{array}$ . Since these two expressions must be equal for any $T$ , it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{t}(\\mathbf{m},\\mathbf{m})\\!=\\!0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that when $R_{t}(\\mathbf{m},\\mathbf{m})$ is constant, the term $\\mathcal{O}\\left(\\frac{1}{T^{2}}\\right)$ reduces to zero, as it includes higher-order time derivatives of $R_{t}$ . ", "page_idx": 23}, {"type": "text", "text": "4. Case $(\\mathbf{y}^{\\prime}\\,{=}\\,\\mathbf{x},\\mathbf{y}\\in\\mathcal{V}\\,{-}\\,\\{\\mathbf{x}\\})$ : In the context of absorbing state diffusion, these states are never observed. Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\nR_{t}(\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{x},\\!\\mathbf{y}\\!\\in\\!\\mathcal{V}\\!-\\!\\{\\mathbf{m},\\!\\mathbf{x}\\})\\!=\\!0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "5. Case $\\left(\\mathbf{y}^{\\prime}\\in{\\mathcal{V}}-\\left\\{\\mathbf{m},\\mathbf{x}\\right\\},\\mathbf{y}\\in\\left\\{\\mathbf{m},\\mathbf{x}\\right\\}\\right)$ : In the context of absorbing state diffusion, these states are never observed. Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\nR_{t}(\\mathbf{y}^{\\prime}\\!\\in\\!\\mathcal{V}\\!-\\!\\{\\mathbf{m},\\!\\mathbf{x}\\},\\!\\mathbf{y}\\!\\in\\!\\{\\mathbf{m},\\!\\mathbf{x}\\})\\!=\\!0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, we can express the forward rate matrix as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\boxed{R_{t}\\!=\\!\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}\\!\\left(\\mathbf{I}\\!-\\!\\mathbf{m}\\mathbf{I}^{\\top}\\right)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It can be seen that the columns of this matrix sum to zero, i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{y^{\\prime}}\\in\\mathcal{V}}R_{t}(\\mathbf{y^{\\prime}},\\mathbf{y})\\!=\\!0\\Longrightarrow R_{t}(\\mathbf{y},\\mathbf{y})\\!=\\!\\sum_{\\mathbf{y^{\\prime}}\\neq\\mathbf{y}}R_{t}(\\mathbf{y^{\\prime}},\\mathbf{y}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which ensures that the probability mass is preserved in the forward diffusion process. Similarly, the reverse rate matrix ${\\tilde{R}}_{t}$ can be written in terms of the forward rate matrix $R_{t}$ as follows [33]: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})\\!=\\!\\left\\{\\begin{array}{l l}{\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}R_{t}(\\mathbf{y},\\mathbf{y}^{\\prime})}&{\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{y}}\\\\ {-\\!\\sum_{\\tilde{\\mathbf{y}}\\neq\\mathbf{y}}\\!\\frac{q_{t}(\\tilde{\\mathbf{y}}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}R_{t}(\\mathbf{y},\\tilde{\\mathbf{y}})}&{\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{y}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.2 NELBO ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Meng et al. [37] introduced the term \u201cconcrete score\u201d for the term $q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})/q_{t}(\\mathbf{y}|\\mathbf{x})$ that appears in ${\\tilde{R}}_{t}$ . Since this quantity is not directly accessible in the reverse diffusion process, we approximate it using a neural network, $\\mathbf{s}_{\\theta}:\\mathcal{V}\\!\\to\\!\\mathcal{V}$ , with parameters $\\theta$ . The approximate reverse posterior $p_{s|t}$ can then be expressed in terms of the approximate reverse rate matrix ${\\tilde{R}}_{t}$ in the following manner: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y})\\!=\\!\\delta_{y^{\\prime},y}\\!+\\!\\tilde{R}_{t}^{\\theta}(\\mathbf{y}^{\\prime},\\!\\mathbf{y})\\frac{1}{T}\\!+\\!\\mathcal{O}\\!\\left(\\frac{1}{T^{2}}\\right)}\\\\ &{\\ R_{t}^{\\theta}(\\mathbf{y}^{\\prime},\\!\\mathbf{y})\\!=\\!\\left\\{\\!\\begin{array}{l l}{\\mathbf{s}_{\\theta}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}R_{t}(\\mathbf{y},\\!\\mathbf{y}^{\\prime})}&{\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{y}}\\\\ {-\\!\\sum_{\\tilde{\\mathbf{y}}\\neq\\mathbf{y}}\\mathbf{s}_{\\theta}(\\mathbf{y})_{\\tilde{\\mathbf{y}}}R_{t}(\\mathbf{y},\\!\\tilde{\\mathbf{y}})}&{\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{y}}\\end{array}\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{s}_{\\theta}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}$ denotes the approximate concrete score $q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})/q_{t}(\\mathbf{y}|\\mathbf{x})$ . Lou et al. [33] propose the following NELBO to train such a model: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot|\\mathbf{x})}\\left[\\sum_{\\mathbf{y^{\\prime}}\\neq\\mathbf{y}}R_{t}(\\mathbf{y},\\mathbf{y}^{\\prime})\\bigg(\\mathbf{s}_{\\theta}(\\mathbf{y})_{\\mathbf{y^{\\prime}}}-\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}\\mathrm{log}\\mathbf{s}_{\\theta}(\\mathbf{y})_{\\mathbf{y^{\\prime}}}+K\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}\\bigg)\\bigg)\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $K(a)\\!=\\!a\\log a-a$ . They mention that this expression can be derived from Benton et al. [4], though they do not provide an explicit derivation. For this reason, we present a rigorous derivation in the following section. ", "page_idx": 24}, {"type": "text", "text": "Proof. Let\u2019s focus on the diffusion loss for this process. As mentioned in the previous section, the reconstruction and prior loss terms reduce to zero. The continuous-time diffusion loss is given by: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{T\\rightarrow\\infty}T\\mathbb{E}_{t\\in\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\},\\mathbf{y}\\sim q_{t}(\\cdot|\\mathbf{x})}[\\mathrm{De}_{\\mathrm{L}}(q_{s}|\\mathbf{(y'|y,x)}\\|p_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y}))]}\\\\ {\\displaystyle=\\operatorname*{lim}_{T\\rightarrow\\infty}T\\mathbb{E}_{t\\in\\{\\frac{1}{T},\\frac{2}{T},\\dots,1\\},\\mathbf{y}\\sim q_{t}(\\cdot|\\mathbf{x})}\\left[\\sum_{\\mathbf{y}^{\\prime}}q_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})\\mathrm{log}\\frac{q_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})}{p_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})}\\right]}\\\\ {\\displaystyle=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot|\\mathbf{x})}\\left[\\operatorname*{lim}_{T\\rightarrow\\infty}T\\sum_{\\mathbf{y}^{\\prime}}q_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})\\mathrm{log}\\frac{q_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})}{p_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot|\\,\\mathbf{x})}\\left[\\underbrace{\\operatorname*{lim}_{\\substack{T\\rightarrow\\infty}}T q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})\\log\\frac{q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})}{p_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})}}_{\\mathrm{Term}\\,1}+\\underbrace{\\operatorname*{lim}_{T\\rightarrow\\infty}T\\sum_{\\substack{\\mathbf{y}^{\\prime}\\neq\\mathbf{y}}}q_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})\\log\\frac{q_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})}{p_{s|t}(\\mathbf{y}^{\\prime}|\\mathbf{y},\\mathbf{x})}}_{\\mathrm{Term}\\,2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let\u2019s simplify these two terms separately. For the derivation, we\u2019ll rely on two key observations: In the limiting case as $T\\to\\infty$ , it follows from (49) that $\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow\\infty}q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})=1}\\end{array}$ and from (59) that $\\mathrm{lim}_{T\\rightarrow\\infty}p_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})\\!=\\!1$ . ", "page_idx": 25}, {"type": "text", "text": "Term 1: ", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\mathrm{lim}}T q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})\\mathrm{log}\\frac{q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})}{p_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})}}\\\\ &{\\therefore\\underset{T\\rightarrow\\infty}{\\mathrm{lim}}q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})\\!=\\!1;\\mathbf{hence},}\\\\ &{=\\underset{T\\rightarrow\\infty}{\\mathrm{lim}}T\\mathrm{log}\\frac{q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})}{p_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})}}\\end{array}$   \nThe above term is in $\\infty\\times0$ indeterminate form; therefore,   \n$=\\operatorname*{lim}_{T\\rightarrow\\infty}T\\big(\\log q_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})\\!-\\!\\log p_{s|t}(\\mathbf{y}|\\mathbf{y},\\mathbf{x})\\big)$   \nSubstituting qs|t and $p_{s|t}$ from (49) and (59), we get:   \n$=\\operatorname*{lim}_{T\\rightarrow\\infty}T\\biggl[\\log\\biggl(1+\\tilde{R}_{t}(\\mathbf{y},\\mathbf{y})\\frac{1}{T}+\\mathcal{O}\\biggl(\\frac{1}{T^{2}}\\biggr)\\biggr)-\\log\\biggl(1+\\tilde{R}_{t}^{\\theta}(\\mathbf{y},\\mathbf{y})\\frac{1}{T}+\\mathcal{O}\\biggl(\\frac{1}{T^{2}}\\biggr)\\biggr)\\biggr]$   \nApplying the Taylor series expansion for $\\log(1\\!+\\!x)$ , we get:   \n$\\begin{array}{r l}&{=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}T\\bigg[\\frac{1}{R}(\\mathbf{y},\\mathbf{y})\\frac{1}{T}+\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg)-\\tilde{R}_{t}^{\\theta}(\\mathbf{y},\\mathbf{y})\\frac{1}{T}-\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg)\\bigg]}\\\\ &{=\\tilde{R}_{t}(\\mathbf{y},\\mathbf{y})+\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}T\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg)-\\tilde{R}_{t}^{\\theta}(\\mathbf{y},\\mathbf{y})-\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}T\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg)}\\\\ &{\\ddots\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}T\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg)=0,\\mathrm{we~get:}}\\\\ &{=\\tilde{R}_{t}(\\mathbf{y},\\mathbf{y})-\\tilde{R}_{t}^{\\theta}(\\mathbf{y},\\mathbf{y})}\\\\ &{\\mathrm{Using~(}\\mathfrak{s h)~a n d~(}\\mathfrak{s0),~w e~g e t:}}\\\\ &{=-\\underset{s^{\\prime}\\neq s}{\\sum}R_{t}(\\mathbf{y},\\mathbf{y}^{\\prime})\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}\\mid\\mathbf{x})}{q_{t}(\\mathbf{y}\\mid\\mathbf{x})}-\\mathrm{s}_{\\theta}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}\\bigg)}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Term 2: ", "page_idx": 25}, {"type": "text", "text": "$\\operatorname*{lim}_{T\\rightarrow\\infty}T\\sum_{\\mathbf{y^{\\prime}}\\neq\\mathbf{y}}q_{s|t}(\\mathbf{y^{\\prime}}|\\mathbf{y},\\mathbf{x})\\mathrm{log}\\frac{q_{s|t}(\\mathbf{y^{\\prime}}|\\mathbf{y},\\mathbf{x})}{p_{s|t}(\\mathbf{y^{\\prime}}|\\mathbf{y},\\mathbf{x})}$   \nSubstituting $q_{s|t}$ and $p_{s|t}$ from (49) and (59), we get:   \n$\\begin{array}{r l}&{=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}T\\underset{\\mathbf{y^{\\prime}}\\not=\\mathbf{y}}{\\sum}\\left[\\bigg[\\frac{q_{s}\\left(\\mathbf{y^{\\prime}}\\mid\\mathbf{x}\\right)}{q_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}\\right)}R_{t}(\\mathbf{y},\\mathbf{y^{\\prime}})\\frac{1}{T}+\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg)\\bigg]\\mathrm{log}\\frac{\\frac{q_{s}\\left(\\mathbf{y^{\\prime}}\\mid\\mathbf{x}\\right)}{q_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}\\right)}R_{t}\\left(\\mathbf{y},\\mathbf{y^{\\prime}}\\right)\\frac{1}{T}+\\mathcal{O}\\left(\\frac{1}{T^{2}}\\right)}{\\mathbf{s}_{\\theta}\\left(\\mathbf{y}\\right)\\mathbf{y^{\\prime}}R_{t}\\left(\\mathbf{y},\\mathbf{y^{\\prime}}\\right)\\frac{1}{T}+\\mathcal{O}\\left(\\frac{1}{T^{2}}\\right)}\\right]}\\\\ &{=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\underset{\\mathbf{y^{\\prime}}\\not=\\mathbf{y}}{\\sum}\\left[\\bigg[\\frac{q_{s}\\left(\\mathbf{y^{\\prime}}\\mid\\mathbf{x}\\right)}{q_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}\\right)}R_{t}(\\mathbf{y},\\mathbf{y^{\\prime}})\\!+\\!T\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg)\\right]\\mathrm{log}\\frac{\\frac{q_{s}\\left(\\mathbf{y^{\\prime}}\\mid\\mathbf{x}\\right)}{q_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}\\right)}R_{t}\\left(\\mathbf{y},\\mathbf{y^{\\prime}}\\right)+T\\mathcal{O}\\left(\\frac{1}{T^{2}}\\right)}{\\mathbf{s}_{\\theta}\\left(\\mathbf{y}\\right)\\mathbf{y^{\\prime}}R_{t}\\left(\\mathbf{y},\\mathbf{y^{\\prime}}\\right)+T\\mathcal{O}\\left(\\frac{1}{T^{2}}\\right)}\\right]}\\end{array}$   \n$\\cdot\\operatorname*{lim}_{T\\to\\infty}T\\mathcal{O}\\left(\\frac{1}{T^{2}}\\right)=0$ , we get:   \n$=\\sum_{\\mathbf{y^{\\prime}}\\neq\\mathbf{y}}\\frac{q_{s}(\\mathbf{y^{\\prime}}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}R_{t}(\\mathbf{y},\\mathbf{y^{\\prime}})\\mathrm{log}\\frac{\\frac{q_{s}(\\mathbf{y^{\\prime}}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}\\underline{{R_{t}(\\mathbf{y}\\cdot\\mathbf{y^{\\prime}})}}}{\\mathbf{s}_{\\theta}(\\mathbf{y})\\mathbf{y^{\\prime}}\\underline{{R_{t}(\\mathbf{y}\\cdot\\mathbf{y^{\\prime}})}}}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n=\\!\\sum_{\\mathbf{y^{\\prime}}\\neq\\mathbf{y}}\\!\\frac{q_{t}(\\mathbf{y^{\\prime}}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}R_{t}(\\mathbf{y},\\mathbf{y^{\\prime}})\\!\\left(\\log\\frac{q_{t}(\\mathbf{y^{\\prime}}|\\mathbf{x})}{q_{t}(\\mathbf{y}|\\mathbf{x})}\\!-\\!\\log\\mathbf{s}_{\\theta}(\\mathbf{y})_{\\mathbf{y^{\\prime}}}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, plugging (63) and (64) into (62) yields us the NELBO as proposed in Lou et al. [33]: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot,\\vert\\mathbf{x})}\\left[-\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{y}}R_{t}(\\mathbf{y},\\mathbf{y}^{\\prime})\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{y}\\vert\\mathbf{x})}-s_{\\theta}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}\\bigg)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{y}}\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{y}\\vert\\mathbf{x})}R_{t}(\\mathbf{y},\\mathbf{y}^{\\prime})\\left(\\log\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{y}\\vert\\mathbf{x})}-\\log s_{\\theta}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}\\right)\\right]}\\\\ &{=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot,\\vert\\mathbf{x})}\\left[\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{y}}R_{t}(\\mathbf{y},\\mathbf{y}^{\\prime})\\bigg(-\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{y}\\vert\\mathbf{x})}+s_{\\theta}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{y}\\vert\\mathbf{x})}\\log\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{y}\\vert\\mathbf{x})}-\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{y}\\vert\\mathbf{x})}\\log s_{\\theta}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $K(a)\\!=\\!a\\mathrm{log}a\\!-\\!a$ . This concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "C.3 Concrete Score for MDLM ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Given a latent variable $\\mathbf{z}_{t}$ and the output of the denoising model, $\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)$ parameterized using SUBS, we aim to recover the concrete score ${\\mathbf{s}}_{\\theta}({\\mathbf{z}}_{t})\\in(\\mathbb{R}^{+}+\\{0\\})^{|\\mathcal{V}|}$ . Note that $\\mathbf{s}_{\\theta}\\big(\\mathbf{z}_{t}\\big)_{\\mathbf{y}}$ is the ratio $\\frac{p_{t}\\left(\\mathbf{y}\\right)}{p_{t}\\left(\\mathbf{z}_{t}\\right)}$ in the reverse process. Since $\\mathbf{x}_{\\theta}$ approximates $\\mathbf{x}$ , we use $p_{t}(\\mathbf{y})\\!=\\!q_{t}(\\mathbf{y}|\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t))$ ; therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{s}_{\\theta}(\\mathbf{z}_{t})_{\\mathbf{y}}\\!=\\!\\frac{p_{t}(\\mathbf{y})}{p_{t}(\\mathbf{z}_{t})}\\!=\\!\\frac{q_{t}(\\mathbf{y}|\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t))}{q_{t}(\\mathbf{z}_{t}|\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t))}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To obtain the score, we first compute $q_{t}\\big(\\mathbf{y}\\vert\\mathbf{x}_{\\theta}\\big(\\mathbf{z}_{t},t\\big)\\big)$ for all possible $\\mathbf{y}$ and $\\mathbf{z}_{t}$ . Using (4), we derive the following expressions under the SUBS parameterization: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad q_{t}(\\mathbf{y}\\!\\neq\\!\\mathbf{m}|\\mathbf{x}_{\\theta}(\\mathbf{z}_{t}\\!=\\!\\mathbf{m},t))\\!=\\!\\alpha_{t}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{y}\\rangle}\\\\ &{\\!\\!\\!\\!q_{t}(\\mathbf{y}\\!\\notin\\!\\{\\mathbf{m},\\mathbf{z}_{t}\\}|\\mathbf{x}_{\\theta}(\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m},t))\\!=\\!0}\\\\ &{\\!\\!\\quad\\quad q_{t}(\\mathbf{y}\\!=\\!\\mathbf{z}_{t}|\\mathbf{x}_{\\theta}(\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m},t))\\!=\\!\\alpha_{t}}\\\\ &{\\!\\!\\quad\\quad q_{t}(\\mathbf{y}\\!=\\!\\mathbf{m}|\\mathbf{x}_{\\theta}(\\mathbf{z}_{t}\\!\\in\\!\\mathcal{V},t))\\!=\\!1\\!-\\!\\alpha_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging these into (66), we get: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbf{s}_{\\theta}\\big(\\mathbf{z}_{t}\\!=\\!\\mathbf{m}\\big)_{\\mathbf{y}\\neq\\mathbf{m}}\\!=\\!\\frac{\\boldsymbol{\\alpha}_{t}}{1-\\boldsymbol{\\alpha}_{t}}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{y}\\rangle}}\\\\ {~~}\\\\ {{\\displaystyle\\mathbf{s}_{\\theta}\\big(\\mathbf{z}_{t}\\!=\\!\\mathbf{m}\\big)_{\\mathbf{y}=\\mathbf{m}}\\!=\\!1}}\\\\ {{\\displaystyle\\mathbf{s}_{\\theta}\\big(\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m}\\big)_{\\mathbf{y}=\\mathbf{m}}\\!=\\!\\frac{1-\\boldsymbol{\\alpha}_{t}}{\\alpha_{t}}}}\\\\ {{\\displaystyle\\mathbf{s}_{\\theta}\\big(\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m}\\big)_{\\mathbf{y}=\\mathbf{z}_{t}}\\!=\\!1}}\\\\ {{\\displaystyle\\mathbf{s}_{\\theta}\\big(\\mathbf{z}_{t}\\!\\neq\\!\\mathbf{m}\\big)_{\\mathbf{y}\\notin\\{\\mathbf{m},\\mathbf{z}_{t}\\}}\\!=\\!0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "These can be consolidated into the following expression: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\boxed{\\mathbf{s}_{\\theta}(\\mathbf{z}_{t})_{\\mathbf{y}}\\!=\\!\\mathbf{y}^{\\top}\\!\\left[\\delta_{\\mathbf{z}_{t},\\mathbf{m}}\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t)\\!+\\!\\left(1\\!-\\!\\delta_{\\mathbf{z}_{t},\\mathbf{m}}\\right)\\!\\frac{1\\!-\\!\\alpha_{t}}{\\alpha_{t}}\\mathbf{m}\\!+\\!\\mathbf{z}_{t}\\right]}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C.4 Reverse Rate Matrix for MDLM ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We can formulate the reverse rate matrix for MDLM using (76) and (56). Recall that the reverse rate matrix $\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})$ is given by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})\\!=\\!\\left\\{\\begin{array}{l l}{\\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}R_{t}(\\mathbf{y},\\mathbf{y}^{\\prime})}&{\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{y}}\\\\ {-\\!\\sum_{\\tilde{\\mathbf{y}}\\neq\\mathbf{y}}\\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{y})_{\\mathbf{y}^{\\prime}}R_{t}(\\mathbf{y},\\tilde{\\mathbf{y}})}&{\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{y}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let\u2019s examine the cases where $\\mathbf y\\!=\\!\\mathbf m$ and $\\mathbf{y}\\neq\\mathbf{m}$ . ", "page_idx": 27}, {"type": "text", "text": "Case $\\mathbf y\\!=\\!\\mathbf m$ : For $\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{m}$ , the reverse rate $\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y}\\!=\\!\\mathbf{m})$ is given by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{R}_{t}(\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{m},\\mathbf{y}\\!=\\!\\mathbf{m})}\\\\ &{=\\mathbf{s}_{\\theta}(\\mathbf{y}\\!=\\!\\mathbf{m})_{\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{m}}R_{t}(\\mathbf{y}\\!=\\!\\mathbf{m},\\!\\mathbf{y}^{\\prime})}\\\\ &{{\\mathrm{Using~}}(7!)\\;{\\mathrm{and}}\\left(56\\right),\\mathrm{we~get:}}\\\\ &{=\\!\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\!\\left\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{y}^{\\prime}\\right\\rangle\\!\\Bigg[\\!-\\!\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}\\!\\Bigg]}\\\\ &{=\\!-\\!\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\!\\left\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{y}^{\\prime}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{m}$ , the reverse rate $\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y}\\!=\\!\\mathbf{m})$ is given by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{R}_{t}(\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{m},\\mathbf{y}\\!=\\!\\mathbf{m})}\\\\ &{=-\\displaystyle\\sum_{\\bar{\\hat{\\tau}}\\neq\\mathbf{m}}\\tilde{R}_{t}(\\bar{\\mathbf{y}},\\mathbf{y}\\!=\\!\\mathbf{m})}\\\\ &{{\\displaystyle\\mathrm{Using}\\,(77),\\,\\mathrm{we\\,get:}}}\\\\ &{=\\displaystyle\\sum_{\\bar{\\hat{\\tau}}\\neq\\mathbf{m}}\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\tilde{\\mathbf{y}}\\rangle}\\\\ &{=\\displaystyle\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\sum_{\\bar{\\hat{\\tau}}\\neq\\mathbf{m}}\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\tilde{\\mathbf{y}}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n=\\frac{\\alpha_{t}^{\\prime}}{1\\!-\\!\\alpha_{t}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Case $\\mathbf{y}\\neq\\mathbf{m}$ : For $\\mathbf{y}^{\\prime}\\neq\\mathbf{y}$ we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{R}_{t}(\\mathbf{y}^{\\prime}\\notin\\{\\mathbf{y},\\mathbf{m}\\},\\mathbf{y}\\neq\\mathbf{m})}\\\\ &{=\\mathbf{s}_{\\theta}(\\mathbf{y}\\neq\\mathbf{m})_{\\mathbf{y}^{\\prime}\\notin\\{\\mathbf{y},\\mathbf{m}\\}}\\underbrace{R_{t}(\\mathbf{y}\\neq\\mathbf{m},\\mathbf{y}^{\\prime}\\notin\\{\\mathbf{y},\\mathbf{m}\\})}_{=\\,0\\mathrm{~from}\\,(\\mathrm{56})}}\\\\ &{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{m}$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{R}_{t}(\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{m},\\!\\mathbf{y}\\!\\neq\\!\\mathbf{m})}\\\\ &{=\\!\\mathbf{s}_{\\theta}(\\mathbf{y}\\!\\neq\\!\\mathbf{m})_{\\mathbf{y}^{\\prime}=\\mathbf{m}}\\!\\underbrace{R_{t}(\\mathbf{y}\\!\\neq\\!\\mathbf{m},\\!\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{m})}_{=\\!\\mathrm{~0~from}\\,(\\boldsymbol{56})}}\\\\ &{=\\!\\mathrm{~0~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, for $\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{y}$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\tilde{R}}_{t}({\\bf y}^{\\prime}\\!=\\!{\\bf y},{\\bf y}\\!\\neq\\!{\\bf m})}}\\\\ {~~}\\\\ {{\\displaystyle=\\!-\\!\\sum_{\\tilde{\\bf y}\\ne\\bf y}\\!\\tilde{R}_{t}(\\tilde{\\bf y},{\\bf y}\\!\\ne\\!{\\bf m})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\quad=-\\underbrace{{\\tilde{R}_{t}}(\\tilde{\\mathbf{y}}\\!=\\!\\mathbf{m},\\mathbf{y}\\!\\neq\\!\\mathbf{m})}_{=\\,0\\,\\mathrm{from}\\,(\\mathbb{S}0)}-\\underbrace{\\sum_{\\tilde{\\mathbf{y}}\\notin\\{\\mathbf{y},\\mathbf{m}\\}}{\\tilde{R}_{t}}(\\tilde{\\mathbf{y}},\\mathbf{y}\\!\\neq\\!\\mathbf{m})}_{=\\,0\\,\\mathrm{from}\\,(79)}}\\\\ &{}&{\\quad=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Summarizing (77), (78), (79), (80), (81), we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{R}_{t}(\\mathbf{y}^{\\prime},\\mathbf{y})\\!=\\!\\left\\{\\!\\!\\begin{array}{l l}{-\\langle\\mathbf{x}_{\\theta}(\\mathbf{y},t),\\mathbf{y}^{\\prime}\\rangle\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}}&{\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{m},\\mathbf{y}\\!=\\!\\mathbf{m}}\\\\ {\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}}&{\\mathbf{y}^{\\prime}\\!=\\!\\mathbf{m},\\mathbf{y}\\!=\\!\\mathbf{m}}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.}\\\\ &{}&{\\!=\\!\\!\\left[-\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}[\\mathbf{y}^{\\prime}]^{\\top}[\\mathbf{x}_{\\theta}(\\mathbf{y},t)\\!-\\!\\mathbf{m}]\\langle\\mathbf{y},\\mathbf{m}\\rangle\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "C.5 Deriving MDLM\u2019s NELBO via CTMC ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Now, we aim to show that substituting the expression for the rate matrix $R_{t}$ in terms of state transition matrix $Q_{t}$ from (56) into (65) and switching from score-parameterization to the SUBS parameterization (Sec. 3.2.3) yields the simplified NELBO for MDLM as given by (10). We present the proof below. Recall that the term $\\langle\\mathbf{a},\\mathbf{b}\\rangle$ denotes the dot product of two vectors a and b. When a and $\\mathbf{b}$ represent two one-hot vectors, this quantity evaluates to 1 if ${\\bf a}\\!=\\!{\\bf b}$ and 0 otherwise. ", "page_idx": 28}, {"type": "text", "text": "Proof. Recall that for absorbing state diffusion, y takes only two possible values, i.e., $\\mathbf{y}\\in\\{\\mathbf{x},\\mathbf{m}\\}$ . Thus, we expand (65) as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{\\Sigma}}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot|\\mathbf{x})}\\Bigg[\\langle\\mathbf{y},\\mathbf{x}\\rangle\\Bigg[\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{x}}R_{t}(\\mathbf{x},\\mathbf{y}^{\\prime})\\bigg(\\mathbf{s}_{\\theta}(\\mathbf{x})\\mathbf{y}_{\\prime}-\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{x}|\\mathbf{x})}\\mathrm{logs}_{\\theta}(\\mathbf{x})\\mathbf{y}_{\\prime}+K\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{x}|\\mathbf{x})}\\bigg)\\bigg)\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad+\\langle\\mathbf{y},\\mathbf{m}\\rangle\\Bigg[\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}R_{t}(\\mathbf{m},\\mathbf{y}^{\\prime})\\bigg(\\mathbf{s}_{\\theta}(\\mathbf{m})\\mathbf{y}^{\\prime}-\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\mathrm{logs}_{\\theta}(\\mathbf{m})\\mathbf{y}^{\\prime}+K\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\bigg)\\bigg)\\Bigg]\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\mathbf{\\nabla}\\cdot R_{t}(\\mathbf{x},\\mathbf{y}^{\\prime}\\neq\\mathbf{x})\\!=\\!0$ from (54), we get: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\b=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot|\\mathbf{x})}\\langle\\mathbf{y},\\mathbf{m}\\rangle\\left[\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}R_{t}(\\mathbf{m},\\mathbf{y}^{\\prime})\\bigg(\\mathbf{s}_{\\theta}(\\mathbf{m})_{\\mathbf{y}^{\\prime}}-\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\log\\mathrm{s}_{\\theta}(\\mathbf{m})_{\\mathbf{y}^{\\prime}}+K\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}|\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\bigg)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Substituting $R_{t}(\\mathbf{m},\\mathbf{y}^{\\prime}\\!\\neq\\!\\mathbf{m})$ from (52), we get: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot\\vert\\mathbf{x})}\\langle\\mathbf{y},\\mathbf{m}\\rangle\\left[\\displaystyle\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}-\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}\\bigg(\\mathbf{s}_{\\theta}(\\mathbf{m})_{\\mathbf{y}^{\\prime}}-\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\log\\mathbf{s}_{\\theta}(\\mathbf{m})_{\\mathbf{y}^{\\prime}}+K\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\bigg)\\bigg)\\right]}\\\\ &{=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot\\vert\\mathbf{x})}\\langle\\mathbf{y},\\mathbf{m}\\rangle\\left[-\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}\\left(\\displaystyle\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}\\mathbf{s}_{\\theta}(\\mathbf{m})_{\\mathbf{y}^{\\prime}}-\\displaystyle\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\log\\mathbf{s}_{\\theta}(\\mathbf{m})_{\\mathbf{y}^{\\prime}}+\\displaystyle\\sum_{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}K\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}{q_{t}(\\mathbf{m}|\\mathbf{x})}\\bigg)\\right)\\right]}\\\\ &{=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot\\vert\\mathbf{x})}\\langle\\mathbf{y},\\mathbf{m}\\rangle\\left[-\\frac{1}{\\mathrm{Rem}}\\underbrace{1}_{\\mathrm{Tem}1}\\underbrace{\\underbrace{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}_{\\mathrm{Tem}2}}_{\\mathrm{Tem}1}\\right]\\underbrace{\\overbrace{\\mathbf{x}^{\\prime}\\neq\\mathbf{m}}^{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})}}_{\\mathrm{Term}2}\\underbrace{\\overbrace{\\mathbf{y}^{\\prime}\\neq\\mathbf{m}}^{\\because}}_{\\mathrm{Term}3}\\mathcal{K}\\bigg(\\frac{q_{t}(\\mathbf{y}^{\\prime}\\vert\\mathbf{x})} \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Term 1: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{y}\\neq\\mathbf{m}}\\mathbf{s}_{\\theta}(\\mathbf{m})_{\\mathbf{y}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n=\\sum_{\\mathbf{y}\\neq\\mathbf{m}}{\\frac{\\alpha_{t}}{1-\\alpha_{t}}}\\langle\\mathbf{x}_{\\theta}(\\mathbf{m},t),\\mathbf{y}\\rangle\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{={\\displaystyle{\\frac{\\alpha_{t}}{1-\\alpha_{t}}}\\sum_{\\mathbf{y}\\neq\\mathbf{m}}}\\langle\\mathbf{x}_{\\theta}(\\mathbf{m},t),\\mathbf{y}\\rangle}\\\\ &{\\mathrel{\\phantom{=}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Term 2: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{v^{\\prime}\\neq v^{\\prime}}{\\sum}\\frac{q(v^{\\prime}|x)}{\\lvert v^{\\prime}\\rvert\\omega}\\mathrm{log}_{s^{\\prime}}(\\mathbf{m})_{v^{\\prime}}}\\\\ &{=\\frac{q(x|x)}{q_{1}(\\mathbf{m})}\\mathrm{log}_{s^{\\prime}}(\\mathbf{m})_{v^{\\prime}}+\\underset{v^{\\prime}\\neq v^{\\prime}}{\\sum}\\frac{q(v^{\\prime}|x)}{\\lvert\\mathbf{\\Omega}(\\mathbf{m})\\rvert\\mathrm{\\Omega}(\\mathbf{m})\\mathrm{]}}\\mathrm{log}_{s^{\\prime}}(\\mathbf{m})_{v^{\\prime}}}\\\\ &{\\overset{,,}{\\geq}\\mathrm{rig}\\left(v^{\\prime}|x|-0\\mathrm{tog}^{\\prime}\\epsilon\\left(\\epsilon\\left(\\mathbf{m},\\mathbf{m}\\right)\\mathrm{for}\\left(\\epsilon\\right)\\right)\\mathrm{wegrt}\\right.}\\\\ &{=\\frac{q(x|x)}{q_{1}(\\mathbf{m})}\\mathrm{log}_{s^{\\prime}}(\\mathbf{m})_{v^{\\prime}}}\\\\ &{\\overset{,,}{\\geq}\\mathrm{tig}\\left(v^{\\prime}\\mathrm{ing}_{s^{\\prime}}\\mathrm{log}_{s^{\\prime}}\\right.}\\\\ &{\\left.=\\frac{\\Omega_{s}}{q_{1}(\\mathbf{m})}\\mathrm{log}_{s^{\\prime}}(\\mathbf{m})_{v^{\\prime}}}\\\\ &{\\mathrm{log}_{s^{\\prime}}(\\mathbf{m})_{v^{\\prime}}\\mathrm{weg}_{s^{\\prime}}}\\\\ &{=\\frac{\\Omega_{s}}{1-\\Omega_{s}}\\mathrm{log}\\left[\\frac{\\Omega_{s}}{1-\\Omega_{s}}\\mathrm{for}\\left(\\epsilon\\mathbf{m},\\epsilon\\right)_{v^{\\prime}}\\right]}\\\\ &{=\\frac{\\Omega_{s}}{1-\\Omega_{s}}\\mathrm{log}_{s^{\\prime}}+\\frac{\\Omega_{s}}{1-\\Omega_{s}}\\mathrm{log}\\left(\\mathrm{log}_{s^{\\prime}}(\\mathbf{m},\\epsilon),x\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Term 3: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\gamma^{\\prime}\\neq\\mathbf{n}}K\\left(\\frac{q_{i}(\\gamma^{\\prime}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\right)}\\\\ &{\\displaystyle=\\sum_{\\gamma^{\\prime}\\neq\\mathbf{n}}\\left[\\frac{q_{i}(\\gamma^{\\prime}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\log\\frac{q_{i}(\\gamma^{\\prime}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\right]}\\\\ &{\\displaystyle=\\frac{q_{i}(\\mathbf{x}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\log\\frac{q_{i}(\\mathbf{x}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}-\\frac{q_{i}(\\mathbf{x}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\left[\\frac{q_{i}(\\gamma^{\\prime}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\log\\frac{q_{i}(\\gamma^{\\prime}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}-\\frac{q_{i}(\\gamma^{\\prime}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\right]}\\\\ &{\\displaystyle=\\cdot q_{i}(\\gamma^{\\prime}|\\mathbf{x})=6\\pi\\mathrm{e}^{\\prime}\\notin\\{\\mathbf{x},\\mathbf{m}\\},\\:\\mathrm{wegrt},}\\\\ &{\\displaystyle=\\frac{q_{i}(\\mathbf{x}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\log\\frac{q_{i}(\\mathbf{x}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}-\\frac{q_{i}(\\mathbf{x}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}}\\\\ &{\\displaystyle=\\frac{\\delta\\exp\\left(\\frac{q_{i}(\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}\\right)}{16\\pi\\mathrm{e}^{\\prime}}\\ln\\frac{q_{i}(\\mathbf{x}|\\mathbf{x})}{q_{i}(\\mathbf{m}|\\mathbf{x})}}\\\\ &{\\displaystyle=\\frac{\\alpha_{i}(\\mathbf{x})}{16\\pi\\mathrm{e}^{\\prime}}\\frac{\\alpha_{i}}{16\\pi\\mathrm{e}^{\\prime}}-\\frac{\\alpha_{i}}{16\\pi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Substituing (84), (85), and (86) in (83) we get, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot,\\vert\\mathbf{x})}\\langle\\mathbf{y},\\mathbf{m}\\rangle\\Bigg[-\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}\\Bigg(\\frac{\\alpha_{t}}{\\!\\!\\sum_{l=\\alpha_{t}}\\!\\!-\\!\\frac{\\alpha_{t}}{1\\!-\\!\\alpha_{t}}\\!\\log\\!\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\!-\\!\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\!\\log\\!\\langle\\mathbf{x}_{\\theta}(\\mathbf{m},t),\\mathbf{x}\\rangle\\!}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\frac{\\alpha_{t}}{\\!\\!\\!\\sum_{l=\\omega_{t}}\\!\\!\\!\\log\\!\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\!-\\!\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\!}\\Bigg)\\Bigg]}\\\\ &{=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot,\\vert\\mathbf{x})}\\langle\\mathbf{y},\\mathbf{m}\\rangle\\left[-\\frac{\\alpha_{t}^{\\prime}}{\\alpha_{t}}\\!\\left(-\\!\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\!\\log\\!\\langle\\mathbf{x}_{\\theta}(\\mathbf{m},t),\\mathbf{x}\\rangle\\right)\\right]}\\\\ &{=\\mathbb{E}_{t\\in[0,1],\\mathbf{y}\\sim q_{t}(\\cdot,\\vert\\mathbf{x})}\\langle\\mathbf{y},\\mathbf{m}\\rangle\\left[\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\!\\log\\!\\langle\\mathbf{x}_{\\theta}(\\mathbf{m},t),\\mathbf{x}\\rangle\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Under the SUBS parameterization, $\\log\\langle\\mathbf{x}_{\\theta}(\\mathbf{z}_{t},t),\\mathbf{x}\\rangle\\!=\\!0$ when $\\mathbf{z}_{t}\\!=\\!\\mathbf{x}$ ; hence $\\langle\\mathbf{y},\\mathbf{m}\\rangle$ can be dropped: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left.=\\boxed{\\mathbb{E}_{t\\in[0,1],{\\mathbf{y}}\\sim q_{t}(.|{\\mathbf{x}})}\\left[\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\mathrm{log}\\langle{\\mathbf{x}}_{\\theta}({\\mathbf{z}}_{t},t),{\\mathbf{x}}\\rangle\\right]}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 30}, {"type": "text", "text": "Appendix D Experimental details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "D.1 Likelihood Evaluation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We use a single monte-carlo estimate for $t$ to evaluate the likelihood. The low discrepancy sampler (D.3) plays a key role in reducing the variance of the estimate as seen in Table 8. ", "page_idx": 30}, {"type": "text", "text": "D.2 Avg. Number of Tokens seen ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Given training_steps, batch_size, context_length, the number of tokens seen by the AR model is given as: ", "page_idx": 30}, {"type": "text", "text": "However, this expression doesn\u2019t hold for a diffusion model, since at each training step, a fraction of the input tokens are masked before being fed to the model. Let $p_{m}$ be the probability of a token being masked at a timestep $t$ . For the log-linear schedule in our experiments, $p_{m}\\!=\\!t$ . Thus, the expected number of tokens seen by the diffusion model is: ", "page_idx": 30}, {"type": "text", "text": "$\\mathbb{E}_{t\\sim\\mathcal{U}[0,1]}$ [training_steps $\\times$ batch_size\u00d7context_length\u00d7pm] $=$ training_steps $\\times$ batch_size $\\times$ context_length\u00d7Et\u223cU[0,1][pm] $=$ training_steps $\\times$ batch_size $\\times$ context_length\u00d7Et\u223cU[0,1][t] \u2235pm =t $=$ training_steps $\\times$ batch_size $\\times$ context_length $\\times0.5$ . \u2235Et\u223cU[0,1][t]=0.5 ", "page_idx": 30}, {"type": "text", "text": "LM1B. Following [1, 33, 26], we train MDLM for 1M training steps with a batch_size $=512$ , and a context length of 128. Like [33] we use a log-linear schedule and hence the number of tokens seen by our model is $\\approx\\!33\\mathrm{{B}}$ (89). Similarly, MDLM trained for 10M steps, saw 327B tokens in expectation. The corresponding AR baseline was trained for $0.5\\mathrm{M}$ and 5M steps to ensure a similar number of tokens was seen. ", "page_idx": 30}, {"type": "text", "text": "OWT. We train SEDD and MDLM for 1M training steps with a batch_size $=~512$ , context_length $=1024$ , and log-linear schedule. Hence, these models saw 262B tokens during training. Similarly, the AR model saw the same number of tokens when trained for $0.5\\mathrm{M}$ steps with the same batch_size and context_length. ", "page_idx": 30}, {"type": "text", "text": "D.3 Low discrepancy sampler ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Toreducevarianceduringtrainingweusealow-discrepancysampler, similartothatproposedinKingma et al. [29]. Specifically, when processing a minibatch of $N$ samples, instead of independently sampling $N$ from a uniform distribution, we partition the unit interval and sample the time step for each sequence $i\\in\\{1,\\ldots,N\\}$ from a different portion of the interval $\\begin{array}{r}{t_{i}\\sim U[\\frac{i-1}{N},\\frac{i}{N}]}\\end{array}$ . This ensures that our sampled timesteps are more evenly spaced across the interval [0,1], reducing the variance of the ELBO. ", "page_idx": 30}, {"type": "text", "text": "D.4 Language Modeling ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "For our forward noise process, we use a log-linear noise schedule similar to Lou et al. [33]. ", "page_idx": 30}, {"type": "text", "text": "We detokenize the One Billion Words dataset following Lou et al. [33], whose code can be found here4. We tokenize the One Billion Words dataset with the bert-base-uncased tokenizer, following He et al. [26]. We pad and truncate sequences to a length of 128. ", "page_idx": 30}, {"type": "text", "text": "We tokenize OpenWebText with the GPT2 tokenizer. We do not pad or truncate sequences \u2013 we concatenate and wrap them to a length of 1,024. When wrapping, we add the eos token in-between concatenated. We additionally set the first and last token of every batch to be eos. Since OpenWebText does not have a validation split, we leave the last 100k docs as validation. ", "page_idx": 31}, {"type": "text", "text": "We parameterize our autoregressive baselines, SEDD, and MDLM with the transformer architecture from Lou et al. [33]. We use 12 layers, a hidden dimension of 768, 12 attention heads, and a timestep embedding of 128 when applicable. Word embeddings are not tied between the input and output. ", "page_idx": 31}, {"type": "text", "text": "We use the AdamW optimizer with a batch size of 512, constant learning rate warmup from 0 to a learning rate of 3e-4 for 2,500 steps. We use a constant learning rate for 1M, 5M, or 10M steps on One Billion Words, and 1M steps for OpenWebText. We use a dropout rate of 0.1. ", "page_idx": 31}, {"type": "text", "text": "D.5 Zeroshot Likelihood ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We evaluate zeroshot likelihoods by taking the models trained on OpenWebText and evaluating likelihoods on the validation splits of 7 datasets: Penn Tree Bank (PTB; Marcus et al. [36]), Wikitext [38], One Billion Word Language Model Benchmark (LM1B; Chelba et al. [8]), Lambada [41], AG News [68], and Scientific Papers (Pubmed and Arxiv subsets; Cohan et al. [10]). We detokenize the datasets following Lou et al. [33]. For the AG News and Scientific Papers (Pubmed and Arxiv), we apply both the Wikitext and One Billion Words detokenizers. Since the zeroshot datasets have different conventions for sequence segmentation, we wrap sequences to 1024 and do not add eos tokens in between sequences. ", "page_idx": 31}, {"type": "text", "text": "D.6 Representation Learning ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Following Devlin et al. [15], we evaluate on all GLUE tasks [65], but exclude WNLI. ", "page_idx": 31}, {"type": "text", "text": "We pre-train a MosaicBERT model on C4 [46] for 70k steps, corresponding to 36B tokens. We pad and truncate the data to 128 tokens using the bert-base-uncased tokenizer. ", "page_idx": 31}, {"type": "text", "text": "MosaicBERT [43] has a similar architecture to bert-base-uncased and has 137M parameters, 12 layers, 12 attention heads, a hidden dimension of 768, an intermediate size of 3072, and ALiBi attention bias [44]. ", "page_idx": 31}, {"type": "text", "text": "For pre-training, we use the following hyperparameters: A global batch size of 4096 with gradient accumulation, a learning rate of 5e-4, linear decay to $0.02\\mathbf{x}$ of the learning rate with a warmup of $0.06\\mathrm{x}$ of the full training duration, and the decoupled AdamW optimizer with 1e-5 weight decay and betas 0.9 and 0.98. ", "page_idx": 31}, {"type": "text", "text": "For diffusion fine-tuning we use AdamW with a warmup of 2,500 steps from a learning rate of 0 to 5e-5, betas 0.95 and 0.999, and batch size 512. We train for $5\\mathrm{k}$ steps total, corresponding to 32M tokens. ", "page_idx": 31}, {"type": "text", "text": "For GLUE evaluation, we use the HuggingFace script found here5. We use the default parameters for all datasets, except for a batch size of 16, which we found helped with smaller datasets. This includes the default of 3 epochs for all datasets and learning rate of 2e-5. ", "page_idx": 31}, {"type": "text", "text": "D.7 Diffusion DNA Models ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Dataset We pre-train the Caduceus MLM [50] on the HG38 human reference genome [11]. Following Schiff et al. [50], we use character- / base pair-level tokenization. The dataset is based on the splits used in Avsec et al. [3]: the training split comprises of 35 billion tokens covering the human genome. This consists of 34,021 segments extended to a maximum length of 1,048,576 (220 segments). We maintain a constant $2^{20}$ tokens per batch. For the Genomics Benchmark tasks, we use 5-fold cross-validation where we split the training set into 90/10 train/validation splits. ", "page_idx": 31}, {"type": "text", "text": "Architecture The Caduceus MLM uses as a backbone a bi-directional variant of the data-dependent SSM Mamba block proposed in Gu et al. [22]. This architecture is ideal as it contains inductive biases that preserve reverse complement (RC) equviariance, respecting the inherent symmetry of double-stranded DNA molecules [35, 50, 69]. ", "page_idx": 31}, {"type": "text", "text": "Training details All models are pre-trained on 10B tokens (10K steps) and fine-tuned on a generative objective for an additional 50B tokens (50K steps). We use a global batch size of 1024 for a context length of 1024 tokens. Downstream task fine-tuning is performed for 16K steps ( 1B tokens). ", "page_idx": 32}, {"type": "text", "text": "For performing Caduceus MLM pre-training, we follow Schiff et al. [50] for the model size configuration, and hyperparameter selection. For pre-training, we use a fixed $15\\%$ mask rate as done in Devlin et al. [15]. Of the \u2019masked\u2019 tokens, $80\\%$ are replaced with [MASK] , $10\\%$ are replaced with a random token from the vocabulary, and $10\\%$ are left unchanged. ", "page_idx": 32}, {"type": "text", "text": "For fine-tuning all Mamba-based models (including Caduceus) on diffusion objectives, we lower the learning rate from 8e-3 to 1e-3. For fine-tuning HyenaDNA [39], we lower the learning rate from 6e-4 to 5e-5. Similar to Gu et al. [22], Schiff et al. [50], we found that Mamba-based models were robust to higher learning rates. We exclude timestep embeddings for all Diffusion DNA experiments, as we show it has minimal impact on generative performance (see Table 12, Suppl. E.5). ", "page_idx": 32}, {"type": "text", "text": "We perform downstream task fine-tuning on the final hidden state embedding from pre-training. We perform mean pooling across the sequence length, which may vary from 200 to approximately 2,000 bps. We report the mean and $\\pm$ on max/min classification accuracy over 5-fold cross-validation (CV) using different random seeds, with early stopping on validation accuracy. For each task, we do a hyperparameter sweep over batch size and learning rate and report the values of the 5-fold CV for the best configuration. ", "page_idx": 32}, {"type": "text", "text": "Genomic Benchmark Task Distributions We use a subset of the Genomic Benchmark tasks with an emphasis on tasks from Human data. The positive samples for each dataset were generated by selecting samples that were annotated, either computationally or experimentally, in previous work (e.g enhancers, promoters, open chromatin regions (OCR)) [20]. These annotations each correspond to subsets of the genome of varying sizes that may exhibit different distributions of DNA than those observed globally over the reference genome. Due to this, the observed dataset may have a different distribution than the data used for pre-training and calculating perplexity. This might in turn lead to a case where perplexity and downstream performance may not necessarily correlate. ", "page_idx": 32}, {"type": "text", "text": "Appendix E Additional Experiments ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "E.1 Noise schedule parameterization ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "As described in Sec. 3.4, the ELBO is invariant to the functional form of $\\alpha_{t}$ . To demonstrate this, we evaluate MDLM, initially trained using a log-linear schedule on OWT, by replacing the noise schedule with various other noise schedules as mentioned below. Following prior works [1, 33, 54], we parameterize $\\alpha_{t}\\!=\\!e^{-\\sigma(t)}$ , where $\\sigma(t)\\!:\\![0,1]\\!\\rightarrow\\!\\mathbb{R}^{+}$ . Various functional forms of $\\sigma(t)$ are listed below: ", "page_idx": 32}, {"type": "text", "text": "Log Linear [1, 33, 54]. The log linear schedule is given as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma(t)\\!=\\!-\\!\\log{(1\\!-\\!t)}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Cosine Squared schedule [24]. The Cosine Squared schedule is given as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma(t)\\!=\\!-\\!\\log\\cos^{2}\\!\\left(\\frac{\\pi}{2}(1\\!-\\!t)\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Cosine schedule. The Cosine schedule is given as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma(t)\\!=\\!-\\!\\log\\cos\\!\\left(\\frac{\\pi}{2}(1\\!-\\!t)\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Linear. The Linear schedule is given as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma(t)\\!=\\!\\sigma_{\\mathrm{max}}t\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\sigma_{\\mathrm{max}}$ is a very large number. In our experiments we set it to $10^{8}$ . ", "page_idx": 32}, {"type": "text", "text": "E.1.1 ELBO Invariance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The function $\\alpha_{t}$ is invertible due to the monotonicity assumption in Sec. 3.1, and so we can perform the following change of variables in (10): $\\gamma\\equiv\\log(1-\\alpha_{t})$ . Let $f:[0,1]\\to\\mathbb{R}^{-}$ be a function such ", "page_idx": 32}, {"type": "text", "text": "that $\\gamma=f(t)$ . Note that $\\alpha_{t}$ goes through a monotonic transformation to obtain $\\gamma$ ; hence, $\\gamma$ is also monotonic in $t$ since $\\alpha_{t}$ is monotonic in $t$ . This implies that the function $f$ is invertible. Let $t\\!=\\!f^{-1}(\\gamma)$ . Then, we can we have the following diffusion loss: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{\\mathrm{NHM},0}^{\\infty}\\!=\\!\\mathbb{E}_{q}\\int_{t_{0}}^{t_{1}-1}\\frac{\\sigma_{t}^{\\prime}}{1-\\alpha_{t}}\\mathrm{log}(\\mathrm{x}_{\\theta}(\\mathrm{z}_{t},t),\\mathrm{x})\\mathrm{d}t}\\\\ &{\\qquad\\qquad=\\!-\\mathbb{E}_{q}\\int_{t_{0}}^{t-1}\\log(\\mathrm{x}_{\\theta}(\\mathrm{z}_{t},t),\\mathrm{x})\\frac{\\mathrm{d}}{\\mathrm{d}t}[\\log(1-\\alpha_{t})]\\mathrm{d}t}\\\\ &{\\qquad\\qquad=\\!-\\mathbb{E}_{q}\\int_{t_{0}}^{t-1}\\log(\\mathrm{x}_{\\theta}(\\mathrm{z}_{t},t),\\mathrm{x})\\frac{\\mathrm{d}}{\\mathrm{d}t}[f(t)]\\mathrm{d}t}\\\\ &{\\qquad\\qquad=\\!-\\mathbb{E}_{q}\\int_{\\gamma_{-}-\\infty}^{\\gamma_{-}}\\log(\\mathrm{x}_{\\theta}(\\mathrm{z}_{f-1}(\\gamma),f^{-1}(\\gamma)),\\mathrm{x})\\mathrm{d}\\gamma}\\\\ &{\\qquad\\qquad=\\!-\\mathbb{E}_{q}\\int_{\\gamma_{-}-\\infty}^{\\gamma_{-}}\\log(\\mathrm{x}_{\\theta}(\\mathrm{\\bar{z}}_{\\gamma},f^{-1}(\\gamma)),\\mathrm{x})\\mathrm{d}\\gamma}\\\\ &{\\qquad\\qquad=\\!-\\mathbb{E}_{q}\\int_{\\gamma_{-}-\\infty}^{\\gamma_{-}}\\log(\\mathrm{\\tilde{x}}_{\\theta}(\\mathrm{\\bar{z}}_{\\gamma},\\gamma),\\mathrm{x})\\mathrm{d}\\gamma}\\\\ &{\\qquad\\qquad=\\!-\\mathbb{E}_{q}\\int_{\\gamma_{-}-\\infty}^{\\gamma_{-}}\\log(\\mathrm{\\tilde{x}}_{\\theta}(\\mathrm{\\bar{z}}_{\\gamma},\\gamma),\\mathrm{x})\\mathrm{d}\\gamma}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{{\\bf z}}_{\\gamma}\\!\\equiv\\!{\\bf z}_{f^{-1}(\\gamma)}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{x}}_{\\theta}(\\tilde{\\mathbf{z}}_{\\gamma},\\gamma)\\!\\equiv\\!\\mathbf{x}_{\\theta}(\\tilde{\\mathbf{z}}_{\\gamma},f^{-1}(\\gamma))\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This new formulation demonstrates that the diffusion loss is invariant to the functional form of $\\alpha_{t}$ . In Table 9, we demonstrate empirically that noise schedules with different functional forms evaluate to the same Likelihood which is consistent with our theory. However, different schedules lead to different per data point variance. Notably, the log-linear schedule exhibits the lowest variance among all the noise schedules considered. ", "page_idx": 33}, {"type": "text", "text": "Table 9: Likelihood in bits per dimension (BPD) for different noise schedules on OWT dataset, is reported along with the mean and variance associated with each noise schedule per data point. We empirically observe that noise schedules with different functional forms yield the same likelihood, consistent with our theory in Sec. 3.4; however, different schedules result in different variances. ", "page_idx": 33}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/6c993b259c07f08f315a9ec5b32bafac1c9e0a71fecd8fbd6e6c06fb1a35bc50.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "E.2 Faster sampling with caching ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In Figure 10, we compare the wall clock times of variaous methods: AR, SEDD, MDLM with caching, and MDLM without caching for generating 64 samples on a single GPU. When sampling in batches, a change of 1 token would necessitate a call to the denoising model. Therefore, smaller batch sizes have a lower likelihood of a token being unmasked. This might lead one to prefer generating samples in smaller batches, as opposed to using a larger batch size that fully saturates the GPU. Table 10 shows that generating samples with a batch size of 1 and using caching is twice as fast as generating samples without caching while fully utilizing the GPU. In Fig. 2, we observe that MDLM without caching yields samples that consistently get better generative perplexity than SEDD. For $T\\!=\\!\\{5k,\\!10k\\}$ , both SEDD and MDLM get better generative perplexity than the AR model. ", "page_idx": 33}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/395efdfc587285fb1ee5dfdf994b934257db7d0b4499668e08147a88f341f4bb.jpg", "table_caption": ["Table 10: Wall clock time reported in minutes to generate 64 samples on a single A5000 GPU. "], "table_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "L4uaAR4ArM/tmp/0cf23bd848c64b82f9de099279f6f50d62af39c951ba744f0e4e1fb3c65ce9a3.jpg", "img_caption": ["Generative perplexities across sample times on OpenWebText "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 2: Generative perplexities across wall clock time for generating 64 samples on OWT using a single 32GB A5000 GPU are compared by varying $T\\in\\{100,500,1000,5000,10000\\}$ in the reverse diffusion process. The samples are generated in mini-batches with a batch size of 16 for AR, SEDD, and MDLM without caching, as it is the largest batch size that fits on this GPU. For MDLM with caching, we vary the batch size. ", "page_idx": 34}, {"type": "text", "text": "E.3 LM1B ablations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We assess the importance of our continuous-time framework by performing ablation on diffusion steps $T$ . In Table 11, we compare NLL and PPL under continuous and discrete T in MDLM. We find that NLL consistently decreases as $T\\!\\to\\!\\infty$ . ", "page_idx": 34}, {"type": "text", "text": "Table 11: Discrete vs continuous time evaluation for MDLM w/o time-conditioning on OWT. MDLM was trained with $T\\!=\\!\\infty$ . We report test perplexity for a discrete $T$ . ", "page_idx": 34}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/e2df40bcdd91f4deee06972d4ac0252f9460ba944998302846b082a7cbcdc191.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "E.4 Train NLL curves on OWT ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In Figure 3, we show that MDLM achieves lower variance loss during training compared to a previous diffusion language model, SEDD. Training is performed over 1M steps on OWT (which corresponds to 524B tokens). ", "page_idx": 34}, {"type": "image", "img_path": "L4uaAR4ArM/tmp/9355d8800de095470d70149740ae37d0daaf850aa8e326e334d1e8f0cac1f4e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 3: Train negative log-likelihood (NLL) curves across 1M gradient steps (524B tokens) on OpenWebText [18]. NLL is logged every 1K steps without value smoothing. ", "page_idx": 35}, {"type": "text", "text": "E.5 Time-conditioning ablation on OWT ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In Table 12, we assess the importance of time conditioning in MDLM on OWT. We observe that time-conditioning has minimal impact on perplexity. Training is performed over 1M steps on OWT (which corresponds to 524B tokens). ", "page_idx": 35}, {"type": "table", "img_path": "L4uaAR4ArM/tmp/18eff808d4bf979617eb209c743397c5026bda9b001416ff0b4fcdbb9ca6b41b.jpg", "table_caption": ["Table 12: Ablation on time-conditioning in MDLM on OWT. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "E.6 Unconditional Samples ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Here, we present some unconditional samples generated by MDLM trained on OWT with a context length of $L\\!=\\!1024$ for $T\\!=\\!\\!\\{1000,\\!10000\\}\\!$ . ", "page_idx": 35}, {"type": "text", "text": "E.6.1 $\\mathbf{T}\\,{=}\\,1000$ ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Example 1 $<|$ endoftext $|>\\ a\\ 17\\!-\\!10$ victory and a trip to the playoffs. ", "page_idx": 35}, {"type": "text", "text": "The last wildcard seed: New York Jets, Houston and the last potable playoff spot. The last-second home wins: New Orleans and Carolina, 21-21. ", "page_idx": 35}, {"type": "text", "text": "The Saints finish sixth with the highest regular season (42) NFC wins. They lost 14 of their 13 games in the conference playoffs. ", "page_idx": 35}, {"type": "text", "text": "The Cardinals were in Group A in Round 1 with Game 2, Round 3, Round 4 and Quarter Game 5, but they made their last trip to the playoffs off North Carolina on the road as even North Carolina. ", "page_idx": 35}, {"type": "text", "text": "True to their reputation, the Cards swept the Saints in the first round, but knocked it out at home. No Panthers went to the playoffs more than the Saints. ", "page_idx": 35}, {"type": "text", "text": "Don Jean no longer is the South Carolina Panther. ", "page_idx": 36}, {"type": "text", "text": "The Cardinals thought that provided that he had a chance to be an NFL player. ", "page_idx": 36}, {"type": "text", "text": "\"I did,\" said defensive end Lorenzo Williams with a laugh as he exited his car at the airport. \"Also, I won that game.\" ", "page_idx": 36}, {"type": "text", "text": "KC win brings Carson back home. ", "page_idx": 36}, {"type": "text", "text": "Griffin made promise on Sunday to never exactly give up the dunk. Although he failed to score 40 points in the playoffs, he has had better luck in them this year. ", "page_idx": 36}, {"type": "text", "text": "With turnovers and fumbles returning, he has to play out because the team doesn\u2019t trust him. He\u2019s long years of injuries, turnovers and calls because he knows he can play that way for everybody. ", "page_idx": 36}, {"type": "text", "text": "Griffin is no stranger to Saints fans. ", "page_idx": 36}, {"type": "text", "text": "\"Players want him to know them,\" someone from South Carolina said after coming out against the Panthers \u2014 in their best home home Week 7 win Sunday \u2014 in an 11-9 rout. South Carolina did win its final three and passed the Saints, 24-1. ", "page_idx": 36}, {"type": "text", "text": "Although the Cardinals are in the South, they am a step behind. ", "page_idx": 36}, {"type": "text", "text": "They still have little time left to take down the West Coast wild card. There is no chance they get another victory. ", "page_idx": 36}, {"type": "text", "text": "The West was out by Beshear in their first round games last season, losing by 63 to the 49ers. ", "page_idx": 36}, {"type": "text", "text": "The outcome will be tough. ", "page_idx": 36}, {"type": "text", "text": "\"Now we\u2019re so close, let\u2019s figure out the time to win,\" Brees said. \"We still have a few games left; I\u2019m glad about that.\" ", "page_idx": 36}, {"type": "text", "text": "South Carolina takes the revenge. ", "page_idx": 36}, {"type": "text", "text": "When asked about his second time since Super 4, Brees shot back that he understood. ", "page_idx": 36}, {"type": "text", "text": "\"You can doubt the answer but I think that was a no-brainer. In time, you try to prove an answer wrong,\" Brees said. \"I think his ability will be as cool as Julio Jones\u2019 ability, but having that time [out] to my season was difficult overall. I did what I was expecting to do. Hopefully they\u2019ll tell me to try again.\" ", "page_idx": 36}, {"type": "text", "text": "After their late win, the NFL calls the Saints reschedule\u2019must try.\u2019 \"Because Saints,\" those who am there still say it, \"focus on defense and, offense is defense.\" ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "ESPN said Saints\u2019s star receiver Dashon Jeffishard, turning heads on long passes and connecting with open defenders, already had their 20-yard overall score from the field. When Jeffishard finished with three passing he set up. He obviously had no difference; it was his first snap-off. ", "page_idx": 36}, {"type": "text", "text": "With his changes in his starting lineup, Brees was just hoping they had little to prove against Carolina over the weekend. ", "page_idx": 36}, {"type": "text", "text": "\"I felt like we didn\u2019t have quite enough focus on and there was so puny coverage that I wanted more of our guys at the same position so we\u2019d up our game,\" he said. ", "page_idx": 36}, {"type": "text", "text": "Brees said South Carolina was well at linebacker. ", "page_idx": 36}, {"type": "text", "text": "\"If that\u2019s part of it, if you\u2019re going to try and stay with what you\u2019re going [out with]. What\u2019d you want? Smart play,\" Brees said. \"What would you say? That you\u2019re always ready to play. So you\u2019re going out there strong and ready to go to play football.\" ", "page_idx": 37}, {"type": "text", "text": "That said, New Orleans was damned shy when it came to Carolina. ", "page_idx": 37}, {"type": "text", "text": "\"My guys admit to feeling it a little bit [Sunday but] I say to them that they always knew, \u2019I don\u2019t think that was necessarily how I would beat you, that will give them their confidence,\" Brees said. ", "page_idx": 37}, {"type": "text", "text": "\"It was really hard because I\u2019ve obviously learned a lot of detail about how to deal with everyone and as hard as I have to be, I also feel part of the stuff that they\u2019ve been through on the team, like they\u2019re still going to go through things they know are somewhat right, but they feel a lot of pressure so it\u2019s got to be important to get it right now to get it in the future.\" ", "page_idx": 37}, {"type": "text", "text": "Could all ask for more roses? ", "page_idx": 37}, {"type": "text", "text": "Let\u2019s just take a slip, South Carolina, and face the NFC<|endoftext|> ", "page_idx": 37}, {"type": "text", "text": "Example 2 <|endoftext|> Memorial Hospital. ", "page_idx": 37}, {"type": "text", "text": "Valia and Hill had been working with the Coast Guard in response to public questions, and when they were reached couldn\u2019t comment on the new information, Chapman said. ", "page_idx": 37}, {"type": "text", "text": "People referred to Valia during the years from Hill\u2019s family in Ants, and she cut in contact with their family and friends in 2016. ", "page_idx": 37}, {"type": "text", "text": "\"Each day they stepped on the bus, when they left they saw me on TV,\" she said. ", "page_idx": 37}, {"type": "text", "text": "After separating from their family recently, Valia, 32, also moved into a Richmond house last October. ", "page_idx": 37}, {"type": "text", "text": "Read or Share this story: http://usat.ly/1NNC4zY<|endoftext|>CIVIL C. \"Marky\" Hogan has been charged with homicides with a few days remaining after the April 2 purdade high school shooting where an undercover medical examiner and two other state and Illinois police officers was using heroin to go see a therapist. ", "page_idx": 37}, {"type": "text", "text": "DICEZ TV\u2019s Zach Putler reported Tuesday that Hogan was charged with felony drug possession by the Chicago Police Department at the preliminary hearing on Monday. Putler interviewed on Monday. Authorities could offer a limit until Cook County takes Tuesday afternoon or they have to assign plea agreements. ", "page_idx": 37}, {"type": "text", "text": "Dogan said in a news conference he made during a conference call Wednesday in Chicago that he believes the people who used him as a legal tool in the killing and fired employees hired for suffering also participated. ", "page_idx": 37}, {"type": "text", "text": "He said the couple\u2019s request to an attorney Monday will let the charges finally play out. Their lawyer did not respond Wednesday. ", "page_idx": 37}, {"type": "text", "text": "Dogan would not give away to possibility that he speculated in a statement that he would escape and return unless shot. ", "page_idx": 37}, {"type": "text", "text": "Chicago police initially said the other drug charges failed to raise enough evidence to establish why the killers were charged last year, raising the possibility that the drug mix contributed to a reason for their arrest. But a new statement was made Tuesday by a man ", "page_idx": 37}, {"type": "text", "text": "who worked as the shooter in his unit on campus, and suggested the charges might be related to his work at university supervision. ", "page_idx": 38}, {"type": "text", "text": "His attorney and the university\u2019s president last week signaled that the incident of the bat gun was not a police investigation at Wednesday\u2019s conference. ", "page_idx": 38}, {"type": "text", "text": "Michael Durin, Illinois State University spokesman said he did not meet with university officials at the conference, and that university officials don\u2019t have any updates yet either. ", "page_idx": 38}, {"type": "text", "text": "\"The fact that the Defendants were charged is a major factor in why it would get this much attention,\" the university spokesperson said. \"Given that all the matters are not being resolved for months and months, any new specific information and other concerns they may be tasked with investigating now are understandable.\" ", "page_idx": 38}, {"type": "text", "text": "After city police began looking for evidence in connection to Hogan\u2019s April shooting, Durin said he had not noticed it until the Chicago Police Department found a person who was producing a bind gun on Illinois State campus. That same department found that 14 officers shot and were injured during a standoff, but it led to the launch of a combination of unrelated and related investigations leading to homicides charges in May 2015. ", "page_idx": 38}, {"type": "text", "text": "A memo from special school investigators suggests it had identified the drug fentanyl, and says the department had described the individual-oriented and inconsistent use of the gun, as well as the substance administered by CODC. ", "page_idx": 38}, {"type": "text", "text": "Dogan\u2019s allegations claim that a 2009 police gravesite package showed water running over campus and shows that the dental show photos of supposed victims were reinterpreted. ", "page_idx": 38}, {"type": "text", "text": "David Mann, a member of the Police Department, said he spoke exclusively to News 1 on condition of anonymity because university officials can\u2019t review documents immediately, and university officials had to change information that had been a consideration. ", "page_idx": 38}, {"type": "text", "text": "\"We didn\u2019t change our information until he personally told the drug overdose problem,\" he said. \"He said that drug dealing wasn\u2019t really a focal point.\" ", "page_idx": 38}, {"type": "text", "text": "He however, in a May 15 statement, also was \"proleased\" with university and state officialsin the U.S. Attorney\u2019s Office. \"We do not have any way to estimate the crime syndicate, given that the finding in the case does not preclude a separate review of standard CPD policy directed at any school employee, and whether that employee worked or went out of work,\" Mann said. ", "page_idx": 38}, {"type": "text", "text": "The lawsuit alleges that donations from a full visit with Hogan\u2019s workers and thousands of dollars, spent on gift cards, cigars, stock tickets to trips across the country, as well as other financial accounts and income from social members of former employees, were missing from buyers bought in stock. ", "page_idx": 38}, {"type": "text", "text": "Both employees and co-participants remain employed at the university. ", "page_idx": 38}, {"type": "text", "text": "On the Illinois StateUniversity Facebook page<|endoftext| $>$ Image Steve Healey, the Cook Electronics Co. Ltd., general manager of Aug. 8-Feb. 16 at a recent internal session meeting (see link) held in The Apple Building in London. This July 1939 photo shows Peggy Deaver in 1986. (Photo courtesy the Cook Electronics Co. Ltd., displayed here under G. Healey in prison dress.) $<|$ endoftext|>Steve GIRO\u2019s H-P<|endoftext|> ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "E.6.2 $\\mathbf{T=10000}$ ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Example 1 $<$ |endoftext|> has been presenting a number of temporary measures to help help resolve a crisis. ", "page_idx": 39}, {"type": "text", "text": "The last crisis was like this, but we have to resolve it at the specified level; we have to stay low. The people of Greece will be interested in the effectiveness of the measures. They will not only manage these measures, but also they will help in order to cope with the problems of the fiscal stability. ", "page_idx": 39}, {"type": "text", "text": "However, we do not want to dispose assets for the treasury. This also, so we will work on developing the national economy, and also paying on the national debt. ", "page_idx": 39}, {"type": "text", "text": "This affects the national incomes ", "page_idx": 39}, {"type": "text", "text": "So, as of 2007-2011, we use the government\u2019s temporary measures as a measure, helping resolve the crisis. In addition, we are able to pay for the borrowing costs. Additionally, we pay $\\hphantom{0}\\leq4\\,4\\,0$ billion to settle debt issues, which can never be settled by default in a country. ", "page_idx": 39}, {"type": "text", "text": "These temporary measures will be aimed on several fronts, because the government will have three different partners in the system, in my right. ", "page_idx": 39}, {"type": "text", "text": "Firstly, we will be allowed to borrow a lot more upon the addition of the emergency measures and these temporary measures will help provide for the repayment of our debts before we are forced into a crisis as a result of our borrowing bills. Secondly, in the case of this, we will have resort to temporary measures in the revenue budget for Greece. The budget costs the government another $\\mathbb{S}\\,5\\,.\\,2$ billion a year. ", "page_idx": 39}, {"type": "text", "text": "So what you propose - if it happens again, does this mean that, since 2010, will you resolve the deficits which will occur on the basis of what we already have? ", "page_idx": 39}, {"type": "text", "text": "The fiscal situation ", "page_idx": 39}, {"type": "text", "text": "We would be able to settle our debts by the end of June which is the end. That said, we are taking our part as one of the most important countries in Europe, not only to make a proper transfer of the money but also to rely on it in the economy. However, first of all, we cannot achieve this on a day-to-day basis. ", "page_idx": 39}, {"type": "text", "text": "It is still true that we have decided to be able to deal with the economic situation of the country, but there might be another change in the fiscal situation, and therefore, we will try to negotiate on the situation at the end of June and over the summer. ", "page_idx": 39}, {"type": "text", "text": "The changes in the fiscal situation would be up to the parliament of management, bureaucrats, judges and a legitimate parliament of Greece. ", "page_idx": 39}, {"type": "text", "text": "Is the government planning to talk about thethe \u2019temporary measures\u2019 of Greece? ", "page_idx": 39}, {"type": "text", "text": "We will continue the process to operate through the temporary measures. This is not a temporary measure at this point, because after a crisis, not thereyet at crisis level, you can still have enough investment purchases until the end of the month. ", "page_idx": 39}, {"type": "text", "text": "Again the government decided to create a temporary measure and now it depends upon a particular event, such as that there\u2019s another liquidity crunch. It is better that the government and the authorities decided to create a temporary measure effective in June at fair sum monthly bond rates. ", "page_idx": 40}, {"type": "text", "text": "The temporary measures will also enhance the government\u2019s economic status, especially when following the measures at the end of the month. ", "page_idx": 40}, {"type": "text", "text": "Temporary measures is a real tool for growth, not just for the economy. ", "page_idx": 40}, {"type": "text", "text": "Knowing that there are several measures in place to increase our supply, for example, the level offor profit on public sector enterprises is certain, under all of these temporary measures the increases in output, after that, will increase the external demand and the internal demand. ", "page_idx": 40}, {"type": "text", "text": "We will be able to create the demand, and also strengthen the government\u2019s credibility through fiscal organization. What is important here, here is that we will apply these measures to our reserves, and at the same time, we apply these measures to the debt level, which will also be the aim of debt-free Greece. ", "page_idx": 40}, {"type": "text", "text": "So, first of all, everything is certain ofwhat continues to be collected by the government. Given the situation and after the release of the last data on October 16, you also recognize that this will not be any kind of non-payment. ", "page_idx": 40}, {"type": "text", "text": "In the case of the payment against the equipment, we will be able to manage with the measures. ", "page_idx": 40}, {"type": "text", "text": "What does government expect in its plans to create a fiscal consolidation for the public sector and the new budget. ", "page_idx": 40}, {"type": "text", "text": "Regarding this is the temporary measure, we will be able to cope with the troubled finances. However, I do not think it is any measure which threatens the fiscal stability of the economy. However, that is not a temporary measure, a permanent measure. ", "page_idx": 40}, {"type": "text", "text": "On the other hand, there will be our ongoing work on construction in the ministry. If this falls, we will continue work on job creation, the expansion of the economy. ", "page_idx": 40}, {"type": "text", "text": "Also, also the government mentioned the new government reforms, which increased labor hours for the employees, which will further the economic growth, and the second aspect of budget as well and this is government welfare, which will improve the quality of life. We will<|endoftext|> ", "page_idx": 40}, {"type": "text", "text": "Example 2 $<|$ endoftext|> him. He said: \u201cWhat are you doing?\u201d ", "page_idx": 40}, {"type": "text", "text": "I hesitated before answering. $\\mathrm{}^{\\mathrm{u}}\\mathrm{BO}\\mathrm{y}$ , this is so exciting. You need a better girl. Is she?\u201d ", "page_idx": 40}, {"type": "text", "text": "And I said, \u201cYou don\u2019t have a brain. You have no brain anymore.\u201d ", "page_idx": 40}, {"type": "text", "text": "After a minute, he had walked back and said on his own, working through that, he thought he had got himself going in a new direction. ", "page_idx": 40}, {"type": "text", "text": "He could\u2019ve been a better boy in the first three years. ", "page_idx": 40}, {"type": "text", "text": "\u201cYou\u2019ll only have once before it starts.\u201d ", "page_idx": 40}, {"type": "text", "text": "MVP ", "page_idx": 40}, {"type": "text", "text": "The story is always, \u201cThat\u2019s what the other guy has to do.\u201d He was the guy who had to do anything. He had to reason with school officials. My cousin mentioned to me that some of my friends almost doubled over at one meeting. ", "page_idx": 41}, {"type": "text", "text": "I\u2019d picked up a lot of the money I owed him from high people in me; he liked my grades. Drop-outs didn\u2019t consider me high enough to let me go hang out. He hung up when I challenged him after practice to show a new talk. He started making, and, quite, never ", "page_idx": 41}, {"type": "text", "text": "I first saw him C. morning, in the sixth grade class. He wouldn\u2019t hang up with him on point at team meetings. He started talking about things about me: ${\\mathfrak{n}}_{\\mathrm{~T~}},{\\mathfrak{m}}$ an M, I\u2019ll get an A. Tonight.\u201d Having had that conversation over lunch, my heart touched mine with pride. He came, my boy. Now he looks like he\u2019s going back to school. I don\u2019t know if he\u2019s going to sue. Let\u2019s just have a two-bedroom apartment, a $\\lesssim5\\,0\\,0-$ dollar condo for renting, and a pool. And then he was back. ", "page_idx": 41}, {"type": "text", "text": "That was a part of my life as I think about it. It was the school year. ", "page_idx": 41}, {"type": "text", "text": "I never saw a guy come up at the locker room and show a new talk. That day one day, I told the high school, \u201cWe\u2019ll show up one day right here, we can have a little fun,\u201d and after this, I remember a small handful of the boys made friends, and they never, ever showed up for a new talk. ", "page_idx": 41}, {"type": "text", "text": "I call them \u201cM\u2019s kids. I always remember him, I remember his ass up his ass, getting ready for a freshman orientation out there. He\u2019ll show everything. ", "page_idx": 41}, {"type": "text", "text": "$\\mathrm{He}^{\\prime}\\perp\\perp$ show if $\\mathtt{I}^{\\prime}\\,\\mathrm{m}$ freshman, I\u2019ll act I was going to play junior. In a few years $\\mathbb{T}^{\\prime}\\perp\\!\\!\\!\\perp$ try it, then he\u2019ll make sure he\u2019s going to judge me. He will come over to me one day. ", "page_idx": 41}, {"type": "text", "text": "Then one day, the senior class was sitting on the bench, pressing his ball on the floor of the locker room, the referee was just standing the knelt it down. ", "page_idx": 41}, {"type": "text", "text": "And when he heard about that, another boy, three of his friends, and one of his cousins were on the other side of the room. The boys\u2019 class was filling in with his new brother and his new cousin and his new M.M\u2019s player. ", "page_idx": 41}, {"type": "text", "text": "The senior class watched me walk me through the chair to the bench.   \nEveryone passed by the boy. Just on his toes on a foot, too. ", "page_idx": 41}, {"type": "text", "text": "He [and a girl] passed over his head and, as I looked at them, he carried me into the locker room. And the biggest part of the story, was a mistake. ", "page_idx": 41}, {"type": "text", "text": "With his elbows out, he pulled me down on my shoe, my other, sort of a- don\u2019t know what they were; palebelly somethings, like bleeding very much, or on little toes. He was up on the stairs and everybody watches, with men and high school kids, who saw him in the locker room. And he caught a breath. Then his old man approached me and, disappeared into the middle of the room. He took off his vest, fast enough as to herd him into the locker room. I walked into the room and read him little cards with my own eyes to make notes, to pull him under my shoes. ", "page_idx": 41}, {"type": "text", "text": "I told them: \u201cListen, because I say this today, when you talk to \u2019em today, \u201cJust make sure you talk is more than you\u2019ll show. He\u2019s just listening to me, and he\u2019s telling me I\u2019m going to be there for him.\u201d ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "I\u2019m always the one who wants to do something important about you than I show up. I walk around and ask, I want a message from you, \u201cKeep it going. It<|endoftext|> ", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 43}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 43}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 43}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 43}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 43}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the   \npaper\u2019s contributions and scope?   \nAnswer: [Yes]   \nJustification: Claims are addressed ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our method under-performs compared to autoregressive models. We also discuss other limitations in the paper. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: They are in the proofs. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide all hyperparameters necesessary to reproduce the experiments and will provide code. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: We will release all code after the paper is accepted. The datasets are already public. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We provide detailed hyperparameters for all experiments. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Many of our tabels include error bars and standard deviations Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 45}, {"type": "text", "text": "Justification: We conduct all experiments on 8x 3090s, 8xA6000s, 8xA100s, or 8xH100s. The largest models on OpenWebText take 2 weeks to train on 8xA100, the LM1B models only take 2 days to train on the same hardware ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We follow standard practices Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our model will allow for more controllable text generation models, and do not increase the capability of current autoregressive models ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: These models are trained on trivial datasets and unlikely to cause any harm compared to state of the art language models. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All assets are publically available and we respect the licenses for all the data. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 46}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide no new assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}, {"type": "text", "text": "End of NEURIPS CHECKLIST. Must be at end of document after appendix ", "page_idx": 48}]