[{"figure_path": "ieYdf9TZ2u/tables/tables_25_1.jpg", "caption": "Table 1: Comparison of Next-DiT with DeiT [79] on ImageNet classification.", "description": "This table compares the performance of Next-DiT and DeiT-base on ImageNet classification.  It shows the number of parameters, training epochs, resolution used, and the Top-1 accuracy achieved by each model.  Two sets of results are presented: one for a fixed resolution of 224x224, and another using flexible resolution. The flexible resolution results demonstrate Next-DiT's ability to handle various image sizes effectively.", "section": "D.3 Experiments"}, {"figure_path": "ieYdf9TZ2u/tables/tables_26_1.jpg", "caption": "Table 2: The detailed training setting of MV-Next-DiT.", "description": "This table details the training settings used for the MV-Next-DiT model.  It breaks down the training process into three stages, each with varying image resolutions and the number of views (N).  For each stage, it provides information on the pre-training model used, the total number of image batches processed, the learning rate, the number of training iterations, and the computational resources (A100 GPUs and hours) required. This allows readers to understand the computational cost and the progression of the training pipeline used to develop the multi-view image generation model.", "section": "E.1 Image- and Text-conditional Multi-View Generation"}, {"figure_path": "ieYdf9TZ2u/tables/tables_26_2.jpg", "caption": "Table 3: Comparison of capabilities between MV-Next-DiT and other mutli-view methods.", "description": "This table compares the capabilities of MV-Next-DiT with other existing multi-view generation methods.  It shows the base model used, the resolution of the generated images, the type of conditioning (text or image), and the number of inference views each method can generate.", "section": "E.1 Image- and Text-conditional Multi-View Generation"}, {"figure_path": "ieYdf9TZ2u/tables/tables_28_1.jpg", "caption": "Table 4: The comparison with baseline models on the MusicCaps Evaluation set. We borrow the results of Mousai, Melody, and MusicLM from MusicGen [21].", "description": "This table presents a comparison of the proposed text-to-music generation model's performance against several baseline models on the MusicCaps Evaluation dataset.  The objective metrics used for comparison are the FAD (Fr\u00e9chet Audio Distance) and KL (Kullback-Leibler) divergence, which measure the difference between generated audio and ground truth audio. Lower values indicate better performance. Subjective metrics include MOS-Q (Mean Opinion Score for Quality) and MOS-F (Mean Opinion Score for Faithfulness), assessing the perceived audio quality and the alignment between the generated audio and its text prompt; higher values are preferred.  Note that the results for Mousai, Melody, and MusicLM were taken from the MusicGen paper.", "section": "Experiments of Text-to-Music Generation"}, {"figure_path": "ieYdf9TZ2u/tables/tables_29_1.jpg", "caption": "Table 6: Ablation studies. We use DiT to denote the same model architecture but with DDPM formulation.", "description": "This table presents the results of ablation studies conducted on the Lumina-Next model.  It compares the performance of the Next-DiT model (with the proposed architecture) against variations:  removing the dual-encoder,  using only the Audioset dataset for training, and using a different model architecture (DiT using DDPM formulation). The metrics used for comparison are FAD (Fr\u00e9chet Audio Distance), KL (Kullback-Leibler divergence), and CLAP (CLIP score). Lower FAD and KL values indicate better audio generation quality, while a higher CLAP score implies better alignment between generated audio and text captions.", "section": "E.2 Text-Conditional Audio and Music Generation"}, {"figure_path": "ieYdf9TZ2u/tables/tables_30_1.jpg", "caption": "Table 6: Ablation studies. We use DiT to denote the same model architecture but with DDPM formulation.", "description": "This table presents the ablation study results for the Lumina-Next model.  It compares the performance of the Next-DiT model against variations where components are removed or altered:  removing the dual encoder, training only with Audioset, and using the DDPM formulation instead of the flow matching method. The results are evaluated using FAD, KL, and CLAP metrics, showing the impact of the different components on the model's overall performance.", "section": "E.2 Text-Conditional Audio and Music Generation"}, {"figure_path": "ieYdf9TZ2u/tables/tables_30_2.jpg", "caption": "Table 7: Quantitative results of point cloud generation. We multiplied the value of CD by 10<sup>3</sup>.", "description": "This table presents a quantitative comparison of different point cloud generation models.  The models are evaluated using three metrics: Minimum Matching Distance (MMD), Coverage (COV), and Chamfer Distance (CD). Lower MMD values indicate better performance. Higher COV values represent a higher proportion of correctly generated points.  The table compares the performance of the proposed model ('Ours') against several existing models (PC-GAN, TreeGAN, PointFlow, ShapeGF, and PDiffusion) for two different shapes: Airplane and Chair.", "section": "E.3 Label- and Text-Conditional Point Cloud Generation"}]