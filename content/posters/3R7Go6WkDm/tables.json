[{"figure_path": "3R7Go6WkDm/tables/tables_7_1.jpg", "caption": "Table 1: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms. Better values are in bold. Except some clean cases, post-hoc selection is always better, often more than doubling the improvement over no transform. See Tabs. 6 and 8 in App. E for standard deviations.", "description": "This table compares the performance of naive model selection versus post-hoc model selection using two different post-hoc transformations (SWA+TS and SWA+Ens+TS).  Naive selection chooses the best performing model based on its initial performance without any post-hoc transformations.  Post-hoc selection instead selects the model based on its performance after applying the chosen post-hoc transformations.  Better values (lower test loss and test error) are bolded. The table demonstrates that post-hoc selection consistently outperforms naive selection, often leading to more than double the improvement over not using any transformations.  Standard deviations for the results are available in Appendix E.", "section": "Post-Hoc Selection: Leveraging Post-Hoc Reversal in Practice"}, {"figure_path": "3R7Go6WkDm/tables/tables_18_1.jpg", "caption": "Table 5: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms on some real-world datasets. Better values are in bold.", "description": "This table compares the performance of naive model selection (selecting the best performing model based on the base metric) versus post-hoc selection (selecting the best-performing model based on the post-hoc metric after applying SWA+TS or SWA+Ens+TS transforms) on several real-world datasets.  The results show that post-hoc selection often leads to better results, especially on datasets with noise in the labels.", "section": "4.2.3 Hyperparameter-Wise Post-Hoc Reversal"}, {"figure_path": "3R7Go6WkDm/tables/tables_18_2.jpg", "caption": "Table 3: Training Details.", "description": "This table details the training hyperparameters used for each dataset in the paper.  It shows the model architecture, whether or not pre-training was used, the optimizer, learning rate, weight decay, learning rate schedule, number of epochs, and batch size.  The datasets include image datasets (CIFAR-10/100-N, FMOW), text datasets (Yelp, Guanaco), tabular data (Folktables), and graph datasets (Collab, Reddit). Each dataset is trained with a specific set of hyperparameters optimized for that dataset.", "section": "C Dataset and Training Details"}, {"figure_path": "3R7Go6WkDm/tables/tables_18_3.jpg", "caption": "Table 4: Noise levels for CIFAR-N (%), reproduced from [74].", "description": "This table shows the approximate percentage of noisy labels in different label sets of the CIFAR-N dataset. The noise is introduced by human annotators, resulting in various levels of label noise across different sets. The sets include 'Clean', 'Aggre' (aggregate of Rand1, Rand2, Rand3), 'Rand1', 'Rand2', 'Rand3', and 'Worst'.  The percentages represent the approximate noise level in each set, ranging from 0% (Clean) to 40% (Worst). This table is used in the paper to illustrate and define different levels of noise present in the datasets used for the experiments.", "section": "C Dataset and Training Details"}, {"figure_path": "3R7Go6WkDm/tables/tables_19_1.jpg", "caption": "Table 5: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms on some real-world datasets. Better values are in bold.", "description": "This table compares the performance of naive model selection and post-hoc selection methods using two different post-hoc transforms (SWA+TS and SWA+Ens+TS) across several real-world datasets.  The naive approach selects models based on their performance before applying the post-hoc transforms, while the post-hoc approach selects models based on their performance after applying the transforms.  Better results (lower test loss and error) are shown in bold, highlighting the effectiveness of post-hoc selection, particularly in improving generalization performance.", "section": "Post-Hoc Selection Results for Remaining Datasets"}, {"figure_path": "3R7Go6WkDm/tables/tables_20_1.jpg", "caption": "Table 6: Detailed results for CIFAR-N datasets. Base denotes no transform and Final denotes the SWA+Ens+TS transform. Gain shows performance improvement. \u0394 shows change from naive selection to post-hoc selection. Since Base and Gain columns involve 8 individual runs, we report mean\u00b1std. dev. of the metric. C-10-N, C-100-N-C and C-100-N-F are shorthands for CIFAR-10-N, CIFAR-100-N Coarse and CIFAR-100-N Fine respectively.", "description": "This table presents a detailed breakdown of the experimental results on CIFAR-N datasets.  It compares the performance of models with no transformation (Base), with SWA+TS and SWA+Ens+TS transformations (Final), and highlights the improvement achieved by post-hoc selection compared to the naive selection method.  The table includes metrics such as test loss and test error, with mean and standard deviation across multiple runs, showing results for various noise levels and dataset variations (Clean, Aggre, Rand1, Rand2, Rand3, Worst).", "section": "4.2 Experiments"}, {"figure_path": "3R7Go6WkDm/tables/tables_21_1.jpg", "caption": "Table 7: Detailed results for LLM instruction tuning. Better values are in bold. Since Base and Gain columns involve 8 individual runs, we report mean\u00b1std. dev. of the metric.", "description": "This table presents the detailed results for the LLM instruction tuning experiments. It compares the performance of different methods (None, SWA+TS, SWA+Ens+TS) using various metrics (Perplexity, Error, MMLU). The results are divided into those obtained using naive selection and post-hoc selection. For better readability, the values that indicate better performance are highlighted in bold. Because the base and gain calculations involve 8 separate runs, mean and standard deviation values are presented for these metrics.", "section": "7.1 LLM Instruction Tuning"}, {"figure_path": "3R7Go6WkDm/tables/tables_21_2.jpg", "caption": "Table 1: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms. Better values are in bold. Except some clean cases, post-hoc selection is always better, often more than doubling the improvement over no transform. See Tabs. 6 and 8 in App. E for standard deviations.", "description": "This table compares the performance of naive model selection (selecting models based on their base performance before applying post-hoc transformations) against post-hoc selection (selecting models based on their performance after applying post-hoc transformations like SWA+TS and SWA+Ens+TS).  The results show that post-hoc selection significantly improves performance in most noisy datasets, often more than doubling the improvement compared to not using any transformations.  Standard deviations are available in appendix tables 6 and 8.", "section": "Post-Hoc Selection: Leveraging Post-Hoc Reversal in Practice"}, {"figure_path": "3R7Go6WkDm/tables/tables_23_1.jpg", "caption": "Table 1: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms. Better values are in bold. Except some clean cases, post-hoc selection is always better, often more than doubling the improvement over no transform. See Tabs. 6 and 8 in App. E for standard deviations.", "description": "This table compares the performance of naive model selection (selecting models based on the base metrics before applying post-hoc transforms) against the proposed post-hoc selection method (selecting models based on metrics after applying post-hoc transforms).  It shows test loss and test error for various datasets and transform combinations (SWA+TS and SWA+Ens+TS).  The results demonstrate that post-hoc selection generally yields better results, often significantly improving upon both naive selection and the performance of models without post-hoc transformations. Standard deviations are available in supplementary tables.", "section": "Post-Hoc Selection: Leveraging Post-Hoc Reversal in Practice"}, {"figure_path": "3R7Go6WkDm/tables/tables_23_2.jpg", "caption": "Table 9: Naive vs post-hoc (ours) selection for CIFAR-N trained with cross-entropy (CE) loss. Better values are in bold.", "description": "This table compares the performance of naive and post-hoc model selection methods on CIFAR-N datasets using cross-entropy loss.  It shows test loss and test error for different noise levels (Clean, Aggre, Rand1, Rand2, Rand3, Worst) and for different transforms (None, SWA+TS, SWA+Ens+TS). Bold values highlight the better performance achieved with post-hoc selection.", "section": "H Noise-Aware Training"}, {"figure_path": "3R7Go6WkDm/tables/tables_23_3.jpg", "caption": "Table 1: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms. Better values are in bold. Except some clean cases, post-hoc selection is always better, often more than doubling the improvement over no transform. See Tabs. 6 and 8 in App. E for standard deviations.", "description": "This table compares the performance of naive model selection (selecting the best-performing model before applying post-hoc transformations) against the proposed post-hoc selection method (selecting models based on post-hoc metrics).  The results demonstrate post-hoc selection's effectiveness across various datasets and transformations (SWA+TS and SWA+Ens+TS).  Post-hoc selection consistently outperforms naive selection, often resulting in more than double the improvement compared to using no post-hoc transformations.  Standard deviations for all values are available in Appendix E.", "section": "Post-Hoc Selection: Leveraging Post-Hoc Reversal in Practice"}]