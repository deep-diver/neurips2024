[{"type": "text", "text": "Post-Hoc Reversal: Are We Selecting Models Prematurely? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rishabh Ranjan1,\u2217 Saurabh Garg2, Mrigank Raman2, Carlos Guestrin1,3, Zachary Lipton2 1Stanford University, 2Carnegie Mellon University, 3Chan Zuckerberg Biohub {ranjanr,guestrin}@stanford.edu, {sgarg2,mrigankr,zlipton}@cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Trained models are often composed with post-hoc transforms such as temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to improve performance, robustness, uncertainty estimation, etc. However, such transforms are typically applied only after the base models have already been finalized by standard means. In this paper, we challenge this practice with an extensive empirical study. In particular, we demonstrate a phenomenon that we call post-hoc reversal, where performance trends are reversed after applying post-hoc transforms. This phenomenon is especially prominent in high-noise settings. For example, while base models overfit badly early in training, both ensembling and SWA favor base models trained for more epochs. Post-hoc reversal can also prevent the appearance of double descent and mitigate mismatches between test loss and test error seen in base models. Preliminary analyses suggest that these transforms induce reversal by suppressing the influence of mislabeled examples, exploiting differences in their learning dynamics from those of clean examples. Based on our findings, we propose post-hoc selection, a simple technique whereby post-hoc metrics inform model development decisions such as early stopping, checkpointing, and broader hyperparameter choices. Our experiments span real-world vision, language, tabular and graph datasets. On an LLM instruction tuning dataset, post-hoc selection results in $>1.5\\times$ MMLU improvement compared to naive selection.2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many widely used techniques in deep learning operate on trained models; we refer to these as post-hoc transforms. Examples include temperature scaling (TS) [19], stochastic weight averaging (SWA) [28] and ensembling [39]. These techniques have shown promise for improving predictive performance, robustness, uncertainty estimation, out-of-distribution generalization, and few-shot performance [4, 6, 39, 56, 84]. Typically, the pre-training and post-hoc stages are isolated. The workflow is: (1) pick model architecture, training recipe, hyperparameters, etc. to optimize for individual model performance; (2) train one or more models; (3) pick best-performing checkpoints; (4) apply post-hoc transforms. We refer to this procedure as naive selection. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we demonstrate interesting drawbacks of naive selection. In a large-scale empirical study, we uncover post-hoc reversal\u2014a phenomenon whereby post-hoc transforms reverse performance trends between models (Fig. 1). We demonstrate post-hoc reversal with respect to training epochs, model sizes, and other hyperparameters like learning rate schedules. We further establish that posthoc reversal is a robust phenomenon by experimenting on real-world datasets across domains and modalities, with diverse model classes and training setups. ", "page_idx": 0}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/13c19531a672b611a5e66865f325eaffc2b4cce6535461691d8604f3ef22a753.jpg", "img_caption": ["Figure 1: An illustration of the phenomenon of post-hoc reversal on the FMoW dataset: base performance at epoch $t_{2}$ is worse than at epoch $t_{1}$ $(b_{2}>b_{1})$ ), but post-hoc performance is better $(p_{2}<p_{1})$ . The current practice of naive selection considers base metrics to pick models at epoch $t_{1}$ . Our proposed technique of post-hoc selection instead uses post-hoc metrics to pick models at epoch $t_{2}$ , resulting in $>2\\times$ improvement over naive selection in both test loss and error. $\\mathrm{SWA+Ens+TS}$ refers to the post-hoc transform obtained by composing SWA, ensemble (Ens) and temperature scaling (TS). Base curves show mean of 8 runs, models from which constitute the ensembles. Individual runs are shown in lighter colors. See Fig. 5 for more detailed curves on this dataset. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Post-hoc reversal is most prominent on noisy datasets (Fig. 2). Other phenomena exacerbated by noise include catastrophic overftiting [50],double descent [55], and loss-error mismatch [19]. While these phenomena pose challenges to model development, post-hoc reversal suggests a path to alleviate them. Noise can arise not only from labeling errors, but also from inherent uncertainty in the prediction task, such as in next token prediction [60]. Indeed, severe performance degradation has limited multi-epoch training of large language models (LLMs) [81]. Here too, post-hoc reversal reveals a promising path for sustained performance improvements over longer training. ", "page_idx": 1}, {"type": "text", "text": "The core intuition for post-hoc reversal is that models continue to learn generalizable patterns from clean examples, even when spurious patterns learnt from mislabeled examples worsen the overall performance. Post-hoc transforms exploit differences in the learning dynamics of clean and mislabeled examples [42] to reinforce the influence of the former, while suppressing that of the latter. When strong enough, this effect leads to reversal. We show evidence for these intuitions in $\\S\\ S$ . ", "page_idx": 1}, {"type": "text", "text": "Based on our findings, we propose post-hoc selection\u2014a simple technique whereby base models are selected based on post-transform performance. The technique is practical as the transforms of interest can be cheaply incorporated into the validation phase of the training loop. Post-hoc selection significantly improves the performance of the transformed models, with $>2\\times$ improvements over naive selection in some cases (Fig. 2). In terms of absolute performance, post-hoc selection leads to $>3$ -point reduction in test error over naive selection on a satellite imaging dataset (Fig. 1). The reduction is even higher $>5$ points) when using out-of-distribution (OOD) val/test splits for the same dataset. On an LLM instruction tuning dataset, under our procedure a composed transform of SWA, ensemble and TS gives $>1.5\\times$ MMLU improvement over a naive application of the same transform on prematurely selected models. ", "page_idx": 1}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/a8d0df2a79b28d56aeda766c3c31ea1d29f589827b94d06aa5e90be474df4c04.jpg", "img_caption": ["Figure 2: A comparison of naive and post-hoc selection on label sets from CIFAR-10/100-N (abbr. C-10/100-N) for the $\\mathrm{SWA+TS}$ transform. On noisy label sets, post-hoc selection is often $>2\\times$ better. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A slew of empirical works [10, 17, 31, 55, 57, 58] have revealed both challenges and opportunities for improving the understanding and practice of deep learning. Our work expands this list with a novel phenomenon tying together noisy data learning and post-hoc transforms. Orthogonal to our work, a number of training-stage strategies for noisy data have been proposed (see [69] for a survey). ", "page_idx": 2}, {"type": "text", "text": "TS belongs to a family of calibration techniques [2, 19] proposed with the goal of producing wellcalibrated probabilities. Ensembling is a foundational technique in machine learning, with simple variants routinely used in deep learning [3, 39]. SWA [28] is the culmination of a line of work [18, 25] seeking to cheaply approximate ensembling. Despite their prevalence, a thorough understanding of best practices for wielding these techniques is lacking, especially in the context of noisy data. Our work fills this gap. For a more detailed discussion on related work, see App. A. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries and Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We describe our learning setup in $\\S\\,3.1$ , with emphasis on noisy data, a key focus of this work. In $\\S\\ 3.2$ , we introduce the post-hoc transforms we study. ", "page_idx": 2}, {"type": "text", "text": "3.1 Learning on Noisy Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Setup. We consider multi-class classification with $C$ classes, input $\\textbf{x}\\in\\:\\mathcal{X}$ and label $y\\,\\in\\,\\mathcal{V}\\,=$ $\\{1,\\ldots,C\\}$ . Training, validation and test sets are drawn i.i.d. from the data distribution $\\mathcal{D}$ . A classifier $\\bar{f}\\colon\\Theta\\times\\bar{\\boldsymbol{x}^{\\prime}}\\bar{\\rightarrow\\mathbb{R}^{C}}$ outputs the logit vector $\\mathbf{z}=f(\\mathbf{x};\\boldsymbol{\\theta})$ , given parameter vector $\\uptheta\\,\\in\\,\\Theta$ . Predicted probability of class $k$ is $\\mathbb{P}_{f}[y=\\bar{k^{\\prime}}|\\mathbf{\\epsilonx}]=\\sigma(\\mathbf{z})_{k}$ , where $\\sigma$ is the softmax function. ", "page_idx": 2}, {"type": "text", "text": "Noise. Data $\\mathcal{D}$ is said to be clean if $\\mathbb{P}_{\\mathcal{D}}[y\\mid\\mathbf{x}]$ is one-hot for all $\\mathbf{x}$ , i.e., $\\mathbb{P}_{\\mathcal{D}}[y\\mid\\mathbf{x}]=\\mathbf{1}\\{y=y^{*}(\\mathbf{x})\\}$ for some labeling function $y^{*}\\colon\\mathcal{X}\\rightarrow\\mathcal{Y}$ . Then, for any example input $\\mathbf{x}^{(i)}$ in the dataset, the observed label is $\\boldsymbol{y}^{(i)}=\\boldsymbol{y}^{*}(\\mathbf{x}^{(i)})$ . When $\\mathbb{P}_{\\mathcal{D}}[y\\mid\\mathbf{x}]$ is not one-hot, $\\mathcal{D}$ is said to be noisy and the observed label is only a stochastic sample $\\boldsymbol{y}^{(i)}\\sim\\mathbb{P}_{\\mathcal{D}}[\\boldsymbol{y}\\mid\\mathbf{x}=\\mathbf{x}^{(i)}]$ from the underlying conditional distribution. Noise can arise due to (1) non-determinism in the prediction target (2) insufficient information in the input context, and (3) annotation errors. See App. B.1 for illustrated examples. ", "page_idx": 2}, {"type": "text", "text": "Metrics. A metric $\\mathcal{M}\\colon\\mathbb{R}^{C}\\,\\times\\,\\mathcal{y}\\,\\rightarrow\\,\\mathbb{R}$ compares the predicted logits ${\\bf z}$ with the observed label $y$ . $\\mathcal{M}_{f}(\\mathbf{0})=\\mathcal{M}[f(\\mathbf{\\cdot}\\,;\\mathbf{0})]\\,=\\,\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[\\mathcal{M}(f(\\mathbf{x};\\mathbf{0}),y)]$ denotes the metric computed over $\\mathcal{D}$ given $f$ and $\\uptheta$ . We use two metrics (1) classification error, or simply error, with $\\mathcal{M}^{\\mathrm{error}}(\\mathbf{z},y)=$ $\\mathbf{1}\\{\\mathrm{arg\\,max}_{k}\\,\\mathbf{z}_{k}\\neq y\\}$ and (2) cross-entropy loss, or simply loss, with $\\mathcal{M}^{\\mathrm{loss}}(\\mathbf{z},y)\\,=\\,-\\log\\sigma(\\mathbf{z})_{y}$ . The exponentiated loss, also called perplexity, is common in language modeling, where it is computed on a per-token basis. A standard result states that loss is minimized if and only if the ground truth conditional probability is recovered [20]. See App. B.1 for additional background. ", "page_idx": 2}, {"type": "text", "text": "3.2 Post-Hoc Transforms in Machine Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 1 (Post-Hoc Transform) A post-hoc transform $\\tau$ maps a classifier $f\\colon\\Theta\\times\\mathcal{X}\\rightarrow\\mathcal{Y}\\:t o$ another classifier $\\mathcal{T}\\circ f\\colon\\Theta^{K}\\times\\mathcal{X}\\rightarrow\\dot{\\mathcal{Y}},$ for some $K$ . ", "page_idx": 2}, {"type": "text", "text": "Temperature Scaling (TS). TS [19] involves scaling the logits with a temperature $\\tau\\in\\mathbb{R}$ obtained by optimizing the cross-entropy loss over the validation set, with model parameters fixed (Eqn. 1). Temperature scaling preserves error as it does not affect the predicted class. We use the torchcal [63] implementation, which optimizes the temperature on GPU with Newton\u2019s method [15]. ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{\\mathrm{TS}}\\circ f)(\\mathbf{x};\\theta)=\\frac{1}{\\tau}f(\\mathbf{x};\\theta),\\;\\mathrm{with}\\;\\tau=\\arg\\operatorname*{min}_{\\tau}\\mathcal{M}_{\\mathrm{val}}^{\\mathrm{loss}}\\left[\\frac{1}{\\tau}f(\\mathbf{\\theta}\\cdot;\\theta)\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Ensembling. In this method, predictions from an ensemble of classifiers are combined. In deep learning, simply averaging the temperature-scaled logits is effective (Eqn. 2). $\\theta_{1},\\ldots,\\theta_{K}$ are obtained from multiple training runs with the same architecture and dataset, with stochasticity from mini-batch sampling and random initialization, if applicable. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left(\\mathcal{T}_{\\mathrm{Ens}}\\circ f\\right)\\left(\\mathbf{x};\\boldsymbol{\\Theta}_{1},\\ldots,\\boldsymbol{\\Theta}_{K}\\right)=\\frac{1}{K}\\sum_{k=1}^{K}\\frac{1}{\\tau_{k}}f(\\mathbf{x};\\boldsymbol{\\Theta}_{k}),\\ \\mathrm{with}\\ \\tau_{k}=\\arg\\operatorname*{min}_{\\tau}\\mathcal{M}_{\\mathrm{val}}^{\\mathrm{loss}}\\left[\\frac{1}{\\tau}f(\\cdot;\\boldsymbol{\\Theta}_{k})\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Stochastic Weight Averaging (SWA). SWA [28] involves averaging weights $\\theta_{1},\\ldots,\\theta_{K}$ from the same training run (Eqn. 3). BatchNorm statistics are recomputed after averaging, if required. We pick checkpoints at epoch boundaries. Unlike Izmailov et al. [28], we do not skip the initial epochs (warmup) or modify the learning rate schedule3. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(\\mathcal T_{\\mathrm{SWA}}\\circ f\\right)\\left(\\mathbf{x};\\theta_{1},\\ldots,\\theta_{K}\\right)=f\\left(\\mathbf{x};\\frac{1}{K}\\sum_{i=1}^{K}\\theta_{i}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Compositions. TS, ensembling and SWA can be readily composed. In particular, we consider $S W\\!A\\!+\\!T S$ and $S W\\!A\\!+\\!E n s\\!+\\!T S$ , for single- and multi-model settings respectively. We denote them with $\\mathcal{T}_{\\mathrm{S+T}}=\\mathcal{T}_{\\mathrm{TS}}\\circ\\mathcal{T}_{\\mathrm{SWA}}$ and $\\mathcal{T}_{\\mathrm{S+E+T}}=\\mathcal{T}_{\\mathrm{TS}}\\circ\\mathcal{T}_{\\mathrm{Ens}}\\circ\\mathcal{T}_{\\mathrm{SWA}}$ (explicit forms in App. B.2). ", "page_idx": 3}, {"type": "text", "text": "4 Post-Hoc Reversal: Formalization and Empirical Study ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To use post-hoc transforms, one must first select models to apply them to. Current practice is to select the best-performing model independent of post-hoc transforms, rationalized by an implicit monotonicity assumption \u2013 \u201cbetter-performing models result in better performance after transformation\u201d. As we shall see, this assumption is often violated in practice. We call such violations post-hoc reversal. In $\\S\\,4.1$ , we formalize post-hoc reversal and discuss ways to detect it. In $\\S\\,^{4.2}$ , we empirically study various kinds of post-hoc reversal with special practical relevance. ", "page_idx": 3}, {"type": "text", "text": "4.1 Definitions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "First, we give a general definition of post-hoc reversal (Def. 2). If Def. 2 holds with $\\varphi_{k}$ \u2019s which are optimal for the base metric $\\mathcal{M}_{f}$ , then naive selection becomes suboptimal as it picks $\\varphi_{k}$ \u2019s, but $\\uptheta_{k}$ \u2019s are better under the post-hoc metric $\\mathcal{M}_{\\mathcal{T}\\circ f}$ . Since the entire space of parameter tuples $\\Theta^{K}$ can be large, we study post-hoc reversal restricted to indexed parameters (Def. 3). Indices can be, for example, training epochs $(\\S\\,4.2.1)$ , model sizes $(\\S\\ 4.2.2)$ or hyperparameter configurations $(\\S\\ 4.2.3)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Post-hoc reversal) Let a post-hoc transform $\\tau$ map a classifier $f\\colon\\Theta\\times\\mathcal{X}\\rightarrow\\mathcal{Y}\\,t$ o $\\mathcal{T}\\circ f\\colon\\Theta^{K}\\times\\mathcal{X}\\rightarrow\\mathcal{V}$ . $\\tau$ applied to $f$ exhibits post-hoc reversal for a metric $\\mathcal{M}$ if there exist $\\left(\\Theta_{1},...,\\Theta_{K}\\right)$ , $\\left(\\pmb{\\varphi}_{1},\\ldots,\\pmb{\\varphi}_{K}\\right)\\,\\in\\,\\Theta^{K}$ such that $\\bar{\\mathcal{M}}_{f}(\\pmb{\\theta}_{k})\\,\\geq\\,\\mathcal{M}_{f}(\\dot{\\pmb{\\varphi}_{k}})$ for all $k\\;=\\;1,\\ldots,K$ but $\\mathcal{M}_{\\mathcal{T}\\circ f}\\big(\\Theta_{1},...,\\Theta_{K}\\big)<\\mathcal{M}_{\\mathcal{T}\\circ f}\\big(\\Phi_{1},...,\\Phi_{K}\\big)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Index-wise post-hoc reversal) Let $\\mathcal{T}$ be a set of indices and $\\mathcal{P}\\colon\\mathcal{T}\\rightarrow\\Theta^{K}$ map indices to parameter tuples. When Def. 2 holds with $\\left(\\Theta_{1},\\ldots,\\Theta_{K}\\right)=\\mathcal{P}(s),\\left(\\varphi_{1},\\ldots,\\varphi_{K}\\right)=\\mathcal{P}(t)$ for some $s,t\\in\\mathcal{T}$ , we call it index-wise post-hoc reversal. ", "page_idx": 3}, {"type": "text", "text": "Diagnosis. To enable a visual diagnosis of post-hoc reversal, we define base and post-hoc curves (Def. 4) and a relaxed notion of post-hoc reversal for them (Def. 5). Post-hoc reversal is characterized by non-monotonicity between the base and post-hoc curves, i.e., there exist regions where one improves while the other worsens. This happens, for instance, when one curve exhibits double descent but the other doesn\u2019t. Different optimal indices for the two curves is another indicator of post-hoc reversal. ", "page_idx": 3}, {"type": "text", "text": "Definition 4 (Base and post-hoc curves) The base and post-hoc curves $\\mathcal{M}^{b a s e}$ , $\\mathcal{M}^{p o s t}\\colon\\mathcal{T}\\ \\rightarrow\\ \\mathbb{R}$ are given by $\\begin{array}{c c l}{\\mathcal{M}^{b a s e}(t)}&{=}&{\\frac{1}{K}\\sum_{k=1}^{K}\\mathcal{M}_{f}\\big(\\boldsymbol{\\Theta}_{k}\\big)}\\end{array}$ and $\\mathcal{M}^{p o s t}(t)\\;\\;=\\;\\;\\mathcal{M}_{\\mathcal{T}\\circ f}(\\Theta_{1},\\ldots,\\Theta_{K})$ , where $(\\theta_{1},\\ldots,\\theta_{K})=\\mathcal{P}(t)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 5 (Post-hoc reversal for curves) Base and post-hoc curves $\\mathcal{M}^{b a s e}$ , $\\mathcal{M}^{p o s t}\\colon\\mathcal{T}\\rightarrow\\mathbb{R}$ exhibit post-hoc reversal when there exist $s,t\\in\\mathcal{T}$ such that $\\mathcal{M}^{b a s e}(s)\\geq\\mathcal{M}^{b a s e}(t)$ but $\\mathcal{M}^{p o s t}(s)<$ $\\mathcal{M}^{p o s t}(t)$ . ", "page_idx": 3}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/ffced43c3f60a1fe203b290660cac2bfe21a73fb6ecde5cdb328c452ad0394d5.jpg", "img_caption": ["Figure 3: Loss and error for CIFAR-10-N Clean (approx. $0\\%$ noise), Rand1 (approx. $17\\%$ noise) and Worst (approx. $40\\%$ noise). Except for ensemble curves, mean of 8 runs is shown; individual runs are in lighter shades. Ensembles comprise models from these 8 runs. For example, observe post-hoc reversal for C-10-N Worst: (1) error plot: from epoch 5 to 50, solid red (base) curve worsens but solid orange (SWA) curve improves; (2) error plot: solid red (base) curve has a double descent but dashed red (ensemble) curve does not; (3) loss plots: solid red (base) curve has a double descent pre-TS but not post-TS; (4) error plot: best error is at approx. epoch 5 for solid red (base) curve but at approx. epoch 60 for dashed orange (SWA ensemble) curve. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.2 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.2.1 Epoch-Wise Post-Hoc Reversal ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When the indices in Def. 3 are training epochs, we call it epoch-wise post-hoc reversal. We use $\\boldsymbol{\\uptheta}_{t}$ to denote the model at the end of epoch $t$ . For ensembles, a superscript $j$ denotes the $j$ -th training run (out of $N$ runs). $t\\in\\mathcal T$ maps to parameters $\\mathcal{P}(t)\\in\\Theta^{K}$ ( $K=1$ for TS; $N$ for ensemble; and $t$ for SWA) as follows: $\\mathcal{P}_{\\mathrm{TS}}(t)=(\\boldsymbol{\\Theta}_{t})$ $\\mathbf{\\Phi}_{t});\\mathcal{P}_{\\mathrm{Ens}}(t)=(\\Theta_{t}^{1},\\dots,\\Theta_{t}^{N})^{4}.;\\mathcal{P}_{\\mathrm{SWA}}(t)=(\\Theta_{1},\\dots,\\Theta_{t}).$ . ", "page_idx": 4}, {"type": "text", "text": "Experimental setup. We focus on the CIFAR-N dataset [74]. CIFAR-10-N uses the same images as CIFAR-10 but provides multiple human-annotated label sets, allowing the study of realistic noise patterns of varying levels in a controlled manner. Clean is the original label set; Rand1,2,3 are 3 sets of human labels; Aggre combines Rand1,2,3 by majority vote; and Worst combines them by picking an incorrect label, if possible. Similarly CIFAR-100-N has two label sets, Clean and Noisy, with the latter being human-labeled. We train ResNet18 [21] models for 100 epochs with a cosine annealed learning rate. Additional details on datasets and training setup are in App. C. Fig. 3 shows test curves on CIFAR-10-N Clean, Rand1 and Worst. Other label sets and CIFAR-100-N are in App. E. For clarity, we omit the SWA base curve $\\mathcal{M}_{\\mathrm{SWA}}^{\\mathrm{base}}(t)=(\\mathcal{M}_{f}(\\boldsymbol{\\Theta}_{1})+\\cdot\\cdot\\cdot+\\mathcal{M}_{f}(\\boldsymbol{\\Theta}_{t}))/t$ in the plots, and simply re-use the curve $\\mathcal{M}^{\\mathrm{base}}(t)=\\mathcal{M}_{f}(\\pmb{\\theta}_{t})$ to compare with the post-hoc SWA curve. While deviating from Def. 4, this better reflects the current practice of early stopping on the latest epoch\u2019s base metric. ", "page_idx": 4}, {"type": "text", "text": "Observations. First, we focus on the base curves: $(I)$ Overftiting: As noise increases, test curves go from a single descent to a double descent to a U-shaped curve with increased overftiting. (2) Double descent: Noise amplifies double descent, and the second descent worsens with increasing noise (as compared to the first). (3) Loss-error mismatch: Loss overftis more drastically than error, leading to a mismatch with higher noise. Optimal models for loss and error can be different. ", "page_idx": 4}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/5d56ed17091cf38fbd818d92519465cb4717b7c601bbff2bb1efe8c617304415.jpg", "img_caption": ["Figure 4: C-10-N Worst test curves against model size. Best width for solid blue curves is $\\sim10$ but for dashed orange curves, it is $\\sim50$ for error and $\\sim25$ for post-TS loss. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/2b86389e6bb96f13eb923334d33eddd388e87f8499d62864f51a3697c7fa39c9.jpg", "img_caption": ["Figure 5: FMoW test curves for 3 LR schedules. Note that the preTS loss is significantly higher than the post-TS loss. For example, observe post-hoc reversal w.r.t. cosine and constant LRs at epoch 50 between: (1) solid blue (base) and dashed blue (ensemble) error curves; (2) solid blue (base) and solid orange (SWA) post-TS loss curves; (3) solid blue (base) curves for pre-TS and post-TS loss. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Next, we consider the general impact of post-hoc transforms: (4) Performance improvements: TS, SWA and ensemble always improve performace, both individually and in composition with larger gaps for noisy label sets. (5) Post-hoc reversal: Post-hoc reversal manifests as non-monotonicity between the base and post-hoc curves, especially for noisy label sets. (6) SWA vs Ensemble: SWA can recover much of the ensemble gain, but the optimal epoch often differs a lot from the base curve. (7) Smoother curves: Base curves fluctuate wildly, but SWA and ensemble curves are smooth, making them more reliable for early stopping. ", "page_idx": 5}, {"type": "text", "text": "Finally, we discuss some benefits from post-hoc reversal: (8) Overfitting: All transforms reduce overfitting, often reverting performance degradation. (9) Double descent: SWA, ensemble and compositions flatten the double descent peak. TS, on the other hand, leads to a double descent for some cases where there was none before. (10) Loss-error mismatch: TS aligns the loss and error curves, enabling simultaneously good loss and error. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Model-Wise Post-Hoc Reversal ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, indices represent model sizes. Models of all sizes are trained for $T$ epochs, large enough for convergence. Following [55], we avoid early stopping. Notation-wise, we add a subscript to $\\uptheta$ to indicate the model size $s$ . Parameters are indexed as follows: $\\mathcal{P}_{\\mathrm{TS}}(s)\\,=\\,(\\uptheta_{T,s})$ ; $\\mathcal{P}_{\\mathrm{Ens}}(\\bar{s})=$ $(\\Theta_{T,s}^{1},\\cdot\\cdot\\cdot,\\Theta_{T,s}^{N})$ ; $\\mathcal{P}_{\\mathrm{SWA}}(s)=(\\Theta_{1,s},...\\,,\\Theta_{T,s}).$ . ", "page_idx": 5}, {"type": "text", "text": "Experimental setup. We parameterize a family of ResNet18s by scaling the number of fliters in the convolutional layers. Specifically, we use $[k,2k,4k,8k]$ filters for width $k$ . The standard ResNet18 corresponds to $k=64$ . Otherwise the training setup is same as before. Fig. 4 shows the curves. Concretely, the index set $\\mathcal{T}=\\{2,4,\\ldots,64\\}$ is the set of ResNet widths $k$ described above. ", "page_idx": 5}, {"type": "text", "text": "Observations. Post-hoc transforms improve performance (up to $\\approx10$ points for error) and mitigate double descent. Further, we see yet another way in which higher-capacity models are better: they give better results under post-hoc transforms even when lower-capacity base models perform better. ", "page_idx": 5}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/7da683e38a25557ad22df42c06d35be470670f8a6e6ea7e0d5b26057db530644.jpg", "img_caption": ["Figure 6: Evolution of the fit/memorization of clean and mislabeled examples during training, for base and SWA models on C-10-N Worst. Train error drops earlier for the clean subset. In the regime of post-hoc reversal (shaded), SWA further lowers the train error on the clean subset, while raising it on the mislabeled subset. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["% predictions Clean subset 2400 Mislabeled subset % 0 0 20 40 60 80 100 Epochs Figure 7: Flipping of predicted class between consecutive epochs, for clean and mislabeled train subsets of C-10-N Worst. $\\%$ of examples flipped is about twice as high for the mislabeled subset, suggesting an unstable influence on the decision boundary. 3 2 1 0 20 40 60 80 100 Epochs "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["Figure 8: Optimal temperature for TS on C-10-N Worst increases with epochs, indicating increasing overconfidence of the neural network. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2.3 Hyperparameter-Wise Post-Hoc Reversal ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In general, the index set $\\mathcal{T}$ can contain any hyperparameter configurations. Here, we consider two hyperparamters: learning rate schedule and training epochs. To avoid repeating CIFAR-N epoch-wise curves, we experiment on a fresh dataset, FMoW. ", "page_idx": 6}, {"type": "text", "text": "Experimental setup. We experiment on learning rates (LRs) and training epochs, with index set $\\bar{\\mathcal{Z}}\\bar{=}\\{\\mathsf{c o n s t},\\mathsf{e x p},\\bar{\\mathsf{c o s}}\\}\\times\\{\\bar{1},\\ldots,T\\}$ . Here, const, exp and cos refer to constant, exponentially decaying and cosine annealed LRs respectively, and $T$ is the total number of epochs. We train DenseNet121 [26] models on the FMoW dataset [9] which constitutes a 62-way classification of land use from satellite images. For more details, see App. C. Fig. 5 shows the curves. ", "page_idx": 6}, {"type": "text", "text": "LR-wise observations. We see some interesting instances of post-hoc reversal: (1) constant LR has the worst base performance but the best post-hoc performance; (2) under SWA and TS (composed), the curves continue to improve at the later epochs for constant LR, but not for the decaying $\\dot{\\mathrm{LRs}^{5}}$ . ", "page_idx": 6}, {"type": "text", "text": "Epoch-wise observations. Epoch-wise post-hoc reversal occurs for all LR schedules. SWA and ensembling convert the double descent into a strong single descent, with approx. 10-point improvement in error for the latter. For constant LR, this also changes the optimal epoch. SWA only recovers about half of the ensemble gain, and perhaps surprisingly, ensembling SWA models is not better than ensembling alone. Pre-TS loss curves show a strong mismatch with the error curves, but TS enables simultaneously good loss and error with the last epoch models. Overall, these observations reinforce the trends gleaned from the CIFAR-N experiments. ", "page_idx": 6}, {"type": "text", "text": "5 Intuitions for Post-Hoc Reversal ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we give hypotheses for post-hoc reversal, backed by experimental evidence. ", "page_idx": 6}, {"type": "text", "text": "Ensembling and SWA delay catastrophic overfitting. Models learn generalizable patterns from clean examples, and spurious patterns from mislabeled ones. The latter causes overfitting. When noise is low, the former dominates and overfitting is benign. Otherwise, overfitting is catastrophic. Ensembling and SWA improve fitting of clean examples, and reduce memorization of mislabeled ones. When this overturns the dominance of spurious patterns, we observe reversal. ", "page_idx": 6}, {"type": "text", "text": "Fig. 6 validates this intuition for SWA on CIFAR-10-N Worst. Fig. 7 further suggests the underlying mechanism \u2014 predictions on the mislabeled train subset fluctuate much more during training, allowing SWA to easily revert their memorization. In App. G, we extend this analysis to ensembling and solidify the intuition further by visualizing decision boundaries on a synthetic dataset. This explanation also applies to flattening of the double descent peak, which is a manifestation of catastrophic overfitting. ", "page_idx": 6}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/7d509b077e79363d0b5df7ce4031eb6d8977ef02f9cbc6db4fe146177859dbc2.jpg", "table_caption": ["Table 1: Naive vs post-hoc (ours) selection for $\\mathrm{SWA+TS}$ and $\\mathrm{SWA+Ens+TS}$ transforms. Better values are in bold. Except some clean cases, post-hoc selection is always better, often more than doubling the improvement over no transform. See Tabs. 6 and 8 in App. E for standard deviations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "TS mitigates loss-error mismatch. Once a neural net has fti a train example, the cross-entropy loss on it can be lowered by simply upscaling the weights of the linear output layer. This makes the model overconfident later in training, as shown in [19]. For a mislabeled example, this leads to worse loss on similar test instances. The test error is not affected as it is independent of the scale of the logits. In high-noise settings, test loss can worsen due to memorization of mislabeled examples, even as the test error improves from continued learning on clean examples, leading to loss-error mismatch. TS fixes this by downscaling the logits. Indeed, one finds that the temperature (as obtained with a held-out set) increases with epochs (Fig. 8). ", "page_idx": 7}, {"type": "text", "text": "Post-hoc reversal can occur against epochs, model sizes or other hyperparameters. Different variants of post-hoc reversal can be unified via effective model complexity (EMC), introduced in [55] to unify epoch- and model-wise double descent. EMC measures memorization capacity, which plays a key role in post-hoc reversal. EMC increases with epochs and model size. Further, EMC increases with epochs more rapidly for constant LR than annealed LR, explaining our observations in $\\S\\ 4.2.3$ . ", "page_idx": 7}, {"type": "text", "text": "6 Post-Hoc Selection: Leveraging Post-Hoc Reversal in Practice ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our findings from $\\S4$ motivate the principle of post-hoc selection, where model development decisions take post-hoc transforms into account. For concreteness, we discuss the choice of checkpoints from training runs under the $\\mathrm{SWA+TS}$ and $\\mathrm{SWA+Ens+TS}$ transforms. Checkpoint selection reduces to the selection of the final epoch $\\widehat{T}$ , as SWA uses all checkpoints up to that epoch. ${\\mathcal{M}}_{\\mathrm{val}}$ denotes a metric of choice computed on the validation set. ", "page_idx": 7}, {"type": "text", "text": "$\\mathbf{SW}\\mathbf{A}\\mathbf{+}\\mathbf{TS}.$ . Naive selection picks epoch $\\widehat{T}=\\arg\\operatorname*{min}_{T}\\mathcal{M}_{f}^{\\mathrm{val}}(\\boldsymbol{\\theta}_{T})$ . In contrast, post-hoc selection picks $\\begin{array}{r}{\\widehat{T}=\\arg\\operatorname*{min}_{T}\\mathcal{M}_{\\mathcal{T}_{\\mathrm{s+T}}\\circ f}^{\\mathrm{val}}\\big((\\Theta_{t})_{t=1}^{T}\\big)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "$\\mathbf{S}\\mathbf{W}\\mathbf{A}\\mathbf{+}\\mathbf{E}\\mathbf{n}\\mathbf{s}\\mathbf{+}\\mathbf{T}\\mathbf{S},$ . Here we have $N$ different training runs to pick epochs for. Naive selection picks $\\widehat{T}_{j}=\\arg\\operatorname*{min}_{T}\\mathcal{M}_{f}^{\\mathrm{val}}(\\boldsymbol{\\Theta}_{T}^{j})$ for each run independently. In contrast, post-hoc selection would ideally pick $\\widehat{T}_{1},\\dots,\\widehat{T}_{N}=\\arg\\operatorname*{min}_{T_{1},\\dots,T_{N}}\\mathcal{M}_{\\mathcal{T}_{\\mathrm{S}+\\mathrm{E}+\\mathrm{T}^{0}}f}^{\\mathrm{val}}((\\Theta_{t}^{1})_{t=1}^{T_{1}},\\dots,(\\Theta_{t}^{N})_{t=1}^{T_{N}})$ MvTaSl+E+T\u25e6f((\u03b8t1)tT=11, . . . , (\u03b8tN )tT=N1) which jointly minimizes the ensemble performance. This being computationally expensive, we instead minimize under the constraint $\\dot{\\widehat{T}}_{1}=\\cdot\\cdot=\\widehat{T}_{N}{}^{6}$ ", "page_idx": 7}, {"type": "text", "text": "Results. Tab. 1 compares naive and post-hoc selection strategies for CIFAR-N and FMoW. Except for some clean label sets, post-hoc selection is always better than naive selection, often with $>2\\times$ improvement from post-hoc selection as compared to naive selection. It remains effective with outof-distribution (OOD) val/test sets, as seen for FMoW (we use ID and OOD splits from WILDS [34]). For some datasets, like C-100-N Noisy, post-hoc selection is only marginally better on test error. Often, in such cases, the error floor is already quite high (e.g., C-100-N Noisy has $\\sim40\\%$ noise and ResNet-18 has $\\sim10\\%$ error on clean C-100, so a test error of $\\sim50\\%$ is already impressive), and test loss is a more appropriate metric. ", "page_idx": 7}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/68413ba2bd0810f4a04578f6e55fe961f581e4e10aa69d519cc35b13c4f27e54.jpg", "img_caption": ["Figure 9: Perplexity and causal language modeling (CLM) error on the Guanaco test set, and MMLU accuracy (higher is better) for instruction tuning LLaMA-2-7B. Shading indicates post-hoc reversal. Base and $\\mathrm{SWA}{+}\\mathrm{TS}$ curves are mean of 8 runs; $\\scriptstyle\\mathrm{SWA+Ens+TS}$ ensembles models from these runs. Individual runs are not shown as they have high variance (see Tab. 7 in App. E). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Early stopping. We advocate monitoring post-hoc metrics for early stopping. Only a running average needs to be updated for SWA, and TS involves a quick single-parameter optimization. Further, while the base curves can fluctuate wildly between consecutive runs, $\\mathrm{SWA+TS}$ curves are considerably smoother (see Figs. 3, 11 and 10), making them more reliable for automated early stopping. One can similarly monitor metrics for $\\mathrm{SWA+Ens+TS}$ under parallel training runs. ", "page_idx": 8}, {"type": "text", "text": "7 Experiments Across Domains and Modalities ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In $\\S\\ 4$ and $\\S\\ 6$ , we introduced post-hoc reversal and selection with experiments on the CIFAR-N and FMoW datasets. In this section, we supplement our experimental analysis with additional experiments across diverse domains and modalities to demonstrate the generality of our findings. ", "page_idx": 8}, {"type": "text", "text": "7.1 LLM Instruction Tuning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Language models are pre-trained or fine-tuned with a self-supervised objective of predicting the next token in a text corpus. There might be many acceptable tokens following a given prefix, albeit with different probabilities. Thus next token prediction is noisy and one might reasonably expect to see post-hoc reversal. In this section, we test this hypothesis for the task of fine-tuning LLMs to follow instructions (instruction tuning [72]). Instruction tuning datasets are naturally small [85] and amenable to multi-epoch training where catastrophic overfitting becomes an important concern. Recent works [53, 81] have argued for data repetitions for LLM pre-training as well, but such experiments are beyond the scope of this paper. ", "page_idx": 8}, {"type": "text", "text": "Experimental setup. We fine-tune LLaMA-2-7B [70] on the Guanaco dataset [12] of chat completions. We evaluate perplexity and causal language modeling (CLM) error on the test set, and also the MMLU accuracy [24] to better contextualize model improvements. Fig. 9 shows the curves. Tab. 7 in App. E gives exact numbers, and App. F explores sub-epoch checkpointing. For TS, we use a shared temperature parameter to scale the logits of all tokens and leave more involved strategies like long-horizon temperature scaling [66] to future work. ", "page_idx": 8}, {"type": "text", "text": "Observations. We observe post-hoc reversal between epochs 1 and 2 for perplexity and error, and between epochs 2 and 3 for MMLU. Both $\\mathrm{SWA}{+}\\mathrm{TS}$ and $\\mathrm{SWA+Ens+TS}$ transforms show significant improvements, much of which is only realized under post-hoc selection. ", "page_idx": 8}, {"type": "text", "text": "7.2 Other Text, Tabular and Graph Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we further expand our experimental coverage to text, tabular and graph classification datasets from real-world applications. ", "page_idx": 8}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/97b03a9e85975a242160de8c5ad71931b501c7a7e197c9313bd013f356c29e48.jpg", "img_caption": ["Figure 10: Test curves for 3 real-world noisy datasets. Note that the pre-TS loss is significantly higher than the post-TS loss. Examples of post-hoc reversal between the base curves given by the solid blue lines and the post-hoc curves given by the dashed orange lines (SWA ensemble): (1) optimal epoch is different for base and post-hoc curves for error and post-TS loss on all datasets; (2) for error on Yelp, base curve shows double descent but post-hoc curve does not; (3) for error on Income, base curve overftis catastrophically at approx. epoch 5 but post-hoc curve continues improving till approx. epoch 20; (4) for error on Reddit-12k, base curve does not show double descent but post-hoc curve does. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Experimental setup. We consider the following tasks: (1) sentiment classification on the Yelp reviews dataset [5] (text) with a pre-trained transformer BERT [13], (2) prediction tasks on census data from Folktables [14] (tabular) with MLPs and (3) community detection on the Reddit and Collab datasets [82] (graph) with graph neural networks (GNNs). Folktables has 5 prediction tasks: Income, PublicCoverage, Mobility, Employment and TravelTime. Reddit has 2 versions: Reddit- $^\\mathrm{5k}$ and Reddit-12k. For more details, see App. C. Figure 10 shows curves for Yelp, Income and Reddit-12k. Tab. 5 in App. D compares naive and post-hoc selection on all datasets. ", "page_idx": 9}, {"type": "text", "text": "Observations. Post-hoc reversal is a recurring feature across datasets, transforms and metrics. The 3 datasets show different patterns between the base and post-hoc curves, showing that post-hoc reversal can take a variety of forms. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We empirically studied temperature scaling (TS), ensembling, stochastic weight averaging (SWA) and their compositions, and found that these transforms can reverse model peformance trends (post-hoc reversal). Based on our findings, we presented the simple technique of post-hoc selection, and showed that it outperforms naive selection. We validated our findings and proposals over diverse settings. ", "page_idx": 9}, {"type": "text", "text": "Our work has broad implications for the field of deep learning. It shows that current practices surrounding the use of post-hoc transforms leave much room for improvement. This is especially true for noisy data, which is pervasive in real-world applications. Future directions include better strategies for checkpoint selection, developing a theoretical understanding, investigating impacts on scaling laws, and characterizing other instances of post-hoc reversal. ", "page_idx": 9}, {"type": "text", "text": "Summary of practical recommendations. We advocate for the use of TS, ensembling and SWA across deep learning applications. Further, such transforms should be tightly integrated into the model development pipeline, following the methodology outlined in the paper. In particular: (1) apply $\\mathrm{SWA+TS}$ and $\\scriptstyle\\mathrm{SWA+Ens+TS}$ transforms for better results in the single- and multi-model settings respectively; (2) track temperature-scaled loss to overcome loss-error mismatch; (3) monitor post-hoc metrics to avoid premature early stopping; (4) make hyperparameter decisions informed by post-transform performance; (5) use post-hoc selection to pick model checkpoints. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "ZL acknowledges Amazon AI, Salesforce Research, Facebook, UPMC, Abridge, the PwC Center, the Block Center, the Center for Machine Learning and Health, and the CMU Software Engineering Institute (SEI) via Department of Defense contract FA8702-15-D-0002, for their generous support of ACMI Lab\u2019s research on machine learning under distribution shift. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Taiga Abe, E. Kelly Buchanan, Geoff Pleiss, and John P Cunningham. Pathologies of predictive diversity in deep ensembles. ArXiv, abs/2302.00704, 2023.   \n[2] Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with biascorrected calibration is hard-to-beat at label shift adaptation. In International Conference on Machine Learning, pages 222\u2013232. PMLR, 2020. [3] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.   \n[4] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. Advances in Neural Information Processing Systems, 35:8265\u20138277, 2022.   \n[5] Nabiha Asghar. Yelp dataset challenge: Review rating prediction. arXiv preprint arXiv:1605.05362, 2016.   \n[6] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 34:22405\u201322418, 2021.   \n[7] John Chen, Qihan Wang, and Anastasios Kyrillidis. Mitigating deep double descent by concatenating inputs. Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021.   \n[8] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023.   \n[9] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6172\u20136180, 2018.   \n[10] Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. ArXiv, abs/2103.00065, 2021.   \n[11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \n[12] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[14] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in Neural Information Processing Systems, 34, 2021.   \n[15] Reuben Feinman. Pytorch-minimize: a library for numerical optimization with autograd, 2021. URL https://github.com/rfeinman/pytorch-minimize.   \n[16] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757, 2019.   \n[17] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv: Learning, 2018.   \n[18] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in neural information processing systems, 31, 2018.   \n[19] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[20] Trevor J. Hastie, Robert Tibshirani, and Jerome H. Friedman. The elements of statistical learning. 2001.   \n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[22] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 558\u2013567, 2019.   \n[23] Reinhard Heckel and Fatih Yilmaz. Early stopping in deep networks: Double descent and how to eliminate it. ArXiv, abs/2007.10099, 2020.   \n[24] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[25] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017.   \n[26] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[27] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. Advances in Neural Information Processing Systems, 35:29262\u201329277, 2022.   \n[28] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.   \n[29] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.   \n[30] Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In International Conference on Machine Learning, 2019.   \n[31] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.   \n[32] Amr Khalifa, Michael C Mozer, Hanie Sedghi, Behnam Neyshabur, and Ibrahim Alabdulmohsin. Layer-stack temperature scaling. arXiv preprint arXiv:2211.10193, 2022.   \n[33] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[34] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021.   \n[35] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 491\u2013507. Springer, 2020.   \n[36] Dan Kondratyuk, Mingxing Tan, Matthew Brown, and Boqing Gong. When ensembling smaller models is more efficient than single large models. arXiv preprint arXiv:2005.00570, 2020.   \n[37] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyf,i et al. Openassistant conversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.   \n[38] Ludmila I. Kuncheva and Christopher J. Whitaker. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Machine Learning, 51:181\u2013207, 2003.   \n[39] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Neural Information Processing Systems, 2016.   \n[40] Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for scalable image classifier training with label noise. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5447\u20135456, 2017.   \n[41] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion: A survey. arXiv preprint arXiv:2309.15698, 2023.   \n[42] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Earlylearning regularization prevents memorization of noisy labels. Advances in neural information processing systems, 33:20331\u201320342, 2020.   \n[43] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Earlylearning regularization prevents memorization of noisy labels. ArXiv, abs/2007.00151, 2020.   \n[44] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. ArXiv, abs/2202.14026, 2022.   \n[45] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by overparameterization. In International Conference on Machine Learning, pages 14153\u201314172. PMLR, 2022.   \n[46] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer: A vision-language model with an ensemble of experts. arXiv preprint arXiv:2303.02506, 2023.   \n[47] Raphael Gontijo Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule them all: Overlapping features of training methods. ArXiv, abs/2110.12899, 2021.   \n[48] Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. Routing to the expert: Efficient reward-guided ensemble of large language models. arXiv preprint arXiv:2311.08692, 2023.   \n[49] Xiaoding Lu, Adian Liusie, Vyas Raina, Yuwen Zhang, and William Beauchamp. Blending is all you need: Cheaper, better alternative to trillion-parameters llm. arXiv preprint arXiv:2401.02994, 2024.   \n[50] Neil Rohit Mallinar, James B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: A taxonomy of overfitting. $A r X i\\nu$ , abs/2207.06569, 2022.   \n[51] Prem Melville and Raymond J. Mooney. Constructing diverse classifier ensembles using artificial training examples. In International Joint Conference on Artificial Intelligence, 2003.   \n[52] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663, 2020.   \n[53] Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. ArXiv, abs/2305.16264, 2023.   \n[54] Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. ArXiv, abs/2003.01897, 2020.   \n[55] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, 2021.   \n[56] Yaniv Ovadia, Emily Fertig, Jie Jessie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. In Neural Information Processing Systems, 2019.   \n[57] Vardan Papyan, Xuemei Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences of the United States of America, 117:24652 \u2013 24663, 2020.   \n[58] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overftiting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.   \n[59] V Qu\u2019etu and E Tartaglione. Can we avoid double descent in deep neural networks. arXiv preprint arXiv:2302.13259, 2023.   \n[60] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.   \n[61] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. Advances in Neural Information Processing Systems, 35:10821\u201310836, 2022.   \n[62] Alexandre Ram\u00e9, Nino Vieillard, L\u00e9onard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models. arXiv preprint arXiv:2401.12187, 2024.   \n[63] Rishabh Ranjan. torchcal: post-hoc calibration on GPU, 2023. URL https://github.com/ rishabh-ranjan/torchcal.   \n[64] Sunny Sanyal, Atula Tejaswi Neerkaje, Jean Kaddour, Abhishek Kumar, et al. Early weight averaging meets high learning rates for llm pre-training. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023), 2023.   \n[65] Rylan Schaeffer, Mikail Khona, Zachary Robertson, Akhilan Boopathy, Kateryna Pistunova, Jason W. Rocks, Ila Rani Fiete, and Oluwasanmi Koyejo. Double descent demystified: Identifying, interpreting & ablating the sources of a deep learning puzzle. ArXiv, abs/2303.14151, 2023.   \n[66] Andy Shih, Dorsa Sadigh, and Stefano Ermon. Long horizon temperature scaling. In International Conference on Machine Learning, pages 31422\u201331434. PMLR, 2023.   \n[67] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in rlhf. arXiv preprint arXiv:2312.08358, 2023.   \n[68] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust deep learning. In International Conference on Machine Learning, 2019.   \n[69] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[70] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[71] Dongdong Wang, Boqing Gong, and Liqiang Wang. On calibrating semantic segmentation models: Analyses and an algorithm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23652\u201323662, 2023.   \n[72] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021.   \n[73] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. ArXiv, abs/2110.12088, 2021.   \n[74] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. arXiv preprint arXiv:2110.12088, 2021.   \n[75] Ross Wightman, Hugo Touvron, and Herv\u00e9 J\u00e9gou. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021.   \n[76] Andrew G Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. Advances in neural information processing systems, 33:4697\u20134708, 2020.   \n[77] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965\u201323998. PMLR, 2022.   \n[78] Ruixuan Xiao, Yiwen Dong, Haobo Wang, Lei Feng, Runze Wu, Gang Chen, and Junbo Zhao. Promix: Combating label noise via maximizing clean sample utility. In Edith Elkind, editor, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pages 4442\u20134450. International Joint Conferences on Artificial Intelligence Organization, 8 2023. doi: 10.24963/ijcai.2023/494. Main Track.   \n[79] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2691\u20132699, 2015.   \n[80] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.   \n[81] Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. ArXiv, abs/2305.13230, 2023.   \n[82] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365\u20131374, 2015.   \n[83] Mert Yuksekgonul, Linjun Zhang, James Y. Zou, and Carlos Guestrin. Beyond confidence: Reliable models should also consider atypicality. ArXiv, abs/2305.18262, 2023.   \n[84] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[85] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. ", "page_idx": 15}, {"type": "text", "text": "A Expanded Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Phenomena. Empirical works like double descent [55], grokking [58], scaling laws [31], neuralcollapse [57], edge-of-stability [10], lottery-ticket-hypothesis [17] have revealed both challenges and oppotunities for improving the understanding and practices of deep neural network training. Post-hoc reversal expands this list as a novel phenomenon regarding learning dynamics under the lens of post-hoc transforms. It is most intimately connected with double descent, offering a way to mitigate it. Some works [7, 23, 54, 59, 65, 76] show other mitigations, such as regularization and data augmentation. ", "page_idx": 16}, {"type": "text", "text": "Temperature Scaling (TS). TS belongs to a family of post-hoc calibration techniques [2, 19, 32, 66, 83], with the unique property of preserving classification error. Recently, calibration has been applied to large vision and language models [11, 71, 84]. While loss-error mismatch has been reported before [11, 19], to the best of our knowledge, we are the first to report post-hoc reversal with TS. ", "page_idx": 16}, {"type": "text", "text": "Ensembling. Ensembling is a foundational technique in machine learning, encompassing bagging, boosting, etc. In deep learning, a uniform ensemble is most popular [3, 39], although recent work on ensembling LLMs has explored more efficient routing-based ensembles [29, 46, 48, 49]. Various works have explored strategies to form optimal ensembles [36, 47, 51, 77], generally based on model diversity [38], but recently Abe et al. [1] have warned against this. In contrast, our recommendation for forming ensembles relies directly on the validation performance of the ensemble, introducing no proxies, and still being computationally cheap. ", "page_idx": 16}, {"type": "text", "text": "Stochastic Weight Averaging (SWA). SWA [28] is the culmination of a line of work [18, 25] which seek to cheaply approximate ensembling. It has inspired numerous works which average weights in some form [4, 6, 27, 41, 61, 77] often in combination with ensembling. Recently, weight averaging has shown up in the LLM space [62, 64]. While these works generally apply SWA with a fixed training time determined independently, we present SWA in the role of early stopping and model selection. In practice, SWA has often been found to be unreliable7, and is often skipped from training recipes even when considered [35, 75]. Our work sheds some light on this, offering a rather counter-intuitive choice of models to include in the weight average for best results. ", "page_idx": 16}, {"type": "text", "text": "Noise. Many training strategies have been introduced to deal with noisy data (see [69] for a survey). However, the efficacy of simple post-hoc transforms has been left unexplored. Further, most of these works are motivated by labeling errors, which leaves some of the core practical considerations for dealing with general noisy data unaddressed. For instance, access to a clean validation set is assumed and test loss is overlooked as an important metric [43, 44]. We also entirely avoid experiments on synthetic noise, informed by recent work which questions the transferability of findings to realistic noise patterns [30, 73]. Some recent datasets [30, 40, 68, 73, 79] make it possible to study realistic noise along with known noise estimates. Noise due to insufficient information in the input context (Fig. 11) has also been studied under different settings, such as for RLHF [67]. ", "page_idx": 16}, {"type": "text", "text": "Multi-epoch training of LLMs. Multi-epoch training of LLMs runs into severe catastrophic overfitting. Xue et al. [81] examine the contributing factors and explore possible solutions. They find that regularization is not helpful, except for dropout. Muennighoff et al. [53] study scaling laws considering data repetitions. Complementarily, we put forward post-hoc transforms as an effective solution with our post-hoc selection methodology. This is especially important for fine-tuning LLMs, e.g. in instruction tuning [72], where [85] and [8] advocate for fine-tuning with a smaller amount of higher quality samples for more epochs. ", "page_idx": 16}, {"type": "text", "text": "B Expanded Preliminaries and Background ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Learning on Noisy Data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figures 12, 11 and 13 illustrate various sources of noise: aleatoric unertainty, epistemic uncertainty and annotation errors. Below we provide some background on Bayes-optimal classifier and use it to introduce the clean error metric and Bayes loss/error as measures of noise level. ", "page_idx": 16}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 11: Data can be noisy due to insufficient information in the input context (epistemic uncertainty). Figures 11a and 11b show satellite images from the FMoW dataset. The labels are correct, as corroborated by external map data. However, they cannot be determined with full certainty from the images alone. ", "page_idx": 17}, {"type": "text", "text": "Figure 12: Data can be noisy due to nondeterminism in the prediction target (aleatoric uncertainty). Figure shows a message tree from the OpenAssistant Converstations (OASST1) Dataset. A chatbot can continue a conversation satisfactorily in many different ways, making next token prediction noisy. ", "page_idx": 17}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/37fad993368d15a947b7cbb955a438275c57c21cb5c89af6b8d520b2ded7b2db.jpg", "img_caption": ["Figure 13: Data can be noisy due to annotation errors. Figures 13a, 13b and 13c are mislabeled images from CIFAR-10. 13d, 13e and 13f are ambiguous images from CIFAR-100 with multiple correct labels among the given classes. ( $\\mathbf{L}=$ label in dataset, $\\mathbf{C}=$ correct label, $\\mathbf{A}=$ alternative label) "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Bayes-optimal classifier. $f_{\\mathcal{D}}$ , given by $f_{\\mathcal{D}}(\\mathbf{x})_{k}=\\log\\mathcal{P}_{\\mathcal{D}}[y=k\\mid\\mathbf{x}]$ minimizes both $\\mathcal{M}_{\\mathcal{D}}^{\\mathrm{error}}$ and $\\mathcal{M}_{\\mathcal{D}}^{\\mathrm{loss}}$ , and is called the Bayes-optimal classifier for $\\mathcal{D}$ . The Bayes error $\\mathcal{M}_{\\mathcal{D}}^{\\mathrm{error}}[f_{\\mathcal{D}}]$ and Bayes loss $\\mathcal{M}_{\\mathcal{D}}^{\\mathrm{loss}}[f_{\\mathcal{D}}]$ are measures of the noise level. $y^{*}(\\mathbf{x})\\,=\\,\\arg\\operatorname*{max}_{k}\\,f_{\\mathcal{D}}(\\mathbf{x})_{k}$ is sometimes called the clean label. Using $y^{\\ast}$ , one may define the clean data distribution $\\widetilde{\\mathcal{D}}$ with $\\mathcal{P}_{\\widetilde{\\mathcal{D}}}[\\mathbf{x}]=\\mathcal{P}_{\\mathcal{D}}[\\mathbf{x}]$ and $\\mathcal{P}_{\\widetilde{\\mathcal{D}}}[y\\mid\\mathbf{x}]=\\mathbf{1}\\{y=\\bar{y^{*}}(\\mathbf{x})\\}$ . The clean error $\\mathcal{M}_{\\mathcal{\\widetilde{D}}}^{\\mathrm{error}}$ is a common metri c in the la b el noise literature but not a focus of our work as $y^{\\ast}$ is typically inac cessible in more general noisy settings. ", "page_idx": 17}, {"type": "text", "text": "B.2 Post-Hoc Transforms in Machine Learning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The explicit forms of the composed transforms $\\mathrm{SWA}{+}\\mathrm{TS}$ and $\\mathrm{SWA+Ens+TS}$ (denoted as $\\mathcal{T}_{\\mathrm{S+T}}$ and $\\mathcal{T}_{\\mathrm{S+E+T}})$ are given by Equations 4 and 5 respectively. For $\\mathcal{T}_{\\mathrm{S+E+T}}$ , parameters $\\Theta_{1}^{l},\\dots,\\Theta_{K_{l}}^{l}$ are weightaveraged and the $L$ resulting models are ensembled, followed by temperature scaling. $\\tau_{l}$ is the temperature for weight-averaged models, and $\\tau_{\\mathrm{Ens}}$ is the temperature for the ensemble. As before, they are obtained by optimizing the cross-entropy loss over the validation set, with model parameters fixed. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{s}+\\Gamma^{0}}f\\!\\left(\\mathbf{x};\\mathbf{\\uptheta}_{1},\\ldots,\\mathbf{\\uptheta}_{K}\\right)=\\frac{1}{\\tau}f\\left(\\mathbf{x};\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{\\uptheta}_{i}\\right),\\;\\mathrm{with}\\;\\tau=\\arg\\operatorname*{min}_{\\tau}\\mathcal{M}_{\\mathrm{val}}^{\\mathrm{loss}}\\left[\\frac{1}{\\tau}f\\left(\\cdot;\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{\\uptheta}_{i}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n(7_{\\mathsf{S+E+T}}\\circ f)\\left(\\mathbf{x};\\boldsymbol{\\Theta}_{1}^{1},\\dots,\\boldsymbol{\\Theta}_{K_{1}}^{1},\\dots,\\boldsymbol{\\Theta}_{1}^{L},\\dots,\\boldsymbol{\\Theta}_{K_{L}}^{L}\\right)=\\frac{1}{7\\mathrm{Ens}}\\frac{1}{L}\\sum_{l=1}^{L}\\frac{1}{\\tau_{l}}f\\left(\\mathbf{x};\\frac{1}{K_{l}}\\sum_{k=1}^{K_{l}}\\boldsymbol{\\Theta}_{k}^{l}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/ae7a3b00c56ae49c14efad1735b76f721253929e13c905eca57b1d868f8bac36.jpg", "table_caption": ["Table 2: Dataset Details. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/90652068d48d5f9b70f1913b136472ed5d6d92af8fdc6dab3a7cd22c14447ba2.jpg", "table_caption": ["Table 3: Training Details. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Dataset and Training Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Tabs. 2 and 3 summarize the datasets and training details for our experiments. They are described in detail below. We trained our models under these hyperparameters on 48 GB A6000 GPUs in a single-GPU setup, except for LLaMA-2-7B fine-tuning on Guanaco, for which we used 80 GB A100 GPUs. Single model training completes in a few hours for all datasets except FMoW and Guanaco, on which training took upto 12 hours. We experiment most extensively on the CIFAR-N datasets, where our optimized script can train a single model in 3-5 minutes on an A6000 GPU. ", "page_idx": 18}, {"type": "text", "text": "CIFAR-N [74]. CIFAR-10-N uses the same images as CIFAR-10 but provides multiple humanannotated label sets. Clean is the original label set; Rand1,2,3 are 3 sets of human labels; Aggre combines Rand1,2,3 by majority vote; and Worst combines them by picking an incorrect label, if possible. CIFAR-100 has 2 variants, a fine-grained one with 100 classes and a coarse-grained one with 20 classes, obtained by grouping the fine-grained classes. Correspondingly, there are CIFAR-100-N Coarse and CIFAR-100-N Fine datasets. They have two label sets each: Clean and Noisy, with the latter being human-labeled. In the main paper, CIFAR-100-N refers to the fine-grained version. ", "page_idx": 18}, {"type": "text", "text": "By cross-referencing with the original labels, it is possible to estimate the noise levels. These are shown in Table 4. ", "page_idx": 18}, {"type": "text", "text": "CIFAR-N allows access to clean labels. In the literature, the validation and test sets for CIFAR-N typically use the clean labels [42, 45, 78]. However, access to clean labels is a luxury only available for label noise settings. Even there, obtaining clean labels is expensive, as it requires careful expert annotation. For other sources of noise it might not even be feasible to obtain clean labels. Hence, we restrict ourselves to using noisy (i.i.d. to train) validation and test sets. Since CIFAR-N only provides human labels for the original 50k CIFAR-10/100 train images, we split these into 40k/5k/5k images for train/val/test sets. ", "page_idx": 18}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/c8e6e7e593d1aefe5e62e871fd350d547b42d665d12876b04cf2fb74d84212b5.jpg", "table_caption": ["Table 4: Noise levels for CIFAR-N $(\\%)$ , reproduced from [74]. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/c9ce45a6c74b45dcd19ea28c6cfaec522dfd4d41ccc6c63beeb475dd7888d368.jpg", "table_caption": ["Table 5: Naive vs post-hoc (ours) selection for $\\scriptstyle\\mathrm{SWA+TS}$ and $\\scriptstyle\\mathrm{SWA+Ens+TS}$ transforms on some real-world datasets. Better values are in bold. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "FMoW [9, 34]. This is the version of the original FMoW dataset [9] as used in the WILDS benchmark [34]. For FMoW (ID) we use the in-distribution val and test sets, and for FMoW (OOD), we use the out-of-distribution val and test sets, where the val set is shifted with respect to the train set, and the test set is shifted with respect to both the train and val sets. All splits are as provided by WILDS. The input is an RGB satellite image (rescaled to $224\\times224$ pixels) and the label is one of 62 building or land use categories. The labels were obtained by a combination of human annotation and cross-referenced geographical information. The original dataset provides additional metadata about location, time, sun angles, physical sizes, etc. which is ignored in the WILDS dataset (and hence in ours). While the labels have low noise compared to the ground-truth, this dataset is noisy because of insufficient information. It is hard to disambiguate the building or land use category with full certainty by looking at the satellite image alone. See Figure 11. Models and training setup are as used in [9, 34], except for the LR schedule, where we experiment with multiple alternatives. ", "page_idx": 19}, {"type": "text", "text": "Guanaco [12]. This is a subset of the OASST1 dataset [37] containing only the highest-rated paths in the conversation tree. We follow the fine-tuning setup from [12], except that we use vanilla fine-tuning without any quantization or low-rank adapters. ", "page_idx": 19}, {"type": "text", "text": "Yelp [5]. This is a subset of the Yelp Dataset Challenge 2015 dataset with $25\\mathrm{k}$ reviews in the train set and $5\\mathrm{k}$ reviews each in the validation and test sets. The input is a review text and the label is one of 5 classes (1 to 5 stars). Assigning a rating to a review is intrinsically non-deterministic as different reviewers might have different thresholds for the star ratings. This introduces noise in the data. ", "page_idx": 19}, {"type": "text", "text": "Folktables [14]. Folktables consists of 5 classification tasks based on the US Census: Income, Employment, Health, TravelTime and PublicCoverage. The data is tabular. The available feature columns do not contain sufficient information to predict the targets with full certainty, even if the Census recorded the ground-truth labels with high accuracy. This results in noise. ", "page_idx": 19}, {"type": "text", "text": "Collab and Reddit [52, 82]. These datasets are from TUDataset [52], and were originally introduced by Yanardag and Vishwanathan [82]. Collab is a scientific collaboration dataset. The input is an ego-network of a researcher and the label is the field of the researcher (one of High Energy Physics, Condensed Matter Physics and Astro Physics). The Reddit-5k and Reddit-12k datasets (originally called REDDIT-MULTI-5K and REDDIT-MULTI-12K) are balanced datasets where the input is a graph which corresponds to an online discussion thread from the social network site Reddit. Nodes correspond to users and there is an edge if one user responded to another\u2019s comment. The task is to predict which subreddit a discussion graph belongs to. Reddit- $5\\mathrm{k}$ is smaller with $5\\mathrm{k}$ examples and 5 classes. Reddit-12k is bigger with 12k examples and 11 classes. ", "page_idx": 19}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/5c576ef23055a3ddcc16855dd8bec65ad8d3224a21c2b947eceea645b000943a.jpg", "table_caption": ["Table 6: Detailed results for CIFAR-N datasets. Base denotes no transform and Final denotes the $\\scriptstyle\\mathrm{SWA+Ens+TS}$ transform. Gain shows performance improvement. $\\Delta$ shows change from naive selection to post-hoc selection. Since Base and Gain columns involve 8 individual runs, we report mean $\\pm$ std. dev. of the metric. C-10-N, C-100-N-C and C-100-N-F are shorthands for CIFAR-10-N, CIFAR-100-N Coarse and CIFAR-100-N Fine respectively. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Post-Hoc Selection Results for Remaining Datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 5 compares naive and post-hoc selection for datasets not covered in the main paper. Post-hoc selection is mostly better than naive selection, although with varying margins. Post-hoc selection is sometimes worse, but only marginally8. ", "page_idx": 20}, {"type": "text", "text": "E Detailed Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Tables 6, 7, and 8 provide detailed results for CIFAR-N, LLM instruction tuning, and other datasets respectively. ", "page_idx": 20}, {"type": "text", "text": "F Optimal Checkpointing for Small Number of Epochs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Throughout the main paper, we use a checkpoint interval of 1 epoch. For small-epoch settings, such as LLM pre-training or fine-tuning, it might be better to checkpoint more frequently, at fractional epochs. In this section, we investigate the impact of checkpoint interval on the best MMLU score, and the epoch at which it is achieved, for the LLM instruction tuning setup of $\\S\\,7.1$ . ", "page_idx": 20}, {"type": "text", "text": "Table 7: Detailed results for LLM instruction tuning. Better values are in bold. Since Base and Gain columns involve 8 individual runs, we report mean $\\pm$ std. dev. of the metric. ", "page_idx": 21}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/09e3e734f107186af97495bb5e363fc8916d0415a27e27b893f5e72166bc46a4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/914e33dbad9f2da5f69f395179dce8b3142f86850e5ba90b12a11033464015db.jpg", "table_caption": ["Table 8: Detailed results for other datasets. See Table 6 caption for a description. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figs. 14a and 14b show the results. We find that a checkpointing interval of 0.7 epochs gives the best results, with higher and lower intervals performing slightly worse. This makes sense\u2014higher intervals include too few checkpoints for SWA, lower ones include too many weaker checkpoints from earlier in training. ", "page_idx": 21}, {"type": "text", "text": "Also, we find that the optimal epoch is shifted further at smaller checkpointing intervals (by about 2 epochs when the checkpointing interval is 0.1 epochs), showing that post-hoc reversal is even more important in this setting. This is likely because with more checkpoints being averaged, even more overfitted checkpoints can be accomodated while still increasing the overall performance. ", "page_idx": 21}, {"type": "text", "text": "G Visualizing Post-Hoc Reversal on a Synthetic Dataset ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here, we replicate post-hoc reversal on a synthetic dataset with 2 input features, with the aim of visualizing learnt decision surfaces to solidify our intuitions. ", "page_idx": 21}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/3a7741a32758896b73b265a5066efe3204c057aef0e3d789f7efa0c871ffbd8d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 14: Best MMLU and epoch at which it is achieved vs checkpointing interval, for the LLM instruction tuning setup of $\\S\\,7.1$ . Checkpointing every 0.7 epochs gives the best results. Best epoch is shifted further at smaller checkpointing intervals, i.e. post-hoc reversal is more prominent in this setting. ", "page_idx": 22}, {"type": "text", "text": "Figure 15: The synthetic dataset setup in $\\S\\ \\mathrm{G}$ exhibits post-hoc reversal between epochs 440 and 1000. ", "page_idx": 22}, {"type": "image", "img_path": "3R7Go6WkDm/tmp/63b9cf9311d68c44127268a42c032d56fde8f5adf978f236b2921defb778b262.jpg", "img_caption": ["Figure 16: Decision surfaces of 2 models and the ensemble (of 16 models) on a synthetic 2D dataset of spirals, at epochs 440 and 1000, between which post-hoc reversal occurs (Fig. 15). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "We train 4-layer MLPs with 512 ReLU units per hidden layer on a 2-class spirals dataset of 1000 training examples, with $20\\%$ of the labels flipped at random. We train 16 MLPs and track the mean test error across epochs, as well as the test error of the ensemble (Fig. 15). ", "page_idx": 22}, {"type": "text", "text": "As per [3, 16] ensembling and SWA help when the data has a \"multi-view\" structure, or equivalently, the loss landscape has multiple modes. This is hard to achieve for 2D datasets, so instead we simulate the effect by training each MLP on a random $50\\%$ subsample of the training data. ", "page_idx": 22}, {"type": "text", "text": "Fig. 16 shows decision surfaces at epochs 440 and 1000 for 2 MLPs and the ensemble. Decision boundaries are spiky around noisy examples and smoother around clean ones. While the generalizable parts of the spiral are retained in the ensemble, the effects of noisy examples are diminished. Between epochs 440 and 1000, individual models spike around noisy examples more prominently than they learn new parts of the spiral, but the ensemble surface is relatively unchanged, except for small improvements to learning the spiral. ", "page_idx": 22}, {"type": "text", "text": "This reinforces our intuitions from $\\S\\ S$ that mislabeled examples have a more unstable influence on the decision boundary, and post-hoc transforms exploit this to reduce their impact, while amplifying generalizable patterns learnt from clean examples. ", "page_idx": 22}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/023719c469929ee4e451856b59ecf66f332da3be13941cb0c96bb95196678bb6.jpg", "table_caption": ["Table 9: Naive vs post-hoc (ours) selection for CIFAR-N trained with cross-entropy (CE) loss. Better values are in bold. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/13e8e813a22fb4d8b7fa7048bd17a17eafd27dfea217e98503c6ca269afb30ba.jpg", "table_caption": ["Table 10: Naive vs post-hoc (ours) selection for CIFAR-N trained with SOP loss. Better values are in bold. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "H Noise-Aware Training ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "While our experiments in the main paper use the standard cross-entropy (CE) loss, here we consider two leading training objectives from the label noise literature: (1) SOP [45] and (2) ELR [42]. Tables 9, 10 and 11 compare naive and post-hoc selection strategies for CIFAR-N datasets under CE, SOP and ELR losses respectively. Here again we find that post-hoc selection is superior to naive selection in general. We also note that the differences between CE, SOP and ELR are minimal. This is likely because we use i.i.d. (and therefore noisy) validation and test sets, unlike the original papers which use clean validation and test sets. ", "page_idx": 23}, {"type": "table", "img_path": "3R7Go6WkDm/tmp/d3ebefd993a77678a56f81c0ff5d3fb13e3a4c65daab26e334adaff9a4ff2a7c.jpg", "table_caption": ["Table 11: Naive vs post-hoc (ours) selection for CIFAR-N trained with ELR loss. Better values are in bold. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "I Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We find post-hoc reversal to be an important phenomenon when the base curve exhibits performance degradation due to overfitting. However, under some scenarios, the base curve shows a monotonic improvement in performance with additional training (or increasing model size). Examples include: (1) the data has low noise, (2) the training is heavily regularized, and (3) there is an abundance of data, so that a single data point is not repeated enough to cause overfitting. In such cases, post-hoc selection outcomes are similar to naive selection. Since our suggested approach only ensembles models trained for the same number of epochs during post-hoc selection, it does not subsume the naive selection search space, leading to marginally worse performance sometimes, although this can be easily overcome in practice. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Please see App. I. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Code to reproduce experiments is available at https://anonymous.4open. science/r/post-hoc-reversal. Comprehensive dataset and training details are provided in App. C. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see code (including instructions to download data and reproduce the main results) at https://anonymous.4open.science/r/post-hoc-reversal. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see App. C. In particular, see Tab. 3 for training details. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Tabs. 6 and 8 for detailed results with standard deviations. We don\u2019t report error bars for ensembles as it is computationally prohibitive to train many independent ensembles. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Please see App. C for training details, including compute resources used. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the broader impact of our work in Sec. 8. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All datasets and models used in this paper are under permissive licenses allowing for research use. We have credited their authors by exhaustively citing sources. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]