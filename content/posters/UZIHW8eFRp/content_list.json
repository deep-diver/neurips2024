[{"type": "text", "text": "A Tractable Inference Perspective of Offline RL ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuejie ${\\bf L i u^{1,3*}}$ , Anji ${\\bf L i u}^{2*}$ , Guy Van den Broeck2, Yitao Liang1 ", "page_idx": 0}, {"type": "text", "text": "1Institute for Artificial Intelligence, Peking University 2Computer Science Department, University of California, Los Angeles 3School of Intelligence Science and Technology, Peking University xjliu@stu.pku.edu.cn, liuanji@cs.ucla.edu guyvdb@cs.ucla.edu, yitaol@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A popular paradigm for offilne Reinforcement Learning (RL) tasks is to first fti the offilne trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. In addition to obtaining accurate sequence models, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an important role in offline RL. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates undermine the beneftis brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern tractable generative models to bridge the gap between good sequence models and high expected returns at evaluation time. Empirically, Trifle achieves 7 state-of-the-art scores and the highest average scores in 9 Gym-MuJoCo benchmarks against strong baselines. Further, Trifle significantly outperforms prior approaches in stochastic environments and safe RL tasks with minimum algorithmic modifications. 3 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in deep generative models have opened up the possibility of solving offline Reinforcement Learning (RL) [27] tasks with sequence modeling techniques (termed RvS approaches). Specifically, we first fit a sequence model to the trajectories provided in an offline dataset. During evaluation, the model is tasked to sample actions with high expected returns given the current state. Leveraging modern deep generative models such as GPTs [5] and diffusion models [18], RvS algorithms have significantly boosted the performance on various RL problems [1, 6]. ", "page_idx": 0}, {"type": "text", "text": "Despite its appealing simplicity, it is still unclear whether expressive modeling alone guarantees good performance of RvS algorithms, and if so, on what types of environments. This paper discovers that many common failures of RvS algorithms are not caused by modeling problems. Instead, while useful information is encoded in the model during training, the model is unable to elicit such knowledge during evaluation. Specifically, this issue is reflected in two aspects: (i) inability to accurately estimate the expected return of a state and a corresponding action sequence to be executed given near-perfect learned transition dynamics and reward functions; (ii) even when accurate return estimates exist in the offline dataset and are learned by the model, it could still fail to sample rewarding actions during evaluation.4 At the heart of such inferior evaluation-time performance is the fact that highly non-trivial conditional generation is required to stimulate high-return actions [32, 3]. Therefore, other than expressiveness, the ability to efficiently and exactly answer various queries (e.g., computing the expected returns), termed tractability, plays an equally important role in RvS approaches. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Having observed that the lack of tractability is an essential cause of the underperformance of RvS algorithms, this paper studies whether we can gain practical benefits from using Tractable Probabilistic Models (TPMs) [35, 7, 23], which by design support exact and efficient computation of certain queries? We answer the question in its affirmative by showing that we can leverage a class of TPMs that support computing arbitrary marginal probabilities to significantly mitigate the inference-time suboptimality of RvS approaches. The proposed algorithm Trifle (Tractable Inference for Offilne RL) has three main contributions: ", "page_idx": 1}, {"type": "text", "text": "Emphasizing the important role of tractable models in offline RL. This is the first paper that demonstrates the possibility of using TPMs on complex offilne RL tasks. The superior empirical performance of Trifle suggests that expressive modeling is not the only aspect that determines the performance of RvS algorithms, and motivates the development of better inference-aware RvS approaches. ", "page_idx": 1}, {"type": "text", "text": "Competitive empirical performance. Compared against strong offline RL baselines (including RvS, imitation learning, and offline temporal-difference algorithms), Trifle achieves the state-of-the-art result on 7 out of 9 Gym-MuJoCo benchmarks [14] and has the best average score. ", "page_idx": 1}, {"type": "text", "text": "Generalizability to stochastic environments and safe-RL tasks. Trifle can be extended to tackle stochastic environments as well as safe RL tasks with minimum algorithmic modifications. Specifically, we evaluate Trifle in 2 stochastic OpenAI-Gym [4] environments and action-space-constrained MuJoCo environments, and demonstrate its superior performance against all baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Offline Reinforcement Learning. In Reinforcement Learning (RL), an agent interacts with an environment that is defined by a Markov Decision Process (MDP) $\\langle S,A,\\mathcal{R},\\mathcal{P},d_{0}\\rangle$ to maximize its cumulative reward. Specifically, the $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ is the reward function, $\\mathcal{P}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{S}$ is the transition dynamics, and $d_{0}$ is the initial state distribution. Our goal is to learn a policy $\\pi(a|s)$ that maximizes the expected return $\\mathbb{E}[\\sum_{t=0}^{T}\\gamma^{t}r_{t}]$ , where $\\gamma\\in(0,1]$ is a discount factor and $T$ is the maximum number of steps. ", "page_idx": 1}, {"type": "text", "text": "Offline RL [27] aims to solve RL problems where we cannot freely interact with the environment. Instead, we receive a dataset of trajectories collected using unknown policies. An effective learning paradigm for offline RL is to treat it as a sequence modeling problem (termed RL via Sequence Modeling or RvS methods) [20, 6, 13]. Specifically, we first learn a sequence model on the dataset, and then sample actions conditioned on past states and high future returns. Since the models typically do not encode the entire trajectory, an estimated value or return-to-go (RTG) (i.e., the Monte Carlo estimate of the sum of future rewards) is also included for every state-action pair, allowing the model to estimate the return at any time step. ", "page_idx": 1}, {"type": "text", "text": "Tractable Probabilistic Models. Tractable Probabilistic Models (TPMs) are generative models that are designed to efficiently and exactly answer a wide range of probabilistic queries [35, 7, 37]. One example class of TPMs is Hidden Markov Models (HMMs) [36], which support linear time (w.r.t. model size and input size) computation of marginal probabilities and more. Probabilistic Circuits (PCs) [7] are a general class of TPMs. As shown in Figure 1, PCs consist of input nodes $\\circledcirc$ that represent simple distributions (e.g., Gaussian, Categorical) over one or more variables as well as sum $\\circleddash$ and product $\\circledtimes$ nodes that take other nodes as input and gradually form more complex distributions. Specifically, product nodes model factorized distributions over their inputs (mixture weights are labeled on the corresponding edges Please refer to Appx. B for a more detailed introduction to ", "page_idx": 1}, {"type": "image", "img_path": "UZIHW8eFRp/tmp/e7396d433689d9f07f1edd9b6cfbdee4ad82c15cd67e518f14b1b0d4231e3049.jpg", "img_caption": ["Figure 1: An example PC over boolean variables $X_{1},\\ldots,X_{4}$ . Every node\u2019s probability given input $x_{1}x_{2}\\bar{x_{3}}x_{4}$ is labeled in blue. $p(x_{1}x_{2}\\bar{x_{3}}x_{4})=0.22$ . , and sum nodes build weighted mixtures in Fig. 1) over their input distributions. PCs. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "UZIHW8eFRp/tmp/5f08786fbd88a41510310e0c32e150f533fe9ef87c5459c32d814e5b267e911a.jpg", "img_caption": ["Figure 2: RvS approaches suffer from inference-time suboptimality. Left: There is a strong positive correlation between the average estimated returns by Trajectory Transformers (TT) and the actual returns in 6 Gym-MuJoCo environments (MR, M, and ME denote medium-replay, medium, and medium-expert, respectively), which suggests that the sequence model can distinguish rewarding actions from the others. Middle: Despite being able to recognize high-return actions, both TT and DT [6] fail to consistently sample such action, leading to bad inference-time optimality; Trifle consistently improves the inference-time optimality score. Right: We substantiate the relationship between low inference-time optimality scores and unfavorable environmental outcomes by showing a strong positive correlation between them. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Recent advancements have extensively pushed forward the expressiveness of modern PCs [30, 31, 9], leading to competitive likelihoods on natural image and text datasets compared to even strong Variational Autoencoder [43] and Diffusion model [22] baselines. This paper leverages such advances and explores the benefits brought by PCs in offline RL tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Tractability Matters in Offline RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Practical RvS approaches operate in two main phases \u2013 training and evaluation. In the training phase, a sequence model is adopted to learn a joint distribution over trajectories of length $T$ : $\\bar{\\{}(s_{t},a_{t},r_{t},\\bar{\\mathrm{{RTG}}}_{t})\\}_{t=0}^{T}.$ .5 During evaluation, at every time step $t$ , the model is tasked to discover an action sequence $a_{t:T}:=\\{a_{\\tau}\\}_{\\tau=t}^{T}$ (or just $a_{t}$ ) that has high expected return as well as high probability in the prior policy $p\\big(a_{t:T}\\big|s_{t}\\big)$ , which prevents it from generating out-of-distribution actions: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(a_{t:T}|s_{t},\\mathbb{E}[V_{t}]\\geq v):=\\frac{1}{Z}\\cdot\\left\\{\\!\\!\\begin{array}{l l}{p(a_{t:T}|s_{t})}&{\\mathrm{if}\\;\\mathbb{E}_{V_{t}\\sim p(\\cdot|s_{t},a_{t})}[V_{t}]\\geq v,}\\\\ {0}&{\\mathrm{otherwise},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Z$ is a normalizing constant, $V_{t}$ is an estimate of the value at time step $t$ , and $v$ is a pre-defined scalar chosen to encourage high-return policies. Depending on the problem, $V_{t}$ could be the labeled RTG from the dataset (e.g., $\\mathrm{RTG}_{t}$ ) or the sum of future rewards capped with a value estimate (e.g., $\\begin{array}{r}{\\sum_{\\tau=t}^{T-1}r_{\\tau}+\\mathrm{RTG}_{T})}\\end{array}$ [13, 20]. ", "page_idx": 2}, {"type": "text", "text": "The above definition naturally reveals two key challenges in RvS approaches: (i) training-time optimality (i.e., \u201cexpressivity\u201d): how well can we fit the offline trajectories, and (ii) inference-time optimality: whether actions can be unbiasedly and efficiently sampled from Equation (1). While extensive breakthroughs have been achieved to improve the training-time optimality [1, 6, 20], it remains unclear whether the non-trivial constrained generation task of Equation (1) hinders inferencetime optimality. In the following, we present two general scenarios where existing RvS approaches underperform as a result of suboptimal inference-time performance. We attribute such failures to the fact that these models are limited to answering certain query classes (e.g., autoregressive models can only compute next token probabilities), and explore the potential of tractable probabilistic models for offline RL tasks in the following sections. ", "page_idx": 2}, {"type": "text", "text": "Scenario #1 We first consider the case where the labeled RTG belongs to a (near-)optimal policy. In this case, Equation (1) can be simplified to $p(a_{t}|s_{t},\\mathbb{E}[V_{t}]\\geq v)$ (choose $V_{t}:=\\mathrm{RTG}_{t})$ ) since onestep optimality implies multi-step optimality. In practice, although the RTGs are suboptimal, the predicted values often match well with the actual returns achieved by the agent. Take Trajectory ", "page_idx": 2}, {"type": "text", "text": "Transformer (TT) [20] as an example, Figure 2 (left) demonstrates a strong positive correlation between its predicted returns $\\mathbf{\\dot{X}}$ -axis) and the actual cumulative rewards (y-axis) on six MuJoCo [42] benchmarks, suggesting that the model has learned the \u201cgoodness\u201d of most actions. In such cases, the performance of RvS algorithms depends mainly on their inference-time optimality, i.e., whether they can efficiently sample actions with high predicted returns. Specifically, let $a_{t}$ be the action taken by a RvS algorithm at state $s_{t}$ , and $R_{t}:=\\mathbb{E}[\\mathrm{RTG}_{t}]$ is the corresponding estimated expected value. We define a proxy of inference-time optimality as the quantile value of $R_{t}$ in the estimated state-conditioned value distribution $p(V_{t}|s_{t})$ .6 The higher the quantile value, the more frequent the RvS algorithm samples actions with high estimated returns. ", "page_idx": 3}, {"type": "text", "text": "We evaluate the inference-time optimality of Decision Transformers (DT) [6] and Trajectory Transformers (TT) [20], two widely used RvS algorithms, on various environments and offline datasets from the Gym-MuJoCo benchmark suite [14]. As shown in Figure 2 (middle), the inference-time optimality is averaged (only) around 0.7 (the maximum possible value is 1.0) for most settings. And these runs with low inference-time optimality scores receive low environment returns (Fig. 2 (right)). ", "page_idx": 3}, {"type": "text", "text": "Scenario #2 Achieving inference-time optimality becomes even harder when the labeled RTGs are suboptimal (e.g., they come from a random policy). In this case, even estimating the expected future return of an action sequence becomes highly intractable, especially when the transition dynamics of the environment are stochastic. Specifically, to evaluate a state-action pair $(s_{t},a_{t})$ , since $\\mathrm{RTG}_{t}$ is uninformative, we need to resort to the multi-step estimate $\\begin{array}{r}{V_{t}^{\\mathrm{m}}:=\\sum_{\\tau=t}^{t^{\\prime}-1}r_{\\tau}+}\\end{array}$ $\\mathrm{RTG}_{t^{\\prime}}$ $(t^{\\prime}>t)$ , where the actions $a_{t:t^{\\prime}}$ are jointly chosen to maximize the expected return. Take autoregressive models as an example. Since the variables are arranged following the sequential order $\\cdots,s_{t},a_{t},r_{t}$ , $\\mathrm{RTG}_{t}$ , $\\boldsymbol{s}_{t+1},\\ldots.$ , we need to explicitly sample $s_{t+1:t^{\\prime}}$ before proceed to compute the rewards and the RTG in $V_{t}^{\\mathrm{m}}$ . In stochastic environments, estimating $\\mathbb{E}[V_{t}^{\\mathrm{m}}]$ could suffer from high variance as the stochasticity from the intermediate states accumulates over time. ", "page_idx": 3}, {"type": "text", "text": "As we shall illustrate in Section 6.2, compared to environments with near-deterministic transition dynamics, estimating the expected returns in stochastic environments using intractable sequence models is hard, and Trifle can significantly mitigate this problem with its ability to marginalize out intermediate states and compute $\\mathbb{E}[V_{t}^{\\mathrm{m}}]$ efficiently and exactly. ", "page_idx": 3}, {"type": "text", "text": "4 Exploiting Tractable Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The previous section demonstrates that apart from modeling, inference-time suboptimality is another key factor that causes the underperformance of RvS approaches. Given such observations, a natural follow-up question is whether/how more tractable models can improve the evaluation-time performance in offline RL tasks? While there are different types of tractabilities (i.e., the ability to compute different types of queries), this paper focuses on studying the additional benefit of exactly computing arbitrary marginal/condition probabilities. This strikes a proper balance between learning and inference as we can train such a tractable yet expressive model thanks to recent developments in the TPM community [9, 30]. Note that in addition to proposing a competitive RvS algorithm, we aim to highlight the necessity and benefit of using more tractable models for offline RL tasks, and encourage future developments on both inference-aware RvS methods and better TPMs. As a direct response to the two failing scenarios identified in Section 3, we first demonstrate how tractability could help even when the labeled RTGs are (near-)optimal (Sec. 4.1). We then move on to the case where we need to use multi-step return estimates to account for biases in the labeled RTGs (Sec. 4.2). ", "page_idx": 3}, {"type": "text", "text": "4.1 From the Single-Step Case... ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider the case where the RTGs are optimal. Recall from Section 3 that our goal is to sample actions from $p(a_{t}|s_{t},\\mathbb{E}[V_{t}]\\geq v)$ (where $V_{t}\\!:=\\!\\mathrm{RTG}_{t})$ ). Prior works use two typical ways to approximately sample from this distribution. The first approach directly trains a model to generate return-conditioned actions: $p(a_{t}|s_{t},\\mathrm{RTG}_{t})$ [6]. However, since the RTG given a state-action pair is stochastic,7 sampling from this RTG-conditioned policy could result in actions with a small probability of getting a high return, but with a low expected return [32, 3]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "An alternative approach leverages the ability of sequence models to accurately estimate the expected return (i.e., $\\mathbb{E}[\\mathrm{RTG}_{t}]\\$ ) of state-action pairs [20]. Specifically, we first sample from a prior distribution $p(a_{t}|s_{t})$ , and then reject actions with low expected returns. Such rejection sampling-based methods typically work well when the action space is small (in which we can enumerate all actions) or the dataset contains many high-rewarding trajectories (in which the rejection rate is low). However, the action could be multi-dimensional and the dataset typically contains many more low-return trajectories in practice, rendering the inference-time optimality score low (cf. Fig. 2). ", "page_idx": 4}, {"type": "text", "text": "Having examined the pros and cons of existing approaches, we are left with the question of whether a tractable model can improve sampled actions (in this single-step case). We answer it with a mixture of positive and negative results: while computing $p(a_{t}|s_{t},\\bar{\\mathbb{E}}[V_{t}]\\dot{\\geq}v)$ is NP-hard even when $p(a_{t},V_{t}|s_{t})$ follows a simple Naive Bayes distribution, we can design an approximation algorithm that samples high-return actions with high probability in practice. We start with the negative result. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $a_{t}:=\\{a_{t}^{i}\\}_{i=1}^{k}$ be a set of $k$ boolean variables and $V_{t}$ be a categorical variables with two categories 0 and 1. For some $s_{t}$ , assume the joint distribution over $a_{t}$ and $V_{t}$ conditioned on $s_{t}$ follows a Naive Bayes distribution: $\\begin{array}{r}{p(a_{t},V_{t}|s_{t}):=p(V_{t}|s_{t})\\cdot\\prod_{i=1}^{k}p(a_{t}^{i}|V_{t},s_{t})}\\end{array}$ , where $a_{t}^{i}$ denotes the $i^{t h}$ variable of $a_{t}$ . Computing any marginal over the random variables is tractable yet conditioning on the expectation $p(a_{t}|s_{t},\\mathbb{E}[V_{t}]\\geq v)$ is NP-hard. ", "page_idx": 4}, {"type": "text", "text": "The proof is given in Appx. A. While it seems hard to directly draw samples from $p(a_{t}|s_{t},\\mathbb{E}[V_{t}]\\geq v)$ , we propose to improve the aforementioned rejection sampling-based method by adding a correction term to the original proposal distribution $p(a_{t}|s_{t})$ to reduce the rejection rate. Specifically, the prior is often represented by an autoregressive model such as GPT: $\\begin{array}{r}{p_{\\mathrm{GPT}}(a_{t}|s_{t}):=\\prod_{i=1}^{k}p_{\\mathrm{GPT}}(a_{t}^{i}|s_{t},a_{t}^{<i})}\\end{array}$ , where $k$ is the number of action variables and $a_{t}^{i}$ is the ith variable of $a_{t}$ . We propose to sample every dimension of $a_{t}$ autoregressively following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall i\\in\\{1,\\dots,k\\}\\qquad\\tilde{p}(a_{t}^{i}|s_{t},a_{t}^{<i};v):=\\frac{1}{Z}\\cdot p_{\\mathrm{GPT}}(a_{t}^{i}|s_{t},a_{t}^{<i})\\cdot p_{\\mathrm{TPM}}(V_{t}\\geq v|s_{t},a_{t}^{\\leq i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Z$ is a normalizing constant and $p_{\\mathrm{TPM}}(V_{t}\\!\\geq\\!v|s_{t},a_{t}^{\\le i})$ is a correction term that leverages the ability of the TPM to compute the distribution of $V_{t}$ given incomplete actions (i.e., evidence on a subset of action variables). Note that while Equation (2) is mathematically identical to $p(a_{t}|s_{t},V_{t}\\geq v)$ when $p=p_{\\mathrm{TPM}}=p_{\\mathrm{GPT}}$ , this formulation gives us the flexibility to use the prior policy (i.e., $p_{\\mathrm{GPT}}\\big(a_{t}^{i}|s_{t},a_{t}^{<i}\\big))$ represented by more expressive autoregressive generative models. ", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 2 (middle), compared to using $p(a_{t}|s_{t})$ (as done by TT), the inference-time optimality scores increase significantly when using the distribution specified by Equation (2) (as done by Trifle) across various Gym-MuJoCo benchmarks. ", "page_idx": 4}, {"type": "text", "text": "4.2 ...To the Multi-Step Case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Recall that when the labeled RTGs are suboptimal, our goal is to sample from $p\\big(a_{t:t^{\\prime}}|s_{t},\\mathbb{E}[V_{t}^{\\mathrm{m}}]\\!\\geq\\!v\\big)$ , where V tm := t\u03c4=\u2212t1 $\\begin{array}{r}{V_{t}^{\\mathrm{m}}{\\mathrel{\\mathop:}}{=}\\sum_{\\tau=t}^{t^{\\prime}-1}r_{\\tau}{+}\\mathrm{RTG}_{t^{\\prime}}}\\end{array}$ is the multi-step value estimate. However, as shown in the second scenario in Section 3, it is hard even to evaluate the expected return of an action sequence due to the inability to marginalize out intermediate states $s_{t+1:t^{\\prime}}$ . Empowered by PCs, we can solve this problem by computing the expectation efficiently as it can be broken down into computing conditional probabilities $p(\\bar{r}_{\\tau}|s_{t},\\bar{a}_{t:t^{\\prime}})(t\\!\\leq\\!\\tau\\!<\\!t^{\\prime})$ and $p(\\mathrm{R}\\mathrm{{TG}}_{t^{\\prime}}|s_{t},a_{t:t^{\\prime}})$ (see Appx. C.2 for details): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\!\\left[V_{t}^{\\mathrm{m}}\\right]=\\sum_{\\tau=t}^{t^{\\prime}-1}\\mathbb{E}_{r_{\\tau}\\sim p\\left(\\cdot\\mid s_{t},a_{t:t^{\\prime}}\\right)}\\left[r_{\\tau}\\right]+\\mathbb{E}_{\\mathrm{RTG}_{t^{\\prime}}\\sim p\\left(\\cdot\\mid s_{t},a_{t:t^{\\prime}}\\right)}\\left[\\mathrm{RTG}_{t^{\\prime}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We are now left with the same problem discussed in the single-step case \u2013 how to sample actions with high expected returns (i.e., $\\mathbb{E}[V_{t}^{\\mathrm{m}}]\\!\\!\\}$ ). Similar to Equation (2), we add correction terms that bias the action (sequence) distribution towards high expected returns. Specifically, we augment the original action probability $\\textstyle\\prod_{\\tau=t}^{t^{\\prime}}p(a_{\\tau}|s_{t},a_{<\\tau})$ with terms of the form $p(V_{t}^{\\mathrm{m}}\\!\\geq\\!v|s_{t},a_{\\leq\\tau})$ This leads to: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{p}(a_{t:t^{\\prime}}|s_{t};v)\\!:=\\!\\prod_{\\tau=t}^{t^{\\prime}}\\tilde{p}(a_{\\tau}|s_{t},a_{<\\tau};v),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/3000dc9cdd3dc5cdbc686e833516c903eb244299f505fe5e8ed6d44ad0ed77ea.jpg", "table_caption": ["Table 1: Normalized Scores on the standard Gym-MuJoCo benchmarks. The results of Trifle are averaged over 12 random seeds (For DT-base and DT-Trifle, we adopt the same number of seeds as [6]). Results of the baselines are acquired from their original papers. "], "table_footnote": ["where $\\begin{array}{r}{\\tilde{p}(a_{\\tau}|s_{t},a_{<\\tau};v)\\propto p(a_{\\tau}|s_{t},a_{<\\tau})\\cdot p(V_{t}^{\\mathrm{m}}\\!\\geq\\!v|s_{t},a_{\\le\\tau}),}\\end{array}$ , $a_{<\\tau}$ and $a_{\\le\\tau}$ represent $a_{t:\\tau-1}$ and $a_{t:\\tau}$ , respectively.8 In practice, while we compute $p(V_{t}^{\\mathrm{m}}\\,\\geq\\,v|s_{t},a_{\\leq\\tau})$ using the PC, $p(a_{\\tau}|s_{t},a_{<\\tau})\\,=$ $\\mathbb{E}_{s_{t+1:\\tau}}[p(\\stackrel{.}{a_{\\tau}}|s_{\\leq\\tau},\\stackrel{.}{a_{<\\tau}})]$ can either be computed exactly with the TPM or approximated (via Monte Carlo estimation over $s_{t+1:\\tau})$ using an autoregressive generative model. In summary, we approximate samples from $p(a_{t:t^{\\prime}}|s_{t},\\mathbb{E}[V_{t}]\\ge v)$ by first sampling from $\\tilde{p}(a_{t:t^{\\prime}}|s_{t};v)$ , and then rejecting samples whose (predicted) expected return is smaller than $v$ . "], "page_idx": 5}, {"type": "text", "text": "5 Practical Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The previous section has demonstrated how to efficiently sample from the expected-value-conditioned policy (Eq. (1)). Based on this sampling algorithm, this section further introduces the proposed algorithm Trifle (Tractable Inference for Offilne RL). The high-level idea of Trifle is to obtain good action (sequence) candidates from $p(a_{t}|s_{t},\\mathbb{E}[V]\\ge v)$ , and then use beam search to further single out the most rewarding action. Intuitively, by the definition in Equation (1), the candidates are both rewarding and have relatively high likelihoods in the offline dataset, which ensures the actions are within the offline data distribution and prevents overconfident estimates during beam search. ", "page_idx": 5}, {"type": "text", "text": "Beam search maintains a set of $N$ (incomplete) sequences each starting as an empty sequence. For ease of presentation, we assume the current time step is 0. At every time step $t$ , beam search replicates each of the $N$ actions sequences into $\\lambda\\in\\mathbb{Z}^{\\bar{+}}$ copies and appends an action $a_{t}$ to every sequence. Specifically, for every partial action sequence $a_{<t}$ , we sample an action following $p(a_{t}|s_{0},a_{<t},\\mathbb{E}[V_{t}]\\geq v)$ , where $V_{t}$ can be either the single-step or the multi-step estimate depending on the task. Now that we have $\\lambda\\cdot N$ trajectories in total, the next step is to evaluate their expected return, which can be computed exactly using the PC (see Sec. 4.2). The $N$ -best action sequences are kept and proceed to the next time step. After repeating this procedure for $H$ time steps, we return the best action sequence. The first action in the sequence is used to interact with the environment. Please refer to Appx. C for detailed descriptions of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "Another design choice is the threshold value $v$ . While it is common to use a fixed high return throughout the episode, we follow [12] and use an adaptive threshold. Specifically, at state $s_{t}$ , we choose $v$ to be the $\\epsilon$ -quantile value of $p\\big(V_{t}|s_{t}\\big)$ , which is computed using the PC. ", "page_idx": 5}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section takes gradual steps to study whether Trifle can mitigate the inference-time suboptimality problem in different settings. First, in the case where the labeled RTGs are good performance indicators (i.e., the single-step case), we examine whether Trifle can consistently sample more rewarding actions (Sec. 6.1). Next, we further challenge Trifle in highly stochastic environments, where existing RvS algorithms fail catastrophically due to the failure to account for the environmental randomness (Sec. 6.2). Finally, we demonstrate that Trifle can be directly applied to safe RL tasks (with action constraints) by effectively conditioning on the constraints (Sec. 6.3). Collectively, this section highlights the potential of TPMs on offline RL tasks. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.1 Comparison to the State of the Art ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As demonstrated in Section 3 and Figure 2, although the labeled RTGs in the Gym-MuJoCo [14] benchmarks are accurate enough to reflect the actual environmental return, existing RvS algorithms fail to effectively sample such actions due to their large and multi-dimensional action space. Figure 2 (middle) has demonstrated that Trifle achieves better inference-time optimality. This section further examines whether higher inference-time optimality scores could consistently lead to better performance when building Trifle on top of different RvS algorithms, i.e., combining $p_{\\mathrm{TPM}}$ (cf. Eq. (2)) with different prior policies $p_{\\mathrm{GPT}}$ trained by the corresponding RvS algorithm. ", "page_idx": 6}, {"type": "text", "text": "Environment setup The Gym-MuJoCo benchmark suite collects trajectories in 3 locomotion environments (HalfCheetah, Hopper, Walker2D) and constructs 3 datasets (Medium-Expert, Medium, Medium-Replay) for every environment, which results in $3\\times3=9$ tasks. For every environment, the main difference between the datasets is the quality of its trajectories. Specifically, the dataset \u201cMedium\" records 1 million steps collected from a Soft Actor-Critic (SAC) [16] agent. The \u201cMediumReplay\" dataset adopts all samples in the replay buffer recorded during the training process of the SAC agent. The \u201cMedium-Expert\" dataset mixes 1 million steps of expert demonstrations and 1 million suboptimal steps generated by a partially trained SAC policy or a random policy. The results are normalized such that a well-trained SAC model hits 100 and a random policy has a 0 score. ", "page_idx": 6}, {"type": "text", "text": "Baselines We build Trifle on top of three effective RvS algorithms: Decision Transformer (DT) [6], Trajectory Transformer (TT) [20] as well as its variant $\\mathrm{TT}(+\\mathrm{Q})$ where the RTGs estimated by summing up future rewards in the trajectory are replaced by the Q-values generated by a well-trained IQL agent [24]. In addition to the above base models, we also compare Trifle against many other strong baselines: (i) Decision Diffuser (DD) [1], which is also a competitive RvS method; (ii) Offilne TD learning methods IQL [24] and CQL [26]; (iii) Imitation learning methods like the variant of BC [34] which only uses $10\\%$ of trajectories with the highest return, and TD3 $\\scriptstyle\\left(+\\mathrm{{BC}}\\right)$ [15]. ", "page_idx": 6}, {"type": "text", "text": "Since the labeled RTGs are informative enough about the \u201cgoodness\u201d of actions, we implement Trifle by adopting the single-step value estimate following Section 4.1, where we replace $p_{\\mathrm{GPT}}$ with the policy of the three adopted base methods, i.e., $p_{\\mathrm{TT}}\\bar{(a_{t}\\vert s_{t})}$ , $p_{\\mathrm{TT(+Q)}}(a_{t}|s_{t})$ and $p_{\\mathrm{DT}}\\big(a_{t}|s_{t}\\big)$ . ", "page_idx": 6}, {"type": "text", "text": "Empirical Insights Results are shown in Table 1.9 First, to examine the benefti brought by TPMs, we compare Trifle with three base policies, as the main algorithmic difference is the use of the improved proposal distribution (Eq. (2)) for sampling actions. We can see that Trifle not only achieves a large performance gain over TT and DT in all environments, but also significantly outperforms $\\mathrm{TT}(+\\mathrm{Q})$ where we have access to more accurate labeled values, indicating that Trifle can enhance the inference-time optimality of base policy reliably and benefit from any improvement of the training-time optimality. See Appx. E.1 for more results and ablation studies. ", "page_idx": 6}, {"type": "text", "text": "Moreover, compared with all baselines, Trifle achieves the highest average score of 83.1. It also succeeds in achieving 7 state-of-the-art scores out of 9 benchmarks. We conduct further ablation studies on the rejection sampling component and the adaptive thresholding component (i.e., selecting $v$ ) in Appx. F. ", "page_idx": 6}, {"type": "text", "text": "6.2 Evaluating Trifle in Stochastic Environments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section further challenges Trifle on stochastic environments with highly suboptimal trajectories as well as labeled RTGs in the offilne dataset. As demonstrated in Section 3, in this case, it is even hard to obtain accurate value estimates due to the stochasticity of transition dynamics.Section 4.2 demonstrates the potential of Trifle to more reliably estimate and sample action sequences under suboptimal labeled RTGs and stochastic environments. This section examines this claim by comparing the five following algorithms: ", "page_idx": 6}, {"type": "image", "img_path": "UZIHW8eFRp/tmp/3334d74f4dda164cc551c1aeb2f2d65ff73a0187636682aa7f88a6963f7da988.jpg", "img_caption": ["Figure 3: (a) Stochastic Taxi environment; (b) Stochastic FrozenLake Environment; (c) Average returns on the stochastic environment. All the reported numbers are averaged over 1000 trials. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "(i) Trifle that adopts $V_{t}=\\mathrm{RTG}_{t}$ (termed single-step Trifle or s-Trifle); (ii) Trifle equipped with $\\begin{array}{r}{V_{t}=\\sum_{\\tau=t}^{t^{\\prime}}r_{\\tau}+\\mathrm{RTG}_{t^{\\prime}}}\\end{array}$ (termed multi-step Trifle or m-Trifle; see Appx. E.2 for additional details); (iii) T T [20]; (iv) DT [6] (v) Dichotomy of Control (DoC) [47], an effective framework to deal with highly stochastic environments by designing a mutual information constraint for DT training, which is a representative baseline while orthogonal to our efforts. ", "page_idx": 7}, {"type": "text", "text": "We evaluate the above algorithms on two stochastic Gym environments: Taxi and FrozenLake. Here we choose the Taxi benchmark for a detailed analysis of whether and how Trifle could overcome the challenges discussed in Section 4.2. Among the first four algorithms, s-Trifle and DT do not compute the \u201cmore accurate\u201d multi-step value, and TT approximates the value by Monte Carlo samples. Therefore, we expect their relative performance to be DT $\\approx\\mathbf{S}$ -Trifle $<\\mathrm{TT}<\\mathrm{m}$ -Trifle. ", "page_idx": 7}, {"type": "text", "text": "Environment setup We create a stochastic variant of the Gym-Taxi Environment [11]. As shown in Figure 3a, a taxi resides in a grid world consisting of a passenger and a destination. The taxi is tasked to first navigate to the passenger\u2019s position and pick them up, and then drop them off at the destination.There are 6 discrete actions available at every step: (i) 4 navigation actions (North, South, East, or West), (ii) Pick-up, (iii) Drop-off. Whenever the agent attempts to execute a navigation action, it has 0.3 probability of moving toward a randomly selected unintended direction. At the beginning of every episode, the location of the taxi, the passenger, and the destination are randomly initialized randomly. The reward function is defined as follows: (i) -1 for each action undertaken; (ii) an additional $+20$ for successful passenger delivery; (iii) -4 for hitting the walls; (iv) -5 for hitting the boundaries; (v) -10 for executing Pick-up or Drop-off actions unlawfully (e.g., executing Drop-off when the passenger is not in the taxi). ", "page_idx": 7}, {"type": "text", "text": "Following the Gym-MuJoCo benchmarks, we collect offline trajectories by running a Q-learning agent [45] in the Taxi environment and recording the first 1000 trajectories that drop off the passenger successfully, which achieves an average return of -128. ", "page_idx": 7}, {"type": "text", "text": "Empirical Insights We first examine the accuracy of estimated returns for s-Trifle, m-Trifle, and TT. DT is excluded since it does not explicitly estimate the value of action sequences. Figure 4 illustrates the correlation between predicted and ground-truth returns of the three methods. First, s-Trifle performs the worst since it merely uses the inaccurate $\\mathrm{RTG}_{t}$ to approximate the ground-truth return. Next, thanks to its ability to exactly compute the multi-step value estimates, m-Trifle outperforms TT, which approximates the multi-step value with Monte Carlo samples. ", "page_idx": 7}, {"type": "text", "text": "We proceed to evaluate their performance in the stochastic Taxi environment. As shown in Figure 3c, the relative performance of the first four algorithms is $\\mathrm{DT}<\\mathrm{TT}<\\mathrm{s}$ -Trifle $<\\mathbf{m}$ -Trifle, which largely aligns with the anticipated results. The only \u201csurprising\u201d result is the superior performance of s-Trifle compared to TT. One plausible explanation for this behavior is that while TT can better estimate the given actions, it fails to efficiently sample rewarding actions. ", "page_idx": 7}, {"type": "text", "text": "Notably, Trifle also significantly outperforms the strong baseline DoC, demonstrating its potential in handling stochastic transitions. To verify this, we further evaluate Trifle on the stochastic FrozenLake environment. Apart from fixing the stochasticity level $\\textstyle p={\\frac{1}{3}}$ ,10 the experiment design follows the DoC paper [47]. For data collection, we perturb the policy of a well-trained DQN (with an average return of 0.7) with the $\\epsilon$ -greedy strategy. Here $\\epsilon$ is a proxy of offline dataset quality and varies from 0.3 to 0.7. As shown in Figure 3c, when the offline dataset contains many successful trials $(\\epsilon=0.3)$ ), all methods perform closely to the optimal policy. As the rollout policy becomes more suboptimal (with the increase of $\\epsilon$ ), the performances of DT and TT drop quickly, while Trifle still works robustly and outperforms all baselines. ", "page_idx": 7}, {"type": "image", "img_path": "UZIHW8eFRp/tmp/023b8f6107d77379a27b0e886b4a1443b35f8137cb997dcb71da347254274aad.jpg", "img_caption": ["Figure 4: Correlation between average estimated returns and true environmental returns for s-Trifle (w/ single-step value estimates), TT, and m-Trifle (w/ multi-step value estimates) in the stochastic Taxi domain. $R$ denotes the correlation coefficient. The results demonstrate that (i) multi-step value estimates (TT and $\\mathbf{m}$ -Trifle) are better than single-step estimates (s-Trifle), and (ii) exactly computed multi-step estimates (m-Trifle) are better than approximated ones (TT) in stochastic environments. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.3 Action-Space-Constrained Gym-MuJoCo Variants ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section demonstrates that Trifle can be readily extended to safe RL tasks by leveraging TPM\u2019s ability to compute conditional probabilities. Specifically, besides achieving high expected returns, safe RL tasks require additional constraints on the action or states to be satisfied. Therefore, define the constraint as $c$ , our goal is to sample actions from $p(a_{t}|s_{t},\\mathbb{E}[V_{t}]\\geq\\bar{v},c)$ , which can be achieved by conditioning on $c$ in the candidate action sampling process. ", "page_idx": 8}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/89c3bd62d9055e77a6cba5a7ced3a2c3bccce756cdb86c6faf8f2bdef99f049f.jpg", "table_caption": ["Table 2: Normalized Scores on the Action-SpaceConstrained Gym-MuJoCo Variants. The results of Trifle and TT are both averaged over 12 random seeds, with mean and standard deviations reported. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Environment setup In MuJoCo environments, each dimension of $a_{t}$ represents the torque applied on a certain rotor of the hinge joints at timestep $t$ . We consider action space constraints in the form of \u201cvalue of the torque applied to the foot rotor $\\le A^{\\,\\circ}$ , where $A=0.5$ is a threshold value, for three MuJoCo environments: Halfcheetah, Hopper, and Walker2d. Note that there are multiple foot joints in Halfcheetah and Walker2d, so the constraint is applied to multiple action dimensions.11 For all settings, we adopt the \u201cMed-Expert\u201d offline dataset as introduced in Section 6.1. ", "page_idx": 8}, {"type": "text", "text": "Empirical Insights The key challenge in these action-constrained tasks is the need to account for the constraints applied to other action dimensions when sampling the value of some action variable. For example, autoregressive models cannot take into account constraints added to variable $a_{t}^{i+1}$ when sampling $a_{t}^{i}$ . Therefore, while enforcing the action constraint is simple, it remains hard to simultaneously guarantee good performance. As shown in Table 2, owing to its ability to exactly condition on the action constraints, Trifle outperforms TT significantly across all three environments. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In offilne reinforcement learning tasks, our goal is to utilize a dataset collected by unknown policies to derive an improved policy without further interactions with the environment. Under this paradigm, we wish to generalize beyond naive imitation learning and stitch good parts of the behavior policy. To pursue such capabilities, many recent works frame offilne RL tasks as conditional modeling problems that generate actions with high expected returns [6, 1, 12] or its proxies such as immediate rewards [25, 39, 40]. Recent advances in this line of work can be highly credited to the powerful expressivity of modern sequence models, since by accurately fitting past experiences, we can obtain 2 types of information that potentially imply high expected returns: (i) transition dynamics of the environment, which serves as a necessity for planning in model-based fashion [8], (ii) a decent policy prior which acts more reasonably than a random policy to improve from [20]. ", "page_idx": 9}, {"type": "text", "text": "While prior works on model-based RL (MBRL) also leverage models of the transition dynamics and the reward function [21, 17, 2], RvS approaches focus more on directly modeling the correlation between actions and their end-performance. Specifically, MBRL approaches focus on planning only with the environment model. Despite being theoretically appealing, MBRL requires heavy machinery to account for the accumulated errors during rollout [19, 41] and out-of-distribution problems [48, 38]. All these problems add a significant burden on the inference side, which makes MBRL algorithms less appealing in practice. In contrast, while RvS algorithms can mitigate this inference-time burden by directly learning the correlation between actions and returns, the suboptimality of labeled returns could degrade their performance. One potential solution is to combine RvS algorithms with temporaldifference learning to correct errors in the labeled returns [49, 46]. ", "page_idx": 9}, {"type": "text", "text": "While also aiming to mitigate the problem caused by suboptimal labeled RTGs, our work takes a different route \u2014 by leveraging TPMs to mitigate the inference-time computational burden. Specifically, we identified major problems caused by the lack of tractability in the sequence models, and show that with the ability to compute more queries efficiently, we can partially solve both identified problems. ", "page_idx": 9}, {"type": "text", "text": "Limitations. One major limitation of Trifle is its dependency on expressive TPMs trained on sequential data \u2014 if the TPMs are inaccurate, then Trifle will also have inferior performance. Another limitation is that current implementations of PCs are not as efficient as neural network packages, which could slow down the execution of Trifle. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was funded in part by the National Science and Technology Major Project (2022ZD0114902), DARPA ANSR program under award FA8750-23-2-0004, the DARPA PTG Program under award HR00112220005, and NSF grant #IIS-1943641. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh International Conference on Learning Representations, 2022.   \n[2] Brandon Amos, Samuel Stanton, Denis Yarats, and Andrew Gordon Wilson. On the model-based stochastic value gradient for continuous reinforcement learning. In Learning for Dynamics and Control, pages 6\u201320. PMLR, 2021.   \n[3] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? Advances in Neural Information Processing Systems, 35:1542\u20131553, 2022.   \n[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [6] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021. [7] YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying framework for tractable probabilistic models. oct 2020.   \n[8] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in neural information processing systems, 31, 2018.   \n[9] Alvaro HC Correia, Gennaro Gala, Erik Quaeghebeur, Cassio de Campos, and Robert Peharz. Continuous mixtures of tractable probabilistic models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7244\u20137252, 2023.   \n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[11] Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition. Journal of artificial intelligence research, 13:227\u2013303, 2000.   \n[12] Wenhao Ding, Tong Che, Ding Zhao, and Marco Pavone. Bayesian reparameterization of rewardconditioned reinforcement learning with energy-based models. arXiv preprint arXiv:2305.11340, 2023.   \n[13] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offilne rl via supervised learning? In International Conference on Learning Representations, 2021.   \n[14] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020.   \n[15] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052\u20132062. PMLR, 2019.   \n[16] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.   \n[17] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. Advances in neural information processing systems, 28, 2015.   \n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[19] Taher Jafferjee, Ehsan Imani, Erin Talvitie, Martha White, and Micheal Bowling. Hallucinating value: A pitfall of dyna-style planning with imperfect environment models. arXiv preprint arXiv:2006.04363, 2020.   \n[20] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273\u20131286, 2021.   \n[21] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.   \n[22] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n[23] Doga Kisa, Guy Van den Broeck, Arthur Choi, and Adnan Darwiche. Probabilistic sentential decision diagrams. In Fourteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2014.   \n[24] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \n[25] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint arXiv:1912.13465, 2019.   \n[26] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative $\\mathbf{q}$ -learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.   \n[27] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[28] Anji Liu, Mathias Niepert, and Guy Van den Broeck. Image inpainting via tractable steering of diffusion models. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[29] Anji Liu and Guy Van den Broeck. Tractable regularization of probabilistic circuits. Advances in Neural Information Processing Systems, 34:3558\u20133570, 2021.   \n[30] Anji Liu, Honghua Zhang, and Guy Van den Broeck. Scaling up probabilistic circuits by latent variable distillation. In The Eleventh International Conference on Learning Representations, 2022.   \n[31] Xuejie Liu, Anji Liu, Guy Van den Broeck, and Yitao Liang. Understanding the distillation process from deep generative models to tractable probabilistic circuits. In International Conference on Machine Learning, pages 21825\u201321838. PMLR, 2023.   \n[32] Keiran Paster, Sheila McIlraith, and Jimmy Ba. You can\u2019t count on luck: Why decision transformers and rvs fail in stochastic environments. Advances in Neural Information Processing Systems, 35:38966\u201338979, 2022.   \n[33] Robert Peharz, Robert Gens, Franz Pernkopf, and Pedro Domingos. On the latent variable interpretation in sum-product networks. IEEE transactions on pattern analysis and machine intelligence, 39(10):2030\u20132044, 2016.   \n[34] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1, 1988.   \n[35] Hoifung Poon and Pedro Domingos. Sum-product networks: a new deep architecture. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 337\u2013346, 2011.   \n[36] Lawrence Rabiner and Biinghwang Juang. An introduction to hidden markov models. ieee assp magazine, 3(1):4\u201316, 1986.   \n[37] Tahrima Rahman, Prasanna Kothalkar, and Vibhav Gogate. Cutset networks: A simple, tractable, and scalable approach for improving the accuracy of chow-liu trees. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II 14, pages 630\u2013645. Springer, 2014.   \n[38] Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based offline reinforcement learning. Advances in neural information processing systems, 35:16082\u201316097, 2022.   \n[39] Juergen Schmidhuber. Reinforcement learning upside down: Don\u2019t predict rewards\u2013just map them to actions. arXiv preprint arXiv:1912.02875, 2019.   \n[40] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja\u00b4skowski, and J\u00fcrgen Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877, 2019.   \n[41] Erin Talvitie. Self-correcting models for model-based reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \n[42] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \n[43] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. Advances in neural information processing systems, 33:19667\u201319679, 2020.   \n[44] Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, and Guy Van den Broeck. A compositional atlas of tractable circuit operations for probabilistic inference. In Advances in Neural Information Processing Systems 34 (NeurIPS), dec 2021.   \n[45] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279\u2013292, 1992.   \n[46] Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl. In International Conference on Machine Learning, pages 38989\u201339007. PMLR, 2023.   \n[47] Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Dichotomy of control: Separating what you can control from what you cannot. arXiv preprint arXiv:2210.13435, 2022.   \n[48] Mingde Zhao, Zhen Liu, Sitao Luan, Shuyuan Zhang, Doina Precup, and Yoshua Bengio. A consciousnessinspired planning agent for model-based reinforcement learning. Advances in neural information processing systems, 34:1569\u20131581, 2021.   \n[49] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In international conference on machine learning, pages 27042\u201327059. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To improve the clarity of the proof, we first simplify the notations in Thm. 1: define $\\mathbf{X}$ as the boolean action variables $a_{t}:=\\{a_{t}^{i}\\}_{i=1}^{k}$ , and $Y$ as the variable $V_{t}$ , which is a categorical variable with two categories 0 and 1. We can equivalently interpret $Y$ as a boolean variable where the category 0 corresponds to $\\boldsymbol{\\mathrm{F}}$ and 1 corresponds to $\\mathbb{T}$ . Dropping the condition on $s_{t}$ everywhere for notation simplicity, we have converted the problem into the following one: ", "page_idx": 13}, {"type": "text", "text": "Assume boolean variables $\\mathbf{X}:=\\{X_{i}\\}_{i=1}^{k}$ and $Y$ follow a Naive Bayes distribution: $p(\\pmb{x},y):=$ $p(y)\\cdot\\prod_{i}p(x_{i}|y)$ . We want to prove that computing $p(\\mathbf{\\boldsymbol{x}}|\\mathbb{E}[y]\\geq v)$ , which is defined as follows, is NP-hard. ", "page_idx": 13}, {"type": "equation", "text": "$$\np(\\pmb{x}|\\mathbb{E}[y]\\geq v):=\\frac{1}{Z}\\left\\{p(\\pmb{x})\\begin{array}{l l}{\\mathrm{if}\\;\\mathbb{E}_{y\\sim p(\\cdot|x)}[y]\\geq v,}\\\\ {\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the definition of $Y$ as a categorical variable with two categories 0 and 1, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{y\\sim p(\\cdot|x)}[y]=p(y=\\mathbb{T}|x)\\cdot1+p(y=\\mathbb{F}|x)\\cdot0=p(y=\\mathbb{T}|x).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, we can rewrite $p(\\mathbf{\\boldsymbol{x}}|\\mathbb{E}[y]\\geq v)$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\np(\\pmb{x}|\\mathbb{E}[y]\\geq v):=\\frac{1}{Z}\\cdot p(\\pmb{x})\\cdot\\mathbb{1}[p(y=\\mathbb{T}|\\pmb{x})\\geq v],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbb{I}[\\cdot]$ is the indicator function. In the following, we show that computing the normalizing constant $\\begin{array}{r}{\\bar{Z}:=\\sum_{x}p(\\pmb{x})\\cdot\\mathbb{1}[p(y=\\mathbb{T}|\\pmb{x})\\geq v]}\\end{array}$ is NP-hard by reduction from the number partition problem, which  is a known NP-hard problem. Specifically, for a set of $k$ numbers $n_{1},\\ldots,n_{k}$ $(\\forall i,n_{i}\\in$ $\\bar{\\mathbb{Z}}^{+}.$ ), the number partition problem aims to decide whether there exists a subset $S\\subseteq[k]$ (define $[k]:=\\{1,\\ldots,k\\}\\,$ ) that partition the numbers into two sets with equal sums: $\\textstyle\\sum_{i\\in S}n_{i}={\\dot{\\sum_{j\\notin S}n_{j}}}$ . ", "page_idx": 13}, {"type": "text", "text": "For every number partition problem $\\{n_{i}\\}_{i=1}^{k}$ , we define a corresponding Naive Bayes distribution $p(\\pmb{x},y)$ with the following parameterization: $p(y=\\mathbb{T})=0.5$ and12 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall i\\in[k],\\ p(x_{i}=\\mathtt{T}|y=\\mathtt{T})=\\frac{1-e^{-n_{i}}}{e^{n_{i}}-e^{-n_{i}}}\\ \\ \\mathrm{and}\\ \\ p(x_{i}=\\mathtt{T}|y=\\mathtt{F})=e^{n_{i}}\\cdot\\frac{1-e^{-n_{i}}}{e^{n_{i}}-e^{-n_{i}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is easy to verify that the above definitions lead to a valid Naive Bayes distribution. Further, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall i\\in[k],\\,\\,\\log\\frac{p(x_{i}=\\Upsilon|y=\\Upsilon)}{p(x_{i}=\\Upsilon|y=\\Upsilon)}=n_{i}\\,\\,\\,\\mathrm{and}\\,\\,\\,\\log\\frac{p(x_{i}=\\Upsilon|y=\\Upsilon)}{p(x_{i}=\\Upsilon|y=\\Upsilon)}=-n_{i}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We pair every partition $S$ in the number partition problem with an instance $\\textbf{\\em x}$ such that $\\forall i$ , $x_{i}=\\mathbb{T}$ if $i\\in S$ and $x_{i}=\\mathbf{F}$ otherwise. Choose $v=2/3$ , the normalizing constant $Z$ can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\nZ=\\sum_{\\pmb{x}\\in\\mathsf{v a l}(\\mathbf{X})}p(\\pmb{x})\\cdot\\mathbb{1}\\left[p(y=\\mathbb{T}|\\pmb{x})\\geq2/3\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall the one-to-one correspondence between $S$ and $\\textbf{\\em x}$ , we rewrite $p(y=\\mathbb{T}|\\pmb{x})$ with the Bayes formula: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=\\mathbb{T}|x)=\\frac{p\\left(y=\\mathbb{T}\\right)\\prod_{i}p\\left(x_{i}|y=\\mathbb{T}\\right)}{p\\left(y=\\mathbb{T}\\right)\\prod_{i}p\\left(x_{i}|y=\\mathbb{T}\\right)+p\\left(y=\\mathbb{F}\\right)\\prod_{i}p\\left(x_{i}|y=\\mathbb{F}\\right)},}\\\\ &{\\qquad\\qquad=\\frac{1}{1+e^{-\\sum_{i}\\log\\frac{p\\left(x_{i}|y=\\mathbb{T}\\right)}{p\\left(x_{i}|y=\\mathbb{F}\\right)}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "12Note that we assume the naive Bayes model is parameterized using log probabilities. ", "page_idx": 13}, {"type": "equation", "text": "$$\n=\\frac{1}{1+e^{-(\\sum_{i\\in S}n_{i}-\\sum_{j\\notin S}n_{j})}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last equation follows from Equation (5). After some simplifications, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{1}\\big[p(y=\\mathsf{T}|x)\\ge2/3\\big]=\\mathbb{1}\\big[\\sum_{i\\in S}n_{i}-\\sum_{j\\notin S}n_{j}\\ge1\\big].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plug back to Equation (6), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z=\\displaystyle\\sum_{S\\subseteq[k]}p(\\pmb{x})\\cdot\\mathbb{1}\\big[\\sum_{i\\in S}n_{i}-\\displaystyle\\sum_{j\\notin S}n_{j}\\geq1\\big],}\\\\ &{\\quad=\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{S\\subseteq[k]}p(\\pmb{x})\\cdot\\mathbb{1}\\big[\\sum_{i\\in S}n_{i}-\\displaystyle\\sum_{j\\notin S}n_{j}\\not=0\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last equation follows from the fact that (i) if $\\textbf{\\em x}$ satisfy $\\begin{array}{r}{\\sum_{i\\in S}n_{i}-\\sum_{j\\notin S}n_{j}\\geq1}\\end{array}$ then $\\bar{\\pmb{x}}$ has $\\begin{array}{r}{\\sum_{i\\in S}n_{i}-\\sum_{j\\notin S}n_{j}\\le-1}\\end{array}$ and vise versa, and (ii) $\\sum_{i\\in S}n_{i}-\\sum_{j\\notin S}n_{j}$ must be an integer. Note that for every solution $S$ to the number partition problem, $\\textstyle\\sum_{i\\in S}n_{i}-\\sum_{j\\notin S}n_{j}=0$ holds. Therefore, there exists a solution to the defined number partition problem if Z < 21. \u25a1 ", "page_idx": 14}, {"type": "text", "text": "B Introduction to Probabilistic Circuits ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Probabilistic circuits (PCs) represent a wide class of TPMs that model probability distributions with a parameterized directed acyclic computation graph (DAG). Specifically, a PC $p(\\mathbf{X})$ defines a joint distribution over a set of random variables $\\mathbf{X}$ by a single root node $n_{r}$ . A PC contains three kinds of computational nodes: input $\\circledcirc$ , sum $\\circleddash$ , and product $\\otimes$ . The example PC in Figure 1 defines a joint distribution over 4 random variables $X_{1},X_{2},X_{3},X_{4}$ . Each leaf node in the DAG serves as an input node that encodes a univariate distribution (e.g., Guassian, Categorical), while sum nodes or product nodes are inner nodes, distinguished by whether they are doing mixture or factorization over their child distributions (denoted $\\mathfrak{i n}(n))$ . Formally, PCs define probability distributions in the following recursive way: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{n}(\\pmb{x}):=\\left\\{\\sum_{c\\in\\mathrm{in}(n)}\\theta_{n,c}\\cdot p_{c}(\\pmb{x})\\begin{array}{l l}&{\\mathrm{if~}n\\mathrm{~is~an~input~unit,}}\\\\ {\\mathrm{~if~}n\\mathrm{~is~a~sum~unit,}}\\\\ &{\\mathrm{if~}n\\mathrm{~is~a~product~unit,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\theta_{n,c}$ represents the parameter corresponding to edge $(n,c)$ in the DAG. For sum nodes, we have $\\begin{array}{r}{\\sum_{c\\in\\sin(n)}\\theta_{n,c}=1,\\theta_{n,c}\\geq0}\\end{array}$ , and we assume w.l.o.g. that a PC alternates between the sum and product layers before reaching its inputs. ", "page_idx": 14}, {"type": "text", "text": "B.1 Tractability and Expressivity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The expressivity of PCs comes from the ability to combine simpler distributions with sum and product nodes to form more complex distributions. Therefore, to increase the capacity of a PC, we can add more nodes to its DAG or find a better structure that is more tailored to the target data distribution. On the other hand, tractability, the ability to answer certain probabilistic queries efficiently and exactly, is guaranteed by certain structural properties of PCs. For example, with smoothness and decomposability defined in the following, PCs can compute arbitrary marginal and conditional probability in time linear with respect to its size (number of edges in its DAG). ", "page_idx": 14}, {"type": "text", "text": "Definition 1 (Decomposability). A PC is decomposable if for every product unit $n$ , its children have disjoint scopes: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall c_{1},c_{2}\\in\\mathfrak{i n}(n)\\,(c_{1}\\neq c_{2}),\\ \\phi(c_{1})\\cap\\phi(c_{2})=\\emptyset.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition 2 (Smoothness). A PC is smooth if for every sum unit $n$ , its children have the same scope: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall c_{1},c_{2}\\in\\mathsf{i n}(n),\\;\\phi(c_{1})=\\phi(c_{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As a key procedure used in Trifle, we describe how to compute marginal queries given a smooth and decomposable PC. First, we assign probabilities to all input nodes. For an input node defined on variable $X$ , if evidence on $X$ is provided in the query, we set the output probability following Equation (7). Otherwise, we set the output probability to 1. Next, we do a feedforward pass over all inner (sum and product) nodes following Equation (7). The final output at the root node is the desired marginal probability. ", "page_idx": 15}, {"type": "text", "text": "In addition to marginal and conditional probabilities, PCs can efficiently and exactly compute other queries including maximize a-posterior and various information-theoretic queries given some additional structural constraints. Please refer to [44] for a comprehensive overview. ", "page_idx": 15}, {"type": "text", "text": "B.2 Adopted PC Structure And Parameter Learning Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For all tasks/offilne datasets, we adopt the Hidden Chow-Liu Tree (HCLT) PC structure proposed by [29] as it has been shown to perform well across different data types. ", "page_idx": 15}, {"type": "text", "text": "Following the definition in Equation (7), a PC takes as input a sample $\\textbf{\\em x}$ and outputs the corresponding probability $p_{n}({\\boldsymbol{x}})$ . Given a dataset $\\mathcal{D}$ , the PC optimizer takes the PC parameters (consisting of sum edge parameters and input node/distribution parameters) as input and aims to maximize the MLE objective $\\textstyle\\sum_{x\\in D}\\log p_{n}(x)$ . Since PCs can be deemed as latent variable models with hierarchically nested lat ent space [33], the Expectation-Maximization (EM) algorithm is usually the default choice for PC parameter learning. We adopt the full-batch EM algorithm proposed in [35]. ", "page_idx": 15}, {"type": "text", "text": "Before tuning the parameters with EM, we adopt the latent variable distillation (LVD) technique proposed in [30] to initialize the PC parameters. Specifically, the neural embeddings used for LVD are acquired by a BERT-like Transformer [10] trained with the Masked Language Model task. To acquire the embeddings of a subset of variables $\\phi$ , we feed the Transformer with all other variables and concatenate the last Transformer layer\u2019s output for the variables $\\phi$ . Please refer to the original paper for more details. ", "page_idx": 15}, {"type": "text", "text": "We use the same quantile dataset discretized from the original Gym-MuJoCo dataset as done by TT [20], where each raw continuous variable is divided into 100 categoricals, and each categorical represents an equal amount of probability mass under the empirical data distribution. ", "page_idx": 15}, {"type": "text", "text": "C Algorithm Details of Trifle ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section provides a detailed description of the algorithmic procedure of Trifle with single-/multistep value estimates. ", "page_idx": 15}, {"type": "text", "text": "C.1 Trifle with Single-/Multi-Step Value Estimates ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Similar to other RvS algorithms, Trifle first trains sequence models given truncated trajectories $\\{(s_{t},a_{t},r_{t},\\mathrm{RTG}_{t})\\}_{t}$ . Specifically, we fit two sequence models: an autoregressive Transformer following prior work [20] as well as a PC, where the training details are introduced in Appx. B.2. ", "page_idx": 15}, {"type": "text", "text": "During the evaluation phase, at time step $t$ , Trifle is tasked to generate $a_{t}$ given $s{\\leq}t$ and other relevant information (such as rewards collected in past steps). As introduced in Section 5, Trifle generally works in two phases: rejection sampling for action generation and beam search for action selection. The main algorithm is illustrated in Algorithm 1, where we take the current state $s_{t}$ as well as the past trajectory $\\tau_{<t}$ as input, utilize the specified value estimate $f_{v}$ as a heuristic to guide beam search, and output the best trajectory. Note that $f_{v}$ is a subroutine of our algorithm that uses the trained sequence models to compute certain quantities, which will be detailed in subsequent parts. After that, we extract the current action $a_{t}$ from the output trajectory to execute in the environment. ", "page_idx": 15}, {"type": "text", "text": "At the first step of the beam search, we perform rejection sampling to obtain a candidate action set $\\mathbf{a_{t}}$ (line 4 of Algorithm 1). The concrete rejection sampling procedure for s-Trifle is detailed in Algorithm 2. The major modification of m-Trifle compared to $\\mathrm{\\bfS}$ -Trifle is the adoption of a multistep value estimate instead of the single-step value estimate, which is also shown in Algorithm 3. Specifically, Algorithm 3 is used to replace the value function $f_{v}$ shown in Algorithm 1. ", "page_idx": 15}, {"type": "text", "text": "1: Input: past trajectory $\\tau_{<t}$ , current state $s_{t}$ , beam width $N$ , beam horizon $H$ , scaling ratio $\\lambda$ , sequence   \nmodel $\\dot{\\mathcal{M}}$ , value function $f_{v}$ $\\triangleright f_{v}=\\mathbb{E}[V_{t}]$ for s-Trifle and $\\overline{{\\mathbb{E}}}[V_{t}^{m}]$ for m-Trifle   \n2: Output: The best action $a_{t}$   \n3: Let $\\mathbf{x_{t}}\\gets\\mathsf{c o n c a t}(\\tau_{<t},s_{t}).\\mathbf{reshape}(1,-1).\\mathbf{repeat}(N,\\mathsf{d i m}=0)$ \u25b7Batchify the input trajectory   \n4: Perform rejection sampling to obtain $\\mathbf{a_{t}}$ using Algorithm 2 \u25b7cf. Algorithm 2   \n5: Initialize $X_{0}=\\mathsf{c o n c a t}(\\mathbf{x_{t}},\\mathbf{a_{t}})$   \n6: foreach $t=1,...,H$   \n7: $X_{t-1}\\gets X_{t-1}.\\mathtt{r e p e a t}(\\lambda,\\mathtt{d i m}=0)$ $\\triangleright$ Scale the number of trajectories from $N$ to $\\lambda N$   \n8: $\\mathcal{C}_{t}\\gets\\{\\mathsf{c o n c a t}(\\mathrm{x}_{\\mathrm{t}-1},x)\\mid\\forall\\mathrm{x}_{\\mathrm{t}-1}\\in X_{t-1}$ , sample $x\\sim p_{\\mathcal{M}}(\\cdot\\mid\\mathbf{x_{t-1}})\\}$ \u25b7Candidate next-token   \nprediction   \n9: $\\mathbf{\\hat{\\boldsymbol{X}}}_{t}\\gets\\mathbf{topk}_{\\boldsymbol{X}\\in\\mathcal{C}_{t}}\\left(f_{v}(\\boldsymbol{X}),\\mathbf{k}=\\boldsymbol{N}\\right)$ \u25b7keep $N$ most rewarding trajectories   \n10: end foreach   \n11: $X_{m}\\gets\\operatorname{argmax}_{X\\in X_{H}}\\ f_{v}(X)$   \n12: return $a_{t}$ in $X_{m}$ ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Rejection Sampling with Single-step Value Estimate ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: Input: past trajectory $\\tau{<}t$ , current state $s_{t}$ , dimension of action $k$ , rejection rate $\\delta>0$   \n2: Output: The sampled action $a_{t}^{1:k}$   \n3: Let $\\boldsymbol{x}_{t}\\gets\\mathsf{c o n c a t}(\\tau_{<t},s_{t})$ 4: for $i=1,...,k$ do 5: Compute $p_{\\mathrm{GPT}}\\!\\left(a_{t}^{i}\\mid x_{t},a_{t}^{<i}\\right)$ Note that $a_{t}^{<1}=\\emptyset$ . 6: Compute $\\begin{array}{r}{p_{\\mathrm{TPM}}(V_{t}\\mid x_{t},a_{t}^{<i})=\\sum_{a_{t}^{i:k}}p_{\\mathrm{TPM}}(V_{t},a_{t}^{i:k}\\mid x_{t},a_{t}^{1:k})}\\end{array}$ \u25b7The marginal can be efficiently computed by PC in linear time. See the algorithm in Appx. B.1. 7: Compute $\\bar{v_{\\delta}}=\\operatorname*{max}_{v}\\{v\\in\\mathrm{val}(V_{t})\\mid p_{\\mathrm{TPM}}(V_{t}\\geq v\\mid\\hat{x_{t}^{\\prime}},\\hat{a_{t}^{<i}})\\geq1-\\delta\\}$ , for each $a_{t}^{i}\\in\\mathtt{v a l}(A_{t}^{i})$ 8: Compute $\\begin{array}{r}{\\tilde{p}(a_{t}^{i}\\mid x_{t},a_{t}^{<i};v_{\\delta})=\\frac{1}{Z}\\cdot p_{\\mathrm{GPT}}(a_{t}^{i}\\mid x_{t},a_{t}^{<i})\\cdot p_{\\mathrm{TPM}}(V_{t}\\geq v_{\\delta}\\mid x_{t},a_{t}^{\\leq i})\\gg}\\end{array}$ Apply Equation (2) 9: Sample $a_{t}^{i}\\sim\\tilde{p}(a_{t}^{i}\\mid x_{t},a_{t}^{<i};v_{\\delta})$   \n10: end for   \n11: return $a_{t}^{1:k}$ ", "page_idx": 16}, {"type": "text", "text": "Algorithm 3 Multi-step Value Estimate ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: Input: $\\tau_{\\le t}=(s_{0},a_{0},...,s_{t},a_{t})$ , sequence model $\\mathcal{M}$ , terminal timestep $t^{\\prime}>t$ , discount $\\gamma$   \n2: Output: The multi-step value estimate $\\mathbb{E}\\big[V_{t}^{\\mathrm{m}}\\big]$   \n3: Sample future actions $a_{t+1},...,a_{t^{\\prime}}$ from $\\mathcal{M}$   \n4: Compute $\\begin{array}{r}{p_{\\mathrm{TPM}}(r_{h}\\mid\\tau_{\\leq t},a_{t+1:h})=\\sum_{s_{t+1:h}}p_{\\mathrm{TPM}}(r_{h},s_{t+1:h}\\mid\\tau_{\\leq t^{\\prime}})}\\end{array}$ for $h\\in[t+1,t^{\\prime}]$ \u25b7Marginalize   \nover intermediate states $s t{+1:}h$   \n5: Compute $\\begin{array}{r}{p_{\\mathrm{TPM}}(\\mathrm{RTG}_{t^{\\prime}}\\mid\\tau_{\\leq t},a_{t+1:t^{\\prime}})=\\sum_{s_{t+1:t^{\\prime}}}p_{\\mathrm{TPM}}(\\mathrm{RTG}_{t^{\\prime}}\\mid\\tau_{\\leq t^{\\prime}})}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "$\\mathbb{E}\\big[V_{t}^{\\mathrm{m}}\\big]=\\sum_{h=t}^{t^{\\prime}}\\gamma^{h-t}\\mathbb{E}_{r_{h}\\sim p_{\\mathrm{TPM}}(\\cdot\\,|\\,r_{\\le t},a_{t+1:h})}\\big[r_{h}\\big]+\\gamma^{t^{\\prime}+1-t}\\mathbb{E}_{\\mathrm{RTG}_{t^{\\prime}}\\sim p_{\\mathrm{TPM}}(\\cdot\\,|\\,r_{\\le t},a_{t+1:t^{\\prime}})}\\big[\\mathrm{RTG}_{t^{\\prime}}\\big]$ 7: return $\\mathbb{E}\\big[V_{t}^{\\mathrm{m}}\\big]$ ", "page_idx": 16}, {"type": "text", "text": "C.2 Computing Multi-Step Value Estimates ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present an efficient algorithm that computes Equation (3). From the decomposition of Equation (3), we can calculate $\\mathbb{E}\\left[V_{t}^{\\mathrm{m}}\\right]$ if we have the probabilities $p(r_{\\tau}|s_{t},a_{t:t^{\\prime}})(t\\leq\\tau<t^{\\prime})$ and $p(\\mathrm{RTG}_{t^{\\prime}}|s_{t},a_{t:t^{\\prime}})$ . A simple approach would be to compute each of the $t^{\\prime}\\!-\\!t\\!+\\!1$ probabilities separately using the algorithm described in Appx. B.1 (recall that conditional probabilities are quotient of the corresponding marginal probabilities). However, this approach has an undesired time complexity that scales linearly with respect to $t^{\\prime}\\!-\\!t\\!+\\!1$ . ", "page_idx": 16}, {"type": "text", "text": "Following [28], we describe an algorithm that can compute all desired quantities using a single feedforward and a backward pass to the PC. ", "page_idx": 17}, {"type": "text", "text": "The forward pass. The forward pass is similar to the one described in Appx. B.1. Specifically, we set the evidence as $s_{t},a_{t:t^{\\prime}}$ and execute the forward pass. ", "page_idx": 17}, {"type": "text", "text": "The backward pass. The backward pass consists of two steps: (i) traverse all nodes parents before children to compute a statistic termed flow for every node $n$ : $\\mathtt{f l o w}_{n}$ ; (ii) compute the target probabilities using the flow of all input nodes. Recall that we assume without loss of generality that PCs alternate between sum and product layers. We further assume that all parents of input nodes are product nodes. We define the flow of the root node as 1. The flow of other nodes is defined recursively as (define $p_{n}$ as the forward probability of node $n$ ): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathtt{f l o w}_{n}:=\\left\\{\\sum_{m\\in\\mathtt{p a}(n)}\\left(\\theta_{m,n}\\cdot p_{n}/p_{m}\\right)\\cdot\\mathtt{f l o w}_{m}\\quad n\\mathrm{~is~a~product~node},\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ${\\mathsf{p a}}(n)$ is the set of parents of node $n$ . ", "page_idx": 17}, {"type": "text", "text": "Next for every variable $X\\in\\{R_{t},\\ldots,R_{t^{\\prime}-1},\\mathrm{RTG}_{t^{\\prime}}\\}$ , we first collect all input nodes defined on $X$ . Define the set of input nodes as $S$ . We have that ", "page_idx": 17}, {"type": "equation", "text": "$$\np(x|s_{t},a_{t:t^{\\prime}}):=\\frac{1}{Z}\\sum_{n\\in S}\\mathsf{f l o w}_{n}\\cdot f_{n}(x),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $f_{n}$ is defined in Equation (7) and $Z$ is a normalizing constant. ", "page_idx": 17}, {"type": "text", "text": "D Inference-time Optimality Score ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We define the inference-time optimality score as a proxy for inference-time optimality. This score is primarily defined over a state-action pair $\\left({{s}_{t}},{{a}_{t}}\\right)$ at each inference step. In Figure 2 (middle) and Figure 2 (right), each sample point represents a trajectory, and the corresponding inference-time optimality score is defined over the entire trajectory by averaging the scores of all inference steps. ", "page_idx": 17}, {"type": "text", "text": "The specific steps for calculating the score for a given inference step $t$ , given $s_{t}$ and a policy $p(a_{t}\\mid s_{t})$ , are as follows: ", "page_idx": 17}, {"type": "text", "text": "1. Given $s_{t}$ , sample $a_{t}$ from $p_{\\mathrm{TT}}\\big(a_{t}\\mid s_{t}\\big),p_{\\mathrm{DT}}\\big(a_{t}\\mid s_{t}\\big),\\,,$ or $p_{\\mathrm{Trifle}}(a_{t}\\mid s_{t})$ .   \n2. Compute the state-conditioned value distribution $p^{s}(V_{t}\\mid s_{t})$ .   \n3. Compute $R_{t}\\,:=\\,\\mathbb{E}_{V_{t}\\sim p^{a}(\\mathrm{RTG}_{t}|s_{t},a_{t})}[V_{t}]$ , which is the corresponding estimated expected   \nvalue.   \n4. Output the quantile value $S_{t}$ of $R_{t}$ in $p^{s}(V_{t}\\mid s_{t})$ . ", "page_idx": 17}, {"type": "text", "text": "To approximate the distributions $p^{s}(V_{t}\\mid s_{t})$ and $p^{a}(V_{t}\\mid s_{t},a_{t})$ (where $V_{t}=\\mathrm{RTG}*t)$ in steps 2 and 3, we train two auxiliary GPT models using the offline dataset. For instance, to approximate $p^{s}(V_{t}\\mid s_{t})$ , we train the model on sequences $(s*t-k,V_{t-k},\\ldots,s_{t},V_{t})$ . ", "page_idx": 17}, {"type": "text", "text": "Intuitively, $p^{s}(V_{t}\\ |\\ s_{t})$ approximates $\\begin{array}{r}{p(V_{\\!\\;\\!t}\\mid s_{t}):=\\sum_{a_{t}}p(V_{\\!\\;\\!t}\\mid s_{t},a_{t})\\cdot p(a_{t}\\mid s_{t})}\\end{array}$ . Therefore, $S_{t}$ indicates the percentile of the sampled action in term s of achieving a high expected return, relative to the entire action space. ", "page_idx": 17}, {"type": "text", "text": "E Additional Experimental Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Gym-MuJoCo ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Sampling Details. We take the single-step value estimate by setting $V_{t}=\\mathrm{RTG}_{\\mathrm{t}}$ and sample $a_{t}$ from Equation (2). When training the GPT used for querying $p_{\\mathrm{GPT}}(a_{t}^{i}|\\check{s}_{t},a_{t}^{<i})$ , we adopt the same model specification and training pipeline as TT or DT. When computing $p_{\\mathrm{TPM}}(V_{t}\\,\\geq\\,v|s_{t},a_{t}^{\\leq i})$ , we first use the learned PC to estimate $p(V_{t}|s_{t})$ by marginalizing out intermediate actions $a_{t:t^{\\prime}}$ and select the $\\epsilon$ -quantile value of $p(V_{t}|s_{t})$ as our prediction threshold $v$ for each inference step. Empirically we fixed $\\epsilon$ for each environment and $\\epsilon$ ranges from 0.1 to 0.3. ", "page_idx": 17}, {"type": "text", "text": "Beam Search Hyperparameters. The maximum beam width $N$ and planning horizon $H$ that Trifle uses across 9 MuJoCo tasks are 15 and 64, respectively. ", "page_idx": 18}, {"type": "text", "text": "Comparison with Value-Based Algorithms. To shed light on how Trifle compares to methods that directly optimize the Q values while filtering actions by conditioning on high returns (as done in RvS algorithms), we compare Trifle with Q-learning Decision Transformer [46], which incorporates a contrastive Q-learning regime into the RvS framework. As shown in the table below, Trifle outperforms QDT in all six adopted MuJoCo benchmarks: ", "page_idx": 18}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/cb82c1ee2ce09d10cdde587674a777e0168b960e5713f86e64a60c6c38d5496d.jpg", "table_caption": ["Table 3: Normalized Scores of QDT and Trifle on Gym-MuJoCo benchmarks "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.2 Stochastic Taxi Environment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Hyperparameters. Except for s-Trifle, the sequence length $K$ modeled by TT, DT, and m-Trifle is all equal to 7. The inference algorithm of TT follows that of the MuJoCo experiment and DT follows its implementation in the Atati benchmark. Notably, during evaluation, we condition the pretrained DT on 6 different RTGs ranging from -100 to -350 and choose the best policy resulting from $\\mathrm{RTG}{=}{-300}$ to report in Figure 3c. Beam width $N=8$ and planning horizon $H=3$ hold for TT and m-Trifle. ", "page_idx": 18}, {"type": "text", "text": "Additional Results on the Taxi benchmark. Besides the episode return, we adopt two metrics to better evaluate the adopted methods: (i) #penalty: the average number of executing illegal actions within an episode; (ii) $P(\\mathtt{f a i l u r e})$ : the probability of failing to transport the passenger to the destination within 300 steps. ", "page_idx": 18}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/e9af2f4a4f804c83081d26c81819c8dd25d58de7a2be0e461f50faaac16c980c.jpg", "table_caption": ["Table 4: Results on the stochastic Taxi environment. All the reported numbers are averaged over 1000 trials. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Ablation Study Regarding Action Filtering. In an attempt to justify the effectiveness/necessity of exact inference, we compare Trifle with value-based action flitering/value estimation in the following: ", "page_idx": 18}, {"type": "text", "text": "To begin with, we implemented the traditional Policy Evaluation algorithm on the Taxi offilne dataset described in Section 6.2 of the paper. The policy evaluation algorithm is based on the Bellman update: ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ(s_{t},a_{t})\\gets Q(s_{t},a_{t})+\\alpha\\big[r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})-Q({s_{t},a_{t}})\\big)\\big]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we use the obtained Q function, denoted $Q_{\\mathrm{taxi}}$ , to perform the following ablation studies. We still choose TT as our base RvS model. Recall that given $s_{t}$ , TT first samples $a_{t}$ from its learned prior policy $p_{T T}\\big(a_{t}|s_{t}\\big)$ , which are subsequently fed to a beam search procedure that uses the learned value function $p_{T T}(V_{t}|s_{t},a_{t})$ to select the best action. Therefore, we consider ablations on two key components of TT: (i) the prior policy $p_{T T}(V_{t}|s_{t},a_{t})$ used to sample actions, and (ii) the value function $\\dot{p}_{T T}(V_{t}|s_{t},a_{t})$ used to evaluate and select actions. ", "page_idx": 18}, {"type": "text", "text": "1. $\\mathbf{T}\\mathbf{T}+Q_{\\mathbf{taxi}}$ action flitering: weigh the prior policy $p_{T T}\\big(a_{t}|s_{t}\\big)$ with each action\u2019s exponentiated $Q_{\\mathrm{taxi}}$ value (i.e., $\\bar{e x p}(Q_{\\mathrm{taxi}}\\bar{(s_{t},a_{t})}))$ , but still adopt TT\u2019s value estimation. In other words, in this experiment, we only use $Q_{\\mathrm{taxi}}$ to improve the sampling quality as $\\mathrm{\\bfS}$ -Trifle does.   \n2. $\\mathbf{T}\\mathbf{T}+Q_{\\mathbf{taxi}}$ value estimation: replace $p_{T T}(V_{t}|s_{t},a_{t})$ with $Q_{\\mathrm{taxi}}(s_{t},a_{t})$ for action evaluation and selection, but still use TT\u2019s prior policy $p_{T T}\\big(a_{t}|s_{t}\\big)$ .   \n3. $\\mathbf{TT}+\\mathbf{full}\\;Q_{\\mathbf{taxi}};$ : simultaneously use $e x p(Q_{\\mathrm{taxi}}(s_{t},a_{t}))$ for action filtering and $Q_{\\mathrm{taxi}}(s_{t},a_{t})$ for action evaluation. ", "page_idx": 19}, {"type": "text", "text": "We present the results of these ablation studies as follows: ", "page_idx": 19}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/81a2b522eb675bb733f1b8064a0d169597c5379f765dadadfecafc67b2ccee05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "From these results, we draw the following conclusions: ", "page_idx": 19}, {"type": "text", "text": "\u2022 m-Trifle and s-Trifle achieve the best performance. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The rank of scores: $\\mathrm{TT}+$ full $Q_{\\mathrm{taxi}}>\\mathrm{TT}+Q_{\\mathrm{taxi}}$ value estimation $>\\mathrm{TT}+Q_{\\mathrm{taxi}}$ action flitering $>$ TT suggests that using $Q_{\\mathrm{taxi}}$ for both action filtering and value estimation is beneficial; combining the two leads to the best performance. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Specifically, the fact that s-Trifle outperforms the $Q_{\\mathrm{taxi}}$ based action filtering demonstrates that our flitration with exact inference is much more effective. The superior performance of m-Trifle also provides strong evidence that explicit marginalization over future states leads to better value estimation. ", "page_idx": 19}, {"type": "text", "text": "F Additional Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Ablation Studies on Rejection Sampling and Beam Search ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The key insight of Trifle to solve challenges elaborated in Scenario $\\#1$ is to utilize tractable probabilistic models to better approximate action samples from the desired distribution $p(a_{t}|s_{0:t},\\mathbb{E}[V_{t}]\\geq v)$ . We highlight that the most crucial design choice of our method for this goal is that: Trifle can effectively bias the per-action-dimension generation process of any base policy towards high expected returns, which is achieved by adding per-dimension correction terms $p_{T P M}(V_{t}\\geq v|s_{t},a_{t}^{\\leq i})$ (Eq. (2) in the paper) to the base policy. ", "page_idx": 19}, {"type": "text", "text": "While the rejection sampling method can help us obtain more unbiased action samples through a post value(expected return)-estimation session, we only implement this component for TT-based Trifle (not for DT-based Trifle) for fair comparison, as the DT baseline doesn\u2019t perform explicit value estimation or adopt any rejection sampling methods. Therefore, the success of DT-based Trifle strongly justifies the effectiveness of the TPM components. Moreover, the beam search algorithm also comes from TT. Although it is a more effective way to do rejection sampling, it is not the necessary component of Trifle, either. ", "page_idx": 19}, {"type": "image", "img_path": "UZIHW8eFRp/tmp/ce52cc2b21b3193a1feed91c1b09d7ff6072bf7755d0dcf2f8907574045a21e0.jpg", "img_caption": ["Figure 5: Scaling Curves of Inference Time. (Fix beam width $=32$ ) "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 5: Ablations over Beam Search Hyperparameters on Halfcheetah Med-Replay. (a) With $H=1$ , the beam search degrades to naive rejection sampling (b) With $W=1$ , the algorithm doesn\u2019t perform rejection sampling. It samples a single action and applies it to the environment directly. ", "page_idx": 20}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/69eac614244a8bb46bb6dd70624cb207dc92950926fb09553041278f470bcf26.jpg", "table_caption": ["(a) Varying Planning Horizon ", "(b) Varying Beam Width "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "For TT-based Trifle, we adopted the same beam search hyperparameters as reported in the TT paper. We conduct ablation studies on beam search hyperparameters in Table 5 to investigate the effectiveness of Trifle\u2019s each component. From Table 5, we can observe that: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Trifle consistently outperforms TT across all beam search hyperparameters and is more robust to variations of both planning horizon and beam width.   \n\u2022 (a) Trifle w/ naive rejection sampling $\\gg$ TT w/ naive rejection sampling (b) Trifle w/o rejection sampling \u00bb TT w/o rejection sampling. In both cases, Trifle can positively guide action generation.   \n\u2022 Trifle w/ beam search $>$ Trifle w/ naive rejection sampling $>$ Trifle w/o rejection sampling \u00bb TT w/ naive rejection sampling. Although other design choices like rejection sampling/beam search help to better approximate samples from the high-expected-return-conditioned action distribution, the per-dimension correction terms computed by Trifle play a very significant role. ", "page_idx": 20}, {"type": "text", "text": "F.2 Computational Efficiency Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since TPM-related computation consistently requires 1.45s computation time across different horizons, the relative slowdown of Trifle is diminishing as we increase the beam horizon. Specifically, in the Gym-Mujuco benchmark, the time consumption for one step (i.e., one interaction with the environment) of TT and Trifle with different beam horizons are listed here (Figure 5 (left) also plots an inference-time scaling curve of Trifle vs TT with varying horizons): ", "page_idx": 20}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/2e16e180f44c87f52fc701d3a684a15fcba4c819ce9732abb79a4edc7d10e4a1.jpg", "table_caption": ["Table 6: The one-step inference runtime of the Gym-MuJuCo benchmark "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Moreover, Figure 5 (right) shows that Trifle\u2019s runtime (TPM-related) scales linearly w.r.t. the number of action variables, which indicates its efficiency for handling high-dimensional action spaces. ", "page_idx": 21}, {"type": "text", "text": "Trifle is also efficient in training. It only takes 30-60 minutes ( 20s per epoch, 100-200 epochs) to train a PC on one GPU for each Gym-Mujuco task (Note that a single PC model can be used to answer all conditional queries required by Trifle). In comparison, training the GPT model for TT takes approximately 6-12 hours (80 epochs). ", "page_idx": 21}, {"type": "text", "text": "F.3 Ablation Studies on the Adaptive Thresholding Mechanism ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The adaptive thresholding mechanism is adopted when computing the term $p_{T P M}(V_{t}\\geq v|s_{t},a_{t}^{\\leq i})$ of Equation (2), where $i\\in\\{1,\\ldots,k\\}$ , $k$ is the number of action variables and $a_{t}^{i}$ is the ith variable of $a_{t}$ . Instead of using a fixed threshold $v$ , we choose $v$ to be the $\\epsilon$ -quantile value of the distribution $p_{T P M}(V_{t}|s_{t},a_{t}^{<i})$ computed by the TPM, which leverage the TPM\u2019s ability to exactly compute marginals given $^{\\ast\\ast}$ incomplete\\*\\* actions (marginalizing out $a_{t}^{i:k}$ ). Specifically, we compute $v$ using $v=m a x_{r}\\{r\\in\\mathbb{R}|p_{T P M}(V_{t}\\geq r|s_{t},a_{t}^{<i})\\geq1-\\epsilon\\}$ }. Empirically we fixed $\\epsilon$ for each Gym-MuJoCo environment and $\\epsilon=0.2$ or 0.25, which is selected by running grid search on $\\epsilon\\in[0.1,0.25]$ . ", "page_idx": 21}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/7d699442be4909e8e26b4e477b0f8e16dbea2942761632096d1b9e7c15aee8ad.jpg", "table_caption": ["(a) Ablations over Adaptive Thresholding (Varying \u03f5) on Halfcheetah Med-Replay "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "UZIHW8eFRp/tmp/3812fdb060c8119f4a4603499411c5e12ef212bc0801ab40ea64505ac4f641c6.jpg", "table_caption": ["Table 7: Comparison of Adaptive and Fixed Thresholding Mechanisms ", "(b) Performance of Fixed Thresholding (Varying v) "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We report the performance of TT-based Trifle with variant $\\epsilon$ vs TT on Halfcheetah Med-Replay benchmark in Table 7a. We can see that Trifle is robust to the hyperparameter $\\epsilon$ and consistently outperforms the base policy TT. ", "page_idx": 21}, {"type": "text", "text": "We also conduct ablation studies comparing the performance of the adaptive thresholding mechanism with the fixed thresholding mechanism on two environments in Table 7b. Specifically, given that $V_{t}$ is discretized to be a categorical variable with 100 categoricals (0-99), we fix $v$ to be 90,80,70,60,50,40,30 respectively. ", "page_idx": 21}, {"type": "text", "text": "The table shows that the adaptive approach consistently outperforms the fixed value threshold in both environments. Additionally, the performance variation of fixing $v$ is larger compared to fixing $\\epsilon$ as different $v$ can be optimal for different states. ", "page_idx": 21}, {"type": "text", "text": "G Potential Negative Societal Impact ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This paper proposes a new offline RL algorithm, which aims to produce policies that achieve high expected returns given a pre-collected dataset of trajectories generated by some unknown policies. When there are malicious trajectories in the dataset, our method could potentially learn to mimic such behavior. Therefore, we should only train the proposed agent in verified and trusted offline datasets. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All claims regarding empirical performance are justified by the results in Section 6. The claims regarding the importance of tractability are justified by empirical evidence shown in Section 3. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed limitations of our work in Section 7. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: For Thm. 1, we have elaborated the full assumptions in its main body. A proof is provided in Appx. A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our code is available at https://github.com/liebenxj/Trifle.git. Moreover, we have provided full algorithm details (including algorithm tables) in Section 4 and 5 and Appx. C. Adopted hyperparameters are described in Section 6 and Appx. E. The code as well as all trained models will be released if this paper gets accepted. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our code is available at https://github.com/liebenxj/Trifle.git ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 23}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Detailed settings of the experiments can be found in Section 6 and Appx. E. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We reported mean and standard deviation over 12 or more runs in most applicable experiments, e.g., Table 1, 2 and 3 and Figure 3c. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have a discussion regarding runtime in Appx. F.2. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and the research conducted in the paper conforms to it. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper studies a specific type of offilne RL algorithm. The proposed method itself is only tested on simulated environments such as games and thus has no immediate societal impact. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new data and does not involve large language models. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have cited the owners/authors of the benchmarks we used. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]