[{"figure_path": "UZIHW8eFRp/figures/figures_1_1.jpg", "caption": "Figure 1: An example PC over boolean variables X1,..., X4. Every node's probability given input X1X2X3X4 is labeled in blue. p(x1X2X3X4) = 0.22.", "description": "This figure shows an example of a Probabilistic Circuit (PC).  PCs are a type of tractable probabilistic model used in the paper.  The figure illustrates the structure of a PC, showing input nodes (representing simple distributions over variables X1 to X4), product nodes (modeling factorized distributions), and sum nodes (representing weighted mixtures). Each node's probability is labeled, demonstrating how the PC computes a joint probability distribution.", "section": "Tractable Probabilistic Models"}, {"figure_path": "UZIHW8eFRp/figures/figures_2_1.jpg", "caption": "Figure 2: RvS approaches suffer from inference-time suboptimality. Left: There is a strong positive correlation between the average estimated returns by Trajectory Transformers (TT) and the actual returns in 6 Gym-MuJoCo environments (MR, M, and ME denote medium-replay, medium, and medium-expert, respectively), which suggests that the sequence model can distinguish rewarding actions from the others. Middle: Despite being able to recognize high-return actions, both TT and DT [6] fail to consistently sample such action, leading to bad inference-time optimality; Trifle consistently improves the inference-time optimality score. Right: We substantiate the relationship between low inference-time optimality scores and unfavorable environmental outcomes by showing a strong positive correlation between them.", "description": "This figure demonstrates the inference-time suboptimality problem in RvS algorithms. The left panel shows a strong positive correlation between estimated and actual returns, indicating the model's ability to identify rewarding actions. However, the middle panel reveals that both TT and DT struggle to consistently sample these actions, resulting in low inference-time optimality scores. Trifle significantly improves this score. The right panel confirms the negative impact of low inference-time optimality on actual returns.", "section": "3 Tractability Matters in Offline RL"}, {"figure_path": "UZIHW8eFRp/figures/figures_7_1.jpg", "caption": "Figure 3: (a) Stochastic Taxi environment; (b) Stochastic FrozenLake Environment; (c) Average returns on the stochastic environment. All the reported numbers are averaged over 1000 trials.", "description": "This figure shows the stochastic Taxi and FrozenLake environments and a table summarizing the average returns achieved by different RL algorithms in these environments.  The Taxi environment involves a taxi navigating a grid to pick up a passenger and drop them off at a destination, with stochasticity in movement.  The FrozenLake environment is a grid world where the agent must navigate to a goal while avoiding holes, also with stochastic movement. The table displays average returns for various algorithms (m-Trifle, s-Trifle, TT, DT, DoC), with varying levels of stochasticity (epsilon values of 0.3, 0.5, and 0.7).  The results highlight Trifle's robustness in stochastic environments.", "section": "Evaluating Trifle in Stochastic Environments"}, {"figure_path": "UZIHW8eFRp/figures/figures_8_1.jpg", "caption": "Figure 2: RvS approaches suffer from inference-time suboptimality. Left: There is a strong positive correlation between the average estimated returns by Trajectory Transformers (TT) and the actual returns in 6 Gym-MuJoCo environments (MR, M, and ME denote medium-replay, medium, and medium-expert, respectively), which suggests that the sequence model can distinguish rewarding actions from the others. Middle: Despite being able to recognize high-return actions, both TT and DT [6] fail to consistently sample such action, leading to bad inference-time optimality; Trifle consistently improves the inference-time optimality score. Right: We substantiate the relationship between low inference-time optimality scores and unfavorable environmental outcomes by showing a strong positive correlation between them.", "description": "This figure shows the strong positive correlation between estimated and actual returns in 6 Gym-MuJoCo environments, indicating that the model identifies rewarding actions. However, both TT and DT struggle to consistently sample these actions, highlighting the suboptimality of the inference time. Trifle effectively addresses this issue by improving the inference-time optimality scores and achieving better results.", "section": "3 Tractability Matters in Offline RL"}, {"figure_path": "UZIHW8eFRp/figures/figures_20_1.jpg", "caption": "Figure 2: RvS approaches suffer from inference-time suboptimality. Left: There is a strong positive correlation between the average estimated returns by Trajectory Transformers (TT) and the actual returns in 6 Gym-MuJoCo environments (MR, M, and ME denote medium-replay, medium, and medium-expert, respectively), which suggests that the sequence model can distinguish rewarding actions from the others. Middle: Despite being able to recognize high-return actions, both TT and DT [6] fail to consistently sample such action, leading to bad inference-time optimality; Trifle consistently improves the inference-time optimality score. Right: We substantiate the relationship between low inference-time optimality scores and unfavorable environmental outcomes by showing a strong positive correlation between them.", "description": "This figure shows that while sequence models in reinforcement learning can identify high-return actions, they often fail to consistently sample them. This is due to suboptimal inference-time performance, and the paper's proposed Trifle method addresses this issue.", "section": "3 Tractability Matters in Offline RL"}]