[{"figure_path": "da0ZJatRCN/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Posteriors of f, df/dx, |df/dx|, and (df/dx)\u00b2 are computed from a GP surrogate given six observations of f (black dots). Posteriors are shown as posterior mean (line) and 95% credible interval (shaded). (Right) Acquisition functions are computed from these posteriors, targeting f and derivative sensitivity measures. Dotted vertical lines show the maximizer. Acquisition functions that directly target DGSMs, not just f generally, are required to learn the DGSMs efficiently.", "description": "This figure shows the posterior distributions of the function (f), its derivative (df/dx), the absolute value of the derivative (|df/dx|), and the square of the derivative ((df/dx)\u00b2) obtained from a Gaussian Process (GP) model.  Six observations of the function are used to build the model. The left panels display posterior means (lines) and 95% credible intervals (shaded regions) for each quantity.  The right panels show acquisition functions for these quantities, illustrating the information gain from additional evaluations at each point, to help understand how choosing points for evaluations impacts the uncertainty in these sensitivity measures (DGSMs). The dotted vertical lines indicate the points that maximize the information gain for each quantity.", "section": "1 Introduction"}, {"figure_path": "da0ZJatRCN/figures/figures_8_1.jpg", "caption": "Figure 2: RMSE (mean over 50 runs, two standard errors shaded) for learning the DGSM, for 10 test problems. Results are shown for active learning methods targeting the raw derivative. Active learning targeting the derivative consistently outperformed space-filling designs and active learning targeting f. Derivative information gain was generally the best-performing acquisition function.", "description": "This figure displays the RMSE for estimating the derivative-based global sensitivity measure (DGSM) using different active learning methods across 10 benchmark problems.  The results show that methods targeting the derivative (raw, absolute, and squared) consistently outperform space-filling methods (QR) and methods that target only the function itself (fVAR, fIG).  Among the derivative-targeting methods, the information gain acquisition function generally performs best, indicating its effectiveness in efficiently estimating the DGSM with limited evaluations.", "section": "5.1 Experimental Setup"}, {"figure_path": "da0ZJatRCN/figures/figures_8_2.jpg", "caption": "Figure 3: RMSE results as in Fig. 2, here for the absolute derivative acquisition functions. These also outperformed the baselines, with absolute derivative information gain generally the best.", "description": "This figure shows the root mean squared error (RMSE) for estimating the absolute derivative-based global sensitivity measures (DGSMs) using different active learning acquisition functions and baselines across various benchmark problems.  The results demonstrate that the proposed active learning strategies, specifically those targeting the absolute derivative quantities, consistently outperform both space-filling methods (QR) and general-purpose active learning methods that focus on the function itself (fVAR and fIG).  In particular, active learning methods that utilize information gain about the absolute derivative provide superior performance.", "section": "5.2 Results and Discussion"}, {"figure_path": "da0ZJatRCN/figures/figures_9_1.jpg", "caption": "Figure 2: RMSE (mean over 50 runs, two standard errors shaded) for learning the DGSM, for 10 test problems. Results are shown for active learning methods targeting the raw derivative. Active learning targeting the derivative consistently outperformed space-filling designs and active learning targeting f. Derivative information gain was generally the best-performing acquisition function.", "description": "This figure presents the root mean squared error (RMSE) for estimating derivative-based global sensitivity measures (DGSMs) across ten different test problems.  The RMSE is averaged over 50 runs, with error bars representing two standard errors.  The results compare several active learning methods: those targeting the raw derivative, those targeting the function itself, and a space-filling design as a baseline.  The figure demonstrates that active learning methods directly targeting the derivative's quantities significantly outperform the function-focused and space-filling approaches, highlighting the effectiveness of the proposed methods.  Within the derivative-targeted methods, those using derivative information gain show the best performance.", "section": "5.1 Experimental Setup"}, {"figure_path": "da0ZJatRCN/figures/figures_13_1.jpg", "caption": "Figure 5: An empirical evaluation of active subspace methods on the task of learning the square and absolute DGSM, with derivative information gain, derivative square information gain, and quasirandom as points of comparison.", "description": "This figure compares the performance of three active subspace methods (Trace, Var1, and Var2) against the derivative information gain acquisition function (DIG), and quasirandom sequence (QR) baseline on three benchmark problems (Hartmann3, Hartmann4, and Vehicle Safety Weight). The y-axis represents the root mean squared error (RMSE) of the estimated DGSM, while the x-axis represents the number of active learning iterations. The results show that the derivative information gain acquisition function generally outperforms the other methods in terms of RMSE, indicating its higher sample efficiency and accuracy in learning DGSMs.", "section": "Additional Results"}, {"figure_path": "da0ZJatRCN/figures/figures_14_1.jpg", "caption": "Figure 6: Differential entropy of a folded normal distribution as a function of the underlying normal variance \u03c3\u00b2. The figure compares a high-precision numerical estimate with three approximation schemes, of which scheme 1 provides a high-fidelity analytic approximation.", "description": "This figure compares numerical calculation of differential entropy for folded normal distribution with three approximations. The x-axis represents variance (\u03c3\u00b2) and the y-axis represents differential entropy. Three approximations and the numerical result are plotted for comparison. Approximation 1 shows high fidelity to numerical result. Approximation 2 shows instability and Approximation 3 shows a reasonable approximation, but not as precise as Approximation 1.", "section": "B Entropy Approximations"}, {"figure_path": "da0ZJatRCN/figures/figures_16_1.jpg", "caption": "Figure 1: (Left) Posteriors of f, df/dx, |df/dx|, and (df/dx)\u00b2 are computed from a GP surrogate given six observations of f (black dots). Posteriors are shown as posterior mean (line) and 95% credible interval (shaded). (Right) Acquisition functions are computed from these posteriors, targeting f and derivative sensitivity measures. Dotted vertical lines show the maximizer. Acquisition functions that directly target DGSMs, not just f generally, are required to learn the DGSMs efficiently.", "description": "The figure shows posterior distributions of the function and its derivatives given some observations and the corresponding acquisition functions for active learning.  The left panel demonstrates how a Gaussian process (GP) surrogate model is used to estimate the posterior distributions of the function (f), its derivative (df/dx), the absolute value of the derivative (|df/dx|), and the square of the derivative ((df/dx)\u00b2). The right panel displays the information gain of each quantity, which is used as an acquisition function for selecting the next observation point in active learning.  The figure highlights that directly targeting DGSMs (derivative-based global sensitivity measures) in the acquisition function is crucial for efficient learning, as opposed to just focusing on the function's value.", "section": "1 Introduction"}, {"figure_path": "da0ZJatRCN/figures/figures_18_1.jpg", "caption": "Figure 2: RMSE (mean over 50 runs, two standard errors shaded) for learning the DGSM, for 10 test problems. Results are shown for active learning methods targeting the raw derivative. Active learning targeting the derivative consistently outperformed space-filling designs and active learning targeting f. Derivative information gain was generally the best-performing acquisition function.", "description": "The figure shows the RMSE for learning the derivative-based global sensitivity measures (DGSMs) for 10 different test problems.  It compares the performance of several active learning methods against space-filling designs and active learning methods that target learning the function itself.  The results demonstrate that active learning directly targeting the derivatives significantly outperforms the other approaches, with the derivative information gain method generally achieving the best performance.", "section": "5.1 Experimental Setup"}, {"figure_path": "da0ZJatRCN/figures/figures_18_2.jpg", "caption": "Figure 2: RMSE (mean over 50 runs, two standard errors shaded) for learning the DGSM, for 10 test problems. Results are shown for active learning methods targeting the raw derivative. Active learning targeting the derivative consistently outperformed space-filling designs and active learning targeting f. Derivative information gain was generally the best-performing acquisition function.", "description": "This figure compares the root mean squared error (RMSE) of different active learning methods for estimating derivative-based global sensitivity measures (DGSMs).  Ten different test problems were used.  The results show that active learning methods which directly target the derivative consistently outperform methods which use space-filling designs or target only the function itself.  Furthermore, amongst the derivative-targeting methods, the derivative information gain acquisition function generally performed best.", "section": "5.2 Results and Discussion"}, {"figure_path": "da0ZJatRCN/figures/figures_19_1.jpg", "caption": "Figure 2: RMSE (mean over 50 runs, two standard errors shaded) for learning the DGSM, for 10 test problems. Results are shown for active learning methods targeting the raw derivative. Active learning targeting the derivative consistently outperformed space-filling designs and active learning targeting f. Derivative information gain was generally the best-performing acquisition function.", "description": "The figure displays the RMSE (root mean squared error) for estimating derivative-based global sensitivity measures (DGSMs) using various active learning methods.  Ten different test problems are included, each represented by a separate subplot.  The RMSE values are averaged over 50 runs with error bars showing two standard deviations.  The results indicate that active learning strategies focusing directly on the derivative significantly outperform space-filling designs and active learning methods that only target the function itself. Among the derivative-focused methods, those employing information gain generally produce the most accurate estimates.", "section": "5. Experiments"}, {"figure_path": "da0ZJatRCN/figures/figures_20_1.jpg", "caption": "Figure 11: All acquisition functions evaluated on all benchmark problems for both the absolute and squared DGSMs. A superset of the results in Figs. 2, 3, and 4.", "description": "This figure shows the RMSE results for all acquisition functions (maximum variance, variance reduction, and information gain for raw, absolute, and squared derivatives) applied to all benchmark problems (Ishigami1, Ishigami2, Branin, Hartmann3, Hartmann4, Car Side Impact Weight, Vehicle Safety Weight, Vehicle Safety Acceleration, Gsobol6, Gsobol10, Gsobol15, Morris, and a-function) for both absolute and squared DGSMs. It provides a comprehensive overview of the performance of various acquisition functions across different problem settings. This figure is a superset of the results shown in Figures 2, 3, and 4, offering a more complete comparison across all the test problems.", "section": "5.2 Results and Discussion"}, {"figure_path": "da0ZJatRCN/figures/figures_21_1.jpg", "caption": "Figure 2: RMSE (mean over 50 runs, two standard errors shaded) for learning the DGSM, for 10 test problems. Results are shown for active learning methods targeting the raw derivative. Active learning targeting the derivative consistently outperformed space-filling designs and active learning targeting f. Derivative information gain was generally the best-performing acquisition function.", "description": "The figure shows the RMSE (root mean squared error) results for learning the Derivative-based Global Sensitivity Measures (DGSMs) for ten different test problems.  It compares different active learning methods that target the raw derivative, with error bars showing the standard error over 50 runs.  Space-filling designs (QR) and active learning methods targeting the function f (fVAR and fIG) serve as baselines. The results demonstrate that active learning targeting the derivative consistently outperforms the baselines, with derivative information gain methods generally showing the best performance.", "section": "5.2 Results and Discussion"}, {"figure_path": "da0ZJatRCN/figures/figures_21_2.jpg", "caption": "Figure 12: Synthetic and real-world experiments evaluated using NDCG. The NDCG of the square DGSMs and absolute DGSMs is reported with the mean and two standard errors of 50 repeats.", "description": "The figure presents the results of NDCG (Normalized Discounted Cumulative Gain), a ranking metric evaluating the ability of different methods to accurately determine the order of importance of variables for both absolute and squared DGSMs (Derivative-Based Global Sensitivity Measures).  Results are shown for various synthetic and real-world datasets, comparing the performance of active learning methods (targeting raw, absolute, and squared derivatives) against space-filling designs and general active learning techniques targeting the function directly.  The mean and two standard errors are calculated from 50 repeated runs of each experiment, providing a measure of the statistical reliability of the results.", "section": "5.2 Results and Discussion"}, {"figure_path": "da0ZJatRCN/figures/figures_22_1.jpg", "caption": "Figure 11: All acquisition functions evaluated on all benchmark problems for both the absolute and squared DGSMs. A superset of the results in Figs. 2, 3, and 4.", "description": "This figure shows the RMSE results for all acquisition functions (QR, fVAR, fIG, DV, DVr, DIG, DAbV, DAbVr, DAbIG1, DAbIG3, DSqV, DSqVr, DSqIG1, DSqIG2) across all benchmark problems (Branin, Hartmann3, Hartmann4, Ishigami1, Ishigami2, Gsobol6, Gsobol10, Gsobol15, Morris, Vehicle Safety Weight, Vehicle Safety Acceleration, Car Side Impact Weight) for both the absolute and squared DGSMs.  It is a more comprehensive version of Figures 2, 3, and 4, showing all methods and problems together. Each line represents the mean RMSE across 50 trials, with shaded areas indicating the standard error of the mean.", "section": "5.2 Results and Discussion"}, {"figure_path": "da0ZJatRCN/figures/figures_22_2.jpg", "caption": "Figure 11: All acquisition functions evaluated on all benchmark problems for both the absolute and squared DGSMs. A superset of the results in Figs. 2, 3, and 4.", "description": "This figure shows the RMSE results for all acquisition functions (maximum variance, variance reduction, information gain) targeting the raw, absolute, and squared derivatives, as well as the baselines (QR, fVAR, fIG), across all benchmark problems (synthetic and real-world).  It provides a comprehensive comparison of all methods, expanding upon the results presented separately in Figures 2, 3, and 4.", "section": "5.2 Results and Discussion"}, {"figure_path": "da0ZJatRCN/figures/figures_23_1.jpg", "caption": "Figure 11: All acquisition functions evaluated on all benchmark problems for both the absolute and squared DGSMs. A superset of the results in Figs. 2, 3, and 4.", "description": "This figure is a comprehensive visualization of the performance of various acquisition functions (QR, QR-P, DAbV, DSqV, DIG, DAbIG1, DAbIG3, DSqIG1, DSqIG2) across a range of benchmark problems for both absolute and squared DGSMs.  It expands upon the results presented individually in Figures 2, 3, and 4, providing a unified view of their performance across all problems. The y-axis represents the RMSE (Root Mean Squared Error) of DGSM estimation, while the x-axis shows the number of active learning iterations. This allows for a direct comparison of the effectiveness of each acquisition function in learning DGSMs across different problem complexities and characteristics.", "section": "5.2 Results and Discussion"}]