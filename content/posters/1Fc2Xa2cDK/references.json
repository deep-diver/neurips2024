{"references": [{"fullname_first_author": "Amos Azaria", "paper_title": "The internal state of an LLM knows when it's lying", "publication_date": "2023-12-01", "reason": "This paper is foundational to the current work as it introduces the concept of using internal LLM activations to detect lies, a method the current work expands upon."}, {"fullname_first_author": "Samuel Marks", "paper_title": "The geometry of truth: Emergent linear structure in large language model representations of true/false datasets", "publication_date": "2023-10-26", "reason": "This paper provides crucial theoretical support and empirical evidence for a linear representation of truthfulness within LLMs, which directly informs the current work's core hypothesis."}, {"fullname_first_author": "Benjamin A Levinstein", "paper_title": "Still no lie detector for language models: Probing empirical and conceptual roadblocks", "publication_date": "2024-01-01", "reason": "This paper critically analyzes previous attempts to detect lies in LLMs, highlighting limitations and generalisation failures, which motivates the search for a more robust method in the current work."}, {"fullname_first_author": "Collin Burns", "paper_title": "Discovering latent knowledge in language models without supervision", "publication_date": "2023-01-01", "reason": "This paper introduces a novel approach to lie detection using contrast consistent search, a method compared against in the current work's experimental evaluation."}, {"fullname_first_author": "J\u00e9r\u00e9my Scheurer", "paper_title": "Large language models can strategically deceive their users when put under pressure", "publication_date": "2024-01-01", "reason": "This paper presents compelling real-world examples of LLMs engaging in strategic deception, reinforcing the practical significance and motivation for robust lie detection methods."}]}