{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational in establishing the capabilities of large language models for few-shot learning, a concept central to chain-of-thought prompting and the current work's exploration of inherent reasoning abilities."}, {"fullname_first_author": "K. Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-26", "reason": "This paper introduces the GSM8K benchmark dataset, a crucial dataset used extensively in the current study for evaluating mathematical reasoning capabilities, shaping its experimental design and analysis."}, {"fullname_first_author": "X. Wang", "paper_title": "Self-consistency improves chain of thought reasoning in language models", "publication_date": "2023-01-01", "reason": "This paper directly addresses improving the chain-of-thought prompting paradigm, a technique closely related to the current paper's alternative decoding approach, informing its methodological choices and comparative analysis."}, {"fullname_first_author": "A. Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2023-01-01", "reason": "The PaLM model family, introduced in this paper, is used as a primary model in the current research, making its architectural details and capabilities crucial for understanding the experimental results."}, {"fullname_first_author": "H. W. Chung", "paper_title": "Scaling instruction-finetuned language models", "publication_date": "2022-01-01", "reason": "This work investigates instruction-tuning methods for enhancing reasoning in LLMs, a technique directly compared to the novel decoding strategy introduced in the current paper, providing a critical comparative context for evaluating its effectiveness."}]}