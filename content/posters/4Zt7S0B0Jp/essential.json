{"importance": "This paper is crucial because **it challenges the common belief that large language models (LLMs) cannot reason without prompting**.  By demonstrating that inherent reasoning capabilities can be unlocked simply by altering the decoding process, it opens exciting avenues for improving LLMs and understanding their inner workings. This research also provides a more effective method for evaluating LLMs' reasoning abilities.", "summary": "LLMs can reason effectively without prompting by simply adjusting the decoding process to reveal inherent chain-of-thought paths.", "takeaways": ["Chain-of-thought (CoT) reasoning paths are frequently inherent in LLMs and can be elicited without prompting.", "A novel CoT-decoding method effectively extracts CoT paths by considering alternative top-k tokens, improving reasoning performance.", "CoT-decoding bypasses the confounders of prompting, enabling a better understanding of LLMs' intrinsic reasoning capabilities."], "tldr": "Prior research on enhancing large language models' (LLMs) reasoning capabilities has heavily relied on prompting techniques, often involving manual prompt engineering.  These methods, while effective, obscure the LLMs' intrinsic reasoning abilities and introduce human biases.  This limits our understanding of LLMs' true potential and hinders efforts to improve their reasoning capabilities without relying on extensive, task-specific training.\nThis study introduces a novel approach called CoT-decoding which **elicits CoT reasoning paths by simply altering the decoding process, bypassing prompting altogether**. The research finds that CoT paths are frequently inherent in LLMs' decoding sequences and that the presence of a CoT path correlates with a higher confidence in the model's answer.  The proposed CoT-decoding method effectively extracts these paths, demonstrating substantial performance improvements on various reasoning benchmarks without any additional training or prompting.", "affiliation": "Google DeepMind", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "4Zt7S0B0Jp/podcast.wav"}