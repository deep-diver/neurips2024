[{"figure_path": "AbZyNGWfpN/figures/figures_1_1.jpg", "caption": "Figure 1: (a) The high memory usage of sparse tuning arises from taking the whole weight matrix as learnable parameters, in addition to the storage of the tunable weight indexes (typically represented as a binary mask). (b) Our framework (SNELL) only stores the learnable low-rank matrices in the optimizer. (c) Memory usage comparison on pre-trained models with different depths.", "description": "This figure compares the memory usage of three different methods for fine-tuning neural networks: full fine-tuning, sparse tuning, and the proposed SNELL method.  Panel (a) illustrates that full fine-tuning and sparse tuning both store the entire weight matrix in the optimizer, leading to high memory usage, while sparse tuning adds the overhead of storing indices of tunable weights. In contrast, panel (b) shows that SNELL only stores low-rank matrices, significantly reducing memory usage. Finally, panel (c) provides a quantitative comparison of memory usage across models of varying sizes, demonstrating that SNELL consistently achieves the lowest memory footprint.", "section": "1 Introduction"}, {"figure_path": "AbZyNGWfpN/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of our SNELL strategy. Given two learnable low-rank matrices, we merge them using a non-linear kernel function (left). This merging process is equivalent to mapping the matrices to higher-rank matrices and then performing matrix multiplication. Then we sparsified this merged adaptation matrix using a competition-based sparsification mechanism (right). This mechanism zeros out weights with small absolute values based on the specified percentage of s.", "description": "This figure illustrates the SNELL (Sparse tuning with kerNELized LoRA) framework. The left side shows the kernelized LoRA which merges two low-rank matrices using a nonlinear kernel function. This process is depicted as mapping the low-rank matrices to a higher dimensional space, where the inner product of the mappings is the high rank matrix. The right side shows the competition-based sparsification, which is a mechanism to sparsify the weights of the merged high-rank matrix based on the absolute values of the weights without storing extra index. Weights with smaller absolute values will be zeroed out according to the sparsity ratio (s).", "section": "3 Methodology"}, {"figure_path": "AbZyNGWfpN/figures/figures_9_1.jpg", "caption": "Figure 2: Overview of our SNELL strategy. Given two learnable low-rank matrices, we merge them using a non-linear kernel function (left). This merging process is equivalent to mapping the matrices to higher-rank matrices and then performing matrix multiplication. Then we sparsified this merged adaptation matrix using a competition-based sparsification mechanism (right). This mechanism zeros out weights with small absolute values based on the specified percentage of s.", "description": "This figure illustrates the SNELL framework.  The left side shows how two low-rank matrices (A and B) are merged using a nonlinear kernel function, resulting in a higher-rank adaptation matrix. This is done to enhance the model's ability to adapt to downstream tasks by increasing the expressiveness of the sparse tuning.  The right side details the competition-based sparsification mechanism. This mechanism eliminates the need to store tunable weight indexes by promoting competition among weights, causing the weights with small absolute values to be zeroed out based on a specified sparsity ratio (s).  This reduces memory usage while maintaining effectiveness.", "section": "3.2 Kernelized LORA"}, {"figure_path": "AbZyNGWfpN/figures/figures_9_2.jpg", "caption": "Figure 5: The optimal sparsity ratio of SNELL-8 on different downstream tasks (left) and the average optimal sparsity ratio within each group (right) in VTAB-1k benchmark. The pre-trained model is the ConvNeXt-B pre-trained on ImageNet-21k.", "description": "This figure visualizes the optimal sparsity ratios determined for SNELL-8 across various downstream tasks within the VTAB-1k benchmark.  The left panel shows the optimal sparsity ratio for each individual task, highlighting the variability in the optimal setting depending on the specific characteristics of each task. The right panel shows the average optimal sparsity ratio for the three groups of tasks within VTAB-1k (Natural, Specialized, and Structured). This aggregated view reveals trends in optimal sparsity across different task types within the dataset. The pre-trained model used was ConvNeXt-B, pre-trained on ImageNet-21k.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/figures/figures_19_1.jpg", "caption": "Figure 1: (a) The high memory usage of sparse tuning arises from taking the whole weight matrix as learnable parameters, in addition to the storage of the tunable weight indexes (typically represented as a binary mask). (b) Our framework (SNELL) only stores the learnable low-rank matrices in the optimizer. (c) Memory usage comparison on pre-trained models with different depths.", "description": "This figure compares the memory usage of different sparse tuning methods. (a) illustrates that traditional sparse tuning methods store the entire weight matrix and a binary mask indicating the tunable weights, resulting in high memory consumption. (b) shows the proposed SNELL framework, which only stores low-rank matrices in the optimizer, significantly reducing memory usage. (c) provides a quantitative comparison of memory usage across models with varying depths, demonstrating the scalability and efficiency of SNELL.", "section": "1 Introduction"}, {"figure_path": "AbZyNGWfpN/figures/figures_19_2.jpg", "caption": "Figure 2: Overview of our SNELL strategy. Given two learnable low-rank matrices, we merge them using a non-linear kernel function (left). This merging process is equivalent to mapping the matrices to higher-rank matrices and then performing matrix multiplication. Then we sparsified this merged adaptation matrix using a competition-based sparsification mechanism (right). This mechanism zeros out weights with small absolute values based on the specified percentage of s.", "description": "This figure illustrates the SNELL framework, which consists of two main stages: kernelized LoRA and competition-based sparsification.  The left side shows how two low-rank matrices (A and B) are merged using a nonlinear kernel function, effectively increasing the rank of the resulting adaptation matrix (\u0394W).  This allows for a more expressive sparse tuning compared to standard LoRA. The right side depicts the competition-based sparsification process, which dynamically prunes less important weights based on a sparsity ratio (s), avoiding the need to explicitly store tunable weight indexes and thus reducing memory usage.", "section": "3.2 Kernelized LORA"}]