{"references": [{"fullname_first_author": "Alexanderson, S.", "paper_title": "Listen, denoise, action! Audio-driven motion synthesis with diffusion models", "publication_date": "2023-MM-DD", "reason": "This paper is highly relevant because it introduces a method for audio-driven motion synthesis using diffusion models, a technique also employed in the current paper's approach."}, {"fullname_first_author": "Lee, H.-Y.", "paper_title": "Dancing to music", "publication_date": "2019-MM-DD", "reason": "This is a foundational paper in music-to-motion generation that the current paper builds upon and improves."}, {"fullname_first_author": "Dhariwal, P.", "paper_title": "Diffusion models beat gans on image synthesis", "publication_date": "2021-MM-DD", "reason": "This paper establishes the effectiveness of diffusion models in image synthesis, a core technique leveraged by the current work for motion and music generation."}, {"fullname_first_author": "Radford, A.", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-MM-DD", "reason": "This paper introduces a method for learning transferable visual models, relevant to the current work's approach of aligning motion and music modalities."}, {"fullname_first_author": "Yu, J.", "paper_title": "Long-term rhythmic video soundtracker", "publication_date": "2023-MM-DD", "reason": "This paper addresses long-term rhythmic synchronization in audio-visual data, a key challenge also tackled by the presented research."}]}