[{"figure_path": "YscR3LBIi7/tables/tables_1_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art audio-visual generation works, including but not limited to motion-music generation.", "description": "This table compares MoMu-Diffusion with other state-of-the-art audio-visual generation methods.  It shows whether each method supports joint generation (music and motion created simultaneously), uses pre-training, handles long-term synthesis, and utilizes a latent space for representation.", "section": "1 Introduction"}, {"figure_path": "YscR3LBIi7/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art audio-visual generation works, including but not limited to motion-music generation.", "description": "This table compares MoMu-Diffusion with other state-of-the-art methods for audio-visual generation, focusing on aspects like joint generation, pre-training, long-term synthesis capabilities, and the use of latent space.  It highlights MoMu-Diffusion's strengths in handling long-term synthesis and utilizing latent space, contrasting it with methods that may lack these capabilities or rely on pre-training.", "section": "1 Introduction"}, {"figure_path": "YscR3LBIi7/tables/tables_6_1.jpg", "caption": "Table 4: Results on the Figure Skating with beat-matching metrics.", "description": "This table presents the results of beat-matching metrics for the Figure Skating dataset.  The metrics used are Beats Coverage Scores (BCS), Coverage Standard Deviation (CSD), Beat Hit Scores (BHS), Hit Standard Deviation (HSD), and F1 score.  These metrics quantitatively evaluate how well the synthesized music aligns with the ground truth music in terms of rhythmic beat synchronization.  The table compares the performance of MoMu-Diffusion against several baseline methods (Foley, CMT, D2MGAN, CDCD, and LORIS) across two different lengths of Figure Skating sequences: 25 seconds and 50 seconds. Higher BCS and BHS, and lower CSD and HSD values generally indicate better beat synchronization.", "section": "5.1 Motion-to-Music Generation"}, {"figure_path": "YscR3LBIi7/tables/tables_7_1.jpg", "caption": "Table 5: Results on the AIST++ Dance and BHS Dance datasets with beat-matching metrics.", "description": "This table presents the performance comparison of MoMu-Diffusion against two baseline methods (D2M and DiffGesture) on two datasets (AIST++ Dance and BHS Dance) for music-to-motion generation. The beat-matching performance is evaluated using five metrics: BCS\u2191, CSD\u2193, BHS\u2191, HSD\u2193, and F1\u2191.  Higher values for BCS\u2191 and BHS\u2191 indicate better beat alignment, while lower values for CSD\u2193 and HSD\u2193 represent improved beat consistency. F1\u2191 is the harmonic mean of BCS\u2191 and BHS\u2191, providing a holistic measure of beat-matching accuracy. The results show that MoMu-Diffusion significantly outperforms both baselines, demonstrating superior beat-matching capabilities.", "section": "5.2 Music-to-Motion Generation"}, {"figure_path": "YscR3LBIi7/tables/tables_7_2.jpg", "caption": "Table 6: Results on the AIST++ Dance and BHS Dance datasets with generation quality metrics.", "description": "This table presents the quantitative results of music-to-motion generation on two datasets: AIST++ Dance and BHS Dance.  The metrics used are FID (Fr\u00e9chet Inception Distance), measuring the realism of the generated motion, Diversity, reflecting the variety of motions generated, and Mean KLD (Kullback-Leibler Divergence), indicating the difference between the generated motion and the original motion.  Lower FID and Mean KLD values are better, whereas higher Diversity is preferred. The table compares the performance of three methods: D2M, DiffGesture, and the authors' proposed MoMu-Diffusion.  MoMu-Diffusion shows significantly better performance in terms of realism and diversity, with lower values on FID and Mean KLD, and higher Diversity scores.", "section": "5.2 Music-to-Motion Generation"}, {"figure_path": "YscR3LBIi7/tables/tables_8_1.jpg", "caption": "Table 7: Ablation study on motion-to-music and music-to-motion generations. We use the FAD/FID as the quality assessment and the F1 score as the beat-matching assessment.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components and design choices on the MoMu-Diffusion model.  Seven different model variants are compared, each removing or modifying a specific aspect of the model, such as using directional vectors for motion encoding, removing the mel-spectrogram, disabling rhythmic contrastive learning, or removing the feed-forward transformer. The performance of each variant is assessed using FAD and FID scores for music quality and F1 scores for beat-matching accuracy. This allows the authors to isolate and quantify the contribution of different parts of their model.", "section": "5.3 Analysis and Ablation Study"}, {"figure_path": "YscR3LBIi7/tables/tables_14_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art audio-visual generation works, including but not limited to motion-music generation.", "description": "This table compares MoMu-Diffusion with other state-of-the-art audio-visual generation methods, focusing on its ability to perform joint generation, pretraining, long-term synthesis, and latent space usage.  It highlights MoMu-Diffusion's advantages in these areas compared to other models.", "section": "1 Introduction"}, {"figure_path": "YscR3LBIi7/tables/tables_14_2.jpg", "caption": "Table 1: Comparison with the state-of-the-art audio-visual generation works, including but not limited to motion-music generation.", "description": "This table compares MoMu-Diffusion with other state-of-the-art audio-visual generation methods, focusing on aspects like joint generation capability, pre-training, long-term synthesis, and the use of latent spaces.  It highlights MoMu-Diffusion's advantages in handling long-term motion-music synthesis and its use of a latent space for multi-modal generation.", "section": "1 Introduction"}, {"figure_path": "YscR3LBIi7/tables/tables_16_1.jpg", "caption": "Table 10: Ablation study of the cross-guidance step T<sub>c</sub> on the AIST++ Dance dataset.", "description": "This table presents the ablation study results focusing on the impact of the cross-guidance step (T<sub>c</sub>) in the MoMu-Diffusion model.  It shows how varying the value of T<sub>c</sub> (from 0.9T to 0.1T) affects both music quality (FAD and F1 scores) and motion quality (FID and F1 scores) during the joint generation process on the AIST++ Dance dataset. The results highlight the importance of finding the optimal balance between multi-modal alignment and quality of generated output.", "section": "5 Experiments"}]