[{"figure_path": "YscR3LBIi7/figures/figures_0_1.jpg", "caption": "Figure 1: The pipeline of MoMu-Diffusion. MoMu-Diffusion integrates the alignment of motion and music through the novel Bidirectional Contrastive Rhythmic Auto-Encoder (BiCoR-VAE). Leveraging the aligned latent space, MoMu-Diffusion facilitates both cross-modal and multi-modal generations.", "description": "This figure illustrates the architecture of MoMu-Diffusion, a novel framework for generating long-term and synchronous motion-music sequences.  It starts with a motion sequence and music mel-spectrogram which are encoded into an aligned latent space by a Bidirectional Contrastive Rhythmic Variational Auto-Encoder (BiCoR-VAE). This aligned latent space is then fed into a multi-modal Transformer-based diffusion model that generates various types of motion-music sequences including cross-modal (motion-to-music or music-to-motion), multi-modal (joint generation), and variable-length generations while maintaining beat matching. ", "section": "1 Introduction"}, {"figure_path": "YscR3LBIi7/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the proposed MoMu-Diffusion framework. MoMu-Diffusion contains two integral components: a bidirectional contrastive rhythmic Variational Autoencoder (BiCoR-VAE) designed to learn the aligned latent space, and a Transformer-based diffusion model responsible for sequence generation. This framework is adept at facilitating both cross-modal and multi-modal joint generations, offering a robust approach to the integrated synthesis of motion and music.", "description": "This figure shows a detailed overview of the MoMu-Diffusion framework. It illustrates the two main components: a BiCoR-VAE (Bidirectional Contrastive Rhythmic Variational Autoencoder) for aligning latent representations of motion and music, and a Transformer-based diffusion model for generating sequences.  The figure visually explains how these components work together to enable cross-modal (motion-to-music and music-to-motion), multi-modal, and variable-length generation. It showcases the process of modality alignment, cross-guidance sampling strategy and the final sequence generation.", "section": "3 Bidirectional Contrastive Rhythmic VAE (BiCoR-VAE)"}, {"figure_path": "YscR3LBIi7/figures/figures_6_1.jpg", "caption": "Figure 8: Example of beat matching on the AIST++ Dance (motion-to-music). The red dashes indicate the extracted musical beats. The red arrow points to the video frame at that particular moment.", "description": "This figure shows examples of beat matching in the motion-to-music generation task using the AIST++ Dance dataset.  It visually compares the extracted musical beats (produced by the model) with the ground truth musical beats, alongside the corresponding video frames. Red dashes highlight the detected beats, and red arrows point to the relevant video frames, demonstrating the temporal synchronization achieved between the generated music and the input motion.", "section": "5.1 Motion-to-Music Generation"}, {"figure_path": "YscR3LBIi7/figures/figures_7_1.jpg", "caption": "Figure 11: Example of beat matching on the AIST++ Dance (music-to-motion). The red dashes indicate the extracted kinematic beats of the synthesized motion. The red arrow points to the frame of the synthesized motion sequence at that particular moment.", "description": "This figure shows an example of beat matching in music-to-motion generation using the proposed MoMu-Diffusion model. The top panel displays the generated motion sequence, with each frame showing the 2D pose of a dancer. The middle panel shows the kinematic beats extracted from the generated motion, represented as vertical bars. The bottom panel displays the reference musical beats from the input music, also shown as vertical bars. Red arrows highlight specific frames to illustrate correspondence between motion, kinematic beats, and musical beats.  The figure demonstrates the model's ability to generate motion that is synchronized with the rhythm of the input music.", "section": "5.2 Music-to-Motion Generation"}, {"figure_path": "YscR3LBIi7/figures/figures_8_1.jpg", "caption": "Figure 6: Results of human evaluation on motion-to-music and music-to-motion generations.", "description": "This figure presents the results of a user study comparing the realism and quality of motion-to-music and music-to-motion generation using different methods: MoMu-Diffusion (with and without BiCoR-VAE), LORIS, D2M, and real data.  The bar chart displays the percentage of participants who preferred each method's generated results in terms of realism and match between the generated content and the reference music/motion. The results show that MoMu-Diffusion with BiCoR-VAE outperforms other methods in both tasks, demonstrating the effectiveness of the proposed model and the importance of the aligned latent space.", "section": "5.3 Analysis and Ablation Study"}, {"figure_path": "YscR3LBIi7/figures/figures_17_1.jpg", "caption": "Figure 7: Three failure cases and the corrected results.", "description": "The figure shows three examples of failure cases and their corresponding corrected results in the model's generation of human motion. In each example, the left side presents the failure cases where there are some distortions or abnormalities in the generated motion. The right side displays the corrected results where the distortions or abnormalities have been removed, and the generated motion is improved.", "section": "F More Qualitative Results"}, {"figure_path": "YscR3LBIi7/figures/figures_18_1.jpg", "caption": "Figure 8: Example of beat matching on the AIST++ Dance (motion-to-music). The red dashes indicate the extracted musical beats. The red arrow points to the video frame at that particular moment.", "description": "This figure shows examples of beat matching results for the motion-to-music generation task on the AIST++ Dance dataset.  Each row represents a different dance clip.  The top section displays frames from the video. The middle section displays the musical beats generated by the MoMu-Diffusion model. The bottom section displays the ground truth musical beats. Red dashes highlight the detected beats, and red arrows indicate the corresponding video frame for visual comparison. The figure aims to demonstrate the model's ability to accurately align generated music with the rhythm of the input motion.", "section": "5.1 Motion-to-Music Generation"}, {"figure_path": "YscR3LBIi7/figures/figures_19_1.jpg", "caption": "Figure 8: Example of beat matching on the AIST++ Dance (motion-to-music). The red dashes indicate the extracted musical beats. The red arrow points to the video frame at that particular moment.", "description": "This figure shows examples of beat matching results for motion-to-music generation on the AIST++ Dance dataset.  Each example displays a sequence of video frames, along with a representation of the musical beats generated by the MoMu-Diffusion model (in blue) and the ground truth musical beats (in red).  Red dashes mark the detected beats, and red arrows highlight the corresponding frame in the video sequence.", "section": "5.1 Motion-to-Music Generation"}, {"figure_path": "YscR3LBIi7/figures/figures_20_1.jpg", "caption": "Figure 10: Example of beat matching on the Figure Skating (motion-to-music). The red dashes indicate the extracted musical beats. The red arrow points to the video frame at that particular moment.", "description": "This figure shows examples of beat matching in the Figure Skating dataset for motion-to-music generation.  For each example, there are three sections: the original video frames, the musical beats generated by the MoMu-Diffusion model, and the ground truth musical beats. Red dashes mark the detected beats in the generated and ground truth music, aligning them with specific frames in the video. The red arrows highlight the visual correspondence between the beats and the video frames.", "section": "5 Experiments"}, {"figure_path": "YscR3LBIi7/figures/figures_21_1.jpg", "caption": "Figure 11: Example of beat matching on the AIST++ Dance (music-to-motion). The red dashes indicate the extracted kinematic beats of the synthesized motion. The red arrow points to the frame of the synthesized motion sequence at that particular moment.", "description": "This figure shows five examples of music-to-motion generation using the MoMu-Diffusion model.  Each example displays the generated motion sequence (stick figures), the corresponding kinematic beats detected in the generated motion, and the reference musical beats from the input music. The red dashes highlight the detected kinematic beats, and the red arrows indicate the frame of the generated motion sequence that aligns with a specific point in the music.  The visualization demonstrates the model's ability to synchronize the generated motion with the rhythmic structure of the music.", "section": "5.2 Music-to-Motion Generation"}]