[{"figure_path": "xcF2VbyZts/figures/figures_1_1.jpg", "caption": "Figure 1: (a) End-to-end learning-based framework for social relation reasoning. A dedicated neural network is trained end-to-end with full training data. (b) We propose a modular framework with foundation models for social relation reasoning. Our proposed SocialGPT first employs VFMs to extract visual information into textual format, and then perform text-based reasoning with LLMs, using either our manually designed SocialPrompt or optimized prompts.", "description": "This figure compares two different approaches to social relation reasoning. (a) shows a traditional end-to-end learning approach, where a single neural network is trained on labeled image data to directly predict the social relationship. This approach is limited in terms of generalizability and interpretability. (b) shows the proposed SocialGPT framework, which uses a modular design combining Vision Foundation Models (VFMs) and Large Language Models (LLMs). VFMs extract visual information and convert it into a textual description, which is then fed to LLMs for reasoning and prediction. This approach offers better generalizability and interpretability.  SocialPrompt, a manually designed prompt, or an optimized prompt is used to instruct the LLM. ", "section": "1 Introduction"}, {"figure_path": "xcF2VbyZts/figures/figures_1_2.jpg", "caption": "Figure 1: (a) End-to-end learning-based framework for social relation reasoning. A dedicated neural network is trained end-to-end with full training data. (b) We propose a modular framework with foundation models for social relation reasoning. Our proposed SocialGPT first employs VFMs to extract visual information into textual format, and then perform text-based reasoning with LLMs, using either our manually designed SocialPrompt or optimized prompts.", "description": "This figure compares two different approaches to social relation reasoning. (a) shows the traditional end-to-end learning-based framework, which uses a single neural network trained on labeled data. (b) presents the proposed SocialGPT framework, which uses a modular approach that combines vision foundation models (VFMs) and large language models (LLMs). The VFMs extract visual information from the image, which is then converted into text and fed into the LLMs for reasoning. The LLMs generate the final answer.  SocialGPT offers better generalizability and interpretability compared to the end-to-end method.", "section": "1 Introduction"}, {"figure_path": "xcF2VbyZts/figures/figures_2_1.jpg", "caption": "Figure 2: The framework of SocialGPT, which follows the \"perception with VFMs, reasoning with LLMs\" paradigm. SocialGPT converts an image into a social story in the perception phase, and then employs LLMs to generate explainable answers in the reasoning phase with SocialPrompt.", "description": "This figure illustrates the SocialGPT framework.  It shows how the system processes an input image: First, the Vision Foundation Models (VFMs) extract visual information, converting it into a textual \"social story.\" This story includes details obtained from dense captions and task-oriented captions generated by querying a VFM (BLIP-2) about the image using SAM (Segment Anything Model) for object segmentation. The resulting social story, enriched with symbols for easy reference, is then fed into Large Language Models (LLMs) along with a structured prompt called \"SocialPrompt.\" This prompt guides the LLMs to reason about the social story and produce an explainable answer regarding the social relationship depicted in the image.  The SocialPrompt is divided into four segments: System, Expectation, Context, and Guidance, each playing a crucial role in the reasoning process.", "section": "3 SocialGPT"}, {"figure_path": "xcF2VbyZts/figures/figures_4_1.jpg", "caption": "Figure 3: An example of social story generation.", "description": "This figure shows an example of how the SocialGPT framework generates a social story from an input image.  The process begins with an image depicting a family at a golf course.  The image is processed using the Segment Anything Model (SAM) to identify objects and people, which are labeled with symbols for reference.  BLIP-2 is then used to generate detailed captions for each object or person.  Finally, all the information extracted is fused into a coherent and human-readable textual story. This story provides detailed information on the objects and people present, as well as the setting and scene of the image. The created social story serves as input for the LLM during the reasoning phase.", "section": "3 SocialGPT"}, {"figure_path": "xcF2VbyZts/figures/figures_5_1.jpg", "caption": "Figure 2: The framework of SocialGPT, which follows the \"perception with VFMs, reasoning with LLMs\" paradigm. SocialGPT converts an image into a social story in the perception phase, and then employs LLMs to generate explainable answers in the reasoning phase with SocialPrompt.", "description": "This figure illustrates the SocialGPT framework, which consists of two main phases: perception and reasoning. In the perception phase, Vision Foundation Models (VFMs) process the input image and convert it into a textual social story. This story includes a detailed description of the image content, including objects and their relationships. The reasoning phase uses Large Language Models (LLMs) to analyze the social story and answer questions about the social relations depicted in the image. The LLMs are guided by a structured prompt called SocialPrompt, which ensures the LLMs provide explainable answers. The overall framework demonstrates the combination of visual perception and textual reasoning for social relation recognition.", "section": "3 SocialGPT"}, {"figure_path": "xcF2VbyZts/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization results of interpretability. We show the SocialGPT perception and reasoning process. We see that our model predicts correct social relationships with plausible explanations.", "description": "This figure visualizes the interpretability of the SocialGPT model.  It shows the three stages of the process: 1) The input image showing people interacting; 2) The segmentation masks generated by SAM, highlighting the identified individuals; 3) The social story generated by the model based on the image content and 4) The model's output (answer) with an explanation justifying the relationship prediction. The explanations provided by the model demonstrate its ability to provide clear and reasoned outputs. This highlights SocialGPT's capability of not only correctly identifying social relationships but also explaining its reasoning process, making it a more interpretable model compared to traditional, black-box methods.", "section": "Qualitative Analysis"}, {"figure_path": "xcF2VbyZts/figures/figures_9_1.jpg", "caption": "Figure 4: Visualization results of interpretability. We show the SocialGPT perception and reasoning process. We see that our model predicts correct social relationships with plausible explanations.", "description": "This figure visualizes SocialGPT's ability to correctly identify social relationships and provide reasonable explanations.  It shows the system's process, starting from the input image, to the creation of SAM masks, a generated social story, and finally, an interpretable answer.  Two examples are given to illustrate both successful predictions and the reasoning process leading to these predictions.  The examples highlight the model's ability to accurately interpret visual cues and translate them into a coherent narrative that the LLM can readily use to make its determination.", "section": "5.4 Qualitative Analysis"}, {"figure_path": "xcF2VbyZts/figures/figures_13_1.jpg", "caption": "Figure 6: The comparisons of the default SAM masks and our SAM masks.", "description": "This figure shows the comparison between the default SAM masks and the SAM masks generated by the proposed method in the paper. The default SAM masks tend to over-segment objects into multiple fine-grained parts, while the proposed method produces more comprehensive masks. For example, in the top row, the default SAM masks segment the three girls into many small regions of hair, face, hands, etc. The improved method produces larger, coherent masks that better represent the objects as wholes. The same is true for the lower row of images, showing a group of women.  The improved masks should be more suitable for creating image captions and for social relation reasoning.", "section": "A More Implementation Details"}, {"figure_path": "xcF2VbyZts/figures/figures_14_1.jpg", "caption": "Figure 2: The framework of SocialGPT, which follows the \"perception with VFMs, reasoning with LLMs\" paradigm. SocialGPT converts an image into a social story in the perception phase, and then employs LLMs to generate explainable answers in the reasoning phase with SocialPrompt.", "description": "This figure illustrates the SocialGPT framework which consists of two main phases: the perception phase and the reasoning phase.  In the perception phase, Vision Foundation Models (VFMs) are used to process an input image and convert it into a textual representation called a \"social story.\" This social story contains information about the people and objects in the image, including their attributes and relationships.  The reasoning phase then uses Large Language Models (LLMs) to analyze this social story and answer questions about the social relationships depicted in the image. The LLMs are guided by a structured prompt called SocialPrompt, which helps to ensure that the answers are accurate and interpretable. The whole framework is a modular design that combines VFMs and LLMs for better social relation recognition.", "section": "3 SocialGPT"}]