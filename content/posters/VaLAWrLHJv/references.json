{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-00-00", "reason": "This paper introduces LoRA, the core method improved upon in this paper."}, {"fullname_first_author": "Alex Wang", "paper_title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "publication_date": "2018-00-00", "reason": "This paper describes GLUE, a benchmark dataset used for evaluating the performance of the models in this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-00", "reason": "This paper introduces Llama 2, one of the large language models used in the experiments of this paper."}, {"fullname_first_author": "Kaiming He", "paper_title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "publication_date": "2015-00-00", "reason": "This paper introduces the Kaiming initialization, a method used in the initialization strategy of LoRA."}, {"fullname_first_author": "Xavier Glorot", "paper_title": "Understanding the difficulty of training deep feedforward neural networks", "publication_date": "2010-00-00", "reason": "This paper discusses Xavier initialization, which is related to the initialization methods discussed in this paper."}]}