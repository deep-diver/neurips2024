[{"figure_path": "VaLAWrLHJv/tables/tables_6_1.jpg", "caption": "Table 1: Results of fine-tuning T5-base using Full-FT and various LoRA variants on a subset of GLUE.", "description": "This table presents the results of fine-tuning the T5-base model on a subset of the GLUE benchmark using several methods: full fine-tuning (Full-FT), vanilla LoRA, and various LoRA variants.  The performance is evaluated based on the average accuracy across five different tasks within the GLUE benchmark: MNLI, SST-2, CoLA, QNLI, and MRPC.  The table allows for a comparison of the performance and efficiency of different parameter-efficient fine-tuning techniques against full fine-tuning.", "section": "4.1 Experiments on Natural Language Understanding"}, {"figure_path": "VaLAWrLHJv/tables/tables_7_1.jpg", "caption": "Table 2: Results of fine-tuning Llama 2-7b using Full-FT and various LoRA variants, tested on MT-Bench, GSM8K, and Human-eval. LoRA-GA significantly outperforms Vanilla LoRA and approaches the performance of Full Finetune. Unless otherwise specified, the LoRA rank is set to 8.", "description": "This table presents the performance comparison of different fine-tuning methods on three downstream tasks: MT-Bench (multi-turn dialogue), GSM8K (mathematics), and HumanEval (code generation).  The methods compared include full fine-tuning (Full), vanilla LoRA, several LoRA variants (PISSA, rsLoRA, LoRA+, DORA, AdaLoRA), and the proposed LoRA-GA method.  The results demonstrate that LoRA-GA significantly outperforms vanilla LoRA and achieves performance comparable to full fine-tuning, especially on the GSM8K and HumanEval tasks, even when using a relatively low rank (8).  Results are also shown for LoRA-GA with higher ranks (32 and 128) which further improve the performance.  The table highlights the effectiveness of LoRA-GA in accelerating convergence and improving performance across different model sizes and task types.", "section": "4.2 Experiment on Large Language Model"}, {"figure_path": "VaLAWrLHJv/tables/tables_7_2.jpg", "caption": "Table 1: Results of fine-tuning T5-base using Full-FT and various LoRA variants on a subset of GLUE.", "description": "This table presents the results of fine-tuning a T5-base model on a subset of the GLUE benchmark using several methods: Full Fine-tuning (Full-FT), Vanilla LoRA, several LoRA variants (PISSA, rsLoRA, LoRA+, AdaLoRA), and LoRA-GA.  The performance is measured by the average accuracy across different tasks within the GLUE subset (MNLI, QNLI, SST-2, CoLA, and MRPC).  The table highlights the superior performance of LoRA-GA compared to the baseline methods, demonstrating its effectiveness in achieving performance comparable to full fine-tuning.", "section": "4.1 Experiments on Natural Language Understanding"}, {"figure_path": "VaLAWrLHJv/tables/tables_7_3.jpg", "caption": "Table 4: Performance of different settings in the ablation study. Results are shown for MT-Bench, GSM8K, and Human-eval on Llama 2 7b, as well as the average performance on a subset of GLUE on T5-Base. Detailed results can be found in Table 9.", "description": "This table presents the results of an ablation study comparing the performance of different initialization methods for LoRA on several benchmark datasets (MT-Bench, GSM8K, Human-eval) and a subset of GLUE. The methods compared include the baseline LoRA, LoRA with Gaussian initialization, LoRA with stable output (+SO), LoRA with gradient approximation (+GA), and the proposed LoRA-GA method.  The table shows the performance of each method in terms of various metrics specific to the benchmark datasets, providing a quantitative comparison to highlight the impact of each component of LoRA-GA.", "section": "4.3 Ablation Study"}, {"figure_path": "VaLAWrLHJv/tables/tables_8_1.jpg", "caption": "Table 1: Results of fine-tuning T5-base using Full-FT and various LoRA variants on a subset of GLUE.", "description": "This table presents the results of fine-tuning the T5-base model on a subset of the GLUE benchmark using different methods: Full Fine-tuning (Full-FT), vanilla LoRA, several LoRA variants (PISSA, rsLoRA, LoRA+, AdaLoRA), and LoRA-GA.  The performance is measured by the average accuracy across five GLUE tasks (MNLI, SST-2, CoLA, QNLI, and MRPC), with the size of each task's dataset shown in the table.  The results highlight the performance improvements achieved by LoRA-GA compared to other methods, especially on smaller datasets.  The \u00b1 values represent the standard deviation observed over 3 different random seeds for each experiment.", "section": "4.1 Experiments on Natural Language Understanding"}, {"figure_path": "VaLAWrLHJv/tables/tables_9_1.jpg", "caption": "Table 6: Performance comparison of initialization schemes on GSM8k using models trained on MetaMathQA subset.", "description": "This table presents the performance comparison of three different initialization schemes (ArB2r, A2rBr, and Random) for the LoRA-GA method on the GSM8k dataset.  Models were trained on a subset of the MetaMathQA dataset. The \"Performance\" column shows the accuracy achieved by each initialization scheme, demonstrating the impact of the initialization strategy on model performance.", "section": "4.6 Impact of Sampled Batch Size"}, {"figure_path": "VaLAWrLHJv/tables/tables_9_2.jpg", "caption": "Table 7: Gradient similarity metrics (vs. batch size 2048) and model performance on GSM8k using models trained on MetaMathQA subset.", "description": "This table presents the results of an experiment evaluating the impact of different batch sizes on the quality of gradient approximation in the LoRA-GA method.  Two metrics, Sign Similarity and Magnitude Similarity, measure how well the gradients from smaller batch sizes approximate the gradients from a larger (2048) batch size, serving as a proxy for the full dataset.  The table also shows the model's performance (on the GSM8k dataset) corresponding to each batch size.  This helps to assess the tradeoff between the accuracy of gradient approximation and model performance with varying batch sizes. ", "section": "4.6 Impact of Sampled Batch Size"}, {"figure_path": "VaLAWrLHJv/tables/tables_17_1.jpg", "caption": "Table 8: Coverage of gradient matrix across different layers in LLaMA 2-7B", "description": "This table presents the coverage of the gradient matrix across different layers in the LLaMA 2-7B model for three different LoRA ranks (8, 32, and 128).  The mean and minimum coverage values are shown for each rank, indicating how well the low-rank approximation captures the gradient information.  Higher ranks generally lead to better coverage.", "section": "B.2 Evaluating the Rank of the Gradient Matrix"}, {"figure_path": "VaLAWrLHJv/tables/tables_17_2.jpg", "caption": "Table 1: Results of fine-tuning T5-base using Full-FT and various LoRA variants on a subset of GLUE.", "description": "This table presents the results of fine-tuning a T5-base model on a subset of the GLUE benchmark using different methods: full fine-tuning (Full-FT), vanilla LoRA, several LoRA variants, and LoRA-GA.  The table shows the accuracy achieved by each method on five different GLUE tasks (MNLI, SST-2, CoLA, QNLI, MRPC) and an average accuracy across all tasks.  This demonstrates the performance improvements achieved by LoRA-GA compared to other methods, particularly in achieving performance comparable to full fine-tuning.", "section": "4.1 Experiments on Natural Language Understanding"}, {"figure_path": "VaLAWrLHJv/tables/tables_17_3.jpg", "caption": "Table 10: Performance comparison of different methods on MT-Bench, GSM8K, and Human-eval with learning rate le-5", "description": "This table presents a comparison of the performance of several methods (Full Fine-tuning, LoRA, PiSSA, rsLoRA, LoRA+, and LoRA-GA) on three different benchmark datasets (MT-Bench, GSM8K, and Human-eval).  The results are specifically for a learning rate of 1e-5.  The table showcases the performance differences between the methods, highlighting LoRA-GA's improved performance compared to the baseline LoRA method.", "section": "B.4 Experimental result with different learning rate"}, {"figure_path": "VaLAWrLHJv/tables/tables_17_4.jpg", "caption": "Table 11: Performance comparison of different methods on MT-Bench, GSM8K, and Human-eval with learning rate 5e-5", "description": "This table presents the performance comparison of different methods (Full, LoRA, PISSA, rsLoRA, LoRA+, and LoRA-GA) on three different benchmark datasets (MT-Bench, GSM8K, and Human-eval) using Llama 2-7B model. The experiments were conducted with learning rate of 5e-5.  Each entry shows the average performance across different runs and their standard deviation.  The results highlight the relative performance of each method on various tasks including multi-turn dialogue, mathematical reasoning, and code generation.", "section": "4.2 Experiment on Large Language Model"}, {"figure_path": "VaLAWrLHJv/tables/tables_18_1.jpg", "caption": "Table 12: Performance comparison of different methods on full MetaMathQA dataset training for multiple epochs.", "description": "This table presents the performance of three different LoRA variants (LoRA, LoRA+, and LoRA-GA) on the full MetaMathQA dataset across four training epochs.  The results are averaged over two random seeds.  The table demonstrates the performance improvement of LoRA-GA over time, showing its superiority to the other LoRA methods in terms of accuracy on this mathematical reasoning dataset.", "section": "4.5 Experiments on the Full MetaMathQA Dataset"}]