[{"type": "text", "text": "SPEAR: Exact Gradient Inversion of Batches in Federated Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dimitar I. Dimitrov1, Maximilian Baader2, Mark Niklas M\u00fcller2,3, Martin Vechev2 ", "page_idx": 0}, {"type": "text", "text": "1 INSAIT, Sofia University \"St. Kliment Ohridski\" 2 ETH Zurich 3 LogicStar.ai {dimitar.iliev.dimitrov}@insait.ai 1 {mbaader, mark.mueller, martin.vechev}@inf.ethz.ch 2 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning is a framework for collaborative machine learning where clients only share gradient updates and not their private data with a server. However, it was recently shown that gradient inversion attacks can reconstruct this data from the shared gradients. In the important honest-but-curious setting, existing attacks enable exact reconstruction only for batch size of $b=1$ , with larger batches permitting only approximate reconstruction. In this work, we propose SPEAR, the first algorithm reconstructing whole batches with $b>1$ exactly. SPEAR combines insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely fliter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers high-dimensional ImageNet inputs in batches of up to $b\\lesssim25$ exactly while scaling to large networks. Finally, we show theoretically that much larger batches can be reconstructed with high probability given exponential time. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated Learning has emerged as the dominant paradigm for training machine learning models collaboratively without sharing sensitive data [2]. Instead, a central server sends the current model to all clients which then send back gradients computed on their private data. The server aggregates the gradients and uses them to update the model. Using this approach sensitive data never leaves the clients\u2019 machines, aligning it better with data privacy regulations such as the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). ", "page_idx": 0}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/48ee1c17d5bc2ef10206d3ca278187395e492fd1b85610ff0d13f90b2ea6cedb.jpg", "img_caption": ["Figure 1: A sample of four images from a batch of $b=20$ , reconstructed using our SPEAR (top) or the prior state-of-the-art Geiping et al. [1] (mid), compared to the ground truth (bottom). "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Gradient Inversion Attacks Recent work has shown that an honest-but-curious server can use the shared gradient updates to recover the sensitive client data [3, 4]. However, while exact reconstruction was shown to be possible for batch sizes of $b=1$ [5, 6], it was assumed to be infeasible for larger batches. This led to a line of research on approximate methods that sacrificed reconstruction quality in order to recover batches of $b>1$ inputs [7, 8, 9]. In this paper we challenge this fundamental assumption and, for the first time, show that exact reconstruction is possible for batch sizes $b>1$ . ", "page_idx": 0}, {"type": "text", "text": "This Work: Exact Reconstruction of Batches We propose the first gradient inversion attack reconstructing inputs exactly for batch sizes $b>1$ in the honest-but-curious setting. In Fig. 1, we show the resulting reconstructions versus approximate methods [1] for a batch of $b=20$ images. ", "page_idx": 0}, {"type": "text", "text": "Our approach leverages two key properties of gradient updates in fully connected ReLU networks: First, these gradients have a specific low-rank structure due to small batch sizes $b\\ll n,m$ compared to the input dimensionality $n$ and the hidden dimension $m$ . Second, the (unknown) gradients with respect to the inputs of the first ReLU layer are sparse due to the ReLU function itself. We combine these properties with ideas from sparsely-used dictionary learning [10] to propose a sampling-based algorithm, called SPEAR (Sparsity Exploiting Activation Recovery) and show that it succeeds with high probability for $b<m$ . While SPEAR scales exponentially with batch size $b$ , we provide a highly parallelized GPU implementation, which empirically allows us to reconstruct batches of size up to $b\\lesssim25$ exactly even for large inputs (IMAGENET) and networks (widths up to 2000 neurons and depths up to 9 layers) in around one minute per batch. ", "page_idx": 1}, {"type": "text", "text": "Main Contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 The first gradient inversion attack showing theoretically that exact reconstruction of complete batches with size $b\\!>\\!1$ in the honest-but-curious setting is possible. \u2022 SPEAR: a sampling-based algorithm leveraging low rankness and ReLU-induced sparsity of gradients for exact gradient inversion that succeeds with high probability. \u2022 A highly parallelized GPU implementation of SPEAR, which we empirically demonstrate to be effective across a wide range of settings and make publicly available on GitHub. ", "page_idx": 1}, {"type": "text", "text": "2 Method Overview ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first introduce our setting before giving a high-level overview of our attack SPEAR, whose sketch is shown in Fig. 2. ", "page_idx": 1}, {"type": "text", "text": "Setting We consider a neural network $\\pmb{f}$ containing a linear layer $z\\,=\\,W x\\,+$ $^{b}$ followed by ReLU activations $\\textit{\\textbf{y}}=$ $\\mathrm{ReLU}(z)$ trained with a loss function $\\mathcal{L}$ . Let now $\\boldsymbol{X}\\in\\mathbb{R}^{n\\times b}$ be a batch of $b$ inputs to the linear layer $Z=W X\\!+\\!(b|\\!\\!\\perp\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot\\!\\!\\!\\!|b\\!\\!\\!\\!)$ , with weights $W\\in\\mathbb{R}^{m\\times n}$ , bias $\\pmb{b}\\in\\mathbb{R}^{m}$ and output $\\boldsymbol{Z}\\in\\mathbb{R}^{m\\times b}$ . Further, let $\\mathbfcal{Y}\\in$ $\\mathbb{R}^{m\\times b}$ be the result of applying the ReLU activation to $Z$ , i.e., $Y=\\operatorname{ReLU}(Z)$ and assume $b\\leq m,n$ . The goal of SPEAR is ", "page_idx": 1}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/6f85ff5c76407310dbb0dbdad4bccef8c00435b389b6bb39f48aec0ac754ea28.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 2: Overview of SPEAR. The gradient $\\frac{\\partial\\mathcal{L}}{W}$ is decomposed to $\\boldsymbol{R}$ and $\\textbf{\\emph{L}}$ . Sampling gives $N$ proposal directions, which we fliter down to $c$ candidates via a sparsity criterion with threshold $\\tau*m$ . A greedy selection method selects batchsize $b$ directions. Scale recovery via $\\frac{\\partial{\\mathcal{L}}}{\\partial b}$ returns the disaggregation matrix $Q$ and thus the inputs $\\mathbf{\\deltaX}$ . ", "page_idx": 1}, {"type": "text", "text": "to recover the inputs $\\mathbf{\\deltaX}$ (up to permutation) given the gradients $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial W}}$ and $\\frac{\\partial{\\mathcal{L}}}{\\partial b}$ (see Fig. 2, i). ", "page_idx": 1}, {"type": "text", "text": "Low-Rank Decomposition We first show that the weight gradient $\\begin{array}{r}{{\\frac{\\partial{\\mathcal{L}}}{\\partial\\mathbf{W}}}\\ =\\ {\\frac{\\partial{\\mathcal{L}}}{\\partial{\\boldsymbol{Z}}}}{\\boldsymbol{X}}^{\\top}}\\end{array}$ naturally has a low rank b \u2264 m, n (Theorem 3.1) and can therefore be decomposed as \u2202\u2202WL $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}}=L R}\\end{array}$ with $\\pmb{L}\\in\\mathbb{R}^{m\\times b}$ and $\\b{R}\\in\\mathbb{R}^{b\\times n}$ using SVD (Fig. 2, ii). We then prove the existence disaggregation matrix $\\pmb{Q}\\,=\\,(\\pmb{q}_{1}|\\dots|\\pmb{q}_{b})\\,\\in\\,\\mathrm{GL}_{b}(\\mathbb{R})$ , allowing us to express the inputs as $X^{\\top}=Q^{-1}{\\overline{{R}}}$ and activation gradients as \u2202\u2202ZL $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial Z}=L Q}\\end{array}$ (Theorem 3.2). Next, we leverages the sparsity of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ to recover $Q$ exactly. ", "page_idx": 1}, {"type": "text", "text": "(RSeeLc. U3 .I2n).d ucWeed  thSepna rlseivteyrageW eth issh ospwa rtshiatty  tRoe LshUo lwa ytherast,  inwdituhc eh isgpha rpsreo baactbiivliattyi,o nth egrrea deiexinstts $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ matrices $\\mathbf{\\Delta}_{L_{A}}\\in\\mathbb{R}^{b-1\\times b}$ of $\\textbf{\\emph{L}}$ , such that their kernel is an unscaled column $\\overline{{\\pmb{q}}}_{i}$ of our disaggregation matrix $Q$ , i.e., $\\ker(L_{A})=\\operatorname{span}(\\pmb{q}_{i})$ , for all $i\\in\\{1,\\ldots,b\\}$ (Theorem 3.3). Given these unscaled colmuns $\\overline{{\\pmb q}}_{i}$ , we recover their scale by leveraging the bias gradient $\\frac{\\partial{\\mathcal{L}}}{\\partial b}$ (Theorem 3.5). ", "page_idx": 1}, {"type": "text", "text": "Sampling and Filtering Directions To identify the submatrices $L_{A}$ of $\\textbf{\\emph{L}}$ which induce the directions $\\overline{{\\pmb q}}_{i}$ , we propose a sampling approach (Sec. 4.1): We randomly sample $b-1$ rows of $\\textbf{\\emph{L}}$ to obtain an LA and thus proposal direction q\u2032i = ker(LA) (Fig. 2 iii). Crucially, the product Lq\u2032i = \u2202\u2202zLi recovers a column of the sparse activation gradient $\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{Z}}$ for correct directions $\\overline{{\\pmb q}}_{i}^{\\prime}$ and a dense linear combination of such columns for incorrect ones. This sparsity gap allows the large number $N$ of proposal directions obtained from submatrices $L_{A}$ to be flitered to $c\\gtrsim b$ unique candidates (Fig. 2 iv). ", "page_idx": 1}, {"type": "text", "text": "Greedy Direction Selection We now have to select the correct $b$ directions from our set of $c$ candidates (Fig. 2, v). To this end, we build an initial solution $Q^{\\prime}$ from the $b$ directions inducing the highest sparsity in $\\frac{\\partial\\mathcal{L}}{\\partial Z}^{\\prime}=L Q^{\\prime}$ . To assess the quality of this solution $Q^{\\prime}$ , we introduce the sparsity matching score $\\sigma$ which measures how well the sparsity of the activation gradients $\\frac{\\partial\\mathcal{L}}{\\partial Z}^{\\prime}$ matches the ReLU activation pattern induced by the reconstructed input $X^{\\prime\\top}=Q^{\\prime-1}R$ . Finally, we greedily optimize $Q^{\\prime}$ to maximize the sparsity matching score, by iteratively replacing an element $\\ensuremath{\\boldsymbol{q}}_{i}^{\\prime}$ of $Q^{\\prime}$ with the candidate direction $\\pmb q_{j}^{\\prime}$ yielding the greatest improvement in $\\sigma$ until convergence. We can then validate the resulting input $X^{\\top}=Q^{-1}R$ by checking whether it induces the correct gradients. We formalize this as Alg. 1 in Sec. 5 and show that it succeeds with high probability for $b<m$ . ", "page_idx": 2}, {"type": "text", "text": "3 Gradient Inversion via Sparsity and Low-Rankness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we will demonstrate that both low rankness and sparsity arise naturally for gradients of fully connected ReLU networks and explain theoretically how we recover $\\mathbf{\\deltaX}$ . Specifically, in Sec. 3.1, we first argue that\u2202\u2202WL = \u2202\u2202ZL follows direclty from the chain rule. We then show that for every decomposition $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial W}=L R}\\end{array}$ , there exists an unknown disaggregation matrix $Q$ allowing us to reconstruct $X^{\\top}=Q^{-1}\\bar{R}$ and $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial Z}=L Q}\\end{array}$ . The remainder of the section then focuses on recovering $Q$ . To this end, we show in Sec. 3.2 that ReLU layers induce sparsity in $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ , which we then leveraged in Sec. 3.3 to reconstruct the columns of $Q$ up to scale. Finally, in Sec. 3.4, we show how the scale of $Q$ \u2019s columns can be recovered from $\\frac{\\partial{\\mathcal{L}}}{\\partial b}$ . Unless otherwise noted, we defer all proofs to App. B. ", "page_idx": 2}, {"type": "text", "text": "3.1 Explicit Low-Rank Representation of $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}}$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first show that the weight gradients $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial W}}$ can be written as follows: ", "page_idx": 2}, {"type": "text", "text": "Theorem 3.1. The network\u2019s gradient w.r.t. the weights $W$ can be represented as the matrix product: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}}=\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Z}}\\pmb{X}^{T}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For batch sizes $b\\leq n,m$ , the dimensionalities of $\\frac{\\partial\\mathcal{L}}{\\partial Z}\\in\\mathbb{R}^{m\\times b}$ and $\\boldsymbol{X}\\in\\mathbb{R}^{n\\times b}$ in Eq. 1 directly yield that the rank of $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}}$ is at most $b$ . This confirms the observations of Kariyappa et al. [9] and shows that $\\mathbf{\\deltaX}$ and $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ correspond to a specific low-rank decomposition of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial W}}$ . ", "page_idx": 2}, {"type": "text", "text": "To actually find this decomposition and thus recover $\\mathbf{\\deltaX}$ , we first consider an arbitrary decomposition of the form\u2202\u2202WL $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial W}\\,=\\,L R}\\end{array}$ , where $\\pmb{L}\\in\\mathbb{R}^{m\\times b}$ and $\\b{R}\\in\\mathbb{R}^{b\\times n}$ are of maximal rank. We chose the decomposition obtained via the reduced SVD decomposition of\u2202\u2202WL $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{W}}=\\boldsymbol{U}\\boldsymbol{S}\\boldsymbol{V}}\\end{array}$ by setting $L=U S^{\\frac{1}{2}}$ and $R=S^{\\frac{1}{2}}V$ , where $U\\in\\mathbb{R}^{n\\times b}$ , $\\boldsymbol{S}\\in\\mathbb{R}^{b\\times b}$ and $\\boldsymbol{V}\\in\\mathbb{R}^{b\\times n}$ . We now show that there exists an unique disaggregation matrix $Q$ recovering $\\mathbf{\\deltaX}$ and $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ from $\\textbf{\\emph{L}}$ and $\\boldsymbol{R}$ : ", "page_idx": 2}, {"type": "text", "text": "Theorem 3.2. If the gradient $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ and the input matrix $\\mathbf{\\deltaX}$ are of full-rank and $b\\leq n,m_{*}$ , then there exists an unique matrix $Q\\in\\mathbb{R}^{b\\times b}$ of full-rank s.t. $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial Z}=L Q}\\end{array}$ and $X^{T}=Q^{-1}R.$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 3.2 is a direct application of Lemma B.1 shown in App. B, a general linear algebra result stating that under most circumstances different low-rank matrix decompositions can be transformed into each other via an unique invertible matrix. Crucially, this implies that recovering the input $\\mathbf{\\deltaX}$ and the gradient \u2202\u2202ZL matrices is equivalent to obtaining the unique disaggregation matrix $Q$ . Next, we show how the ReLU-induced sparsity patterns in $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ or $\\mathbf{\\deltaX}$ can be leveraged to recover $Q$ exactly. ", "page_idx": 2}, {"type": "text", "text": "3.2 ReLU-Induced Sparsity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "tRhee LcUo nascitdievraetido lni nlaeyare rlsa ycearn)  ionrd iunc te hsep ianrspiutty  (bifo tthh ei nR tehLe Ug raacdtiievnatt $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ p(rife ctehde eRs ethLeU l iancetiavr altaioyne rs).ucceeds ", "page_idx": 2}, {"type": "text", "text": "Gradien Sparsity If a ReLU activation succeeds the linear layer, i.e., $Y=\\operatorname{ReLU}(Z)$ , we have $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial Z}=\\frac{\\partial\\mathcal{L}}{\\partial Y}\\,\\bar{\\odot}\\,\\mathbb{I}_{[Z>0]}}\\end{array}$ , where $\\odot$ is the elementwise multiplication and $\\mathbb{1}_{[Z>0]}$ is a matrix of 0s and 1s with each entry indicating if the corresponding entry in $Z$ is positive. At initialization, roughly half of the entries in $Z$ are positive, making $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ sparse with $\\sim0.5$ of the entries $=0$ . ", "page_idx": 2}, {"type": "text", "text": "Input Sparsity ReLUs also introduce sparsity if the linear layer in question is preceded by a ReLU activation. Here, $X=\\operatorname{ReLU}(\\tilde{Z})$ will again be sparse with $\\sim0.5$ of the entries $=0$ at initialization. Note that for all but the first and the last layer of a fully connected network, we have sparsity in both, $\\mathbf{\\deltaX}$ and $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ . Due to the symmetry of their formulas in Theorem 3.2, our method can be applied in iasl l stpharresee ,a rciosrirnegs pspoanrdsiintyg  steot ttihneg sf.ir sItn  ltahyee rr eomf aai nfudlelry  ocfo tnhnise cwteodr kn, etwwe oarsks.u mWee  nw.ol.wo .dge. stchraitb eo nhloy $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ leverage this sparsity to compute the disaggregation matrix $Q$ and thus recover the input batch $\\mathbf{\\deltaX}$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Breaking Aggregation through Sparsity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our exact recovery algorithm for the disaggregation matrix $Q$ is based on the following insight: ", "page_idx": 3}, {"type": "text", "text": "If we can construct two submatrices $\\mathbf{A}\\in\\mathbb{R}^{b-1\\times b}$ and $L_{A}\\in\\mathbb{R}^{b-1\\times b}$ by choosing $b-1$ rows with the same indices from $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ and $\\textbf{\\emph{L}}$ , respectively, such that $\\pmb{A}$ has full rank and an all-zero $i^{\\mathrm{th}}$ column, then the kernel $\\ker(L_{A})$ of $L_{A}$ contains a column $\\pmb q_{i}$ of $Q$ up to scale. We formalize this as follows: Theorem 3.3. Let $\\mathbf{A}\\in\\mathbb{R}^{b-1\\times b}$ be a submatrix of $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ s.t. its $i^{t h}$ column is 0 for some $i\\in\\{1,\\ldots,b\\}$ . Further, let $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ , $\\mathbf{\\deltaX}$ , and $\\pmb{A}$ be of full rank and $Q$ be as in Theorem 3.2. Then, there exists a full-rank submatrix $\\breve{L_{A}}\\in\\mathbb{R}^{b-1\\times b}$ of $\\textbf{\\emph{L}}$ s.t. $\\mathrm{span}(\\pmb{q}_{i})=\\ker(\\pmb{L}_{A})$ for the $i^{t h}$ column $\\pmb q_{i}$ of $Q=(\\pmb{q}_{1}|\\cdot\\cdot\\cdot|\\pmb{q}_{b})$ . ", "page_idx": 3}, {"type": "text", "text": "Proof. Pick an $i\\in\\{1,\\ldots,b\\}$ . By assumption, there exists a submatrix $\\pmb{A}\\in\\mathbb{R}^{b-1\\times b}$ of $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ of rank $b\\!-\\!1$ whose $i^{\\mathrm{th}}$ column is 0. To construct $L_{A}$ , we take rows from $\\textbf{\\emph{L}}$ with indices corresponding to $\\pmb{A}$ \u2019s row indices in $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ . As $\\frac{\\partial{\\mathcal{L}}}{\\partial{Z}}$ and $\\mathbf{\\deltaX}$ have full rank, by Theorem 3.2, we know that $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial Z}=\\mathbf{\\bar{\\boldsymbol{L}}}Q}\\end{array}$ , and hence $A=L_{A}Q$ . Multiplying from the right with $e_{i}$ yields $0=A e_{i}=L_{A}Q e_{i}\\stackrel{\\leftarrow}{=}L_{A}q_{i}$ , and hence $\\ker(L_{A})\\supseteq\\operatorname{span}(\\pmb{q}_{i})$ . Further, as $\\operatorname{rank}(A)=b-1$ and $\\operatorname{rank}(Q)=b$ , we have that $\\bar{\\mathrm{rank}}({\\cal L}_{A})=$ $b-1$ . By the rank-nullity theorem $\\dim(\\ker(L_{A}))=1$ and hence $\\ker(L_{A})=\\operatorname{span}({q}_{i})$ . \u53e3 ", "page_idx": 3}, {"type": "text", "text": "As $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ is not known a priori, we can not simply search for such a set of rows. Instead, we have to sample submatrices of at random and then filter them using the approach discussed in Sec. 4. However, we will show in Sec. 5.2 that we will find suitable submatrices with high probability for $b<m$ due to the sparsity of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ and the large number $\\binom{m}{b-1}$ of possible submatrices. We will now discuss how to recover the scale of the columns $\\pmb q_{i}$ given their unscaled directions $\\overline{{\\pmb q}}_{i}$ forming $\\overline{{Q}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.4 Obtaining $Q$ : Recovering the Scale of columns in $\\overline{{Q}}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a set of $b$ correct directions ${\\overline{{Q}}}\\,=\\,({\\overline{{q_{1}}}}|\\cdot\\cdot\\cdot|{\\overline{{q_{b}}}})$ , we can recover their scale, enabling us to reconstruct $\\mathbf{\\deltaX}$ , as follows. We first represent the correctly scaled columns as $q_{i}=s_{i}\\cdot\\overline{{\\pmb{q}_{i}}}$ with the unknown scale parameters $s_{i}\\in\\mathbb{R}$ . Now, recovering the scale is equivalent to computing all $s_{i}$ . To this end, we leverage the gradient w.r.t. the bias $\\frac{\\partial{\\mathcal{L}}}{\\partial b}$ ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.4. The gradient w.r.t. the bias b can be written in the form $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{b}}=\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Z}}\\left[\\,\\frac{1}{\\vdots}\\,\\right]}\\end{array}$ Thus, the coefficients $s_{i}$ can be calculated as: Theorem 3.5. For any left inverse $L^{-L}$ of $\\textbf{\\emph{L}}$ , we have $\\left[\\begin{array}{c}{s_{1}}\\\\ {\\vdots}\\\\ {s_{b}}\\end{array}\\right]=\\overline{{Q}}^{\\,-1}L^{-L}\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{b}}$ ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.5 allows us to directly obtain the true matrix $Q={\\overline{{Q}}}\\operatorname{diag}(s_{1},\\ldots,s_{b})$ from the unscaled matrix $\\overline{{Q}}$ . We now discuss how to recover $\\overline{{Q}}$ via sampling and filtering candidate directions $\\overline{{\\pmb q}}_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "4 Efficient Filtering and Validation of Candidates ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the previous section, we saw that given the correct selection of submatrices $L_{A}$ , we can recover $Q$ directly. However, we do not know how to pick $L_{A}$ a priori. To solve this, we rely on a sampling approach: We first randomly sample submatrices $L_{A}$ of $\\textbf{\\emph{L}}$ and corresponding direction candidates $\\overline{{\\pmb q}}^{\\prime}$ spanning $\\ker(L_{A})$ . However, checking whether $\\overline{{\\pmb q}}^{\\prime}$ is a valid direction is not straightforward as we do not know $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ and hence can not observe $\\pmb{A}$ directly as reconstructing $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial Z}=L Q}\\end{array}$ requires the full $Q$ ", "page_idx": 3}, {"type": "text", "text": "To address this, we fliter the majority of wrong proposals $\\overline{{\\pmb q}}^{\\prime}$ using deduplication and a sparsity-based criterion (Sec. 4.1), leaving us with a set of candidate directions $\\stackrel{\\mathcal{C}}{C}=\\{\\mathbf{\\dot{\\overline{{q}}}}_{j}^{\\prime}\\}_{j\\in\\{1,\\dots,c\\}}$ . We then select the correct directions in $\\mathcal{C}$ greedily based on a novel sparsity matching score (Sec. 4.2). ", "page_idx": 4}, {"type": "text", "text": "4.1 Efficient Filtering of Directions $\\overline{{\\pmb q}}^{\\prime}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Filtering Mixtures via Sparsity It is highly likely $\\begin{array}{r}{(p=(1-\\frac{1}{2^{b-1}})^{b})}\\end{array}$ that a random submatrix of $\\textbf{\\emph{L}}$ will not correspond to an $\\pmb{A}$ with any 0 column. We fliter these directions by leveraging the following insight. The kernel of such submatrices is spanned by a linear combination $\\overline{{\\pmb q}}^{\\prime}=\\sum_{i}^{}\\dot{\\alpha_{i}}\\overline{{\\pmb q}}_{i}$ . Thus $L\\overline{{{q}}}^{\\prime}$ will be a linear combination of sparse columns of $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ . As this sparsity structure is random, linear combinations will have much lower sparsity with high probability. We thus discard all candidates $\\overline{{\\pmb q}}^{\\prime}$ with sparsity of $L\\overline{{{q}}}^{\\prime}$ below a threshold $\\tau$ , chosen to make the probability of falsely rejecting a correct direction $\\begin{array}{r}{p_{f r}(\\tau,m)=\\frac{1}{2^{m}}\\sum_{i=0}^{\\lfloor m\\cdot\\tau\\rfloor}\\binom{m}{i}}\\end{array}$ , obtained from the cumulative distribution function of the binomial distribution, small. For example for $m=400$ and $p_{f r}(\\tau,m)<10^{-5}$ , we have $\\tau=0.395$ We obtain the candidate pool $\\mathcal{C}=\\{\\overline{{\\pmb{q}}}_{j}^{\\prime}\\}_{j\\in\\{1,...,c\\}}$ from all samples that were not filered this way. ", "page_idx": 4}, {"type": "text", "text": "Filtering Duplicates As it is highly likely to have multiple full-rank submatrices $\\pmb{A}$ , whose $i^{\\mathrm{th}}$ column is 0, we expect to sample the same proposal $\\overline{{\\pmb q}}_{i}^{\\prime}$ multiple times. We remove these duplicates to substantially reduce our search space. ", "page_idx": 4}, {"type": "text", "text": "4.2 Greedy Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While filtering duplicates and linear combinations significantly reduces the number $c$ of candidates, we usually still have to select a subset of $b<c$ . Thus, we have $\\binom{c}{b}$ possible $b$ sized subsets, each inducing a candidate $Q^{\\prime}$ and thus $X^{\\prime}$ . A naive approach is to compute the gradients for all $X^{\\prime}$ and compare them to the ground truth. However, this is computationally infeasible even for moderate $c$ . To address this, we propose a greedy two-stage procedure optimizing a novel sparsity matching score $\\lambda$ , which resolves the computational complexity issue above while also accurately selecting the correct batch elements and relying solely on \u2202\u2202ZL and $Z^{\\prime}$ . As both can be computed directly via $Q^{\\prime}$ , the procedure is local and does not need to backpropagate gradients. Next, we explain the first stage. ", "page_idx": 4}, {"type": "text", "text": "Dictionary Learning [10] As a first stage, we leverage a component of the algorithm proposed by Spielman et al. [10] for sparsely-used dictionary learning. This approach is based on the insight that the subset of column vectors $\\boldsymbol{B}=\\{\\overline{{\\boldsymbol{q}}}_{i}^{\\prime}\\}_{i=1}^{b}$ , yielding the sparsest full-rank gradient matrix $\\textstyle{\\frac{\\partial L}{\\partial Z}}$ \u2202L is often correct. As the scaling of $\\overline{{\\pmb q}}_{i}^{\\prime}$ does not change the sparsity of the resulting $\\textstyle{\\frac{\\partial L}{\\partial Z}}$ , we can construct the subset $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ by greedily collecting the $b$ directions $\\overline{{\\pmb q}}_{i}^{\\prime}$ with the highest corresponding sparsity that still increase the rank of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . While this method typically recovers most directions $\\overline{{\\pmb q}}_{i}$ , it often misses directions whose gradients $\\frac{\\partial L}{\\partial z_{i}}$ are less sparse by chance. ", "page_idx": 4}, {"type": "text", "text": "Sparsity Matching We alleviate this issue by introducing a second stage to the algorithm where we greedily optimize a novel correctness measure based solely on the gradients of the linear layer, which we call the sparsity matching coefficient $\\lambda$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1. Let $\\lambda_{-}$ be the number of non-positive entries in $_{z}$ whose corresponding entries in \u2202\u2202ZL are 0. Similarly, let \u03bb+ be the number of positive entries in Z whose corresponding entries in $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ are not 0. We call their normalized sum the sparsity matching coefficient $\\lambda$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda=\\frac{\\lambda_{-}+\\lambda_{+}}{m\\cdot b}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, this describes how well the pre-activation values $Z$ match the sparsity pattern of the gradients $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ induced by the ReLU layer (See Sec. 3.2). While this sparsity matching coefficient $\\lambda$ can take values between 0 and 1, it is exactly $\\lambda=1$ for the correct $\\mathbf{\\deltaX}$ , if the gradient $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Y}}$ w.r.t. the ReLU output is dense, which is usually the case. We note that $\\lambda$ can be computed efficiently for arbitrary full rank matrix Q\u2032 by computing \u2202\u2202ZL\u2032 $\\frac{\\partial\\mathcal{L}}{\\partial Z}^{\\prime}=L Q^{\\prime}$ and $Z^{\\prime}=W X^{\\prime}+(b|\\ldots|b)$ for $X^{\\prime\\top}=Q^{\\prime-1}R$ . ", "page_idx": 4}, {"type": "text", "text": "To optimize $\\lambda$ , we initialize $\\overline{{\\boldsymbol{Q}}}^{\\prime}$ with the result of the greedy algorithm in Spielman et al. [10], and then greedily swap the pair of vectors $\\overline{{\\pmb q}}_{i}^{\\prime}$ improving $\\lambda$ the most, while keeping the rank, until convergence. ", "page_idx": 4}, {"type": "text", "text": "5 Final Algorithm and Complexity Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we first present our final algorithm SPEAR (Sec. 5.1) and then analyse its expected complexity and failure probability (Sec. 5.2). ", "page_idx": 5}, {"type": "text", "text": "5.1 Final Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We formalize our gradient inversion attack SPEAR in Alg. 1 and outline it below. First, we compute the low-rank decomposition $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial W}=L R}\\end{array}$ of the weight gradient $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial W}}$ via reduced SVD, allowing us to recover the batch size b as the rank of \u2202\u2202WL (Line 2). We now sample (at most $N$ ) submatrices $L_{A}$ of $\\textbf{\\emph{L}}$ and compute proposal directions $\\overline{{\\pmb q}}_{i}^{\\prime}$ as their kernel $\\ker(L_{A})$ via SVD (Lines $4-$ 5). We note that our implementation parallelizes both sampling and SVD computation (Lines 4\u20135) on a GPU. We then fliter the proposal directions $\\overline{{\\pmb q}}_{i}^{\\prime}$ based on their sparsity (Line 6), adding them to our candidate pool $\\mathcal{C}$ ", "page_idx": 5}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/5cc92bee179a616bea8be632bc364c6084783a7d41497e265553c1659a9c1afe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "if they haven\u2019t been recovered already and are sufficiently sparse (Line 7). Once our candidate pool contains at least $b$ directions, we begin constructing candidate input reconstructions $X^{\\prime}$ using our two-stage greedy algorithm GREEDYFILTER (Line 8), discussed in Sec. 4.2. If this reconstruction leads to a solution with sparsity matching coefficient $\\lambda\\,=\\,1$ , we terminate early and return the corresponding solution (Line 9). Otherwise, we continue sampling until we have reached $N$ samples and return the best reconstruction we can obtain from the resulting candidate pool (Line 14). The pseudocode for COMPUTESIGMA (Alg. 2) and GREEDYFILTER (Alg. 3) are shown in App. C. ", "page_idx": 5}, {"type": "text", "text": "5.2 Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we will analyze SPEAR w.r.t. the number of submatrices we expect to sample until we have recovered all $b$ correct directions $\\overline{{\\pmb q}}_{i}$ (Lemma 5.2), and the probability of failing to recover all $b$ correct directions despite checking all possible submatrices of $\\textbf{\\emph{L}}$ (Lemma 5.3). For an analysis of the number of submatrices we have to sample until we have recovered all $b$ correct directions $\\overline{{\\pmb q}}_{i}$ with high probability, we point to Lemma B.2. Further, as before, we defer all proofs also to App. B. ", "page_idx": 5}, {"type": "text", "text": "Expected Number of Required Samples To determine the expected number of required samples until we have recovered the correct $b$ direction vectors $\\overline{{\\pmb q}}_{i}$ , we first compute a lower bound on the probability $q$ of sampling a submatrix which satisfies the conditions of Theorem 3.3 for an arbitrary column $i$ in $\\overline{{Q}}$ and then use the coupon collector problem to compute the expected number of required samples. ", "page_idx": 5}, {"type": "text", "text": "We can lower bound the probability of a submatrix $\\boldsymbol{A}\\in\\mathbb{R}^{b-1\\times b}$ , randomly sampled as $b\\!-\\!1$ rows of $\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{Z}}$ , having exactly one all-zero column and being full rank as follows: ", "page_idx": 5}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/abd8ed499becea8833aa50b8f285b9164e0609c92463ba13b67910f0f434a677.jpg", "img_caption": ["Figure 3: Visualizations of the u ound $(p_{\\mathrm{fail}}^{\\mathrm{ub}}$ failure probability of SPEAR for different batch sizes $b$ and network widths $m$ for $p_{f r}=10^{-9}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Lemma 5.1. Let $A~\\in~\\mathbb{R}^{b-1\\times b}$ be submatrix of the gradient $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ obtained by sampling $b\\mathrm{~-~}1$ rows uniformly at random without replacement, where each element of $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ is distributed i.i.d. as $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Z}_{j,k}}=\\zeta|\\boldsymbol{\\epsilon}|}\\end{array}$ with $\\epsilon\\sim\\mathcal{N}(\\mu=0,\\sigma^{2}>0)$ and $\\zeta\\sim B e r n o u l l i(p=\\textstyle{\\frac{1}{2}})$ ). We \u2202thZen have the probability $q$ of $\\pmb{A}$ having exactly one all-zero column and being full rank lower bounded by: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nq\\geq{\\frac{b}{2^{b-1}}}\\left(1-({\\frac{1}{2}}+o_{b-1}(1))^{b-1}\\right)\\geq{\\frac{b}{2^{b-1}}}(1-0.939^{b-1}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We can now compute the expected number of submatrices $n_{\\mathrm{total}}^{*}$ we have to draw until we have recovered all $b$ correct direction vectors using the Coupon Collector Problem: ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.2. Assuming i.i.d. submatrices $\\pmb{A}$ following the distribution outlined in Lemma 5.1 and using Alg. 1, we have the expected number of submatrices $n_{t o t a l}^{*}$ required to recover all b correct direction vectors as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nn_{t o t a l}^{*}=\\frac{1}{q}\\sum_{k=0}^{b-1}\\frac{b}{b-k}=\\frac{b H_{b}}{q}\\approx\\frac{1}{q}(b\\log(b)+\\gamma b+\\frac{1}{2}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $H_{b}$ is the $b^{t h}$ harmonic number and $\\gamma\\approx0.57722$ the Euler-Mascheroni constant. ", "page_idx": 6}, {"type": "text", "text": "We validate this result experimentally in Fig. 4 where we observe excellent agreement for wide networks $m\\gg b$ ) and obtain, e.g., $n_{\\mathrm{total}}^{*}\\approx1.8\\times10^{5}$ for a batch size of $b=16$ . ", "page_idx": 6}, {"type": "text", "text": "Failure Probability We now analyze the probability of SPEAR failing despite considering all possible submatrices of $\\textbf{\\emph{L}}$ and obtain: ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.3. Under the same assumptions as in Lemma 5.1, we have an upper bound on the failure probability $p_{f a i l}^{u b}$ of Alg. 1 even when sampling exhaustively as: ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{f a i l}^{u b}\\leq b\\left(1-\\sum_{k=b-1}^{m}{\\binom{m}{k}}\\frac{1}{2^{m}}\\left(1-0.939^{(b-1)\\binom{k}{b-1}}\\right)\\right)+1-(1-p_{f r})^{b},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p_{f r}$ is the probability of falsely rejecting a correct direction $\\overline{{\\pmb q}}^{\\prime}$ via our sparsity fliter (Sec. 4.1). ", "page_idx": 6}, {"type": "text", "text": "If we assume the full-rankness of submatrices $\\pmb{A}$ to i) occur with probability $\\begin{array}{r}{1-(\\frac{1}{2}-o_{b-1}(1))^{b-1}}\\end{array}$ for $o_{b-1}(1)\\approx0$ (true for large $b$ [11]) and ii) be independent between submatrices, we instead obtain: ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\mathrm{fail}}^{\\mathrm{approx}}\\approx1-\\left(\\sum_{k=b-1}^{m}{\\binom{m}{k}}\\frac{1}{2^{m}}\\left(1-0.5^{(b-1){\\binom{k}{b-1}}}\\right)\\right)^{b}+1-(1-p_{f r})^{b}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We illustrate this bound in Fig. 3 and empirically validate this bound in Fig. 8 and observe the true failure probability to lie between pfaapilpr and pfuabil. ", "page_idx": 6}, {"type": "text", "text": "6 Empirical Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we empirically evaluate the effectiveness of SPEAR on MNIST [13], CIFAR-10 [14], TINYIMAGENET [15], and IMAGENET [16] across a wide range of settings. In addition to the reconstruction quality metrics PSNR and LPIPS, commonly used to evaluate gradient inversion attacks, we report accuracy as the portion of batches for which we recovered the batch up to numerical errors and the number of sampled submatrices (number of iterations). ", "page_idx": 6}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/0f54f366cec81848759cdc0f5cef659e0b43287073dd446d6ae43f8d202adab9.jpg", "table_caption": ["Table 1: Comparison to prior work in the image domain. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Experimental Setup For all experiments, we use our highly parallelized PyTorch [17] GPU implementation of SPEAR. Unless stated otherwise, we run all experiments on CIFAR-10 batches of size $b=20$ using a 6 layer ReLU-activated FCNN with width $m=200$ and set $\\tau$ to achieve a false rejection rate of $\\bar{p}_{f r}\\leq\\mathrm{i}0^{-5}$ . We supply ground truth labels to all methods except SPEAR. ", "page_idx": 6}, {"type": "text", "text": "6.1 Comparison to Prior Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Table 1, we compare SPEAR against prior gradient inversion attacks from the image domain on the IMAGENET dataset rescaled to $256\\times256$ resolution. In particular, we compare to Geiping et al. $[1]^{1}$ , as well as, the recent CI-Net [12]. As CI-Net only considers networks with the less common Sigmoid activations, we report its performance on both ReLU and Sigmoid versions of our network. ", "page_idx": 6}, {"type": "text", "text": "We observe that while CI-Net obtains very good reconstructions with the Sigmoid network (PSNR of 38), SPEAR still achieves a much higher PSNR (124) as it is exact. Further, for the more common ReLU acti", "page_idx": 7}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/2b28613b080497be93766cb745f4de4f9f61ee29c9ce3bd7d0b87dae608f9993.jpg", "table_caption": ["Table 2: Results vs prior work in the tabular domain. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "vations, the performance of CI-Net drops significantly to a $\\mathrm{PSNR}<16$ compared to 19.6 for Geiping et al. [1]. Additionally, SPEAR is much faster compared to both Geiping et al. [1] and CI-Net, taking $10\\times$ and $100\\times$ less time, respectively. Finally, we want to emphasize that both prior works rely on strong prior knowledge, including label information and knowledge of the structure of images, whereas we assume no information at all about the data distribution and still achieve much better results in only a fraction of the time taken. ", "page_idx": 7}, {"type": "text", "text": "To confirm the versatility of SPEAR, we compare it to the SoTA attack in the tabular domain, Tableak [8], in Table 2. We see that due to the exact nature of our attack, we recover both continuos and discrete features better on the ADULT dataset [18] with $b=16$ , while still being $6\\times$ faster. ", "page_idx": 7}, {"type": "text", "text": "6.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate SPEAR on MNIST, CIFAR-10, TINYIMAGENET and IMAGENET at two different resolutions, reporting results in Table 3. Across datasets, SPEAR ", "page_idx": 7}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/e0ff12851d580917e97a099f07dec201c41623b7c5fba3a3e4f181ac19fe7573.jpg", "table_caption": ["Table 3: Reconstruction quality across 100 batches. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "can reconstruct almost all batches perfectly, achieving PSNRs of 100 and above even at a batch size of $b=20$ for images as large as $720\\times720$ in $<3$ minutes. We provide additional results on heterogeneous data and trained networks in App. E, as well as, on the FedAvg protocol in App. F. ", "page_idx": 7}, {"type": "text", "text": "Effect of Batch Size $b$ We evaluate the effect of batch size $b$ on accuracy and the required number of iterations $n_{\\mathrm{total}}^{*}$ for a wide $m=2000)$ and narrow $(m=200)$ ) network. While $n_{\\mathrm{total}}^{*}$ increases exponentially with $b$ , for both networks, the narrower network requires about 20 times more iterations than the wider network (see Fig. 4). While trends for the wider network $(m\\gg b)$ are perfectly described by our theoretical results in Sec. 5.2, some independence assumptions are violated for the narrower network, explaining the larger number of required iterations. While we can recover all batches perfectly for the wider network, we see a sharp drop in accuracy from $99\\%$ at $b=20$ to $63\\%$ at $b=24$ (see Fig. 6) for the narrower network. This is due to increasingly more batches requiring more than the $N=2\\times\\bar{10}^{9}$ submatrices we sample at most. ", "page_idx": 7}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/4279591e9a11573e5f3ea5786c620a3363929370dfb49b1eb6e8d0b6f91b2975.jpg", "img_caption": ["Figure 4: Effect of batch size $b$ on the number of required submatrices. Expectation from Lemma 5.2 dashed and median ( $10^{\\mathrm{th}}$ to $90^{\\mathrm{th}}$ percentile shaded) depending on network width $m$ solid. We always evaluate $10^{4}$ submatrices in parallel, explaining the plateau. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Effect of Network Architecture We visualize the performance of SPEAR across different network widths and depths in Fig. 5. We observe that while accuracy is independent of both (given sufficient width $m\\gg b$ ), the number of required iterations reduces with increasing width $m$ . We provide further ablations on the effect of our two-stage flitering in App. E.3 and DPSGD noise in App. E.6. ", "page_idx": 7}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/01d60f26da802826f08e3a71a18f5ef649011586d45f1e2abdfbea8d89e6749f.jpg", "img_caption": ["Figure 5: Accuracy (green) and number of median iterations (blue) for different network widths $m$ at $L=6$ (left) and depths $L$ at $m=200$ (right). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Effect of Layer Depth Our experiments so far focused on recovering inputs to the first layer of FCNNs. However, SPEAR\u2019s capabilities extend beyond this, as highlighted in Sec. 3.2. To demonstrate this, we use SPEAR to reconstruct the inputs to all FC layers followed by a ReLU activation in a 6-layer FCNN with a width of $m=400$ at initialization. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The results, presented in Table 4, show that SPEAR successfully recovers the inputs to all layers almost perfectly. However, attacking later layers is more computationally expensive. Specifically, the runtime for $l=5$ increases to 70 minutes/batch resulting in 17 batches that timed-out. This increased computational cost is due to the initialization of the network, which causes the outputs of later layers to be dominated by their bias terms with their inputs being almost irrelevant. This issue is mitigated after a few training steps, as weights and biases adjust to better reflect the relationships between inputs and outputs. We find that after 5000 gradient steps the time per batch reduces to $<1$ min at an accuracy of $>95\\%$ for layer $l=5$ . ", "page_idx": 8}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/2964514875f662b950ac89c209a72fa6b038c919bbb266a694a265c039cd8693.jpg", "table_caption": ["Table 4: Effect of the attacked layer\u2019s depth l $1\\le l\\le6)$ on reconstruction time and quality for 100 TINYIMAGENET batches of size $b=20$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.3 Scaling SPEAR via Optimization-based Attacks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As we prove theoretically in Sec. 5.2 and verify practically in App. E.5, in the common regime where the batch size $b$ is much smaller than dimensions of the attacked linear layer w.h.p. the input information is losslessly represented in the client gradient. However, in practice for $b>25$ the exponential sampling complexity of SPEAR becomes a bottleneck that prevents the recovery of the input (see Fig. 4). ", "page_idx": 8}, {"type": "text", "text": "Table 5: Comparison between the reconstruction quality of Geiping et al. [1] and a version of SPEAR that uses Geiping et al. [1] to speed up its search procedure evaluated on 10 TINYIMAGENET batches. ", "page_idx": 8}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/41c7378411405172c686bde7d24db71ce0d8dac09571c7b7b511e8ab638623de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In this section, we propose a method for alleviating the exponential sampling complexity by combining SPEAR with an approximate reconstruction method to get a prior on which submatrices $L_{A}$ satisfy the conditions of Theorem 3.3, i.e., have corresponding matrices $\\pmb{A}$ containing a 0-column. To this end, we first obtain an estimate of the client pre-activation values $\\widetilde{Z}$ based on the approximate input reconstructions from Geiping et al. [1]. As large negative pre-activation values inZ are much more likely to correspond to negative pre-activation values in the true $Z$ , and, thus,  to 0s in gradients $\\frac{\\partial\\mathcal{L}}{\\partial Z}$ , we record the locations of the $3b$ largest negative values for each column of $\\widetilde{Z}$ . Importantly, by choosing the locations this way, we ensure that each group of 3b locations correspond to locations of likely 0s in same column of $\\frac{\\partial\\boldsymbol{\\dot{\\mathcal{L}}}}{\\partial\\boldsymbol{Z}}$ . Restricting the sampling of the row indices of $L_{A}$ and $\\pmb{A}$ only within each group of locations, ensures that $L_{A}$ is very likely to satisfying the conditions of Theorem 3.3. ", "page_idx": 8}, {"type": "text", "text": "We confirm the effectiveness of this approach in a preliminary study, shown in Table 5, that demonstrates the combined approach allows a substantial increase in the batch size SPEAR can scale to (up to 100), thus effectively eliminating its exponential complexity. The results show that the combined approach drastically improves the reconstruction quality of Geiping et al. [1] as well, as unlike Geiping et al. [1], it achieves exact reconstruction. Importantly, we observe that even for the 4 batches SPEAR failed to recover in Table 5, SPEAR still reconstructs $>97$ of the 100 directions $\\overline{{\\pmb{q}_{i}}}$ correctly, suggesting that future work can further improve upon our results. ", "page_idx": 8}, {"type": "text", "text": "6.4 Feature Inversion in Convolutional Neural Networks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Following the Cocktail Party Attack (CPA) [9], we experiment with using SPEAR to recover the input features to the first linear layer of a pretrained VGG16 convolutional network with size $25088\\times4096$ for IMAGENET batches of $b=16$ and use them in a feature inversion (FI) attack to approximately recover the client images. We ", "page_idx": 8}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/721a517a62749544d7c34f93b1732baeea06c0a60061a4a03a8a16955122fce9.jpg", "table_caption": ["Table 6: Comparison between the reconstructions on VGG16 for Geiping et al. [1], CPA [9], and SPEAR for 10 IMAGENET batches $\\mathit{b}=16$ ). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "show the results of our experiments, based on the CPA\u2019s code and parameters, in Table 6. We see the inverted features drastically improve quality of the final reconstructions, and that SPEAR achieves almost perfect feature cosine similarity, resulting in better overall reconstruction versus CPA. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we discuss how we relate to prior work. ", "page_idx": 9}, {"type": "text", "text": "Gradient Inversion Attacks Since gradient inversion attacks have been introduced [3], two settings have emerged: In the malicious setting, the server does not adhere to the training protocol and can adversarially engineer network weights that maximize leaked information [19, 20, 21, 22]. In the strictly harder honest-but-curious setting, the server follows the training protocol but still aims to reconstruct client data. We target the honest-but-curious setting, where prior work has either recovered the input exactly for batch sizes of $b=1$ [5, 6], or approximately for $b>1$ [1, 23, 7, 8, 9]. In this setting, we are the first to reconstruct inputs exactly for batch sizes $b>1$ . ", "page_idx": 9}, {"type": "text", "text": "Most closely related to our work is Kariyappa et al. [9] which leverage the low-rank structure of the gradients to frame gradient inversion as a blind source separation problem, improving their approximate reconstructions. In contrast, we derive an explicit low-rank representation and additionally leverage gradient sparsity reconstruct inputs exactly. ", "page_idx": 9}, {"type": "text", "text": "Unlike a long line of prior work, we rely neither on any priors on the data distribution [8, 24, 25, 26] nor on a reconstructed classification label [1, 27, 7, 23, 8]. This allows our approach to be employed in a much wider range of settings where neither is available. ", "page_idx": 9}, {"type": "text", "text": "Defenses Against Gradient Inversion Defenses based on Differential Privacy [28] add noise to the computed gradients on the client side, providing provable privacy guarantees at the cost of significantly reduced utility. Another line of work increases the empirical difficulty of inversion by increasing the effective batch size, by securely aggregating gradients from multiple clients [29] or doing multiple gradient update steps locally before sharing an aggregated weight update [4]. Finally, different heuristic defenses such as gradient pruning [3] have been proposed, although their effectiveness has been questioned [30]. ", "page_idx": 9}, {"type": "text", "text": "Sparsely-used Dictionary learning Recovering the disaggregation matrix $Q$ is related to the well-studied problem of sparsely-used dictionary learning. However, there the aim is to find the sparsest coefficient matrix (corresponding to our ${\\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathcal{Z}}}}.$ ) and dense dictionary $(Q^{-1})$ approximately encoding a signal $(L)$ . In contrast, we do not search for the sparsest solution yielding an approximate reconstruction but a solution that exactly induces consistent $\\mathbf{\\deltaX}$ and $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ , which happens to be sparse. Sparsely-used dictionary learning is known to be NP-hard [31] and typically solved approximately [32, 10, 33]. However, under sufficient sparsity, it can be solved exactly in polynomial time [10]. While our \u2202\u2202ZL are not sparse enough, we still draw inspiration from Spielman et al. [10] in Sec. 4. ", "page_idx": 9}, {"type": "text", "text": "8 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We focus on recovering the inputs to fully connected layers with ReLU activations such as they occur at the beginning of fully connected networks or as aggregation layers of many other architectures. Extending our approach to other layers is an interesting direction for future work. ", "page_idx": 9}, {"type": "text", "text": "Further, our approach scales exponentially with batch size $b$ . While SPEAR\u2019s massive parallelizability and its ability to be combined with optimization-based attacks, as shown in Sec. 6.3, can partially mitigate the computational complexity, future research is still required to make reconstruction of batches of size $b>100$ practical. ", "page_idx": 9}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose SPEAR, the first algorithm permitting batches of $b>1$ elements to be recovered exactly in the honest-but-curious setting. We demonstrate theoretically and empirically that SPEAR succeeds with high probability and that our highly parallelized GPU implementation is effective across a wide range of settings, including batches of up to 25 elements and large networks and inputs. ", "page_idx": 9}, {"type": "text", "text": "We thereby demonstrate that contrary to prior belief, an exact reconstruction of batches is possible in the honest-but-curious setting, suggesting that federated learning on ReLU networks might be inherently more susceptible than previously thought. To still protect client privacy, large effective batch sizes, obtained, e.g., via secure aggregation across a large number of clients, might prove instrumental by making reconstruction computationally intractable. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially funded by the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure). ", "page_idx": 10}, {"type": "text", "text": "This work has been done as part of the EU grant ELSA (European Lighthouse on Secure and Safe AI, grant agreement no. 101070617) . Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission. Neither the European Union nor the European Commission can be held responsible for them. ", "page_idx": 10}, {"type": "text", "text": "The work has received funding from the Swiss State Secretariat for Education, Research and Innovation (SERI). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and Michael Moeller. Inverting gradientshow easy is it to break privacy in federated learning? NeurIPS, 2020.   \n[2] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017. [3] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In NeurIPS, 2019.   \n[4] Dimitar Iliev Dimitrov, Mislav Balunovi\u00b4c, Nikola Konstantinov, and Martin Vechev. Data leakage in federated averaging. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id $=$ e7A0B99zJf.   \n[5] Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. Privacypreserving deep learning via additively homomorphic encryption. IEEE Trans. Inf. Forensics Secur., (5), 2018.   \n[6] Junyi Zhu and Matthew B. Blaschko. R-GAP: recursive gradient attack on privacy. In ICLR, 2021.   \n[7] Jiahui Geng, Yongli Mou, Feifei Li, Qing Li, Oya Beyan, Stefan Decker, and Chunming Rong. Towards general deep leakage in federated learning. arXiv, 2021. [8] Mark Vero, Mislav Balunovi\u00b4c, Dimitar I. Dimitrov, and Martin T. Vechev. Data leakage in tabular federated learning. ICML, 2022.   \n[9] Sanjay Kariyappa, Chuan Guo, Kiwan Maeng, Wenjie Xiong, G Edward Suh, Moinuddin K Qureshi, and Hsien-Hsin S Lee. Cocktail party attack: Breaking aggregation-based privacy in federated learning using independent component analysis. In International Conference on Machine Learning, pages 15884\u201315899. PMLR, 2023.   \n[10] Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In Conference on Learning Theory, pages 37\u20131. JMLR Workshop and Conference Proceedings, 2012.   \n[11] Konstantin Tikhomirov. Singularity of random bernoulli matrices. Annals of Mathematics, 191 (2):593\u2013634, 2020.   \n[12] Chi Zhang, Zhang Xiaoman, Ekanut Sotthiwat, Yanyu Xu, Ping Liu, Liangli Zhen, and Yong Liu. Generative gradient inversion via over-parameterized networks in federated learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5126\u20135135, 2023.   \n[13] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.   \n[14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[15] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7), 2015. ", "page_idx": 10}, {"type": "text", "text": "[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. ", "page_idx": 11}, {"type": "text", "text": "[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024\u20138035, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html.   \n[18] Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.   \n[19] Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia Shumailov, and Nicolas Papernot. When the curious abandon honesty: Federated learning is not private. arXiv, 2021.   \n[20] Liam H. Fowl, Jonas Geiping, Wojciech Czaja, Micah Goldblum, and Tom Goldstein. Robbing the fed: Directly obtaining private data in federated learning with modified models. In ICLR, 2022.   \n[21] Liam Fowl, Jonas Geiping, Steven Reich, Yuxin Wen, Wojtek Czaja, Micah Goldblum, and Tom Goldstein. Decepticons: Corrupted transformers breach privacy in federated learning for language models. ICLR, 2022.   \n[22] Yuxin Wen, Jonas Geiping, Liam Fowl, Micah Goldblum, and Tom Goldstein. Fishing for user data in large-batch federated learning via gradient magnification. In ICML, 2022.   \n[23] Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, and Pavlo Molchanov. See through gradients: Image batch recovery via gradinversion. In CVPR, 2021.   \n[24] Mislav Balunovi\u00b4c, Dimitar I. Dimitrov, Nikola Jovanovi\u00b4c, and Martin T. Vechev. LAMP: extracting text from gradients with language model priors. In NeurIPS, 2022.   \n[25] Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. Recovering private text in federated learning of language models. Advances in Neural Information Processing Systems, 35:8130\u20138143, 2022.   \n[26] Zhuohang Li, Jiaxin Zhang, Luyang Liu, and Jian Liu. Auditing privacy defenses in federated learning via generative gradient leakage. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10132\u201310142, 2022.   \n[27] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients. arXiv, 2020.   \n[28] Mart\u00edn Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In CCS, 2016.   \n[29] Kallista A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on user-held data. NIPS, 2016.   \n[30] Mislav Balunovi\u00b4c, Dimitar Iliev Dimitrov, Robin Staab, and Martin T. Vechev. Bayesian framework for gradient leakage. In ICLR, 2022.   \n[31] Andreas M Tillmann. On the computational intractability of exact and approximate dictionary learning. IEEE Signal Processing Letters, 22(1):45\u201349, 2014.   \n[32] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):4311\u20134322, 2006.   \n[33] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and the geometric picture. IEEE Transactions on Information Theory, 63(2):853\u2013884, 2016.   \n[34] Terence Tao and Van Vu. On random pm 1 matrices: singularity and determinant. In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pages 431\u2013440, 2005.   \n[35] Ravi Parameswaran. Statistics for experimenters: an introduction to design, data analysis, and model building. JMR, Journal of Marketing Research (pre-1986), 16(000002):291, 1979.   \n[36] Zac Chen. H2 Maths Handbook. Educational Publishing House, 2011. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this work, we demonstrate that contrary to prior belief, an exact reconstruction of batches is possible in the honest-but-curious setting for federated learning. As our work demonstrates the susceptibility of federated learning systems using ReLU networks, this work inevitably advances the capabilities of an adversary. Nonetheless, we believe this to be an important step in accurately assessing the risks and utilities of federated learning systems. ", "page_idx": 13}, {"type": "text", "text": "To still protect client privacy, large effective batch sizes, obtained, e.g., via secure aggregation across a large number of clients, might prove instrumental by making reconstruction computationally intractable. As gradient information and network states can be stored practically indefinitely, our work highlights the importance of proactively protecting client privacy in federated learning not only against current but future attacks. This underlines the importance of related work on provable privacy guarantees obtained via differential privacy. ", "page_idx": 13}, {"type": "text", "text": "B Deferred Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 3.1. The network\u2019s gradient w.r.t. the weights $W$ can be represented as the matrix product: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}}=\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Z}}\\pmb{X}^{T}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. We will use Einstein notation for this proof: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\frac{\\partial C}{\\partial W_{i}^{j}}}={\\frac{\\partial C}{\\partial Z_{i}^{j}}}{\\frac{\\partial Z_{i}^{i}}{\\partial W_{i}^{j}}}}&{}\\\\ {={\\frac{\\partial C}{\\partial Z_{i}^{j}}}{\\frac{\\partial(W_{k}^{m}X_{m}^{m}+\\lambda_{k}^{i}\\delta_{i}^{i})}{\\partial W_{i}^{j}}}}\\\\ {={\\frac{\\partial C}{\\partial Z_{i}^{j}}}{\\frac{\\partial W_{k}^{m}X_{m}^{m}}{\\partial W_{i}^{j}}}}\\\\ {={\\frac{\\partial C}{\\partial Z_{i}^{j}}}{\\frac{\\partial W_{k}^{m}X_{m}^{m}}{\\partial W_{i}^{j}}}{\\frac{\\partial}{\\partial W_{i}^{j}}}}\\\\ {={\\frac{\\partial C}{\\partial Z_{k}^{j}}}{\\frac{\\partial W_{k}^{m}X_{m}^{m}}{\\partial W_{i}^{j}}}{\\frac{\\partial}{\\partial W_{i}^{j}}}}\\\\ {={\\frac{\\partial C}{\\partial Z_{k}^{i}}}{\\frac{\\partial}{\\partial Z_{i}^{j}}}{\\frac{\\partial^{i}}{\\partial Z_{j}^{k}}}{\\frac{\\partial^{i}}{\\partial Z_{k}^{i}}}}\\\\ {={\\frac{\\partial C}{\\partial Z_{i}^{j}}}X_{j}^{i}}\\\\ {={\\frac{\\partial C}{\\partial Z_{j}^{i}}}{\\frac{\\partial^{i}}{\\partial Z_{j}^{k}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We note that $\\delta_{k}^{\\mathrm{~\\it~i~}}$ is the Kronecker delta, that is $\\delta_{k}{}^{i}=1$ if $k=i$ and 0 otherwise. Further, $\\delta^{l}=1$ for all $l$ . Hence we arrive at Eq. 1. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Lemma B.1. Let $b,n,m\\in\\mathbb{N}$ such that $b<n,m$ . Further, let $\\pmb{A},\\pmb{L}\\in\\mathbb{R}^{m\\times b}$ and $\\pmb{B},\\pmb{R}\\in\\mathbb{R}^{b\\times n}\\;l$ be matrices of maximal rank, satisfying $A B=L R$ . Then there exists a unique disaggregation matrix $Q\\in G L_{b}(\\mathbb{R})$ s.t. $A=L Q$ , and $\\bar{B^{=}}\\,\\bar{Q}^{^{-1}}R$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. As $b\\leq n,m$ and the matrices $A\\in\\mathbb{R}^{m\\times b}$ and $\\b{B}\\in\\mathbb{R}^{b\\times n}$ have full rank, we know that there exists ", "page_idx": 13}, {"type": "text", "text": "\u2022 a left inverse $\\mathbf{A}^{-L}\\in\\mathbb{R}^{b\\times m}$ for $\\mathbf{1}\\colon A^{-L}A=I_{b}$ and \u2022 a right inverse $B^{-R}\\in\\mathbb{R}^{n\\times b}$ for $B{\\mathrm{:~}}B B^{-R}=I_{b}$ . ", "page_idx": 13}, {"type": "text", "text": "Thus, it follows from ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal A}^{-L}{\\cal L}R{\\cal B}^{-R}={\\cal A}^{-L}{\\cal A}{\\cal B}{\\cal B}^{-R}={\\cal I}_{b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "that $({\\bf A}^{-L}L)^{-1}=R B^{-R}$ . We now set $Q=R B^{-R}$ . ", "page_idx": 13}, {"type": "text", "text": "This $Q$ satisfies the required properties: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bullet\\ B=Q^{-1}R:}}\\\\ {{\\qquad\\cdot\\ A=L Q:}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nQ^{-1}R=A^{-L}L R=A^{-L}A B=B,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nL Q=L R B^{-R}=A B B^{-R}=A,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Uniqueness: Assume we have $Q_{1}$ and $Q_{2}$ that satisfy $L Q_{1}=L Q_{2}=A$ . As $\\textbf{\\emph{L}}$ is of rank $b$ and $b\\leq m$ , there exists a left inverse ${\\check{L}}^{-L}$ for $L\\colon\\dot{L}^{-L}L=I_{b}$ . Applying this left inverse to $L Q_{\\mathrm{1}}=L Q_{\\mathrm{2}}$ , directly yields $Q_{1}=Q_{2}$ , and hence we get uniqueness. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.4. The gradient w.r.t. the bias b can be written in the form $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{b}}=\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Z}}\\left[\\,\\frac{1}{\\lambda}\\right]}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. We use again Einstein notation. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathcal{L}}{\\partial b_{i}}=\\frac{\\partial\\mathcal{L}}{\\partial Z_{k}^{i}}\\frac{\\partial Z_{k}^{i}}{\\partial b_{i}}}\\\\ &{\\phantom{\\frac{\\partial\\mathcal{L}}{\\partial b_{i}}}=\\frac{\\partial\\mathcal{L}}{\\partial Z_{k}^{i}}\\frac{\\partial\\bigl(W_{k}^{\\,m}X_{m}^{\\,l}+b_{k}\\delta^{l}\\bigr)}{\\partial b_{i}}}\\\\ &{\\phantom{\\frac{\\partial\\mathcal{L}}{\\partial b_{i}}}=\\frac{\\partial\\mathcal{L}}{\\partial Z_{k}^{i}}\\frac{\\partial b_{k}\\delta^{l}}{\\partial b_{i}}}\\\\ &{\\phantom{\\frac{\\partial\\mathcal{L}}{\\partial b_{i}}}=\\frac{\\partial\\mathcal{L}}{\\partial Z_{k}^{i}}\\delta^{i}\\delta^{l}}\\\\ &{\\phantom{\\frac{\\partial\\mathcal{L}}{\\partial b_{i}}}=\\frac{\\partial\\mathcal{L}}{\\partial Z_{i}^{i}}\\delta^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.5. For any left inverse $L^{-L}$ of $\\textbf{\\emph{L}}$ , we have $\\left[\\begin{array}{c}{s_{1}}\\\\ {\\vdots}\\\\ {s_{b}}\\end{array}\\right]=\\overline{{Q}}^{\\,-1}L^{-L}\\frac{\\partial\\mathcal{L}}{\\partial b}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. The proof is straight forward. Using Theorem 3.4 and Theorem 3.2, we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\overline{{{Q}}}^{-1}L^{-L}\\frac{\\partial\\mathcal{L}}{\\partial b}=\\overline{{{Q}}}^{-1}L^{-L}\\frac{\\partial\\mathcal{L}}{\\partial\\overline{{{Q}}}}\\left[\\frac{1}{\\vdots}\\right]}}\\\\ {{\\qquad\\qquad=\\overline{{{Q}}}^{-1}L^{-L}L\\overline{{{Q}}}\\left[\\frac{1}{\\vdots}\\right]}}\\\\ {{\\qquad\\qquad=\\overline{{{Q}}}^{-1}Q\\left[\\frac{1}{\\vdots}\\right]}}\\\\ {{\\qquad\\qquad=\\overline{{{Q}}}^{-1}\\overline{{{Q}}}\\mathrm{diag}(s_{1},\\ldots,s_{b})\\left[\\frac{1}{\\vdots}\\right]}}\\\\ {{\\qquad\\qquad=\\left[\\frac{s_{1}}{s_{b}}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 5.1. Let $A~\\in~\\mathbb{R}^{b-1\\times b}$ be submatrix of the gradient $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ obtained by sampling $b\\mathrm{~-~}1$ rows uniformly at random without replacement, where each element of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ is distributed i.i.d. as $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial Z_{j,k}}=\\zeta|\\epsilon|}\\end{array}$ with $\\epsilon\\sim\\mathcal{N}(\\mu=0,\\sigma^{2}>0)$ and $\\zeta\\sim B e r n o u l l i(p=\\textstyle{\\frac{1}{2}})$ . We then have the probability $q$ of $\\pmb{A}$ having exactly one all-zero column and being full rank lower bounded by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq\\geq{\\frac{b}{2^{b-1}}}\\left(1-({\\frac{1}{2}}+o_{b-1}(1))^{b-1}\\right)\\geq{\\frac{b}{2^{b-1}}}(1-0.939^{b-1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We have the probability of one of the $b$ columns being all zero as $\\frac{b}{2^{b-1}}$ if the network has full rank, all other columns will not be all-zero. ", "page_idx": 15}, {"type": "text", "text": "Further, we have the probability of the submatrix $\\mathbb{1}_{A>0}$ being full rank conditioned on column $i$ being all-zero as the probability of the matrix described by remaining $b-1$ columns being nonsingular. This probability is $1\\stackrel{{\\textstyle-}}{-}({\\textstyle\\frac{1}{2}}+o_{b-1}(1))^{b-1}$ [11] where $\\mathrm{lim}_{b\\to\\infty}\\,o_{b-1}(1)=0$ , which can be lower-bounded with $1-0.939^{b-1}$ [34]. We thus obtain their joint probability as their product. ", "page_idx": 15}, {"type": "text", "text": "Lemma 5.2. Assuming i.i.d. submatrices $\\pmb{A}$ following the distribution outlined in Lemma 5.1 and using Alg. 1, we have the expected number of submatrices $n_{t o t a l}^{*}$ required to recover all $b$ correct direction vectors as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nn_{t o t a l}^{*}=\\frac{1}{q}\\sum_{k=0}^{b-1}\\frac{b}{b-k}=\\frac{b H_{b}}{q}\\approx\\frac{1}{q}(b\\log(b)+\\gamma b+\\frac{1}{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $H_{b}$ is the $b^{t h}$ harmonic number and $\\gamma\\approx0.57722$ the Euler-Mascheroni constant. ", "page_idx": 15}, {"type": "text", "text": "Proof. As we sample submatrices $\\pmb{A}$ uniformly at random with replacement, assuming them to be i.i.d. is well justified for the regime of $m\\gg b$ . The the number $n$ of submatrices drawn between scuorcrceecsts  dpirroecbtaiboinli tvye owrsit $\\pmb q_{i}$ e txhpuesc tfaotliloonw .i stArisb uwtieo dn $\\mathbb{P}[n=k]=q(1-q)^{k-1}$ rws $q$ $\\begin{array}{r}{n^{*}=\\mathbb{E}[n]=\\frac{1}{q}}\\end{array}$ $\\pmb q_{i}$ uniformly at random from the columns of $\\overline{{Q}}$ , we have the probability of drawing a new direction vector $\\pmb q_{i}$ as $\\frac{b-k}{b}$ for $k$ already drawn direction vectors. Again via the expectation of the Geometric distribution we obtain the expected number $c^{*}$ of correct direction vectors we have to draw until we have recovered all b distinct ones as the solution of the Coupon Collector Problem c\u2217= bk\u2212=10b\u2212bk = $b H_{b}\\approx b\\log(b)+\\gamma b+{\\frac{1}{2}}$ . The proof concludes with the linearity of expectation. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Maximum Number of Samples Required with High Probability We now compute the number of samples $n_{\\mathrm{total}}^{p}$ required to recover all $b$ correct directions with high probability $1-p$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. In the same setting as Lemma 5.2, we have an upper bound $n_{t o t a l}^{p}$ on the number of submatrices we need to sample until we have recovered all $b$ correct direction vectors by solving the following quadratic inequality for ntpotal ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{p}{2}\\leq\\Phi\\left(\\frac{b\\log(2b/p^{*})-n_{t o t a l}^{p}q}{\\sqrt{n_{t o t a l}^{p}q(1-q)}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Phi$ is the cumulative distribution function of the standard normal distribution and $\\boldsymbol{p}^{*}{}={}$ $p-1+(1-p_{f r})^{b}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. At a high level, bound the number of valid directions $c^{p}$ we need to discover until we recover all $b$ distinct ones and then the number of submatrices $n_{\\mathrm{total}}^{p}$ we need to sample to obtain these $c^{p}$ directions, each with probability $1-{\\frac{p}{2}}$ , before applying the union bound. ", "page_idx": 15}, {"type": "text", "text": "However, we first note that with probability $1-(1-p_{f r})^{b}$ we will (repeatedly) reject a correct direction due to a lack of induced sparsity and thus fail irrespective of the number of samples we draw. We thus correct our failure probability budget from $p$ to $p^{*}=p-1+(1-p_{f r})^{b}$ , using the union bound. ", "page_idx": 15}, {"type": "text", "text": "We now show how to compute the upper bound on the number of correct directions $c^{p}$ we need to find until we have found all $b$ distinct directions. To this end, we bound the probability of not sampling the $i^{\\mathrm{th}}$ direction $\\overline{{\\pmb{q}}}_{i}$ after finding $c$ candidates as $\\begin{array}{r}{p_{\\neg i}=\\,\\bigl(1-\\frac1b\\bigr)^{c}\\,\\le\\,e^{-\\frac\\ c b}}\\end{array}$ . We can then bound the probability of missing any of the $b$ dire ctions using the union bound as $\\begin{array}{r}{p_{\\neg\\mathrm{all}}\\le\\sum_{i=1}^{b}p_{\\neg i}=b e^{-\\frac{c^{p}}{b}}}\\end{array}$ We thus obtain the minimum number of correct directions to find all distinct ones with probability at least $\\frac{p^{*}}{2}$ as $c^{p}\\geq b\\log(2b/p^{*})$ . ", "page_idx": 15}, {"type": "text", "text": "We can now compute the number $n_{\\mathrm{total}}^{p}$ of samples required to find $c^{p}$ submatrices satisfying the condition of Theorem 3.3 for some $i$ with probability $\\textstyle1-{\\frac{p}{2}}$ . To this end, we approximate the Binomial distribution $B(n,q)$ with the normal distribution ${\\mathcal{N}}(n q,{\\bar{n}}q(1-q))$ [35], which is generally precise if $\\operatorname*{min}(n q,n(q-1))>9$ [36], which holds for $b\\geq5$ . We th u\u2212s obtain the number of samples $n_{\\mathrm{total}}^{p}$ required to find $c^{p}$ valid directions with high probability $\\textstyle1-{\\frac{p^{*}}{2}}$ by solving $\\begin{array}{r}{\\frac{p^{*}}{2}=\\Phi(\\frac{c^{p}-n_{\\mathrm{total}}^{p}q}{\\sqrt{n_{\\mathrm{total}}^{p}q(1-q)}})}\\end{array}$ for $n_{\\mathrm{total}}^{p}$ which boils down to a quadratic equation. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "By the union bound, we have that the total failure probability of not finding all $b$ correct directions is at most $p$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "For a batch size of $b=10$ and $p=10^{-8}$ , we, e.g., obtain $n\\approx4\\times10^{4}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 5.3. Under the same assumptions as in Lemma 5.1, we have an upper bound on the failure probability $p_{f a i l}^{u b}$ of Alg. 1 even when sampling exhaustively as: ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{f a i l}^{u b}\\leq b\\left(1-\\sum_{k=b-1}^{m}{\\binom{m}{k}}\\frac{1}{2^{m}}\\left(1-0.939^{(b-1){\\binom{k}{b-1}}}\\right)\\right)+1-(1-p_{f r})^{b},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $p_{f r}$ is the probability of falsely rejecting a correct direction $\\overline{{\\pmb q}}^{\\prime}$ via our sparsity fliter (Sec. 4.1). ", "page_idx": 16}, {"type": "text", "text": "Proof. We will first compute the probability of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ not containing a submatrix $\\pmb{A}$ satisfying the conditions of Theorem 3.3 for all $i\\in\\{1,\\ldots,b\\}$ and then the probability of us failing to discover it despite exhaustive sampling. ", "page_idx": 16}, {"type": "text", "text": "We observe that the number $k$ of rows in $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ with a zero $i^{\\mathrm{th}}$ entry is binomially distributed with success probability $\\frac{1}{2}$ . For each $k\\geq b-1$ , we can construct $\\scriptstyle{\\binom{k}{b-1}}$ submatrices $\\pmb{A}$ with an all-zero $i^{\\mathrm{th}}$ column. The probability of any such submatrix having full rank is $1-(\\textstyle{\\frac{1}{2}}-o_{b-1}(1))^{b-1}>1-0.939^{b-1}$ [11, 34]. ", "page_idx": 16}, {"type": "text", "text": "We thus have the probability of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ containing at least one submatrix $\\pmb{A}$ with full rank and an all-zero $i^{\\mathrm{th}}$ column as $\\begin{array}{r}{\\sum_{k=b-1}^{m}\\binom{m}{k}\\frac{1}{2^{m}}\\left(1-0.939^{(b-1)\\binom{k}{b-1}}\\right)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Using the union bound, we thus obtain an upper bound on the probability of $\\frac{\\partial{\\mathcal{L}}}{\\partial Z}$ not containing any submatrix $\\pmb{A}$ with full rank and an all-zero $i^{\\mathrm{th}}$ column for all $i\\in\\{1,\\ldots,b\\}$ . ", "page_idx": 16}, {"type": "text", "text": "To compute the probability of us failing to discover an existing submatrix despite exhaustive sampling, we first note that we have the probability $p_{f r}$ of an arbitrary column in $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ being less sparse than our threshold $\\tau$ . Thus, with probability $1-(1-p_{f r})^{b}$ we will discard at least one correct direction due to it inducing an unusually dense column in \u2202\u2202ZL . ", "page_idx": 16}, {"type": "text", "text": "We now obtain the overall failure probability via the union bound. ", "page_idx": 16}, {"type": "text", "text": "C Deferred Algorithms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we present the Algorithms COMPUTELAMBDA and GREEDYFILTER referenced in Sec. 5.1. ", "page_idx": 16}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/5f7772864e9c4c8224c2a43c78fac0091c6226147ab76232a406b4ae7082d1e9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Algorithm 3 GREEDYFILT   \n1: function GREEDYFILTER(L, R, W , b, \u2202\u2202bL , C)   \n2: $B\\leftarrow\\{\\}$   \n3: while rank of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is $B$ do   \n4: Select the sparsest vector $\\overline{{\\pmb q}}_{i}^{\\prime}$ from $\\mathcal{C}\\setminus\\mathcal{B}$   \n5: $B\\leftarrow B\\cup\\{\\bar{\\pmb q}_{i}^{\\prime}\\}$   \n6: if $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is not of full rank then   \n7: $B\\leftarrow B\\setminus\\{\\overline{{\\pmb{q}}}_{i}^{\\prime}\\}$   \n8: end if   \n9: end while   \n10:   \n11: $\\lambda\\gets$ COMPUTELAMBDA $\\begin{array}{r}{(L,R,W,b,\\frac{\\partial\\mathcal{L}}{\\partial b},B)}\\end{array}$   \n12: while not changed do   \n13: changed $\\leftarrow$ False   \n14: for $(\\bar{\\pmb q}_{i}^{\\prime},\\overline{{\\pmb q}}_{j}^{\\prime})$ in $B\\times({\\mathcal{C}}\\setminus B)$ do   \n15: $\\begin{array}{r l}&{\\dot{\\mathcal{B}}^{\\prime}\\gets\\mathcal{B}\\setminus\\{\\overline{{\\mathbf{q}_{i}^{\\prime}}}\\}\\cup\\{\\overline{{\\mathbf{q}_{j}^{\\prime}}}\\}}\\\\ &{\\lambda^{\\prime}\\gets\\mathrm{CoMPUTELAMBDA}\\left(L,R,W,b,\\frac{\\partial\\mathcal{L}}{\\partial b},\\mathcal{B}^{\\prime}\\right)}\\\\ &{\\mathbf{if}\\;\\lambda^{\\prime}>\\lambda\\;\\mathbf{then}}\\\\ &{\\qquad\\mathcal{B}\\gets\\mathcal{B}^{\\prime}}\\\\ &{\\lambda\\gets\\lambda^{\\prime}}\\\\ &{\\mathrm{chan\\,oed\\leftarrowTrue}}\\end{array}$   \n16:   \n17:   \n18:   \n19:   \n20:   \n21: end if   \n22: end for   \n23: end while   \n24: $\\begin{array}{r l}&{Q\\gets\\mathrm{{FIXSCALE}}\\left(\\mathcal{B},\\mathbf{L},\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{b}}\\right)}\\\\ &{X^{T}\\gets Q^{-1}\\cdot R}\\\\ &{{\\bf r e t u r n}\\,\\lambda,X}\\end{array}$   \n25:   \n26:   \n", "page_idx": 17}, {"type": "table", "img_path": "", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Dataset Licenses ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this work, we use the commonly used MNIST [13], CIFAR-10 [14], TINYIMAGENET [15] and IMAGENET [16] image datasets. No information regarding licensing has been provided on their respective websites. Further, we use Adult tabular dataset under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. ", "page_idx": 17}, {"type": "text", "text": "E Deferred Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Main Results with Error Bars ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide the results from our main experiment in Table 3, alongside $95\\%$ confidence intervals. ", "page_idx": 17}, {"type": "text", "text": "E.2 Experiments on Label-Heterogeneous Data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide experiments on heterogeneous client data. In particular, we look at the extreme case where each client has data only from a single class. As label repetition makes optimization-based attacks harder [1, 23, 7], the results presented in Table 8 for the TinyImageNet ", "page_idx": 17}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/0fbcc1962fa5c64afb54d63bf6294d685613d31a440d491cd1ef6beb3157207a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 6: Effect of the second stage of our reconstruction algorithm discussed in Sec. 4.2, depending on the batch size $b$ . ", "page_idx": 18}, {"type": "text", "text": "dataset show another advantage of our algorithm, namely, SPEAR works regardless of the label distribution, providing even better reconstruction results compared to Table 3 for single-label batches. ", "page_idx": 18}, {"type": "text", "text": "Table 8: Mean reconstruction quality metrics across 100 batches for batches only containing samples from only one class in the same setting as Table 3. ", "page_idx": 18}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/992e8635b1b4529da0bf0eb9b37ca47495fdcc3cc527ce5427a6c681443b22c2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.3 Effectivness of our 2-Stage Greedy Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we compare reconstruction success rate (accuracy) with and without the second stage of our greedy algorithm discussed in Sec. 4.2 in Fig. 6. We observe that the second stage filtering becomes increasingly important for larger batch size $b$ . ", "page_idx": 18}, {"type": "text", "text": "E.4 Effect of Training on SPEAR ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/6b2c43ac2ae218499f95bbf35d274325f7bdf6b0ffc751cee9b85fbac274ed0b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: Effect of training (on MNIST) on the effectiveness of SPEAR at a batch size of $b=10$ evaluated on the MNIST training (a) and test (b) sets. ", "page_idx": 18}, {"type": "text", "text": "In this section, we demonstrate how training effects SPEAR\u2019s performance. To this end, we train a network on MNIST and evaluate SPEAR periodically during training both on the train and test datasets, visualizing results in Fig. 7. We observe that SPEAR performance is very similar between the two datasets we evaluate on. Further, we see that SPEAR performs very well on trained networks, with the number of required steps by the algorithm being even lower those those on untrained networks. However, if the minimum column sparsity of \u2202\u2202ZL drops significantly, as is the case for the checkpoints around 1000 training steps in the illustrated run. SPEAR\u2019s performance drops slightly. ", "page_idx": 18}, {"type": "text", "text": "E.5 Failure Probabilities ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we validate experimentally our theoretical results on SPEAR\u2019s failure rate for several batch sizes $b$ (Lemma 5.3). As this requires exhaustive sampling of all $\\binom{m}{b-1}$ submatrices of $\\textbf{\\emph{L}}$ we only consider small batch sizes $b\\leq10$ and networks $m\\leq40$ . We show the results in Fig. 8 where we observe that the empirical failure probability (blue) with $95\\%$ Clopper-Pearson confidence bounds generally agrees with the analytical approximation (solid line) and always lies below the analytical upper bound (dashed line). We conclude that in most settings, the number of required samples rather than complete failure is the limiting factor for SPEAR\u2019s performance. ", "page_idx": 19}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/304c3751de9837934f7c752693d2c0ec4e818123a3c81b64283d50f81e09f6a5.jpg", "img_caption": ["Figure 8: Empirical failure probability (blue) with $95\\%$ Clopper-Pearson confidence bounds (shaded blue) compared to the analytical upper bound (dashed line) and approximation (solid line) of the failure probability for different batch sizes $b$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.6 Results under DPSGD ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we show experimental results on reconstructing images from gradients defended using DP-SGD [28]. In Table 9, we report results on the TINYIMAGENET dataset, $b=20$ , with noise levels $\\sigma\\leq1.0\\times10^{-4}$ and gradient clipping that constrains the $\\ell_{2}$ norm of the composite gradient vector, combining the gradients of all layers, to a maximum value of $C\\in[1,2]$ . We chose the maximum value $\\sigma$ to be close to median gradient magnitude of the first linear layer which in our experiments was also $\\approx1.0\\times10^{-4}$ . We chose the range for $C$ such that for the upper bound 2, most individual input gradients are not clipped, while for the lower bound 1 almost all are. ", "page_idx": 19}, {"type": "text", "text": "Adapting SPEAR to Noisy Gradients In the experiments presented in Table 9, we make several adjustments to SPEAR to better handle the noise added by DPSGD. First, we apply looser thresholds in our sparsity filtering at Line 6 in Alg. 1 to account for the noise added to the sparse entries of $\\textstyle{\\frac{\\partial{\\mathcal{L}}}{\\partial Z}}$ . To account for the imperfect reconstructions in this setting, we also perform our early stopping (Line 9 in Alg. 1) when the sparsity matching coefficient $\\gamma$ reaches a lower value than 1. Further, we sample matrices $L_{A}$ of larger size $(b+1\\times b)$ to increase the numerical stability of our solutions under noise. While sampling larger $L_{A}$ is more computationally expensive, as $b+1$ instead of $b-1$ entries in $\\pmb{A}$ are required to be correctly sampled as 0, the resulting directions $\\pmb q_{i}$ are more numerically stable as they are obtained as a solution of an overdetermined system of linear equations. Note that if $\\pmb{A}$ is assumed to be of rank $b-1$ , Theorem 3.3 remains valid for these larger matrices $L_{A}$ . Finally, due to our looser sparsity filtering described above we encounter more incorrect directions $\\overline{{\\pmb q}}_{i}$ . We tackle this issue by only keeping $\\overline{{\\pmb q}}_{i}$ that correspond to matrices $L_{A}$ of rank exactly $b-1$ . Under our assumption in Theorem 3.3, those are exactly the vectors $\\overline{{\\pmb q}}_{i}$ that correspond to $\\pmb{A}$ of the correct rank $b-1$ . Note that we apply these changes only for $\\sigma>0$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Invariance to Gradient Clipping In Table 9, we observe that the quality of our reconstructions is not affected by the clipping constant $C$ . This is not a coincidence, but rather a mathematical fact. To see this, note that the observed gradients w.r.t. $W$ under clipping are given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial\\pmb{\\mathscr{L}}}{\\partial\\pmb{W}}=\\sum_{i=1}^{b}c_{i}\\frac{\\partial\\mathscr{L}}{\\partial\\pmb{W}_{i}}=\\sum_{i=1}^{b}c_{i}\\frac{\\partial\\mathscr{L}}{\\partial\\pmb{Z}_{i}}\\pmb{X}_{i},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $c_{i}\\in\\mathbb{R}$ are the unknown to the attacker factors applied by the clipping procedure to each individual input gradient $\\frac{\\partial\\mathcal{L}}{\\partial{\\pmb W}_{i}}$ . One can adapt the proof to Theorem 3.1, to show that $\\begin{array}{r}{{\\frac{\\partial{\\dot{\\mathcal{L}}}}{\\partial\\pmb{W}}}={\\frac{\\partial{\\dot{\\mathcal{L}}}}{\\partial\\pmb Z}}\\pmb X^{T}}\\end{array}$ , where we define $\\frac{\\partial\\dot{\\mathcal{L}}}{\\partial Z_{i}}$ to be the clipped gradient w.r.t $_{z}$ , consisting of the columns $\\begin{array}{r}{\\frac{\\partial{\\dot{\\mathcal{L}}}}{\\partial Z_{i}}=c_{i}\\frac{\\partial{\\mathcal{L}}}{\\partial Z_{i}}}\\end{array}$ We also observe that one can adapt Theorem 3.4 to work directly on the clipped gradients as well, resulting in the formula \u2202\u2202\u02d9bL =\u2202\u2202\u02d9ZL $\\begin{array}{r}{\\frac{\\partial\\stackrel{.}{\\mathcal{L}}}{\\partial\\pmb{b}}=\\frac{\\partial\\stackrel{.}{\\mathcal{L}}}{\\partial\\pmb{Z}}\\left[\\,\\stackrel{1}{\\vdots}\\,\\right]}\\end{array}$ for the clipped gradient w.r.t. $^{b}$ . The formula follows from the observation that in our setting the same clipping factor $c_{i}$ is applied to the gradients of each layer, including\u2202b \u2202Land $\\frac{\\partial\\mathcal{L}}{\\partial{\\pmb W}_{i}}$ . By applying the rest of the theoretical results of the paper without change but on clipped gradients $\\frac{\\partial{\\dot{z}}}{\\partial z}$ , instead of the original unclipped gradients $\\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathbf{Z}}}$ , we conclude that SPEAR is directly applicable on the clipped client gradient and that applying it on those still recover the true input matrix $\\mathbf{\\deltaX}$ without the need of knowing the clipping constants $c_{i}$ . ", "page_idx": 20}, {"type": "text", "text": "Robustness to Noise From Table 9, we observe that SPEAR is very robust to noise. We emphasize in particular that even when noise of similar size to the size of the gradients in expectation is applied, we still obtain a reconstruction with $\\mathrm{PSNR}>28$ . This is similar to the PSNR of 29.3 that Geiping et al. [1] achieves \\*without any noise\\* which is commonly considered unacceptable information leakage. These experiments suggest that to efficiently defend against SPEAR using noise, one needs to apply such high magnitudes that training will likely be significantly impeded. ", "page_idx": 20}, {"type": "text", "text": "F SPEAR under FedAvg Updates ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we first demonstrate theoretically that SPEAR can be generalized to attack FedAvg [4] client updates, and then present empirical results confirming that SPEAR is indeed very effective under FedAvg protocols with different number of epochs $\\mathcal{E}$ , local client learning rates $\\eta$ , and, even works, when mini-batches of size $b_{\\mathrm{mini}}$ are used. ", "page_idx": 20}, {"type": "text", "text": "Generalizing SPEAR to FedAvg Updates Assuming that a client uses all of its data points, $\\mathbf{\\deltaX}$ , in each local gradient step of the FedAvg protocol, i.e. $b_{\\mathrm{mini}}=b$ , the client computes and subsequently shares with the server the following updated linear layer weights: ", "page_idx": 20}, {"type": "equation", "text": "$$\nW^{\\varepsilon}=W^{0}-\\eta\\sum_{e=1}^{\\varepsilon}{\\frac{\\partial{\\mathcal{L}}}{\\partial W^{e}}}=W^{0}-\\eta\\sum_{e=1}^{\\varepsilon}{\\frac{\\partial{\\mathcal{L}}}{\\partial Z^{e}}}\\cdot X^{T}=W^{0}-\\eta\\left(\\sum_{e=1}^{\\varepsilon}{\\frac{\\partial{\\mathcal{L}}}{\\partial Z^{e}}}\\right)\\cdot X^{T},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $W^{0}$ is the global model sent by the server, $\\mathbf{\\Delta}W^{e}$ represent the local client weights after $e$ client epochs, and $\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}^{e}}$ and $\\frac{\\partial{\\mathcal{L}}}{\\partial{Z}^{e}}$ are the weight and output gradients at epoch $e$ . ", "page_idx": 20}, {"type": "text", "text": "We empirically observe that sparsity patterns of the different local gradients $\\frac{\\partial{\\mathcal{L}}}{\\partial{Z^{e}}}$ are usually similar. This is expected as these patterns correspond to the ReLU activation patterns for the layer outputs $Z^{e}$ ghts $W^{e}$ . As the sparsity patterns for the individual gradients are simila $\\mathbf{\\deltaX}$ r sum $\\begin{array}{r}{\\sum_{e=1}^{\\varepsilon}\\frac{\\partial\\mathcal{L}}{\\partial Z^{e}}}\\end{array}$ also shares this sparsity pattern and is, thus, also sparse. As the server knows $W^{0}$ and it can subtract it from the client\u2019s shared weights $W^{\\mathcal{E}}$ and apply Theorem 3.3, as before, on the sparse matrix $\\sum_{e=1}^{\\mathcal{E}}\\frac{\\partial\\mathcal{L}}{\\partial Z^{e}}$ to obtain the corresponding matrix $Q$ and client data $\\mathbf{\\deltaX}$ . We note that while our sparsity matching coefficient will typically not reach 1 for the final reconstruction in this setting, as there is some mismatch between the sparsity patterns of the different output gradients $\\frac{\\partial{\\mathcal{L}}}{\\partial{Z}^{e}}$ , we have found that SPEAR remains practically effective regardless. ", "page_idx": 20}, {"type": "text", "text": "Table 9: Reconstruction quality across 100 batches of size $b=20$ computed on TINYIMAGENET for gradients computed with DPSGD [28] with different noise levels $\\sigma$ and gradient clipping levels $C$ . ", "page_idx": 21}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/f2ba2675529f16f4937ca0ca814f80b653a4ed3f7928a173390217969b1cbe4d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We note that SPEAR can be even be generalized to FedAvg protocols that use random mini-batches $X^{e}$ of size $b_{\\mathrm{mini}}<b$ sampled from $\\mathbf{\\deltaX}$ at each local step. This is the case, as each local client gradient $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{W}^{e}}\\,=\\,\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{Z}^{e}}(\\pmb{X}^{e})^{T}}\\end{array}$ , can be represented as $\\overline{{\\frac{\\partial{\\mathcal{L}}}{\\partial Z^{e}}}}X^{T}$ , where $\\overline{{\\frac{\\partial{\\mathcal{L}}}{\\partial{Z^{e}}}}}$ is derived from $\\frac{\\partial{\\mathcal{L}}}{\\partial{Z}^{e}}$ by adding 0 columns at batch positions corresponding to batch elements not in $X^{e}$ . Importantly, as $\\overline{{\\frac{\\partial{\\mathcal{L}}}{\\partial{Z^{e}}}}}$ only adds 0 columns to $\\frac{\\partial{\\mathcal{L}}}{\\partial{Z^{e}}}$ , the sparsity of $\\overline{{\\frac{\\partial{\\mathcal{L}}}{\\partial{Z^{e}}}}}$ can only increase, allowing to conclude that $\\sum_{e=1}^{\\ensuremath{\\varepsilon}}\\frac{\\overline{{\\partial{\\mathcal{L}}}}}{\\partial Z^{e}}$ remains sparse, and, thus, Theorem 3.3 can still be applied to it. ", "page_idx": 21}, {"type": "text", "text": "Experiments with FedAvg Updates Next, we show empirically the effectiveness of SPEAR for FedAvg updates. In Table 10, we show the results of attacking clients with $b=20$ datapoints from the TINYIMAGENET dataset for different number of local client epochs $\\mathcal{E}$ . We observe that even for $\\mathcal{E}=50$ gradient steps we recover data from most batches, with quality similar to the quality achieved when attacking individual gradients. This is expected as Theorem 3.3 still holds, as described in the previous paragraph. The slight dip in the fraction of reconstructed batches for larger number of steps $\\mathcal{E}$ can be attributed to some client batches inducing larger discrepancy between the sparsity patterns of \u2202L compared to others, resulting in their sum being much less sparse. Further, Table 10 also shows that SPEAR can attack client updates that take $b/b_{\\mathrm{mini}}=4$ local steps per epoch for $\\mathcal{E}=20$ epochs. Interestingly, while a total of 80 gradient steps are taken in this scenario the results are closer to the $b_{\\mathrm{mini}}=20,\\mathcal{E}=20$ setting, instead of the $b_{\\mathrm{mini}}=20,\\mathcal{E}=50$ setting. This can be explained by the increased sparsity of the individual expanded gradients $\\overline{{\\frac{\\partial{\\mathcal{L}}}{\\partial{Z^{e}}}}}$ . ", "page_idx": 21}, {"type": "text", "text": "Finally, we experiment with different local client learning rates $\\eta$ and show the results in Table 11. We observe that even for large learning rates SPEAR still recovers its inputs well, showing that while the individual weights $\\boldsymbol{W}^{e}$ can change a lot, their induced sparsity on $\\bar{\\frac{\\partial{\\mathcal{L}}}{\\partial Z^{e}}}$ remains consistent. ", "page_idx": 21}, {"type": "text", "text": "G Additional Visualisations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we present additional visualisations of the reconstructions obtained by SPEAR. First, in Fig. 9 we show an extended comparison between the images recovered by our method and Geiping ", "page_idx": 21}, {"type": "text", "text": "Table 10: Reconstruction quality across 100 FedAvg client updates computed on TINYIMAGENET batches of size $b=20$ for different number of epochs $\\mathcal{E}$ and different mini batch sizes $b_{\\mathrm{mini}}$ . ", "page_idx": 22}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/b3e806ac87189504ef39e70e075160e47c16a7e9064e225f3624c4e73caa30d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 11: Reconstruction quality across 100 FedAvg client updates computed on TINYIMAGENET batches of size b = 20 for different local client learning rates \u03b7. ", "page_idx": 22}, {"type": "table", "img_path": "lPDxPVS6ix/tmp/cf4d640d4699101f90c30df76ff5f464464fb7c70ea32e9a18738ba30997b197.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/2e8ed2659fe497a5b54906fd89922efdccf41e863cb6e3c67ec545544874202d.jpg", "img_caption": ["Figure 9: The reconstructions of all images from Fig. 1, reconstructed using our SPEAR (top) or the prior state-of-the-art Geiping et al. [1] (mid), compared to the ground truth (bottom). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "et al. [1] on the TINYIMAGENET batch first shown in Fig. 1. In Fig. 9 we operate in the same setting as Table 8, namely batches of only a single class. We observe that while some images are reconstructed well by Geiping et al. [1], most of the images are of poor visual quality, with some even being hard to recognize. In contrast, all of our reconstructions are pixel perfect. This in particular also means, that SPEAR\u2019s reconstructions improve in fine-detail recovery even upon the well recovered images of Geiping et al. [1]. This is expected as our attack is exact (up numerical errors). ", "page_idx": 22}, {"type": "text", "text": "Further, to show the results in Fig. 9 are representative, in Fig. 10\u201312 we provide additional visualizations of the reconstructions obtained by SPEAR corresponding to the $10^{\\mathrm{{th}}}{,}50^{\\mathrm{{th}}}$ , and $90^{\\mathrm{th}}$ percentiles of the PSNRs obtained in the TINYIMAGENET experiment reported in Table 3. We observe that only 1 sample has visual artefacts for the $10^{\\mathrm{th}}$ percentile batch (top left image in Fig. 10) and that the $\\mathrm{50^{th}}$ and $90^{\\mathrm{th}}$ percentile batches contain only perfect reconstructions. We theoreticize that the visual artefact in Fig. 10 is a result of a numerical instability issue and that using $L_{A}$ of bigger size as described in App. E.6 one could further alleviate it in exchange of additional computation. ", "page_idx": 22}, {"type": "text", "text": "Finally, we demonstrate what happens to SPEAR reconstructions in the rare case when the algorithm fails to recover all correct directions $\\overline{{q}}_{i}$ from the batch gradient. In Fig. 13, we show the only such batch for the TINYIMAGENET experiment reported in Table 3. The batch has 2 wrong directions and still achieves an average PSNR of 91.2 (the worst PSNR obtained in this experiment), which is still much higher compared to prior work. Further, all but 2 images are affected by the failure. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/97f1fb3675e089481e87c9301317f7e786da4aa853377e28a42086c0a5ce4b0b.jpg", "img_caption": ["Figure 10: Visualisation of the images reconstructed by SPEAR from the batch whose PSNR is at the $10^{\\mathrm{th}}$ percentile based on the set of 100 TINYIMAGENET reconstructions reported in Table 3. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/e4c84e834309b2efcdadf6e802fd9a94924a701ba32996f1c4c2bacc77e36fee.jpg", "img_caption": ["Figure 11: Visualisation of the images reconstructed by SPEAR from the batch whose PSNR is at the $50^{\\mathrm{th}}$ percentile based on the set of 100 TINYIMAGENET reconstructions reported in Table 3. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/64d8fff8533c8d0ccb9fb66c6e2bf2d76e501b5f399b8c75ad11b0cc0a4155dc.jpg", "img_caption": ["Figure 12: Visualisation of the images reconstructed by SPEAR from the batch whose PSNR is at the $90^{\\mathrm{th}}$ percentile based on the set of 100 TINYIMAGENET reconstructions reported in Table 3. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "lPDxPVS6ix/tmp/e06dda0e05daec61c0ab8ae4178eef121bfacd25d1695ad0463b6da205cd2869.jpg", "img_caption": ["Figure 13: Visualisation of the images reconstructed by SPEAR from the only batch from the 100 TINYIMAGENET reconstructions reported in Table 3, where not all recovered directions $\\overline{{q}}_{i}^{\\prime}$ are correct. SPEAR recovered 2/20 wrong directions, resulting in the left most images being wrongly recovered. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main claim of our abstract and introduction sections is that we introduce the first algorithm to reconstruct whole batches of data exactly when $b>1$ in the important honest-but-curious setting. To this end, we provide high-level overview of our algorithm SPEAR in Sec. 2 and deeper technical explanation in Sec. 3 that lays in great mathematical details how and why our algorithm is able to recover user data under ReLU-induced sparsity. We further claim we provide efficient GPU implementation recovering inputs to fullyconnected networks fast and precisely even for high-dimensional inputs when $b\\lesssim25$ Our experiments in Sec. 6 show that for these batch sizes on high-dimensional inputs like IMAGENET images our algorithm is significantly faster compared to prior work, while also recovering the images up to numerical precision. Finally, we claim that our method, despite its exponential runtime, can in theory recover the client inputs for much larger batch sizes $b$ , under sufficient compute. These claims are supported by our theoretical analysis in Sec. 5.2 and our experiments in Sec. 6.3. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We discuss our limitations in a separate limitation section (Sec. 8). The broader impact of our work is discussed in App. A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide proofs for our theorems either immediately following the theorems themselves or in App. B. We explicitly state all theorem assumptions. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide code in the accompanying GitHub repository. Further, Sec. 3 we give all technical details needed to reimplement our algorithm. Finally, in Sec. 6 we explain in detail how our experiments were performed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide the code in the accompanying GitHub repository alongside installation instructions and example commands. We rely only on publicly available datasets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes we list all details of our experiments in Sec. 6. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide error bars for our main experiments in App. E.1, as well as, our failure probability verification experiments in App. E.5. We didn\u2019t provide error bars for our other experiments due to computational limitation. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provide information about the amount and types of compute needed to conduct our experiments in Sec. 6. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not introduce new models or datasets. We use only standard datasets in our experiments and provide discussion of their licenses in App. D. We discuss our broader impact on privacy of federated learning in App. A. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide discussion of our broader impact in App. A. We discuss possible mitigations to our attack in App. E.6 ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our code is publicly available at our GitHub repository under license that only permits educational and academic uses and explicitly forbids malicious uses including obtaining, accessing, or disclosing private or confidential information about individuals. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We use only standard datasets in our experiments and provide discussion of their licenses in App. D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not introduce new datasets or models. We provide the code in our GitHub repository alongside instructions for installations. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]