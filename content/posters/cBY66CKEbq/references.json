{"references": [{"fullname_first_author": "A. Agarwal", "paper_title": "Model-based reinforcement learning with a generative model is minimax optimal", "publication_date": "2020-00-00", "reason": "This paper establishes the minimax optimality of model-based reinforcement learning with a generative model, a foundational result for offline RL."}, {"fullname_first_author": "J. Blanchet", "paper_title": "Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage", "publication_date": "2023-00-00", "reason": "This paper provides a provably efficient algorithm for offline RL under model mismatch, addressing data sparsity and model uncertainty with a novel double pessimism principle."}, {"fullname_first_author": "G. Li", "paper_title": "Settling the sample complexity of model-based offline reinforcement learning", "publication_date": "2022-00-00", "reason": "This paper resolves the sample complexity of model-based offline RL, offering a tighter theoretical guarantee for the effectiveness of pessimism-based methods."}, {"fullname_first_author": "L. Shi", "paper_title": "Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity", "publication_date": "2022-00-00", "reason": "This paper proposes a distributionally robust model-based offline RL framework with near-optimal sample complexity, enhancing the robustness to model uncertainty."}, {"fullname_first_author": "T. Xie", "paper_title": "Policy finetuning: Bridging sample-efficient offline and online reinforcement learning", "publication_date": "2021-00-00", "reason": "This paper introduces policy finetuning to bridge offline and online RL, improving sample efficiency and addressing the limitations of solely relying on offline data."}]}