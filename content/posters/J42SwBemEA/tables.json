[{"figure_path": "J42SwBemEA/tables/tables_7_1.jpg", "caption": "Table 1: Results (mean\u00b1std) on DM_Control with the default setting at 500K environment steps.", "description": "This table presents the mean and standard deviation of the results obtained by different reinforcement learning methods on the DeepMind Control Suite.  The experiments were conducted under default settings, with each method trained for 500,000 environment steps. The table compares the performance of SAC, DrQ, DBC, MICo, SimSR, and SCR across eight different tasks within the DeepMind Control Suite, showing the average scores achieved by each algorithm in each task.", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/tables/tables_7_2.jpg", "caption": "Table 2: Results (mean\u00b1std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose.", "description": "This table presents the results of experiments conducted on the DeepMind Control Suite with a distraction setting.  Eight different continuous control tasks are evaluated, and the mean and standard deviation of the scores are shown for each task.  The distraction setting includes background video distraction, object color distraction, and camera pose distraction.  The results show the performance of various algorithms (SAC, DrQ, DBC, MICO, SimSR, and SCR) under these challenging conditions, with SCR outperforming the others in most tasks. ", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/tables/tables_9_1.jpg", "caption": "Table 3: Average success rates for six tasks in Meta-World.", "description": "This table presents the average success rates achieved by different reinforcement learning methods (SAC, DrQ, DBC, MICo, SimSR, and SCR) across six tasks within the Meta-World environment.  The success rate is a metric indicating the percentage of successful task completions. The results showcase the superior performance of the proposed SCR method compared to other baselines.  The values provided are the means and standard deviations calculated across five different seeds (random initializations).", "section": "4 Experiments"}, {"figure_path": "J42SwBemEA/tables/tables_17_1.jpg", "caption": "Table 1: Results (mean\u00b1std) on DM_Control with the default setting at 500K environment steps.", "description": "This table presents the results of experiments conducted on the DeepMind Control Suite, using the default setting with 500K environment steps. It compares the performance of different reinforcement learning methods, including SAC, DrQ, DBC, MICO, SimSR, and SCR. For each method, the mean and standard deviation of the scores are given for eight different tasks. This allows for a comparison of the sample efficiency of the different methods.", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/tables/tables_19_1.jpg", "caption": "Table 1: Results (mean\u00b1std) on DM_Control with the default setting at 500K environment steps.", "description": "This table presents the results of eight different tasks from the DeepMind Control Suite.  The results are the average performance (mean \u00b1 standard deviation) of several reinforcement learning methods over 10 different runs, each running for 500,000 environment steps. The \"default setting\" refers to the standard environment configuration without any added distractions. The table allows comparison of various algorithms in a standard setting to assess relative performance.", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/tables/tables_20_1.jpg", "caption": "Table 2: Results (mean\u00b1std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose.", "description": "This table presents the results of experiments conducted on the DeepMind Control Suite with a distraction setting.  The results show the mean and standard deviation of scores achieved by various reinforcement learning methods (SAC, DrQ, DBC, MICO, SimSR, and SCR) across eight different control tasks.  The distraction setting introduces background video, object color, and camera pose variations to increase the difficulty of the tasks.  The table highlights the performance of each algorithm in this challenging scenario, showcasing their ability to generalize to more complex and realistic environments.", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/tables/tables_20_2.jpg", "caption": "Table 2: Results (mean\u00b1std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose.", "description": "This table presents the results of experiments conducted on the DeepMind Control Suite with a distraction setting.  The results (mean \u00b1 standard deviation) are shown for eight different control tasks, comparing the performance of several reinforcement learning algorithms. The algorithms include SAC (a baseline), DrQ (data augmentation), DBC (bisimulation metric), MICo (another behavioral metric), SimSR (yet another behavioral metric), and SCR (the authors' proposed State Chrono Representation).  The distraction setting makes the tasks more challenging by introducing variations in background video, object color, and camera pose.", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/tables/tables_21_1.jpg", "caption": "Table 3: Average success rates for six tasks in Meta-World.", "description": "This table presents the average success rates achieved by different reinforcement learning methods across six distinct tasks within the Meta-World environment.  The results highlight the performance comparison between various methods, showcasing their effectiveness in handling the complexities of robotic manipulation tasks in a simulated environment.", "section": "4.4 Experiments on Meta-World"}]