[{"figure_path": "B3rZZRALhk/tables/tables_5_1.jpg", "caption": "Table 1: Comparison between different model architectures. We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. '-' denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. 'X' indicates diverged runs. '*' is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.", "description": "This table compares the performance of five different diffusion model architectures across two datasets (ImageNet-1k and CC12M) at two resolutions (256x256 and 512x512). It shows the FID (Fr\u00e9chet Inception Distance) and CLIP scores for each model.  The table includes results from previously published papers, the authors' re-implementations of those models, and finally their improved models. It highlights how architectural choices and training strategies affect model performance.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_6_1.jpg", "caption": "Table 2: Control conditioning. We study different facets of control conditioning and their impact on the model performance. (a-b) We report FIDtrain on ImageNet-1k@256 using 250 sampling steps. 120k training iterations. (a) Influence of the parametrization. LPIPS computation considers all resolutions [64, 128, 256, 512, 1024] while LPIPS/HR exclude 64 and 128 (b) Size conditioning effect on FID at inference. (c) Influence of control conditioning. Models trained on CC12M@256. (124k iterations)", "description": "This table presents a study on control conditioning in diffusion models. It shows the impact of different control mechanisms (parametrization, size conditioning, crop, and random flip) on the model's performance, measured by FID (Fr\u00e9chet Inception Distance) and LPIPS (Learned Perceptual Image Patch Similarity). The experiments are conducted on ImageNet-1k and CC12M datasets at 256 resolution.", "section": "3.3 Control conditioning"}, {"figure_path": "B3rZZRALhk/tables/tables_6_2.jpg", "caption": "Table 1: Comparison between different model architectures. We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. '-' denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. 'X' indicates diverged runs. '*' is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.", "description": "This table compares the performance of five different diffusion model architectures on ImageNet-1k and CC12M datasets at 256x256 and 512x512 resolutions.  It shows a comparison between results reported in prior publications and the authors' reimplementation of those models, as well as their improved models.  The table highlights the FID (Fr\u00e9chet Inception Distance) and CLIPscore metrics, showcasing improvements achieved through architectural refinements and advanced training strategies.  The use of '-' indicates missing data from original publications, while 'X' signifies training failures.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_7_1.jpg", "caption": "Table 3: Text padding. Our noisy replication embedding vs. baseline zero-padding. Models trained on CC12M@256.", "description": "This table presents the results of an experiment comparing different text padding methods used in a diffusion model trained on the CC12M dataset at 256x256 resolution.  The experiment assesses the impact of replacing padding tokens with noisy copies of existing tokens ('replicate' padding) versus the standard zero-padding method on FID (Fr\u00e9chet Inception Distance) and CLIP scores. The results show that noisy replication padding improves both FID and CLIP scores compared to zero-padding, indicating that this technique enhances the model's ability to condition on text prompts and improve its performance.", "section": "3.4 Transferring weights between datasets and resolutions"}, {"figure_path": "B3rZZRALhk/tables/tables_7_2.jpg", "caption": "Table 4: Effect of pre-training across datasets and resolutions. Number of (pre-)training iterations given in thousands (k) per row. Relative improvements in FID and CLIP score given as percentage in parenthesis.", "description": "This table shows the results of pre-training models on different datasets (ImageNet-22k and CC12M) at different resolutions before fine-tuning on the target datasets. The table demonstrates the impact of pre-training on model performance (measured by FID and CLIP score) when transferring knowledge across datasets and resolutions. The relative improvements in FID and CLIP scores are shown in parentheses, indicating the performance gain achieved through pre-training.", "section": "3.4 Transferring weights between datasets and resolutions"}, {"figure_path": "B3rZZRALhk/tables/tables_7_3.jpg", "caption": "Table 4: Effect of pre-training across datasets and resolutions. Number of (pre-)training iterations given in thousands (k) per row. Relative improvements in FID and CLIP score given as percentage in parenthesis.", "description": "This table shows the results of pre-training models at 256 resolution on ImageNet-1k before fine-tuning on ImageNet-22k and CC12M at 512 resolution.  It compares the FID and CLIP scores achieved with different pre-training iteration counts and fine-tuning iteration counts on both datasets.  The relative improvements in FID and CLIP scores compared to the baseline are also presented, highlighting the impact of pre-training on model performance.", "section": "3.4 Transferring weights between datasets and resolutions"}, {"figure_path": "B3rZZRALhk/tables/tables_7_4.jpg", "caption": "Table 4: Effect of pre-training across datasets and resolutions. Number of (pre-)training iterations given in thousands (k) per row. Relative improvements in FID and CLIP score given as percentage in parenthesis.", "description": "This table presents the results of experiments evaluating the impact of pre-training on the performance of diffusion models.  Two different model architectures (UNet and mmDiT) were evaluated. The table compares the FID scores obtained when training models from scratch versus when transferring weights from pre-trained models on ImageNet-1k (at 256 resolution) before fine-tuning on either CC12M or ImageNet-22k at 512 resolution. The results show significant improvements in FID and CLIP score when using transfer learning, demonstrating the effectiveness of this technique for improving training efficiency and model performance. The relative improvement is shown in parentheses.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_15_1.jpg", "caption": "Table 1: Comparison between different model architectures. We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. '-' denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. 'X' indicates diverged runs. '*' is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.", "description": "This table compares the performance of five different diffusion model architectures across two datasets (ImageNet-1k and CC12M) at two resolutions (256x256 and 512x512).  It contrasts results from the original papers describing the models with the authors' own reimplementation of those models and finally their improved models. The table shows FID and CLIP scores, highlighting the best-performing architecture (mmDiT-XL/2) and the impact of improvements made by the authors.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_16_1.jpg", "caption": "Table 1: Comparison between different model architectures. We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. '-' denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. 'X' indicates diverged runs. '*' is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.", "description": "This table compares the performance of five different diffusion model architectures (UNet, DiT-XL2, mDT-v2-XL/2, PixArt-a-XL/2, mmDiT-XL/2) across two datasets (ImageNet-1k and CC12M) at two resolutions (256x256 and 512x512). The table presents results from the original papers, the authors' reimplementations, and their improved models, allowing for a comparison of the models' performance and the impact of the authors' training techniques.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_19_1.jpg", "caption": "Table 1: Comparison between different model architectures. We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. '-' denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. 'X' indicates diverged runs. '*' is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.", "description": "This table compares the performance of five different diffusion model architectures across two datasets (ImageNet-1k and CC12M) at two resolutions (256x256 and 512x512 pixels).  The comparison includes results from previously published papers, the authors' re-implementations of those papers, and finally their own improved models using new conditioning mechanisms and pre-training strategies.  FID (Fr\u00e9chet Inception Distance) and CLIP-COCO scores are used to evaluate the image generation quality.  The table highlights the best-performing architecture and demonstrates the improvements achieved by the authors' methods.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_19_2.jpg", "caption": "Table 1: Comparison between different model architectures. We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. '-' denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. 'X' indicates diverged runs. '*' is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.", "description": "This table compares the performance of different diffusion model architectures on ImageNet-1k and CC12M datasets at 256x256 and 512x512 resolutions.  It shows a comparison between results reported in previous publications, the authors' re-implementations of those models, and finally, the authors' improved models.  The table highlights the impact of architectural choices and training strategies on model performance, using FID (Fr\u00e9chet Inception Distance) and CLIPscore as evaluation metrics.  The '-' indicates missing data from original papers, 'X' indicates training failures, and '*' signifies differences in FID calculation compared to another study.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_19_3.jpg", "caption": "Table 1: Comparison between different model architectures. We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. '-' denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. 'X' indicates diverged runs. '*' is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.", "description": "This table compares the performance of five different diffusion model architectures on ImageNet-1k and CC12M datasets at 256x256 and 512x512 resolutions.  It shows a comparison of results from previously published papers, the authors' reimplementations of those models, and finally, the authors' improved results using novel conditioning and training strategies. The table highlights improvements in FID (Fr\u00e9chet Inception Distance) scores, demonstrating the effectiveness of the proposed methods.  The '-' indicates missing data from original papers, 'X' represents failed training runs, and '*' notes discrepancies in evaluation details with a specific prior work.", "section": "3 Experimental evaluation"}, {"figure_path": "B3rZZRALhk/tables/tables_20_1.jpg", "caption": "Table 2: Control conditioning. We study different facets of control conditioning and their impact on the model performance. (a-b) We report FIDtrain on ImageNet-1k@256 using 250 sampling steps. 120k training iterations. (a) Influence of the parametrization. LPIPS computation considers all resolutions [64, 128, 256, 512, 1024] while LPIPS/HR exclude 64 and 128.", "description": "This table presents the results of experiments on control conditioning, exploring its various aspects and impact on model performance. It focuses on ImageNet-1k dataset with 256 resolution, using 250 sampling steps and 120k training iterations.  The table is divided into subsections showing: (a) the influence of parametrization on FIDtrain and LPIPS (with and without higher resolutions), (b) size conditioning's effect on FID at inference, and (c) control conditioning's effect on FID and CLIP score on the CC12M dataset at 256 resolution with 124k iterations.  The results show how different methods of control conditioning affect the final performance metrics.", "section": "3.3 Control conditioning"}]