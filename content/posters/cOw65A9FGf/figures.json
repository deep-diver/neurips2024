[{"figure_path": "cOw65A9FGf/figures/figures_3_1.jpg", "caption": "Figure 1: The four rows depict the original image, its associated attention map, the generated adversarial example, and the attention map of the adversarial example. Labels in black indicate the ground truth, while those in red represent mis-classified labels for the adversarial examples.", "description": "This figure shows a comparison of original images and their corresponding adversarial examples.  Each row presents an image, its attention map highlighting the areas the model focused on during classification, the adversarial version of that image created using the PGD attack method, and finally, the attention map for the adversarial example.  The goal is to visually demonstrate how adversarial attacks subtly alter the image, causing a shift in the model's attention and leading to misclassification (incorrect labels are shown in red).", "section": "3.2 Text-Guided Attention based Interpretation of Adversarial Attacks"}, {"figure_path": "cOw65A9FGf/figures/figures_4_1.jpg", "caption": "Figure 2: An overview of our TGA-ZSR framework: We generate adversarial examples and feed them into the target image encoder. To enhance the adversarial robustness of the CLIP model and maintain its generalization, we introduce text-guided attention. This involves refining the framework for adversarial examples through the Attention Refinement module and constraining the model to prevent significant drift via the Attention-based Model Constraint module.", "description": "This figure illustrates the TGA-ZSR framework.  Adversarial examples are generated and fed into a target image encoder.  The framework incorporates two modules: the Attention Refinement module, which aligns text-guided attention from adversarial and clean examples, and the Attention-based Model Constraint module, which maintains model performance on clean samples while improving robustness.  Text embeddings from a frozen text encoder provide guidance throughout the process.", "section": "3.3 Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR)"}, {"figure_path": "cOw65A9FGf/figures/figures_9_1.jpg", "caption": "Figure 1: The four rows depict the original image, its associated attention map, the generated adversarial example, and the attention map of the adversarial example. Labels in black indicate the ground truth, while those in red represent mis-classified labels for the adversarial examples.", "description": "This figure visualizes the effect of adversarial attacks on image classification using a vision-language model.  It shows four rows, each representing an original image, its corresponding attention map (highlighting the areas the model focuses on), a generated adversarial example (a slightly altered version of the original designed to fool the model), and the attention map of the adversarial example. The difference in attention maps between the original and adversarial images illustrates how the adversarial perturbation changes the model's focus, often leading to misclassification.  The labels in black indicate correct classifications, while labels in red indicate misclassifications due to the adversarial attack.", "section": "3.2 Text-Guided Attention based Interpretation of Adversarial Attacks"}]