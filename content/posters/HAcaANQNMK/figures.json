[{"figure_path": "HAcaANQNMK/figures/figures_1_1.jpg", "caption": "Figure 1: Perplexity\u00b2 versus model size for GPT3 and Llama2 models and comparison to compressed models using ESPACE.", "description": "This figure shows the relationship between model size (number of weights) and perplexity (a measure of how well a language model predicts a text) for several large language models (LLMs).  The baseline perplexity for GPT3 and Llama2 models of different sizes are plotted.  Importantly, it shows how the perplexity changes when these models are compressed using the ESPACE technique.  Lower perplexity values indicate better performance.  The results show that ESPACE achieves a significant reduction in model size while maintaining relatively good accuracy.", "section": "Model Compression Studies"}, {"figure_path": "HAcaANQNMK/figures/figures_3_1.jpg", "caption": "Figure 2: Decompositions in GEMMs: (a) baseline multiplication of weight matrix and activation tensor, (b) truncated SVD on the weight matrix, and (c) proposed approach of inserting a static matrix to project activations. With ESPACE, all weights are available for training, while inference compression is achieved via per-computation of (PTW).", "description": "This figure illustrates three different GEMM (General Matrix Multiplication) decompositions. (a) shows a standard GEMM where the weight matrix and activation tensor are directly multiplied. (b) demonstrates weight decomposition using truncated SVD, where the weight matrix is approximated by the product of two smaller matrices, reducing the number of parameters but potentially sacrificing accuracy. (c) presents the ESPACE method, which inserts a static projection matrix before the activation tensor. This projection reduces the dimensionality of the activations, enabling compression at inference time without affecting the weight matrix during training. The pre-computed product of the projection and weight matrices is used for inference, leading to efficient model compression.", "section": "2 Dimensionality Reduction & Projections"}, {"figure_path": "HAcaANQNMK/figures/figures_7_1.jpg", "caption": "Figure 3: Validation perplexity for GPT3-22B when ESPACE is progressively applied to its GEMM layers. The order of layer selection is based on a layer-wise sensitivity analysis.", "description": "This figure shows the validation perplexity of GPT3-22B as ESPACE is progressively applied to its GEMM layers.  The x-axis represents the compressed model size as a percentage of the original size. The y-axis is the validation perplexity.  The black star represents the baseline perplexity. The blue line shows the perplexity when ESPACE is applied without retraining (out-of-the-box). The red squares show the perplexity after retraining the compressed model. The figure highlights that out-of-the-box compression is nearly lossless up to around 20%. After 40% compression, there is a sharp increase in perplexity, but retraining improves the accuracy, thus achieving a healing process.  The order in which the layers are compressed is determined by a layer-wise sensitivity analysis.", "section": "4 Model Compression Studies"}, {"figure_path": "HAcaANQNMK/figures/figures_9_1.jpg", "caption": "Figure 4: Comparison to related works compressing Llama2-7B using matrix factorization techniques.", "description": "This figure compares the performance of ESPACE to three other methods for compressing the Llama2-7B language model: ASVD, SVD-LoRa, and SliceGPT.  The y-axis represents the increase in perplexity compared to the baseline uncompressed model, while the x-axis shows the percentage of model size retained after compression.  The graph shows that ESPACE achieves a lower perplexity increase at various compression rates compared to the other three methods, suggesting that ESPACE is an improvement in activation-centric tensor decomposition compression.", "section": "4 Model Compression Studies"}, {"figure_path": "HAcaANQNMK/figures/figures_18_1.jpg", "caption": "Figure 5: Sensitivity studies on the choice of projection construction for (a) GPT3-1.3B, (b) GPT3-8B, (c) GPT3-22B. For each layer, we apply ESPACE out-of-the-box using the six various candidates for the projection matrix P constructed in Section 3. The black line corresponds to the baseline perplexity.", "description": "This figure shows the results of sensitivity analysis on the choice of projection matrix P for constructing ESPACE.  Six different methods for constructing P, each optimizing a different metric (MSE, NMSE, GO-MSE, GO-MSE with L2 normalization, NL-MSE, NL-MSE with L2 normalization), were tested on three different GPT-3 models (1.3B, 8B, and 22B parameters). For each layer in the model, ESPACE was applied out-of-the-box using each of the six projection matrices.  The resulting validation perplexities are plotted for each layer and each method. The black line in each plot represents the baseline perplexity without ESPACE.", "section": "3 Eigen Static Principal Activation Component Estimation"}, {"figure_path": "HAcaANQNMK/figures/figures_19_1.jpg", "caption": "Figure 5: Sensitivity studies on the choice of projection construction for (a) GPT3-1.3B, (b) GPT3-8B, (c) GPT3-22B. For each layer, we apply ESPACE out-of-the-box using the six various candidates for the projection matrix P constructed in Section 3. The black line corresponds to the baseline perplexity.", "description": "This figure shows the sensitivity analysis of different projection matrix P constructions on the validation perplexity for three different GPT-3 models with different sizes. For each layer in each model, ESPACE is applied out-of-the-box using six different projection matrices and the validation perplexity is measured. The results are presented in three subfigures for GPT-3 1.3B, 8B, and 22B, respectively. Each subfigure displays the validation perplexity for each layer with different choices of projection matrices using different metrics. The black line indicates the baseline perplexity.", "section": "4 Model Compression Studies"}, {"figure_path": "HAcaANQNMK/figures/figures_19_2.jpg", "caption": "Figure 3: Validation perplexity for GPT3-22B when ESPACE is progressively applied to its GEMM layers. The order of layer selection is based on a layer-wise sensitivity analysis.", "description": "This figure shows the validation perplexity of the GPT3-22B model as the ESPACE compression technique is progressively applied to its GEMM (general matrix multiplication) layers.  The layers were added sequentially, starting with the ones that caused the least increase in perplexity when compressed individually.  The x-axis represents the percentage of the model's layers compressed, and the y-axis represents the validation perplexity. The figure helps to show the relationship between compression rate and accuracy degradation.", "section": "4 Model Compression Studies"}, {"figure_path": "HAcaANQNMK/figures/figures_20_1.jpg", "caption": "Figure 8: Progressive out-of-the-box application of ESPACE on GPT3-{1.3B, 8B} and Llama2-{7B, 13B}. The plot for GPT3-22B was provided in the main text in Figure 3. The progressive application of ESPACE is based on the ranking of layers from least to most destructive based on validation perplexity sensistivity in Figure 7.", "description": "This figure shows the results of progressively applying ESPACE to the layers of four different LLMs (GPT3-1.3B, GPT3-8B, Llama2-7B, and Llama2-13B) without retraining. The layers are ordered from least to most impactful on validation perplexity, as determined by a separate sensitivity analysis (Figure 7). The graph plots validation perplexity against the compressed model size (%).  The black star represents the baseline perplexity of the uncompressed model. The blue line shows the perplexity when ESPACE is applied out-of-the-box, and the red squares show the perplexity after retraining the compressed model.  The figure highlights an inflection point where accuracy degradation accelerates (around 20% for GPT3 and Llama2), and a \"healing\" phase following retraining where performance improves.", "section": "4 Model Compression Studies"}]