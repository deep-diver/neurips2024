[{"figure_path": "VpINEEVLX0/figures/figures_1_1.jpg", "caption": "Figure 1: Class distributions in the Kindle dataset change over time and the model trained on new tasks tends to forget old tasks.", "description": "This figure shows two subfigures. (a) shows the distribution of classes in the Kindle dataset across five different time periods (tasks).  It highlights that the class distribution shifts over time, with some classes becoming more prevalent and others less so. (b) displays the F1 score (a measure of classification accuracy) obtained by a model on each of the five tasks.  Importantly, it demonstrates that a model trained on later tasks tends to perform poorly on the earlier ones, a phenomenon known as catastrophic forgetting.", "section": "2 Preliminaries"}, {"figure_path": "VpINEEVLX0/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of TACO. At t-th time period, the model takes in the coarsened graph G<sub>t-1</sub> from the last time period and the original graph G<sub>t</sub> from the current time period, and combine them into G<sup>\u03b5</sup>; for the same time period, the selected important node set is updated with the new nodes; the model is then trained on G<sup>\u03b5</sup> with both the new nodes and the super-nodes from the past; finally G<sup>\u03b5</sup> is coarsened to G<sub>t</sub> for the next time period.", "description": "This figure illustrates the TACO framework's workflow.  It shows how, at each time step (t), the coarsened graph from the previous time step (G<sub>t-1</sub>) is combined with the current time step's original graph (G<sub>t</sub>) to create a combined graph (G<sup>\u03b5</sup>).  Important nodes are selected and updated with new nodes, and the model is trained on this combined graph (G<sup>\u03b5</sup>). Finally, G<sup>\u03b5</sup> undergoes coarsening to produce the reduced graph (G<sub>t</sub>) for the next time step.", "section": "3 Methodology"}, {"figure_path": "VpINEEVLX0/figures/figures_8_1.jpg", "caption": "Figure 3: (a) The test macro-F1 scores of the GCN model trained on the coarsened graphs with different reduction rates on three datasets. (b)-(d) t-SNE visualization of node embeddings of the DBLP test graph with a reduction rate of 0, 0.5, and 0.9 on the training graph respectively.", "description": "This figure shows the results of experiments evaluating the impact of graph reduction rates on the performance of a Graph Convolutional Network (GCN) model for node classification.  The first subplot (a) presents the test macro-F1 scores across three datasets (Kindle, DBLP, ACM) at varying reduction rates, demonstrating how coarsening affects performance. Subplots (b), (c), and (d) visualize the t-SNE embeddings of test nodes from the DBLP dataset after training on graphs coarsened with reduction rates of 0, 0.5, and 0.9, respectively. These visualizations illustrate how the node embeddings change with different reduction rates.", "section": "4.4.2 Graph reduction rates"}, {"figure_path": "VpINEEVLX0/figures/figures_8_2.jpg", "caption": "Figure 1: Class distributions in the Kindle dataset change over time and the model trained on new tasks tends to forget old tasks.", "description": "This figure shows two subfigures. Subfigure (a) presents the distribution of classes in the Kindle dataset across five different time periods (tasks).  It visually demonstrates how the class distribution shifts over time. Subfigure (b) shows the F1 scores obtained by a Graph Convolutional Network (GCN) when trained sequentially on the different tasks. It illustrates the phenomenon of catastrophic forgetting, where the model's performance on older tasks degrades as it learns new tasks. This highlights the problem of continual graph learning and motivates the need for improved methods that can preserve previously learned knowledge.", "section": "2 Preliminaries"}, {"figure_path": "VpINEEVLX0/figures/figures_21_1.jpg", "caption": "Figure 3: (a) The test macro-F1 scores of the GCN model trained on the coarsened graphs with different reduction rates on three datasets. (b)-(d) t-SNE visualization of node embeddings of the DBLP test graph with a reduction rate of 0, 0.5, and 0.9 on the training graph respectively.", "description": "This figure shows the results of experiments evaluating the impact of graph reduction rates on the performance of a GCN model for node classification.  Subfigure (a) presents a graph illustrating the relationship between reduction rate and macro-F1 score across three datasets (Kindle, DBLP, and ACM). Subfigures (b), (c), and (d) use t-SNE to visualize the node embeddings from the DBLP dataset for reduction rates of 0, 0.5, and 0.9 respectively.  The visualizations help to assess how well the essential topological information is preserved during the graph coarsening process at different reduction rates.", "section": "4.4.2 Graph reduction rates"}, {"figure_path": "VpINEEVLX0/figures/figures_26_1.jpg", "caption": "Figure 3: (a) The test macro-F1 scores of the GCN model trained on the coarsened graphs with different reduction rates on three datasets. (b)-(d) t-SNE visualization of node embeddings of the DBLP test graph with a reduction rate of 0, 0.5, and 0.9 on the training graph respectively.", "description": "This figure shows the results of experiments evaluating the impact of graph reduction rate on model performance and node embedding similarity.  (a) plots the macro-F1 scores on test sets across three different datasets for various graph reduction ratios. (b)-(d) visualize the t-SNE embeddings of test nodes from the DBLP dataset, comparing embeddings for different reduction rates (0, 0.5, 0.9) to show how well the topological information is preserved after graph coarsening.", "section": "4.4.2 Graph reduction rates"}, {"figure_path": "VpINEEVLX0/figures/figures_28_1.jpg", "caption": "Figure 3: (a) The test macro-F1 scores of the GCN model trained on the coarsened graphs with different reduction rates on three datasets. (b)-(d) t-SNE visualization of node embeddings of the DBLP test graph with a reduction rate of 0, 0.5, and 0.9 on the training graph respectively.", "description": "This figure demonstrates the effect of graph reduction rate on the performance of a GCN model for node classification. It shows that the model performs relatively stably even with significant reduction on two datasets while the performance is more sensitive to the reduction rate for another dataset. The t-SNE visualization of node embeddings shows similar patterns of node embeddings between original and coarsened graphs with low reduction rates, while more significant changes are observed with higher reduction rates.", "section": "4.4.2 Graph reduction rates"}, {"figure_path": "VpINEEVLX0/figures/figures_28_2.jpg", "caption": "Figure 8: Memory usages of different methods over tasks.", "description": "This figure shows a comparison of memory usage across different continual graph learning methods (joint_train, ER_reservior, SSM, SSRM, TACO) over multiple tasks for three different datasets (Kindle, DBLP, ACM).  The y-axis represents memory usage in MB, and the x-axis represents the task number. The figure demonstrates that TACO maintains relatively stable memory usage regardless of the number of tasks, while other methods show a more significant increase in memory usage as the number of tasks grows.", "section": "4.4.2 Graph reduction rates"}]