[{"type": "text", "text": "A Topology-aware Graph Coarsening Framework for Continual Graph Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaoxue Han Stevens Institute of Technology xhan26@stevens.edu ", "page_idx": 0}, {"type": "text", "text": "Zhuo Feng Stevens Institute of Technology zfeng12@stevens.edu ", "page_idx": 0}, {"type": "text", "text": "Yue Ning Stevens Institute of Technology yue.ning@stevens.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) experience \"catastrophic forgetting\" in continual learning setups, where they tend to lose previously acquired knowledge and perform poorly on old tasks. Rehearsal-based methods, which consolidate old knowledge with a replay memory buffer, are a de facto solution due to their straightforward workflow. However, these methods often fail to adequately capture topological information, leading to incorrect input-label mappings in replay samples. To address this, we propose $\\mathbf{TAC}\\mathbb{O}$ , a topology-aware graph coarsening and continual learning framework that stores information from previous tasks as a reduced graph. Throughout each learning period, this reduced graph expands by integrating with a new graph and aligning shared nodes, followed by a \"zoom-out\" reduction process to maintain a stable size. We have developed a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph while preserving essential topological information. We empirically demonstrate that the learning process on the reduced graph can closely approximate that on the original graph. We compare $\\mathbf{TAC}\\mathbb{O}$ with a wide range of state-of-the-art baselines, proving its superiority and the necessity of preserving high-quality topological information for effective replaying. Our code is available at: https://github.com/hanxiaoxue114/TACO. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph neural networks (GNNs) are oblivious: they fail to consider pre-existing knowledge or context outside of the information they were trained on. In offline settings, this problem can be mitigated by making multiple passes through the dataset with batch training. However, in a continual learning (also known as incremental learning or lifelong learning) setup, [43, 20, 40, 39], the model learns a sequence of tasks incrementally, where each task is defined as a learning session on a subgraph. This problem becomes more intractable as the model has no access to previous data, resulting in drastic degradation of model performance on old tasks. To tackle the issue of \u201ccatastrophic forgetting\u201d in GNNs, several approaches have been proposed. Among them, rehearsalbased methods [18, 57, 48, 55] are the most common due to their straightforward workflow and efficacy in consolidating old knowledge with affordable additional memory. When performing node classification tasks, these methods utilize memory buffers to save node samples during the rehearsal process of online graph training. However, they often fail to adequately capture topological information which is important in downstream tasks. As demonstrated by Z\u00fcgner et al. [58], even small changes in edges can alter the expected node labels, causing the model to learn incorrect input (feature, structure)-label mappings from the replay samples which creates vulnerability and security issues in critical domains [42]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address this, we propose a dynamic graph coarsening framework, $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ , that efficiently preserves high-quality topology information for experience replay. TA $\\mathbb{C}\\mathbb{O}$ operates through a straightforward yet effective workflow: At the end of each training task, it reduces the current graph into a compressed form that maximally preserves its properties, allowing it to serve as a proxy for the original graph; for the next task, the reduced graph and the new graph are combined by aligning co-existing nodes, and the model is trained on this new combined graph. The former step preserves information from the intra-task edges (edges connecting nodes from the same task), while the latter step allows us to retrieve the inter-task (edges connecting nodes from different tasks) ones. Noticing that most existing graph reduction algorithms focus solely on preserving graph topology properties and are often inefficient, we propose an efficient graph reduction algorithm, RePro, as a component of the CGL framework. ", "page_idx": 1}, {"type": "text", "text": "RePro leverages the proximities between learned node representations to effectively preserve node features and spectral properties during the reduction process. Additionally, we present a strategy, Node Fidelity Preservation, to ensure that certain nodes are not compressed, thereby maintaining the quality of the reduced graph. We theoretically prove that Node Fidelity Preservation can mitigate the problem of vanishing minority classes in the process of graph reduction. We claim that the simplicity of $\\mathbf{TAC}\\mathbb{O}$ makes it highly modular and adaptable. We conduct extensive experiments and perform comprehensive ablation studies to evaluate the effectiveness of $\\mathbf{TAC}\\mathbb{O}$ and RePro. We also compare our method with multiple state-of-the-art methods [20, 26, 29, 6, 57, 7, 28, 30, 55] for both CGL and graph coarsening tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Setup explanations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We notice that most existing research on CGL [26, 57, 55] focuses on either task-incrementallearning (task-IL) [45] or a tranductive [54, 3] setting, where the sequential tasks are independent graphs containing nodes from non-overlapping class sets. In this setting, the model only needs to distinguish the classes included in the current task. For instance, if there are 10 classes in total, and this experimental setting divides these classes into 5 tasks. Task 1 focuses on classifying classes 1 and 2, while task 2 classifies classes 3 and 4, and so on. Since GNNs often forget knowledge of old classes and show trivial performance, the improvement of the CGL framework can be easily highlighted with large margins. However, we argue that this setting may not accurately simulate real-life scenarios. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to tackle a more realistic inductive and Generalized Class-incremental-learning (generalized class-IL) [32] setting. In real-world graphs, nodes and edges are often associated with a time stamp indicating their appearing time, and graphs keep expanding with new nodes and edges. For instance, in a citation network, each node representing a paper cites (forms an edge with) other papers when it is published. Each year more papers are published, and the citation graph also grows rapidly. ", "page_idx": 1}, {"type": "text", "text": "In such cases, it is necessary to train a model incrementally and dynamically because saving or retraining the model on the full graph can be prohibitively expensive in space and time. So we split a streaming graph into subgraphs based on time periods and train a model on each subgraph sequentially for the node classification task. In such cases, subgraphs are correlated with each other through the edges connecting them (e.g. a paper cites another paper from previous time stamps). The structural information represented by edges may change from previous tasks (inductive). Also, since the tasks are divided by time periods instead of class labels, there are overlapping classes between new tasks and old ", "page_idx": 1}, {"type": "image", "img_path": "VpINEEVLX0/tmp/f94796261eda48cff0f8b628bcb20023db0b999debda4821340e6f1df5abeb92.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Class distributions in the Kindle dataset change over time and the model trained on new tasks tends to forget old tasks. ", "page_idx": 1}, {"type": "text", "text": "tasks, so the model needs to perform classification across all classes (generalized class-IL). ", "page_idx": 1}, {"type": "text", "text": "We further demonstrate the necessity of preserving old knowledge when learning on a streaming graph. We take the Kindle e-book co-purchasing network [14, 31] as an example. We split the graph into 5 subgraphs based on the first appearing time of each node (i.e., an e-book). We observe a gradual shift of node class distribution over time, as shown in Figure 1(a). Furthermore, even for nodes in the same class, their features and neighborhood patterns can shift [18]. Also, in real-life situations, tasks may have different class sets (e.g. new fields of study emerge and old fields of study become obsolete), which exacerbates the forgetting problem. The F1 scores of node classification tasks using a Graph Convolutional Network (GCN) [19] show that the model performs significantly worse on previous tasks when trained on new ones without strategies to alleviate forgetting, as shown in Figure 1(b). Although this paper focuses on generalized class- $\\mathit{I L}$ setting, we also conduct experiments on data splits with traditional class-IL setting to prove the generalizability of $\\mathbf{TAC}\\mathbb{O}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Problem statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our main objective is to construct a continual learning framework on streaming graphs to overcome the catastrophic forgetting problem. Suppose a GNN model is trained for sequential node classification tasks with no access to previous training data, but it can utilize a memory buffer with a limited capacity to store useful information. The goal is to optimize the prediction accuracy of a model on all tasks in the end by minimizing its forgetting of previously acquired knowledge when learning new tasks. In this work, we focus on time-stamped graphs, and the tasks are defined based on the time stamps of nodes in a graph. For each task, the GNN model is trained on a subgraph where source nodes belonging to a specified time period, and all tasks are ordered in time. Node attributes and class labels are only available if the nodes belong to the current time period. The aforementioned setup closely resembles real-life scenarios. We formulate the above problems as follows. ", "page_idx": 2}, {"type": "text", "text": "Problem 1. Continual learning on time-stamped graphs. We are given a time-stamped expanding graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E},A,X,Y)$ , where $\\mathcal{V}$ denotes the node set, $\\mathcal{E}$ denotes the edge set, $\\bar{A}\\in\\bar{\\mathbb{R}}^{|\\mathcal{V}|\\times|\\bar{\\nu_{|}}}$ and $X\\,\\in\\,\\mathbb{R}^{|\\mathcal{V}|\\,\\times\\,d_{X}}$ denote the adjacency matrix and node features, respectively; $Y$ is a one-hot embedding matrix denoting class labels. Each node $v\\in\\mathcal{V}$ is assigned to a time period $\\tau(v)$ . We define a sequence of subgraphs, $\\mathcal{G}_{1},...,\\mathcal{G}_{k}$ , such that each subgraph $\\mathcal{G}_{t}=(\\mathcal{V}_{t},\\mathcal{E}_{t},A_{t},X_{t})$ from $\\mathcal{G}$ is formulated based on the following rules: ", "page_idx": 2}, {"type": "text", "text": "where $s$ is a source node and $o$ (or $v$ ) is a target node. We can assume $\\tau(o)\\leq\\tau(s)$ for $(s,o)\\in\\mathcal{E}$ (e.g. in a citation network, a paper can not cite another paper published in the future). The nodes on each subgraph $\\mathcal{G}_{t}$ , are divided into three sets for training, validation, and test. We implement a GNN to perform node classification tasks and sequentially train the model on $\\mathcal{G}_{1},...,\\mathcal{G}_{k}$ . When the model is trained with a new task $T_{t}$ , it has no access to $\\mathcal{G}_{1},...,\\mathcal{G}_{t-1}$ and $\\mathcal{G}_{t+1},...,\\mathcal{G}_{k}$ . However, a small memory buffer is allowed to preserve useful information from previous tasks. The objective is to optimize the overall performance of the model on test nodes from all tasks when the model is incrementally trained with new tasks. ", "page_idx": 2}, {"type": "text", "text": "Problem 2. Graph coarsening. Given a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ with $n=|\\nu|$ nodes, the goal of graph coarsening is to reduce it to a target size $n^{\\prime}$ with a specific ratio $\\gamma$ where $n^{\\prime}=\\lfloor\\gamma\\cdot n\\rfloor$ , $0<\\gamma<1$ . We construct the coarsened graph $\\mathcal{G}^{r}\\,=\\,(\\mathcal{V}^{r},\\mathcal{E}^{r})$ through partitioning $\\mathcal{V}$ to $n^{\\prime}$ disjoint clusters $\\left(C_{1},...,C_{n^{\\prime}}\\right)$ , so that each cluster becomes a node in $\\mathcal{G}_{r}$ . The construction of these clusters (i.e., the partition of a graph) depends on coarsening strategies. The node partitioning/clustering information can be represented by a matrix $Q\\in\\mathbb{B}^{n\\times\\Bar{n^{\\prime}}}$ . If we assign every node $i$ in cluster $C_{j}$ with the same weight, then $Q_{i j}=1$ ; If node $i$ is not assigned to cluster $C_{j}$ , $Q_{i j}=0$ . Let $c_{j}$ be the number of node in $C_{j}$ and $C=\\mathrm{diag}(c_{1},...,c_{n^{\\prime}})$ . The normalized version of $Q$ is $P=Q C^{1/2}$ . It is easy to prove $P$ has orthogonal columns $'P P^{-1}=I)$ , and $P_{i j}=1/\\sqrt{c_{j}}$ if node $i$ belongs to $C_{j}$ ; $P_{i j}=0$ if node $i$ does not belong to $C_{j}$ . The detailed coarsening algorithm will be discussed in the next section. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ , a simple yet effective continual learning framework to consolidate graph knowledge learned from proceeding tasks by replaying previous \u201cexperiences\u201d to the model. We observe that the majority of experience replay methods, including those tailored for GNN, do not adequately maintain the intricate graph topological properties from previous tasks. Moreover, in a streaming graph setup they fail to capture the inter-dependencies among tasks that result from the presence of overlapping nodes. The inter-dependencies are essential for capturing the dynamic \u201creceptive field\u201d (neighborhood) of nodes and improving the performance on both new and old tasks [18]. To overcome these limitations, we design a new replay method that preserves both the node attributes and graph topology from previous tasks. Our intuition is that, if we store the original graphs from the old task, minimal old knowledge would be lost, but it is also exceedingly inefficient and goes against the initial intention of continual learning. Thus, as an alternative, we coarsen the original graphs to a much smaller size which preserves the important properties (such as node features and graph topologies) of the original graphs. We propose an efficient graph coarsening algorithm based on Node Representation Proximity as a key component of $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ . Additionally, we develop a strategy called Node Fidelity Preservation for selecting representative nodes to retain high-quality information. An overview of the proposed framework is provided in Figure 2. The pseudocode of $\\mathbf{TAC}\\mathbb{O}$ is described in Algorithm 1 and RePro is described in Algorithm 2 in Appendix D.2. ", "page_idx": 2}, {"type": "image", "img_path": "VpINEEVLX0/tmp/df7c8c90f19de81e83a18cef4627ad3f0e7a2fb6cb40acef88bbd5b73d1652f5.jpg", "img_caption": ["Figure 2: An overview of $\\mathbf{TAC}\\mathbb{O}$ . At $t$ -th time period, the model takes in the coarsened graph $\\mathcal{G}_{t-1}^{r}$ from the last time period and the original graph $\\mathcal{G}_{t}$ from the current time period, and combine them into $\\mathcal{G}_{t}^{c}$ ; for the same time period, the selected important node set is updated with the new nodes; the model is then trained on $\\mathcal{G}_{t}^{c}$ with both the new nodes and the super-nodes from the past; finally $\\mathcal{G}_{t}^{c}$ is coarsened to $\\mathcal{G}_{t}^{r}$ for the next time period. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Overall framework We summarize the procedure of our framework as three steps: combine, reduce, and generate. At task $t$ , we combine the new graph $\\mathcal{G}_{t}$ with the reduced graph $\\mathcal{G}_{t-1}^{r}$ from the last task. Then we reduce the combined graph $\\mathcal{G}_{t}^{c}$ to a set of clusters. At last, we generate the contributions of nodes in each cluster to form a super-node in the reduced graph $\\mathcal{G}_{t}^{r}$ . The last step decides the new node features and the adjacency matrix of the reduced graph. We convey the details of each step below. ", "page_idx": 3}, {"type": "text", "text": "3.1 Step 1: Combine ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use $\\mathcal{M}$ (e.g., a hash table) to denote the mapping of each original node to its assigned cluster (super-node) in a reduced graph $\\mathcal G^{r}$ . In the beginning, we initialize $\\mathcal{G}_{0}^{r}$ as an empty undirected graph and $\\mathcal{M}_{0}$ as an empty hash table. At task $t$ , the model holds copies of $\\mathcal{G}_{t-1}^{r}$ , $\\mathcal{M}_{t-1}$ and an original graph $\\mathcal{G}_{t}$ for the current task. $\\mathcal{G}_{t}$ contains both new nodes from task $t$ and old nodes that have appeared in previous tasks. We first \u201ccombine\u201d $\\mathcal{G}_{t}$ with $\\mathcal{G}_{t-1}^{r}$ to form a new graph $\\mathcal{G}_{t}^{c}$ by checking the hash table $\\mathcal{M}_{t-1}$ and aligning the co-existing nodes in $\\mathcal{G}_{t}$ and $\\mathcal{G}_{t-1}^{r}$ . By doing so, we retrieve the inter-task edges. We train the model $f$ to perform node classification tasks on the combined graph $\\mathcal{G}_{t}^{c}=(A_{t}^{c},X_{t}^{c},Y_{t}^{c})$ with the objective arg min\u03b8 $\\ell(f(A_{t}^{c},X_{t}^{c},\\theta),Y_{t}^{c})$ , where $f$ is a $L$ -layer GNN model (e.g. GCN), $\\theta$ is trainable parameters of the model, and $\\ell$ is the loss function. In this work, new nodes and old nodes in $\\mathcal{G}_{t}^{c}$ contribute equally to the loss during the training process. However, it remains an option to assign distinct weights to these nodes to ensure a balance between learning new information and consolidating old knowledge. We describe a more detailed process in Appendix D.1. ", "page_idx": 3}, {"type": "text", "text": "3.2 Step 2: Reduce ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We decide how nodes are grouped into clusters and each cluster forms a new super-node in the reduced graph. We propose an efficient graph coarsening method, RePro, by leveraging the Representation Proximities of nodes to reduce the size of a graph through merging \u201csimilar\u201d nodes to a super-node. Node representations are automatically learned via GNN models without extra computing processes. We think two nodes are deemed similar based on three factors: 1) feature similarity, which evaluates the closeness of two nodes based on their features; 2) neighbor similarity, which evaluates two nodes based on their neighborhood characteristics; 3) geometry closeness, which measures the distance between two nodes in a graph (e.g., the length of the shortest path between them). Existing graph coarsening methods concentrate on preserving spectral properties, only taking graph structures into account and disregarding node features. However, estimating spectral similarity between two nodes is typically time-consuming, even with approximation algorithms, making it less scalable for our applications where graphs are dynamically expanded and coarsened. Thus, we aim to develop a more time-efficient algorithm that considers the aforementioned similarity measures. ", "page_idx": 4}, {"type": "text", "text": "To get started, we have the combined graph $\\mathcal{G}_{t}^{c}$ to be coarsened. We train a GNN model with $L$ $L=2$ in our case) layers on $\\mathcal{G}_{t}^{c}$ , such that the node embedding of $\\mathcal{G}_{t}^{c}$ at the first layer (before the activation function) is denoted by ", "page_idx": 4}, {"type": "equation", "text": "$$\nH\\in\\mathbb{R}^{n_{t}^{c}\\times d^{h}}=\\mathrm{GNN}^{(1)}(A_{t}^{c},X_{t}^{c},\\theta),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d^{h}$ is the size of the first hidden layer in GNN. The similarity between every connected node pair $(u,v)=e\\in\\mathcal{E}_{t}^{c}$ is calculated based on cosine similarity as $\\begin{array}{r}{\\beta(e)=\\frac{H_{u}\\cdot H_{v}}{\\|H_{u}\\|\\|H_{v}\\|}}\\end{array}$ , where $\\beta(e)$ is the similarity score between the two end nodes of the edge , is the embedding for node , and $\\Vert\\cdot\\Vert$ is the second norm of a vector. We then sort all edges of $\\mathcal{G}_{t}^{c}$ such that $\\beta(e_{1})\\overset{\\cdot}{\\geq}\\beta(e_{2})\\geq\\ldots\\geq\\beta(e_{m_{t}^{c}})$ . Based on this sorted list of edges, we recursively merge two end nodes (i.e., assign the nodes to the same cluster), until the target size is reached. The class label of the merged node is determined based on the majority votes of the original nodes. The new adjacency matrix and node feature matrix are discussed in Step 3. The time complexity of our coarsening process is $\\mathcal{O}(d^{h}\\cdot m_{t}^{c})$ where $m_{t}^{c}$ is the number of edges in the current graph $\\mathcal{G}_{t}^{c}$ . ", "page_idx": 4}, {"type": "text", "text": "Node Fidelity Preservation After multiple rounds of coarsening, the quality of a graph deteriorates as its node features and labels are repeatedly processed. Furthermore, the use of a majority vote to determine the label of a cluster can lead to the gradual loss of minority classes and cause a \u201cvanishing minority class\u201d problem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Consider $n$ nodes with c classes, such that the class distribution of all nodes is represented by $\\mathbf{p}=p_{1},p_{2},...,p_{c},$ , where $\\begin{array}{r}{\\sum_{i=1}^{c}p_{i}=1.}\\end{array}$ . If these nodes are randomly partitioned into $n^{\\prime}$ clusters such that $n^{\\prime}=\\lfloor\\gamma\\cdot n\\rfloor$ , $0<\\gamma<1$ and the class label for each cluster is determined via majority voting. The class distribution of all the clusters is $\\mathbf{p}^{\\prime}=p_{1}^{\\prime},p_{2}^{\\prime},...,p_{c}^{\\prime}$ where $\\mathbf{p}_{i}^{\\prime}$ is computed as the ratio of clusters labeled as class $i$ and $\\textstyle\\sum_{i=1}^{c}p_{i}^{\\prime}=1$ . Let $k$ be one of the classes, and the rest of the class are balanced $p_{1}=...=p_{k-1}=p_{k+1}=...=p_{c}$ . It holds that: ", "page_idx": 4}, {"type": "text", "text": "2. When $p_{k}<1/c,\\mathbb{E}[p_{k}^{\\prime}]<p_{k},$ , and $\\mathbb{E}\\big[\\frac{p_{k}^{\\prime}}{p_{k}}\\big]$ decreases as $n^{\\prime}$ decreases. There exists a $p^{m i n}$ such that $0<p^{m i n}<1$ , and when $p_{k}<p^{m i n},\\mathbb{E}\\big[\\frac{p_{k}^{\\prime}}{p_{k}}\\big]$ decrease as $p_{k}$ decreases. ", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 3.1 is provided in Appendix D.3. Theorem 3.1 shows that as the ratio of a class decreases, its decline becomes more severe when the graph is reduced. Eventually, the class may even disappear entirely from the resulting graph. To combat these issues, we suggest preserving representative nodes in a \u201creplay buffer\u201d denoted as $\\mathcal{V}_{t}^{r b}$ . We adopt three strategies from [6] to select representative nodes, namely Reservoir Sampling, Ring Buffer, and Mean of Features. The replay buffer has a fixed capacity and is updated as new nodes are added. During the coarsening process, we prevent the selected nodes in $\\mathcal{V}_{t}^{r b}$ from being merged by imposing a penalty on the similarity score $\\bar{\\beta}(e)$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\beta(e)=\\frac{H_{u}\\cdot H_{v}}{\\|H_{u}\\|\\|H_{v}\\|}-\\mathbb{1}(u\\in\\mathcal{V}_{t}^{r b}\\;\\mathrm{or}\\;v\\in\\mathcal{V}_{t}^{r b})\\cdot\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\epsilon$ is a constant and $\\epsilon>0$ . It is worth noting that we do not remove the edges of these nodes from the list of candidates. Instead, we assign a high value to the penalty $\\epsilon$ . This is to prevent scenarios where these nodes play a critical role in connecting other nodes. Removing these nodes entirely may lead to the graph not being reduced to the desired size due to the elimination of important paths passing through them. We make the following observation: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Observation 3.1. Node Fidelity Preservation with buffer size b can alleviate the declination of a minority class $k$ when $p_{k}$ decreases and $n^{\\prime}$ decreases, and prevent class k from vanishing when $p_{k}$ is small. See Appendix D.3 for further discussions. ", "page_idx": 5}, {"type": "text", "text": "Note that RePro does not require any additional parameters or training time. It only relies on the learned node embeddings from the graph neural network in the continual learning. We train a GNN model on a combined graph at each time step for node classification tasks. The node embeddings learned from the GNN model at different layers are representative of the nodes\u2019 neighborhoods. We use this fact and propose to measure the similarity of two nodes based on the distance between their embedding vectors. However, it takes quadratic time to calculate the pair-wise distance among nodes, thus we make a constraint that only connected nodes can be merged. Since connectivity has also been used to estimate the geometry closeness of two nodes [36], by doing so, we are able to cover the three similarity measures as well as reduce the time complexity to linear time in terms of the number of edges to calculate node similarities. ", "page_idx": 5}, {"type": "text", "text": "Our proposed approach based on node representations seems to be distinct from spectral-based methods, but they share a similar core in terms of the preserving of graph spectral properties. See Appendix D.4 for more details. ", "page_idx": 5}, {"type": "text", "text": "3.3 Step 3: Generate ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From the last step, we get the membership matrix $Q_{t}\\in\\mathbb{B}^{n_{t}^{c}\\times n_{t}^{r}}$ where $n_{t}^{c}$ is the number of nodes in the combined graph, $n_{t}^{r}\\,=\\,\\lfloor\\gamma\\cdot n_{t}^{c}\\rfloor$ is the number of nodes in the coarsened graph and $\\gamma$ is the coarsening ratio. $Q_{t}[i,j]\\;=\\;1$ denotes that node $i$ is assigned to super-node $j$ . Otherwise, $Q_{t}[i,j]=0$ . ", "page_idx": 5}, {"type": "text", "text": "A simple way to normalize $Q_{t}$ is assuming each node contributes equally to their corresponding super-node (e.g. $Q_{t}[i,j]=1/\\sqrt{c_{j}}$ for all any node $i$ that belongs to cluster/supernode $j]$ ). However, nodes might have varying contributions to a cluster depending on their significance. Intuitively, when a node is identified as a very popular one that is directly or indirectly connected with a large number of other nodes, preserving more of its attributes can potentially mitigate the effects of the inevitable \u201cinformation degrading\u201d caused by the graph coarsening procedure. To address the above issue, we propose to use two different measures to decide a node\u2019s importance score: 1) node degree: the number of 1-hop neighbors of the node. 2) neighbor degree sum: the sum of the degrees of a node\u2019s 1-hop neighbors. In this step, we propose to normalize $Q_{t}$ to $P_{t}$ utilizing these node importance information. We calculate the member contribution matrix $P_{t}\\in\\mathbb{R}^{n_{t}^{c}\\times n_{t}^{r}}$ . Let $i$ be a node belonging to cluster $C_{j}$ at timestamp $t$ , and $s_{i}>0$ be the importance score of node $i$ (node degree or neighbor degree sum), then pt,(ij) = $\\begin{array}{r}{p_{t,(i j)}=\\sqrt{\\frac{s_{i}}{\\sum_{v\\in C_{j}}s_{v}}}}\\end{array}$ v\u2208sCij sv . It is straightforward to prove that P t\u22a4 Pt = I still holds. Once we have $P_{t}$ , we get the new reduced graph $\\mathcal{G}_{t}^{r}=(A_{t}^{r},X_{t}^{r},Y_{t}^{r})$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{t}^{r}=Q_{t}^{\\top}A_{t}^{c}Q_{t},\\quad X_{t}^{r}=P_{t}^{\\top}X_{t}^{c},\\quad Y_{t}^{r}=\\arg\\operatorname*{max}(P_{t}^{\\top}Y_{t}^{c}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the label for each cluster is decided by a (weighted) majority vote. Only partial nodes are labeled, and the rows of $Y_{t}^{c}$ for unlabelled nodes are zeros and thus do not contribute to the vote. ", "page_idx": 5}, {"type": "text", "text": "Through training all tasks, the number of nodes in the reduced graph $\\mathcal G^{r}$ is upper-bounded by $\\frac{1-\\gamma}{\\gamma}\\cdot\\bar{(n_{\\mathrm{MAX}})}$ , where $n_{\\mathrm{MAX}}$ is the largest number of the new nodes for each task (See Appendix D.5 for proof); when the reduction ratio $\\gamma$ is 0.5, the expression above is equivalent to $n_{\\mathrm{MAX}}$ , meaning the size of the reduced graph is roughly the same size with the original graph for each task. The number of edges m is bounded by n2MAX , but we observe generally $\\bar{m}\\ll\\bar{n_{\\mathrm{MAX}}^{2}}$ in practice. We claim that such a memory buffer size is reasonable for storing topology information. For instance, the default buffer size of the Cora-full dataset in SSM [55] is 4,200, which is approximately seven times the average graph size. ", "page_idx": 5}, {"type": "text", "text": "4 Empirical evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments on time-stamped graph datasets: Kindle [14, 31], DBLP [44] and ACM [44] to evaluate the performance of $\\mathbf{TAC}\\mathbb{O}$ . See Appendix E.1 for the details of the datasets and E.2 for hyperparameter setup. ", "page_idx": 6}, {"type": "text", "text": "4.1 Comparison methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare the performance of $\\mathbf{TAC}\\mathbb{O}$ with SOTA continual learning methods including EWC [20], GEM [29], TWP [26], OTG [12], ERGNN [57], SSM [55], DyGrain [18], IncreGNN [48], SSRM [41], CaT [27], and DeLoMe [33]. EWC and GEM were previously not designed for graphs, so we train a GNN on new tasks but ignore the graph structure when applying continual learning strategies. ERGNN-rs, ERGNN-rb, and ERGNN-mf are ERGNN methods with different memory buffer updating strategies: Reservoir Sampling (rs), Ring Buffer (rb), and Mean of Features (mf) [6]. SSRM is an additional regularizer to be applied on top of a CGL framework; we choose ERGNN-rs as the base CGL model. Besides, finetune provides the estimated lower bound without any strategies applied to address forgetting problems, and joint-train provides an empirical upper bound where the model has access to all previous data during the training process. ", "page_idx": 6}, {"type": "text", "text": "We also compare RePro with five representative graph coarsening SOTA methods. We replace the coarsening algorithm in $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ with different coarsening algorithms. Alge. JC [7], Aff.GS [28], Var. edges [30], and Var. neigh [30] are graph spectral-based methods; FGC [21] considers both graph spectrals and node features. We follow a standard implementation [15] for the first four methods. ", "page_idx": 6}, {"type": "text", "text": "4.2 Evaluation metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use Average Performance $(\\mathrm{AP\\uparrow})$ ) and Average Forgetting $(\\mathrm{AF}\\downarrow)$ ) [6] to evaluate the performance on test sets. AP and AF are defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{A P}=\\frac{1}{T}\\sum_{j=1}^{T}a_{T,j},\\;\\;\\mathsf{A F}=\\frac{1}{T}\\sum_{j=1}^{T}\\operatorname*{max}_{l\\in\\{1,\\ldots,T\\}}a_{l,j}-a_{T,j},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathrm{T}$ is the total number of tasks and $a_{i,j}$ is the prediction metric of the model on the test set of task $j$ after it is trained on task $i$ . The prediction performance can be measured with different metrics. In this paper, we use macro F1 and balanced accuracy score (BACC). F1-AP and F1-AF indicate the AP and the AF for macro F1 and likewise for BACC-AP and BACC-AF. We calculate the macro F1 and BACC scores for multi-class classification [13]. ", "page_idx": 6}, {"type": "text", "text": "4.3 Main results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of $\\mathbf{TAC}\\mathbb{O}$ and other baselines on three datasets with three backbone GNN models, including GCN [19], GAT [46], and GIN [49]. We only report the node classification performance with GCN in Table 1 due to the space limit. See Appendix F.1 for results on GAT and GIN. We report the average values and the standard deviations over 10 runs. It shows that $\\mathbf{TAC}\\mathbb{O}$ outperforms the best SOTA CGL baseline method with high statistical significance, as evidenced by p-values below 0.05 reported from a t-test. Additionally, we note that despite being Experience Replay-based methods, ER-rs, ER-rb, and ER-mf do not perform as well as SSM and $\\mathbf{TAC}\\mathbb{O}$ , highlighting the importance of retaining graph structural information when replaying experience nodes to the model. Furthermore, we infer that $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ outperforms SSM due to its superior ability to preserve graph topology information and capture task correlations through co-existing nodes. ", "page_idx": 6}, {"type": "text", "text": "4.4 Ablation studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.4.1 Graph coarsening methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of RePro by replacing the graph coarsening module of $\\mathbf{TAC}\\mathbb{O}$ with five widely used coarsening algorithms while keeping all other components unchanged. For each method, we report the average time in seconds consumed to coarsen the graph. We also report a trade-off value which is defined as the coarsening time (in seconds) needed to increase or decrease the AP/AF by $1\\%$ compared with fine-tuning, as shown in Table 2. For instance, the trade-off for F1-AP is defined as $\\tau_{\\Delta\\mathrm{F}1-\\mathrm{AP}}=\\mathrm{time}/(\\mathrm{F}1-\\mathrm{AP}-\\mathrm{F}1-\\mathrm{AP}$ for fine-tuning). Complete results are provided in Appendix F.2. The results demonstrate that RePro is considerably more efficient in computing time compared to all other models and achieves the best trade-off across all metrics. ", "page_idx": 6}, {"type": "table", "img_path": "VpINEEVLX0/tmp/a0a75fbde7f4dabc38a841150c28f0ba28c6706769830335d4c40ab31c91ad84.jpg", "table_caption": ["Table 1: Node classification performance with GCN as the backbone on three datasets (averaged over 10 trials). Standard deviation is denoted after $\\pm$ . "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "VpINEEVLX0/tmp/51aea84be623f40816e4bb8796f8fa43798f7556c6c86361cfc6ea398b8452f2.jpg", "table_caption": ["Table 2: Coarsen runtime (seconds) and trade-off results of different coarsening methods on three datasets with GCN (average over 10 trials). Boldface indicates the best result of each column. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4.2 Graph reduction rates ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We examine how RePro preserves original graph information. Following the setting in [15], we train GNNs from scratch on original and coarsened graphs, then compare their prediction performance on the test nodes from the original graph. Using subgraphs from the first task of three datasets as original graphs, we train GCN models on coarsened graphs with different coarsening rates $1-\\gamma$ . Figure 3 (a) shows that prediction performance is relatively stable as graphs are reduced for DBLP and ACM, but F1 scores are more sensitive to the reduction rate on Kindle. This may be due to overfitting on smaller datasets. Although reduced graphs may not preserve all information, they can still be used to consolidate learned knowledge and reduce forgetting in CGL paradigm. We also test if the model learns similar node embeddings on coarsened and original graphs. In Figure 3 (b)-(d), we visualize test node embeddings on the DBLP dataset for different reduction rates using t-SNE. We observe similar patterns for 0 and 0.5 reduction rates, and gradual changes as the reduction rate increases. ", "page_idx": 7}, {"type": "text", "text": "4.4.3 Performance after training on each task ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first investigate the performance of the model on the first task when more tasks are learned with different CGL frameworks. We also visualize the model\u2019s performance in terms of AP-F1 using the four approaches on all previous tasks after training on each task on the Kindle dataset, as shown in Figure 4. It demonstrates that the application of different CGL methods can alleviate the catastrophic forgetting problem on the Kindle dataset as we point out in the introduction part to varying degrees. Also, we observe that the performance of experience replay-based methods (SSM and $\\mathbf{TAC}\\mathbb{O})$ are more stable through learning more tasks, but the regularization-based method, TWP, experiences more fluctuations. We deduce that this is caused by the fact that regularization-based methods can better prevent the model from drastically forgetting previous tasks even when the current task has more distinct distributions. ", "page_idx": 7}, {"type": "image", "img_path": "VpINEEVLX0/tmp/4c8be3ab08b6794429fd8702092424d79dc0f10f9353929f5d8d7eed4a5bd998.jpg", "img_caption": ["Figure 3: (a) The test macro-F1 scores of the GCN model trained on the coarsened graphs with different reduction rates on three datasets. (b)-(d) t-SNE visualization of node embeddings of the DBLP test graph with a reduction rate of 0, 0.5, and 0.9 on the training graph respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.5 Efficiency analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To ensure fair comparisons, for experience replay-based methods, we adjust the memory buffer size so each model stores comparable average numbers of replay nodes. We report the average memory usage of each model on each task in Table 10 in Appendix G.3. Since TACO needs extra space to save topology information, its memory usage is slightly larger compared to methods that don\u2019t preserve graph structures. We provide memory usages of the representative methods across tasks in Table 8. ", "page_idx": 8}, {"type": "text", "text": "4.6 Additional studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For those readers who are interested, we also evaluate the short-term forgetting (G.1) and the performance of the CGL frameworks when each task is assigned with shorter time intervals (G.2). We study the effectiveness of the Node Fidelity Preservation (G.4), and the effects of different important node selection strategies (G.5). We also evaluate $\\mathbf{TAC}\\mathbb{O}$ on traditional class-incremental learning (G.6) to prove its generalizability. We present those results in Appendix G. ", "page_idx": 8}, {"type": "image", "img_path": "VpINEEVLX0/tmp/eeebaa036a76cbdffefae10665cd3f4f62a7bcda0a42d9ae6953c1651137c231.jpg", "img_caption": ["Figure 4: F1 score on the test set (x-axis) after training on each task (y-axis) on Kindle dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Regularization, Expansion, and Rehearsal are common approaches to overcome the catastrophic forgetting problem [43] in continual learning. Regularization methods [20, 4, 52, 10] penalize parameter changes that are considered important for previous tasks. Although this approach is efficient in space and computational resources, it suffers from the \u201cbrittleness\u201d problem [43, 35] where the previously regularized parameters may become obsolete and unable to adapt to new tasks. Expansion-based methods [40, 51, 16, 23] assign isolated model parameters to different tasks and increase the model size when new tasks arrive. Such approaches are inherently expensive, especially when the number of tasks is large. Rehearsal-based methods consolidate old knowledge by replaying the model with past experiences. A common way is using a small episodic memory of previous data or generated samples from a generative model when it is trained on new tasks. While generative methods [1, 2, 8, 34] may use less space, they also struggle with catastrophic forgetting problems and over-complicated designs [38]. Experience replay-based methods [39, 5, 6, 43, 29], on the other hand, have a more concise and straightforward workflow with remarkable performance demonstrated by various implementations with a small additional working memory. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Continual graph learning Most existing CGL methods adapt regularization, expansion, or rehearsal methods on graphs. For instance, [26] address catastrophic forgetting by penalizing the parameters that are crucial to both task-related objectives and topology-related ones. [41] mitigate the impact of the structure shift by minimizing the input distribution shift of nodes. Rehearsal-based methods [18, 57, 48] keep a memory buffer to store old samples, which treat replayed samples as independent data points and fail to preserve their structural information. [55] preserve the topology information by storing a sparsified $L$ -hop neighborhood of replay nodes. However, storing topology information of nodes through this method is not very efficient and the information of uncovered nodes is completely lost; also it fails to capture inter-task correlations in our setup. Besides, [12] present an approach to address both the heterophily propagation issue and forgetting problem with a triad structure replay strategy: it regularizes the distance between the nodes in selected closed triads and open triads, which is hard to be categorized into any of the two approaches. [27] uses a condensed graph as the memory buffer and employs a \"Training in Memory\" scheme that directly learns on the memory buffer to alleviate the data imbalance problem. However, the condensed graph contains only sampled nodes with self-loops, resulting in the loss of valuable topology information. [33] stores the learned representations of the sampled nodes as a memory store, which can preserve graph structure information. However, it still cannot handle inter-task edges, and the stored representations are inadequate for dealing with the dynamic receptive field of old nodes when new nodes form connections with them. Some CGL work [47, 50] focuses on graph-level classification where each sample (a graph) is independent of other samples, whereas our paper mainly tackles the problem of node classification where the interdependence of samples plays a pivotal role. ", "page_idx": 9}, {"type": "text", "text": "To better highlight the advantages of our proposed method, we provide a comparison between our method and these experience-based methods in Table 3. Our method, $\\mathbf{TAC}\\mathbb{O}$ , is the only one that can preserve graph topology, handle inter-task edges, and consider the growing receptive field of old nodes. ", "page_idx": 9}, {"type": "table", "img_path": "VpINEEVLX0/tmp/08a76590b10d83f8707545753711edf11dfe67a6ee83d82c2cc62654d7a5d4ce.jpg", "table_caption": ["Table 3: Comparision between experience replay-based CGL methods. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a novel CGL framework, $\\mathbf{TAC}\\mathbb{O}$ , which stores useful information from previous tasks with a dynamically reduced graph to consolidate learned knowledge. Additionally, we propose an efficient embedding proximity-based graph coarsening method, RePro, that can preserve important graph properties. We present a Node Fidelity Preservation strategy and theoretically prove its capability in preventing the vanishing minority class problem. We demonstrate the effectiveness of $\\mathbf{TAC}\\mathbb{O}$ and RePro on real-world datasets with a realistic streaming graph setup. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their helpful comments. This work is supported in part by the US National Science Foundation under grants 2047843, 2437621, and 2212370. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alessandro Achille, Tom Eccles, Lo\u00efc Matthey, Christopher P. Burgess, Nick Watters, Alexander Lerchner, and Irina Higgins. Life-long disentangled representation learning with cross-domain latent homologies. CoRR, abs/1808.06508, 2018.   \n[2] Lucas Caccia, Eugene Belilovsky, Massimo Caccia, and Joelle Pineau. Online learned continual compression with stacked quantization module. CoRR, abs/1911.08019, 2019.   \n[3] Antonio Carta, Andrea Cossu, Federico Errica, and Davide Bacciu. Catastrophic forgetting in deep graph networks: an introductory benchmark for graph classification. CoRR, abs/2103.11750, 2021.   \n[4] Arslan Chaudhry, Puneet Kumar Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. CoRR, abs/1801.10112, 2018. [5] Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with A-GEM. CoRR, abs/1812.00420, 2018.   \n[6] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet Kumar Dokania, Philip H. S. Torr, and Marc\u2019Aurelio Ranzato. Continual learning with tiny episodic memories. CoRR, abs/1902.10486, 2019.   \n[7] Jie Chen and Ilya Safro. Algebraic distance on graphs. SIAM Journal on Scientific Computing, 33(6):3468\u20133490, 2011. [8] Kamil Deja, Pawe\u0142 Wawrzy\u00b4nski, Daniel Marczak, Wojciech Masarczyk, and Tomasz Trzcin\u00b4ski. Binplay: A binary latent autoencoder for generative replay continual learning. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2021.   \n[9] Chenhui Deng, Zhiqiang Zhao, Yongyu Wang, Zhiru Zhang, and Zhuo Feng. Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding. CoRR, abs/1910.02370, 2019.   \n[10] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertaintyguided continual learning with bayesian neural networks. CoRR, abs/1906.02425, 2019.   \n[11] Matthew Fahrbach, Gramoz Goranci, Richard Peng, Sushant Sachdeva, and Chi Wang. Faster graph embeddings via coarsening. CoRR, abs/2007.02817, 2020.   \n[12] Kaituo Feng, Changsheng Li, Xiaolu Zhang, and Jun Zhou. Towards open temporal graph neural networks, 2023.   \n[13] Margherita Grandini, Enrico Bagli, and Giorgio Visani. Metrics for multi-class classification: an overview, 2020.   \n[14] Ruining He and Julian J. McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. CoRR, abs/1602.01585, 2016.   \n[15] Zengfeng Huang, Shengzhong Zhang, Chong Xi, Tang Liu, and Min Zhou. Scaling up graph neural networks via graph coarsening. CoRR, abs/2106.05150, 2021.   \n[16] Ghassen Jerfel, Erin Grant, Thomas L. Grifftihs, and Katherine A. Heller. Online gradient-based mixtures for transfer modulation in meta-learning. CoRR, abs/1812.06080, 2018.   \n[17] Yu Jin, Andreas Loukas, and Joseph JaJa. Graph coarsening with preserved spectral properties. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 4452\u20134462. PMLR, 26\u201328 Aug 2020.   \n[18] Seoyoon Kim, Seongjun Yun, and Jaewoo Kang. Dygrain: An incremental learning framework for dynamic graphs. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 3157\u20133163. International Joint Conferences on Artificial Intelligence Organization, 7 2022. Main Track.   \n[19] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. CoRR, abs/1609.02907, 2016.   \n[20] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, mar 2017.   \n[21] Manoj Kumar, Anurag Sharma, and Sandeep Kumar. A unified framework for optimizationbased graph coarsening, 2022.   \n[22] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3734\u2013 3743. PMLR, 09\u201315 Jun 2019.   \n[23] Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model for task-free continual learning. CoRR, abs/2001.00689, 2020.   \n[24] Jiongqian Liang, Saket Gurukar, and Srinivasan Parthasarathy. MILE: A multi-level framework for scalable graph embedding. CoRR, abs/1802.09612, 2018.   \n[25] Chuang Liu, Yibing Zhan, Jia Wu, Chang Li, Bo Du, Wenbin Hu, Tongliang Liu, and Dacheng Tao. Graph pooling for graph neural networks: Progress, challenges, and opportunities, 2023.   \n[26] Huihui Liu, Yiding Yang, and Xinchao Wang. Overcoming catastrophic forgetting in graph neural networks. CoRR, abs/2012.06002, 2020.   \n[27] Yilun Liu, Ruihong Qiu, and Zi Huang. Cat: Balanced continual graph learning with graph condensation, 2023.   \n[28] Oren E. Livne and Achi Brandt. Lean algebraic multigrid (lamg): Fast graph laplacian linear solver. SIAM Journal on Scientific Computing, 34(4):B499\u2013B522, 2012.   \n[29] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continuum learning. CoRR, abs/1706.08840, 2017.   \n[30] Andreas Loukas. Graph reduction with spectral and cut guarantees. CoRR, abs/1808.10650, 2018.   \n[31] Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. Image-based recommendations on styles and substitutes. CoRR, abs/1506.04757, 2015.   \n[32] Fei Mi, Lingjing Kong, Tao Lin, Kaicheng Yu, and Boi Faltings. Generalized class incremental learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 970\u2013974, 2020.   \n[33] Chaoxi Niu, Guansong Pang, and Ling Chen. Graph continual learning with debiased lossless memory replay, 2024.   \n[34] Oleksiy Ostapenko, Mihai Marian Puscas, Tassilo Klein, Patrick J\u00e4hnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. CoRR, abs/1904.03137, 2019.   \n[35] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard Turner, and Mohammad Emtiyaz E Khan. Continual deep learning by functional regularisation of memorable past. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 4453\u20134464. Curran Associates, Inc., 2020.   \n[36] Ying Pan, De-Hua Li, Jian-Guo Liu, and Jing-Zhang Liang. Detecting community structure in complex networks via node similarity. Physica A: Statistical Mechanics and its Applications, 389(14):2849\u20132857, 2010.   \n[37] Yunsheng Pang, Yunxiang Zhao, and Dongsheng Li. Graph pooling via coarsened graph infomax. CoRR, abs/2105.01275, 2021.   \n[38] German Ignacio Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. CoRR, abs/1802.07569, 2018.   \n[39] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[40] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016.   \n[41] Junwei Su and Chuan Wu. Towards robust inductive graph incremental learning via experience replay, 2023.   \n[42] Lichao Sun, Yingtong Dou, Carl Yang, Kai Zhang, Ji Wang, Philip S. Yu, Lifang He, and Bo Li. Adversarial attack and defense on graph data: A survey. IEEE Transactions on Knowledge and Data Engineering, page 1\u201320, 2022.   \n[43] Binh Tang and David S. Matteson. Graph-based continual learning, 2020.   \n[44] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: Extraction and mining of academic social networks. In KDD\u201908, pages 990\u2013998, 2008.   \n[45] Gido M. van de Ven and Andreas S. Tolias. Three scenarios for continual learning. CoRR, abs/1904.07734, 2019.   \n[46] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks, 2017.   \n[47] Yuening Wang, Yingxue Zhang, and Mark Coates. Graph structure aware contrastive knowledge distillation for incremental learning in recommender systems. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management, CIKM \u201921, page 3518\u20133522, New York, NY, USA, 2021. Association for Computing Machinery.   \n[48] Di Wei, Yu Gu, Yumeng Song, Zhen Song, Fangfang Li, and Ge Yu. Incregnn: Incremental graph neural network learning by considering node and parameter importance. In Database Systems for Advanced Applications: 27th International Conference, DASFAA 2022, Virtual Event, April 11\u201314, 2022, Proceedings, Part I, page 739\u2013746, Berlin, Heidelberg, 2022. Springer-Verlag.   \n[49] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?, 2019.   \n[50] Yishi Xu, Yingxue Zhang, Wei Guo, Huifeng Guo, Ruiming Tang, and Mark Coates. Graphsail: Graph structure aware incremental learning for recommender systems. CoRR, abs/2008.13517, 2020.   \n[51] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. CoRR, abs/1708.01547, 2017.   \n[52] Friedemann Zenke, Ben Poole, and Surya Ganguli. Improved multitask learning through synaptic intelligence. CoRR, abs/1703.04200, 2017.   \n[53] Xikun Zhang, Dongjin Song, Yixin Chen, and Dacheng Tao. Topology-aware embedding memory for continual learning on expanding networks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, volume 33 of KDD \u201924, page 4326\u20134337. ACM, August 2024.   \n[54] Xikun Zhang, Dongjin Song, and Dacheng Tao. CGLB: Benchmark tasks for continual graph learning. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[55] Xikun Zhang, Dongjin Song, and Dacheng Tao. Sparsified subgraph memory for continual graph representation learning. In 2022 IEEE International Conference on Data Mining (ICDM), pages 1335\u20131340, 2022.   \n[56] Xikun Zhang, Dongjin Song, and Dacheng Tao. Ricci curvature-based graph sparsification for continual graph representation learning. IEEE Transactions on Neural Networks and Learning Systems, pages 1\u201313, 2023.   \n[57] Fan Zhou, Chengtai Cao, Ting Zhong, Kunpeng Zhang, Goce Trajcevski, and Ji Geng. Overcoming catastrophic forgetting in graph neural networks with experience replay. CoRR, abs/2003.09908, 2020.   \n[58] Daniel Z\u00fcgner, Amir Akbarnejad, and Stephan G\u00fcnnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery, KDD \u201918. ACM, July 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work only considers situations where nodes and edges are added to streaming graphs with a single relation type. In the future, we plan to investigate CGL methods when nodes and edges can be deleted or modified. Moreover, we will generalize our method to complex graphs such as multi-relation graphs and broader graph tasks such as link prediction for recommendation systems. ", "page_idx": 14}, {"type": "text", "text": "B Broader impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "By alleviating the forgetting problem in graph continual learning, the proposed $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ model retains old knowledge while integrating new information. This is crucial for maintaining accurate and reliable models in critical domains with constantly evolving data, such as healthcare, biomedical informatics, and knowledge graphs. ", "page_idx": 14}, {"type": "text", "text": "C Related work on Graph Coarsening ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Scalability is a major concern in graph learning. Extensive studies aim to reduce the number of nodes in a graph, such that the coarsened graph approximates the original graph [17, 30, 7, 28]. In recent years, graph coarsening techniques are also applied for scalable graph representation learning [24, 9, 11, 15] and graph pooling [37, 25, 22]. Most graph coarsening methods [17, 30, 7, 28] aim to preserve certain spectral properties of graphs by merging nodes with high spectral similarity. However, such approaches usually result in high computational complexity especially when a graph needs to be repetitively reduced. Also, the aforementioned methods rely solely on graph structures but ignore node features. [21] propose to preserve both spectral properties and node features. However, it models the two objectives as separate optimization terms, thus the efficiency problem from the spectral-based methods remains. ", "page_idx": 14}, {"type": "text", "text": "D Supplemental Methodology ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Overall framework ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the proposed framework, during the training process, we use a reduced graph $\\mathcal G^{r}$ as an approximate representation of previous graphs, and a function $\\mathcal{M}(\\cdot)$ (e.g., a hash table) to map each original node to its assigned cluster (super-node) in $\\mathcal{G}^{r}$ (e.g., if an original node $i$ is assigned to cluster $j$ , then $\\mathcal{M}(i)=\\bar{j})$ . Both $\\mathcal{G}_{t}^{r}$ and $\\mathcal{M}_{t}(\\cdot)$ are updated after the $t$ -th task. ", "page_idx": 14}, {"type": "text", "text": "To get started we initialize $\\mathcal{G}_{0}^{r}$ as an empty undirected graph and $\\mathcal{M}_{\\mathrm{0}}$ as an empty hash table. At task $T_{t}$ , the model holds copies of $\\mathscr{G}_{t-1}^{r},\\mathscr{M}_{t-1}$ and an original graph $\\mathcal{G}_{t}$ for the current task. ", "page_idx": 14}, {"type": "text", "text": "We combine $\\mathcal{G}_{t}$ with $\\mathcal{G}_{t-1}^{r}$ to form a new graph $\\mathcal{G}_{t}^{c}$ according to the following procedure: ", "page_idx": 14}, {"type": "text", "text": "1. Initialize the combined graph $\\mathcal{G}_{t}^{c}=(A_{t}^{c},X_{t}^{c},Y_{t}^{c})$ as $\\mathcal{G}_{t-1}^{r}$ such that $A_{t}^{c}=A_{t-1}^{r}$ , $X_{t}^{c}=X_{t-1}^{r}$ , and $Y_{t}^{c}=Y_{t-1}^{r}$ ;   \n2. Case $^{\\,I}$ : new source node and new target node. For each edge $(s,o)\\in\\mathcal{E}_{t}$ , if $\\tau(s)=\\tau(o)=t$ , we add $s$ and $o$ to $\\mathcal{G}_{t}^{c}$ , and we add an undirected edge (s,o) to $\\mathcal{G}_{t}^{c}$ ;   \n3. Case 2: new source node and old target node. If $\\tau(s)=t$ , but $\\tau(o)<t$ , and $o\\in\\mathcal{M}_{t-1}$ , we add $s$ to $\\mathcal{G}_{t}^{c}$ , and we add an undirected edge $(s,\\mathcal{M}_{t-1}(o))$ to $\\mathcal{G}_{t}^{c}$ ;   \n4. Case 3: deleted target node. If $\\tau(o)<t$ and $o\\not\\in\\mathcal{M}_{t-1}$ , we ignore the edge.   \n5. When a new node $v$ is added to $\\mathcal{G}_{t-1}^{c}$ , it is also added to $\\mathcal{M}_{t-1}$ and is assigned to a new cluster. ", "page_idx": 14}, {"type": "text", "text": "It is worth noting that we use directed edges to better explain the above procedure. In our implementation, the combined graph is undirected since the directness of the edges of the combined graph is not critical in learning node embeddings in a graph like a citation network. ", "page_idx": 14}, {"type": "text", "text": "D.2 Pseudocode ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The pseudocode of the proposed $\\mathbf{TAC}\\mathbb{O}$ is described in Algorithm 1 and the graph coarsening process RePro is described in Algorithm 2 ", "page_idx": 15}, {"type": "table", "img_path": "VpINEEVLX0/tmp/f47e760b28e9d3202626e4b7a54da58359e200a7b6a06195cd9733170dbbe2d2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D.3 Node Fidelity Preservation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 4.1. Consider n nodes with c classes, such that the class distribution of all nodes is represented by $\\mathbf{p}=p_{1},p_{2},...,p_{c},$ , where $\\begin{array}{r}{\\sum_{i=1}^{c}p_{i}=1.}\\end{array}$ . If these nodes are randomly partitioned into $n^{\\prime}$ clusters such that $n^{\\prime}=\\lfloor\\gamma\\cdot n\\rfloor$ , $0<\\gamma<1$ and the class label for each cluster is determined via majority voting. The class distribution of all the clusters is $\\mathbf{p}^{\\prime}=p_{1}^{\\prime},p_{2}^{\\prime},...,p_{c}^{\\prime}$ where $p_{i}^{\\prime}$ is computed as the ratio of clusters labeled as class $i$ and $\\textstyle\\sum_{i=1}^{c}p_{i}^{\\prime}=1$ . Let $k$ be one of the classes, and the rest of the class are balanced $p_{1}=...=p_{k-1}=p_{k+1}=...=p_{c}$ . It holds that: ", "page_idx": 15}, {"type": "text", "text": "1. If $p_{k}=1/c$ and all classes are balanced $p_{1}=p_{2}=...=p_{c},$ , then $\\mathbb{E}[p_{k}^{\\prime}]=p_{k}$ . ", "page_idx": 15}, {"type": "text", "text": "2. When $p_{k}<1/c,\\mathbb{E}[p_{k}^{\\prime}]<p_{k},$ , and $\\mathbb{E}\\big[\\frac{p_{k}^{\\prime}}{p_{k}}\\big]$ decreases as $n^{\\prime}$ decreases. There exists a $p^{m i n}$ such that $0<p^{m i n}<1$ , and when $p_{k}<p^{m i n}$ , $\\mathbb{E}\\big[\\frac{p_{k}^{\\prime}}{p_{k}}\\big]$ decrease as $p_{k}$ decreases. ", "page_idx": 15}, {"type": "text", "text": "Proof. We prove the theorem by deriving the value of $\\mathbb{E}[p_{k}^{\\prime}]$ . Since $\\mathbb{E}[p_{k}^{\\prime}]$ is invariant to the order of the classes, for convenience, we consider $i=1$ without losing generability. The probability of the first class after the partitioning is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[p_{1}^{\\prime}]=\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n-n^{\\prime}}\\mathbb{E}[n_{a}]q(a,\\mathbf{p}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "1: Input: The original graph $\\mathcal{G}$ , node embedding matrix $H$ , node sets $V^{r b}$ , and the reduction rate $\\gamma$   \n2: Output: partition matrix $Q$ , normalized partition matrix $P$ , and the mapping function $\\mathcal{P}(\\cdot)$   \n3: Initialize the mapping function $\\mathcal P(\\cdot)$ such that $\\mathcal{P}(v)=v$ for $v\\in\\mathcal{V}$   \n4: $n^{r}\\leftarrow|\\nu|$   \n5: $n^{\\mathrm{target}}\\leftarrow\\lfloor r\\cdot\\vert\\mathcal{V}\\vert\\rfloor$   \n6: Sort all edges $e\\in\\mathcal{E}$ in the descending order based on their similarity scores calculated according   \nto Eq. 2   \n7: for each edge $e=(u,v)\\in\\mathcal{E}$ do   \n8: if $\\mathcal{P}(u)\\neq\\mathcal{P}(v)$ then   \n9: $//$ if $u$ and $v$ are in different clusters   \n10: Merge the clusters of $u$ and $v$ such that $\\mathcal{P}(u)=\\mathcal{P}(v)$   \n11: $n^{r}=n^{r}-1$   \n12: end if   \n13: if $n^{r}\\leq n^{\\mathrm{target}}\\,.$ then   \n14: Break   \n15: end if   \n16: end for   \n17: Construct Q with P(\u00b7); compute P based on pt,(ij) =  v\u2208sCi sv .   \n18: Return $Q,P,\\mathcal{P}(\\cdot)$ ", "page_idx": 16}, {"type": "text", "text": "where $\\mathbb{E}[n_{a}]$ is the expectation of the number of clusters containing $a$ nodes, and $q(a,\\mathbf{p})$ is the probability that class 1 is the majority class in a cluster with size $a$ . ", "page_idx": 16}, {"type": "text", "text": "Property D.1. The expectation of the number of clusters containing a node is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[n_{a}]=n^{\\prime}\\times{\\binom{n-n^{\\prime}+1}{a-1}}\\left({\\frac{1}{n^{\\prime}}}\\right)^{a-1}\\left(1-{\\frac{1}{n^{\\prime}}}\\right)^{n-n^{\\prime}-(a-1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Define $I_{j}$ to be the indicator random variable for the $j^{t h}$ cluster, such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nI_{j}={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~the~}}j^{t h}{\\mathrm{~cluster~contains~exactly~}}a{\\mathrm{~samples}},}\\\\ {0}&{{\\mathrm{otherwise}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We first compute the expected value of $I_{j}$ for a single cluster. Let\u2019s calculate the probability that $a-1$ out of the $n-n^{\\prime}$ samples are allocated to the $j^{t h}$ cluster: ", "page_idx": 16}, {"type": "text", "text": "(a) There are $\\scriptstyle{\\binom{n-n^{\\prime}}{a-1}}$ ways to choose $a-1$ samples from the $n-n^{\\prime}$ remaining samples. ", "page_idx": 16}, {"type": "text", "text": "(b) The probability that each of these $a-1$ samples is placed in the $j^{t h}$ cluster is $\\left({\\frac{1}{n^{\\prime}}}\\right)^{a-1}$ ", "page_idx": 16}, {"type": "text", "text": "(c) The remaining $n-n^{\\prime}-a+1$ samples from our original pool of $n-n^{\\prime}$ should not be allocated to the $j^{t h}$ cluster. The probability that all of them avoid this cluster is $\\textstyle\\left(1-{\\frac{1}{n^{\\prime}}}\\right)^{n-n^{\\prime}-(a-1)}$ ", "page_idx": 16}, {"type": "text", "text": "Thus, the probability that the $j^{t h}$ cluster contains exactly $x$ samples is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[I_{j}]={\\binom{n-n^{\\prime}}{a-1}}\\left({\\frac{1}{n^{\\prime}}}\\right)^{a-1}\\left(1-{\\frac{1}{n^{\\prime}}}\\right)^{n-n^{\\prime}-(a-1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The expected number of clusters out of all $n^{\\prime}$ clusters that contain exactly $a$ samples is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[n_{a}]=\\sum_{j=1}^{n^{\\prime}}\\mathbb{E}[I_{j}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given that each $I_{j}$ is identically distributed, we can simplify the sum: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[n_{a}]=n^{\\prime}\\times\\mathbb{E}[I_{j}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substituting the expression derived for $\\mathbb{E}[I_{j}]$ from step 2, we obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[n_{a}]=n^{\\prime}\\times{\\binom{n-n^{\\prime}}{a-1}}\\left({\\frac{1}{n^{\\prime}}}\\right)^{a-1}\\left(1-{\\frac{1}{n^{\\prime}}}\\right)^{n-n^{\\prime}-(a-1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is easy to show that when $p_{1}\\,=\\,p_{2}\\,=\\,...\\,=\\,p_{c}$ , it holds that $\\textstyle q(a,\\mathbf{p})={\\frac{1}{c}}$ since all classes are equivalent and have equal chances to be the majority class. Thus: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[p_{1}^{\\prime}]=\\displaystyle\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n^{\\prime}}\\mathbb{E}[n_{a}]q(a,\\mathbf{p})}\\\\ &{\\qquad=\\displaystyle\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n^{\\prime}}\\mathbb{E}[n_{a}]\\frac{1}{c}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n^{\\prime}}n^{\\prime}\\times\\left(\\frac{n^{\\prime}}{a-1}\\right)\\left(\\frac{1}{n^{\\prime}}\\right)^{a-1}\\left(1-\\frac{1}{n^{\\prime}}\\right)^{n-n^{\\prime}-(a-1)}\\frac{1}{c}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{n^{\\prime}}n^{\\prime}\\frac{1}{c}}\\\\ &{\\qquad=\\frac{1}{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To compute $q(a,\\mathbf{p})$ , we need to consider the situations in which class 1 is the exclusive majority class and the cases where class 1 ties with one or more other classes for having the most samples. In the second case, we roll a die to select the majority class from all the classes that have most samples. ", "page_idx": 17}, {"type": "text", "text": "To start, we consider the situation when class 1 has the most nodes and no other classes tie with it.   \nWe enumerate all possible combinations of class assignments and calculate the probability of each. ", "page_idx": 17}, {"type": "equation", "text": "$$\nq_{0}(a,{\\mathbf{p}})=\\sum_{i_{1}=1}^{a}\\sum_{i_{2}=0}^{i_{1}-1}\\cdots\\sum_{i_{c}=0}^{i_{1}-1}\\mathbb{1}\\{\\sum_{k=1}^{c}i_{k}=a\\}f(\\mathbf{i},a,{\\mathbf{p}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf i=i_{1},i_{2}...i_{c}$ , and ", "page_idx": 17}, {"type": "equation", "text": "$$\nf({\\bf i},a,{\\bf p})=\\prod_{k=1}^{c}{\\binom{a-i_{1}\\ldots-i_{k}}{i_{k}}}p_{k}^{i_{k}}(1-p_{k})^{a-i_{1}-\\ldots-i_{k}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{k}={\\binom{p_{1}}{\\frac{1-p_{1}}{c-1}}}{\\begin{array}{l l}{{\\mathrm{if}}\\;k=1}\\\\ {{\\mathrm{otherwise}}}\\end{array}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We then consider the situation when class 1 ties with another class. We first select another class that ties in with class 1, and we enumerate the possibility of other classes. We multiply the whole term by $\\frac{1}{2}$ as there is an equal chance to select either class as the majority class. ", "page_idx": 17}, {"type": "equation", "text": "$$\nq_{1}(a,{\\bf p})=\\frac{1}{2}\\sum_{j_{1}=2}^{c}\\left(\\sum_{i_{1}=1}^{a}\\sum_{i_{2}=0}^{i_{1}-1}\\cdots\\sum_{i_{j_{1}}=i_{1}}^{i_{1}}\\cdots\\sum_{i_{c}=0}^{i_{1}-1}\\mathbb{1}\\{\\sum_{k=1}^{c}i_{k}=a\\}f({\\bf i},a,{\\bf p})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We extend the above equation to a general case where class 1 ties with $\\boldsymbol{\\mathrm{k}}$ classes $(1\\leq k\\leq c-1)$ . Here we need to select $\\boldsymbol{\\mathrm{k}}$ classes that tie with class 1. Class 1 now shares a $\\textstyle{\\frac{1}{k+1}}$ chance to be selected as the majority class with the selected $k$ classes. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle q_{k}(a,\\mathbf{p})=\\frac{1}{k}\\sum_{j_{1}=2}^{c-k+1}\\sum_{j_{2}=j_{1}}^{c-k+2}\\cdots\\sum_{j_{k}=j_{k-1}}^{c}}\\\\ {\\displaystyle\\cdot\\left(\\sum_{i_{1}=1}^{a}\\sum_{i_{2}=0}^{i_{1}-1}\\cdots\\sum_{i_{j_{1}}=i_{1}}^{i_{1}}\\cdots\\sum_{i_{j_{k}}=i_{1}}^{i_{1}}\\cdots\\sum_{i_{c}=0}^{i_{1}-1}\\mathbb{1}\\{\\sum_{k=1}^{c}i_{k}=a\\}f(\\mathbf{i},a,\\mathbf{p})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we combine all the cases, and the probability that class 1 is the majority class is: ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(a,{\\mathbf p})=\\sum_{k=0}^{c-1}q_{k}(a,{\\mathbf p}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The expectation of $\\frac{p_{1}^{\\prime}}{p_{1}}$ is thus: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\frac{p_{1}^{\\prime}}{p_{1}}]=\\frac{1}{p_{1}}\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n^{\\prime}}\\mathbb{E}[n_{a}]q(a,\\mathbf{p}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To study the behavior of $\\mathbb{E}\\big[\\frac{p_{1}^{\\prime}}{p_{1}}\\big]$ when $p_{1}$ changes, we derive the following derivative: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{d\\mathbb{E}[\\frac{p_{1}^{\\prime}}{p_{1}}]}{d p_{1}}=\\frac{d\\frac{1}{p_{1}}\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n^{\\prime}}\\mathbb{E}[n_{a}]q(a,{\\bf p})}{d p_{1}}}}\\\\ &{}&{=\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n^{\\prime}}\\mathbb{E}[n_{a}]\\frac{d\\frac{q(a,{\\bf p})}{p_{1}}}{d p_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{l_{\\frac{d}{p_{1}}}^{q(a,\\mathbf{p})}}{d p_{1}}=\\displaystyle\\sum_{k=0}^{c-1}\\frac{d\\frac{q_{k}^{\\prime}(a,p)}{p_{1}}}{d p_{1}}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{2}\\sum_{j_{1}=2}^{c-k+1}\\sum_{j_{2}=j_{1}}^{c-k+2}\\cdots\\sum_{j_{k}=j_{k-1}}^{c}\\left(\\sum_{i_{1}=1}^{a}\\sum_{i_{2}=0}^{i_{1}-1}\\cdots\\sum_{i_{j_{1}}=i_{1}}^{i_{1}}\\cdots\\sum_{i_{j_{k}}=i_{1}}^{i_{1}}\\cdots\\sum_{i_{c}=0}^{i_{1}-1}\\mathbb{1}\\{\\sum_{k=1}^{c}i_{k}=a\\}\\frac{d\\frac{f(\\mathbf{i},a,\\mathbf{p})}{p_{1}}}{d p_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To find $\\frac{d{\\frac{f({\\bf i},a,{\\bf p})}{p_{1}}}}{d p_{1}}$ , we first separate the terms that are independent of $p_{1}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{f\\left(\\mathbf{I}_{n},\\mathbf{p}\\right)}{p_{1}}=\\frac{1}{p_{1}}\\frac{\\hat{\\mathbf{I}}}{\\displaystyle{\\prod_{i=1}^{n}\\left(1-\\hat{\\mathbf{I}}_{n}-\\mathbf{I}_{n}\\right)}p_{i}^{\\alpha_{i}}}(1-p_{1})^{n-1-n-\\alpha_{i}-n}}\\\\ {=\\frac{1}{p_{1}}\\left(\\frac{\\hat{\\mathbf{I}}}{\\displaystyle{\\prod_{i=1}^{n}\\left(\\mathbf{I}_{n}-\\mathbf{I}_{n}\\right)}}\\right)\\frac{\\hat{\\mathbf{I}}}{\\displaystyle{\\prod_{i=1}^{n}\\mathbf{I}}}P_{i}^{\\alpha_{i}(1-p_{1})-n-\\alpha_{i}-n}}\\\\ {=\\left(\\prod_{i=1}^{n}\\left(\\binom{n-1}{i-n}\\right)\\right)p_{i}^{\\alpha_{i}-1}(1-p_{1})^{n-\\alpha_{i}}}\\\\ {\\times\\frac{(1-p_{1})\\sum_{i=1}^{n}\\alpha_{i}!}{(\\alpha-1)!}(1-p_{1})^{\\alpha_{i}-1}}\\\\ {=u\\times\\gamma_{i}^{\\alpha_{i}-1}(1-p_{1})^{\\alpha_{i}-1}(1-p_{1})^{\\alpha_{i}-1}}\\\\ {=u\\times\\gamma_{i}^{\\alpha_{i}-1}\\sum_{1=1}^{n}\\left(\\underbrace{\\mathbf{I}-\\mathbf{I}_{n}}_{\\alpha}\\right)\\sum_{i=1}^{n-1}\\alpha_{i}(p_{1}+c_{2})^{\\sum_{i=1}^{n}\\alpha_{i}-1-n}}\\\\ {=u\\times\\gamma_{i}^{\\alpha_{i}-1}\\times(1-p_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $u$ is independent of $p_{1}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nu=\\left(\\prod_{k=1}^{c}\\left({a-i_{1}}...-i_{k}\\right)\\right)({\\frac{1}{c-1}})^{\\sum_{k=2}^{c}i_{k}+\\sum_{k=2}^{c}a-i_{2}...-i_{k}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We observe that $\\frac{f({\\bf i},\\!a,\\!{\\bf p})}{p_{1}}$ demonstrates different behaviors for $a=1$ and $a>1$ and discuss the two cases separately. ", "page_idx": 19}, {"type": "text", "text": "(1) When $a=1$ , it holds that $i_{1}=1,\\theta=0,\\phi=0$ , and $\\psi=0$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{f(\\mathbf{i},a,\\mathbf{p})}{p_{1}}=u\\times p_{1}^{0}(1-p_{1})^{0}(p_{1}+c-2)^{0}=u.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In such case, $\\frac{d{\\frac{f(\\mathbf{i},a,\\mathbf{p})}{p_{1}}}}{d p_{1}}$ is independent with $p_{1}$ and remain constant when $p_{1}$ changes. ", "page_idx": 19}, {"type": "text", "text": "(2) When $a>1$ , it holds that $i_{1}\\leq1,\\theta\\geq0$ , $\\phi\\geq0$ , $\\psi\\geq0$ , and $\\theta+\\phi>0$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d\\frac{f(\\mathbf{i},a,\\mathbf{p})}{p_{1}}}{d p_{1}}={u}\\cdot{p^{\\theta-1}(1-p)^{\\phi-1}(p+c-2)^{\\psi-1}}\\cdot{v},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{v=\\theta(1-p)(p+c-2)+\\psi p(1-p)-\\phi p(p+c-2)}}\\\\ {{\\ \\ =(-\\theta-\\phi-\\psi)p^{2}+(\\theta+\\psi+(\\phi-\\theta)(c-2))p+\\theta(c-2).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When $0<p_{1}<1$ and $u>0$ , $\\begin{array}{r}{\\frac{d\\frac{f(\\mathbf{i},a,\\mathbf{p})}{p_{1}}}{d p_{1}}=0}\\end{array}$ if and only if $v=0$ , and the corresponding value of $p_{1}$ is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{1}^{0}=\\displaystyle\\frac{-\\left(\\theta-\\theta\\cdot(c-2)+\\phi-\\psi\\cdot n\\right)-\\sqrt\\Delta}{2\\left(-\\theta-\\phi-\\psi\\right)},}}\\\\ {{\\displaystyle}}\\\\ {{p_{1}^{1}=\\displaystyle\\frac{-\\left(\\theta-\\theta\\cdot(c-2)+\\phi-\\psi\\cdot n\\right)+\\sqrt\\Delta}{2\\left(-\\theta-\\phi-\\psi\\right)},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Delta=\\left(\\theta-\\theta\\cdot\\left(c-2\\right)+\\phi-\\psi\\cdot\\left(c-2\\right)\\right)^{2}-4\\left(-\\theta-\\phi-\\psi\\right)\\left(\\theta\\cdot\\left(c-2\\right)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is easy to show that $\\Delta>0$ , and since $(-\\theta-\\phi-\\psi)<0$ , $v$ is concave, $v>0$ when $p_{1}^{1}<p<p_{1}^{0}$ . Also, it is observed that when $p_{1}=0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nv=\\theta(c-2)\\geq0;\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "when $p_{1}=1$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nv=-\\phi(c-1)<0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus it must be held that $0<p_{1}^{0}<1$ and $p_{1}^{1}\\leq0$ , and for any $(\\mathbf{i},a>1)$ , there exist a $0<p_{1}(\\mathbf{i},a>$ $1)^{0}<1$ such that when $\\begin{array}{r}{p_{1}<p_{1}^{0}(\\mathbf{i},a>1),\\frac{d\\frac{f(\\mathbf{i},a,\\mathbf{p})}{p_{1}}}{d p_{1}}>0.}\\end{array}$ Let ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{1}^{\\mathrm{min}}=\\operatorname*{min}_{\\forall a\\in\\{2,\\ldots,n^{\\prime}\\},{\\bf i}\\in{\\bf I}}p_{1}^{0}({\\bf i},a),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{I}$ is all possible $_\\mathrm{i}$ , then it holds that $0<p_{1}^{\\mathrm{min}}<1$ , and when $p_{1}<p_{1}^{\\mathrm{min}}$ , $\\begin{array}{r}{\\frac{d\\frac{q(a,\\mathbf{p})}{p_{1}}}{d p_{1}}>0}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Next, we show that $\\mathbb{E}\\big[\\frac{p_{1}^{\\prime}}{p_{1}}\\big]$ decreases as $n^{\\prime}$ decreases when $p_{1}<1/c$ . We first rewrite $\\mathbb{E}\\big[\\frac{p_{1}^{\\prime}}{p_{1}}\\big]$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}[\\frac{p_{1}^{\\prime}}{p_{1}}]=\\!\\frac{1}{p_{1}}\\frac{1}{n^{\\prime}}\\sum_{a=1}^{n^{\\prime}}\\mathbb{E}[n_{a}]q(a,{\\bf p})}\\ ~}\\\\ {{\\displaystyle~~~~~~=\\frac{1}{p_{1}}\\sum_{a=1}^{n^{\\prime}}\\binom{n-n^{\\prime}}{a-1}\\left(\\frac{1}{n^{\\prime}}\\right)^{a-1}\\left(1-\\frac{1}{n^{\\prime}}\\right)^{n-n^{\\prime}-(a-1)}q(a,{\\bf p})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First, we show that $q(a,\\mathbf{p})$ is smaller for larger $a$ when $p_{1}<1/c$ . The intuition is that when a new node is added to a cluster originally with $a-1$ nodes, the new node has a higher probability of being another class, and the likelihood of class 1 becoming the majority class decreases. ", "page_idx": 20}, {"type": "text", "text": "Next, we show that when n\u2032 increases, na\u2212\u2212n1\u2032  n1\u2032 a\u22121  1 \u2212n1\u2032 n\u2212n\u2212(a\u22121) becomes more leftskewed, that it gives a smaller value for large $a$ . The intuition is that, as the value of $n^{\\prime}$ increases, the average cluster size is expected to decrease. As a result, a large $a$ becomes farther from the average cluster size, and the probability of a cluster having exactly $a$ nodes decreases, leading to a decrease in the ratio of clusters with size $a$ . ", "page_idx": 20}, {"type": "text", "text": "With the above observations, it can be concluded that when $p_{1}<1/c$ , the value of $\\mathbb{E}\\big[\\frac{p_{1}^{\\prime}}{p_{1}}\\big]$ decreases as $n^{\\prime}$ decreases. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Observation 4.1. Node Fidelity Preservation with buffer size b can alleviate the declination of $a$ minority class $k$ when $p_{k}$ decreases and $n^{\\prime}$ decreases, and prevent class $k$ from vanishing at small when $p_{k}$ is small. ", "page_idx": 20}, {"type": "text", "text": "The mechanism of Node Fidelity Preservation is to \u201chold\u201d $b$ clusters such that each cluster only has 1 node. We already show that when $a=1$ , $\\frac{q(a,\\mathbf{p})}{q_{1}}$ is independent of $q_{1}$ and so $\\mathbb{E}[p_{1}^{\\prime}]=p_{1}$ . By doing so, we make sure that among the $b$ nodes, class 1 does not decline as $p_{1}$ or $n^{\\prime}$ decline. ", "page_idx": 20}, {"type": "text", "text": "We demonstrate the effect of Node Fidelity Preservation with examples. We assign $n\\,=\\,1000$ , $c=2,3$ , and $b=\\lfloor n^{\\prime}/5\\rfloor$ . we plot change of $\\mathbb{E}\\big[\\frac{p_{1}^{\\prime}}{p_{1}}\\big]$ and $\\mathbb{E}[p_{1}^{\\prime}]$ at different $n^{\\prime}$ and $p_{1}$ separately in Figure 5. We draw the trend with Node Fidelity Preservation (NFP) using dash lines and without NFP using solid lines. From the figure, we observe that without Node Fidelity Preservation being applied, the ratio $\\mathbb{E}\\big[\\frac{p_{1}^{\\prime}}{p_{1}}\\big]$ approaches zero when $n^{\\prime}$ is small, resulting in a vanishing minority class. The application of Node Fidelity Preservation prevents the ratio from approaching zero and makes sure class 1 won\u2019t disappear when $p_{1}$ is small. ", "page_idx": 20}, {"type": "image", "img_path": "VpINEEVLX0/tmp/4ecf15e5b84b531aa745bb8599504a85107b3f9ba40062df60575b0d9a0bdfa6.jpg", "img_caption": ["Figure 5: $\\mathbb{E}[p_{1}^{\\prime}]$ and $\\mathbb{E}\\big[\\frac{p_{1}^{\\prime}}{p_{1}}\\big]$ against $p_{1}$ at different reduction rate $\\gamma$ for $c=2$ and $c=3$ . The dashed lines represent trends with Node Fidelity Preservation (NFP), and the solid lines represent trends without NFP. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.4 Node Representation Proximity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Spectral-based methods aim to preserve the spectral properties of a graph. Specifically, the Laplacian matrices of the original graph and the reduced graph are compared with each other [30]. The combinatorial Laplacian of a graph $\\mathcal{G}$ , $L\\in\\mathbb{R}^{n\\times n}$ is defined as $L=D-A$ , where $A\\in\\mathbb{R}^{n\\times n}$ is the adjacency matrix of $\\mathcal{G}$ , and $D\\in\\mathbb{R}^{n\\times n}$ is its diagonal degree matrix. The combinatorial Laplacian of the reduced graph, $L^{\\prime}\\in\\mathbb{R}^{n^{\\prime}\\times n^{\\prime}}$ , is calculated as $L^{\\prime}=P^{\\mp}L P^{+}$ , where $P^{+}$ is the pseudo inverse of the normalized coarsening matrix $P\\in\\mathbb{R}^{n\\times n^{\\prime}}$ , and $P^{\\mp}$ is the transposed pseudo inverse of $P$ . Since $L$ and $L^{\\prime}$ have different sizes, they can not be directly compared. Instead, the following induced semi-norms are defined: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|x\\|_{L}={\\sqrt{x^{\\top}L x}},\\;\\|x^{\\prime}\\|_{L^{\\prime}}={\\sqrt{x^{\\prime\\top}L^{\\prime}x^{\\prime}}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $x\\in\\mathbb{R}^{n}$ , and $x^{\\prime}$ is the projection of $x$ on $n^{\\prime}$ -space such that $x^{\\prime}=P x$ . The closeness between $L$ to $L^{\\prime}$ can be defined as how close $\\Vert x\\Vert_{L}$ is to $\\|\\boldsymbol{x}^{\\prime}\\|_{L^{\\prime}}$ . $L$ and $L^{\\prime}$ are considered equivalent if it holds $\\|x^{\\prime}\\|_{L^{\\prime}}=\\|x\\|_{L}$ for any $x\\in\\mathbb{R}^{n}$ . We next show that a partitioning made by merging nodes sharing the same neighborhood structure results in an equivalent Laplacian of the reduced graph as the original graph. ", "page_idx": 21}, {"type": "text", "text": "Theorem D.1. Let $i$ and $j$ be two nodes of $\\mathcal{G}$ , $A\\in\\mathbb{R}^{n\\times n}$ be the adjacency matrix of $\\mathcal{G}$ and $\\tilde{A}=A\\!+\\!I$ , $D$ be the diagonal matrix of $A$ and $\\tilde{D}=D+I$ , $P\\in\\mathbb{R}^{n\\times n-1}$ be the normalized coarsening matrix by merging $i$ and $j$ to one node, $L$ and $L^{\\prime}$ be the combinatorial Laplacian of $G$ and the reduced graph. It holds that $\\tilde{A}_{i}=\\tilde{A}_{j},\\tilde{D}_{i}=\\tilde{D}_{j}\\Rightarrow\\|x^{\\prime}\\|_{L^{\\prime}}=\\|x\\|_{L}$ for any $x\\in\\mathbb{R}^{n}$ and $x^{\\prime}=P x$ , where $A_{i}$ is the $i$ -th row for a matrix A. ", "page_idx": 21}, {"type": "text", "text": "Proof. Given that $\\tilde{A}_{i}=\\tilde{A}_{j}$ and $\\tilde{D}_{i}=\\tilde{D}_{j}$ , denote these common rows as $\\tilde{A}_{i j}$ and ${\\tilde{D}}_{i j}$ , respectively. The norm $\\|{\\boldsymbol{x}}\\|_{L}$ is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|x\\|_{L}=x^{T}L x,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $L=D-A$ is the combinatorial Laplacian of $\\mathcal{G}$ . ", "page_idx": 21}, {"type": "text", "text": "Similarly, for the coarsened graph, the norm $\\|\\boldsymbol{x}^{\\prime}\\|_{L^{\\prime}}$ is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|x^{\\prime}\\|_{L^{\\prime}}=(P x)^{T}L^{\\prime}(P x),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $L^{\\prime}$ is the combinatorial Laplacian of the coarsened graph. ", "page_idx": 21}, {"type": "text", "text": "We need to show that $\\|x\\|_{L}=\\|x^{\\prime}\\|_{L^{\\prime}}$ . ", "page_idx": 22}, {"type": "text", "text": "First, consider the left-hand side norm: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|x\\|_{L}=x^{T}L x=x^{T}(D-A)x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Given the structure of $\\tilde{A}$ and $\\tilde{D}$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|x\\|_{L}=\\sum_{k}\\tilde{D}_{k k}x_{k}^{2}-\\sum_{k,l}\\tilde{A}_{k l}x_{k}x_{l}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When merging nodes $i$ and $j$ , the coarsening matrix $P$ combines these two nodes into one. Thus, $x^{\\prime}=P x$ . ", "page_idx": 22}, {"type": "text", "text": "The matrix $P$ is such that it preserves the sum of entries corresponding to the merged nodes, maintaining the structure of the Laplacian. Therefore, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\nP^{T}P=I\\quad({\\mathrm{except~for~the~merged~nodes}}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The reduced Laplacian $L^{\\prime}$ after merging $i$ and $j$ incorporates the sum of the rows and columns corresponding to $i$ and $j$ in $\\tilde{A}$ and $\\tilde{D}$ . Since $\\tilde{A}_{i}\\;=\\;\\tilde{A}_{j}$ and $\\tilde{D}_{i}\\;=\\;\\tilde{D}_{j}$ , the rows and columns corresponding to $i$ and $j$ are identical, thus preserving the overall structure and norm. ", "page_idx": 22}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|x^{\\prime}\\|_{L^{\\prime}}=(P x)^{T}L^{\\prime}(P x)=x^{T}L x=\\|x\\|_{L}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.5 Proof of the size of the reduced graph ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. The number of nodes of the reduced graph at task $t$ is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{n=(1-\\gamma)(\\dots((1-\\gamma)n_{1}+n_{2})\\dots+n_{t})}}\\\\ &{\\le(1-\\gamma)(\\dots((1-\\gamma)n_{\\mathrm{MAX}}+n_{\\mathrm{MAX}})\\dots+n_{\\mathrm{MAX}})}\\\\ &{=((1-\\gamma)+(1-\\gamma)^{2}+\\dots+(1-\\gamma)^{t})n_{\\mathrm{MAX}}}\\\\ &{\\le\\cfrac{1-\\gamma}{\\gamma}n_{\\mathrm{MAX}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.6 Discussion on utilizing soft labels as an alternative to majority voting ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Another potential solution to address the \u201cvanishing class\u201d problem caused by the majority voting is to use soft labels to represent the cluster label instead. However, such an approach may come with several drawbacks. First, using hard labels is memory-efficient, and requires only a single digit to represent a sample\u2019s label. In contrast, soft labels use a vector for storage, with a size equivalent to the model\u2019s output dimension. This distinction in memory usage is negligible for models with few output classes. However, in scenarios where the model predicts among a large number of classes, the increased memory demand of soft labels becomes significant and cannot be overlooked. Second, although a model can learn to predict a soft label during the training phase, most applications necessitate deterministic predictions in the testing or inference phase. We are concerned that training with soft labels might lead the model towards indeterministic and ambiguous predictions, potentially undermining its practical applicability. The last concern is that when using soft labels, instead of a classification task, the model is performing a regression task. To predict single nodes with deterministic labels, it is considered a suboptimal approach to model it as a regression task due to the unaligned optimization objectives and loss functions. Also, regression is believed to be a less difficult task to learn compared to classification task as discrete optimization is generally harder than continuous optimization. Training the model with an easier task may restrict its performance during the test phase. ", "page_idx": 22}, {"type": "text", "text": "Table 4: Statistics of datasets. \u201cInterval\u201d indicates the length of the time interval for each task. \u201c# Item\u201d indicates the total number of items/papers in each dataset. ", "page_idx": 23}, {"type": "table", "img_path": "VpINEEVLX0/tmp/95d7005fb4d2da92601f418387cbb0d0e0d1fe045a9b66fe839bcc4eaa40182d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Supplemental experiment setups ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Details of the datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The Kindle dataset contains items from the Amazon Kindle store; each node representing an item is associated with a timestamp indicating its release date, and each edge indicates a \u201cfrequently co-purchased\u201d relation between items. The DBLP and ACM datasets are citation datasets where each node represents a paper associated with its publishing date, and a node\u2019s connection to other nodes indicates a citation relation. For the Kindle dataset, we select items from six categories: Religion & Spirituality, Children\u2019s eBooks, Health, Fitness & Dieting, SciFi & Fantasy, Business & Money, and Romance. For the DBLP dataset, we select papers published in 34 venues and divide them into four classes: Database, Data Mining, AI, and Computer Vision. For the ACM dataset, we select papers published in 66 venues and divide them into four classes: Information Systems, Signal Processing, Applied Mathematics, and AI. For each of the datasets, we select nodes from a specified time period. We build a graph and make a constraint that the timestamp of the target node is not allowed to be larger than the source node, which should naturally hold for citation datasets as one can not cite a paper published in the future. Then we split each graph into subgraphs by edges based on the timestamps of the source nodes. To simulate a real-life scenario that different tasks may have different sets of class labels, at each task for the Kindle dataset, we randomly select one or two classes and mask the labels of the nodes from selected class(s) during the training and the test phase; for the DBLP and ACM datasets, we randomly select one class and mask the labels of the nodes from the selected class during the training and the test phase. The summary of each dataset is provided in Table 4. ", "page_idx": 23}, {"type": "text", "text": "E.2 Hyper-parameter setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For each task, we randomly split all nodes into training, validation, and test sets with the ratio of $30/20/50$ . For the baseline CGL models the memory strengths are searched from $\\{10^{i}|i\\in[-5...5]\\}$ or $\\left\\lbrace0.{\\dot{1}},0.2...0.9\\right\\rbrace$ . For baselines that utilize a memory buffer, we calibrate their memory buffer sizes to ensure that their memory usage is on a similar scale to that of $\\mathbf{TAC}\\mathbb{O}$ . For $\\mathbf{TAC}\\mathbb{O}$ , by default the reduction ratio is 0.5; memory buffer size for Node Fidelity Preservation is 200; node degree is used to determine a node\u2019s importance score, and Reservoir Sampling is chosen as the node sampling strategy. We chose GCN and GAT as the GNN backbones. For both of them, we set the number of layers as 2 and the hidden layer size as 48. For GAT, we set the number of heads as 8. For each dataset, we generate 10 random seeds that split the nodes into training, validation, and test sets and select the masked class. We run all models on the same random seeds and the results are averaged over 10 runs. All experiments were conducted on a NVIDIA GeForce RTX 3090 GPU. ", "page_idx": 23}, {"type": "text", "text": "F Additional results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Main results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We present more results of the performance of $\\mathbf{TAC}\\mathbb{O}$ and other baselines on three datasets and two additional backbone GNN models, GAT (Table 5) and GIN (Table 6). The results show that our proposed method outperforms other continual learning approaches consistently with different GNN backbone models. ", "page_idx": 23}, {"type": "text", "text": "Table 5: Performance comparison of node classification in terms of F1 and BACC with GAT on three datasets (average over 10 trials). Standard deviation is denoted after $\\pm$ . ", "page_idx": 24}, {"type": "table", "img_path": "VpINEEVLX0/tmp/c1f91d10ed98fc244ab509b6be178b2b3c68b1a51b0e64f7e03d9788219c363d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.2 Graph coarsening methods ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We present the trade-offs in terms of all matrices of $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ with its graph coarsening module RePro replaced by five other widely used graph coarsening algorithms with GCN as the backbone GNN model in Table 7. ", "page_idx": 24}, {"type": "text", "text": "Table 6: Performance comparison of node classification in terms of F1 and BACC with GIN on three datasets (average over 10 trials). Standard deviation is denoted after $\\pm$ . ", "page_idx": 25}, {"type": "table", "img_path": "VpINEEVLX0/tmp/4f6dae46c221fd8bbeccb41c8c4ddd9a3d3ae1e0b26a98c40ccd91a91fb3eebe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G Additional ablation studies and analysis ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "G.1 Short-term forgetting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The average forgetting (AF) measures the decline in model performance after learning all tasks, which only captures the assess the model\u2019s long-term forgetting behavior. To evaluate the short-term forgetting of the model, we introduce a new metric, termed \"short-term average forgetting\" (AF-st), ", "page_idx": 25}, {"type": "text", "text": "Table 7: Coarsen runtime and trade-offs of $\\mathbf{TAC}\\mathbb{O}$ variations with different coarsening methods on three datasets with GCN (average over 10 trials). Boldface indicates the best result of each column. ", "page_idx": 26}, {"type": "table", "img_path": "VpINEEVLX0/tmp/36616f442ff21d0565061c9a2164438ec8aba0edff485f29821839ff27984954.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "VpINEEVLX0/tmp/74bbd0effe401f6ca655e5ac0206df19bd68e840fb59bc2b1a693591eef1ec1d.jpg", "img_caption": ["Figure 6: Average performance of the $\\mathbf{TAC}\\mathbb{O}$ with and without Node Fidelity Preserving (NFP). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "which measure the decline in model performance on the most recent task when it learns a new one: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{AF-st}=\\frac{1}{T}\\sum_{j=2}^{T}a_{j-1,j-1}-a_{j,j-1},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $T$ is the total number of task, and $a_{i,j}$ is the prediction metric of model on test set of task $j$ after it is trained on task $i$ . We report the AF-st in terms of F1 score with GCN as backbone on three datasets in Table 8. ", "page_idx": 26}, {"type": "text", "text": "G.2 Shorter time interval ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We investigate the performance of $\\mathbf{TAC}\\mathbb{O}$ and other baselines when each task is assigned with a shorter time interval. For the DBLP and ACM datasets, we divided them into one-year time intervals, resulting in a total of 20 tasks. We have included the AP-f1 and AF-f1 scores of all baselines utilizing GCN as their backbone in Table 9. Our findings indicate that, compared to our previous dataset splitting approach, most CGL methods exhibit a slight decline in performance, but TACO continues to outperform the other baseline models. ", "page_idx": 26}, {"type": "text", "text": "G.3 Efficiency Analysis ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We analyze the efficiency of $\\mathbf{TAC}\\mathbb{O}$ and other experience-replay-based CGL baselines in terms of training time and memory usage. We report the averaged total training time (including the time to learn model parameters and the time to store the memory/coarsen graphs), and the averaged memory ", "page_idx": 26}, {"type": "text", "text": "Table 8: The averaged short-term forgetting in terms of F1 score $(\\%)$ with GCN as the backbone on three datasets (averaged over 10 trials). ", "page_idx": 27}, {"type": "table", "img_path": "VpINEEVLX0/tmp/81b678f716e1c4815ddd4cd20dd5eeb229a44656be5b00b5a688e117976d232d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "VpINEEVLX0/tmp/74db2cccec88d06c5ad590f63aaa823aa3a4d01a36886ec614c9b462aaa251c7.jpg", "table_caption": ["Table 9: Node classification performance with GCN as the backbone on two datasets (averaged over 10 trials) with shorter time intervals and more tasks. Standard deviation is denoted after $\\pm$ . "], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "VpINEEVLX0/tmp/f3fe9198f6720c3b286fece969af69ff8bd655325fe9d7e67162f6b51cd10c84.jpg", "img_caption": ["Figure 7: Average performance of the $\\mathbf{TAC}\\mathbb{O}$ with different node sampling strategies: ReserviorSampling (rs), Ring Buffer (rb), and Mean Feature (mf). "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "VpINEEVLX0/tmp/cb6e0d7bdbfd6eca5e70d45269aaf731c327467c7de62e6b4d97187ffd28d983.jpg", "img_caption": ["Figure 8: Memory usages of different methods over tasks. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "usage of the model (including the memory to store data for current task and the memory buffer to store information of previous tasks) for each task in Table 10. We find that on average, SSM uses less memory than $\\mathbf{TAC}\\mathbb{O}$ on the Kindle dataset. However, on the DBLP and ACM datasets, SSM\u2019s memory usage is either more or similar. It\u2019s important to note that SSM maintains a sparsified graph that expands as additional tasks are introduced. As a result, SSM\u2019s memory continues to grow with the increasing number of tasks. In contrast, $\\mathbf{TAC}\\mathbb{O}$ , with its dynamic coarsening algorithm, consistently maintains a relatively stable memory regardless of the number of tasks. ", "page_idx": 28}, {"type": "text", "text": "G.4 Node Fidelity Preservation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We investigate the effectiveness of the Node Fidelity Preservation strategy by removing it from $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ and compare the average performance of the variant of $\\mathbf{TAC}\\mathbb{O}$ with its default version. We report the average performance of the model on the three datasets in Figure 6. We observe that on DBLP dateset, the application of Node Fidelity Preservation improves the performance, while on the other two datasets, the model performs comparably or marginally better without Node Fidelity Preservation. Note that our proposal of Node Fidelity Preservation is intended as a preventative measure, not as a means of enhancing model performance. Node Fidelity Preservation aims to prevent the situation ", "page_idx": 28}, {"type": "text", "text": "Table 10: The averaged memory usage (MB) for each task of experience-replay-based methods. ", "page_idx": 28}, {"type": "table", "img_path": "VpINEEVLX0/tmp/62e995cc5656df23fb5c4db951bf54962e1e2e978d347e79a48af597a32e9b59.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 11: Statistics of datasets. \u201cInterval\u201d indicates the length of the time interval for each task.   \n\u201c# Item\u201d indicates the total number of items/papers in each dataset. ", "page_idx": 29}, {"type": "table", "img_path": "VpINEEVLX0/tmp/a539fa7e8c6fe54101288279a4126203a622da2c05bf599798c1e9afd0a0e340.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "where minority classes vanish from the reduced graph due to unbalanced class distribution. Therefore, the improvement may not be noticeable if the class distribution is relatively balanced and node degradation is insignificant. In such cases, preserving the selected nodes may prevent the graph from coarsening to the optimized structure, which could even make the performance worse. ", "page_idx": 29}, {"type": "text", "text": "G.5 Important node sampling strategies ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We investigate how different important node sampling strategies affect the performance of the model. We report the average node classification performance of $\\mathbf{TAC}\\mathbb{O}$ with different node sampling strategies, Reservior-Sampling, Ring Buffer, and Mean Feature on the three datasets in Figure 7. It shows that $\\mathbf{TAC}\\mathbb{O}$ achieves better performance with Reservior-Sampling and Mean Feature. We deduce that it is caused by the fact that Ring Buffer operates on a first in, first out (FIFO) basis, that it only retains the most recent samples for each class, making it fail to preserve information from the distant past. ", "page_idx": 29}, {"type": "text", "text": "G.6 Performance on traditional class-incremental learning setup ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We also evaluate $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ on traditional class-incremental learning setup to prove its generability. We conduct experiments on three commonly used datasets in GCL literature including cora, coauthor-cs, and corafull. The details about their statistics and splitting are provided in Table 11. The performance of $\\mathbf{TAC}\\mathbb{\\mathbb{O}}$ with GCN as backbone in comparison with other baselines is provided in Table 12, which shows that the forgetting could be more severe when tasks share non-overlapping class sets, and $\\mathbf{TAC}\\mathbb{O}$ \u2019s rule on alleviating the forgetting is more evidently demonstrated. ", "page_idx": 29}, {"type": "table", "img_path": "VpINEEVLX0/tmp/84bfdde8acf3e121851389747b4a16529df6becc42d15373358b60eea11de8a1.jpg", "table_caption": ["Table 12: Node classification performance in terms of F1 and BACC with GCN on three datasets under task-IL settings (average over 10 trials). Standard deviation is denoted after $\\pm$ . "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope, as demonstrated through both theoretical analysis and empirical study. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We include a separate section in the paper to discuss the limitations of our works. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the full set of assumptions and a complete (and correct) proof for each theoretical result. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper provides all necessary implementation details, including pseudocode, to reproduce the main experimental results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We submit source code, data, and comprehensive instructions to faithfully reproduce the main experimental results. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide all the training and test details in Appendix E. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We report standard deviation and p-values to demonstrate statistical significance. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide information about the GPU used for the experiments and also report and analyze memory and time usage. ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and confirm the research conducted in the paper conforms with it. ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discuss the positive social impact of this work in a separate section and do not foresee any negative social impact. ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We use open-source datasets and properly cite them. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We submit a zip flie containing the code for our proposed method, along with detailed implementation instructions. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No human subjects were involved in the research conducted in this paper. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No human subjects were involved in the research conducted in this paper. ", "page_idx": 32}]