[{"figure_path": "ChnJ3W4HFG/figures/figures_5_1.jpg", "caption": "Figure 1: Accuracy (and 95% confidence intervals over 10 runs) of predicted labels from KNN under 20% symmetric label noise. CelebA and Waterbirds achieve strong performance with a large number of nearest neighbors, but CMNIST struggles as the number of neighbors or rounds grows too large.", "description": "This figure shows the accuracy of KNN label spreading on three different datasets (CelebA, Waterbirds, and CMNIST) under 20% symmetric label noise.  The x-axis represents the number of nearest neighbors considered, and the y-axis represents the accuracy. The plots show that CelebA and Waterbirds achieve high accuracy with a large number of neighbors, indicating that the embeddings are well-separated.  CMNIST, however, shows decreased performance as the number of neighbors increases, likely due to less well-separated embeddings in this dataset.", "section": "4.2 Empirical Evidence for Label Spreading"}, {"figure_path": "ChnJ3W4HFG/figures/figures_6_1.jpg", "caption": "Figure 1: Accuracy (and 95% confidence intervals over 10 runs) of predicted labels from KNN under 20% symmetric label noise. CelebA and Waterbirds achieve strong performance with a large number of nearest neighbors, but CMNIST struggles as the number of neighbors or rounds grows too large.", "description": "The figure shows the accuracy of KNN label spreading on three datasets (CelebA, Waterbirds, and CMNIST) under 20% symmetric label noise.  The x-axis represents the number of rounds of label spreading, and the y-axis represents the accuracy. Each line represents a different number of nearest neighbors (k) used in the KNN algorithm. CelebA and Waterbirds show high accuracy with a large number of nearest neighbors, while CMNIST shows lower accuracy and is more sensitive to the number of neighbors and rounds.", "section": "4.2 Empirical Evidence for Label Spreading"}, {"figure_path": "ChnJ3W4HFG/figures/figures_6_2.jpg", "caption": "Figure 2: tSNE projection of the 2048 dimensional latent embeddings into a 2 dimensional space for visualization. We see that CelebA and Waterbirds show clear class separation while CMNIST has more hierarchical clustering. This could lead to decreased performance of label spreading.", "description": "This figure shows the t-SNE visualization of 2048-dimensional latent embeddings reduced to 2 dimensions for three different datasets: CelebA, Waterbirds, and CMNIST.  The visualization reveals the different clustering patterns for each dataset. CelebA and Waterbirds exhibit clear separation between classes, while CMNIST shows a more hierarchical and less clearly separated structure. This difference in clustering patterns can explain why label spreading, which relies on neighbor information, might perform differently on these datasets. The less well-separated clusters in CMNIST could hinder the effectiveness of label propagation because noisy labels from neighboring clusters are more likely to negatively impact the accuracy.", "section": "4.2 Empirical Evidence for Label Spreading"}, {"figure_path": "ChnJ3W4HFG/figures/figures_14_1.jpg", "caption": "Figure 3: Label noise causes the RAD algorithm to select, and then upweight, the incorrect error set. We show that RAD fails under sufficient SLN for imbalanced settings.", "description": "This figure visualizes the effect of label noise on the RAD algorithm's error set selection and subsequent upweighting.  The left panel (a) shows the error set, where points are colored according to their true class labels. We can see that noisy examples are incorrectly included in the error set. The right panel (b) illustrates the upweighted points from this error set. The upweighting process disproportionately emphasizes noisy majority class samples, leading to a biased retraining set that hurts the worst group accuracy. This demonstrates how RAD fails under sufficient symmetric label noise (SLN) in imbalanced settings.", "section": "B RAD Fails Under Label Noise"}, {"figure_path": "ChnJ3W4HFG/figures/figures_15_1.jpg", "caption": "Figure 4: RAD trained with a-loss is able to capture minority points at all noise levels, but an increasing number of noisy majority points are selected as noise increases. This leads to poor downstream fairness", "description": "This figure shows how the number of true minority samples and noisy minority samples selected by the RAD algorithm changes with increasing levels of noise.  It demonstrates that while the algorithm can capture some true minority samples even with high noise, the number of noisy majority samples selected dramatically increases, which negatively impacts downstream fairness.", "section": "4.3 Robustness of kNN to Noisy Embeddings"}]