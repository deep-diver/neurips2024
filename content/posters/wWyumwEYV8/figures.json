[{"figure_path": "wWyumwEYV8/figures/figures_1_1.jpg", "caption": "Figure 1: We showcase CounterAnimal examples from the class of ice bear, separated into easy and hard groups with different backgrounds (i.e., snow and grass). The zero-shot performance of CLIP-LAION400M-ViT-B/32 drops from 97.62% (easy) to 70.91% (hard).", "description": "This figure displays example images from the CounterAnimal dataset, specifically focusing on the \"ice bear\" class.  The dataset is designed to highlight CLIP's reliance on spurious correlations.  The images are divided into two groups: \"easy\" (ice bears in snowy backgrounds) and \"hard\" (ice bears in grassy backgrounds).  The significant drop in zero-shot accuracy from the easy group to the hard group (97.62% to 70.91%) demonstrates how the model's performance degrades when spurious correlations are disrupted.  The difference shows that the model relies heavily on the background as a spurious feature for correct classification, rather than on the intrinsic visual features of the ice bear itself.", "section": "1 Introduction"}, {"figure_path": "wWyumwEYV8/figures/figures_2_1.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure compares the performance of various CLIP and ImageNet models on an \"easy\" versus \"hard\" task, designed to evaluate robustness against spurious correlations.  The x-axis represents performance on the easy task, and the y-axis shows performance on the hard task.  Points closer to the y=x line indicate better robustness, as performance on both tasks is similar.  The figure shows that larger models and those trained on higher quality data exhibit improved robustness. ImageNet models are shown to be more robust than many CLIP models.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_2_2.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure compares the performance of different vision-language models (VLMs) and ImageNet models on the CounterAnimal dataset.  The x-axis represents the performance on the \"easy\" subset of the dataset (animals in common backgrounds), while the y-axis represents performance on the \"hard\" subset (animals in uncommon backgrounds).  Points closer to the y=x line indicate better robustness, as performance is consistent across both sets. The figure demonstrates that CLIP models, especially those trained on higher-quality data, are more robust to this type of spurious correlation compared to ImageNet models,  Larger models generally perform better.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_4_1.jpg", "caption": "Figure 3: The data layout across various animal classes. The horizontal axis denotes the class IDs and the vertical axis denotes the number of photos for the easy and hard groups, respectively.", "description": "This figure shows the distribution of images in the CounterAnimal dataset. Each bar represents a different animal class. The height of the blue portion of the bar indicates the number of images in the \"easy\" group (images with common backgrounds), and the height of the orange portion represents the number of images in the \"hard\" group (images with uncommon backgrounds).  The figure visually displays the class imbalance and the distribution of easy versus hard samples across different animal categories in the CounterAnimal dataset.", "section": "2.2 Characteristics of CounterAnimal"}, {"figure_path": "wWyumwEYV8/figures/figures_4_2.jpg", "caption": "Figure 4: The 1 vs. 1000 performance drop (%) with CLIP-LAION400M-ViT-B/32. The horizontal axis denotes the class IDs and the vertical axis denotes the percentage points of decline.", "description": "This figure shows the performance drop in accuracy between easy and hard groups for each animal class in the CounterAnimal dataset.  The x-axis represents the class ID, and the y-axis shows the difference in zero-shot accuracy between easy and hard samples, indicating the impact of spurious correlations associated with background changes. A larger drop indicates a stronger reliance on spurious features.", "section": "Characteristics of CounterAnimal"}, {"figure_path": "wWyumwEYV8/figures/figures_5_1.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure presents a comparison of the performance of different vision models on the CounterAnimal dataset. The x-axis represents the easy group's performance, and the y-axis represents the hard group's performance.  Each point represents a specific model, with the size of the point indicating the size of the model's backbone and the color of the point indicating the size of the pre-training dataset used to train the model. The models are categorized as CLIP models, ImageNet models, and advanced Large Vision Language Models (LVLMs). The figure shows that CLIP models are less robust to spurious correlations than ImageNet models and that the robustness of CLIP models can be improved by scaling up the parameters and using higher-quality pre-trained data.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_6_1.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure compares the performance of various vision-language models (VLMs) and ImageNet models on the CounterAnimal dataset.  It shows the zero-shot accuracy on \"easy\" (images with common backgrounds) versus \"hard\" (images with less common but plausible backgrounds) samples. The size of the markers indicates the model's size (backbone), and the color indicates the size of the pre-training data.  High-quality pre-training data (DataComp, Data Filtering Networks) lead to improved robustness against spurious correlations.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_6_2.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure compares the performance of various CLIP and ImageNet models, including more advanced large vision language models like MiniGPT4 and LLaVA. The x-axis represents the performance on \"easy\" samples (animals in commonly seen backgrounds), and the y-axis represents the performance on \"hard\" samples (animals in uncommon backgrounds).  The size of the markers represents the model size (backbone scale), and their color represents the scale of pre-training data.  The plot shows that larger CLIP models generally perform better on both easy and hard samples but still exhibit a performance drop from easy to hard samples, indicating the influence of spurious correlations.  In contrast, ImageNet models show greater robustness to spurious correlations.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_8_1.jpg", "caption": "Figure 10: Illustration of ColoredCOCO.", "description": "This figure shows examples from the ColoredCOCO dataset. The dataset contains images of objects with backgrounds of a specific color (training data), and test data contains images of the same objects with backgrounds of different colors.  This illustrates the concept of spurious correlations, where the model might incorrectly associate the object with the color of its background rather than the object itself. This dataset is used in the paper to evaluate the robustness of CLIP models against spurious correlations.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_8_2.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure compares the performance of various vision-language models (VLMs) and ImageNet models on the CounterAnimal dataset.  The x-axis represents the performance on the 'easy' subset of the data, and the y-axis represents the performance on the 'hard' subset.  The 'easy' set contains images of animals in typical backgrounds, whereas the 'hard' set contains images of the same animals in less common backgrounds. The figure aims to illustrate the models' robustness to spurious features (correlations learned from the training data which are not generally applicable to new data).  The size of the marker indicates the model's size and color represents the amount of training data used.  The diagonal line (y=x) represents a perfect trend where the model generalizes equally across both 'easy' and 'hard' sets. The plot demonstrates that larger CLIP models (larger markers) are more robust to spurious features, while increasing the size of the training dataset does not always improve robustness in the same manner.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_12_1.jpg", "caption": "Figure 3: The data layout across various animal classes. The horizontal axis denotes the class IDs and the vertical axis denotes the number of photos for the easy and hard groups, respectively.", "description": "This figure shows the distribution of images in the CounterAnimal dataset.  Each bar represents an animal class. The height of the left portion of each bar shows the number of \"easy\" images (animals in commonly occurring backgrounds), and the right portion shows the number of \"hard\" images (animals in less common backgrounds) for that class. This visualization helps to understand the class-wise distribution of samples for each group and provides insights into the dataset's construction.", "section": "2.2 Characteristics of CounterAnimal"}, {"figure_path": "wWyumwEYV8/figures/figures_18_1.jpg", "caption": "Figure 12: Examples of MultiColoredMNIST dataset.", "description": "This figure shows 16 examples from the MultiColoredMNIST dataset.  Each image is a digit from the MNIST dataset, but colored with different shades. This dataset is used to evaluate the robustness of CLIP models to spurious correlations between the digit and its color, where the color acts as a spurious feature.  The variations in color across the examples highlight the controlled nature of the spurious correlation introduced.", "section": "D.3 More Details on ColoredCOCO Experiments"}, {"figure_path": "wWyumwEYV8/figures/figures_21_1.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure compares the performance of various models (CLIP, ImageNet models, MiniGPT4, and LLaVA) on easy and hard subsets of the CounterAnimal dataset.  The x-axis represents the easy subset performance, and the y-axis represents the hard subset performance. Points closer to the y=x line indicate better robustness against spurious correlations.  The size of the markers indicates the size of the model backbone, and the color indicates the scale of the pre-training data. CLIP models trained on higher quality data (CLIP-DC and CLIP-DFN) show better robustness than those trained on standard datasets. ImageNet models demonstrate greater robustness to spurious features than most CLIP models.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_23_1.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure shows the comparison of \"easy\" and \"hard\" performance on the CounterAnimal dataset for various models, including CLIP models with different backbone sizes and training data, ImageNet models, and more advanced Large Vision Language Models (LVLMs). The x-axis represents the \"easy\" group's performance and the y-axis represents the \"hard\" group's performance. The size of the markers indicates the backbone scale, and the color indicates the pre-training data size. The linear fit of the trends helps visualize the effective robustness. The ideal scenario (no bias learned) is represented by the y=x line.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_28_1.jpg", "caption": "Figure 13: The easy versus hard performance (%) for CLIP and ImageNet models on CounterAnimal-I. The 1 vs. 1000 setup is considered.", "description": "This figure compares the performance of CLIP and ImageNet models on the CounterAnimal-I dataset.  CounterAnimal-I is a variation of the CounterAnimal dataset, where the easy and hard splits are determined using ImageNet models instead of CLIP models. The x-axis represents the performance on easy examples, while the y-axis represents performance on hard examples. The diagonal line (y=x) indicates perfect performance; points above the line suggest a model performs better on hard examples, and vice versa.  The plot shows that ImageNet models are generally more robust to the spurious correlations captured by CounterAnimal-I than CLIP models.", "section": "3.1 Generality of the Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_29_1.jpg", "caption": "Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., y = x, where the models will not learn any bias.", "description": "This figure compares the performance of various vision models (CLIP, ImageNet models, MiniGPT4, and LLaVA) on the CounterAnimal dataset.  The x-axis represents the 'easy' group performance (animals in commonly seen backgrounds), while the y-axis represents the 'hard' group performance (animals in less-common backgrounds). Each point represents a specific model, with marker size indicating model scale and color indicating the scale of pre-training data. High-quality pre-trained data (DataComp and Data Filtering Networks) are highlighted. The plots show how well different models perform when the background context changes and how well CLIP-based models handle this shift relative to other model types.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/figures/figures_32_1.jpg", "caption": "Figure 4: The 1 vs. 1000 performance drop (%) with CLIP-LAION400M-ViT-B/32. The horizontal axis denotes the class IDs and the vertical axis denotes the percentage points of decline.", "description": "This figure shows the performance drop of CLIP-LAION400M-ViT-B/32 model for each animal class in the CounterAnimal dataset, in the context of \"1 vs. 1000\" setup. The horizontal axis represents the class IDs while the vertical axis shows the percentage drop in accuracy between the easy group (animals in commonly appeared backgrounds) and the hard group (animals in less commonly but still plausible backgrounds). A higher bar indicates a larger performance drop, suggesting a stronger reliance on spurious features related to background for that specific animal class.  The figure visually depicts how much the model's performance is affected by the background changes for each animal.", "section": "Characteristics of CounterAnimal"}, {"figure_path": "wWyumwEYV8/figures/figures_32_2.jpg", "caption": "Figure 13: The easy versus hard performance (%) for CLIP and ImageNet models on CounterAnimal-I. The 1 vs. 1000 setup is considered.", "description": "This figure shows a comparison of the performance of CLIP and ImageNet models on the CounterAnimal-I dataset, using a 1 vs. 1000 setup. The x-axis represents the performance on the \"easy\" subset of the dataset, while the y-axis represents the performance on the \"hard\" subset. The diagonal line represents the case where the performance is identical on both easy and hard subsets. Points above this line indicate better performance on the \"hard\" set compared to the \"easy\" set, and vice versa.  The plot shows that CLIP models demonstrate greater sensitivity to the spurious features than the ImageNet models. Different colored markers and sizes might indicate different model architectures or sizes, or the training datasets used.", "section": "E Ablation Studies"}, {"figure_path": "wWyumwEYV8/figures/figures_32_3.jpg", "caption": "Figure 13: The easy versus hard performance (%) for CLIP and ImageNet models on CounterAnimal-I. The 1 vs. 1000 setup is considered.", "description": "This figure compares the performance of CLIP and ImageNet models on the CounterAnimal-I dataset.  The x-axis represents the easy set's performance, and the y-axis represents the hard set's performance.  Each point represents a model, and the size of the point may indicate the model's scale or complexity. The diagonal line represents equal performance on easy and hard sets. Points above the line indicate better generalization than points below. The plot visually demonstrates CLIP models' reliance on spurious features (their performance is more significantly impacted by the shift from 'easy' to 'hard' groups), while ImageNet models seem less affected.", "section": "E Ablation Studies"}, {"figure_path": "wWyumwEYV8/figures/figures_32_4.jpg", "caption": "Figure 13: The easy versus hard performance (%) for CLIP and ImageNet models on CounterAnimal-I. The 1 vs. 1000 setup is considered.", "description": "This figure shows the performance comparison between CLIP and ImageNet models on the CounterAnimal-I dataset using a 1 vs. 1000 setup.  The x-axis represents the easy group's performance, and the y-axis represents the hard group's performance.  Each point represents a specific model, with the size of the point indicating the model scale.  The trend lines help visualize how well each type of model handles spurious correlations. The diagonal line (y=x) indicates perfect performance (no difference between easy and hard groups).  Points above the line suggest better robustness to spurious correlations than those below.", "section": "E Ablation Studies"}, {"figure_path": "wWyumwEYV8/figures/figures_32_5.jpg", "caption": "Figure 13: The easy versus hard performance (%) for CLIP and ImageNet models on CounterAnimal-I. The 1 vs. 1000 setup is considered.", "description": "This figure shows the performance comparison between CLIP and ImageNet models on the CounterAnimal-I dataset. The x-axis represents the easy set performance, and the y-axis represents the hard set performance. The diagonal line represents the ideal case where the performance is equal for both easy and hard sets. The points above the diagonal line indicate that CLIP models are more robust than ImageNet models to spurious correlations. The points below the line indicate that ImageNet models are more robust than CLIP models. The figure demonstrates that CLIP models are more sensitive to changes in background compared to ImageNet models, suggesting that CLIP models rely on spurious correlations more heavily than ImageNet models.", "section": "E Ablation Studies"}]