[{"figure_path": "wWyumwEYV8/tables/tables_1_1.jpg", "caption": "Table 1: 1 vs. 1000 results of exemplary animal classes within the CounterAnmial dataset for CLIP-LAION400M-ViT-B/32. \"bkg\" denotes the background label, \"accu\" (%) denotes the zero-shot accuracy, and \"drop\" (%) denotes the drop in accuracy between easy and hard groups.", "description": "This table presents the zero-shot accuracy of the CLIP-LAION400M-ViT-B/32 model on a subset of the CounterAnimal dataset.  The dataset is designed to highlight the model's reliance on spurious features (correlations between animal classes and their backgrounds).  For each animal class, two groups of images are shown: an \"easy\" group with commonly associated backgrounds and a \"hard\" group with less common backgrounds. The table shows the accuracy for each group and the difference (drop) in accuracy between them.  This difference indicates the degree to which the model is relying on the background as a spurious feature rather than solely on the animal itself for classification.  Larger drops in accuracy suggest a greater reliance on spurious features.", "section": "3.1 Generality of the Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_5_1.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the results of a zero-shot classification task on the CounterAnimal dataset using various CLIP models.  The \"easy\" column shows the accuracy of the model on images with common backgrounds, while \"hard\" shows accuracy on images with uncommon backgrounds. The \"drop\" column shows the difference between \"easy\" and \"hard\" accuracy, representing the model's sensitivity to spurious correlations based on backgrounds.  The table includes models with different backbones (e.g., ViT-B/16, ViT-L/14) and pre-trained on different datasets (e.g., LAION400M, LAION2B, OpenAI, DataComp1B, DFN2B), highlighting the impact of model architecture and training data on robustness against spurious features.  Models pre-trained on high-quality datasets are marked with an asterisk (*).", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_6_1.jpg", "caption": "Table 3: The 1 vs. 1000 performance for ImageNet models on CounterAnimal.", "description": "This table presents the results of a 1 vs. 1000 classification task on the CounterAnimal dataset using various ImageNet models.  The \"easy\" column shows the accuracy when animals are presented in their common backgrounds, while the \"hard\" column shows the accuracy in uncommon backgrounds. The \"drop\" column calculates the difference between easy and hard accuracies, reflecting the impact of spurious correlations on the model's performance.  It demonstrates the comparative robustness of ImageNet models against spurious correlations compared to CLIP models, showing that ImageNet models are more robust to these specific spurious features.", "section": "3.3 Evaluations for other Learning Paradigms"}, {"figure_path": "wWyumwEYV8/tables/tables_7_1.jpg", "caption": "Table 4: The 1 vs. 20 results of CounterAnimal for advanced LVLMs and several CLIP models. More results of CLIP models and ImageNet models can be found in Appendix F.", "description": "This table presents the results of the 1 vs. 20 evaluation setup on the CounterAnimal dataset.  The 1 vs. 20 setup uses the top 20 most confusing classes for each animal class (determined by CLIP-LAION400M-ViT-B/32) as the candidate label space, making the evaluation more computationally efficient for large language models. The table shows the \"easy\" and \"hard\" group performance and the performance drop for several advanced large vision language models (LVLMs), including MiniGPT4-Viccuna7B and LLaVA1.5-7B, as well as several CLIP models with different backbones and pre-training datasets.  Appendix F contains more results for CLIP and ImageNet models.", "section": "3.3 Evaluations for other Learning Paradigms"}, {"figure_path": "wWyumwEYV8/tables/tables_13_1.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the zero-shot accuracy of various CLIP models on the CounterAnimal dataset.  The \"easy\" column shows the accuracy when animals are shown in typical backgrounds, while the \"hard\" column shows accuracy when animals are shown in less common backgrounds. The \"drop\" column indicates the difference between easy and hard accuracy, representing the impact of spurious correlations. The table also notes which pre-trained datasets had high-quality data (*). This provides a quantitative assessment of how well different CLIP models are robust to spurious background correlations.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_14_1.jpg", "caption": "Table 6: Adopted versions of CLIP checkpoints employed in our main experiments.", "description": "This table lists the specific versions of CLIP checkpoints used in the paper's main experiments.  It details the backbone architecture (e.g., ViT-B/16, ViT-L/14), the pre-training dataset (e.g., LAION400M, LAION2B, DataComp1B), and the corresponding checkpoint identifier (e.g., E31, S34B B88K, XL S13B B90K). This information is crucial for reproducibility, allowing readers to replicate the experimental results using the exact same model versions.", "section": "C Experimental Configurations"}, {"figure_path": "wWyumwEYV8/tables/tables_17_1.jpg", "caption": "Table 7: Comparison between CLIPs and standard supervised learning on ColoredCOCO", "description": "This table presents a comparison of the performance of CLIP models and standard supervised learning models on the ColoredCOCO dataset.  The ColoredCOCO dataset was specifically designed to evaluate the robustness of models against spurious correlations. The table shows the in-distribution and out-of-distribution performance for various models and approaches, including zero-shot, object-centric, and object-background prompts. The \"drop\" column indicates the difference in performance between in-distribution and out-of-distribution settings. This table provides empirical evidence for the study's claims on the limitations of the CLIP objective in achieving additional robustness.", "section": "3.3 Evaluations for other Learning Paradigms"}, {"figure_path": "wWyumwEYV8/tables/tables_19_1.jpg", "caption": "Table 8: Comparison of standard supervised learning and contrastive learning on MultiColoredMNIST dataset.", "description": "This table presents the results of experiments comparing standard supervised learning and contrastive learning on the MultiColoredMNIST dataset.  The results are broken down by the number of classes, number of samples, invariant feature correlation (Pinv), spurious feature correlation (Pspu), and training method (Contrastive or Supervised). For each configuration, the table shows the performance on classification tasks using two different test sets: one where the classes and colors are randomly correlated, and another where they are reversely correlated.  The results are presented as mean \u00b1 standard deviation of the classification accuracy.  The table helps analyze how different training paradigms and data characteristics affect the model's robustness to spurious correlations.", "section": "3.3 Evaluations for other Learning Paradigms"}, {"figure_path": "wWyumwEYV8/tables/tables_21_1.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the zero-shot accuracy results of various CLIP models on the CounterAnimal dataset.  It shows the performance on both \"easy\" and \"hard\" subsets of the data, representing scenarios where the model is expected to perform well and poorly, respectively. The difference between \"easy\" and \"hard\" accuracies (the \"drop\" column) indicates the model's vulnerability to spurious correlations. The table includes different CLIP model architectures (backbones) and pre-training datasets, highlighting the impact of these factors on robustness.  Models trained on high-quality datasets are marked with an asterisk (*), allowing for a comparison of performance influenced by data quality.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_22_1.jpg", "caption": "Table 11: Selected animal object and background names in CounterAnimal and CounterAnimal-I. We bold the background names differently between CounterAnimal and CounterAnimal-I.", "description": "This table compares the animal classes and their corresponding easy and hard background labels in both the original CounterAnimal dataset and a modified version, CounterAnimal-I.  The difference highlights how different models (CLIP vs. ImageNet) identify and use spurious correlations within the data. The bolded background names showcase how easy and hard groups differ depending on which model (CLIP or ImageNet) created the splits.", "section": "E Ablation Studies"}, {"figure_path": "wWyumwEYV8/tables/tables_22_2.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the results of a zero-shot classification experiment using various CLIP models on the CounterAnimal dataset.  The experiment compares performance on \"easy\" and \"hard\" subsets of the dataset, representing different levels of spurious correlation. The table shows the accuracy achieved on each subset, and the difference between the two ('drop'), for various CLIP models using different backbones (e.g. ViT-B/16, ViT-L/14) and pre-training datasets (e.g. LAION400M, OpenAI, DataComp1B, LAION2B, DFN2B). Models trained on higher-quality datasets are marked with an asterisk. The 'drop' column highlights the sensitivity of CLIP models to spurious features, indicating a reliance on spurious correlations present in the data.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_24_1.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the zero-shot accuracy results of various CLIP models on the CounterAnimal dataset.  The \"easy\" column shows the accuracy when the animal images are presented with their typical backgrounds, while the \"hard\" column shows the accuracy when presented with less typical, but still plausible, backgrounds.  The \"drop\" column indicates the difference in accuracy between easy and hard conditions.  The table highlights the impact of different backbones (model architectures), pre-training datasets, and dataset quality on the model's robustness to spurious features. Models trained on higher quality data (marked with *) generally show smaller accuracy drops, indicating increased robustness.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_25_1.jpg", "caption": "Table 13: The 1 vs. 1000 results for CLIP checkpoints on CounterAnimal. The pre-train datasets with high-quality data are marked by *. ", "description": "This table presents the results of a 1 vs. 1000 classification task on the CounterAnimal dataset using various CLIP models.  The \"easy\" and \"hard\" columns represent the accuracy of the models on image-text pairs with common and uncommon backgrounds, respectively. The \"drop\" column shows the performance difference between the easy and hard groups.  Models trained on higher-quality datasets are marked with an asterisk (*). The table helps to assess the robustness of CLIP models to spurious features by comparing their performance on easy and hard subsets.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_25_2.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the results of a zero-shot classification experiment using various CLIP models on the CounterAnimal dataset.  The experiment compares the performance of CLIP models with different backbones (e.g., ViT-B/16, ViT-B/32, ViT-L/14), pretrained on different datasets (including LAION400M, LAION2B, DataComp1B, and DFN2B).  The \"easy\" and \"hard\" columns represent the accuracy on subsets of the data designed to highlight the reliance of CLIP models on spurious features. The \"drop\" column shows the difference in accuracy between the \"easy\" and \"hard\" sets, indicating the sensitivity of the model to spurious correlations. Models trained on higher-quality datasets (marked with an asterisk) generally show smaller accuracy drops.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_26_1.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the zero-shot accuracy of various CLIP models on the CounterAnimal dataset.  The \"easy\" column shows the accuracy when the animal images are presented with common backgrounds, while the \"hard\" column shows accuracy with uncommon backgrounds. The \"drop\" column is the difference between \"easy\" and \"hard\" accuracies, indicating the model's sensitivity to spurious correlations caused by background changes.  The table includes models with different backbones (e.g., ViT-B/16, ViT-L/14) and pre-trained on various datasets (e.g., LAION400M, LAION2B, OpenAI, DataComp1B, DFN2B).  Models trained on higher quality datasets (marked with *) are expected to show less of a drop in accuracy.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_26_2.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the results of a zero-shot classification task using various CLIP models on the CounterAnimal dataset.  The task involves classifying images of animals into their respective classes. The 'easy' group represents images with commonly seen backgrounds, while the 'hard' group has less common backgrounds, designed to highlight the impact of spurious features.  The table shows the accuracy for both groups ('easy' and 'hard'), along with the performance drop, which represents the difference in accuracy between the easy and hard groups. The asterisk (*) indicates that the model is trained on high-quality data. This data helps in analyzing CLIP's robustness to spurious correlations.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_27_1.jpg", "caption": "Table 20: The 1 versus 20 performance on CounterAnimal for ImageNet models.", "description": "This table presents the results of a 1 vs. 20 evaluation setup on the CounterAnimal dataset using various ImageNet models.  The \"easy\" and \"hard\" columns show the model's accuracy on subsets of data designed to be easily and hardly classified, respectively, based on spurious correlations. The \"drop\" column indicates the difference between the \"easy\" and \"hard\" accuracies, representing the model's vulnerability to spurious features.", "section": "3.3 Evaluations for other Learning Paradigms"}, {"figure_path": "wWyumwEYV8/tables/tables_27_2.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the results of a zero-shot classification task using various CLIP models on the CounterAnimal dataset.  The \"easy\" and \"hard\" columns represent the accuracy of the model on subsets of the data, where the \"easy\" subset contains images with commonly associated backgrounds and the \"hard\" subset contains images with less commonly associated backgrounds. The \"drop\" column shows the difference in accuracy between the easy and hard subsets. This table demonstrates the effect of various model architectures (backbones) and pre-training data (datasets) on the models' susceptibility to spurious correlations present in the data. High quality data is marked with an asterisk (*).", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_27_3.jpg", "caption": "Table 20: The 1 versus 20 performance on CounterAnimal for ImageNet models.", "description": "This table presents the results of a 1 vs. 20 evaluation setup on the CounterAnimal dataset using various ImageNet models.  The 1 vs. 20 setup uses a smaller subset of the ImageNet labels (the top 20 most confusing ones, determined by the CLIP model's performance), making it computationally less expensive than a full 1 vs. 1000 evaluation.  The table shows the performance (easy and hard) and the performance drop for various ImageNet models with different backbones. This allows for a comparison of ImageNet model robustness against the spurious correlations captured in the CounterAnimal dataset, using a more efficient evaluation method than the full 1 vs 1000 setup.", "section": "3.3 Evaluations for other Learning Paradigms"}, {"figure_path": "wWyumwEYV8/tables/tables_30_1.jpg", "caption": "Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the results of a zero-shot classification experiment using various CLIP models on the CounterAnimal dataset.  It shows the accuracy achieved by each model on \"easy\" and \"hard\" subsets of the dataset. The \"easy\" subset contains images with common backgrounds, while the \"hard\" subset contains images with less common backgrounds that challenge the model's robustness.  The table also indicates which pre-trained datasets were of higher quality. The difference between easy and hard subset accuracies reveals the model's sensitivity to spurious correlations.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}, {"figure_path": "wWyumwEYV8/tables/tables_31_1.jpg", "caption": "Table 13: The 1 vs. 1000 results for CLIP checkpoints on CounterAnimal. The pre-train datasets with high-quality data are marked by *.", "description": "This table presents the results of a 1 vs. 1000 classification task, where the goal is to correctly identify the object from 1000 possible ImageNet classes. The table evaluates various CLIP models with different backbones (e.g., ViT-B/16, ViT-B/32, ViT-L/14) and pre-trained on different datasets (e.g., LAION400M, LAION2B, OpenAI, DataComp1B, DFN2B). The \"easy\" and \"hard\" columns represent the accuracy of the models on subsets of images with commonly and less commonly seen background scenarios, respectively. The \"drop\" column shows the performance difference between the easy and hard groups, indicating the model's robustness to background variations. Models trained on high-quality datasets (marked with *) generally show smaller performance drops.", "section": "3.2 Scaling up May Relieve Spurious Correlations"}]