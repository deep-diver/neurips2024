[{"type": "text", "text": "Understanding Multi-Granularity for Open-Vocabulary Part Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiho Choi1\u2217, Seonho Lee1\u2217, Seungho Lee2, Minhyun Lee2, Hyunjung Shim1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Graduate School of Artificial Intelligence, KAIST, Republic of Korea 2School of Integrated Technology, Yonsei University, Republic of Korea {jihochoi, glanceyes, kateshim}@kaist.ac.kr, {seungholee, lmh315}@yonsei.ac.kr ", "page_idx": 0}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/05b686af566064b476e7b1140e9c48203a8eff5021e2ce2ae728842cd6d0bbe6.jpg", "img_caption": ["Figure 1: Prediction results of our PartCLIPSeg for unseen categories in the Pascal-Part-116 [7, 46] validation set. A \u201cdog\u201d is unseen during training. The final prediction of PartCLIPSeg utilizes (b) object-level context and (c) generalized parts, incorporating disjoint activation among (e)\u2013(i) parts, and enhancing activation for smaller parts (e.g., (h) \u201cnose\u201d). "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Open-vocabulary part segmentation (OVPS) is an emerging research area focused on segmenting fine-grained entities using diverse and previously unseen vocabularies. Our study highlights the inherent complexities of part segmentation due to intricate boundaries and diverse granularity, reflecting the knowledge-based nature of part identification. To address these challenges, we propose PartCLIPSeg, a novel framework utilizing generalized parts and object-level contexts to mitigate the lack of generalization in fine-grained parts. PartCLIPSeg integrates competitive part relationships and attention control, alleviating ambiguous boundaries and underrepresented parts. Experimental results demonstrate that PartCLIPSeg outperforms existing state-of-the-art OVPS methods, offering refined segmentation and an advanced understanding of part relationships within images. Through extensive experiments, our model demonstrated a significant improvement over the state-of-theart models on the Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets. Our code is available at https://github.com/kaist-cvml/part-clipseg. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The pursuit of understanding parts and multi-granularity in computer vision [7, 13, 21] mirrors the innate complexities of animal instincts. For example, a \u201ccheetah\u201d instinctively targets an \u201cimpala\u2019s neck\u201d during a hunt, demonstrating its ability to distinguish specific parts. This ability extends to applications such as robot commands [44], fine-grained controls on image editing [31], and more sophisticated image generation [45]. Part segmentation aims to mimic this ability by recognizing intricate details (e.g., parts) within objects, going beyond simple object-level segmentation to achieve detailed and diverse entity recognition. ", "page_idx": 1}, {"type": "text", "text": "Recognizing parts is more challenging than recognizing whole objects due to their complexity and diversity. Parts often have ambiguous boundaries not only defined by visual cues but also require a broader spectrum of contextual information, reflecting their knowledge-based nature. For example, the \u201chead\u201d of a \u201cdog\u201d may include only the \u201cface\u201d or also the \u201cneck\u201d depending on the annotators\u2019 perspective [7, 21]. ", "page_idx": 1}, {"type": "text", "text": "To address difficulties in part segmentation, Open-Vocabulary Part Segmentation (OVPS) [40, 44, 46] has evolved by leveraging the knowledge of powerful Vision-Language Models (VLMs) like CLIP [38] or ALIGN [24]. Especially, it aims to achieve adaptive recognition and processing of previously unseen categories with the aid of pre-trained VLMs, pushing the boundaries of vocabularies in traditional part segmentation. By utilizing Oracle supervision of base classes during training, recent studies in OVPS exploit part-level knowledge of base classes to generalize to novel classes. Recently, VLPart [40] uses DINO [5] features to map correspondences between base and novel classes and creates pseudo labels for the novel categories. OV-PARTS [46] addresses the ambiguity of part boundaries by introducing object mask prompts and transferring knowledge of base class through a few-shot approach. These methods successfully extract knowledge from VLMs and extend it to novel classes, achieving significant performance improvements in open-vocabulary settings. ", "page_idx": 1}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/6d93f7e031f92dadc84d1fdf6d97c0a8d4b48ed51ac3c0807c77515455550c22.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 2: Limitations of existing OVPS methods in predicting unseen categories. (a) Lack of generalization: Classification of a \u201cdog\u2019s parts\u201d involving categories like \u201ccats\u201d and \u201csheep\u201d, \u201cdog\u2019s tail\u201d misclassified as \u201csheep\u2019s ear\u201d. (VLPart [40]) (b) Ambiguous boundaries: Vague boundary output of \u201caeroplane\u2019s body\u201d. (c) Missing underrepresented parts: Neglecting parts such as \u201cbeak\u201d and \u201cleg\u201d. (CLIPSeg [32, 46]). ", "page_idx": 1}, {"type": "text", "text": "However, through empirical analysis of existing OVPS methods, we observed several common limitations in Figure 2. (Lack of generalization in (a)) Despite understanding part-level information, they often misidentify parts at the object level, e.g., a \u201cdog\u2019s leg\u201d as a \u201ccat\u2019s leg\u201d. Also, part-level misclassification occurs as the knowledge of parts in the base class fails to generalize to a novel class, e.g., \u201cdog\u2019s tail\u201d as a \u201csheep\u2019s ear\u201d. (Ambiguous boundaries of parts in (b)) They fail to maintain non-overlapping relationships between parts, frequently resulting in overlaps, e.g., an \u201cairplane\u2019s wing\u201d overlapping with its \u201cbody\u201d or the presence of empty spaces where no part is predicted. (Missing underrepresented parts in (c)) They ignore small and less frequent parts, causing prediction bias based on part size. ", "page_idx": 1}, {"type": "text", "text": "To overcome these limitations, we propose a novel framework called PartCLIPSeg, which consists of three main components. First, we devise generalized parts with object-level contexts to address the lack of generalization issue as the upper side of Figure 1. It explicitly obtains object-level and part-level pseudo-labels from VLMs and trains the OVPS model to satisfy both types of supervision. This guides the model to learn object boundaries while recognizing both part and object-level classes. Then, we suggest an attention control for minimizing the overlap between predicted parts, ensuring that parts are clearly separated as the lower side of Figure 1. In this way, we effectively leverage internal part information to learn ambiguous part boundaries. Finally, we enhance the activation related to certain parts by normalizing the activation scale of CLIP\u2019s self-attention information. It prevents small and less frequent areas from being ignored in pseudo-labels. This strategy ensures that the smallest granularity levels are retained in the final prediction. Through these three modules, PartCLIPSeg effectively addresses the challenges of existing OVPS methods and achieves robust multi-granularity segmentation. As a result, the proposed method achieves significant improvements in mIoU for both unseen and the harmonic mean when compared to previous state-of-the-art methods on Pascal-Part-116, ADE20K-Part-234, and PartImageNet in both Pred-All and Oracle-Obj settings. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Open-Vocabulary Semantic Segmentation. Open-vocabulary [19, 55] semantic segmentation (OVSS) goes beyond traditional semantic segmentation, which is restricted to predefined categories, by enabling predictions for unseen classes. Pioneering works focused on aligning predefined text embeddings with pixel-level visual features [4, 48, 56]. By leveraging large-scale Vision-Language Models (VLMs) like CLIP [38] and ALIGN [24], OVSS enables zero-shot segmentation through rich multi-modal features learned from extensive image-text pairs. MaskCLIP [58] modified CLIP\u2019s image encoder to directly handle visual and text features for segmenting novel classes. Some works proposed two-stage strategy [15, 16, 18, 20, 29, 30, 51, 52]: first, models generate class-agnostic mask proposals [9, 10]; then, a pre-trained VLM predicts the category for each region. Some studies have introduced diffusion models to improve mask generation quality [51] or fine-tuned CLIP to enhance classification capabilities [20, 29]. Other studies have adopted a single-stage framework [11, 27, 32, 49, 54, 59]. They use pre-trained CLIP models to align pixel-level visual features with text features. CLIPSeg [32] adds a transformer-based pixel decoder with a FiLM [17] module to fuse multi-modal features. ZegCLIP [59] enhances segmentation by incorporating learnable tokens. SAN [53] adopted a side adapter network for a CLIP-aware end-to-end approach to predict proposal-wise classification. FC-CLIP [54] uses a frozen convolutional CLIP to predict class-agnostic masks and classifies using mask-pooled features [54]. CAT-Seg [11] and SED [49] generate pixellevel cost maps and refine them for segmentation. ", "page_idx": 2}, {"type": "text", "text": "Part Segmentation. Part segmentation aims to identify the individual parts of objects, a task that is more complex and costly due to the smaller and more diverse nature of parts compared to whole objects. To tackle this, various datasets like Pascal-Part [7], PartImageNet [21], ADE20k-Part [57], Cityscapes-Panoptic-Part [13], and PACO [2] provide diverse and detailed part annotations. Earlier studies [7, 12, 22, 23, 43] used self-supervised constraints and contrastive settings for effective part-level entity segmentation. Recent studies extended this to open-vocabulary scenarios [35, 40, 46], opening new avenues for handling diverse parts. By leveraging class-agnostic detectors [35] and Vision-Language Models like CLIP [40, 46], part segmentation has extended its generalization ability to unseen parts. Our work builds upon and extends methodologies from these studies. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As illustrated in Figure 2, we identified three primary challenges of open-vocabulary part segmentation (OVPS): lack of generalization, overlapping parts, and missing underrepresented parts. Recognizing object-specific parts (such as \u201cdog\u2019s torso\u201d) cannot be determined solely by looking at each part in isolation; it is imperative to consider both generalized part information and the overall context of the object. Furthermore, some parts may have overlapping meanings across different granularity labels (e.g., \u201ceye\u201d, \u201cface\u201d, and \u201chead\u201d). This implies that predictions should consider direct guidance for each part as well as the relationships between different parts. These intricate spatial and functional dependencies between parts are crucial for achieving a holistic understanding and precise predictions in fine-grained entity segmentation tasks. ", "page_idx": 2}, {"type": "text", "text": "Based on this motivation, we propose a novel OVPS method, PartCLIPSeg. This method leverages generalized part information combined with object-level context to tackle the lack of generalization problem (see Section 3.2). Also, we directly minimize the overlap among part predictions to improve the part boundaries (see Section 3.3.1). Finally, we normalize the scale of attention activation from various parts for handling missing underrepresented parts (see Section 3.3.2). The overall architecture of our method is shown in Figure 3. ", "page_idx": 2}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/2f3b8248af794e9311f45ec5b19466e9898b0e9d9057b4e45ad0eb9826f64285.jpg", "img_caption": ["Figure 3: The overall architecture of PartCLIPSeg. The embeddings derived from the object category name and the part category name are conditioned using the FiLM operation. Each embedding, modified through attention control, is subsequently reconstructed to predict the final object-specific part results. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "OVPS aims to segment an image into a set of object-specific part categories $\\mathbf{C}_{\\mathrm{obj-part}}^{\\mathrm{test}}$ (e.g., \u201cdog\u2019s head,\u201d \u201ccar\u2019s front\u201d) in the test set, where the image is , and $H$ and $W$ are the height and width. During training, image-mask pairs $\\lbrack(\\overline{{\\mathcal{L}}}_{k},\\mathcal{M}_{k})\\}$ are used, consisting of images $\\mathcal{T}_{k}$ and corresponding ground-truth mask $\\mathcal{M}_{k}$ which only contains the object-specific part categories Ctorbaji-npart (e.g., \u201ccat\u2019s head,\u201d \u201cbus\u2019s front\u201d) in the train set. ", "page_idx": 3}, {"type": "text", "text": "Zero-Shot Part Segmentation. Open-vocabulary is a generalized zero-shot task, allowing the zero-shot segmentation protocol to evaluate zero-shot part segmentation performance. In this setting, train and test category names are divided into seen (base) and unseen (novel) sets, respectively, with disjoint object-specific category names; $\\left\\{{\\bf C}_{\\mathrm{obj-part}}^{\\mathrm{unseen}}\\cap{\\bf C}_{\\mathrm{obj-part}}^{\\mathrm{seen}}=\\emptyset\\right\\}.$ ", "page_idx": 3}, {"type": "text", "text": "Cross-Dataset Part Segmentation. In this setting, the model is trained on one dataset and evaluated on another without fine-tuning. This means that the category names of the train and test sets come from different datasets, denoted as $\\mathbf{C}_{\\mathrm{obj-part}}^{\\mathrm{train}}\\neq\\mathbf{C}_{\\mathrm{obj-part}}^{\\mathrm{test}}$ . Considering the domain gap between the datasets, such as differences in granularity, this setting is more challenging. ", "page_idx": 3}, {"type": "text", "text": "3.2 Generalized Parts with Object-level Contexts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the lack of generalization problem, we propose leveraging generalized parts with objectlevel contexts. The concept of generalized parts involves identifying and utilizing common structural components that are shared across different object-level categories. For instance, many animals have parts like \u201chead\u201d or \u201ctorso\u201d which, although functionally and visually distinct, may share certain underlying characteristics. By introducing generalized parts from object-specific parts, our PartCLIPSeg can efficiently recognize and segment these object-specific parts across diverse object classes, significantly enhancing the model\u2019s ability to generalize from seen to unseen categories. ", "page_idx": 3}, {"type": "text", "text": "Although generalized parts help distinguish the part-level categories, the visual information of a part may not suffice for accurately classifying their object-level categories. For instance, predicting the \u201cleg\u201d part of an animal can be challenging to identify when solely examining the part as it may not clearly indicate to which animal it belongs. For this reason, there have been attempts to incorporate object-level guidance [33, 40, 46] in part segmentation. However, object-level guidance without a generalized part may lose contextual information and miss hierarchical relationships. ", "page_idx": 3}, {"type": "text", "text": "By integrating object contexts with generalized parts, PartCLIPSeg employs object-level guidance that captures the holistic essence of the object to which parts belong. This integration allows for a more precise understanding and classification of parts, improving the overall performance of OVPS. ", "page_idx": 3}, {"type": "text", "text": "Object and Part Embedding Generation. We modified the architecture of CLIPSeg [32, 46], which adopted CLIP [38] encoder-decoder architecture for semantic segmentation. However, it is worth noting that our approach of utilizing generalized parts with object-level context is orthogonal to other previously proposed object-level segmentation methods [5, 26, 28, 39]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The proposed approach begins by parsing an object-specific part category name, $\\mathbf{c}_{\\mathrm{obj-part}}\\in\\mathbf{C}_{\\mathrm{obj-par}}$ rt, into separate components: an object category name $(\\mathbf{c}_{\\mathrm{{obj}}})$ and a generalized part category name $(\\mathbf{c}_{\\mathrm{part}})$ , e.g., \u201ccat\u201d and \u201ctorso\u201d. Then, the CLIP text encoder, $\\bar{\\mathrm{CLIP}}_{T}^{*}(\\cdot)$ , is used to transform these category names into their respective CLIP embeddings (eoTbj and epTart). It will condition the image features, eI, derived from the CLIP image encoder, $\\mathrm{CLIP}_{\\mathcal{T}}^{*}(\\cdot)$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{[\\mathrm{obj}\\,|\\,\\mathrm{part}]}^{\\mathcal{T}}=\\mathrm{CLIP}_{\\mathcal{T}}^{*}(\\mathbf{c}_{[\\mathrm{obj}\\,|\\,\\mathrm{part}]}),\\mathbf{e}^{\\mathcal{Z}}=\\mathrm{CLIP}_{\\mathcal{Z}}^{*}(\\mathcal{Z}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where \u2217denotes frozen pre-trained models. By using Feature-wise Linear Modulation (FiLM) [36, 42], each category name embeddings respectively modulate the image features as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{[\\mathrm{obj}\\,|\\,\\mathrm{part}]}^{\\mathcal{Z}}=\\mathbf{e}^{\\mathcal{Z}}\\oplus\\mathrm{FiLM}(\\mathbf{e}_{[\\mathrm{obj}\\,|\\,\\mathrm{part}]}^{\\mathcal{T}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\oplus$ is an element-wise sum. FiLM is an adaptive affine transformation widely used for multimodal or conditional tasks. It helps retrieve adequate conditioning for the image features. The modulated image features, $\\mathbf{e}_{[\\mathrm{obj}\\;|\\;\\mathrm{part}]}^{\\mathcal{T}}$ , corresponding to each object and part category name, pass through a decoder module. The decoder module will be discussed in detail in Section 3.3. They then proceed through a transposed convolution model. Finally, the output mask of the object $\\hat{s}^{o}$ and part $\\hat{s}^{p}$ are evaluated with ground-truth mask of objects, $s^{o}$ , and parts, $s^{p}$ . Oracle supervision for the object and parts mask is simply computed from a combination of object-specific parts annotations: $s\\in\\mathcal{M}$ . ", "page_idx": 4}, {"type": "text", "text": "Object-specific Part Construction. We utilize previously computed generalized part embeddings $(\\mathbf{e}_{\\mathrm{part}}^{\\mathcal{T}},\\mathbf{e}_{\\mathrm{part}}^{\\mathcal{T}})$ rt) and object embeddings (eoIbj, $(\\mathbf{e}_{\\mathrm{obj}}^{\\mathcal{T}},\\mathbf{e}_{\\mathrm{obj}}^{\\mathcal{T}})$ to reconstruct object-specific part embeddings. This process involves separate operations on modulated image features and category name embeddings. ", "page_idx": 4}, {"type": "text", "text": "Initially, we project the concatenated results of the object category name with the generalized part category name. This is to synthesize the embeddings for the target object-specific part category name. The approach ensures that the resultant embeddings are highly representative of parts and contextually relevant. The equivalent operation is applied to both object-level image features and part-level image features to generate object-specific image features as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{\\mathrm{obj-part}}^{[\\mathcal{T}|\\mathcal{Z}]}=\\mathsf{P r o j}(\\left[\\mathbf{e}_{\\mathrm{obj}}^{[\\mathcal{T}|\\mathcal{Z}]}\\mid\\mathbf{e}_{\\mathrm{part}}^{[\\mathcal{T}|\\mathcal{Z}]}\\right]).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The resulting object-specific part embeddings are further refined by a FiLM process. Combined with the respective object-specific image features, final modulated object-specific part embeddings, $\\mathbf{e}_{\\mathrm{obj-part}}$ is computed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{\\mathrm{obj-part}}=\\mathbf{e}_{\\mathrm{obj-part}}^{\\mathcal{T}}\\oplus\\mathrm{FiLM}(\\mathbf{e}_{\\mathrm{obj-part}}^{\\mathcal{T}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These embeddings are then processed through a deconvolution layer to produce the final segmentation masks $s\\in\\mathcal{M}$ . This step ensures that the embeddings are precisely aligned to enhance the definition and accuracy of the object-specific part masks. It effectively bridges the gap between object and part-level categorical information with object-specific parts information. ", "page_idx": 4}, {"type": "text", "text": "Object, Part, and Object-specific Part Mask Supervision. The mask supervision is provided for three distinct categories: object-specific parts, objects, and generalized parts. This multi-faceted supervision enables our model to effectively disentangle generalized parts from objects, thereby facilitating a more nuanced learning process for OVPS. This disentanglement is crucial for the model to accurately recognize and differentiate between various object categories and their corresponding parts. It enhances the model\u2019s ability to handle complex segmentation tasks with unseen objectspecific parts. The overall mask guidance loss can be defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{mask}}=\\sum_{i=1}^{|\\mathbf{C}_{\\mathrm{obj-par}}|+1}\\underbrace{\\{1-\\mathrm{BCE}\\bigl(s_{i},\\hat{s}_{i}\\bigr)\\}}_{\\mathrm{object-specitic~part}}+\\lambda_{\\mathrm{obj}}\\sum_{i=1}^{|\\mathbf{C}_{\\mathrm{obj}}|+1}\\underbrace{\\{1-\\mathrm{BCE}\\bigl(s_{i}^{o},\\hat{s}_{i}^{o}\\bigr)\\}}_{\\mathrm{object~guidance}}+\\lambda_{\\mathrm{part}}\\sum_{i=1}^{|\\mathbf{C}_{\\mathrm{part}}|}\\underbrace{\\{1-\\mathrm{BCE}\\bigl(s_{i}^{p},\\hat{s}_{i}^{p}\\bigr)\\}}_{\\mathrm{generalized~part~guidance}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $|\\mathbf{C}_{\\mathrm{obj-part}}|+1$ and $|\\mathbf{C}_{\\mathrm{{obj}}}|+1$ are for uncategory (or background) prediction. The disentangled object and part generalization with object-specific parts guidance provides a clue to the lack of generalization problem. ", "page_idx": 4}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/28cea80f5f8a2ba57220d8238226236cf00b7035df783a8997c4df037a83ae39.jpg", "img_caption": ["Figure 4: Example of attention control using separation and enhance losses. The proposed method manipulates attention maps to accurately identify and segment small parts. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 Attention Control for Ambiguity and Omission ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we address the previously mentioned challenges: (1) ambiguity in part boundaries and (2) omission of small or infrequently appearing parts. The main reason for these challenges is the incomplete guidance from knowledge-based, multi-granularity characteristics of parts. To overcome these, we adopt unsupervised methods traditionally used in fine-grained recognition and part discovery studies [7, 12, 43]. Specifically, we utilize approaches for adjusting self-attention activation inspired by the recent diffusion methods [6, 25, 41]. ", "page_idx": 5}, {"type": "text", "text": "We assume that the distribution of self-attention activation maps for visual tokens belonging to the same object-specific part mask should exhibit inter-similarity characteristics [41], implying similar distributions. To this end, we first compute the average self-attention map $\\mathcal{A}_{\\mathcal{M}_{\\mathbf{c}}}$ for each objectspecific part mask $\\mathcal{M}_{\\bf c}$ , where $\\mathbf{c}\\in\\mathbf{C}_{\\mathrm{obj-part}}$ represents an object-specific part category. This is done by summing the self-attention activation maps from channels specifically corresponding to object $\\mathbf{c}_{\\mathrm{obj}}$ and part $\\mathbf{c}_{\\mathrm{part}}$ , across all spatial tokens $(h,w)$ within the mask, as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{A}_{\\sf{M}_{\\mathrm{c}}}=\\frac{1}{|\\mathcal{M}_{\\mathrm{c}}|}\\sum_{\\substack{(h,w)\\in\\mathcal{M}_{\\mathrm{c}}}}\\left(\\mathcal{A}_{\\sf c o b j}[h,w,\\vdots,:]+\\mathcal{A}_{\\sf c p a r}[h,w,\\vdots,:]\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Subsequently, the self-attention map $\\mathcal{A}_{\\mathcal{M}_{\\bf c}}$ for the object-specific part mask is refined through minmax normalization, followed by the application of a Gaussian fliter to smooth the initial activation as in [6, 50]. Therefore, the dimensions of both the original and normalized self-attention maps for the object-specific part masks are as follows: $\\mathcal{A}_{\\mathcal{M}_{\\mathrm{c}}},\\mathcal{A}_{\\mathcal{M}_{\\mathrm{c}}}^{\\tt n o r m}\\in\\mathbb{R}^{H\\times W}$ . ", "page_idx": 5}, {"type": "text", "text": "3.3.1 Minimizing Part Overlaps for Ambiguity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the self-attention of the decoder layers, competition between object-specific parts helps define boundaries that cannot be sufficiently established by supervision alone. Using the previously obtained normalized attention map, our method generates parts with minimized intersections, inspired by [1, 3, 25, 37, 47]. This approach effectively mitigates the ambiguity issue in part boundaries. Specifically, the normalized attention activation map $\\mathcal{A}_{\\mathcal{M}_{\\mathbf{c}}}^{\\mathrm{norm}}$ is first binarized based on an arbitrary threshold $\\gamma$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{B}_{\\mathcal{M}_{\\mathbf{c}}}(h,w)=\\mathbf{1}_{\\{\\mathcal{A}_{\\mathcal{M}_{\\mathbf{c}}}^{\\mathrm{norm}}(h,w)\\geq\\gamma\\}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $B_{\\mathcal{M}_{\\mathbf{c}}}$ denotes binarized attention map for part mask $\\mathcal{M}_{\\bf c}$ . From now on, $\\mathbf{C}_{\\mathrm{obj-part}}$ is simply denoted as $\\mathbf{C}$ . The separation loss $\\mathcal{L}_{\\mathrm{sep}}$ , which indicates the degree of intersection between objectspecific parts, is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{sep}}=\\frac{1}{|\\mathbf{C}|}\\left|\\frac{\\left\\{(h,w)~\\middle|~\\sum_{\\mathbf{c}\\in\\mathbf{C}}\\mathcal{B}_{\\mathcal{M}_{\\mathbf{c}}}(h,w)>1\\right\\}}{\\left\\{(h,w)~\\middle|~\\sum_{\\mathbf{c}\\in\\mathbf{C}}\\mathcal{B}_{\\mathcal{M}_{\\mathbf{c}}}(h,w)\\geq1\\right\\}}\\right|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where separating activation mitigates the challenge of ambiguous boundaries between parts. ", "page_idx": 5}, {"type": "text", "text": "3.3.2 Enhancing Part Activation for Omission ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To address the omission problem, we employ a method inspired by attention controls in modern diffusion-based approaches [3, 6]. This approach enhances the activation within the self-attention ", "page_idx": 5}, {"type": "text", "text": "activation map to enhance underrepresented parts before normalization. Specifically, for each objectspecific part mask, the maximum value within the attention map is identified. Subsequently, among all object-specific parts, the minimum activation of the part with the maximum value is enhanced as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{enh}}=1-\\operatorname*{min}_{\\mathbf{c}\\in\\mathbf{C}}\\left(\\operatorname*{max}_{(h,w)\\in\\mathcal{M}_{\\mathbf{c}}}\\mathcal{A}_{\\mathcal{M}_{\\mathbf{c}}}[h,w]\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "thereby boosting its representational efficacy. In this way, the enhancement loss $\\mathcal{L}_{\\mathrm{enh}}$ provides sufficient guidance for small or infrequently occurring parts, effectively mitigating the omission problem. ", "page_idx": 6}, {"type": "text", "text": "The training objective for PartCLIPSeg integrates three key loss components as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{al1}}=\\mathcal{L}_{\\mathrm{mask}}+\\lambda_{\\mathrm{sep}}\\mathcal{L}_{\\mathrm{sep}}+\\lambda_{\\mathrm{enh}}\\mathcal{L}_{\\mathrm{enh}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where (1) $\\mathcal{L}_{\\mathtt{m a s k}}$ for generalized parts with object-level context, (2) $\\mathcal{L}_{\\mathrm{sep}}$ for addressing ambiguous boundaries, (3) $\\mathcal{L}_{\\mathrm{enh}}$ for handling missing underrepresented parts, and $\\lambda_{\\mathtt{s e p}}$ and $\\lambda_{\\mathrm{enh}}$ are hyperparmeters. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We evaluate our method on three part segmentation datasets: Pascal-Part-116 [7, 46], ADE20K-Part-234 [46, 57], and PartImageNet [21]. Pascal-Part-116 [7, 46] consists of 8,431 training images and 850 test images. It is a modified version of PascalPart [7] by removing direction indicators for certain part classes and merging them to avoid overly complex part definitions. This dataset contains a total of 116 object part classes across 17 object categories. ADE20K-Part-234 [46, 57] consists of 7,347 training images and 1,016 validation images. It provides instance-level object mask annotations along with their corresponding part mask annotations, including 44 objects and 234 parts. PartImageNet [21] contains 16k training images and $2.9\\mathbf{k}$ validation images, segmented into 158 object classes from ImageNet [14] and organizes them into 11 super-categories. For this study, we select 40 object classes that represent common categories to assess cross-dataset performance effectively. More details about the datasets can be found in the supplementary materials. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Protocols. We use two evaluation protocols for the performance of OVPS: (1) Pred-All setting, where the ground truth object-level mask and object class are not provided, and (2) OracleObj setting, where the ground truth object-level mask and object class are known. In particular, the Pred-Obj setting in OV-PARTS [46] uses predicted masks from the off-the-shelf segmentation model. In contrast, our Pred-All setting is a more challenging and practical setting because it does not rely on additional predicted masks or foundation models but solely uses the predicted object masks from the proposed model. For both evaluation protocols, we used mean Intersection over Union (mIoU) as an evaluation metric, which is widely used to measure segmentation performance. Additionally, we utilized the harmonic mean of the results from the seen and unseen categories as the final evaluation metric. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We build upon CLIPSeg [32, 46], a CLIP-based encoder-decoder model.   \nThe implementation details can be found in the supplementary material. ", "page_idx": 6}, {"type": "text", "text": "4.2 Performance Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Zero-Shot Part Segmentation. We compare our PartCLIPSeg to previous methods [11, 32, 46, 52] on three OVPS benchmarks [7, 57]. As shown in Table 1, PartCLIPSeg consistently outperforms previous approaches by significant margins on Pascal-Part-116, demonstrating its zero-shot ability, with performance improvements of $3.94\\%$ in the Pred-All setting and $3.55\\%$ in the Oracle-Obj setting. The more challenging ADE20K-Part-234 dataset, which is a fine-grained segmentation dataset, further highlights the effectiveness of PartCLIPSeg. As shown in Table 2, PartCLIPSeg achieves a harmonic mean mIoU of $11.38\\%$ in the Pred-All setting, outperforming the best-performing baseline by $7.85\\%$ . In the Oracle-Obj setting, it achieves $38.60\\%$ , which is $4.45\\%$ higher than the best baseline. Notably, PartCLIPSeg shows significant performance improvement in unseen categories, demonstrating its strong generalizability. Considering that performance in unseen categories is crucial in a zero-shot scenario, these results are significant despite some performance degradation in seen categories. We also evaluated PartCLIPSeg on PartImageNet. According to Table 3, PartCLIPSeg shows a notable improvement over CLIPSeg. ", "page_idx": 6}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/6cabbfe7504e8a78ed5c6ac3b5417f052b5ebc0492251a93a40777fd6e554723.jpg", "table_caption": ["Table 1: Comparison of zero-shot performance with state-of-the-art methods on Pascal-Part-116. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/7bc74e1f1bbd19aa8607024df9398b00b3fb6112a206b47e74efbdbc274c1753.jpg", "table_caption": [], "table_footnote": ["1 The best score is bold and the second-best score is underlined. The standard error of an average of 5 results is reported. These are the same for all experiments. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We present the segmentation results of PartCLIPSeg in comparison to state-of-the-art open-vocabulary part segmentation methods [11, 32, 40] on Pascal-Part-116. Specifically, we focus on qualitative performance on unseen categories such as \u201cdog\u201d, \u201csheep\u201d, \u201ccar\u201d, and \u201cbird\u201d. As shown in Figure 5 for the Pred-All and Figure 6 for the Oracle-Obj setting, the proposed method effectively segments target parts regardless of the need for predefined masks during inference. Notably, PartCLIPSeg excels at identifying smaller, often overlooked part classes such as \u201ceye\u201d, \u201ctail\u201d, and \u201cheadlight\u201d. Additionally, our method effectively segments multiple objects and their respective parts, a challenge for other methods, demonstrating the effectiveness of PartCLIPSeg in zero-shot part segmentation. Its improved performance on unseen categories and higher accuracy in challenging environments highlight the robustness and generalization capabilities of PartCLIPSeg. Consistent improvements on Pascal-Part-116, ADE20K-Part-234, and PartImageNet demonstrate that PartCLIPSeg sets a new standard in open-vocabulary part segmentation. ", "page_idx": 7}, {"type": "text", "text": "Cross-Dataset Part Segmentation. Table 4 validated the efficacy of our approach in a cross-dataset setting, where category names, annotation style, and granularity of mask may vary. Additionally, unlike zero-shot situations within the same dataset, there are differences in the types and diversity of parts. Initially, we trained our model on PartImageNet and ADE20K-Part-234 respectively. Subsequent tests on Pascal-Part-116 [7, 46] showed that PartCLIPSeg outperforms CLIPSeg in both the Pred-All and Oracle-Obj settings, confirming our method\u2019s superiority on generalization in different datasets. ", "page_idx": 7}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/23a15e4b36f315a373b07e4ce8936e127dd09566369e7bd71c5dd99d4b99861e.jpg", "table_caption": ["Table 4: Cross-dataset performance. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/a08a2376575a513e900cfa629f1e4d3cefb7bbbc476dd5845692f608bcfc1367.jpg", "table_caption": ["Table 5: Impact of attention control losses. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/1d772b4c95dc273123661676d09cc8d9f143d595c405f023782fac5365d1ae0a.jpg", "img_caption": ["(a) Ground-truth (b) VLPart [40] (c) CLIPSeg [38, 46] (d) CAT-Seg [11, 46] (e) PartCLIPSeg (Ours) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Qualitative results of zero-shot part segmentation on Pascal-Part-116 in Pred-All setting.   \nAnnotations for unseen categories (bird, car, dog, sheep, etc.) are not included in the train set. ", "page_idx": 8}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/a35e7ddd71e0bc20cabc5b73341e4213f59eb46ed024b5ea670b055ffae254fe.jpg", "table_caption": ["Table 6: Performance on mean Boundary IoU $(\\uparrow)$ on Pascal-Part-116 in Oracle-Obj setting. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/22bf5d6c97042ff93a5439f5be4720d7bba83b9b0fb1f2833155d0d6bfb967dd.jpg", "table_caption": ["Table 7: Impact of PartCLIPSeg for small parts on Pascal-Part-116 in Oracle-Obj setting. (mIoU) "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we analyze the impact of each training loss on PartCLIPSeg. We focus on the roles of the separation and enhancement losses, examining how they contribute to improved segmentation accuracy. ", "page_idx": 8}, {"type": "text", "text": "Separation & Enhancement Losses. We conducted an ablation study to investigate the effect of the separation loss $\\mathcal{L}_{\\mathrm{sep}}$ and the enhancement loss $\\mathcal{L}_{\\mathrm{enh}}$ on the performance of PartCLIPSeg in Table 5. On Pascal-Part-116, eliminating both losses resulted in a lower harmonic mean of 29.20 in Pred-All and a harmonic mean of 38.20 in Oracle-Obj. Introducing $\\mathcal{L}_{\\mathrm{sep}}$ without ${\\mathcal{L}}_{\\mathrm{enh}}$ improved the harmonic mean in both Pred-All and Oracle-Obj setups. Using both losses led to the highest harmonic means of 30.67 and 38.79, respectively. Similarly, for ADE20K-Part-234, employing both losses resulted in the best performance, with harmonic means of 19.63 in Pred-All and 38.60 in Oracle-Obj. These results highlight the importance of both separation and enhancement losses in improving performance. ", "page_idx": 8}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/2804430676fd125dd8e57c4249cf39649d9f4d4f1323422a0210f645607ffd63.jpg", "img_caption": ["(a) Ground-truth (b) VLPart [40] (c) CLIPSeg [38, 46] (d) CAT-Seg [11, 46] (e) PartCLIPSeg (Ours) ", "Figure 6: Qualitative results of zero-shot part segmentation on Pascal-Part-116 in Oracle-Obj setting. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "To verify the effectiveness of boundary creation of PartCLIPSeg, we examined an additional qualitative metric, Boundary IoU [8]. The results demonstrated high Boundary IoU performance, confirming that PartCLIPSeg effectively resolves ambiguous boundary issues as shown in table 6. ", "page_idx": 9}, {"type": "text", "text": "Impact of PartCLIPSeg for Underrepresented Parts. We investigate the effect of the enhancement loss ${\\mathcal{L}}_{\\mathrm{enh}}$ on OVPS model performance, especially with respect to underrepresented parts. In Table 7, we compare our PartCLIPSeg with CLIPSeg [32, 46] on small parts such as \u201ceye\u201d, \u201cneck\u201d, and \u201cleg\u201d of animals in Pascal-Part-116. As shown in the table, PartCLIPSeg consistently outperforms CLIPSeg with significant improvements in most cases. Notably, there is an impressive performance increase of $35.93\\%\\mathrm{p}$ for \u201ccow\u2019s leg\u201d. These improvements highlight the effectiveness of the enhancement loss in accurately segmenting small and intricate parts, demonstrating its crucial role in improving overall performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we introduced PartCLIPSeg, a state-of-the-art OVPS method that addresses three primary challenges in OVPS. PartCLIPSeg utilizes generalized parts and object-level guidance to effectively solve identification issues. Then, it separates parts by minimizing their overlaps in attention maps, thus learning ambiguous part boundaries. Additionally, we implemented an enhancement loss function to improve the detection of underrepresented parts. Through extensive experimentation, we have confirmed the superior performance of PartCLIPSeg. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the MSIP (NRF-2022R1A2C3011154, RS-2023-00219019), KEIT grant funded by the Korean government (MOTIE) (No. 2022-0-00680, No. 2022-0-01045), the IITP grant funded by the Korean government (MSIT) (No. 2021-0-02068 Artificial Intelligence Innovation Hub, RS-2019-II190075 Artificial Intelligence Graduate School Program (KAIST)), and Samsung Electronics Co., Ltd (IO230508-06190-01). ", "page_idx": 10}, {"type": "text", "text": "Bibliography ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Aishwarya Agarwal, Srikrishna Karanam, KJ Joseph, Apoorv Saxena, Koustava Goswami, and Balaji Vasan Srinivasan. A-star: Test-time attention segregation and retention for text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2283\u20132293, 2023. [2] Juan Antonio Balbuena, Ra\u00fal M\u00edguez-Lozano, and Isabel Blasco-Costa. Paco: a novel procrustes application to cophylogenetic analysis. PloS one, 8(4):e61048, 2013. [3] Zhipeng Bao, Yijun Li, Krishna Kumar Singh, Yu-Xiong Wang, and Martial Hebert. Separateand-enhance: Compositional finetuning for text-to-image diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 1\u201310, 2024. [4] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick P\u00e9rez. Zero-shot semantic segmentation. Advances in Neural Information Processing Systems, 32, 2019. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021. [6] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):1\u201310, 2023. [7] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what you can: Detecting and representing objects using holistic models and body parts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1971\u20131978, 2014. [8] Bowen Cheng, Ross Girshick, Piotr Doll\u00e1r, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15334\u201315342, 2021. [9] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. Advances in neural information processing systems, 34: 17864\u201317875, 2021.   \n[10] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1290\u20131299, 2022.   \n[11] Seokju Cho, Heeseong Shin, Sunghwan Hong, Seungjun An, Seungjun Lee, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. arXiv preprint arXiv:2303.11797, 2023.   \n[12] Subhabrata Choudhury, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Unsupervised part discovery from contrastive reconstruction. Advances in Neural Information Processing Systems, 34:28104\u201328118, 2021.   \n[13] Daan de Geus, Panagiotis Meletis, Chenyang Lu, Xiaoxiao Wen, and Gijs Dubbelman. Partaware panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5485\u20135494, 2021.   \n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[15] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11583\u201311592, 2022.   \n[16] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary universal image segmentation with maskclip. arXiv preprint arXiv:2208.08984, 2022.   \n[17] Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville, and Yoshua Bengio. Feature-wise transformations. Distill, 3(7):e11, 2018.   \n[18] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In European Conference on Computer Vision, pages 540\u2013557. Springer, 2022.   \n[19] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.   \n[20] Kunyang Han, Yong Liu, Jun Hao Liew, Henghui Ding, Jiajun Liu, Yitong Wang, Yansong Tang, Yujiu Yang, Jiashi Feng, Yao Zhao, et al. Global knowledge calibration for fast open-vocabulary segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 797\u2013807, 2023.   \n[21] Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xiaoding Yuan, Jie-Neng Chen, Shuai Liu, Cheng Yang, Qihang Yu, and Alan Yuille. Partimagenet: A large, high-quality dataset of parts. In European Conference on Computer Vision, pages 128\u2013145. Springer, 2022.   \n[22] Ju He, Jieneng Chen, Ming-Xian Lin, Qihang Yu, and Alan L Yuille. Compositor: Bottom-up clustering and compositing for robust part and object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11259\u201311268, 2023.   \n[23] Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz. Scops: Self-supervised co-part segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 869\u2013878, 2019.   \n[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n[25] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7701\u20137711, 2023.   \n[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015\u20134026, 2023.   \n[27] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren\u00e9 Ranftl. Languagedriven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022.   \n[28] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023.   \n[29] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7061\u20137070, 2023.   \n[30] Yong Liu, Sule Bai, Guanbin Li, Yitong Wang, and Yansong Tang. Open-vocabulary segmentation with semantic-assisted calibration. arXiv preprint arXiv:2312.04089, 2023.   \n[31] Zongdai Liu, Feixiang Lu, Peng Wang, Hui Miao, Liangjun Zhang, Ruigang Yang, and Bin Zhou. 3d part guided image editing for fine-grained object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11336\u201311345, 2020.   \n[32] Timo L\u00fcddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7086\u20137096, 2022.   \n[33] Umberto Michieli and Pietro Zanuttigh. Edge-aware graph matching network for part-based semantic segmentation. International Journal of Computer Vision, 130(11):2797\u20132821, 2022.   \n[34] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11): 39\u201341, 1995.   \n[35] Tai-Yu Pan, Qing Liu, Wei-Lun Chao, and Brian Price. Towards open-world segmentation of parts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15392\u201315401, 2023.   \n[36] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[37] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. arXiv preprint arXiv:2306.05427, 2023.   \n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[39] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024.   \n[40] Peize Sun, Shoufa Chen, Chenchen Zhu, Fanyi Xiao, Ping Luo, Saining Xie, and Zhicheng Yan. Going denser with open-vocabulary part segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15453\u201315465, 2023.   \n[41] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse, attend, and segment: Unsupervised zero-shot segmentation using stable diffusion. arXiv preprint arXiv:2308.12469, 2023.   \n[42] Mehmet Ozgur Turkoglu, Alexander Becker, H\u00fcseyin Anil G\u00fcnd\u00fcz, Mina Rezaei, Bernd Bischl, Rodrigo Caye Daudt, Stefano D\u2019Aronco, Jan Wegner, and Konrad Schindler. Film-ensemble: probabilistic deep learning via feature-wise linear modulation. Advances in neural information processing systems, 35:22229\u201322242, 2022.   \n[43] Robert van der Klis, Stephan Alaniz, Massimiliano Mancini, Cassio F Dantas, Dino Ienco, Zeynep Akata, and Diego Marcos. Pdisconet: Semantically consistent part discovery for finegrained recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1866\u20131876, 2023.   \n[44] Zifu Wan, Yaqi Xie, Ce Zhang, Zhiqiu Lin, Zihan Wang, Simon Stepputtis, Deva Ramanan, and Katia P Sycara. Instructpart: Affordance-based part segmentation from language instruction. In AAAI-2024 Workshop on Public Sector LLMs: Algorithmic and Sociotechnical Design, 2024.   \n[45] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. arXiv preprint arXiv:2402.03290, 2024.   \n[46] Meng Wei, Xiaoyu Yue, Wenwei Zhang, Shu Kong, Xihui Liu, and Jiangmiao Pang. Ov-parts: Towards open-vocabulary part segmentation. Advances in Neural Information Processing Systems, 36, 2024.   \n[47] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1206\u20131217, 2023.   \n[48] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero-and few-label semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8256\u20138265, 2019.   \n[49] Bin Xie, Jiale Cao, Jin Xie, Fahad Shahbaz Khan, and Yanwei Pang. Sed: A simple encoderdecoder for open-vocabulary semantic segmentation. arXiv preprint arXiv:2311.15537, 2023.   \n[50] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7452\u2013 7461, 2023.   \n[51] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2955\u20132966, 2023.   \n[52] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model. In European Conference on Computer Vision, pages 736\u2013753. Springer, 2022.   \n[53] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2945\u20132954, 2023.   \n[54] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip. Advances in Neural Information Processing Systems, 36, 2024.   \n[55] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14393\u201314402, 2021.   \n[56] Hang Zhao, Xavier Puig, Bolei Zhou, Sanja Fidler, and Antonio Torralba. Open vocabulary scene parsing. In Proceedings of the IEEE International Conference on Computer Vision, pages 2002\u20132010, 2017.   \n[57] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633\u2013641, 2017.   \n[58] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European Conference on Computer Vision, pages 696\u2013712. Springer, 2022.   \n[59] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11175\u201311185, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 Discussion Appendix A \u2013 Limitations & Future Work Appendix A.1 \u2013 Social Impact Appendix A.2   \n\u2022 Experimental Details Appendix B \u2013 Datasets Details Appendix B.1 \u2013 Implementation Details Appendix B.2 \u2013 Computational Resource Appendix B.3   \n\u2022 Additional Quantitative Evaluation Appendix C   \n\u2022 Additional Ablation Appendix D \u2013 Impact of Object-Level and Part-Level Guidance Appendix D.1 \u2013 Qualitative Ablation on Attention Control Losses Appendix D.2 \u2013 Ablation on the Hyperparameter in Attention Control Appendix D.3   \n\u2022 Additional Qualitative Results and Qualitative Analysis Appendix E \u2013 CLIP Embedding Appendix E.1 \u2013 Additional Qualitative Results Appendix E.2 ", "page_idx": 14}, {"type": "text", "text": "A Discussion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Limitations & Future Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We share some limitations of our model and outline directions for future research. Our model is based on semantic segmentation, which does not allow for the discrimination of individual parts as instances. Consequently, parts such as \u201cPaw 1\u201d from Dog 1 and \u201cPaw $2^{\\bullet}$ from $D o g\\;2$ are assigned the same label. We plan to address this limitation in our future work to enhance the model\u2019s capability to distinguish between similar parts from different instances. ", "page_idx": 14}, {"type": "text", "text": "Furthermore, we believe that adding more inductive biases related to the relationships between parts, similar to key point detection which incorporates structural understanding, could yield higher-quality results. ", "page_idx": 14}, {"type": "text", "text": "Currently, our focus has been on object-specific parts, essentially mapping different granularity of vocabulary visually. Advanced methods could allow us to more effectively handle a broader variety of input categories, further enhancing our model\u2019s applicability and performance. ", "page_idx": 14}, {"type": "text", "text": "A.2 Social Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This study explores open-vocabulary part segmentation, a technique that expands segmentation models to include fine-grained categories not encountered during training. The approach\u2019s robust nature allows for segmentation across various categories, proving invaluable for applications requiring flexibility and adaptability. ", "page_idx": 14}, {"type": "text", "text": "Open-vocabulary part segmentation could greatly influence several advanced fields. In robotics, for example, robots can precisely identify and handle a wide array of objects and components, essential for tasks from manufacturing assembly lines to complex medical surgeries. This adaptability allows robots to function in new settings without extensive retraining. ", "page_idx": 14}, {"type": "text", "text": "In healthcare, this technology enhances diagnostic processes by allowing for the segmentation of novel anatomical structures in medical imaging. This could facilitate earlier disease detection by identifying subtle, non-cataloged abnormalities essential for diagnosis. ", "page_idx": 14}, {"type": "text", "text": "In image editing, open-vocabulary part segmentation enables sophisticated manipulation by letting editors modify image fine-grained components not predefined in their software. This is especially beneficial in the creative industries, where precise adjustments can improve output quality and foster innovation. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Adopting open-vocabulary part segmentation promises to enhance the efficiency, accessibility, and effectiveness of these technologies, particularly in handling real-world variability and unpredictability. ", "page_idx": 15}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Datasets Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1.1 Pascal-Part-116 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the Pascal-Part-116 dataset [7, 46], we target the following object-specific category names in Table A1. Among these, \u201cbird\u201d, \u201ccar\u201d, \u201cdog\u201d, \u201csheep\u201d, and \u201cmotorbike\u201d are designated as unseen categories, encountered for the first time during inference in the zero-shot part segmentation setting. ", "page_idx": 15}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/9480a69f0f647c886e2c7e5f12a6d85b9f22d7cfe19769176e2422d84c4cdd45.jpg", "table_caption": ["Table A1: List of object-specific classes in Pascal-Part-116. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.1.2 ADE20K-Part-234", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the ADE20K-Part-234 dataset [57], we target specific object categories listed in Table A2. The dataset includes 44 object classes and detailed subdivisions into over 200 part categories. Notably, \u201cbench\u201d, \u201cbus\u201d, \u201cfan\u201d, \u201cdesk\u201d, \u201cstool\u201d, \u201ctruck\u201d, \u201cvan\u201d, \u201cswivel chair\u201d, \u201coven\u201d, \u201cottoman\u201d, and \u201ckitchen island\u201d are identified as novel classes and are encountered for the first time during inference in our zero-shot part segmentation setting. ", "page_idx": 15}, {"type": "text", "text": "B.1.3 PartImageNet ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "PartImageNet [21] is a dataset derived from ImageNet [14], consisting of approximately 24,000 images across 158 classes. Each class has annotations for parts. All classes belong to one of 11 superclasses, organized using the hierarchical information from WordNet [34]. ", "page_idx": 15}, {"type": "text", "text": "Previous open-vocabulary part segmentation research [40] primarily used PartImageNet to evaluate cross-dataset settings. In our study, we use PartImageNet not only for cross-dataset evaluation but also to assess model performance in zero-shot settings specific to PartImageNet. ", "page_idx": 15}, {"type": "text", "text": "To measure more generalized performance, we select 40 classes out of the 158. We maintain the proportion of existing superclasses as much as possible. For each superclass, at least $50\\%$ of the categories are designated as seen categories, with the remaining being unseen categories. Therefore, there are 25 seen classes and 15 unseen classes in our PartImageNet evaluation dataset. ", "page_idx": 15}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/388662128579624d48a7b328efeb17abcc32584cb620e23b09c9cf0e0a856764.jpg", "table_caption": ["Table A2: List of object-specific classes in ADE20K-Part-234. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "We conduct the dataset evaluation as follows: Models are trained on a training dataset composed of seen classes. Segmentation performance are then assessed on a validation dataset containing both seen and unseen classes. Evaluations were conducted in both Pred-All and Oracle-Obj settings. ", "page_idx": 16}, {"type": "text", "text": "Table A3: List of selected object classes per superclass. We choose 40 object classes from 158 categories to evaluate performance on PartImageNet and in a cross-dataset setting. Object categories that are both underlined and in bold represent the unseen classes, which are emphasized for their unique characteristics within each superclass. ", "page_idx": 16}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/c3a5ddb6f012512d13bf247e7cb8c0c0cd372194b5eea68b3f56792e9104b25b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/198000973025c9fd2fd00e13044b14e224a28747292478ddec126f504d03e9b7.jpg", "img_caption": ["Figure A1: Example of part annotations in PartImageNet on our experiment. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our model implementation is based on the CLIPSeg [32] architecture, as described in the OVPARTS [46]. We utilized the pre-trained CLIP ViT-B/16 [38, 59] image encoder and text encoder for our experiments. ", "page_idx": 17}, {"type": "text", "text": "The model is trained using the ADAMW optimizer with a base learning rate of 0.0001 over 20,000 iterations, with a batch size of 8 images. We employ a WarmupPolyLR learning rate scheduler to manage the learning rate throughout the training process. To ensure model stability, we apply gradient clipping with a maximum gradient norm of 0.01. ", "page_idx": 17}, {"type": "text", "text": "We save model parameters every 1,000 iterations during training. The best-performing parameters are selected based on the highest validation evaluation scores. For example, the evaluation result on the Pascal-Part-116 dataset in the Oracle-Obj setting is derived from the checkpoint saved at the 5,000-step mark, which yields the best validation performance. ", "page_idx": 17}, {"type": "text", "text": "We evaluated several baseline methods\u2014 $.Z S S e g+$ , CLIPSeg [32], and CAT-Seg [11]\u2014which are fine-tuned on our datasets. $Z\\mathrm{SSeg+}$ is a modified version of ZSseg [52], utilizing different fine-tuning methods according to [46]. It employs a ResNet-101c backbone and Compositional Prompt Tuning based on CoOp. ", "page_idx": 17}, {"type": "text", "text": "CLIPSeg and CAT-Seg models are pre-trained on object datasets; however, we fine-tuned these models on each part-level dataset. CAT-Seg, based on ResNet-101 and using ViT-B/16 as CLIP\u2019s visual encoder, achieved comparable performance by computing cost volumes and subsequently applying cost aggregation\u2014a process that enhances segmentation by aggregating matching costs between image features. Specifically, CAT-Seg uses the frozen upsampling decoder but fine-tuned CLIP\u2019s image and text encoders. Conversely, we fine-tune the CLIPSeg decoder to better identify small segments and define clear boundaries. CLIPSeg, based on the ViT-B/16 architecture, is finetuned on the visual adapter, text embeddings, and transformer decoder to enhance its segmentation capabilities. ", "page_idx": 17}, {"type": "text", "text": "B.3 Computational Resource ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All our experiments are conducted on $8\\times\\mathrm{{NVIDIA}}$ A6000 GPUs. ", "page_idx": 17}, {"type": "text", "text": "As shown in Table A4, PartCLIPSeg offers advantages in both the number of parameters and memory consumption compared to other baselines on the Pascal-Part-116 dataset. With 152.4 million parameters, it is more efficient than ZSSeg and CAT-Seg, and comparable to CLIPSeg. In terms of GPU memory usage, PartCLIPSeg requires 24.4 GB, which is lower than both CAT-Seg and CLIPSeg. ", "page_idx": 17}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/9dfdde3c6df229240abb12306b150a474264dcb3725306154963d3d78017b76d.jpg", "table_caption": ["Table A4: Computational resources on PascalPart-116 with batch size 8. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "For PartCLIPSeg, although the number of parameters is larger than CLIPSeg because of computations related to attention control, there is an advantage in not having to maintain weights for each objectspecific part due to the use of generalized parts. These efficiencies become more pronounced as the number of generalized parts shared among object classes increases. By leveraging shared representations for generalized parts, PartCLIPSeg reduces redundancy and memory requirements. This makes our model particularly advantageous in datasets where object classes have many common parts, leading to more efficient training and inference without compromising performance. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "C Additional Quantitative Evaluation ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/3ca1e6dec20a94a630a24b558d95e66ea10cf65813d505af9a25ec40de9c40f8.jpg", "table_caption": ["Table A5: Recall performance on Pascal-Part116 under the Oracle-Obj setting. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/dc30a3dd8f52b93096df692bb44547860f77644d7d631007826e25de4dd93c86.jpg", "table_caption": ["Table A6: Recall performance on ADE20K-Part234 under the Oracle-Obj setting. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "In this section, we present an additional evaluation metric that focuses on specific challenges within the Open-Vocabulary Part Segmentation (OVPS) task as shown in Figure 2. The Recall metric is used to assess how well the model captures underrepresented parts, addressing the challenge of underrepresented parts. Higher values in recall indicate that the model effectively captures these seldom-occurring parts, thereby addressing the challenge of underrepresented parts in OVPS. ", "page_idx": 18}, {"type": "text", "text": "PartCLIPSeg consistently achieves higher recall on both seen and unseen classes across both datasets as shown in Tables A5 and A6. The improved harmonic mean indicates that our model is more effective at identifying underrepresented parts, thereby addressing one of the core challenges in OVPS. ", "page_idx": 18}, {"type": "text", "text": "We further analyze the impact of the attention control losses $\\mathcal{L}_{\\mathrm{sep}}$ and $\\mathcal{L}_{\\mathrm{enh}}$ on the recall. By comparing the recall metric with and without these losses, we assess their effectiveness in enhancing the representation of seldom-occurring parts. From Tables A5 and A6, we observe that incorporating the attention control losses enhances the model\u2019s performance on unseen classes, which often include underrepresented parts. The increases in harmonic mean suggest that the attention control losses help the model to better capture these seldom-occurring or small parts. ", "page_idx": 18}, {"type": "text", "text": "D Additional Ablation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Impact of Object-Level and Part-Level Guidance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table A7: Ablation on $\\lambda_{\\mathsf{o b j}}$ , $\\lambda_{\\mathtt{p a r t}}$ , and attention control on Pascal-Part-116 in Oracle-Obj setting. ", "page_idx": 18}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/377bc16daacf21a446e8ad434a57653bd5293cd45107ebecd6ebe3c46be17ce5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "We conduct additional experiments to verify the impact of object-level and part-level label guidance on model performance as shown in Table A7. Specifically, we vary the weights $\\lambda_{\\mathsf{o b j}}$ and $\\lambda_{\\mathtt{p a r t}}$ in Equation (5), setting each to 0 or 1, to assess the influence of object-level and part-level supervision on the overall performance. Additionally, we evaluate the effect of the attention control losses, $\\mathcal{L}_{\\mathrm{sep}}$ and $\\mathcal{L}_{\\mathrm{enh}}$ , by including or excluding them. ", "page_idx": 18}, {"type": "text", "text": "As shown in Table A7, both object-level and part-level guidance positively impact model performance on the Pascal-Part-116 dataset under the Oracle-Obj setting. When neither object-level nor part-level supervision is applied, the harmonic mean is 36.58. Introducing object-level guidance alone increases the harmonic mean IoU to 38.07, while part-level guidance alone raises it to 38.46. Combining both guidances yields the best performance with a harmonic mean IoU of 38.79. ", "page_idx": 18}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/cf7c7f7b69e4144330d111486c3df3be5b56e40b9198df2babb7a292a93f4a06.jpg", "img_caption": ["Figure A2: Comparison of results using only $\\mathcal{L}_{\\mathtt{s e p}}$ (top) with both $\\mathcal{L}_{\\mathtt{s e p}}$ and $\\mathcal{L}_{\\mathrm{enh}}$ (bottom). The heatmap illustrates attention activation for the \u201csheep\u2019s neck\u201d class. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "hE6ZxU0N3c/tmp/7f972e899429e860ddda6cdd99992db2ac2340a34f7a1c6add6427a0b604e36a.jpg", "table_caption": ["Table A8: Effect of varying threshold $\\gamma$ on Pascal-Part-116 in Oracle-Obj setting. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Additionally, removing the attention control losses $\\mathcal{L}_{\\mathtt{s e p}}$ and $\\mathcal{L}_{\\mathrm{enh}}$ while keeping both $\\lambda_{\\mathsf{o b j}}$ and $\\lambda_{\\mathtt{p a r t}}$ at 1.0 results in a lower Harmonic mean of 38.20. This indicates that the attention control losses contribute to better distinguishing between seen and unseen classes. ", "page_idx": 19}, {"type": "text", "text": "D.2 Qualitative Ablation on Attention Control Losses ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The separation loss reduces the overlap between different parts, while the enhancement loss strengthens the activation of underrepresented parts. As shown in Figure A2, when only the separation loss $\\mathcal{L}_{\\mathtt{s e p}}$ is applied (top), smaller parts adjacent to larger parts may be diminished. Specifically, \u201csheep\u2019s neck\u201d is not properly highlighted because minimizing the intersection can cause larger parts, such as the \u201csheep\u2019s torso\u201d and \u201csheep\u2019s head\u201d, to overshadow smaller ones. When both losses $\\mathcal{L}_{\\mathrm{sep}}$ and $\\mathcal{L}_{\\mathrm{enh}}$ are utilized (bottom), the model accurately segments the small part\u2014\u201csheep\u2019s neck\u201d\u2014as the enhancement loss boosts its representation, preventing it from being overwhelmed by larger neighboring parts. ", "page_idx": 19}, {"type": "text", "text": "This demonstrates that the separation and enhancement losses complement each other. Their combined use is essential to effectively distinguish and represent both large and small parts within an object, leading to improved segmentation performance. ", "page_idx": 19}, {"type": "text", "text": "D.3 Ablation on the Hyperparameter in Attention Control ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To evaluate the sensitivity of our method to the hyperparameter threshold $\\gamma$ in Equation (8), we conducted experiments on the Pascal-Part-116 dataset under the Oracle-Obj setting. We varied $\\gamma$ from 0.1 to 0.5 and measured the performance in terms of mIoU for seen and unseen classes, as well as the harmonic mean. ", "page_idx": 19}, {"type": "text", "text": "As shown in Table A8, our method is robust to the choice of $\\gamma$ within the range of 0.1 to 0.5. The harmonic mean remains relatively stable, with the best performance achieved at $\\gamma=0.3$ . While there is a slight variation in performance across different values of $\\gamma$ , the changes are not significant, indicating that our method does not heavily depend on the exact value of this hyperparameter. ", "page_idx": 19}, {"type": "text", "text": "E.1 CLIP Embedding ", "page_idx": 20}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/787ebdc2e70bb5585a9d6b120c6d3b6050f94d0b256c27950f20afcba853f555.jpg", "img_caption": ["E Additional Qualitative Results and Qualitative Analysis "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure A3: The t-SNE visualization of text embeddings from a pre-trained CLIP model on the classes of the Pascal-Part-116 dataset. ", "page_idx": 20}, {"type": "text", "text": "The t-SNE visualization of text embeddings from a pre-trained CLIP [38, 32, 58] model on the PascalPart-116 dataset [7, 46] reveals intriguing insights into the model\u2019s understanding of categories. Notably, similar classes such as \u201ccats\u201d and \u201cdogs\u201d are clustered closely within the embedding space. This proximity indicates a shared semantic space for categories that are visually or contextually related. ", "page_idx": 20}, {"type": "text", "text": "Additionally, we observed that object-specific parts sharing generalized parts, such as \u201ccar\u2019s license plate\u201d and \u201cbus\u2019s license plate\u201d, are also positioned near each other. This clustering suggests that the CLIP recognizes and leverages common parts across different objects that share common characteristics. Further analysis shows that object-specific classes containing parts like \u201cmuzzle\u201c and \u201cpaw\u201c are distributed in similar regions of the space. This consistency across different object categories emphasizes the CLIP\u2019s ability to generalize part-level features effectively. ", "page_idx": 20}, {"type": "text", "text": "Leveraging CLIP\u2019s text embeddings provides a significant zero-shot capability in the visual domain. This capability can be extended to part-level categories, demonstrating the potential for sophisticated unsupervised or zero-shot learning approaches in fine-grained object and part recognition tasks. ", "page_idx": 20}, {"type": "text", "text": "E.2 Additional Qualitative Results E.2.1 Oracle-Obj Setting ", "page_idx": 21}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/26d4ef2447fdfd7288bd1d8b21d61b976a2f3d9a7bbf76af501f497ad6c2366e.jpg", "img_caption": ["Figure A4: Comparison of VLPart, CLIPSeg, CAT-Seg, and our model on the Pascal-Part-116 dataset in Oracle-Obj setting. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.2.2 Pred-All Setting ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "hE6ZxU0N3c/tmp/dba24a1a3708da0d637c095655944f6ad51989f91ca2aa05ccf84ad2fa397281.jpg", "img_caption": ["Figure A5: Comparison of VLPart, CLIPSeg, CAT-Seg, and our model on the Pascal-Part-116 dataset in Pred-All setting. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope in Section 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The limitations & future works are included at supplementary materials. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The anonymized repository of our implementation and instructions of reproduction are provided. (in the abstract section) ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The anonymized repository of our implementation and instructions of reproduction are provided. (in the abstract section) ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The experimental setting and details are provided in section 4 and supplementary materials. Also, the anonymized repository of our implementation and reproduction instructions are provided. (URL is in the abstract section) ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The standard error of an average of 5 results is reported in Section 4 of the proposed model. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The experimental resources, setting and details are provided in section 4 and supplementary materials. Also, the anonymized repository of our implementation and reproduction instructions are provided. (URL is in the abstract section) ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: - ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The broader of impact is discussed in supplementary materials. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We properly credited previous works and codes in section 4. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The anonymized repository of our implementation and reproduction instructions are provided. (URL is in the abstract section) ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: [NA] Guidelines: [NA] ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]