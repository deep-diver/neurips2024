[{"figure_path": "QbsPz0SnyV/figures/figures_1_1.jpg", "caption": "Figure 1: The influence of labels fitting on performance gaps (best view in color), where Ls and Lu denote the loss with one-hot labels and uniform labels (label free).", "description": "This figure shows the impact of different label fitting strategies on the performance gap between audio and video modalities in a multimodal learning task.  Three label types are compared: one-hot labels (Ls), uniform labels (Lu), and a combination of both (0.7Ls + 0.3Lu, which is a label smoothing technique). The accuracy of the model is shown for each label type, separately for audio and video modalities. The results indicate that using uniform labels (Lu) or a label smoothing strategy reduces the performance gap between the two modalities, suggesting that the process of fitting category labels plays a role in the modality imbalance problem.", "section": "1 Introduction"}, {"figure_path": "QbsPz0SnyV/figures/figures_4_1.jpg", "caption": "Figure 1: The influence of labels fitting on performance gaps (best view in color), where Ls and Lu denote the loss with one-hot labels and uniform labels (label free).", "description": "This figure shows the impact of different label fitting strategies on the performance gap between audio and video modalities in a multimodal learning task.  Three label strategies were used: one-hot labels (Ls), uniform labels (Lu), and a mix of one-hot and uniform labels (0.7Ls + 0.3Lu). The results demonstrate that using uniform labels reduces the performance gap, suggesting that the difference in learning ability between modalities is partly due to the difficulty of fitting category labels.", "section": "1 Introduction"}, {"figure_path": "QbsPz0SnyV/figures/figures_8_1.jpg", "caption": "Figure 2: Visualizations of the modality gap distance on the CREMA-D dataset.", "description": "This figure visualizes the modality gap distance for four different multimodal learning methods: CONCAT, G-Blend, MLA, and the proposed method.  Each subplot shows a scatter plot of audio and image feature vectors projected onto a two-dimensional space. The lines connecting points represent the distance between audio and image representations of the same sample. The \"Gap Distance\" value quantifies the overall separation between audio and image features.  The figure demonstrates that the proposed method achieves a larger modality gap than the other methods, suggesting a more effective separation of features and potentially improved performance.", "section": "4.5 Further Analysis"}, {"figure_path": "QbsPz0SnyV/figures/figures_8_2.jpg", "caption": "Figure 3: Change of \u03b1 on different datasets. We illustrate the value of the heuristic integration strategy for comparison.", "description": "This figure shows the change of the weighting parameter \u03b1 over the course of training for different datasets using two dynamic integration strategies: learning-based and heuristic.  The x-axis represents the normalized epoch number (#epoch/#total_epochs), and the y-axis shows the value of \u03b1. Each line represents a different dataset (KineticsSounds, CREMA-D, Sarcasm, Twitter2015), with the heuristic strategy's \u03b1 change shown for comparison. The figure demonstrates how the optimal \u03b1 value for balancing the classification loss and modality matching loss varies depending on the dataset and the chosen strategy. The learning-based strategy dynamically adjusts \u03b1 to improve performance, while the heuristic strategy uses a predefined decreasing function.", "section": "4.4 Effectiveness of Integration Learning"}, {"figure_path": "QbsPz0SnyV/figures/figures_9_1.jpg", "caption": "Figure 4: Visualization on Twitter2015 dataset. Our proposed method tends to perform feature learning first and then fit the learned features to the category labels.", "description": "The figure visualizes the attention maps generated by GradCAM for the CONCAT and Ours-LB methods on the Twitter2015 dataset at epochs 1, 7, and 15.  It shows how the models focus on different aspects of the image during training. CONCAT shows less focused attention throughout the epochs, whereas Ours-LB demonstrates a more focused attention that evolves over time, suggesting a learning process that prioritizes feature extraction before classification.", "section": "4.5 Further Analysis"}]