[{"figure_path": "W433RI0VU4/tables/tables_3_1.jpg", "caption": "Table 1: The comparison of graph similarity and solving properties between the original instances and the generated instances using G2MILP [19], a popular MILP generation framework. Here we use 100 original instances to generate 1,000 instances.", "description": "This table compares the performance of G2MILP, a learning-based MILP instance generation method, against the original instances.  It shows the graph similarity (structural distributional similarity score), average solving time, and the proportion of feasible instances generated.  The results highlight the challenges G2MILP faces in generating high-quality instances, particularly in maintaining feasibility.", "section": "3.1 Challenges of Low-Quality Generation"}, {"figure_path": "W433RI0VU4/tables/tables_7_1.jpg", "caption": "Table 2: Structural Distributional similarity scores between the generated instances and the original ones. The higher score implies higher similarity. Timeout implies the generation time is over 200h.", "description": "This table presents the structural distributional similarity scores achieved by different MILP instance generation methods (Bowly, G2MILP, and MILP-Studio) across four benchmark datasets (CA, FA, IP, WA).  Higher scores indicate greater similarity between the generated and original instances.  The 'Timeout' entry indicates that the generation process exceeded the 200-hour time limit.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_7_2.jpg", "caption": "Table 3: Average solving time (s) and feasible ratio (in parentheses) of the instances. We set the solving time limit to be 1,000s. We mark the values closest to those in original instances in bold.", "description": "This table compares the average solving time and the percentage of feasible instances generated by different methods (Bowly, G2MILP, and MILP-Studio with different modification ratios) against the original instances across four benchmark datasets (CA, FA, IP, and WA).  The solving time limit was set to 1000 seconds.  The table highlights that MILP-Studio consistently maintains a higher feasible ratio and closer solving times to the original data, indicating the method's success in preserving the properties of the original instances.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_8_1.jpg", "caption": "Table 4: Average solving time (s) and feasible ratio (in parentheses) of the instances. We set the solving time limit to be 1,000s. We mark the values closest to those in original instances in bold.", "description": "This table compares the performance of different MILP instance generation methods on four benchmark datasets (CA, FA, IP, WA).  The metrics used are the average solving time (with a 1000-second time limit) and the percentage of feasible instances generated.  The goal is to assess which method best preserves the computational hardness and feasibility of the original instances.", "section": "5.3 Improving the Performance of Learning-based Solvers"}, {"figure_path": "W433RI0VU4/tables/tables_19_1.jpg", "caption": "Table 5: Comparison of branching accuracy on the testing datasets. \\textquotesingle Trivial samples\\textrquotesingle implies that the computational hardness of the generated instances is so low that the solver does not need to perform branching to solve them. As a result, we are unable to collect any branching samples for these trivial instances. We mark the best performance in bold. We can see that MILP-StuDio can consistently improve the branching accuracy of the learned models.", "description": "This table presents the branching accuracy results on testing datasets for several methods.  The methods involve training a Graph Neural Network (GNN) for branching decisions using different data generation techniques (GNN-11000, GNN, GNN+Bowly, GNN+G2MILP, GNN+MILP-StuDio).  The table shows that MILP-StuDio consistently leads to better branching accuracy compared to the other data generation methods.  Note that some methods resulted in trivial samples, meaning the generated instances were too simple to require branching.", "section": "F.1 Experiments on Learning to Branch"}, {"figure_path": "W433RI0VU4/tables/tables_19_2.jpg", "caption": "Table 6: Comparison of solving performance in learning-to-branch between our approach and baseline methods, under a 1,000-second time limit. 'Obj' represents the objective values achieved by different methods and 'Time' denotes the average time used to find the solutions. '\u2193' indicates that lower is better. We mark the best values in bold. \u201cTrivial samples' implies that the computational hardness of the generated instances is so low that the solver does not need to perform branching to solve them. As a result, we are unable to collect any branching samples for these trivial instances.", "description": "This table compares the performance of learning-to-branch models trained on instances generated by different methods (GNN, GNN+Bowly, GNN+G2MILP, GNN+MILP-Studio) and the original instances. The performance is measured by the objective values (Obj) and solving times (Time). Lower values for both Obj and Time are better.  The table shows that MILP-Studio consistently improves the performance of the learning-to-branch model.", "section": "F.1 Experiments on Learning to Branch"}, {"figure_path": "W433RI0VU4/tables/tables_20_1.jpg", "caption": "Table 7: Structural Distributional similarity scores between the generated instances and the original ones in FA. We compare the effect of different generation operators and modification ratios. We find that the mix-up operator can generate the most realistic instances.", "description": "This table shows the structural similarity scores between instances generated by different methods (G2MILP and MILP-Studio with different operators and modification ratios) and the original instances for the FA dataset.  The mix-up operator in MILP-Studio consistently achieves the highest similarity scores, suggesting it produces instances most similar to the real-world data.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_20_2.jpg", "caption": "Table 8: Average solving time (s) and feasibility of instances solved by Gurobi. \u03b7 is the masking ratio. Numbers in the parentheses are feasible ratios in the instances. The instances generated by the G2MILP are found to have an extremely low feasible ratio.", "description": "This table shows the average time taken by the Gurobi solver to solve instances generated using different methods and modification ratios.  It also shows the percentage of generated instances that were feasible (i.e., had a valid solution).  The results highlight that G2MILP struggles to generate feasible instances, unlike the proposed MILP-Studio method.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_21_1.jpg", "caption": "Table 9: Comparison of prediction loss on the testing datasets for different generation operators. We set the modification ratio \u03b7 = 0.05. We mark the best values in bold.", "description": "This table compares the prediction loss on testing datasets for the Predict and Search algorithm using instances generated by different operators in the MILP-Studio framework.  The modification ratio (\u03b7) is set to 0.05 for all methods. The best performance for each benchmark (FA and IP) is highlighted in bold.  This helps to evaluate the impact of the different generation operators on the performance of the Predict and Search algorithm.", "section": "F.2 More Results on Different Generation Operators and Modification Ratios"}, {"figure_path": "W433RI0VU4/tables/tables_21_2.jpg", "caption": "Table 10: Comparison of solving performance in PS between different operators of MILP-StuDio, under a 1,000-second time limit. In the IP benchmark, all the approaches reach the time limit of 1,000s, thus we do not consider the Time metric. '\u2193' indicates that lower is better.", "description": "This table compares the performance of the Predict-and-Search (PS) algorithm when using different block manipulation operators within the MILP-Studio framework.  It shows the objective value, absolute gap from the best-known solution, and solving time for the FA (capacitated facility location) and IP (item placement) benchmark problems. The results indicate the effectiveness of different operators in improving the PS algorithm's performance.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_22_1.jpg", "caption": "Table 11: Comparison of prediction loss using different modification ratios on the testing datasets. The notation 'infeasible' implies that a majority of the generated instances are infeasible and thus cannot be used as the training data for PS. We mark the best values in bold.", "description": "This table presents the prediction loss results for the Predict-and-Search (PS) algorithm using instances generated with different modification ratios (0.01, 0.05, and 0.10) by both G2MILP and MILP-Studio.  The results are broken down by dataset (FA and IP).  The 'infeasible' label indicates that the generated instances were mostly infeasible and therefore unusable for training.  The table highlights the superior performance of MILP-Studio across all modification ratios and datasets, demonstrating its ability to generate high-quality, feasible instances.", "section": "F.2 More Results on Different Generation Operators and Modification Ratios"}, {"figure_path": "W433RI0VU4/tables/tables_22_2.jpg", "caption": "Table 6: Comparison of solving performance in learning-to-branch between our approach and baseline methods, under a 1, 000-second time limit. 'Obj' represents the objective values achieved by different methods and 'Time' denotes the average time used to find the solutions. '\u2193' indicates that lower is better. We mark the best values in bold. \u201cTrivial samples' implies that the computational hardness of the generated instances is so low that the solver does not need to perform branching to solve them. As a result, we are unable to collect any branching samples for these trivial instances.", "description": "This table compares the performance of different methods in solving the learning-to-branch task using the FA and IP benchmarks.  The methods include the original GNN model trained on a subset of instances and GNN models trained on instances generated by different methods (Bowly, G2MILP, and MILP-Studio) with various modification ratios. The table shows the objective values and average solving times for each method and dataset.  The 'Trivial samples' entry notes instances where the computational hardness was too low to require branching.", "section": "F.1 Experiments on Learning to Branch"}, {"figure_path": "W433RI0VU4/tables/tables_22_3.jpg", "caption": "Table 2: Structural Distributional similarity scores between the generated instances and the original ones. The higher score implies higher similarity. Timeout implies the generation time is over 200h.", "description": "This table presents the graph structural distributional similarity scores between generated and original instances for four different MILP datasets (CA, FA, IP, WA) using three different generation methods (Bowly, G2MILP, MILP-Studio) and three different modification ratios (0.01, 0.05, 0.10).  A higher score indicates greater similarity between the generated and original instances' structures.  The 'Timeout' indicates that the generation time exceeded 200 hours for the Bowly method on the WA dataset.", "section": "5 Experiments"}, {"figure_path": "W433RI0VU4/tables/tables_23_1.jpg", "caption": "Table 14: Comparison of solving performance in Gurobi hyperparameter tuning. We mark the best performance in bold.", "description": "This table presents the results of hyperparameter tuning for the Gurobi solver using different instance generation methods.  The \"gapabs\" column shows the absolute difference between the objective function value found by Gurobi and the best-known solution (BKS). The \"Time\" column shows the average solving time in seconds.  The results demonstrate that tuning Gurobi with instances generated by MILP-StuDio leads to a significant improvement in solving time, achieving the best performance among all methods tested.", "section": "5. Experiments"}, {"figure_path": "W433RI0VU4/tables/tables_23_2.jpg", "caption": "Table 15: The similarity score, computational hardness, and feasibility ratio between the original and generated instances on the non-structural large-scale Setcover benchmark (larger than that in Appendix E.4). We set the solving time limit 300s and \u03b7 = 0.05. We mark the best performance in bold.", "description": "This table compares the similarity, solving time, and feasibility of instances generated by different methods against the original instances from the Setcover benchmark.  The metrics assess how well each method preserves the characteristics of the original instances.  The solving time is capped at 300 seconds, and the modification ratio (\u03b7) is set to 0.05.  MILP-Studio demonstrates superior performance in maintaining the characteristics of the original data.", "section": "F.4 More Results on Non-Structured MILPs"}, {"figure_path": "W433RI0VU4/tables/tables_23_3.jpg", "caption": "Table 16: Comparison of solving performance in PS between different operators of MILP-StuDio, under a 1,000-second time limit. In the IP benchmark, all the approaches reach the time limit of 1,000s, thus we do not consider the Time metric. '\u2193' indicates that lower is better.", "description": "This table compares the performance of the Predict-and-Search (PS) algorithm using instances generated by different operators of the MILP-StuDio framework.  It shows the objective value achieved (Obj), the absolute gap between that objective and the best-known solution (gapabs), and the time taken to find the solution (Time). The results indicate that the MILP-StuDio framework improves the performance of PS, regardless of the specific operator used.", "section": "Extensive Experiment Results"}, {"figure_path": "W433RI0VU4/tables/tables_24_1.jpg", "caption": "Table 17: The similarity score, computational hardness, and feasibility ratio between the original and generated instances on the non-structural MIS benchmark. We set the solving time limit 300s and \u03b7 = 0.05. We mark the best performance in bold.", "description": "This table presents a comparison of the similarity scores, solving times, and feasibility ratios for the original MIS instances and those generated using three different methods: Bowly, G2MILP, and MILP-StuDio.  The solving time limit was set to 300 seconds, and the modification ratio (\u03b7) was 0.05.  The best performing method for each metric is highlighted in bold.  The results show that MILP-StuDio and G2MILP generate instances with high similarity to the originals, while maintaining 100% feasibility,  unlike Bowly which had significantly faster solving times.", "section": "F.4 More Results on Non-Structured MILPs"}, {"figure_path": "W433RI0VU4/tables/tables_24_2.jpg", "caption": "Table 18: The performance of the PS solver trained by instances generated by different methods on the non-structural MIS benchmark. We set the solving time limit 300s and \u03b7 = 0.05. We mark the best performance in bold.", "description": "This table compares the performance of the Predict-and-Search (PS) solver when trained using instances generated by different methods (Gurobi, PS, Bowly, G2MILP, and MILP-Studio). The performance is evaluated on a non-structural MIS benchmark with a 300-second time limit and a modification ratio of 0.05. The metrics reported include solving time, objective value, and the absolute gap between the objective value and the best-known solution.", "section": "F.5 More Results on Real-world Industrial Dataset"}, {"figure_path": "W433RI0VU4/tables/tables_24_3.jpg", "caption": "Table 3: Average solving time (s) and feasible ratio (in parentheses) of the instances. We set the solving time limit to be 1,000s. We mark the values closest to those in original instances in bold.", "description": "This table compares the average solving time and the percentage of feasible instances generated by different methods (Bowly, G2MILP, and MILP-StuDio with different modification ratios) against the original instances. The solving time is capped at 1000 seconds.  The bold values indicate which generated instances are closest to the original data in terms of solving time. This provides a measure of how well each method preserves the computational hardness of the original MILP instances.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_25_1.jpg", "caption": "Table 20: The partial solution size parameter (ko, k\u2081, \u0394) and neighborhood parameter \u0394.", "description": "This table shows the values of hyperparameters used in the Predict-and-Search (PS) algorithm for different benchmarks (CA, FA, IP, WA).  These hyperparameters control the size of the partial solution considered and the search neighborhood explored during the solving process.", "section": "H.1 Implementation of Predict-and-Search"}, {"figure_path": "W433RI0VU4/tables/tables_31_1.jpg", "caption": "Table 2: Structural Distributional similarity scores between the generated instances and the original ones. The higher score implies higher similarity. Timeout implies the generation time is over 200h.", "description": "This table presents the structural distributional similarity scores between the generated instances and the original instances for four different MILP problem benchmarks (CA, FA, IP, WA). The similarity scores are calculated using 11 graph statistics to measure the similarity between the original and generated instances and are shown for three different modification ratios (\u03b7 = 0.01, \u03b7 = 0.05, \u03b7 = 0.10).  Higher scores indicate greater similarity.  For the WA benchmark, the generation time exceeded 200 hours for one method.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_31_2.jpg", "caption": "Table 2: Structural Distributional similarity scores between the generated instances and the original ones. The higher score implies higher similarity. Timeout implies the generation time is over 200h.", "description": "This table presents the structural distributional similarity scores between the generated instances and the original instances for four different MILP datasets (CA, FA, IP, WA).  The similarity is calculated using 11 graph statistics. A higher score indicates greater similarity between the generated and original instances.  The table also notes when the generation time exceeded 200 hours, indicating that the generation process did not complete for those instances.", "section": "5.2 Similarity between the Generated and the Original Instances"}, {"figure_path": "W433RI0VU4/tables/tables_32_1.jpg", "caption": "Table 22: Statistical information of the benchmarks we used in this paper.", "description": "This table presents a statistical summary of four MILP problem benchmarks used in the paper: Combinatorial Auctions (CA), Capacitated Facility Location (FA), Item Placement (IP), and Workload Appointment (WA).  For each benchmark, the table provides the number of constraints, the number of variables, the number of blocks identified in the constraint coefficient matrices (CCMs), the types of constraints (M-Cons, B-Cons, D-Cons, DB-Cons), and the types of variables (Bl-Vars, Bd-Vars). This information is crucial for understanding the characteristics of the datasets and for comparing the performance of different MILP generation methods.", "section": "Experiment Settings"}, {"figure_path": "W433RI0VU4/tables/tables_32_2.jpg", "caption": "Table 2: Structural Distributional similarity scores between the generated instances and the original ones. The higher score implies higher similarity. Timeout implies the generation time is over 200h.", "description": "This table presents the structural distributional similarity scores between instances generated by different methods and the original instances.  The similarity is measured using 11 graph statistics, and a higher score indicates greater similarity. The table shows results for four different datasets (CA, FA, IP, WA) and three different modification ratios (\u03b7 = 0.01, 0.05, 0.10) for each method.  The 'Bowly', 'G2MILP', and 'MILP-StuDio' methods are compared. Note that for the WA dataset, the Bowly method timed out, meaning it took over 200 hours to complete.", "section": "5.2 Similarity between the Generated and the Original Instances"}]