{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture and the attention mechanism, which are foundational to the work in this paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-05-01", "reason": "This paper introduced the Vision Transformer (ViT), which directly applies the Transformer architecture to image classification, a significant advance in computer vision that directly motivates the current work."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-10-01", "reason": "This paper introduced the Swin Transformer, a hierarchical Transformer that improves efficiency and effectiveness for high-resolution images, serving as an important building block and comparison point in this work."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-07-01", "reason": "This paper introduced DeiT, a data-efficient Transformer model that uses knowledge distillation to improve performance, which this paper improves upon and compares against."}, {"fullname_first_author": "Wenhai Wang", "paper_title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions", "publication_date": "2021-10-01", "reason": "This paper introduced the Pyramid Vision Transformer (PVT), which uses a hierarchical approach to improve efficiency, providing another important comparison model for this paper."}]}