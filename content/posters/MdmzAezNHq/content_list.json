[{"type": "text", "text": "Differential Privacy in Scalable General Kernel Learning via $K$ -means Nystr\u00f6m Random Features ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bonwoo Lee Jeongyoun Ahn\u2217 Cheolwoo Park\u2217 KAIST Daejeon, 34141 South Korea righthim@kaist.ac.kr, jyahn@kaist.ac.kr, parkcw2021@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the volume of data invested in statistical learning increases and concerns regarding privacy grow, the privacy leakage issue has drawn significant attention. Differential privacy has emerged as a widely accepted concept capable of mitigating privacy concerns, and numerous differentially private (DP) versions of machine learning algorithms have been developed. However, existing works on DP kernel learning algorithms have exhibited practical limitations, including scalability, restricted choice of kernels, or dependence on test data availability. We propose DP scalable kernel empirical risk minimization (ERM) algorithms and a DP kernel mean embedding (KME) release algorithm suitable for general kernels. Our approaches address the shortcomings of previous algorithms by employing Nystr\u00f6m methods, classical techniques in non-private scalable kernel learning. These methods provide data-dependent low-rank approximations of the kernel matrix for general kernels in a DP manner. We present excess empirical risk bounds and computational complexities for the scalable kernel DP ERM, KME algorithms, contrasting them with established methodologies. Furthermore, we develop a private data-generating algorithm capable of learning diverse kernel models. We conduct experiments to demonstrate the performance of our algorithms, comparing them with existing methods to highlight their superiority. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As data collection and access continue to expand, protecting privacy has emerged as a crucial concern. Differential privacy (DP), introduced by Dwork et al. (2006b), stands as the current gold standard in data privacy. It provides a rigorous framework for privacy in statistical procedures, wherein noise is added proportional to the maximum deviation induced by the change of a single individual in the dataset. The field has seen significant advances in differentially private (DP) algorithms for machine learning. Notably, DP empirical risk minimization (ERM) has become a key area of research, attracting extensive attention (Chaudhuri et al., 2011; Kifer et al., 2012; Bassily et al., 2019; Wang et al., 2019; Feldman et al., 2020). These efforts focus on developing privacy-preserving statistical models with high generalization capabilities across various ERM problems, including regularized regression, logistic regression, support vector machines, and some other ERM problems equipped with non-convex or non-smooth loss functions. However, a vast majority of the current DP ERM algorithms primarily address linear ERM, suitable for datasets with linear structures. ", "page_idx": 0}, {"type": "text", "text": "Kernel methods hold significant importance in machine learning due to their ability to capture intricate, non-linear structures. Kernel-based learning is ubiquitous in both supervised and unsupervised settings as well as in hypothesis testing problems. For an extensive review on the subject, see Muandet et al. (2017). This work focuses on the supervised kernel learning problem via ERM as well as kernel mean embedding (KME) for probability distributions. Developing a DP framework for general kernels poses a significant hurdle due to their reliance on the local attributes of the data. For instance, not every kernel is amenable to a well-behaved DP solution. As a rather extreme case, the Kronecker delta kernel, $k(x,y)=\\mathbf{1}(x=y)$ , yields a meaningful solution only when the test data exactly matches one of the training instances. Consequently, the solution becomes highly sensitive to the data changes, exhibiting substantial deviations with even a single data alteration. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As a result, there has been relatively little DP research regarding kernel ERM, especially compared to linear counterparts. Chaudhuri et al. (2011) suggested transforming kernel ERM into a linear form via random features, which facilitates scaling-down operations. However, their method is limited to translation-invariant kernels, excluding a wide variety of other kernels, such as polynomial kernels or the pyramid match kernel from the computer vision domain (Grauman and Darrell, 2007), the chi-squared kernel in hand gesture recognition (Abadi et al., 2015), Histogram intersection kernel in image classification (Maji et al., 2013), Bayesian kernel in protein prediction (Alashwal et al., 2009), and the diffusion kernel in manifold learning (Lafferty and Lebanon, 2005). Hall et al. (2013) proposed perturbing the output function of kernel ERM. Although their work can be applied to general kernels, it lacks scalability in practice (e.g., $O(n^{3})$ in kernel ridge regression). Additionally, it requires access to the training data for every prediction, posing a potential threat of privacy leakage. The DP kernel ERM proposed by Jain and Thakurta (2013) can work with general kernels; however, its utility is guaranteed only when test data are accessible. ", "page_idx": 1}, {"type": "text", "text": "KME is a pivotal tool in diverse applications such as two-sample testing and synthetic data generation (Harder et al., 2021). It enables the comparison of probability distributions via metric in Reproducing Kernel Hilbert Space (RKHS), by embedding these distributions into elements of the RKHS. Additionally, Balog et al. (2018) found utility of KME for the private release of data. The embedding itself acts as a concise representation of the distributional information of the data. However, their methods appear to suffer in high-dimensional settings. Moreover, it is important to note that the DP functional release algorithm suggested by Hall et al. (2013) cannot be directly applied to DP KME. This limitation arises because the output is not a member of the RKHS, which is a crucial distinction since statistical methods related to KME heavily depend on operations within the RKHS. ", "page_idx": 1}, {"type": "text", "text": "Our study is motivated by the frequent oversight in prior research of the practical challenges in real-world implementations, such as scalability, private implementation without access to test data, accurate DP KME release, and the lack of information on future private data use. We propose algorithms designed for practical DP kernel ERM by integrating the Nystr\u00f6m method. Although the Nystr\u00f6m method has been a state-of-the-art technique with random Fourier features in scalable kernel learning, this work is the first to apply it to DP kernel learning. The proposed algorithms are designed to be scalable and are suitable even for unregularized learning or non-convex loss functions. Moreover, we demonstrate that our Nystr\u00f6m-based techniques can be used to construct a DP estimate of KME, which can be utilized for private two-sample tests via maximal mean discrepancy, for example. We also address the cases where the model is unknown, such as in public releases of private data, by employing an algorithm that releases versatile private data suitable for kernel ERM with diverse objectives. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "DP ERM. ERM serves as a fundamental tool in machine learning, with substantial literature on DP ERM dating back to Dwork et al. (2006b). Among the current works, most of which regard linear ERM, main approaches include output and objective perturbation methods (Chaudhuri et al., 2011; Kifer et al., 2012; Jain and Thakurta, 2014; McSherry and Talwar, 2007; Gopi et al., 2022, 2023; Mangoubi and Vishnoi, 2022), and gradient perturbation methods (Bassily et al., 2014; Wang et al., 2017; Song et al., 2013). However, Iyengar et al. (2019) pointed out that most output or objective perturbation-based algorithms suffer from scalability issues in their implementation. For example, they need to directly access optimal solutions in order to release private models, thereby reducing scalability, particularly in kernel learning. Additionally, they often require strong assumptions on learning problems to guarantee privacy, such as double differentiable loss or a strongly convex regularizer, which limits the range of applicable learning scenarios (Iyengar et al., 2019). In contrast, gradient or SGD-based output perturbation methods (Wu et al., 2017; Feldman et al., 2020) are relatively unconstrained by learning restrictions or scalability concerns. While a vast amount of literature exists on linear ERM, not all DP kernel ERM algorithms can capitalize on these findings. ", "page_idx": 1}, {"type": "text", "text": "DP kernel ERM and KME. To adapt techniques from DP linear ERM to kernel ERM, one needs random features of the kernel suitable for DP learning. Random Fourier features were suggested as a candidate for translation-invariant kernels (Chaudhuri et al., 2011), but none have been proposed for general kernels. Hall et al. (2013) and Jain and Thakurta (2013) addressed DP kernel ERM algorithms for general kernels without using random features. However, Jain and Thakurta (2013) assumes accessible test data, which may be inappropriate in certain cases. On the other hand, Hall et al. (2013) suggests a sophisticated method for releasing a private model but requires strong regularization. Moreover, both algorithms need the true solution before releasing the private model, resulting in scalability issues. The discussion on DP KME has been somewhat distant from DP kernel ERM, initially introduced by Balog et al. (2018) as a technique for private database release. Balog et al. (2018) developed private algorithms for releasing KME for both general and translation-invariant kernels. However, their methods suffer from the curse of dimensionality in both cases. Additionally, we note that the techniques in Hall et al. (2013) and Jain and Thakurta (2013) cannot be directly adapted to DP KME release. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2 Main Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Scalable DP kernel ERM for general kernels. We develop scalable algorithms for DP kernel ERM that accommodate general kernels, particularly non-translation-invariant ones. The Nystr\u00f6m method serves as a principal tool that enables much-desired scalability in practical applications. Our work is the first to propose a DP $K$ -means Nystr\u00f6m method and incorporate it in DP kernel ERM. Also, to the best of our knowledge, this is the first attempt to compare the theoretical and experimental efficacy of scalable private kernel learning algorithms. The proposed algorithms reduce time complexity and memory costs, akin to the Nystr\u00f6m method in non-private settings. Furthermore, experiments in Section 4 demonstrate the superior performance of our algorithms for different learning problems. We present a brief comparison of our approach and three existing methods in Table 1. ", "page_idx": 2}, {"type": "table", "img_path": "MdmzAezNHq/tmp/9e3c5aa073bec33f0ac5782c2211b78bdf4be57feac5dc0c95b4fb46eadba4cc.jpg", "table_caption": ["Table 1: Comparison of DP kernel ERM algorithms in terms of restrictions for privacy guarantee. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "DP KME. We propose a data-dependent DP KME releasing algorithm based on the DP $K$ -means Nystr\u00f6m method. The error of the DP KME is shown to be primarily related to how accurately the kernel matrix is estimated by the Nystr\u00f6m method. Our empirical study suggests its superiority compared to alternative DP KME releasing algorithms. ", "page_idx": 2}, {"type": "text", "text": "Private data release for versatile kernel learning. We consider the offline setting for database release, where the database owner releases data without knowledge of the models or statistics desired by users. We provide a DP algorithm that generates a dataset yielding excess empirical risk bounds of $O(n^{-c})$ for the logistic and the Linex (Ma et al., 2019) losses if it is used for kernel ERM. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Differential Privacy ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Among the numerous variants of DP, this work primarily employs $(\\epsilon,\\delta)$ -DP, as it is widely applicable to various statistical procedures. Let $\\mathcal{X}$ be the data space. We consider a randomized algorithm $M:{\\mathcal{X}}^{n}\\rightarrow{\\mathcal{P}}\\left({\\mathcal{Z}}\\right)$ , where $\\mathcal{P}(\\mathcal{Z})$ is a family of distributions over an output space $\\mathcal{Z}$ . Let $\\bar{D}\\sim D^{\\prime}$ indicate that datasets $D$ and $D^{\\prime}$ are neighbors, meaning they differ only by a single individual. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 $(\\epsilon,\\delta)$ -DP, Dwork et al. (2006a)). An algorithm $M$ is $(\\epsilon,\\delta)$ -DP if the following holds: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{D,D^{\\prime}\\subset\\mathcal{X}^{n},D\\sim D^{\\prime}}\\operatorname*{sup}_{A\\subset\\mathcal{Z}}\\mathbb{P}\\left(M(D)\\in A\\right)-e^{\\epsilon}\\mathbb{P}\\left(M(D^{\\prime})\\in A\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 states that an algorithm is DP if its randomness is proportional to the deviation in the output due to a single data change, as demonstrated in Proposition 1. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 (Gaussian mechanism, Nikolov et al. (2013)). If a deterministic algorithm $A:$ $\\chi^{n}\\stackrel{\\mathbf{\\mu}^{}}{\\rightarrow}\\mathbb{R}^{d}$ satis\u221afies $\\begin{array}{r}{\\operatorname*{sup}_{D,D^{\\prime}\\subset\\mathcal{X}^{n},D\\sim D^{\\prime}}\\lVert A(D)-A(D^{\\prime})\\rVert_{2}\\leq\\Delta,}\\end{array}$ , an algorithm defined by $M(D):=$ A(D) +\u2206 1+ 2 log \u03b41 \u03b5 is $(\\epsilon,\\delta)$ -DP where $\\varepsilon$ follows the standard normal distribution on $\\mathbb{R}^{d}$ . \u03f5 ", "page_idx": 2}, {"type": "text", "text": "Statistical procedures, including the algorithms we propose, typically involve multiple steps, such as data-dependent evaluations and multiple references to the data. Proposition 2 provides a privacy guarantee for such procedures. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Composition theorem, Dwork et al. (2006a)). For algorithms $M_{1}:{\\mathcal{X}}^{n}\\to{\\mathcal{P}}({\\mathcal{Z}})$ and $M_{2}:\\mathcal{X}^{n}\\times\\mathcal{Z}\\to\\mathcal{P}(\\mathcal{W})$ , the algorithm $M:\\mathcal{X}^{n}\\rightarrow\\mathcal{Z}$ defined by $M(D)=M_{2}(D,M_{1}(D))$ for $D\\subset\\mathcal{X}^{n}$ is $(\\epsilon_{1}+\\epsilon_{2},\\delta_{1}+\\delta_{2})$ -DP if $M_{1}$ is $(\\epsilon_{1},\\delta_{1})$ -DP and $M_{2}(\\cdot,z)$ is $(\\epsilon_{2},\\delta_{2})$ -DP for every $z\\in{\\mathcal{Z}}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Kernel, RKHS, and Random Features ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Kernel methods learn a non-linear structure by using a non-linear map, called a feature map $\\phi$ , to transform the data $\\{x_{i}\\}_{i=1}^{n}$ and analyze the linear structure of $\\{\\phi(x_{i})\\}_{i=1}^{n}$ . An appropriate choice of $\\phi$ captures the underlying complex structure of the data. The feature maps are inherently determined by the kernel function $k$ , a positive definite function with a corresponding RKHS $(\\mathcal{H}_{k},\\|\\cdot\\|_{\\mathcal{H}_{k}})$ , satisfying $\\langle\\phi(x),\\phi(y)\\rangle_{\\mathcal{H}_{k}}=k(x,\\stackrel{\\cdot}{y})$ . Within the RKHS, the outputs of the feature map behave akin to vectors in Euclidean space, allowing one to learn the non-linear structure through a linear model. ", "page_idx": 3}, {"type": "text", "text": "A random feature is a map $\\varphi\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}^{m}$ that approximates the kernel function in a way that $k(x,y)\\approx\\langle\\varphi(x),\\varphi(y)\\rangle$ , providing a linearized version of kernel learning, as will be discussed in Sections 3.2 and 3.3. Traditionally, $k(x,y)\\approx\\mathbb{E}[\\langle\\varphi(x),\\varphi(y)\\rangle]$ is required for a random feature map, but we adopt a more broad terminology. ", "page_idx": 3}, {"type": "text", "text": "2.2.1 Nystr\u00f6m Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Nystr\u00f6m methods are low-rank kernel matrix approximation methods that facilitate scalable kernel learning. For given dataset $\\{x_{i}\\}_{i=1}^{n}\\subset\\mathcal{X}$ , and a kernel matrix $\\mathbf{K}$ , the approximation $\\hat{\\bf K}$ is calculated as $\\hat{\\textbf{K}}:=\\mathbf{\\DeltaK}_{\\mathbf{ZX}}{}^{T}\\mathbf{K}_{\\mathbf{Z}}{}^{\\dagger}\\mathbf{K}_{\\mathbf{ZX}}$ where $\\mathbf{K}_{\\mathbf{Z}}\\,:=\\,[k(z_{i},z_{j})]_{m\\times m}$ , and $\\mathbf{K}_{\\mathbf{Z}\\mathbf{X}}\\,:=\\,[k(z_{i},x_{j})]_{m\\times n}$ . Here, $Z=\\{z_{1},\\ldots,z_{m}\\}\\subset{\\mathcal{X}}$ are some pre-chosen landmark points, and $\\dagger$ represents the Moore\u2013Penrose inverse. Then, $\\hat{\\bf K}$ can be interpreted as projections of data points onto the plane in the RKHS: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{K}}=[\\langle\\mathrm{proj}_{S}\\phi(x_{i}),\\mathrm{proj}_{S}\\phi(x_{j})\\rangle_{\\mathcal{H}_{k}}]_{n\\times n}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S=\\operatorname{span}\\{\\phi(z_{i})|1\\leq i\\leq m\\}\\subset\\mathcal{H}_{k}$ . Thus, the accuracy of $\\hat{\\bf K}$ , or the accuracy of scalable kernel learning via $\\hat{\\bf K}$ , depends on how closely the subspace $\\boldsymbol{S}$ resembles the data $\\{\\phi(x_{i})\\}_{i=1}^{n}$ , the corresponding elements in the RKHS of the original data. Consequently, Nystr\u00f6m methods operate by selecting landmark points that effectively represent the original data, as demonstrated in the past studies using data-dependent landmark points such as $K$ -means centroids or subsamples (Kumar et al., 2012; Zhang et al., 2008; He and Zhang, 2018). ", "page_idx": 3}, {"type": "text", "text": "2.3 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "DP kernel ERM. For a given dataset $D=\\{(x_{i},y_{i})\\}_{i=1}^{n}\\subset(\\mathcal{X}\\times\\mathcal{Y})^{n}$ , a loss function $l:\\mathbb{R}^{2}\\to\\mathbb{R}_{\\geq0}$ , and a regularization parameter $\\lambda>0$ , we consider the following regularized kernel ERM: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{f\\in\\mathcal{H}_{k}}{\\arg\\operatorname*{min}}\\,\\hat{L}^{\\lambda}(f;D)\\triangleq\\frac{1}{n}\\sum_{i=1}^{n}l(\\langle f,\\phi(x_{i})\\rangle_{\\mathcal{H}_{k}},y_{i})+\\frac{\\lambda}{2}\\|f\\|_{\\mathcal{H}_{k}}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The minimization finds a function $f$ that best explains the data under the given loss, and the regularization parameter $\\lambda$ prevents over-fitting. We measure the quality of the model by the excess empirical risk of $f$ , defined by $\\begin{array}{r}{\\hat{L}^{\\lambda}(f;D)-\\operatorname*{min}_{g\\in\\mathcal{H}_{k}}\\hat{L}^{\\lambda}(g;D)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "DP KME. For a given data $\\{x_{i}\\}_{i=1}^{n}\\ \\subset\\ \\mathcal{X}^{n}$ , our objective is to design a DP KME release mechanism: an $(\\epsilon,\\delta)$ -DP mechanism $M:\\,\\mathcal{X}^{n}\\,\\rightarrow\\,\\mathcal{P}\\left(\\mathcal{H}_{k}\\right)$ that is sufficiently close to the KME $\\mu_{X}\\,:=\\,\\mathbb{E}\\left[\\phi(X)\\right]$ in the RKHS norm with high probability. Since it is known that the empirical KME \u00b5\u02c6X := \u03d5(x1)+\u00b7\u00b7\u00b7+\u03d5(xn) converges to $\\mu_{X}$ with $O_{p}(n^{-\\frac{1}{2}})$ (Muandet et al., 2017), our DP KME algorithm will focus on releasing $\\hat{\\mu}_{X}$ . ", "page_idx": 3}, {"type": "text", "text": "Versatile DP kernel ERM. We explore a DP public dataset release tailored for kernel ERM. Unlike mechanisms developed in DP kernel ERM, which train models for specific loss functions $l$ and regularization parameters $\\lambda$ , we develop a private dataset release $(\\epsilon,\\delta)$ -DP mechanism $M:\\mathcal{X}^{n}\\rightarrow$ ${\\mathcal{Z}}^{n}$ . This mechanism enables learning a kernel ERM for post-given, possibly infinitely many $l$ functions and $\\lambda$ values. ", "page_idx": 3}, {"type": "text", "text": "3 Proposed Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Proofs for the theorems presented in this section are deferred to Appendix 6.3. ", "page_idx": 4}, {"type": "text", "text": "3.1 DP $K$ -means Nystr\u00f6m Approximation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section presents a pivotal tool for achieving scalability for general DP kernel learning: DP $K$ - means Nystr\u00f6m method. It is instrumental for our DP algorithms to make predictions using released private functions without needing access to the original data, and to be easily incorporated into the current linear DP ERM framework, unlike Hall et al. (2013). ", "page_idx": 4}, {"type": "text", "text": "As demonstrated by Chaudhuri et al. (2011) and Balog et al. (2018), DP kernel learning can effectively utilize the established methods of DP linear learning through random features. However, the use of random Fourier features depends on specific kernel characteristics, particularly translation-invariance, which complicates the development of random features for more general kernels. To overcome this challenge, we adopt the Nystr\u00f6m approximation scheme, which is suitable for constructing random features for a wider range of kernels. As discussed in Section 2.2.1, selecting landmark points that accurately capture the data structure is crucial for the Nystr\u00f6m method. Following the landmark selection guidelines outlined in Zhang et al. (2008), namely the $K$ -means approach, we provide a detailed rationale behind this criterion to demonstrate how effectively these points represent the underlying data structure. ", "page_idx": 4}, {"type": "text", "text": "Define \u03c6(ZNys) : X \u2192 Rm as \u03c6(ZN $\\varphi_{Z}^{(N y s)}\\;:=\\;(\\langle\\mathrm{proj}_{S}\\phi(x),b_{1}\\rangle_{\\mathcal{H}_{k}},\\ldots,\\langle\\mathrm{proj}_{S}\\phi(x),b_{m}\\rangle_{\\mathcal{H}_{k}})$ where $\\{b_{i}\\}_{i=1}^{m}$ is an orthonormal basis of $\\boldsymbol{S}$ . Here, $\\varphi^{(N y s)}$ is our random feature map. Our goal is to select suitable landmarks that minimize the approximation error of $\\varphi^{(N y s)}$ for the kernel $k$ at $\\{(x_{i},x_{j})\\}_{n\\times n}$ , which can be expressed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}\\Big(k(x_{i},x_{j})-\\langle\\varphi_{Z}^{(N y s)}(x_{i}),\\varphi_{Z}^{(N y s)}(x_{j})\\rangle\\Big)^{2}=\\frac{1}{n^{2}}\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the error described in Eq. (1) is also known as the Nystr\u00f6m approximation error in the literature on low-rank kernel approximation. The accuracy of Nystr\u00f6m-based kernel learning depends on the accuracy of the kernel matrix approximation: $\\|\\mathbf{K}{-}\\hat{\\mathbf{K}}\\|_{2}$ , which is upper bounded by $\\|\\mathbf{K}{-}\\hat{\\mathbf{K}}\\|_{F}$ . However, optimizing the landmark selection by minimizing LHS in Eq. (1) under DP is challenging because the problem is neither convex nor Lipschitz, making DP optimization difficult. Theorem 1 offers a comprehensive approach to identifying appropriate landmark points by solving the $K$ -means problem, rather than directly addressing the complex challenge. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. If a kernel $k$ is $c^{\\prime}$ -Lipschitz2 for both of its arguments, the Nystr\u00f6m approximation error is bounded by the quantization error of the landmark points for the original dataset: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{F}\\leq2c^{\\prime}\\sqrt{n\\sum_{i=1}^{n}\\operatorname*{min}_{z_{j}\\in Z}\\|x_{i}-z_{j}\\|_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 1 sheds light on the connection between low-rank kernel approximation and data clustering. Since the Nystr\u00f6m method is essentially an orthogonal projection of data onto $\\boldsymbol{S}$ in RKHS and the LHS of Eq. (2) resembles the common clustering objective, the theorem essentially suggests that a proper clustering can identify a subspa\u221ace $\\boldsymbol{S}$ that is closely aligned with the data. We emphasize that the bound stated in Theorem 1 is $\\bar{O(\\sqrt{n e})}$ , where $\\begin{array}{r}{e=\\dot{\\sum_{i=1}^{n}}\\mathrm{\\bar{m}i n}_{z_{j}\\in Z}\\|x_{i}-z_{j}\\|_{2}^{2}}\\end{array}$ . The bound is tighter than the $O(n e\\|\\mathbf{K}_{\\mathbf{Z}}{}^{-1}\\|_{F})$ bound provided in Zhang et al. (2008), if the sample size $n$ and the number of landmark points $m$ are sufficiently large such that $\\mathbf{K}\\mathbf{z}$ has small eigenvalues, even though the assumption on kernel is equivalent to the one made in Zhang et al. (2008). ", "page_idx": 4}, {"type": "text", "text": "In Algorithm 1, we present a DP algorithm for $K$ -means-based Nystr\u00f6m approximation. The obtained orthonormal basis $\\{b_{i}\\}_{i=1}^{n}$ of $\\boldsymbol{S}$ and their corresponding random feature map $\\varphi^{(N y s)}$ can be used in the subsequent DP kernel learning. For the DP $\\mathbf{K}$ -means step in line 2, the algorithm from the diffprivlib package implemented by ${\\mathrm{IBM}}^{\\,3}$ can be used. ", "page_idx": 4}, {"type": "text", "text": "Input: given data $\\{x_{1},\\ldots,x_{n}\\}$ , random features of dimension $m$ , cluster threshold parameter $m_{0}$ , and privacy parameters $\\epsilon,\\delta$ . ", "page_idx": 5}, {"type": "text", "text": "Output: a random feature mapping $\\varphi^{(N y s)}:\\mathcal{X}\\to\\mathbb{R}^{m}$ , and $\\left\\{b_{1},\\ldots,b_{m}\\right\\}\\subset\\mathcal{H}_{k}$ . 1. $K\\gets\\lfloor m_{0}\\epsilon\\rfloor$ . 2. $\\bar{\\{z_{1},\\dots,z_{k}\\}}\\leftarrow$ centroids of $\\epsilon/2$ -DP $K$ -means for $\\{x_{1},\\ldots,x_{n}\\}$ . 3. If $K<m$ , draw $\\{z_{k+1},\\ldots,z_{m}\\}$ from some distribution $Q$ . $4.\\,\\mathbf{U}\\pm\\mathbf{U}^{T}\\leftarrow$ the unitary diagonalization of a matrix $[k(z_{i},z_{j})]_{m\\times m}$ . 5. $\\begin{array}{r}{\\varphi^{(N y s)}\\gets\\ \\varphi^{(N y s)}(x):=\\frac{1}{R}{\\sum^{\\dagger\\frac{1}{2}}}{\\mathbf{U}}^{T}[k(z_{1},x),\\dotsc,k(z_{m},\\bar{x})]^{T}}\\end{array}$ , where $R={\\sqrt{\\operatorname*{max}_{x\\in\\mathcal{X}}k(x,x)}}$ . 6. $\\begin{array}{r}{b_{i}\\leftarrow\\sum_{j=1}^{m}[\\mathbf{\\Sigma}^{\\dagger\\frac{1}{2}}\\mathbf{U}^{T}]_{i j}\\phi(z_{j})}\\end{array}$ for $i=1,2,\\dots,m$ . 7. return $\\varphi^{(N y s)}$ , $\\{b_{1},\\dots,b_{m}\\}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Algorithm $^{\\,l}$ is $\\textstyle{\\frac{1}{2}}\\epsilon{-}D P,$ and $R[\\varphi^{(N y s)}(x)]_{i}=\\langle p r o j_{S}\\phi(x),b_{i}\\rangle_{\\mathcal{H}_{k}}$ for all $i$ and $x\\in\\mathscr{X}$ ", "page_idx": 5}, {"type": "text", "text": "Although the privacy remains unaffected by the threshold parameter $m_{0}$ , it does influence the quality of the landmark points: the DP $K$ -means centroids. Setting $K=m$ and omitting the thresholding step will result in small cluster sizes for large $m$ , making the centroid estimates more susceptible to the noise added in the DP $K$ -means step. This issue is exacerbated by a small privacy budget $\\epsilon$ , which further amplifies the noise. To mitigate this effect, thresholding is performed using $\\lfloor m_{0}\\epsilon\\rfloor$ . After applying cluster size thresholding, additional $m-K$ landmark points are generated in line 3 from a distribution $Q$ . While $Q$ can be chosen arbitrarily, we choose it as a mixture of $K$ truncated normal distributions centered at $z_{i}\\mathbf{s}$ , incorporating data information extracted from the DP $K$ -means. ", "page_idx": 5}, {"type": "text", "text": "3.2 Scalable DP Kernel ERM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Using the output of Algorithm 1, we can construct a DP kernel ERM for general kernels, as presented in Algorithm 2. Since the generation of random features is performed differentially privately, any existing DP linear ERM algorithms can be subsequently applied to achieve DP, in accordance with the composition theorem in Proposition 2. As an illustration, the method in Kifer et al. (2012), detailed in Algorithm 5 in Appendix 6.2, is adapted for general kernels in line 2 of Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 DP kernel ERM for general kernels ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: given data $\\{x_{1},\\ldots,x_{n}\\}$ , integer $m$ , and kernel $k$ . Output: $\\widetilde{f}\\in\\mathcal{H}_{k}$ .   \n1. $\\varphi^{(N y s)}$ , $\\left\\{b_{1},\\ldots,b_{m}\\right\\}\\gets\\epsilon/2$ -DP feature map and basis, output of Algorithm 1.   \n2. $u\\gets$ by solving $(\\epsilon/2,\\delta)$ -DP linear ERM for data $\\{(\\phi^{(N y s)}(x_{i}),y_{i})\\}_{i=1}^{n}$ :   \n3. $\\begin{array}{r}{\\widetilde f:=\\sum_{i=1}^{n}u_{i}b_{i}}\\end{array}$ .   \n4. returnf ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Algorithm 2 is $(\\epsilon,\\delta){-}D P.$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Line 1 of Algorithm 2 transforms the data points to random features, and line 2 solves the DP linear ERM with respect to the newly produced data $\\{\\varphi^{(N y s)}(x_{i})\\}_{i=1}^{n}$ . This step reduces the computational complexity, for example, from $\\dot{O}(n^{3})$ to $O(n m^{2}+n m d)$ in kernel ridge regression. ", "page_idx": 5}, {"type": "text", "text": "The linearization through random features introduces extra learning errors beyond the DP linear ERM error, specifically the approximation error for the given kernel, which is related to the $\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{2}$ , as shown in Theorem 4. The experiment results in Appendix 6.1 suggest that our DP random feature algorithm approximates the Gaussian kernel better than random Fourier features for small $m$ . ", "page_idx": 5}, {"type": "text", "text": "Also, we note that although the privacy budgets were allocated equally for step 1 and 2, it may not be the optimal choice. The Fig. 3 demonstrates the utility can be improved in other privacy allocations. However, we fix the allocation as half since tuning the optimal allocation may involve privacy leakage. We included the detail in Appendix 6.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. If the loss function is $c$ -Lipschitz with respect to its first argument, the excess empirical risk of Algorithm 2 equipped with ( \u03f52, \u03b4)-DP linear ERM algorithm is En,m(\u03b2) + c\u2225K2\u03bb\u2212nK\u02c6\u22252 with probability at least $1-\\beta$ where $\\mathcal{E}_{n,m}\\mathopen{}\\mathclose\\bgroup\\left(\\beta\\aftergroup\\egroup\\right)$ is an empirical risk bound of the linear ERM algorithm satisfied with probability at least $1-\\beta$ for inputs in a unit ball. ", "page_idx": 6}, {"type": "text", "text": "3.3 DP Kernel Mean Embedding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section introduces a DP KME method using $K$ -means Nystr\u00f6m approximation in Algorithm 3. An important consideration in DP KME is that the released private embedding should be an element of the RKHS. Note that the noise level in the release of the coefficient vector $w$ in line 3 of the algorithm is determined by Proposition 1, ensuring accuracy proportional to the sample size $n$ . ", "page_idx": 6}, {"type": "text", "text": "Algorithm 3 DP kernel mean embedding (DP KME) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: given data $\\{x_{1},\\ldots,x_{n}\\}$ , integer $m$ . ", "page_idx": 6}, {"type": "text", "text": "Output: a DP kernel mean embedding $f\\in\\mathcal{H}_{k}$ 1. \u03c6(Nys), $\\left\\{b_{1},\\ldots,b_{m}\\right\\}\\gets\\epsilon/2$ -DP featur\u221ae map and basis, output of Algorithm 1. 2. $\\begin{array}{r}{\\cdot w\\leftarrow R\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\varphi^{(N y s)}(x_{i})+\\frac{4\\left(1+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)}{n\\epsilon}\\varepsilon\\right)}\\end{array}$ where $\\varepsilon\\sim N\\left(0,I_{m\\times m}\\right)$ 3 $\\begin{array}{l}{{\\bf\\nabla}\\cdot\\widetilde{\\mu}_{X}\\leftarrow\\sum_{i=1}^{m}w_{i}b_{i}}\\\\ {{\\bf\\nabla}\\cdot{\\bf r e t u r n}\\:\\widetilde{\\mu}_{X}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. Algorithm 3 is $(\\epsilon,\\delta){-}D P.$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The following theorem regards the difference between our DP KME and the conventional empirical KME estimate $\\hat{\\mu}_{X}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. The DP KME error in the RKHS norm is given as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mu}_{X}-\\hat{\\mu}_{X}\\|_{\\mathcal{H}_{k}}\\leq\\frac{\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{2}}{\\sqrt{n}}+\\frac{2R\\sqrt{2m}\\left(1+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)}{n\\epsilon}\\left(\\sqrt{m}+\\sqrt{2\\log\\frac{1}{\\beta}}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability at least $1-\\beta$ . ", "page_idx": 6}, {"type": "text", "text": "According to Theorem 6, the accuracy of DP KME depends on the accuracy of the low-rank approximation of $\\mathbf{K}$ . By adaptively selecting landmark points according to the data, we can effectively reduce the approximation error. The following theorem confirms the advantage of using the data distribution in DP KME. ", "page_idx": 6}, {"type": "text", "text": "Theorem 7. If the landmarks are selected independently of the data, the KME error of Algorithm 3 would become: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\vert\\widetilde{\\mu}_{X}^{(i n d)}-\\hat{\\mu}_{X}\\right\\vert_{\\mathcal{H}_{k}}\\le2R\\left(\\frac{\\beta^{\\prime}}{n}+\\sqrt{\\frac{\\beta^{\\prime}}{n}}+\\frac{\\left(\\sqrt{2}+2\\sqrt{\\log\\frac{1}{\\delta}}\\right)}{n\\epsilon}\\left(\\sqrt{m}+\\sqrt{\\beta^{\\prime}}\\right)\\right)+\\Vert\\mu_{X}-p r o j_{S}\\mu_{X}\\Vert_{\\mathcal{H}_{k}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability at least $1-\\beta$ where $\\begin{array}{r}{\\beta^{\\prime}=2\\log{\\frac{2}{\\beta}}}\\end{array}$ , and \u00b5(Xind)denotes the DP KME obtained by data-independently selected landmark points. ", "page_idx": 6}, {"type": "text", "text": "Theorem 7 addresses the scenario where landmark points are selected independently of the data, as suggested by Balog et al. (2018). The error depends on how closely the distribution of the landmark points $\\boldsymbol{S}$ resembles that of the data. Consequently, disregarding the data distribution $\\mu_{X}$ can lead to suboptimal results in kernel learning. ", "page_idx": 6}, {"type": "text", "text": "3.4 Data Release for Versatile DP Kernel ERM ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we consider scenarios where sensitive data must be released without knowing which ERM model(s) will be trained on it. We propose a framework for releasing privacy-preserving datasets suitable for kernel ERMs that ensures robust accuracy with diverse loss functions. The non-interactive local DP ERM framework utilizing a polynomial approximation of the gradient of the objective is employed for the purpose. The proposed Algorithm 4 is a modified version of Algorithm 5 from Zheng et al. (2017), tailored for kernel learning. Denote $\\|\\mathcal{D}\\|:=\\operatorname*{max}_{y\\in\\mathcal{Y}}|y|$ . ", "page_idx": 6}, {"type": "text", "text": "Input: given data $\\left\\{(x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\right\\}$ , integers $m,p$ , and privacy budget $(\\epsilon,\\delta)$ . Output: DP data ", "page_idx": 7}, {"type": "text", "text": "1. $\\varphi^{(N y s)}$ , $\\left\\{b_{1},\\ldots,b_{m}\\right\\}\\gets\\epsilon/2$ -DP feature map and basis, output of Algorithm 1. 2. \u00b5 \u21902+2\u221a2 log \u03b41 wher $\\begin{array}{r l}{{3}.}&{\\widetilde{\\gamma}_{i}\\quad\\leftarrow\\quad\\bigg(y_{i}\\varphi^{(N y s)}(x_{i})+\\frac{1}{\\mu}\\varepsilon_{0}^{x},y_{i}\\varphi^{(N y s)}(x_{i})+\\frac{1}{\\mu}\\varepsilon_{1}^{x},\\dots,y_{i}\\varphi^{(N y s)}(x_{i})+\\frac{1}{\\mu}\\varepsilon_{\\frac{p}{2}\\frac{(p+1)}{2}}^{x}\\bigg)}\\\\ &{\\mathfrak{s}_{0}^{x}\\sim N(0,4\\Vert\\mathcal{Y}\\Vert^{2}I_{m\\times m})\\mathrm{~and~}\\varepsilon_{j}^{x}\\sim N(0,4p(p+1)\\Vert\\mathcal{Y}\\Vert^{2}I_{m\\times m})\\mathrm{~for~}j=1,\\dots,\\frac{p(p+1)}{2}.}\\end{array}$ 4. return {\u03b3i}in=1 ", "page_idx": 7}, {"type": "text", "text": "Theorem 8. Algorithm 4 is $(\\epsilon,\\delta){-}D P.$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this versatile learning problem, we add a new constraint that the parameter space has a radius $r>0$ , which is a common restriction in private ERM contexts (Kifer et al., 2012). Then the ERM problem can be written as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathcal{S},\\|f\\|_{\\mathcal{H}_{k}}\\leq r}\\frac{1}{n}\\sum_{i}l\\big(\\langle f,\\phi(x_{i})\\rangle_{\\mathcal{H}_{k}},y\\big)+\\frac{\\lambda}{2}\\|f\\|_{\\mathcal{H}_{k}}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We assume the following to ensure an efficient polynomial approximation of the loss by constraining its form. Note that the assumption accommodates a broader range of existing losses; for instance, logistic loss and smooth variants of hinge loss, compared to the assumption in Zheng et al. (2017). ", "page_idx": 7}, {"type": "text", "text": "Assumption 1. The loss function satisfies $l(\\hat{y},y)=l_{0}(\\hat{y}y)$ for convex, $c$ -Lipschitz $l_{0}:\\mathbb{R}\\rightarrow\\mathbb{R}_{\\geq0}$ , which is $b$ -smooth i.e., it is differentiable and satisfies $l_{0}(t_{1})-l_{0}(t_{2})\\leq l_{0}^{\\prime}(t_{2})(t_{1}-t_{2})+\\frac{b}{2}(t_{1}-t_{2})^{2}$ . Theorem 9 states that the optimization in Eq. (3) can be solved using the output of Algorithm 4 with a theoretical utility guarantee. ", "page_idx": 7}, {"type": "text", "text": "Theorem 9. Under Assumptions $^{\\,l}$ , for a regularization parameter $\\lambda>0$ and a pth degree polynomial $h_{i}$ , there exists an algorithm $\\boldsymbol{A}_{l,\\lambda}$ that takes the output of Algorithm $^{4}$ and returns a classifier with excess empirical risk: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(\\left(\\alpha^{2}+c^{2}+a_{\\infty}^{2}\\left(\\frac{r p}{\\mu}\\right)^{2p+1}\\right)\\frac{m R^{2}\\|\\mathcal{V}\\|^{2}\\log^{2}\\frac{1}{\\beta}}{n\\lambda\\mu^{2}}+\\alpha R\\|\\mathcal{V}\\|r\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with probability $1-\\beta$ , where $\\alpha:=\\,\\|l_{0}^{\\prime}(\\hat{y}y)-h(\\hat{y}y)\\|_{\\infty}$ on $|\\hat{y}y|\\leq\\ r\\|y\\|R,\\ a_{\\infty}$ is the maximum absolute value of the coefficients of $h$ , and $\\widetilde O$ is a big $O$ notation ignoring $\\log n$ factors. ", "page_idx": 7}, {"type": "text", "text": "Remarks. The bound in Theorem 9 relies heavily on the quality of the polynomial approximation of the gradient. For example, the Huber $10\\mathrm{ss}^{4}$ used for support vector machines (SVM), gives a poor guarantee: $O\\left((\\log n)^{-\\mathrm{i}}\\right)$ with Chebyshev polynomial approximation $h$ due to the lack of smoothness of the gradient. In contrast, smooth losses, such as the logistic loss $l(\\hat{y},y)=\\log(1+e^{-\\hat{y}y})$ , and the Linex loss $l(\\hat{y},y)=e^{a(1-\\hat{y}y)}-a(1-\\hat{y}y)-1$ , offer faster convergence of guaranteed excess empirical loss bounds: $O(n^{-c})$ for some $c>0$ . The algorithm $\\boldsymbol{A}_{l,\\lambda}$ in Theorem 9, detailed in Algorithm 7 in Appendix 6.2, utilizes an inexact oracle gradient method as outlined in Algorithm 3 of Dvurechensky and Gasnikov (2016). ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we demonstrate our proposed DP methods for DP kernel learning with simulated and real data. Specifics on the data generation can be found in Appendix 6.1. ", "page_idx": 7}, {"type": "text", "text": "The first example is designed to demonstrate the benefit of scalable DP kernel ERM for general kernels. The data comes from two classes with a polynomial boundary between them, making the non-translation-invariant polynomial kernel the most preferable choice for a kernel function. We consider three kernels: 3rd-order polynomial, Gaussian RBF, and linear kernels. Learning with the former two kernels is conducted as a DP linear ERM (SVM with Huber loss using the algorithm in Kifer et al. (2012)) with $m=200$ random features obtained by Algorithm 1 and random Fourier features, respectively. The test data classification accuracy shown in Figure 1(a) clearly demonstrates the superior performance of the polynomial kernel across a wide range of the privacy budget. ", "page_idx": 7}, {"type": "image", "img_path": "MdmzAezNHq/tmp/9dd336e937d1efc3f181086f4f68c4ac3c45d22e8048a7e3aecab96c6a53efd0.jpg", "img_caption": ["Figure 1: (a) Comparison of classification accuracy of scalable DP kernel ERMs with different kernels over a range of the privacy budget (b) Comparison of embedding errors of the proposed DP KME with alternative approaches. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We assess the estimation error of DP KME algorithms using the adult dataset (see Appendix 6.1 for details) with a Gaussian kernel for a fixed number of landmark points $m$ . Although Algorithm 3 uses the $K$ -means-based Nystr\u00f6m method, we also explore a subsampling-based version (outlined in Algorithm 6 in Appendix 6.2) with a privacy budget that is 100 times larger, as it demonstrates meaningful results in low privacy regions. We compare DP KME algorithms employing both methods, as well as Algorithm 1 in Balog et al. (2018). Figure 1(b) shows the superior performance of $K$ - means-based method among the three methods. The higher errors for Balog et al. can be attributed to the misalignment of distributions between random samples and the data, causing a larger error than the error added by the noise for privacy guarantee. In contrast, Nystr\u00f6m methods are data-dependent, incorporating data information into their kernel approximation. However, in highly private scenarios with small $\\epsilon$ values, this capability is compromised, leading to accuracy loss. Furthermore, the $K$ -means-based Nystr\u00f6m method outperforms subsampling. This is partly due to the characteristic of the DP $K$ -means algorithm, which adds noise inversely proportional to the cluster size, whereas the subsampling-based method adds noise indiscriminately. Consequently, DP $K$ -means ensures that larger, more significant clusters are less perturbed, and selects important landmark points with greater accuracy, leading to improved performance in DP kernel learning. Also, it appears that the quality of the subsampling-based landmark points deteriorates easily with the addition of noise. Kernel approximation through DP subsamples is more susceptible to degradation under privacy constraints compared to DP $K$ -means. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion, Limitations, and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we propose a $K$ -means Nystr\u00f6m-based private scalable kernel learning framework that is applicable for general kernels. We have developed DP kernel ERM, KME, and public data release mechanisms for versatile kernel learning within this framework. Theoretical and empirical investigations verify that our new framework is superior to existing ones, offering better performance with fewer constraints. ", "page_idx": 8}, {"type": "text", "text": "The Nystr\u00f6m method employed for the kernel matrix approximation has a few discussion points. First, we point out that utilizing DP $K$ -means for scalability does not necessarily imply that the data must contain discernible clusters; instead, it helps ensure that the landmark points more effectively retain the prominent features of the data. Second, a supervised Nystr\u00f6m method, if feasible, could yield better results in certain cases. Specifically, in kernel ridge regression, focusing on a few eigenvectors aligned with the response, rather than the entire kernel matrix, might require less DP noise and enable more efficient learning. ", "page_idx": 8}, {"type": "text", "text": "Even though our proposed framework is designed to work with general kernels, the current implementation is limited to handling Euclidean data due to its dependence on $K$ -means. A potential remedy for this limitation would be to consider a $K$ -medoids type clustering method, which ensures that the centroids are selected among actual observations. Further investigation into DP $K$ -medoids Nystr\u00f6m approximation and the development of subsequent private kernel learning methods are recommended as areas for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No.2022-0-00937, Solving the problem of increasing the usability and usefulness of synthetic data algorithms for statistical data.) The work of Jeongyoun Ahn was partially supported by the National Research Foundation of Korea (NRF2021R1A2C1093526, NRF-2022M3J6A1063021, RS-2023-00218231). The work of Cheolwoo Park was partially supported by the National Research Foundation of Korea (NRF-2021R1A2C1092925, NRF-2022M3J6A1063021). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abadi, W., Fezari, M., and Hamdi, R. (2015). Bag of visualwords and chi-squared kernel support vector machine: A way to improve hand gesture recognition. In Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication, pages 1\u20135.   \nAlashwal, H., Deris, S. B., and Othman, R. M. (2009). A bayesian kernel for the prediction of proteinprotein interactions. World Academy of Science, Engineering and Technology, International Journal of Computer, Electrical, Automation, Control and Information Engineering, 3(3):705\u2013710.   \nBalog, M., Tolstikhin, I., and Sch\u00f6lkopf, B. (2018). Differentially private database release via kernel mean embeddings. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 414\u2013422.   \nBassily, R., Feldman, V., Talwar, K., and Guha Thakurta, A. (2019). Private stochastic convex optimization with optimal rates. In Advances in Neural Information Processing Systems, volume 32.   \nBassily, R., Smith, A., and Thakurta, A. (2014). Private empirical risk minimization: Efficient algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pages 464\u2013473.   \nBecker, B. and Kohavi, R. (1996). Adult. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5XW20.   \nChapelle, O. (2007). Training a Support Vector Machine in the Primal. In Large-Scale Kernel Machines. The MIT Press.   \nChaudhuri, K., Monteleoni, C., and Sarwate, A. D. (2011). Differentially private empirical risk minimization. J. Mach. Learn. Res., 12(3):1069\u20131109.   \nDong, J., Roth, A., and Su, W. J. (2022). Gaussian differential privacy. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3\u201337.   \nDvurechensky, P. and Gasnikov, A. (2016). Stochastic intermediate gradient method for convex problems with stochastic inexact oracle. J Optim Theory Appl, 171:121\u2013145.   \nDwork, C., Kenthapadi, K., McSherry, F., Mironov, I., and Naor, M. (2006a). Our data, ourselves: Privacy via distributed noise generation. In Advances in Cryptology - EUROCRYPT 2006, pages 486\u2013503.   \nDwork, C., McSherry, F., Nissim, K., and Smith, A. (2006b). Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography, pages 265\u2013284.   \nFeldman, V., Koren, T., and Talwar, K. (2020). Private stochastic convex optimization: optimal rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, page 439\u2013449.   \nGopi, S., Lee, Y. T., and Liu, D. (2022). Private convex optimization via exponential mechanism. In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178, pages 1948\u20131989.   \nGopi, S., Lee, Y. T., Liu, D., Shen, R., and Tian, K. (2023). Private convex optimization in general norms. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 5068\u20135089.   \nGrauman, K. and Darrell, T. (2007). The pyramid match kernel: Efficient learning with sets of features. J. Mach. Learn. Res., 8:725\u2013760.   \nHall, R., Rinaldo, A., and Wasserman, L. (2013). Differential privacy for functions and functional data. J. Mach. Learn. Res., 14(1):703\u2013727.   \nHarder, F., Adamczewski, K., and Park, M. (2021). Dp-merf: Differentially private mean embeddings with randomfeatures for practical privacy-preserving data generation. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130, pages 1819\u20131827.   \nHe, L. and Zhang, H. (2018). Kernel k-means sampling for nystr\u00f6m approximation. IEEE Transactions on Image Processing, 27(5):2108\u20132120.   \nIyengar, R., Near, J. P., Song, D., Thakkar, O., Thakurta, A., and Wang, L. (2019). Towards practical differentially private convex optimization. In 2019 IEEE Symposium on Security and Privacy (SP), pages 299\u2013316.   \nJain, P. and Thakurta, A. (2013). Differentially private learning with kernels. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 118\u2013126.   \nJain, P. and Thakurta, A. G. (2014). (near) dimension independent risk bounds for differentially private learning. In Proceedings of the 31st International Conference on Machine Learning, volume 32, pages 476\u2013484.   \nKifer, D., Smith, A., and Thakurta, A. (2012). Private convex empirical risk minimization and high-dimensional regression. In Proceedings of the 25th Annual Conference on Learning Theory, volume 23, pages 25.1\u201325.40.   \nKumar, S., Mohri, M., and Talwalkar, A. (2012). Sampling methods for the nystr\u00f6m method. J. Mach. Learn. Res., 13:981\u20131006.   \nLafferty, J. and Lebanon, G. (2005). Diffusion kernels on statistical manifolds. J. Mach. Learn. Res., 6:129\u2013163.   \nLaurent, B. and Massart, P. (2000). Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302\u20131338.   \nMa, Y., Zhang, Q., Li, D., and Tian, Y. (2019). Linex support vector machine for large-scale classification. IEEE Access, 7:70319\u201370331.   \nMaji, S., Berg, A. C., and Malik, J. (2013). Efficient classification for additive kernel svms. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):66\u201377.   \nMangoubi, O. and Vishnoi, N. (2022). Sampling from log-concave distributions with infinity-distance guarantees. In Advances in Neural Information Processing Systems, volume 35, pages 12633\u2013 12646.   \nMcSherry, F. and Talwar, K. (2007). Mechanism design via differential privacy. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pages 94\u2013103.   \nMoro, S., Rita, P., and Cortez, P. (2014). Bank Marketing. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5K306.   \nMuandet, K., Fukumizu, K., Sriperumbudur, B., Sch\u00f6lkopf, B., et al. (2017). Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2):1\u2013 141. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Nikolov, A., Talwar, K., and Zhang, L. (2013). The geometry of differential privacy: the sparse and approximate cases. In Proceedings of the Forty-Fifth Annual ACM Symposium on Theory of Computing, STOC \u201913, page 351\u2013360. ", "page_idx": 11}, {"type": "text", "text": "Pinelis, I. (1994). Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, 22(4):1679\u20131706. ", "page_idx": 11}, {"type": "text", "text": "Song, S., Chaudhuri, K., and Sarwate, A. D. (2013). Stochastic gradient descent with differentially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pages 245\u2013248. ", "page_idx": 11}, {"type": "text", "text": "Wang, D., Chen, C., and Xu, J. (2019). Differentially private empirical risk minimization with non-convex loss functions. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 6526\u20136535. PMLR. ", "page_idx": 11}, {"type": "text", "text": "Wang, D., Ye, M., and Xu, J. (2017). Differentially private empirical risk minimization revisited: Faster and more general. In Advances in Neural Information Processing Systems, volume 30, pages 2722\u20132731. ", "page_idx": 11}, {"type": "text", "text": "Wu, X., Li, F., Kumar, A., Chaudhuri, K., Jha, S., and Naughton, J. (2017). Bolt-on differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1307\u20131322. ", "page_idx": 11}, {"type": "text", "text": "Yang, T., Li, Y.-f., Mahdavi, M., Jin, R., and Zhou, Z.-H. (2012). Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison. In Advances in Neural Information Processing Systems, volume 25, pages 476\u2013484. ", "page_idx": 11}, {"type": "text", "text": "Zhang, K., Tsang, I. W., and Kwok, J. T. (2008). Improved nystr\u00f6m low-rank approximation and error analysis. In Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 1232\u20131239. ", "page_idx": 11}, {"type": "text", "text": "Zheng, K., Mou, W., and Wang, L. (2017). Collect at once, use effectively: Making non-interactive locally private learning possible. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 4130\u20134139. PMLR. ", "page_idx": 11}, {"type": "text", "text": "6 Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "6.1 Experiment details and additional results ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We provide specifics on the two experiments: DP kernel ERM conducted on a simulated dataset and DP KME performed on the adult dataset. We use five Intel $^\\mathrm{\\textregistered}$ Xeon $^\\mathrm{\\textregistered}$ Gold 6248 processors for the computing resources in the simulation and one in the other experiments. ", "page_idx": 11}, {"type": "text", "text": "Privacy parameter. Throughout the experiments, we use privacy parameters $\\mathit{\\Pi}(\\epsilon,\\delta)\\quad\\in\\ {\\cal E}$ $\\{(10^{-1},\\stackrel{\\star}{n^{-2}}),(10^{-0.5},n^{-2}),\\stackrel{\\vee}{(10^{0},n^{-2})},(10^{0.5},n^{-2}),(10^{1},n^{-2})\\}$ , where $n$ represents the dataset size used in the algorithm. ", "page_idx": 11}, {"type": "text", "text": "Algorithm hyperparameters. Throughout the experiments, the hyperparameters for Algorithm 1 are set as follows: the distribution $Q$ is defined as a mixture of $m$ truncated normal distributions, each supported on $[0,1]^{d}$ and centered at $\\{z_{i}\\}_{i=1^{m}}$ , with standard deviation $\\sigma_{i}:=\\operatorname*{max}_{j\\neq i}\\|z_{j}-z_{i}\\|_{2}$ . Additionally, the thresholding parameter $m_{0}$ is set to $\\lfloor0.01n\\rfloor$ . ", "page_idx": 11}, {"type": "text", "text": "6.1.1 Scalable DP Kernel ERM ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Simulation dataset generation. The feature vector of the simulated dataset is drawn independently from a mixture of truncated normal distributions supported on $[0,1]^{200}$ . The following is the density formula: ", "page_idx": 11}, {"type": "equation", "text": "$$\nf(x)=\\frac{1}{4}\\sum_{i=1}^{4}d N_{t}(\\mu_{i},0.04I_{200\\times200}),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $d N_{t}(\\mu,\\sigma I_{200\\times200})$ is the density of the coordinate-wise truncated normal distribution, that is: ", "page_idx": 11}, {"type": "equation", "text": "$$\n(X_{1},\\dots,X_{200})\\sim d N_{t}(\\mu,\\sigma^{2}I_{200\\times200})\\Leftrightarrow X_{i}\\sim Z_{i}|Z_{i}\\in[0,1]\\,\\mathrm{val}\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The mean values of the truncated normal distribution are as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mu_{1}}&{=}&{\\underbrace{\\left(0.7,\\ldots,0.7\\right)}_{200}}\\\\ {\\mu_{2}}&{=}&{\\underbrace{\\left(0,\\ldots,0\\right)}_{200}}\\\\ {\\mu_{3}}&{=}&{\\underbrace{\\left(0.5,\\ldots,0.5,\\underbrace{0,\\ldots,0}_{100}\\right)}_{100}}\\\\ {\\mu_{4}}&{=}&{\\underbrace{\\left(0,\\ldots,0,0.5,\\ldots,0.5\\right)}_{100}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The response variable $y$ is obtained as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\ny=\\mathrm{sign}\\left(\\left(\\left[Z\\left(x-{\\frac{1}{2}}\\mathbf{1}\\right)\\right)^{\\otimes3}\\right]w+\\mathrm{noise}\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\otimes$ denotes the coordinate-wise power operation, $Z$ is a $20\\times200$ random matrix with entries drawn from the standard normal distribution, $w$ is a random vector following $N(\\mathbf{0},I_{20\\times20})$ , and 1 and 0 are vectors with all components equal to 0 or 1, respectively. The noise is assigned according to the standard normal distribution. Additionally, in the DP polynomial kernel ERM, we preprocess the data to ensure their norm remains below 1, a privacy-preserving procedure. The experiment was repeated 10 times, 1,000,000 training samples and 200,000 test samples were generated for each run to solve and evaluate the accuracy of the DP kernel ERM. ", "page_idx": 12}, {"type": "text", "text": "DP Kernel ERM hyperprameter. The regularization parameter $\\lambda\\quad\\qquad\\in$ $\\{10^{-5},10^{-4},10^{-3},10^{-2.5},1\\dot{0}^{-2}\\}$ is used in tuning. Figure 1a shows the highest accuracy achieved for each privacy parameter across these values of $\\lambda$ . ", "page_idx": 12}, {"type": "text", "text": "Preprocessing of the data in simulation Additionally, we constrain the $\\ell_{2}$ norm of the data to 1 when employing both linear ERM and polynomial kernel ERM. This step is essential for DP kernel ERM, particularly when the kernel is unbounded, as it requires the boundedness of the data. ", "page_idx": 12}, {"type": "text", "text": "Kernel used in DP Kernel ERM. The Gaussian kernel $e^{-\\frac{\\|x-y\\|_{2}^{2}}{512}}$ and the 3rd-order kernel $\\textstyle\\left({\\frac{\\langle x,y\\rangle+1}{2}}\\right)^{3}$ are used. The hyperparameter $\\sigma$ of the Gaussian kernel is selected by a grid search in a non-private kernel SVM setting: we generate 1000 training data and 200 test data, then compare the test error of the non-private kernel SVM for the Gaussian kernel e\u2212 2\u03c32 over $e^{-\\frac{\\|x-y\\|_{2}^{2}}{2\\sigma^{2}}}$ $\\sigma=2^{-2},2^{-1},\\ldots,2^{7}$ . The $\\sigma$ with the lowest test error is selected. Each non-private kernel SVM for the kernel is repeated 10 times, and the averaged errors are compared. The 3rd-order kernel is scaled to ensure its maximum value is at least 1 for data within a unit ball. ", "page_idx": 12}, {"type": "text", "text": "6.1.2 DP KME release experiment ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Data description. The real dataset, the adult dataset(Becker and Kohavi, 1996), can be downloaded from the UCI Machine Learning Repository. It consists of 48,842 observations for binary classification whether the income is above 50K or less than 50K. ", "page_idx": 12}, {"type": "text", "text": "Adult dataset processing. In the data processing, we follow the conventional practice of existing works on DP ERM, for example, Chaudhuri et al. (2011); Iyengar et al. (2019). Continuous features (all positive) underwent scaling by their maximum values, while categorical features were expanded via one-hot encoding. To eliminate collinearity, we omit the last column in each of the one-hot encoded features, resulting in a dimensionality of 103. ", "page_idx": 12}, {"type": "text", "text": "Kernel choice. For the empirical KME release, we use the entire dataset to produce a DP KME based on the Gaussian kernel $k(x,y)=e^{-\\frac{\\|x-y\\|_{2}^{2}}{2}}$ , and $m=\\lfloor{\\sqrt{n}}\\rfloor$ dimension of random features for comparison. ", "page_idx": 12}, {"type": "text", "text": "Approximation evaluation. The approximation error is evaluated by the RKHS distance between the empirical KME and the DP KME. The experiment is repeated 10 times, and the average error along with the standard deviation for each privacy parameter $\\epsilon$ are displayed in Figure 1b. ", "page_idx": 12}, {"type": "text", "text": "DP KME algorithms. We compare Algorithm 1 from Balog et al. (2018) with our DP KME algorithms, which use DP $K$ -means and subsampling-based Nystr\u00f6m methods. The DP $K$ -meansbased algorithm corresponds to Algorithm 3, while the subsampling-based algorithm is described in Algorithm 6. Algorithm 1 from Balog et al. (2018) is implemented using their code, with a single modification: the random sample distribution is changed from the normal distribution to the uniform distribution on the unit cube, as our data is preprocessed to fit within this unit cube. ", "page_idx": 13}, {"type": "text", "text": "6.1.3 Additional experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Landmark quality: kernel approximation experiments. We evaluate the performance of three DP landmark-based methods and random Fourier features-based low-rank kernel approximation, across various settings of rank $m$ . The rank of the approximation corresponds to the number of the landmark points in the Nystr\u00f6m methods and the dimension of the random features in the random Fourier features method. Notably, even when the number of landmark points is small, random Fourier features tend to perform less effectively than Nystr\u00f6m-based methods. ", "page_idx": 13}, {"type": "text", "text": "The kernel matrices are calculated from the Gaussian kernels $k(x,y)=e^{-\\frac{\\|x-y\\|_{2}^{2}}{2\\sigma^{2}}}$ for $\\sigma=2^{i}$ with $i=0,1,2,3$ across the entire adult datasets. The approximation error is assessed by the relative error with respect to the Frobenius norm5: $\\frac{||\\mathbf{K}-\\hat{\\mathbf{K}}||_{F}}{||\\mathbf{K}||_{F}}$ . The experiments are repeated 10 times, and the averages of the relative errors along with their standard deviations are presented in Figure 2. ", "page_idx": 13}, {"type": "text", "text": "Privacy allocation. In Algorithm 2, we allocate the privacy budgets equally between the construction of DP random features and DP ERM. Figure 3 compares the accuracies of DP kernel ERM using Algorithm 2 with various privacy allocations. The blue line shows allocating $20\\%$ of the privacy budget to the DP kernel random features construction and $80\\%$ to the DP ERM achieves better accuracy than allocating them equally. A possible heuristic rule for determining the optimal privacy allocation ratio is to allocate the budget depending on the strength of the clustered structures in the data. For instance, when the number of landmarks $m$ is small relative to the sample size $n$ , it is advisable to allocate a larger portion of the privacy budget to the linear ERM rather than the DP $K$ -means. This is because DP $K$ -means algorithms are more accurate when $m$ is small. A smaller $m$ typically results in larger cluster sizes. Since DP $K$ -means algorithms acquire private centroids by averaging the members of each cluster privately, larger clusters tend to lead to more accurate centroids given fixed privacy budgets. Therefore, we can afford to allocate more privacy resources to linear ERM. ", "page_idx": 13}, {"type": "text", "text": "DP KME experiments. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In what follows, we present a comparison between Algorithm 3 and other algorithms (Algorithm 6 and Algorithm 1 in Balog et al. (2018)) using datasets other than adult datasets in Figure 4. The datasets used include bank, CDC, credit, and MNIST, which are described below. Due to the large size of these datasets, only 30,000 examples are utilized in the experiments. ", "page_idx": 13}, {"type": "text", "text": "Bank. The Bank dataset pertains to telemarketing phone calls made by a Portuguese banking institution from 2008 to 2013 (Moro et al., 2014). It includes information on clients\u2019 financial and social backgrounds, as well as contact details from the bank, with a binary label indicating whether a deposit subscription was made. This dataset is available in the UCI database (https://archive. ics.uci.edu/dataset/222/bank+marketing). It contains 45,211 examples, comprising 5,289 positive cases and 39,922 negative cases. The dataset features 6 numerical variables and 8 categorical variables, which are one-hot encoded. ", "page_idx": 13}, {"type": "text", "text": "CDC. The CDC dataset is part of the CDC\u2019s BRFSS 2015, which consists of responses collected from Americans in the CDC\u2019s annual health-related telephone survey. This dataset includes relevant information for diabetes prediction and is available in the UCI database (https://www.archive. ics.uci.edu/dataset/891/cdc+diabetes+health+indicators). It contains demographic information, health history, personal details, and diabetes diagnoses of the respondents. The dataset comprises 253,680 examples, with 218,334 negative cases and 35,346 positive cases. It includes 21 numerical features along with a binary label. ", "page_idx": 13}, {"type": "text", "text": "Credt. The Credit dataset is related to the transactions made by European credit cardholders and is available on Kaggle (https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). This ", "page_idx": 13}, {"type": "image", "img_path": "MdmzAezNHq/tmp/0768f076b58507c4d778b3fa6917d600f9d9497872ce4cec4ca688e9b85b721d.jpg", "img_caption": ["Figure 2: Comparison of four methods for various settings. In each figure, the $x$ axis represents the privacy budget, and the y axis \u2225K\u2225K\u2212\u2225K\u02c6F\u2225F. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "MdmzAezNHq/tmp/80a4703f49aeeb5164574655ac596c9e380f7104e5a818b916b41da39e6ed331.jpg", "img_caption": ["Figure 3: Classification accuracy under varying privacy budget allocations "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "MdmzAezNHq/tmp/a1dd7407ff7b17f0878d3d63da3c35338417e9c5eb01e2bfe96b31cbe2a0a72e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 4: DP KME estimation results for Bank data ( $1^{\\mathrm{st}}$ column), CDC data $2^{\\mathrm{nd}}$ column), Credit data $3^{\\mathrm{rd}}$ column), and MNIST data $\\mathrm{{4^{th}}}$ column), using a Gaussian kernel with $\\sigma=2$ ( $1^{\\mathrm{st}}$ row), and polynomial kernels of degree 2 ( $\\!2^{\\mathrm{nd}}$ row) and 4 $3^{\\mathrm{rd}}$ row) ", "page_idx": 15}, {"type": "text", "text": "dataset contains 30 numerical features, two of which are \u2018Time\u2019 and \u2018Amount,\u2019 indicating the time and amount of each transaction. The remaining 28 features are principal components derived from principal components analysis due to confidentiality concerns. The dataset includes a total of 284,807 examples, with 492 labeled as fraudulent and 284,315 as non-fraudulent. ", "page_idx": 15}, {"type": "text", "text": "MNIST. The MNIST dataset contains 60,000 images of handwritten digits and can be downloaded using the PyTorch package. Each image is represented by 784 numerical features, corresponding to the pixels in a $28\\!\\times\\!28$ image, along with a categorical label indicating the digits from 0 to 9. ", "page_idx": 15}, {"type": "text", "text": "6.2 Additional algorithms ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Denote $\\lVert\\cdot\\rVert_{o p}$ the operator norm of a square matrix. ", "page_idx": 15}, {"type": "table", "img_path": "MdmzAezNHq/tmp/e643d35d0fc4c2aae1b6265bd7f65d031c4b03e7c4515c0a07714e1571776fb8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm 6 outputs the DP Nystr\u00f6m-based random feature map using a subsampling Nystr\u00f6m method. The algorithm is $\\epsilon$ -DP. The proof is in the Appendix. Unlike Algorithm 1, no thresholding parameter is included since subsampling treats $m$ subsampled data equally whereas $K$ -means centroids receive noise inversely to the cluster size they belong. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 7 uses the output of Algorithm 4 to solve the kernel ERM for a given polynomial $h$ that approximates the gradient of the given loss function, and a regularization parameter $\\lambda\\,>\\,0$ . No measure for privacy guarantee is included in Algorithm 7 since the algorithm uses only the privatized data, the output of Algorithm 4, and does not reference the original data. ", "page_idx": 15}, {"type": "text", "text": "Input: given data $\\{\\boldsymbol{x}_{1},\\ldots,\\boldsymbol{x}_{n}\\}\\subset[0,1]^{d}$ , random features of dimension $m$ , and privacy parameter $\\epsilon,\\delta$ . ", "page_idx": 16}, {"type": "text", "text": "Output: a random feature mapping $\\varphi^{(N y s)}:\\mathbb{R}^{d}\\to\\mathbb{R}^{m}$ , and $\\left\\{b_{1},\\ldots,b_{m}\\right\\}\\subset\\mathcal{H}_{k}$ . ", "page_idx": 16}, {"type": "text", "text": "1. $K\\gets\\lfloor m_{0}\\epsilon\\rfloor$ . $\\{z_{1}^{0},\\bar{\\dots},z_{m}^{0\\bar{\\big)}}\\}\\leftarrow m$ uniform subsamples of $\\{x_{1},\\ldots,x_{n}\\}$ without replacement. log 1+ nd (e2\u03f5 \u22121) \u03b5i where \u03b5i is random vector with i.i.d. components following a Laplace distribution with density $\\begin{array}{r}{\\nu(x)=\\frac{1}{2}e^{-\\frac{|x|}{2}}}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "$\\mathbf{\\Psi}^{1,\\mathbf{\\Delta}\\mathbf{U}\\Sigma\\mathbf{U}^{T}}\\leftarrow$ the unitary diagonalization of a matrix $[k(z_{i},z_{j})]_{m\\times m}$ . 5. $\\begin{array}{r}{\\varphi^{(N y s)}\\gets\\ \\varphi^{(N y s)}(x):=\\frac{1}{R}{\\sum^{\\dagger\\frac{1}{2}}}{\\mathbf{U}^{T}}[k(z_{1},x),\\dotsc,\\dot{k}(z_{m},\\dot{x})]^{T}.}\\end{array}$ . 6. $\\begin{array}{r}{b_{i}\\leftarrow\\sum_{j=1}^{m}[\\mathbf{\\Sigma}^{\\dagger\\frac{1}{2}}\\mathbf{U}^{T}]_{i j}\\phi(z_{j})}\\end{array}$ for $i=1,2,\\dots,m$ . 7. return $\\varphi^{(N y s)},\\left\\{b_{1},\\dots,b_{m}\\right\\}$ ", "page_idx": 16}, {"type": "text", "text": "Algorithm 7 Versatile DP kernel ERM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: given output $\\{\\widetilde{z}_{i}\\}_{i=1}^{n}$ of Algorithm 4, integer $m$ , $h$   \nOutput: DP solution $f$ 1. for $s=0,\\ldots,n-1$ do 2. $\\begin{array}{r}{\\widetilde{G}(w_{s};\\widetilde{z}_{i})\\gets\\left(\\sum_{k=0}^{p}a_{k}\\prod_{j=1}^{k}w_{s}^{T}\\widetilde{z}_{i,\\frac{k(k-1)}{2}+j}\\right)\\widetilde{z}_{i0}+\\lambda w_{s}}\\end{array}$ 3. Apply Algorithm 3 in Dvurechensky and Gasnikov (2016) using $\\hat{G}$ as the inexact gradient. 4. end for 5. return $\\begin{array}{r}{\\widetilde{f}=\\sum_{i=1}^{m}[w_{n}]_{i}b_{i}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "6.3 Proofs of theorems ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "6.3.1 Pinelis\u2019s inequality ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "According to Theorem 3.1 in Pinelis (1994), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{j}\\lvert\\lvert f_{j}\\rvert\\rvert_{\\mathcal{H}_{k}}\\geq r\\right)\\leq2e^{-\\lambda r+\\left\\lVert\\sum_{j=1}^{\\infty}\\mathbb{E}_{j-1}\\left[e^{\\lambda\\lVert d_{j}\\rVert\\mathcal{H}_{k}}-1-\\lambda\\rVert d_{j}\\rVert\\varkappa_{k}\\right]\\rVert_{\\infty}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any $\\lambda\\:>\\:0$ , where $\\{f_{j}\\}_{j=1}^{\\infty}$ is a martingale on $\\mathcal{H}_{k}$ and $d_{j}\\;=\\;f_{j}\\,-\\,f_{j-1}$ with $d_{0}~=~0$ . Let $\\begin{array}{r}{f_{j}=\\sum_{i=1}^{j}\\frac{1}{n}(\\phi(x_{i})-\\mu_{X}-\\mathrm{proj}_{S}(\\phi(x_{i})-\\mu_{X}))}\\end{array}$ if $j\\leq n$ and $f_{j}\\,=\\,f_{n}$ for $j>n$ . Then, $f_{j}$ is a martingale on $\\mathcal{H}_{k}$ so according to Eq. (4), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}\\left(\\left\\|\\displaystyle\\sum_{i=1}^{j}\\frac{1}{n}(\\phi(x_{i})-\\mu_{X}-\\mathrm{proj}_{S}(\\phi(x_{i})-\\mu_{X}))\\right\\|_{\\mathcal{H}_{k}}\\ge r\\right)}&{\\le}&{\\mathbb{P}\\left(\\underset{j}{\\operatorname*{sup}}\\|f_{j}\\|_{\\mathcal{H}_{k}}\\ge r\\right)}\\\\ &{}&{\\le}&{2e^{-\\lambda r+n\\left(e^{\\frac{2R}{n}\\lambda}-1-\\frac{2R}{n}\\lambda\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $\\|\\phi(x_{i})\\,-\\,\\mu_{X}\\|_{\\mathcal{H}_{k}}\\ \\leq\\ 2R$ and a function $e^{x}\\,-\\,1\\,-\\,x$ increases on $(0,\\infty)$ . Setting $\\lambda\\ =$ $\\textstyle{\\frac{n}{2R}}\\log\\left(1+{\\frac{r}{2R}}\\right)$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\sum_{i=1}^{j}w_{i}(\\phi(x_{i})-\\mu_{X}-\\mathtt{p r o j}_{S}(\\phi(x_{i})-\\mu_{X}))\\right\\rVert_{\\mathcal{H}_{k}}\\geq r\\right)\\leq e^{\\frac{n r}{2R}-n\\left(\\frac{r}{2R}+1\\right)\\log\\left(1+\\frac{r}{2R}\\right)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Setting $r=2R t$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\sum_{i=1}^{j}w_{i}(\\phi(x_{i})-\\mu_{X}-\\operatorname{proj}_{S}(\\phi(x_{i})-\\mu_{X}))\\right\\rVert_{\\mathcal{H}_{k}}\\geq2R t\\right)\\leq e^{-n t-n(1+t)\\log(1+t)}\\leq e^{-\\frac{n t^{2}}{2(1+t)}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $\\begin{array}{r l r}{\\log\\frac{1}{1+t}}&{=}&{-\\frac{t}{1+t}\\;-\\;\\frac{1}{2}\\left(\\frac{t}{1+t}\\right)^{2}\\;-\\;\\cdot\\cdot\\;\\leq\\;\\;-\\frac{t}{1+t}\\;-\\;\\frac{1}{2}\\left(\\frac{t}{1+t}\\right)^{2}}\\end{array}$ . Putting $\\begin{array}{l l l}{t}&{=}&{\\frac{1}{n}\\log{\\frac{1}{\\beta}}\\ +}\\end{array}$ $\\textstyle{\\sqrt{{\\frac{2}{n}}\\log{\\frac{1}{\\beta}}+{\\frac{1}{n^{2}}}\\log^{2}{\\frac{1}{\\beta}}}}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\lVert\\sum_{i=1}^{j}w_{i}(\\phi(x_{i})-\\mu_{X}-\\mathrm{proj}_{S}(\\phi(x_{i})-\\mu_{X}))\\right\\rVert_{\\mathcal{H}_{k}}\\ge2R\\left(\\frac1n\\log\\frac1\\beta+\\sqrt{\\frac2n\\log\\frac1\\beta+\\frac1{n^{2}}\\log^{2}\\frac1\\beta}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at most $\\beta$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\sum_{i=1}^{j}w_{i}(\\phi(x_{i})-\\mu_{X}-\\mathrm{proj}_{S}(\\phi(x_{i})-\\mu_{X}))\\right\\rVert_{\\mathcal{H}_{k}}\\geq2R\\left(\\frac2n\\log\\frac1\\beta+\\sqrt{\\frac2n\\log\\frac1\\beta}\\right)\\right)\\leq\\beta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 1 (Lemma 1 in Laurent and Massart (2000)). $\\begin{array}{r}{\\mathbb{P}\\left(\\|\\varepsilon\\|_{2}\\leq\\sqrt{n}+\\sqrt{2\\log\\frac{1}{\\beta}}\\right)\\geq1-\\beta f}\\end{array}$ or $\\beta\\in(0,1]$ where $\\varepsilon$ is a n diemnsional standard normal distribution. ", "page_idx": 17}, {"type": "text", "text": "6.3.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Cauchy-Swartz inequality gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{n^{2}}\\|\\mathbf{K}-\\bar{\\mathbf{K}}\\|_{F}^{2}}&{=\\phantom{\\displaystyle\\frac{1}{n^{2}}}\\displaystyle\\sum_{i,j=1}^{n}\\Big(k(x_{i},x_{j})-\\langle\\varphi_{Z}^{(N_{R})}(x_{i}),\\varphi_{Z}^{(N_{R})}(x_{j})\\rangle_{\\mathcal{H}_{k}}\\Big)^{2}}\\\\ &{=\\phantom{\\displaystyle\\frac{1}{n^{2}}}\\displaystyle\\sum_{i,j=1}^{n}\\big(\\langle\\phi(x_{i})-\\mathrm{proj}_{S}\\phi(x_{i}),\\phi(x_{j})-\\mathrm{proj}_{S}\\phi(x_{j})\\rangle_{\\mathcal{H}_{k}}\\big)^{2}}\\\\ &{\\leq\\phantom{\\displaystyle\\frac{1}{n^{2}}}\\displaystyle\\sum_{i,j=1}^{n}\\|\\phi(x_{i})-\\mathrm{proj}_{S}\\phi(x_{i})\\|_{\\mathcal{H}_{k}}^{2}\\|\\phi(x_{j})-\\mathrm{proj}_{S}\\phi(x_{j})\\|_{\\mathcal{H}_{k}}^{2}}\\\\ &{=\\phantom{\\displaystyle\\left(\\frac{1}{n}\\displaystyle\\sum_{i=1}^{N}\\|\\phi(x_{i})-\\mathrm{proj}_{S}\\phi(x_{i})\\|_{\\mathcal{H}_{k}}^{2}\\right)^{2}}}\\\\ &{\\leq\\phantom{\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{N}\\|\\phi(x_{i})-\\mathrm{proj}_{S}\\phi(x_{i})\\|_{\\mathcal{H}_{k}}^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Also, the Lipschitzness of $k$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|\\phi(x_{i})-\\mathrm{proj}_{S}\\phi(x_{i})\\|_{\\mathcal{H}_{k}}^{2}}&{\\leq}&{k(x_{i},x_{i})-\\displaystyle\\frac{k(x_{i},z_{j})^{2}}{k(z_{j},z_{j})}}\\\\ &{\\leq}&{k(x_{i},z_{j})+c^{\\prime}\\|x_{i}-z_{j}\\|_{2}-\\displaystyle\\frac{k(x_{i},z_{j})^{2}}{k(x_{i},z_{j})+c^{\\prime}\\|x_{i}-z_{j}\\|_{2}}}\\\\ &{\\leq}&{\\displaystyle\\frac{(k(x_{i},z_{j})+c^{\\prime}\\|x_{i}-z_{j}\\|_{2})^{2}-k(x_{i},z_{j})^{2}}{k(x_{i},z_{j})+c^{\\prime}\\|x_{i}-z_{j}\\|_{2}}}\\\\ &{\\leq}&{c^{\\prime}\\|x_{i}-z_{j}\\|_{2}\\displaystyle\\frac{2k(x_{i},z_{j})+c^{\\prime}\\|x_{i}-z_{j}\\|_{2}}{k(x_{i},z_{j})+c^{\\prime}\\|x_{i}-z_{j}\\|_{2}}}\\\\ &{\\leq}&{2c^{\\prime}\\|x_{i}-z_{j}\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for every $z_{j}\\in Z$ . Thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{F}^{2}\\leq2c^{\\prime}\\sqrt{n\\sum_{i=1}^{n}\\lVert x_{i}-z_{j}\\rVert_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Choosing $z_{j}\\mathbf{s}$ to be the $K$ -means centroids, we obtain the desired. ", "page_idx": 17}, {"type": "text", "text": "6.3.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The post-processing property of the differential privacy guarantees privacy since the landmark points $z_{1},\\dots,z_{m}$ were obtained differentially privately. ", "page_idx": 17}, {"type": "text", "text": "Additionally, we show $\\{b_{i}\\}_{i=1}^{m}$ is an orthonormal basis of $\\boldsymbol{S}$ , and $\\begin{array}{r l}{R[\\varphi^{(N y s)}]}&{{}=}\\end{array}$ $\\langle\\mathrm{proj}_{S}\\phi(x)\\dot{,}b_{i}\\rangle_{\\mathcal{H}_{k}}$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{\\langle b_{i},b_{j}\\rangle_{\\mathcal{H}_{k}}}}&{{=}}&{{\\displaystyle\\sum_{k,l}[{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}{\\bf{U}}^{T}]_{i k}\\langle\\phi(z_{k}),\\phi(z_{l})\\rangle_{\\mathcal{H}_{k}}[{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}{\\bf{U}}^{T}]_{j l}}}\\\\ {{\\displaystyle}}&{{=}}&{{\\displaystyle\\sum_{k,l}[{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}{\\bf{U}}^{T}]_{i k}[{\\bf{K}}_{\\bf{Z}}]_{k l}[{\\bf{U}}{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}]_{l j}}}\\\\ {{\\displaystyle}}&{{=}}&{{[{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}{\\bf{U}}^{T}{\\bf{K}}_{\\bf{Z}}{\\bf{U}}{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}]_{i j}}}\\\\ {{\\displaystyle}}&{{=}}&{{[{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}{\\bf{\\Sigma}}\\Sigma{\\bf{\\Sigma}}^{\\dagger\\,\\frac{1}{2}}]_{i j}}}\\\\ {{\\displaystyle}}&{{=}}&{{\\bf{\\Sigma}}(i=j\\leq t_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $t_{0}$ is the rank of $\\Sigma$ or equivalently the rank of $\\mathbf{K}_{\\mathbf{Z}}$ . Thus the members of $\\{b_{i}\\}_{i=1}^{m}$ are orthonormal to each other or are zero. Ignoring the zero terms we get the orthonormal subset of $\\boldsymbol{S}$ . Also, the dimension of the spanned subspace $\\boldsymbol{S}$ coincides to the rank of $\\mathbf{K}\\mathbf{z}$ . Thus the subset is also a basis. ", "page_idx": 18}, {"type": "text", "text": "Finally, $R[\\varphi^{(N y s)}]=\\langle{\\mathrm{proj}}_{S}\\phi(x),b_{i}\\rangle_{\\mathcal{H}_{k}}$ follows from the direct calculation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{\\langle\\mathrm{proj}_{S}\\phi(x),b_{i}\\rangle_{\\mathcal{H}_{k}}}}&{{:=}}&{{\\langle\\phi(x),b_{i}\\rangle_{\\mathcal{H}_{k}}}}\\\\ {{}}&{{=}}&{{\\langle\\phi(x),\\displaystyle\\sum_{j=1}^{m}[{\\bf{E}}^{\\dagger}{}^{\\frac{1}{2}}{\\bf{U}}^{T}]_{i j}\\phi(z_{j})\\rangle_{\\mathcal{H}_{k}}}}\\\\ {{}}&{{=}}&{{\\displaystyle\\sum_{j=1}^{m}[{\\bf{E}}^{\\dagger}{}^{\\frac{1}{2}}{\\bf{U}}^{T}]_{i j}\\langle\\phi(x),\\phi(z_{j})\\rangle_{\\mathcal{H}_{k}}}}\\\\ {{}}&{{=}}&{{\\displaystyle\\sum_{j=1}^{m}[{\\bf{E}}^{\\dagger}{}^{\\frac{1}{2}}{\\bf{U}}^{T}]_{i j}k(z_{j},x)}}\\\\ {{}}&{{=}}&{{R\\displaystyle\\mathcal{C}^{(N\\it{y s})}(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "6.3.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The algorithm is a composition of two DP algorithms so the $(\\epsilon,\\delta)$ -DP is guaranteed by Proposition 2. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Denote the DP linear algorithm as $M$ then the excess empirical risk is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{L}^{\\lambda}\\left(\\underset{i}{\\sum}[M(\\varphi^{(N y s)}(x_{1}),\\dots,\\varphi^{(N y s)}(x_{n}))]_{i}b_{i}\\right)-\\underset{f\\in\\mathcal{H}_{k}}{\\min}\\hat{L}^{\\lambda}(f)}\\\\ {=}&{\\hat{L}^{\\lambda}(M(\\phi(x_{1}),\\dots,\\phi(x_{n})))-\\underset{f\\in\\mathcal{S}}{\\min}\\hat{L}^{\\lambda}(f)+\\underset{f\\in\\mathcal{S}}{\\min}\\hat{L}^{\\lambda}(f)-\\underset{f\\in\\mathcal{H}_{k}}{\\min}\\hat{L}^{\\lambda}(f)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $b_{i}\\mathbf s$ are the output of Algorithm 1, the basis of $\\boldsymbol{S}$ , and $[M(\\varphi^{(N y s)}(x_{1}),\\dots,\\varphi^{(N y s)}(x_{n}))]_{i}$ denotes the ith component of the output of the DP linear algorithm from input $\\{\\varphi^{(N y s)}(x_{i})\\}_{i=1}^{n}$ . ", "page_idx": 18}, {"type": "text", "text": "The random feature error is bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in S}\\hat{L}^{\\lambda}(f)-\\operatorname*{min}_{f\\in\\mathcal{H}_{k}}\\hat{L}^{\\lambda}(f)\\leq\\frac{L\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{2}}{2\\lambda n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by Lemma 1 in the proof of Theorem 2 in Yang et al. (2012). Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{L}^{\\lambda}\\left(\\sum_{i}[M(\\varphi^{(N y s)}(x_{1}),\\dots,\\varphi^{(N y s)}(x_{n}))]_{i}b_{i}\\right)-\\operatorname*{min}_{f\\in\\mathcal{H}_{k}}\\hat{L}^{\\lambda}(f)\\le\\mathcal{E}_{n,m}(\\beta)+\\frac{L\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{2}}{2\\lambda n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with probability $1-\\beta$ . ", "page_idx": 18}, {"type": "text", "text": "6.3.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. We show releasing $w$ in line 2 is $(\\epsilon/2,\\delta)$ -DP. Then Proposition 2 gaurantees the overall algorithm satisfies $(\\epsilon,\\delta)$ -DP. ", "page_idx": 19}, {"type": "text", "text": "The $\\ell_{2}$ norm of the difference between outputs of deterministic algorithm releasing $\\textstyle{\\frac{1}{n}}\\sum R\\varphi^{(N y s)}(x_{i})$ for neighboring datasets is $\\frac{2R}{n}$ . Applying Proposition 1 guarantees that releasing $w$ is $(\\epsilon/2,\\delta)$ -DP. ", "page_idx": 19}, {"type": "text", "text": "6.3.6 Proof of Theorem 6 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Denote \u00b5\u02c6(XN $\\begin{array}{r}{\\hat{\\mu}_{X}^{(N y s)}\\;:=\\;\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}R[\\varphi^{(N y s)}(x_{i})]_{j}b_{j}}\\end{array}$ . Since $\\begin{array}{r l r}{\\lefteqn{R\\sum_{j=1}^{m}R[\\varphi^{(N y s)}(x_{i})]_{j}b_{j}\\!\\!\\!}}\\end{array}=$ $\\mathrm{proj}_{S}\\phi(\\boldsymbol{x}_{i})$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\left\\|\\hat{\\mu}_{X}-\\hat{\\mu}_{X}^{(N y s)}\\right\\|_{\\mathcal{H}_{k}}}&{=}&{\\displaystyle\\frac{1}{n}\\left\\|\\sum_{i=1}^{n}\\phi(x_{i})-\\mathrm{proj}_{S}\\phi(x_{i})\\right\\|_{\\mathcal{H}_{k}}}\\\\ &{=}&{\\displaystyle\\frac{\\sqrt{\\mathbf{1}^{T}(\\mathbf{K}-\\hat{\\mathbf{K}})\\mathbf{1}}}{n}}\\\\ &{\\le}&{\\displaystyle\\frac{\\|\\mathbf{K}-\\hat{\\mathbf{K}}\\|_{2}}{\\sqrt{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Also, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\mu}_{X}-\\widehat{\\mu}_{X}^{(N y s)}\\right\\|_{\\mathcal{H}_{k}}=\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varphi^{(N y s)}(x_{i})-w\\right\\|_{2}=\\frac{2R\\left(1+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)}{n\\epsilon}\\|\\varepsilon\\|_{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and from Lemma 1, we have $\\begin{array}{r}{\\mathbb{P}\\left(\\|\\varepsilon\\|_{2}\\leq\\sqrt{m}+\\sqrt{2\\log\\frac{1}{\\beta}}\\right)\\geq1-\\beta}\\end{array}$ . Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\mu}_{X}-\\widehat{\\mu}_{X}^{(N y s)}\\right\\|_{\\mathcal{H}_{k}}\\le\\frac{2R\\left(1+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)}{n\\epsilon}\\left(\\sqrt{m}+\\sqrt{2\\log\\frac{1}{\\beta}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability $1-\\beta$ . ", "page_idx": 19}, {"type": "text", "text": "6.3.7 Proof of Theorem 7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Denote \u00b5\u02c6(Xin $\\begin{array}{r}{\\hat{\\mu}_{X}^{\\mathrm{(ind)}}\\,:=\\,\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}R[\\varphi^{(i)}(x_{i})]_{j}b_{j}}\\end{array}$ where $\\varphi^{(i)}$ is a random feature map from Algorithm 1 replacing $\\{z_{i}\\}_{i=1}^{k}$ to data independent landmark. Since $\\begin{array}{r l r}{\\lefteqn{R\\sum_{j=1}^{m}R[\\varphi^{(i)}(x_{i})]_{j}b_{j}\\!\\!\\!}}\\end{array}=$ $\\mathrm{proj}_{S}\\phi({\\boldsymbol{x}}_{i})$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mu}_{X}^{\\mathrm{(ind)}}-\\hat{\\mu}_{X}\\right\\|_{\\mathcal{H}_{k}}\\;\\;=\\;\\;\\left.\\frac{1}{n}\\left\\|\\sum_{i=1}^{n}\\phi(x_{i})-\\mathrm{proj}_{S}\\phi(x_{i})\\right\\|_{\\mathcal{H}_{k}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The Pinelis inequality gives: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\sum_{i=1}^{j}\\frac{1}{n}(\\phi(x_{i})-\\mu_{X}-\\mathrm{proj}_{S}(\\phi(x_{i})-\\mu_{X}))\\right\\rVert_{\\mathcal{H}_{k}}\\geq2R\\left(\\frac2n\\log\\frac1\\beta+\\sqrt{\\frac2n\\log\\frac1\\beta}\\right)\\right)\\leq\\beta.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\hat{\\mu}_{X}^{\\mathrm{(ind)}}-\\hat{\\mu}_{X}\\right\\rVert_{\\mathcal{H}_{k}}\\leq2R\\left(\\frac2n\\log\\frac1\\beta+\\sqrt{\\frac2n\\log\\frac1\\beta}\\right)+\\left\\lVert\\mu_{X}-\\mathrm{proj}_{S}\\mu_{X}\\right\\rVert_{\\mathcal{H}_{k}}\\right)\\geq1-\\beta.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Also, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\|\\widetilde{\\mu}_{X}^{\\mathrm{(ind)}}-\\widehat{\\mu}_{X}^{\\mathrm{(ind)}}\\right\\|_{\\mathcal{H}_{k}}}&{=}&{\\frac{2R\\,\\left(1+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)}{n\\epsilon}\\|\\varepsilon\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and from Lemma 1, we have $\\begin{array}{r}{\\mathbb{P}\\left(\\|\\varepsilon\\|_{2}\\leq\\sqrt{m}+\\sqrt{2\\log\\frac{1}{\\beta}}\\right)\\geq1-\\beta}\\end{array}$ . Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\mu}_{X}^{\\mathrm{(ind)}}-\\widehat{\\mu}_{X}\\right\\|_{\\mathcal{H}_{k}}\\leq2R\\left(\\frac{2}{n}\\log\\frac{1}{\\beta}+\\sqrt{\\frac{2}{n}\\log\\frac{1}{\\beta}}+\\mathrm{err}_{p r i v}\\right)+\\|\\mu_{X}-\\mathrm{proj}_{S}\\mu_{X}\\|_{\\mathcal{H}_{k}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability $1-2\\beta$ where $\\begin{array}{r}{\\operatorname{err}_{p r i v}=\\frac{\\left(\\sqrt{2}+2\\sqrt{\\log\\frac{1}{\\delta}}\\right)}{n\\epsilon}\\left(\\sqrt{m}+\\sqrt{2\\log\\frac{1}{\\beta}}\\right)\\!.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "6.3.8 Proof of Theorem 8 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We show line 3 is $(\\epsilon/2,\\delta)$ -DP. Then the Proposition 2 guarantees the overall algorithm satisfies $(\\epsilon,\\delta)$ -DP. ", "page_idx": 20}, {"type": "text", "text": "Proposition 3 (Composition theorem for Gaussian noise addition, Dong et al. (2022)). For $\\mu_{1}$ , $\\mu_{2}>0$ , and deterministic algorithms $A_{1}:{\\mathcal{X}}^{n}\\rightarrow{\\mathcal{Z}}$ and $A_{2}:{\\mathcal{X}}^{n}\\times{\\mathcal{Z}}\\to{\\mathcal{Z}},$ , define algorithms $M_{1}:\\mathcal{X}^{n}\\rightarrow$ $\\mathcal{P}(\\mathcal{Z})$ and $M_{2}:\\mathcal{X}^{n}\\times\\mathcal{Z}\\rightarrow\\mathcal{P}(\\mathcal{Z})$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{{\\cal M}_{1}(D)}}&{{=}}&{{A_{1}(D)+\\displaystyle\\frac{\\Delta_{1}}{\\mu_{1}}\\varepsilon_{1}}}\\\\ {{{\\cal M}_{2}(D,z)}}&{{=}}&{{A_{2}(D,z)+\\displaystyle\\frac{\\Delta_{2}}{\\mu_{2}}\\varepsilon_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta_{1}=\\operatorname*{sup}_{D\\sim D^{\\prime}}\\|A_{1}(D)-A_{1}(D^{\\prime})\\|_{2}}\\end{array}$ , and $\\Delta_{2}=\\operatorname*{sup}_{z\\in{\\mathcal{Z}}}\\operatorname*{sup}_{D\\sim D^{\\prime}}\\|A_{1}(D,z)-A_{1}(D^{\\prime},z)\\|_{2}$ . Then $M_{3}\\,:\\,\\mathcal{X}^{n}\\,\\rightarrow\\,\\mathcal{P}(\\mathcal{Z})$ defined by ${\\cal M}_{3}(D)\\;=\\;{\\cal M}_{2}(D,{\\cal M}_{1}(D))$ is $(\\epsilon,\\delta)$ -DP if $\\sqrt{\\mu_{1}^{2}+\\mu_{2}^{2}}\\;=\\;$ $\\frac{1+\\sqrt{2\\log\\,{\\frac{1}{\\delta}}}}{\\epsilon}$ ", "page_idx": 20}, {"type": "text", "text": "The $\\ell_{2}$ norm of the difference between outputs of deterministic algorithm releasing $\\left(y_{1}\\varphi^{(N y s)}(x_{1}),\\dots,y_{n}\\varphi^{(N y s)}(x_{n})\\right)$ for neighboring datasets is at most $2\\|\\mathcal{V}\\|$ since $\\begin{array}{r}{\\|y\\varphi^{(N y s)}\\|\\leq}\\end{array}$ $\\|\\mathcal{V}\\|$ . Thus if we apply Proposition 1 repeatedly to algorithms ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{M_{0}(D)}}&{{:=}}&{{\\displaystyle\\left(y_{1}\\varphi^{(N y s)}(x_{1})+\\frac{\\sqrt{2}}{\\mu}\\varepsilon_{1}^{0},\\ldots,y_{n}\\varphi^{(N y s)}(x_{n})+\\frac{\\sqrt{2}}{\\mu}\\varepsilon_{n}^{0}\\right)}}\\\\ {{M_{i}(D)}}&{{:=}}&{{\\displaystyle\\left(y_{1}\\varphi^{(N y s)}(x_{1})+\\frac{p(p+1)}{\\mu}\\varepsilon_{1}^{i},\\ldots,y_{n}\\varphi^{(N y s)}(x_{n})+\\frac{p(p+1)}{\\mu}\\varepsilon_{n}^{i}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\varepsilon_{i}^{j}$ are normal random vector with mean 0 and covarianc\u221ae $4\\|\\mathcal{V}\\|^{2}I_{m\\times m}$ , releasing the outputs of $M_{0},M_{1},\\ldots,M_{\\frac{p(p+1)}{2}}$ at once is $(\\epsilon/2,\\delta)$ -DP since 2+2 \u03f52 log \u03b41. Therefore the overall algorithm satisfies $(\\epsilon,\\delta)$ -DP. ", "page_idx": 20}, {"type": "text", "text": "6.3.9 Proof of Theorem 9 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Denote $\\begin{array}{r}{h(\\hat{y}y)=\\sum_{k=0}^{p}a_{k}(y\\hat{y})^{k}}\\end{array}$ . We show that Algorithm 7 satisfies the theorem. ", "page_idx": 20}, {"type": "text", "text": "Definition 6.1 (Dvurechensky and Gasnikov (2016), modified). A convex function $f:Q\\subset\\mathbb{R}^{d}\\to\\mathbb{R}$ is endowed with a $(\\delta,b,\\sigma)$ stochastic oracle if an inexact gradient function $\\widetilde{g}:Q\\to\\mathbb{R}^{d}$ and inexact function $f_{0}:Q\\to\\mathbb{R}$ were given such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{0\\leq f(y)-f_{0}(x)-\\mathbb{E}\\left[\\widetilde{g}(x)\\right]^{T}(y-x)}&{\\leq}&{\\frac{b}{2}\\|y-x\\|_{2}^{2}+\\delta}\\\\ {\\mathbb{E}\\left[\\|\\widetilde{g}-\\mathbb{E}\\left[\\widetilde{g}(x)\\right]\\|_{2}^{2}\\right]}&{\\leq}&{\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 2 (Dvurechensky and Gasnikov (2016)). For a $\\lambda$ -strongly convex function $f$ endowed with $a$ $(\\delta,b,\\sigma)$ stochastic oracle, the sequecne $\\{w_{i}\\}_{i=1}^{n}$ generated by the algorithm $^3$ in Dvurechensky and Gasnikov (2016): ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(w_{n})-\\mathop{\\arg\\operatorname*{min}}_{\\|w\\|_{2}\\leq r}f(w)\\geq O\\left(\\frac{\\sigma^{2}\\left(1+\\log\\frac{{\\lambda^{2}}r^{2}n}{\\beta\\sigma^{2}}\\right)^{2}}{2n\\lambda}+\\delta^{2}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability at least $1-\\beta$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Set $Q\\,=\\,\\{x\\,\\in\\,\\mathbb{R}^{m}\\,:\\,|x|\\!\\le\\,r\\}$ . Then, the projection mapping $\\varphi^{(N y s)}\\,:\\,\\mathcal{X}\\,\\to\\,\\mathbb{R}^{m}$ maps $x\\in\\mathscr{X}$ to $\\begin{array}{r}{\\frac{1}{R}(\\langle\\mathrm{proj}_{S}\\phi(x),b_{1}\\rangle_{\\mathcal{H}_{k}},\\ldots,\\langle\\mathrm{proj}_{S}\\phi(x),b_{m}\\rangle_{\\mathcal{H}_{k}})}\\end{array}$ where $\\{b_{i}\\}_{i=1}^{m}$ is a orthonormal basis of $S$ transform the kernel ERM to linear ERM. Denote $w=(\\langle\\mathrm{proj}_{S}f,b_{1}\\rangle_{\\mathcal{H}_{k}},\\ldots,\\langle\\mathrm{proj}_{S}f,b_{m}\\rangle_{\\mathcal{H}_{k}})$ then constraint $\\|f\\|_{\\mathcal{H}_{k}}\\leq r$ is equivalent to $\\|w\\|_{2}\\leq r$ since $f\\in S$ . Then, the transformed linear ERM is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\|w\\|_{2}\\leq r}\\frac{1}{n}\\sum_{i=1}^{n}l(w^{T}\\varphi^{(N y s)}(x_{i}),y_{i})+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For given $x,y$ , denote $\\begin{array}{r}{j(w;x,y):=l(w^{T}\\varphi^{(N y s)}(x),y)+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}}\\end{array}$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{w}l(w^{T}\\varphi^{(N y s)}(x),y)=l_{0}^{\\prime}(w^{T}y\\varphi^{(N y s)}(x))y\\varphi^{(N y s)}(x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denote ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{g(w,x,y)}}&{{:=}}&{{h(y w^{T}\\varphi^{(N y s)}(x))}}\\\\ {{\\widetilde{g}(w,x,y)}}&{{:=}}&{{\\displaystyle\\sum_{k=0}^{p}a_{k}\\prod_{j=1}^{k}w^{T}\\widetilde{z}_{\\frac{k(k-1)}{2}+j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\widetilde{z}\\,=\\,\\bigg(y x+\\frac{\\sqrt{2}}{\\mu}\\varepsilon_{0}^{x},y x+\\frac{\\sqrt{p(p+1)}}{\\mu}\\varepsilon_{1}^{x},\\cdot\\cdot\\cdot,y x+\\frac{\\sqrt{p(p+1)}}{\\mu}\\varepsilon_{\\frac{p}{2}(p+1)}^{x}\\bigg)}\\end{array}$ following the notation in Algorithm 4. Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{j(w_{1};x,y)-j(w_{2};x,y)-(g(w_{2},x,y)y\\varphi^{(N y s)}(x)+\\lambda w_{2})^{T}(w_{1}-w_{2})}\\\\ {\\le}&{(l_{0}^{\\prime}(w_{2}^{T}y\\varphi^{(N y s)}(x))-g)y\\varphi^{(N y s)}(x)^{T}(w_{1}-w_{2})+\\displaystyle\\frac{\\lambda+b R^{2}\\|y\\|^{2}}{2}\\|w_{1}-w_{2}\\|_{2}^{2}}\\\\ {\\le}&{2R\\|y\\|\\alpha r+\\displaystyle\\frac{\\lambda+b R^{2}\\|y\\|^{2}}{2}\\|w_{1}-w_{2}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and similarly ", "page_idx": 21}, {"type": "equation", "text": "$$\nj(w_{1};x,y)-j(w_{2};x,y)-\\left(g(w_{2},x,y)\\varphi^{(N y s)}(x)+\\lambda w_{2}\\right)^{T}\\!(w_{1}-w_{2})\\geq-2R\\Vert y\\Vert\\alpha r.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we evaluate the variance of the gradient estimator: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|g(w,x_{i},y_{i})y_{i}\\varphi^{(N y s)}(x_{i})+\\lambda w-\\widetilde{g}(w,x_{i},y_{i})\\widetilde{z}_{i}-\\lambda w\\|_{2}^{2}\\right]}\\\\ {\\le}&{2\\mathbb{E}\\left[\\|g y\\varphi^{(N y s)}(x_{i})-\\widetilde{g}\\widetilde{z}_{i0}\\|_{2}^{2}\\right]+2\\mathbb{E}\\left[\\|\\widetilde{g}y_{i}\\varphi^{(N y s)}(x_{i})-\\widetilde{g}\\widetilde{z}_{i0}\\|_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $g,\\widetilde{g}$ stands for $g(w,x_{i},y_{i}),\\widetilde{g}(w,x_{i},y_{i})$ . The former can be bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\big|\\partial g(e^{T_{\\ell}})\\big|^{(X_{\\ell})}(x_{\\ell})-\\bar{g}_{W}\\varphi^{(X_{\\ell})}(x_{\\ell})\\big|^{2}\\right]}&{=}&{y_{\\ell}^{2}\\big||e^{(X_{\\ell})(y_{\\ell})}(x_{\\ell})|\\big|^{2}\\mathrm{Sup}(\\ell)}\\\\ &{\\leq}&{\\|y\\|^{2}R_{\\ell}^{2}\\sum_{k=0}^{P}\\Bigg(\\alpha_{k}\\displaystyle\\prod_{j=1}^{k}w^{T_{\\ell}}\\bar{z}_{i,k,\\frac{(k-1)-1}{2}+j}\\Bigg)}\\\\ &{\\leq}&{\\|y\\|^{2}R_{\\ell}^{2}\\sum_{k=0}^{P}\\mathbb{E}\\left[\\left(\\alpha_{k}\\displaystyle\\prod_{j=1}^{N}w^{T_{\\ell}}\\bar{z}_{i,k,\\frac{(k-1)-1}{2}+j}\\right)^{2}\\right]}\\\\ &{=}&{\\|y\\|^{2}R_{\\ell}^{2}\\sum_{k=0}^{P}\\alpha_{k}^{\\frac{1}{2}}\\prod_{j=1}^{k}\\mathbb{E}\\left[\\big(w^{T_{\\ell}}\\bar{z}_{i,k,\\frac{(k-1)-1}{2}+j}\\big)^{2}\\right]}\\\\ &{\\leq}&{\\|y\\|^{2}R_{\\ell}^{2}\\sum_{k=0}^{P}\\alpha_{k}^{\\frac{1}{2}}\\prod_{j=1}^{k}||y||^{2}\\sigma^{\\frac{1}{2}}\\left(1+\\frac{p(p+1)}{\\mu^{2}}\\right)}\\\\ &{=}&{a_{\\mathrm{se}}^{2}\\|y\\|^{2}R_{\\ell}^{2}\\frac{\\|y\\|^{2+2}r^{2+2}\\big(1+\\frac{p(p+1)}{\\mu^{2}}\\big)^{\\frac{1}{\\rho}+1}-1}{\\|y\\|^{2}\\sigma^{2}\\big(1+\\frac{p(p+1)}{\\mu^{2}}\\big)-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the latter can be bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|\\widetilde{g}y_{i}\\varphi^{(N y s)}(x_{i})-\\widetilde{g}\\tilde{z}_{i0}\\|_{2}^{2}\\right]}&{=}&{\\mathbb{E}\\left[\\widetilde{g}^{2}\\right]\\mathbb{E}\\left[\\|y_{i}\\varphi^{(N y s)}(x_{i})-\\tilde{z}_{i}\\|_{2}^{2}\\right]}\\\\ &{\\leq}&{(\\mathrm{Var}(\\widetilde{g})+(c+\\alpha)^{2})\\mathbb{E}\\left[\\|y_{i}\\varphi^{(N y s)}(x_{i})-\\tilde{z}_{i}\\|_{2}^{2}\\right]}\\\\ &{\\leq}&{\\frac{4m\\|\\mathcal{Y}\\|^{2}}{\\mu^{2}}(\\mathrm{Var}(\\widetilde{g})+(c+\\alpha)^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since $|\\mathbb{E}\\left[\\widetilde{g}\\right]|=|h(w^{T}y_{i}\\varphi^{(N y s)}(x_{i}))|\\!\\le|l_{0}^{\\prime}|\\!+\\!|h-l_{0}^{\\prime}|\\!\\le c+\\alpha$ . Finally ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|g y_{i}\\varphi^{(N y s)}(x_{i})-\\mathbb{E}_{(x,y)\\sim\\mathcal{P}}\\left[\\nabla_{w}l(w^{T}\\varphi^{(N y s)}(x_{i});y_{i})\\right]\\|_{2}^{2}\\right]}\\\\ {\\le}&{2\\mathbb{E}\\left[\\|g y_{i}\\varphi^{(N y s)}(x_{i})-\\nabla_{w}l(w^{T}\\varphi^{(N y s)}(x_{i});y_{i})\\|_{2}^{2}\\right]}\\\\ &{+2\\mathbb{E}\\left[\\|\\nabla_{w}l(w^{T}\\varphi^{(N y s)}(x_{i});y)-\\mathbb{E}_{(x,y)\\sim\\mathcal{P}}\\left[\\nabla_{w}l(w^{T}\\varphi^{(N y s)}(x_{i});y)\\right]\\|_{2}^{2}\\right]}\\\\ {\\le}&{2R^{2}\\|y\\|^{2}(\\mathbb{E}\\left[(g-l_{0}^{\\prime}(w^{T}\\varphi^{(N y s)}(x_{i});y))^{2}\\right]+4c^{2})}\\\\ {\\le}&{(\\alpha^{2}+4c^{2})R^{2}\\|y\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and combining the bound of $\\operatorname{Var}({\\widetilde{g}})$ we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\|\\widetilde{G}(w;\\tilde{z}_{i})-\\mathbb{E}_{(x,y)\\sim\\mathcal{P}}\\left[\\nabla_{w}l(w^{T}\\varphi^{(N y s)}(x_{i});y)\\right]\\|_{2}^{2}\\right]\\le O\\left(\\frac{a_{\\infty}^{2}r^{2p}p^{2p+1}}{\\mu^{2p+4}}+\\alpha^{2}+c^{2}\\right)\\frac{m R^{2}\\|y\\|^{2}}{\\mu^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the algorithm has $\\begin{array}{r}{\\bigg(4R\\|y\\|\\alpha r,\\frac{\\lambda+b R^{2}\\|y\\|^{2}}{2},O\\left(R\\|y\\|\\sqrt{\\frac{a_{\\infty}^{2}r^{2p}p^{2p+1}}{\\mu^{2p+4}}+\\alpha^{2}+c^{2}}\\right)\\bigg)}\\end{array}$ oracle. By Lemma 2, the excess empirical risk is bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(\\left(\\alpha^{2}+c^{2}+a_{\\infty}^{2}\\left(\\frac{r p}{\\mu}\\right)^{2p+1}\\right)\\frac{m R^{2}\\|\\mathcal{V}\\|^{2}\\log^{2}\\frac{1}{\\beta}}{n\\lambda\\mu^{2}}+\\alpha R\\|\\mathcal{V}\\|r\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability $1-\\beta$ . ", "page_idx": 22}, {"type": "text", "text": "6.3.10 Proof of privacy of the DP subsampling-based Nystr\u00f6m method ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. For a sequence $x_{i_{1}},\\ldots,x_{i_{k}}$ , denoted by $S$ without duplication, let $M(S)=v_{S}+\\eta\\varepsilon$ where $v_{S}:=[x_{i_{1}},\\ldots,x_{i_{k}}]^{T}$ is a flattened vector obtained from the sequence with increasing index order, i.e., $i_{1}<\\cdots<i_{k}$ , and $\\varepsilon$ be the random vector in $\\mathbb{R}^{k d}$ with i.i.d. components following the Laplace distribution given in the algorithm. Note that our algorithm first release the output of $M(S_{m})$ for $\\begin{array}{r}{\\eta=\\frac{d}{\\log\\left(1+\\frac{n}{m}\\left(e^{\\epsilon}-1\\right)\\right)}}\\end{array}$ where $S_{m}$ is a random subset of size $m$ uniformly drawn from $D$ , in line 2 and 3. The operations in line 4-7 does not refer to the data thus no privacy leakage. Therefore we show the algorithm M(Sm) is \u03f5-DP if \u03b7 =log(1+ nd (e\u03f5\u22121)). However, we will assign the value of $\\eta$ at the very last of our proof, and leave $\\eta$ to be undetermined until then. ", "page_idx": 22}, {"type": "text", "text": "Denote $S_{m},S_{m}^{\\prime}$ the length $m$ uniform random sequences of $D$ and $D^{\\prime}$ , respectively, and $S_{m-1}$ the length $m-1$ random sequence of $D\\cap D^{\\prime}$ . Denote Lap $(v,\\eta)$ be the distribution of the $v+\\eta\\varepsilon$ for $\\boldsymbol{v}\\in\\mathbb{R}^{k d}$ . If we denote the distributions of the outputs of the algorithm for $D$ and $D^{\\prime}$ as $\\mu_{D}$ and $\\mu{_{D^{\\prime}}}$ then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mu_{D}}}&{{\\sim}}&{{\\displaystyle\\frac{(n-m)!}{n!}\\sum_{S_{m}\\subset D}\\mathrm{Lap}\\left(v_{S_{m}},\\eta\\right)}}\\\\ {{\\mu_{D^{\\prime}}}}&{{\\sim}}&{{\\displaystyle\\frac{(n-m)!}{n!}\\sum_{S_{m}^{\\prime}\\subset D^{\\prime}}\\mathrm{Lap}\\left(v_{S_{m}^{\\prime}},\\eta\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Denote ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mu_{1}}}&{{=}}&{{\\displaystyle\\frac{(n-m-1)!}{(n-1)!}\\sum_{x_{n}\\notin S_{m}}\\mathrm{Lap}\\left(v_{S_{m}},\\eta\\right)}}\\\\ {{\\mu_{2}}}&{{=}}&{{\\displaystyle\\frac{(n-m)!}{m((n-1)!)}\\sum_{x_{n}\\in S_{m}}\\mathrm{Lap}\\left(v_{S_{m}},\\eta\\right)}}\\\\ {{\\mu_{1}^{\\prime}}}&{{=}}&{{\\displaystyle\\frac{(n-m-1)!}{(n-1)!}\\sum_{x_{n}^{\\prime}\\notin S_{m}^{\\prime}}\\mathrm{Lap}\\left(v_{S_{m}^{\\prime}},\\eta\\right)}}\\\\ {{\\mu_{2}^{\\prime}}}&{{=}}&{{\\displaystyle\\frac{(n-m)!}{m((n-1)!)}\\sum_{x_{n}^{\\prime}\\in S_{m}^{\\prime}}\\mathrm{Lap}\\left(v_{S_{m}^{\\prime}},\\eta\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mu_{D}}}&{{=}}&{{\\displaystyle\\frac{n-m}{n}\\mu_{1}+\\frac{m}{n}\\mu_{2}}}\\\\ {{\\mu_{D^{\\prime}}}}&{{=}}&{{\\displaystyle\\frac{n-m}{n}\\mu_{1}^{\\prime}+\\frac{m}{n}\\mu_{2}^{\\prime}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\mu_{1}=\\mu_{1}^{\\prime}$ . Thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle)_{1+\\frac{m}{n}(e^{\\epsilon}-1)}(\\mu_{D}||\\mu_{D^{\\prime}})}&{=}&{\\displaystyle\\int\\left(\\frac{n-m}{n}\\mu_{1}+\\frac{m}{n}\\mu_{2}-\\left(1+\\frac{m}{n}(e^{\\epsilon}-1)\\right)\\left(\\frac{n-m}{n}\\mu_{1}^{\\prime}+\\frac{m}{n}\\mu_{2}^{\\prime}\\right)\\right)_{+}}\\\\ &{=}&{\\displaystyle\\frac{m}{n}\\int\\left(\\mu_{2}-e^{\\epsilon}\\left(\\left(1-\\frac{1+\\frac{m}{n}(e^{\\epsilon}-1)}{e^{\\epsilon}}\\right)\\mu_{1}^{\\prime}+\\left(\\frac{1+\\frac{m}{n}(e^{\\epsilon}-1)}{e^{\\epsilon}}\\right)\\mu_{2}^{\\prime}\\right)\\right)_{+}}\\\\ &{=}&{\\displaystyle\\frac{m}{n}D_{\\epsilon^{\\epsilon}}\\left(\\mu_{2}||\\left(1-\\frac{1+\\frac{m}{n}(e^{\\epsilon}-1)}{e^{\\epsilon}}\\right)\\mu_{1}^{\\prime}+\\left(\\frac{1+\\frac{m}{n}(e^{\\epsilon}-1)}{e^{\\epsilon}}\\right)\\mu_{2}^{\\prime}\\right)}\\\\ &{\\leq}&{\\displaystyle\\frac{m}{n}\\operatorname*{max}\\{D_{\\epsilon^{\\epsilon}}\\left(\\mu_{2}||\\mu_{1}^{\\prime}\\right),D_{e^{\\epsilon}}\\left(\\mu_{2}||\\mu_{2}^{\\prime}\\right)\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since $\\mu_{1}=\\mu_{1}^{\\prime}$ . Also ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{D_{e^{\\epsilon}}\\left(\\mu_{2}||\\mu_{1}^{\\prime}\\right)}&{\\le}&{\\displaystyle\\frac{(m-1)!}{(n-m)((n-1)!)}\\sum_{S_{m-1}}\\sum_{z}D_{e^{\\epsilon}}\\left(v_{S_{m-1}\\cup\\{x_{n}\\}}+\\eta\\varepsilon||v_{S_{m-1}\\cup\\{z\\}}+\\eta\\varepsilon^{\\prime}\\right)}\\\\ &{\\le}&{\\displaystyle\\frac{(m-1)!}{(n-m)((n-1)!)}\\sum_{S_{m-1}}\\sum_{z}\\delta_{\\varepsilon}\\left(\\frac{z-x_{n}}{\\eta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $z$ are selected uniformly in $D^{\\prime}-S_{m-1}-\\{x_{n}^{\\prime}\\}$ , $\\varepsilon$ and $\\varepsilon^{\\prime}$ are independent random vectors with i.i.d. components following Laplace distribution given in the algorithm, and $\\delta_{\\epsilon}(c)$ denotes $D_{e^{\\epsilon}}(c+\\varepsilon||\\varepsilon^{\\prime})$ . Although we refer to $S_{m-1},\\{x_{n}\\},\\{z\\}$ as sets for convenience, they are actually sequences. $\\{x_{n}\\}$ or $\\{z\\}$ can be added at any position in $S_{m-1}$ , and each such case should be treated differently. However, by comparing the case where $x_{n}$ is added to the ith position of $S_{m-1}$ with the case where $z$ is added to the ith position of $S_{m-1}$ , we can reach the desired conclusion. ", "page_idx": 23}, {"type": "text", "text": "It is known that if the $\\ell_{1}$ norm difference of the output of an deterministic algorithm $\\mathcal{A}:\\mathcal{D}\\rightarrow\\mathbb{R}^{d}$ is at most $\\epsilon$ then $A(D)+\\varepsilon$ is $\\epsilon$ -DP (Dwork et al., 2006b) or equivalently $D_{e^{\\epsilon}}(c\\bar{+}\\varepsilon||\\varepsilon^{\\prime})=0$ if $\\lVert c\\rVert_{1}=\\epsilon$ for vector $c$ . Thus if we set $\\begin{array}{r}{\\eta=\\frac{d}{\\epsilon}}\\end{array}$ then the inequality gives $D_{e^{\\epsilon}}(\\mu_{2}||\\mu_{1}^{\\prime})=0$ . ", "page_idx": 23}, {"type": "text", "text": "Thus, the algorithm is $\\begin{array}{r}{\\log\\left(1+\\frac{m}{n}(e^{\\epsilon}-1)\\right)}\\end{array}$ -DP if $\\begin{array}{r}{\\eta=\\frac{d}{\\epsilon}}\\end{array}$ . Replacing $\\epsilon$ to $\\begin{array}{r}{\\log\\left(1+\\frac{n}{m}(e^{\\frac{\\epsilon}{2}}-1)\\right)}\\end{array}$ we arrive at the conclusion that the algorithm is \u03f52-DP if \u03b7 = $\\begin{array}{r}{\\eta=\\frac{d}{\\log\\left(1+\\frac{n}{m}(e^{\\frac{\\epsilon}{2}}-1)\\right)}}\\end{array}$ so we are done. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract outlines the motivation for the scalable DP kernel learning using the Nystr\u00f6m method, and theoretical and experimental analyses. Also, the introduction details the limitations of existing works and our contributions to the field. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ", "page_idx": 23}, {"type": "text", "text": "\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: In the Section 5, the limitations of our algorithms were discussed, and possible breakthroughs were suggested. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Proofs for every theorems are given in the Appendix(Section 6.3), and every assumptions required for the theorems are described in the theorem statement. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We gave detail explanation for our experiment setting including simulation data distribution, accessible real data, data preprocessing procedures, and hyper-parameters used in the experiments required for the reproduction in the Appendix(Section 6.1). Also, instruction for the reproduction is given in the supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We gave a publicly accessible link or the distribution description of the data in the Appendix(Section 6.1), which are also implemented in the attached code. Also, the code attached in the supplementary is written by python, and does not use any non-open-sourced or non-free libraries. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In the Appendix(Section 6.1), the experimental details and additional experiments are elaborated, including the size of the training and test data, preprocessing procedures, and the hyperparameters used in the experiment. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report the number of the repetitions made in our experiments, with the standard deviations which are displayed in the plots. The Appendix(Section 6.1) details the calculation methods. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The computing resources used in the experiments are given in the Appendix(Section 6.1). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work conforms to the NeurIPS Code of Ethics since it does not involve human subjects or participants, and the datasets used are either simulated or existing public datasets with proper respect for copyright. The research is intended for beneficial applications, and any potential harm would only arise from misuse, which is not our primary goal. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Although, our work lies in the area of the privacy, our main focus is reducing computational complexity and memory cost of the kernel learning under differential privacy which is irrelevant to significant societal impacts. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our algorithms and data do not contain information or data that have high risk of misuse. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 28}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We used the adult,bank,CDC,credit and MNIST data sets, giving proper credit by citation in the Appendix(Section 6.1). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The algorithms are explained and elaborated in the paper. Also, code implementations are attached in the supplementary material with instructions and annotations. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No human subject experiments or research are included in our work. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not include crowdsourcing or research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]