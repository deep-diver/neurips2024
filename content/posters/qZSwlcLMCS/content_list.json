[{"type": "text", "text": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiatao $\\mathbf{Gu}^{\\dagger*}$ , Ying Shen\u2662\u2217, Shuangfei Zhai\u2020, Yizhe Zhang\u2020, Navdeep Jaitly\u2020, Josh Susskind\u2020 \u2020Apple \u2662Virginia Tech \u2217equal contribution \u2020{jgu32, szhai, yizzhang,njaitly, jsusskind}@apple.com \u2662yings@vt.edu ", "page_idx": 0}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/50fbec639d3661d274bf9e20e7dfe4ed5236f61de6e652cc106eb7b814c24804.jpg", "img_caption": ["Figure 1: Comparison of the generated image samples given the caption \u201ca cat sat on the mat\u201d. Our models generate more diverse images with the help of autoregressive latent modeling. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process. In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs. Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latents, demonstrating its capability to effectively control the image generation process. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have become pervasive in many text-to-image generation tasks for their ability to generate high-quality images based on textual descriptions. A pivotal mechanism in these models is classifier-free guidance (CFG) [Ho and Salimans, 2021], which effectively steers the sampling process towards better alignment to textual prompts and improved sampling quality at the same time. CFG can be interpreted as tuning the temperature of the conditional distribution, whereas increasing the guidance scale sharpens the conditional distribution. This guides the generation to focus on regions of high conditional probability, effectively reducing sampling noise which is typically of lower density. However, while high CFG improves sampling quality, it simultaneously narrows the diversity in the generated samples. This manifests in the models\u2019 inability to produce diverse images from the same caption, even when there are variations in the initial noise that seeds the generation process. For instance, given a fixed textual description, \u201ca cat sits on a mat\u201d, existing text-to-image diffusion models predominantly produce image samples depicting cats with similar colors and patterns, as illustrated in Figure 1. Such limited visual diversity hinders the practical application of diffusion models in scenarios where a wide range of creative and diverse visual interpretations are desired from identical textual inputs. It also poses challenges in scenarios demanding the representation of underrepresented data or accommodating a wide range of user preferences. Therefore, enhancing diversity in diffusion models without compromising the quality remains a critical research problem. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To tackle this, we introduce Kaleido, a general framework that improves diffusion models with autoregressive priors. Kaleido first defines a discrete encoding of images (eg, detailed captioning, bounding boxes), which captures desirable abstractions of images that\u2019s not included in the default text prompts. Next, Kaleido integrates an encoder-decoder language model that encodes the original text caption and autoregressively predicts the discrete latent tokens. Lastly, the diffusion model is conditioned on both the original text prompt and the autoregressively generated discrete latents and generates an image. This enriched conditioning allows Kaleido to produce a more diverse array of high-quality images, even at high guidance scales. We explore various forms of latents, including textual descriptions, detection bounding boxes, object blobs, and abstract visual tokens \u2013 all designed to refine and guide the conditional image generation process. ", "page_idx": 1}, {"type": "text", "text": "We experiment on both class and text conditioned image generation benchmarks 1. We show that Kaleido not only outperforms standard diffusion models in terms of diversity but also maintains the high quality of the generated image. Additionally, the generated latents effectively control the characteristics of the generated images, ensuring that the image samples closely align with the intended latent variables. This modeling of latent tokens not only increases the diversity of image outputs but also provides a degree of interpretability and control over the image generation process. ", "page_idx": 1}, {"type": "text", "text": "To summarize, Kaleido exhibits the following advantages: ", "page_idx": 1}, {"type": "text", "text": "1. Kaleido promotes the diversity in generated image samples even with high CFG, allowing the image generation of both high quality and diversity. 2. The generated latent variables are interpretable, offering an explainable mechanism behind the image generation process, and facilitating an understanding of how different latents affect the outputs. 3. Kaleido provides a fine-grained, editable interface that allows users to adjust the discrete latent codes before final image production, granting greater flexibility and control over the output. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Autoregressive Image Generation The success of large language models (LLMs) in NLP has demonstrated their scalability and universality of modeling any complex data, motivating the development of using autoregressive models for image generation. Typically, autoregressive image generation operates on discrete image tokens obtained from vector-quantization (VQ) [Van Den Oord et al., 2017]. More precisely, given an image $\\pmb{x}\\in\\mathbb{R}^{3\\times H\\times W}$ , we first obtain a sequence of discrete tokens $z_{1:N}\\,=\\,\\mathcal{E}(\\pmb{x})$ which approximately reconstructs the input with a learned decoder $\\mathcal{D}(z_{1:N})\\approx\\pmb{x}$ . Then, an autoregressive model is learned to predict the discrete tokens one after another, mirroring the sequential language modeling: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\theta}^{\\mathrm{AR}}=\\sum_{n=1}^{N}\\log P_{\\theta}(z_{n}|z_{0:n-1},c),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $^c$ is the condition (e.g., class, text prompt, etc.), and $z_{\\mathrm{0}}$ is a special start token. At inference time, we first sample from the learned distribution, and then pass the sampled latents to the decoder $(\\mathcal{D})$ to get the final output. Such VQ-based paradigm has been the foundation for various text-toimage [Esser et al., 2021, Yu et al., 2021, Zheng et al., 2022, Yu et al., 2022] and multi-modal generation [Team et al., 2023, Team, 2024]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "However, these methods share a common limitation: they primarily rely on discretization, which struggles to capture all the nuances of an image when using a limited length of discrete image token sequence. To generate higher-resolution images, a longer sequence of image tokens is necessary. Yet, this inherently leads to increased capacity demands. For instance, Yu et al. [2022] requires 20B parameters to work properly. Additionally, the left-to-right properties of these autoregressive models prevent the rewriting of previously generated image tokens, resulting in suboptimal image quality. ", "page_idx": 2}, {"type": "text", "text": "Diffusion-based Image Generation Diffusion models [Sohl-Dickstein et al., 2015, Ho et al., 2020] are latent variable models with a pre-determined posterior distribution and are trained using a denoising objective, which has quickly become the new de-facto approach for image generation. Unlike autoregressive models which predict images as a sequence, diffusion-based models iteratively generate the whole image in a non-autoregressive fashion. Specifically, given an image x \u2208R3\u00d7H\u00d7W and a signal-noise schedule $\\left\\{\\alpha_{t},\\sigma_{t}\\right\\}$ where the signal-to-noise ratio (SNR) $(\\bar{\\alpha_{t}^{2}}/\\sigma_{t}^{2})$ decreases monotonically with $t$ , we define a series of latent variables $\\mathbf{\\Delta}\\mathbf{x}_{t},t=0,\\ldots,T$ that adhere to: ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{t}|\\pmb{x})=N(\\pmb{x}_{t};\\alpha_{t}\\pmb{x},\\sigma_{t}^{2}I),\\mathrm{~and~}q(\\pmb{x}_{t}|\\pmb{x}_{s})=N(\\pmb{x}_{t};\\alpha_{t|s}\\pmb{x}_{s},\\sigma_{t|s}^{2}I),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{\\boldsymbol{x}}_{0}=\\mathbf{\\boldsymbol{x}}$ , $\\alpha_{t|s}=\\alpha_{t}/\\alpha_{s}$ , and $\\sigma_{t|s}^{2}=\\sigma_{t}^{2}-\\alpha_{t|s}^{2}\\sigma_{s}^{2}$ for $s<t$ . The model then learns to reverse this process using a backward model $p_{\\theta}(\\mathbf{\\boldsymbol{x}}_{s}|\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{c}})$ , which reformulates a denoising objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\theta}^{\\mathrm{DM}}=\\mathbb{E}_{t\\sim[1,T],\\mathbf{x}_{t}\\sim q(\\mathbf{x}_{t}|\\mathbf{x})}\\left[\\omega_{t}\\cdot||\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},c)-x||_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{x}_{\\theta}(\\pmb{x}_{t},\\pmb{c})$ is a neural network (typically a UNet [Ronneberger et al., 2015] or Transformer [Peebles and Xie, 2022]) that maps the noisy input $\\pmb{x}_{t}$ to its clean version $\\textbf{\\em x}$ , based on the time step $t$ and conditional input $^c$ ; $\\omega_{t}\\in\\mathbb{R}^{+}$ is a loss weighting factor. In practice, $\\pmb{x}_{\\theta}$ can be re-parameterized with noise- or v-prediction [Salimans and Ho, 2022] for enhanced performance, and can be applied on raw pixel space [Saharia et al., 2022, Gu et al., 2023] or latent space [Rombach et al., 2022]. ", "page_idx": 2}, {"type": "text", "text": "Classifier-free Guidance An intriguing property of conditional diffusion models is that we can easily guide the iterative sampling process for better sampling quality. For instance, Ho and Salimans [2021] introduced Classifier-free Guidance $(C F G)$ , which utilizes the diffusion model itself to perform guidance at test time. More specifically, we perform sampling using the following linear combination: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{}x}_{\\theta}(x_{t},c)=\\gamma\\cdot(x_{\\theta}(x_{t},c)-x_{\\theta}(x_{t}))+x_{\\theta}(x_{t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma$ is the guidance weight, and $\\pmb{x}_{\\theta}(\\pmb{x}_{t})=\\pmb{x}_{\\theta}(\\pmb{x}_{t},\\pmb{c}=\\emptyset)$ is the unconditional denoising output. During training, we drop the condition $^c$ with certain probability $p_{\\mathrm{uncond}}$ to facilitate unconditional prediction. When $\\gamma>1$ , CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation. ", "page_idx": 2}, {"type": "text", "text": "Compared to autoregressive models, diffusion models are more flexible in adjusting sample steps, allowing for the utilization of noise schedules to learn different frequencies. Additionally, with the use of CFG, diffusion models can achieve higher quality images with much fewer parameters than autoregressive models. However, it\u2019s notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both. ", "page_idx": 2}, {"type": "text", "text": "3 Kaleido Diffusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose Kaleido, a general framework that integrate an autoregressive prior with diffusion model to enhance image generation. As illustrated in Fig. 2, Kaleido comprises two major components: an AR model that generates latent tokens as abstract representations, and a latent-augmented diffusion model that iteratively synthesizes images based on these latents together with the original condition. For following sections, we first describe the importance to introduce additional latents in standard diffusion models $(\\S\\,3.1)$ , and show how we can model them with AR models $(\\S\\ 3.2)$ . The training and inference procedure are described in $\\S\\ 3.3$ and $\\S\\ 3.4$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Latent-augmented Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As demonstrated in Ho and Salimans [2021], diffusion with CFG (Eq. (4)) is equivalent to follow ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{x}\\log\\tilde{p}_{\\theta}(x|c)=\\gamma\\left[\\nabla_{x}\\left(\\log p_{\\theta}(x|c)-\\log p_{\\theta}(x)\\right)\\right]+\\nabla_{x}\\log p_{\\theta}(x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/85a9754c38b777899a76361d96d2f73bc02c27e619e0cca87909f6f2e67a6fe6.jpg", "img_caption": ["Figure 2: Training pipeline of the proposed Kaleido diffusion. ", "where standard diffusion models implicitly learn mode selection step together with generation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "which can be interpreted as sampling from a \u201ctemperature-adjusted\u201d distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx\\sim\\tilde{p}_{\\theta}(x|c)\\propto p_{\\theta}(x)\\left[p_{\\theta}(c|x)\\right]^{\\gamma},\\ \\ \\ \\mathrm{where}\\ \\ \\ p_{\\theta}(c|x)\\propto p_{\\theta}(x|c)/p_{\\theta}(x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $\\gamma$ can be seen as inverse temperature, which sharpens the conditional distribution $p_{\\theta}({\\pmb c}|{\\pmb x})$ when $\\gamma>1$ . That is to say, CFG is crucial as it guides the generation to only focus on high-probability regions, avoiding sampling noise (which tends to have low density). However, sharpening the distribution also reduces the diversity, causing undesirable phenomena like \u201cmode collapse\u201d. This is because $^c$ (e.g., class label, text prompt, etc.) normally does not contain all the information that describes $\\textbf{\\em x}$ . Suppose we introduce a hypothetical variable $_{\\textit{z}}$ to represent the \u201cmodes\u201d of $\\textbf{\\em x}$ which we care $\\mathrm{most}-p_{\\theta}(z|c)$ , and leave $p_{\\theta}(x|z,c)$ to model other variations including local noise. In this case, CFG will simultaneously sharpen both distributions, considering: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}|c)=\\sum_{z}\\underbrace{p_{\\theta}(z|c)}_{\\mathrm{mode~selection}}\\cdot\\underbrace{p_{\\theta}(\\mathbf{x}|z,c)}_{\\mathrm{image~variation}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, a natural solution is to explicitly model \u201cmode selection\u201d before applying diffusion steps so that the mode distribution will not be distorted by guidance. In this way, the sampling procedure (Eq. (6)) is modified as two steps: $z\\sim p_{\\theta}(z|c),x\\sim\\bar{p}_{\\theta}(x|z,c)$ , where CFG can be applied after $_{z}$ is sampled. From the perspective of score function, we rewrite $\\tilde{p}_{\\theta}(x|c)$ as $\\tilde{p}_{\\theta}(x|c,z)$ in Eq. (5): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{x}\\log\\tilde{p}_{\\theta}(x|c,z)=\\gamma\\left[\\nabla_{x}\\left(\\log p_{\\theta}(x|c)+\\,\\log p_{\\theta}(z|x,c)\\;-\\log p_{\\theta}(x)\\right)\\right]+\\nabla_{x}\\log p_{\\theta}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Compared to standard diffusion process, the highlighted term above pushes the updating direction towards the sampled modes at each step. This ensures diverse generation as long as $\\bar{p}_{\\theta}(z|c)$ is diverse. ", "page_idx": 3}, {"type": "text", "text": "A Toy Example We visualize the effect of explicitly introducing latent priors using a toy dataset with two main classes, each containing two modes. We compare two models: a standard diffusion model conditioned on the major class ID, and a latent-augmented model incorporating subclass ID as priors. Fig. 3 shows that while the standard diffusion model tends to converge to one mode (subclass) with increased guidance, the latent-augmented model captures all modes, showing the benefti of latent priors for improving diversity under high guidance. In practice, given the challenge of identifying all \u201cmodes\u201d in real-world data distribution, we next propose to employ an autoregressive model to universally model various latent modes. ", "page_idx": 3}, {"type": "text", "text": "3.2 Autoregressive Latent Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To capture the complex distribution of real images, it is clearly impossible to assign classes for each mode. However, it is non-trivial to determine (1) the best representations for modes $_{z}$ ; (2) the suitable generative model that can model $p_{\\theta}(z|c)$ . Fortunately, the modes that humans can perceive from an image are largely abstract, and such abstract semantics are easily represented in discrete symbols. For example, we can easily describe content differences through natural language, create composite images based on spatial locations, and imagine novel visual concepts from experience. Therefore, it is logical to use such abstract discrete tokens, i.e., $\\boldsymbol{z}=[z_{1},\\dots,z_{N}]$ . Naturally, the most convenient way to model such distribution is using an autoregressive model $\\bar{p_{\\theta}}(z|c)$ . Note that this is distinct from the conventional autoregressive image generation $(\\S\\,2)$ , as we only learn such models as \u201clatent modes\u201d, where the sampled $z_{1:N}$ are not supposed to reconstruct an image directly. As a result, it eases the modeling difficulty and improves the sampling performance. ", "page_idx": 3}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/4ef93cef63abad53ce4ec08cc0ffc6079e28b882e26891665928c22c3969bf95.jpg", "img_caption": ["Figure 3: Effect of augmented latents. The first row displays the sampling results from the standard diffusion model, while the second row shows the results from the latent-augmented diffusion models. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In this work, we explore four types of abstract latents: textual descriptions (text), detection bounding boxes (bbox), object blobs (blob), and visual tokens (voken), all of which can be predicted from multimodal large language models (MLLMs) [Bai et al., 2023, Liu et al., 2024, Ge et al., 2023a] given the condition-image pair $(c,x)$ . Each type aims to enrich the mode-to-image correspondence, covering different aspects of image formation. These extracted tokens can either be predicted separately or modeled together with a single autoregressive model. Fig. 4 shows the examples of these four generated latent variables. The methodology for constructing the training dataset for these variables is detailed in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "3.3 Joint Learning of Autoregressive and Diffusion Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Similar to other latent variable models like VAEs [Kingma and Welling, 2013], Kaleido can be trained to maximize the evidence lower bound (ELBO) as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\log p_{\\theta}(x|c)\\ge\\mathbb{E}_{z\\sim q(z|x,c)}[\\underbrace{\\log p_{\\theta}(z|c)}_{\\mathcal{L}^{\\mathrm{AR}}~\\mathrm{Eq.(1)}}+\\underbrace{\\log p_{\\theta}(x|z,c)}_{\\mathcal{L}^{\\mathrm{DM}}~\\mathrm{Eq.(3)}}]+\\mathcal{H}\\left[q(z|x,c)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $q$ is the inference model, and $\\mathcal{H}(q)$ is the entropy. In this paper, we always assume a fixed inference process (as explained $\\S\\ 3.2)$ . Therefore, the entropy term can be omitted, and we can efficiently sample and store $_{z}$ for the entire dataset before training starts. We illustrate the training pipeline in Fig. 2. Compared to standard diffusion models which typically involves a context encoder and a denoising network, Kaleido integrates the additional autoregressive decoder for modeling the discrete latents. Such decoder uses cross-attention to gather the encoder states at every step, and the final decoder layer states are concatenated with the encoder as the inputs for diffusion. Following common practices, we freeze the context encoder during training, and jointly optimize the autoregressive decoder together with the denoising model. The training objectives (Eq. (9)) is equivalent to the combination of both models, denoted as $\\mathcal{L}\\,=\\,\\mathcal{L}^{\\mathrm{DM}}+\\mathbf{\\bar{\\eta}}\\cdot\\bar{\\mathcal{L}}^{\\mathrm{AR}}$ , with $\\eta$ as a hyperparameter for balancing the contributions of the autoregressive and diffusion models in practice. ", "page_idx": 4}, {"type": "text", "text": "3.4 Interpretable and Controllable Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "During the inference stage, given the provided textual description, the autoregressive model will first predict the discrete latents before image generation. These latents, being predominantly humanreadable, add a layer of interpretability to the image-generation process, allowing humans to observe its internal \u201cthought\u201d process. This transparency also provides users with the flexibility to modify the latents as desired. To incorporate user modification, the altered latents are re-input into the autoregressive decoder to obtain the modified final hidden states. The latent-augmented diffusion model then synthesizes the final image conditioned on the updated representation. ", "page_idx": 4}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/88aed2c5e64741b101d9291d0232d3ae5f17a2f4fa2478a8bc5a4b71680dc40b.jpg", "img_caption": ["Figure 4: A Variety of Discrete Tokens. Original caption: \u201cDog laying on a human\u2019s lap\u201d "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset We validate our approach on both class- and text-conditioned image generation benchmarks. For the former, we use ImageNet [Deng et al., 2009], and we learn the textto-image models on CC12M [Changpinyo et al., 2021], a large image-text pair dataset where each image is accompanied by a descriptive alt-text. All models are trained to synthesize at ", "page_idx": 5}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/4ae93b94d8fc577f5f19e6f2607ec326620a5d4aa881359cd472b6208dde09af.jpg", "img_caption": ["Figure 5: Comparison with guidance weights. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "$256\\times256$ . We generate all four types of latents as discussed in Appendix A for both datasets. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics To assess the performance of our models, we employ Fr\u00e9chet Inception Distance (FID) [Heusel et al., 2017] to capture the overall performance (considering both quality and diversity) of the generated images, and use Recall [Kynk\u00e4\u00e4nniemi et al., 2019] to specifically measure the diversity of the generated images. Furthermore, we employ two additional quantitative assessments of diversity: Mean Similarity Score (MSS) and Vendi scores [Friedman and Dieng]. We use SSCD [Pizzi et al., 2022] as the pretrained feature extractor for calculating both MSS (SSCD) and Vendi (SSCD). Additionally, we utilize DiNOv2 [Oquab et al.] as the feature extractor for Vendi (DiNOv2), based on evidence from Stein et al. [2024] that suggests DiNOv2 provides a richer evaluation of generative models. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details and Baseline We implement Kaleido with Matryoshka Diffusion Models (MDM) [Gu et al., 2023], a recently proposed approach that generates images directly in the raw pixel space with efficient training. The default MDM consists of a frozen T5-XL [Radford et al., 2021] context encoder and a nested UNet-based denoiser. We initialize the additional autoregressive decoder with the decoder of T5-XL, and make the parameters trainable. The vocabulary is resized to adapt special visual tokens. For fair comparison, we use MDM with the same hyper-parameters as our baseline model, and train both types in almost identical settings on 64 A100 GPUs. Additionally, we compare Kaleido with the Condition Annealed Diffusion Sampler (CADS) [Sadat et al.], a general sampling strategy that enhances the diversity of diffusion models by annealing the conditioning signal during inference. Given that CADS is applicable to different model architectures, we also evaluate CADS integrated with both baseline model MDM $(\\mathbf{MDM}+\\mathbf{CADS})$ ) and our model $\\mathrm{(ours+CADS)}$ ). ", "page_idx": 5}, {"type": "text", "text": "4.2 Quantitative Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Fig. 5 quantitatively compares Kaleido with the baseline diffusion models (MDM) with various guidance scales on ImageNet. Both metrics are evaluated with 50K samples against the full training set, where both our models and the baseline use DDPM sampling with 250 steps. Our findings reveal that Kaleido consistently enhances the diversity of samples without compromising their quality across different CFG, evidenced by the general improvement in both FID and Recall. Moreover, while the baseline\u2019s FID increases and Recall decreases significantly with higher CFG, Kaleido demonstrates a steadier performance profile. ", "page_idx": 5}, {"type": "table", "img_path": "qZSwlcLMCS/tmp/6bf51ea12029e5255bbb177d63bb444df3486d495c295873dab712ffdcb56ad1.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of quality and diversity on ImageNet. FID-50K, Precision, and Recall are evaluated on 50K samples, while MSS and Vendi scores assess diversity on $1\\mathrm{K}\\times10$ samples. "], "page_idx": 6}, {"type": "text", "text": "To further investigate, we examine image quality and diversity between Kaleido and baseline models. As shown in Table 1, Kaleido outperforms the $\\mathbf{MDM}+\\mathbf{CADS}$ combination in terms of FID-50K and precision, demonstrating that our method more effectively maintains high image quality while generating diverse samples. Furthermore, integrating CADS with our model yields the best FID-50K results. Note that precision cannot accurately evaluate models with diverse outputs since a model producing high-quality but non-diverse samples could artificially achieve high precision [Sadat et al.]. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we assess the diversity of the generated images using 10K samples. Following CADS, we select 1, 000 random classes from ImageNet and generate 10 samples per class. Table 1 shows that both Kaleido and CADS significantly enhance sample diversity. While CADS achieves better performance in diversity, our model maintains superior image quality. Additionally, the methodologies used in CADS are complementary to ours, suggesting potential beneftis from integrating CADS with our Kaleido. In fact, incorporating CADS into our model not only further improves image quality but also improves diversity, achieving the best scores in FID-50K, MSS (SSCD), and Vendi (DiNOv2). ", "page_idx": 6}, {"type": "text", "text": "Lastly, we provide visual comparisons for class- and text-conditioned image generation in Fig. 11. Notably, we observe that $\\mathbf{MDM}+\\mathbf{CADS}$ fails to generate cats of diverse breeds from the prompt \u201ca cat sleeping on the bed.\u201d In contrast, Kaleido can produce images of cats from various breeds with more diverse surrounding environments, showcasing its superior diversity capabilities. This observation contrasts with the trend of diversity scores in Table 1, suggesting that these diversity metrics may not fully capture certain aspects of diversity. ", "page_idx": 6}, {"type": "text", "text": "4.3 Qualitative Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Diversity of Generated Images We present a comparative analysis of the images generated by Kaleido against baseline models (MDM). Fig. 7 demonstrates the comparison between baseline models and Kaleido on two conditional generation tasks: the class-conditioned image generation and the text-to-image generation. In both tasks, Kaleido consistently produces more diverse images from identical condition (class or textual description) across varying CFG scales. For instance, in the task of class-to-image generation, the baseline diffusion models generate predominantly frontal views of a \u201chusky\u201d at high CFG, while Kaleido produces diverse images depicting huskies in various poses and numbers. A similar improvement in diversity is observed in the text-to-image generation as well, highlighting the robustness of Kaleido in generating diverse images under identical conditions. ", "page_idx": 6}, {"type": "text", "text": "Control from Latent Tokens We show the efficacy of latent variables in guiding the image generation process in Fig. 6. Fig. 6 demonstrates images generated with different types of latent variables: (a) textual descriptions, (b) object blobs, (c) detection bounding boxes, (d) visual tokens, and (e) combined latents, which integrate textual descriptions, detection bounding boxes and visual tokens. We visualize the generated latents tokens alongside the resulting images, showing how closely the images generated by Kaleido align with the latent tokens. Such alignment is evident in fine-grained visual information \u2013 such as object appearance, background, and atmosphere \u2013, spatial location and orientation of different objects, and the stylistic elements of generated images. This alignment confirms that Kaleido can effectively interpret and utilize generated latent variables to guide and refine the image generation process. ", "page_idx": 6}, {"type": "text", "text": "Latent Editing Fig. 8 showcases the impact of latent editing in image generation. The first row displays images generated using autoregressively produced latent tokens. In the second row, we demonstrate the effect of manual modifications to the textual descriptions: changing \u201clog\u201d to \u201ccobblestones\u201d and \u201ca body of water\u201d to \u201cforest\u201d. These changes result in a modified image where a frog is now positioned on cobblestones with a forest background. Additionally, by further augmenting the bounding box of a cup to a different position, we observe that the cup\u2019s position in the image changes accordingly, while most other visual elements remain unchanged. The precise control of image characteristics via latent editing underscores Kaleido\u2019s flexibility and controllability, offering a powerful interactive interface for users to customize the generated images. Furthermore, the high fidelity of the re-generated images to their original versions indicates Kaleido\u2019s potential for applications requiring personalization or customizations. ", "page_idx": 6}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/63bba6001fa2a6d33e2933b9d5150835ee5c041fbe431a2733d9055172793710.jpg", "img_caption": ["Figure 6: Example of generation with various latents.. This figure showcases images generated with different types of latents: (a) textual descriptions, (b) object blobs, (c) detection bounding boxes, (d) visual tokens, and (e) combined latents (textual descriptions $^+$ detection bounding boxes $^+$ visual tokens). Each row shows two sets of generated images sampled with one type of latents. Each set displays a visualization of the generated latents tokens (left) and a collage of images (right) sampled using the same latent tokens but different noises. The image tokens capture visual details difficult to convey through text, such as artistic style. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Augmenting Diffusion Models Various enhancements have been proposed to improve the versatility and controllability of diffusion models with augmented latents. Innovations such as Diffusion AE [Preechakul et al., 2022] integrates diffusion models with a learnable encoder that extracts high-level semantics and enables the diffusion model to add details directly in image space. Further efforts have focused on incorporating specific control signals, such as bounding boxes, layout, and segmentation masks to guide and control the image generation process. [Balaji et al., 2022, Li et al., 2023, Zheng et al., 2023a, Hu et al., 2023]. Recently, BlobGen [Nie et al., 2024] proposes to ground existing text-to-image diffusion models on object blobs \u2013 tilted ellipses that capture spatial details of the objects \u2013 for compositional generation. While these approaches improve the models\u2019 capacity to adhere to specified spatial layouts, they often necessitate modifications to the attention mechanism, potentially limiting their generality. In contrast, our method enhances the generative capabilities of diffusion models without altering the model architecture. ", "page_idx": 7}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/e0c5ce2ac912d3df380833a5a3546cca80070a20d4fa74b1fd2f786644133811.jpg", "img_caption": ["Figure 7: Diversity comparison to standard diffusion model. Images sampled under varying CFG scales $(\\gamma)$ . Panels (a) and (c) display images from the baseline models, while panels (b) and (d) show images from Kaleido. From top to bottom, as the CFG increases, the standard diffusion models exhibit reduced diversity, while Kaleido consistently maintains diversity across guidance scales. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Connecting Diffusion Models with LLMs The remarkable success of Large Language Models (LLMs) and diffusion models has spurred interest in connecting these models, aiming to leverage the capabilities of LLMs in understanding and generating complex data and combine it with the powerful image synthesis capabilities of diffusion models [Ge et al., 2023b, Zheng et al., 2023b, Sun et al., 2023]. Ge et al. [2023b], Zheng et al. [2023b] propose image tokenizers that encodes images into visual tokens, enabling multimodal language modeling. This line of work focuses on empowering LLM with image generation ability by aligning its output embedding space with the pre-trained diffusion models. Our work leverages the LLMs\u2019 robust capabilities in textural understanding and generation to model the generation of abstract latents from the original text. These latents are then integrated with latent-augmented diffusion model, enabling a more interpretable and diverse image generation process. ", "page_idx": 8}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/1897da48c627ef9ae95d65fca78aaab5ccf4d11935183316b27ccd308c694659.jpg", "img_caption": ["Input: \u03b1 photo of a frog drinking coffee ", "Figure 8: Effect of sequential latent editing. The top row displays images generated with autoregressively produced latent tokens. The middle row shows the re-generated images after applying latent editing to the textural description, and the bottom row presents re-generated images after further edits to the bounding box, showing the impact of step-by-step latent editing. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Our approach also distinguishes itself from the re-captioning method introduced in DALL-E 3 [Betker et al., 2023]. Unlike re-captioning, which typically replaces the original captions with more descriptive captions, our method retains the original condition and supplements it with latent variables of various forms (beyond textual captions like bbox, blob and \u201cvokens\u201d). The sampled latents serves as a unifying interface for various types of inputs, and introduce diversity compared to recaptioning where no sampling is involved at inference time. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we address the challenge of improving sample diversity under high CFG in diffusion models. We introduce Kaleido Diffusion, which combines an autoregressive prior with a latentaugmented diffusion model. Results show Kaleido increases diversity without compromising quality, even at high CFG. With human interpretable latent tokens, Kaleido offers an explainable mechanism behind the image generation process and provides a fine-grained editable interface, enabling precise user control over the generated images. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.   \nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediff:i Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.   \nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.   \nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-scale Hierarchical Image Database. IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.   \nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.   \nDan Friedman and Adji Bousso Dieng. The vendi score: A diversity evaluation metric for machine learning. Transactions on Machine Learning Research.   \nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023a.   \nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Planting a seed of vision in large language model. In The Twelfth International Conference on Learning Representations, 2023b.   \nJiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua M Susskind, and Navdeep Jaitly. Matryoshka diffusion models. In The Twelfth International Conference on Learning Representations, 2023.   \nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \nVincent Tao Hu, David W Zhang, Yuki M Asano, Gertjan J Burghouts, and Cees GM Snoek. Selfguided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18413\u201318422, 2023.   \nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015\u20134026, 2023.   \nTuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019.   \nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511\u201322521, 2023.   \nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \nWeili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image generation with dense blob representations. arXiv preprint arXiv:2405.08246, 2024.   \nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research.   \nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022.   \nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A selfsupervised descriptor for image copy detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14532\u201314542, 2022.   \nKonpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10619\u201310629, 2022.   \nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net : Convolutional Networks for Biomedical Image Segmentation. International Conference on Medical Image Computing and ComputerAssisted Intervention, pages 234\u2013241, 2015.   \nSeyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann M Weber. Cads: Unleashing the diversity of diffusion models through condition-annealed sampling. In The Twelfth International Conference on Learning Representations.   \nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.   \nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \nGeorge Stein, Jesse Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Ross, Valentin Villecroze, Zhaoyan Liu, Anthony L Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024. ", "page_idx": 12}, {"type": "text", "text": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In International Conference on Learning Representations, 2021.   \nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. Transactions on Machine Learning Research, 2022.   \nChuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:23412\u201323425, 2022.   \nGuangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22490\u201322499, 2023a.   \nKaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239, 2023b. ", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Auto-regressive Latent Modeling ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this work, we explore four types of abstract latents, including textual descriptions (text), detection bounding boxes (bbox), object blobs (blob), and visual tokens (voken). Each type is designed to enrich the mode-to-image correspondence, covering different aspects of image formation. Examples of these abstract latents are illustrated in Fig. 4. In the following paragraphs, we detail the methodology employed in constructing the training dataset for these abstract latents. Additionally, Fig. 9 outlines the pipeline for the step-by-step generation of these abstract latents. ", "page_idx": 13}, {"type": "text", "text": "Textual descriptions Typical text-image datasets often provide captions that fail to fully capture the details of the image. For instance, as shown in Fig. 4, the original caption \u201cDog laying on a human\u2019s lap\u201d omits crucial details such as the presence of \u201claptop\u201d, which is essential for accurate image ", "page_idx": 13}, {"type": "text", "text": "Textual descriptions (caption) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Original caption: {}   \nUsing the information provided in the caption above, Please provide a detailed description of the image in 50-80 words, incorporating relevant information from the caption and expanding on the visual elements: ", "page_idx": 13}, {"type": "text", "text": "- Include names, objects, events, and locations mentioned in the caption - Do not include placeholders like <PERSON> in the caption - Describe people, characters, animals, and notable entities - Mention the setting, background, and overall environment - Note colors, lighting, composition, and style aspects - Refer to any text, symbols, or logos in the image ", "page_idx": 13}, {"type": "text", "text": "Combine caption details with your observations to create a comprehensive description of the key elements and overall scene, focusing on the most salient aspects of the image. ", "page_idx": 13}, {"type": "text", "text": "Textual descriptions (label) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Object labels: {}   \nUsing the provided object labels, generate a detailed description of the image in 50-80 words, incorporating the relevant information about prominent objects identified and expanding on the visual elements:   \n- Describe each labeled object, including its size, shape, and placement in the scene   \n- Depict relations between objects, such as proximity or arrangement   \n- Highlight interactions or functions implied by the objects   \n- Describe the surrounding environment or context that complements the labeled objects   \n- Any notable features or characteristics of the objects, like color, texture, or design elements   \n- Describe people, characters, animals, and notable entities   \n- Mention the setting, background, and overall environment   \n- Note colors, lighting, composition, and style aspects   \n- Refer to any text, symbols, or logos in the image ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Craft a comprehensive depiction of the scene based on the identified objects, utilizing both the labels and contextual observations to enrich the description. ", "page_idx": 13}, {"type": "text", "text": "Captions with grounding ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Generate the caption in English with grounding: ", "page_idx": 13}, {"type": "text", "text": "Table 2: The instruction for prompting Qwen-VL to generate detailed textual descriptions and captions with grounding. ", "page_idx": 13}, {"type": "text", "text": "generation. To address this, we employ detailed textual descriptions as latent variables. These textual descriptions supplement the original captions by providing additional information that might be missing from the original captions. Specifically, we leverage Qwen-VL-Chat [Bai et al., 2023], a large visual language model, designed for effective instruction-following across a variety of multimodal tasks. We instruct Qwen-VL-Chat to produce a detailed textual description given the original caption and corresponding image. The specific instructions used for generating the textual descriptions are detailed in Table 2 under the section Textual descriptions (caption). Fig. 4 shows an example of the generated detailed textual description that provides a more comprehensive depiction of the scenes than the original captions, thus allowing for a richer image generation. ", "page_idx": 14}, {"type": "text", "text": "Additionally, for the ImageNet dataset, which consists of label-image pairs for class-to-image generation, we instruct Qwen-VL-Chat to generate detailed descriptions based on the class label and corresponding image. The instruction for this procedure is similarly documented in Table 2 under the section Textual descriptions (label). ", "page_idx": 14}, {"type": "text", "text": "Detection bounding boxes The spatial location of objects within an image is also crucial information for accurate representation of the image, yet such information is typically absent in textual descriptions. To incorporate this spatial information into the image generation process, we use detection bounding boxes as one type of abstract latents. Specifically, we use Qwen-VL [Bai et al., 2023] to prompt the model to \u201cGenerate the caption in English with grounding:\u201d. This approach results in captions where the spatial locations of objects are explicitly annotated within the text. For instance, as shown in Fig. 4, the caption with grounding for this example is: \u201cDog (1, 33, 995, 995) resting head on owner\u2019s lap (1, 630, 785, 998) while they work on a laptop (39, 336, 999, 972).\u201d Each bounding box is described in a string format $\\cdot{x_{1}},{y_{1}},{x_{2}},{y_{2}}^{,*}$ , where $x_{1}$ , $y_{1}$ and $x_{2}$ , $y_{2}$ are the coordinates of the top-left and bottom-right corner, respectively. All the coordinates are normalized to a $[0,1000]$ range. The coordinates string is treated as part of the text, obviating the need for an additional positional vocabulary. ", "page_idx": 14}, {"type": "text", "text": "Object Blobs Inspired by Nie et al. [2024], we utilize object blobs as the abstract latents that contain more advanced spatial information. An object blob is defined as a tilted ellipse that specifies the position, size, and orientation of an object within an image. Specifically, a blob is represented as \u201c $(x_{c}$ , $y_{c}$ , rmajor, $r_{m i n o r},\\theta)^{*}$ \u201d where $(x_{c},y_{c})$ denotes the center point of the ellipse, $r_{m a j o r}$ and $r_{m i n o r}$ are the radii of its semi-major and semi-minor axes, respectively, and $\\theta\\in[0,180)$ denotes the orientation angle of the ellipse. To extract the blobs for meaningful objects, we leverage the results from bounding box detection and employ SAM [Kirillov et al., 2023] to generate the segmentation maps using the bounding boxes as prompts. Subsequently, an ellipse fitting algorithm is applied to these segmentation maps to determine the blob parameters for each identified object. This method allows for a more precise representation of objects\u2019 spatial characteristics, thus improving the integration of spatial and structural information within the image generation process. ", "page_idx": 14}, {"type": "text", "text": "Visual Tokens Representing images via discrete visual tokens, especially using technologies like Vector Quantized Variational Autoencoder (VQ-VAE) [Van Den Oord et al., 2017], has become a prevalent technique in generative modeling due to its ability to encode high-dimensional image data into a more manageable, discrete space. In this work, we utilize SEED [Ge et al., 2023b], a VQ-based image tokenizer, to encode an image into a sequence of abstract discrete image tokens. These tokens encapsulate high-level semantic information of the visual elements in the image, serving as potent latent variables for guiding the diffusion model. The visual tokens are concatenated with the delimiter \u201c#\u201d, forming a sequence of visual tokens represented as $'I_{1}\\#I_{2}\\#...\\#I_{32}\"$ , where each \u201c ${\\mathbf{}}I_{i}{\\mathbf{}}^{,,,}$ denotes the image token id. ", "page_idx": 14}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Architecture ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this paper, we use the following NestedUNet architecture proposed in Gu et al. [2023] to implement the denoising model. The total number of parameters is about 500M. For the autoregressive prior, we employ T5-XL [Raffel et al., 2020] for all experiments regardless of the input latent types. Both the denoiser and T5 decoder receive gradients and are trained end-to-end. ", "page_idx": 14}, {"type": "text", "text": "config: resolutions $=$ [256 ,128 ,64] resolution_channels $=$ [64 ,128 ,256] inner_config: resolutions $=$ [64 ,32 ,16] resolution_channels $=$ [256 ,512 ,768] num_res_blocks $=$ [2 ,2 ,2] num_attn_layers_per_block $=$ [0 ,1 ,5] num_head $\\mathtt{s}\\!=\\!8$ , schedul $\\varrho=:$ cosine \u2019 num_res_blocks $=$ [2 ,2 ,1] num_attn_layers_per_block $=$ [0 ,0 ,0] schedul $\\mathtt{o}=\\mathtt{'}$ cosine -shift4 \u2019 emb_channel $\\mathbf{s}=\\mathbf{1}024$ , num_lm_attn_layer $\\mathtt{s}\\!=\\!2$ , lm_feature_projected_channel $\\mathbf{s}=\\mathbf{1}\\,0\\,24$ ", "page_idx": 15}, {"type": "text", "text": "B.2 Training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For all experiments, we share all the following training parameters for both the baseline model and the proposed Kaleido Diffusion. ", "page_idx": 15}, {"type": "text", "text": "default training config: batch_siz $e=\\mathtt{5}\\,12$ num_update $\\mathtt{s}=\\mathtt{4}00$ _000 optimizer $=\\cdot$ \u2019adam \u2019 adam_beta $\\mathtt{1}=0$ .9 adam_beta $_{2=0}$ .99 adam_eps $\\mathfrak{z}=1$ .e-8 learning_rat $\\e=1\\,\\ e\\mathrm{~-~}4$ learning_rate_warmup_steps $=\\!10$ _000 weight_decay $=\\!0$ .0 gradient_clip_norm $^{=2}$ .0 ema_decay $=\\!0$ .9999 mixed_precision_training $=$ bp16 ", "page_idx": 15}, {"type": "text", "text": "All experiments are performed on 64 A100 GPUs which takes roughly 2 weeks for training $400\\mathbf{k}$ steps for both ImageNet and CC12M datasets. For text-to-image models, we perform an additional 400k steps progressive training at $64\\times64$ resolution, while we train the entire model from scratch directly at $256\\times256$ for ImageNet. Due to the memory cost of the T5-decoder, we can only fti $4\\sim8$ images per GPU, causing at least $\\times3$ slower training compared to the original MDM models. ", "page_idx": 15}, {"type": "text", "text": "B.3 Learned Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To demonstrate the effectiveness of various latents, we train our model with 5 types including text, bbox, blob, voken, and combined for text-to-image generation. For combined setting, we use the autoregressive model to predict ", "page_idx": 15}, {"type": "equation", "text": "$$\nc o m b i n e d=t e x t\\mid b b o x\\mid\\nu o k e n\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in a sequential way such that the latter latents will be controlled by earlier latents. We also trained models on ImageNet using combined latents for quantitative comparison. ", "page_idx": 15}, {"type": "text", "text": "C Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Training Complexity: The enhanced diffusion model may require more complex and extended training processes compared to standard models. This could lead to increased computational costs and longer development times, potentially limiting accessibility for smaller organizations or individual researchers. ", "page_idx": 15}, {"type": "text", "text": "Difficulty in Finding Optimal Latents: Identifying the most effective latent variables to achieve the desired output diversity can be challenging. This process might involve extensive experimentation and fine-tuning, which can be time-consuming and resource-intensive. Additionally, covering a broader range of modes, such as depth and semantic maps, adds another layer of complexity to the model development, requiring sophisticated techniques to integrate these diverse forms of data effectively. ", "page_idx": 16}, {"type": "text", "text": "Memory Usage: The improved diffusion model, with its increased output diversity, might demand higher memory usage due to the integration of the heavy language models. However, potential strategies such as partial training or joint training with LLMs could be explored to mitigate this issue. These methods could help distribute the computational load more effectively and reduce the memory footprint during the training process. ", "page_idx": 16}, {"type": "text", "text": "D Impact Statement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The proposed method to enhance diffusion models and increase output diversity has significant social implications. By advancing the diversity and accuracy of generated outputs, this technology can be leveraged in various fields such as art, media, and content creation, providing more inclusive and representative outputs that reflect a broader spectrum of human experiences and creativity. Moreover, in areas like healthcare and education, diverse and precise models can lead to more personalized and effective solutions, addressing the unique needs of individuals and communities. This innovation also promotes ethical AI practices by reducing biases in model outputs, fostering a more equitable digital landscape. Ultimately, the enhanced diffusion models will contribute to the democratization of AI, making sophisticated tools accessible to a wider range of users and applications, thereby driving societal progress and innovation. ", "page_idx": 16}, {"type": "text", "text": "E Color Cluster as Additional Latent ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We have included an alternative approach that constructs latent tokens without relying on additional knowledge. Specifically, we train a model using color clusters as latent tokens. For each color channel (R, G, and B) within the range of $0-255$ , we equally segment it into eight clusters, resulting in a total of $8\\times8\\times8=512$ color clusters. Given an image, we resize it to 4x4 pixels and assign a color cluster ID to each pixel based on its RGB value. The image is then encoded into a sequence of color cluster IDs (e.g., $^{\\leftarrow}C_{1}\\#C_{2}\\#...\\#C_{512}{}^{,,})$ , with each $C_{i}$ representing a color cluster ID. This sequence serves as the condition for training our Kaleido diffusion. ", "page_idx": 16}, {"type": "text", "text": "In Fig. 10, we showcase images generated using color clusters as latent tokens on ImageNet. Our results demonstrate that, compared to the baseline MDM, our Kaleido diffusion can generate more diverse images with latent tokens derived purely from color clustering. This highlights that Kaleido diffusion\u2019s capability to generate diverse images is independent of distilled external knowledge, confirming that our approach can produce varied images without the aid of any other pre-trained models. ", "page_idx": 16}, {"type": "text", "text": "F Qualitative Comparison with CADS ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Fig. 11 shows the visual comparisons for class- and text-conditioned image generation with CADS. ", "page_idx": 16}, {"type": "text", "text": "G Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We show additional results randomly sampled from our models. For all results including the baseline model, we use DDPM sampling with 250 steps. ", "page_idx": 16}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/5b6503240eac8db5b75a51f4b7372dc6331852e882f79fad7c58c082853a69cf.jpg", "img_caption": ["Textual Descriptions "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Detection Bounding Boxes ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/de4bc9771dd721b1bb6e76251f926dde12d6d44252fea314711af35fef3708be.jpg", "img_caption": ["Object Blobs "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/1756bd01fdfcb9994a415a133622ae5f090055a9c7be7c3b93dda517eee20956.jpg", "img_caption": ["Figure 9: Pipeline for generating various discrete latents. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Kaleido-MDM with Color Clusters as Latents (Ours) ", "page_idx": 18}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/e5614340ad2878cbd1615c92074036cfabc46f4937e57ee98c0358427b77e406.jpg", "img_caption": ["Figure 10: Kaleido-MDM with color clusters as latents on ImageNet (class: lemon) "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/b6ac9f3d9d8bfcc803bf0be7123a2a7c59e60d36f6f002246631c405e539bd80.jpg", "img_caption": ["(a) Class-conditioned Image Generation (class: Bal Eagle). "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/44a76ed6d8ced530a5eb8fc4351652db90001d47015959072808369fb53d0683.jpg", "img_caption": ["Figure 11: Qualitative Comparison with CADS ", "(b) Text-conditioned Image Generation (prompt: a cat sleeping on the bed). "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Baseline Diffusion Model (MDM) ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/789a2564bf1a3351b3951d5bb99a4df4ed47e59507cc9c0345b5be0052759e33.jpg", "img_caption": ["Kaleido Diffusion Model (Ours) "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/f44b844a852c9089a509cb5341dafd8e0d84b387535d3d3afd6ada7a05e1b3aa.jpg", "img_caption": ["Figure 12: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion on ImageNet $256\\times256$ . The guidance scale is set 4.0. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Baseline Diffusion Model (MDM) ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/d2f51bdb09e22356996a2d11168e0049b92837d49a571bcc1e79cd42150165ea.jpg", "img_caption": ["Kaleido Diffusion Model (Ours) "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/91171ced07d7a5c19fdd8477414576c3a2605de7a59260e546133030389df3cb.jpg", "img_caption": ["Figure 13: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion on ImageNet $256\\times256$ . The guidance scale is set 4.0. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/8e71352768e9d03eb4ba7d1bdf1c9ea2408103942192c7d9b0c97430f9919757.jpg", "img_caption": ["Figure 14: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion (using text,bbox,blob,voken latents) on CC12M $256\\times256$ given the same condition. We visualize the generated bounding-boxes and blobs for the ease of visualization. The guidance scale is set 7.0. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/2a2843ed95a51eba8b5bff59c015d5ef5cee8b0bc6d97d17b05901c78b75fc63.jpg", "img_caption": ["Figure 15: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion (using text,bbox,blob,voken latents) on CC12M $256\\times256$ given the same condition. We visualize the generated bounding-boxes and blobs for the ease of visualization. The guidance scale is set 7.0. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "qZSwlcLMCS/tmp/7ff95c195cba0af9f0e4ab162d7a2b4786ca71c062852a1a90db2602d39ea3e9.jpg", "img_caption": ["Input: A dog is reading a thick book "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 16: Interactive example of editing the generation process by manipulating the autoregressive predicted latents. The top row displays images generated using autoregressively produced latent tokens, and the subsequent rows show the images re-generated after applying editing on the latents. The guidance scale is set 7.0. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We made the main claim of \u201cimproving diffusion models with autoregressive latent modeling\u201d in both the abstract and introduction. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Due to page limits, we include discussion of limitations in appendix section. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This is not a theory paper Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include the model hyperparameters both in main paper and appendix for people to reproduce. Both datasets we used are open-sourced. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Code will be released after acceptance and internal review. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We specified the details about training and testing Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Due to the costly nature of each run, we did not include error bars ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We mention the compute requirements in the experiments and appendix Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper conducted in code of ethics ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Due to page limits, we include societal impacts in the appendix Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 27}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We did not plan to release pretrained models therefore no risk for misuse Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We cited all the papers for codes and datasets we used. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no newly released assets ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no crowdsourcing steps. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]