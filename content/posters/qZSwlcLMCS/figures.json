[{"figure_path": "qZSwlcLMCS/figures/figures_0_1.jpg", "caption": "Figure 1: Comparison of the generated image samples given the caption \"a cat sat on the mat\". Our models generate more diverse images with the help of autoregressive latent modeling.", "description": "This figure compares the results of standard diffusion models and Kaleido Diffusion models.  Given the same text prompt (\"A cat sat on the mat\"), the standard diffusion model produces images of cats that are visually similar. In contrast, Kaleido Diffusion, which incorporates an autoregressive latent modeling technique, generates a wider variety of images, showcasing improved diversity in the generated samples.", "section": "Abstract"}, {"figure_path": "qZSwlcLMCS/figures/figures_3_1.jpg", "caption": "Figure 2: Training pipeline of the proposed Kaleido diffusion.", "description": "This figure illustrates the two-stage training pipeline of Kaleido diffusion. Stage I involves extracting discrete latent tokens (textual descriptions, bounding boxes, object blobs, and visual tokens) from the input image using latent extractors (e.g., Multimodal Large Language Models). These latents serve as abstract representations of the image. In Stage II, an autoregressive model (e.g., T5-decoder) generates the latents, which are then concatenated with the context encoder's output. This combined information is fed into a diffusion model (e.g., Matryoshka Diffusion Model), which iteratively synthesizes the final image. The autoregressive model and diffusion model are jointly trained to generate high-quality images while maintaining diversity.", "section": "3 Kaleido Diffusion"}, {"figure_path": "qZSwlcLMCS/figures/figures_4_1.jpg", "caption": "Figure 3: Effect of augmented latents. The first row displays the sampling results from the standard diffusion model, while the second row shows the results from the latent-augmented diffusion models.", "description": "This figure demonstrates the effect of explicitly introducing latent priors in a toy dataset with two main classes, each containing two modes.  It compares two models: a standard diffusion model conditioned on the major class ID, and a latent-augmented model incorporating subclass ID as priors.  It shows that while the standard diffusion model tends to converge to one mode (subclass) with increased guidance, the latent-augmented model captures all modes, highlighting the benefit of latent priors for improving diversity under high guidance.", "section": "3.2 Autoregressive Latent Modeling"}, {"figure_path": "qZSwlcLMCS/figures/figures_5_1.jpg", "caption": "Figure 4: A Variety of Discrete Tokens. Original caption: \u201cDog laying on a human's lap\u201d", "description": "This figure shows four different types of discrete tokens that can be used as latent variables in the Kaleido Diffusion model.  These tokens represent different levels of abstraction and detail about the image, such as a detailed textual description, bounding boxes around objects, visual tokens from a vector quantized model, and blob-like representations of objects. These diverse representations aim to enhance the diversity of generated images. The original caption \"Dog laying on a human's lap\" provides minimal context, whereas the detailed representations offer richer information for image generation, leading to more diversity.", "section": "3 Kaleido Diffusion"}, {"figure_path": "qZSwlcLMCS/figures/figures_5_2.jpg", "caption": "Figure 5: Comparison with guidance weights.", "description": "The figure shows FID and Recall scores with varying guidance weights for both the baseline model (MDM) and the proposed Kaleido Diffusion model.  It demonstrates Kaleido's improved diversity (higher Recall) and maintained quality (lower FID) across various guidance weight settings, especially showing its robustness compared to the baseline model at high guidance weights.", "section": "4.2 Quantitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_7_1.jpg", "caption": "Figure 2: Training pipeline of the proposed Kaleido diffusion.", "description": "This figure illustrates the training pipeline of the Kaleido diffusion model. It shows two main stages: Stage I involves the extraction of latent tokens (discrete latents) using latent extractors such as Multimodal LLMs (MLLMs), and Stage II entails the joint training of an autoregressive model (e.g., T5-decoder) and a diffusion model (e.g., MDM). The autoregressive model takes the context encoder output and the discrete latents as input to generate new discrete latents.  These new latents are concatenated with the context encoder output and passed to the diffusion model for image generation. The figure highlights the flow of information and the interaction between the two models in the training process.", "section": "3 Kaleido Diffusion"}, {"figure_path": "qZSwlcLMCS/figures/figures_8_1.jpg", "caption": "Figure 7: Diversity comparison to standard diffusion model. Images sampled under varying CFG scales (\u03b3). Panels (a) and (c) display images from the baseline models, while panels (b) and (d) show images from Kaleido. From top to bottom, as the CFG increases, the standard diffusion models exhibit reduced diversity, while Kaleido consistently maintains diversity across guidance scales.", "description": "This figure compares the image diversity generated by the proposed Kaleido diffusion model and a standard diffusion model under various classifier-free guidance (CFG) scales.  The top two rows show samples generated from the standard diffusion model and the bottom two rows from the Kaleido model for two different tasks (class-to-image and text-to-image). As CFG increases, the standard diffusion model shows a sharp decrease in image diversity, while Kaleido maintains a consistently high level of diversity.", "section": "4.3 Qualitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_9_1.jpg", "caption": "Figure 2: Training pipeline of the proposed Kaleido diffusion.", "description": "This figure illustrates the two-stage training pipeline of Kaleido Diffusion. Stage I focuses on extracting discrete latent tokens (e.g., textual descriptions, object blobs, bounding boxes, visual tokens) from the input image using various methods such as Multimodal Language Models (MLLMs) and autoregressive models. Stage II involves joint training of an autoregressive model (e.g., T5-decoder) to predict these latent tokens given the original caption and a latent-augmented diffusion model (e.g., MDM) to generate images conditioned on both the original caption and the autoregressively generated latent tokens.  The context encoder and the autoregressive decoder share attention with the diffusion model to ensure effective integration of textual and visual information.", "section": "3 Kaleido Diffusion"}, {"figure_path": "qZSwlcLMCS/figures/figures_17_1.jpg", "caption": "Figure 2: Training pipeline of the proposed Kaleido diffusion.", "description": "This figure illustrates the training pipeline of the Kaleido diffusion model. It shows two main stages: latent token extraction and autoregressive and diffusion joint training. In the first stage, discrete latents are extracted from an image using different methods (e.g., multi-modal large language models). In the second stage, an autoregressive model (e.g., T5 decoder) is used to generate the latent tokens which are then concatenated with the original context and fed into a diffusion model (e.g., MDM) to generate the final image. This architecture is designed to combine the advantages of both autoregressive and diffusion models for improved image generation.", "section": "3 Kaleido Diffusion"}, {"figure_path": "qZSwlcLMCS/figures/figures_17_2.jpg", "caption": "Figure 9: Pipeline for generating various discrete latents.", "description": "This figure illustrates the process of generating different types of discrete latent tokens from an image and caption using various models.  It demonstrates how textual descriptions, bounding boxes, object blobs, and visual tokens are extracted.  The process starts with an image and caption as input, which are then processed by different models (e.g., QWen-VL for captions and bounding boxes, Segment Anything (SAM) for object blobs, SEED for visual tokens) to extract the various discrete latents.", "section": "3.2 Autoregressive Latent Modeling"}, {"figure_path": "qZSwlcLMCS/figures/figures_17_3.jpg", "caption": "Figure 2: Training pipeline of the proposed Kaleido diffusion.", "description": "This figure illustrates the training pipeline of the Kaleido diffusion model.  It shows the two main stages: Stage I, Latent tokens Extraction, which uses various latent extractors (e.g., MLLMs) to extract discrete latent tokens from the input image and caption. The output of this stage is then passed to Stage II: Autoregressive and Diffusion Joint Training. In stage II, an autoregressive model (e.g., T5-decoder) predicts discrete latents based on the contextual information from the context encoder, and these latent tokens are concatenated with the original caption as additional input to the diffusion model (e.g., MDM). The diffusion model then learns to generate images based on both the original caption and the autoregressively generated discrete latent tokens. ", "section": "3 Kaleido Diffusion"}, {"figure_path": "qZSwlcLMCS/figures/figures_18_1.jpg", "caption": "Figure 10: Kaleido-MDM with color clusters as latents on ImageNet (class: lemon)", "description": "This figure visually compares the image generation results between the baseline model (MDM) and the proposed Kaleido model using color clusters as latent tokens.  The left panel shows images generated by the baseline MDM, showcasing a limited diversity in lemon images. The middle panel displays the AR-predicted color clusters used as input to Kaleido. The right panel presents the images generated by Kaleido, demonstrating a notable increase in the diversity and variety of lemon images, highlighting the effectiveness of incorporating color clusters as latent variables.", "section": "4.2 Quantitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_18_2.jpg", "caption": "Figure 7: Diversity comparison to standard diffusion model. Images sampled under varying CFG scales (\u03b3). Panels (a) and (c) display images from the baseline models, while panels (b) and (d) show images from Kaleido. From top to bottom, as the CFG increases, the standard diffusion models exhibit reduced diversity, while Kaleido consistently maintains diversity across guidance scales.", "description": "This figure compares image diversity generated by standard diffusion models and Kaleido Diffusion under different classifier-free guidance (CFG) scales. It shows that while standard diffusion models lose diversity as CFG increases, Kaleido maintains a diverse range of images.", "section": "4.3 Qualitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_18_3.jpg", "caption": "Figure 11: Qualitative Comparison with CADS", "description": "This figure compares the image generation results of the baseline model (MDM) and Kaleido model using different sampling strategies (DDPM and CADS) for both class-conditioned and text-conditioned image generation tasks.  Each column shows samples generated with the same prompt but using different methods. It visually demonstrates that Kaleido improves image diversity compared to the baseline model, particularly with the CADS sampling strategy.", "section": "4.3 Qualitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_19_1.jpg", "caption": "Figure 12: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion on ImageNet 256 \u00d7 256. The guidance scale is set 4.0.", "description": "This figure shows uncurated image samples generated by both the baseline diffusion model (MDM) and the proposed Kaleido diffusion model on the ImageNet dataset. The images are 256x256 pixels, and the classifier-free guidance (CFG) scale is set to 4.0. The figure aims to visually demonstrate the diversity of generated images by each model.", "section": "4.3 Qualitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_19_2.jpg", "caption": "Figure 12: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion on ImageNet 256 \u00d7 256. The guidance scale is set 4.0.", "description": "This figure displays uncurated samples generated by both the baseline diffusion model (MDM) and the proposed Kaleido Diffusion model.  The images showcase a variety of subjects (space shuttle, cockatoo, sea turtles) illustrating the diversity achieved by the two models, particularly when compared under the same guidance scale (4.0).  Kaleido shows greater visual variety than MDM.", "section": "4.3 Qualitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_20_1.jpg", "caption": "Figure 13: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion on ImageNet 256 \u00d7 256. The guidance scale is set 4.0.", "description": "This figure shows uncurated samples generated by both the baseline diffusion model (MDM) and the Kaleido diffusion model.  The images illustrate the diversity of samples generated by each model for three different classes from ImageNet: rhinoceros beetles, llamas, and broccoli.  The guidance scale used was 4.0, a parameter that controls the balance between fidelity to the prompt and diversity in the generated images. The figure helps to visually demonstrate the improved diversity offered by the Kaleido model.", "section": "4.3 Qualitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_20_2.jpg", "caption": "Figure 1: Comparison of the generated image samples given the caption \"a cat sat on the mat\". Our models generate more diverse images with the help of autoregressive latent modeling.", "description": "This figure compares image samples generated by a standard diffusion model and the proposed Kaleido diffusion model, both using the same caption prompt: \"a cat sat on the mat\". The goal is to highlight the increased diversity of images produced by Kaleido, which incorporates autoregressive latent modeling to enhance the variety of generated samples.", "section": "Abstract"}, {"figure_path": "qZSwlcLMCS/figures/figures_21_1.jpg", "caption": "Figure 14: Uncurated samples for both the baseline (MDM) and the proposed Kaleido Diffusion (using text, bbox,blob,voken latents) on CC12M 256 \u00d7 256 given the same condition. We visualize the generated bounding-boxes and blobs for the ease of visualization. The guidance scale is set 7.0.", "description": "This figure compares image samples generated by the baseline diffusion model (MDM) and the Kaleido diffusion model using different types of latent variables (text, bounding boxes, object blobs, and visual tokens) for the same input condition.  The goal is to illustrate the diversity of image generation achieved by Kaleido, especially in comparison to the MDM model, which shows less diversity. The guidance scale (CFG) is set to 7.0, indicating a high level of control over the generation process.", "section": "4.3 Qualitative Results"}, {"figure_path": "qZSwlcLMCS/figures/figures_22_1.jpg", "caption": "Figure 4: A Variety of Discrete Tokens. Original caption: \u201cDog laying on a human's lap\u201d", "description": "This figure displays various types of discrete tokens generated by different methods: textual descriptions, detection bounding boxes, object blobs, and visual tokens. Each row shows a different representation of the same image, demonstrating how different types of latents can capture various aspects of the image content and spatial information. The original caption is also provided for context.", "section": "Experiments"}, {"figure_path": "qZSwlcLMCS/figures/figures_23_1.jpg", "caption": "Figure 2: Training pipeline of the proposed Kaleido diffusion.", "description": "This figure shows the training pipeline of the Kaleido diffusion model. It consists of two main stages: 1) Latent tokens extraction, where discrete latents are extracted from the input image using various methods such as multi-modal LLMs. 2) Autoregressive and diffusion joint training, where an autoregressive model generates latent tokens and a latent-augmented diffusion model synthesizes images based on these latents and the original condition. The pipeline combines the strengths of both autoregressive and diffusion models to enhance image generation.", "section": "3 Kaleido Diffusion"}]