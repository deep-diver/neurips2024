[{"figure_path": "dYIqAZXQNV/figures/figures_4_1.jpg", "caption": "Figure 1: Contrasting the assignment of kernel weights to local neighborhood nodes for traditional CNN convolution kernels and the satisficing mapping sub-kernels of a QGCL layer. Traditional CNN convolution kernel is depicted with its natural kernel weights masks while QGCL sub-kernels are shown with their corresponding quantizing kernel masks on graph neighborhoods. Note that the angular quantization bins have inclusive angular lower bounds and exclusive angular upper bounds, such that nodes falling on the edges are mapped to unique sub-kernels (e.g., the node in (h.) on the 135\u00b0edge maps to the green mask sub-kernel.)", "description": "This figure compares the assignment of kernel weights in traditional CNNs and QGCNs.  It illustrates how the natural convolutional masks of CNNs are mapped to sub-kernel masks in QGCNs, highlighting the differences in how neighborhood nodes are assigned to sub-kernels in both methods. The image illustrates an example of a 4x4 image graph, a Navier-Stokes FEM graph, and sample local neighborhoods from each, showing the distribution of sub-kernels.", "section": "3.2 A satisficing mapping generalizes local convolutional kernel masks"}, {"figure_path": "dYIqAZXQNV/figures/figures_5_1.jpg", "caption": "Figure 2: QuantNet and Quantized Graph Residual Layer (QGRL). [Left] A learnable network for dynamic quantization of nodes to subkernels in different local neighborhoods. The message passing framework in PyTorch provides the source and target nodes across all edges so QGCL doesn't have any computation overheads in defining the input tensors fed into QuantNet. The output of QuantNet is the satisficing mapping used to filter the receptive fields of the QGCL subkernels. [Right] An architectural retrofit of QGCL, incorporating 2 residual blocks: (1) outer residual block for the QGCL and (2) an inner residual block for learning features from input graph messages. The network combines all features dynamically via MLP-III to prepare the final node messages for the layer.", "description": "This figure shows the architecture of QuantNet and Quantized Graph Residual Layer (QGRL). QuantNet is a learnable network that dynamically quantizes nodes into subkernels within local neighborhoods.  QGRL is a residual network architecture that incorporates QGCL, improving its robustness and performance.", "section": "3.4 Integrating QGCNs with a residual architecture"}, {"figure_path": "dYIqAZXQNV/figures/figures_15_1.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure demonstrates how the proposed method, Quantized Graph Convolutional Networks (QGCNs), generalizes 2D convolutional neural networks (CNNs) to graph data.  Panel (a) shows a standard convolutional kernel with relative spatial coordinates of kernel weights. Panel (b) illustrates the convolution operation between a 3x3 kernel and a 4x4 image, breaking it down into steps.  Panel (c) shows a sample local neighborhood on a graph with relative spatial coordinates of nodes, highlighting how the concept extends to graph data structures. Finally, the lower part of the figure shows how the 2D convolution can be decomposed into a linear combination of 1D convolutions, illustrating the connection between CNNs and QGCNs.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_15_2.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure demonstrates how the proposed Quantized Graph Convolutional Layer (QGCL) generalizes 2D CNN convolution to graphs.  Panel (a) shows a standard 3x3 convolution kernel with the relative spatial coordinates of kernel weights highlighted. Panel (b) illustrates the convolution process, showing how the kernel slides across the image data, generating partial outputs, which are summed together to create the final output. Panel (c) demonstrates a graph representation of a local neighborhood (e.g., a small patch of pixels) with relative spatial coordinates (analogous to pixels). Panel (d) conceptually shows how a 2D convolution can be viewed as a linear combination of multiple 1D convolutions, which helps to bridge the gap between the array-based convolution of a CNN and the graph-based convolution of the QGCL. The figure highlights the use of relative positional descriptors to assign natural relative positions and descriptor labels to sub-kernels, enabling a generalization of CNN convolution to graphs.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_16_1.jpg", "caption": "Figure 5: Neural GNN Architecture for Node Classification Task. All GNNs we tested had the same architecture as depicted. The generic architecture has 2 blocks, the first is a convolutional layer followed by batch normalization and then a ReLU activation function. The second block in the architecture sums up features from three identical blocks, each of which is a convolutional layer followed by a ReLU activation function. For any given GNN, its message passing layer was substituted into the convolutional layer, depicted in the figure, to derive the overarching architecture. In doing so, we guaranteed an iso-architecture comparison.", "description": "This figure shows the architecture used for node classification tasks, ensuring a fair comparison across different GNN models.  The architecture consists of two main blocks: the first uses convolutional layers with batch normalization and ReLU activation, and the second sums features from three identical blocks of convolutional layers with ReLU activation. The specific message passing layer for each GNN was substituted into the convolutional layer to maintain a consistent architecture for comparison.", "section": "Integrating QGCNs with a residual architecture"}, {"figure_path": "dYIqAZXQNV/figures/figures_16_2.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure illustrates how the concept of a convolutional kernel, commonly used in CNNs for image processing, can be extended to graph data.  Panel (a) shows a convolutional kernel with the relative spatial coordinates of its weights explicitly labeled. Panel (b) visually breaks down the convolution process, showing how the kernel interacts with a small portion of the input image (4x4) to compute partial outputs, which are then summed to obtain the final output. Panel (c) translates this concept to a graph, depicting a sample neighborhood (local structure in the graph) with the relative positional coordinates of its nodes. Panel (d) highlights that 2D convolutions in CNNs can be viewed as a linear combination of 1D convolutions, which is the basis for understanding how it generalizes to graphs.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_17_1.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure demonstrates how the proposed Quantized Graph Convolutional Layer (QGCL) extends the standard convolutional layer from image data to graph data. It contrasts the assignment of kernel weights to local neighborhood nodes in a traditional CNN convolutional kernel and the QGCL's satisficing mapping sub-kernels. It visually depicts the natural kernel weights masks of a CNN convolutional kernel and the quantizing filter masks of QGCL sub-kernels on graph neighborhoods, highlighting how QGCL handles nodes falling on the edges of angular quantization bins.  The figure uses a simple 4x4 image and a Navier-Stokes FEM mesh graph as examples, illustrating how the method naturally generalizes to different graph types.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_17_2.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure compares the assignment of kernel weights in traditional CNNs versus QGCLs.  Panel (a) shows a CNN kernel with relative spatial coordinates; panel (b) visualizes the convolution operation stages; panel (c) shows a sample local neighborhood with relative spatial coordinates; panel (d) shows a 1D convolution of weights; and the last panel shows partial outputs.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_17_3.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure illustrates how the proposed Quantized Graph Convolutional Layer (QGCL) generalizes the traditional convolutional layer of CNNs to graph data.  It contrasts the assignment of kernel weights in a traditional CNN (using natural kernel weight masks) with the QGCL's approach using quantizing filter masks. The satisficing mapping approach is highlighted, showing how local graph neighborhoods are divided into sub-kernels based on relative angular displacements of nodes.  The figure emphasizes that this approach reduces to the standard 2D CNN convolution when applied to image data represented as a graph.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_18_1.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure contrasts the assignment of kernel weights in traditional CNNs versus the QGCL (Quantized Graph Convolutional Layer).  Panel (a) shows a 3x3 convolutional kernel with weights and relative spatial coordinates. Panel (b) visualizes a convolution operation: how the kernel weights are applied to a 4x4 image, yielding partial and final outputs. Panel (c) depicts a graph representation of a local image neighborhood with relative spatial coordinates of nodes. Panel (d) reinterprets a 2D convolution as a combination of multiple 1D convolutions. This highlights how the QGCL uses relative positional information to generalize the CNN convolution to graphs, even when the spatial arrangement of nodes is irregular, as opposed to the fixed order in array data.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_18_2.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure illustrates how the proposed Quantized Graph Convolutional Layer (QGCL) generalizes the 2D convolution operation from array data to graph data. It contrasts the assignment of kernel weights to nodes in local neighborhoods for traditional CNN convolution kernels and the satisficing mapping sub-kernels of QGCL. It shows that the QGCL uses relative positional descriptors to rank nodes within their neighborhoods, which leads to a more natural generalization of CNNs to graphs.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_18_3.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure illustrates how the proposed Quantized Graph Convolutional Network (QGCN) generalizes the CNN convolution to graph data.  It contrasts the assignment of kernel weights in traditional CNNs with the QGCN's approach.  Panel (a) shows a standard 3x3 convolutional kernel with weights and their relative positions. Panel (b) visually represents how the kernel operates on a 4x4 image; the kernel weights are applied to the overlapping receptive fields to produce a single output value for each output pixel position. Panel (c) shows how this concept is extended to graph data. The nodes within a local neighborhood are assigned sub-kernel masks, similar to how kernel weights are assigned to pixels in an image. Panel (d) further clarifies that a 2D convolution can be viewed as a linear combination of 1D convolutions.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_18_4.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure compares the assignment of kernel weights to local neighborhood nodes for traditional CNN convolution kernels and the satisficing mapping sub-kernels of a QGCL layer.  Panel (a) shows a standard convolutional kernel, (b) illustrates the convolution operation, (c) shows a sample neighborhood from a 4x4 image, (d) depicts a 1D convolution operation, (e) shows sample local neighborhoods of graph data, (f) shows a neighborhood with natural filter mask superposed, (g) and (h) show neighborhoods with quantizing filter masks superposed. The figure highlights how the proposed method quantizes graph neighborhoods into sub-kernels using relative angular displacements of nodes.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_19_1.jpg", "caption": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels.", "description": "This figure contrasts the assignment of kernel weights to local neighborhood nodes for traditional CNN convolution kernels and the satisficing mapping sub-kernels of a QGCL layer.  It visually demonstrates how traditional CNNs use fixed-size, regularly ordered neighborhoods (like pixels in an image), while QGCLs adapt this to graphs by using a \"satisficing mapping\" to quantize graph neighborhoods into sub-kernels based on relative angular positions of nodes. This allows QGCLs to generalize CNN convolution to graphs.", "section": "3 Proposed methodology"}, {"figure_path": "dYIqAZXQNV/figures/figures_19_2.jpg", "caption": "Figure 1: Contrasting the assignment of kernel weights to local neighborhood nodes for traditional CNN convolution kernels and the satisficing mapping sub-kernels of a QGCL layer. Traditional CNN convolution kernel is depicted with its natural kernel weights masks while QGCL sub-kernels are shown with their corresponding quantizing kernel masks on graph neighborhoods. Note that the angular quantization bins have inclusive angular lower bounds and exclusive angular upper bounds, such that nodes falling on the edges are mapped to unique sub-kernels (e.g., the node in (h.) on the 135\u00b0edge maps to the green mask sub-kernel.)", "description": "This figure compares how traditional CNNs and QGCLs assign kernel weights to local neighborhood nodes.  It highlights the difference in how the methods handle neighborhoods with varying node positions and shows how QGCLs use angular quantization to map nodes to sub-kernels, improving generalization to graphs.", "section": "3.2 A satisficing mapping generalizes local convolutional kernel masks"}, {"figure_path": "dYIqAZXQNV/figures/figures_21_1.jpg", "caption": "Figure 1: Contrasting the assignment of kernel weights to local neighborhood nodes for traditional CNN convolution kernels and the satisficing mapping sub-kernels of a QGCL layer. Traditional CNN convolution kernel is depicted with its natural kernel weights masks while QGCL sub-kernels are shown with their corresponding quantizing kernel masks on graph neighborhoods. Note that the angular quantization bins have inclusive angular lower bounds and exclusive angular upper bounds, such that nodes falling on the edges are mapped to unique sub-kernels (e.g., the node in (h.) on the 135\u00b0edge maps to the green mask sub-kernel.)", "description": "This figure compares the assignment of kernel weights in traditional CNNs and QGCNs.  It highlights how CNNs use fixed kernel weights for local neighborhoods with fixed ordering, while QGCNs use learnable sub-kernels that are more flexible for graphs with irregular structures. The satisficing mapping approach divides the local graph neighborhood into sub-kernels based on the angle of nodes relative to the center node. This allows for consistent mapping across diverse graphs. ", "section": "3.2 A satisficing mapping generalizes local convolutional kernel masks"}]