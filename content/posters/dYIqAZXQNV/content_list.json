[{"type": "text", "text": "Generalizing CNNs to Graphs with Learnable Neighborhood Quantization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Isaac Osafo Nkansah1 Neil Gallagher1 Ruchi Sandilya1 Conor Liston1 Logan Grosenick1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Department of Psychiatry and BMRI, Weill Cornell Medicine, Cornell University, New York, NY, USA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Convolutional neural networks (CNNs) have led to a revolution in analyzing array data. However, many important sources of data, such as biological and social networks, are naturally structured as graphs rather than arrays, making the design of graph neural network (GNN) architectures that retain the strengths of CNNs an active and exciting area of research. Here, we introduce Quantized Graph Convolution Networks (QGCNs), the first framework for GNNs that formally and directly extends CNNs to graphs. QGCNs do this by decomposing the convolution operation into non-overlapping sub-kernels, allowing them to fit graph data while reducing to a 2D CNN layer on array data. We generalize this approach to graphs of arbitrary size and dimension by approaching sub-kernel assignment as a learnable multinomial assignment problem. Integrating this approach into a residual network architecture, we demonstrate performance that matches or exceeds other stateof-the-art GNNs on benchmark graph datasets and for predicting properties of nonlinear dynamics on a new finite element graph dataset. In summary, QGCNs are a novel GNN framework that generalizes CNNs and their strengths to graph data, allowing for more accurate and expressive models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many important real-world scenarios involve data structured as graphs. For example, neural networks (both biological and artificial) are typically represented as directed graphs where individual neurons propagate information to other neurons along edges. Digital networks (e.g. social networks, the internet) are graphs made up of links between digital objects, and chemical structures can be modeled as graphs made up of bonds between atoms. It can be challenging to accurately model such graph data, creating a barrier to studying these problems. Indeed, this work was motivated by our own experiments in modeling brain networks from neural data, where we have found that existing methods are often not expressive enough to effectively capture the types of phenomena we are interested in. ", "page_idx": 0}, {"type": "text", "text": "In recent years, the prevailing approach for learning from graph data has shifted towards methods inspired by convolutional neural networks (CNNs) [20, 21, 8, 17, 25, 4, 5]. The focus on extending the convolutional layer of CNNs to graph data is motivated by the strong and successful inductive bias of CNNs, which use trainable filters that efficiently model local structure in array-structured data (e.g., images). CNNs have been effectively employed in numerous domains, including natural language processing [26, 43] and image recognition [7]. Because graph data often exhibit strong patterns of local correlation like those seen in language and image data, it is reasonable to expect they might similarly benefit from shared local filters. ", "page_idx": 0}, {"type": "text", "text": "Early work extending CNNs to graphs focused on spectral methods [6, 3], which can suffer from high runtime and memory complexity [40]. In contrast, spatial methods aim to generalize the convolution operation explicitly from array data to graph data. Recent spatial GNN methods approach this problem by either adapting the convolution operation to graphs [11, 1, 27, 28] or by adapting graphs to fti the CNN convolutional operation [10]. Existing spatial methods, however, do not truly generalize the CNN convolution layer. ", "page_idx": 1}, {"type": "text", "text": "Spatial Graph Convolutional Networks (SGCNs) [5], for example, claim to generalize CNNs to graph data as during inference on array data an SGCN will have spatial filters that resemble those of a CNN. But an SGCN will not reduce to an equivalent CNN when trained on array data, and is thus not a proper generalization. This relates to a central challenge of extending convolutions to graph-structured data: in CNNs, local neighborhoods have fixed sizes and fixed ordering of the nodes within the neighborhood, a convenience that does not hold true for more general graphs. We believe that bridging the gap between CNNs and GCNs and properly generalizing the powerful local inductive bias of CNNs to GNNs will lead to improved learning for many types of graph data. ", "page_idx": 1}, {"type": "text", "text": "To this end, we introduce the Quantized Graph Convolution Layer (QGCL), which adds to the spatial graph neural network literature a proper generalization of the CNN convolution layer to graphs. We do this by first \"quantizing\" the convolution operation for CNNs into an equivalent set of non-overlapping sub-kernels applied to local geometry. Second, we describe a specific set of sub-kernels for graphs that are equivalent to a 2D convolutional kernel based on a satisficing mapping, which relies on relative angular displacements of nodes to quantize graph neighborhoods into sub-kernels. To generalize the QGCL to arbitrary graphs, we extend it to be able to learn neighborhood quantizations from data, using a network we term QuantNet. Furthermore, we design a residual network around the QGCL architecture, which we term Quantized Graph Residual Layer (QGRL), to make the layer more robust to model depth effects like vanishing gradients. As we were initially inspired by the regularity of widely-used finite element method (FEM) graphs, we provide a new benchmark data set for FEM (based on Navier-Stokes fluid flow on an adaptive mesh graph) and demonstrate that a QGRL-based architecture (called Quantized Graph Residual Network or QGRN) is highly competitive on such data. Next, we show that QGRNs enable competitive performance across nineteen inductive learning graph datasets. Finally, we demonstrate that incorporating QGRLs leads to superior performance in a supervised autoencoder model applied to a public EEG recordings and emotional states dataset [18]. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. Introducing the Quantized Graph Convolutional Network (QGCN) framework, which generalizes CNNs to graphs.   \n2. Empirical and formal validation that a QGCN using the satisficing mapping sub-kernels reduces to a 2D CNN on image graphs.   \n3. An end-to-end learnable quantization network (QuantNet) that extends QGCNs to arbitrary graphs.   \n4. A residual network inspired architecture, Quantized Graph Residual Networks (QGRNs), that further improves QGCN performance.   \n5. Benchmarking of QGRNs on a new Navier-Stokes FEM dataset and 19 other public benchmark graph datasets for graph classification and node classification.   \n6. Showing QGRLs improve joint modeling of emotional states and EEG data in a supervised autoencoder architecture. ", "page_idx": 1}, {"type": "text", "text": "2 Relevant work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "GCNs Although inspired by spectral theory, Graph Convolutional Networks (GCNs) [17] are practically understood as a spatial GNN method as they aggregate node features within local neighborhoods (normalizing by the node degree of the central/target node) and then transform the resulting aggregated features into new features for the central/target nodes. Because all neighboring node features are scaled by fixed weights and then aggregated in the input features space, this method\u2019s number of trainable free parameters differs from CNNs (failing to generalize the CNN convolution layer). In contrast, CNNs learn embeddings of each node feature separately and the ability of each node\u2019s features to embed in a different point in the output feature space independently makes CNNs more flexible than GCNs. ", "page_idx": 1}, {"type": "text", "text": "SGCNs Spatial Graph Convolutional Networks (SGCNs) [5], a recent novel CNN-inspired GCN architecture, improve on GCNs by using graph node positional descriptors to rank nodes within their neighborhoods. They extend GCNs by using MLPs that project the relative spatial/positional descriptors of nodes into output feature space. Though the authors claim that SGCNs are equivalent to CNNs for inference, SGCNs and CNNs exhibit a different inductive bias during training. There is additional difficulty in matching CNN and SGCN model parameters and determining how scaling different parts of SGCN architecture translates into equivalent CNN adaptions. A strength of SGCNs is that they can consume pseudo-positional descriptors, making them more general than GCNs. ", "page_idx": 2}, {"type": "text", "text": "KerGNNs Kernel GNNs [9] define kernels as sub-graphs with trainable adjacency matrices and node features. The trainable node features for sub-graphs parallel CNN kernel weights, and the learned adjacency matrices allow for different topologies of sub-graph kernels to be learned. In CNNs, the adjacency matrix of the convolving kernel is fixed, hence kerGNNs generalize the CNN convolution operation well in this sense. However, the size of the direct product graph (which captures the relationship between local sub-graph patches and the convolving sub-graph kernel) grows quadratically with the sizes of the local neighborhood and the convolving sub-graph kernel. For graphs with large local neighborhoods, the computation of the adjacency matrix of the direct product graph per local neighborhood (effectively a cubic runtime complexity across the data) becomes very expensive. Further the authors suggest the use of additional trainable weights for the base random walk kernels, causing further divergence with regular CNN convolution layer. ", "page_idx": 2}, {"type": "text", "text": "LGCLs Inspired by [27], the Learnable Graph Convolutional Layer [10] approach is unlike the aforementioned methods, instead adapting graph data into a form that a regular CNN convolution operator can use. It does this by applying max pooling on the feature vectors of the local graph neighborhoods. This does not generalize the CNN convolution operation (which uses all the features within the local neighborhood and not a sub-sampled set). Further, max pooling sub-sampling constrains the model to ascribe more importance to large features; a constraint absent in CNNs. ", "page_idx": 2}, {"type": "text", "text": "GATs GATs extend the power of transformers and attention networks to graphs and have been shown to be highly performant on graph data [38, 39, 2, 36]. In current GATs, every node attends to its neighbors either in a static or dynamic fashion. The attention mechanism effectively yields edge-aware feature scalars from source nodes whose messages must be aggregated for the target node. This is akin to CNNs applying different kernel weights to different node features in local image graph neighborhoods. ", "page_idx": 2}, {"type": "text", "text": "DeeperGCNs Early works [22] adapted residual connections, inspired by ResNets [12], to deep graph networks to deal with the problem of vanishing gradients and over-smoothing [24]. A more recent innovation in this space, GENConv [23], builds on these residual connections (first demonstrating how powerful these connections alone are for deep networks) and innovates generalized messaging passing aggregators, learnable message normalization layers etc. to compete with state-of-the-art performance on standard graph dataset benchmarks. Other works such as DropEdge [31], which propose randomly removing graph edges, and PairNorm [44] which develops a normalization layer to tackle the problems aforementioned, are also noteworthy. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Extending CNNs to graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Quantized Graph Convolution Networks (QGCNs) are an extension of CNNs to graph data. We begin with a formal description of the convolutional layer, which is the core component of CNNs, in order to motivate the Quantized Graph Convolution Layer (QGCL). For simplicity, we focus on the convolutional layer in two dimensions with a stride size of one operating on $\\dot{\\mathbf{G}}\\in\\mathbb{R}^{C^{\\prime}\\times D^{\\prime}\\times F^{\\prime}}$ , where $\\mathbf{G}$ is structured in a way such that proximity and adjacency in the space composed of the first two dimensions has meaning, but that the ordering of those 2D planes along the third dimension is arbitrary. As an example, 2D image data with multiple color channels exhibits this structure. In this case, a convolutional layer will generate an output feature map $\\mathbf{O}\\in\\mathbb{R}^{C\\times D\\times F}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{0}_{c,d,:}=\\left(\\sum_{j=0}^{J-1}\\sum_{k=0}^{K-1}\\mathbf{W}_{j,k,:,:}\\mathbf{G}_{j+c,k+d,:}\\right)+b,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{\\mathsf{W}}\\in\\mathbb{R}^{J\\times K\\times F\\times F^{\\prime}}$ and $\\pmb{b}\\in\\mathbb{R}^{F}$ are the weights and bias terms of the convolutional kernel, respectively. To produce a map that is the same size as the original input, zero-padding can be added along the edges of $\\mathbf{G}$ before applying the convolutional layer to result in $\\mathbf{0}_{c,d,}$ : being defined for $c\\in[0,C^{\\prime}-\\bar{1}],d\\in[0,D^{\\prime}-1\\bar{]}$ . ", "page_idx": 3}, {"type": "text", "text": "We can refactor the kernel parameters in Eq. 1 to have a single index $h$ iterating over the 2D space traversed by $j$ and $k$ above: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{O}_{c,d,:}=\\sum_{h=0}^{J K-1}\\left(\\hat{\\pmb{W}}_{h,:,:}\\sum_{j=0}^{J-1}\\sum_{k=0}^{K-1}\\mathbf{1}_{(h=j K+k)}\\mathbf{G}_{j+c,k+d,:}\\right)+\\hat{\\pmb{B}}_{h,:},\\qquad\\hat{\\pmb{B}}_{h,:}=\\frac{\\pmb{b}}{J K}\\,\\forall h,\\qquad\\hat{\\pmb{B}}_{h,:}=\\hat{\\pmb{\\tau}}_{h,:}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mathbf{W}}_{j K+k,:,:}$ corresponds to $\\pmb{{\\mathsf{W}}}_{j,k,:,:}$ : in Eq. 1. The indicator function $1_{(h=j K+k)}$ has the effect of creating a mask over a single element in the 2D space defined by $j$ and $k$ , with a one-to-one correspondence between each element and each value of $h$ . Thus the convolutional layer can be decomposed into a set of sub-kernels (i.e., weight matrices $\\hat{\\mathbf{W}}_{h,:,:}$ and bias vectors $\\hat{B}_{h,:})$ along with a corresponding set of masks on the input space. This formulation of the convolutional layer allows for interesting possibilities by designing a different set of mask functions; for example, one could produce a well-defined output O with the same dimensions as $\\mathbf{G}$ without the need for zero-padding. ", "page_idx": 3}, {"type": "text", "text": "In the Eq.2, the masks implicitly compare the location of elements in $\\mathbf{G}$ to the relative position of the current output within the larger output tensor O. We generalize the ideas above to graphs by allowing for masks that operate on node pairs, rather than comparing pairs of elements in tensor data. When generalizing the convolution to graph input, we want the output to be a graph as well. Here, we limit ourselves to outputting graphs with identical structure to the input and only considering the local neighborhood of a node when calculating the features of the corresponding output node. We formally define output of the quantized graph convolution layer as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{o}(v)=\\sum_{h=0}^{H-1}\\left(\\hat{\\pmb{W}}_{h,:,:}\\sum_{v^{\\prime}\\in\\mathcal{N}(v)}1_{((v,v^{\\prime})\\in\\mathbb{M}_{h})}\\pmb{a}(v^{\\prime})\\right)+\\hat{\\pmb{B}}_{h,:},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{o}(v)\\,\\in\\,\\mathbb{R}^{F}$ provides the feature vector of the output node at the same relative location as input node $v$ , $\\mathcal{N}(v)$ is the set of nodes in the local neighborhood around $v$ , $\\mathbb{M}_{h}$ is the set of node pairs selected by the mask corresponding to $\\hat{\\mathbf{W}}_{h,:,:}$ , and ${\\bf a}(v)\\in\\mathbb{R}^{F^{\\prime}}$ retrieves the feature vector of node $v$ . In the context of QGCNs, we refer to each $\\hat{\\mathbf{W}}_{h,:,:}$ : as a sub-kernel. It is the process of using binary masks of fixed cardinality to quantize the space of potential nodes in a local neighborhood that gives quantized graph convolution networks their name. This framework is sufficient to include most practical use cases of the convolutional layer. For example, convolutional kernels with any dimension larger than three can be represented by considering tensor elements to be connected in the graph representation if the kernel would apply to one node while the kernel is positioned with the other node at its \u2018center\u2019. Two noteworthy exceptions that do not fit in the QGCN framework are stride sizes larger than one or convolutional kernels with odd dimensions (to be explored in future work). ", "page_idx": 3}, {"type": "text", "text": "3.2 A satisficing mapping generalizes local convolutional kernel masks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we show how the sub-kernel masks $\\left(\\mathbb{M}_{h}\\right)$ ) associated with convolutional kernels can be extended to the case of graphs with (pseudo-)positional information. The masks associated with CNN convolutions are functions of the relative position of tensor elements (nodes) to the position of the current element in the output tensor (see Eq. 2). We refer to these as the natural convolutional masks (see Fig. 1 f). For graphs with positional information, it is possible to formalize a method for choosing a set of sub-kernel masks that would produce standard convolutional layer masks when applied to tensor data that has been converted to graph form. For simplicity we consider the case of a convolution in two dimensions where data is converted to a 2D positional graph by assuming that edges exist only between adjacent elements (nodes) and extend this case to handle all 2D positional graphs. Note that in the absence of positional information, any other information associated with nodes that can be embedded into a 2D space can be treated as pseudo-positional information to enable this more general approach. ", "page_idx": 3}, {"type": "text", "text": "As seen in Eq. 2, each mask is defined by the indicator function $1_{(h=j K+k)}$ , which selects a single element. When dealing with tensor data, the relative position of the elements selected by these masks remains fixed as the convolutional kernel moves to different output positions. In contrast, we cannot assume that the nodes in a local neighborhood will always be located in the same relative positions for all local neighborhoods within a graph. Because we desire a set of masks that is applicable to all 2D positional graphs, our masks must define a way to potentially map all of 2D space to a set of sub-kernels. One simple option is dividing 2D space into regularly spaced non-overlapping segments defined by angular position relative to the center node of the local neighborhood that the map is being applied to. Then the center-neighbor node pair $(\\boldsymbol{v},\\boldsymbol{v}^{\\prime})$ is part of the $h^{\\bar{t}h}$ sub-kernel mask $\\left(\\mathbb{M}_{h}\\right)$ when ", "page_idx": 3}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/a22ced6fd525842ba9c051a07232ece106d38171f1b56cdb9b1985e4bf45e473.jpg", "img_caption": ["Figure 1: Contrasting the assignment of kernel weights to local neighborhood nodes for traditional CNN convolution kernels and the satisficing mapping sub-kernels of a QGCL layer. Traditional CNN convolution kernel is depicted with its natural kernel weights masks while QGCL sub-kernels are shown with their corresponding quantizing kernel masks on graph neighborhoods. Note that the angular quantization bins have inclusive angular lower bounds and exclusive angular upper bounds, such that nodes falling on the edges are mapped to unique sub-kernels (e.g., the node in (h.) on the $135^{\\circ}$ edge maps to the green mask sub-kernel. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{M}_{h}=\\left\\{(v,v^{\\prime})\\mid2\\pi\\frac{h-1}{H-1}+\\phi\\le\\theta(v,v^{\\prime})<2\\pi\\frac{h}{H-1}+\\phi\\right\\}_{\\forall v\\in V,v^{\\prime}\\in\\mathcal{N}(v)},\\;h\\in[1,H-1],}\\\\ &{\\theta(v,v^{\\prime})=\\tan^{-1}\\left(\\frac{p_{y}(v^{\\prime})-p_{y}(v)}{p_{x}(v^{\\prime})-p_{x}(v)}\\right)\\in[0,2\\pi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta$ is the angle of the neighbor node relative to the center node, $p_{x}$ and $p_{y}$ return the $x$ and $y$ coordinates of a node, respectively, and $H$ is the total number of sub-kernels. The $0^{t h}$ sub-kernel is applied only to the node pair $(v,v)$ made up of the center node and itself. The offset angle $\\phi$ is an optional hyperparameter that can be used to select the starting point from which the space is divided. To select $H$ , we choose the smallest number that results in all nodes within a local neighborhood being assigned to a different sub-kernel, which we refer to as a satisficing mapping. It is easy to see that separating the local elements of tensor data based on this method produces the same assignments as the natural convolutional kernel masks (Fig. 1 f). Algorithm 2 in Appendix D outlines an efficient process for determining the value of $H$ that satisfies this condition. If the sub-kernel masks are chosen in this way and, importantly, the bias values for each sub-kernel are tied to the same value (i.e. $\\begin{array}{r}{\\hat{B}_{h,:}=\\frac{\\pmb{b}}{J K},\\forall h)}\\end{array}$ , then we arrive at a set of quantized graph convolution sub-kernels that will behave on 2D positional graph data identically to a standard 2D convolutional layer on equivalent tensor data (see Appendix $\\mathbf{C}$ for proof and below for empirical validation). ", "page_idx": 4}, {"type": "text", "text": "With this satisficing mapping approach, the process of assigning nodes in every local neighborhood to sub-kernels (see Algorithm 1 in Appendix D) incurs a computational cost of $O(|V|^{\\frac{\\l}{2}})$ in each forward pass. In the case of homogeneous graph meshes, choosing to cache the satisficing mapping incurs at worst $O(|V|^{2})$ space complexity. Using Algorithm 2 for determining the minimum number of subkernels for the convolution such that a satisficing mapping is honored adds a constant cost on top of Algorithm 1. We note that this is only one of many possible ways in which the natural convolutional mask and sub-kernels described in Eq. 2 can be extended to positional graph data. ", "page_idx": 5}, {"type": "text", "text": "3.3 Learning neighborhood quantization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we introduce a method for generating masks that assign nodes to sub-kernels in arbitrary dimensions and regardless of whether positional information is present. Specifically, we frame quantization as a learnable multinomial classification problem where for a learnable model assigns a sub-kernel to each center-neighbor node pair in a local neighborhood. This approach was inspired by the idea of dilated convolutions in CNNs, akin to learning the spacings of the kernel elements during CNN convolution [15]. To learn the quantization, we introduce QuantNet, an MLP that projects node features or (pseudo-)positional descriptors into a higher dimensional space where we difference the target and source features and then project this difference to a vector representing assignment weights for each sub-kernel (see Fig. 2). The mask $\\mathbb{M}_{h}$ associated with sub-kernel $h$ contains the ordered node pair $(\\boldsymbol{v},\\boldsymbol{v}^{\\prime})$ when QuantNet $Q$ assigns the node pair to $h$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{M}_{h}=\\{(v,v^{\\prime})|Q(v,v^{\\prime})=h\\}_{\\forall v\\in V,v^{\\prime}\\in\\mathcal{N}(v)}\\,,}\\\\ &{Q(v,v^{\\prime})=\\operatorname*{argmax}(\\operatorname{softmax}(U_{2}(U_{1}(v;\\eta)-U_{1}(v^{\\prime};\\eta);\\theta))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $U_{1}$ is a high dimensional MLP projector with parameters $\\eta$ for the input features (spatial descriptors, node features, etc.), $U_{2}$ is a low dimensional projector with parameters $\\theta$ for the difference in high dimensional features projected by $U_{1}$ , $v$ is a node in $V$ (the node set of the input graph), and $\\mathcal{N}(v)$ denotes the local neighborhood node set of $v$ . Note argmax in Eq. 5 is symbolic; it represents any differentiable function that outputs discrete categorical samples, for example, a custom argmax implemented with a straight-through gradient estimator or Gumbel-Softmax with hard sampling (our implementation uses Gumbel-Softmax with hard sampling) [13]. The QuantNet network architecture is shown in Fig. 2. Finally, we reiterate that because QuantNet can use any vector in place of positional information, QGCL becomes extensible to graphs without explicit positional information. ", "page_idx": 5}, {"type": "text", "text": "3.4 Integrating QGCNs with a residual architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A common and successful approach used to address vanishing gradients and over-smoothing in GNNs is residual learning, inspired by the success of ResNets for CNNs [12]. We adapt this framework ", "page_idx": 5}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/a12a6fbcde0695148611d9011f608b2e21da5a5875181135071655d6ce8a48b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: QuantNet and Quantized Graph Residual Layer (QGRL). [Left] A learnable network for dynamic quantization of nodes to subkernels in different local neighborhoods. The message passing framework in PyTorch provides the source and target nodes across all edges so QGCL doesn\u2019t have any computation overheads in defining the input tensors fed into QuantNet. The output of QuantNet is the satisficing mapping used to filter the receptive fields of the QGCL subkernels. [Right] An architectural retrofit of QGCL, incorporating 2 residual blocks: (1) outer residual block for the QGCL and (2) an inner residual block for learning features from input graph messages. The network combines all features dynamically via MLP-III to prepare the final node messages for the layer. ", "page_idx": 5}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/67e33635c18c2d461a2431c43785b08fe841c1915834a7552108587ed9c0bb5e.jpg", "table_caption": ["Table 1: Standard image datasets. CNN and QGCN model accuracies (mean \u00b1 S.D.). "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/fbe64d11cfdeaa9f563a1c7c1120eddec524296593b9a6532bf72e5c18bc2b48.jpg", "table_caption": ["Table 2: Custom Graph Datasets. QGRN and SGCN Performance Comparison "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "to QGCNs, arriving at the architecture shown in the right panel of Fig. 2, which we call Quantized Graph Residual Layer (QGRL). Notice that QGRL subsumes and generalizes QGCL. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Empirical validation of equivalence with CNNs on 2D images ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "First, we confirmed that the QGCL performs similarly to the CNN convolutional layer when applied to image data. We considered three standard 2D image datasets that vary in complexity: MNIST [21], Fashion-MNIST [41], and CIFAR10 [19]. MNIST contains gray-scaled images of handwritten digits of shape $28\\mathrm{x}28\\mathrm{x}1$ , FashionMNIST contains fashion images of shape $28\\mathrm{x}28\\mathrm{x}1$ , and CIFAR10 consists of color images of shape $32\\mathrm{x}32\\mathrm{x}3$ from 10 categories. Figure 4 in Appendix E shows the different CNN models trained for the different standardized datasets. We created a 3-layer CNN and its equivalent QGCN model for the MNIST dataset, 6-layer network models for FashionMNIST, and 9-layer network models for CIFAR10. The equivalent QGCN models have the same architecture as the CNN models, except that QGCN uses QGCL layers internally in place of traditional convolutional layers. All models were trained 5 times on each dataset, with different random parameter initializations and random ordering of the training data for each run, using cross-entropy loss and the Adam optimizer [16] with a learning rate of 0.01 for 200 epochs. In order to establish equivalence between CNN and QGCN while avoiding full-dataset ceiling effects we separately trained models fit at three different sample sizes (yielding different bias-variance trade-offs) by varying the dataset train-test splits (see Appendix F). ", "page_idx": 6}, {"type": "text", "text": "Table 1 shows how QGCN performs almost identically to CNN across the different standard image datasets. Appendix F (table 8) shows the expanded version of 1, showing different train-test splits, devised to explore bias-variance trade-offs. Additionally, Appendix G shows training loss and train/test set accuracy profiles over a wide range of learning rates for both CNN and its parametermatched equivalent QGCN (not QGRN) to show how model behaviors are very similar even in different bias-variance trade-off regimes. These results confirm how both models follow exceedingly similar loss trajectories during training and have the same accuracy profiles, empirically supporting our formal proof of CNN and QGCN equivalence on image data. ", "page_idx": 6}, {"type": "text", "text": "4.2 Graph Classification: Datasets with Positional Descriptors ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we compared QGRN to SGCN on graph datasets that have positional descriptors, including a novel FEM fixed-mesh graph dataset. The graph benchmark datasets: AIDS, Letters (high/low/med) were post-processed to extract out their positional node descriptors into separate positional attributes that QGRN and SGCN use. This was to show that QGRN is able to use positional descriptors when they exist and is able to perform competitively with models such as SGCN designed specifically to use positional descriptors. Table 2 provides test set accuracy, as well as model size and computational complexity for each model on each positional graph dataset. We highlight that QGRN performs equal to or better than SGCN on all positional graph datasets we tested. ", "page_idx": 6}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/77b92139b02371964cf01add84b13a8ef927cc828d6701e2c37af1c47c92f69d.jpg", "table_caption": ["Table 3: Graph kernels benchmark datasets - I. Test Accuracy $(\\%)$ across different GCNs "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2.1 Custom FEM Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare QGRNs and matched SGCNs on our new simulated Navier-Stokes non-linear dynamics benchmark datasets for binary and denary classification. We simulated the \u201cflow past a cylinder\u201d problem on an adaptive mesh with the underlying two-dimensional flow geometry depicted in Appendix H Fig. 15a. For binary classification, we separated laminar and turbulent flows based on distinct Reynold\u2019s number $(R e)$ values while for denary classification we used evenly spaced $R e$ values. We created three datasets: Navier-Stokes-Binary (NS-Binary) for easier binary classification and Navier-Stokes-Denary-1 (NS-Denary-1) and Navier-Stokes-Denary-2 (NS-Denary-2) for more challenging denary classification, with NS-Denary-2 being most challenging (most closely spaced $R e$ values; see Appendix H). QGRNs matched SGCN performance on the binary task and outperformed SGCNs on the more challenging denary tasks. ", "page_idx": 7}, {"type": "text", "text": "4.3 Graph Classification: Generic Graph Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finally, we compared QGRNs to matched (in model parameter count) GNN models using inductive learning datasets from Benchmark Data Sets for Graph Kernels [14], namely: AIDS, COIL-DEL, Frankenstein, Enzymes, Letter (low/med/high), Mutagenicity, Proteins, Proteins-Full, Mutag and Synthie. See Appendix I for a description of each dataset. We trained on a number of novel GNN architectures including Transformer networks (GAT, TransformerConv), showing how QGRN maintains superior performance over its competitors on many of the benchmark datasets (which lack positional descriptors). We size all models relative to QGRN to have a matched number of parameters for fair comparison. Given the simplicity of some of the models, this effort of establishing equivalence yields slightly different architectures, however, all architectures are constrained to have the same depth. More details are provided in the dataset configuration section of the provided code. All models were trained with the Adam optimizer using cross-entropy loss for 500 epochs at 4 different learning rates (0.1, 0.01, 0.001, 0.0001). Appendix subsection K shows how QGCN wall clock time varies compared to other GNN methods with matching parameter sizes. Each run is repeated 3 times and we report the best accuracy for each model across these learning rates. ", "page_idx": 7}, {"type": "text", "text": "Tables 3, 4 and 12 showcase QGRN matching and outperforming all GNN methods across a diverse sampling of inductive graph learning problems. All datasets appearing in these tables either do not have positional descriptors or have their positional attributes collapsed into the individual node features. We do this because many of the generalized GNNs in the literature such as ChebConv, GCNConv, GraphConv etc. are not able to handle positional descriptors as separate attributes from node features. In the tables, we see clearly how QGRN matches or outperforms all models on all benchmark graph classification tasks. ", "page_idx": 7}, {"type": "text", "text": "Finally, there are additional experiments we carried out such as how QGRN fares in deeper networks (see Appendix O), how different quantizations impact model performance (see Appendix P), how to determine the number of quantization bins for a given dataset (see Appendix Q) and finally a comparison of QGRN with leaderboard performances from papers with code (see Appendix M). ", "page_idx": 7}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/ec147b93c0e29a183aa64ea3571211ec3c9cc28c43208ae20188a26f99fcc4b5.jpg", "table_caption": ["Table 4: Graph kernels benchmark datasets - II. Test Accuracy $(\\%)$ across different GCNs "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/e766a62d272c9ea48fba111ccdb0095eebe9e6a8e51ee3d80b554f674e1159cf.jpg", "table_caption": ["Table 5: Node Classification Datasets. Test Accuracy $(\\%)$ across different GNNs "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4 Node Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we evaluated the performance of QGRNs on various node classification tasks. We tested on multiple types of node classification datasets, including citation networks (like Cora, PubMed), Wikipedia hyperlinks networks (like such as the Chameleon dataset) and product relations networks (such as Amazon Computers) [33, 35, 42]. We highlight that the datasets used in this exploration exhibit different degrees of homophily and heterophily properties. The Chameleon and Squirrel datasets exhibit strong heterophily while all the others exhibit stronger homophily. We chose a single architecture that proved reasonably performant across all the models we compared against (see Figure 5 in Appendix E). We include another novel GNN, EGConv [37] to highlight the competitiveness of our method to recent methods. We trained all models across a range of learning rates (0.1, 0.05, 0.01, 0.005, 0.001) for 2000 epochs and mimicked early stopping by caching the model state that produced the largest validation set accuracy. Given the now apparent fact that Message Passing Neural Networks generally struggle with heterophilic datasets, we designed the generic architecture with edge directionality awareness, as inspired by authors Rossi et. al. [32]. ", "page_idx": 8}, {"type": "text", "text": "We see in Table 5 and Tables 18 and 19 in Appendix L, that QGRNs performed competitively across the homophilic datasets and appreciably well on the two heterophilic datasets. On the Chameleon heterophilic dataset QGRN performs moderately well, outperforming all comparison models except GeneralConv. The Squirrel dataset, which is the most heterophilic of those we tested, proved more challenging for our QGRN models. Overall, QGRNs perform reasonably well on node classification tasks, especially in cases where the dataset exhibits homophilic properties. ", "page_idx": 8}, {"type": "text", "text": "4.5 Supervised Autoencoder Model of Emotional States in EEG data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A major motivation behind these models was the need for more expressive ways to model graphical data with positional information captured from complex geometrical surfaces. As a demonstration of the practical value of QGRNs beyond standardized benchmark datasets, we developed a supervised autoencoder (SAE) to model brain networks related to valence in the publicly available DEAP dataset [18]. The last 42 s of each recording in the DEAP EEG dataset were divided into sliding windows of $3\\mathrm{~s~}$ with $50\\%$ overlap, then were z-scored and spectral power was calculated in four frequency bands ${}^{4-8}\\,\\mathrm{Hz}$ , $8{-}12\\,\\mathrm{Hz}$ , $12{-}30\\ \\mathrm{Hz}$ , and $30{-}44\\ \\mathrm{Hz})$ for each of the 32 electrodes. Power features were used as node attributes in a fully-connected graph containing all electrodes. We trained a separate model for each subject, dividing the data for each subject according to a $64\\%/16\\%/20\\%$ train/validation/test split. The generative objective for the autoencoder was mean-squared error (MSE) and the supervised objective was cross-entropy (CE) for classifying whether subjects self-rated their emotional state as positive valence $\\mathrm{(\\geq5)}$ or negative valence $(<5)$ on a 9-point scale. Models were pre-trained for 1000 epochs on just the generative objective, then for another 100 epochs with one of 3 values of weight (100, 1000, or 10000) on the classification objective. Validation sets were used to select the weight and number of training epochs with highest area under the receiver operating curve (AUC). As shown in table 6, we found that using QGRN layers in the same autoencoder architecture compared to SGCN layers resulted in better generative and supervised loss values on the held out test sets. In addition, the QGRN-based model resulted in appreciably better classification performance (measured by AUC) for this difficult classification problem. ", "page_idx": 8}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/9a13bd250506f991b49b0e485f21d1c7ea5f6fa4dda452d8feb9484dc8adc26c.jpg", "table_caption": ["Table 6: EEG SAE test set performance. All values are presented as the mean $\\pm$ SEM over all subjects. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduces Quantized Graph Convolution Networks (QGCNs), a flexible framework for designing graph neural networks that extends the benefits of CNNs\u2019 strong local inductive bias to graphs. QGCNs \"quantize\" the space of possible neighbor nodes in a local neighborhood into a fixed set of sub-kernels. We show, both theoretically and empirically, that QGCNs are a generalization of CNNs to graphs with positional information. We extend QGCNs to arbitrary graphs by introducing the QuantNet method for learning sub-kernel assignment in a QGCN. We then show that embedding a QGCL within a residual network architecture gives state-of-the-art results on a suite of benchmark graph and node classification tasks, in addition to a novel Navier-Stokes FEM dynamics classification dataset. Finally, we demonstrate that QGCLs improve the performance of a supervised autoencoder to jointly model EEG data and emotional state. ", "page_idx": 9}, {"type": "text", "text": "One significant limitation of the current work is that the implementation of the QGCL is not yet efficient as demonstrated in the comparison of wall clock runtime with other models in Appendix K. Future work will look into parallelized subkernel operations to demonstrate wall clock runtime competitive with similar models in the literature. QGCNs also do not generalize all CNNs, as it cannot represent convolutional layers with odd-numbered kernel sizes (i.e. no \u2019center\u2019 element) or stride sizes other than one. Finally, the use of the QGCL layer in more complex architectures such as U-Nets could be explored in future work, alongside examining the performance of this model on more inductive and transductive tasks to assess its strength in various learning scenarios. ", "page_idx": 9}, {"type": "text", "text": "We have shown that QGCNs are a true generalization of CNNs and therefore are capable of maintaining the same powerful inductive bias that has led to such great success in the application of CNNs. In our experience, QGCNs are more expressive and efficient at learning complex local patterns of correlation in graph data than competing methods, and we expect that QGCNs can be successfully applied in many domains where graph data are prevalent. We expect that future research into different masking functions for QGCN sub-kernels (in addition to the angular satisficing mapping and QuantNet masking functions we outline here) will further extend the potential usefulness of QGCNs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "NG is supported by the NIH CTSA TL1 training program via NIH/NCATS Grant #2TL1TR2386. LG is supported by NIH R01MH131534, R01MH118388, New Venture Fund 202423, a Whitehall Foundation grant (WF 2021-08-089), a Cornell Center for Pandemic Prevention Research seed grant, and an A2 Collective pilot grant (PennAITech, NIA). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. [2] S. Brody, U. Alon, and E. Yahav. How attentive are graph attention networks?, 2022. [3] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013. [4] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling, 2018. [5] T. Danel, P. Spurek, J. Tabor, M. S\u00b4mieja, \u0141. Struski, A. S\u0142owik, and \u0141. Maziarka. Spatial graph convolutional networks. In International Conference on Neural Information Processing, pages 668\u2013675. Springer, 2020. [6] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29, 2016. [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009. [8] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015. [9] A. Feng, C. You, S. Wang, and L. Tassiulas. Kergnns: Interpretable graph neural networks with graph kernels. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6614\u20136622, 2022. [10] H. Gao, Z. Wang, and S. Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1416\u20131424, 2018. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015. [13] E. Jang, S. Gu, and B. Poole. Categorical Reparameterization with Gumbel-Softmax, Aug. 2017. arXiv:1611.01144 [cs, stat]. [14] K. Kersting, N. M. Kriege, C. Morris, P. Mutzel, and M. Neumann. Benchmark data sets for graph kernels, 2016. [15] I. Khalfaoui-Hassani, T. Pellegrini, and T. Masquelier. Dilated convolution with learnable spacings, 2023. [16] D. P. Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [17] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [18] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras. Deap: A database for emotion analysis; using physiological signals. IEEE transactions on affective computing, 3(1):18\u201331, 2011. [19] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. [20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541\u2013551, 1989. [21] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. [22] G. Li, M. M\u00fcller, A. Thabet, and B. Ghanem. Deepgcns: Can gcns go as deep as cnns?, 2019. [23] G. Li, C. Xiong, A. Thabet, and B. Ghanem. Deepergcn: All you need to train deeper gcns, 2020. ", "page_idx": 10}, {"type": "text", "text": "[24] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional networks for semi-supervised learning, 2018.   \n[25] R. Li, S. Wang, F. Zhu, and J. Huang. Adaptive graph convolutional neural networks, 2018.   \n[26] M. M. Lopez and J. Kalita. Deep learning applied to nlp, 2017.   \n[27] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pages 2014\u20132023. PMLR, 2016.   \n[28] G. Nikolentzos, P. Meladianos, A. J.-P. Tixier, K. Skianis, and M. Vazirgiannis. Kernel graph convolutional neural networks. In Artificial Neural Networks and Machine Learning\u2013ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I 27, pages 22\u201332. Springer, 2018.   \n[29] C. Norberg. Flow around a circular cylinder: aspects of fluctuating lift. Journal of fluids and structures, 15(3-4):459\u2013469, 2001.   \n[30] K. Riesen and H. Bunke. Iam graph database repository for graph based pattern recognition and machine learning. In N. da Vitoria Lobo, T. Kasparis, F. Roli, J. T. Kwok, M. Georgiopoulos, G. C. Anagnostopoulos, and M. Loog, editors, Structural, Syntactic, and Statistical Pattern Recognition, pages 287\u2013297, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg.   \n[31] Y. Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards deep graph convolutional networks on node classification, 2020.   \n[32] E. Rossi, B. Charpentier, F. D. Giovanni, F. Frasca, S. G\u00fcnnemann, and M. Bronstein. Edge directionality improves learning on heterophilic graphs, 2023.   \n[33] B. Rozemberczki, C. Allen, and R. Sarkar. Multi-scale attributed node embedding, 2021.   \n[34] M. Sch\u00e4fer, S. Turek, F. Durst, E. Krause, and R. Rannacher. Benchmark computations of laminar flow around a cylinder. Springer, 1996.   \n[35] O. Shchur, M. Mumme, A. Bojchevski, and S. G\u00fcnnemann. Pitfalls of graph neural network evaluation, 2019.   \n[36] Y. Shi, Z. Huang, W. Wang, H. Zhong, S. Feng, and Y. Sun. Masked label prediction: Unified massage passing model for semi-supervised classification, 2020.   \n[37] S. A. Tailor, F. L. Opolka, P. Li\u00f2, and N. D. Lane. Do we need anisotropic graph neural networks?, 2022.   \n[38] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, Y. Bengio, et al. Graph attention networks. stat, 1050(20):10\u201348550, 2017.   \n[39] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph attention networks, 2018.   \n[40] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):4\u201324, 2021.   \n[41] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[42] Z. Yang, W. W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings, 2016.   \n[43] W. Yin, K. Kann, M. Yu, and H. Sch\u00fctze. Comparative study of cnn and rnn for natural language processing, 2017.   \n[44] L. Zhao and L. Akoglu. Pairnorm: Tackling oversmoothing in gnns, 2020. ", "page_idx": 11}, {"type": "text", "text": "Please find the code-base for the paper here: https://github.com/Grosenick-Lab-Cornell/ QuantNets ", "page_idx": 12}, {"type": "text", "text": "B Nomenclature disambiguation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The table 7 is meant to assist with disambiguating different layer, network and model names introduced in this paper. ", "page_idx": 12}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/e804774bcfb7869cc59408e34bb7d7755c0776a439ae766fa8708cd5754c544b.jpg", "table_caption": ["Table 7: Glossary. Terms used in this paper and quick reference meaning "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "C The satisficing mapping QGCL for 2D positional graphs extends the 2D local convolutional layer ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section we formally prove that the satisficing mapping QGCL described in Section 3.2 is identical to the 2D convolutional kernel on the local neighborhood of array elements (i.e. $3\\times3$ kernel) as described in Section 3.1. ", "page_idx": 12}, {"type": "text", "text": "Theorem 1. The satisficing mapping QGCL is identical to a $3\\times3$ convolutional layer when applied to 2D array data converted to a 2D positional graph by forming edges between adjacent elements (i.e. nodes). ", "page_idx": 12}, {"type": "text", "text": "Proof. We have already defined the output of the satisficing mapping QGCL in (3) and (3.2); and we have shown that the output of a standard convolutional layer is given by (2) in Section 3.1. Here, it is sufficient to show that these two are equivalent when using 2D array data that is represented as a 2D positional graph. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sigma(v)=\\sum_{h=0}^{\\infty}\\left(\\hat{\\ W}_{h,\\iota;\\,\\sum_{\\bf\\ell}}\\sum_{e^{\\prime}\\in\\mathcal{N}(v)}1_{((e,v^{\\prime})\\in\\mathcal{M}_{h})}a(v^{\\prime})\\right)+\\hat{B}_{h,\\iota}}\\\\ {\\displaystyle}&{~~=\\sum_{h=0}^{\\infty}\\left(\\hat{\\ W}_{h,\\iota;\\,\\sum_{j=0}^{h}\\sum_{k=0}^{2}1}1_{(h-3)+k}a(v_{j+\\epsilon,k+d}^{\\prime})\\right)+\\hat{B}_{h,\\iota}}\\\\ {\\displaystyle}&{~~=\\sum_{h=0}^{\\infty}\\left(\\hat{\\ W}_{h,\\iota;\\,\\sum_{j=0}^{h}\\sum_{k=0}^{2}1}(\\hat{\\tau}_{h-3})\\hat{\\Psi}_{j}+\\hat{\\epsilon}_{,k+d,*}\\right)+\\hat{B}_{h,\\iota}}\\\\ {\\displaystyle}&{~~=\\sum_{h=0}^{\\infty}\\left(\\hat{\\ W}_{h,\\iota;\\,\\sum_{j=0}^{h}\\sum_{k=0}^{2}1}(\\hat{\\tau}_{h-3,\\iota+k})\\hat{\\mathbf{G}}_{j+\\epsilon,k,\\iota+d,*}\\right)+\\frac{b}{9}}\\\\ {\\displaystyle}&{~~=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proof begins by restating the definition of the QGCL in (6). As described in Section 3.2, the sub-kernel masks are chosen such that within each local neighborhood $\\mathcal{N}(v)$ each mask $\\mathbb{M}_{h}$ is selective of a single neighbor node $v^{\\prime}$ . Thus we can iterate over each sub-kernel and mask by iterating over the rows and columns of the equivalent convolutional kernel, giving (7). We use $v_{a,b}^{\\prime}$ to represent the node associated with the array data $\\mathbf{G}_{a,b,}$ , which gives (8). In Section 3.2, we define the bias vectors associated with each sub-kernel B\u02c6h,: =JbK , , giving (9). This is equivalent to the form given in (2) in Section 3.1, completing our proof. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "This equivalence indicates that both types of convolutional models will produce identical output vectors during inference when given the same parameters $(\\hat{\\mathsf{W}},\\boldsymbol{b})$ and input data (G). This also means that gradients with respect to the parameters will be identical for the purposes of backpropagation, because f(x) = g(x) implies \u2202\u2202fx = \u2202\u2202gx. ", "page_idx": 13}, {"type": "text", "text": "D Quantizing local neighborhoods with satisficing mapping ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/258e98162c88ba516e33c9ce28c585aaa2641e15b80698f1af01f3899637d928.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Algorithm 1 shows in pseudo-code the operation to be done deterministically in each forward pass iteration of the QGCL model with satisficing mapping kernels. Notice that we have $O(k|V|^{2})$ , where $\\vert V\\vert$ is the cardinality of the node set of the graph and $k$ is the constant overhead of computation in each neighborhood to map nodes to specific kernels: this is the complexity of the function named gski. For homogeneous graphs with fixed graph meshes, this computation could be cached, adding a memory/space cost of $\\bar{O(|V|^{2})}$ , exemplified in the strongly connected graph scenario. ", "page_idx": 14}, {"type": "text", "text": "D.1 Determining the minimum number of sub-kernels per QGCL layer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 2 QGCL minimum sub-kernel number ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: $G=(V,E,P)$ and $u b$ , where $V$ is the node set of $G$ , $E$ is the adjacency matrix of $G$ ,   \n$P$ is the positional descriptors for all nodes in $V$ and $u b$ is the upper-bound on number of sub  \nkernels in the QGCL layer.   \nHelper Functions:   \nmnd - extracts graph\u2019s max node degree   \nexnb - extracts local graph neighborhoods   \nabsk - assigns angular bins to sub-kernels   \ncrad - computes relative angular distances   \nansk - assigns neighborhood nodes to sub-kernels   \ninai - determines if neighborhood assignment is injective   \n$M\\leftarrow\\mathrm{mnd}(G)$   \nif $M\\geq u b$ then return ub   \nelse $N\\gets\\mathrm{exnb}(E)$ for $i=M$ until $u b$ do $k\\gets i$ \u25b7k - number of sub-kernels q \u2190360 b \u2190abksk(k, q) for $n\\in N$ do a \u2190crad(n) is_injective \u2190inai(ansk(a, b)) if not is_injective then break if is_injective then return $k$ return ub ", "page_idx": 14}, {"type": "text", "text": "We introduce Algorithm 2 to establish the minimum number of sub-kernels necessary to ensure satisficing mapping across any arbitrary static graph dataset. This algorithm runs once during the first training epoch to initialize the minimum number of sub-kernels necessary for the particular dataset. This cost occurs just once for the very first batch of training data; afterwards the mapping of sub-kernels to local neighborhood nodes can be cached so that subsequent batches and nodes can reuse it. The runtime complexity of the algorithm is $O(|V|^{2})$ , where $|V|$ represents the cardinality of the graph node set. This worst-case asymptotic complexity is observed in strongly connected graphs. The quadratic dependency on $|V|$ arises from the inner for loop, which iterates through each node within the graph (resulting in an $O(|V|)$ operation) and computes relative angular displacements for each local neighborhood (another $O(|V|)$ worst-case-complexity operation in the case of strongly connected graphs) to assign nodes to their respective sub-kernels. Caching incurs $O(|V|^{2})$ space cost in the case of strongly connected graphs where the same node in every neighborhood maps to a different subkernel. ", "page_idx": 14}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/da22d15ef289f18b83c5c17159c04ffdba6cfe03444825eefca17807052bedb2.jpg", "img_caption": ["(d) RethinkingCNN 2D convolutions as a linear combination of 1D convolutions "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 3: Visualization of natural ranking of nodes within local neighborhoods of image graph data via relative positional descriptors, which imposes a natural relative positional descriptor label on the convolving sub-kernels. ", "page_idx": 15}, {"type": "text", "text": "E Model Architectures ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/de4e7013025910aa316b1dca10104707e50939d8f1a70ab07d42ff9172b3bcbf.jpg", "img_caption": ["(c) CIFAR10 CNN architecture "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 4: CNN models trained for different standardized datasets: MNIST, FashionMNIST and CIFAR10. QGCN and SGCN architectures were exactly the same as the CNN architectures except with the convolutional layers replaced with QGCL and SGCN layers. ", "page_idx": 15}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/9726217190ea8957cad8f41348c0589767a7a8ada0efa6ed36e510633293e74d.jpg", "img_caption": ["Node Classification Task Architecture "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Neural GNN Architecture for Node Classification Task. All GNNs we tested had the same architecture as depicted. The generic architecture has 2 blocks, the first is a convolutional layer followed by batch normalization and then a ReLU activation function. The second block in the architecture sums up features from three identical blocks, each of which is a convolutional layer followed by a ReLU activation function. For any given GNN, it\u2019s message passing layer was substituted into the convolutional layer, depicted in the figure, to derive the overarching architecture. In doing so, we guaranteed an iso-architecture comparison. ", "page_idx": 16}, {"type": "text", "text": "F Standard Datasets: Full Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We trained CNN and QGCN models on sub-sampled splits of the standard image datasets described in the main paper. The splits chosen were: (train, test) $)=(100,\\,20)$ , (1000, 200) and (10000, 1000), with equal sampling across categories. These sub-sampled versions of the data allowed a greater variance in model performance, facilitating clearer comparisons across methods. Here, we compared training loss, and train/test accuracies across various learning rates for both CNN and QGCN. We train all models three times on different splits of the datasets. ", "page_idx": 16}, {"type": "text", "text": "Table 8 provides results collated across various train-test splits, in effort to capture model performance in different bias-variance regimes. ", "page_idx": 16}, {"type": "text", "text": "G Learning Rate Charts ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/7a47ab2408914b422a0b125a3ab8e9872a8659a9e30ac27daf6e377bd5108496.jpg", "img_caption": ["Figure 6: MNIST dataset results Figure for 100:20 dataset split "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/2f086a1610b8525e5288e93ebb00b0fe87c2974c9ef9d32a3b21ce41bf4ef0f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/eea01f728e0e5dc3530b3956b7cc18b572dbc158b51e0659fb19805cc297cd8c.jpg", "img_caption": ["Figure 7: MNIST dataset results Figure for 1000:200 dataset split "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/dcd30b28255eeff214ca3e7006c056ef815bda474aa60b79b456b3afedfa3d4b.jpg", "img_caption": ["Figure 8: MNIST dataset results Figure for 10000:1000 dataset split "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/f7d350a17a2cf3422ab188dd9b3dde6f21621a081e6281e7db4a8550acf38435.jpg", "img_caption": ["Figure 9: Fashion-MNIST dataset results Figure for 100:20 dataset split "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/31b34085c042c78d25ab92fd093391b0b68dd8778159e25464f79a57cd767818.jpg", "img_caption": ["Figure 10: Fashion-MNIST dataset results Figure for 1000:200 dataset split "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/48eb4b171bf7f276360a1e3a2b69671169a95885a4cf6617b2cbd6cd6749267a.jpg", "img_caption": ["Figure 11: Fashion-MNIST dataset results Figure for 10000:1000 dataset split "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/060ab1030fd141d2c8a7027c8444663f6699c3b4dcc889409ab6740baeb4614b.jpg", "img_caption": ["Figure 12: CIFAR-10 dataset results Figure for 100:20 dataset split "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/8692eed09e7e5048bd15a7882e2a6b82dbaa71860fa942f4a7508c507da43a05.jpg", "img_caption": ["Figure 13: CIFAR-10 dataset results Figure for 1000:200 dataset split "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/8b210ea3ec7cfe946cae2be9a9c54f4ae0cfc2c53be7d71d0acc452a0a018c4a.jpg", "img_caption": ["Figure 14: CIFAR-10 dataset results Figure for 10000:1000 dataset split "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "H Custom FEM Dataset ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/87abf4e8e00aedabbe4b7c38af6ec7583878448e878e0408756d9952f5ef7106.jpg", "img_caption": ["Figure 15: Underlying geometry and the flow around a circular cylinder. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "We introduce several simulated Navier-Stokes non-linear dynamics benchmark datasets for both binary and denary classification tasks on FEM graphs. We simulated the \u201cflow past a cylinder\" problem on an adaptive mesh with the underlying two-dimensional flow geometry depicted in Figure 15a. We assumed a fluid density of $\\rho=1.0$ and dynamics governed by the time-dependent Navier-Stokes Equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{t}-v\\Delta u+u\\Delta u+\\nabla p=0,\\nabla\\cdot u=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $u$ represents velocity and $p$ represents pressure. We adopted a kinematic velocity of $v=$ 0.001. For the lower and upper walls, as well as the boundary of the cylinder, no-slip boundary conditions were imposed. On the left edge, we prescribed a parabolic inflow profile as $u(0,y)\\stackrel{!}{=}$ 4Uy0(.04.4121\u2212y), 0 with a maximum velocity U = $\\begin{array}{r}{U=\\frac{3v R e}{2D}}\\end{array}$ . Here, $R e$ and $D$ denote the Reynolds number and the diameter of the cylinder, respectively. On the right edge, do-nothing boundary conditions define the outflow $\\begin{array}{r}{v\\frac{\\delta u}{\\delta n}-p n=0}\\end{array}$ with $n$ denoting the outer normal vector. We employed FEniCS library for solving the governing Navier-Stokes equations, using the adaptively refined mesh as shown in Figure 1b. for the spatial discretization in the finite element implementation. We conducted simulations to produce a dataset of flow velocities, covering a range of Reynolds numbers spanning values 20 to 120. At lower Reynolds numbers, the flow exhibits a stationary behavior. However, as the Reynolds number increases, a fascinating phenomenon known as Karman vortex shedding emerges. This phenomenon results in the flow adopting a time-periodic behavior, characterized by vortex shedding occurring behind the cylinder ([29, 34]). It\u2019s important to note that in our simulations, the primary flow direction is horizontal, emphasizing the significance of the $\\mathbf{X}$ -component of velocity. ", "page_idx": 19}, {"type": "text", "text": "We generated the graph dataset representing fluid velocity components in a standard PyTorch Geometric format. We explored two inductive learning tasks with the graph dataset: binary and denary classifications. For binary, we separated laminar and turbulent flows based on distinct $R e$ values, while for denary classification, we used evenly spaced $R e$ values across a range. We created three datasets, as illustrated in Table 9: Navier-Stokes-Binary (NS-Binary) for binary classification and Navier-Stokes-Denary-1 (NS-Denary-1) and Navier-Stokes-Denary-2 (NS-Denary-2) for denary classification, with NS-Denary-2 being more challenging. Training data for each $R e$ value is from the initial time period, and test data is from later time steps in the simulation, so models predict classes for future time points they have not seen. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Table 9: The table captures the different $R e$ ranges we considered for the different custom datasets and the step sizes. Notice in the binary case that $\\mathit{R e}\\mathrm{~=~}20\u201340$ are grouped in laminar class and 100-120 into the turbulent flow class. ", "page_idx": 20}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/9733bd0300b50f73587a8b271c341b4089a355eba83fda83562fadc9835dbee6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 10: Shown in the table are different custom dataset splits we have provided as part of this paper. The third column captures the training time period per $R e$ from which train data were aggregated from the FEM time series solutions ", "page_idx": 20}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/cfaf8cf105f37f2408c9a30a79258504f52b63c317065ece6ce568e89e39dea3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "I Benchmark Datasets for Graph Kernels ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The diverse set of Benchmark Data Sets for Graph Kernels we used for validating QGRN and other GNN methods are as introduced below: ", "page_idx": 21}, {"type": "text", "text": "AIDS. 2000 sample binary classification dataset of molecular compounds with classes active and inactive, representing activity against HIV. ", "page_idx": 21}, {"type": "text", "text": "COIL-DEL. 3900 sample graph dataset extracted from images of 100 different objects taken at different poses (with pose interval of 5 degrees) ", "page_idx": 21}, {"type": "text", "text": "Enzymes. 600 sample size enzymes dataset with 6 classes (each of which represents one of the 6 EC top-level classes) from the BRENDA enzyme database. ", "page_idx": 21}, {"type": "text", "text": "Letter (low/med/high). 2250 sample datasets of graphs representing distorted letter drawing with 15 classes, each corresponding to a different Roman alphabet letter. low, med and high represent different degrees of distortions applied to the hand-drawn letter graphs. ", "page_idx": 21}, {"type": "text", "text": "Mutagenicity. 4337 sample binary classification dataset of chemical compounds with classes mutagen and non-mutagen. ", "page_idx": 21}, {"type": "text", "text": "Proteins and Proteins-Full. 1113 sample size binary classification datasets for classifying protein graphs into enzymes or non-enzymes. ", "page_idx": 21}, {"type": "text", "text": "Proteins-Full differs in that it has many more node features: Protein $\\mathrel{\\mathop:}=3$ and Proteins-Full=31 ", "page_idx": 21}, {"type": "text", "text": "Frankenstein. A 4337 sample binary mutagenicity classification dataset modified from the BURSI dataset with classes, mutagen and non-mutagen. This dataset has continuous node features with very high dimensionality: 781 features per node. ", "page_idx": 21}, {"type": "text", "text": "Mutag. 188 sample binary classification dataset of chemical compounds and their mutagenic effect on bacterium ", "page_idx": 21}, {"type": "text", "text": "Synthie. A 400 sample Erd\u00f6s-R\u00e9nyi synthetic graphs dataset with 4 classes generated with probabilistic models. Figure 16 shows the distribution of the dataset samples across their different classes. ", "page_idx": 21}, {"type": "image", "img_path": "dYIqAZXQNV/tmp/65415875f4f6158add916b08f740f1b1c729dfa063462e0efd8479c9f36d38fd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 16: Distribution of samples by classes across all datasets used in this paper. We exclude COIL-DEL because it has 100 classes each with 39 samples and hence would render the plot illegible. ", "page_idx": 21}, {"type": "text", "text": "J Graph kernels benchmark datasets: additional results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Tables 11 and 12 contain additional results worth highlighting. ", "page_idx": 21}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/7b2bcaad6e162025be7bd717b4780ef22cf730fb1bd62e655c31f64eb2642134.jpg", "table_caption": ["Table 11: Standard datasets. CNN and QGCN model accuracies on standard datasets "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/c34a3814b89ced47f2d5dbcab53c376ed90fbc385b4dc6bf4f0481bfcb3db96b.jpg", "table_caption": ["Table 12: Graph kernels benchmark datasets - III. Test Accuracy $(\\%)$ across different GCNs "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "K Graph kernels benchmark datasets: model parameters and wall clocks ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/e7f081d02068e282aa8f7fc0c5a11dd742cdb4c42b1b6c4dc108673e4c464551.jpg", "table_caption": ["Table 13: Graph kernels benchmark datasets - I. Model sizes (number of parameters) "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/ad73babb29cdb3e896a31632a4cc8781f1f5ab5fba2364f69a7a324127456446.jpg", "table_caption": ["Table 14: Graph kernels benchmark datasets - I. Model sizes (number of parameters) "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/fee5ba06242b2f0260f7a2dffa8cfbe7a382e37c16be8a35cfd2b3cd56f91061.jpg", "table_caption": ["Table 15: Graph kernels benchmark datasets - I. Google TPU Inference latency. Wall clock (in ms) "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/a7868b8e9db69a09dec5ddf62a0762af884adfd62ed838138132aa6b01010004.jpg", "table_caption": ["Table 16: Graph kernels benchmark datasets - II. Google TPU Inference latency. Wall clock (in ms) "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/3006649fba043a70feea7599fc396b1a5b993fd2d809e986a772eb63947ed1fc.jpg", "table_caption": ["Table 17: Graph kernels benchmark datasets - III. Google TPU Inference latency. Wall clock (in ms) "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "L Graph Datasets: Node Classification ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Tables 18 and 19, representing homophilic and heterophilic dataset results respectively, provide the full set of datasets, against which we benchmarked, for node classification tasks. ", "page_idx": 24}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/73e065550903f8152c58bddcb88524bf44c053cc8278cae023e5e7e2d6ba9087.jpg", "table_caption": ["Table 18: Homophilic node classification datasets. Test Accuracy $(\\%)$ across different GCNs "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/42b7a793089f0d7faf48d24eccce6a0966b0d1e5e415ae9da03704ae2726c5ad.jpg", "table_caption": ["Table 19: Heterophilic node classification datasets. Test Accuracy $(\\%)$ across different GCNs "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "M Leader Board - Comparison with QGRNs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our search for state of the art performance was limited to Papers with Code, which has only a subset of the datasets we trained on. This, we believe, is indicative of the fact that in literature, a subset of these benchmarks are chosen for different types of downstream tasks. Our method mostly focused on inductive classification tasks. After searching thoroughly through Papers with Code, we found what we believe to be the full subset of data sets with comparable SOTA results below: ", "page_idx": 24}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/2f8154dbe102348fe6b755ed629fcd4ee186749ecd654ff3f63730c1f09f47c3.jpg", "table_caption": ["Table 20: Leader-board (Papers with Code). Comparison of QGRN performance to leading models "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "N IAM Graph Database - Comparison with QGRNs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The conference paper, \u201cIAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning\u201d ([30]), provides k-NN classifier-based results that the authors intended as \u201ca first reference system to compare other algorithms with\u201d. Clearly these do not represent SOTA results, but we include them as another baseline comparison available for the data sets considered in our paper: ", "page_idx": 24}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/3747c39f858b148c570024fbc700746504f0a37244a5183c3d24b36426f5c86d.jpg", "table_caption": ["Table 21: IAM Graph Database Repository. Comparison of QGRN performance to $\\boldsymbol{\\mathrm{k}}$ -NN classifier "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "O Training Deeper QGRNs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We present some preliminary results on training deeper QGRNs, in an effort to understand the model\u2019s ability to overcome GCNs inability to go deep. In this exploration, we trained different depths of the same QGRN model each on 2 different datasets. These datasets roughly capture the extremes of dataset difficulty in the paper. We trained on the AIDS dataset: a small binary classification 1600:400 train:test split dataset, and the Letters (high) dataset: a more complex dataset with 15 classes and train:test $=1725{:}525$ split. We trained these models with ADAM optimizer and cross entropy loss, under the same conditions as used elsewhere in the paper (see Sec. 4.3). ", "page_idx": 25}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/92c89eda90147cabacebb9aa52a8aee7d62adf796d55912efc42a711523c60d0.jpg", "table_caption": ["Table 22: Deeper QGCN and QGRN networks. Sample results illustrating impact of deeper network on model performance "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Discussion: From the results compiled in table 22, we do see that as the depth of the network increases there is a modest reduction in overall model performance. The results also reveal that the extent of model performance regression depends on the complexity of the dataset. The AIDS dataset is less sensitive to model depth: scaling the model depth by 6x (from 3 to 18) results in just a $0.5\\%$ loss in mean test accuracy. Letters (high), on the other hand, sees more performance loss at the same 6x depth, i.e., $5\\%$ loss in performance. Though part of the regression could be attributed to insufficient number of epochs for the deeper networks, but we speculate that it is largely due to our residual network retrofit in our QGRN layer not being sufficient to mitigate the effects of depth (e.g., the vanishing gradient problem). There are several exciting findings in the literature that have suggested methods such as customized aggregators instead of standard addition and softmax aggregation in the message passing network (layer level innovations). Others approach this problem architecturally, e.g., borrowing from successful architectures such as ResNets (as we did), using skip connections, pooling and sampling layers, drop-out inspired methods, and so on. We highlight that our paper\u2019s focus is primarily extending CNN\u2019s convolution operation to arbitrary graphs, which we demonstrate is the case through our equivalence analysis. In this context, we highlight that deep vanilla CNNs also face this same challenge of being difficult to train. In the world of CNNs, this problem is combated via architectural innovations such as batch normalization, use of better activations like ReLU, skip connections etc. In this light, our layer-level innovation with quantizable neighborhoods (replicating ", "page_idx": 25}, {"type": "text", "text": "CNN-like convolution) also stands to benefit from such architectural designs that allow for robust and deeper network architectures, which we leave to future work. ", "page_idx": 26}, {"type": "text", "text": "P Performance impact of quantization ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here, we explore a pilot comparison of satisficing (angular) mapping and QuantNet to understand how quantizing the neighborhoods uniformly vs learning the quantization affects model performance. We were able to compile together some sample results that provide some insight into this, which we share in Table 23. In Table 24, we considered the impact of using pseudo-positional descriptors (here, the entire node attributes) as opposed to explicit spatial descriptors as positional descriptors. ", "page_idx": 26}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/70d111255f9660cb55df6e0568d8a6e685dcd3703dcab20c0d6b8d421b2a364d.jpg", "table_caption": ["Table 23: Deeper satisficing mapping (SM) and QuantNet networks. Sample results illustrating impact of deeper network on model performance "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/470e6f592d15b37ea8f9d1e522a09228d16a99b34c3fe6551b82e4b856395984.jpg", "table_caption": ["Table 24: 3-layer QGRN model analysis. Sample results for QGRN model trained with and without positional descriptors. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Discussion: In Table 23, we examined 2 different datasets representing the extremes of classification difficulty: AIDS (a binary classification task with 2000 samples) and Letters-high (a dataset with 15 classes and 2250 total samples). We notice from Table 5 that in the easier inductive task, both uniform and learned quantization methods yield similar performance. Learned quantization outperforms uniform quantization in the harder learning task, Letters (high), where the ability to learn which bins activation input elements should be clustered, becomes beneficial. We also observe that the relative model performance improvement of $1\\!-\\!2\\%$ persists between the satisficing (angular) mapping and QuantNet, even with increasing model depth, adding more validation to quantization learning as a useful property. In Table 24, we focused only on QuantNet-based QGRN and trained the model on a sampling of datasets which had positional attributes. We prepared from the same datasets, variants with positional attributes and variants without positional attributes for the quantization learning. Intuitively, we\u2019d expect that with quantization learning from explicit positional descriptors, binning should be close to optimal, thereby leading to optimal model performance. This is what the experimental results show. Without explicit positional descriptors (here QGRN uses the entire node attributes, of which explicit positional attributes are inclusive), the model achieves near-optimal test accuracies. It is interesting to point out that the model performance of the variants without positional descriptors is upper bounded by the model performance on variants with positional descriptors. ", "page_idx": 26}, {"type": "text", "text": "Q Empirically determining quantization bins ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The number of bins is set by choosing the number of subkernels, which is user defined and hence a hyper-parameter of our model. As such, the number of bins can be tuned via any hyper-parameter optimization method, such as Grid search, Bayesian search etc. For image datasets, this choice is immediately obvious: the number of bins must be the total number of neighboring nodes in any given neighborhood (8 neighbors $+\\,1$ central node). For the uniform quantization case, we developed an algorithm (an algorithm pseudo-code, Algorithm D.1) to determine satisficing mapping across all neighborhoods. For QGRN with learnable quantization, the number of bins is a hyper-parameter. We present a sample hyper-parameter search for select graph datasets from the TUDatasets benchmark in Table 25. ", "page_idx": 27}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/7132ee5b2fe4fbefb4153fa7e6714ef0cad154b379bd21e9f26d85f3cf84cae0.jpg", "table_caption": ["Table 25: Number of bins - hyper-parameter search. Hyper-parameter search of optimal number of bins/subkernels for QGRN "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Discussion: Notice the classification test accuracy variations as we sweep the number of bins/subkernels. The trend is non-linear, mostly peaking between 3 - 7 bins/subkernels. This is partly explained by the fact that most datasets here have an average neighborhood size of 5 - 7, which provides a reasonable partitioning size for all neighborhoods; the analogy here is CNN\u2019s bin size of 9, which is a consequence of full regular neighborhoods having a size of 9 pixels. As the number of subkernels grow, each subkernel sees a smaller subset of the training data; additionally, bin data size distribution imbalance results in uneven training of the corresponding bin subkernels, which results in regression of the overall model\u2019s performance. A pathological case worth mentioning here would be defining an arbitrarily large number of bins (far greater than the dataset\u2019s average neighborhood size), leading no or very little training of subkernels: the weights that see any node features are bound to overfti on the small set of node features they see while those not seeing any node features will never get to train. During test time, this blend of over-fitted and under-fitted subkernel weights will result in large regression in the overall model\u2019s performance. On the other extreme, with a single learnable subkernel, all types of nodes which ideally may belong to different bins are collapsed into the same bin, making it hard for the single subkernel to learn common messages across the entire node set of the graph for message passing. This is an example of underfitting (high bias) scenario. The optimal bin size is intuitively expected to be around the average neighborhood size of the given dataset, which is what we see in the experimental results summarized in Table 25. ", "page_idx": 27}, {"type": "text", "text": "R Compute resources for experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "All experimental results provided in this paper were the results of runs on Google Colab premium offering of GPUs and TPUs. As much as specific details on these systems are publicly available, we have tabulated them below. The two main accelerator options we used were the A100 GPUs and TPUs. In table 26, we outline some of the system specifications. It is worth noting that the set of datasets we explored in the TUDataset benchmark were all moderately sized, hence Google Colab\u2019s $\\mathrm{Pro+}$ offering was more than sufficient for us. This was also the case for results collated on the standard dataset splits and our own custom Navier-Stokes FEW dataset. Not included in the table is Tesla V100, which at the time of finalizing this paper, Colab had deprecated support for it. ", "page_idx": 27}, {"type": "text", "text": "S Addressing Latency Concerns ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Regarding model training and inference speed, there is much room for future work to develop new algorithms for choosing subkernel masks that are faster than those we introduce here. We see the primary contribution of this paper as the introduction of the quantized convolutions theoretical framework, which we demonstrated with 2 options for subkernel selection: ", "page_idx": 27}, {"type": "table", "img_path": "dYIqAZXQNV/tmp/ad7663a65a106c218efcd2f8185f3497cf931cf5e5b03e5b7287f035b7d15e62.jpg", "table_caption": ["Table 26: Compute Resources. Google Colab $\\mathrm{Pro+}$ offering "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "1. 2D angular-quantization (satisficing mapping).   \n2. Flexible learnable-quantization (QuantNet). ", "page_idx": 28}, {"type": "text", "text": "There are many possible algorithms for choosing sub-kernel masks within this framework, and we suspect that future research into this area could be very fruitful. For example, some practical ways satisficing mapping & QuantNet might be sped up include: ", "page_idx": 28}, {"type": "text", "text": "1. Separating out tensor operations in the message preparation, propagation and update stages of the message-passing neural networks (MPNN) and leveraging the operator fusion capability of Torch JIT Script to optimize these operation sets.   \n2. Parallelizing the execution of the sub-kernel convolutions, with dedicated low level CUDA kernels, instead of using grouped convolutions (as we do in our current implementation).   \n3. Using depth-wise separable convolutions: this will reduce the model complexity (in terms of number of parameters, hence resulting in a proportional reduction in model runtime complexity. Depth-wise separable convolutions trade off model flexibility for model size. This means this optimization would need to be carried out carefully to ensure that QGCN/QGRN doesn\u2019t regress significantly in performance. It is worth noting that CNNs also have been sped up in this way for edge platforms. ", "page_idx": 28}, {"type": "text", "text": "The above listed are by no means exhaustive but we believe these will be good starting points for optimizing QGCN/QGRN to make them competitive with existing highly optimized MPNNs in the literature. We do hope that further research into QGCN/QGRNs will allow them to eventually be useful for numerous applications (as happened historically for CNNs over time). ", "page_idx": 28}, {"type": "text", "text": "T Impact statement ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As the paper\u2019s primary innovation is a learning model that can be trained to perform different tasks on graphs, it by itself doesn\u2019t pose any obvious risks. The potential for negative societal impact depends on the dataset and downstream learning tasks a user of this model designs for. We strongly recommend for such a model to be used in compliance with all ethical standards appropriate to the domain in which it is targeted to be deployed. We accentuate the need to ensure that datasets on which this model is trained reflect not only the status quo but take into account broader social, political, economical and religious contexts so that they mitigate potential harms from any potential bias or misuse of the model\u2019s learning abilities. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Claim 1: QGCNs contain a CNN-equivalent convolution layer, termed QGCL, as a subset. We show this in Section 3.1. Claim 2: QGCNs using the the satisficing mapping strategy are empirically and formally equivalent to 2D CNNs applied to image data. We introduce the satisficing mapping as a masking approach that matches the natural convolutional masks in Section 3.2; we show that performance is empirically equivalent between QGCNs and CNNs in Table 1 and Figures 6, 7, 8, 9, 10, 11, 12, 13, 14. Claim 3: We present an end-to-end learnable quantization network that extends QGCNs to arbitrary graphs. See Section 3.3. Claim 4: A residual network inspired architecture (QGRNs) that further improve QGCN performance. See Section 3.4. Claim 5: Benchmarking of QGRNs on a novel Navier-Stokes FEM dataset and 19 other public benchmark graph datasets. See Section 4. Claim 6: Showing QGRLs improve joint modeling of emotional states and EEG data in a supervised autoencoder architecture. See Section 4.5. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Limitation are mentioned briefly throughout the paper and discussed in detail in Section 5. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Sections 3.1, 3.2, and C. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide all code and data necessary to reproduce every experimental result that we describe in this paper. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide all experimental code and data. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide sufficient details to make sense of the results in the core paper.   \nFull details are provided in the available code. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We report for all relevant results (both in the main paper and the appendix) the mean and standard deviations, wherever applicable. The error bars shown on the learning rate plots in Figures 6, 7, 9, 10 and 12 are all also standard deviation. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All experimental results reported were generated on Google Colab\u2019s offering of GPUs and TPUs. As much as specific details on these systems are publicly available, we have included them in the paper, here in Appendix section R. ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: To the best of our knowledge, this paper fully conforms with the Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We address impacts in Section T. ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not believe this paper poses any such risks. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All datasets, other than our custom datasets, have been cited properly. The various GNN models we compared to our QGRN models have also all been cited properly, including SGCN (which was cloned from the original paper\u2019s GitHub repository). For SGCN, the cloned repository includes the copyright and permission notices, as defined within the MIT License. Other than these datasets and models, no other assets were used in the project. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: New assets (code, datasets: Navier Stokes FEM datasets) are well documented. Appendix section H details how the FEM dataset was constructed. Additionally, the GitHub repository contains relevant links for where to download the datasets. All the datasets used in this paper were packaged into a common dictionary object that our experimentation infrastructure could use. The details are all provided in the README page of the GitHub repository. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: This paper did not involve crowdsourcing or research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: This paper did not involve crowdsourcing or research with human subjects. ", "page_idx": 31}]