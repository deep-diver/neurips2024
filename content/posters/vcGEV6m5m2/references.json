{"references": [{"fullname_first_author": "Ben Mildenhall", "paper_title": "NeRF: Representing scenes as neural radiance fields for view synthesis", "publication_date": "2020-00-00", "reason": "This paper introduced NeRFs, a foundational technique for novel view synthesis that many subsequent methods, including the one described in this paper, build upon."}, {"fullname_first_author": "Ben Kerbl", "paper_title": "3D Gaussian splatting for real-time radiance field rendering", "publication_date": "2023-07-00", "reason": "This paper introduced 3D Gaussian splatting, the core rendering technique used in this work, enabling fast and high-quality image generation."}, {"fullname_first_author": "Ayush Tewari", "paper_title": "Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video", "publication_date": "2021-00-00", "reason": "This paper addressed a key challenge in novel view synthesis by handling non-rigid motion in dynamic scenes, which is also tackled in this paper."}, {"fullname_first_author": "Andrea Tagliasacchi", "paper_title": "Template-free articulated neural point clouds for reposable view synthesis", "publication_date": "2023-00-00", "reason": "This paper is a closely related work that also tackles template-free articulated object modeling from videos, allowing for comparison and analysis of this paper's contributions."}, {"fullname_first_author": "Alexey Noguchi", "paper_title": "Watch it move: Unsupervised discovery of 3D joints for re-posing of articulated objects", "publication_date": "2022-00-00", "reason": "This paper is another closely related work that focuses on template-free discovery of 3D joints for re-posing, providing a baseline and approach for comparison against this paper's method."}]}