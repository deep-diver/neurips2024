[{"type": "text", "text": "$C^{2}M^{3}$ : Cycle-Consistent Multi-Model Merging ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Donato Crisostomi Marco Fumero Sapienza University of Rome Institute of Science and Technology Austria crisostomi@di.uniroma1.it fumero@di.uniroma1.it ", "page_idx": 0}, {"type": "text", "text": "Daniele Baieri Florian Bernard Emanuele Rodol\\`a Sapienza University of Rome University of Bonn Sapienza University of Rome baieri@di.uniroma1.it fb@uni-bonn.de rodola@di.uniroma1.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we present a novel data-free method for merging neural networks in weight space. Differently from most existing works, our method optimizes for the permutations of network neurons globally across all layers. This allows us to enforce cycle consistency of the permutations when merging $n\\geq3$ models, allowing circular compositions of permutations to be computed without accumulating error along the path. We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging sets of models in scenarios spanning varying architectures and datasets. We finally show that, when coupled with activation renormalization, our approach yields the best results in the task. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the early days of deep learning, modes \u2014 parameters corresponding to local minima of the loss landscape \u2014 were considered to be isolated. Being depicted as points at the bottom of convex valleys, they were thought to be separated by high-energy barriers that made the transition between them impossible. However, a series of recent works have gradually challenged this perspective, first showing that modes can be actually connected by paths of low energy [10, 14], and later that, in some cases, these paths may even be linear [13]. While linear paths in [13] could only be obtained after training the equally-initialized models for a few epochs, follow-up work [11] speculated that the isolation of modes is a result of the permutation symmetries of the neurons. In fact, given a layer $W_{\\ell}$ of a fixed network $A$ , a large number of functionally-equivalent networks can be obtained by permuting the neurons of $W_{\\ell}$ by some permutation $P$ and then anti-permuting the columns of the subsequent layer $W_{\\ell+1}$ . This intuition led to the conjecture that all modes lie in the same convex region of the parameter space, denoted as basin, when taking into account all possible permutations of the neurons of a network.This motivated a series of works trying to align different modes by optimizing for the neuron permutations [1, 21, 29, 36]. This has strong implications for model merging, where different models, possibly trained with different initializations [1, 29, 34] or on different datasets and tasks [1, 36], are aggregated into a single one. In this work, we focus on the data-free setting, aligning networks based on some similarity function that is computed directly over the neurons themselves. To this end, we follow Ainsworth et al. [1] and formalize the problem of model merging as an assignment problem, proposing a new algorithm that is competitive with previous approaches while allowing global constraints to be enforced. ", "page_idx": 0}, {"type": "text", "text": "The problem We investigate the problem of merging $n>2$ models, noting that existing pairwise approaches such as [1] do not guarantee cycle consistency of the permutations (see Figure 1). As shown in Figure 2b and Figure 2a, going from a model $A$ to a model $C$ through a model $B$ , and then mapping back to $A$ , results in a different model than the starting one \u2014 specifically, the target ", "page_idx": 0}, {"type": "image", "img_path": "iD18l6prA7/tmp/912f1950d9bad1429fd0a16116cac3cca94b3189e9d965ffafc36513675dc906.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Cycle-Consistent Multi-Model Merging over three models $A,B,C$ . Left: existing methods seek pairwise permutations that map between models; note that $P^{A C}\\stackrel{\\cdot}{\\circ}{P^{C B}}\\circ P^{B A}\\not=I$ in general, unless this is explicitly enforced. Right: our method computes permutations $P^{A}$ , $P^{B}$ , $\\overline{{P^{C}}}$ from each model to a universe $U$ , such that a pairwise permutation $P^{B A}$ mapping $A$ to $B$ can be obtained as $P^{B A}={\\cal P}^{B}({\\cal P}^{A})^{\\top}$ . This way, cycle-consistency is enforced by design and $P^{A C}\\circ P^{C B}\\circ P^{B A}=I$ . ", "page_idx": 1}, {"type": "text", "text": "model ends up in a completely different basin. More formally, for these methods, the composition of permutations along any cycle does not result in the identity map. This also holds for the $n=2$ case, where the permutations optimized to align model $A$ to model $B$ are not guaranteed to be the inverse of those mapping $B$ to $A$ ; this makes the alignment pipeline brittle, as it depends on an arbitrary choice of a mapping direction. ", "page_idx": 1}, {"type": "text", "text": "Contribution To address this issue, we introduce a novel alignment algorithm that works for the general case with $n\\geq2$ models, while guaranteeing cycle consistency. The key idea is to factorize each permutation mapping $B$ to $A$ as $P^{A B}=P^{A}(P^{B})^{\\top}$ , where $\\left(P^{B}\\right)^{\\top}$ maps $B$ to a common space denoted as universe, and $P^{A}$ maps from the universe back to $A$ . This formulation ensures cycle consistency by design, as any cyclic composition of such permutations equals the identity. ", "page_idx": 1}, {"type": "text", "text": "Our numerical implementation is based on the Frank-Wolfe algorithm [12], and optimizes for the permutations of all the layers simultaneously at each step, naturally taking into account the inter-layer dependencies in the process. This desirable property is in contrast with other approaches such as Ainsworth et al. [1], which seek the optimal permutations for each layer separately, and thus can not ensure coherence across the entire network. ", "page_idx": 1}, {"type": "text", "text": "We run an extensive comparison of our approach with existing ones both in the standard pairwise setting and in merging $n>2$ models, spanning a broad set of architectures and datasets. We then quantitatively measure the influence of architectural width, confirming the existing empirical evidence on its role in linear mode connectivity. Further, we assess how the performance of the merged model depends on the number of models to aggregate, and show that the decay is graceful. We finally analyze the basins defined by the models when mapped onto the universe, and investigate when and to what extent these are linearly connected. ", "page_idx": 1}, {"type": "text", "text": "Wrapping up, our contributions are four-fold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a new data-free weight matching algorithm based on the Frank-Wolfe algorithm [12] that globally optimizes for the permutations of all the layers simultaneously;   \n\u2022 We generalize it to the case of $n\\geq2$ models, enforcing guaranteed cycle-consistency of the permutations by employing a universal space as a bridge;   \n\u2022 We leverage the multi-model matching procedure for model merging, using the universal space as aggregation point;   \n\u2022 We conduct an extensive analysis showing how the merge is affected by the number of models, their width and architecture, as well as quantitatively measuring the linear mode connectivity in the universe basin. ", "page_idx": 1}, {"type": "text", "text": "Finally, to foster reproducible research in the field, we release a modular and reusable codebase containing implementations of our approach and the considered baselines.1 ", "page_idx": 1}, {"type": "image", "img_path": "iD18l6prA7/tmp/3b560f2de565f401a9642ba5ea6bda91f2fa33bccf4f4af473afcd822aa060ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "", "img_caption": ["(a) Loss and accuracy curves for a model $A$ and the (b) Accumulated error obtained when cyclically model mapped back after a cyclic permutation. Mod- permuting models $A,\\ B$ and $C$ as in Figure 1. els cyclically permuted with Git Re-Basin end up $P_{A\\rightarrow B\\rightarrow C\\rightarrow A}$ refers to the composition $P_{A C}\\circ P_{C B}\\circ$ in a different basin than the one they started from. $P_{B A}$ and $d(\\cdot)$ is the $\\ell_{2}$ loss. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Existing methods accumulate error when cyclically mapping a model through a series of permutations, while $C^{2}M^{3}$ correctly maps the model back to the starting point. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Mode connectivity As introduced in Section 1, mode connectivity studies the geometry of the loss landscape with a particular interest on the regions corresponding to local minima. Following Frankle et al. [13], we assess the connectivity for two given modes by computing their loss barrier: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. (Loss barrier) Given two points $\\Theta_{A},\\Theta_{B}$ and a loss function $\\mathcal{L}$ such that $\\mathcal{L}\\left(\\Theta_{A}\\right)\\approx$ $\\mathcal{L}\\left(\\Theta_{B}\\right)$ , the loss barrier is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\in\\left[0,1\\right]}\\mathcal{L}\\left((1-\\lambda)\\Theta_{A}+\\lambda\\Theta_{B}\\right)-\\frac{1}{2}\\left(\\mathcal{L}\\left(\\Theta_{A}\\right)+\\mathcal{L}\\left(\\Theta_{B}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, this quantity measures the extent of the loss increase when linearly moving from the basin of a mode to the other. When two modes share the same basin, the loss does not increase at all and results in a barrier close to zero. ", "page_idx": 2}, {"type": "text", "text": "Weight-space symmetries Following the rich line of works on mode connectivity and model merging [1, 11, 13, 29, 36], we start from the essential insight of neuron permutation invariance in neural networks. Let us focus on the simple case of a Multi-Layer Perceptrons (MLP), where we can write the computation for an intermediate layer $W_{\\ell}\\in\\mathbb{R}^{d_{\\ell+1}\\breve{\\times}d_{\\ell}}$ as $z_{\\ell+1}=\\sigma\\left(W_{\\ell}z_{\\ell}+b_{\\ell}\\right)$ , with $z_{\\ell}$ being the input at the $\\ell$ -layer and $\\sigma$ denoting an element-wise activation function. For the sake of a clear exposure, we consider the bias $b_{\\ell}=0$ in the following. If apply a permutation matrix $P\\in\\mathbb{P}$ to the rows of the $W_{\\ell}$ matrix (i.e. the neurons), we obtain $z_{\\ell+1}^{\\prime}=\\sigma\\left(P W_{\\ell}z_{\\ell}\\right)$ . Being an element-wise operator, $\\sigma$ commutes with $P$ and can be neglected wlog. Since $z_{\\ell+1}^{\\prime}\\neq z_{\\ell}$ when ${\\boldsymbol{P}}\\neq{\\boldsymbol{I}}$ , we can still nullify the effect of the permutation by anti-permuting the columns of the subsequent layer for the inverse permutation of $P$ , i.e. $P^{\\top}$ . In fact, ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{\\ell+2}^{\\prime}=W_{\\ell+1}P^{\\top}z_{\\ell+1}^{\\prime}=W_{\\ell+1}\\underbrace{P^{\\top}P}_{I}W_{\\ell}z_{\\ell}=z_{\\ell+2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "making pairs of models that only differ by a permutation of the neurons de facto functionally equivalent. Given the enormous number of such permutations, it stands to reason that the resulting weight-space symmetries act as a major factor in the isolation of modes. ", "page_idx": 2}, {"type": "text", "text": "Solving for the permutation Given the above considerations, Entezari et al. [11] speculated that all models end up in a single basin after having accounted for permutation symmetries. Assuming this to hold at least in practical cases, Ainsworth et al. [1] proposed a simple algorithm to find the permutations matching two models by maximizing a local version of the sum of bi-linear problems: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{\\{P_{\\ell}\\in\\mathbb{P}\\}}\\;\\sum_{\\ell=1}^{L}\\langle W_{\\ell}^{A},P_{\\ell}W_{\\ell}^{B}P_{\\ell-1}^{T}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $P_{0}:=I$ . Noting that Equation (1) is NP-hard, Ainsworth et al. [1] tackle this problem by considering one layer at a time, relaxing the bi-linear problems to a set of linear ones that can be efficiently solved with any Linear Assignment Problem (LAP) solver, e.g., the Hungarian algorithm. This layer-wise linearization of the objective function, however, corresponds to high variance in the results that depend on the random order of the layers during optimization. See Table 7 for an empirical evaluation confirming this issue. ", "page_idx": 3}, {"type": "text", "text": "Renormalizing the activations Notwithstanding the quality of the obtained matching, the loss barrier can still be high due to the mismatch in the statistics of the activations. In fact, REPAIR [21] empirically shows the presence of a decay in the variance of the activations after the interpolation. They further show that the loss can be drastically reduced by \u201crepairing\u201d the mean and variance of the activations, forcing the statistics of the merged network to interpolate those of the endpoint networks. We refer the reader to Appendix A.4 for an in-depth explanation. ", "page_idx": 3}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now propose a novel algorithm to tackle the weight matching problem, first introducing its formulation in the pairwise case and then generalizing it to match and merge a larger number $n$ of models in a cycle-consistent fashion. ", "page_idx": 3}, {"type": "text", "text": "Pairwise matching As we have seen, the NP-hardness of Equation (1) demands for a relaxation of the problem to be tackled. Differently from Ainsworth et al. [1], we opt to maintain the objective global with respect to the layers and instead iteratively optimize its linear approximation via the the Frank-Wolfe algorithm [12]. This procedure requires the computation of the gradient of Equation (1) with respect to each permutation $P_{i}$ , thus we have to account for two contributions for each $\\nabla_{P_{i}}$ , i.e., its gradient from permuting the rows of $W_{i}$ and the one from permuting the columns of $W_{i+1}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{P_{i}}f=\\underbrace{W_{i}^{A}P_{i-1}\\big(W_{i}^{B}\\big)^{\\top}}_{\\mathrm{from\\,permuting\\;rows}}+\\underbrace{\\big(W_{i+1}^{A}\\big)^{\\top}P_{i+1}W_{i+1}^{B}}_{\\mathrm{from\\,permuting\\;columns}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Frank-Wolfe algorithm then uses the gradient to iteratively update the solution by linearly interpolating between the current solution and the projected gradient. We refer to Lacoste-Julien [24] for theoretical guarantees of convergence. The full algorithm is reported in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "Generalization to $n$ models In order to generalize to $n$ models, we jointly consider all pairwise problems ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{P_{i}^{p q}\\in\\mathbb{P}}\\;\\sum_{p=1}^{n}\\sum_{q=1}^{n}\\sum_{i=1}^{L}\\langle W_{i}^{p},P_{i}^{p q}W_{i}^{q}(P_{i-1}^{p q})^{\\top}\\rangle,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the superscript $p q$ indicates that the permutations maps model $q$ to model $p$ , with $P_{0}^{p q}:=I$ . In order to ensure cycle consistency by construction we replace the quadratic polynomial by a fourthorder polynomial. Dropping the layer subscript for the sake of clear exposure, we replace the pairwise matchings $P^{p q}$ in the objective of Equation (3) by factorizing the permutations into object-to-universe matchings $P^{p q}=P^{p}\\circ\\left(P^{q}\\right)^{\\intercal}$ so that each model $q$ can be mapped back and forth to a common universe $u$ with a permutation and its transpose, allowing to map model $q$ to model $p$ by composition of $\\left(P^{q}\\right)^{\\top}\\left(q\\rightarrow u\\right)$ and $P^{p}$ $\\left[u\\rightarrow p\\right]$ ). This way, the objective of Equation (3) becomes ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{p\\neq q}^{n}\\sum_{i=1}^{L}\\langle W_{i}^{p},P_{i}^{p}(P_{i}^{q})^{\\top}W_{i}^{q}(P_{i-1}^{p}(P_{i-1}^{q})^{\\top})^{\\top}\\rangle=\\sum_{p\\neq q}^{n}\\sum_{i=1}^{L}\\langle(P_{i}^{p})^{\\top}W_{i}^{p}P_{i-1}^{p},(P_{i}^{q})^{\\top}W_{i}^{q}P_{i-1}^{q}\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As stated by Theorem 3.1, the permutations we obtain using Equation (4) are cycle consistent. We refer the reader to Bernard et al. [5] for the proof and a complete discussion of the subject. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Given a set of n models $p_{0},\\ldots,p_{n}$ and object-to-universe permutations $P_{i}^{p_{j}}$ computed via Equation (4), the pairwise correspondences defined by $P_{i}^{p_{l}p_{j}}=P_{i}^{p_{l}}\\circ\\left(P_{i}^{p_{j}}\\right)^{T}$ are cycle-consistent, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{i}^{p_{1}p_{j}}\\circ\\dots\\circ P_{i}^{p_{3}p_{2}}\\circ P_{i}^{p_{2}p_{1}}=I\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all layer indices i, $2\\le j\\le n$ . ", "page_idx": 3}, {"type": "text", "text": "Similarly to the pairwise case, the approach requires computing the gradients for the linearization. This time, however, each $\\nabla_{P_{i}^{A}}f$ has four different contributions: one from permuting the rows of its corresponding layer, one from anti-permuting the columns of the subsequent layer, and two other contributions that arise from the symmetric case where $A$ becomes $B$ . In detail, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{P_{\\ell}^{A}}=\\nabla_{P_{\\ell}^{A}}^{\\mathrm{rows}}+\\nabla_{P_{\\ell}^{A}}^{\\mathrm{cols}}+\\nabla_{P_{\\ell}^{A}}^{\\mathrm{rows,\\ominus}}+\\nabla_{P_{\\ell}^{A}}^{\\mathrm{cols,\\ominus}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{P_{\\ell}^{A}}^{\\mathrm{rows}}=W_{\\ell}^{A}P_{\\ell-1}^{A}(P_{\\ell-1}^{B})^{\\top}(W_{\\ell}^{B})^{\\top}P_{\\ell}^{B}}&{}&{\\nabla_{P_{\\ell}^{A}}^{\\mathrm{cols}}=(W_{\\ell+1}^{A})^{\\top}P_{\\ell+1}^{A}\\,(P_{\\ell+1}^{B})^{\\top}\\,W_{\\ell+1}^{B}\\,P_{\\ell}^{B}}\\\\ {\\nabla_{P_{\\ell}^{A}}^{\\mathrm{rows,\\xi}}=W_{\\ell}^{B}P_{\\ell-1}^{B}(P_{\\ell-1}^{A})^{\\top}(W_{\\ell}^{A})^{\\top}P_{\\ell}^{A}}&{}&{\\nabla_{P_{\\ell}^{A}}^{\\mathrm{cols,\\xi}}=(W_{\\ell+1}^{B})^{\\top}P_{\\ell+1}^{B}\\,(P_{\\ell+1}^{A})^{\\top}\\,W_{\\ell+1}^{A}\\,P_{\\ell}^{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "See Algorithm 1 for a complete description of the procedure. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Frank-Wolfe for $n$ -Model Matching ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Require: Weights of $n$ models $M_{i=1}^{N}$   \ntolerance $\\epsilon>0$   \nEnsure: Approximate solution to Equation (4)   \n1: $\\mathbf{P}^{k}\\leftarrow$ identity matrices   \n2: repeat   \n3: for $(p,q)\\in[1,\\ldots,n]\\times[1,\\ldots,n]$ do   \n4: for $i=1$ to $L$ do   \n5: p,k , $P_{i-1}^{p,k}\\gets$ permutations over rows and columns of $\\boldsymbol{W}_{i}^{p}$ respectively   \n6: $P_{i}^{q,k}$ $P_{i-1}^{q,k}\\gets$ permutation over rows and columns of $\\boldsymbol{W}_{i}^{q}$ respectively   \n7: $\\nabla_{P_{i}^{p,k}}f\\pm(W_{\\ell+1}^{p})^{\\top}P_{\\ell+1}^{p}\\,(P_{\\ell+1}^{q})^{\\top}\\,W_{\\ell+1}^{q}\\,P_{\\ell}^{q}$   \n8: $\\begin{array}{r}{\\nabla_{P_{i-1}^{p,k}}f\\pm(W_{\\ell+1}^{p})^{\\top}P_{\\ell+1}^{p}\\,(P_{\\ell+1}^{q})^{\\top}\\,W_{\\ell+1}^{q}\\,P_{\\ell}^{q}}\\end{array}$   \n9: end for   \n10: end for   \n11: for $P_{i}^{k}\\in\\mathbf{P}^{k}$ do   \n12: $\\Pi_{i}\\leftarrow\\mathrm{LAP}(\\nabla_{P_{i}^{K}}f)$   \n13: end for   \n14: $\\alpha\\leftarrow\\operatorname{line\\search}(f,\\mathbf{P}^{k},\\Pi)$   \n15: for $P_{i}^{k}\\in\\mathbf{P}^{k}$ do   \n16: $P_{i}^{k+1}=(1-\\alpha)P_{i}^{k}+\\alpha\\;\\Pi_{i}$   \n17: end for   \n18: until $\\|f(A,B,\\mathbf{P}^{k+1})-f(A,B,\\mathbf{P}^{k})\\|<\\epsilon$   \n19: return $\\mathbf{P}^{k}$ ", "page_idx": 4}, {"type": "text", "text": "Merging in the universe space Looking at the loss landscape resulting from interpolating models in Figure 3, we see that the loss curves are much lower when the models are interpolated in the universe space. In fact, the originally disconnected modes end up in the same basin when mapped onto the universe, making it suitable to average the models. Therefore, our merging method aggregates the models by taking the mean of the weights in the universe space, as detailed in Algorithm 2. ", "page_idx": 4}, {"type": "text", "text": "Require: $N$ models $A_{1},\\dotsc,A_{N}$ with $L$ layers   \nEnsure: merged model $M$   \n1: $\\{P_{1},\\dotsc,\\stackrel{\\subset}{P}_{N}\\}\\leftarrow\\mathrm{Frank-Wolfe}(M_{1},\\dotsc,M_{N})$   \n2: for $i=1$ to $N$ do   \n3: $M_{i}^{\\mathrm{uni}}\\gets$ map to universe $(A_{i},P_{i})$   \n45:: $\\begin{array}{r}{\\overbrace{M^{\\mathrm{uni}}}^{\\substack{}}\\leftarrow\\frac{1}{N}\\sum_{i=1}^{N}M_{i}^{\\mathrm{uni}}}\\end{array}$   \n6: return M uni ", "page_idx": 4}, {"type": "image", "img_path": "iD18l6prA7/tmp/b0670e7f1b23e10ab91c395608ca1962d42f1b03065f7e0f0905a66f5357db5b.jpg", "img_caption": ["Figure 3: 2D projection of the loss landscape when matching three modes $\\Theta_{A},\\Theta_{B},\\Theta_{C}$ ; the models $\\pi(\\bar{\\Theta}_{A}),\\pi(\\bar{\\Theta_{B}}\\bar{)},\\bar{\\pi}(\\Theta_{C})$ are their resulting images in the universe, and lie in the same basin. Red zones indicate low-loss regions (typically basins), while blue zones indicate high-loss ones. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now evaluate the quality of our proposed framework both in matching models and in the subsequent merging operation. Approaches suffixed with a $\\dagger$ indicate the application of REPAIR. ", "page_idx": 5}, {"type": "text", "text": "Matching and merging two models As described in Section 3, our formalization can readily be used to match $n=2$ models. In this case, the energy is given by Equation (1) and the permutations are not factorized. We compare the performance of our approach against the Git Re-Basin algorithm [1] and the naive baseline that aggregates the models by taking an unweighted mean on the original model weights without applying any permutation. In this setting, our method performs on par with the state-of-the-art. Differently from the latter, however, we do not depend on the random choice of layers, as the optimization is performed over all layers simultaneously. As presented in Figure 4, this results in Git Re-Basin exhibiting variations of up to $10\\%$ in accuracy depending on the optimization seed, while our method shows zero variance. We refer the reader to Appendix B.1 for a thorough evaluation of $C^{2}M^{3}$ over a set of different datasets and architectures. In summary, our approach is able to match two models with the same accuracy as the state-of-the-art, while being deterministic and independent of the random choice of layers. ", "page_idx": 5}, {"type": "image", "img_path": "iD18l6prA7/tmp/9a308166e67b029d07f6b6228c8d92e02ca876ee5dcfeac32e89bf27f5500c15.jpg", "img_caption": ["Figure 4: Accuracy of the interpolated model using Git Re-Basin [1] over different optimization seeds. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Matching and merging $n$ models We now evaluate $C^{2}M^{3}$ in matching and merging $n$ models. The matching is given by the factorized permutations obtained by Algorithm 1. We compare against two baselines: the simple approach of naively averaging the weights without any matching, and the MergeMany approach proposed by Ainsworth et al. [1]. The latter is reported in Appendix A for convenience. As reported in Table 1, $C^{2}M^{3}$ obtains far superior results in terms of accuracy and loss in all considered settings, with accuracy gains as high as $+20\\%$ . Moreover, our approach natively yields cycle-consistent permutations: Figure 2b shows that Git Re-Basin [1] accumulates significant error when computing the distance between the source model and the model obtained by applying a cyclic series of permutations, while our approach is able to perfectly recover the source model. This is further confirmed in Figure 2a, where we show the loss and accuracy curves when interpolating between a model $A$ and the model mapped back after a cyclic permutation. ", "page_idx": 5}, {"type": "table", "img_path": "iD18l6prA7/tmp/9279cb17417176c468d2fc0feab9a400cbf83570cdce391cd84b4a0aadf4fc02.jpg", "table_caption": [], "table_footnote": ["Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation. "], "page_idx": 6}, {"type": "text", "text": "Models cyclically permuted with Git Re-Basin end up in a different basin than the one they started from, while our cycle-consistent approach ensures that the target model is exactly the same as the source. Wrapping up, our approach matches and merges n models with a significant improvement in performance over the state-of-the-art, while ensuring cycle-consistent permutations. ", "page_idx": 6}, {"type": "text", "text": "Model similarity before and after mapping As we can see in Figure 5, the cosine similarity of the weights of the models is $3\\times$ higher after mapping the latter to the universe. This suggests that the initial quasi-orthogonality of models is at least partially due to neuron permutation symmetries. We also report in Appendix C.1.2 the similarity of the representations between pairs of models. Interestingly, the latter does not change before and after mapping to the universe, but only if we consider a similarity measure that is invariant to orthogonal transformations such as CKA [22]. When using a measure that does not enjoy this property, such as the Euclidean distance, the representations become much more similar in the universe space. In short, the models are $3\\times$ more similar in the universe space and the mapping affects the representations as an orthogonal transformation. ", "page_idx": 6}, {"type": "image", "img_path": "iD18l6prA7/tmp/3a1fbd4c120508de82e4c75c353440b0f35323cbc4dc2a6bb25e03a8b79f8872.jpg", "img_caption": ["Figure 5: Cosine similarity of the weights of 5 ResNet20 trained on CIFAR10 with $2\\times$ width. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "iD18l6prA7/tmp/f950e3fc477f820293df35e7a028e09b3663780cf71cef7ac8f46a4b2b78ae17.jpg", "img_caption": ["Figure 6: Interpolation curves of VGG models in the universe. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Effect of activation renormalization Our empirical evidence also points out the benefits of the REPAIR operation [21] that is performed after the merging. In fact, the detrimental effect of model averaging on the activation statistics [21] still applies when taking the mean of $n$ models instead of two. Our results clearly show the benefti of REPAIR, making it a key ingredient of our overall framework. Requiring meaningful interpolation endpoints to be effective, REPAIR has lower benefit when employed on the MergeMany algorithm of Ainsworth et al. [1]. In fact, iteratively taking means of different random model subsets and aligning the left-out models to the mean is a more complex process than interpolating between some endpoint models. By taking the mean of models in the universe space, we are instead effectively interpolating between endpoint models that can be used for the computation of the statistics in Equation (8). Figure 6 shows the benefti of using the repair operation on 5 VGG models trained on CIFAR10 mapped to the universe space. Specifically we fix one model \u201ca\u201d and we linearly interpolate in the universe space with respect to the other models, measuring accuracy before and after applying REPAIR. Other than boosting performance, we observe that the latter reduces the variance over interpolation paths, resulting in the interpolation curves of all the models overlapping. Overall, using the models in the universe as meaningful endpoints to gather activation statistics, our approach can fully leverage activation renormalization techniques such as REPAIR. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "iD18l6prA7/tmp/ae4da2b30874cbcebf17cfda88b28670ccf62d61f46b085d268e788735e77aaa.jpg", "img_caption": ["Figure 7: Accuracy and loss when increasing the number $n$ of models to match and merge. ", "(a) MLPs trained over MNIST. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "iD18l6prA7/tmp/a8b35fb4a09da3759252534c52cc8b9e6fa531e4726f7d11b89db84d00f2e47b.jpg", "img_caption": ["(b) ResNet20 models trained over CIFAR10. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Increasing $n$ In this experiment, we show how the merged model behaves when increasing the number of aggregated models. As we can see in Figure $^{7\\mathrm{a}}$ , increasing the number of MLPS up to 20 causes the performance to slightly deteriorate in a relative sense, but remaining stable in an absolute sense as it doesn\u2019t fall below $98\\%$ . More surprisingly, Figure 7b shows that for a ResNet20 architecture with $4\\times$ width the loss and accuracy are not monotonic, but rather they seem to slightly fluctuate. This may hint at the merging process being more influenced by the composition of the model set, than by its cardinality. Intuitively, a model that is difficult to match with the others will induce a harder optimization problem, possibly resulting in a worse merged model. We dive deeper in the effect of the composition of the set of models in Appendix C.2. In short, our approach is effective in merging a larger number of models, suggesting promise in federated settings. ", "page_idx": 7}, {"type": "text", "text": "Varying widths We now measure how architectural width affects model merging, taking into consideration ResNet20 architectures with width $W\\;\\in\\;\\{1,2,4,8,16\\}$ . As we can see in Figure 8, width greatly increases the performance of the merged model, reaching the zero-loss barrier first observed in [1] when $W=16$ . This is in line with the observations relating linear mode connectivity and network widths [11, 1], and confirms the intuition that the merging is only effective when modes can be linearly connected. ", "page_idx": 7}, {"type": "image", "img_path": "iD18l6prA7/tmp/2b069c1b0c7fdb3b28f64c0c22874f976645c10c41ee022113eee576402d6af2.jpg", "img_caption": ["Figure 9: Accuracy of the merged model when mapping towards one arbitrary model (a, b, c, d, e) versus using $C^{2}M^{3}$ and the universe space. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "iD18l6prA7/tmp/c13f573634aaef08d5002b2927e6390d581d77ad66a4abe88184ade56626ba84.jpg", "img_caption": ["Figure 8: Accuracy and loss when merging 3 ResNet20s trained over CIFAR10 with different widths. $\\dagger$ indicates models after applying REPAIR. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Alternative: fixing one model as universe Alternatively, one could achieve cycle consistency by using one of the source models as reference and learning pairwise maps towards this one. This, however, would require arbitrarily selecting one of the models, making the overall merging dependent on an arbitrary choice. To see why this matters, we merged 5 $\\mathtt{R e s N e t20-4x}$ by choosing one model as reference and aggregating the models in its basin. Figure 9 shows severe oscillations in the results, with one model reaching an accuracy as low as $65\\%$ , while our approach performs as the best possible reference. This approach, moreover, does not address multimodel merging, as it is intrinsically pairwise: in a multi-task setting, models optimally mapped to a reference basin would only be able to solve the task solved by the reference model. This would prevent merging to be used for models containing complementary information, such as knowledge fusion [19] ", "page_idx": 7}, {"type": "image", "img_path": "iD18l6prA7/tmp/673b29e5e215e7a0a6bbcc56a603fda4ed19e9f3948313f0b446f45b20dd5556.jpg", "img_caption": ["(a) 2D visualization of accuracy and loss of the models sampled from the pairwise interpolation lines. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "iD18l6prA7/tmp/0afd09ab2acd1c791bd5c1f377313d96949b962e03d03b88ed9977919ed52dc5.jpg", "img_caption": ["(b) 3D visualization of the loss of the models sampled from the pairwise interpolation lines. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 10: Linear mode connectivity before and after mapping to the universe for 3 ResNet20-2\u00d7 models trained over CIFAR10 according to Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "or multi-task merging [36]. In our setting, instead, the universe model must by design be a function of all the models and act as a midpoint, hence aggregating information from all the models. ", "page_idx": 8}, {"type": "text", "text": "Linear mode connectivity in the universe Figure 10 shows that the loss curves of models interpolated in the universe are much lower than those interpolated in the original space, suggesting that the models are more connected in the former. These results, together with the loss landscape observed in Figure 3, encourage merging the models in the universe space due to the lower loss barrier. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Mode Connectivity and model merging. Mode connectivity studies the weights defining local minima. Frankle et al. [13] studied linear mode connectivity of models that were trained for a few epochs from the same initialization and related it to the lottery ticket hypothesis. Without requiring the same initialization, Entezari et al. [11] speculated that all models share a single basin after having solved for the neuron permutations. Model merging aims at aggregating different models into a single one to inherit their capacities without incurring in the cost and burden of ensembling. In this regard, Singh and Jaggi [34] proposed an optimal-transport based weight-matching procedure, while Git Re-Basin [1] proposed three matching methods and the MergeMany procedure seen in Section 4. Subsequently, REPAIR [21] showed that a significant improvement in performance of the interpolated model may be obtained by renormalizing its activactions rather than changing matching algorithm. Differently from all these works, we consider merging $n$ models and propose a principled way to perform it with cycle-consistency guarantees. ", "page_idx": 8}, {"type": "text", "text": "Cycle consistency. Ensuring cycle consistency of pairwise maps is a recurring idea in the computer vision and pattern recognition literature. In the realm of deep learning, earlier studies addressing multi-graph matching achieved cycle consistency by synchronizing ex-post the predicted pairwise permutations [40, 41]. The alternative approach using an object-to-universe matching framework, which we adopt here, inherently ensures cycle consistency by construction, as demonstrated in [4, 16, 31]. To the best of our knowledge, none of the existing works tackles cycle-consistent alignment of neural models. We refer to Appendix A.1 for a more detailed list of related works. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we treated the problem of model matching and merging. We first introduced a novel weight matching procedure based on the Frank-Wolfe algorithm that optimizes for the permutation matrices of all layers jointly, and then generalized it to the case of $n$ models. Guaranteeing cycleconsistency, the latter poses a principled way to merge a set of models without requiring an arbitrary reference point. We then showed the approach to yield superior performance compared to existing ones in merging multiple models in a set of scenarios spanning different architectures and datasets. We believe the formalism to elegantly fit the requirement for the merging operation to unify the different models into a cohesive one, rather than mapping all of them to one of the models in the set. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the ERC grant no.802554 (SPECGEO), PRIN 2020 project no.2020TA3K9N (LEGO.AI), and PNRR MUR project PE0000013-FAIR. Marco Fumero is supported by the MSCA IST-Bridge fellowship which has received funding from the European Union\u2019s Horizon 2020 research and innovation program under the Marie Sk\u0142odowska-Curie grant agreement No 101034413. We thank Simone Scardapane for the helpful feedback on the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git Re-Basin: Merging models modulo permutation symmetries. In The Eleventh International Conference on Learning Representations, 2022.   \n[2] Federica Arrigoni and Andrea Fusiello. Synchronization problems in computer vision with closed-form solutions. International Journal of Computer Vision, 128, 01 2020. doi: 10.1007/ s11263-019-01240-x.   \n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[4] Florian Bernard, Johan Thunberg, Paul Swoboda, and Christian Theobalt. Hippi: Higherorder projected power iterations for scalable multi-matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.   \n[5] Florian Bernard, Daniel Cremers, and Johan Thunberg. Sparse quadratic optimisation over the stiefel manifold with application to permutation synchronisation. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar.org/CorpusID:238253392.   \n[6] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusma\u02dco, et al. Flower: A friendly federated learning research framework. arXiv preprint arXiv:2007.14390, 2020.   \n[7] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00b4e van Schaik. Emnist: an extension of mnist to handwritten letters, 2017.   \n[8] Luca Cosmo, Emanuele Rodol\\`a, Andrea Albarelli, Facundo M\u00b4emoli, and Daniel Cremers. Consistent partial matching of shape collections via sparse modeling. Computer Graphics Forum, 36(1):209\u2013221, 2017. doi: 10.1111/cgf.12796.   \n[9] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \n[10] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A Hamprecht. Essentially no barriers in neural network energy landscape. March 2018.   \n[11] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. October 2021.   \n[12] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3(1-2):95\u2013110, 1956. doi: https://doi.org/10.1002/nav.3800030109.   \n[13] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proc. of ICML, volume 119 of Proceedings of Machine Learning Research, pages 3259\u20133269. PMLR, 2020.   \n[14] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson. Loss surfaces, mode connectivity, and fast ensembling of DNNs. February 2018.   \n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.   \n[16] Qi-Xing Huang and Leonidas Guibas. Consistent shape maps via semidefinite programming. In Proceedings of the Eleventh Eurographics/ACMSIGGRAPH Symposium on Geometry Processing, SGP \u201913, page 177\u2013186, Goslar, DEU, 2013. Eurographics Association.   \n[17] Qixing Huang, Fan Wang, and Leonidas Guibas. Functional map networks for analyzing and exploring large shape collections. ACM Trans. Graph., 33(4), jul 2014. ISSN 0730-0301. doi: 10.1145/2601097.2601111. URL https://doi.org/10.1145/2601097.2601111.   \n[18] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448\u2013456. pmlr, 2015.   \n[19] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models. The Eleventh International Conference on Learning Representations (ICLR), December 2022.   \n[20] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models, 2023.   \n[21] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. REPAIR: REnormalizing permuted activations for interpolation repair. In The Eleventh International Conference on Learning Representations, January 2023.   \n[22] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. May 2019.   \n[23] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL https: //api.semanticscholar.org/CorpusID:18268744.   \n[24] Simon Lacoste-Julien. Convergence rate of frank-wolfe for non-convex objectives. arXiv preprint arXiv:1607.00345, 2016.   \n[25] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion: A survey, 2023.   \n[26] Ekdeep Singh Lubana, Eric J Bigelow, Robert P. Dick, David Krueger, and Hidenori Tanaka. Mechanistic mode connectivity. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 22965\u201323004. PMLR, 23\u201329 Jul 2023.   \n[27] Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym, and Haggai Maron. Equivariant deep weight space alignment, 2023.   \n[28] Deepti Pachauri, Risi Kondor, and Vikas Singh. Solving the multi-way matching problem by permutation synchronization. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/ 2013/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf.   \n[29] Fidel A Guerrero Pe\u02dcna, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20237\u201320246, 2023.   \n[30] Xingyu Qu and Samuel Horvath. Rethinking model re-basin and linear mode connectivity, 2024. URL https://arxiv.org/abs/2402.05966.   \n[31] Frank R. Schmidt, Eno T\u00a8oppe, Daniel Cremers, and Yuri Boykov. Intrinsic mean for semimetrical shape retrieval via graph cuts. In Fred A. Hamprecht, Christoph Schn\u00a8orr, and Bernd Ja\u00a8hne, editors, Pattern Recognition, pages 446\u2013455, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg. ISBN 978-3-540-74936-3.   \n[32] Xinchu Shi, Haibin Ling, Weiming Hu, Junliang Xing, and Yanning Zhang. Tensor power iteration for multi-graph matching. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5062\u20135070, 2016. doi: 10.1109/CVPR.2016.547.   \n[33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition, 2015.   \n[34] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. In Hugo Larochelle, Marc\u2019aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[35] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21:343\u2013348, 1967. URL https://api. semanticscholar.org/CorpusID:50329347.   \n[36] George Stoica, Daniel Bolya, Jakob Brandt Bjorner, Pratik Ramesh, Taylor Hearn, and Judy Hoffman. Zipit! merging models from different tasks without training. In The Twelfth International Conference on Learning Representations.   \n[37] Paul Swoboda, Dagmar Kainmu\u00a8ller, Ashkan Mokarian, Christian Theobalt, and Florian Bernard. A convex relaxation for multi-graph matching. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11148\u201311157, 2019. doi: 10.1109/CVPR.2019. 01141.   \n[38] Lanhui Wang and Amit Singer. Exact and stable recovery of rotations for robust synchronization. Information and Inference: A Journal of the IMA, 2(2):145\u2013193, 2013.   \n[39] Lirui Wang, Kaiqing Zhang, Allan Zhou, Max Simchowitz, and Russ Tedrake. Fleet policy learning via weight merging and an application to robotic tool-use, 2023.   \n[40] Runzhong Wang, Junchi Yan, and Xiaokang Yang. Neural graph matching network: Learning lawler\u2019s quadratic assignment problem with extension to hypergraph and multiple-graph matching. CoRR, abs/1911.11308, 2019.   \n[41] Runzhong Wang, Junchi Yan, and Xiaokang Yang. Graduated assignment for joint multi-graph matching and clustering with application to unsupervised graph matching network learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19908\u201319919. Curran Associates, Inc., 2020.   \n[42] Christopher Zach, Manfred Klopschitz, and Marc Pollefeys. Disambiguating visual relations using loop constraints. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1426\u20131433, 2010. doi: 10.1109/CVPR.2010.5539801.   \n[43] Allan Zhou, Kaien Yang, Kaylee Burns, Adriano Cardace, Yiding Jiang, Samuel Sokota, J. Zico Kolter, and Chelsea Finn. Permutation equivariant neural functionals, 2023.   \n[44] Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, and Wei Hu. Going beyond linear mode connectivity: The layerwise linear feature connectivity, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Introduction ", "page_idx": 12}, {"type": "text", "text": "2 Background 3 ", "page_idx": 12}, {"type": "text", "text": "3 Approach ", "page_idx": 12}, {"type": "text", "text": "Experiments 6 ", "page_idx": 12}, {"type": "text", "text": "5 Related work 9 ", "page_idx": 12}, {"type": "text", "text": "6 Conclusions 9 ", "page_idx": 12}, {"type": "text", "text": "Additional details 14 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Extended related work 14   \nA.2 Pairwise Frank-Wolfe Algorithm 14   \nA.3 MergeMany Algorithm 15   \nA.4 REPAIR 15   \nA.5 Convergence and efficiency 15   \nA.6 Architectural details . 16   \nA.7 Datasets, hyperparameters and hardware details 17   \nA.8 Proofs 18   \nB Additional experiments 19   \nB.1 Pair-wise model matching and merging 19   \nB.1.1 ResNet with BatchNorm 19   \nB.2 Initialization strategies 19   \nB.3 Variance of the results in Git Re-Basin 20   \nB.4 Large-scale matching: ResNet50s trained over ImageNet 20   \nB.5 Federated Learning 21   \nAdditional analysis 22   \nC.1 Similarity of models 22   \nC.1.1 Representation-level similarity 22   \nC.1.2 Weight-level similarity . 22   \nC.2 Merging different subsets . 22   \nD Discussion 24   \nD.1 On the cycle-consistency of $C^{2}M^{3}$ 24   \nD.2 Limitations 24   \nD.3 Societal impact and broader vision 24 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we report in-depth explanations and additional experimental details. In particular, Appendix A.1 extensively outlines the most related works, Appendix A.2 shows the Frank-Wolfe algorithm for the pairwise case, while Appendix A.3 describes the MergeMany procedure presented in [1] for merging multiple models. We also report the REPAIR method in Appendix A.4. Finally, we show how the matching algorithm empirically converges in Appendix A.5. ", "page_idx": 13}, {"type": "text", "text": "A.1 Extended related work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We report here a thorough review of works that are relevant to our research, providing a comprehensive understanding of the context of our work. ", "page_idx": 13}, {"type": "text", "text": "Linear mode connectivity Mode connectivity is interested in modes, i.e. model parameters at convergence. In this regard, Frankle et al. [13] first studied the connectivity of the parameters of models that were trained for a few epochs from the same initialization, while Garipov et al. [14] investigated whether these can be connected through a high-accuracy path without requiring the same initialization. Simultaneously, Draxler et al. [10] proposed an algorithm to find a Minimum Energy Path (MEP) between two modes of a neural network, showing that these paths are mostly flat in both the training and test landscapes. This implies that many minima actually live in a shared low-loss valley rather than in distinct basins. On a different perspective, Zhou et al. [43] proposed to study a class of neural functionals which are permutation-equivariant by design. Recent research proposes to study model behavior in the weight space beyond linear mode connectivity: Lubana et al. [26] show that different \u201cmechanisms\u201d in related models prevent simple paths of low loss in the weight space, while Zhou et al. [44] studied the linear connections between the linear features of each layer of differently trained models. ", "page_idx": 13}, {"type": "text", "text": "Model merging Model merging [1, 29, 34, 20, 39, 36] has seen a surge of interest in the last years as a mean to ensemble models without incurring in the added computational cost. One of the first works in this direction is Singh and Jaggi [34], who proposed an optimal-transport based weight-matching procedure. Later, Ainsworth et al. [1] proposed three matching methods, one of which being data-free. Closer to our global optimization, Pen\u02dca et al. [29] proposed a gradient-descent based procedure that iteratively updates soft permutation matrices maintaining their bistochasticity via a differentiable Sinkhorn routine. When the models to match have been trained on different tasks, Stoica et al. [36] introduce a more general \u201czip\u201d operation that accounts for features that may be task-specific and further allow obtaining multi-headed models. Most recently, Navon et al. [27] proposed aligning models in the embedding space of a deep weight-space architecture. Finally, weight merging proved useful for large language models [20] and robotics [39]. For a complete survey of mode connectivity and model merging, we refer the reader to [25]. ", "page_idx": 13}, {"type": "text", "text": "Cycle consistency Cycle consistency is a recurrent idea in computer vision and pattern recognition, where it appears under different names (e.g., \u201csynchronization\u201d, \u201cloop constraints\u201d, or \u201cmulti-way matching\u201d) depending on the task. In the area of multi-view 3D reconstruction, Zach et al. [42] were probably the first to make an explicit attempt at finding solutions meeting the cycle-consistency requirement, although without ensuring theoretical guarantees on the result. In geometry processing, Cosmo et al. [8] ensured cycle-consistent alignment of collections of 3D shapes using an $n$ -fold extension of the Gromov-Wasserstein distance with sparsity constraints. Overall, cycle consistency is a recurring idea in the computer vision [38, 42, 2] graph matching [28, 37, 32] and geometry processing literature [17, 8, 4]. ", "page_idx": 13}, {"type": "text", "text": "A.2 Pairwise Frank-Wolfe Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As introduced in Section 3, we optimize a layer-global objective by iteratively optimizing its linear approximation via the the Frank-Wolfe algorithm [12]. We compute the gradient of Equation (1) with respect to each permutation $P_{i}$ , as the sum of two contributions for each $\\nabla_{P_{i}}$ : one from permuting the rows of $W_{i}$ and another from permuting the columns of $W_{i+1}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{P_{i}}f=\\underbrace{W_{i}^{A}P_{i-1}\\big(W_{i}^{B}\\big)^{\\top}}_{\\mathrm{from\\,nermutin\\,o\\,rows}}+\\underbrace{\\big(W_{i+1}^{A}\\big)^{\\top}P_{i+1}W_{i+1}^{B}}_{\\mathrm{from\\,normutin\\,o\\,columne}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We report in Algorithm 3 the Frank-Wolfe algorithm for the pairwise case. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Frank-Wolfe for pairwise Weight Matching   \nRequire: Weights of two models $A$ and $B$ with $L$ layers, tolerance $\\epsilon>0$   \nEnsure: Approximate solution to Equation (1)   \n1: $\\mathbf{P}^{k}\\leftarrow$ identity matrices   \n2: repeat   \n3: for $i=1$ to $L$ do   \n4: $P_{i}^{k}\\gets$ permutation acting on rows of $W_{i}$   \n5: $P_{i-1}^{k}\\gets$ permutation acting on columns of $W_{i}$   \n6: $\\nabla_{P_{i}^{K}}f\\mathrel{+}=W_{i}^{A}P_{i-1}^{k}(W_{i}^{B})^{\\top}$   \n7: $\\nabla_{P_{i-1}^{k}}f\\mathrel{+}=(W_{i}^{A})^{\\top}P_{i}^{k}W_{i}^{B}$   \n8: end for   \n9: for $P_{i}^{k}\\in\\mathbf{P}^{k}$ do   \n10: $\\Pi_{i}\\overset{\\cdot}{\\leftarrow}\\mathrm{LAP}(\\nabla_{P_{i}^{K}}f)$   \n11: end for   \n12: $\\alpha\\gets\\mathrm{LINESEARCH}(f,\\mathbf{P}^{k},\\Pi)$   \n13: for $P_{i}^{k}\\in\\mathbf{P}^{k}$ do   \n14: $P_{i}^{k+1}=(1-\\alpha)P_{i}^{k}+\\alpha\\;\\Pi_{i}$   \n15: end for   \n16: until $\\|f(A,B,\\mathbf{P}^{k+1})-f(A,B,\\mathbf{P}^{k})\\|<\\epsilon$   \n17: return Pk ", "page_idx": 14}, {"type": "text", "text": "A.3 MergeMany Algorithm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 4 reports the MergeMany procedure originally proposed by Ainsworth et al. [1] for merging multiple models, mainly consisting in alternating matching and aggregation until convergence. In practice, at each iteration, the procedure picks a reference model at random and matches all the other models to it. Then, they are all aggregated by averaging the weights. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 4 MERGEMANY   \nRequire: Model weights $\\Theta_{1},\\hdots,\\Theta_{N}$   \nEnsure: A merged set of parameters $\\Tilde{\\Theta}$ .   \n1: repeat   \n2: for $i\\in$ RANDOMPERMUTATION $(1,\\ldots,N)$ do   \n3: $\\begin{array}{r}{\\Theta^{\\prime}\\leftarrow\\frac{1}{N-1}\\sum_{j\\in\\{1,\\dots,N\\}\\backslash\\{i\\}}\\Theta_{j}}\\end{array}$   \n4: $\\pi\\leftarrow$ PERMUTATIONCOORDINATEDESCENT $(\\Theta^{\\prime},\\Theta_{i})$ 5: $\\Theta_{i}\\leftarrow\\pi(\\Theta_{i})$   \n6: end for   \n7: until convergence   \n8: return $\\textstyle{\\frac{1}{N}}\\sum_{j=1}^{N}\\Theta_{j}$ ", "page_idx": 14}, {"type": "text", "text": "A.4 REPAIR ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Observing a decay in the variance of the activations of the aggregated model, Jordan et al. [21] proposed REPAIR, which renormalizes the activations of the merged model to match the statistics of the original models. In particular, given two endpoint models with activations $X_{1}$ and $X_{2}$ , the activations $X_{\\alpha}$ of the interpolated model are renormalized to have statistics: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[X_{\\alpha}\\right]=\\left(1-\\alpha\\right)\\cdot\\mathbb{E}\\left[X_{1}\\right]+\\alpha\\cdot\\mathbb{E}\\left[X_{2}\\right]}\\\\ &{\\mathrm{std}\\left(X_{\\alpha}\\right)=\\left(1-\\alpha\\right)\\cdot\\mathrm{std}\\left(X_{1}\\right)+\\alpha\\cdot\\mathrm{std}\\left(X_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.5 Convergence and efficiency ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We report here the convergence of our matching algorithm. In particular, Figure 11 shows the objective values during the optimization, exhibiting the expected monotonic increase, while Figure 12 shows the step sizes result of the line search at each iteration. Interestingly, Figure 12a shows that the step sizes are generally decreasing, but descend in an alternating manner. This is likely due to the fact that the permutations are obtained as consecutive interpolations, where even steps result in a soft permutation matrix that is the average of the current and next permutation matrix, while odd steps generally result in a hard permutation matrix with entries in [0, 1]. Figure 13 finally shows the intermediate permutation values during the optimization: at each step, the entries of the permutation matrix are the linear interpolation of the current solution and the projected gradient with factor $\\alpha$ given by the step size. The red values in the figure represent entries currently being updated, which are neither 1 (blue) or 0 (yellow). ", "page_idx": 15}, {"type": "image", "img_path": "iD18l6prA7/tmp/5031f0e8b519c40f12a165eb7d2bfe85484b9fe94dcefad14c5708b26822aa42.jpg", "img_caption": ["Figure 11: Objective values during the optimization. As guaranteed by the Frank-Wolfe algorithm, the objective value increases monotonically. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "iD18l6prA7/tmp/d7ee5b8cd247912dc4bb68db36de054414f15dcc8f5165135b9d95eab22d875d.jpg", "table_caption": ["We report in Appendix A.5 the wall-clock time when merging $n=2,3$ ResNet20 models having $1\\times$ , $2\\times$ , $4\\times$ , $8\\times$ and $16\\times$ width, together with their number of parameters. ", "Table 2: Wall-clock time for merging $n=3$ ResNet20 models with different widths. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "As can be inferred from the table, the scaling laws depend on the complexity of the resulting matching problem and cannot be predicted merely from the number of parameters, with a 4-fold increase in parameters resulting in no increase in runtime for the first three columns, a double increase in the second-last column and a 5-fold increase in the last. Compared to MergeMany, our approach enjoys a milder increase in running time when increasing the number of parameters. For simpler settings, however, MergeMany is significantly faster. Being the two approaches on the same order of magnitude and given the one-time nature of model merging, we believe this aspect to be of secondary importance, especially considering merging to be, in many cases, an alternative to training a model from scratch. ", "page_idx": 15}, {"type": "text", "text": "A.6 Architectural details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We report here the architectural details of all the architectures we have used in the experiments. ", "page_idx": 15}, {"type": "text", "text": "Multi-Layer Perceptrons We use a simple MLP mapping input to a 256-dimensional space followed by 3 hidden layers of 512, 512 and 256 units respectively, followed by an output layer mapping to the number of classes. We use ReLU activations for all layers except the output layer, where we use a log softmax activation. ", "page_idx": 15}, {"type": "text", "text": "ResNet We consider a ResNet20 [15] architecture composed by three ResNet block groups, each containing three residual blocks. The model starts with an initial convolutional layer followed by normalization and ReLU activation. It then passes through the three block groups with increasing channel sizes (determined by the widen factor) and varying strides, followed by global average pooling and a fully connected layer that outputs class logits. As normalization layers, we consider both the most commonly used BatchNorm [18] and, for the sake of comparing with Git Re-Basin, also LayerNorm [3]. The results in the main manuscript are all obtained with LayerNorm, while we report the results with BatchNorm in Appendix B.1.1. ", "page_idx": 15}, {"type": "image", "img_path": "iD18l6prA7/tmp/8b20593c8a5b540da9e5c725dc6080abbe2361c2ceaf3f61e60955ce817a3de0.jpg", "img_caption": ["(a) Step sizes for all iterations. (b) Step sizes for odd iterations. (c) Step sizes for even iterations. ", "Figure 12: Step sizes during the optimization. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "iD18l6prA7/tmp/5448d355e773eb49eea77a229da10a116906b667584732be5177cdc4d4ea58a1.jpg", "img_caption": ["Figure 13: First 6 steps of Algorithm 3 for one permutation matrix. At each step, the new solution is given by the linear interpolation of the current solution and the gradient of Equation (1). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "VGG We employ a VGG16 [33] architecture with LayerNorm [3] normalization layers. The model has the following convolutional layer dimensions, with \u201cM\u201d indicating the presence of a max-pooling layer ", "page_idx": 16}, {"type": "equation", "text": "$$\n64,64,M,128,128,M,256,256,256,M,512,512,512,M,512,512,512,M,\\ldots\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The convolutional layers are organized in 5 blocks, each containing 2 or 3 convolutional layers, followed by a max-pooling layer. The final classifier is composed of three fully connected layers with 512 hidden dimension and ReLU activations. ", "page_idx": 16}, {"type": "text", "text": "A.7 Datasets, hyperparameters and hardware details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We employ the most common datasets for image classification tasks: MNIST [9], CIFAR-10 [23], EMNIST [7] and CIFAR-100 [23], having 10, 10, 26 and 100 classes respectively. We use the standard train-test splits provided by torchvision for all datasets. ", "page_idx": 16}, {"type": "text", "text": "We use the same hyperparameters as Git Re-Basin where possible to ensure a fair comparison. In particular, we train most of our models with a batch size of 100 for 250 epochs, using SGD with momentum 0.9, a learning rate of 0.1, and a weight decay of $10^{-4}$ . We use a cosine annealing learning rate scheduler with a warm restart period of 10 epochs and a minimum learning rate of 0. We report each and every one of the hyperparameters used for each experiment, as well as all the trained models, in a WandB dashboard2. ", "page_idx": 17}, {"type": "text", "text": "All of the experiments were carried out using consumer hardware, in particular mostly on a 32GiB RAM machine with a 12th Gen Intel(R) Core(TM) i7-12700F processor and an Nvidia RTX 3090 GPU, except for some of the experiments that were carried on a 2080. Our modular and reusable codebase is based on PyTorch, leveraging PyTorch Lightning to ensure reproducible results and modularity and NN-Template3to easily bootstrap the project and enforce best practices. ", "page_idx": 17}, {"type": "text", "text": "A.8 Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem A.1. The gradient of the objective function ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{n-1}\\sum_{q=p+1}^{n}\\sum_{\\ell=1}^{L}\\langle(P_{\\ell}^{p})^{\\top}W_{\\ell}^{p}P_{\\ell-1}^{p},(P_{\\ell}^{q})^{\\top}W_{\\ell}^{q}P_{\\ell-1}^{q}\\rangle\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is Lipschitz continuous, implying our algorithm obtains a stationary point at a rate of $\\mathcal{O}(1/\\sqrt{t})\\;[24].$ ", "page_idx": 17}, {"type": "text", "text": "Proof. We recall that, for each layer permutation $P^{A}=\\{P_{1}^{A},P_{2}^{A},\\dots,P_{L}^{A}\\}$ of model $A$ , we can define the gradient of our objective function relatively to the model $B$ we are matching towards: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(P_{\\ell}^{A})=\\nabla_{P_{\\ell}^{A}}^{\\mathrm{rows}}+\\nabla_{P_{\\ell}^{A}}^{\\mathrm{cols}}+\\nabla_{P_{\\ell}^{A}}^{\\mathrm{rows,\\Longleftrightarrow}}+\\nabla_{P_{\\ell}^{A}}^{\\mathrm{cols,\\Longleftrightarrow}}=}\\\\ &{\\qquad\\quad\\left[W_{\\ell}^{A}P_{\\ell-1}^{A}(P_{\\ell-1}^{B})^{\\top}(W_{\\ell}^{B})^{\\top}+(W_{\\ell+1}^{A})^{\\top}P_{\\ell+1}^{A}(P_{\\ell+1}^{B})^{\\top}W_{\\ell+1}^{B}\\right]P_{\\ell}^{B}+}\\\\ &{\\qquad\\quad\\left[W_{\\ell}^{B}P_{\\ell-1}^{B}(P_{\\ell-1}^{A})^{\\top}(W_{\\ell}^{A})^{\\top}+(W_{\\ell+1}^{B})^{\\top}P_{\\ell+1}^{B}(P_{\\ell+1}^{A})^{\\top}W_{\\ell+1}^{A}\\right]P_{\\ell}^{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To prove Lipschitz continuity, we need to show there exists a constant $C$ such that $\\forall\\:\\:p\\:=$ $1,\\dot{\\dots},n,\\ \\ell\\ =\\ 1,\\dots,L\\ \\quad\\|f(P_{\\ell}^{p})-f(Q_{\\ell}^{p})\\|\\,\\le\\,C\\|P_{\\ell}^{p}-Q_{\\ell}^{p}\\|$ . To simplify passages, we only consider a fixed $\\ell$ and perform a generic analysis. We begin by observing that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f(P_{\\ell}^{p})-f(Q_{\\ell}^{p})=}}\\\\ {{\\displaystyle\\sum_{q\\in[1,n]\\setminus\\{p\\}}\\left[W_{\\ell}^{p}P_{\\ell-1}^{p}(P_{\\ell-1}^{q})^{\\top}(W_{\\ell}^{q})^{\\top}+(W_{\\ell+1}^{p})^{\\top}P_{\\ell+1}^{p}(P_{\\ell+1}^{q})^{\\top}W_{\\ell+1}^{q}\\right](P_{\\ell}^{q}-Q_{\\ell}^{q})+}}\\\\ {{\\displaystyle\\left[W_{\\ell}^{q}P_{\\ell-1}^{q}(P_{\\ell-1}^{p})^{\\top}(W_{\\ell}^{p})^{\\top}+(W_{\\ell+1}^{q})^{\\top}P_{\\ell+1}^{q}(P_{\\ell+1}^{p})^{\\top}W_{\\ell+1}^{p}\\right](P_{\\ell}^{p}-Q_{\\ell}^{p})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The last form of the above equation can be rewritten as a sum of the two sums: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{q\\in[1,n]\\backslash\\{p\\}}\\left[W_{\\ell}^{p}P_{\\ell-1}^{p}(P_{\\ell-1}^{q})^{\\top}(W_{\\ell}^{q})^{\\top}\\right.+\\left.(W_{\\ell+1}^{p})^{\\top}P_{\\ell+1}^{p}(P_{\\ell+1}^{q})^{\\top}W_{\\ell+1}^{q}\\right](P_{\\ell}^{q}-Q_{\\ell}^{q})+}\\\\ {\\displaystyle\\sum_{q\\in[1,n]\\backslash\\{p\\}}\\left[W_{\\ell}^{q}P_{\\ell-1}^{q}(P_{\\ell-1}^{p})^{\\top}(W_{\\ell}^{p})^{\\top}\\right.+\\left.(W_{\\ell+1}^{q})^{\\top}P_{\\ell+1}^{q}(P_{\\ell+1}^{p})^{\\top}W_{\\ell+1}^{p}\\right](P_{\\ell}^{p}-Q_{\\ell}^{p})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since the first term does not depend on either $P_{\\ell}^{p}$ or $Q_{\\ell}^{p}$ , we assume as a worst case that its norm is 0. Then, we remove transposes (since $\\|M\\|=\\|M^{\\top}\\|,$ ) and apply the triangle inequality and the sub-multiplicative property of matrix norms: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{q\\in[1,n]\\setminus\\{p\\}}\\|P_{\\ell}^{p}-Q_{\\ell}^{p}\\|\\left(\\|W_{\\ell}^{q}\\|\\|P_{\\ell-1}^{q}\\|\\|P_{\\ell-1}^{p}\\|\\|W_{\\ell}^{p}\\|+\\|W_{\\ell+1}^{q}\\|\\|P_{\\ell+1}^{q}\\|\\|P_{\\ell+1}^{p}\\|\\|W_{\\ell+1}^{p}\\|\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C=\\operatorname*{max}_{q\\in[1,n]\\backslash\\{p\\}}\\big\\{\\|W_{\\ell}^{q}\\|\\|P_{\\ell-1}^{q}\\|\\|P_{\\ell-1}^{p}\\|\\|W_{\\ell}^{p}\\|+\\|W_{\\ell+1}^{q}\\|\\|P_{\\ell+1}^{q}\\|\\|P_{\\ell+1}^{p}\\|\\|W_{\\ell+1}^{p}\\|\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|f(P_{\\ell}^{p})-f(Q_{\\ell}^{p})\\|\\leq C\\sum_{q\\in[1,n]\\setminus\\{p\\}}\\|P_{\\ell}^{p}-Q_{\\ell}^{p}\\|=C(n-1)\\|P_{\\ell}^{p}-Q_{\\ell}^{p}\\|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "2Link concealed to preserve anonymity.   \n3https://github.com/grok-ai/nn-template ", "page_idx": 17}, {"type": "table", "img_path": "iD18l6prA7/tmp/81aaa1d51e9b895d3a5629370bf622dfafc0d80bfc46d8fa496d531efc6bd95e.jpg", "table_caption": [], "table_footnote": ["Table 4: Mean and standard deviation of the test and train loss barrier for each method when matching $n=2$ models on CIFAR10. "], "page_idx": 18}, {"type": "text", "text": "we conclude that $f(P_{\\ell}^{p})$ is Lipschitz continuous for all models and all layers, with Lipschitz constant $C(n-1)$ depending on both the norm of the weights matrices and the number of models. ", "page_idx": 18}, {"type": "text", "text": "B Additional experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We report additional experiments and results in this section. In particular, Appendix B.1 presents a complete evaluation of our matching method for the pairwise case, showing it to be generally competitive with the state-of-the-art Git Re-Basin algorithm [1] and to outperform it on architectures employing BatchNorm [18] normalization. We then discuss different permutation initialization strategies in Appendix B.2. ", "page_idx": 18}, {"type": "table", "img_path": "iD18l6prA7/tmp/91f2ca8abdee065910a9fe60aea043b6bdf835d4c813380005f5429566c11778.jpg", "table_caption": ["B.1 Pair-wise model matching and merging "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 3: Mean and standard deviation of the test and train loss barriers for each method when matching $n\\,=\\,2$ models on CIFAR100. ", "page_idx": 18}, {"type": "text", "text": "barrier is significantly lower for Frankmore robust to the complexity of the dataset. ", "page_idx": 18}, {"type": "text", "text": "As described in Section 3, our formalization can readily be used to match $n=2$ models. In this case, the energy is given by Equation (1) and the permutations are not factorized. We compare the performance of our approach against the Git Re-Basin algorithm [1] and the naive baseline that aggregates the models by taking an unweighted mean on the original model weights without applying any permutation. From the data presented in Table 3, we observe that the approach is competitive with Git Re-Basin [1], with the two methods exhibiting analogously low test barrier on CIFAR10. Focusing on the ResNet20 architecture, we can see that width plays the same role in both approaches, with the barrier decreasing as it increases. We can also appreciate how, while the same architecture resulted in similar barriers for the two approaches on CIFAR10, the Wolfe in CIFAR100, possibly suggesting that the latter is ", "page_idx": 18}, {"type": "text", "text": "B.1.1 ResNet with BatchNorm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We also report the results of a ResNet20 with $2\\times$ width using BatchNorm [18] layers instead of LayerNorm [3] ones. This version, as noted in [21], is in fact harder to match but also the one that is commonly used in practice. We can see in Table 5 that the Frank-Wolfe matcher is able to achieve a lower barrier than Git Re-Basin, indicating the approach to be more robust to architectures using different normalization layers. ", "page_idx": 18}, {"type": "table", "img_path": "iD18l6prA7/tmp/180d8a1f5b259e480825fa802aef80a7ebc035f3622d6f5e0d9074cfe73249df.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 5: Mean and stddev of the test and train loss barriers on $\\mathrm{2\\ResNet}20\\!-\\!2\\times$ models with BatchNorm normalization. ", "page_idx": 18}, {"type": "text", "text": "B.2 Initialization strategies ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As introduced in Algorithm 1, we initialize each $N$ -dimensional permutation to be the $N\\times N$ identity matrix. We now compare this strategy against two alternatives that provide doubly stochastic ", "page_idx": 18}, {"type": "table", "img_path": "iD18l6prA7/tmp/1d1015f1fa0e6573280bf3d19f2b8f58dee94980213f1813e94ac2bbdc60f704.jpg", "table_caption": [], "table_footnote": ["Table 7: Accuracy of the interpolated model using Git Re-Basin [1] over different pairs of models $(1,2),(1,3),(2,\\dot{3})$ by changing random seed $i=1,\\dots,9$ in the weight matching procedure. "], "page_idx": 19}, {"type": "table", "img_path": "iD18l6prA7/tmp/0a81c2081797d4a4ec0e2bf6cdbde074b3034bdd68d89ad46b58d3a354118011.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 6: Test barrier of the interpolations of 3 ResNet $20^{-2}\\times$ models using different initializations. ", "page_idx": 19}, {"type": "text", "text": "matrices, i.e., such that their rows and columns sum to one: i) the Sinkhorn initialization [35] that initializes the permutation matrix as the solution of the Sinkhorn-Knopp algorithm [35]; ii) the barycenter of doubly stochastic matrices, i.e. the matrix where each element is given by $1/N$ .Table 6 shows the test barrier of the interpolations of three ResNet $;20^{-2}\\times$ models $a,b$ , and $c$ when using the different strategies over 10 different trials. We can see that the constant initializations (identity and barycenter) work well in general, with the additional benefti of having 0 variance in the results. On the other hand, if computational cost is not a concern, one can still choose to run a pool of trials with different Sinkhorn initializations and finally select the best one, trading this way efficiency with some extra accuracy points. ", "page_idx": 19}, {"type": "text", "text": "B.3 Variance of the results in Git Re-Basin ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As introduced in Section 4, Git Re-Basin [1] depends on a random choice of layers, resulting in variations of up to $10\\%$ in accuracy depending on the optimization seed, while our method shows zero variance. While we have already seen the results for a model pair in Figure 4, we report, for completeness, the results of matching and averaging models with Git Re-Basin using different optimization seeds for additional pairs. As can be seen in Table 7, the trend is confirmed over these ones, with results significantly oscillating and our approach always above or on par with their mean. ", "page_idx": 19}, {"type": "text", "text": "B.4 Large-scale matching: ResNet50s trained over ImageNet ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For this experiment, we matched three different ResNet50s trained over ImageNet. We used three publicly available pretrained checkpoints from timm, namely ${\\tt a1\\_i n1\\tt k^{4}}$ , ${\\mathsf{c i}}_{-1\\mathrm{n}1\\mathbf{k}^{5}}$ and ram in1k 6. As Table 8 shows, $C^{2}M^{3}$ underperforms the baseline in this case. To see why, we report in Figure 14 the pairwise accuracies obtained using pairwise weight matching over all the ResNet50 checkpoints available in timm. Let us focus on the triplet (am, a2, ram) and replace the model names with (a, b, c) for clarity. We see that, while the mergings (a, b) and (b, c) result in high-accuracy models, the merging (a, c) yields poor results. Given the cycle consistency of our method, we inherit the difficulty of the hardest pair, which in this case is (a, c). It is worth noting that this behavior is not present in the other cases we investigated in this work, and might be due to ", "page_idx": 19}, {"type": "table", "img_path": "iD18l6prA7/tmp/32b779afa8a6ea6acdb065247d64ac6b677ecb675b42dd775bca943214a5ef34.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 8: Accuracy and loss of the interpolated model using different matchers over three ResNet50 models trained on ImageNet. ", "page_idx": 19}, {"type": "text", "text": "the considered models being trained with different training schedules and hyperparameters. Future research could investigate new strategies to handle such cases, e.g. by iteratively merging models by following a max-accuracy path in an accuracy weighted graph. ", "page_idx": 19}, {"type": "image", "img_path": "iD18l6prA7/tmp/04a96f3b7fcd01c44958aeeae6e59a651f67dbc04564d8952db342932d151451.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "iD18l6prA7/tmp/687214a62e5699dfc51bb7ce1f79cb9432b0ae30aa6ad42985af0b7c1b19fe48.jpg", "table_caption": ["Figure 14: Pairwise accuracies obtained using Git Re-Basin [1] over different ResNet50 models trained on ImageNet. Models are available from the timm library. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 9: Accuracy over 10 clients in a federated learning scenario. We report the accuracy for 50 aggregation rounds, with each client training for 20 local epochs. We report one every five rounds for the sake of clarity. ", "page_idx": 20}, {"type": "text", "text": "B.5 Federated Learning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We here report the results of a preliminary experiment where we ran our framework in a federated learning scenario. To this end, we have used the state-of-the-art federated learning library Flower 7 [6] and employed our matching scheme over a set of 10 clients over CIFAR10, each adopting a small CNN model. We observe the following: ", "page_idx": 20}, {"type": "text", "text": "\u2022 When all the clients start from the same initialization, our approach has no benefit and falls back to standard averaging. In fact, the optimization process quickly returns identity matrices as permutations, suggesting the models already share the same basin. \u2022 When instead we initialize the clients from different random initializations, Tables 9 and 10 show that our approach visibly outperforms FedAVG. In particular, the benefits get more pronounced when increasing the number of local epochs. This is in line with the intuition that standard averaging becomes less effective when clients drift due to prolonged local training and too infrequent aggregation. ", "page_idx": 20}, {"type": "text", "text": "While these results are not sufficient to claim an overall supremacy of the approach for the task due to the limited evaluation and choice of models, they show the approach to be promising for the problem and encourage further research. ", "page_idx": 20}, {"type": "table", "img_path": "iD18l6prA7/tmp/1be82fccfe33e10600044a0563aee706566d19c44004b771b74fa9b8b0aaf86c.jpg", "table_caption": [], "table_footnote": ["Table 10: Accuracy over 10 clients in a federated learning scenario. We report the accuracy for 10 aggregation rounds, with each client training for 30 local epochs. "], "page_idx": 21}, {"type": "text", "text": "C Additional analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we report additional analyses that complement the results presented in the main text. We first analyze in Appendix C.1 how mapping to universe affects the similarity of the models; then, we evaluate how the composition of the match set affects the accuracy of the merged model in Appendix C.2. ", "page_idx": 21}, {"type": "text", "text": "C.1 Similarity of models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We analyze here how similar are models before and after being mapped to the universe space, first by comparing their representations and then by comparing their weights. ", "page_idx": 21}, {"type": "text", "text": "C.1.1 Representation-level similarity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Figures 15a and 15b show the Centered Kernel Alignment (CKA) [22] of the representations of 5 ResNet20 models trained on CIFAR10 with $2\\times$ width. The linear version of CKA is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{CKA}(X,Y)={\\frac{\\mathrm{HSIC}(X,Y)}{\\sqrt{\\mathrm{HSIC}(X,X)\\,\\mathrm{HSIC}(Y,Y)}}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathrm{HSIC}(X,Y)\\,=\\,\\frac{1}{(N-1)^{2}}\\,\\mathrm{tr}(\\mathbf{X}\\mathbf{H}\\mathbf{X}^{\\top}\\mathbf{H})}\\end{array}$ , $\\begin{array}{r}{\\mathbf{H}=\\mathbf{I}-\\frac{1}{N}\\mathbf{1}\\mathbf{1}^{\\top}}\\end{array}$ is a centering matrix, and 1 is a vector of $N$ ones. The denominator is introduced to scale CKA between zero and one, where a value of one indicates equivalent representations. CKA is invariant to orthogonal transformations and isotropic scaling. Being permutations orthogonal transformations, CKA stays exactly the same after mapping the models to the universe. On the contrary, the Euclidean distance of the representations of the models significantly decreases after mapping to the universe, as shown in Figures 15c and 15d. ", "page_idx": 21}, {"type": "text", "text": "C.1.2 Weight-level similarity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We have seen in Figure 5 that the cosine similarity of the weights is higher after mapping the weights to the universe. This suggests that the models are more similar in the universe, which is consistent with the fact that it constitutes a convenient space to merge them. We report here for completeness the Figure 16 the Euclidean distance of the weights of 5 ResNet20 models trained on CIFAR10 with $2\\times$ width, showing the same trend as the cosine similarity. ", "page_idx": 21}, {"type": "text", "text": "C.2 Merging different subsets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We merge subsets of $k<5$ models from the set of 5 models $a,b,c,d,e$ to gauge the effect of the match set composition over the accuracy of the merged model. As shown in Figure 17, we run two different merging schemes: in the former (left column), we globally match all the 5 models jointly and then consider subsets only at the aggregation step. In the second analysis (right column), we instead consider model subsets from the start, therefore running the whole matching procedure on the $k$ models before averaging them. This way, we aim to disentangle the error resulting from imperfect matching from the one naturally resulting from the aggregation. We highlight a few notable aspects: ", "page_idx": 21}, {"type": "text", "text": "1. While the accuracies are expectedly higher when matching a subset with permutations expressly optimized for that same subset (right column), this is not the case for $n=2$ , in which the permutations resulting from matching the superset of 5 models yield better results when merging pairs of them. This hints at the added constraint of cycle consistency over a wide number of models adding in some cases an advisable prior over the search space. ", "page_idx": 21}, {"type": "image", "img_path": "iD18l6prA7/tmp/2babe8b8f1113cdc984e9e5e9e6535f6bda0fc17a787bd2b0752b49a92cbdbc6.jpg", "img_caption": ["(c) Before mapping to universe. ", "(d) After mapping to universe. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 15: Cented Kernel Alignment and Euclidean distances of the representations of 5 ResNet20 trained on CIFAR10 with $2\\times$ width. ", "page_idx": 22}, {"type": "image", "img_path": "iD18l6prA7/tmp/3223e6bf3c64fe2af69d0f7a887872f5baad803ae8eb62beae92ecd494b44733.jpg", "img_caption": ["(a) Before mapping to universe. ", "(b) After mapping to universe. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 16: Euclidean distance of the weights of 5 ResNet20 trained on CIFAR10 with $2\\times$ width. ", "page_idx": 22}, {"type": "text", "text": "2. The particular composition of the match set has a significant impact over the matching and subsequent merge operation, yielding differences of up to $\\approx20$ accuracy points for the downstream model.   \n3. The standard deviations before the repair operation (red semi-transparent bars in the plots) are way lower when optimizing for the permutations over the superset of all 5 models; this suggests that the matching difficulty is spread over all the maps jointly, eventually yielding more stable results. ", "page_idx": 23}, {"type": "text", "text": "D Discussion ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We discuss in this section the limitations of our work, as well as potential future societal impact. ", "page_idx": 23}, {"type": "text", "text": "D.1 On the cycle-consistency of $C^{2}M^{3}$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our method is natively cycle-consistent due to the mathematical formulation of the optimization problem. If we were to not desire cycle consistency, the matching method would fall back to the $n=2$ Frank-Wolfe (FW) case presented in Section 3. One would then have to transform the pairwise matching problem to a $n$ -way matching problem, e.g. by using the $n=2$ FW procedure as matching step in the MergeMany [1] algorithm. Results for the $n=2$ FW matching are reported in Table 3. ", "page_idx": 23}, {"type": "text", "text": "D.2 Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "From what we have observed in our experiments, permutations satisfying linear mode connectivity of the models are hard to find for most architectures and datasets. In fact, given that there is no practical way to prove or disprove the conjecture for which most models end up in the same basin modulo permutations of the neurons, we cannot be sure that a certain set of models even allows finding such permutations, let alone that the permutations found are the optimal ones. We therefore encourage the community not to rely on the existence of such permutations in general. However, we have also shown that we can always find permutations that improve the resulting aggregated model, which is a promising practical result for model merging. As for all the existing works concerning linear mode connectivity and model merging, the resulting models that we obtain are sensible to a wide variety of factors, from training hyperparameters to the optimization algorithm used. Several works have already observed the phenomenon in practice: among these, Ainsworth et al. [1] mention among the known failure modes of their approaches models trained with SGD and too low learning rate, or ADAM coupled with too high learning rate. Jordan et al. [21] show that the chosen normalization layer incredibly affects the accuracy of the resulting merged model, while Qu and Horvath [30] observe learning rate, weight decay, and initialization method to play a strong role as well. Being a mostly empirical field, most of the technical choices that we make in our work mirror the ones made in previous works and are not based on a solid theoretical foundation. We therefore release all our code and encourage the community to investigate further on what training and optimization hyperparameters effect linear mode connectivity and model merging. ", "page_idx": 23}, {"type": "text", "text": "D.3 Societal impact and broader vision ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The work presented in this paper serves as an additional tool for the community to improve the efficiency of deep learning models. By merging models, we can reduce the computational cost of training and inference, as well as the memory footprint of the models. In fact, by aggregating the information of a set of models into a single one with the same architecture, practitioners can benefti of the effects of ensembling without incurring in its computational cost. Moreover, merging is in many cases a practical necessity to guarantee confidentiality and privacy of user data, as it allows to train models on different subsets of the data, e.g. originating from different clients, and then merge them to obtain a single model integrating all the information. This is particularly important in the context of federated learning, where the data is distributed among different clients and cannot be shared. We believe that the work presented in this paper can be a stepping stone towards more efficient and privacy-preserving deep learning models, and we encourage the community to further investigate the potential of model merging in these contexts. ", "page_idx": 23}, {"type": "image", "img_path": "iD18l6prA7/tmp/a1efd68475cc9b3f1fa7f3edcb7671c7689f37e2214107fc17845840cd5a3b30.jpg", "img_caption": ["(a) Subsets of 4 out of 5 jointly matched models. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "iD18l6prA7/tmp/4efb62a1eac4a841c01128a53def75cd60502ca7b962e840b5624d59ddb9f47f.jpg", "img_caption": ["(b) Subsets of 4 matched models out of 5 models. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "iD18l6prA7/tmp/fd2d76abecd0da85d394682e4aeccad3295743a295d0239584ad1ac09c5d30f1.jpg", "img_caption": ["(c) Subsets of 3 out of 5 jointly matched models. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "iD18l6prA7/tmp/8682ce32a2631a27ad89e96d4d11b56e87df841d5e83270d9438e549d5a1e8a1.jpg", "img_caption": ["(d) Subsets of 3 matched models out of 5 models. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "iD18l6prA7/tmp/8726bcd7536bd6cd62a34c32158812d6fc071489b4e7027cb3ac92a0e7901b5f.jpg", "img_caption": ["(e) Subsets of 2 out of 5 jointly matched models. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "iD18l6prA7/tmp/39a0bf6d7eb7b6d126189b09738d4cb782d3d6dbcaeb1485ac2aad0f1ee69471.jpg", "img_caption": ["(f) Subsets of 2 matched models out of 5 models. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 17: Accuracy of the resulting model when merging different model subsets. (left) performance of models obtained from aggregating subsets of $k<5$ models that were matched jointly. (right) analoguous results for subsets of $k$ models that are instead matched independently, i.e., by only optimizing for the permutations that align those $k$ models and discarding the remaining ones. The semi-transparent bands represent the standard deviation of the accuracy. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the claims are supported by experiments and sound reasoning. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discuss limitations and assumptions of our work in Appendix D ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper mostly leverages existing theoretical results and properly cites each of these ones. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We describe in great detail all of the presented algorithms and we report all the technical details for the architectures. We also provide a modular and reusable codebase that respects the highest software engineering standards with configuration parameters provided as separate yaml files. All the experiments were performed by setting reproducible seeds that are logged in WandB. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide a modular and reusable codebase that respects the highest software engineering standards with configuration parameters provided as separate yaml flies. Code is highly reproducible and machine agnostic due to the extensive use of frameworks and libraries such as PyTorch Lightning, WandB, Hydra and NN-template. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The most important details are provided in the paper, with the yaml configuration files in the code providing all the remaining minor details. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We report means and standard deviations for all our experiments when variance is present. The only entries that do not have standard deviation are those that result in a deterministic result that is not affected by random seed. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: we reported in Appendix A the equipment used for the experiments, as well as a discussion on the efficiency of our methods. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our research totally complies with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss in Appendix D the potential positive societal impact of our work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We adequately cite all of the used datasets and architectures. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: While we provide a codebase that is extensible and modular enough to be reused by many researchers in the field, we are not releasing any libraries or datasets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]