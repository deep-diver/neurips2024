[{"figure_path": "iD18l6prA7/tables/tables_6_1.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table shows the accuracy and loss for different model merging methods.  It compares a naive averaging method, MergeMany, and the proposed C2M\u00b3 approach, both with and without activation renormalization (REPAIR). Results are shown for different model architectures (MLP, ResNet, VGG16) and datasets (EMNIST, CIFAR10, CIFAR100). The best-performing method is highlighted in bold. The table demonstrates the superior performance of the C2M\u00b3 approach, especially when coupled with REPAIR.", "section": "Experiments"}, {"figure_path": "iD18l6prA7/tables/tables_15_1.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table shows the accuracy and loss of merged models resulting from merging 5 models with different initializations.  It compares the performance of the proposed C2M\u00b3 method with a naive averaging approach and the MergeMany algorithm.  The results are presented for different architectures (MLP, ResNet, VGG16) and datasets (EMNIST, CIFAR10, CIFAR100). The best results for each configuration are highlighted in bold.  The '+' symbol indicates that REPAIR (activation renormalization) was applied to the merged model. The table highlights the consistent improvement in accuracy and reduction in loss achieved by C2M\u00b3 compared to the baseline methods.", "section": "Experiments"}, {"figure_path": "iD18l6prA7/tables/tables_18_1.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table presents the accuracy and loss of the merged model obtained by merging five models trained with different initializations using different model merging methods: Naive averaging, MergeMany, C2M\u00b3, and C2M\u00b3 with the REPAIR post-processing technique.  The results are shown for various datasets and model architectures (MLP, ResNet, and VGG16).  The table highlights the superiority of the C2M\u00b3 approach, especially when combined with REPAIR, in achieving higher accuracy and lower loss compared to other methods.  It demonstrates the effectiveness of the proposed cycle-consistent multi-model merging method in improving model performance.", "section": "Experiments"}, {"figure_path": "iD18l6prA7/tables/tables_18_2.jpg", "caption": "Table 3: Mean and standard deviation of the test and train loss barriers for each method when matching n = 2 models on CIFAR100.", "description": "This table shows the mean and standard deviation of the training and testing loss barriers for different model matching methods on the CIFAR100 dataset. Three methods are compared: a naive averaging method, the Git Re-Basin method, and the Frank-Wolfe method proposed in the paper.  The results indicate the performance of each method in aligning two models by measuring the loss barrier between them.", "section": "B Additional experiments"}, {"figure_path": "iD18l6prA7/tables/tables_18_3.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table shows the accuracy and loss of merged models trained with different initializations, comparing the proposed C2M\u00b3 method against baselines.  It highlights the improvement achieved by C2M\u00b3 and the effect of post-processing using the REPAIR method.", "section": "Experiments"}, {"figure_path": "iD18l6prA7/tables/tables_19_1.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table shows the accuracy and loss for a merged model created using different methods. Five models were initially trained with different initializations. The methods compared include a naive averaging of weights, the MergeMany approach, and the proposed C2M\u00b3 approach, both with and without activation renormalization (REPAIR).  The results are shown for various architectures (MLP, ResNet, VGG16) and datasets (EMNIST, CIFAR10, CIFAR100). The best results for each scenario are bolded, highlighting the effectiveness of the C2M\u00b3 method, particularly when coupled with REPAIR.", "section": "Experiments"}, {"figure_path": "iD18l6prA7/tables/tables_19_2.jpg", "caption": "Table 6: Test barrier of the interpolations of 3 ResNet20-2\u00d7 models using different initializations.", "description": "This table presents the test barrier values obtained when interpolating between three ResNet20 models with 2x width using different initialization strategies for the permutation matrices: identity matrix (id), barycenter of doubly stochastic matrices (barycenter), and Sinkhorn initialization (Sinkhorn).  The results show that the constant initializations (identity and barycenter) perform reasonably well, offering the added benefit of zero variance in results.  However, if computation cost is not an issue, running multiple trials with Sinkhorn initialization and selecting the best result could potentially improve accuracy slightly, though this trades efficiency for marginal gains.", "section": "Additional experiments"}, {"figure_path": "iD18l6prA7/tables/tables_19_3.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table presents the accuracy and loss values achieved by different model merging methods on various datasets when merging 5 models with different initializations.  The methods compared include a naive averaging of weights, the MergeMany approach, and the proposed C2M\u00b3 approach, both with and without activation renormalization (REPAIR). The table highlights the superior performance of the C2M\u00b3 method, especially when coupled with REPAIR, showcasing its ability to effectively merge models trained with diverse initializations while achieving better accuracy and lower loss than existing methods.", "section": "Experiments"}, {"figure_path": "iD18l6prA7/tables/tables_20_1.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table compares the accuracy and loss of a merged model created using different methods, including a naive averaging approach, the MergeMany approach, and the proposed C2M\u00b3 method, both with and without activation renormalization (REPAIR).  Five models, each trained with different initializations, are merged. The results show that C2M\u00b3 significantly outperforms the other methods in terms of accuracy and loss, particularly when REPAIR is applied.", "section": "Experiments"}, {"figure_path": "iD18l6prA7/tables/tables_21_1.jpg", "caption": "Table 1: Accuracy of the merged model when merging 5 models trained with different initializations. The best results are highlighted in bold. \u2020 denotes models after the REPAIR operation.", "description": "This table presents the accuracy and loss of models obtained by merging five models that were trained with different initializations.  The results are compared for different architectures (MLP, ResNet, and VGG16) on different datasets (EMNIST, CIFAR10, and CIFAR100). Two different merging methods are shown: C2M\u00b3 and MergeMany.  A naive approach (averaging weights without any matching) is also included as a baseline. The table shows that C2M\u00b3 consistently outperforms both the MergeMany algorithm and the naive baseline in terms of accuracy and loss. It also shows that applying the REPAIR technique to the merged models can improve the results further.", "section": "Experiments"}]