[{"figure_path": "faj2EBhdHC/tables/tables_7_1.jpg", "caption": "Table 2: CNA systematically improves graph classification accuracy (\u2191).", "description": "This table presents the results of graph classification experiments comparing the performance of GNNs with and without CNA modules.  It shows that across four different GNN architectures (GCN, GraphSAGE, TransformerConv, and GAT), incorporating CNA consistently leads to a significant increase in accuracy.  The improvement is substantial and consistent across all the tested models. ", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_7_2.jpg", "caption": "Table 3: CNA reduces the NMSE (\u2193) on two multiscale node regression datasets.", "description": "This table presents the normalized mean squared error (NMSE) results for different GNN models on two multiscale node regression datasets: Chameleon and Squirrel.  It compares the performance of various GNN models (GCN, GAT, PairNorm, GCNII, G2-GCN, G2-GAT, and Trans.Conv) with and without the proposed CNA module. Lower NMSE values indicate better performance. The results show that adding the CNA module consistently improves the performance of the TransformerConv model on both datasets, significantly reducing the NMSE.", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_7_3.jpg", "caption": "Table 4: Comparison of our method CNA with the leaderboard on Papers with Code (PwC),\u00b2 as of writing on a diverse set of node classification datasets from five typical collections. CNA outperforms the respective leaders, and thereby all compared methods, in eight out of eleven cases (73%). For some, it does so by a significant margin, e.g., on the popular Cora and CiteSeer datasets.", "description": "This table compares the performance of the proposed CNA method against state-of-the-art results reported on Papers with Code for eleven node classification datasets.  It shows that CNA achieves better accuracy than the best-performing methods on eight out of the eleven datasets, significantly outperforming others on datasets such as Cora and CiteSeer.", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_8_1.jpg", "caption": "Table 5 & Figure 5: CNA allows for (i) compact and (ii) accurate models: The separate treatment of super-nodes boosts expressivity, making GNNs more compact.", "description": "This table and figure compare the performance and the number of parameters of various GNN models, including those enhanced with CNA modules, on the ogbn-arxiv dataset.  The results show that CNA enables the creation of smaller models that achieve comparable or better accuracy than larger, more complex models.", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_15_1.jpg", "caption": "Table 7: The datasets we used to evaluate CNA. The table contains statistics for the node classification and property prediction tasks. For regression, we used the Chameleon and Squirrel datasets, but with each page\u2019s log average web traffic as the target value.", "description": "This table presents the characteristics of the datasets used in the paper's experiments to evaluate the CNA module. For each dataset, it lists the number of nodes, edges, features, classes, the node homophily ratio, and whether the classes are balanced.  It also notes which datasets were used for node classification, node property prediction, and regression tasks.", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_15_2.jpg", "caption": "Table 8: The graph-level datasets we used to evaluate CNA. The table contains statistics for the graph-level classification.", "description": "This table presents information about the graph-level datasets used to evaluate the Cluster-Normalize-Activate (CNA) modules.  For each dataset, the number of graphs, average number of nodes, average number of edges, number of features, number of classes, and whether the classes are balanced are shown. This helps understand the characteristics of the datasets used for evaluating the effectiveness of CNA in graph-level classification tasks.", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_16_1.jpg", "caption": "Table 9: Hyperparameters for node classification, node property prediction and graph-level classification.", "description": "This table lists the hyperparameters used for node classification, node property prediction, and graph-level classification tasks in the paper's experiments.  It includes details such as the architecture type, number of epochs, number of layers, number of clusters, number of hidden units, learning rate, learning rate for activation function, and weight decay for each dataset used in the experiments. These hyperparameters were tuned for optimal performance on each task and dataset.", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_16_2.jpg", "caption": "Table 10: Hyperparameters for node regression (TransformerConv), the ablation study (*), and Table 1 (*).", "description": "This table presents the hyperparameters used for node regression using the TransformerConv architecture and for the ablation study.  The ablation study uses a different number of layers (4) and hidden units (280) compared to the main TransformerConv setup (2 layers and 64 hidden units).  The values include the number of epochs, number of layers, number of clusters, number of hidden units, learning rate (LR), activation learning rate (LR Act.), and weight decay. The '*' denotes entries that deviate from the main setup.", "section": "4 Experiments"}, {"figure_path": "faj2EBhdHC/tables/tables_16_3.jpg", "caption": "Table 1: CNA consistently increases the accuracy (\u2191) of each architecture on Cora.", "description": "This table presents a comparison of different graph neural network (GNN) architectures on the Cora dataset.  The \"Baseline\" column shows the accuracy achieved by each architecture without the proposed Cluster-Normalize-Activate (CNA) module. The \"CNA\" column shows the improvement in accuracy when the CNA module is added.  The results demonstrate that the CNA module consistently improves the accuracy across all architectures tested on the Cora dataset.", "section": "4 Experiments"}]