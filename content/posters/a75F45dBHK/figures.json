[{"figure_path": "a75F45dBHK/figures/figures_2_1.jpg", "caption": "Figure 2.1: Orchid block architecture. This diagram illustrates the structure of the Orchid block. The core operation is a convolution (denoted by *), efficiently implemented in the frequency domain using FFT. Element-wise multiplication is denoted by . On the right side, two different conditioning networks, introduced in equations (2) and (3) as shift-invariant convolution kernels, are illustrated. In this model, the convolution is performed efficiently in the spectral domain, so the kernel in the frequency domain, h = h0 + h(x), is computed. The block also includes MLPs for linear projection and pointwise mixing of features at the beginning, that is common design choice used in various sequence modeling architectures.", "description": "The figure shows the architecture of the Orchid block, which is the core building block of the Orchid model.  It uses a data-dependent convolution, where the convolution kernel is dynamically adapted based on the input sequence. Two different conditioning networks are shown, both designed to maintain shift equivariance in the convolution operation.  The convolution is performed efficiently in the frequency domain using FFT.  The block also includes MLPs for linear projections and pointwise mixing of features.", "section": "3 Orchid Operator"}, {"figure_path": "a75F45dBHK/figures/figures_17_1.jpg", "caption": "Figure C.1: Comparison of Local Conv1D Choices: Evaluation of different local convolution options used in the conditioning network. Conditioning networks of type I (Equation 2) (1 layer Conv1D in time + 1 layer in frequency), 2 layer Conv1D in time, 2 layer Conv1D in frequency, and 3 layer Conv1D in time + 3 layer in frequency.", "description": "The figure shows the performance of different conditioning network architectures in Orchid on the synthetic in-context learning task. The x-axis represents the training steps, and the y-axis represents the test accuracy. Four lines represent the four different architectures: 1 layer Conv1D in time + 1 layer in frequency, 2 layers in time, 2 layers in frequency, and 3 layers in time + 3 layers in frequency. This experiment aims to investigate the optimal architecture for the conditioning network in Orchid.", "section": "C.1 Synthetic In-context Learning"}, {"figure_path": "a75F45dBHK/figures/figures_17_2.jpg", "caption": "Figure C.2: Comparison of different \u03c3() on conditioning network of Type II (cross-correlation in equation 4).", "description": "This figure compares the performance of different activation functions (\u03c3) used in the Type II conditioning network for data-dependent convolution. The Type II network uses cross-correlation to achieve shift invariance. The activation functions compared are Tanh(), Sigmoid(), Softsign(), SoftShrink(), and Identity(). The results show that removing the non-linearity (Identity()) provides the best performance, slightly better than Softshrink() and Tanh().  Type II networks with Identity() and SoftShrink() show faster convergence than Type I.", "section": "C.2 Model Architecture Ablation"}, {"figure_path": "a75F45dBHK/figures/figures_18_1.jpg", "caption": "Figure C.3: Test accuracy of in-context learning on the associative recall task with a vocabulary size of 20 and sequence length of 128, comparing different model components. Type I refers to conditioning networks of type I (based on absolute value in Equation 2). Orthonormal indicates transforms that utilize orthogonal and normalized bases.", "description": "The figure shows the test accuracy of the in-context learning task on the associative recall task using different Fourier transforms (DCT and DFT) and different types of conditioning networks (Type I and orthonormal). The results suggest that using orthonormal DCT with Type I conditioning network shows the best performance.", "section": "C.3 Language Modeling"}, {"figure_path": "a75F45dBHK/figures/figures_21_1.jpg", "caption": "Figure C.4: Forward and backward runtime comparison of different attention mechanisms (FlashAttention, Attention, and Orchid) with varying sequence lengths.", "description": "This figure compares the forward and backward pass runtime of three different attention mechanisms: FlashAttention, the standard attention mechanism, and Orchid. The x-axis represents the sequence length (L), and the y-axis represents the runtime in milliseconds (ms).  The plot shows that Orchid's runtime scales sublinearly with sequence length, unlike the standard attention mechanism, which scales quadratically. FlashAttention shows a similar runtime to Orchid in the forward pass but a slightly higher runtime during backpropagation. This demonstrates Orchid's computational efficiency, particularly for long sequences.", "section": "C.6 Runtime Benchmark"}]