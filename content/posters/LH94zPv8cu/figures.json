[{"figure_path": "LH94zPv8cu/figures/figures_0_1.jpg", "caption": "Figure 1: Inpainting results for \"a robot sitting on a bench\". As the input video shifts smoothly, our output frames stay consistent.", "description": "This figure displays the results of a video inpainting task.  The top row shows the input video frames with a portion of each frame masked. The middle row shows the results generated by the \"How I Warped Your Noise\" method, and the bottom row presents the results obtained using the proposed \"Warped Diffusion\" method.  The figure demonstrates that even with smooth shifting in the input video, the \"Warped Diffusion\" method effectively maintains consistent and coherent inpainting across frames, unlike the \"How I Warped Your Noise\" method which shows some inconsistency.", "section": "1 Introduction"}, {"figure_path": "LH94zPv8cu/figures/figures_2_1.jpg", "caption": "Figure 2: Visualization of Warped Diffusion applied to video super-resolution. (a) We develop a function space diffusion model that super-resolves images given samples from a Gaussian process (GP). To extend the image model to videos, (b) we extract warping transformations between consecutive input frames using optical flow. (c) We use the flow to warp the GP sample from the previous frame. (d) To ensure temporal consistency, we introduce equivariance self-guidance in the ODE sampler.", "description": "This figure illustrates the Warped Diffusion method applied to video super-resolution.  It shows the process, starting with a function space diffusion model trained on images and Gaussian process samples. Optical flow is used to warp the GP samples from the previous frame to the current frame. Equivariance self-guidance is used in the ODE sampler to ensure temporal consistency in the output high-resolution frames.", "section": "2 Functional Video Generation"}, {"figure_path": "LH94zPv8cu/figures/figures_2_2.jpg", "caption": "Figure 2: Visualization of Warped Diffusion applied to video super-resolution. (a) We develop a function space diffusion model that super-resolves images given samples from a Gaussian process (GP). To extend the image model to videos, (b) we extract warping transformations between consecutive input frames using optical flow. (c) We use the flow to warp the GP sample from the previous frame. (d) To ensure temporal consistency, we introduce equivariance self-guidance in the ODE sampler.", "description": "This figure illustrates the Warped Diffusion framework for video super-resolution. It shows how a function space diffusion model is extended from images to videos using optical flow to warp Gaussian process samples between frames. Equivariance self-guidance ensures temporal consistency in the generated video.", "section": "2 Functional Video Generation"}, {"figure_path": "LH94zPv8cu/figures/figures_6_1.jpg", "caption": "Figure 2: Visualization of Warped Diffusion applied to video super-resolution. (a) We develop a function space diffusion model that super-resolves images given samples from a Gaussian process (GP). To extend the image model to videos, (b) we extract warping transformations between consecutive input frames using optical flow. (c) We use the flow to warp the GP sample from the previous frame. (d) To ensure temporal consistency, we introduce equivariance self-guidance in the ODE sampler.", "description": "This figure illustrates the Warped Diffusion method applied to video super-resolution.  It shows the process in four steps:\n(a) A function space diffusion model is used to super-resolve images from Gaussian process samples.\n(b) Optical flow is used to extract warping transformations between consecutive input video frames.\n(c) These transformations are then used to warp the Gaussian process samples from the previous frame.\n(d) Equivariance self-guidance is introduced in the ODE sampler to maintain temporal consistency in the generated high-resolution video frames.", "section": "2 Functional Video Generation"}, {"figure_path": "LH94zPv8cu/figures/figures_8_1.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "This figure shows the self-warping error, a measure of temporal consistency, for different methods on the inpainting task. The x-axis represents the frame index, and the y-axis represents the self-warping error in both latent space (left) and pixel space (right). The figure shows that the proposed method (Ours (gp)) achieves significantly lower warping error compared to baselines, indicating better temporal consistency.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_18_1.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "This figure visualizes the temporal consistency of different methods by showing the self-warping error with respect to the first frame for the inpainting task.  The x-axis represents the frame index, and the y-axis represents the self-warping error, which measures how consistent the model's predictions are across time.  Lower values indicate better temporal consistency. The figure compares several methods, including the proposed Warped Diffusion method, and several baselines such as Fixed Noise, Resample Noise, and How I Warped Your Noise.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_18_2.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "This figure shows the self-warping error for the inpainting task. The self-warping error measures how consistent the model's predictions are across time. In this experiment, the input frame is shifted smoothly, and the figure shows the self-warping error in both latent space and pixel space. The results show that Warped Diffusion is the only method that achieves temporal consistency while maintaining high reconstruction performance.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_18_3.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "The figure shows the self-warping error for the inpainting task. The self-warping error measures how consistent the model's predictions are across time. For each frame in a video, the model generates an inpainting result. The self-warping error is calculated by comparing the generated frames and seeing how similar they are after being warped to align with a reference frame (either the first frame or the previous frame). Lower self-warping error indicates better temporal consistency. The x-axis represents the frame index in the video, and the y-axis represents the self-warping error.  The plot shows multiple lines, each representing a different method (Fixed noise, Resample noise, How I Warped Your Noise, GP noise warping, and the proposed method). The plot shows that the proposed method (Ours (gp)) significantly outperforms all baselines in terms of temporal consistency.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_18_4.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "This figure shows the self-warping error for the inpainting task in both latent and pixel space. The x-axis represents the frame index while the y-axis represents the warping error. Different methods are compared, including \"Fixed (gp)\", \"Resample (gp)\", \"GP Noise Warping\", \"How I Warped Your Noise (indep)\", and \"Ours (gp)\". The figure demonstrates how the proposed method (\"Ours (gp)\") significantly outperforms other methods in maintaining temporal consistency across frames.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_18_5.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "This figure shows the self-warping error, a measure of temporal consistency, for different methods in video inpainting.  The x-axis represents the frame index, while the y-axis represents the self-warping error. Different lines represent different noise warping methods (Fixed, Resample, How I Warped Your Noise, GP Noise Warping, and Ours).  The results demonstrate that our method (Ours) significantly outperforms other methods in maintaining temporal consistency across frames, achieving a significantly lower self-warping error.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_18_6.jpg", "caption": "Figure 5: Warping errors w.r.t. first generated frame (top-row) and prev. generated frame (bottom row) for the 8\u00d7 super-resolution task for real videos.", "description": "This figure shows the self-warping error for the super-resolution task on real videos.  The error is measured in both latent and pixel space, and separately for warping relative to the first frame and relative to the previous frame.  The results demonstrate the temporal consistency (or lack thereof) of different methods for handling noise during the super-resolution process of video frames.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_19_1.jpg", "caption": "Figure 6: Visualization of independent noise and noise from a Gaussian Process.", "description": "The figure visualizes the difference between independent noise and noise sampled from a Gaussian process.  The left panel (a) shows a sample of independent noise, exhibiting a high degree of randomness and lack of spatial correlation. In contrast, the right panel (b) displays a noise sample generated from a Gaussian process, characterized by smoother variations and visible spatial correlation. This highlights the key difference in the structure of noise utilized in the proposed Warped Diffusion method, which employs Gaussian processes for its functional noise model to handle interpolation and inpainting tasks within the video generation process.", "section": "3.1 Gaussian Processes (GPs)"}, {"figure_path": "LH94zPv8cu/figures/figures_20_1.jpg", "caption": "Figure 7: Inpainting examples. Left column: inputs by randomly masking images from the COYO dataset. Right column: inpainting outputs from our SDXL fine-tuned model with correlated noise.", "description": "This figure shows the inpainting results on images from the COYO dataset. The left column shows the input images with randomly masked regions. The right column presents the results of inpainting these masked regions using the authors' proposed method, which is a fine-tuned Stable Diffusion XL model trained with correlated noise. This visually demonstrates the model's ability to generate realistic and coherent inpainting results.", "section": "Experimental Results"}, {"figure_path": "LH94zPv8cu/figures/figures_21_1.jpg", "caption": "Figure 7: Inpainting examples. Left column: inputs by randomly masking images from the COYO dataset. Right column: inpainting outputs from our SDXL fine-tuned model with correlated noise.", "description": "This figure shows inpainting results using the proposed Warped Diffusion method.  The left column displays images from the COYO dataset with randomly masked regions, simulating missing parts of a video frame.  The right column presents the inpainted results generated by the model after fine-tuning with correlated noise. The comparison highlights the model's ability to reconstruct missing video content in a temporally coherent manner.", "section": "4 Experimental Results"}, {"figure_path": "LH94zPv8cu/figures/figures_22_1.jpg", "caption": "Figure 8: Super-resolution examples. Left column: downsampled inputs from the COYO dataset. Right column: super-resolution outputs from our SDXL fine-tuned model with correlated noise.", "description": "This figure shows examples of super-resolution results obtained using the proposed Warped Diffusion method. The left column displays the downsampled input images from the COYO dataset, while the right column presents the corresponding super-resolution outputs generated by the fine-tuned Stable Diffusion XL model trained with correlated noise.  The results visually demonstrate the model's ability to enhance the resolution of low-resolution images while maintaining details and sharpness.", "section": "4 Experimental Results"}, {"figure_path": "LH94zPv8cu/figures/figures_23_1.jpg", "caption": "Figure 2: Visualization of Warped Diffusion applied to video super-resolution. (a) We develop a function space diffusion model that super-resolves images given samples from a Gaussian process (GP). To extend the image model to videos, (b) we extract warping transformations between consecutive input frames using optical flow. (c) We use the flow to warp the GP sample from the previous frame. (d) To ensure temporal consistency, we introduce equivariance self-guidance in the ODE sampler.", "description": "This figure visualizes the Warped Diffusion method applied to video super-resolution. It shows four steps: (a) a function space diffusion model is developed to super-resolve images using Gaussian process samples; (b) optical flow extracts warping transformations between consecutive input frames; (c) these transformations warp the Gaussian process sample from the previous frame; and (d) equivariance self-guidance in the ODE sampler ensures temporal consistency. This illustrates the process of extending an image diffusion model to work effectively on videos.", "section": "2 Functional Video Generation"}, {"figure_path": "LH94zPv8cu/figures/figures_24_1.jpg", "caption": "Figure 9: Schematic visualization of Equivariance Self-Guidance (see Algorithm 1).", "description": "This figure provides a schematic overview of the Equivariance Self-Guidance process used in Algorithm 1.  It shows how, for a given frame (frame j), the noise is warped using optical flow, then passed through the diffusion model. The output from this is then used as guidance to ensure that the subsequent frame (frame j+1) maintains temporal consistency.  The process is shown for several steps in the sampling chain, to demonstrate how the self-guidance mechanism improves the temporal coherence of the generated video.", "section": "3 Method: Warped Diffusion"}, {"figure_path": "LH94zPv8cu/figures/figures_24_2.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "This figure shows the self-warping error over time for the inpainting task.  The self-warping error measures the consistency of the model's predictions across different frames as the input is shifted.  Lower values indicate greater temporal consistency. The figure compares several methods, including 'Fixed Noise', 'Resample Noise', 'How I Warped Your Noise', 'GP Noise Warping', and the proposed 'Ours'. The results demonstrate the superior temporal consistency of the proposed 'Ours' method.", "section": "4.2 Noise Warping and Equivariance Self Guidance"}, {"figure_path": "LH94zPv8cu/figures/figures_24_3.jpg", "caption": "Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.", "description": "This figure shows the self-warping error over time for different methods in the inpainting task, where the input frame is shifted at each time step.  The \"self-warping error\" measures the consistency of the model's output across frames, reflecting the temporal coherence of the generated video. Lower values indicate better temporal consistency.  The figure demonstrates that the proposed method (\"Ours\") significantly outperforms the baselines in terms of temporal consistency. ", "section": "4.2 Noise Warping and Equivariance Self Guidance"}]