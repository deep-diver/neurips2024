[{"figure_path": "x33oWJQyH0/figures/figures_2_1.jpg", "caption": "Figure 1: Network architecture. Encoder: (1) an image x is passed through a CNN \u03c8 to obtain n embedding maps e\u2081, ..., en, (2) a maximum of each map is found using softargmax to obtain latent variables [z\u2081,x, z\u2081,y, ..., zn,x, zn,y]. Decoder: (1) Gaussians \u00ea\u2081, ..., \u00ean are rendered at the positions given by the latent variables, (2) the Gaussian maps are concatenated with positional encodings and passed through a CNN \u03c6 to obtain the predicted image \u00ee. Finally, x and \u00ee are used to compute reconstruction loss L(\u00ee, x).", "description": "This figure illustrates the architecture of the proposed unsupervised object detection method.  The encoder uses a CNN (\u03c8) to process the input image (x) and generate embedding maps (e\u2081, ..., en).  A soft argmax function then extracts latent variables (z\u2081,x, z\u2081,y, ..., zn,x, zn,y) representing object positions. The decoder takes these latent variables, renders Gaussians (\u00ea\u2081, ..., \u00ean) at those positions, combines them with positional encodings, and uses another CNN (\u03c6) to reconstruct the predicted image (\u00ee). The reconstruction loss L(\u00ee, x) is used for training.", "section": "Method"}, {"figure_path": "x33oWJQyH0/figures/figures_4_1.jpg", "caption": "Figure 2: Position errors. (a) Maximum position error due to encoder, given by s\u03c8/2 + so/2 \u2013 1. The maximum error occurs when the encoder and the object are as far away from each other as possible while still overlapping by one pixel. (b) Maximum position error due to decoder, given by s\u03c6/2 \u2013 so/2 + \u2206G. The maximum error occurs when some part of the Gaussian at position z + \u2206G is within the decoder receptive field (RF) but is as far away from the rendered object as possible.", "description": "This figure illustrates the maximum position errors that can occur during the encoding and decoding processes. (a) shows the maximum error introduced by the encoder, which happens when the object and the encoder's receptive field are positioned such that they overlap minimally. (b) illustrates the maximum error introduced by the decoder, which occurs when a portion of the Gaussian used to represent the object in the latent space lies within the decoder's receptive field but is maximally distant from the object's center.", "section": "4 Theoretical Results"}, {"figure_path": "x33oWJQyH0/figures/figures_5_1.jpg", "caption": "Figure 2: Position errors. (a) Maximum position error due to encoder, given by s\u03c8/2 + so/2 \u2013 1. The maximum error occurs when the encoder and the object are as far away from each other as possible while still overlapping by one pixel. (b) Maximum position error due to decoder, given by s\u03c6/2 \u2013 so/2 + \u2206G. The maximum error occurs when some part of the Gaussian at position z + \u2206G is within the decoder receptive field (RF) but is as far away from the rendered object as possible.", "description": "This figure shows the maximum possible error in object position estimation due to the limitations of the encoder and decoder.  (a) illustrates the error from the encoder, showing that the maximum error occurs when the receptive field is only partially overlapping with the object, resulting in an error proportional to the sum of half the receptive field and half the object size. (b) illustrates the error from the decoder, showing that it depends on the receptive field size, object size, and the Gaussian rendering variance. The maximum error occurs when the rendered Gaussian is at the edge of the decoder's receptive field, but the center of the object is far from the Gaussian's center.", "section": "4 Theoretical Results"}, {"figure_path": "x33oWJQyH0/figures/figures_6_1.jpg", "caption": "Figure 4: Synthetic experiment results showing position error as a function of the encoder receptive field size s\u03c8, decoder receptive field size s\u03c6, object size so, and Gaussian standard deviation \u03c3g, as the remaining factors are fixed to s\u03c8 = 9, s\u03c6 = 25, so = 9, \u03c3g = 0.8 (in a,b,c) or to s\u03c8 = 9, s\u03c6 = 11, so = 7 (in d). Theoretical bounds are denoted by a blue line (with 4 shaded regions denoting 1 to 4 standard deviations of the probabilistic bound) and experimental results by red dots.", "description": "This figure shows the results of synthetic experiments validating the theoretical bounds on position error derived in the paper.  It demonstrates the maximum position error as a function of four key variables: the encoder receptive field size, the decoder receptive field size, object size, and the standard deviation of the Gaussian used in the rendering process. Each subplot shows the relationship between the theoretical bound (blue line) and the experimental data (red dots) for one of the four variables. The shaded blue regions represent the probabilistic bound within one to four standard deviations of the mean.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "x33oWJQyH0/figures/figures_7_1.jpg", "caption": "Figure 4: Synthetic experiment results showing position error as a function of the encoder receptive field size s\u03c8, decoder receptive field size s\u03c6, object size so, and Gaussian standard deviation \u03c3g, as the remaining factors are fixed to s\u03c8 = 9, s\u03c6 = 25, so = 9, \u03c3g = 0.8 (in a,b,c) or to s\u03c8 = 9, s\u03c6 = 11, so = 7 (in d). Theoretical bounds are denoted by a blue line (with 4 shaded regions denoting 1 to 4 standard deviations of the probabilistic bound) and experimental results by red dots.", "description": "This figure presents the results of synthetic experiments validating the theoretical bounds derived in the paper.  It shows how the maximum position error of the object detection method varies as a function of four key parameters: encoder receptive field size, decoder receptive field size, object size, and Gaussian standard deviation. Each subplot displays the theoretical bound (blue line with shaded regions representing standard deviations) and the experimentally observed position errors (red dots). The consistency between theoretical predictions and experimental results supports the validity of the proposed method.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "x33oWJQyH0/figures/figures_8_1.jpg", "caption": "Figure 6: Proportion of position errors within 2 standard deviations of the theoretical bound (%), reported for different object sizes and methods. Results from table (b) are visualised in plot (a).", "description": "This figure displays the percentage of position errors that fall within two standard deviations of the theoretical bound, categorized by object size and method (Ours, CutLER, SAM).  The plot visually represents the data presented in the accompanying table (6b), showing the percentage of errors within the theoretical bounds for different object sizes (9, 12, 15, 18, 21, 24 pixels). It highlights the consistent high accuracy of the proposed method in contrast to the other two methods, with the proposed method achieving 100% accuracy across all object sizes.", "section": "5.2 CLEVR Experiments"}, {"figure_path": "x33oWJQyH0/figures/figures_8_2.jpg", "caption": "Figure 4: Synthetic experiment results showing position error as a function of the encoder receptive field size s\u03c8, decoder receptive field size s\u03c6, object size so, and Gaussian standard deviation \u03c3g, as the remaining factors are fixed to s\u03c8 = 9, s\u03c6 = 25, so = 9,\u03c3\u03b1 = 0.8 (in a,b,c) or to s\u03c8 = 9, s\u03c6 = 11, so = 7 (in d). Theoretical bounds are denoted by a blue line (with 4 shaded regions denoting 1 to 4 standard deviations of the probabilistic bound) and experimental results by red dots.", "description": "This figure presents the results of synthetic experiments validating the theoretical bounds derived in the paper.  It shows the position error as a function of four key variables: encoder receptive field size, decoder receptive field size, object size, and Gaussian standard deviation. Each subplot displays the experimental data points (red dots) compared to the theoretical bounds (blue line, with shaded regions representing various standard deviations of the probabilistic bound for the decoder error).  The results demonstrate a strong agreement between the theoretical predictions and the experimental findings, validating the accuracy of the theoretical analysis.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "x33oWJQyH0/figures/figures_12_1.jpg", "caption": "Figure 2: Position errors. (a) Maximum position error due to encoder, given by s\u03c8/2 + so/2 \u2013 1. The maximum error occurs when the encoder and the object are as far away from each other as possible while still overlapping by one pixel. (b) Maximum position error due to decoder, given by s\u03c6/2 \u2013 so/2 + \u2206G. The maximum error occurs when some part of the Gaussian at position z + \u2206G is within the decoder receptive field (RF) but is as far away from the rendered object as possible.", "description": "This figure illustrates the maximum position errors that can arise during the encoding and decoding processes in the proposed unsupervised object detection method. (a) shows the maximum error introduced by the encoder, which occurs when the encoder and object are maximally separated while still overlapping. (b) shows the maximum error introduced by the decoder, which occurs when a portion of the Gaussian used for rendering is within the decoder's receptive field but maximally distant from the rendered object.", "section": "4 Theoretical Results"}, {"figure_path": "x33oWJQyH0/figures/figures_13_1.jpg", "caption": "Figure 1: Network architecture. Encoder: (1) an image x is passed through a CNN \u03c8 to obtain n embedding maps e1, ..., en, (2) a maximum of each map is found using softargmax to obtain latent variables z1,x, z1,y, ..., zn,x, zn,y. Decoder: (1) Gaussians \u00ea\u2081, \u2026\u2026\u2026, \u00ean are rendered at the positions given by the latent variables, (2) the Gaussian maps are concatenated with positional encodings and passed through a CNN \u03c6 to obtain the predicted image \u00ee. Finally, x and \u00ee are used to compute reconstruction loss L(\u00ee, x).", "description": "This figure shows the architecture of the proposed unsupervised object detection method. The encoder consists of a convolutional neural network (CNN) followed by a soft argmax function to extract object positions.  The decoder consists of a Gaussian rendering function followed by another CNN to reconstruct an image. The latent variables represent the object positions.", "section": "Method"}, {"figure_path": "x33oWJQyH0/figures/figures_14_1.jpg", "caption": "Figure 4: Synthetic experiment results showing position error as a function of the encoder receptive field size s\u03c8, decoder receptive field size s\u00f8, object size so, and Gaussian standard deviation \u03c3g, as the remaining factors are fixed to s\u03c8 = 9, s\u00f8 = 25, so = 9, \u03c3g = 0.8 (in a,b,c) or to s\u03c8 = 9, s\u00f8 = 11, so = 7 (in d). Theoretical bounds are denoted by a blue line (with 4 shaded regions denoting 1 to 4 standard deviations of the probabilistic bound) and experimental results by red dots.", "description": "This figure shows the results of synthetic experiments to validate the theoretical bounds derived in the paper.  Each subfigure presents the position error plotted against one of the four variables (encoder receptive field, decoder receptive field, object size, and Gaussian standard deviation), while holding the others constant. The blue line represents the theoretical upper bound on the position error, with shaded areas indicating varying levels of uncertainty.  Red dots show the experimentally observed position errors. The close correspondence between the experimental results and theoretical bounds supports the paper's claims.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "x33oWJQyH0/figures/figures_15_1.jpg", "caption": "Figure 1: Network architecture. Encoder: (1) an image x is passed through a CNN \u03c8 to obtain n embedding maps e1, ..., en, (2) a maximum of each map is found using softargmax to obtain latent variables z1,x, z1,y, ..., zn,x, zn,y. Decoder: (1) Gaussians \u00ea\u2081, ..., \u00ean are rendered at the positions given by the latent variables, (2) the Gaussian maps are concatenated with positional encodings and passed through a CNN \u03c6 to obtain the predicted image \u00ee. Finally, x and \u00ee are used to compute reconstruction loss L(\u00ee, x).", "description": "This figure illustrates the architecture of the unsupervised object detection method proposed in the paper. The encoder consists of a CNN followed by a soft argmax function to extract object positions, which are represented as latent variables. The decoder consists of a Gaussian rendering function followed by another CNN to reconstruct the image. The latent variables, representing the object positions, are used to render Gaussian maps at the corresponding positions. These maps, along with positional encodings, are then passed through a CNN to generate the predicted image. Finally, a reconstruction loss is computed between the original image and the predicted image.", "section": "Method"}, {"figure_path": "x33oWJQyH0/figures/figures_15_2.jpg", "caption": "Figure 1: Network architecture. Encoder: (1) an image x is passed through a CNN \u03c8 to obtain n embedding maps e1, ..., en, (2) a maximum of each map is found using softargmax to obtain latent variables z1,x, z1,y, ..., zn,x, zn,y. Decoder: (1) Gaussians \u00ea\u2081, \u2026\u2026\u2026, \u00ean are rendered at the positions given by the latent variables, (2) the Gaussian maps are concatenated with positional encodings and passed through a CNN \u03c6 to obtain the predicted image \u00ee. Finally, x and \u00ee are used to compute reconstruction loss L(\u00ee, x).", "description": "This figure illustrates the architecture of the proposed unsupervised object detection method.  The encoder uses a CNN to process an input image and extract embedding maps. A soft argmax function then identifies the peak locations within these maps, representing the latent variables (object positions). The decoder takes these latent variables, renders Gaussian functions at those locations, and uses another CNN to reconstruct the image from these rendered Gaussians and positional encodings.  The reconstruction loss is calculated between the input and reconstructed images for training.", "section": "Method"}]