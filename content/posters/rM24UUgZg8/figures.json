[{"figure_path": "rM24UUgZg8/figures/figures_1_1.jpg", "caption": "Figure 7: The figure shows the examples of t-SNE results of query-key space and attention results of the baseline and the model employing our solution. The query and key regions are separate in the baseline while our solutions align the regions, enabling the model to focus on salient global features and incorporate self-relations into image features.", "description": "This figure visualizes the impact of the proposed method on query-key space and attention maps.  The left panel (a) shows the baseline model, where query and key regions are distinctly separated, leading to a collapsed attention map. This is indicated by the visualization of the attended keys, which only represent a small region of the image. Conversely, the right panel (b) illustrates the improved model with the proposed method, where query and key regions are better aligned, creating a richer attention map that incorporates more global relationships, as evidenced by the attended keys covering a broader area of the image. The figure highlights the effectiveness of the proposed method in activating self-attention.", "section": "Qualitative Analysis"}, {"figure_path": "rM24UUgZg8/figures/figures_3_1.jpg", "caption": "Figure 2: (a) shows the purity levels in query regions with the baseline on 7Scenes, referring the Eq. 2. Note that the purity is 1.0 when the query region is composed only with queries, but slightly lower than 1.0 when a small subset of keys resides in the query region. According to (a), statistical evidence supports the prevalent occurrence of the blending of a few keys into the query region across the entire dataset, both in the position and orientation transformer encoders. (b) illustrates the increasing tendency of distance between the query region and the key region in the encoder. They lean away each other even at the beginning of the training. Here, the distance between query region and key region is an average value across layers and heads.", "description": "This figure shows the results of a statistical analysis of the query-key space in the transformer encoder of the baseline model.  (a) shows the purity levels of the query regions which indicates how many keys are mixed with the queries. The higher the purity is, the more the region is composed of queries. (b) shows the tendency of the distance between query and key regions during the first epoch of training, showing that they are distanced in the embedding space even before training.", "section": "4 Problem Analysis"}, {"figure_path": "rM24UUgZg8/figures/figures_4_1.jpg", "caption": "Figure 3: The figure shows L2 distances between the top-left token and other tokens based on the fixed 2D sinusoidal positional encoding and learnable positional embedding, respectively. Here, the learnable positional embedding is the result of training with the baseline. The fixed positional encoding preserves the order of input sequences, but in the case of the learnable positional embedding, tokens not aligned at the same height or width were all treated randomly further away.", "description": "This figure visualizes the distances between tokens using fixed and learnable positional embeddings in a transformer encoder for both position and orientation. The fixed embedding maintains positional order, while the learned embedding exhibits randomness, hindering the model's ability to learn geometric relationships.", "section": "4.2 Undertrained Positional Embedding"}, {"figure_path": "rM24UUgZg8/figures/figures_5_1.jpg", "caption": "Figure 4: Fig. 4 illustrates the training pipeline with our solutions. We apply additional objectives \nLQKA, and LQKA to the model to activate the self-attention modules. Specifically, queries Q and\nkeys K interact with each other by forcing the centroid of query region q and the centroid of key\nregion k to become closer. Here, we encode all input queries and keys with fixed 2D sinusoidal\npositional encoding to ensure active interaction between Q and K with reliable positional clues.", "description": "This figure illustrates the training pipeline of the proposed method. It shows how the additional objectives LQKA and LQKA are applied to activate the self-attention modules by aligning query and key regions. Fixed 2D sinusoidal positional encoding is also used to ensure interaction between queries and keys.", "section": "5 Activating Self-Attention for MS-APR"}, {"figure_path": "rM24UUgZg8/figures/figures_8_1.jpg", "caption": "Figure 5: The figure shows the attention entropy of the baseline and ours for each encoder layer. It demonstrates that our solutions significantly improve the utilization of encoder's learning capacity.", "description": "The figure shows a comparison of attention entropy for each layer of the position and orientation transformer encoders between the baseline model and the model with the proposed solutions.  The attention entropy is a metric reflecting the capacity of the self-attention mechanism. Higher entropy indicates better utilization of the self-attention mechanism, and thus better learning capacity. As shown in the plot, our model shows significantly higher attention entropy across all encoder layers, indicating that our approach effectively improves the capacity of the encoder self-attention and enhances the learning capability of the model.", "section": "6.3 Quantitative Analysis"}, {"figure_path": "rM24UUgZg8/figures/figures_8_2.jpg", "caption": "Figure 6: The results show the change of purity of query region P, defined in Eq. 2, across 7Scenes dataset. Note that the purity is 1.0 when the query region is composed only with queries, but slightly lower than 1.0 when few keys resides in the query region. Compared to the baseline, there are no cases where only a few keys are blended into the query region with our solution.", "description": "This figure shows histograms of the purity of the query regions for both position and orientation transformer encoders in the 7Scenes dataset. Purity measures the proportion of queries in the query region.  A purity of 1.0 indicates only queries are present; lower purity indicates keys are also present. The histograms for the model with the proposed solution show a significant shift towards higher purity, indicating that the query and key regions are more effectively interleaved, preventing the collapse of self-attention.", "section": "6.4 Qualitative Analysis"}, {"figure_path": "rM24UUgZg8/figures/figures_9_1.jpg", "caption": "Figure 7: The figure shows the examples of t-SNE results of query-key space and attention results of the baseline and the model employing our solution. The query and key regions are separate in the baseline while our solutions align the regions, enabling the model to focus on salient global features and incorporate self-relations into image features.", "description": "This figure visualizes the impact of the proposed method on the query-key space and attention mechanism using t-SNE. The top row shows the baseline model, where query and key regions are clearly separated. The bottom row shows the model with the proposed solution, demonstrating a significant overlap between query and key regions. This increased interaction between queries and keys allows the model to focus on salient global features and effectively incorporate self-relations into image features, improving performance.", "section": "Qualitative Analysis"}, {"figure_path": "rM24UUgZg8/figures/figures_14_1.jpg", "caption": "Figure A2: The figure shows the t-SNE results of query-key space from the model with our solutions across even layers and heads. The problem we point out is resolved with our solutions; similar query subsets and key subsets are grouped together.", "description": "This figure shows the visualization of the query-key space using t-SNE.  It compares the baseline model's query-key space with the model incorporating the proposed solutions. The baseline model shows distinct separation between query and key regions, whereas the improved model shows that the query and key regions are highly intertwined. This indicates that the proposed solutions successfully address the issue of distorted query-key space, leading to the activation of self-attention mechanism.", "section": "A.2 Visualization"}, {"figure_path": "rM24UUgZg8/figures/figures_15_1.jpg", "caption": "Figure A2: The figure shows the t-SNE results of query-key space from the model with our solutions across even layers and heads. The problem we point out is resolved with our solutions; similar query subsets and key subsets are grouped together.", "description": "This figure shows the visualization of the query-key space using t-SNE for the model with the proposed solutions.  It demonstrates that the problem of separated query and key regions is resolved, and similar subsets of queries and keys are clustered together. This visualization supports the claim that the proposed method effectively activates self-attention by improving the interaction between queries and keys in the embedding space.", "section": "Appendix"}]