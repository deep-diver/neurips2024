[{"type": "text", "text": "Permutree Process ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 This paper presents a Bayesian nonparametric (BNP) method based on an innova  \n2 tive mathematical concept of the permutree, which has recently been introduced in   \n3 the field of combinatorics. Conventionally, combinatorial structures such as permu  \n4 tations, trees, partitions and binary sequences have frequently appeared as building   \n5 blocks of BNP models, and these models have been independently researched and   \n6 developed. However, in practical situations, there are many complicated problems   \n7 that require master craftsmanship to combine these individual models into a single   \n8 giant model. Therefore, a framework for modelling such complex issues in a unified   \n9 manner has continued to be demanded. With this motivation, this paper focuses for   \n10 the first time in the context of machine learning on a tool called the permutree. It   \n11 encompasses permutations, trees, partitions, and binary sequences as its special   \n12 cases, while also allowing for interpolations between them. We exploit the fact that   \n13 permutrees have a one-to-one correspondence with special permutations to propose   \n14 a stochastic process on permutrees, and further propose a data modeling strategy.   \n15 As a significant application, we demonstrate the potential for phylogenetic analysis,   \n16 which involve coalescence, recombination, multiple ancestors, and mutation. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Various combinatorial structures - Permutations, trees, partitions, and binary sequences have been   \n19 frequently utilized in Bayesian modeling, and conventionally, various models have been studied   \n20 separately for each subject. Permutations have been used in a wide range of applications such   \n21 as Bayesian ranking [101, 63, 110, 73], matrix reordering [70, 81, 99], and the traveling salesman   \n22 problem [102, 17, 105]. Various random permutation models, such as the Mallows model [54, 12, 16],   \n23 the permuton models [37, 7, 51, 6] and the modified Chinese restaurant process [57], have been   \n24 employed in Bayesian modeling. Trees are typically used for hierarchical clustering [22, 21] and   \n25 multiple resolution regression [47, 25, 20]. In the Bayesian literature, the Dirichlet diffusion tree [62,   \n26 45], the Mondrian process [88, 87] and the P\u00f3lya tree [56, 48, 27, 15] are particularly popular models.   \n27 Partitions and binary sequences are fundamental tools in machine learning, with numerous examples   \n28 of their usage in clustering, factor analysis, feature selection, and more. For the modeling of partitions   \n29 and binary sequences, the Dirichlet process mixture model [23, 79, 100, 59], the Pitman-Yor process   \n30 mixture model [76], the Chinese restaurant process [74], and the stick-breaking process [89] for   \n31 random partitions, and the beta-Bernoulli process and the Indian buffet process [29, 95, 93] for   \n32 random binary sequences have frequently been employed.   \n33 Combination of different combinatorial structures - In real-world applications of machine learning,   \n34 it is often a useful strategy to combine several different combinatorial structures to model data, rather   \n35 than using only one combinatorial structure. For example, the combination of partitioning and factor   \n36 models is particularly popular, including the infinite factorial hidden Markov models [24, 97], the   \n37 subset infinite relational model [39], the infinite latent factor model with the infinite mixuture model   \n38 [111, 112] and the kernel beta process [83]. As another example, the combination of tree structures   \n39 and partitioning have also been actively studied, including the hierarchical Dirichlet process [94], the   \n40 nested Dirichlet process [84, 64], their hybrid models [2, 71, 50], the infinite context-free grammar   \n41 [49] and the tree-structured stick-breaking process [1, 65]. Furthermore, permutations are occasionally   \n42 employed in conjunction with clustering to analyze relational data [61]. As we have discussed so far,   \n43 this kind of strategy of combining multiple models into a single model is one promising direction for   \n44 research and development. However, advancing research in this direction necessitates the evaluation   \n45 of an enormous number of models in a combinatorial fashion, which becomes infeasible due to the   \n46 exponentially increasing number of potential combinations. Consequently, we are striving to initiate   \n47 a paradigm shift towards exploring an entirely new approach capable of unifying these models.   \n48 Key insight - In our pursuit of creating a unified model capable of encompassing permutations, trees,   \n49 partitions, and binary sequences, we are incorporating the concept of permutrees [75], which has   \n50 recently emerged in the field of combinatorics, into the realm of Bayesian nonparametric (BNP)   \n51 machine learning. Permutrees not only serve as a framework that includes permutations, trees,   \n52 partitions, and binary sequences as distinct cases but also exhibit intriguing properties of interpolation   \n53 between them. Figure 1 (a)-(d) provides a concise visual representation of the key characteristics.   \n54 Our contributions - The main contribution of this paper is to produce, by using the concept of   \n55 permutrees, a stochastic process that can represent combinatorial structures such as permutations,   \n56 binary trees, partitions and binary sequences in a unified manner for the first time. Section 3   \n57 exploits the one-to-one correspondence between permutations and certain permutations using a   \n58 two-dimensional marked point process to construct this process, which we call a permutree process.   \n59 Section 4 derives a data modelling strategy using this stochastic process by analogy with the stick  \n60 breaking process that is frequently used in BNP machine learning. Section 5 demonstrates the   \n61 application of phylogenetic analysis of DNA sequence data dealing with multiple biological events   \n62 such as coalescence, recombination, mutation and multiple ancestry in a unified manner. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "OiTr2v90t7/tmp/ee1518571a9d33b2817c61dce08ac2c116868398e2c6ad5ee324bd793ce3ff56.jpg", "img_caption": ["Figure 1: Overview of new combinatorial structures invented in [75]. Left: A permutree that is a combinatorial object that includes the concepts of permutations, binary trees, clusters, factors, etc., but can also interpolate between them. The permutree is, as defined, a \u201cdirected\u201d tree, but for visibility, the direction of the edges from Parent to Child is omitted in the diagrams. Middle: Variant concepts required to represent stochastic processes on permutrees indirectly through decorated permutations in Section 3. Right: Special cases of permutrees. Remark 2.1 provides details on each interpretation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "63 2 Preliminaries: Permutree and related objects ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "64 Permutree [75] - A permutree is a new mathematical tool invented recently in the field of combi  \n65 natorics, which not only represent permutations, trees, partitions, and binary sequences as special   \n66 cases, but can also interpolate between them [75]. Let us begin with the definition of a permutree.   \n67 We consider a directed tree $\\mathbf{T}$ with a vertex set $\\mathbf{V}$ of $n$ ( $\\mathrm{\\Delta}n\\in\\mathbb{N}$ ) vertices of degree at least 2, and a set   \n68 of terminal nodes of degree 1 (See also Figure 1 (a)). For technical reasons (discussed immediately   \n69 below), we dare to pay particular and explicit attention here to the set $\\mathbf{V}$ of the \u201cinterior vertices\u201d   \n70 (i.e., vertices of degree at least 2) other than the terminal nodes. Each vertex $\\textbf{v}\\in\\textrm{V}$ is assigned   \n71 a natural number $p(\\mathbf{v})$ as a label, using the bijective vertex labeling (one-to-one correspondence)   \n72 $p:\\mathbf{V}\\rightarrow[n]:=\\{\\bar{1},\\bar{2},...\\,,n\\}$ based on the following permutree requirements (Definition 1 in [75]):   \n74 (C2) If a vertex v has a left parent (or child), then all labels in the subtree of the left ancestor   \n75 (or descendant) of $\\mathbf{v}$ are smaller than $p(\\mathbf{v})$ . If $\\mathbf{v}$ has a right parent (or child), then all labels   \n76 in the subtree of the right ancestor (or descendant) of $\\mathbf{v}$ are greater than $p(\\mathbf{v})$ .   \n77 A directed tree $\\mathbf{T}$ that satisfies the above requirements can be expressed more intuitively and clearly   \n78 by introducing the notion of decorations to the vertices $\\mathbf{V}$ . See also Figure 1 (e). We introduce   \n79 the $n$ -tuple decorations $\\delta(\\mathbf{T})\\;:=\\;(\\delta(\\mathbf{T})_{1},\\dots,\\delta(\\mathbf{T})_{n})\\;\\in\\;\\{\\Phi,\\otimes,\\oslash,\\emptyset\\}^{n}$ , defined as follows: (i)   \n80 $\\delta(\\mathbf{T})_{p(\\mathbf{v})}=\\mathbb{\\Phi}$ if $\\mathbf{v}$ has one parent and one child, (ii) $\\delta(\\mathbf{T})_{p(\\mathbf{v})}=\\otimes$ i f $\\mathbf{v}$ has two parents and two   \n81 children, (ii i) $\\delta(\\mathbf{T})_{p(\\mathbf{v})}=\\mathbb{\\oslash}$ if $\\mathbf{v}$ has one parent (lower in Figure  1  (e)) and two children (upper),   \n82 and (iv) $\\delta(\\mathbf{T})_{p(\\mathbf{v})}=\\mathbb{\\varpi}$ if $\\mathbf{v}$ has two parents (lower) and one child (upper). The symbolic feature of   \n83 permutrees can represent various combination objects in a unified manner as follows:   \n84 Remark 2.1. (See Example 4 in $I75J.$ ) Permutation - Permutrees with decoration $\\Phi^{n}$ have a one-to  \n85 one correspondence with permutations of $[n]$ . For example, by reading the horiz o ntal labels in the   \n86 order of the natural number of vertical labels, Figure $^{\\,l}$ (i) represents a permutation 436152. Binary   \n87 tree - Permutrees with decoration $\\mathbb{O}^{n}$ have a one-to-one correspondence with rooted planar binary   \n88 trees on n vertices. See Figure $l\\;(j)$ for an example. Cambrian tree - Permutrees with decoration   \n89 $\\{\\oslash,\\oslash\\}^{n}$ are exactly the Cambrian trees proposed in [82, 13]. See Figure $1\\ (k)$ for an example.   \n90 Bi n a ry sequence - Permutrees with decoration $\\otimes^{n}$ have a one-to-one correspondence with binary   \n91 sequences with length $n-1$ . The ith element of  t he binary sequence is determined according to the   \n92 following procedure: for any $i\\in[n-1].$ , there exists $p(\\mathbf{v})=i$ and $p(\\mathbf{w})=i+1,$ , and if v is the   \n93 parent of w, output 1, otherwise output 0. See Figure 1 (l) for 10010 as an example.   \n94 Now that we have summarized the important property of permutrees, we will describe the findings   \n95 necessary to construct a stochastic process on a permutree, which is the main focus of this paper. As   \n96 a motivation for describing the following findings, imagine actually drawing an instance of permutree   \n97 on a hand-drawn blackboard. At this point, we notice that the horizontal positional relationship of   \n98 vertices $\\mathbf{V}$ is explicitly given by the natural number label $p(\\cdot\\in\\mathbf{V})$ , however, the vertical positional   \n99 relationship is still ambiguous (In Figure 1, (f) is identical to (g) in terms of the permutree, but distinct   \n100 in terms of the increasing tree). Hence, in order to construct a stochastic process on permutrees in   \n101 a concise and clear manner, a mechanism to control the vertical positioning of the vertices of the   \n102 permutrees is required. With this motivation in mind, we introduce two useful notions, an increasing   \n103 tree (Figure 1 (g)) and a leveled permutree (Figure 1 (h)).   \n104 Leveled permutree - To define the leveled permutree, we start by introducing an additional notion   \n105 of an increasing tree. We consider a directed tree $\\mathbf{T}$ with vertex set $\\mathbf{V}$ . Each vertex $\\textbf{v}\\in\\textbf{V}$ is   \n106 assigned a natural number label $q(\\mathbf{v})$ , using the bijective vertex labeling (one-to-one correspondence)   \n107 $q:\\bar{\\mathbf{V}}\\rightarrow[n]$ such that, if $\\mathbf{v}\\in\\mathbf{V}$ is the parent of $\\mathbf{w}\\in\\mathbf{V}$ , then $q(\\mathbf{v})<q(\\mathbf{w})$ is satisfied. Intuitively,   \n108 the function $q$ serves to label the vertices $\\mathbf{V}$ from 1 to $n$ vertically from bottom to top (Figure 1 (g)).   \n109 Then, a leveled permutree is a directed tree $\\mathbf{T}$ with a vertex set $\\mathbf{V}$ endowed with two bijective vertex   \n110 labelings $p,q:\\mathbf{V}\\rightarrow[n]$ which respectively define a permutree and an increasing tree. By using two   \n111 types of labels $p$ and $q$ , the horizontal and vertical arrangement of the vertices $\\mathbf{V}$ can be explicitly   \n112 specified, as shown in Figure 1 (h). The leveled permutree is a useful tool when considering the   \n113 generative model of the permutree in Section 3, because its specification is clear.   \n114 The notion of a leveled permutree so far has improved the prospects for dealing with permutrees.   \n115 However, leveled permutrees are still combinatorial and geometric, and are not yet easy to handle   \n116 computationally (in terms of Bayesian modeling, which is the main objective of this paper). Finally,   \n117 we would like to wrap up this section by revealing one of the most important aspects of leveled   \n118 permutrees: their relationship to decorated permutations.   \n119 Decorated permutation - For the description of decorated permutations, the notion of a permutation   \n120 table should be prepared first. A permutation table is a geometrical representation of a permutation $\\sigma$   \n121 with $n$ length by the $(n\\times n)$ -table, with rows labeled by positions from bottom to top and columns   \n122 labeled by values from left to right, and with a dot at column $i$ and row $\\sigma(i)$ for all $i\\,\\in\\,[n]$ [9].   \n123 Figure 2 (left) shows an example for a permutation 536214. Now that we are ready, we move on to   \n124 the description of a decorated permutation. A decorated permutation is a permutation table where   \n125 each dot is decorated by $\\Phi$ , , $\\oslash$ , or $\\oslash$ . Figure 2 (left bottom) shows an illustration of a decorated   \n126 permutation. One of the important properties of decorated permutations is shown below.   \n127 Proposition 2.2. (See Proposition 8 in $I75J.$ ) There exists one-to-one correspondence between   \n128 decorated permutations with decorations $\\hat{\\delta}\\in\\{\\Phi,\\otimes,\\oslash,\\oslash\\}^{n}$ and leveled permutrees with $\\delta(\\mathbf{T})=\\hat{\\delta}$ .   \n129 Now that we have reviewed the permutree findings, the next and subsequent sections will address   \n130 three challenges: (i) How can we construct a stochastic process that can represent any permutree (in   \n131 Section 3)? (ii) How can we construct a BNP prior model of the data using the stochastic process on   \n132 the permutree (in Section 4)? (iii) What likelihood models can we combine the BNP prior with in   \n133 actual machine learning applications (in Section 5)? ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "OiTr2v90t7/tmp/7d961bb2ba8c799633bafe0954378cd7179f3f0de30fbdbb3b319b5c481112d4.jpg", "img_caption": ["Figure 2: Left: One-to-one correspondence between decorated permutations and leveled permutrees. Right: Permutree process as marked point process - We introduce an intensity function $\\lambda$ on the plane $\\mathrm{[0,1]}\\times\\mathrm{[0,1]}$ (top left). Next, we generate random locations $l_{1},\\ldots,l_{n}$ from the Poisson point process with intensity $\\lambda$ (middle left). Then, for each random location, we independently assign one of the decorations $\\{\\Phi,\\otimes,\\oslash,\\oslash\\}$ from the categorical distribution as a random mark $m_{i}$ $(i=1,\\ldots,n)$ (bottom left). By  r e ad i n g  the positional relationship of the points as a permutation table, the resulting marked point $\\{(l_{i},\\bar{m_{i}}):i=1,2,\\dots n\\}$ can be converted to a decorated permutation. Furthermore, by the transformation used in Proposition 2.2 [75], the decorated permutation can be converted to a leveled permutree, as follows. First, we draw auxiliary lines (dashed lines colored red) below decorations $\\otimes$ , $\\oslash$ and above decorations $\\otimes$ , $\\oslash$ . From this point on, we will stretch the permutree edges, and it is  i m p ortant to emphasize tha t  th e  permutree edges do not cross these auxiliary lines. Next, focusing on the auxiliary lines extending to the bottom, we can view these as dividing the lower region into smaller subregions (indicated by the red ovals). The edges are then extended one by one from each subregion. As we extend the edges from the bottom to the top, when they reach the height of each vertex, we connect the adjacent edges to that vertex (indicated by the gray box). By doing this until all vertices are covered, we obtain a leveled permutree. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "134 3 Permutree processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "135 The goal of this section is to construct a stochastic process that can represent any permutree; ideally, as   \n136 is the basic philosophy of BNP, that stochastic process should also be able to simultaneously represent   \n137 randomness with respect to complexity (in the context of permutrees, the number of vertices). In fact,   \n138 our construction below can represent every permutree with an unlimited number of finite or infinite   \n139 number of vertices in a unified manner, depending on certain hyperparameters. One thing to note in   \n140 advance is that the stochastic process described in this section does not refer to any modeling of data.   \n141 We will discuss data modeling in more detail in the next Section 4.   \n142 Key insight - Our strategy is to use point processes. Recall that, as discussed in Section 2, permutrees   \n143 can be represented through leveled permutrees (surjection), and furthermore, leveled permutrees have   \n144 a one-to-one correspondence (bijection) with decorated permutations (Proposition 2.2). Thanks to   \n145 these facts, instead of dealing directly with permutrees (seemingly difficult to handle), we can obtain   \n146 a model of permutrees indirectly by considering a model of decorated permutations. So how can we   \n147 model decorated permutations? We represent the random decorated permutations as a marked point   \n148 process by considering random permutations as a point process and random decorations as marks.   \n149 Marked point process for decorated permutations - We consider a marked point process consisting   \n150 of a point process and associated marks, which can be expressed as $\\{(l_{i},m_{i}):i=1,2,\\ldots\\}$ , where   \n151 $\\begin{array}{r}{\\boldsymbol{l}_{1},\\boldsymbol{l}_{2},\\ldots}\\end{array}$ are locations and $m_{1},m_{2},\\ldots$ . are associated marks. Specifically, we employ the following   \n152 Poisson process on a 2-dimensional plane $[0,1]\\times[0,1]$ with discrete marks (Figure 2 right):   \n153 \u2022 Random locations - We draw the random locations $\\begin{array}{r}{l_{1},l_{2},\\ldots-l_{1},}\\end{array}$ from a Poisson point process on   \n154 the plane $[0,1]\\times[0,1]$ with the intensity function $\\lambda:[0,1]\\times[0,1]\\to\\mathbb{R}^{+}$ , where $\\mathbb{R}^{+}=\\{r:$   \n155 $r>0,r\\in\\mathbb{R}\\}$ . Although not essential, for the sake of simplicity, we use a homogeneous Poisson   \n156 point process, that is, $\\lambda(A)=\\mu\\cdot\\operatorname{Leb}(A)$ for all measurable subset $A$ of $[0,1]\\times[0,1]$ , where   \n157 $\\operatorname{Leb}(\\cdot)$ indicates the Lebesgue measure, and $0<\\mu<\\infty$ is a tunable variable. For convenience,   \n158 let $\\mathbf{\\boldsymbol{l}}_{i}=(l_{i,1},l_{i,2})$ , where $l_{i,1}$ and $l_{i,2}$ are the horizontal and vertical positions, respectively.   \n159 \u2022 Random marks - We draw the random marks $m_{1},m_{2},\\ldots,m_{n}$ independently from a categorical   \n160 distribution on $\\{\\Phi,\\otimes,\\oslash,\\oslash\\}$ : Categorica $(c_{\\mathbb{D}},c_{\\otimes},c_{\\emptyset},c_{\\emptyset})$ , where $c_{*}~\\geq~0$ $\\displaystyle\\left(\\ast\\,\\in\\,\\{\\Phi,\\otimes,\\bar{\\Theta},\\varpi\\}\\right)$   \n161 denotes the proba bil i ty  t ha t  decoration $^*$ is ad opte d.   \n162 Transformation to leveled permutree - The above marked point process can immediately lead to a   \n163 random leveled permutree with the following procedure. Recall that, as discribed in Section 2, the   \n164 leveled permutree is defined by (i) the decorations on the vertices $\\mathbf{V}$ and (ii) the two bijective vertex   \n165 labelings $p,q:\\mathbf{V}\\rightarrow[n]$ . For the decoration of vertices, we consider the point set of the marked   \n166 point process as the vertex set $\\mathbf{V}$ , and the mark $m_{i}$ assigned to the $i^{\\th}$ -th point as the decoration of   \n167 the $i$ -th vertex $\\mathbf{v}_{i}\\in\\mathbf{V}$ . Thus, the remainder to be considered is the setting of two functions $p$ and   \n168 $q$ . By construction, we can obtain the indices $a_{1},\\ldots,a_{n}$ so that the random positions $l_{1},\\ldots,l_{n}$   \n169 are in ascending order in the horizontal direction, that is, $l_{a_{1},1}<l_{a_{2},1}<\\cdot\\cdot<l_{a_{n},1}$ (Recall that   \n170 $\\boldsymbol{l}_{i}=(l_{i,1},l_{i,2})$ , and $l_{i,1}$ represents the horizontal position). Similarly, in the vertical direction, we can   \n171 obtain the indices $b_{1},\\ldots,b_{n}$ so that $l_{b_{1},2}<l_{b_{2},2}<\\cdot\\cdot<l_{b_{n},2}$ . Now, if we choose to set $p(\\mathbf{v}_{a_{i}})=i$   \n172 and $q(\\mathbf{v}_{b_{i}})=i$ for $i=1,2,\\dots,n$ , then $p$ and $q$ satisfy the requirement of bijective functions. By the   \n173 above, we have seen that indeed the marked point process provides us with what we need to define a   \n174 leveled permutree, that is, the vertex decorations and two bijective functions $p$ and $q$ . Finally, Figure   \n175 2 (right) show the procedure for explicitly converting a marked point process to a random leveled   \n176 permutree. Inheriting Proposition 2.2 and the result (with the proof procedure) of [75, Proposition 8],   \n177 we can confirm that this transformation is well defined (See Appendix E for details). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "178 4 Data modeling with permutree process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "179 The purpose of this section is to show how the permutree process described earlier can be used for   \n180 modeling actual data. More specifically, this consists of the following two issues:   \n181 \u2022 How to represent data using permutrees: As permutrees themselves are simply mathematical   \n182 objects, we must be clear about how we relate them to data modeling and analysis. In fact,   \n183 there are many possible ways to describe data by permutrees. We consider the situation where   \n184 a data path (a lineage to describe the data in conjunction with some likelihood model, such as   \n185 the evolutionary model in Section 5) from one of the lower terminal nodes to one of the upper   \n186 terminal nodes on the permutree is assigned to each data (Figure 3 (a), top). For example, if we   \n187 restrict the permutree to one of its special cases, the binary tree, this data path is attributed to   \n188 the path from the root to the terminal node, which is a situation commonly used in hierarchical   \n189 clustering (Figure 3 (a), bottom).1 We show a strategy to represent this random data path using a   \n190 special variant of the nested Chinese restaurant process [10].   \n191 \u2022 How to \u201cimplement\u201d a permutree process: In the previous section, we have shown that a marked   \n192 point process with an intensity function $\\lambda:[0,1]\\times[0,1]\\rightarrow\\mathbb{R}^{+}$ can be used for the stochastic   \n193 process on permutrees. On the other hand, another important topic is to clarify how to implement   \n194 models (or more practically, what intensity function $\\lambda$ to use) suitable for data analysis. Our   \n195 strategy is to use the analogy of the stick-breaking process [89] to represent the infinite number   \n196 of marked points generated from the marked point process, which is the entity of the permutree   \n197 process. This can be viewed as a special case of using beta intensity in the horizontal direction and   \n198 uniform intensity in the vertical direction as the intensity function $\\lambda$ of the permutree process.   \n199 Experts in the BNP field might remind themselves that there are many other strategy options for the   \n200 above topics in the light of the various findings that have emerged in the history of the development   \n201 of the BNP method over the last 20 years. We will, for the sake of space, summarize in Appendix D   \n202 the various ideas and their respective advantages and disadvantages with respect to those historical   \n203 findings, including whether it is possible to extend the conventional tool of the \u201cordered\u201d Chinese   \n204 restaurant process [77, 85] for random binary trees to random permutrees. The main body of this   \n205 paper focuses on the most straighforward strategy. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "OiTr2v90t7/tmp/1d56e97dfe51b142fd1fcdc007085f5513a0f902dc3856a830ad9cfd522195f8.jpg", "img_caption": ["", ""], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: (a) Data path - We consider each data as having a data path (a lineage to describe the data in conjunction with some likelihood model) from one of the lower terminal nodes to one of the upper terminal nodes on the permutree (top). This will convince us of its generality and applicability, as it is attributed to the hierarchical clustering from one of the terminal nodes to the root when restricting the permutree to the special case of a binary tree (bottom). (b) Marked stick breaking process - Inspired by the stick-breaking representation [89] for the construction of Dirichlet processes [23], in order to represent a random permutree of infinite size, we can represent the random vertex positions of the permutree by the stick-breaking process in the horizontal direction and uniform random measures in the vertical direction. (c) Two-table Chinese restaurant process (Variant of two-class Dirichlet allocation) - The data allocated to the lower terminal nodes are successively merged and distributed depending on the mark of each vertex, according to the law of the \u2019the rich get richer\u2019, to select paths. ", "page_idx": 5}, {"type": "text", "text": "206 Permutree of infinite size - We first generate random positions $l_{1},l_{2},.\\dotsc,l_{k}=(l_{k,1},l_{k,2}),.\\dotsc$ in   \n207 the point process of the permutree process, as shown in Figure 3 (b), using the stick-breaking process: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{k}\\sim\\operatorname{Beta}(1,\\alpha),\\quad l_{k,1}=\\sum_{i=1}^{k}\\Big\\{\\beta_{i}\\prod_{i^{\\prime}=1}^{i-1}(1-\\beta_{i^{\\prime}})\\Big\\},\\quad l_{k,2}\\sim\\operatorname{Uniform}([0,1]),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "208 where $\\alpha>0$ is the concentration parameter. As in the original permutree process, each point mark   \n209 $m_{k}$ $(k=1,2,\\dots)$ is generated from a categorical distribution: $m_{k}\\sim\\mathrm{Categorical}(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset})$ .   \n210 As mentioned earlier, by the procedure in Figure 2 (right), we can transform this sampl e (i. e., a  set  of   \n211 infinite number of marked points) drawn from the permutree process into a uniquely single permutree.   \n212 Data assignments to bottom terminal nodes - Next, we can represent data modeling by the paint  \n213 box scheme for the random permutree generated from the marked stick-breaking process described   \n214 earlier. We associate one uniform random variable $U_{j}$ for each data indexed by $j\\,=\\,1,2,\\dots,N$   \n215 $(N\\in\\mathbb{N})$ : $U_{j}\\sim\\mathrm{Uniform}([0,1])$ . Similar to Kingman\u2019s representation to the exchangeable partitions,   \n216 called paintbox schemes [44, 11], we choose which terminal node on the lower edge of the permutree   \n217 to assign the $j$ th data to, depending on which stick in the stick-breaking process this random variable   \n218 $U_{j}$ is located on $[0,1]$ , as shown in Figure 3 (b).   \n219 Data path modeling - Finally, we model the path assignment for each data by choosing a path that   \n220 starts at this assigned lower terminal node and reaches one of the upper terminal nodes through the   \n221 following two-table Chinese restaurant process (i.e., variant of two-class Dirichlet allocation):   \n22 \u2022 $\\oslash$ - We break up the set of data flowing in, following the left-right table-assignement operation   \n23 b elow2: the first data is chosen uniformly at random from either the left or the right table. For the   \n24 nth data, the left table is chosen with probability $(\\mathcal{N}_{\\mathrm{Left}}+\\gamma/2)/(n+\\gamma)$ and the right table with   \n25 probability $(\\mathcal{N}_{\\mathrm{Right}}+\\gamma/2)/(n+\\gamma)$ , where $\\gamma>0$ is a hyperparameter, and ${\\mathcal{N}}_{\\mathrm{Left}}$ and $\\mathcal{N}_{\\mathrm{Right}}$   \n26 are the number of data allocated so far to the left and right tables respectively.   \n27 \u2022 $\\ensuremath{\\mathbb{O}}-\\ensuremath{\\mathbb{W}}\\ensuremath{\\mathbf{e}}$ merge the sets of data flowing from the two lower branches and feed them into the upper.   \n28 \u2022 $\\otimes$ - It would be straightforward to perform operations whose marks are $\\oslash$ and $\\oslash$ together.   \n29 A nother promising option is the representation of data flowing from the le ft  paren t  to the left   \n30 child and from the right parent to the right child. This can be interpreted as giving the mark $\\otimes$   \n31 the ability to partitioning. This interpretation also plays an important role in the validity of fin it e   \n32 truncation, which will be discussed below.   \n234 For notational simplicity, we will denote the random variable for the $j$ th data path by $Z_{j}$ . For a   \n235 sample $z$ of data paths between the upper and lower terminal nodes of the permutree (specified by a   \n236 sequence of edges), the above generative probabilistic model allows us to evaluate the probability   \n237 $\\mathbb{P}[\\bar{Z}_{j}=z]$ of the $j$ th data choosing a data path sample $z$ .   \n238 Property #1: Exchangeability - Random data paths based on the generative probability model   \n239 described above have exchangeability, an important property common to most BNP models [3, 36, 41].   \n240 Simply put, the model probability is invariant to the indexing of the data. As a result, it follows the   \n241 philosophy of BNP models that even if the actual data to be observed is finite, the model itself, with   \n242 infinite complexity, can reflect the uncertainty due to unobserved data. More specifically, this can be   \n243 summarised as the following statement:   \n244 Proposition 4.1 (Exchangeability). For any permutation $\\sigma$ of length $N$ $[N\\in\\mathbb{N})$ , we have $\\mathbb{P}[Z_{1}=$   \n245 $z_{1},\\bar{Z}_{2}=z_{2},\\ldots,Z_{N}=\\bar{z_{N}}]=\\bar{\\mathbb{P}}[Z_{\\sigma(1)}=\\bar{z_{1}},Z_{\\sigma(2)}=z_{2},\\ldots,\\bar{Z_{\\sigma(N)}}=\\bar{z_{N}}],$ where $z_{j}$ $(j\\in[N])$ is   \n246 a sample of paths of random permutrees. (See Appendix A.1 for proof.)   \n247 Property #2: Validity of finite truncation - The above generative probability model requires in   \n248 principle an infinite number of random variables for its description, but finite truncation works   \n249 reasonably well for a finite number of actual observed data. This poses an inherently non-trivial   \n250 challenge that is not present in the validity of approximating the stick-breaking process [89] for the   \n251 Dirichlet process [23] with a finite number of stick-breaking procedures, which is a typical topic   \n252 in the past [94, 87, 67]. The reason for this non-triviality is that the substructure of a permutree   \n253 with infinite size is, in principle, affected by an infinite numnber of all marked vertices. Therefore,   \n254 restricting the structure of the permutree to only some marked vertices may have a significant impact   \n255 on the structure of the permutree. However, as the following statement shows, the substructure of the   \n256 permutree has the good property that it depends only on a subset of marked vertices.   \n257 Proposition 4.2 (Finite truncation). In the above generative probability model of data indexed by   \n258 $j\\,=\\,1,2,\\dots,N$ ( $N\\,\\in\\,\\mathbb{N}$ ), we consider an event that all random variables $U_{j}$ $(j\\,=\\,1,\\ldots,N)$ ,   \n259 representing the horizontal position of the jth data, falls in the range $[0,1-\\epsilon)$ as a situation with $a$   \n260 sufficiently high probability $\\begin{array}{r}{\\mathbb{P}[\\wedge_{j=1}^{N}0\\leq U_{j}<1-\\epsilon]=\\prod_{j=1}^{N}\\mathbb{P}[0\\leq U_{j}<1-\\epsilon]>1-\\epsilon\\cdot\\mathcal{O}(N)}\\end{array}$ ,   \n261 where $\\epsilon>0$ is a tiny real value. In this situation, there exists some natural number $K<\\infty$ , and   \n262 all data paths are assigned with probability 1 only to paths on the finite-size random permutree   \n263 generated from the random marked points $l_{1},l_{2},\\ldots,l_{K}$ . (See Appendix A.2 for proof.) ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "264 5 Application to phylogenetic permutree analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "265 This section presents an application example of using the prior model representation of data using   \n266 permutrees, which has been described in Section 4, in conjunction with a likelihood model in a   \n267 specific application. One of the most promising applications of permutrees would be phylogenetic   \n268 tree analysis for DNA molecular sequence data (e.g., CAGTC). DNA sequences from one or more   \n269 populations are related by a branching structure known as genealogy. The complex correlative   \n270 structure of a collection of DNA sequences can be represented as a phylogenetic tree, a record of   \n271 coalescence, recombination, and mutation events in the history of the target organism: coalescence   \n272 refers to the event in which two sequences are attributed to a common ancestor, recombination refers   \n273 to the event in which a lineage splits into two sub-lineages when looking back in time from the   \n274 present to the past, and mutation refers to the change of each letter of a DNA sequence over time.   \n275 Challenges of conventional methods - The most standard structure that has been used in phylogenetic   \n276 analysis is the binary tree [66, 78, 58, 103, 98, 113, 107, 106, 60]. In fact, binary trees are very   \n277 well suited to represent coalescence events in genealogy. However, one drawback of binary tree   \n278 models is that they are not suitable for representing recombination events in a way that is compatible   \n279 with coalescence events. To circumvent this drawback, the ancestral recombination graphs (ARGs)   \n280 have sometimes been used as models that can represent both coalescence and recombination at the   \n281 same time [52, 42, 80, 72, 90, 28]. However, it is not easy to model or infer ARGs directly, and   \n282 often indirect ways of representing models by other perspectives (e.g., the fragmentation-coagulation   \n283 process [92, 19]) have been explored, or approximate models (e.g., the coalescent hidden Markov   \n284 model [34, 53] and the sequentially Markov coalescent model [80]) have been considered. Moreover,   \n285 conventional phylogenetic tree analysis, including not only ARGs but also binary tree models,   \n286 generally imposes a strong assumption that observed DNA sequences or observed taxa have a   \n287 single ancestor. In other words, this implies that the inferred phylogenetic tree should be a strongly   \n288 connected graph. Needless to say, such an assumption is reasonable for taxa that have been carefully   \n289 selected by biologists. On the other hand, when we want to use a large number of taxa that are too   \n290 large to be selected by experts as observation data (i.e., the situation that BNP methods are really   \n291 aiming for), a mechanism that allows multiple ancestors to be inferred in a data-driven manner will   \n292 be very useful. In light of the above, phylogenetic tree analysis requires a model that can represent   \n293 coalescence, recombination, multiple ancestors, and mutation in a unified manner.   \n294 Phylogenetic permutree - As input observation data, we used DNA (molecular) sequences observed   \n295 at letter length $S$ over $N$ species. For example, the sequence GAGTAC (i.e., $N=1$ species) has   \n296 length $S=6$ . We regard these DNA sequences as following a phylogenetic permutree. Specifically,   \n297 we represent coalescence, recombination, multiple ancestry, and mutation events in genealogy by   \n298 combining the four types of the decorations $\\mathbb{O},\\mathbb{O},\\otimes,\\mathbb{O}$ with the following interpretations. We note   \n299 that, to be consistent with the traditional no ta ti o n  o f  p hylogenetic tree analysis, the past (upper) to   \n300 present (lower) direction as biological events is the opposite of the parent (lower) to child (upper)   \n301 direction of the permutree as a purely mathematical object that we have used in the diagrams so far.   \n302 \u2022 Coalescence $\\mathbb{O}-\\mathbf{A}$ coalescence event represents two lineages (bottom side of Figure 4 (b))   \n303 having a com m on ancestral lineage (top side).   \n304 \u2022 Recombination $\\oslash$ - A recombination event represents the joining of two exclusive sub  \n305 sequences of two  l ineages (top side of Figure 4 (c)) by one lineage (bottom).   \n306 \u2022 Partition $\\otimes$ - We give the decoration $\\otimes$ the role of division so that a single permutree can   \n307 represent  a  phylogenetic tree with mul ti ple ancestors. Specifically, as shown in Figure 4 (d),   \n308 we connect the two left edges and connect the two right edges resulting in two tree structures   \n309 unconnected to each other on either side of decoration $\\otimes$ .   \n310 \u2022 Backward in time $\\Phi$ (optional) - We assume that no mutation occurs while going back in time   \n311 from a vertex to a  v ertex with $\\Phi$ (Figure 4 (e)). This allows us to set the mutation rate in the   \n312 evolutionary model as a single  p arameter common to all branches, and the mutation rate can be   \n313 adjusted according to the permutree itself.   \n314 Evolutionary models on permutrees - Statistical models of gene mutation have a history of more   \n315 than half a century, and a vast number of models have been proposed. An excellent recent review   \n316 article can be found, for example, in [4]. For simplicity, we adopt two of the most popular models,   \n317 the Jukes-Cantor model (JC) [40] and the generalized time reversible model (GTR) [91], for DNA   \n318 sequences (i.e., words with A, G, C, and $\\intercal$ as letters of the alphabet $\\{\\mathsf{A},\\mathsf{G},\\mathsf{C},\\mathsf{T}\\}$ , such as CCTAAG).   \n319 JC is defined as a Markov process in which (1) all letters are independently generated from a uniform   \n320 categorical distribution on $\\{\\mathsf{A},\\mathsf{G},\\mathsf{C},\\mathsf{T}\\}$ as initialization and (2) one letter (e.g., A) changes to another   \n321 letter (e.g., $\\sf G)$ after $t$ seconds with probability $(1\\!-\\!\\exp(-4\\alpha t))/4$ or does not mutate with probability   \n322 $(1+3\\exp(-4\\alpha t))/4$ , where $\\alpha\\left(>0\\right)$ is a hyperparameter representing the mutation rate. Simply put,   \n323 JC means that the transition probabilities of letters in mutation are fixed. GTR, on the other hand, can   \n324 be regarded as a more flexible version of the JC model, in which the letter transition probabilities   \n325 themselves are also estimated from the data as hidden parameters.   \n26 Demonstration - We use the following three benchmark datasets [60] for DNA sequences: RMPS   \n27 $N\\,=\\,64$ species, $S\\,=\\,1008$ length) [86], HWB ( $N\\,=\\,41$ , $S\\,=\\,1137$ ) [33], and ZB ( $N\\,=\\,50$ ,   \n28 $S=1133)$ ) [108]. In addition, to establish a situation where the permutree notion would be useful   \n29 (i.e., multiple ancestry derived from exclusive disconnected graphs), we extract the sequences of   \n30 these datasets by $S\\,=\\,1000$ length from the beginning and mix them to create a dataset we call   \n31 COMB $\\langle N=155\\rangle$ ). We use the marked stick-breaking process (referred to as MSBP; Section 4)   \n32 as our proposed model. Since MSBP can easily adjust the representational capabilities of of its   \n33 own model, as ablation studies, we use MSBP-bTree as the one restricted to binary trees (with the   \n34 prior $(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset})\\sim\\mathrm{Dirichlet}(\\epsilon/2,0,0,\\epsilon/2))$ , MSBP-cTree as the one restricted to Cambrian   \n35 trees (w ith  Diri chle t $\\left.\\dot{(\\epsilon/3,0,\\epsilon/3,\\epsilon/3)}\\right)$ , and MSBP-pTree as the main proposal permutrees (with   \n36 Dirichlet $\\left.\\cdot(\\epsilon/4,\\epsilon/4,\\epsilon/4,\\epsilon/4,\\epsilon/4)\\right\\}$ ), where we set $\\epsilon\\:=\\:0.01$ . For the evolutionary model, we employ   \n37 the mutation rate $\\alpha\\,\\sim\\,\\mathrm{Gamma}(\\epsilon^{\\prime},\\epsilon^{\\prime})$ , where $\\epsilon^{\\prime}\\,=\\,0.1$ . We only present the case of $K\\,=\\,100$   \n38 as the truncation level here, while we report the other cases in Appendix C. We compare these   \n39 models to the hierarchical Dirichlet process hidden Markov model (HDPHMM) [8, 94, 5], the   \n40 fragmentation-coagulation process (FCP) [92], and the binary tree model with the MrBayes [38, 46],   \n41 the probabilistic path Hamilton Monte Carlo (ppHMC) [18], and the nested combinatorial sequential   \n42 Monte Carlo (ncSMC) [60]. It is noted that HDPHMM and FCP do not use evolutionary models   \n43 because they represent sequence data directly without tree structure. We held out $20\\%$ letters of the   \n44 input sequences for testing, and each model was trained using the remaining $80\\%$ of the letters. Each   \n45 inference method uses MCMC to estimate the posterior distribution by the following 100 samples:   \n46 each method extracts 5 MCMC runs with different random numbers, and each MCMC run is sampled   \n47 every 50 iterations after 2000 burn-in until 3000 iterations. We evaluate the models using perplexity   \n48 as a criterion: perplexity $(\\cdot)=\\exp(-(\\log p(\\cdot))/E)$ , where $E$ is the number of missing letters in the   \n49 input sequences. Figure 5 shows the comparison of the prediction performance of each method for   \n50 the four sets of data. As an overall trend, it can be seen that the Cambrian tree and permutree models   \n51 show better prediction performance than the binary tree model, which has limited expressive power. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "OiTr2v90t7/tmp/0f3f724a2bd21f121503fe07d43cb1454ffada944ef62c3f05e409da486bc033.jpg", "img_caption": ["Figure 4: (a) Phylogenetic permutree can simultaneously and unifiedly represent (b) coalescence, (c) recombination, multiple ancestry through (d) partition, and (e) mutation. We note that the past (upper) to present (lower) direction (indicated by $\\downarrow$ ) as biological events is the opposite of the parent (lower) to child (upper) direction (indicated by $\\uparrow$ ) of the permutree as a purely mathematical object. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "OiTr2v90t7/tmp/470b58a8e30149132ec68588b5b6a49101e21660b78afa5bb6ff896a0e3659d1.jpg", "img_caption": ["Figure 5: Experimental results of test perplexity (mean\u00b1std) comparison for real-world data. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "352 6 Discussion and limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "353 This paper (i) imports the notion of permutrees, recently invented in combinatorics, to Bayesian   \n354 analysis, (ii) proposes the stochastic process that can represent various models such as permutations,   \n355 trees, partitions, and factors in a unified manner, (iii) and applies it to phylogenetic permutree analysis.   \n356 Limitations - While our proposed permutree process can represent various combinatorial structures   \n357 in a unified \u201cprior model,\u201d the likelihood model that describes the data (as we have shown in the   \n358 context of phylogenetic tree analysis in Section 5, for example) must be prepared separately by the   \n359 user or engineer. Thus, while the permutree process is a tool that allows data-driven inference of the   \n360 model structure as a broad framework, the design of the likelihood model needs to be carefully done   \n361 manually. In the near future, the exploration of representing this likelihood model in some kind of   \n362 black box function model would be an important research direction.   \n363 Remaining challenge - In the technical context of the BNP field, an important topic is whether a   \n364 marginalized representation of the marked stick-breaking process, an infinite-dimensional intermedi  \n365 ate random variable in the representation of data paths with exchangeability described in Section 4,   \n366 can be obtained. This topic is a question closely related to the Aldous-Hoover-Kallenberg representa  \n367 tion theorem for exchangeability in general [3, 36, 41]. As a more familiar analogy, it corresponds   \n368 to the fact that if we marginalise the stick-breaking process representation in a Dirichlet process   \n369 infinite mixture model, then we obtain the Chinese restaurant process representation. Our strategy   \n370 and budding attempts on this question are summarized in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "371 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "372 [1] Ryan Prescott Adams, Zoubin Ghahramani, and Michael I. Jordan. Tree-structured stick breaking for   \n373 hierarchical data. In Advances in Neural Information Processing Systems, pages 19\u201327, 2010.   \n374 [2] Priyanka Agrawal, Lavanya Sita Tekumalla, and Indrajit Bhattacharya. Nested hierarchical Dirichlet   \n375 process for nonparametric entity-topic analysis. In Machine Learning and Knowledge Discovery in   \n376 Databases, volume 8189, pages 564\u2013579, 2013.   \n377 [3] David J. Aldous. Representations for partially exchangeable arrays of random variables. Journal of   \n378 Multivariate Analysis, 11:581\u2013598, 1981.   \n379 [4] Miguel Arenas. Trends in substitution models of molecular evolution. Frontiers in Genetics, 6, 2015.   \n380 [5] Marius Bartcus, Faicel Chamroukhi, and Herv\u00e9 Glotin. Hierarchical Dirichlet process hidden Markov   \n381 model for unsupervised bioacoustic analysis. In 2015 International Joint Conference on Neural Networks,   \n382 pages 1\u20137. IEEE, 2015.   \n383 [6] Fr\u00e9d\u00e9rique Bassino, Mathilde Bouvel, Valentin F\u00e9ray, Lucas Gerin, Micka\u00ebl Maazoun, and Adeline   \n384 Pierrot. Universal limits of substitution-closed permutation classes. Journal of the European Mathematical   \n385 Society, 22(11):3565\u20133639, 2019.   \n386 [7] Fr\u00e9d\u00e9rique Bassino, Mathilde Bouvel, Valentin F\u00e9ray, Lucas Gerin, and Adeline Pierrot. The Brownian   \n387 limit of separable permutations. The Annals of Probability, 46(4):2134\u2014-2189, 2018.   \n388 [8] Matthew J. Beal, Zoubin Ghahramani, and Carl Edward Rasmussen. The infinite hidden Markov model.   \n389 In Advances in Neural Information Processing Systems 14, pages 577\u2013584. MIT Press, 2001.   \n390 [9] Anders Bj\u00f6rner and Michelle L. Wachs. Permutation statistics and linear extensions of posets. Journal of   \n391 Combinatorial Theory, Series A, 58(1):85\u2013114, 1991.   \n392 [10] D. M. Blei, M. I. Jordan, T. L. Grifftihs, and J. B. Tenenbaum. Hierarchical topic models and the nested   \n393 Chinese restaurant process. pages 17\u201324, 2003.   \n394 [11] Tamara Broderick, Jim Pitman, and Michael I. Jordan. Feature allocations, probability functions, and   \n395 paintboxes. Bayesian Analysis, 8(4):801 \u2013 836, 2013.   \n396 [12] R\u00f3bert Busa-Fekete, Dimitris Fotakis, Bal\u00e1zs Sz\u00f6r\u00e9nyi, and Emmanouil Zampetakis. Identity testing for   \n397 Mallows model. In Advances in Neural Information Processing Systems, pages 23179\u201323190, 2021.   \n398 [13] Gr\u00e9gory Chatel and Vincent Pilaud. Cambrian Hopf algebras. arXiv:1411.3704, 2014.   \n399 [14] Anand Vir Singh Chauhan, Shivshankar Reddy, Maneet Singh, Karamjit Singh, and Tanmoy Bhowmik.   \n400 Deviation-based marked temporal point process for marker prediction. In Machine Learning and   \n401 Knowledge Discovery in Databases. Research Track, volume 12975, pages 289\u2013304, 2021.   \n402 [15] William Cipolli and Timothy Hanson. Supervised learning via smoothed P\u00f3lya trees. Advances in Data   \n403 Analysis and Classification, 13(4):877\u2013904, 2019.   \n404 [16] Fabien Collas and Ekhine Irurozki. Concentric mixtures of Mallows models for top-k rankings: sampling   \n405 and identifiability. In International Conference on Machine Learning, volume 139, pages 2079\u20132088,   \n406 2021.   \n407 [17] Tiago Tiburcio da Silva, Ant\u00f4nio Augusto Chaves, Horacio Hideki Yanasse, and Henrique Pacca Loureiro   \n408 Luna. The multicommodity traveling salesman problem with priority prizes: a mathematical model and   \n409 metaheuristics. Computational and Applied Mathematics, 38(4), 2019.   \n410 [18] Vu Dinh, Arman Bilge, Cheng Zhang, and Frederick A. Matsen IV. Probabilistic path hamiltonian monte   \n411 carlo. In International Conference on Machine Learning, volume 70, pages 1009\u20131018, 2017.   \n412 [19] Lloyd T. Elliott and Yee Whye Teh. Scalable imputation of genetic data with a discrete fragmentation  \n413 coagulation process. In Advances in Neural Information Processing Systems, pages 2861\u20132869, 2012.   \n414 [20] Xuhui Fan, Bin Li, Ling Luo, and Scott A. Sisson. Bayesian nonparametric space partitions: A survey. In   \n415 International Joint Conference on Artificial Intelligence, pages 4408\u20134415, 2021.   \n416 [21] Xuhui Fan, Bin Li, and Scott A. Sisson. The binary space partitioning-tree process. In International   \n417 Conference on Artificial Intelligence and Statistics, pages 1859\u20131867, 2018.   \n418 [22] Xuhui Fan, Bin Li, Yi Wang, Yang Wang, and Fang Chen. The Ostomachion Process. In AAAI Conference   \n419 on Artificial Intelligence, pages 1547\u20131553, 2016.   \n420 [23] Thomas Ferguson. Bayesian analysis of some nonparametric problems. Annals of Statistics, 2(1):209\u2013230,   \n421 1973.   \n422 [24] Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahramani. The infinite factorial hidden Markov model.   \n423 In Advances in Neural Information Processing Systems, pages 1697\u20131704, 2008.   \n424 [25] Shufei Ge, Shijia Wang, Yee Whye Teh, Liangliang Wang, and Lloyd Elliott. Random tessellation forests.   \n425 In Advances in Neural Information Processing Systems, pages 9575\u20139585. 2019.   \n426 [26] Weina Ge and Robert T. Collins. Marked point processes for crowd counting. In IEEE Computer Society   \n427 Conference on Computer Vision and Pattern Recognition, pages 2913\u20132920, 2009.   \n428 [27] Bernhard Gittenberger and Veronika Kraus. The degree profile of random P\u00f3lya trees. Journal of   \n429 Combinatorial Theory, Series A, 119(7):1528\u20131557, 2012.   \n430 [28] Robert C. Griffiths and Paul Marjoram. Ancestral inference from samples of DNA sequences with   \n431 recombination. Journal of Compututational Biology, 3(4):479\u2013502, 1996.   \n432 [29] Thomas L. Grifftihs and Zoubin Ghahramani. Infinite latent feature models and the Indian buffet process.   \n433 In Advances in Neural Information Processing Systems, 2005.   \n434 [30] Ruocheng Guo, Jundong Li, and Huan Liu. INITIATOR: noise-contrastive estimation for marked temporal   \n435 point process. In International Joint Conference on Artificial Intelligence,, pages 2191\u20132197. ijcai.org,   \n436 2018.   \n437 [31] Tobias Hatt and Stefan Feuerriegel. Early detection of user exits from clickstream data: A Markov   \n438 modulated marked point process model. In WWW: The Web Conference, pages 1671\u20131681, 2020.   \n439 [32] Xiaoyu He and Matthew Kwan. Universality of random permutations. arXiv:1911.12878, 2019.   \n440 [33] Daniel A. Henk, Alex Weir, and Meredith Blackwell. Laboulbeniopsis termitarius, an ectoparasite of   \n441 termites newly recognized as a member of the laboulbeniomycetes. Mycologia, 95(4):561\u2013564, 2003.   \n442 [34] Asger Hobolth, Ole F Christensen, Thomas Mailund, and Mikkel H Schierup. Genomic relationships and   \n443 speciation times of human, chimpanzee, and gorilla inferred from a coalescent hidden Markov model.   \n444 PLOS Genetics, 3(2):1\u201311, 02 2007.   \n445 [35] Sujun Hong and Hirotaka Hachiya. Multi-stream based marked point process. In Asian Conference on   \n446 Machine Learning, volume 157, pages 1269\u20131284, 2021.   \n447 [36] Douglas N. Hoover. Relations on probability spaces and arrays of random variables. Technical report,   \n448 Institute of Advanced Study, Princeton, 1979.   \n449 [37] Carlos Hoppen, Yoshiharu Kohayakawa, Carlos Gustavo Moreira, Balazs Rath, and Rudini Menezes   \n450 Sampaio. Limits of permutation sequences. Journal of Combinatorial Theory, Series B, 103(1):93\u2014-113,   \n451 2013.   \n452 [38] John P. Huelsenbeck and Fredrik Ronquist. MrBayes: bayesian inference of phylogenetic trees. Bioinform.,   \n453 17(8):754\u2013755, 2001.   \n454 [39] Katsuhiko Ishiguro, Naonori Ueda, and Hiroshi Sawada. Subset infinite relational models. In International   \n455 Conference on Artificial Intelligence and Statistics, volume 22, pages 547\u2013555, 2012.   \n456 [40] Thomas H. Jukes and Charles R. Cantor. Chapter 24 - evolution of protein molecules. In Mammalian   \n457 Protein Metabolism, pages 21\u2013132. Academic Press, 1969.   \n458 [41] Olav Kallenberg. Symmetries on random arrays and set-indexed processes. Journal of Theoretical   \n459 Probability, 5(4):727\u2013765, 1992.   \n460 [42] Steven Kelk. Review of recombinatorics: The algorithmics of ancestral recombination graphs and explicit   \n461 phylogenetic networks by dan gusfield. SIGACT News, 47(1):12\u201315, 2016.   \n462 [43] Hideaki Kim, Tomoharu Iwata, Yasuhiro Fujiwara, and Naonori Ueda. Read the silence: Well-timed   \n463 recommendation via admixture marked point processes. In AAAI Conference on Artificial Intelligence,   \n464 pages 132\u2013139, 2017.   \n465 [44] John Frank Charles Kingman. The representation of partition structures. Journal of the London Mathe  \n466 matical Society, s2-18(2):374\u2013380, 1978.   \n467 [45] David A. Knowles, Jurgen Van Gael, and Zoubin Ghahramani. Message passing algorithms for the   \n468 Dirichlet diffusion tree. In International Conference on Machine Learning, pages 721\u2013728, 2011.   \n469 [46] Lidia Kuan, Frederico Pratas, Leonel Sousa, and Pedro Tom\u00e1s. MrBayes sMC3. International Journal of   \n470 High Performance Computing Applications, 32(2):246\u2013265, 2018.   \n471 [47] Balaji Lakshminarayanan, Daniel Roy, and Yee Whye Teh. Mondrian forests: Efficient online random   \n472 forests. In Advances in Neural Information Processing Systems, 06 2014.   \n473 [48] Michael Lavine. Some Aspects of P\u00f3lya Tree Distributions for Statistical Modelling. The Annals of   \n474 Statistics, 20(3):1222 \u2013 1235, 1992.   \n475 [49] Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. The infinite PCFG using hierarchical   \n476 Dirichlet processes. In Joint Conference on Empirical Methods in Natural Language Processing and   \n477 Computational Natural Language Learning, pages 688\u2013697, 2007.   \n478 [50] Tengfei Ma, Issei Sato, and Hiroshi Nakagawa. The hybrid nested/hierarchical Dirichlet process and its   \n479 application to topic modeling with word differentiation. In AAAI Conference on Artificial Intelligence,   \n480 pages 2835\u20132841, 2015.   \n481 [51] Micka\u00ebl Maazoun. On the Brownian separable permuton. Combinatorics, Probability and Computing,   \n482 29(2):241\u2013266, 2019.   \n483 [52] Ali Mahmoudi, Jere Koskela, Jerome Kelleher, Yao-ban Chan, and David J. Balding. Bayesian inference   \n484 of ancestral recombination graphs. PLoS Computational Biology, 18(3), 2022.   \n485 [53] Thomas Mailund, Julien Y. Dutheil, Asger Hobolth, Gerton Lunter, and Mikkel H. Schierup. Estimating   \n486 divergence time and ancestral effective population size of bornean and sumatran orangutan subspecies   \n487 using a coalescent hidden Markov model. PLoS Genetics, 7(3):1\u201315, 03 2011.   \n488 [54] Colin L. Mallows. Non-null ranking models. Biometrika, 44:114\u2013130, 1957.   \n489 [55] Luca Mancini and Anna Maria Paganoni. Marked point process models for the admissions of heart failure   \n490 patients. Statistical Analysis and Data Mining, 12(2):125\u2013135, 2019.   \n491 [56] Daniel Mauldin, William D. Sudderth, and Stanley C. Williams. P\u00f3lya trees and random distributions.   \n492 The Annals of Statistics, 20(3):1203 \u2013 1221, 1992.   \n493 [57] Peter Mccullagh. Random permutations and partition models, 2010.   \n494 [58] Gr\u00e1inne McGuire, Michael C. Denham, and David J. Balding. MAC5: Bayesian inference of phylogenetic   \n495 trees from DNA sequences incorporating gaps. Bioinform., 17(5):479\u2013480, 2001.   \n496 [59] Jeffrey W. Miller and Matthew T. Harrison. Mixture models with a prior on the number of components.   \n497 Journal of the American Statistical Association, 113(521):340\u2013356, 2018.   \n498 [60] Antonio Khalil Moretti, Liyi Zhang, Christian A. Naesseth, Hadiah Venner, David M. Blei, and Itsik   \n499 Pe\u2019er. Variational combinatorial sequential Monte Carlo methods for Bayesian phylogenetic inference. In   \n500 Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, volume 161, pages 971\u2013981, 2021.   \n501 [61] Masahiro Nakano, Akisato Kimura, Takeshi Yamada, and Naonori Ueda. Baxter permutation process. In   \n502 Advances in Neural Information Processing Systems, 2020.   \n503 [62] Radford M. Neal. Density modeling and clustering using Dirichlet diffusion trees. Bayesian Statistics,   \n504 7:619\u2013629, 2003.   \n505 [63] Quoc Phong Nguyen, Sebastian Tay, Bryan Kian Hsiang Low, and Patrick Jaillet. Top-k ranking bayesian   \n506 optimization. In AAAI Conference on Artificial Intelligence, pages 9135\u20139143, 2021.   \n507 [64] Kai Ni, Lawrence Carin, and David B. Dunson. Multi-task learning for sequential data via ihmms and the   \n508 nested dirichlet process. In International Conference on Machine Learning, volume 227, pages 689\u2013696,   \n509 2007.   \n510 [65] Lukasz P. Olech, Michal Spytkowski, Halina Kwasnicka, and Zbigniew Michalewicz. Hierarchical   \n511 data generator based on tree-structured stick breaking process for benchmarking clustering methods.   \n512 Information Sciences, 554:99\u2013119, 2021.   \n513 [66] Gary J. Olsen, Hideo Matsuda, Ray Hagstrom, and Ross A. Overbeek. fastdnaml: a tool for construction   \n514 of phylogenetic trees of DNA sequences using maximum likelihood. Computer Applications in the   \n515 Biosciences, 10(1):41\u201348, 1994.   \n516 [67] Peter Orbanz. Projective limit random probabilities on polish spaces. Electronic Journal of Statistics, 5,   \n517 2011.   \n518 [68] Mathias Ortner, Xavier Descombes, and Josiane Zerubia. Building outline extraction from digital elevation   \n519 models using marked point processes. International Journal of Computer Vision, 72(2):107\u2013132, 2007.   \n520 [69] Mathias Ortner, Xavier Descombes, and Josiane Zerubia. A marked point process of rectangles and   \n521 segments for automatic analysis of digital elevation models. IEEE Transactions on Pattern Analysis and   \n522 Machine Intelligence, 30(1):105\u2013119, 2008.   \n523 [70] Daniel Osei-Kuffuor, Ruipeng Li, and Yousef Saad. Matrix reordering using multilevel graph coarsening   \n524 for ILU preconditioning. SIAM Journal on Scientific Computing, 37(1), 2015.   \n525 [71] John W. Paisley, Chong Wang, David M. Blei, and Michael I. Jordan. Nested hierarchical Dirichlet   \n526 processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):256\u2013270, 2015.   \n527 [72] Laxmi Parida. Ancestral recombinations graph: A reconstructability perspective using random-graphs   \n528 framework. Journal of Compututational Biology, 17(10):1345\u20131370, 2010.   \n529 [73] Agyemang Paul, Zhefu Wu, Kai Liu, and Shufeng Gong. Robust multi-objective visual Bayesian   \n530 personalized ranking for multimedia recommendation. Applied Intelligence, 52(4):3499\u20133510, 2022.   \n531 [74] Rainer Picard and Jim Pitman. Combinatorial Stochastic Processes: Ecole d\u2019Et\u00e9 de Probabilit\u00e9s de   \n532 Saint-Flour XXXII - 2002. Lecture Notes in Mathematics. Springer Berlin Heidelberg, 2006.   \n533 [75] Vincent Pilaud and Viviane Pons. Permutrees. Electronic Notes in Discrete Mathematics, 61:987\u2013993,   \n534 2017.   \n535 [76] J. Pitman and M. Yor. The two-parameter poisson-dirichlet distribution derived from a stable subordinator.   \n536 IBM Journal of Research and Development, 2(25):855\u2013900, 1997.   \n537 [77] Jim Pitman and Matthias Winkel. Regenerative tree growth: Binary self-similar continuum random trees   \n538 and poisson-dirichlet compositions. The Annals of Probability, 37(5):1999\u20132041, 2009.   \n539 [78] Andrew Rambaut and Nicholas C. Grassly. Seq-gen: an application for the monte carlo simulation of DNA   \n540 sequence evolution along phylogenetic trees. Computer Applications in the Biosciences, 13(3):235\u2013238,   \n541 1997.   \n542 [79] Carl Edward Rasmussen. The infinite Gaussian mixture model. In Advances in Neural Information   \n543 Processing Systems, 2000.   \n544 [80] Matthew D. Rasmussen, Melissa J. Hubisz, Ilan Gronau, and Adam Siepel. Genome-wide inference of   \n545 ancestral recombination graphs. PLoS Genetics, 10(5):1\u201327, 05 2014.   \n546 [81] Parisa Rastin and Basarab Matei. Incremental matrix reordering for similarity-based dynamic data sets.   \n547 In Neural Information Processing, volume 10638, pages 76\u201384, 2017.   \n548 [82] Nathan Reading. Cambrian lattices. Advances in Mathematics, 205(2):313\u2013353, 2006.   \n549 [83] Lu Ren, Yingjian Wang, David B. Dunson, and Lawrence Carin. The kernel beta process. In Advances in   \n550 Neural Information Processing Systems, pages 963\u2013971, 2011.   \n551 [84] Abel Rodriguez and Kaushik Ghosh. Nested partition models. Technical report, JackBaskin School of   \n552 Engineering, 2009.   \n553 [85] Dane Rogers and Matthias Winkel. A Ray\u2013Knight representation of up-down Chinese restaurants.   \n554 Bernoulli, 28(1):689\u2014-712, 2021.   \n555 [86] Amy Y. Rossman, John M. McKemy, Rebecca A. Pardo-Schultheiss, and Hans-Josef Schroers. Molecular   \n556 studies of the bionectriaceae using large subunit rDNA sequences. Mycologia, 93(1):100\u2013110, 2001.   \n557 [87] Daniel Roy. Computability, inference and modeling in probabilistic programming. PhD thesis, Mas  \n558 sachusetts Institute of Technology, 2011.   \n559 [88] Daniel Roy and Yee Whye Teh. The Mondrian process. In Advances in Neural Information Processing   \n560 Systems, 2009.   \n561 [89] Jayaram Sethuraman. A constructive definition of Dirichlet priors. Statistica Sinica, 4:639\u2013650, 1994.   \n562 [90] Kyung-Ah Sohn and Eric P. Xing. Hidden markov dirichlet process: Modeling genetic recombination in   \n563 open ancestral space. In Advances in Neural Information Processing Systems, pages 1305\u20131312. MIT   \n564 Press, 2006.   \n565 [91] Simon Tavar\u00e9. Some probabilistic and statistical problems in the analysis of dna sequences. 1986.   \n566 [92] Yee Whye Teh, Charles Blundell, and Lloyd T. Elliott. Modelling genetic variations using fragmentation  \n567 coagulation processes. In Advances in Neural Information Processing Systems, pages 819\u2013827, 2011.   \n568 [93] Yee Whye Teh, Dilan Gr\u00fcr, and Zoubin Ghahramani. Stick-breaking construction for the Indian buffet   \n569 process. In International Conference on Artificial Intelligence and Statistics, volume 2, pages 556\u2013563.   \n570 PMLR, 21\u201324 Mar 2007.   \n571 [94] Yee Whye Teh, Michael I. Jordan, Matthew Beal, and David Blei. Hierarchical Dirichlet processes.   \n572 Journal of the AmericanStatistical Association, 101:1566\u20131581, 2006.   \n573 [95] Romain Thibaux and Michael I. Jordan. Hierarchical beta processes and the Indian buffet process. In   \n574 International Conference on Artificial Intelligence and Statistics, volume 2, pages 564\u2013571, 2007.   \n575 [96] \u00c1kos Utasi and Csaba Benedek. A 3-D marked point process model for multi-view people detection. In   \n576 IEEE Conference on Computer Vision and Pattern Recognition, pages 3385\u20133392, 2011.   \n577 [97] Isabel Valera, Francisco J. R. Ruiz, and Fernando P\u00e9rez-Cruz. Infinite factorial unbounded-state hidden   \n578 Markov model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(9):1816\u20131828,   \n579 2016.   \n580 [98] Ivan Vogel, Frantisek Zedek, and Pavel Ocenasek. Constructing phylogenetic trees based on intra-group   \n581 analysis of human mitochondrial DNA. In Human Interface and the Management of Information, volume   \n582 6771 of Lecture Notes in Computer Science, pages 165\u2013169. Springer, 2011.   \n583 [99] Gauthier Van Vracem and Siegfried Nijssen. Iterated matrix reordering. In Machine Learning and   \n584 Knowledge Discovery in Databases, volume 12977, pages 745\u2013761, 2021.   \n585 [100] S. Walker. Sampling the Dirichlet mixture model with slices. Communications in Statistics Simulation   \n586 and Computation, 36(1):45\u201354, 2007.   \n587 [101] Chao Wang, Hengshu Zhu, Chen Zhu, Chuan Qin, and Hui Xiong. Setrank: A setwise bayesian approach   \n588 for collaborative ranking from implicit feedback. In AAAI Conference on Artificial Intelligence, pages   \n589 6127\u20136136, 2020.   \n590 [102] Yong Wang and Jeffrey B. Remmel. A binomial distribution model for the traveling salesman problem   \n591 based on frequency quadrilaterals. Journal of Graph Algorithms and Applications, 20(2):411\u2013434, 2016.   \n592 [103] Dan Wei and Qingshan Jiang. A DNA sequence distance measure approach for phylogenetic tree   \n593 construction. In Fifth International Conference on Bio-Inspired Computing: Theories and Applications,   \n594 pages 204\u2013212. IEEE, 2010.   \n595 [104] Weichang Wu, Junchi Yan, Xiaokang Yang, and Hongyuan Zha. Decoupled learning for factorial marked   \n596 temporal point processes. In International Conference on Knowledge Discovery & Data Mining, pages   \n597 2516\u20132525, 2018.   \n598 [105] Liang Xin, Wen Song, Zhiguang Cao, and Jie Zhang. NeuroLKH: Combining deep learning model with   \n599 lin-kernighan-helsgaun heuristic for solving the traveling salesman problem. In Advances in Neural   \n600 Information Processing Systems, pages 7472\u20137483, 2021.   \n601 [106] Cheng Zhang. Improved variational Bayesian phylogenetic inference with normalizing flows. In Advances   \n602 in Neural Information Processing Systems, 2020.   \n603 [107] Cheng Zhang and Frederick A. Matsen IV. Variational Bayesian phylogenetic inference. In International   \n604 Conference on Learning Representations.   \n605 [108] Ning Zhang and Meredith Blackwell. Molecular phylogeny of dogwood anthracnose fungus (discula   \n606 destructiva) and the diaporthales. Mycologia, 93(2):355\u2013365, 2001.   \n607 [109] Ping Zhang, Rishabh K. Iyer, Ashish Tendulkar, Gaurav Aggarwal, and Abir De. Learning to select   \n608 exogenous events for marked temporal point process. In Advances in Neural Information Processing   \n609 Systems, pages 347\u2013361, 2021.   \n610 [110] Qian Zhang and Fuji Ren. Prior-based bayesian pairwise ranking for one-class collaborative filtering.   \n611 Neurocomputing, 440:365\u2013374, 2021.   \n612 [111] Mingyuan Zhou, Haojun Chen, John W. Paisley, Lu Ren, Guillermo Sapiro, and Lawrence Carin.   \n613 Non-parametric bayesian dictionary learning for sparse image representations. In Advances in Neural   \n614 Information Processing Systems, pages 2295\u20132303, 2009.   \n615 [112] Mingyuan Zhou, Hongxia Yang, Guillermo Sapiro, David B. Dunson, and Lawrence Carin. Dependent   \n616 hierarchical beta process for image interpolation and denoising. In International Conference on Artificial   \n617 Intelligence and Statistics, volume 15, pages 883\u2013891, 2011.   \n618 [113] Quan Zou, Shixiang Wan, and Xiangxiang Zeng. Hptree: Reconstructing phylogenetic trees for ultra-large   \n619 unaligned DNA sequences via NJ model and hadoop. In IEEE International Conference on Bioinformatics   \n620 and Biomedicine, pages 53\u201358. IEEE Computer Society, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "621 A Properties of marked stick-breaking process ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "622 This section provides proofs of Propositions 4.1 and 4.2 concerning two properties of the marked   \n623 stick-breaking process omitted in Section 4 of the main text. ", "page_idx": 14}, {"type": "text", "text": "624 A.1 Exchangeability ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "625 One of the most important properties of random data paths drawn from the generative probabilistic   \n626 model described in Section 4 is exchangeability, that is, the model probability is invariant to the   \n627 indexing of the data. More specifically, this can be summarised as the following statement:   \n628 Proposition A.1 (Exchangeability; Proposition 4.1). For any permutation $\\sigma$ of length $N$ $(N\\in\\mathbb{N})$ ),   \n629 we have $\\mathbb{P}[Z_{1}={z_{1}},Z_{2}\\bar{\\mathrm{~}}{={z_{2}},\\bar{\\mathrm{~}}...,Z_{N}\\bar{\\mathrm{~}}{={z_{N}}}}]\\,=\\,\\mathbb{P}[Z_{\\sigma(1)}\\,\\bar{\\mathrm{~}}{={z_{1}}},Z_{\\sigma(2)}\\,=\\,{z_{2}},...,Z_{\\sigma(N)}\\,=\\,z_{N}]$ ,   \n630 where $z_{j}$ $\\mathrm{~\\boldsymbol{j}~}\\in\\left[N\\right])$ is a sample of paths of random permutrees. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "631 Proof. Broadly as a whole, we will check two following facts: ", "page_idx": 14}, {"type": "text", "text": "632 (i) The random data assignments to bottom terminal nodes by the stick-breaking process [89]   \n633 and the Kingman\u2019s paintbox scheme [44] are themselves exchangeable.   \n634 (ii) The selection of data paths by the two-table Chinese restaurant process is exchangeable.   \n635 Exchangeability of data assignments to bottom terminal nodes - We denote the index of the stick   \n636 of the stick-breaking process to which the $j$ th $(j=1,\\ldots,N)$ data is assigned by the random variable   \n637 $Z_{j}^{\\mathrm{bottom}}$ . It follows from the model construction that, given a random partition of $[0,1]$ drawn from   \n638 the stick-breaking process, the random variable $U_{j}$ $(j=1,\\cdot\\cdot\\cdot,N)$ is independent. As a result, for   \n639 any permutation $\\sigma$ of length $N$ , we have ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big[Z_{1}^{(\\mathrm{bottom})}=s_{N},\\ldots,Z_{N}^{(\\mathrm{bottom})}=s_{N}\\Big]=\\displaystyle\\prod_{j=1}^{N}\\Big[Z_{j}^{(\\mathrm{bottom})}=s_{j}\\Big]=\\displaystyle\\prod_{j=1}^{N}\\Big[Z_{\\sigma(j)}^{(\\mathrm{bottom})}=s_{j}\\Big]}\\\\ {=\\mathbb{P}\\Big[Z_{\\sigma(1)}^{(\\mathrm{bottom})}=s_{N},\\ldots,Z_{\\sigma(N)}^{(\\mathrm{bottom})}=s_{N}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "640 where $\\mathrm{s}_{j}\\,\\left(j=1,\\ldots,N\\right)$ is a sample of stick indices $(\\in\\mathbb{N})$ . ", "page_idx": 14}, {"type": "text", "text": "641 Exchangeability of data path selection - Given the assignment of data to the terminal nodes, the   \n642 choice of data paths follows a chain of the two-table Chinese restaurant process (see Figure 3 (c) in   \n643 the main text) according to the decoration of each inner vertex of the permutree. It should be noted   \n644 that in the two-table Chinese restaurant process, the data paths are chosen deterministically when   \n645 the decorations are $\\Phi,\\otimes$ , and $\\oslash$ . Therefore, we only need to focus on the case of table partitioning   \n646 (Figure 3 (c), top)  w he n the  d ecoration is $\\oslash$ . It immediately from the model construction that the   \n647 probability of partitioning the data when th e  decoration is $\\oslash$ is obtained as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big[Z_{1}=z_{1},\\ldots,Z_{N}=z_{N}\\ |\\ Z_{1}^{(\\mathrm{bottom})}=s_{N},\\ldots,Z_{N}^{(\\mathrm{bottom})}=s_{N}\\Big]}\\\\ &{\\quad=\\prod_{r}\\frac{\\Big\\{(1+\\frac{\\gamma}{2})\\cdots(\\bigwedge_{\\mathrm{Left}}^{(r)}+\\frac{\\gamma}{2})\\Big\\}\\cdot\\Big\\{(1+\\frac{\\gamma}{2})\\cdots(\\bigwedge_{\\mathrm{Right}}^{(r)}+\\frac{\\gamma}{2})\\Big\\}}{(1+\\gamma)(2+\\gamma)\\cdots(n^{(r)}+\\gamma)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "648 where the variable $s_{j}$ is the index of bottom terminal nodes (i.e., the stick index of the stick-breaking   \n649 process on $[0,1])$ included in the path sample $z_{j}$ , the variable $n^{(r)}$ represents the number of data   \n650 flowing to the $r$ th permutree vertex from the bottom in the vertical direction in the collection of data   \n651 path samples $z_{1},\\dots,z_{N}$ , and $\\mathcal{N}_{\\mathrm{Left}}^{(r)}$ and $\\mathcal{N}_{\\mathrm{Right}}^{(r)}$ represent the number of data to be partitioned into   \n652 the left and right tables at the $r$ thth vertex (if the decoration at that vertex is $\\oslash$ ), respectively. It is   \n653 important to note that the probability of selecting this datapath depends only on  t he number of data in   \n654 the division of the table at each vertex. That is, in other words, it does not depend on the index of the   \n655 data as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big[Z_{1}=z_{1},\\ldots,Z_{N}=z_{N}\\ |\\ Z_{1}^{(\\mathrm{bottom})}=s_{N},\\ldots,Z_{N}^{(\\mathrm{bottom})}=s_{N}\\Big]}\\\\ &{\\qquad=\\mathbb{P}\\Big[Z_{\\sigma(1)}=z_{1},\\ldots,Z_{\\sigma(N)}=z_{N}\\ |\\ Z_{\\sigma(1)}^{(\\mathrm{bottom})}=s_{N},\\ldots,Z_{\\sigma(N)}^{(\\mathrm{bottom})}=s_{N}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "656 for any permutation $\\sigma$ with length $N$ . Thus, it can be checked that the selection of data paths is   \n657 exchangeable. From Equations 2 and 4, we have completed our proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "658 A.2 Validity of finite truncation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "659 The generative probability model (described in Section 4) requires in principle an infinite number of   \n660 random variables for its description, but finite truncation works reasonably well for a finite number of   \n661 actual observed data. More specifically, we can summarize this property as follows:   \n662 Proposition A.2 (Finite truncation; Proposition 4.2). In the generative probability model (described   \n663 in Section 4) of data indexed by $j=1,2,\\dots,N\\;(N\\in\\mathbb{N})$ , we consider an event that all random   \n664 variables $U_{j}$ $(j=1,\\ldots,N)$ , representing the horizontal position of the jth data, falls in the range   \n665 $[0,1-\\epsilon)$ as a situation with a sufficiently high probability $\\begin{array}{r}{\\mathbb{P}[\\wedge_{j=1}^{N}0\\leq U_{j}<1-\\epsilon]=\\prod_{j=1}^{N}\\mathbb{P}[0\\leq}\\end{array}$   \n666 $U_{j}<1-\\epsilon]>1-\\epsilon\\cdot{\\mathcal{O}}(N)$ , where $\\epsilon>0$ is a tiny real value. In this situation, there exists some   \n667 natural number $K<\\infty$ , and all data paths are assigned with probability 1 only to paths on the   \n668 finite-size random permutree generated from the random marked points $l_{1},l_{2},\\ldots,l_{K}$ .   \n669 Proof. It follows from the construction that the uniformly random random random variables $U_{j}$   \n670 $(j\\,=\\,1,\\ldots,N)$ are independent, so that the probability of an event for which all those random   \n671 variables fall within the range $[0,1^{\\epsilon})$ can be checked as follows. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big[\\land_{j=1}^{N}0\\le U_{j}<1-\\epsilon\\Big]=\\prod_{j=1}^{N}\\mathbb{P}\\Big[0\\le U_{j}<1-\\epsilon\\Big]=\\big(1-\\epsilon\\big)^{N}>1-\\epsilon N.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "672 Then, from the construction of the marked stick-breaking process on $[0,1]$ , since there are countably   \n673 infinite number of marked points in the range $[1-\\epsilon,1]\\stackrel{\\_}{\\times}[0,1]$ , the probability that there exists some   \n674 natural number $K<\\infty$ and the corresponding decoration of it is $\\otimes$ is 1. That is, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big[K<\\infty\\;\\;\\wedge\\;\\;m_{K}=\\otimes\\;\\;\\wedge\\;\\;1-\\epsilon\\leq l_{K,1}\\leq1\\Big]=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "675 From the construction of the two-table Chinese restaurant process (described in Section 4) and   \n676 the permutree requirement (C2) (described in Section 2), the data path assigned to the 1st to $K\\mathrm{{th}}$   \n677 bottom terminal nodes in the stick-breaking process never reaches the $(K+1)\\mathfrak{t h}$ and subsequent   \n678 indexed permutree vertices. Therefore, each have a data path only on the edges of the finite permutree   \n679 consisting of the 1st to $K$ th marked vertices. From the above, we have completed the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "680 B Relationship between permutree process and other stochastic processes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "681 The purpose of this section is to provide additional information to help the reader better understand   \n682 the characteristics of the permutree process as marked point process.   \n683 We clarify the relationship between the permutree process and other existing stochastic processes.   \n684 Specifically, the permutree process can lead to the uniform random permutations and the Mondrian   \n685 process as its special cases. These relationships can be derived immediately from the fact that each   \n686 can be expressed as a Poisson process of some sort. We will discuss each of these in specific detail ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "OiTr2v90t7/tmp/0a7f0f8c5d521783cf331d73ba0b3de5785b6f852fb25e7107c45061a35c6905.jpg", "img_caption": ["(a) Uniform permutation "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "OiTr2v90t7/tmp/0165f7b6565e949a90475d1763978b7f18227bd19670464e3409ec6298b65a09.jpg", "img_caption": ["(b) Mondrian process "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: Relationship between permutree process and other existing stochastic processes. (a) Uniform random permutation - If we restrict the decoration weights for marks to $(c_{\\mathbb{O}},c_{\\otimes},c_{\\mathbb{O}},c_{\\mathbb{O}})=$ $(1,0,0,0)$ and make the Poisson process homogeneous, the permutree process lead s to  a st och astic process that generates a uniform random permutation. This relation follows immediately from the following fact: If a collection of i.i.d. uniform random variables $U_{1},U_{2},\\cdot\\cdot\\cdot\\sim\\mathrm{Uniform}([0,1])$ is ordered in ascending order, it follows a uniform random permutation. (b) Mondrian process - If we restrict the decoration weights to $(c_{\\Phi},c_{\\otimes},c_{\\emptyset},c_{\\emptyset})=\\bar{(}0,0,1,0)$ and set $\\lambda(\\cdot)=\\mu\\cdot\\mathrm{Leb}(\\cdot)$ , the permutree process leads to a stochastic p roce ss t hat  simulates a Mondrian process [88, 87] on $[0,1]$ with the intensity $\\mu$ and the budget 1. By viewing the vertical position in the marked point process as the moment when the event of the cut in the Markov process (i.e., the Mondrian process) occurs, and the horizontal position as the location where the cut occurs, the special permutation process described above can be reduced to a Mondrian process. ", "page_idx": 16}, {"type": "text", "text": "687 below. First of all, for self-containment, the core of the permutree process is restated, although it is   \n688 the same as that detailed in the body of this paper.   \n689 Permutree process - We consider a marked point process consisting of a point process and associate   \n690 marks, which can be expressed as $\\{(l_{i},m_{i})\\,:\\,i\\,=\\,1,2,\\ldots\\}$ , where $\\boldsymbol{l}_{1},\\boldsymbol{l}_{2},\\boldsymbol{\\ldots}$ . are locations and   \n691 $m_{1},m_{2},\\ldots$ are associated marks. Specifically, we employ the following Poisson process on a   \n692 2-dimensional plane $[0,1]\\times[0,1]$ with discrete marks:   \n693   \n694   \n695   \n696   \n697   \n698   \n699 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 Random locations - Draw the random locations $\\begin{array}{r}{\\boldsymbol{l}_{1},\\boldsymbol{l}_{2},\\ldots}\\end{array}$ from a Poisson point process on the plane $[0,1]\\times[0,1]$ with the intensity function $\\lambda\\,:\\,[0,1]\\,\\times\\,[0,1]\\,\\,\\bar{\\to}\\,\\,\\mathbb{R}^{+}$ , where $\\mathbb{R}^{+}=\\Bar{\\{\\boldsymbol{r}:\\boldsymbol{r}\\stackrel{.}{>}0,\\boldsymbol{\\dot{r}}\\in\\dot{\\mathbb{R}}\\}}$ . For notational convenience, we use $\\bar{l}_{i}=(l_{i,1},l_{i,2})\\;(\\in\\mathbb{R}^{2})$ , where $l_{i,1}$ and $l_{i,2}$ are the horizontal and vertical positions, respectively.   \n\u2022 Random marks - Draw the random marks $m_{1},m_{2},\\ldots,m_{n}$ independently from a categorical distribution on $\\{\\Phi,\\otimes,\\oslash,\\oslash\\}$ : Categorica $(c_{\\mathbb{D}},c_{\\otimes},c_{\\emptyset},c_{\\emptyset})$ , where $c_{*}$ $\\displaystyle\\dot{(*\\,\\in\\,\\{\\Phi,\\otimes,\\bar{\\otimes},\\varpi\\})}$ denotes the proba bil i ty  t ha t  decoration $^*$ is ad opte d. ", "page_idx": 16}, {"type": "text", "text": "700 Connection to uniform random permutation - If we restrict the decoration weights for marks to   \n701 $(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset})=(1,0,0,0)$ and make the Poisson process homogeneous (i.e., make the intensity   \n702 fu ncti on $\\lambda$ un iform), the permutree process leads to a stochastic process that generates a uniform   \n703 random permutation. This fact can be easily derived by interpreting the permutation process as   \n704 follows. See also Figure 6 (a). By construction, we can obtain the indices $a_{1},\\ldots,a_{n}$ so that the   \n705 random positions $\\boldsymbol{l}_{1},\\ldots,\\boldsymbol{l}_{n}$ are in ascending order in the horizontal direction, that is, $l_{a_{1},1}<l_{a_{2},1}<$   \n706 $\\cdots<l_{a_{n},1}$ . If we choose to set $p(\\mathbf{v}_{a_{i}})\\,=\\,i$ for the $i$ -th vertex $\\mathbf{v}_{i}$ $(i=1,2,\\dots)$ ) of the resulting   \n707 permutree, then $p$ can lead to a permutation. The following fact shows that $p$ corresponds to a uniform   \n708 random permutation:   \n709 Proposition B.1. (See, for example, Lemma 2.2 in $I32J.$ ) A uniform random permutation $\\sigma$ with   \n710 length n can be obtained via a sequence of n i.i.d. Uniform $([0,1])$ random variables $W_{1},\\dots,W_{n}$   \n711 (Note that their values are distinct with probability 1), by taking $\\sigma$ to be the unique permutation for   \n712 which $W_{\\sigma(1)}<\\cdot\\cdot\\cdot<W_{\\sigma(n)}$ .   \n713 Connection to Mondrian process - If we restrict the decoration weights to $\\displaystyle(c_{\\mathbb{\\Phi}},c_{\\otimes},c_{\\mathbb{\\otimes}},c_{\\mathbb{\\varnothing}})\\,=$   \n714 $(0,0,1,0)$ and set $\\lambda(\\cdot)=\\mu\\cdot\\mathrm{Leb}(\\cdot)$ , the permutree process leads to a stochastic proc ess t hat s imu lates   \n715 a Mondrian process [88, 87] on $[0,1]$ with the intensity $\\mu$ and the budget 1. This fact can be easily   \n716 derived by interpreting the permutation process as follows. In the above setup, the sample generated   \n717 by the permutation process can be restricted to a binary tree by following the procedure described in ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "OiTr2v90t7/tmp/dbb7ce99d320a4a71bc09ec718c1cf9ecc5dc528d955e5478f2f20ae1416ebfa.jpg", "img_caption": ["(a) $(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset})=(3/4,0,0,1/4)$ with $\\mu=10$ (top), $\\mu=20$ (middle), and $\\mu=40$ (bottom). "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "OiTr2v90t7/tmp/85cb5763ada98ec329ccb721e1d7a3c6725cab240b89a031799db017d5b8384b.jpg", "img_caption": ["(b) $(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset})=(1/4,3/4,0,0)$ with $\\mu=10$ (top), $\\mu=20$ (middle), and $\\mu=40$ (bottom). "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "OiTr2v90t7/tmp/6347a2285a88ec2c08c60695f80e2fa73d868531b2549dadcf2ff1531ca57507.jpg", "img_caption": ["(c) $(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset})=(0,0,1/2,1/2)$ with $\\mu=10$ (top), $\\mu=20$ (middle), and $\\mu=40$ (bottom). "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "OiTr2v90t7/tmp/6053b69d65b7b19102c37b9bcf055595ab267a2f7a5802ce6e5d2eddbdb2e0b4.jpg", "img_caption": ["(d) $\\left(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset}\\right)=(1/4,1/4,1/4,1/4)$ with $\\mu=10$ (top), $\\mu=20$ (middle), and $\\mu=40$ (bottom). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 7: Samples drawn from permutree process with intensity $\\lambda(\\cdot)=\\mu\\cdot{\\mathrm{Leb}}(\\cdot)$ and decoration weights $(c_{\\mathbb{D}},c_{\\otimes},c_{\\mathbb{D}},c_{\\mathbb{D}})$ . Ten samples are generated for each parameter setting. ", "page_idx": 17}, {"type": "text", "text": "718 Section 3 (Figure 6) of the main text. From the fundamental properties of the Poisson process, the   \n719 vertical interval between two adjacent vertices at random locations follows an exponential distribution   \n720 $\\operatorname{Exp}(\\mu)$ . Imagine the time evolution in the partition of a line segment of length 1 horizontally drawn   \n721 from bottom to top, as shown in Figure 6 (b). The time evolution of this partition can be viewed as a   \n722 Markov process with an intensity $\\mu$ and a time limit of 1. Furthermore, if we consider the horizontal   \n723 location of the marked point process as the position where the line segment of length 1 is cut, we can   \n724 consider this time evolution as a hierarchical partition of the line segment. Therefore, this can be   \n725 regarded as a Mondrian process. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "726 C Bayesian inference for phylogenetic permutree (omitted in Section 5) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "727 This section reveals the Bayesian inference algorithm for phylogenetic permutree analysis using our   \n728 permutree processes. For the sake of generality, we will use the marked point process representation   \n729 described in Section 3 in particular as a permutree process. This argument can also be applied,   \n730 with minor modifications, to the special case of the marked stick-breaking process (with its finite   \n731 truncation) described in Section 4.   \n732 Overview - Standard Bayesian inference algorithms such as Markov chain Monte Carlo (MCMC)   \n733 methods can be realized by sequentially iterating the following two update rules: (i) updating the   \n734 permutree process (ii) updating the evolutionary model. Since the latter can be supported by standard   \n735 inference methods to evolutionary models, it is the updating method of the former that is particularly   \n736 important here. For the former, various inference algorithms that have been proposed for generic   \n737 marked point processes and their extensions [68, 69, 26, 96, 43, 104, 30, 55, 31, 14, 35, 109] would be   \n738 applicable, since the entity of the permutree process is a marked point process as shown in Section 3   \n739 of the main text. This section describes a useful inference method that exploits an important property   \n740 of Poisson processes, namely, that a certain Poisson process can be obtained by thinning operations   \n741 from another Poisson process with higher intensity. Section C.1 provides a brief description of the   \n742 thinning operation for the Poisson process as a preliminary to our MCMC method. Then, Section C.2   \n743 once again writes down the whole generative probabilistic model, since it should be possible to see at   \n744 a glance what the parameters to be inferred are in the permutree process and its phylogenetic tree   \n745 described in Section 5. Finally, Section C.3 describes the MCMC inference algorithm. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "746 C.1 Preliminaries: thining operations for Poisson processes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "747 Our MCMC method uses important properties of Poisson processes. Specifically, we will discuss   \n748 how to represent a certain Poisson process via another Poisson process with higher intensity.   \n749 Homogeneous Poisson process - In this paper, we mainly consider homogeneous Poisson processes   \n750 on $[0,1]\\times[0,1]$ , i.e., where the intensity function is given by a constant. A Poisson process on   \n751 $[0,1]\\times[0,1]$ with intensity $\\mu$ (where $0<\\mu<\\infty)$ is a stochastic process for a random set of points,   \n752 where the number of points belonging to $(x_{1},x_{2}]\\times(y_{1},y_{2}]$ follows a Poisson distribution with the   \n753 parameter $\\mu(x_{2}-x_{1})(y_{2}-y_{1})$ for any $0\\,\\leq\\,x_{1}\\,<\\,x_{2}\\,\\leq\\,1,$ $0\\,\\leq\\,y_{1}\\,<\\,y_{2}\\,\\leq\\,1$ . For notational   \n754 simplicity, we will denote a Poisson process on $[0,1]\\times[0,1]$ with intensity $\\mu$ by $\\mathrm{PP}(\\mu,[0,1]\\times[0,1])$ .   \n755 We also recall that in the main text, we defined this homogeneous Poisson process as the Poisson   \n756 process with the intensity function $\\lambda(\\cdot)\\,=\\,\\mu\\cdot{\\mathrm{Leb}}(\\cdot)$ as an equivalent expression, where $\\operatorname{Leb}(\\cdot)$   \n757 indicates the Lebesgue measure.   \n758 Thinning operation on Poisson processes - One of the most interesting properties of Poisson   \n759 processes is that a Poisson process with a certain intensity can be obtained by applying the thinning   \n760 operation from a Poisson process with a higher intensity. More specifically, a Poisson process with   \n761 intensity $\\mu$ can be constructed as follows:   \n762 (i) We generate a random set of points from a Poisson process with intensity $\\nu\\left(>\\mu\\right)$ .   \n763 (ii) For each point generated, independently decide whether or not to accept it with probabil  \n764 ity $\\mu/\\nu$ .   \n765 (iii) The set of only accepted points can be regarded as following the Poisson process with   \n766 intensity $\\mu$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "767 C.2 Full description of phylogenetic permutree with permutree process ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "768 As input observation data, we consider DNA (molecular) sequences $\\pmb{x}_{j}$ ( $\\mathit{\\Pi}^{j}=1,\\ldots,N)$ observed at   \n769 letter length $S$ over $N$ species. For example, the sequence $\\mathbf{\\Delta}\\mathbf{x}_{j}=\\mathsf{G A G T A C}$ has length $S=6$ . Figure   \n770 8 (top) shows the observation DNA sequences as an $S\\times\\bar{N}$ matrix. The four colors represented ", "page_idx": 18}, {"type": "image", "img_path": "OiTr2v90t7/tmp/d8a7b96c0589ce6328aa2dede0ec29cf2e9abb154815f5aa618b46720db72cb9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "OiTr2v90t7/tmp/2c85b29291c78a0214bb4033a5765f7f13302fc1ba98357d37ae3f570844510c.jpg", "img_caption": ["Number of observation sequences "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: Observed DNA sequences (top), phylogenetic permutree (middle), and observed DNA serquences assigned to leaf nodes of phylogenetic permutree (bottom). Note that each leaf node (lower) of the phylogenetic permutree does not necessarily have to be the assignment of a single observation DNA sequence. The blue dividing line in the figure below represents a group of DNA sequences where each parcel corresponds to one leaf (lower) node. The phenomenon that each observed sequences within the same group is different is due to mutation events based on the evolutionary model. ", "page_idx": 19}, {"type": "text", "text": "771 by each element of the matrix correspond to the four different letters A,C,G, and T. We regard   \n772 these DNA sequences as following a phylogenetic tree based on a permutree. First, we generate the   \n773 marked points $\\{(l_{i},m_{i}):i=1,\\bar{2},\\bar{\\dots}\\;\\overset{.}{n}\\}$ and the corresponding permutree T from the permutree   \n774 process. We recall that the transformation from marked points to permutree can be performed by   \n775 the transformation in Figure 9 (which we will call MPP2PT). Then, we represent coalescence,   \n776 recombination, multiple ancestry, and mutation events in genealogy by combining the four types of   \n777 the decorations $\\Phi,\\Phi,\\otimes,\\Phi$ with the following interpretations:   \n778   \n779   \n780   \n781   \n782   \n783   \n784   \n785   \n786   \n787 ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 Coalescence $\\mathbb{O}-\\mathbf{A}$ coalescence event represents two lineages having a common ancestral lineage.   \n\u2022 Recombination $\\oslash-\\mathbf{A}$ recombination event represents the joining of two exclusive subsequences of two   lineages by one lineage.   \n\u2022 Partition $\\otimes-\\,\\mathrm{We}$ give the decoration $\\otimes$ the role of division so that a single permutree can repres e nt a phylogenetic tree with   multiple ancestors. Specifically, we connect the two left edges and similarly connect the two right edges to lead to two unconnected tree structures on either side of decoration $\\otimes$ .   \n\u2022 Backward in time $\\Phi$ (optional) - We suppose that no mutation occurs when going back in time from a verte x  to a vertex with $\\Phi$ . ", "page_idx": 19}, {"type": "text", "text": "788 Figure 10 shows an intuitive illustration of the above interpretation of the transformation from a   \n789 permutree $\\mathbf{T}$ to a phylogenetic permutree $\\tau$ . We will refer to this transformation as $\\mathrm{PT}2\\mathrm{PP}:\\mathbf{T}\\mapsto T$ .   \n790 Here, we will use the vertical coordinate $l_{i,2}$ of each marked point as a representation of how far   \n791 back in time each vertex is in the phylogenetic tree. Each vertex $v$ of the phylogenetic tree $\\tau$ shall   \n792 have a hidden DNA sequence $\\boldsymbol{h}_{v}$ (i.e., a sequence of length $S$ with each element having the letter   \n793 from A,C,G, and ${\\sf T}$ ), which shall mutate according to the gene evolutionary models, such as the   \n794 Jukes-Cantor model (JC) [40] and the generalized time reversible model (GTR) [91]. Figures 11 and   \n795 12 show examples of the evolution of the hidden DNA sequences $(h_{v})_{v\\in{\\mathcal{T}}}$ (e.g., sequence length   \n796 $S=10$ ) on the phylogenetic tree $\\tau$ in the mutation-prone and mutation-resistant cases, respectively.   \n797 For notational convenience, we will denote the gene evolutionary models with mutation rate $\\alpha\\left(>0\\right)$   \n798 on the phylogenetic tree $\\tau$ and mark locations $l_{1},\\ldots,l_{n}$ by $\\mathrm{Evo}({\\mathcal T},(l_{i})_{i=1}^{n},\\alpha)$ . Finally, each of the   \n799 $N$ input sequences is independently assigned to a data path from the two-table Chinese restaurant   \n800 process (refered to as $2\\mathrm{tCRP}$ ) with the concentration parameter $\\gamma>0$ . In short, the overall model   \n801 can be summarized as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{l_{1},l_{2},\\dots l_{n}\\sim\\mathrm{PP}(\\mu,[0,1]\\times[0,1])}&{\\quad:L o c a t i o n s}\\\\ {(c_{\\Phi},c_{\\otimes},c_{\\Phi})\\sim\\mathrm{Dirichlet}(\\epsilon/4,\\epsilon/4,\\epsilon/4)}&{\\quad:D e c o r a t i o n~w e i g h t s}\\\\ {m_{i}\\sim\\mathrm{Categorical}(c_{\\Phi},c_{\\otimes},c_{\\Phi})}&{\\quad:M a r k s}\\\\ {\\textbf{T}\\gets\\mathrm{MPP}2\\mathrm{PT}((l_{i},m_{i})_{i=1}^{n})}&{\\quad:P e r m u t r e e}\\\\ {T\\gets\\mathrm{PT}2\\mathrm{PP}(\\mathrm{T})}&{\\quad:P h y l o g e n e t i c~p e r m u t r e}\\\\ {\\alpha\\sim\\mathrm{Gamma}(\\epsilon^{\\prime},\\epsilon^{\\prime})}&{\\quad:M u t a t i o n~R a t e}\\\\ {(h_{v})_{v\\in T}\\sim\\mathrm{Evo}(T,(l_{i})_{i=1}^{n},\\alpha)}&{\\quad:D M\\epsilon~e v o l u t i o n}\\\\ {Z_{1},\\dots,Z_{N}\\sim2\\mathrm{tCRP}(\\gamma)}&{\\quad:D a t a~p a t h s}\\\\ {x_{j}\\sim\\mathrm{Evo}(T|z_{j},l_{Z_{i}},\\alpha)}&{\\quad:O b s e r v a t i o n~s e q u e n c e}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "802 for $i=1,2,\\dots,n$ and $j=1,\\dots,N$ , where $\\tau_{|z_{j}}$ refers to a phylogenetic tree (a tree consisting   \n803 of one edge and two vertices at either end) from which only the leaf nodes and their children are   \n804 extracted from the phylogenetic tree $\\tau$ . Since the variables $\\epsilon$ and $\\epsilon^{\\prime}$ are hyperparameters for the   \n805 non-informative prior distributions, it is standard to use them fixed to tiny values. Can we then   \n806 consider how to directly infer the above generative probability model? Certainly, it is possible in   \n807 principle to infer the above generative probability model as it is by direct updating of the permutree   \n808 process as shown in Figure 13. However, in such direct inference, the complexity $n$ is often strongly   \n809 affected by bad local modes, and often the Markov chain is entangled in the local optima, resulting in   \n810 slow convergence. Therefore, using the properties of Poisson processes described in the preparation,   \n811 a method can be considered to reduce the influence of such local optima by taking the dare to have   \n812 redundant model parameters. Equation (7) and (9) can be rewritten as follows. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\hat{l}_{1},\\hat{l}_{2},\\dots,\\hat{l}_{K}\\sim\\mathrm{PP}(\\mu,[0,1]\\times[0,1])}&&{\\colon R e d u n d a n t\\ l o c a t i o n s}\\\\ &{\\hat{m}_{i}\\sim\\mathrm{Categorical}(c_{\\Phi},c_{\\Phi},c_{\\Phi})}&&{\\colon R e d u n d a n t\\ m a r k s}\\\\ &{\\quad\\quad\\quad\\quad\\,b_{i}\\sim\\mathrm{Bernoulli}(\\mu/\\nu)}&&{\\colon B i n a r y\\ i n d i c a t o r s}\\\\ &{l_{1},l_{2},\\dots l_{n}\\gets\\left\\{\\hat{l}_{1},\\dots,\\hat{l}_{K}\\ |\\ b_{i}=1\\left(i=1,\\dots,K\\right)\\right\\}}&&{\\colon L o c a t i o n s}\\\\ &{m_{1},m_{2},\\dots\\cdot m_{n}\\gets\\left\\{\\hat{m}_{1},\\dots,\\hat{m}_{K}\\ |\\ b_{i}=1\\left(i=1,\\dots,K\\right)\\right\\}}&&{\\colon M a r k s}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "813 for $i=1,\\ldots,K$ . The above is the full phylogenetic tree model based on the permutree. One point to   \n814 recall here is that permutrees includes binary trees and Cambrian trees as special cases (as discussed   \n815 in Remark 2.1 of the main text). Therefore, the permutree process can be attributed to various   \n816 models by adjusting the prior distribution for the decoration weights $(c_{\\mathbb{D}},c_{\\otimes},c_{\\mathbb{D}},c_{\\mathbb{D}})$ . Figure 14 (left)   \n817 shows the permutree process with $(c_{\\Phi},c_{\\otimes},c_{\\otimes},c_{\\emptyset})\\,\\sim\\,\\mathrm{Dirichlet}(\\epsilon/2,0,0,\\epsilon/2)$ as BINARYTREE   \n818 (restricting the expressive power to b ina ry tr ees ). Figure 14 (right) shows $(c_{\\mathbb{O}},c_{\\otimes},c_{\\mathbb{O}},c_{\\mathbb{O}})\\,\\sim$   \n819 Dirichlet $(\\epsilon/3,0,\\epsilon/3,\\epsilon/3)$ as CAMBRIANTREE (restricting it to Cambrian trees).   \n820 Joint probability - For notational convenience, we use $\\pmb{X}\\,:=\\,(\\pmb{x}_{1},\\ldots,\\pmb{x}_{N})$ , $\\pmb{H}\\,:=\\,(\\pmb{h}_{v})_{v\\in\\mathcal{T}}$ ,   \n821 $\\boldsymbol{Z}:=\\:(Z_{1},\\ldots,Z_{N})$ , $\\pmb{b}~:=~(b_{1},\\ldots,b_{K})$ , $\\hat{L}\\;:=\\;(\\hat{l}_{1},...,\\hat{l}_{K})$ , $\\hat{\\pmb{m}}~:=~\\left(\\hat{m}_{1},...,\\hat{m}_{K}\\right)$ , and $\\c=$ ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "OiTr2v90t7/tmp/95e9074409028a404dfc925d4d295781aabaaf614f38d4d26e75ca47c5ef25e7.jpg", "img_caption": ["Figure 9: (Reprinted from the main text.) Left: Marked point process - We introduce an intensity function $\\lambda$ (e.g., uniform measure) on the plane $[0,1]\\times[0,1]$ (top figure). Then, we generate random locations $\\boldsymbol{l}_{1},\\ldots,\\boldsymbol{l}_{n}$ from the Poisson point process of the intensity $\\lambda$ (middle figure). Finally, for each random location, we independently assign one of the decorations $\\{\\Phi,\\otimes,\\oslash,\\oslash\\}$ from the uniform categorical distribution as a random mark $m_{i}$ $(i=1,\\ldots,n)$ ) (bottom  f ig ur e ).  T he resulting marked point process $\\{(l_{i},m_{i}):i=1,2,\\dots n\\}$ can be regarded as a random decorated permutation. Right: Transformation to random permutree - Note that the marked points generated from the marked point process can be considered as a decorated permutation by noting its horizontal and vertical ordering. Since decorated permutations and leveled permutrees have a one-to-one correspondence, we are guaranteed to be able to construct their bijective transformation. First, auxiliary lines (red dashed lines) are drawn below decorations $\\otimes,\\,\\emptyset$ and above decorations $\\otimes,\\otimes$ . From this point on, we will extend the permutree edges, but it is im p o rt ant to emphasize that the   pe r mutree edges do not cross these auxiliary lines. Next, if we look at the auxiliary lines extending all the way to the bottom, we can see that this divides the lower region into smaller sub-regions (indicated by the red oval). Then, one edge is extended from each sub-region. The edges are extended from bottom to top, and when the height of each vertex is reached, the edges adjacent to that vertex are connected (indicated by the gray boxes). This is done until all vertices are covered, resulting in a leveled permutree. Finally, if we forget about the vertical position of each vertex in the leveled permutree and focus only on its structure as a directed tree, we obtain the corresponding permutree. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "822 $(c_{\\mathbb{D}},c_{\\otimes},c_{\\mathbb{D}},c_{\\mathbb{D}})$ . We obtain the following joint probability density function: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{P_{\\mathrm{joint}}(X,H,b,Z,\\hat{L},\\hat{m},c)=P_{\\mathrm{obs}}\\left(X;\\hat{L},\\hat{m},b,z,\\alpha\\right)\\cdot P_{\\mathrm{evo}}\\left(H;\\hat{L},\\hat{m},b,\\alpha\\right)}\\\\ &{}&{P_{\\mathrm{PP}}\\left(\\hat{L};\\nu\\right)\\cdot P_{\\mathrm{Bernoulli}}(b;\\mu/\\nu)\\cdot P_{\\mathrm{Categorical}}\\left(\\hat{m};c\\right)}\\\\ &{}&{\\cdot P_{\\mathrm{Dirichlet}}\\left(c;\\epsilon\\right)\\cdot P_{\\mathrm{Gamma}}\\left(\\alpha;\\epsilon^{\\prime}\\right)\\cdot P_{\\mathrm{2tCRP}}\\left(Z;\\gamma\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "823 where $P_{\\mathrm{obs}}$ is the probability density function (PDF) of Equation (15), $P_{\\mathrm{evo}}$ is PDF of Equation (13),   \n824 $P_{\\mathrm{PP}}$ is PDF of Equation (16), and subsequent terms are PDFs of the standard distributions. The   \n825 posterior distribution of the parameters $H,b,w,z,\\hat{L},\\hat{m},c$ to be estimated is proportional to this   \n826 joint probability density. ", "page_idx": 21}, {"type": "text", "text": "827 C.3 Bayesian inference algorithm for phylogenetic permutree ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "828 We can construct the MCMC algorithm by iteratively repeating the following update rules for the   \n829 DNA evolution $\\pmb{H}$ on the phylogenetic permutree, the binary indicators $^{b}$ , the leaf node weights $\\mathbf{\\nabla}w$ ,   \n830 the observation assignments $_{z}$ , the redundant locations $\\hat{L}$ , the redundant marks $\\hat{m}$ , and the decoration   \n831 weights $^c$ .   \n832 Update rule for DNA evolution $\\pmb{H}$ - We recall that each element of the matrix $\\pmb{H}=(H_{s,j})_{S\\times N}$   \n833 consists of one of the letters A, C, $\\odot$ , or $\\intercal$ . Using Equation (21), we calculate the joint probability that   \n834 each element $H_{s,j}$ is A, C, G, or $\\mathsf{T}$ , respectively, and let $p_{\\mathsf{A}},p_{\\mathsf{C}},p_{\\mathsf{G}}$ , or $p_{\\mathsf{T}}$ denote them respectively.   \n835 Then we obtain the following Gibbs update rule: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "OiTr2v90t7/tmp/2487a3ac547547718f0e03fff5fc7d2eb513e4c201c80032340214a741f8022b.jpg", "img_caption": ["Figure 10: Intuitive illustration of transformation PT2PP from permutree (left) to phylogenetic permutree (right). The number assigned to each vertex v represents the function $q(\\mathbf{v})$ (i.e., the order of the vertices vertically from bottom to top). We can regard this transformation as giving the role of the partition (red dotted line in the right figure) to the decoration $\\otimes$ (i.e., the vertex with two parents and two children in the left figure). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\nH_{s,j}\\sim\\operatorname{Categorical}(p_{\\mathsf{A}},p_{\\mathsf{C}},p_{\\mathsf{G}},p_{\\mathsf{T}})\\qquad(s=1,\\ldots,S,\\mathrm{and~}j=1,\\ldots,N)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "836 Update rule for binary indicators $^{b}$ - For each $i=1,\\ldots,K$ , we can obtain the Gibbs update rule   \n837 derived by calculating the posterior probability ratio for $b_{i}=0$ and $b_{i}=1$ using Equation (21).   \n838 Specifically, we suppose that the value of the joint density for $b_{i}=0$ is $\\pi_{0}$ and the value for $b_{i}=1$ is   \n839 $\\pi_{1}$ , and then we obtain the following update rule: ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{i}\\sim\\mathrm{Bernoulli}\\left(\\pi_{1}/(\\pi_{0}+\\pi_{1})\\right)\\qquad(i=1,\\ldots,K).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "840 Update rule for leaf node weights $\\mathbf{\\nabla}w$ - From the conjugacy of the Dirichlet and Categorical   \n841 distributions, we obtain the following Gibbs update rule: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bigl(w_{1}\\ldots,w_{|\\mathcal{L}N(T)|}\\bigr)\\sim\\mathrm{Dirichlet}\\Bigl(\\mathcal{N}_{1}+\\epsilon^{\\prime\\prime},\\ldots,\\mathcal{N}_{|\\mathcal{L}N(T)|}+\\epsilon^{\\prime\\prime}\\Bigr)\\qquad(i=1,\\ldots,K),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "842 where ${\\mathcal N}_{i}$ $(i\\;=\\;1,...\\,,|\\mathcal{L}\\mathcal{N}(\\mathcal{T})|)$ indicates the number of the observation sequences $\\pmb{x}_{j}$ $(j\\ =$   \n843 $1,\\ldots,N)$ which is assigned to the $i$ th leaf node of the phylogenetic permutree $\\tau$ .   \n844 Update rule for observation assignments $_{z}$ - Using Equation (21), we calculate the joint probability   \n845 that each observation sequence $\\pmb{x}_{j}$ $(j=1,\\ldots,N)$ is assigned to the $i$ th $(i=1,\\dots,|\\mathcal{L}\\mathcal{N}(\\mathcal{T})|)$ leaf   \n846 node of the phylogenetic permutree $\\tau$ , and let $\\bar{w}_{i}$ denote it. Then we obtain the following Gibbs   \n847 update rule: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\nz_{j}\\sim\\mathrm{Categorical}\\Big(\\bar{w}_{1},\\cdot\\cdot\\cdot,\\bar{w}_{|\\mathcal{L}\\mathcal{N}(\\mathcal{T})|}\\Big)\\qquad(j=1,\\cdot\\cdot\\cdot,N)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "848 Update rule for redundant locations $\\hat{L}$ - We use the simple Metropolis-Hastings (MH) method.   \n849 For each position, we generate a new candidate sample from the normalized probability measure $\\hat{\\lambda}$ of   \n850 the intensity function $\\lambda$ , that is, $\\hat{l}_{i}\\sim\\hat{\\lambda}\\;(i=1,\\cdot\\cdot\\cdot,K)$ , and decide whether to adopt it through the   \n851 MH acceptance/rejection scheme using Equation (21).   \n852 Update rule for redundant marks $\\hat{m}$ - Using Equation (21), we calculate the joint probability   \n853 that each element $\\hat{m}_{i}$ has $\\Phi,\\Phi,\\otimes,\\Phi$ , respectively, and let $p_{\\mathbb{O}},p_{\\mathbb{O}},p_{\\otimes},p_{\\mathbb{O}}$ denote them respectively.   \n854 Then we obtain the Gibbs   up d a te  r u le: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{m}_{i}\\sim\\mathrm{Categorical}\\Big(p_{\\mathbb{0}},p_{\\mathbb{0}},p_{\\mathbb{0}},p_{\\mathbb{0}}\\Big)\\qquad(i=1,\\ldots,K)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "image", "img_path": "OiTr2v90t7/tmp/7f7a68230fae5547102a5ec4cf413b86ea68d3f9950e86468710e87acf61e030.jpg", "img_caption": ["Figure 11: Phylogenetic trees and DNA evolution through Jukes-Cantor evolutionary model with mutation rate $\\alpha=0.1$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "855 Update rule for decoration weights $^c$ - From the conjugacy of the Dirichlet and Categorical   \n856 distributions, we obtain the following Gibbs update rule: ", "page_idx": 23}, {"type": "equation", "text": "$$\n(c_{\\emptyset},c_{\\emptyset},c_{\\emptyset},c_{\\emptyset})\\sim\\mathrm{Dirichlet}\\Big(N_{\\emptyset}+\\epsilon/4,N_{\\emptyset}+\\epsilon/4,N_{\\emptyset}+\\epsilon/4,N_{\\emptyset}+\\epsilon/4\\Big),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "857 where $\\mathcal{N}_{*}\\left(*\\in\\{\\mathbb{O},\\mathbb{O},\\otimes,\\mathbb{O}\\}\\right)$ indicates the number of the marks $\\hat{m}_{i}$ ${\\mathit{\\Omega}}^{\\prime}{\\mathit{i}}=1,\\ldots,K)$ which has the   \n858 decoration $^*$ . ", "page_idx": 23}, {"type": "text", "text": "859 C.4 Empirical impact of finite truncation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "860 To investigate the empirical impact of finite censoring on the marked stick-breaking process described   \n861 in Section 4, we report in Figure 15 the prediction performance for different levels of finite censoring,   \n862 $K=25$ , 50, 100 and 150, in the same experimental setup as in the main text (Section 5). It can be   \n863 seen that when the level of finite truncations is extremely restricted, the prediction performance has   \n864 been reduced, while when some level of censoring is ensured, the prediction performance is not so   \n865 reduced. This can be seen as reflecting the fact that the marked stick-breaking process can adjust its   \n866 own real model complexity in a data-driven manner according to the observation data. ", "page_idx": 23}, {"type": "image", "img_path": "OiTr2v90t7/tmp/82473f88da80f527d0442d674ec5ae92659c56c15fa36264e656c6bac5851cea.jpg", "img_caption": ["Figure 12: Phylogenetic trees and evolution of DNA lineages through Jukes-Cantor evolutionary model with mutation rate $\\alpha=0.001$ (i.e., a situation where mutations are almost unlikely to occur). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "OiTr2v90t7/tmp/670034020388331c2054dc0da0ca90643a7e683ce5d4e35239f6f27b7f769d04.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 13: Illustration of simplest inference method for permutree process as marked point process. The current leveled permutree in Markov chain Monte Carlo inference corresponds to a certain state of the marked point process (left). One marked point (slightly enlarged and colored red) is chosen to be a candidate for updating. The region to be updated is quantized (center) to generate a new candidate marked point (slightly enlarged and colored blue) from the conditional posterior probability (or some proposal distribution). Finally, the generated new marked points are updated or not by the Metropolis-Hastings scheme, which is the next state of the Markov chain (right). ", "page_idx": 24}, {"type": "text", "text": "867 D Remaining challenges ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "868 The main difficulty in applying the permutree process to data modeling is how to handle its unlimited   \n869 finite or infinite model complexity (i.e., number of vertices). Roughly speaking, it is not possible   \n870 in principle to naively implement a model with infinite parameters on current computers. This is a   \n871 central topic in the BNP field, and we have historically had two policies. One is to represent models   \n872 of infinite complexity such that finite truncation works reasonably well. This corresponds just to   \n873 the representation methods for the stick-breaking process [89] in Dirichlet process infinite mixture   \n874 models [79] and the beta-Bernoulli process in infinite factor models [95, 93]. The other method is a   \n875 model representation in which, in conjunction with the finite amount of observed data, the model   \n876 activates only as many of the potentially infinite number of parameters as necessary. This corresponds   \n877 to the Chinese restaurant process [74, 76] in the mixure models or the Indian buffet process [29] in   \n878 the factor models. While Section 4 focuses on the former policy, this section will explore the latter. ", "page_idx": 24}, {"type": "image", "img_path": "OiTr2v90t7/tmp/01c235cbf4bb120c03bd8679d46414fbd689c7bed185e7b8d5f2d43c6d1c7e3e.jpg", "img_caption": ["Figure 14: Binary tree (left) and Cambrian tree (right) attributed from permutree by restricting decoration weights. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "OiTr2v90t7/tmp/ef16db0fe9045dd78c23d4f1440da2e927a81bd9148eccc62e26b6ba5a901bff.jpg", "img_caption": ["Figure 15: Effect of prediction performance on finite truncation level $K=25$ , 50, 100 and 150 of marked stick-breaking process. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "879 D.1 Preliminary: ordered Chinese restaurant process ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "880 We begin our discussion with a representation using oCRP for binary trees, a special case of   \n881 permutrees. Let $\\theta>0$ be the concentration parameters and $\\alpha>0$ the discount parameter. We will   \n882 now take a so-called spinal decomposition (Figure 16 left) of the binary tree. In the metaphor of CRP,   \n883 the customer can be viewed as as seeking a dish at the terminal node of the binary tree. The new   \n884 customer can then either proceed to the existing subtree or create a new branch on one of the edges,   \n885 according to the proportions shown in Figure 16 right. This can be viewed as CRP with random   \n886 ordering of the CRP tables.   \n887 Ordered Chinese restaurant process (oCRP) [77, 85] - This stochastic process is a generative   \n888 prbabilistic model that constructs a random binary tree by means of a recursive structure as follows.   \n889 Let $\\alpha$ and $\\theta$ be the discount parameter and the concentration parameter, respectively.   \n90 \u2022 The first customer goes straight from the root to form one terminal node.   \n91 \u2022 The second customer forms a split between the root and the terminal node where the first   \n92 customer is located. At this stage, the advanced subtree of the second customer is assigned   \n93 a weight of $1-\\alpha$ and each edge of the split spinal cord is assigned a weight of $\\alpha$ and $\\theta$ ,   \n94 respectively.   \n95 \u2022 The third and subsequent guests determine their own destination according to the proportion   \n96 of weights assigned to the subtree and each edge on the spinal cord. If it chooses an edge on   \n97 the spinal cord, it creates a new branch there to become a subtree and assigns weight $\\alpha$ to   \n98 the newly created edge on the spinal cord. If it moves on to an existing subtree, it recursively   \n99 determines its own destination according to the nested oCRPs on that subtree and adds 1 to   \n00 the weight of the subtree.   \n901 One will notice that this stochastic process is very similar to the standard Chinese restaurant process   \n902 (CRP). If each subtree is considered a table, the probability that a new customer will sit at an existing   \n903 table is proportional to the weight of the number of customers already sitting at that table minus   \n904 the discount parameter $\\alpha$ . It will be seen that this is the same situation as in the standard CRP   \n905 corresponding to the well-known Pitman-Yor process [76]. However, it differs from the standard CRP   \n906 in that when a new table is seated, that table is determined by reference to the order of the existing   \n907 tables. For this reason, this stochastic process is called an \u201cordered\u201d CRP. ", "page_idx": 25}, {"type": "image", "img_path": "OiTr2v90t7/tmp/eb637e24169a3171b7afb6ebd77617ca9bd06b27acf1d1d738168be351ef327e.jpg", "img_caption": ["Figure 16: Standard Chinese restaurant process (left) for random partition and \u201cordered\u201d Chinese restaurant process for random binary tree. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "908 One of the most important properties of oCRPs is exchangeablity: ", "page_idx": 26}, {"type": "text", "text": "909 Theorem D.1 (Proposition 1 (a) [77]). A random binary tree generated by the nested oCRPs with the   \n910 discount parameter $\\alpha$ and the concentration parameter $\\theta$ has exchangeable leaf labels for all $n\\not=1$   \n911 if and only if $\\alpha=\\theta=1/2$ . ", "page_idx": 26}, {"type": "text", "text": "912 D.2 Our attempt: Chinese restaurant street ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "913 Strategy sketch and advance notice - Recalling the requirements of (C1) and (C2) for the definition   \n914 of permutrees (Section 2), we could introduce the following metaphor of CRP (See also Figure 17):   \n915 \u2022 Each customer is looking for a dish in a Chinese restaurant street and prefers a street that is popular   \n916 with other customers, but is also willing to explore new streets on a whim. The development of the   \n917 streets, with one customer after another searching for a dish, represents the permutree evolution.   \n918 \u2022 The Chinese restaurant street has a recursive structure. The boulevard (main street) has side   \n919 streets, and each side street becomes the next boulevard, with its own next side streets recursively.   \n920 This recursive structure would be reminiscent of an existing oCRP or nested CRP that recursively   \n921 calls smaller CRPs in the overall process. When a side street becomes a cross street, it represents   \n922 a vertex with $\\otimes$ . If the side street extends only one way, it corresponds to a vertex with $\\oslash$ or $\\mathbb{\\Phi}$ .   \n923 Chinese restaurant street (CRS) - CRS is given by the recursive structure of the streets, consisting   \n924 of boulevards and side streets. Let $\\theta_{1}>0$ and $\\theta_{2}\\geq0$ be the concentration parameters and $\\alpha>0$   \n925 the discount parameter. Figure 18 illustrates the situation where a new boulevard is a small CRS in   \n926 a large CRS with the recursive structure. A CRS at a certain level consists of a boulevard and side   \n927 streets, where each side of the boulevard is weighted by the concentration parameters $\\theta_{1},\\theta_{2}$ and the   \n928 discount parameter $\\alpha$ , and each side street is assigned a weight equal to the number of customers who   \n929 proceeded to it minus the discount parameter $\\alpha$ . When the next customer enters this boulevard, the   \n930 next destination is determined according to the ratio of those weights. It would have been a wishful   \n931 idea if this vanilla CRS could be used as a permutree model, but unfortunately, it does not satisfy the   \n932 requirements (C1) and (C2) as it is.   \n933 Properties - (1) The most important feature of CRS is that it is an extension of the existing oCRP.   \n934 Specifically, in the case of $\\theta_{2}=0$ (i.e., a situation where both boulevards and side streets grow only   \n935 to one side), CRS is equivalent to oCRP for random binary trees. (2) Another important property   \n936 is exchangeability (i.e., invariance of probabilities with respect to the order in which customers   \n937 enter the process), which is often the case with variants of CRPs. For our CRS, we can show that   \n938 it is exchangeable in the case of $\\alpha=\\theta_{1}+\\theta_{2}=1/2$ , inheriting the result of exchangeability [77,   \n939 Proposition 1] of oCRP. This property would be helpful in Bayesian inference.   \n940 Theorem D.2. A random tree generated CRS with the discount parameter \u03b1 and the two concentration   \n941 parameters $\\theta_{1}$ and $\\theta_{2}$ (described in Section 3.2 of the main text) has exchangeable leaf labels for all   \n942 $n\\not=1$ if and only if $\\alpha=\\theta_{1}+\\theta_{2}=1/2$ .   \n943 Proof. This can be verified by inheriting the exchangeability of oCRPs described in Theorem D.1.   \n944 We shall consider each subtree in oCRP as a table and assign natural numbers of labels to the tables,   \n945 starting from 1 according to the order in which the tables were generated. By reducing the resolution   \n946 of the leaf labels in Theorem D.1 to table labels, the ordered tables generated by oCRP are also   \n947 exchangeable. Then, for the random tree generated by CRS, if we consider each subtree as a table   \n948 and set $\\theta_{1}+\\theta_{2}=\\theta$ , this is also attributed to the random ordered tables of oCRP with the discount   \n949 parameter $\\alpha$ and the concentration parameter $\\theta$ . Thus, by repeatedly applying the fact that the table   \n950 labels of oCRP are exchangeable only when $\\alpha=\\theta=\\theta_{1}+\\theta_{2}=1/2$ , until each table eventually   \n951 becomes a leaf node, we can confirm that the CRS has exchangeable leaf labels. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "OiTr2v90t7/tmp/ad5778d6e03c493554f174ce29ff769ad7921432e354c33a8305f8272871a7a1.jpg", "img_caption": ["Figure 17: Overview of Chinese restaurant street "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "952 E Validity of transformation from marked points to permutree ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "953 This section verifies that the marked points $\\{(l_{i},m_{i}):i=1,2,\\dots n\\}$ generated from the marked   \n954 point process are correctly transformed into permutrees by the algorithm in Figure 9 (Algorithm 1 of   \n955 the main text). This can be verified by the following procedure, which is similar to the method in the   \n956 proof of Proposition 8 in [75].   \n957 (i) There is exactly one strand in each section separated along the auxiliary line (the red dotted   \n958 line in Figure 9). This can be shown by mathematical induction on the number of nodes in   \n959 the permutree.   \n960 (ii) The graph created by the algorithm in Figure 9 has no cycles. This can be shown by using the   \n961 proof by contradiction. If the graph had cycles, it would cross the red dotted line. However,   \n962 by construction, the graph never crosses the red dotted line.   \n963 (iii) From (i) and (ii), the graph generated by the algorithm in Figure 9 is a tree, and furthermore,   \n964 since the red dotted line separates the left and right ancestor and descendant sub-trees, this   \n965 is a leveled permutree. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "OiTr2v90t7/tmp/eaf0351ff7d8a2a305bcf60b7c37445814f6a841a147426986ce5d8dae47d319.jpg", "img_caption": ["Figure 18: Generative model of Chinese restaurant street - (a): Suppose a customer encounters a new boulevard. The boulevard has a forward road and a backward road on both sides of it with an entrance in between. When the new boulevard is opened up, which direction is the forward or backward road is determined with probability $1/2$ . The figure shows the case where the forward road is up. Given two concentration parameters $\\theta_{1}>0$ and $\\theta_{2}\\geq0$ , the first customer to enter through the entrance chooses the forward road with probability $\\theta_{1}/(\\theta_{1}+\\theta_{2})$ , otherwise the backward road, and receives the dish being served at the end. We assign weights $\\theta_{1}$ and $\\theta_{2}$ to the forward and back roads, respectively. At this stage, the one that the customers did not choose between the forward and backward roads (the lower backward road, represented by the dashed line in the figure) has not yet been activated. Until both the forward and backward roads are activated, the entrance itself serves as another endpoint of this boulevard. (b): The next customer coming to this boulevard, entering through the entrance to this boulevard, will proceed to the side street on that side in the proportion according to the weights assigned to each side. This side street itself corresponds to the boulevard in the next smaller CRS in the recursive structure. That is, the forward direction of this side street (i.e., whether it extends to the left or right first in the figure) is determined by this customer with probability $1/2$ . The concept of whether the forward or backward road is chosen first on each side street determines whether the vertex corresponding to this side street in the permutree structure extends initially to the parent side or to the child side. (c): Given a discount parameter $\\alpha\\geq0$ , this side street is assigned a weight of $1-\\alpha$ , which is the number of customers who have taken the side street minus the discount parameter $\\alpha$ . The edge divided by the side street is assigned a weight of the discount parameter $\\alpha$ . (d)-(e): When the next customer decides where to go based on the ratio of weights assigned to edges and side streets, she/he may choose an inactive road (dashed line in (c)). In that case, after the new side street is added (which also releases the entrance termination facility), the backstreet beyond it (dashed line in (d)) will continue to remain inactive. (f)-(g): The above procedures are repeated sequentially and recursively. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "966 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "967 1. Claims   \n968 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n969 paper\u2019s contributions and scope?   \n970 Answer: [Yes]   \n971 2. Limitations   \n972 Question: Does the paper discuss the limitations of the work performed by the authors?   \n973 Answer: [Yes]   \n974 Justification: Section 6 clarifies the limitations and remaining challenges.   \n975 3. Theory Assumptions and Proofs   \n976 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n977 a complete (and correct) proof?   \n978 Answer: [Yes]   \n979 Justification: Appendix A provides all details for theoretical statements in Section 4.   \n980 4. Experimental Result Reproducibility   \n981 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n982 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n983 of the paper (regardless of whether the code and data are provided or not)?   \n984 Answer: [Yes]   \n985 Justification: Appendix C provides all details for our Bayesian inference algorithms.   \n986 5. Open access to data and code   \n987 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n988 tions to faithfully reproduce the main experimental results, as described in supplemental   \n989 material?   \n990 Answer: [Yes]   \n991 6. Experimental Setting/Details   \n992 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n993 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n994 results?   \n995 Answer: [Yes]   \n996 7. Experiment Statistical Significance   \n997 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n998 information about the statistical significance of the experiments?   \n999 Answer: [Yes]   \n1000 8. Experiments Compute Resources   \n1001 Question: For each experiment, does the paper provide sufficient information on the com  \n1002 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1003 the experiments?   \n1004 Answer: [Yes]   \n1005 9. Code Of Ethics   \n1006 Question: Does the research conducted in the paper conform, in every respect, with the   \n1007 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1008 Answer: [Yes]   \n1009 10. Broader Impacts   \n1010 Question: Does the paper discuss both potential positive societal impacts and negative   \n1011 societal impacts of the work performed?   \n1012 Answer: [Yes]   \n1013 Justification: In general, phylogenetic tree analysis can sometimes have significant impacts   \n1014 in our daily lives. For example, during the SARS-CoV-2 pandemic, it has recently been   \n1015 used to analyze the spread and evolution of pathogens [52, 60]. We believe that the per  \n1016 mutree process proposed in this study could be an important elemental technology for such   \n1017 phylogenetic tree analysis in the future. In order to focus on permutree methodology in   \n1018 machine learning, this paper uses the classical model for the DNA evolution model as is.   \n1019 However, it will be necessary to improve the model to a more sophisticated model that   \n1020 can represent real-world phenomena more precisely in order to provide some insights for   \n1021 real-world applications.   \n1022 11. Safeguards   \n1023 Question: Does the paper describe safeguards that have been put in place for responsible   \n1024 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1025 image generators, or scraped datasets)?   \n1026 Answer: [NA]   \n1027 12. Licenses for existing assets   \n1028 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1029 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1030 properly respected?   \n1031 Answer: [NA]   \n1032 13. New Assets   \n1033 Question: Are new assets introduced in the paper well documented and is the documentation   \n1034 provided alongside the assets?   \n1035 Answer: [NA]   \n1036 14. Crowdsourcing and Research with Human Subjects   \n1037 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1038 include the full text of instructions given to participants and screenshots, if applicable, as   \n1039 well as details about compensation (if any)?   \n1040 Answer: [NA]   \n1041 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1042 Subjects   \n1043 Question: Does the paper describe potential risks incurred by study participants, whether   \n1044 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1045 approvals (or an equivalent approval/review based on the requirements of your country or   \n1046 institution) were obtained?   \n1047 Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}]