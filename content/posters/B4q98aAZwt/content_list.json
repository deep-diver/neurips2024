[{"type": "text", "text": "Genetic-guided GFlowNets for Sample Efficient Molecular Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hyeonah ${\\bf K i m}^{1*}$ Minsu ${\\bf K}{\\bf i m}^{1}$ Sanghyeok Choi1 Jinkyoo Park1,2 Korea Advanced Institute of Science and Technology (KAIST), 2OMELET ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The challenge of discovering new molecules with desired properties is crucial in domains like drug discovery and material design. Recent advances in deep learning-based generative methods have shown promise but face the issue of sample efficiency due to the computational expense of evaluating the reward function. This paper proposes a novel algorithm for sample-efficient molecular optimization by distilling a powerful genetic algorithm into deep generative policy using GFlowNets training, the off-policy method for amortized inference. This approach enables the deep generative policy to learn from domain knowledge, which has been explicitly integrated into the genetic algorithm. Our method achieves state-of-theart performance in the official molecular optimization benchmark, significantly outperforming previous methods. It also demonstrates effectiveness in designing inhibitors against SARS-CoV-2 with substantially fewer reward calls. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Discovering new molecules is one of the fundamental tasks in the chemical domain, with applications in drug discovery [1] and material design [2]. Particularly, de novo molecular design focuses on generating novel molecules with desired properties from scratch. In this context, deep learning-based generative methods have emerged, showing promising results (e.g., [3, 4, 5, 6, 7]). However, these methods still face a key challenge: the reward function is computationally expensive (e.g., assessing binding affinity through docking simulations), while the molecule space is combinatorially large. ", "page_idx": 0}, {"type": "text", "text": "Sample-efficient molecular optimization is thus crucial for discovering high-reward molecular structures with limited reward calls, especially for real-world applicability. The recently proposed benchmark, Practical Molecular Optimization (PMO) [8], has extensively assessed the sample efficiency of various algorithms, including reinforcement learning [3, 9], active learning [10], variational autoencoders [4, 5], generative flow networks (GFlowNets) [6, 11], and classical optimization methods like Bayesian optimization [12] and genetic algorithms [13, 14]. Interestingly, the PMO benchmark has revealed a shift in algorithm rankings, with classical algorithms often outperforming recently proposed methods such as GFlowNets when the sample efficiency is considered. ", "page_idx": 0}, {"type": "text", "text": "Recent investigations [15, 16], including those highlighted by the PMO benchmark [8], indicate that the classical frameworks, especially genetic algorithms (GA), still exhibit competitive performances compared to recently proposed deep learning methods. These studies underscore that GAs effectively navigate the chemical space using domain-specific genetic operators. In contrast, deep learning methods usually do not leverage domain-specific knowledge, relying instead on deep networks to autonomously learn these insights. It can lead to inefficient training processes due to the lack of expert guidance [17]. To address this limitation, Genetic Expert Guided Learning (GEGL) [17] has been introduced, which enhances deep learning by distilling GA-generated samples into a deep generative policy using maximum likelihood estimation. This approach enables the deep generative policy to implicitly utilize domain-specific knowledge from the GA as a form of guidance. However, despite its successes, GEGL may face challenges in generalizing to unexplored regions and in learning a peaky distribution from samples, as it maximizes likelihood equally across all high-reward samples without adequately configuring the reward landscape. ", "page_idx": 0}, {"type": "image", "img_path": "B4q98aAZwt/tmp/b91c4494142669d4885659d355baae9fd7deb67d76da21ccb9a0e89bdc663675.jpg", "img_caption": ["Figure 1: Overview of Genetic GFN. Our generative policy is trained to sample molecules proportional to rewards, and the genetic search refines them to higher-reward samples. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address this, we propose a novel molecular optimization algorithm that integrates domainspecialized genetic algorithms into GFlowNets, an off-policy training method that trains policy to sample proportional to their rewards. As illustrated in Fig. 1, we first generate diverse candidates using the current policy and refine the candidates into higher-reward samples using GAs. Subsequently, we fine-tune the policy with a GFlowNet using the collected samples. Unlike the MLE training, which can be stuck in local modes when the reward landscape is peaky, GFlowNet trains the policy to sample molecules proportional to rewards. To enhance sample efficiency, we perform unsupervised pretraining on chemical datasets and regularize the GFlowNets policy with KL-divergence to align generative probability with the dataset distribution, focusing on the compact valid space. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be interpreted from both perspectives of GFlowNets and GAs: ", "page_idx": 1}, {"type": "text", "text": "GA increases the exploitation power of GFlowNets. The proposed algorithm incorporates an effective off-policy exploration into GFlowNets based on domain-specialized GA. This approach aligns with recent studies that utilize off-policy explorations to guide toward the high-reward regions [18]. Our key contribution lies in explicitly leveraging domain knowledge about chemical structures and effectively distilling it into GFlowNets, which enable exceptional performance in real-world tasks beyond small-scale molecular generations. This contribution is crucial for the field of GFlowNets, which have struggled with sample-efficient molecular optimization tasks, even when using proxy reward models for active learning [10, 8]; see Section 4.2. ", "page_idx": 1}, {"type": "text", "text": "GFlowNets increases population diversity of GA. The proposed algorithm generates diversified samples using GFlowNets, enabling GA to effectively improve samples; our method can be regarded as a new genetic algorithm with deep generative policy-based population resetting. Our GFlowNet policy, parameterized over the whole space, periodically resets the population by diversely sampling individuals proportional to rewards, mitigating premature convergence of GA [19, 20]. Experiments show that ours outperforms recent GAs in the PMO benchmark; see Section 5.1. ", "page_idx": 1}, {"type": "text", "text": "Our extensive experiments demonstrate the effectiveness and practical applicability of the proposed method. First, our method achieves the highest total score of 16.213 across 23 oracles in the Practical Molecular Optimization benchmark [8], outperforming all other baselines. Second, we conduct in in silico experiments for designing SARS-CoV-2 inhibitors. The proposed method successfully generates inhibitors with ten times fewer reward calls. Moreover, our method effectively balances optimization and diversity, achieving higher scores with increased diversity compared to previously top-ranked methods. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Sample efficient de novo molecular optimization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Molecular design is the process of proposing new molecules likely to exhibit desirable outcomes. Compared to traditional virtual screening approaches, which identify suitable molecules from virtual libraries with a large number of molecules known a priori, de novo approaches seek to generate molecule structures anew. The desired properties can be measured using score functions $\\scriptscriptstyle\\mathcal{O}$ , called oracle. Formally, molecular design can be formulated as arg $\\operatorname*{max}_{x\\in{\\mathcal{X}}}O(x)$ , where $x$ is a molecule, and $\\mathcal{X}$ denotes the chemical space which comprises all possible molecules. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The publication of standard benchmarks and datasets has facilitated the assessment of de novo design methods (e.g., GuacaMol [21], Therapeutics Data Commons (TDC) [22]). The score functions are designed to consider various properties, such as the presence and absence of substructures, similarity, isomers, structural features, physicochemical properties, biological activity, and binding affinity (i.e., docking score). Notably, the PMO benchmark [8] offers a unified framework that comprehensively evaluates the sample efficiency of a range of molecular design methods. ", "page_idx": 2}, {"type": "text", "text": "2.2 Generative flow networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Generative flow networks (GFlowNets) [23] are introduced as a new class of probabilistic models to sample a discrete compositional object $x\\in\\mathscr{X}$ from the target distribution, i.e., $P(x)\\propto e^{-\\mathcal{E}(x)}$ . In general, direct sampling from the target distribution is challenging since the partition function $\\begin{array}{r}{Z=\\sum_{x\\in\\mathcal{X}}e^{-\\mathcal{E}(x)}}\\end{array}$ is intractable when the sample space is combinatorially large. Hence, GFlowNets sample an object from an unnormalized distribution as a constructive generative process, where discrete actions iteratively modify a state \u2014 a partially constructed object. We define a trajectory as $\\boldsymbol{\\tau}=\\left(s_{0},\\dots,s_{T}\\right)$ , where $s_{T}$ is a terminal state corresponding to a fully constructed object $x$ . ", "page_idx": 2}, {"type": "text", "text": "A GFlowNet models flow $F$ of particles along a directed acyclic graph (DAG). The source and sink nodes of the DAG correspond to the initial state $s_{0}$ and terminal states $s_{T}$ , respectively. The trajectory flow $F(\\tau)$ is defined as a flow through the trajectory $\\tau$ , and the state flow $F(s)$ is defined as the sum of trajectory flows that include the state $s$ , i.e., $\\begin{array}{r}{\\dot{F(s)}=\\sum_{s\\in\\tau}F(\\tau)}\\end{array}$ . The edge flow $F(s\\rightarrow s^{\\prime})$ the sum of trajectory flows through the edge from state $s$ to $s^{\\prime}$ , i.e., $\\begin{array}{r}{F(s\\to s^{\\prime})=\\sum_{(s,s^{\\prime})\\in\\tau}F(\\tau)}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "From the flow function $F$ , we derive two policy distributions. The forward policy $P_{F}\\big(s^{\\prime}|s\\big)$ is the probability of transitioning from state $s$ to its child state $s^{\\prime}$ , defined as the edge flow $F\\dot{(}s\\rightarrow s^{\\prime}{)}$ normalized by the state flow $F(s)$ , i.e., $P_{F}(s^{\\prime}|s)=F(s\\rightarrow s^{\\prime})/F(s)$ . Similarly, the backward policy $P_{B}\\big(\\boldsymbol{s}|\\boldsymbol{s}^{\\prime}\\big)$ is the probability of moving from state $s^{\\prime}$ to its parent state $s$ , defined as $P_{B}(s|s^{\\prime})=\\bar{F}(s\\stackrel{.}{\\to}\\,$ $s^{\\prime})/F(s^{\\prime})$ . Utilizing these forward and backward policies, GFlowNets can derive an optimal sampler $\\begin{array}{r}{P(s_{T})=\\prod P_{F}(s_{t}\\bar{|}s_{t-1})=R(s_{T})/Z}\\end{array}$ if balance conditions (e.g., [6, 24, 25, 26, 27]) are satisfied. ", "page_idx": 2}, {"type": "text", "text": "Trajectory balance loss [24]. One of the most popular conditions is trajectory balance $(T B)$ , which directly parameterizes $P_{F},\\,P_{B}$ , and flow of initial state (i.e., partition function) $Z$ to satisfy the following trajectory balance condition: ", "page_idx": 2}, {"type": "equation", "text": "$$\nZ\\prod_{t=1}^{n}P_{F}\\big(s_{t}|s_{t-1}\\big)=R(s_{T})\\prod_{t=1}^{n}P_{B}\\big(s_{t-1}|s_{t}\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, this equation is converted into a loss function to be minimized along sampled trajectories, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{TB}}(\\tau;\\boldsymbol{\\theta})=\\left(\\log\\frac{Z_{\\theta}\\prod_{t=1}^{n}P_{F}(s_{t}|s_{t-1};\\boldsymbol{\\theta})}{R(x)\\prod_{t=1}^{n}P_{B}(s_{t-1}|s_{t};\\boldsymbol{\\theta})}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In GFlowNet training, employing exploratory behavior policies or replay training is allowed since GFlowNet can be trained in an off-policy manner, which is a key advantage [23, 24, 28, 18]. ", "page_idx": 2}, {"type": "text", "text": "3 Genetic-guided GFlowNets ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section describes how the desired molecules are discovered with Genetic GFN. We model the generation process of molecules as a string-based constructive process. First, we pretrain the policy to learn the distribution of valid molecules. During the optimization phase, we iteratively generate molecules and update the policy with GFlowNet training using high-reward molecules. Particularly, we introduce graph-based genetic search to refine generated samples. ", "page_idx": 2}, {"type": "text", "text": "1: Set $\\pi_{\\theta}\\leftarrow\\pi_{\\mathrm{pre}}$ , $\\mathcal{D}\\gets\\emptyset$   \n2: while $|\\mathcal{D}|\\leq\\mathrm{num}0\\mathtt{r a c l e}\\;\\mathbf{do}$   \n3: ${\\mathcal{D}}\\leftarrow{\\mathcal{D}}\\cup\\{{\\pmb x},{\\mathcal{O}}({\\pmb x})\\}$ , where $x\\sim\\pi_{\\theta}(\\cdot)$ \u25b7SMILES generation with policy   \n4: Initialize population $\\mathcal{D}_{\\mathrm{pop}}$ from $\\mathcal{D}$ \u25b7Graph-based genetic search   \n5: for $n=1$ to numGen do   \n6: $\\pmb{x}\\leftarrow\\tt C r o s s o v e r(\\pmb{x}_{1},\\pmb{x}_{2})$ , where $(x_{1},x_{2})\\in\\mathcal{D}_{\\mathrm{pop}}$   \n7: x\u2032 \u2190Mutate(x)   \n8: $\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{{\\pmb x}^{\\prime},\\mathcal{O}({\\pmb x}^{\\prime})\\},$ $\\mathcal{D}_{\\mathrm{off}}\\leftarrow\\mathcal{D}_{\\mathrm{off}}\\cup\\{\\pmb{x}^{\\prime},\\mathcal{O}(\\pmb{x}^{\\prime})\\}$   \n9: ${\\mathcal{D}}_{\\mathrm{pop}}\\leftarrow{\\mathtt{S e l e c t}}({\\mathcal{D}}_{\\mathrm{pop}}\\cup{\\mathcal{D}}_{\\mathrm{off}})$   \n10: end for   \n11: for $k=1$ to numReplay do $\\triangleright$ Updating the policy with GFlowNet training   \n12: Get $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ from $\\mathcal{D}$ with rank-based sampling (Eq. (4))   \n13: Update $\\theta$ to minimize $\\begin{array}{r}{\\frac{1}{|B|}\\sum_{\\pmb{x}\\in\\mathcal{B}}\\hat{\\mathcal{L}_{\\mathrm{IB}}}+\\bar{\\alpha}\\mathrm{KL}(\\pi_{\\theta}(\\pmb{x})||\\pi_{\\mathrm{pre}}(\\pmb{x}))}\\end{array}$   \n14: end for   \n15: end while ", "page_idx": 3}, {"type": "text", "text": "3.1 Factorized string-based generative policy and unsupervised pretraining ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Building on insights from previous works [3, 17], we employ a string-based representation strategy, simplifying the molecular generation process by reducing it to a one-directional sequence generation. We adopt a sequence generative policy using a string-based assembly strategy, especially the simplified molecular-input line-entry system (SMILES) [29]. Motivated by REINVENT [3], we parameterize the policy using a recurrent neural network architecture [30]. Then, the probability $\\pi_{\\theta}(x)$ of generating a molecule, can be factorized to $\\begin{array}{r}{\\prod_{t=1}^{n}\\pi_{\\theta}(x_{t}|x_{1},\\dotsc,\\bar{x}_{t-1})}\\end{array}$ , where $x_{1},\\ldots,x_{n}$ are characters of SMILES representation of $\\textbf{\\em x}$ . ", "page_idx": 3}, {"type": "text", "text": "As demonstrated in previous studies, including [3, 17, 8], pretraining is inevitable since training the generative policy from scratch is excessively sample-inefficient. Therefore, our policy is pre-trained to maximize the likelihood of valid molecules on existing chemical datasets $\\mathcal{D}_{\\mathrm{pre}}$ ; note that pretraining does not require oracle information. Precisely, the policy is pretrained to minimize the following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{pre}}({\\boldsymbol{x}})=-\\sum_{t=1}^{n}\\log\\pi_{\\boldsymbol{\\theta}}(x_{t}|x_{1},\\dots,x_{t-1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 GFlowNet training of the generative policy with graph-based genetic search ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To generate desirable molecules with limited reward calls, we iteratively generate samples using two distinct strategies (Section 3.2.1) and fine-tune the policy, initialized with $\\pi_{\\mathrm{pre}}$ , using a GFlowNet by replaying collected samples (Section 3.2.2). The overall procedure is described in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Molecule generation strategies in Genetic GFN ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We employ two distinct molecule generation strategies, SMILES generation with our training policy and graph-based genetic search. These two strategies are synergized to generate diversified and high-reward samples, efficiently searching the vast chemical space. ", "page_idx": 3}, {"type": "text", "text": "SMILES generation with policy. The training policy $\\pi_{\\theta}$ generates SMILES sequences. Since the policy is trained using the trajectory balance loss (see the following subsection), it is the same as sampling $\\textbf{\\em x}$ from $\\begin{array}{r}{\\prod_{t=1}^{T}P_{F}(s_{t}|s_{t-1})\\propto R(s_{T}=\\pmb{x})}\\end{array}$ , where $s_{t-1}$ is represented by previously collected SMILES to ken, and $\\log R({\\pmb x})=-\\beta\\mathcal{O}({\\pmb x})$ with an inverse temperature $\\beta$ . ", "page_idx": 3}, {"type": "text", "text": "Graph-based genetic search. To effectively search the higher-reward region, we employ a genetic algorithm that iteratively evolves populations through crossover, mutation, and selection. We adopt the operations of the graph-based genetic algorithm, Graph GA [13], which has proven to effectively search the molecule space with finely designed genetic operations; please refer to the original paper for details. The genetic search is performed as follows: ", "page_idx": 3}, {"type": "text", "text": "1. Initialize a population $\\mathcal{D}_{\\mathrm{pop}}$ : The initial population is selected from the whole buffer $\\mathcal{D}$ , consisting of samples from the policy and previous genetic search. 2. Generate offspring $\\mathcal{D}_{\\mathrm{off}}$ : A child is generated from randomly chosen two parent molecules by combining the fragments (crossover). Then, the child is randomly modified (mutation). 3. Select a new population $\\mathcal{D}_{\\mathrm{pop}}^{\\prime}$ : Sample from $\\mathcal{D}_{\\mathrm{pop}}\\cup\\mathcal{D}_{\\mathrm{off}}$ , and go back to 2. ", "page_idx": 4}, {"type": "text", "text": "One key advantage is that offspring can have a large distance from the parents in the 1D string space, even if the molecule distances are small, which is beneficial to avoid being stuck in local optima; see the experimental results in Table 3b. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Updating the generative policy with GFlowNets training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Using the generated samples, the policy is fine-tuned using the trajectory balance loss. The off-policy property of GFlowNet losses enables the utilization of refined samples from the genetic search. In particular, for better sample efficiency, we employ replay training with a rank-based reweighed buffer. ", "page_idx": 4}, {"type": "text", "text": "TB loss with KL-divergence penalty. The generative policy is trained using the trajectory balance loss in Eq. (1). Note that we set $P_{B}$ to 1 for simplicity since SMILES generations are conducted in one direction. To ensure that the policy does not deviate excessively from the pretrained policy during training, we introduce a Kullback-Leibler (KL) divergence penalty inspired by the works in language model fine-tuning [31, 32]. Thus, our model is updated to minimize the following loss function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{TB}}(\\tau;\\theta)+\\alpha\\mathrm{KL}(\\pi_{\\theta}(x)||\\pi_{\\mathrm{pre}}(x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi_{\\mathrm{pre}}$ denotes the the pre-trained policy. As a result, $\\pi_{\\theta}$ is trained to generate desired (by $\\mathcal{L}_{\\mathrm{TB}})$ ) and valid (by $\\pi_{\\mathrm{pre.}}$ ) molecules. Note that trajectories on which the proposed loss is minimized are sampled from the experience buffer. ", "page_idx": 4}, {"type": "text", "text": "Rank-based reweighed experience buffer. The rank-based reweighting biases the samples towards high-reward candidates by assigning greater weight to trajectories with higher ranks, thereby enhancing the focus on more promising solutions [33, 34]. The weight is computed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{(k|\\mathcal{D}|+\\mathrm{rank}_{\\mathcal{O},\\mathcal{D}}(\\pmb{x}))^{-1}}{\\sum_{\\pmb{x}\\in\\mathcal{D}}\\left(k|\\mathcal{D}|+\\mathrm{rank}_{\\mathcal{O},\\mathcal{D}}(\\pmb{x})\\right)^{-1}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $k$ is a weight-shifting factor, and $\\operatorname{rank}_{\\mathcal{O},\\mathcal{D}}(\\pmb{x})$ is a relative rank of value of ${\\mathcal{O}}(x)$ in the dataset $\\mathcal{D}$ . Note that we also utilize rank-based sampling in the genetic search (steps 1 and 3). ", "page_idx": 4}, {"type": "text", "text": "4 Related works ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Genetic algorithms for molecular optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Genetic algorithm (GA) is a representative meta-heuristic inspired by the evolutionary process. This subsection focuses on discussing the application of GA in molecular optimization. As one of the seminal works, a graph-based GA (Graph GA) was proposed with sophisticatedly designed operations based on chemical knowledge [13]. Note that our method also adopts Graph GA operations in the genetic search. Various strategies for molecular assembly, not limited to graphs, have been utilized in GA [35, 14, 36]. A recent contribution by [16] introduces an enhanced version of Graph GA. They introduce quantile-uniform sampling to bias the population towards containing higher reward samples while maintaining diversity. Experimental results from Mol GA demonstrate the effectiveness of GAs as strong baselines, achieving state-of-the-art (SOTA) performance in the PMO benchmark. ", "page_idx": 4}, {"type": "text", "text": "4.2 GFlowNets for molecular optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Generative Flow Networks (GFlowNets or GFN) [23, 6] have drawn significant attention in scientific discovery [37], particularly in molecular optimization and biological sequence design [6, 10, 38, 18, 39, 40, 11, 41]. GFlowNets, which are off-policy variational inference methods [42], are closely related to value-based reinforcement learning within soft Markov Decision Processes (soft MDPs) [43, 44], focusing on learning maximum entropy agents [45, 46]. This allows GFlowNets to generate ", "page_idx": 4}, {"type": "text", "text": "Table 1: Mean and standard deviation of AUC top-10 (\u2191) from five independent runs. We use oracle ID (in lexicographical order) instead of a name for better readability, and the best mean scores are denoted in bold for each task. The results of further baselines are provided in Appendix G.2. ", "page_idx": 5}, {"type": "table", "img_path": "B4q98aAZwt/tmp/74840ec7f8162ec23870616b652ed801220abc8adc74a21eb1a85060a7ceb71e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "B4q98aAZwt/tmp/f230371fafde3d3d852272098f228dd8c11139ddb4049108566ef38956b4287f.jpg", "img_caption": ["Figure 2: The optimization curve of the average scores of Top-10 over the score function calls. All optimization curves for 23 oracles are provided in Appendix G.2. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "diverse candidates for chemical and biological structures. Advances in GFlowNets have included new objective functions [6, 24, 25], improved credit assignments [26, 27], and enhanced off-policy exploration strategies [18, 47, 48, 49]. Our approach, an improved off-policy exploration technique for GFlowNets, enhances sample efficiency and scalability for moderate-scale chemical discovery. We present extensive experiments targeting the discovery of larger-scale molecules, with SMILES strings approximately 100 characters long, surpassing the scope of smaller molecule generation tasks commonly addressed in GFlowNets literature [6, 38]. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section provides extensive experimental results, including experiments on the official sampleefficient molecular optimization benchmark and in silico design for SARS-CoV-2 inhibitors. The codes are available at https://github.com/hyeonahkimm/genetic_gfn. ", "page_idx": 5}, {"type": "text", "text": "5.1 Sample efficient molecular optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this experiment section, we compare Genetic GFN with various molecular optimization methods from the perspective of sample efficiency. The performance is primarily quantified by the area under the curve (AUC), with the number of score function calls limited to 10K. Note that we rigorously follow the Practical Molecular Optimization (PMO) benchmark [8]. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Ablation studies. In the GS ablation study $\\left(-\\{\\mathrm{GS}\\}\\right)$ , the generative policy solely generates samples, while $\\epsilon$ -greedy samples from $P_{F}$ mixed with a uniform distribution. The bold text indicates the best value. ", "page_idx": 6}, {"type": "table", "img_path": "B4q98aAZwt/tmp/3cdf0353a62038b87f6c0c74a6d94057dd8733cebdabc53c1558ef4bbdc30c5e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1.1 Main results in the official benchmark of PMO ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As baselines, we employ Top-8 methods from the PMO benchmark since they recorded the best AUC Top-10 in at least one oracle. The baseline methods include various ranges of algorithms and representation strategies. First, REINVENT [3] is an RL method that tunes the policy with adjusted likelihood. Graph GA [13], STONED [14], SMILES GA [21], and SynNet [36] are genetic algorithms that utilize different assembly strategies; they use fragment-based graphs, SELFIES, SMILES, and synthesis, respectively. Additionally, a hill climbing method (SMILES-LSTM-HC [21]) and Bayesian optimization (GP BO [12]) are included. SMILES-LSTM-HC iteratively generates samples and imitates high-reward samples, while GP BO uses a surrogate model with the Gaussian process (GP) and Graph GA to optimize the GP acquisition functions in the inner loop. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we adopt additional methods, Mol GA [16] and GEGL [17]. Mol GA is an advanced version of Graph GA and outperforms other baselines in the PMO benchmark. On the other hand, GEGL is an ablated version of our approach that utilizes imitation learning with a reward-priority queue instead of GFlowNet training with rank-based sampling. For both, we adopt the original implementations23 with hyperparameters searches following the guidelines; see Appendix C. ", "page_idx": 6}, {"type": "text", "text": "The main results in Table 1 report the AUC score of Top-10 candidates with independent five runs with different seeds. In addition, Fig. 2 visually presents the Top-10 average score across the computational budget, i.e., the number of oracle calls, providing a concise overview of the results. Due to the lack of space, the best five results are provided; please check Appendix G.2 for the rest of the results. As shown in Table 1, Genetic GFN outperforms the other baselines with a total of 16.213 and attains the highest AUC Top-10 values in 14 out of 23 score functions. The results of diversity and SA score for each oracle are presented in Appendix G.5. ", "page_idx": 6}, {"type": "text", "text": "5.1.2 Controllability of the scores-diversity trade-off and ablation studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Controllability of the scores-diversity trade-off. In the benchmark, we found a pronounced tradeoff between attaining high evaluation scores within a limited budget and generating diverse molecular candidates. This section demonstrates the controllability of the score-diversity trade-off through adjustments in the inverse temperature $\\beta$ . Decreasing the inverse temperature gives more diverse candidates. The results in Fig. 3 demonstrate adjustments of $\\beta$ can control the trade-off between score and diversity, achieving Pareto-frontier to other baselines in the benchmark. Notably, Genetic GFN with $\\beta=30$ achieves a higher AUC Top-10 with a greater diversity compared to the SOTA GA method (Mol GA: 15.686 with a diversity of 0.465) and RL method ", "page_idx": 6}, {"type": "image", "img_path": "B4q98aAZwt/tmp/0341a7290791c7b3f4462ef26dd26a1f8e0bebfb69b6109c1cdccd5c5eaa6dda.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Average of Top-10 score and diversity. Note that the fragment-based GFlowNet achieves 10.957 with a diversity of 0.816. ", "page_idx": 6}, {"type": "text", "text": "(REINVENT: 15.185 with a diversity of 0.468). Similarly, the weight-shifting factor $k$ in rank-based sampling can control the trade-off; see Appendix G.1. ", "page_idx": 6}, {"type": "text", "text": "Ablation studies. The ablation studies investigate the essential components of our framework: the genetic search (GS) and the KL-divergence penalty. To assess the effectiveness of the genetic search, we also compare its performance against the exploration strategy used in previous GFlowNet studies ", "page_idx": 6}, {"type": "text", "text": "Table 3: Comparison with GFlowNet variants. Notably, samples from the GS have larger SMILES distances than LS, leading to better sample efficiency. The bold text indicates the best value. ", "page_idx": 7}, {"type": "table", "img_path": "B4q98aAZwt/tmp/f39ecbc057e407fb366b540b710eb07a9e1886232bd79e57fe09f6c67507679a.jpg", "table_caption": ["(a) Average and standard deviation of AUC scores (\u2191) ", "(b) Search distances (\u2191) "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "[6, 10]. It samples actions from a GFlowNet sampler mixed with a uniform distribution, similar to $\\epsilon$ -greedy in RL. The results, shown in Table 2, reveal that the removal of either component results in a decline in performance, underscoring the importance of employing a suitable exploration strategy. Detailed results, including statistical analysis, are provided in Appendix G.4. ", "page_idx": 7}, {"type": "text", "text": "5.1.3 Comparisons with GFlowNets variants ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare Genetic GFN with the graph-based GFlowNet [6], GFlowNet-AL [10], and the local search GFlowNet (LS-GFN) [18] using SMILES representations. LS-GFN utilizes Monte Carlo Markov Chain (MCMC) techniques, incorporating partial backtracking and reconstructing solution trajectories with the training policy as the proposal distribution [18]. The experiments are conducted on the PMO benchmark, and we implement LS-GFN with SMILES by replacing our genetic search with a local search. Note that while the original LS-GFN employs the prepend-append MDP, which does not directly apply to SMILES, we use the same one-directional SMILES generation as ours. ", "page_idx": 7}, {"type": "text", "text": "As shown in Table 3a, Genetic GFN outperforms other GFlowNet variants. Notably, generating SMILES is significantly more advantageous than generating graph-based fragments. The performance gap between Genetic GFN and LS-GFN highlights the importance of a proper exploratory policy. To further analyze, we measure the distance between samples before and after searches in $\\mathrm{GSK}3\\beta$ and JNK3. The normalized Levenshtein distances for SMILES and Tanimoto similarity for molecules are reported in Table 3b. The results show that the local search may be inefficient in effectively searching in moderate-scale chemical spaces because its capabilities heavily depend on the current policy, leading to suboptimal search performance. In contrast, our approach leverages a domain-specialized genetic search within the molecule graph space, working as an effective off-policy exploration \u2014 the samples with SMILES representation are used to train the string-based generative policy. ", "page_idx": 7}, {"type": "text", "text": "5.1.4 Sample efficient multi-objective molecular optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "According to Zhu et al. [11], we apply our method to multi-objective tasks: $\\mathrm{GSK3}\\beta\\mathbf{+}\\mathbf{JNK3}$ and $\\mathrm{GSK}3\\beta{+}\\mathrm{JNK}3{+}\\mathrm{QED}{+}\\mathrm{SA}.$ . Notably, $\\mathrm{GSK}3\\beta$ and JNK3 are potential targets of Alzheimer\u2019s Disease treatments [50]. We use a linear combination of each objective with given coefficients, and the performance is measured by hypervolumes with 1K evaluations. We obtained the results from five independent trials using different seeds. Even though Genetic GFN is not designed for multiobjective molecular optimization, it demonstrates notable performance using proper scalar-valued score functions; please see Appendix E for details. ", "page_idx": 7}, {"type": "table", "img_path": "B4q98aAZwt/tmp/9a7dda87138a3faeda74cb422b886fe94597c14c287b19261417ec5b0ddbce9d.jpg", "table_caption": ["Table 4: Average and standard deviation of hypervolumes $(\\uparrow)$ for each task. The baseline results are directly from the HN-GFN paper [11]. The bold text indicates the best value. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1.5 Further analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Active learning with Genetic GFN. Similar to GFlowNet-AL, ours can work as a generative model in multi-round active learning. We compare Genetic GFN-AL with other model-based and active learning methods; please refer to Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Genetic GFN with SELFIES representation. Genetic GFN with SELFIES generation achieves the improved sample efficiency to other SELFIES-based methods; see Appendix G.6. ", "page_idx": 7}, {"type": "image", "img_path": "B4q98aAZwt/tmp/1b3bb7a0326bb87deb771207fd881a1d4e72371966fb1f3e0db97b8dc899dfab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: The final candidates for the RdRp_6YYT target with 100 steps. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Sensitivity analysis. We provide the experimental results by varying the hyperparameters, such as the offspring size and the number of training loops; see Appendix G.8. ", "page_idx": 8}, {"type": "text", "text": "5.2 Designing inhibitors against SARS-CoV-2 targets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we conduct drug discovery experiments for Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-Cov-2), known as the novel coronavirus. One desired property is maximizing the binding affinity to the target protein. The binding affinity is measured with a docking score, which is calculated based on the energies of the interaction between the ligand and the receptor. Typically, the computation of docking scores is expensive since it involves predicting the spatial orientation and binding affinity of the molecule in the active site of the target protein. We employ Quick Vina 2 [51] docking software to assess generated molecules. ", "page_idx": 8}, {"type": "text", "text": "Additionally, QED (Quantitative Estimate of Drug-likeness) and SA (Synthetic Accessibility) are considered to quantify the drug-likeness and difficulties of synthesizing. The higher QED, which ranges [0, 1], and the lower SA, which ranges [0, 10], are desired. Therefore, we define the score function $s(x)$ for designing SARS-Cov-2 inhibitors as a linear combination of normalized scores according to the previous work [52]. Following [53] and [52], the target proteins are selected: PLPro_7JIR, a critical enzyme in the life cycle of SARS-CoV-2, and RdRp_6YYT, which is essential for the replication and the transcription of genes. ", "page_idx": 8}, {"type": "text", "text": "The experiments are conducted with up to 1000 update steps with 128 batch size [52]. As shown in Table 5, ours achieves the highest Top-100 average scores only with 100 steps, which is 10 times fewer than others. Note that the score is recalculated based on the normalized score function in Eq. (5) using average values in the MolRL-MGPT paper [52]; the full results and a more detailed experimental setup are provided in Appendix F. We also report the best candidates of 100 steps in Fig. 4 and in Fig. 5. The final molecules correspond to the Top-1 score molecules from 3 independent runs. ", "page_idx": 8}, {"type": "table", "img_path": "B4q98aAZwt/tmp/e1ac97b380205f2bdde4f19e8fa8b711bf155a4dd3761655464ab9f1b51d5b00.jpg", "table_caption": ["Table 5: Average Top-100 scores (\u2191). Ours outperforms baselines with 10 times fewer steps. The bold denotes the best scores. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper introduces a Genetic-guided GFlowNet (Genetic GFN), which integrates a domain-specific genetic algorithm to guide the GFlowNet policy toward higher-reward samples. The method employs off-policy training with a rank-based reweighted buffer, enhancing the policy as a powerful amortized inference sampler for chemical discovery. Extensive experiments demonstrate that Genetic GFN effectively generates desirable molecules within the high-dimensional chemical space, including long chemical structure sequences (e.g., $\\geq100$ ). On the other hand, our approach can be considered as a novel population reinitialization strategy for genetic algorithms using GFlowNets, which sample diverse objects proportional to rewards. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations and future works. Our method assumes the existence of effective genetic algorithms, which is valid for the molecular design domain. However, designing domain-specific operators for genetic algorithms can be challenging in other fields. One possible future work is to enhance genetic algorithms using the neural policy similar to recent studies [54, 55]. Another direction is to extend our approach to other domains, such as combinatorial optimization. For instance, we could utilize a powerful GA, hybrid genetic search [56], to design a GFlowNet-based solver for routing problems. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. This paper introduces a new generative model, significantly enhancing sample efficiency in molecular optimization. This advance is likely to hold substantial promise for this field, potentially accelerating the development of new therapies and advanced materials. Our research is currently focused on in-silico experiments. The potential safety concerns of discovered molecules are further examined in the subsequent processes, such as in-vitro experiments and pre-clinical tests. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00410082). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] James P Hughes, Stephen Rees, S Barrett Kalindjian, and Karen L Philpott. Principles of early drug discovery. British Journal of Pharmacology, 162(6):1239\u20131249, 2011.   \n[2] Alexander W Hains, Ziqi Liang, Michael A Woodhouse, and Brian A Gregg. Molecular semiconductors in organic photovoltaic cells. Chemical Reviews, 110(11):6689\u20136735, 2010.   \n[3] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of Cheminformatics, 9(1):1\u201314, 2017.   \n[4] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 4(2):268\u2013276, 2018.   \n[5] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International Conference on Machine Learning, pages 2323\u20132332. PMLR, 2018.   \n[6] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. In Advances in Neural Information Processing Systems, volume 34, pages 27381\u201327394, 2021.   \n[7] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-ofdistribution generation. In International Conference on Machine Learning, pages 18872\u201318892. PMLR, 2023.   \n[8] Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: a benchmark for practical molecular optimization. Advances in Neural Information Processing Systems, 35:21342\u201321357, 2022.   \n[9] Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of molecules via deep reinforcement learning. Scientific Reports, 9(1):1\u201310, 2019.   \n[10] Moksh Jain, Emmanuel Bengio, Alex Hern\u00e1ndez-Garc\u0131a, Jarrid Rector-Brooks, Bonaventure FP Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et al. Biological sequence design with GFlowNets. In International Conference on Machine Learning, pages 9786\u20139801. PMLR, 2022.   \n[11] Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sample-efficient multi-objective molecular optimization with GFlowNets. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] Austin Tripp, Gregor N. C. Simm, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. A fresh look at de novo molecular design benchmarks. In NeurIPS 2021 AI for Science Workshop, 2021.   \n[13] Jan H Jensen. A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space. Chemical Science, 10(12):3567\u20133572, 2019.   \n[14] AkshatKumar Nigam, Robert Pollice, Mario Krenn, Gabriel dos Passos Gomes, and Alan Aspuru-Guzik. Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES. Chemical Science, 12(20):7079\u20137090, 2021.   \n[15] AkshatKumar Nigam, Pascal Friederich, Mario Krenn, and Alan Aspuru-Guzik. Augmenting genetic algorithms with deep neural networks for exploring the chemical space. In International Conference on Learning Representations, 2020.   \n[16] Austin Tripp and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Genetic algorithms are strong baselines for molecule generation. arXiv preprint arXiv:2310.09267, 2023.   \n[17] Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization with genetic exploration. In Advances in Neural Information Processing Systems, volume 33, pages 12008\u201312021, 2020.   \n[18] Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search GFlowNets. In International Conference on Learning Representations, 2024.   \n[19] David B Fogel. An introduction to simulated evolutionary optimization. IEEE Transactions on Neural Networks, 5(1):3\u201314, 1994.   \n[20] Hari Mohan Pandey, Ankit Chaudhary, and Deepti Mehrotra. A comparative review of approaches to prevent premature convergence in GA. Applied Soft Computing, 24:1047\u20131077, 2014.   \n[21] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. GuacaMol: benchmarking models for de novo molecular design. Journal of Chemical Information and Modeling, 59(3):1096\u20131108, 2019.   \n[22] Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf H Roohani, Jure Leskovec, Connor W. Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.   \n[23] Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research, 24(210):1\u201355, 2023.   \n[24] Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in GFlowNets. In Advances in Neural Information Processing Systems, volume 35, pages 5955\u20135967, 2022.   \n[25] Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning, pages 23467\u201323483. PMLR, 2023.   \n[26] Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. In International Conference on Machine Learning, pages 26878\u201326890. PMLR, 2023.   \n[27] Hyosoon Jang, Minsu Kim, and Sungsoo Ahn. Learning energy decompositions for partial inference of GFlowNets. In International Conference on Learning Representations, 2024.   \n[28] Heiko Zimmermann, Fredrik Lindsten, Jan-Willem van de Meent, and Christian A Naesseth. A variational perspective on generative flow networks. Transactions on Machine Learning Research, 2023.   \n[29] David Weininger. SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of Chemical Information and Computer Sciences, 28(1):31\u201336, 1988.   \n[30] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.   \n[31] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.   \n[32] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Advances in Neural Information Processing Systems, volume 36, pages 53728\u201353741, 2023.   \n[33] Austin Tripp, Erik Daxberger, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Sample-efficient optimization in the latent space of deep generative models via weighted retraining. In Advances in Neural Information Processing Systems, volume 33, pages 11259\u201311272, 2020.   \n[34] Minsu Kim, Federico Berto, Sungsoo Ahn, and Jinkyoo Park. Bootstrapped training of scoreconditioned generator for offline design of biological sequences. In Advances in Neural Information Processing Systems, volume 36, pages 67643\u201367661, 2023.   \n[35] Naruki Yoshikawa, Kei Terayama, Masato Sumita, Teruki Homma, Kenta Oono, and Koji Tsuda. Population-based de novo molecule generation, using grammatical evolution. Chemistry Letters, 47(11):1431\u20131434, 2018.   \n[36] Wenhao Gao, Roc\u00edo Mercado, and Connor W. Coley. Amortized tree generation for bottomup synthesis planning and synthesizable molecular design. In International Conference on Learning Representations, 2022.   \n[37] Moksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, and Yoshua Bengio. GFlowNets for AI-driven scientific discovery. Digital Discovery, 2(3):557\u2013577, 2023.   \n[38] Max W Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving GFlowNet training. In International Conference on Machine Learning, pages 30956\u201330975. PMLR, 2023.   \n[39] Pouya M Ghari, Alex Tseng, G\u00f6kcen Eraslan, Romain Lopez, Tommaso Biancalani, Gabriele Scalia, and Ehsan Hajiramezanali. Generative flow networks assisted biological sequence editing. In NeurIPS 2023 Generative AI and Biology (GenBio) Workshop, 2023.   \n[40] Miruna Cretu, Charles Harris, Julien Roy, Emmanuel Bengio, and Pietro Li\u00f2. SynFlowNet: Towards molecule design with guaranteed synthesis pathways. In ICLR 2024 Workshop on Generative and Experimental Perspectives for Biomolecular Design, 2024.   \n[41] Hyeonah Kim, Minsu Kim, Sungsoo Ahn, and Jinkyoo Park. Symmetric replay training: Enhancing sample efficiency in deep reinforcement learning for combinatorial optimization. In Proceedings of the 41st International Conference on Machine Learning, pages 24110\u201324136. PMLR, 2024.   \n[42] Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. In International Conference on Learning Representations, 2023.   \n[43] Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy GFlowNets with soft Q-learning. In International Conference on Artificial Intelligence and Statistics, pages 2593\u20132601. PMLR, 2024.   \n[44] Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. In The 40th Conference on Uncertainty in Artificial Intelligence, 2024.   \n[45] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[46] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, pages 1352\u20131361. PMLR, 2017.   \n[47] Jarrid Rector-Brooks, Kanika Madan, Moksh Jain, Maksym Korablyov, Cheng-Hao Liu, Sarath Chandar, Nikolay Malkin, and Yoshua Bengio. Thompson sampling for improved exploration in GFlowNets. In ICML 2023 Structured Probabilistic Inference & Generative Modeling (SPIGM) Workshop, 2023.   \n[48] Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperatureconditional GFlowNets. In International Conference on Machine Learning, 2024.   \n[49] Shuai Guo, Jielei Chu, Lei Zhu, and Tianrui Li. Dynamic backtracking in GFlowNet: Enhancing decision steps with reward-dependent adjustment mechanisms. arXiv preprint arXiv:2404.05576, 2024.   \n[50] Yibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional graph generative model. Journal of Cheminformatics, 10:1\u201324, 2018.   \n[51] Amr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate, and reliable molecular docking with QuickVina 2. Bioinformatics, 31(13):2214\u20132216, 2015.   \n[52] Xiuyuan Hu, Guoqing Liu, Yang Zhao, and Hao Zhang. De novo drug design using reinforcement learning with multiple GPT agents. In Advances in Neural Information Processing Systems, volume 36, pages 7405\u20137418, 2023.   \n[53] David M Rogers, Rupesh Agarwal, Josh V Vermaas, Micholas Dean Smith, Rajitha T Rajeshwar, Connor Cooper, Ada Sedova, Swen Boehm, Matthew Baker, Jens Glaser, et al. SARS-CoV2 billion-compound docking. Scientific Data, 10(1):173, 2023.   \n[54] Tianfan Fu, Wenhao Gao, Connor Coley, and Jimeng Sun. Reinforced genetic algorithm for structure-based drug design. In Advances in Neural Information Processing Systems, volume 35, pages 12325\u201312338, 2022.   \n[55] AkshatKumar Nigam, Robert Pollice, and Al\u00e1n Aspuru-Guzik. Parallel tempered genetic algorithm guided by deep neural networks for inverse molecular design. Digital Discovery, 1(4):390\u2013404, 2022.   \n[56] Thibaut Vidal, Teodor Gabriel Crainic, Michel Gendreau, Nadia Lahrichi, and Walter Rei. A hybrid genetic algorithm for multidepot and periodic vehicle routing problems. Operations Research, 60(3):611\u2013624, 2012.   \n[57] Joshua Meyers, Benedek Fabian, and Nathan Brown. De novo molecular design and generative models. Drug discovery today, 26(11):2707\u20132715, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "B4q98aAZwt/tmp/070c5f387fee8ed00fc7fa23c30caad955e4ef191f6a022936115616b680bc94.jpg", "img_caption": ["Figure 6: Pretraining and Genetic GFN fine-tuning framework "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A Implementation details of Genetic GFN ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Genetic GFN is implemented on top of the PMO benchmark source code (MIT license).4 Mostly, we adopt the REINVENT implementation including the RNN models and experience buffer; the code is included in the benchmark, and the original implementation is also accessible with Apache-2.0 license.5 See the following subsections for details. ", "page_idx": 13}, {"type": "text", "text": "A.1 Network architecture and pretraining ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Network architecture. Our policy network is parameterized using a recurrent neural network containing multiple GRU cells [30]. In molecular optimization, RNN-based models with string molecular representations have proven to be successful [3, 17, 52]. In experiments, we employ the same hyperparameters to directly compare with REINVENT, whose input embedding dimension is 128 and hidden dimension is 512 with three layers. ", "page_idx": 13}, {"type": "text", "text": "Pretraining. According to the PMO benchmark guidelines [8], the pre-training is conducted on ZINC 250K. The overall framework is illustrated in Fig. 6. Since the network architecture is the same as REINVENT, we adopt the provided pretrained model for REINVENT in the PMO benchmark. This allows the direct comparison of fine-tuning approaches. ", "page_idx": 13}, {"type": "text", "text": "A.2 Hyperparameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We mostly follow the hyperparmeter setup of REINVENT and GEGL. For instance, the batch size and learning rate are set as 64 and 0.0005 according to REINVENT in the PMO benchmark. On the other hand, the mutation rate and the number of training loops are set to 0.01 and 8 following GEGL. We use 64 samples for the replay training and population size, the same as the batch size without tuning. Lastly, the learning rate of $Z$ , the partition function, is set to 0.1, also without tuning. ", "page_idx": 13}, {"type": "text", "text": "In contrast, we have searched several hyperparameters, offspring size, the number of GA generations, and KL-divergence coefficient $\\alpha$ . We provide the sensitivity analysis for the offspring size and the number of GA generations in Appendix G.8. Furthermore, we use the inverse temperature $\\beta=10$ and the weight-shifting factor $k=0.01$ , but they can be differently used to control score-diversity trade-off, as explained in Section 5.1.2. ", "page_idx": 13}, {"type": "text", "text": "A.3 Computing resource ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Throughout the experiments, we utilize a 48-core CPU, Intel(R) Xeon(R) Gold 5317 CPU $\\textcircled{a}3.00\\mathrm{GHz}$ , and a single GPU. In the PMO benchmark, runtime varies from less than 10 minutes to several hours for 10K evaluations, depending on tasks and algorithms. However, most of the runtime is consumed in evaluating score functions\u2014the motivation for why the sample efficiency matters. On the other hand, in the SARS-CoV-2 inhibitor design tasks, 1000 training steps with a batch size of 128 require more than 1 day for PlPro_7JIR and 2 days for RdRp_6YYT; more than $95\\%$ of the time is used to evaluation [52]. ", "page_idx": 13}, {"type": "image", "img_path": "B4q98aAZwt/tmp/942d4ff908e17ea76940c6dab5c5be72f6fd16343e70352fd0514a4f71488716.jpg", "img_caption": ["Figure 7: Examples of Graph GA operations. These operations are conducted according to predefined SMARTS patterns to ensure molecule validity, such as adherence to valence rules. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Genetic Operations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section details each operation in our genetic search; see Fig. 7 for illustration. Note that we adopt Graph GA of [13], which has demonstrated its powerful performances and has been adopted by GA-related works like Mol GA [16] and GEGL [17]. ", "page_idx": 14}, {"type": "text", "text": "B.1 Crossover ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A crossover operation is conducted to generate a new candidate (called offspring) by exchanging the genetic information of a pair of selected individuals (parents). This process mimics the crossover of genetic material in biological reproduction. In the context of molecular optimization with graphs, the crossover operation is conducted in two types: \u2018ring crossover\u2019 and \u2018non-ring crossover with a $50\\%$ probability. ", "page_idx": 14}, {"type": "text", "text": "These two main crossover operations perform crossover between two parent molecules by cutting and recombining ring substructures. Ring crossover performs a ring cut specifically designed to target ring structures within the molecule. The ring-cut operation cuts the molecule along two different ring patterns, selected randomly. One of the ring patterns checks for a specific arrangement of four consecutive ring atoms, and the other pattern checks for a ring atom connected to two other ring atoms with a single bond. If a suitable ring pattern is found, it cuts the molecule along that pattern, resulting in two fragments. On the other hand, non-ring crossover cuts a single bond, meaning it is not part of a cyclic (ring) structure within the molecule. The obtained fragments from both parents are recombined to create new molecules by applying predefined reaction SMARTS patterns. These operations are repeated for validity to ensure that the resulting molecules meet structural and size constraints. ", "page_idx": 14}, {"type": "text", "text": "B.2 Mutation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The mutation is a random change that is introduced to the genetic information of some individuals. This step adds diversity to the population and helps explore new regions of the solution space. In this work, we employ seven different mutation processes and randomly select one of these mutations to modify the offspring molecules slightly. The operations consist of atom and bond deletions, appending new atoms, inserting atoms between existing ones, changing bond orders, deleting cyclic bonds, adding cyclic rings, and altering atom types. ", "page_idx": 14}, {"type": "text", "text": "1. Deletion of atom: it selects one of five deletion SMARTS patterns, each representing the removal of a specific number of atoms or bonds. These patterns include the removal of a single atom, a single bond, a bond with two attached atoms, and bonds with multiple attached atoms. The selected pattern is applied to the molecule, deleting the specified atom(s) or bond(s).   \n2. Appending atom: it introduces a new atom to the molecule. The type of atom (e.g., C, N, O) and the type of bond (single, double, or triple) are chosen based on predefined probabilities. The function then generates a reaction SMARTS pattern to append the selected atom to the molecule, forming a new bond.   \n3. Inserting atom: it inserts a new atom between two existing atoms in the molecule. Similar to the appending atom, it selects the type of atom and bond based on predefined probabilities and generates a reaction SMARTS pattern to insert the atom.   \n4. Changing bond order: it randomly selects one of four SMARTS patterns, each representing a change in the bond order between two atoms. These patterns include changing a single bond to a double bond, a double bond to a triple bond, and vice versa.   \n5. Deletion of cyclic bond: it deletes a bond that is a part of a cyclic structure within the molecule. The SMARTS pattern represents the breaking of a cyclic bond while retaining the atoms connected by the bond.   \n6. Adding ring: it introduces a new cyclic ring into the molecule by selecting one of four SMARTS patterns, each representing the formation of a specific ring type. These patterns create different types of cyclic structures within the molecule.   \n7. Changing atom: it randomly selects two types of atoms from a predefined list and generates a SMARTS pattern to change one atom into another. This operation modifies the atom type within the molecule. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Hyperparameter setup for baseline methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this subsection, we present detailed descriptions for hyperparameter tuning of baselines. Except for Mol GA and GEGL, we adopt all the hyperparameters, initial datasets, and pre-trained models provided in the PMO benchmark. For hyperparameters that affect the sample efficiency, such as population size, we have searched for the proper hyperparameters following the guidelines suggested by [8]. In detail, we tune Mol GA and GEGL on the average AUC Top-10, the main performance metric, from 3 independent runs of two oracles, zaleplon_mpo and perindopril_mpo. The best configurations are used in the main experiments. ", "page_idx": 16}, {"type": "text", "text": "Mol GA. As mentioned in Section 5.1, we set the offspring size as 5, the most crucial hyperparameter, according to the original paper [16]. Then, we searched the starting population size in [100, 200, 500, 1000] and the population size [100, 200, 500]. As shown in Fig. 8, we found the best configuration to be 500 and 100 for the starting population size and the population size, respectively. ", "page_idx": 16}, {"type": "image", "img_path": "B4q98aAZwt/tmp/487e586e91648491079038fe4bbd6c1c1a09e930615165d9e1a3110268f88bdd.jpg", "img_caption": ["Figure 8: Hyperparameter tuning results for Mol GA "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "GEGL. In GEGL, the policy sampling size is set as the expert sampling size, and both priority queue sizes (denoted as num_keep) are the same as the original implementation. Thus, we searched the expert sampling size in [64, 128, 512] and the priority queue size in [128, 512, 1024]. Originally, they were set as 8192 and 1024, which are improper to the sample efficient setting. For the training batch size, we use 64, which is the same as ours. We use the pretrained policy provided in the original code and adapt the setup of the rest of the hyperparameters, including mutation rate, learning rate, and the number of training loops. Based on the results in Fig. 9, we set the expert sampling size and priority queue size as 128. ", "page_idx": 16}, {"type": "image", "img_path": "B4q98aAZwt/tmp/8837e3d20fd41a03aa807951949dc2a4257ecaa457ac29303fd5be00aaac6a31.jpg", "img_caption": ["Figure 9: Hyperparameter tuning results for GEGL "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Comparison Genetic GFN-AL with active learning and model-based algorithms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Implementation of Genetic GFN-AL ", "page_idx": 17}, {"type": "text", "text": "We mostly adopt the implementation of GFlowNet-AL, including the training method and the utilization of an acquisition function [10]. The source code is accessible online with an MIT license.6 Additionally, the number of training proxy and generative models are aligned with the PMO benchmark standards, while the replay training and genetic search hyperparameters are set to those used in our method in Genetic GFN. The pseudo-code is as follows. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 2 Multi-round active learning with Genetic GFN ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: Pretrained policy $\\pi_{\\mathrm{pre}}$   \nOutput: Top- $\\mathcal{K}$ elements of discovered molecule dataset $\\mathcal{D}$   \n1: Set $\\pi_{\\theta}\\leftarrow\\pi_{\\mathrm{pre}}$ , $\\mathcal{D}\\gets\\emptyset$   \n2: Initialize the proxy model $f_{\\phi}$   \n3: while $|\\mathcal{D}|\\leq$ numOracle do   \n4: \u25b7 PROXY TRAINING   \n5: for $k=1$ to numTrainProxy do   \n6: Sample $\\textbf{\\em x}$ from $\\pi_{\\theta}$ with GeneticSearch   \n7: $y^{(i)}\\gets\\mathcal{O}(\\pmb{x}^{(i)})$   \n8: 9: Update $\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{({\\pmb x}^{(i)},y^{(i)})\\}_{i=0}^{n}$ $\\phi$ to minimize $\\sum^{\\lfloor\\stackrel{n}{i=0}}\\scriptstyle(\\mathbf{x},y)\\in{\\mathcal{D}}^{\\left(f_{\\phi}(\\mathbf{x})-y\\right)^{2}}$   \n10: end for   \n11: $\\triangleright$ GENERATIVE POLICY TRAINING   \n12: $\\mathcal{D}_{\\mathrm{inner}}\\leftarrow\\emptyset$   \n13: for $l=1$ to numTrainPolicy do   \n14: Get $m^{\\prime}=\\lceil m(1-\\gamma)\\rceil$ samples from $\\pi_{\\theta}$ with GeneticSearch   \n15: Get $m-m^{\\prime}$ samples from $\\mathcal{D}$   \n16: $\\mathcal{D}_{\\mathrm{inner}}\\leftarrow\\mathcal{D}_{\\mathrm{inner}}\\cup\\{(\\pmb{x}^{(i)},\\mathcal{F}(\\mu(\\pmb{x}^{(i)}),\\sigma(\\pmb{x}^{(i)}))\\}_{i=0}^{m}$ \u25b7Acquisition function from [10]   \n17: for $r=1$ to numReplay do   \n18: Get $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ from $\\mathcal{D}_{\\mathrm{inner}}$ with rank-based sampling   \n19: Update $\\theta$ to minimize $\\begin{array}{r}{\\frac{1}{|\\mathcal{B}|}\\sum_{\\left(\\mathbf{\\boldsymbol{x}},f_{\\phi}(\\mathbf{\\boldsymbol{x}})\\right)\\in\\mathring{\\mathcal{B}}}\\mathring{\\mathcal{L}}_{\\mathrm{TB}}^{~\\bullet}+\\alpha\\mathrm{KL}\\!\\left(\\pi_{\\theta}(\\mathbf{\\boldsymbol{x}})||\\pi_{\\mathrm{pre}}(\\mathbf{\\boldsymbol{x}})\\right)}\\end{array}$   \n20: end for   \n21: end for   \n22: end while ", "page_idx": 17}, {"type": "text", "text": "Hyperparmeters. We use the same hyperparameters for the generative model (i.e., Genetic GFN). Since active learning approaches introduce various hyperparameters, such as the training iterations of the proxy and generative models, not only introduce the proxy model, we tried to keep the setup of GFlowNet-AL in the PMO benchmark (e.g., proxy learning rate). We provide hyperparameters in Table 6; note that our proxy model predicts the score using SMILES rather than fragments, unlike the original GFlowNet-AL. Additionally, we adopt $\\gamma$ , the ratio of offline (oracle-touched) data in the inner loop training, and the acquisition function (related with $\\kappa$ ); see the original GFlowNet-AL paper [10]. ", "page_idx": 17}, {"type": "text", "text": "D.2 Experimental results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We compare Genetic GFN-AL with various model-based and active learning methods. Here, GP BO is regarded as a model-based method of Graph GA because GP BO uses Graph when optimizing the GP acquisition function. As shown in Table 7, SMILES Genetic GFN-AL achieves significantly improved performance compared to fragment GFlowNet-AL in the PMO benchmark. It is noteworthy that we further utilize the acquisition function, not directly use the proxy prediction as a reward, and mix the oracle-touched data, as described in the previous section. Therefore, to verify the effectiveness of genetic search in the active learning setup, we implement another baseline, SMILES GFN-AL using $\\epsilon$ -greedy exploration, similar to our self-ablation studies in Section 5.1.2. The results demonstrate that Genetic GFN is beneficial to enhancing sample efficiency in the active learning setting. As pointed out in the PMO benchmark, though the model-based methods (or active learning methods) are known as more sample efficient, they require careful design to achieve superior performances. ", "page_idx": 17}, {"type": "table", "img_path": "B4q98aAZwt/tmp/7d9c11468ae73c1b79cbf56f9f0669f8167695e54a2201ec94db9c2abb9ca6ee.jpg", "table_caption": ["Table 6: Hyperparameters of Genetic GFN-AL "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "B4q98aAZwt/tmp/9f4d409a6394cf5d92b22d4f1a75ad3d08224e07fdbce44971733902377c2711.jpg", "table_caption": ["Table 7: Comparing AL and model-based algorithms. The bold text indicates the best value. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Multi-objective sample efficient molecular optimization tasks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section provides details on multi-objective sample efficient molecular optimization experiments. Even though Genetic GFN targets single (i.e., scalar-valued) objective optimization tasks, ours directly applies to multi-objective tasks using well-defined coefficients. Inspired by the work of Zhu et al. [11], we conduct experiments on two multi-objective tasks: $\\mathrm{GSK3}\\beta+\\mathrm{JNK3}$ and $\\mathrm{GSK3\\beta+}$ ${\\mathrm{JNK}}3+{\\mathrm{QED}}+{\\mathrm{SA}}$ . Genetic GFN is implemented on top of the original code (MIT license).7 ", "page_idx": 19}, {"type": "text", "text": "Interestingly, $\\mathrm{GSK}3\\beta$ and JNK3 are machine-learning-based oracles that estimate inhibiting scores against the Glycogen synthase kinase 3 beta target and the c-Jun N-terminal kinase 3 target. Designing dual inhibitors for both targets can be beneficial to designing treatments for Alzheimer\u2019s Disease [50]. We define scalar-valued score functions using the linear combination of oracle functions with given coefficients. The cost coefficients $\\alpha$ are set following the HN-GFN paper, i.e., $\\alpha=(1,1)$ for $\\mathrm{GSK}3\\beta$ $+\\operatorname{JNK}3$ and $\\alpha=(3,4,2,1)$ for $\\mathrm{GSK3}\\beta+\\mathrm{JNK3}+\\mathrm{QED}+\\mathrm{SA}$ . Since the reward scales become larger, we reduce the inverse temperature from 50 to 25. ", "page_idx": 19}, {"type": "text", "text": "According to the experiment setup of hypernetwork-based GFlowNets (HN-GFN) [11], we limit the reward calls to 1000. Consequently, the batch size, replay training loops, population, and offspring size are adjusted to half. The performance is measured using hypervolumes with five independent runs. Note that we use the same pretrained model in the main experiment for PMO. ", "page_idx": 19}, {"type": "text", "text": "F Designing of SAS-Cov-2 inhibitors ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "According to the previous work [52], we define the score function $s(x)$ for designing SARS-Cov-2 inhibitors as a linear combination of normalized scores, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(x)=0.8\\cdot{\\frac{1}{1+10^{0.625\\left(s_{\\mathrm{decing}}(x)+10\\right)}}}+0.1\\cdot s_{\\mathrm{QED}}(x)+0.1\\cdot{\\frac{10-s_{\\mathrm{SA}}(x)}{9}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The target proteins are as follows: ", "page_idx": 20}, {"type": "text", "text": "\u2022 PLPro_7JIR: PLPro (papain-like protease) is a critical enzyme in the life cycle of SARSCoV-2, which can help in studying the enzyme\u2019s function and in designing inhibitors that could potentially disrupt the virus\u2019s ability to replicate and evade the immune system. The 7JIR represents a C111S mutant version of PLPro.8 \u2022 RdRp_6YYT: RdRp (RNA-dependent RNA polymerase) is essential for the replication of the genome and the transcription of genes in SARS-CoV-2. The protein structure of RdRp is cataloged in the Protein Data Bank (PDB) under the identification code 6YYT.9 ", "page_idx": 20}, {"type": "text", "text": "Genetic GFN is implemented on top of the implementation of MolRL-MGPT (Molecular design using Reinforcement Learning with Multiple GPT agents).10 We employ the same hyperparameters with the experiments on the PMO benchmark, except for the weight-shifting factor (we use $k=0.05]$ ). ", "page_idx": 20}, {"type": "table", "img_path": "B4q98aAZwt/tmp/7f8dcf520d80f757fd7a14b42ee32e2f03282d5b130942b22f70e6d6f3ff36fe.jpg", "table_caption": ["In Table 5, the scores of baselines are computed according to the Eq. (5) using the average values in the MolRL-MGPT [52]. We also provide average scores and standard deviation of docking, QED, SA, and diversity in Table 8 and Table 9. ", "Table 8: The results of Top-100 molecules for PLPr_7JIR target. The docking, QED, and SA of baselines are directly from MolRL-MGPT, and the total score is recalculated according to Eq. (5) using average values. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "B4q98aAZwt/tmp/cde8fed53a57c81af0fb6b27dfeb4ca70b29c08be9c9ded518cc68a48cdf3211.jpg", "table_caption": ["Table 9: The results of Top-100 molecules for RdRp_6YYT target. The docking, QED, and SA of baselines are directly from MolRL-MGPT, and the total score is recalculated according to Eq. (5) using average values. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We provide additional visual results in Fig. 10. There seems to be a trend of increasing molecular complexity and functional diversity over iterations. ", "page_idx": 20}, {"type": "image", "img_path": "B4q98aAZwt/tmp/2679257036252b699dca4cdb033fff30ab02074824c2a14e7b18863c80822e71.jpg", "img_caption": ["(a) PLPro_7JIR ", "(b) RdRp_6YYT "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Examples of Top3 inhibitors for SARS-CoV-2 over steps (seed 1) ", "page_idx": 21}, {"type": "text", "text": "G Additional results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Controllability of the score-diversity using the weight-shifting factor $k$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As mentioned in Section 5.1.2, the weight-shifting factor $k$ in Eq. (4) also can control the scorediversity trade-off. Increasing $k$ gives more diverse candidates by increasing the probability of high-ranked samples. Similar to adjusting the inverse temperature, the results in Table 10 and Fig. 11 demonstrate that adjusting $k$ with fixed $\\beta$ also can effectively control the score-diversity trade-off. ", "page_idx": 22}, {"type": "image", "img_path": "B4q98aAZwt/tmp/7ce0a6bd24da3c32e1f715f43bf71968c7023f82cd8cd6c2124c9ca5a69b8d2c.jpg", "img_caption": ["Figure 11: The average score and diversity with adjustments of $k$ . "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "B4q98aAZwt/tmp/ec535d3d15db9249ceb023c366590913ca72e3a6789a7dff678c7169c16ec42a.jpg", "table_caption": ["Table 10: The score-diversity trade-off by varying $k$ with fixed $\\beta$ "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "G.2 Additional results for the PMO benchmark ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.2.1 Statistical analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provided the results of the t-tests of AUC Top-10 with Mol GA (2nd place) and SMIELS REINVENT (3rd place). As shown in Table 11, both p-values are less than 0.05, so Genetic GFN outperforms baselines with statistical significance. ", "page_idx": 22}, {"type": "table", "img_path": "B4q98aAZwt/tmp/646a2f05551bc00ae235d36901701d9426763dc58d0d20c9aed5cb7bd4251b21.jpg", "table_caption": ["Table 11: The results of t-tests with Mol GA and REINVENT. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "B4q98aAZwt/tmp/7bd847acac2a5e5e20ebafcee7f126b693d14df49622520c2f1beafc1ad578fb.jpg", "table_caption": ["Table 12: Full results of Table 1. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "B4q98aAZwt/tmp/1dcfebe098e4b89f074aecc9d5f0ba5c39466aab32fc1de7744f471b6e1ddf80.jpg", "table_caption": ["Table 13: Full results of Table 1 (continued). "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "B4q98aAZwt/tmp/982134bceb3226bf7cd2afc035fdffdf589a85c0a486a9951489aa6b5181062e.jpg", "img_caption": ["Figure 12: The optimization curves for 23 oracle score functions. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "G.2.3 Ranks with various metrics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "According to the PMO benchmark, we also provide the rank of each method with various metrics. The results in Table 14 show that Genetic GFN achieves the first place in total, not only in the AUC Top-10. ", "page_idx": 25}, {"type": "table", "img_path": "B4q98aAZwt/tmp/d45a429bf92e1c55b30886ab8262934ed6844635a3f8a32b30b8b04ca399761d.jpg", "table_caption": ["Table 14: The ranks of 10 methods based on various performance metrics "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.3 Further studies on valsartan_smarts (#22) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Notably, we have observed that only a few methods achieve non-zero scores on valsartan_smarts (#22) in Table 1. The valsartan SMARTS targets molecules containing a SMARTS pattern related to valsartan while being characterized by physicochemical properties corresponding to the sitagliptin molecule [57]. It measures the arithmetic means of several scores, including (1) binary score about whether it contains a certain SMARTS structure, (2) LogP, (3) TPSA, and (4) Bertz score. Since we utilize a TDC oracle function for evaluations, we provide our empirical observations here. ", "page_idx": 25}, {"type": "text", "text": "Due to the binary score (1 if the certain SMARTS pattern is included), many tries terminate with 0. Especially with a limited number of oracle calls, generating molecules containing a certain substructure is notoriously hard. Other literature shows that other methods achieve high scores with more oracle calls [52]. With 10K calls, even REINVENT and Genetic only succeed in finding non-zero score molecules once out of five independent runs. Another observation is that methods (REINVENT, Genetic GFN, and GEGL) achieving non-zero scores all generate SMILES with RNN-based models. Thus, we have a conjecture that SMILES generation is effective in generating a certain SMARTS pattern. We provide examples of generated molecules with non-zero valsartan_smarts scores. Note that the other four seeds failed. Each run generates similar molecules (see Top1,10,100 samples in Fig. 13 in the additional material), but the samples between the two runs (REINVENT and Genetic GFN) have different structures (the molecule distance between Top1 samples is 0.854). ", "page_idx": 25}, {"type": "image", "img_path": "B4q98aAZwt/tmp/ea95aace05a198ee0e254574e0da3e898a801836057708ba7079f6bae5ebb79d.jpg", "img_caption": ["Figure 13: Examples of molecules with non-zero scores on valsartan_smarts (#22) "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.4 Additional results for ablation studies ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "G.4.1 Statistical analysis ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provided the results of the t-tests of ablation studies. As shown in Table 15, the p-values for ablating genetic search are less than 0.05, so our genetic search is a statistically significant component. ", "page_idx": 26}, {"type": "table", "img_path": "B4q98aAZwt/tmp/0961efb894fa8de438803e598edecc5542fb326aab839437cc420e9a849ca3db.jpg", "table_caption": ["Table 15: The results of t-tests of ablation studies. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.5 Results for diversity and synthesizability ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We report the diversity and synthetic accessibility (SA) score of Top-100 molecules on each oracle. ", "page_idx": 26}, {"type": "image", "img_path": "B4q98aAZwt/tmp/d5b16dee10c58493a0ededacb424d8db43fc42ab214a857be6af41ce442f0c6d.jpg", "img_caption": ["Figure 14: The average diversity of Top-100 molecules (\u2191) "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "B4q98aAZwt/tmp/39d6d66c42d896c71c3c06f48675722a1760bc18a5ff88101084387cf6938569.jpg", "img_caption": ["Figure 15: The average SA score of Top-100 molecules (\u2193) "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.6 Genetic GFN with SELFIES generation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As our method employs a string-based sequence generation model, it is applicable to SELFIES representation. Following the same procedure and setup described in Section 3 and Section 5.1. The results in Table 16 demonstrate that Genetic GFN significantly outperforms SELFIES-REINVENT by achieving 14.986 compared to 14.152, not only the other high-ranked method, GP BO (14.264). Notably, our AUC Top-10 is higher than that of SELFIES-REINVENT in 19 oracles. ", "page_idx": 27}, {"type": "table", "img_path": "B4q98aAZwt/tmp/1769e7e17d2d0dc57a1fd259daf2bbbdaae4414ef99cd0124b86e30503a82204.jpg", "table_caption": ["Table 16: Performance of Genetic GFN with SELFIES generations. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "G.7 Genetic GFN with string-based genetic search ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We also additionally provide experiments that incorporate STONED (GA with SELFIES)[14] as an exploration strategy to guide GFN training instead of Graph GA. Note that STONED only utilizes mutations since designing valid crossover with string representation is challenging. ", "page_idx": 27}, {"type": "table", "img_path": "B4q98aAZwt/tmp/5436e58c03dd966d3a57e8894e627cb80e837b7c98113b357304369c191b1e31.jpg", "table_caption": ["Table 17: Results with different genetic search algorithms "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "G.8 Experiments with varying hyperparameters ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this subsection, we verify the robustness of Genetic GFN for differing hyperparameter setups. We conduct experiments by varying the number of GA generation (refining loops), offspring size, and the number of training inner loops. The results show that our results are robust to each hyperparameter setup by achieving similar or better performance compared to Mol GA in all tested configurations. ", "page_idx": 28}, {"type": "table", "img_path": "B4q98aAZwt/tmp/66e6f36b410f8e592c9685fda5f2f3e79fcadb020b2905340a5833cb1d3b48df.jpg", "table_caption": ["Table 18: Ablation studies for the number of GA generation. $\\mathbf{\\dot{\\omega}}\\times\\mathbf{0}^{\\bullet}$ stands for the results of TB loss training without GA explorations. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "B4q98aAZwt/tmp/b9ff9229bb0b5edb086979eb14b4495fa6a02140713af76126ae3225ae2ac3de.jpg", "table_caption": ["Table 19: Results by varying the offspring size. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "B4q98aAZwt/tmp/2c6c65e926baa4ce40ca898402162b319012abdae3c6e2065b9578a3f2581994.jpg", "table_caption": ["Table 20: Results by varying the number of training inner loops. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The abstract and introduction clearly state our main research claim, which is consistent with the experimental results. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We include our limitations and future works in the discussion section. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide implementation details, including pseudo-codes. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the anonymized link of our implementation. Also, the paper uses the public dataset and benchmark; we provide accessible links in the supplemental material. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide all details regarding experiments in the appendix and the anonymized codes. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The experimental results include standard deviation with independent multiple runs. In addition, statistical tests are conducted for the main results and self-ablation studies. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the computer resources in the appendix (due to lack of spaces). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have checked the paper according to the NeurIPS Code of Ethics; our study does not include human participation and privacy-related data. ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We discuss the broader impact of this study at the end of the paper. ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our study is conducted using the public dataset, and all experimental tasks are from previous publications. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper clearly states the source and license of the original code, data, and models in the appedix. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our experiments are conducted using the already published datasets and benchmarks. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our study does not involve crowdsourcing nor human subjects. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our study does not involve crowdsourcing nor human subjects. ", "page_idx": 30}]