[{"figure_path": "5Hdg5IK18B/figures/figures_1_1.jpg", "caption": "Figure 1: Mixer Hamiltonian affects the performance of QAOA. (a) The optimization trajectories of QAOA with varied mixer Hamiltonians HM. Given a fixed circuit depth p, a tailored HM (highlighted in pink) can more effectively steer the quantum state towards the exact solution compared to the original HM used in QAOA. (b) Transition of the effective dimension deff required in QAOA with increasing p. \u2018ma-QAOA\u2019 denotes a case with independent parameters [17], contrasted with \u2018QAOA\u2019 where parameters are fully correlated. The orange line denotes the average effective dimension over all samples.", "description": "This figure shows how the choice of Mixer Hamiltonian (HM) in the Quantum Approximate Optimization Algorithm (QAOA) impacts its performance.  Panel (a) illustrates that a custom-designed HM can lead to a more direct and efficient path to the optimal solution than the standard HM, especially when the circuit depth (p) is limited. Panel (b) demonstrates that the effective dimension (deff) of the QAOA, which reflects the complexity of the algorithm, changes with increasing circuit depth (p), and this change is different depending on whether the HM uses correlated or independent parameters.", "section": "1 Introduction"}, {"figure_path": "5Hdg5IK18B/figures/figures_4_1.jpg", "caption": "Figure 2: Framework of MG-Net. (a) Training Phase. Initially (left), the cost estimator is trained to precisely predict QAOA performance for specific problem instances, circuit depths, and mixer Hamiltonians. In the subsequent stage (right), with the cost estimator fixed, the mixer generator is trained through unsupervised learning to derive the optimal mixer Hamiltonian that minimizes the cost estimator's output. (b) Inference Phase. Given a problem G and circuit depth p, the mixer generator produces a mixer Hamiltonian, subsequently utilized in a QAOA solver to find the solution.", "description": "This figure illustrates the framework of the Mixer Generator Network (MG-Net).  The training phase involves two stages: first training a cost estimator to predict QAOA performance, then training a mixer generator (unsupervised) to produce optimal mixer Hamiltonians that minimize the cost estimator's output. The inference phase uses only the trained mixer generator to produce a mixer Hamiltonian for a given problem and circuit depth, which is then used by a QAOA solver.", "section": "4 MG-Net"}, {"figure_path": "5Hdg5IK18B/figures/figures_6_1.jpg", "caption": "Figure 2: Framework of MG-Net. (a) Training Phase. Initially (left), the cost estimator is trained to precisely predict QAOA performance for specific problem instances, circuit depths, and mixer Hamiltonians. In the subsequent stage (right), with the cost estimator fixed, the mixer generator is trained through unsupervised learning to derive the optimal mixer Hamiltonian that minimizes the cost estimator's output. (b) Inference Phase. Given a problem G and circuit depth p, the mixer generator produces a mixer Hamiltonian, subsequently utilized in a QAOA solver to find the solution.", "description": "This figure illustrates the framework of the Mixer Generator Network (MG-Net).  The training phase involves two stages: 1) training a cost estimator to predict QAOA performance given problem parameters, circuit depth, and a mixer Hamiltonian, and 2) training a mixer generator (with the cost estimator fixed) to produce an optimal mixer Hamiltonian that minimizes the cost. The inference phase shows how, given a problem and circuit depth, the MG-Net generates a mixer Hamiltonian which is then used by a QAOA solver to find a solution.", "section": "4 MG-Net"}, {"figure_path": "5Hdg5IK18B/figures/figures_7_1.jpg", "caption": "Figure 4: Behavior of cost estimator. (a) The correlation between the estimated cost and the minimum cost for Max-Cut (left) and TFIM (right). Each point represents the result of a problem instance. The dashed line represents that QAOA can find the exact solution y = x. (b) Behavior of cost estimator with extended mixer operator pool {X, Y, XX, YY}. 'label' represents the actual achieved approximation ratio, while 'pred' represents the result predicted by the cost estimator. (c) The achievable cost under various circuit depth p for Max-Cut (left) and TFIM (right). The label 'CE' is the abbreviation of cost estimator. The dashed lines represent the cost achieved by QAOA, while the solid lines represent the cost estimated by our model.", "description": "This figure demonstrates the performance of the cost estimator in MG-Net.  Panel (a) shows the correlation between the estimated cost and the actual minimum cost achieved by QAOA for Max-Cut and TFIM problems, demonstrating the accuracy of the estimator. Panel (b) extends this analysis to a larger set of mixer operators, showing continued accuracy. Panel (c) shows how the achievable cost changes with varying circuit depth (p) for both problems and both fully grouped (FG) and non-grouped (NG) parameter strategies, highlighting the estimator's ability to predict QAOA performance at different depths and parameter settings.", "section": "5.2 Results"}, {"figure_path": "5Hdg5IK18B/figures/figures_8_1.jpg", "caption": "Figure 5: The trainability of the quantum circuits generated by MG-Net for Max-Cut and TFIM. (a) The number #P of trainable parameters of the quantum circuits with mixer Hamiltonian predicted by MG-Net. (b) Comparison of the effective dimension deff of quantum circuits in standard QAOA and MG-Net driven QAOA (labeled as \u2018Ours\u2019). The green and grey solid lines denote the average effective dimension deff of the predicted circuits that can achieve an approximation ratio over 0.995 for Max-Cut and TFIM, respectively. It assesses circuits achieving an approximation ratio r of at least 0.995. (c) The convergence of QAOA with FG, NG and mixer Hamiltonian predicted by MG-Net for Max-Cut on 64-node weighted graphs.", "description": "This figure compares the trainability of quantum circuits generated by MG-Net and standard QAOA for Max-Cut and TFIM problems.  Subfigure (a) shows the number of trainable parameters (#P) for each method as a function of circuit depth (p).  Subfigure (b) compares the effective dimension (deff) required to achieve a high approximation ratio (r \u2265 0.995). Subfigure (c) shows the convergence behavior of the different methods for a Max-Cut problem with 64 nodes.", "section": "Results"}, {"figure_path": "5Hdg5IK18B/figures/figures_22_1.jpg", "caption": "Figure 6: Encoding of problem. The problem graph is first transformed into a quantum circuit, which is subsequently encoded by a DAG.", "description": "This figure illustrates the process of encoding a problem graph into a directed acyclic graph (DAG) representation for use in the MG-Net model.  The problem graph, representing the combinatorial optimization problem, is first converted into a quantum circuit (Uc) where each edge in the problem graph corresponds to a two-qubit gate in the quantum circuit.  This quantum circuit is then represented as a DAG (Gc), where each node represents a two-qubit gate and the edges represent the sequential order of gate execution.  The resulting DAG Gc serves as a structured input for the MG-Net model.", "section": "4.2 Implementation of MG-Net"}, {"figure_path": "5Hdg5IK18B/figures/figures_22_2.jpg", "caption": "Figure 7: Encoding of mixer Hamiltonian. Each qubit in the mixer Hamiltonian is represented as a node in the encoded graph. The type of operator associated with each qubit is encoded in the node feature, while the parameter grouping strategy is encapsulated in the edge features.", "description": "This figure illustrates how the mixer Hamiltonian is encoded as a graph for the MG-Net model.  Each node in the graph represents a qubit, with the node's features indicating the type of Pauli operator (X or Y) applied to that qubit. The edges of the graph represent the parameter sharing strategy. If two qubits share the same trainable parameter (i.e., they are in the same parameter group), an edge connects their corresponding nodes.  This graph representation allows the MG-Net to learn the optimal parameter grouping and operator types for different problems and circuit depths.", "section": "4.2 Implementation of MG-Net"}, {"figure_path": "5Hdg5IK18B/figures/figures_23_1.jpg", "caption": "Figure 8: Implementation of cost estimator. The term 'GraphConv' represents the graph convolution module. 'ReLU' is a commonly used activation function in neural networks. dc and dm represent the dimension of node feature in graph Gc and GM respectively. Pi represents the operator type for the i-qubit and eij represents the weight for edge (i, j).", "description": "This figure shows the architecture of the cost estimator component of the MG-Net model.  It consists of three input branches: one for the problem graph (Gc), one for the mixer Hamiltonian graph (GM), and one for the depth embedding (xp). Each branch uses two layers of graph convolutions followed by ReLU activation and global average pooling (GAP) to extract features. These features are then concatenated and fed into a multi-layer perceptron (MLP) to predict the minimum cost (\u0177) of the QAOA algorithm.", "section": "4.2 Implementation of MG-Net"}, {"figure_path": "5Hdg5IK18B/figures/figures_24_1.jpg", "caption": "Figure 3: Structure of cost estimator and mixer generator. (a) Cost estimator. The cost estimator is comprised of three distinct branches, each dedicated to processing different types of data: the original problem, the candidate mixer Hamiltonian, and the circuit depth. Their outputs are then integrated to predict the cost value achievable by the QAOA circuit. (b) Mixer generator. The mixer generation is divided into two distinct parts: operator type generation and parameter grouping generation. The former is executed as a node classification task, while the latter is approached as a link prediction task.", "description": "This figure shows the architecture of the cost estimator and mixer generator in MG-Net. The cost estimator takes problem, mixer Hamiltonian and circuit depth as input and estimates the QAOA cost. The mixer generator is divided into two parts that generate operator type and parameter grouping. ", "section": "4.2 Implementation of MG-Net"}, {"figure_path": "5Hdg5IK18B/figures/figures_25_1.jpg", "caption": "Figure 10: Topological structure of asymmetric graphs and 2D TFIM.", "description": "This figure shows two types of graphs used in the experiments. The left graph is an asymmetric graph, while the right graph is a 2D TFIM graph. These graphs are used to evaluate the performance of the proposed MG-Net model. The asymmetric graph is used to test the generalizability of the model to non-regular graphs, while the 2D TFIM graph is used to test the model on a more structured graph.", "section": "Experiments"}, {"figure_path": "5Hdg5IK18B/figures/figures_26_1.jpg", "caption": "Figure 11: Comparison of the approximation ratio achieved by 6-qubit QAOA, ma-QAOA and our model for Max-Cut and TFIM with varying p.", "description": "This figure compares the approximation ratios achieved by three different methods (standard QAOA, multi-angle QAOA, and the proposed MG-Net method) for solving Max-Cut and TFIM problems.  The x-axis represents the circuit depth (p), and the y-axis represents the approximation ratio (r). The plot shows that the proposed method consistently achieves a higher approximation ratio than the other two methods across different circuit depths.  It also shows that the performance of ma-QAOA decreases for larger values of p, suggesting potential over-parameterization issues, whereas MG-Net maintains high performance.", "section": "E.3 Approximation ratio with respect to p"}, {"figure_path": "5Hdg5IK18B/figures/figures_27_1.jpg", "caption": "Figure 12: Comparison of the convergence of 16-qubit QAOA, ma-QAOA and our model for Max-Cut and TFIM with varying p.", "description": "This figure compares the convergence speed of three different QAOA methods (standard QAOA, multi-angle QAOA, and the proposed MG-Net method) for solving Max-Cut and TFIM problems.  The convergence is shown for different circuit depths (p = 4, 6, 8, 10).  It demonstrates that MG-Net achieves a lower loss value in fewer iterations compared to the other two methods, highlighting its superior efficiency and convergence performance. ", "section": "E.4 Convergence of QAOA with various mixer Hamiltonian"}, {"figure_path": "5Hdg5IK18B/figures/figures_27_2.jpg", "caption": "Figure 4: Behavior of cost estimator. (a) The correlation between the estimated cost and the minimum cost for Max-Cut (left) and TFIM (right). Each point represents the result of a problem instance. The dashed line represents that QAOA can find the exact solution y = x. (b) Behavior of cost estimator with extended mixer operator pool {X, Y, XX, YY}. 'label' represents the actual achieved approximation ratio, while 'pred' represents the result predicted by the cost estimator. (c) The achievable cost under various circuit depth p for Max-Cut (left) and TFIM (right). The label 'CE' is the abbreviation of cost estimator. The dashed lines represent the cost achieved by QAOA, while the solid lines represent the cost estimated by our model.", "description": "This figure shows the performance of the cost estimator in predicting the minimum cost achievable by QAOA for different problems, circuit depths, and mixer Hamiltonians.  It demonstrates a strong correlation between estimated and actual minimum costs, even with more complex Hamiltonians.  It also shows how the achievable cost changes with varying circuit depth, aligning with theoretical analysis of QAOA convergence.", "section": "5.2 Results"}]