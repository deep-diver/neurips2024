[{"type": "text", "text": "On the Computational Landscape of Replicable Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alkis Kalavasis Amin Karbasi Yale University Yale University   \nalvertos.kalavasis@yale.edu amin.karbasi@yale.edu Grigoris Velegkas Felix Zhou Yale University Yale University   \ngrigoris.velegkas@yale.edu felix.zhou@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study computational aspects of algorithmic replicability, a notion of stability introduced by Impagliazzo, Lei, Pitassi, and Sorrell [2022]. Motivated by a recent line of work that established strong statistical connections between replicability and other notions of learnability such as online learning, private learning, and SQ learning, we aim to understand better the computational connections between replicability and these learning paradigms. Our first result shows that there is a concept class that is efficiently replicably PAC learnable, but, under standard cryptographic assumptions, no efficient online learner exists for this class. Subsequently, we design an efficient replicable learner for PAC learning parities when the marginal distribution is far from uniform, making progress on a question posed by Impagliazzo et al. [2022]. To obtain this result, we design a replicable lifting framework inspired by Blanc, Lange, Malik, and Tan [2023] that transforms in a black-box manner efficient replicable PAC learners under the uniform marginal distribution over the Boolean hypercube to replicable PAC learners under any marginal distribution, with sample and time complexity that depends on a certain measure of the complexity of the distribution. Finally, we show that any pure DP learner can be transformed to a replicable one in time polynomial in the accuracy, confidence parameters and exponential in the representation dimension of the underlying hypothesis class. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The replicability crisis is omnipresent in many scientific disciplines including biology, chemistry, and, importantly, AI [Baker, 2016, Pineau et al., 2019]. A recent article that appeared in Nature [Ball, 2023] explains how the reproducibility crisis witnessed in AI has a cascading effect across many other scientific areas due to its widespread applications in other fields, like medicine. Thus, a pressing task is to design a formal framework through which we can argue about the replicability of experiments in ML. Such an attempt was initiated recently by the pioneering work of Impagliazzo et al. [2022], who proposed a definition of replicability as a property of learning algorithms. ", "page_idx": 0}, {"type": "text", "text": "Definition 1.1 (Replicable Algorithm; Impagliazzo et al., 2022). Let $\\mathcal{R}$ be a distribution over random strings. A learning algorithm $\\boldsymbol{\\mathcal{A}}$ is $n$ -sample $\\rho$ -replicable under distribution $\\mathcal{D}$ if for two independent sets $S,S^{\\prime}\\sim\\,{\\mathcal{D}}^{n}$ it holds that $\\operatorname*{Pr}_{S,S^{\\prime}\\sim\\mathcal{D}^{n},r\\sim\\mathcal{R}}[A(S,r)\\,\\neq\\,A(S^{\\prime},r)]\\,\\le\\,\\rho$ . We will say that $\\boldsymbol{\\mathcal{A}}$ is replicable if the above holds uniformly over all distributions $\\mathcal{D}$ . ", "page_idx": 0}, {"type": "text", "text": "We emphasize that the random string $r$ is shared across the two executions and this aspect of the definition is crucial in designing algorithms that have the same output under two different i.i.d. inputs. Indeed, both Dixon et al. [2023] and Chase et al. [2023b] demonstrated learning tasks for which there are no algorithms that satisfy this strong notion of replicability when the randomness is not shared across executions. This shared random string models the random seed of a learning algorithm in practice, and sharing internal randomness can be easily implemented by sharing said random seed. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Closer to our work, Impagliazzo et al. [2022], Ghazi et al. [2021b], Bun et al. [2023], Kalavasis et al. [2023] established strong statistical connections between replicability and other notions of algorithmic stability and learning paradigms such as differential privacy (DP), statistical queries (SQ), and online learning. Although several of these works provided various computational results, these connections are not as well understood as the statistical ones. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we aim to shed further light on the aforementioned computational connections. In particular, we provide both negative and positive results. Negative results are manifested through computational separations while positive results are presented through computational transformations, as we will see shortly. We emphasize that whenever we work within the PAC learning framework we consider the realizable setting. ", "page_idx": 1}, {"type": "text", "text": "Replicability & Online Learning. The results of Ghazi et al. [2021b], Bun et al. [2023], Kalavasis et al. [2023] established a statistical connection between replicability and online learning. In particular, in the context of PAC learning, these works essentially show that replicable PAC learning is statistically equivalent to online learning since learnability in both settings is characterized by the finiteness of the Littlestone dimension of the underlying concept class. From the above, the first natural question is the following: ", "page_idx": 1}, {"type": "text", "text": "Q1. How does replicability computationally relate to online learning? ", "page_idx": 1}, {"type": "text", "text": "We show that under standard cryptographic assumptions, efficient replicability is separated from efficient online learning. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.2 (Informal, see Theorem 2.1). Assuming the existence of one-way functions, there is a concept class that is replicably PAC learnable in polynomial time, but is not efficiently online learnable by any no-regret algorithm. ", "page_idx": 1}, {"type": "text", "text": "In order to prove the above result, we provide an efficient replicable PAC learner for the concept class of One-Way Sequences $\\mathcal{O}\\backslash\\mathcal{V}\\mathcal{S}$ over $\\{0,1\\}^{d}$ , introduced by Blum [1994]. This algorithm, which relies on a novel replicable subroutine for quantile estimation (cf. Theorem B.6), combined with the cryptographic hardness result of Bun [2020] for online learnability, gives the desired computational separation. For further details, we refer to Section 2. ", "page_idx": 1}, {"type": "text", "text": "Replicability & SQ. The Statistical Query (SQ) framework is an expressive model of statistical learning introduced by Kearns [1998]. This model of learning is a restricted class of algorithms that is only permitted indirect access to samples through approximations of certain carefully chosen functions of the data (statistical queries). The motivation behind the introduction of the SQ framework was to capture a large class of noise-resistant learning algorithms. As such, any SQ algorithm enjoys some inherent stability properties. Since replicability also captures some notion of algorithmic stability, it is meaningful to ask: ", "page_idx": 1}, {"type": "text", "text": "Q2. How does replicability computationally relate to SQ learning? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "For some background about the SQ framework, we refer the reader to Appendix C.2. A manifestation of the intuitive connection between SQ and replicability can be found in one of the main results of Impagliazzo et al. [2022], which states that any efficient SQ algorithm can be efficiently made replicable. In this paper, we investigate the other direction of this connection. ", "page_idx": 1}, {"type": "text", "text": "A particularly interesting computational separation between (distribution-specific) replicability and SQ learning was noticed by Impagliazzo et al. [2022], in the context of realizable binary classification: if we consider the uniform distribution $\\boldsymbol{\\mathcal{U}}$ over the Boolean hypercube $\\{0,1\\}^{d}$ , then the concept class of parities is SQ-hard to learn under $\\boldsymbol{\\mathcal{U}}$ [Kearns, 1998] but admits an efficient PAC learner that is replicable under $\\boldsymbol{\\mathcal{U}}$ . Indeed, with high probability over the random draw of the data, Gaussian elimination, which is the standard algorithm for PAC learning parities under $\\boldsymbol{\\mathcal{U}}$ , gives a unique solution and, as a result, is replicable. ", "page_idx": 1}, {"type": "text", "text": "Based on this observation, Impagliazzo et al. [2022] posed as an interesting question whether parities are efficiently learnable by a replicable learner under other marginal distributions, for which Gaussian elimination is \u201cunstable\u201d, i.e., it fails to give a unique solution (see e.g., Proposition C.4). ", "page_idx": 2}, {"type": "text", "text": "Transforming Distribution-Specific Replicable Learners. Inspired by the question of Impagliazzo et al. [2022], we ask the following more general question for realizable binary classification: ", "page_idx": 2}, {"type": "text", "text": "Q3. For a class $\\mathcal{C}\\subseteq\\{0,1\\}^{\\mathcal{X}}$ and marginal distributions $\\mathcal \u1e0a D \u1e0c _{1},\\mathcal \u1e0a D \u1e0c _{2}$ over $\\mathcal{X}$ , how does replicable PAC learnability of $\\mathcal{C}$ under $\\mathcal{D}_{1}$ and under $\\mathcal{D}_{2}$ relate to one another computationally? ", "page_idx": 2}, {"type": "text", "text": "Our main result is a general black-box transformation from a replicable PAC learner under the uniform distribution over $\\mathcal{X}\\,\\,=\\,\\{0,1\\}^{d}$ to a replicable PAC learner under some unknown marginal distribution $\\mathcal{D}$ . The runtime of the transformation depends on the decision tree complexity of the distribution $\\mathcal{D}$ , a complexity measure that comes from the recent work of Blanc et al. [2023] and is defined as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 1.3 (Decision Tree Complexity; Blanc et al., 2023). The decision tree complexity of a distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ is the smallest integer $\\ell$ such that its probability mass function (pmf) can be computed by a depth-\u2113decision tree (cf. Definition ${\\cal D}.{\\cal I}$ ). ", "page_idx": 2}, {"type": "text", "text": "A decision tree (cf. Definition D.1) $T:\\{0,1\\}^{d}\\to\\mathbb{R}$ is a binary tree whose internal nodes query a particular coordinate $x_{i},i\\in[d]$ (descending left if $x_{i}=0$ and right otherwise) and whose leaves are labelled by real values. Each $\\bar{x}\\in\\{0,1\\}^{d}$ follows a unique root-to-leaf path based on the queried coordinate of internal nodes, and its value $T(x)$ is the value stored at the root. For some $\\mathcal{D}$ over the Boolean hypercube, we say $\\mathcal{D}$ is computed by a decision tree $T$ of depth $\\ell$ if, for any $x\\in\\{0,1\\}^{d}$ , it holds that $\\mathcal D(x)=T(x)$ . ", "page_idx": 2}, {"type": "text", "text": "As an example, note that the uniform distribution requires depth $\\ell=1$ since its pmf is constant while a distribution whose pmf takes $2^{d}$ different values requires depth $\\ell=d$ . However, various natural (structured) distributions have tree complexity much smaller than $d$ . Importantly, the running time overhead of our lifting approach scales proportionally to the decision tree complexity of $\\mathcal{D}$ and hence can be used to obtain novel efficient replicable PAC learners. ", "page_idx": 2}, {"type": "text", "text": "Our replicable lifting framework informally states the following: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.4 (Informal, see Theorem 3.2). Let $\\mathcal{X}=\\{0,1\\}^{d}$ and $m=\\mathrm{poly}(d)$ . Consider a concept class ${\\mathcal{C}}\\subseteq\\{0,1\\}^{x}$ and assume that $\\boldsymbol{\\mathcal{A}}$ is a PAC learner for $\\mathcal{C}$ that is $m$ -sample replicable under the uniform distribution and runs in time $\\mathrm{poly}(m)$ . Then, for some (unknown) monotone1 distribution $\\mathcal{D}$ with decision tree complexity $\\ell$ , there exists an algorithm $\\mathcal{A^{\\prime}}$ that is a PAC learner for $\\mathcal{C}$ that is $(m\\ell)^{\\ell}$ -sample replicable under $\\mathcal{D}$ and runs in time poly $((m\\ell)^{\\ell})$ . ", "page_idx": 2}, {"type": "text", "text": "This implies for instance that for $\\ell=O(1)$ , the blowup is polynomial in the runtime of the uniform learner while if $\\ell=O(\\log d)$ , the running time is quasi-polynomial in $d$ . We note that our result can also be extended to general (non-monotone) distributions $\\mathcal{D}$ if we assume access to a (subcube) conditional sampling oracle. For formal details, we refer the reader to Section 3. ", "page_idx": 2}, {"type": "text", "text": "As an application of this framework, we show how to black-box transform replicable learners for parities that work under the uniform distribution to replicable learners that work under some other unknown distribution $\\mathcal{D}$ , where the sample complexity and running time of the transformation depends on the decision tree complexity of $\\mathcal{D}$ . This result is hence related to the question of Impagliazzo et al. [2022] since we can design distributions $\\mathcal{D}$ over $\\{0,1\\}^{d}$ with small decision tree complexity for which Gaussian elimination fails to be replicable. We refer to Section 4 for the formal statement. ", "page_idx": 2}, {"type": "text", "text": "Corollary 1.5 (Informal, see Corollary 4.3 and Theorem C.5). Let $\\mathcal{X}=\\{0,1\\}^{d}$ and $m=\\mathrm{poly}(d)$ . Then, for any (unknown) monotone distribution $\\mathcal{D}$ with decision tree complexity $\\ell$ , there exists a $P A C$ learner for parities that is $(m\\ell)^{\\ell}$ -sample replicable under $\\mathcal{D}$ and runs in time $\\mathrm{poly}((m\\ell)^{\\ell})$ . ", "page_idx": 2}, {"type": "text", "text": "Moreover, for any $m^{\\prime}=m^{\\Theta(1)}$ , there exists some monotone distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ with decision tree complexity $\\ell=\\Theta(1)$ so that Gaussian elimination, with input $m^{\\prime}$ labeled examples, fails to $P A C$ learn parities replicably under $\\mathcal{D}$ with constant probability. ", "page_idx": 2}, {"type": "text", "text": "Replicability & Privacy. Returning to the works of Ghazi et al. [2021b], Bun et al. [2023], Kalavasis et al. [2023], it is known that replicable PAC learning is statistically equivalent to approximate differentially private PAC learning. We hence ask: ", "page_idx": 3}, {"type": "text", "text": "Q4. How does replicability computationally relate to private learning? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The recent work of Bun et al. [2023] provided a computational separation between approximate differential privacy and replicability. In particular, they showed that a replicable learner can be efficiently transformed into an approximately differentially private one, but the other direction is hard under standard cryptographic assumptions. To be more specific, they provided a concept class that is efficiently learnable by an approximate DP learner, but not efficiently learnable by a replicable learner, assuming the existence of one-way functions. Based on this hardness result, we ask whether we can transform a pure DP algorithm into a replicable one. We show the following result. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.6 (Informal; see Theorem 5.2). Let $\\mathcal{D}_{X Y}$ be a distribution on $\\mathcal{X}\\times\\{0,1\\}$ that is realizable with respect to some concept class $\\mathcal{C}$ . Let $\\boldsymbol{\\mathcal{A}}$ be an efficient pure $D P$ learner for $\\mathcal{C}$ . Then, there is a replicable learner $\\mathcal{A^{\\prime}}$ for $\\mathcal{C}$ that runs in time polynomial with respect to the error, confidence, and replicability parameters but exponential in the representation dimension2 of $\\mathcal{C}$ . ", "page_idx": 3}, {"type": "text", "text": "We reiterate that this transformation is efficient with respect to the correctness, confidence and replicability parameters $\\alpha,\\beta,\\rho$ , but it could be inefficient with respect to some parameter that captures the complexity of the underlying concept class $\\mathcal{C}$ (such as the representation dimension of the class, e.g., the margin parameter in the case of large-margin halfspaces). To the best of our knowledge, the same holds for the transformation of a pure DP learner to an online learner of Gonen et al. [2019] and it would be interesting to show that this is unavoidable. ", "page_idx": 3}, {"type": "text", "text": "The Computational Landscape of Stability. Our work studies several computational aspects of replicability and its connections with other stability notions such as online learning, SQ learning, and differential privacy. Combining our results with the prior works of Blum et al. [2005], Gonen et al. [2019], Ghazi et al. [2021b], Impagliazzo et al. [2022], Bun et al. [2023], Kalavasis et al. [2023] yields the current computational landscape of stability depicted below. ", "page_idx": 3}, {"type": "image", "img_path": "1PCsDNG6Jg/tmp/39c8bc44b931c322a03868f2f50742344b15123bf1e546ef330ce0388783f25f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1.1: The computational landscape of stability. A green double arrow $(\\Rightarrow)$ from tail to head indicates that an efficient learner for a task in the setting of the arrow tail can be black-box transformed into an efficient learner for the same task in the setting of the arrow head. Meanwhile, an orange dashed double arrow $\\scriptstyle(==\\succ$ ) from tail to head indicates that an efficient learner for a task in the tail setting can be black-box transformed into an efficient learner for the same task in the head setting, under some additional assumptions. Finally, a red slashed single arrow $(\\nrightarrow)$ from tail to head indicates that there is a learning task for which an efficient learner exists in the setting of the arrow tail but no efficient learner can exist in the setting of the arrow head, possibly under some cryptographic assumptions. ", "page_idx": 4}, {"type": "text", "text": "The computational landscape of \u201cstable\u201d learning depicted in Figure 1.1 is a byproduct of the following results connecting (i) replicability, (ii) approximate DP, (iii) pure DP, (iv) online learning, and, (v) statistical queries. ", "page_idx": 4}, {"type": "text", "text": "Black-Box Transformations. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1. Pure DP can be efficiently3 transformed to Online [Gonen et al., 2019].   \n2. Replicability can be efficiently transformed to Approximate DP [Ghazi et al., 2021b, Bun et al.,   \n2023, Kalavasis et al., 2023].   \n3. SQ can be efficiently transformed to Approximate DP [Blum et al., 2005].4   \n4. SQ can be efficiently transformed to Replicable [Impagliazzo et al., 2022].   \n5. Pure DP can be efficiently5 transformed to Replicable (this work, Theorem 5.2). ", "page_idx": 4}, {"type": "text", "text": "Caveats of Transformations. The transformations from pure $\\varepsilon$ -DP learners to replicable and online learners may incur exponential computation time in the representation dimension [Beimel et al., 2013] of the underlying hypothesis class. ", "page_idx": 4}, {"type": "text", "text": "Regarding the approximate DP reduction to replicability: The transformation provided by Bun et al. [2023] uses correlation, which necessitates that the output space of the algorithm is finite. To be more precise, based on [Bun et al., 2023], there is an efficient transformation from approximate DP to perfectly generalizing algorithms. Next, the authors use correlated sampling to obtain a replicable learner as follows. Given a perfectly generalizing algorithm $A$ and sample $S$ , the correlated sampling strategy is applied to the distribution of outputs of $A(S)$ . Hence, the output space of $A$ should be finite. In the PAC learning setting, to ensure that the algorithm has finite output space, one sufficient condition is that the domain $\\mathcal{X}$ is finite. The correlated sampling step can be explicitly implemented via rejection sampling from the output space of $A$ . The acceptance probability is controlled by the probability mass function of $A(S)$ . As a result, in general, it is not computationally efficient. For instance, if the finite input space to the correlated sampling strategy is $\\{0,1\\}^{d}$ , then the runtime of the algorithm could be $\\exp(d)$ , since the acceptance probability is exponentially small in the dimension in the worst case. ", "page_idx": 4}, {"type": "text", "text": "For the specific case of PAC learning, there is another transformation from approximate DP to replicability that holds for countable domains $\\mathcal{X}$ that was proposed by Kalavasis et al. [2023], but this approach goes through the Littlestone dimension of the class and might not even be computable in its general form. ", "page_idx": 4}, {"type": "text", "text": "Separations. There is a concept class that can be learned efficiently ", "page_idx": 4}, {"type": "text", "text": "1. by a replicable PAC learner (this work, Theorem 2.1), but not an efficient online one under OWF [Blum, 1994];   \n2. by an approximate DP PAC learner, but not an efficient online one under OWF [Bun, 2020];   \n3. by an online learner, but not an efficient approximate DP PAC one under cryptographic assumptions6 [Bun et al., 2024];   \n4. by a replicable PAC learner under uniform marginals (Impagliazzo et al. [2022]) or more general marginals (this work, Corollary 1.5), but not an efficient SQ one; ", "page_idx": 4}, {"type": "text", "text": "5. by an approximate DP PAC learner, but not an efficient replicable one under OWF [Bun et al., 2023]. ", "page_idx": 5}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Replicability. Pioneered by Impagliazzo et al. [2022], there has been a growing interest from the learning theory community in studying replicability as an algorithmic property. Esfandiari et al. [2023a,b] studied replicable algorithms in the context of multi-armed bandits and clustering. Later, Eaton et al. [2023], Karbasi et al. [2023] studied replicability in the context of Reinforcement Learning (RL) and designed algorithms that achieve various notions of replicability. Recently, Bun et al. [2023] established statistical equivalences and separations between replicability and other notions of algorithmic stability such as differential privacy when the domain of the learning problem is finite and provided some computational and statistical hardness results to obtain these equivalences, under cryptographic assumptions. Subsequently, Kalavasis et al. [2023] proposed a relaxation of the replicability definition of Impagliazzo et al. [2022], showed its statistical equivalence to the notion of replicability for countable domains7and extended some of the equivalences from Bun et al. [2023] to countable domains. Chase et al. [2023b], Dixon et al. [2023] proposed a notion of list-replicability, where the randomness is not shared across the executions of the algorithm, but the outputs are required to belong to a list of small cardinality instead of being identical. Both of these works developed algorithms that work in the realizable setting, and later Chase et al. [2023a] showed that, surprisingly, it is impossible to design list-replicable learning algorithms for infinite classes in the agnostic setting. Recently, Moran et al. [2023] established even more statistical connections between replicability and other notions of stability, by dividing them into two categories consisting of distribution-dependent and distribution-independent definitions. Recent work by Kalavasis et al. [2024] studies replicable algorithms for large-margin halfspaces. ", "page_idx": 5}, {"type": "text", "text": "Computational Separations & Transformations in Stability. The seminal result of Blum [1994] illustrated a computational separation between PAC and online learning. Later, Bun [2020] obtained a similar separation between private PAC and online learning. More recently, Bun et al. [2023] showed that there exists a concept class that admits an efficient approximate DP learner but not an efficient replicable one, assuming the existence of OWFs. Contributing to this line of work, our Theorem 2.1 is of similar flavor. ", "page_idx": 5}, {"type": "text", "text": "Shifting our attention to SQ learning, there is an intuitive similarity between learning from noisy examples and private learning: algorithms for both problems must be robust to small variations in the data (see also Blum et al. [2005]). Parities are a canonical SQ-hard problem, meaning that no efficient SQ algorithm for this class exists. Kasiviswanathan et al. [2011] designed an efficient private learner for learning parities, which dispels the similarity between learning with noise and private learning. Georgiev and Hopkins [2022] showed that while polynomial-time private algorithms for learning parities are known, the failure probability of any such algorithm must be larger than what can be achieved in exponential time, or else $\\mathrm{NP}=\\mathrm{RP}$ . Finally, recent work by Bun et al. [2024] gives a concept class that admits an online learner running in polynomial time with a polynomial mistake bound, but for which there is no computationally efficient approximate differentially private PAC learner, under cryptographic assumptions. ", "page_idx": 5}, {"type": "text", "text": "Moving on to transformations, our Theorem 1.4 builds upon the result of Blanc et al. [2023] who designed a framework that transforms computationally efficient algorithms under uniform marginal distributions, to algorithms that work under some other distribution, where the complexity of the transformation scales with some particular notion of distance between the two distributions. Essentially, our result can be viewed as a replicable framework of the same flavor, with a small additional computational and statistical overhead that scales with the replicability parameter. Finally, our approach to transform a pure DP learner into a replicable learner (cf. Theorem 1.6) is inspired by Gonen et al. [2019], who provided a transformation from pure DP learners to online learners. ", "page_idx": 5}, {"type": "text", "text": "1.3 Notation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In general, we use $\\boldsymbol{\\mathcal{A}}$ to denote an algorithm. For unsupervised problems, we usually denote by $\\mathcal{D}$ the distribution over input examples. In the case of supervised problems, we use $\\mathcal{C}$ to denote the concept class in question, $\\mathcal{D}$ to denote the marginal distribution of the feature domain $\\mathcal{X}$ , and $\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}$ to refer to the joint distribution over labeled examples $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ . Throughout our work, we use $\\alpha,\\beta$ to refer to the error and failure probability parameters of the algorithm, $\\varepsilon,\\delta$ to denote the approximate ", "page_idx": 5}, {"type": "text", "text": "DP parameters, and $\\rho$ for the replicability parameter. In most cases, the feature domain $\\mathcal{X}$ is a subset of a high-dimensional space and we use $d$ to denote the dimension of that space. ", "page_idx": 6}, {"type": "text", "text": "2 Efficient Replicability and Online Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our first main result shows that replicability and online learning are not computationally equivalent, assuming the existence of one-way functions. This hardness result is based on a construction from Blum [1994], who defined a concept class denoted by $\\mathcal{O}\\backslash\\mathcal{V}\\mathcal{S}$ , which is efficiently PAC learnable but not online learnable in polynomial time, assuming the existence of one-way functions. Blum\u2019s construction builds upon the Goldreich-Goldwasser-Micali pseudorandom function generator [Goldreich et al., 1986] to define families of \u201cone-way\u201d labeled sequences $(\\sigma_{1},b_{1}),\\dotsc,(\\sigma_{r},\\stackrel{\\blacktriangledown}{b_{r}})\\in\\{0,1\\}^{d}\\times\\{0,1\\}$ , for some $r=\\omega(\\mathrm{poly}(d))$ . These string-label pairs can be efficiently computed in the forward direction, but are hard to compute in the reverse direction. Specifically, for any $i<j$ , it is easy to compute $\\sigma_{j},b_{j}$ given $\\sigma_{i}$ . On the other hand, it is hard to compute $\\sigma_{i},b_{i}$ given $\\sigma_{j}$ . ", "page_idx": 6}, {"type": "text", "text": "The essence of the difficulty in the online setting is that an adversary can present to the learner the sequence in reverse order, i.e., $\\sigma_{r},\\sigma_{r-1},\\ldots,\\sigma_{1}$ . Then, the labels $b_{i}$ are not predictable by a polynomial time learner. However, in the PAC setting, for any distribution over the sequence $\\{(\\bar{\\sigma}_{i},\\dot{b}_{i})\\}_{i\\in[r]}$ , a learner which is given $n$ labeled examples can identify the string $\\sigma_{i^{*}}$ with smallest index $i^{*}$ in the sample. Then, it can perfectly and efficiently predict the label of any string that comes after it. This approximately amounts to a $(\\mathrm{i}\\mathrm{~-~}^{1}/n)$ fraction of the underlying population. ", "page_idx": 6}, {"type": "text", "text": "It is not hard to see that the PAC learner for $\\mathcal{O}\\backslash\\mathcal{S}$ is not replicable, since the minimum index of a sample can vary wildly between samples. The high-level idea of our approach to making the algorithm replicable is to show that we can replicably identify an approximate minimum index and output the hypothesis that (efficiently) forward computes from that index. Our main result, proven in Appendix B, is as follows. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.1. Let d \u2208N. The following hold: ", "page_idx": 6}, {"type": "text", "text": "(a) [Bun, 2020] Assuming the existence of one-way functions, the concept class $\\mathcal{O}\\backslash\\mathcal{S}$ with input domain $\\{0,1\\}^{d}$ cannot be learned by an efficient no-regret algorithm, i.e., an algorithm that for some $\\eta>0$ achieves expected regret $\\mathbb{E}[\\ddot{R}_{T}]\\leq\\mathrm{poly}(\\bar{d})\\cdot{T}^{1-\\eta}$ using time poly $(d,T)$ in every iteration.8   \n(b) (Theorem B.7) The concept class $\\mathcal{O}\\backslash\\mathcal{V}\\mathcal{S}$ with input domain $\\{0,1\\}^{d}$ can be learned by a $\\rho$ - replicable $(\\alpha,\\beta)$ -PAC learner with sample complexity $m\\,=\\,\\mathrm{\\bar{~}p o l y}(d,1/\\alpha,1/\\rho,\\log(1/\\beta))$ and $\\mathrm{poly}(m)$ running time. ", "page_idx": 6}, {"type": "text", "text": "3 Lifting Replicable Uniform Learners ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present our replicable lifting framework in further detail. First, we will need the following technical definition. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.1 (Closed under Restrictions). A concept class $\\mathcal{C}$ of functions $f:\\{0,1\\}^{d}\\rightarrow\\{0,1\\}$ is closed under restrictions $i f,$ for any $f\\in\\mathcal{C},i\\in[d]$ and $b\\in\\{0,1\\}$ , the restriction $f_{i=b}$ remains in $\\mathcal{C}$ , where $f_{i=b}(x)=f(x_{1},...,x_{i-1},b,x_{i+1},...,x_{d})$ for any $x\\in\\{0,1\\}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Our replicable lifting result works for concept classes that satisfy the closedness under restrictions property. Also, recall that a distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ is monotone (Definition D.4) if whenever $x\\succeq y$ ( $x$ is greater than $y$ in the partial ordering of the poset), it holds $D(x)\\geq D(y)$ . Our algorithm for lifting replicable uniform learners to replicable learners under some unknown distribution $\\mathcal{D}$ is presented in Theorem 3.2. This lifting approach is inspired by the work of Blanc et al. [2023], who designed the non-replicable variant of this transformation. We note that our algorithm, similar to the algorithm of Blanc et al. [2023], only requires sample access to $\\mathcal{D}$ for monotone distributions, while, for arbitrary non-monotone probability measures, our algorithm requires access to a conditional sampling oracle (cf. Definition D.5). Our replicable lifting theorem reads as follows. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2 (Lifting Replicable Uniform Learners). Consider a concept class $\\mathcal{C}$ of functions $f\\,:\\,\\{0,1\\}^{d}\\,\\rightarrow\\,\\{0,1\\}$ closed under restrictions. Suppose we are given black-box access to an algorithm such that for any $\\alpha^{\\prime},\\rho^{\\prime},\\beta^{\\prime}\\in(0,1),$ , given $\\operatorname{poly}(d,1/\\alpha^{\\prime},1/\\rho^{\\prime},\\log(1/\\beta^{\\prime}))$ samples from $\\boldsymbol{\\mathcal{U}}$ , $(i)$ is $\\rho^{\\prime}$ -replicable with respect to the uniform distribution $\\boldsymbol{\\mathcal{U}}$ ,   \n(ii) PAC learns $\\mathcal{C}$ under the uniform distribution to accuracy $\\alpha^{\\prime}$ and confidence $\\beta^{\\prime}$ , and, (iii) terminates in time $\\mathrm{poly}(d,\\bar{1}/\\alpha^{\\prime},1/\\rho^{\\prime},\\log(1/\\beta^{\\prime}))$ . ", "page_idx": 6}, {"type": "text", "text": "Let $\\alpha,\\rho\\quad\\in\\ \\ (0,1)$ and $\\beta\\quad\\in\\ \\ (0,\\rho/3)$ . For $m\\;\\;=\\;\\;\\mathrm{poly}(d,1/\\alpha,1/\\rho,\\mathrm{log}(1/\\beta))$ and $\\textit{M}=$ p $\\mathrm{ply}\\big(d,1/\\alpha,1/\\rho,\\log\\bigl(1/\\beta\\bigr)\\big)^{O(\\ell)}$ , the following cases hold: ", "page_idx": 7}, {"type": "text", "text": "(a) If $\\mathcal{D}$ is a monotone distribution over $\\{0,1\\}^{d}$ representable by a depth- $\\ell$ decision tree, there is an algorithm that draws $M$ samples from $\\mathcal{D}$ , is $\\rho$ -replicable with respect to $\\mathcal{D}$ , PAC learns $\\mathcal{C}$ under $\\mathcal{D}$ with accuracy $\\alpha$ and confidence $\\beta$ , and terminates in $\\mathrm{poly}(M)$ time.   \n(b) If $\\mathcal{D}$ is an arbitrary distribution over $\\{0,1\\}^{d}$ representable by a depth- $\\ell$ decision tree, there is an algorithm that draws $M$ labeled examples as well as $M$ conditional samples (cf. Definition $D.5,$ ) from $\\mathcal{D}$ , is $\\rho$ -replicable with respect to $\\mathcal{D}$ , PAC learns $\\mathcal{C}$ with respect to $\\mathcal{D}$ with accuracy $\\alpha$ and confidence $\\beta$ , and terminates in poly $(M)$ time. ", "page_idx": 7}, {"type": "text", "text": "The high-level idea of the reduction proceeds as follows. Let us consider the realizable PAC setting, i.e., the labels are consistent with some $f^{\\star}\\in{\\mathcal{C}}$ . Let us assume black-box access to a replicable uniform learner $A_{\\mathcal{C}}^{\\mathcal{U}}$ for $\\mathcal{C}$ and access to i.i.d. samples of the form $(x,f^{\\star}(x))$ , where $x\\sim\\mathcal{D}$ . Our goal is to (efficiently) obtain an algorithm ${\\mathcal{A}}_{\\mathcal{C}}^{\\mathcal{D}}$ that achieves small misclassification error with respect to the unknown distribution $\\mathcal{D}$ and target $f^{\\star}$ and is also replicable under $\\mathcal{D}$ . The promise is that $\\mathcal{D}$ has a decision tree representation of depth $\\ell$ .9 ", "page_idx": 7}, {"type": "text", "text": "The first step is to draw enough samples $(x,f^{\\star}(x)),x\\sim{\\cal D}$ , and compute the decision tree representation of $\\mathcal{D}$ . This is an unsupervised learning task and it only uses the feature vectors of the training set. We design a replicable algorithm for this step (cf. Theorem E.7), which could be of independent interest. The next observation is crucial: any discrete distribution on $\\{0,1\\}^{d}$ can be expressed as a mixture of uniform distributions conditioned on non-overlapping sub-cubes. To see this, notice that any root-to-leaf path in the estimated decision tree representation corresponds to a sub-cube of $\\{0,\\boldsymbol{1}\\}^{d}$ and, conditioned on this path, the remaining coordinates follow a uniform law. Hence, after an appropriate re-sampling procedure, one can employ the black-box replicable learner ${\\mathcal{A}}_{\\mathcal{C}}^{\\mathcal{U}}$ to any one of the leaves $t$ of the tree decomposition of $\\mathcal{D}$ and obtain a classifier $f_{t}$ . For this step, the fact that $\\mathcal{C}$ is closed under restrictions is crucial. Intuitively, if we wish to implement this idea in a replicable manner and need overall replicability parameter $\\rho$ , it suffices to use the uniform replicable algorithm in each leaf with parameter $O(\\rho/2^{\\ell})$ since we make at most $2^{\\ell}$ calls to the uniform PAC learner (the decision tree complexity of the target is $\\boldsymbol{\\ell}$ ). Finally, given a test example $x\\sim\\mathcal{D}$ , one computes the leaf $t$ that corresponds to the sub-cube that $x$ falls into and uses the (replicable) output $f_{t}$ of the associated uniform PAC learner to guess the correct label.   \nFor the formal analysis, we refer to Appendix D and Appendix E. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4 Efficient Replicability and SQ Learning: Parities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide an application of the general lifting framework we described in Section 3. One of the main results of the seminal work of Impagliazzo et al. [2022] is that any SQ algorithm can be made replicable. This result allows a great collection of tasks to be solved replicably since the SQ framework is known to be highly expressive [Kearns, 1998]. ", "page_idx": 7}, {"type": "text", "text": "The primary motivation of this section comes from the question of Impagliazzo et al. [2022] on whether the class of parities can be PAC learned by an efficient replicable algorithm when the marginal distribution $\\mathcal{D}$ is not uniform over $\\{0,1\\}^{d}$ . Let us define our concept class of interest.10 ", "page_idx": 7}, {"type": "text", "text": "Definition 4.1 (Affine Parities). Let $S\\subseteq[d]$ be some subset of $[d]$ and $b\\in\\{0,1\\}$ a bias term. Define $f_{S,b}:\\{0,1\\}^{d}\\to\\{0,1\\}$ to be the biased parity of the bits in $S$ , namely $\\textstyle f_{S,b}(x)=b+\\sum_{i\\in S}x_{i}$ . The concept class of affine parities over $\\{0,1\\}^{d}$ is the set ${\\mathcal{C}}=\\{f_{S,b}:S\\subseteq[d],b\\in\\{0,1\\}\\}$ . For any distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ , we consider the supervised learning problem AffParity, where the learner observes i.i.d. samples of the form $(x,f^{\\star}(x))$ , where $x\\sim\\mathcal{D}$ and $f^{\\star}\\in{\\mathcal{C}}$ . ", "page_idx": 7}, {"type": "text", "text": "As observed by Impagliazzo et al. [2022], there is a replicable algorithm for PAC learning parities (i.e., the subclass Parity obtained by setting $b=0$ in AffParity) under the uniform distribution: draw roughly $O(d)$ samples so that with high probability the dataset will contain a basis. Then, in two distinct executions, the sets that standard Gaussian elimination outputs will be the same. A similar approach works for the class of affine parities and is described below. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.2. The concept class of affine parities AffParity over $\\{0,1\\}^{d}$ admits a $\\rho$ -replicable algorithm that perfectly learns any concept with respect to the uniform distribution $\\boldsymbol{\\mathcal{U}}$ with probability of success at least $1-\\beta$ . The algorithm has $O(\\mathrm{poly}(d,\\log{1/\\rho\\beta}))$ sample and time complexity. ", "page_idx": 8}, {"type": "text", "text": "The algorithm that attains the guarantees of Lemma 4.2 is a simple adaptation of the Gaussian elimination for learning standard parities. We provide the pseudocode in Algorithm C.1 and defer the proof of correctness to Appendix C.1. ", "page_idx": 8}, {"type": "text", "text": "Application of Theorem 3.2 Since the class of affine parities over $\\{0,1\\}^{d}$ is closed under restrictions (cf. Lemma C.8) and there is a learner for this class under uniform marginals that is replicable and efficient (cf. Lemma 4.2), we can obtain an algorithm that replicably PAC learns the class AffParity under more general distributions. In particular, the following result is an immediate application of our lifting framework. ", "page_idx": 8}, {"type": "text", "text": "Corollary 4.3. Let $\\alpha,\\rho\\quad\\in\\ \\ (0,1)$ and $\\beta\\quad\\in\\ (0,\\rho/3)$ . Let $\\begin{array}{r l r}{\\mathcal{X}}&{{}=}&{\\{0,1\\}^{d}}\\end{array}$ . For $M\\;=\\;$ $\\mathrm{poly}(d,1/\\alpha,1/\\rho,1/\\beta)^{O(\\ell)}$ , the following cases hold: ", "page_idx": 8}, {"type": "text", "text": "(a) For any (unknown) monotone distribution $\\mathcal{D}$ over $\\mathcal{X}$ with decision tree complexity $\\ell_{i}$ , there exists an algorithm that is a $\\rho$ -replicable learner for the concept class AffParity under $\\mathcal{D}$ , and requires $M$ samples and running time poly $(M)$ to get accuracy $\\alpha$ and confidence $\\beta$ .   \n(a) For any distribution $\\mathcal{D}$ over $\\mathcal{X}$ with decision tree complexity $\\ell_{s}$ , there exists an algorithm that is $a$ $\\rho$ -replicable learner for the concept class AffParity under $\\mathcal{D}$ , and requires $M$ labeled examples, $M$ conditional samples from $\\mathcal{D}$ and running time poly $(M)$ to get accuracy $\\alpha$ and confidence $\\beta$ . ", "page_idx": 8}, {"type": "text", "text": "Back to the Question of Impagliazzo et al. [2022]. Impagliazzo et al. [2022] raised the question of when parities over $\\{0,1\\}^{\\bar{d}}$ can be efficiently PAC learned by a replicable algorithm under some marginal distribution $\\mathcal{D}$ that is not uniform or, more broadly, that causes Gaussian elimination to be non-replicable. Our Corollary 4.3 makes progress on this question. First, we note that in our setting, we do not require knowledge of $\\mathcal{D}$ . Second, one can design examples of (monotone) distributions for which our lifting framework produces a replicable polynomial time PAC learner but naive Gaussian elimination11 fails to be replicable with constant probability (cf. Theorem C.5). Note that even PAC learning parities for the distribution described in Theorem C.5 is SQ-hard. We believe that our lifting framework can be seen as a systematic way of bypassing some instabilities arising from the algebraic structure of standard Gaussian elimination. ", "page_idx": 8}, {"type": "text", "text": "As a final remark, we note that one could potentially design much more complicated (still monotone) distributions than the one of Theorem C.5, where even various adaptations of Gaussian elimination (e.g., pre-processing of the dataset, data deletion) would fail to be replicable but our lifting framework would guarantee replicability (and efficiency, provided small decision tree complexity). On the other hand, it is not evident whether parity learning remains SQ-hard under these \u201charder\u201d distributions. ", "page_idx": 8}, {"type": "text", "text": "5 Efficient Replicability and Private Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we study connections between efficient DP learnability and efficient replicable learnability of a concept class. As we mentioned in the introduction, Bun et al. [2023] and subsequently Kalavasis et al. [2023] established a statistical equivalence between approximate DP learnability and replicable learnability of a concept class when the domain is finite [Bun et al., 2023] or countable [Kalavasis et al., 2023]. Moreover, Bun et al. [2023] showed that this equivalence does not hold when one takes into account the computational complexity of these tasks. ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.1 (Section 4.1 in Bun et al. [2023]). There exists a class that is efficiently PAC learnable by an approximate DP algorithm but, assuming one-way functions exist, it cannot be learned efficiently by a replicable algorithm. ", "page_idx": 8}, {"type": "text", "text": "We remark that there is an efficient converse transformation, i.e., a computationally efficient transformation from a replicable learner to an approximate DP learner [Bun et al., 2023]. Inspired by Gonen et al. [2019], we ask whether one can transform a pure DP learner to a replicable one. Our main result, proven in Appendix F, is the following. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.2 (From Pure DP Learner to Replicable Learner). Let $\\mathcal{X}$ be some input domain, ${\\mathcal{V}}\\,=\\,\\{0,1\\}$ , and $\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}$ be a distribution on $\\mathcal X\\times\\mathcal Y$ that is realizable with respect to some concept class $\\mathcal{C}$ . Let $\\boldsymbol{\\mathcal{A}}$ be a pure $D P$ learner that, for any $\\alpha,\\varepsilon,\\beta\\,\\in\\,(0,1)$ , needs $m(\\alpha,\\varepsilon,\\beta,\\mathcal{C})=$ $\\mathrm{poly}(1/\\alpha,1/\\varepsilon,\\log(1/\\beta),\\mathsf{d i m}(\\mathcal{C}))$ i.i.d. samples from $\\mathcal{D}_{\\mathcal{X}\\mathcal{X}}$ and $\\mathrm{poly}(m)$ running time to output $a$ hypothesis that has error at most $\\alpha$ , with probability $1\\,-\\,\\beta$ in an $\\varepsilon{-}D P$ way. Then, for any $\\alpha^{\\prime},\\stackrel{\\cdot}{\\rho},\\beta^{\\prime}\\in(0,1)$ there is a $\\rho$ -replicable learner $\\mathcal{A^{\\prime}}$ that outputs a hypothesis with error at most $\\alpha^{\\prime}$ with probability at least $1-\\beta^{\\prime}$ and requires po $\\mathrm{iy}(1/\\alpha^{\\prime},1/\\rho,\\log(1/\\beta^{\\prime}),\\dim(\\mathcal{C}))$ i.i.d. samples from $\\mathcal{D}_{X Y}$ and $\\operatorname{poly}(1/\\alpha^{\\prime},1/\\rho,\\log(1/\\beta^{\\prime}))\\cdot\\exp(\\dim(\\mathcal{C}))$ running time. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In the above, we denote by $\\mathsf{d i m}({\\mathcal{C}})$ some dimension that describes the complexity of the concept class $\\mathcal{C}$ that arises in the sample complexity of our pure DP learner. A natural candidate is the representation dimension [Kasiviswanathan et al., 2011]. As we alluded to before, this transformation is efficient with respect to the parameters $\\alpha,\\beta,\\rho$ . On the other side, the sample complexity is polynomial in the representation dimension but the running time is exponential. We leave as an open question if it is possible to avoid this dependence. ", "page_idx": 9}, {"type": "text", "text": "We remark that in principle, one could use the reduction from Bun et al. [2023]. The catch is that this reduction is based on correlated sampling so it requires i) the output space of the algorithm to be finite and ii) even under finite output spaces, it needs exponential time in the size of that space. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have studied the computational aspects of replicability and several connections to other important notions in learning theory including online learning, SQ learning, and DP PAC learning. We believe that there are several interesting questions left open from our work. First, it would be interesting to see if there is a computationally efficient transformation from online learners to replicable learners. Then, it would be important to derive replicable learners from pure DP learners which are efficient with respect to the complexity of the underlying concept class. Regarding parities, it is still open whether we can design efficient replicable algorithms for every distribution $\\mathcal{D}$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Alkis Kalavasis was supported by the Institute for Foundations of Data Science at Yale. Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032), ONR (N00014- 19-1- 2406), and the AI Institute for Learning-Enabled Optimization at Scale (TILOS). Grigoris Velegkas was supported in part by the AI Institute for Learning-Enabled Optimization at Scale (TILOS). Felix Zhou acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private pac learning implies finite littlestone dimension. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 852\u2013860, 2019.   \nNoga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private and online learnability are equivalent. ACM Journal of the ACM (JACM), 69(4):1\u201334, 2022.   \nMonya Baker. 1,500 scientists lift the lid on reproducibility. Nature, 533(7604), 2016.   \nPhilip Ball. Is ai leading to a reproducibility crisis in science? Nature, 624(7990):22\u201325, 2023.   \nAmos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. In Robert D. Kleinberg, editor, Innovations in Theoretical Computer Science, ITCS \u201913, Berkeley, CA, USA, January 9-12, 2013, pages 97\u2013110. ACM, 2013. doi: 10.1145/2422436. 2422450. URL https://doi.org/10.1145/2422436.2422450.   \nGuy Blanc, Jane Lange, Ali Malik, and Li-Yang Tan. Popular decision tree algorithms are provably noise tolerant. In International Conference on Machine Learning, pages 2091\u20132106. PMLR, 2022a.   \nGuy Blanc, Jane Lange, Mingda Qiao, and Li-Yang Tan. Properly learning decision trees in almost polynomial time. Journal of the ACM, 69(6):1\u201319, 2022b.   \nGuy Blanc, Jane Lange, Ali Malik, and Li-Yang Tan. Lifting uniform learners via distributional decomposition. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 1755\u20131767, 2023.   \nAvrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and the statistical query model. Journal of the ACM (JACM), 50(4):506\u2013519, 2003.   \nAvrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: the sulq framework. In Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 128\u2013138, 2005.   \nAvrim L Blum. Separating distribution-free and mistake-bound learning models over the boolean domain. SIAM Journal on Computing, 23(5):990\u20131000, 1994.   \nMark Bun. A computational separation between private learning and online learning. Advances in Neural Information Processing Systems, 33:20732\u201320743, 2020.   \nMark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 389\u2013402. IEEE, 2020.   \nMark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections between replicability, privacy, and adaptive generalization. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 520\u2013527. ACM, 2023. doi: 10.1145/3564246.3585246. URL https: //doi.org/10.1145/3564246.3585246.   \nMark Bun, Aloni Cohen, and Rathin Desai. Private PAC learning may be harder than online learning. In Claire Vernade and Daniel Hsu, editors, International Conference on Algorithmic Learning Theory, 25-28 February 2024, La Jolla, California, USA, volume 237 of Proceedings of Machine Learning Research, pages 362\u2013389. PMLR, 2024. URL https://proceedings.mlr.press/ v237/bun24a.html.   \nCl\u00e9ment L Canonne, Dana Ron, and Rocco A Servedio. Testing probability distributions using conditional samples. SIAM Journal on Computing, 44(3):540\u2013616, 2015.   \nCl\u00e9ment L Canonne, Xi Chen, Gautam Kamath, Amit Levi, and Erik Waingarten. Random restrictions of high dimensional distributions and uniformity testing with subcube conditioning. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 321\u2013336. SIAM, 2021.   \nZachary Chase, Bogdan Chornomaz, Shay Moran, and Amir Yehudayoff. Local borsuk-ulam, stability, and replicability. arXiv preprint arXiv:2311.01599, 2023a.   \nZachary Chase, Shay Moran, and Amir Yehudayoff. Stability and replicability in learning. In 64th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2023, Santa Cruz, CA, USA, November 6-9, 2023, pages 2430\u20132439. IEEE, 2023b. doi: 10.1109/FOCS57990.2023.00148. URL https://doi.org/10.1109/FOCS57990.2023.00148.   \nPeter Dixon, Aduri Pavan, Jason Vander Woude, and N. V. Vinodchandran. List and certificate complexities in replicable learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \nAryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics, pages 642\u2013669, 1956.   \nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \nEric Eaton, Marcel Hussing, Michael Kearns, and Jessica Sorrell. Replicable reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Hossein Esfandiari, Alkis Kalavasis, Amin Karbasi, Andreas Krause, Vahab Mirrokni, and Grigoris Velegkas. Replicable bandits. In The Eleventh International Conference on Learning Representations, 2023a. ", "page_idx": 11}, {"type": "text", "text": "Hossein Esfandiari, Amin Karbasi, Vahab Mirrokni, Grigoris Velegkas, and Felix Zhou. Replicable clustering. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023b.   \nVitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh S Vempala, and Ying Xiao. Statistical algorithms and a lower bound for detecting planted cliques. Journal of the ACM (JACM), 64(2): 1\u201337, 2017.   \nDimitris Fotakis, Alkis Kalavasis, and Christos Tzamos. Efficient parameter estimation of truncated boolean product distributions. In Conference on Learning Theory, pages 1586\u20131600. PMLR, 2020.   \nDimitris Fotakis, Alkis Kalavasis, Vasilis Kontonis, and Christos Tzamos. Efficient algorithms for learning from coarse labels. In Conference on Learning Theory, pages 2060\u20132079. PMLR, 2021.   \nDimitris Fotakis, Alkis Kalavasis, and Christos Tzamos. Perfect sampling from pairwise comparisons. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \nKristian Georgiev and Samuel Hopkins. Privacy induces robustness: Information-computation gaps and sparse mean estimation. Advances in Neural Information Processing Systems, 35:6829\u20136842, 2022.   \nBadih Ghazi, Noah Golowich, Ravi Kumar, and Pasin Manurangsi. Sample-efficient proper pac learning with approximate differential privacy. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 183\u2013196, 2021a.   \nBadih Ghazi, Ravi Kumar, and Pasin Manurangsi. User-level differentially private learning via correlated sampling. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 20172\u201320184, 2021b.   \nSurbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds via functional gradients. Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.   \nOded Goldreich, Shaf iGoldwasser, and Silvio Micali. How to construct random functions. Journal of the ACM (JACM), 33(4):792\u2013807, 1986.   \nAlon Gonen, Elad Hazan, and Shay Moran. Private learning implies online learning: An efficient reduction. Advances in Neural Information Processing Systems, 32, 2019.   \nThemistoklis Gouleakis, Christos Tzamos, and Manolis Zampetakis. Faster sublinear algorithms using conditional sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1743\u20131757. SIAM, 2017.   \nAnupam Gupta, Moritz Hardt, Aaron Roth, and Jonathan Ullman. Privately releasing conjunctions and the statistical query barrier. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 803\u2013812, 2011.   \nRussell Impagliazzo, Rex Lei, Toniann Pitassi, and Jessica Sorrell. Reproducibility in learning. In Stefano Leonardi and Anupam Gupta, editors, STOC \u201922: 54th Annual ACM SIGACT Symposium on Theory of Computing, Rome, Italy, June 20 - 24, 2022, pages 818\u2013831. ACM, 2022. doi: 10.1145/3519935.3519973. URL https://doi.org/10.1145/3519935.3519973.   \nSvante Janson. Tail bounds for sums of geometric and exponential variables. Statistics & Probability Letters, 135:1\u20136, 2018.   \nAlkis Kalavasis, Amin Karbasi, Shay Moran, and Grigoris Velegkas. Statistical indistinguishability of learning algorithms. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 15586\u201315622. PMLR, 2023. URL https://proceedings.mlr.press/v202/ kalavasis23a.html.   \nAlkis Kalavasis, Amin Karbasi, Kasper Green Larsen, Grigoris Velegkas, and Felix Zhou. Replicable learning of large-margin halfspaces. arXiv preprint arXiv:2402.13857, 2024.   \nAmin Karbasi, Grigoris Velegkas, Lin Yang, and Felix Zhou. Replicability in reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \nShiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793\u2013826, 2011.   \nMichael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983\u20131006, 1998.   \nMichael J Kearns and Umesh Vazirani. An introduction to computational learning theory. MIT press, 1994.   \nNick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 2:285\u2013318, 1988.   \nPascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability, pages 1269\u20131283, 1990.   \nShay Moran, Hilla Schefler, and Jonathan Shafer. The bayesian stability zoo. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \nJoelle Pineau, Koustuv Sinha, Genevieve Fried, Rosemary Nan Ke, and Hugo Larochelle. Iclr reproducibility challenge 2019. ReScience C, 5(2):5, 2019. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notation and Preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For standard PAC learning definitions, we refer to the book of Kearns and Vazirani [1994]. ", "page_idx": 13}, {"type": "text", "text": "A.1 Replicable Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Following the pioneering work of Impagliazzo et al. [2022], we consider the definition of a replicable learning algorithm. ", "page_idx": 13}, {"type": "text", "text": "Definition 1.1 (Replicable Algorithm; Impagliazzo et al., 2022). Let $\\mathcal{R}$ be a distribution over random strings. A learning algorithm $\\boldsymbol{\\mathcal{A}}$ is $n$ -sample $\\rho$ -replicable under distribution $\\mathcal{D}$ if for two independent sets $S,S^{\\prime}\\sim\\,{\\mathcal{D}}^{n}$ it holds that $\\operatorname*{Pr}_{S,S^{\\prime}\\sim\\mathcal{D}^{n},r\\sim\\mathcal{R}}[A(S,r)\\,\\neq\\,A(S^{\\prime},r)]\\,\\le\\,\\rho$ . We will say that $\\boldsymbol{\\mathcal{A}}$ is replicable if the above holds uniformly over all distributions $\\mathcal{D}$ . ", "page_idx": 13}, {"type": "text", "text": "In words, $\\boldsymbol{\\mathcal{A}}$ is replicable if sharing the randomness across two executions on different i.i.d. datasets yields the exact same output with high probability. We can think of $\\boldsymbol{\\mathcal{A}}$ as a training algorithm which is replicable if by fixing the random seed, it outputs the exact same model with high probability. One of the most elementary statistical operations we may wish to make replicable is mean estimation. This operation can be phrased more broadly using the language of statistical queries. ", "page_idx": 13}, {"type": "text", "text": "Definition A.1 (Statistical Query Oracle; Kearns, 1998). Let $\\mathcal{D}$ be a distribution over the domain $\\mathcal{X}$ and $\\phi:\\mathcal{X}\\rightarrow\\mathbb{R}$ be a statistical query with true value $\\begin{array}{r}{v^{\\star}:=\\operatorname*{lim}_{n\\rightarrow\\infty}\\phi(X_{1},\\ldots,X_{n})\\in\\mathbb{R}}\\end{array}$ . Here $X_{i}\\sim_{i,i,d}$ . $\\mathcal{D}$ and the convergence is understood in probability or distribution. Let $\\alpha,\\beta\\in(0,1)^{2}$ . $A$ statistical query (SQ) oracle outputs a value v such that $|v-v^{\\star}|\\leq\\alpha$ with probability at least $1-\\beta$ . ", "page_idx": 13}, {"type": "text", "text": "The SQ framework appears in various learning theory contexts (see e.g., Blum et al. [2003], Gupta et al. [2011], Goel et al. [2020], Fotakis et al. [2021] and the references therein). In the SQ model, the learner interacts with an oracle in the following way: the learner submits a statistical query to the oracle and the oracle returns its true value, after adding some noise to it. ", "page_idx": 13}, {"type": "text", "text": "The simplest example of a statistical query is the sample mean $\\begin{array}{r}{\\phi(X_{1},...\\,,X_{n})\\;=\\;\\frac{1}{n}\\sum_{i=1}^{n}X_{i}}\\end{array}$ . Impagliazzo et al. [2022] designed a replicable SQ-query oracle for sample mean queries with bounded co-domain. Esfandiari et al. [2023b] generalized the result to simultaneously estimate the means of multiple random variables with unbounded co-domain under some regularity conditions on their distributions (cf. Theorem B.4). The idea behind both results is to use a replicable rounding technique introduced in Impagliazzo et al. [2022] which allows one to sacrifice some accuracy of the estimator in exchange for the replicability property. ", "page_idx": 13}, {"type": "text", "text": "A.2 Private Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Differential Privacy. A foundational notion of algorithmic stability is that of Differential Privacy (DP) [Dwork et al., 2006]. For $a,b,\\varepsilon,\\delta\\,\\in\\,(0,1)$ , let $a\\approx_{\\varepsilon,\\delta}b$ denote the statement $a\\leq e^{\\varepsilon}b+\\delta$ and $b\\,\\leq\\,e^{\\varepsilon}a+\\delta$ . We say that two probability distributions $P,Q$ are $(\\varepsilon,\\delta)$ -indistinguishable if $P(E)\\approx_{\\varepsilon,\\delta}Q(E)$ for any measurable event $E$ . ", "page_idx": 13}, {"type": "text", "text": "Definition A.2 (Approximate DP Algorithm; Kasiviswanathan et al., 2011). A learning algorithm $A$ is an $n$ -sample $(\\varepsilon,\\delta)$ -differentially private if for any pair of samples $S,S^{\\prime}\\in(\\mathcal{X}\\times\\{0,1\\})^{n}$ that disagree on a single example, the induced posterior distributions $A(S)$ and $A(S^{\\prime})$ are $(\\varepsilon,\\delta)$ -indistinguishable. ", "page_idx": 13}, {"type": "text", "text": "In the previous definition, when the parameter $\\delta=0$ , we say that the algorithm satisfies (pure) $\\varepsilon$ -DP. We remind the reader that, in the context of PAC learning, any hypothesis class $\\mathcal{C}$ can be PAC-learned by an approximate differentially private algorithm if and only if it has finite Littlestone dimension, i.e., there is a qualitative equivalence between online learnability and private PAC learnability [Alon et al., 2019, Bun et al., 2020, Ghazi et al., 2021a, Alon et al., 2022]. ", "page_idx": 13}, {"type": "text", "text": "A.3 Online Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We consider the no-regret model of online learning. Recall Littlestone\u2019s model of (realizable) online learning [Littlestone, 1988] defined via a two-player game between a learner and an adversary. Let $\\mathcal{C}$ be a given concept class. ", "page_idx": 13}, {"type": "text", "text": "In each round of the game $t=1,\\dots,T$ where $T$ is a time horizon known to a (randomized) learner, the interaction is the following: ", "page_idx": 13}, {"type": "text", "text": "1) The adversary selects some features $x_{t}\\in\\{0,1\\}^{d}$ .   \n2) The learner predicts a label $\\hat{b}_{t}\\in\\{0,1\\}$ , potentially using randomization,   \n3) The adversary, who observes the distribution of the choice of the learner but not its realization, chooses the correct label $b_{t}=c(x_{t})$ under the constraint that there exists some $c_{t}^{\\star}\\in\\mathcal{C}$ such that $c_{\\tau}^{\\star}(x_{\\tau})=b_{\\tau}$ , for all $\\tau\\leq t$ . ", "page_idx": 13}, {"type": "text", "text": "The goal of the learner is to minimize its regret, defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\nh=c\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\cal R}_{T}:=\\displaystyle\\operatorname*{max}_{h\\in\\mathcal{C}}\\left\\{\\sum_{t=1}^{T}\\mathbb{1}\\{\\hat{b}_{t}\\neq c(x_{t})\\}-\\mathbb{1}\\{h(x_{t})\\neq c(x_{t})\\}\\right\\}}}\\\\ {{\\mathrm{~}}}\\\\ {{\\quad=\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\hat{b}_{t}\\neq c(x_{t})\\}-0}}\\\\ {{\\mathrm{~}}}\\\\ {{\\quad=\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\hat{b}_{t}\\neq c(x_{t})\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus the learner aims to compete with the best concept in hindsight from the class $\\mathcal{C}$ . However, realizability ensures that the best such concept makes no mistakes so the regret is defined only in terms of the number of mistakes of the learner. ", "page_idx": 14}, {"type": "text", "text": "We now introduce the definition of an efficient no-regret learning algorithm used by Bun [2020]. In the definition below, we write $|c|$ to denote the length of a minimal description of the concept $c$ . ", "page_idx": 14}, {"type": "text", "text": "Definition A.3 (Efficient No-Regret Learning; [Bun, 2020]). We say that a learner efficiently no-regret learns $\\mathcal{C}$ if there is some $\\eta>0$ such that for every adversary, it achieves expected regret ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\mathrm{poly}(d,\\vert c\\vert)T^{1-\\eta}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "using time pol $\\mathrm{y}(d,|c|,T)$ in every round. ", "page_idx": 14}, {"type": "text", "text": "There are two non-standard features of this definition. First, no-regret algorithms are typically only required to achieve sublinear regret $o(T)$ in $T$ , whereas we require it to be strongly sublinear $\\stackrel{\\cdot}{T}^{1-\\dot{\\eta}}$ . A stronger condition like this is needed to make the definition nontrivial since the sample space is finite. Indeed, suppose $T=2^{d}$ , then the trivial random guessing algorithm attains a regret bound of ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\frac{T}{2}}={\\frac{d T}{2d}}}}\\\\ {~~~~={\\frac{d T}{2\\log T}}}\\\\ {~~~=\\mathrm{poly}(d)o(T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Many no-regret algorithms such as the multiplicative weights update algorithm achieve strongly sublinear regret. ", "page_idx": 14}, {"type": "text", "text": "Second, it would be more natural to require the learner to run in poly $(\\log T)$ time, the description length of the time horizon, rather than the value of $T$ itself. The relaxed formulation only makes the separation stronger. ", "page_idx": 14}, {"type": "text", "text": "B Efficient Replicability and Online Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 One-Way Sequences ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our exposition follows that of Bun [2020]. For every dimension $d\\in\\mathbb{N}$ , Blum [1994] defines a concept class $O\\mathcal{W}S_{d}$ consisting of functions over the domain $\\{0,1\\}^{d}$ that can be represented using $\\mathrm{poly}(\\bar{d})$ bits and evaluated in $\\bar{\\mathrm{poly}}(d)$ time. The concepts of $O\\mathcal{W}S_{d}$ are indexed by bit strings $s\\in\\{0,1\\}^{k}$ , where $k=\\lfloor\\sqrt{d}\\rfloor-1$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{O W}S_{d}=\\big\\{c_{s}:s\\in\\{0,1\\}^{k}\\big\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We will usually omit the index $d$ when it is clear from the context. Each $c_{s}:\\{0,1\\}^{d}\\to\\{0,1\\}$ is defined using two efficiently representable and computable functions ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G:\\{0,1\\}^{k}\\times\\{0,1\\}^{k}\\to\\{0,1\\}^{d-k}\\,,}\\\\ &{f:\\{0,1\\}^{k}\\times\\{0,1\\}^{k}\\to\\{0,1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "that are based on the Goldreich-Goldwasser-Micali pseudorandom function family [Goldreich et al., 1986]. We omit the definition of these functions since it does not impede us towards our goal and refer the reader to Blum [1994] for details. Intuitively, $G(i,s)$ computes the string $\\sigma_{i}$ described in the introduction and $f(i,s)$ computes its label $b_{i}$ . We can think of $s$ as the random seed which is generated from some source of true randomness which is then used to construct $G,f$ . ", "page_idx": 14}, {"type": "text", "text": "For convenience, we identify $\\{0,1\\}^{k}\\equiv[2^{k}]$ . Then $c_{s}$ is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nc_{s}(i,\\sigma)=\\binom{1,}{0,}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,s(\\sigma)\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We see that $c_{s}(i,\\sigma)$ encodes both the string $\\sigma_{i}$ as well as its label $b_{i}$ , for every random seed $s\\ \\in$ $\\{0,1\\}^{k}$ . ", "page_idx": 15}, {"type": "text", "text": "The two relevant properties of the strings $\\sigma_{i}$ are summarized below. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.1 (Forward is Easy; Blum, 1994). There is an efficiently computable function ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathtt{F o r w a r d}:\\{0,1\\}^{k}\\times\\{0,1\\}^{k}\\times\\{0,1\\}^{d-k}\\to\\{0,1\\}^{d-k}\\times\\{0,1\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "such that for every $j>i$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathsf{C o m p u t e F o r w a r d}}(j,i,G(i,s))=(G(j,s),f(j,s)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition B.2 (Reverse is Hard; Blum, 1994). Assuming the existence of one-way functions, there exist functions $G:\\{0,1\\}^{k}\\times\\{0,1\\}^{k}\\rightarrow\\{0,1\\}^{d-k}$ and $\\breve{f}:\\{0,1\\}^{k}\\times\\{\\bar{0},1\\}^{k}\\to\\dot{\\{0,1\\}}$ satisfying the following. Let $\\scriptscriptstyle\\mathcal{O}$ be an oracle that on input $(j,i,G(i,s))$ , outputs $(G(j,s),f(j,s))$ for any $j>i$ . Let $A$ be any polynomial time randomized algorithm and let $A^{\\mathcal{O}}$ denote the algorithm with access to the oracle $\\scriptscriptstyle\\mathcal{O}$ . For every $i\\in\\{0,1\\}^{k}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[A^{\\mathcal{O}}(i,G(i,s))=f(i,s)\\right]\\leq\\frac{1}{2}+\\mathrm{negl}(d),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the probability is taken over the internal randomness of $A$ and uniformly random $s\\sim$ $\\mathcal{U}\\left(\\{0,1\\}^{k}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "The above proposition states that no efficient algorithm can outperform random guessing when trying determinew the label of a string, even given access to a \u201ccompute-forward\u201d oracle. ", "page_idx": 15}, {"type": "text", "text": "B.2 Hardness of Efficiently Online Learning $\\mathcal{O}\\backslash\\mathcal{S}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Blum [1994] used Proposition B.2 to show that $\\mathcal{O V}\\mathcal{S}$ cannot be learned in the mistake bound model, a more stringent model of online learning compared to the no-regret setting. Later, Bun [2020] adapted the argument to the no-regret setting, making the separation stronger. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.3 ([Bun, 2020]). Assuming the existence of one-way functions, OWS cannot be learned by an efficient no-regret algorithm. ", "page_idx": 15}, {"type": "text", "text": "Let $|c|$ denote the length of a minimal description of the concept $c$ . Recall that a learner efficiently no-regret learns a class $\\mathcal{C}$ if there exists $\\eta>0$ such that for every adversary and any $c\\in{\\mathcal{C}}$ , it achieves $\\mathbb{E}[R_{T}]=\\mathrm{poly}(d,|c|)\\cdot T^{1-\\eta}$ (strongly sublinear) using time p $\\mathrm{oly}(d,|c|,T)$ in every round. As mentioned in Appendix A.3, this requirement of strongly sublinear regret is necessary to make the definition non-trivial as the random guessing algorithm attains $o(T)$ regret in finite domains. ", "page_idx": 15}, {"type": "text", "text": "B.3 Replicable Query Rounding ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Impagliazzo et al. [2022] designed replicable statistical query oracles (cf. Definition A.1) for bounded co-domains and Esfandiari et al. [2023b] generalized their results to multiple general queries with unbounded co-domain (cf. Theorem B.4), assuming some regularity conditions on the queries. We illustrate the idea behind the rounding procedure applied to the task of mean estimation. An initial observation is that across two executions, the empirical mean concentrates about the true mean with high probability. Next, we discretize the real line into equal-length intervals with a random shift. It can be shown that both points fall into the same random interval with high probability and outputting the midpoint of said interval yields the replicability guarantee. For completeness, we include the pseudocode and a proof of the replicable rounding procedure. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Theorem B.4 (Replicable Rounding; Impagliazzo et al., 2022, Esfandiari et al., 2023b). Let $\\mathcal{D}$ be a distribution over some domain $\\mathcal{X}$ . Let $\\alpha,\\rho\\in(0,1)$ and $\\beta\\in(0,\\rho/3)$ . Suppose we have a sequence of statistical queries $g_{1},\\dotsc,g_{T}:\\mathcal{X}\\rightarrow\\mathbb{R}$ with true values $\\mu_{1},\\dots,\\mu_{T}$ and sampling n independent points from $\\mathcal{D}$ ensures that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{t\\in[T]}\\lvert g_{t}(x_{1},\\ldots,x_{n})-\\mu_{t}\\rvert\\leq\\alpha\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1-\\beta$ . ", "page_idx": 15}, {"type": "text", "text": "Then there is a polynomial-time time postprocessing procedure rRound (cf. Algorithm B.1) such that each composition $\\mathtt{r R o u n d}(g_{t},\\alpha,\\rho)$ is $\\rho$ -replicable. Moreover, the outputs $\\begin{array}{r l}{\\hat{\\mu}_{t}}&{{}=}\\end{array}$ rRound $(g_{t}(x_{1},\\ldots,x_{n}),\\alpha,\\rho)$ satisfy ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{t\\in[T]}\\lvert\\widehat{\\mu}_{t}-\\mu_{t}\\rvert\\leq\\frac{4\\alpha}{\\rho}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with probability at least $1-\\beta$ . ", "page_idx": 16}, {"type": "text", "text": "Note that if we wish for the sequence of rounding steps to be $\\rho$ -replicable overall, we can simply run each rounding step with parameter $\\rho/T$ . ", "page_idx": 16}, {"type": "text", "text": "Algorithm B.1 Replicable Rounding ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: rRound(query values $g_{1},\\ldots,g_{T}$ , accuracy $\\alpha$ , replicability $\\rho$ ):   \n2: $L\\gets{6\\alpha}/{\\rho}$   \n3: Sample $L_{0}\\sim U[0,L)$   \n4: Discretize the real line . . . , [L0 \u2212L, L0), [L0, L0 + L), [L0 + L, L0 + 2L), . . . into disjoint   \nintervals   \n5: for $t=1,\\dots,T$ do   \n6: Round $g_{t}$ to the midpoint of the interval, $\\hat{\\mu}_{t}$   \n7: end for   \n8: return $\\hat{\\mu}\\in\\mathbb{R}^{T}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof (Theorem B.4). Discretize the real line as disjoint intervals. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ldots,[-L,0),[0,L),[L,2L),\\ldots.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We will choose the value of $L$ later. Consider adding a uniformly random offset $L_{0}\\sim U[0,L)$ so the discretization becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\n.\\,.\\,.\\,,[L_{0}-L,L_{0}),[L_{0},L_{0}+L),[L_{0}+L,L_{0}+2L),.\\,.\\,.\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For each $t\\in T$ , we round the estimate $g_{t}(x_{1:n}):=g_{t}(x_{1},\\ldots,x_{n})$ to the midpoint of the interval it falls into. Let ${\\widehat{\\mu}}_{t}$ be the rounded estimate that we output. From hereonforth, we condition on the event $\\mathrm{max}_{t}|g_{t}(x_{1:n})-\\mu_{t}|\\leq\\alpha$ . ", "page_idx": 16}, {"type": "text", "text": "Replicability of Algorithm B.1. Fix $t\\in T$ . Consider the output across two runs $\\hat{\\mu}_{t},\\hat{\\mu}_{t}^{\\prime}$ . As long as the raw estimates $g_{t}(x_{1:n}),g_{t}(x_{1:n}^{\\prime})$ fall in the same interval, the outputs will be exactly the same. This occurs with probability ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{|g_{t}(x_{1:n})-g_{t}(x_{1:n}^{\\prime})|}{L}\\leq\\frac{2\\alpha}{L}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Choosing $L=6\\alpha/\\rho$ ensures this value is at most ${{\\rho}}/3$ . Accounting for the $2\\beta\\leq2\\rho/3$ probability of the estimates $g_{t}(x_{1:n})$ failing to concentrate, the output ${\\widehat{\\mu}}_{t}$ is $\\rho$ -replicable. ", "page_idx": 16}, {"type": "text", "text": "Correctness of Algorithm B.1. The rounding incurs an additive error of at most $L/2$ . The choice of $L={6\\alpha}/{\\rho}$ means the total error is at most ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha+\\frac{3\\alpha}{\\rho}\\leq\\frac{4\\alpha}{\\rho}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.4 Replicable Quantile Estimation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we provide an algorithm that replicably estimates quantiles of a distribution. This will be useful in providing the computational separation between replicable PAC learning and online learning. We believe that it can have applications beyond the scope of our work. ", "page_idx": 16}, {"type": "text", "text": "We first present a well-known concentration inequality regarding CDFs of random variables due to Dvoretzky, Kiefer, and Wolfowitz. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.5 (Dvoretzky\u2013Kiefer\u2013Wolfowitz Inequality; Dvoretzky et al., 1956, Massart, 1990). Let $X_{1},\\ldots,X_{n}$ be i.i.d. random variables with $C D F F$ . Let $F_{n}$ denote the empirical distribution function given by $\\begin{array}{r}{F_{n}(x):=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}\\{X_{i}\\leq x\\}}\\end{array}$ . For any $\\alpha>0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{x\\in\\mathbb{R}}\\lvert F_{n}(x)-F(x)\\rvert>\\alpha\\right]\\le2\\exp\\bigl(-2n\\alpha^{2}\\bigr).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In other words, we require at most $n=\\left(1/2\\alpha^{2}\\right)\\ln(2/\\beta)$ samples to ensure that with probability at least $1-\\beta$ , the empirical CDF uniformly estimates the true CDF with error at most $\\alpha$ . ", "page_idx": 17}, {"type": "text", "text": "The replicable quantile estimation algorithm for discrete and bounded distributions can be found in Algorithm B.2. The high-level idea is to perform a (replicable) binary search over the support of the distribution and to check whether the empirical CDF evaluated at some point $x$ is above or below the target quantile $q$ . ", "page_idx": 17}, {"type": "table", "img_path": "1PCsDNG6Jg/tmp/0a8625a8ad1b7331acda5138e83e58739d3543126531dec79cace08fc2ee91d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Theorem B.6 (Replicable Quantile Estimation). Let $\\alpha,\\rho\\in(0,1)$ and $\\beta\\in(0,\\rho/3)$ . Suppose we have access to ", "page_idx": 17}, {"type": "equation", "text": "$$\nm={\\frac{16\\log_{2}^{2}(R)}{2\\alpha^{2}\\rho^{2}}}\\ln{\\frac{2}{\\beta}}=O\\left({\\frac{\\log^{2}R}{\\alpha^{2}\\rho^{2}}}\\ln{\\frac{1}{\\beta}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "i.i.d. samples from some distribution over $[R]$ with CDF $F$ and $q\\in[0,1]$ is the desired quantile level. Algorithm B.2 terminates in $O(\\mathrm{poly}(n))$ time, is $\\rho$ -replicable, and with probability at least $1-\\beta$ , outputs some $x\\in[R]$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(x)\\geq q-\\alpha,\\qquad F(x-1)<q+\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Our replicable quantile estimation differs from the replicable median algorithm of [Impagliazzo et al., 2022] in at least two ways. Firstly, the replicable median algorithm of [Impagliazzo et al., 2022] seems to rely heavily on properties of (approximate) medians in order to satisfy the approximation guarantees. On the other hand, our algorithm works regardless of the desired quantile. Secondly, their median algorithm relies on a non-trivial recursive procedure while our replicable quantile algorithm is considerably simpler and is based on a concentration of the CDF through the DKW inequality. ", "page_idx": 17}, {"type": "text", "text": "Proof (Theorem B.6). By the DKW inequality (Theorem B.5), sampling ", "page_idx": 17}, {"type": "equation", "text": "$$\nm={\\frac{1}{2{\\bigl(}\\alpha\\rho{\\big/}4\\log_{2}(R){\\bigr)}^{2}}}\\ln{\\frac{2}{\\beta}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "points from our distribution ensures that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in[R]}|F_{n}(x)-F(x)|\\leq{\\frac{\\alpha\\rho}{4\\log_{2}(R)}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1-\\beta$ . The proof is divided into two steps. We first show the correctness of our algorithm and then argue about its replicability. ", "page_idx": 17}, {"type": "text", "text": "Correctness of Algorithm B.2. The loop invariant we wish to maintain is that ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(h)\\geq q-\\alpha,\\qquad F(\\ell)<q+\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is certainly initially true since our distribution is over $[R]=\\{1,\\ldots,R\\}$ and hence there is no mass at or below $\\ell=0$ while all the mass is at or below $h=R$ . ", "page_idx": 17}, {"type": "text", "text": "Let $m_{1},\\ldots,m_{T}$ denote the midpoints chosen by binary search with $T=\\log_{2}(R)$ . By Theorem B.4, if we condition on the success of all executions of $\\mathtt{r R o u n d}(F_{n}(m_{t}),\\alpha\\rho/4\\log_{2}(R),\\rho/\\log_{2}(R))$ , then their outputs $\\widetilde{F}_{n}(m_{t})$ estimate $F(m_{t})$ with additive error at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{4^{\\alpha\\rho}\\!/\\!4\\log_{2}(R)}{\\rho\\!/\\!\\log_{2}(R)}}=\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We update $h\\leftarrow m_{t}$ only if ", "page_idx": 18}, {"type": "equation", "text": "$$\nq\\leq\\widetilde{F}_{n}(m_{t})\\leq F(m_{t})+\\alpha\\implies q-\\alpha\\leq F(m_{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we update $\\ell\\gets m_{t}$ only if ", "page_idx": 18}, {"type": "equation", "text": "$$\nq>\\widetilde{F}_{n}(m_{t})\\geq F(m_{t})-\\alpha\\implies q+\\alpha>F(m_{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence the loop invariant is maintained. At termination, $\\ell=h-1$ with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(h)\\geq q-\\alpha}\\\\ {F(h-1)<q+\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by the loop invariant as desired. ", "page_idx": 18}, {"type": "text", "text": "Replicability of Algorithm B.2. By Theorem B.4, assuming all prior branching decisions are identical, each new branching decision in Algorithm B.2 is ${\\big(}\\rho{\\big/}{\\log}_{2}(R){\\big)}$ -replicable. Since we make at most $\\log_{2}(R)$ decisions, the entire algorithm is $\\rho$ -replicable. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.5 An Efficient Replicable Learner for $O\\mathcal{W}S_{d}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Equipped with the replicable quantile estimator from Theorem B.6, we are ready to present our efficient replicable $O\\mathcal{W}S_{d}$ PAC learner. We outline below a high-level description of our algorithm which can be found in Algorithm B.3. ", "page_idx": 18}, {"type": "text", "text": "1) We first replicably estimate the mass of all the elements that have positive labels. If it is much smaller than some threshold $O(\\alpha)$ , then we output the zero hypothesis.   \n2) Then we take sufficiently many samples to get enough data points with positive label and we run the replicable quantile estimation on the marginal distribution of features with positive label to get some $\\dot{x_{\\mathit{\\Theta}}}\\in[2^{k}]$ .   \n3) Next, we take sufficiently many samples to get a positive point at or below $x$ and then we forward compute the label of $x$ .   \n4) The hypothesis we return is the following: If its input $(i,\\sigma)$ has index less than $i^{*}$ , it outputs 0. If its input is exactly $(i^{*},\\sigma^{*})$ , it outputs the previously computed label. Otherwise, it forward computes the label using $i,\\sigma$ . ", "page_idx": 18}, {"type": "text", "text": "Algorithm B.3 Replicable Learner for $\\mathcal{O W}S_{d}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: rLearnerOWSd(samples $(i_{1},\\sigma_{1}),\\dotsc,(i_{n},\\sigma_{m})$ , accuracy $\\alpha$ , replicability $\\rho$ , confidence $\\beta$ ):   \n2: Let $S_{+}:=(i_{j_{1}},\\sigma_{j_{1}}),\\dots,(i_{j_{n}},\\dot{\\sigma}_{j_{n}})$ be the subsequence of positive samples, where $i_{j_{k}}\\leq i_{j_{k+1}}$ .   \n3: $\\widehat{p}\\gets\\mathtt{r R o u n d}({n}/{m},\\rho\\alpha/48,\\rho/3)$   \n4: if $\\widehat{p}<\\alpha/2$ then   \n5:   return All-zero hypothesis.   \n6: end if   \n7: i\u2217\u2190rQuantileEst $(\\{i_{j_{1}},\\dots,i_{j_{n}}\\},\\alpha/2,\\alpha/4,\\rho/3,\\beta/3)$   \n8: if $i_{j_{1}}\\geq i^{*}$ then   \n9: return \u201cFAILURE\u201d   \n10: end if   \n11: $(\\sigma^{*},b^{*})\\gets\\mathsf{c o m p u t e F o r w a r d}(i^{*},i_{j_{1}},\\sigma_{j_{1}})$   \n12: return hypothesis $h(i,\\sigma):=^{\\prime}$ \u201c   \nIf $i<i^{*}$ , output 0.   \nIf $i=i^{*}$ , output $b^{*}$ if $\\sigma^{*}=\\sigma$ and 0 otherwise.   \nIf $i>i^{*}$ , get (\u03c3\u02c6,b\u02c6) \u2190ComputeForwar $\\boldsymbol{\\mathrm{\\Omega}}(i,i^{*},\\sigma^{*})$ and output $\\hat{b}$ if $\\sigma=\\hat{\\sigma}$ and 0 otherwise.\u201d ", "page_idx": 18}, {"type": "text", "text": "Theorem B.7. Let $\\rho,\\alpha\\in(0,1)$ and $\\beta\\in(0,\\rho/3)$ . Algorithm $B.3$ is an efficient $\\rho$ -replicable $(\\alpha,\\beta)$ - PAC learner for $O\\mathcal{W}S_{d}$ with sample complexity ", "page_idx": 18}, {"type": "equation", "text": "$$\nm=\\operatorname*{max}\\left({\\frac{392}{\\alpha^{2}\\rho^{2}}}\\ln{\\frac{6}{\\beta}},{\\frac{9216k^{2}}{\\alpha^{3}\\rho^{2}}}\\ln{\\frac{6}{\\beta}},{\\frac{32}{\\alpha^{2}}}\\ln{\\frac{6}{\\beta}}\\right)=O\\left({\\frac{d^{2}}{\\alpha^{3}\\rho^{2}}}\\ln{\\frac{1}{\\beta}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and time complexity ", "page_idx": 18}, {"type": "equation", "text": "$$\nO(\\mathrm{poly}(m)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof (Theorem B.7). The indicator random variable $I_{i}:=\\mathbb{1}\\{\\sigma_{i}=1\\}$ is a bounded random variable and its sample mean is precisely $n/m$ . Let $p\\in[0,1]$ denote the probability mass of the positively labeled elements. By an Hoeffding inequality, $|n\\rangle_{m}-p|\\leq\\alpha$ with probability at least ", "page_idx": 19}, {"type": "equation", "text": "$$\n2\\exp\\left(-2m\\alpha^{2}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nm\\geq\\frac{392}{\\alpha^{2}\\rho^{2}}\\ln\\frac{6}{\\beta},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then $|m/n-p|\\,\\leq\\,\\rho\\alpha/48$ with probability at least $\\beta/6$ . By Theorem B.4, the rounded estimate $\\widehat{p}$ is $\\rho/3$ -replicable and satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n|{\\widehat{p}}-p|\\leq\\alpha/4\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability at least $\\beta/6$ . From hereonforth, we condition on the success of this event. ", "page_idx": 19}, {"type": "text", "text": "Correctness of Algorithm B.3. First suppose $\\widehat{p}<\\alpha/2$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\np\\leq\\widehat{p}+\\alpha/4<\\alpha.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then Algorithm B.3 always returns the all-zero hypothesis. In this case, the returned hypothesis only makes mistakes on at most $\\alpha$ fraction of the population and we are content. Otherwise, suppose that $\\widehat{\\boldsymbol{p}}\\ge\\alpha/2$ . Thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{p\\geq\\widehat{p}-\\frac{\\alpha}{4}\\geq\\frac{\\alpha}{4}}\\\\ &{}&{\\frac{n}{m}\\geq p-\\frac{\\rho\\alpha}{16}\\geq\\frac{\\alpha}{8}}\\\\ &{}&{n\\geq\\frac{\\alpha}{8}m}\\\\ &{}&{=\\frac{1152k^{2}}{\\alpha^{2}\\rho^{2}}\\ln{\\frac{6}{\\beta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $F$ denote the CDF of the conditional distribution over the positively labeled samples. By Theorem B.6, the output of rQuantileEst is an index $i^{*}\\in[2^{k}]$ such that with probability at least $\\beta/6$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(i^{*})\\geq\\frac{\\alpha}{2}-\\frac{\\alpha}{4}\\geq\\frac{\\alpha}{4}}\\\\ {F(i^{*}-1)<\\frac{\\alpha}{2}+\\frac{\\alpha}{4}\\leq\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We proceed conditioning on the success of the call to rQuantileEst. ", "page_idx": 19}, {"type": "text", "text": "Since $F(i^{*})\\geq\\alpha/8$ , the probability that $i_{j_{1}}>i^{*}$ is at most ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(1-{\\frac{\\alpha}{8}}\\right)^{n}\\leq e^{-{\\frac{n\\alpha}{8}}}\\leq{\\beta}/{6}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The last inequality is due to $n\\,\\geq\\,\\alpha m/8\\,\\geq\\,\\left(4/\\alpha\\right)\\ln(6/\\beta)$ . We proceed conditioning on the event $i_{j_{1}}\\leq i^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "So either $i_{j_{1}}=i^{*}$ and we know its string $\\sigma^{*}$ and label $b^{*}$ , or $i_{j_{1}}<i^{*}$ and Proposition B.1 assures that we can obtain the string $\\sigma^{*}$ and label $b^{*}$ through forward computation ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\sigma^{*},b^{*})\\gets\\mathsf{c o m p u t e F o r w a r d}(i^{*},i_{j_{1}},\\sigma_{j_{1}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, consider the hypothesis $h$ we output and its action on an input $(i,\\sigma)$ : If $i<i^{*}$ , then $h$ always answers 0 so it is incorrect only on the positive labels. But this happens at most $F(i^{*}-1)<\\alpha$ of the time. If $i\\geq i^{*}$ , $h$ is always correct. Hence the total population error is at most $\\alpha$ as desired. We conditioned on three events, each of which has failure probability at most $\\beta/6$ . Hence the total probability of failure is at most $\\beta/2$ . ", "page_idx": 19}, {"type": "text", "text": "Replicability of Algorithm B.3. By Theorem B.4, the output $\\widehat{p}$ is $\\rho/3$ replicable. By Theorem B.6, the output $i^{*}$ is $\\rho/3$ replicable. From our analysis above, $i_{j_{1}}<i^{*}$ with probability at most $\\beta/6$ in each of two executions. Thus the total probability of outputting different classifiers is at most ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\rho}{3}+\\frac{\\rho}{3}+\\frac{2\\beta}{6}\\leq\\frac{2\\rho}{3}+\\frac{\\rho}{9}\\leq\\rho.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.6 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now restate and prove Theorem 2.1. ", "page_idx": 20}, {"type": "text", "text": "Theorem 2.1. Let $d\\in\\mathbb{N}$ . The following hold:   \n(a) [Bun, 2020] Assuming the existence of one-way functions, the concept class OWS with input domain $\\{0,1\\}^{d}$ cannot be learned by an efficient no-regret algorithm, i.e., an algorithm that for some $\\eta>0$ achieves expected regret $\\mathbb{E}[R_{T}]\\leq\\mathrm{poly}(d)\\cdot T^{1-\\eta}$ using time poly $(d,T)$ in every iteration.12 ", "page_idx": 20}, {"type": "text", "text": "(b) (Theorem B.7) The concept class $\\mathcal{O}\\backslash\\mathcal{V}\\mathcal{S}$ with input domain $\\{0,1\\}^{d}$ can be learned by a $\\rho$ - replicable $(\\alpha,\\beta)$ -PAC learner with sample complexity $m\\,=\\,\\mathrm{\\bar{~}p o l y}(d,1/\\alpha,1/\\rho,\\log(1/\\beta))$ and $\\mathrm{poly}(m)$ running time. ", "page_idx": 20}, {"type": "text", "text": "Proof (Theorem 2.1). Combining the hardness of efficiently online learning $\\mathcal{O}\\backslash\\mathcal{S}$ (cf. Theorem B.3) with the efficient replicable learner of Theorem B.7 completes the the proof of Theorem 2.1. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C Efficient Replicability and SQ Learning: Parities ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 The Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "1PCsDNG6Jg/tmp/373af117d71f037ad4fe95182efe9ea79f3010cea44945d011a95a234eb30b10.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We now repeat and prove Lemma 4.2, which states the correctness of Algorithm C.1. ", "page_idx": 20}, {"type": "text", "text": "Lemma 4.2. The concept class of affine parities AffParity over $\\{0,1\\}^{d}$ admits a $\\rho$ -replicable algorithm that perfectly learns any concept with respect to the uniform distribution $\\boldsymbol{\\mathcal{U}}$ with probability of success at least $1-\\beta$ . The algorithm has $O(\\mathrm{poly}(d,\\log{^{1}\\!/\\!\\rho\\beta}))$ sample and time complexity. ", "page_idx": 20}, {"type": "text", "text": "Proof (Lemma 4.2). For any $i\\in[d]$ , observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y^{(i)}:=f(x^{(i)})+f(x^{(0)}):=w^{\\top}x^{(i)}+b+w^{\\top}x^{(0)}+b=w^{\\top}(x^{(i)}+x^{(0)})=:w^{\\top}z^{(i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus the dataset of offsets $\\{(z^{(i)},y^{(i)})\\}$ uniquely determines the linear function $w$ . Having learned $w$ , we can recover the value of $b$ by evaluating at any point in the original dataset, say at the first point. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C.2 Background on Parities and SQ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Statistical Queries We start with some background on the SQ model. ", "page_idx": 20}, {"type": "text", "text": "Definition C.1 (SQ Learning). A concept class $\\mathcal{C}$ with input space $\\{0,1\\}^{d}$ is learnable from statistical queries with respect to distribution $\\mathcal{D}$ if there is a learning algorithm $\\boldsymbol{\\mathcal{A}}$ such that for any $c\\in{\\mathcal{C}}$ and any $\\alpha>0$ , $\\boldsymbol{\\mathcal{A}}$ produces an $\\alpha$ -approximation of c from statistical queries; furthermore, the running time, the number of queries asked, and the inverse of the smallest tolerance used must be polynomial in $d$ and $^1\\!/\\!\\alpha$ . ", "page_idx": 20}, {"type": "text", "text": "Definition C.2 (SQ Hardness). Consider a concept class $\\mathcal{C}$ with input space $\\{0,1\\}^{d}$ . Fix a tolerance $\\tau=\\mathrm{poly}(1/d)$ and accuracy $\\alpha=O(1)$ . We say that $\\mathcal{C}$ is SQ-hard under distribution $\\mathcal{D}$ if any $S Q$ algorithm requires $\\omega(\\mathrm{poly}(d))$ queries of tolerance at least $\\tau$ to $\\alpha$ -learn $\\mathcal{C}$ . ", "page_idx": 20}, {"type": "text", "text": "The standard way to show SQ hardness is by showing lower bounds on the so-called $S Q$ dimension [Feldman et al., 2017] of the corresponding problems (we omit the formal definition of SQ dimension as it is beyond the scope of the present work). Such lower bounds on this dimension establish lower bounds on the running time of any SQ algorithm for the problem \u2013 not on its sample complexity. ", "page_idx": 20}, {"type": "text", "text": "PAC and SQ Learning Parities The standard class of parities Parity is given by the subset $\\{f_{S,0}:S\\subseteq[d]\\}$ of AffParity. The standard algorithm for learning parity functions works by viewing a set of $n$ labelled examples as a set of $n$ linear equations over the finite field with two elements $\\mathbb{F}_{2}$ . Then, Gaussian elimination is used to solve the system and thus find a consistent parity function. This algorithm is extremely brittle to noise: even for the uniform marginal distribution $\\mathcal{D}=\\mathcal{U}$ , the problem of learning parities over $\\{0,1\\}^{d}$ does not belong to SQ. In particular, a standard result is the following: ", "page_idx": 21}, {"type": "text", "text": "Fact C.3 ([Kearns, 1998]). Even learning parities Parity $\\subset$ AffParity with input space $\\{0,1\\}^{d}$ is SQ-hard under the uniform distribution $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 21}, {"type": "text", "text": "C.3 Gaussian Elimination is not Replicable ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we give a simple example where Gaussian elimination fails to be replicable. ", "page_idx": 21}, {"type": "text", "text": "Proposition C.4. For any $d\\geq3$ and $S\\subseteq[d]$ with $2\\not\\in S\\subseteq[d]$ , the following holds: There is some $n_{d}\\ge1$ , such that for any $n\\geq n_{d}$ , there exists a distribution $\\mathcal{D}$ such that the $n$ -sample Gaussian elimination algorithm for PAC-learning parities $\\textstyle f(x)=\\sum_{i\\in S}x_{i}$ in the realizable setting fails to be $^{1}\\!/\\!17$ -replicable. ", "page_idx": 21}, {"type": "text", "text": "Proof (Proposition $C.4.$ ). Let $e_{1},\\ldots,e_{d}$ be the standard basis of $\\mathbb{Z}_{2}^{d}$ , $p\\in(0,{^2}/3]$ , and consider the distribution $\\mathcal{D}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathcal{D}}(e_{1})=p,\\qquad{\\mathcal{D}}(e_{2})={\\frac{2}{3}}-p,\\qquad{\\mathcal{D}}(e_{i})={\\frac{1}{3(d-2)}},i\\geq3.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The probability of not observing any $e_{1}$ \u2019s after $n$ samples is $(1-p)^{n}$ . We set $p:=1-\\sqrt[n]{1/2}\\leq1/2$ so that $(1-p)^{n}=1/2$ . ", "page_idx": 21}, {"type": "text", "text": "For sufficiently large $n_{d}\\in\\mathbb{N}$ , we observe all $e_{i},i\\geq3$ with probability at least $^{3}\\!/\\!4$ after $n$ samples. Thus with probability at least $\\frac{1}{4}$ , we observe $e_{1},e_{3},\\ldots,e_{d}$ , and with probability at least $^1\\!/\\!4$ , we observe $e_{3},\\ldots,e_{d}$ but not $e_{1}$ . In the first case, we fully recover $S$ since we are promised that $2\\not\\in S$ . In the second case, our algorithm is unable to determine if $1\\in S$ since we do not observe $e_{1}$ and the best it can do is randomly guess. Thus the probability we output different classifiers is at least $2\\cdot1/4\\cdot1/4\\cdot1/2=1/16$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proposition C.4 shows that Gaussian elimination fails in general to be replicable regardless of the number of samples requested. ", "page_idx": 21}, {"type": "text", "text": "C.4 Replicably Learning Affine Parities Beyond the Uniform Distribution ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We restate our main result. ", "page_idx": 21}, {"type": "text", "text": "Theorem C.5. For any dimension $d_{;}$ , accuracy $\\alpha$ , confidence $\\beta$ and replicability $\\rho,$ , there exists some $n=\\mathrm{poly}(d,1/\\alpha,1/\\rho,\\log(1/\\beta))$ such that there exists a monotone distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ which ", "page_idx": 21}, {"type": "text", "text": "(a) Gaussian elimination is not $n$ -sample replicable with probability $\\Omega(1)$ under $\\mathcal{D}$ for the class AffParity,   \n$(b)$ there exists an $n$ -sample $\\rho$ -replicable algorithm for $(\\alpha,\\beta)$ -PAC learning the class AffParity with respect to $\\mathcal{D}$ with $\\mathrm{poly}(n)$ runtime, and   \n(c) even learning the class Parity is SQ-hard under the distribution $\\mathcal{D}$ . ", "page_idx": 21}, {"type": "text", "text": "Before proving Theorem C.5, we derive three useful lemmas. Fix a dataset size $n$ to be defined later. We pick the distribution $\\mathcal{D}$ to be a Boolean product distribution $(p_{1},...,p_{d})\\in[0,1]^{d}$ such that the first $d-1$ coordinates are unbiased $(p_{i}=1/2)$ ) for $i\\in[d-1]$ and the last coordinate to be highly biased towards 1. In particular, pick $p_{d}=\\mathcal{D}_{d}(\\{1\\})=\\,\\sqrt[n]{1/2}\\,\\geq1/2$ . This distribution is monotone by construction. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.6. The naive $n$ -sample Gaussian elimination algorithm fails to be replicable with constant probability over the distribution $\\mathcal{D}=(p_{1},...,p_{d})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Consider two independent draws $S_{1}$ and $S_{2}$ from $\\mathcal{D}$ of size $n$ . With constant probability $1/2\\cdot1/2=1/4$ , $S_{1}$ will contain vectors that have only 1\u2019s in the last coordinate while $S_{2}$ will contain at least one vector with a zero in the last direction. Then there are exactly two hypotheses that satisfy $S_{1}$ , one that contains the last coordinate and one that does not. This can be seen by reducing to an instance of the $(d-1)$ -dimensional parities problem obtained by ignoring the last coordinate and flipping the bits of the labels. ", "page_idx": 21}, {"type": "text", "text": "On the other hand, for $n$ sufficiently large, the subset of vectors S2(1)\u2286S2 that have 1\u2019s in the last direction is satisfied by the same hypotheses as $S_{1}$ . However, only one of the two hypotheses also satisfies the other subset $S_{2}^{(0)}\\subseteq S_{2}$ consisting of entries with a 0 in the last direction. ", "page_idx": 22}, {"type": "text", "text": "Thus the candidate hypotheses that satisfy $S_{1},S_{2}$ differ and the algorithm can at best output a random guess for $S_{1}$ , which will be inconsistent with the output on $S_{2}$ with overall probability at least $1/2\\cdot1/4=1/8$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma C.7. The decision tree complexity of $\\mathcal{D}$ is $\\Theta(1)$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Since $\\mathcal{D}$ is a product distribution, the pmf only takes on 2 different values and thus has depth $\\Theta(1)$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma C.8. The class of affine parities AffParity with input space $\\{0,1\\}^{d}$ is closed under restriction. ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $f$ be an arbitrary affine parity function, i.e., $\\textstyle f(x)=b+\\sum_{i\\in S}x_{i}$ for some $b\\in\\{0,1\\}$ and $S\\subseteq[d]$ . We remark that the operator $^+$ is in $\\mathbb{Z}_{2}$ . Consider an arbitrary direction $i\\in[d]$ and arbitrary bit $\\bar{b^{\\prime}}\\bar{\\in}\\{0,1\\}$ . We will verify that $f_{i=b^{\\prime}}$ remains in the class of affine parities. If $i\\not\\in S$ , then $f_{i=b^{\\prime}}=$ $f\\in$ AffParity. Now, if $i\\in S$ and $c=0$ , it holds that $f_{i=b^{\\prime}}(x)=b+\\dot{\\textstyle\\sum}_{i\\in S\\backslash\\{i\\}}\\,x_{i}\\stackrel{\\cdot}{\\in}$ AffParity and if $b^{\\prime}=1$ , it holds that $\\begin{array}{r}{f_{i=b^{\\prime}}(x)=(b+1)+\\sum_{i\\in S\\backslash\\{i\\}}x_{i}\\in}\\end{array}$ AffParity. Hence, affine parities are closed under restriction. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "We are now ready to prove Theorem C.5. ", "page_idx": 22}, {"type": "text", "text": "Proof (Theorem C.5). ", "page_idx": 22}, {"type": "text", "text": "(a) By Lemma C.6. ", "page_idx": 22}, {"type": "text", "text": "(b) We can employ the lifting algorithm of Theorem 3.2 to replicably and efficiently learn affine parities since (i) the decision tree complexity is constant, (ii) the distribution is monotone, (iii) affine parities are closed under restriction and (iv) we have an efficient replicable algorithm for affine parities with respect to the uniform distribution (cf. Lemma 4.2). ", "page_idx": 22}, {"type": "text", "text": "(c) If we consider the subclass $\\mathcal{C}^{\\prime}=\\{f_{S,0}:S\\subseteq[d-1]\\}$ , we have that $\\mathbb{E}_{x\\sim\\mathcal{D}}[f(x)g(x)]=0$ for any pair of distinct functions $f,g\\,\\in\\,\\mathcal{C}^{\\prime}$ due to the product structure and the unbiasedness of the first $d-1$ coordinates of $\\mathcal{D}$ . This implies the SQ-dimension of the class of parities is at least $2^{d-1}=\\Omega(2^{d})$ and so learning parities is SQ-hard under $\\mathcal{D}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "D Lifting Replicable Uniform Learners ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we will provide our general lifting framework for replicable learning algorithms that is needed in order to show Theorem C.5. ", "page_idx": 22}, {"type": "text", "text": "D.1 Preliminaries for Replicable Uniform Lifting ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We start this section with some definitions which are necessary to formally state our main result of Theorem 3.2. ", "page_idx": 22}, {"type": "text", "text": "Definition D.1 (Decision Tree (DT)). A decision tree ${T:\\{0,1\\}^{d}\\,\\rightarrow\\,\\mathbb{R}}$ is a binary tree whose internal nodes query a particular coordinate, and whose leaves are labelled by values. Each instance $x\\in\\{0,1\\}^{d}$ follows a unique root-to-leaf path in $T$ : at any internal node, it follows either the left or right branch depending on the value of the queried coordinate, until a leaf is reached and its value $T(x)$ is returned. ", "page_idx": 22}, {"type": "text", "text": "Definition D.2 (Decision tree distribution). We say that a distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ is representable by a depth- $\\ell$ decision tree, if its pmf is computable by a depth-\u2113decision tree $T$ . Specifically, each leaf t is labelled by a value $p_{t}$ , so that $\\mathcal{D}(x)=p_{t}$ for all $x\\in t$ . This means that the conditional distribution of all points that reach a leaf is uniform. ", "page_idx": 22}, {"type": "text", "text": "Let us consider a distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ . The definitions above suggest a natural measure of the complexity of $\\mathcal{D}$ : The decision tree complexity. ", "page_idx": 22}, {"type": "text", "text": "Definition D.3 (Decision Tree Complexity). The decision tree complexity of a distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ is the smallest integer $\\ell$ such that its probability mass function (pmf) can be represented by $a$ depth- $\\ell$ decision tree. ", "page_idx": 22}, {"type": "text", "text": "Next, we define a useful structured family of distributions. Let the binary relation $\\succeq$ denote pointwise comparison. ", "page_idx": 23}, {"type": "text", "text": "Definition D.4 (Monotone Distribution). A probability distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ is called monotone if for any $x\\succeq y$ it holds that $D(x)\\geq D(y)~$ . ", "page_idx": 23}, {"type": "text", "text": "Just as in Blanc et al. [2023], for arbitrary non-monotone probability measures, our algorithm requires access to a conditional sampling oracle (cf. Definition D.5) defined below. ", "page_idx": 23}, {"type": "text", "text": "Definition D.5 (Conditional Sampling Oracle; Blanc et al., 2023). A conditional sampling oracle for a distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ proceeds as follows: suppose we condition on some subset $I$ of the $d$ variables having a fixed value $b\\in\\{0,1\\}^{I}$ . In that case, the oracle generates a sample from the true conditional distribution, i.e., draws a sample $x_{-I}\\sim{\\cal D}(\\cdot\\mid x_{I}=b)$ . ", "page_idx": 23}, {"type": "text", "text": "Definition D.6. For a tree $T$ and leaves $\\ell\\in T$ , $\\begin{array}{r}{\\mathbb{E}_{\\ell\\in T}f(\\ell):=\\sum_{\\ell\\in T}2^{-|\\ell|}f(\\ell),}\\end{array}$ , where $|\\ell|$ is the depth of the path to reach leaf $\\ell$ . ", "page_idx": 23}, {"type": "text", "text": "As an example of this notation, note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\ell\\in T}\\left[\\mathbb{E}_{x\\sim\\mathcal{U}^{d}}[f(x)|x\\in\\ell]\\right]=\\mathbb{E}_{\\ell\\in T}\\left[\\sum_{x\\in\\ell}\\frac{f(x)\\mathcal{U}(x)}{\\mathcal{U}(\\ell)}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathcal{U}(\\ell)$ is the uniform mass of the leaf $\\ell$ , i.e., $\\mathcal{U}(\\ell)~=~2^{-n}\\,\\cdot\\,2^{n-|\\ell|}$ . This implies that $\\mathbb{E}_{\\ell\\in T}[\\mathbb{E}_{x\\sim\\mathcal{U}^{d}}[f(x)|x\\in\\ell]]=\\mathbb{E}_{x\\in\\mathcal{U}^{d}}f(x)$ , since the set of leaves partitions the set $\\{0,1\\}^{d}$ . ", "page_idx": 23}, {"type": "text", "text": "D.2 Main Result: Lifting Replicable Uniform Learners ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We now restate our main result Theorem 3.2. For the sake of presentation, we defer its proof to Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Theorem 3.2 (Lifting Replicable Uniform Learners). Consider a concept class $\\mathcal{C}$ of functions   \n$f\\,:\\,\\{0,1\\}^{d}\\,\\rightarrow\\,\\{0,1\\}$ closed under restrictions. Suppose we are given black-box access to an   \nalgorithm such that for any $\\alpha^{\\prime},\\rho^{\\prime},\\beta^{\\prime}\\in(0,1),$ , given $\\operatorname{poly}(d,1/\\alpha^{\\prime},1/\\rho^{\\prime},\\log(1/\\beta^{\\prime}))$ samples from $\\boldsymbol{\\mathcal{U}}$ , (i) is $\\rho^{\\prime}$ -replicable with respect to the uniform distribution $\\boldsymbol{\\mathcal{U}}$ , (ii) PAC learns $\\mathcal{C}$ under the uniform distribution to accuracy $\\alpha^{\\prime}$ and confidence $\\beta^{\\prime}$ , and,   \n(iii) terminates in time pol $\\mathrm{y}(d,1/\\alpha^{\\prime},1/\\rho^{\\prime},\\mathrm{log}(1/\\beta^{\\prime}))$ .   \nLet $\\alpha,\\rho\\quad\\in\\ \\ (0,1)$ and $\\beta\\quad\\in\\ \\ (0,\\rho/3)$ . For $\\begin{array}{r c l}{{m}}&{{=}}&{{\\mathrm{poly}(d,1/\\alpha,1/\\rho,\\log(1/\\beta))}}\\end{array}$ and $\\textit{M}=$   \n$\\mathrm{poly}\\big(d,1/\\alpha,1/\\rho,\\log\\big(1/\\beta\\big)\\big)^{O(\\ell)}$ , the following cases hold:   \n(a) If $\\mathcal{D}$ is a monotone distribution over $\\{0,1\\}^{d}$ representable by a depth- $\\ell$ decision tree, there is an algorithm that draws $M$ samples from $\\mathcal{D}$ , is $\\rho$ -replicable with respect to $\\mathcal{D}$ , PAC learns $\\mathcal{C}$ under $\\mathcal{D}$ with accuracy $\\alpha$ and confidence $\\beta$ , and terminates in poly $(M)$ time.   \n(b) If $\\mathcal{D}$ is an arbitrary distribution over $\\{0,1\\}^{d}$ representable by a depth- $\\ell$ decision tree, there is an algorithm that draws $M$ labeled examples as well as $M$ conditional samples (cf. Definition D.5) from $\\mathcal{D}$ , is $\\rho$ -replicable with respect to $\\mathcal{D}$ , PAC learns $\\mathcal{C}$ with respect to $\\mathcal{D}$ with accuracy $\\alpha$ and confidence $\\beta$ , and terminates in poly $(M)$ time. ", "page_idx": 23}, {"type": "text", "text": "We emphasize that our result for monotone distributions $\\mathcal{D}$ only requires sample access to $\\mathcal{D}$ . For arbitrary non-monotone probability measures, our algorithm requires access to a conditional sampling oracle (cf. Definition D.5) ", "page_idx": 23}, {"type": "text", "text": "E Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Preliminaries ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We start this section with some useful definitions, coming from the work of Blanc et al. [2023]. ", "page_idx": 23}, {"type": "text", "text": "Definition E.1 (Weighting function of distribution). Let $\\mathcal{D}$ be an arbitrary distribution over $\\{0,1\\}^{d}$ .   \nWe define the weighting function $f_{\\cal D}(x)=2^{d}{\\cal D}(x)$ . ", "page_idx": 23}, {"type": "text", "text": "Remark that the scaled pmf satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\mathcal{U}}[f_{\\mathcal{D}}(x)]=\\sum_{x\\in\\{0,1\\}^{d}}2^{-d}\\cdot2^{d}\\mathcal{D}(x)=1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $x\\in\\{0,1\\}^{d}$ and $i\\in[d]$ , we write $x^{\\sim i}$ to denote the binary vector obtained from $x$ by filpping its $i$ -th bit. ", "page_idx": 23}, {"type": "text", "text": "Definition E.2 (Influence of Variables on Distributions). Let $\\mathcal{D}$ be a distribution over $\\{0,1\\}^{d}$ and $f_{\\cal D}(x)=2^{d}\\cdot{\\cal D}(x)$ be its pmf scaled up by the domain size. The influence of a coordinate $i\\in[d]$ on a distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ is the quantity ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Infl}_{i}(f_{\\mathcal{D}}):=\\mathbb{E}_{x\\sim\\mathcal{U}}\\left[|f_{\\mathcal{D}}(x)-f_{\\mathcal{D}}(x^{\\sim i})|\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We also define the total influence $\\begin{array}{r}{\\mathrm{Infl}(f_{\\mathcal{D}})=\\sum_{i\\in[d]}\\mathrm{Infl}_{i}(f_{\\mathcal{D}}).}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Intuitively, the influence of a variable measures how far that variable is from the uniform distribution. Suppose $f_{\\mathcal{D}}$ is computable by a depth- $\\ell$ decision tree $T$ . Then we can write for any $x\\in\\{0,1\\}^{d}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{D}(x)=\\sum_{\\ell\\in T}p_{\\ell}\\cdot\\mathbb{1}\\{x\\in\\ell\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the sum is taken over the leaves of $T$ and $p_{\\ell}:=\\mathcal{D}(y)$ is the (same) mass $\\mathcal{D}$ assigns to every $y\\in\\ell$ . Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Infl}_{i}(f_{\\mathcal{D}}):=\\mathbb{E}_{x\\sim\\mathcal{U}}|f_{\\mathcal{D}(x)}-f_{\\mathcal{D}}(x^{\\sim i})|}\\\\ &{\\phantom{\\mathrm{Tnfl}_{i}(f_{\\mathcal{D}}):=}=\\displaystyle\\sum_{x\\in\\{0,1\\}^{d}}|\\mathcal{D}(x)-\\mathcal{D}(x^{\\sim i})|}\\\\ &{\\phantom{\\mathrm{Tnfl}_{i}(f_{\\mathcal{D}}):=\\mathcal{E}_{x}\\sim\\mathcal{V}(\\mathcal{G})}=\\displaystyle\\sum_{x}\\left|\\sum_{\\ell\\in T}p_{\\ell}\\left(\\mathbb{1}\\{x\\in\\ell\\}-\\mathbb{1}\\{x^{\\sim i}\\in\\ell\\}\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If $i$ is not queried by any internal node of $T$ , then $x,x^{\\sim i}$ always belongs to the same leaf and this value is 0. Otherwise, we can trivially bound this value by 2 using the triangle inequality. This discussion leads to the following observation. ", "page_idx": 24}, {"type": "text", "text": "Fact E.3. If $f_{\\mathcal{D}}$ is computable by a depth- $\\ell$ decision tree, then its total influence is at most ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Infl}(f_{\\mathcal{D}})\\leq2\\ell.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For monotone distributions over the Boolean hypercube, influences have a convenient form: ", "page_idx": 24}, {"type": "text", "text": "Proposition E.4 (Lemma 6.2 in Blanc et al. [2023]). If the distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ is monotone, then for any $i\\in[d]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Infl}_{i}(f_{\\mathcal{D}})=\\mathbb{E}_{\\mathcal{D}}[x_{i}]\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Definition E.5 (Restrictions). Given $a$ sequence of (coordinate, value) pairs $\\pi\\quad=$ $\\{(i_{1},b_{1}),...,(i_{k},b_{k})\\}$ , we use $x_{\\pi}$ to represent $x$ with the coordinates in $\\pi$ overwritten/inserted with their respective values. For $a$ function $f:\\{0,1\\}^{d}\\to\\mathbb{R}$ , we let $f_{\\pi}$ be the function that maps $x$ to $f(x_{\\pi})$ . ", "page_idx": 24}, {"type": "text", "text": "Definition E.6 (Everywhere $\\tau$ -influential; Definition 4 in Blanc et al. [2022b]). For any function $f:\\{0,1\\}^{d}\\to\\vec{\\mathbb{R}},$ , influence threshold $\\tau>0$ and decision tree $T:\\{0,1\\}^{d}\\to\\mathbb{R}_{}$ , we say that $T$ is everywhere $\\tau$ -influential with respect to some $f$ if for every internal node $v$ of $T$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{Infl}_{i(v)}(f_{v})\\geq\\tau\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here $i(v)$ denotes the variable queried at $v$ and $f_{v}$ denotes the restriction of $f$ by the root-to-v path in $T$ , i.e. the variables in the path are fixed to the corresponding internal vertex values. ", "page_idx": 24}, {"type": "text", "text": "E.2 Replicable Proper Learner for Decision Tree distributions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our algorithm that achieves the bounds of Theorem 3.2 proceeds in a two-stage manner: we first replicably learn the decision tree structure of $\\mathcal{D}$ and then use the replicable uniform-distribution learner to learn $f$ restricted to each of the leaves of the tree. To carry out the first stage, we give an algorithm that replicably learns the optimal decision tree decomposition of a distribution $\\mathcal{D}$ : ", "page_idx": 24}, {"type": "text", "text": "Theorem E.7 (Replicably Learning DT distributions). $F i x\\,\\alpha,\\rho\\in(0,1)$ and $\\beta\\in(0,\\rho/3)$ . Let $\\mathcal{D}$ be $a$ distribution over $\\{0,1\\}^{d}$ that is representable by a depth- $\\ell$ decision tree. There is an algorithm that ", "page_idx": 24}, {"type": "text", "text": "(a) is $\\rho$ -replicable with respect to $\\mathcal{D}$ ,   \n$(b)$ returns a depth- $\\ell$ tree representing a distribution $\\mathcal{D}^{\\prime}$ such that $\\mathrm{TV}(\\mathcal{D},\\mathcal{D}^{\\prime})\\leq\\alpha$ with probability at least $1-\\beta$ over the draw of samples,   \n(c) and has running time and sample complexity ", "page_idx": 24}, {"type": "equation", "text": "$$\nN=\\mathrm{poly}(d,1/\\alpha,1/\\rho,\\log(1/\\beta))\\cdot(2\\ell/\\alpha)^{O(\\ell)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For monotone distributions, the algorithm only uses random samples from $\\mathcal{D}$ , and for general distributions, it uses subcube conditional samples (cf. Definition E.12). ", "page_idx": 25}, {"type": "text", "text": "Proof (Theorem E.7). The proof of this result follows directly by applying either Lemma E.11 (for monotone distributions) or Lemma E.13 (for general distributions with subcube conditional sample access) to Theorem E.8, which are provided right after. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Essentially, making the decision tree learning routine replicable boils down to estimating influences on $\\mathcal{D}$ in a replicable manner. This is the content of the upcoming results. ", "page_idx": 25}, {"type": "text", "text": "Theorem E.8 (Replicable Influence $\\Rightarrow{}$ Replicable Decision Tree). Let $\\mathcal{D}$ be a distribution that is representable by a depth-\u2113decision tree. For any $\\rho^{\\prime}\\in(0,1)$ assume access to a $\\rho^{\\prime}$ -replicable algorithm rInflEst for estimating influences of distributions over $\\{0,1\\}^{d}$ using $\\operatorname{poly}(d,1/\\alpha,1/\\rho^{\\prime},\\log(1/\\beta))$ samples and runtime. Then, for any $\\rho\\in(0,1)$ , there is an algorithm rBuildDT (cf. Algorithm $E.l$ ) that ", "page_idx": 25}, {"type": "text", "text": "(a) is $\\rho$ -replicable with respect to $\\mathcal{D}$ ,   \n$(b)$ returns a depth- $\\ell$ tree representing a distribution $\\mathcal{D}^{\\prime}$ such that $\\mathrm{TV}({\\mathcal{D}},{\\mathcal{D}}^{\\prime})\\;\\leq\\;\\alpha$ , with probability at least $1-\\beta$ over the draw of samples,   \n(c) and has running time and sample complexity ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{M=\\mathrm{poly}\\left(d,\\frac{1}{\\alpha},\\frac{d\\left(2\\ell/\\alpha\\right)^{O(\\ell)}}{\\rho},\\log\\frac{d\\left(2\\ell/\\alpha\\right)^{O(\\ell)}}{\\beta}\\right)\\cdot d\\cdot\\left({2\\ell}/{\\alpha}\\right)^{O(\\ell)}}}\\\\ {{\\qquad+O\\left(\\frac{\\left(2\\ell/\\alpha\\right)^{O(\\ell)}}{\\alpha^{2}\\rho^{2}}\\log\\frac{\\left(2\\ell/\\alpha\\right)^{O(\\ell)}}{\\beta}\\right)\\cdot\\left(2\\ell/\\alpha\\right)^{O(\\ell)}}}\\\\ {{\\qquad=\\mathrm{poly}(d,1/\\alpha,1/\\rho,\\log(1/\\beta))\\cdot\\left({2\\ell}/{\\alpha}\\right)^{O(\\ell)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Algorithm E.1 Replicably Building Decision Tree for $\\mathcal{D}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1: rBuildDT: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "2: Input: Access to $\\mathcal{D}$ and $f_{\\mathcal{D}}$ as in Definition E.1, restriction $\\pi$ (cf. Definition E.5), access to rInflEst, depth $\\ell$ , influence threshold $\\tau$ , accuracy $\\alpha$ , confidence $\\beta$ , replicability $\\rho$ . ", "page_idx": 25}, {"type": "text", "text": "3: Output: A decision tree $T$ that minimizes $\\mathbb{E}_{t\\in T}[\\operatorname{Infl}((f_{\\mathcal{D}})_{t})]$ , where $t\\in T$ is the collection of leaves (each corresponding to some restriction) of $T$ among all depth- $\\ell$ , everywhere $\\tau$ -influential (cf. Definition E.6) trees. ", "page_idx": 25}, {"type": "text", "text": "4: for $i\\in[d]$ do ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{r I n f1E s t}[(f_{\\mathcal{D}})_{\\pi},i]\\leftarrow\\mathtt{r I n f1E s t}\\left((f_{\\mathcal{D}})_{\\pi},i,\\operatorname*{min}(\\tau/4,\\alpha/{2d}),\\frac{\\beta}{2d(2\\ell/\\alpha)^{O(\\ell)}},\\frac{\\rho}{2d(2\\ell/\\alpha)^{O(\\ell)}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "7: Let $S\\subseteq[d]$ be the set of variables $i$ so that rInflEst $[(f_{\\mathcal{D}})_{\\pi},i]\\ge3\\tau/4$ . ", "page_idx": 25}, {"type": "text", "text": "8: if $S=\\emptyset$ or $\\ell=0$ then ", "page_idx": 25}, {"type": "text", "text": "9: // replicably estimate $p_{\\pi}:=2^{|\\pi|}\\operatorname*{Pr}_{x\\sim\\mathcal{D}}[x$ is consistent with $\\pi$ ] (cf. Theorem B.4)   \n10: $\\hat{p}_{\\pi}\\gets\\alpha/2$ -accurate, $\\rho/[2(^{2\\ell}/\\alpha)^{O(\\ell)}]$ -replicable, $\\beta/[2(^{2\\ell}/\\alpha)^{O(\\ell)}]$ -confident estimate of $p_{\\pi}$ ", "page_idx": 25}, {"type": "text", "text": "11: return a new leaf node with value $\\hat{p}_{\\pi}$ ", "page_idx": 25}, {"type": "text", "text": "12: else ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "13: for $i\\in S$ do ", "page_idx": 25}, {"type": "text", "text": "14: construct the tree $T_{i}$ with   \n15: $\\mathrm{root}(T_{i})\\gets x_{i}$   \n16: leftSubtre $\\begin{array}{r}{\\boldsymbol{\\mathfrak{z}}(T_{i})\\gets\\mathtt{r B u i l d D T}(\\mathcal{D},\\pi\\cup\\{x_{i}=-1\\},\\ell-1,\\tau,\\alpha,\\beta,\\rho)}\\end{array}$   \n17: r $\\operatorname{ghtSubtree}(T_{i})\\gets\\operatorname{rBuildDT}(\\mathcal{D},\\pi\\cup\\{x_{i}=1\\},\\ell-1,\\tau,\\alpha,\\beta,\\rho)$ ", "page_idx": 25}, {"type": "text", "text": "18: end for ", "page_idx": 25}, {"type": "text", "text": "19: // Among $\\{T_{i}\\}_{i\\in S}$ , return the one that minimizes the estimated total influence ", "page_idx": 25}, {"type": "text", "text": "20: $//\\mathbb{E}_{t\\in T_{i}}$ $\\begin{array}{r}{\\Big[\\sum_{j\\in[d]}\\mathbf{r}\\mathtt{I n f}\\mathtt{l E s t}[(f_{\\mathcal{D}})_{t},j]\\Big]}\\end{array}$ where $(f_{\\mathcal{D}})_{t}=(f_{\\mathcal{D}})_{\\pi(t)}$   \n21: // for the restriction $\\pi(t)$ corresponding to path from the root the leaf $t$   \n22: // Let $\\begin{array}{r}{g(t):=\\sum_{j\\in[d]}\\pmb{\\tau}\\mathtt{I n f1E s t}[(f_{\\mathcal{D}})_{t},j]}\\end{array}$   \n23: $\\begin{array}{r}{i^{*}\\gets\\mathrm{argmin}_{i\\in S}\\bar{\\mathbb{E}_{t\\in T_{i}}}[g(t)]=\\sum_{t\\in T_{i}}2^{-|t|}g(t)=\\sum_{x\\in\\{0,1\\}^{d}}2^{-d}g(t)\\mathbb{1}\\{x\\in t\\}}\\end{array}$   \n24: return $T_{i^{*}}$ ", "page_idx": 25}, {"type": "text", "text": "25: end if ", "page_idx": 25}, {"type": "text", "text": "Before proving Theorem E.8, we state Theorem 3 of Blanc et al. [2023], from which the correctness of Algorithm E.1 follows. Let BuiltDT be the algorithm obtained from rBuildDT (cf. Algorithm E.1) with the following adjustments. ", "page_idx": 26}, {"type": "text", "text": "(i) Replace the replicable influence estimator rInflEst with any (possibly non-replicable) influence estimator and (ii) replace the replicable mean estimator of $p_{\\pi}$ with a simple (non-replicable) sample mean. ", "page_idx": 26}, {"type": "text", "text": "Proposition E.9 (Theorem 3 and Claim 5.5 in Blanc et al. [2023]). Fix $\\alpha,\\beta\\in(0,1)$ . Let $\\mathcal{D}$ be $a$ distribution that is representable by a depth- $\\ell$ decision tree. Given oracle access to an estimator for influences, the algorithm BuildDT with the choice of $\\tau=\\alpha/8\\ell^{2}$ returns a depth- $\\ell$ tree representing $a$ distribution $\\mathcal{D}^{\\prime}$ such that $\\mathrm{TV}({\\mathcal{D}},{\\mathcal{D}}^{\\prime})\\leq\\alpha$ with probability at least $1-\\beta$ . If the oracle terminates in unit time, the running time of the algorithm is $d\\cdot(\\ell/\\alpha)^{O(\\ell)}$ . ", "page_idx": 26}, {"type": "text", "text": "Remark E.10. A few remarks are in order regarding Algorithm E.1. ", "page_idx": 26}, {"type": "text", "text": "1. Blanc et al. [2023] identify the task of learning of a decision tree for a distribution $\\mathcal{D}$ with learning its scaled pmf $f=2^{d}\\mathcal{D}$ . In order to remain consistent with their convention, the values at the tree leaves produced by our replicable decision tree algorithm (cf. Algorithm E.1) also correspond to the scaled pmf. Note that we do not directly use the values at the leaves and hence this convention is immaterial to our application.   \n2. Estimating the influences to an accuracy of $\\tau/{4}$ ensures that any variables in $S$ has influence at least $\\tau/2$ and every variable with influence at least $\\tau$ is captured in $S$ . Estimating the influences to an accuracy of $\\alpha/2d$ ensures that the estimated total influences are accurate up to error at most \u03b1. Note that the computation for $\\mathbb{E}_{t\\in T_{i}}[g(t)]$ (cf. Definition D.6) does not incur additional error since the weights of the sum are appropriately chosen.   \n3. $p_{\\pi}$ is the mean of a $[0,2^{|\\pi|}]$ -bounded random variable. By an Hoeffding bound and Theorem B.4, consuming po $\\mathrm{ly}(2^{\\ell},1/\\alpha,1/\\rho^{\\prime},\\mathrm{log}(1/\\beta^{\\prime}))$ samples from $\\mathcal{D}$ suffices to estimate this quantity up to $\\alpha/[2\\cdot2^{|\\pi|}]$ -accuracy, $\\rho^{\\prime}$ -replicability, and $\\beta^{\\prime}$ -confidence.   \n4. Taking into account the estimation error of influences, each variable in $S$ has influence at least $\\tau/2$ . For any depth- $\\ell$ decision tree, the sum of all variable influences is at most $2\\ell$ (cf. Fact $E.3$ ). For any restriction $\\pi$ , $(f_{\\sc D})_{\\pi}$ is a depth- $\\ell$ decision tree and thus has at most $^{4\\ell}\\!/\\tau$ variables of influence at least $\\tau/2$ . For the choice of $\\tau=\\alpha/{8\\ell^{2}}$ , there can be at most ${\\bigl(}4\\ell/\\tau{\\bigr)}^{\\ell}={\\bigl(}2\\ell/\\alpha{\\bigr)}^{O(\\ell)}$ recursive calls within Algorithm E.1. Each recursive call estimates $d$ influences and at most 1 mean of a bounded random variable. All in all, we need to replicably estimate $d\\cdot({2\\ell}/{\\alpha})^{O(\\ell)}$ variable influences and at most $(2\\ell/\\alpha)^{O(\\ell)}$ bounded means. ", "page_idx": 26}, {"type": "text", "text": "Proof (Theorem E.8). By assumption, for any $\\rho^{\\prime}\\,\\in\\,(0,1)$ , rInflEst is $\\rho^{\\prime}$ -replicable and returns $\\alpha$ -accurate estimates for influences with confidence $\\beta$ using $\\mathrm{poly}(d,1/\\alpha,\\mathrm{log}(1/\\beta),1/\\rho^{\\prime})$ samples and runtime. ", "page_idx": 26}, {"type": "text", "text": "The replicability of Algorithm E.1 follows directly from the replicability of the influence estimators and the replicability of the standard rounding scheme (cf. Theorem B.4). By Remark E.10, it suffices to call the replicable influence estimator and the replicable mean estimator for $p_{\\pi}$ with replicability parameter $\\rho^{\\prime}=\\rho/[2d(2\\ell/\\alpha)^{O(\\ell)}]$ for the union bound and similarly for the confidence parameter. The sample complexity follows from our remark above. ", "page_idx": 26}, {"type": "text", "text": "Let us now condition on the event that each call of rInflEst and estimation of expected total influence is successful. Conditioned on this event, the correctness of the algorithm rBuildDT directly follows from Proposition E.9. In particular, if we condition on the event that each call to rInflEst and each replicable estimate of $p_{\\pi}$ is successful, our algorithm reduces to the BuildDT algorithm of Blanc et al. [2023]. ", "page_idx": 26}, {"type": "text", "text": "E.3 Replicable Influence Estimator for Monotone Marginals ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide the main subroutine of our replicable DT distribution learning algorithm.   \nWe begin with a replicable influence estimator for monotone distributions. ", "page_idx": 26}, {"type": "text", "text": "Lemma E.11 (Replicable Influence Estimator for Monotone Marginals). Fix $i~\\in~[d]$ . For any $\\alpha,\\beta,\\rho\\in(0,1)^{3}$ , there is an efficient $\\rho$ -replicable algorithm rInflEst $(\\mathcal{D},i,\\alpha,\\beta,\\rho)$ such that given ", "page_idx": 26}, {"type": "text", "text": "an unknown monotone distribution $\\mathcal{D}$ over $\\{0,1\\}_{:}^{d}$ , computes an estimate of $\\mathrm{Infl}_{i}(f_{\\mathcal{D}})$ up to accuracy \u03b1 with probability at least $1-\\beta$ using $O(\\dot{\\alpha}^{-2}\\dot{\\rho}^{-2}\\log(1/\\beta))$ time and random samples from $\\mathcal{D}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof (Lemma E.11). By Proposition E.4, since $\\mathcal{D}$ is monotone, we know that $\\mathrm{Infl}_{i}(f_{\\mathcal{D}})=\\mathbb{E}_{\\mathcal{D}}[x_{i}]$ . Thus we can replicably estimate this expectation with $O(\\alpha^{-2}\\rho^{-2}\\log(1/\\beta))$ samples and samplepolynomial runtime through replicable query rounding (cf. Theorem B.4). \u53e3 ", "page_idx": 27}, {"type": "text", "text": "E.4 Replicable Influence Estimator for Arbitrary Distributions ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We further design a replicable influence estimator for arbitrary distributions $\\mathcal{D}$ over $\\{0,1\\}^{d}$ using subcube conditional sampling. ", "page_idx": 27}, {"type": "text", "text": "Definition E.12. A (subcube) conditional sampling oracle for distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ receives as input a subset (subcube) $S\\subseteq\\{0,1\\}^{d}$ and generates a sample from the conditional distribution ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\mathcal{D}}_{S}(x):={\\mathcal{D}}(x\\mid S)={\\frac{{\\mathcal{D}}(x)\\mathbb{1}\\{x\\in S\\}}{{\\mathcal{D}}(S)}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This conditional distribution is essentially the truncated distribution $\\mathcal{D}_{S}$ with truncation set $S$ . The subcube conditioning oracle is well studied by prior work in distribution testing and learning [Canonne et al., 2015, 2021, Fotakis et al., 2022, 2020, Gouleakis et al., 2017]. ", "page_idx": 27}, {"type": "text", "text": "Algorithm E.2 Replicable Influence Estimation for Arbitrary Distributions via Conditional Sampling 1: rInflEst:   \n2: Input: Distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ and $f_{\\mathcal{D}}$ as in Definition E.1, coordinate $i\\in[d]$ , access to conditional sampling oracle for $\\mathcal{D}$ , desired accuracy $\\alpha$ , desired confidence $\\beta$ , desired replicability $\\rho$ .   \n3: Output: A estimate $v$ of $\\mathrm{Infl}_{i}(f_{\\cal D})$ such that $|v-\\mathrm{Infl}_{i}(f_{\\mathcal{D}})|\\leq\\alpha$ with probability $1-\\beta$ .   \n4: $\\tilde{n}\\leftarrow O(\\alpha^{-2}\\rho^{-2}\\log(1/\\beta))$ 5: for $j\\leftarrow1,...,n$ do   \n6: Draw a fresh random sample $x^{(j)}\\sim\\mathcal{D}$   \n7: $S^{(j)}\\gets\\{x^{(j)}\\}\\cup\\{(x^{(j)})^{\\tilde{\\sim}_{i}}\\}$ , where $(x^{(j)})^{\\sim_{i}}$ is $\\boldsymbol{x}^{(j)}$ with its $i$ -th coordinate flipped   \n8: Draw $O(\\alpha^{-2})$ independent samples from the conditional on $S^{(j)}$ distribution $\\mathcal{D}(\\cdot|S^{(j)})$   \n9: Let $p^{(j)}\\in[0,1]$ be the fraction that we observe $\\boldsymbol{x}^{(j)}$   \n10: $q^{(j)}\\gets|p^{(\\bar{j})}-\\bar{(1-p^{(j)})}|$   \n11: end for   \n12: $\\begin{array}{r}{q\\gets\\frac{1}{n}\\sum_{j\\in[n]}q^{(j)}}\\end{array}$   \n13: return $v\\leftarrow\\dot{\\mathbf{r}}\\mathrm{Round}(q,O(\\alpha\\rho),\\rho)$ (see Theorem B.4 with $T=1$ ) ", "page_idx": 27}, {"type": "text", "text": "An adaptation of the proof of Proposition 6.5 in Blanc et al. [2023] combined with the replicability properties of Theorem B.4 gives the following result, which we provide for completeness but do not use in our applications. The algorithm is presented Algorithm E.2. ", "page_idx": 27}, {"type": "text", "text": "Lemma E.13 (Replicable Influence Estimator for Arbitrary Marginals). Let $\\mathcal{D}$ be a distribution over $\\{0,1\\}^{d}$ , $f_{\\mathcal{D}}$ as in Definition $E.l$ , and, assume sample access to a subcube conditional oracle as in Definition $E.I2$ . For any $\\alpha,\\beta,\\rho\\in(0,1)$ and $i\\in[d]$ , Algorithm $E.2$ is $\\rho$ -replicable and, given $O(\\alpha^{-4}\\rho^{-2}\\log(1/\\beta))$ subcube conditional samples from $\\mathcal{D}$ , it computes in sample-polynomial time an estimate v that satisfies $|v-\\mathrm{Infl}_{i}(f_{\\mathcal{D}})|\\leq\\bar{\\alpha}$ with probability at least $1-\\beta$ . ", "page_idx": 27}, {"type": "text", "text": "As a proof sketch, let us call the above algorithm ReplInfEst $(\\mathcal D,i,\\alpha)$ . The correctness of the algorithm follows from the observation that for any distribution $\\mathcal{D}$ , coordinate $i$ and $\\alpha\\in(0,1)$ , for any execution $j\\in[n]$ , it holds that $|\\mathbb{E}q^{(j)}-\\mathrm{Infl}_{i}(f_{\\mathcal{D}})|\\,\\le\\,\\alpha$ . Hence to obtain a high probability estimator, it suffices to obtain ${\\cal O}(\\alpha^{-2}\\log({^1/\\beta}))$ copies of $q^{(j)}$ and take the average $q$ . Finally, to make the algorithm replicable, it suffices to estimate the average $q$ replicably. This can be accomplished at an extra cost of order $1/\\rho^{2}$ using the standard rounding routines (cf. Theorem B.4). Since each iteration requires $^1/\\alpha^{2}$ subcube samples, the sample complexity of our algorithm follows. ", "page_idx": 27}, {"type": "text", "text": "E.5 Useful Subroutines & Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide a set of useful results that we will use in our proofs. ", "page_idx": 27}, {"type": "text", "text": "Proposition E.14 (Corollary D.21 in Esfandiari et al. [2023b]). Let $\\alpha,\\rho\\in(0,1)$ and $\\beta\\in(0,\\rho/3)$ . There is a $\\rho$ -replicable algorithm rFiniteDistrEst $(\\mathcal{D},\\alpha,\\beta,\\rho)$ that outputs parameter estimates $\\bar{p}$ for a finite distribution $\\mathcal{D}$ of support size $N$ such that ", "page_idx": 27}, {"type": "text", "text": "(a) $|\\bar{p}^{(i)}-p^{(i)}|\\leq\\alpha$ for every $i\\in[N]$ with probability at least $1-\\beta$ .   \n$(b)$ $\\bar{p}^{(i)}\\geq0$ for all $i\\in[N]$ .   \n(c) $\\textstyle\\sum_{i}{\\bar{p}}^{(i)}=1$ . ", "page_idx": 28}, {"type": "text", "text": "Moreover, the algorithm has sample complexity ", "page_idx": 28}, {"type": "equation", "text": "$$\nm={\\cal O}\\left(\\frac{\\ln{1/\\beta}+N}{\\alpha^{2}(\\rho-\\beta)^{2}}\\right)={\\cal O}\\left(\\frac{N}{\\alpha^{2}\\rho^{2}}\\log\\frac{1}{\\beta}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and $\\mathrm{poly}(m)$ time complexity. ", "page_idx": 28}, {"type": "text", "text": "Proposition E.15 ([Janson, 2018]). Let $Y_{i}$ be a geometric variable with success rate $q$ . Then $\\textstyle Y:=\\sum_{i=1}^{m}Y_{i}$ is the number of draws until we obtain $m$ successes. Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg\\{Y\\geq\\lambda\\frac{m}{q}\\bigg\\}\\leq\\exp(1-\\lambda).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In other words, it suffices to perform ${\\cal O}({}^{m}/q\\log({}^{1}\\!/\\beta))$ Poisson trials before succeeding $m$ times with probability at least $1-\\beta$ . ", "page_idx": 28}, {"type": "text", "text": "Proposition E.16 (Replicable Boosting of Success Probability). Let $\\alpha,\\rho\\in(0,1).$ , $\\beta\\in(0,\\rho/3)$ , and $\\Delta\\geq0$ . Suppose $\\boldsymbol{\\mathcal{A}}$ is a $\\rho^{\\prime}$ -replicable $(\\alpha+\\Delta,{^1/2})-,$ PAC learner for the concept class $\\mathcal{C}$ under $a$ fixed distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ that uses $m(\\alpha,\\rho^{\\prime})=\\mathrm{poly}(d,1/\\alpha,1/\\rho^{\\prime})$ samples for any $\\rho^{\\prime}\\in(0,1)$ . There is a $\\rho$ -replicable $(2\\alpha+\\Delta,\\beta)$ -PAC learner rBoost $_{A}(\\mathcal{D},\\alpha,\\beta,\\rho)$ for $\\mathcal{C}$ under $\\mathcal{D}$ for any $\\alpha,\\beta,\\rho\\in(0,1)$ that has sample complexity ", "page_idx": 28}, {"type": "equation", "text": "$$\nm\\left(d,\\frac{\\rho}{2\\log(1/\\beta)}\\right)\\cdot O\\left(\\log\\frac{1}{\\beta}\\right)+O\\left(\\frac{\\log^{2}(1/\\beta)}{\\alpha^{2}\\rho^{2}}\\log\\frac{\\log(1/\\beta)}{\\beta}\\right)=\\mathrm{poly}\\left(d,1/\\alpha,1/\\rho,\\log(1/\\beta)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and sample-polynomial running time. ", "page_idx": 28}, {"type": "text", "text": "Proof. Let rBoost $\\mathcal{A}$ be the algorithm that runs $\\boldsymbol{\\mathcal{A}}$ for $n\\,=\\,O(\\log^{1/\\beta})$ times (each with $m=$ $m(1/\\alpha,\\rho/2n)$ samples), replicably estimates the population error of each hypothesis, and outputs the hypothesis which has the lowest estimated error. ", "page_idx": 28}, {"type": "text", "text": "Running $\\boldsymbol{\\mathcal{A}}$ for $n=O(\\log^{1/\\beta})$ times with $m$ samples per run guarantees that each execution is $\\rho/2n$ -replicable so the first step is ${\\rho}/{2}$ -replicable. Moreover, at least one of the hypotheses we output is $(\\alpha+\\Delta)$ -close to the optimal hypothesis with probability at least $1-\\beta/2$ . ", "page_idx": 28}, {"type": "text", "text": "By an Hoeffding bound on the empirical error of a hypothesis, we can estimate the population error of a hypothesis to an accuracy of $\\alpha\\rho/4{\\cdot}2n$ and confidence $\\beta/2n$ using $O(n^{2}\\alpha^{-2}\\rho^{-2}\\log\\(\\grave{n}/\\beta))$ samples. By Theorem B.4, we can make this estimate $\\rho/2n$ -replicable at the cost of reducing the accuracy to $\\alpha$ while maintaining the confidence parameter $\\beta/2n$ . ", "page_idx": 28}, {"type": "text", "text": "It follows that the output hypothesis is $\\rho$ -replicable and with probability at least $1-\\beta$ , its population error is at most $2\\alpha+\\Delta$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "E.6 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we show how to lift replicable algorithms that learn over the uniform distribution to replicable algorithms that learn with respect to arbitrary distributions, where the sample complexity and running time depend on the decision tree complexity of the target distribution. Let $\\mathcal{D}(\\cdot\\mid T)$ denote the conditional distribution on leaves, i.e., $\\textstyle{\\mathcal{D}}(t\\mid T):=\\sum_{x\\in t}{\\bar{\\mathcal{D}}}(x)$ . ", "page_idx": 28}, {"type": "text", "text": "Now, our learning algorithm is designed for an exact tree distrib ution. However, we can only estimate the tree representation of the actual distribution, i.e., the conditional distribution at a leaf subcube is uniform in the tree representation but not necessarily uniform in the input distribution. To show that our learning algorithm can accommodate slight errors in the tree distribution estimation, we introduce the following definition. ", "page_idx": 28}, {"type": "text", "text": "Definition E.17 (Robust Learning). For any concept class $\\mathcal{C}$ and algorithm $\\boldsymbol{\\mathcal{A}}$ , we say that $\\boldsymbol{\\mathcal{A}}$ $(\\alpha,\\beta,c)$ -robustly learns $\\mathcal{C}$ using m samples under the uniform distribution if for any $\\eta>0$ and class ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\eta}=\\{d i s t r i b u t i o n\\;\\mathcal{D}\\;o\\nu e r\\;\\{0,1\\}^{d}\\;:\\;\\mathrm{TV}(\\mathcal{U},\\mathcal{D})\\leq\\eta\\}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "it holds that $A(\\alpha+c\\eta,\\beta)$ -learns $\\mathcal{C}$ using m samples with respect to the distributions in $\\mathcal{D}_{\\eta}$ . ", "page_idx": 28}, {"type": "text", "text": "Proposition E.18 (Proposition 7.2 from [Blanc et al., 2022b]). For any concept class $\\mathcal{C}$ and algorithm $\\mathcal{A}_{:}$ , if $\\boldsymbol{\\mathcal{A}}$ $(\\alpha,\\beta)$ -PAC learns $\\mathcal{C}$ using m samples under the uniform distribution, then the exact same algorithm $\\boldsymbol{\\mathcal{A}}$ also $(\\alpha,\\beta+{1/3},3m)$ -robustly learns $\\mathcal{C}$ using m samples. ", "page_idx": 28}, {"type": "text", "text": "1: rLift   \n2: Input: decision tree $T$ (from rBuildDT), algorithm $\\boldsymbol{\\mathcal{A}}$ for $\\rho^{\\prime}$ -replicable $(\\alpha^{\\prime},\\beta^{\\prime})$ -learning under the uniform distribution using $m(\\alpha^{\\prime},\\rho^{\\prime},\\beta^{\\prime})$ samples, distribution $\\mathcal{D}$ , accuracy $\\alpha$ , confidence $\\beta$ , replicability $\\rho$ .   \n3: Output: A hypothesis $h:\\{0,1\\}^{d}\\rightarrow\\{0,1\\}$   \n4:   \n5: $M_{1}\\gets\\mathrm{poly}(2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta))$   \n6: $M_{2}\\gets m(\\alpha/6,\\rho/2^{\\ell},1/6)\\cdot\\mathrm{poly}(d,2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta))$   \n7: // Estimate leaf distribution (cf. Proposition E.14)   \n8: $\\hat{\\mathcal{D}}(\\cdot\\mid T)\\gets\\mathtt{r F i n i t e D i s t r E s t}(\\mathcal{D}(\\cdot\\mid T),\\alpha/12\\cdot2^{\\ell},\\beta/2,\\rho/3)$ using $M_{1}$ samples   \n9: Draw a dataset $S\\sim\\mathcal{D}^{M_{2}}$ of size $M_{2}$ .   \n10: for each leaf $t\\in T$ do   \n11: if ${\\hat{D}}(t\\mid T)\\geq\\alpha/4{\\cdot}2^{\\ell}$ then   \n12: Let $S_{t}$ be the subset of samples in $S$ that reach $t$ .   \n13: Create a set $S_{t}^{\\prime}$ consisting of points in $S_{t}$ but where all coordinates queried on the root-to-leaf path for $t$ are re-randomized independently, making the marginal distribution uniform.   \n14: For $\\rho^{\\prime}\\,\\in\\,(0,1)$ , choose its parameters so that $\\bar{\\mathcal{A}}=\\,\\mathcal{A}(\\alpha/\\mathrm{6},\\rho^{\\prime},1/\\mathrm{6})$ is $\\rho^{\\prime}$ -replicable and $(\\alpha/6,1/6+1/3,c^{\\prime})$ -robustly learns $\\mathcal{C}$ .   \n15: Get $h_{t}$ by calling rBoost $_A(S_{t}^{\\prime},\\alpha/6,\\rho/3{\\cdot}2^{\\ell},\\beta/2{\\cdot}2^{\\ell})$ (cf. Proposition E.16).   \n16: else   \n17: Get $h_{t}$ that outputs a random guess according to shared randomness.   \n18: end if   \n19: end for   \n20: Return the hypothesis $h$ such that given input $x$ , finds the leaf $t\\in T$ that $x$ follows and outputs $h_{t}(x)$ . ", "page_idx": 29}, {"type": "text", "text": "We are now ready to state the main result of this section, from which the proof of Theorem 3.2 closely follows. ", "page_idx": 29}, {"type": "text", "text": "Theorem E.19. Consider a concept class $\\mathcal{C}$ of functions $f\\;:\\;\\{0,1\\}^{d}\\;\\rightarrow\\;\\{0,1\\}$ closed under restrictions. Fix $\\alpha,\\beta,c,\\rho\\,>\\,0$ and $m,\\ell\\,\\in\\,\\mathbb{N}$ . Suppose we are provided black-box access to an algorithm $\\boldsymbol{\\mathcal{A}}$ that ", "page_idx": 29}, {"type": "text", "text": "$(i)$ is $\\rho^{\\prime}$ -replicable with respect to the uniform distribution for any $\\rho^{\\prime}\\in(0,1).$ ,   \n(ii) $(\\alpha^{\\prime},\\beta^{\\prime}+{}^{1}\\!/\\!3,c)$ -robustly learns $\\mathcal{C}$ for any $\\alpha^{\\prime},\\beta^{\\prime}\\in(0,\\dot{1})$ , and   \n$(i i i)$ consumes $\\begin{array}{r}{m(\\alpha^{\\prime},\\rho^{\\prime},\\beta^{\\prime})=\\mathrm{poly}(d,1/\\alpha^{\\prime},1/\\rho^{\\prime},\\log(1/\\beta^{\\prime}))}\\end{array}$ samples and computation time under the uniform distribution. ", "page_idx": 29}, {"type": "equation", "text": "$$\nM_{1}=\\mathrm{poly}(2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta)),\\;M_{2}=m(\\alpha/6,\\rho/2^{\\ell},1/6)\\cdot\\mathrm{poly}(d,2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta)),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\nM:=M_{1}+M_{2}\\leq\\mathrm{poly}(d,2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta)).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For any function $f^{\\star}\\in{\\mathcal{C}}_{\\mathrm{i}}$ , distribution $\\mathcal{D}$ over $\\{0,1\\}^{d}$ , depth- $\\ell$ decision tree $T$ computing the pmf of a distribution $\\mathcal{D}_{T}$ where $\\mathrm{TV}(\\ensuremath{\\mathcal{D}},\\ensuremath{\\mathcal{D}}_{T})\\le\\alpha/6c$ , the following holds. ", "page_idx": 29}, {"type": "text", "text": "The algorithm rLif $z(T,\\mathcal{A},\\mathcal{D},\\alpha,\\beta,\\rho)$ (cf. Algorithm $E.3$ ) is $\\rho$ -replicable, has $O(M)$ sample complexity, terminates in $\\mathrm{poly}(M)$ time, and its output is $\\alpha$ -close to $f^{\\star}$ with respect to $\\mathcal{D}$ with probability at least $1-\\beta$ . ", "page_idx": 29}, {"type": "text", "text": "Before we prove Theorem E.19, we state two useful lemmas. ", "page_idx": 29}, {"type": "text", "text": "Lemma E.20. Fix $\\alpha,\\rho,\\rho^{\\prime},\\beta\\quad\\in\\quad(0,1),$ , $\\begin{array}{r l r}{\\Delta}&{{}\\geq}&{0,}\\end{array}$ , and $\\ensuremath{\\ell}\\in\\mathrm{~{~\\mathbb~N~}~}$ . Let $\\begin{array}{r l}{m_{\\tt r B o o s t}}&{{}=}\\end{array}$ $\\mathrm{poly}(d,2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta))$ be the number of samples that rBoost (cf. Proposition $E.I6,$ ) requires to boost a $\\rho^{\\prime}$ -replicable $\\left(\\alpha/6+\\Delta,{^1/2}\\right)$ -correct learner (that uses po $\\mathrm{ly}(d,1/\\alpha,1/\\rho^{\\prime})$ samples and running time) to a $\\rho/3{\\cdot}2^{\\ell}$ -replicable learner with accuracy $\\alpha/6+\\Delta+\\alpha/_{6}=\\alpha/_{3}+\\Delta$ and confidence $\\beta/2{\\cdot}2^{\\ell}$ . Condition on the success of rFiniteDistrEst in Algorithm E.3. Then with probability at least $1-{\\beta}/{2}$ , for every $t\\in T$ with ${\\hat{D}}(t\\mid T)\\geq\\alpha/4{\\cdot}2^{\\ell}$ , we observe at least $m_{\\tt T B o o s t}$ samples reaching $t$ from $S$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Condition on the success of the call to rFiniteDistrEst. Then for every $t\\,\\in\\,T$ with ${\\hat{D}}(t\\mid T)\\geq\\alpha/4{\\cdot}2^{\\ell}$ , we have $D(t\\mid T)\\ge\\alpha/6{\\cdot}2^{\\ell}$ . The probability of not observing such a $t$ in a dataset of size $n$ is at most $(1-\\alpha/6{\\cdot}2^{\\ell})^{n}\\leq\\exp(-{\\pi}\\alpha/6{\\cdot}2^{\\ell})$ . By a union bound over the at most ${^{6\\cdot2^{\\ell}}\\!/\\alpha}$ such $t$ , the probability of failing to collect each such $t$ is at most ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{6\\cdot2^{\\ell}}{\\alpha}}\\exp(-{n}\\alpha/6{\\cdot}2^{\\ell}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To drive this below the constant $1/2$ , it suffices to take $n=O(\\ensuremath{\\ell}{\\cdot}2^{\\ell}/\\alpha\\log{1/\\alpha})$ . By Proposition E.15, it suffices to take a sample of size ", "page_idx": 30}, {"type": "equation", "text": "$$\nO(n m_{\\tt r B o o s t}\\log1/\\beta)=O\\left({\\frac{m_{\\tt r B o o s t}\\ell\\cdot2^{\\ell}}{\\alpha}}\\left(\\log{\\frac{1}{\\alpha}}\\right)\\left(\\log{\\frac{1}{\\beta}}\\right)\\right)\\leq M_{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "to collect $m$ copies of each such $t$ with probability at least $1-{\\mathit{\\beta}}^{\\beta}/2$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma E.21 (Lemma B.4 from [Blanc et al., 2022a], Fact 7.4 from [Blanc et al., 2023]). For any distribution $\\mathcal{D}$ and decision tree $T$ computing the pmf of another distribution $\\mathcal{D}_{T}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t\\in T}\\operatorname*{Pr}_{x\\sim\\mathcal{D}}\\left[x\\;r e a c h e s\\;t\\right]\\cdot\\mathrm{TV}(\\mathcal{D}_{t},(\\mathcal{D}_{T})_{t})\\leq2\\mathrm{TV}(\\mathcal{D},\\mathcal{D}_{T})\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $(\\mathcal{D}_{T})_{t}$ (resp. $\\mathcal{D}_{t}$ ) is the conditional of $\\mathcal{D}_{T}$ (resp. $\\mathcal{D}$ ) on the subcube induced by the leaf $t$ . ", "page_idx": 30}, {"type": "text", "text": "Note that in the statement of the lemma above, $\\mathrm{Pr}_{x\\sim\\mathcal{D}}$ [x reaches $t]\\;=\\;{\\mathcal{D}}(t\\;\\mid\\;T)$ and $(\\mathcal{D}_{T})_{t}$ is the uniform distribution on the subcube represented by the leaf $t$ . We are finally ready to prove Theorem E.19. ", "page_idx": 30}, {"type": "text", "text": "Proof (Theorem E.19). We first note that the sample complexity of the call to the routine rFiniteDistrEst is $M_{1}=\\mathrm{poly}(2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta))$ (cf. Proposition E.14). Throughout this proof, we condition on the $1-{\\beta}/{2}$ probability of rFiniteDistrEst succeeding. ", "page_idx": 30}, {"type": "text", "text": "Let $h$ be the output of rLift $(T,A,S)$ . We now argue separately about the accuracy and replicability of the algorithm. ", "page_idx": 30}, {"type": "text", "text": "Accuracy. The accuracy of the learner $h=\\tt r L i f t(T,\\mathcal{A},S)$ is analyzed as follows. For any leaf $t$ of the tree $T$ , let $h_{t}$ be the associated predictor. We have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim D}[h(x)\\neq f^{\\star}(x)]=\\sum_{t\\in T}\\operatorname*{Pr}_{x\\sim\\mathcal{D}}[x{\\mathrm{~reaches~}}t]\\operatorname*{Pr}_{x\\sim\\mathcal{D}_{t}}[h_{t}(x)\\neq f^{\\star}(x)]\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Each hypothesis $h_{t}$ is obtained either by running $\\mathtt{r B o o s t.}A$ (cf. Proposition E.16) on the sample $S_{t}^{\\prime}$ given that ${\\hat{D}}(t\\mid T)\\geq\\alpha/4{\\cdot}2^{\\ell}$ or corresponds to a random guess otherwise. ", "page_idx": 30}, {"type": "text", "text": "On the other hand, from the choice of estimation error to the call for rFiniteDistrEst, each leaf $t$ on which we output the random guess hypothesis must satisfy $\\mathrm{Pr}_{x\\sim\\mathcal{D}}[x\\,\\mathrm{reaches}\\,\\,t]\\,<\\alpha/3.2^{\\ell}$ . The overall error contribution of such leaves is thus at most $\\alpha/3$ . It follows that total error is at most ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim\\mathcal{D}}[h(x)\\neq f^{\\star}(x)]\\leq\\sum_{\\substack{t\\in T:\\hat{\\mathcal{D}}(t|T)\\geq\\alpha/4\\cdot2^{t}}}\\operatorname*{Pr}_{x\\sim\\mathcal{D}}[x\\mathrm{~reaches~}t]\\operatorname*{Pr}_{x\\sim\\mathcal{D}_{t}}\\left[h_{t}(x)\\neq f^{\\star}(x)\\right]+\\frac{\\alpha}{3}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Fix a $t\\in T$ on which we run rBoost $\\mathcal{A}$ . Recall that $\\mathcal{D}_{t}$ and $(\\mathcal{D}_{T})_{t}$ denotes the conditional distribution on the leaf subcube over the coordinates not fixed by $\\pi=\\pi(t)$ where $\\pi$ is the restriction corresponding to $t$ . Here the underlying distributions are the input distribution $\\mathcal{D}$ and tree distribution $\\mathcal{D}_{T}$ . Let $\\mathcal{D}_{t}^{\\prime},(\\mathcal{D}_{T})_{t}^{\\prime}$ denote the distributions over $\\{0,1\\}^{d}$ obtained from $\\mathcal{D}_{t},(\\mathcal{D}_{T})_{t}$ by re-randomizing the coordinates from $\\pi$ . Then $(\\mathcal{D}_{T})_{t}^{\\prime}$ is precisely the uniform distribution $\\boldsymbol{\\mathcal{U}}$ over $\\{0,1\\}^{d}$ and $\\mathcal{D}_{t}^{\\prime}$ satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{\\bf~IV}\\left(D_{t},\\mathcal{A}\\right)}}\\\\ {{\\mathrm{\\bf~\\Lambda~}=\\mathrm{TV}\\big(\\mathcal{D}_{t}^{\\prime},(\\mathcal{D}_{T})_{t}^{\\prime}\\big)}}\\\\ {{\\mathrm{\\bf~\\Lambda~}:=\\displaystyle\\frac{1}{2}\\sum_{x\\in\\{0,1\\}^{d}}|\\mathcal{D}_{t}^{\\prime}(x)-(\\mathcal{D}_{T})_{t}^{\\prime}(x)|}}\\\\ {{\\mathrm{\\bf~\\Lambda~}=\\displaystyle\\frac{1}{2}\\sum_{x\\in t}2^{|\\pi(t)|}|\\mathcal{D}_{t}^{\\prime}(x)-(\\mathcal{D}_{T})_{t}^{\\prime}(x)|}}\\\\ {{\\mathrm{\\bf~\\Lambda~}=\\displaystyle\\frac{1}{2}\\sum_{x\\in t}\\left|\\mathcal{D}_{t}(x)-(\\mathcal{D}_{T})_{t}(x)\\right|\\qquad}}&{{\\mathcal{D}_{t}^{\\prime}(x)=2^{-|\\pi(t)|}\\mathcal{D}_{t}(x),(\\mathcal{D}_{T})_{t}^{\\prime}(x)=2^{-|\\pi(t)|}(\\mathcal{D}_{T})_{t}(x)}}\\\\ {{\\mathrm{\\bf~\\Lambda~}=\\mathrm{TV}(D_{t},(\\mathcal{D}_{T})_{t}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Based on the re-sampling step of the algorithm, we know that any point in $S_{t}^{\\prime}$ is an i.i.d. sample from $\\mathcal{D}_{t}^{\\prime}$ and labeled by the function $f_{t}^{\\star}$ , which lies in $\\mathcal{C}$ thanks to closedness under restrictions. ", "page_idx": 31}, {"type": "text", "text": "By $(\\alpha/6,1/6+1/3,c)$ -robust learnability, $\\boldsymbol{\\mathcal{A}}$ in fact $(\\alpha/6+c\\mathrm{TV}(\\ensuremath{\\mathcal{D}_{t}^{\\prime}},\\mathcal{U}),1/2)$ -PAC learns $\\mathcal{C}$ under the distribution $\\mathcal{D}_{t}^{\\prime}$ . We now apply Lemma E.20 with $\\Delta=c\\mathrm{TV}(D_{t}^{\\prime},\\mathcal{U})$ : the leaves on which we run $\\mathtt{r B o o s t}_{\\mathcal{A}}$ have at least $m_{\\tt T B o o s t}$ samples where $m_{\\tt r B o o s t}\\,=\\,\\mathrm{poly}(d,2^{\\ell},1/\\alpha,1/\\rho,\\log(1/\\beta))$ is the number of samples that rBoost requires to boost a $\\rho^{\\prime}$ -replicable $(\\alpha/6+c\\mathrm{TV}(\\ensuremath{\\mathcal{D}_{t}^{\\prime}},\\mathcal{U}),1/2)$ -correct learner (that uses $\\mathrm{)oly}(d,1/\\alpha,1/\\rho^{\\prime})$ samples and running time) to a $\\rho/3{\\cdot}2^{\\ell}$ -replicable learner with accuracy $\\alpha/6+c\\mathrm{TV}(\\mathcal{D}_{t}^{\\prime},\\mathcal{U})+\\alpha/6=\\alpha/3+c\\mathrm{TV}(\\mathcal{D}_{t}^{\\prime},\\mathcal{U})$ and confidence $\\beta/2{\\cdot}2^{\\ell}$ . ", "page_idx": 31}, {"type": "text", "text": "Then by Proposition E.16, with probability at least $1-\\beta/2{\\cdot}2^{\\ell}$ over the data $S_{t}^{\\prime}$ , the hypothesis $h_{t}$ output by rBoost $\\mathcal{A}$ satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim\\mathcal{D}_{t}}\\left[h_{t}(x)\\neq f^{\\star}(x)\\right]\\leq\\alpha/3+c\\,\\mathrm{TV}(\\mathcal{D}_{t}^{\\prime}\\mathcal{U})=\\alpha/3+c\\,\\mathrm{TV}(\\mathcal{D}_{t},(\\mathcal{D}_{T})_{t})\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "All in all, Combined with the expression of total error above, this means that with probability at least $1-\\beta$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim\\mathcal{D}}[h(x)\\neq f^{\\star}(x)]\\leq2\\alpha/3+c\\sum_{t\\in T}\\operatorname*{Pr}_{x\\sim\\mathcal{D}}[x{\\mathrm{~reaches~}}t]\\;\\mathrm{TV}(\\mathcal{D}_{t},(\\mathcal{D}_{T})_{t})\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We finish by applying Lemma E.21, which ensures that the sum over leaves above is upper bounded by $2\\mathrm{TV}(\\bar{D_{,}}\\bar{D_{T}})\\overset{^{\\cdot}}{\\leq}\\dot{\\alpha_{\\big/3c}}$ and so the total misclassification error is at most $\\alpha$ with probability at least $1-\\beta$ . ", "page_idx": 31}, {"type": "text", "text": "Replicability. We assume we are given the same decision tree in both executions. Next, we note that rFiniteDistrEst is ${{\\rho}}/3$ -replicable and succeeds in two executions with probability at least $1-2\\cdot\\beta/2\\geq1-\\rho/3$ . Conditional on the events above, it suffices to show that for any leaf of the tree, the algorithm $\\mathtt{r B o o s t}_{\\mathcal{A}}$ replicably outputs a hypothesis. Since there are at most $2^{\\ell}$ leaves and each call of $\\mathtt{r B o o s t}_{\\mathcal{A}}$ has replicability parameter $\\rho/3{\\cdot}2^{\\ell}$ , the entire procedure is $\\rho$ -replicable. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Before moving on to the proof of Theorem 3.2, we state one final lemma. ", "page_idx": 31}, {"type": "text", "text": "Lemma E.22. For any concept class $\\mathcal{C}$ and algorithm $\\mathbf{\\mathcal{A}}$ , if A $\\rho$ -replicably $(\\alpha,\\beta)$ -learns $\\mathcal{C}$ using $m$ samples under the uniform distribution, then the exact same algorithm $\\boldsymbol{\\mathcal{A}}$ also $\\rho$ -replicably $(\\alpha,\\beta+{1/3},3m)$ -robustly learns $\\mathcal{C}$ using m samples under the uniform distribution. ", "page_idx": 31}, {"type": "text", "text": "Proof (Lemma E.22). The proof follows directly from the analysis of Proposition 7.2 in Blanc et al. [2023] (cf. Proposition E.18). The replicability of the algorithm is trivially preserved since the algorithm does not change. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "We restate the statement of Theorem 3.2 below for convenience before its formal proof. ", "page_idx": 31}, {"type": "text", "text": "Theorem 3.2 (Lifting Replicable Uniform Learners). Consider a concept class $\\mathcal{C}$ of functions   \n$f\\,:\\,\\{0,1\\}^{d}\\,\\to\\,\\{0,1\\}$ closed under restrictions. Suppose we are given black-box access to an   \nalgorithm such that for any $\\alpha^{\\prime},\\rho^{\\prime},\\beta^{\\prime}\\in(0,1),$ , given $\\operatorname{poly}(d,1/\\alpha^{\\prime},1/\\rho^{\\prime},\\log(1/\\beta^{\\prime}))$ samples from $\\boldsymbol{\\mathcal{U}}$ , (i) is $\\rho^{\\prime}$ -replicable with respect to the uniform distribution $\\boldsymbol{\\mathcal{U}}$ , (ii) PAC learns $\\mathcal{C}$ under the uniform distribution to accuracy $\\alpha^{\\prime}$ and confidence $\\beta^{\\prime}$ , and,   \n$(i i i)$ terminates in time p $\\mathrm{oly}(d,1/\\alpha^{\\prime},1/\\rho^{\\prime},\\log(1/\\beta^{\\prime}))$ .   \nLet $\\alpha,\\rho\\quad\\in\\ \\ (0,1)$ and $\\beta\\quad\\in\\quad(0,\\rho/3)$ . For $\\begin{array}{r c l}{m}&{=}&{\\mathrm{poly}(d,1/\\alpha,1/\\rho,\\log(1/\\beta))}\\end{array}$ and $\\textit{M}=$   \n$\\mathrm{poly}\\big(d,1/\\alpha,1/\\rho,\\log\\(1/\\beta)\\big)^{O(\\ell)}$ , the following cases hold:   \n(a) If $\\mathcal{D}$ is a monotone distribution over $\\{0,1\\}^{d}$ representable by a depth- $\\ell$ decision tree, there is an algorithm that draws $M$ samples from $\\mathcal{D}$ , is $\\rho$ -replicable with respect to $\\mathcal{D}$ , PAC learns $\\mathcal{C}$ under $\\mathcal{D}$ with accuracy $\\alpha$ and confidence $\\beta$ , and terminates in poly $(M)$ time.   \n(b) If $\\mathcal{D}$ is an arbitrary distribution over $\\{0,1\\}^{d}$ representable by a depth- $\\ell$ decision tree, there is an algorithm that draws $M$ labeled examples as well as $M$ conditional samples (cf. Definition $D.5$ ) from $\\mathcal{D}$ , is $\\rho$ -replicable with respect to $\\mathcal{D}$ , PAC learns $\\mathcal{C}$ with respect to $\\mathcal{D}$ with accuracy $\\alpha$ and confidence $\\beta$ , and terminates in $\\mathrm{poly}(M)$ time. ", "page_idx": 31}, {"type": "text", "text": "Proof (Theorem 3.2). Using Theorem E.19, we can prove Theorem 3.2 as follows. First, we use Theorem E.7 to replicably learn the input distribution to total variation distance $\\alpha/6{\\cdot}3m$ with a decision tree. Next, Lemma E.22 asserts that $\\boldsymbol{\\mathcal{A}}$ is $\\rho$ -replicable and $(\\alpha,\\beta+{1/3},3m)$ -robustly learns $\\mathcal{C}$ under the uniform distribution. The result follows by applying Theorem E.19. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "F Efficient Replicability and Private Learning ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "F.1 Replicably Learning Finite Classes ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We first state a useful result which we later leverage as a subroutine appears in Bun et al. [2023]. Theorem F.1 (Replicable Learner for Finite Classes; Theorem 5.13 in [Bun et al., 2023]). Let $\\alpha,\\rho,\\beta\\in(0,1)^{3}$ . Then, any finite concept class $\\mathcal{C}$ is replicably learnable in the agnostic setting with ", "page_idx": 32}, {"type": "equation", "text": "$$\nm(\\alpha,\\rho,\\beta)=O\\left(\\frac{\\log^{2}|\\mathcal{C}|+\\log\\frac{1}{\\rho\\beta}}{\\alpha^{2}\\rho^{2}}\\log^{3}\\frac{1}{\\rho}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "many samples. Moreover, the running time of the algorithm is polynomial in the number of samples and the cardinality of $\\mathcal{C}$ . ", "page_idx": 32}, {"type": "text", "text": "F.2 A Transformation from Pure DP to Replicability ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Our approach borrows some high level ideas from Gonen et al. [2019] who showed a transformation from a pure DP PAC learner to an online learner. In a nutshell, our algorithm works as follows: ", "page_idx": 32}, {"type": "text", "text": "1) First, we create a \u201cdummy\u201d input dataset $\\bar{S}$ which has some constant size13 and we run the algorithm for $\\widetilde{\\Theta}\\left(\\frac{\\log^{3}(1/\\beta)}{\\rho^{2}}\\right)$ times on $\\bar{S}$ , resampling its internal randomness every time.   \n2) With probability $1-\\beta$ , one of the hypotheses will have error at most $3/8$ [Beimel et al., 2013, Gonen et al., 2019].   \n3) We run the replicable agnostic learner for finite classes from Bun et al. [2023].   \n4) We boost the weak replicable learner using Impagliazzo et al. [2022], Kalavasis et al. [2023]. ", "page_idx": 32}, {"type": "text", "text": "We begin by presenting an adaptation of a result that appeared in Beimel et al. [2013], Gonen et al. [2019]. Essentially, it states that when a class is learnable by a pure DP learner, one can fix an arbitrary input $\\bar{S}$ that has constant size, execute the learner on this dataset a constant number of times by resampling its internal randomness, and then with some constant probability, e.g., $^{15}\\!/\\!16$ , the output will contain one hypothesis whose error is bounded away from $1/2$ by some constant, e.g., $^{1}\\!/\\!4$ . This result might seem counter-intuitive at first glance, but learnability by a pure DP learner is a very strong property for a class and the result crucially depends on this property. Below, we present an adaptation of this result which states that the weak learner can be $\\rho$ -replicable and has a probability of success of $1-\\delta$ . An important element of our derivation is a result from Bun et al. [2023] about the sample and computational complexity of agnostically learning a concept class using a replicable algorithm (cf. Theorem F.1). ", "page_idx": 32}, {"type": "text", "text": "We are now ready to state a key lemma for our transformation. ", "page_idx": 32}, {"type": "text", "text": "Lemma F.2 (From Pure DP Learner to Replicable Weak Learner). Let $\\mathcal{X}$ be some input domain, ${\\mathcal{D}}=\\{0,1\\}$ , and $\\mathcal{D}_{X Y}$ be a distribution on $\\mathcal X\\times\\mathcal X$ that is realizable with respect to some concept class $\\mathcal{C}$ . Let $\\boldsymbol{\\mathcal{A}}$ be a pure $D P$ learner such that for any $\\alpha,\\varepsilon,\\beta\\in(0,1)^{3}$ , $\\boldsymbol{\\mathcal{A}}$ needs $m(\\alpha,\\varepsilon,\\beta,\\mathcal{C})$ i.i.d. samples from $\\mathcal{D}_{X Y}$ to output with probability at least $1-\\beta$ a hypothesis with error at most $\\alpha$ in an $\\varepsilon{-}D P$ way. Let us set $m_{0}=m(1/4,1/10,1/2,\\mathcal{C})$ .   \nThen, for any $\\rho,\\beta^{\\prime}\\in(0,1)^{2}$ there is a $\\rho$ -replicable learner $\\mathcal{A^{\\prime}}$ that outputs a hypothesis with error at most $3/8$ with probability at least $1-\\beta^{\\prime}$ and requires $\\begin{array}{r}{\\widetilde{O}\\left(\\mathrm{poly}(m_{0})\\frac{\\log^{3}\\left(1/\\beta^{\\prime}\\right)}{\\rho^{2}}\\right)}\\end{array}$ i.i.d. samples from $\\mathcal{D}_{X Y}$ and $O(\\exp(m_{0})\\log(1/\\beta^{\\prime}))$ oracle calls to $\\boldsymbol{\\mathcal{A}}$ (and hence runtime). ", "page_idx": 32}, {"type": "text", "text": "Proof (Lemma $F.2$ ). We argue about the correctness and replicability of the algorithm separately. ", "page_idx": 32}, {"type": "text", "text": "Correctness. Since $\\mathcal{D}_{X Y}$ is realizable, there exists some $c^{\\star}\\in{\\mathcal{C}}$ such that $\\mathrm{loss}_{\\mathscr{D}_{X Y}}(c^{\\star})=0$ . Let $\\mathrm{range}({\\cal A})=\\{c:\\mathcal{X}\\to\\{0,1\\}:\\exists{\\cal S}\\in\\cup_{n\\in\\mathbb{N}}(\\mathcal{X}\\times\\{0,1\\})^{n},\\exists r\\in\\cup_{n}\\}$ $\\exists r\\in\\cup_{n\\in\\mathbb{N}}\\{0,1\\}^{n}$ , s.t. ${\\mathcal{A}}(S;r)=c\\}$ . Let also $m_{0}\\,=\\,m\\bigl(1/4,1/10,1/2,\\mathcal{C}\\bigr)$ be the number of samples $\\boldsymbol{\\mathcal{A}}$ needs to achieve $\\alpha\\,=\\,^{1}\\!/\\!4,\\varepsilon\\,=$ $1/10,\\beta=1/2$ for the class $\\mathcal{C}$ . Notice that $m_{0}$ is a constant with respect to $\\alpha,\\beta,\\rho,\\epsilon.$ . Let also ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}(\\mathcal{D}_{X Y})=\\left\\{c\\in\\mathrm{range}(A):\\mathrm{loss}_{\\mathcal{D}_{X Y}}(c)\\leq1/4\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By definition, with probability at least $1/2$ over the random draw of an i.i.d. sample of size $m_{0}$ from $\\mathcal{D}_{X Y}$ and the internal randomness of $\\boldsymbol{\\mathcal{A}}$ , the output of the algorithm belongs to $\\mathcal{C}(\\mathcal{D}_{X Y})$ . Thus, there exists a sample $S_{0}\\in(\\mathcal{X}\\times\\{0,1\\})^{m_{0}}$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{r\\sim\\mathcal{R}}[A(S_{0};r)\\in\\mathcal{C}(\\mathcal{D}_{X Y})]\\geq1/2\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Although it is unclear how to identify such a $S_{0}$ , notice that any sample $S\\in(\\mathcal{X}\\times\\{0,1\\})^{m_{0}}$ is an $m_{0}$ -neighbor of $S_{0}$ . Since $\\boldsymbol{\\mathcal{A}}$ is pure DP, it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{r\\sim\\mathcal{R}}[A(S;r)\\in\\mathcal{C}(\\mathcal{D}_{X Y})]\\geq1/2\\cdot e^{-0.1\\cdot m_{0}}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In particular, this holds for the dummy dataset $\\bar{S}$ we fix before observing any data. Then if we sample $\\dot{N_{}}\\dot{=}2\\cdot e^{0.1m_{0}}\\cdot\\log(3/\\beta^{\\prime})=\\Theta(\\exp(m_{0})\\cdot\\log(1/\\beta^{\\prime}))$ i.i.d. random strings $\\boldsymbol{r}_{1},\\ldots,\\boldsymbol{r}_{N}$ from $\\mathcal{R}$ i.i.d. the probability that none of the hypotheses $\\{\\mathcal{A}(\\bar{S};r_{i})\\}_{i\\in[N]}$ is in $\\mathcal{C}(\\mathcal{D}_{X Y})$ is at most ", "page_idx": 33}, {"type": "equation", "text": "$$\n(1-1/2\\exp(-0.1m_{0}))^{2\\exp(0.1m_{0})\\cdot\\log(1/\\beta^{\\prime})}\\leq{\\frac{\\beta^{\\prime}}{3}}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We denote the event that one of the outputs is in $\\mathcal{C}(\\mathcal{D}_{X Y})$ by $\\mathcal{E}$ and we condition on it for the rest of the correctness proof. The next step is to replicably learn the finite concept class $\\bar{\\mathcal{C}}\\,=$ $\\{\\mathcal{A}(\\bar{S};r_{1}),\\ldots,\\mathcal{A}(\\bar{S};r_{N})\\}$ we have constructed using Theorem F.1 with accuracy $1\\bar{/}8$ , confidence ${^{\\beta}}^{\\prime}/3$ , and replicability $\\rho$ . Thus, we can see that $\\begin{array}{r}{O\\left(\\frac{\\log^{2}N+\\log\\frac{1}{\\beta\\rho}}{\\rho^{2}}\\log^{3}\\frac{1}{\\rho}\\right)\\,=\\,\\widetilde O\\left(\\frac{m_{0}^{2}\\log\\left(1/\\beta^{\\prime}\\right)}{\\rho^{2}}\\right)}\\end{array}$ samples suffice for this task. Let ${\\mathcal{E}}^{\\prime}$ be the event that this estimation is correct, which happens with probability at least ${^{\\beta}}^{\\prime}/3$ . Conditioned on that event, outputting the hypothesis generated by the learner gives a solution with error at most $1/4+1/8=3/8$ . Notice that the good events happen with probability at least $1-2\\beta^{\\prime}/3\\geq1-\\beta$ , so the correctness condition is satisfied. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "Replicability. Notice that the first step of the algorithm where we fix a dummy dataset $\\bar{S}$ of size $m_{0}$ and run the algorithm on i.i.d. strings of its internal randomness is trivially replicable, since the dataset and the randomness are the same across the two executions. Then, by the guarantees of Theorem F.1, we know that the output of the algorithm will be the same across two executions with probability at least $1-\\rho$ . Thus, overall we see that with probability at least $1-\\rho$ , the output of our algorithm is the same across the two executions. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "We emphasize that the number of oracle calls to the DP algorithm is constant with respect to the confidence and the correctness parameter, but could, potentially, be exponential with respect to some parameter that depends on the representation of the underlying concept class $\\mathcal{X}$ . ", "page_idx": 33}, {"type": "text", "text": "Equipped with the weak learner from Lemma F.2 we can boost its error parameter using the boosting algorithm that appears in Impagliazzo et al. [2022]. For completeness, we state the result below. ", "page_idx": 33}, {"type": "text", "text": "Theorem F.3 (Replicable Boosting Algorithm [Impagliazzo et al., 2022]). Let $\\mathcal{D}_{X Y}$ be the joint distribution over labeled examples as in Lemma $F.2$ . Fix $\\alpha,\\rho,\\beta^{\\prime}>0$ . Let $\\boldsymbol{\\mathcal{A}}$ be a $\\rho$ -replicable $(\\gamma,\\beta)$ - weak learner with sample complexity $m(\\gamma,\\rho,\\beta),$ , i.e., its error is at most $^{1\\!}/2-\\gamma,$ , with probability at least $1-\\beta$ . Then, there exists an efficient $\\rho$ -replicable boosting algorithm $\\mathcal{A^{\\prime}}$ such that with probability at least $1-\\beta^{\\prime}$ it outputs a hypothesis $h$ with $\\mathrm{loss}_{{\\mathscr D}_{X Y}}(h)\\,\\leq\\,\\alpha$ . The algorithm runs for $T=\\widetilde{O}(\\log(1/\\beta)/(\\alpha\\rho^{2}))$ rounds and requires $T$ oracle calls to the weak learner. Moreover, it requires ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widetilde{O}\\left(\\left(\\frac{m(\\gamma,\\rho/6T,\\beta)}{\\alpha^{2}\\gamma^{2}}+\\frac{1}{\\rho^{2}\\alpha^{3}\\gamma^{2}}\\right)\\log\\frac{1}{\\beta^{\\prime}}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "many samples, where $m(\\gamma,\\rho/6T,\\beta)$ is the sample complexity needed for the weak learner to be $\\rho/6T$ -replicable and $(1/2-\\gamma,\\beta)$ -accurate. ", "page_idx": 33}, {"type": "text", "text": "Combining the discussion above, we see that for any $\\alpha,\\rho,\\beta\\in(0,1)^{3}$ we can transform a pure DP learner to a $\\rho$ -replicable $(\\alpha,\\beta)$ -correct (strong) learner where the transformation is efficient with respect to the accuracy $\\alpha$ , confidence $\\beta$ , and replicability $\\rho$ . This proves Theorem 5.2, which we restate below for convenience. ", "page_idx": 33}, {"type": "text", "text": "Theorem 5.2 (From Pure DP Learner to Replicable Learner). Let $\\mathcal{X}$ be some input domain, ${\\mathcal{V}}\\,=\\,\\{0,1\\}$ , and $\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}$ be a distribution on $\\mathcal X\\times\\mathcal Y$ that is realizable with respect to some concept class $\\mathcal{C}$ . Let $\\boldsymbol{\\mathcal{A}}$ be a pure $D P$ learner that, for any $\\alpha,\\varepsilon,\\beta\\,\\in\\,(0,1)$ , needs $m(\\alpha,\\varepsilon,\\beta,\\mathcal{C})=$ $\\mathrm{poly}(1/\\alpha,1/\\varepsilon,\\log(1/\\beta),\\mathsf{d i m}(\\mathcal{C}))$ i.i.d. samples from $\\mathcal{D}_{\\mathcal{X}\\mathcal{X}}$ and $\\mathrm{poly}(m)$ running time to output $a$ hypothesis that has error at most $\\alpha$ , with probability $1\\,-\\,\\beta$ in an $\\varepsilon{-}D P$ way. Then, for any $\\alpha^{\\prime},\\rho,\\beta^{\\prime}\\in(0,1)$ there is a $\\rho$ -replicable learner $\\mathcal{A^{\\prime}}$ that outputs a hypothesis with error at most $\\alpha^{\\prime}$ with probability at least $1-\\beta^{\\prime}$ and requires po $\\mathrm{ly}(1/\\alpha^{\\prime},1/\\rho,\\log(1/\\beta^{\\prime}),\\mathsf{d i m}(\\mathscr{C}))$ i.i.d. samples from $\\mathcal{D}_{X Y}$ and $\\operatorname{poly}(1/\\alpha^{\\prime},1/\\rho,\\log(1/\\beta^{\\prime}))\\cdot\\exp(\\dim(\\mathcal{C}))$ running time. ", "page_idx": 33}, {"type": "text", "text": "We reiterate that even though our transformation is efficient with respect to the input parameters $\\alpha,\\beta,\\rho$ , it might not be efficient with respect to some parameter that depends on the representation of the concept class. This is because the size of the \u201cdummy\u201d dataset $m_{0}$ could depend on the size of that representation and our transformation requires poly $\\left(m_{0}\\right)$ samples but $\\exp(m_{0})$ running time. This is also the case for transformation from a DP learner to an online learner [Gonen et al., 2019]. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Justification: The main body and appendix of the submission provide proofs for all the claims.   \n2   \n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We formally define the mathematical model our results hold for. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We present full proofs in the main body and the appendix.   \nGuidelines: \u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main ex  \nperimental results of the paper to the extent that it affects the main claims and/or conclusions   \nof the paper (regardless of whether the code and data are provided or not)?   \nAnswer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 36}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the   \nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \nAnswer: [Yes]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Justification: The work is primarily theoretical and does not have any immediate societal impact.   \nGuidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]