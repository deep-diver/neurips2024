[{"type": "text", "text": "Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Seunggeun Chi\u2020 Pin-Hao Huang\u2217, Enna Sachdeva\u2217 Purdue University Honda Research Institute USA chi65@purdue.edu {pin-hao_huang, enna_sachdeva}@honda-ri.com ", "page_idx": 0}, {"type": "text", "text": "Hengbo Ma\u2020 Karthik Ramani Kwonjoon Lee Honda Research Institute USA Purdue University Honda Research Institute USA hengbo.academia@gmail.com ramani@purdue.edu kwonjoon_lee@honda-ri.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of estimating the body movements of a camera wearer from egocentric videos. Current methods for ego-body pose estimation rely on temporally dense sensor data, such as IMU measurements from spatially sparse body parts like the head and hands. However, we propose that even temporally sparse observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. Naively applying diffusion models to generate full-body pose from head pose and sparse hand pose leads to suboptimal results. To overcome this, we develop a two-stage approach that decomposes the problem into temporal completion and spatial completion. First, our method employs masked autoencoders to impute hand trajectories by leveraging the spatiotemporal correlations between the head pose sequence and intermittent hand poses, providing uncertainty estimates. Subsequently, we employ conditional diffusion models to generate plausible full-body motions based on these temporally dense trajectories of the head and hands, guided by the uncertainty estimates from the imputation. The effectiveness of our method was rigorously tested and validated through comprehensive experiments conducted on various HMD setup with AMASS and Ego-Exo4D datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The evolution of augmented reality (AR) devices such as the Apple Vision Pro, Meta Quest 3, Microsoft HoloLens 2, and etc. has dramatically reshaped interactive technologies. These headmounted displays (HMDs) feature inertial measurement units (IMUs) and video capture capabilities, offering a unique egocentric perspective. However, their limited visibility of the user\u2019s body parts poses a significant challenge for accurate egocentric body pose estimation\u2014a key element for immersive AR experiences. ", "page_idx": 0}, {"type": "text", "text": "Previous approaches have tackled this problem by spatially reconstructing the entire body from spatially sparse data. For instance, EgoEgo [16] first estimates head poses using SLAM on the egocentric video, then generates body poses from these estimated head positions. Other methods, such as AvatarPoser [13] and BoDiffusion [3], primarily depend on temporally dense tracking signal from spatially sparse body parts, notably the head and hands. This dependency on specific hardware such as head-mounted displays and hand controllers constrains their versatility and diminishes their applicability in broader AR/VR scenarios where hand controllers might not be used, like sports training or analysis applications where the user needs to move freely without holding any devices, or augmented reality experiences in outdoor environments where carrying controllers is impractical. ", "page_idx": 0}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/07efee032a04437055ffc0429c33dd1ede6dd5d5574add40adaf05f9fbec02d0.jpg", "img_caption": ["Figure 1: Overview of DSPoser. Our goal is to estimate ego-body pose without dependency on hand controllers in an HMD environment. (a) Given the egocentric video and head tracking signals as input, (b) our approach first predicts the hand pose in the frames where hands are visible (dark blue). It then estimates the hand poses in frames with invisible hands (light blue) using imputation, and (c) estimates uncertainty associated with the hand poses where the hands are invisible, (d) The predicted and imputed hand pose is then used with head pose to predict the 3D full body pose. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We observe that even temporally sparse observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. While it is possible to utilize other visible body parts such as feet or elbows, we opted to rely on hand poses. This decision is based on the availability of hand pose detectors [24, 14] and the fact that hands are visible in approximately $20\\%$ of video frames, as demonstrated in Table 8. Unlike previous work that concentrated only on spatial completion, our method incorporates temporal completion by leveraging the intermittent appearance of hands in egocentric videos. This dual completion approach not only enhances the robustness of body pose estimation under varying conditions but also reduces reliance on specific sensor hardware, making it more adaptable to various AR environments. In our setup, we use temporally sparse 3D hand poses from detections in egocentric videos combined with dense head tracking signals to reconstruct the full body. Initially, we temporally complete sparse hand information using a Masked Autoencoder (MAE) [11], which estimates hand pose trajectories by capturing the spatiotemporal correlations between intermittent hand poses and head tracking signals. We develop a probabilistic extension of the MAE to provide uncertainty estimates of the predicted hand pose sequence. Subsequently, using a conditional diffusion model, we spatially reconstruct the full body based on the head tracking signal data and imputed hand trajectories along with their predictive uncertainties. We call our approach DSPoser (Doubly Sparse Poser) because it can effectively utilize data that is doubly sparse (sparse both temporally and spatially), as shown in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "This flexible framework is designed to seamlessly adapt to diverse AR/VR setups and devices, ranging from spatially sparse scenarios (e.g., using only head tracking signal or combining it with hand controllers) to doubly sparse scenarios (utilizing head signal data alongside hand detection from egocentric video). The key advantage lies in the assumption that the HMD\u2019s tracking signal is consistently available, enabling our approach to function across a wide range of environments and hardware configurations. Extensive experiments have proved our model\u2019s versatility and accurate pose estimation capabilities in various settings. Furthermore, our ablation studies highlight the significance of incorporating uncertainty estimates, as this crucial information enhances the overall quality of pose estimation, resulting in more reliable outputs. By addressing both temporal and spatial completion through our double completion approach, we have developed a robust and adaptable solution that reduces dependency on specific sensor hardware, making it well-suited for immersive AR experiences in diverse scenarios, such as sports training, outdoor environments, and beyond. ", "page_idx": 1}, {"type": "text", "text": "In summary, our research presents three key contributions: ", "page_idx": 2}, {"type": "text", "text": "\u2022 A robust and versatile framework for egocentric body pose estimation tailored for HMDs. The framework adapts to various AR/VR settings and can leverage tracking signals available in most modern HMD devices without controllers.   \n\u2022 We decomposed the problem into temporal completion and spatial completion. Our approach captures the uncertainty from hand trajectory imputation to guide the diffusion model for accurate full-body motion generation.   \n\u2022 Extensive evaluations demonstrating the effectiveness of our framework on diverse datasets, outperforming existing methods and underscoring its potential for enhancing user interaction and immersion in AR experiences. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In our work, we aim to estimate the 3D human pose of a HMD user from sequences of RGB video and head tracking signal. We note that head tracking signal data is commonly accessible from IMU in most HMDs, such as Meta Quest and Apple Vision Pro. Suppose we are given an egocentric video $\\mathcal{V}_{\\mathrm{ego}}=\\{\\mathcal{V}_{1},\\ldots,\\mathcal{V}_{T_{w}}\\}$ where $\\mathcal{V}_{\\tau}$ is an RGB image and $T_{w}$ denotes the sequence length, and a corresponding head tracking signal sequence $\\mathcal{T}_{\\mathrm{head}}=\\{\\mathcal{T}_{1},\\ldots,\\mathcal{T}_{T_{w}}\\}$ where $\\mathcal{T}_{\\tau}\\in\\mathbb{R}^{D_{h e a d}}$ and $D_{h e a d}$ is the dimension of head tracking signal including 3D pose. Our goal is to estimate the full body pose $\\mathcal{P}=\\{\\mathcal{P}_{1},\\dotsc,\\mathcal{P}_{T_{w}}\\}$ , where pose state $\\mathcal{P}_{\\tau}\\in\\overline{{\\mathbb{R}^{J\\times\\dot{D}}}}$ at time $\\tau$ , $J$ is the number of body joints and $D$ is the dimensionality of pose state. We solve the problem of estimating $p(\\mathcal{P}|\\mathcal{V}_{e g o},\\bar{\\mathcal{T}_{\\mathrm{head}}})$ by decomposing it in..t.o. 2 the stages of imputation and gen..e..ration, assuming that we have temporally sparse hand data $\\dddot{\\mathcal{H}}$ from hand detection module $f(\\cdot)$ : $\\dddot{\\mathcal{H}}=f(\\mathcal{V}_{e g o})$ . We first temporally complete hand trajectoryH based onH and $\\mathcal{T}_{\\mathrm{head}}$ , which can be written as $p(\\widetilde{\\mathcal{H}}|\\,\\widetilde{\\mathcal{H}},\\mathcal{T}_{\\mathrm{head}})$ . Then, we spatially complete full body pose $\\mathcal{P}$ from the imputed handsH  and $\\mathcal{T}_{\\mathrm{head}}$ , which can be written as $p(\\mathcal{P}|\\widetilde{\\mathcal{H}},\\mathcal{T}_{\\mathrm{head}})$ . SinceH is a probabilistic variable, we need to marginalize overH as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\mathcal{P}|\\mathcal{V}_{e g o},\\mathcal{T}_{\\mathrm{head}})=\\int_{\\widetilde{\\mathcal{H}}}p(\\mathcal{P}|\\widetilde{\\mathcal{H}},\\mathcal{T}_{\\mathrm{head}})p(\\widetilde{\\mathcal{H}}|f(\\mathcal{V}_{e g o}),\\mathcal{T}_{\\mathrm{head}}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/db349acef0f6e1839890f77be0d32db314e90b035bc201035755672710141240.jpg", "img_caption": ["3 Methods ", "Figure 2: Overall pipeline of our proposed work DSPoser, composed of Temporal Completion stage and Spatial Completion stage to tackle pose estimation problem from doubly sparse data. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.1 Detection: Hand Pose Estimation from Egocentric Video ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we estimate the 3D position of the hand from an egocentric camera using a two-step process. First, we use FrankMocap [24] to predict hand poses as SMPL-X parameters [22], from which we extract local 3D hand joint positions relative to the root of the hand model\u2019s kinematic tree, denoted as $\\mathcal{H}_{h}^{3D}\\in\\mathbb{R}^{21\\times3}$ . Simultaneously, we use RTM-Pose [14] to estimate 2D hand joint positions in the image, $\\mathcal{H}_{I}^{2D}\\in\\mathbb{R}^{21\\times2}$ . Finally, we determine the 3D hand joint positions in the camera coordinate system, $\\mathcal{H}_{I}^{3D}=\\mathcal{H}_{h}^{3D}+\\mathbf{d}$ by solving for $\\mathbf{d}\\in\\mathbb{R}^{3}$ that minimizes the reprojection error $\\|\\mathcal{H}_{I}^{2D}-\\mathbf{K}(\\mathcal{H}_{h}^{3D}+\\mathbf{d})\\|_{2}$ . Here, $\\mathbf{K}$ is the intrinsic matrix, obtained by transforming the original camera parameters into a pinhole model through undistortion. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To better constrain the hand trajectories, we attempted to obtain rotation information from the 3D hand detection. However, due to the inconsistent quality of hand detection, the rotational information derived from the hand pose was noisy. Therefore, we decided not to incorporate this rotational information into our hand tracking approach on the Ego-Exo4D dataset. We utilized only the 3D wrist location from the Ego-Exo4D dataset, represented by $D_{h a n d}=3$ . In contrast, for the AMASS dataset, we leveraged both rotational information and 3D location, as this data is readily available, resulting in $D_{h a n d}=9$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Temporal Completion: Hand Trajectory Imputation from Sparse Hand Pose ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Masked Auto-Encoder (MAE) In our work, we employed a Masked Autoencoder (MAE) [1..1..] to impute missing hand trajectories using head tracking signal $\\mathcal{T}_{h e a d}$ and detected hand pose $\\mathcal{H}$ . Inspired by Vision Transformer (ViT), we treated each $\\boldsymbol{\\tau}_{\\u{\\tau}}$ and $\\mathcal{H}_{\\tau}$ at time $\\tau$ as a token similar to an image patch in ViT. To accommodate this, we im..p..lemented two embedding layers, one for head tracking signal $\\mathcal{T}_{\\tau}\\in\\mathbb{R}^{D_{h e a d}}$ and the other for hand $\\widehat{\\mathcal{H}}_{\\tau}\\in\\mathbb{R}^{D_{h a n d}}$ , both projecting into the common token dimension $D_{M}$ . For the AMASS dataset, we follow the head tracking signal representation $D_{\\mathrm{head}}=18$ as in [13]. For the Ego-Exo4D dataset, $D_{\\mathrm{head}}=15$ , which includes head position and left/right IMU signals. Consequently, the total number of token amounts to $3\\times T_{w}$ , where 3 accounts for the head and both hands, and $T_{w}$ is the sequence length. Sinusoidal positional encoding (PE) is used for both the encoder and decoder patches after tests showed it suffices for learning different modalities, compared to learnable PE. In an HMD environment, we assume that the head tracking signal $\\pi_{h e a d}$ is always available, but hand visibility depends on the egocentric video. Thus, masking is applied only to the hand tokens based on their visibilities within egocentric view. ", "page_idx": 3}, {"type": "text", "text": "In contrast to the MAE [11] training approach, which maintains a consistent number of masked patches due to a fixed masking ratio, the count of frames with invisible hand varies across instances in our setup. To address this variability, our encoder selectively applies attention masking to these inputs, ensuring that queries do not attend to tokens where hand is invisible. This attention masking technique adapts dynamically to the fluctuating numbers of missing frames across the instances, enhancing the model\u2019s ability to handle data sparsity effectively. For decoder, we adopted MAE decoder design except the last projection layer to guide the uncertainty. To capture the uncertainty, we split the final projection layer into two heads for mean and variance of a Gaussian distribution. ", "page_idx": 3}, {"type": "text", "text": "Uncertainty-aware MAE Following the [26, 30], to make the MAE aware of the predictive uncertainty of imputed hand pose sequence, we employ the $\\beta$ -NLL loss [26] function to manage uncertainty by using a set of mean heads $\\mu_{i}(\\mathbf{x})$ and variance heads $\\sigma_{i}^{2}(\\mathbf{x})$ , which are derived from $M$ models initialized differently, where $\\mathbf{x}=[\\mathop{\\partial\\v{U}};\\boldsymbol{\\mathcal{T}}]$ is an input to the MAE and $i\\in[1,M]$ . The mean heads $\\mu_{i}(\\mathbf{x})$ and variance heads $\\sigma_{i}^{2}(\\mathbf{x})$ are trained using the Gaussian negative log-likelihood loss, which applies to each sample indexed by $n$ with input ${\\bf x}_{n}$ and ground truth hand pose sequence ${\\bf y}_{n}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}_{\\beta-\\mathrm{NLL}}({\\bf y}_{n},{\\bf x}_{n})=\\mathrm{sg}(\\sigma_{i}^{2\\beta}){\\cal L}_{\\mathrm{NLL}}({\\bf y}_{n},{\\bf x}_{n})\\;\\mathrm{where},\\;}}\\\\ {{\\displaystyle{\\cal L}_{\\mathrm{NLL}}({\\bf y}_{n},{\\bf x}_{n})=\\frac{\\log\\sigma_{i}^{2}({\\bf x}_{n})}{2}+\\frac{(\\mu_{i}({\\bf x}_{n})-{\\bf y}_{n})^{2}}{2\\sigma_{i}^{2}({\\bf x}_{n})}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The $L_{\\mathrm{{NLL}}}$ loss function causes the predicted variance to act as a weighting factor for each data point, emphasizing those with higher variances. The parameter $\\beta$ adjusts the intensity of this weighting. The $\\operatorname{sg}(\\cdot)$ function is used to apply the stop-gradient operation, thus preventing gradients from propagating through this part of the computation. ", "page_idx": 3}, {"type": "text", "text": "After training, we measure the aleatoric (data) uncertainty $\\mathcal{U}_{a l e}(\\cdot)$ by averaging the variances across models, and epistemic (model) uncertainty $\\mathcal{U}_{e p i}(\\cdot)$ by calculating variance of model means, and total uncertainty by adding both uncertainties: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}_{a l e}(\\mathbf{x})=\\mathbb{E}_{i}[\\sigma_{i}^{2}(\\mathbf{x})]\\approx M^{-1}\\sum_{i}\\sigma_{i}^{2}(\\mathbf{x})}\\\\ &{\\mathcal{U}_{e p i}(\\mathbf{x})=\\mathrm{Var}_{i}[\\mu_{i}(\\mathbf{x})]}\\\\ &{\\mathcal{U}_{t o t}(\\mathbf{x})=\\mathcal{U}_{a l e}(\\mathbf{x})+\\mathcal{U}_{e p i}(\\mathbf{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that $\\mathcal{U}_{a l e}(\\cdot)$ and $\\mathcal{U}_{e p i}(\\cdot)$ provide uncertainties for each frame and each pose state dimension. The captured uncertainty is visualized in Figure 3, demonstrating that MAE effectively captures uncertainty. ", "page_idx": 4}, {"type": "text", "text": "3.3 Spatial Completion: Uncertainty-guided Body Pose Generation from Imputed Hand Trajectories and Head Tracking Signal ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We employed the VQ-Diffusion [23] to generate full body poses from imputed hand trajectories and head tracking signal. The exposition of VQ-Diffusion can be found in Section D.1 of the Appendix. As illustrated in Figure 2, our motion generation module is designed to generate human motion sequences from the temporally dense hand and head trajectories with uncertainty obtained from the MAE model. ", "page_idx": 4}, {"type": "text", "text": "VQ-VAE We first train the VQ-VAE to represent human motion with a discrete codebook representation as described in Appendix D.1. We mostly followed the architectural design and training methods of [35]. After the codebook representation is learned by the VQ-VAE, we utilize this latent codebook representation to train a denoising diffusion model. ", "page_idx": 4}, {"type": "text", "text": "Denoising Transformer Motivated by the work of VQ-Diffusion, we design a denoising transformer that estimates the distribution $p(\\mathbf{z}_{0}|\\mathbf{z}_{t},\\mathbf{y})$ . An overview of our proposed model is depicted in Figure 2. We closely follow the implementation of [4]. To incorporate the diffusion step $t$ into the network, we employ the adaptive layer normalization (AdaLN) [2, 15]. We concatenated the estimated hand and head trajectory with codebook after a embedding layer, to match the dimension with codebook representation. Finally, we use the decoder to decode $\\mathbf{z}_{\\mathrm{0}}$ to obtain a full body pose sequence. ", "page_idx": 4}, {"type": "text", "text": "Uncertainty Guidance We introduce several strategies to guide the denoising process using uncertainty estimates of imputed hand trajectories: sampling, dropout, and distribution embedding. ", "page_idx": 4}, {"type": "text", "text": "For sampling, we sample a hand sequence from the distribution $\\widetilde{\\mathcal{H}}\\sim\\mathcal{N}(\\mu^{*}(\\mathbf{x}),\\sqrt{\\mathcal{U}^{*}(\\mathbf{x})})$ and regard it as the conditioning vector $\\mathbf{y}$ , where $\\begin{array}{r}{\\mu^{*}(\\mathbf{x})=\\mathbb{E}_{i}[\\mu_{i}(\\mathbf{x})]\\approx M^{-1}\\sum_{i}\\mu_{i}(\\mathbf{x})}\\end{array}$ and $\\mathcal{U}^{*}(\\mathbf{x})$ is measured by one of Eq. (9), (10), and (11). While it would be ideal to sample multiple times to better approximate the marginalization in Equation 1, we find just using one sample provides a competitive performance. ", "page_idx": 4}, {"type": "text", "text": "For dropout, we set each dimension of $\\mu({\\bf x})$ to zero with a certain probability, which is determined by the corresponding dimension of $\\mathcal{U}^{*}(\\mathbf{x})$ , and denote the result as $y$ . The probability of $d_{\\cdot}$ -th dimension of $\\mu({\\bf x})$ being zero is $p_{d}=1-(\\mathcal{U}_{d}^{*}(\\mathbf{\\dot{x}})-\\mathcal{U}_{d\\,m i n}^{*}(\\mathbf{x})/(\\mathcal{U}_{d\\,m a x}^{*}(\\mathbf{\\dot{x}})-\\mathcal{U}_{d\\,m i n}^{*}(\\mathbf{x}))$ where $\\mathcal{U}_{d}^{*}(\\mathbf{x})$ is the $d$ -th dimension of $\\mathcal{U}^{*}(\\mathbf{x})$ , $\\mathcal{U}_{d}^{*}m i n^{\\left(\\mathbf{x}\\right)}$ , $\\mathcal{U}_{d\\,m a x}^{*}(\\mathbf{x})$ are the minimum and maximum values over the sequence length, respectively. ", "page_idx": 4}, {"type": "text", "text": "For distribution embedding [28], we embed the Gaussian distribution $\\mathcal{N}(\\mu^{*}(\\mathbf{x}),\\sqrt{\\mathcal{U}^{*}(\\mathbf{x})})$ to a vector by concatenating the $\\mu^{*}(\\mathbf{x})$ and $\\mathcal{U}^{*}(\\mathbf{x})$ in the feature dimension. The resulting embedding will be further concatenated with the head pose sequence to form a conditioning vector $\\mathbf{y}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Datasets $\\&$ Evaluation Metrics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Ego-Exo4D dataset Ego-Exo4D [9] contains simultaneous captures of egocentric (first-person) and exocentric (third-person) video perspectives of participants performing complex activities like sports, dance, and mechanical tasks. The dataset comprises 1,422 hours of video ranging from 1 to 42 minutes per video. In addition to video, it provides camera poses, IMU data, and human pose annotations. Specifically for the egopose task, it includes separate training and validation video sets containing 334 and 83 videos respectively. Our problem formulation of ego body pose estimation differs from the ego body pose prediction task from [9], which aims to predict a single future frame given a specific time window. ", "page_idx": 4}, {"type": "text", "text": "AMASS dataset The AMASS dataset [19] is a large human motion database that unifies different existing optical marker-based MoCap datasets by converting them into realistic 3D human meshes represented by SMPL [17] model parameters. Following the AvatarPoser [13] evaluation, we used the CMU [5], BMLrub [29], and HDM05 [21] subsets from the AMASS dataset and their preprocessing of tracking signal information. Since AMASS does not include RGB images, we set $D_{h a n d}=9$ assuming that 3D hand position and 6D rotation is available when the hand is \"visible\". To determine visibility, we compute the angle between the ${\\bf Z}$ -axis vector of the head rotation and the vector from the head position to the hand. We define the hand as \"visible\" if this angle is within a $45^{\\circ}$ range, corresponding to a $90^{\\circ}$ field of view (FoV) of HMD devices. ", "page_idx": 4}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/094e73e8a60688a71c0cf787d0142e4948f114824d8cec0c6f62874ba9a23423.jpg", "table_caption": ["Table 1: Performance comparisons across baseline models for doubly sparse video data on the AMASS test set. We report MPJRE $[^{\\circ}]$ , MPJPE [cm], and MPJVE $[\\mathrm{cm}/\\mathrm{s}]$ , with the best results highlighted in boldface. Models trained by us are marked with \u2217. The notationdata denotes temporally sparse data,data indicates imputed data, and all other cases involve dense data. $T_{s}$ indicates the sliding win dow, $\\mathbf{X}$ indicates the input of our whole pipeline, and $\\mathbf{y}$ indicates the input of denoising Transformer. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Evaluation metric We evaluate our results using the following metrics: Mean Per Joint Position Error (MPJPE), Mean Per Joint Velocity Error (MPJVE), and Mean Per Joint Rotation Error (MPJRE), following the evaluation of [13, 3]. Since Ego-Exo4D dataset doesn\u2019t have the annotations for 6D rotation, MPJRE is reported only for AMASS. We report all values with the confidence interval of $95\\%$ . We also provide details on MPJPE across hands, upper body above the pelvis, and lower body below the pelvis, denoted as Hand PE, Upper PE, and Lower PE, respectively. ", "page_idx": 5}, {"type": "text", "text": "4.2 Full Body Pose Estimation from Doubly Sparse data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To demonstrate the effectiveness of our framework on doubly sparse egocentric video data, we investigated the results of our framework, DSPoser, on the AMASS dataset and Ego-Exo4D, as shown in Table 1 and Table 2, respectively. Since the task of body pose estimation from doubly sparse data is newly introduced in our paper, we compare our results to other baselines, EgoEgo [16], Bodiffusion [3], AvatarPoser [13], and AvatarJLM [3]. Those baselines are designed to estimate human body poses from spatially sparse data. EgoEgo estimates body poses from head poses, and the others estimate body poses from head and hand tracking signals. We report the experimental results using the sampling strategy with aleatoric uncertainty unless otherwise stated. To train the baslines on temporally sparse data, we extend the algorithm as follows: (1) Interpolation: we imputed hand poses with linear interpolation; (2) $M A E$ : we use our trained MAE to impute the hand trajectory. In $T_{s}=1$ setup, we report our result after averaging 16 samples while the result in $T_{s}=20$ setup is from a single sample. ", "page_idx": 5}, {"type": "text", "text": "As shown in Table 1, DSPoser consistently outperforms baseline methods on AMASS across all metrics, underscoring the effectiveness of our two-stage approach for ego-body pose estimation. DSPoser achieves notable improvements in MPJPE for both sliding window sizes, $T_{s}\\,=\\,20$ and $T_{s}=1$ . For $T_{s}=20$ , DSPoser reduces MPJPE from $7.35\\;\\mathrm{cm}$ to $5.51\\;\\mathrm{cm}$ , significantly outperforming the Bodiffusion extension, which uses MAE to impute invisible hands. For $T_{s}~=~1$ , DSPoser achieves superior MPJPE compared to AvatarJLM, though it showswlimitations in MPJVE due to the stochasticity of the diffusion model. In the experimental results presented in Table 2, our DSPoser model demonstrates superior performance on the Ego-Exo4D validation set. The model outperforms existing baselines, achieving a lower MPJPE of $16.84~\\mathrm{cm}$ , which represents an improvement over the next best model by $5.49\\,\\mathrm{cm}$ . Additionally, DSPoser achieves an MPJVE of $39.86\\;\\mathrm{cm/s}$ , improving upon the basline of naive extension of Bodiffusion by $7.64\\;\\mathrm{cm/s}$ . ", "page_idx": 5}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/7005602d94dacd6230f0c2389ba0b4d36a62f85598f6133fde230e663e88a825.jpg", "table_caption": ["Table 2: Performance comparisons across baseline models for doubly sparse video data on the EgoExo4D validation set. We report MPJPE [cm] and MPJVE $[\\mathrm{cm}/\\mathrm{s}]$ , with the best results highlighted in boldface. Models trained by us are marked with \u2217. The notationData denotes temporally sparse data, data indicates imputed data, and all other cases involve dense data. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/5835c96a2140382c626149dfa66e202c4470f81156684f11bd270c46dff7ea06.jpg", "table_caption": ["Table 3: Performance comparisons across baseline models on the AMASS test set. We report MPJRE [\u00b0], MPJPE [cm], and MPJVE $[\\mathrm{cm}/\\mathrm{s}]$ , with the best results highlighted in boldface. Note that $^{\\ddagger}$ is trained only with dense data without uncertainty. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "It is evident that by incorporating temporally sparse hand pose data, our DSPoser framework significantly enhances pose estimation accuracy. For instance, on the AMASS dataset, MPJPE improved dramatically from $12.08\\;\\mathrm{cm}$ to $5.51\\;\\mathrm{cm}$ , while on the Ego-Exo4D dataset, it improves from $19.12\\;\\mathrm{cm}$ to $16.84\\,\\mathrm{cm}$ in $T_{s}=20$ setup. This indicates that even sparse hand trajectory data, when effectively utilized, can provide crucial information for refining the accuracy of ego body pose estimation. Our method\u2019s ability to harness sparsely available data underscores its potential in applications where capturing dense sequence is challenging. ", "page_idx": 6}, {"type": "text", "text": "4.3 Full Body Pose Estimation from Spatially Sparse data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the versatility of our framework, we conduct experiments on spatially sparse video data. In the temporally dense data setup, where there is no uncertainty regarding hand poses, the dense data directly works as a condition y for spatial completion on the right side of Figure 2. Table 3 presents the results, demonstrating that DSPoser performs comparably to baseline models designed specifically for dense data setups on MPJPE and MPJRE metrics, underscoring the versatility of our dual approach in handling dense data scenarios. As discussed in Section 4.2, the higher MPJVE error results from the inherent stochasticity of the diffusion model. ", "page_idx": 6}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/4d51bc222b5b2c97b524c9023a1d2a0cfb7446f4d028d4624475e74fd5ebf9b2.jpg", "table_caption": ["Table 4: Ablation study for uncertainty guidance strategy "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/6ae01c20e1dd4c6eed462f997604434796b161bf39fcb70b8d89f464a27504d5.jpg", "table_caption": ["Table 5: Ablation study for different types uncertainty "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/380e730960d7aedfec1d7d936de1ef5f3ddb188cde90e498d34542e3e481d96c.jpg", "table_caption": ["Table 6: Ablation study for $\\beta$ for uncertainty capturing wi "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "th MAE. Table 7: Hand detection accu-Table 8: Hand visibility ratio fo racy on Ego-Exo4D dataset. AMASS and Ego-Exo4D dataset ", "page_idx": 7}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/ce891216901f29400961c0daa099c7d0990a7a2fdb8ec2114989b5b296044ec1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Based on the ablation study results shown in Tables 4 and 5, we can analyze the impact of different uncertainty guidance strategies and types of uncertainty on the performance of the model for body pose estimation. The ablation study is conducted with AMASS dataset with the sliding window $T_{s}\\,=\\,20$ to better analyze the effect of the uncertainty guidance. Table 4 investigates the effects of various uncertainty guidance strategies, including no uncertainty guidance, sample, distribution embedding, and dropout. The results suggest that incorporating uncertainty guidance through these strategies can improve the model\u2019s performance across different metrics. The sampling strategy achieves the best performance, with the lowest MPJPE of 5.51, MPJVE of 24.19, and MPJRE of 4.09, indicating its effectiveness in capturing uncertainty and improving pose estimation accuracy. ", "page_idx": 7}, {"type": "text", "text": "Table 5 examines the contributions of different types of uncertainty, including epistemic uncertainty, aleatoric uncertainty, and total uncertainty. The results show that accounting for aleatoric uncertainty leads to the best overall performance. This suggests that considering data uncertainty can provide complementary information and improve the robustness of the pose estimation model. Overall, the ablation study highlights the importance of incorporating uncertainty guidance and considering different types of uncertainty in the model design for accurate and reliable body pose estimation. ", "page_idx": 7}, {"type": "text", "text": "In Table 6, we analyzed the effect of different $\\beta$ values on the AMASS dataset during the uncertainty capturing process of the Masked Auto-Encoder (MAE). The results, shown in the table, indicate that $\\beta=0.5$ provides the best temporal completion for head and hand 3D positions from the doubly sparse input. Therefore, we set $\\beta$ to 0.5 for training the MAE. ", "page_idx": 7}, {"type": "text", "text": "4.5 Hand Detection Accuracy and Hand Visibility Statistics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We investigate the error of the hand detector applied to the Ego-Exo4D dataset in terms of MPJPE, as shown in Table 7. The detection results indicate an average error of less than $10\\;\\mathrm{cm}$ . We also analyze the visibility statistics for the AMASS and Ego-Exo4D datasets in Table 8. In the AMASS dataset, at least one hand is visible in $18\\%$ of all frames with a $90^{\\circ}$ field of view (FoV), whereas in the Ego-Exo4D dataset, at least one hand is visible in $27\\%$ of all frames. ", "page_idx": 7}, {"type": "text", "text": "4.6 Qualitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We visualized the aleatoric uncertainty in Figure 3, captured by a model trained using MAE on the AMASS dataset. In cases of partial visibility, as shown in Figure 3 (a-1) and (a-2), the uncertainty range is notably small. Conversely, in frames where the subject is completely obscured, the uncertainty range increases significantly. Even in fully invisible scenarios, the model captures a range of uncertainty, likely influenced by head movements. Most of the estimated frames fall within the $\\pm2\\sigma$ range. ", "page_idx": 7}, {"type": "text", "text": "We also visualized the qualitative results on the Ego-Exo4D dataset and AMASS dataset in Figure 4. The qualitative results for AMASS show that our method improves the estimation results when sparse ", "page_idx": 7}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/00849e0cd8e1802243a0c8a438865f3628b374d31d211818e8a360ca78babd98.jpg", "img_caption": ["Figure 3: Uncertainty visualization of the right hand pose captured by the MAE. Gray areas represent frames where the hand is invisible, and white areas denote visible frames. We depict aleatoric uncertainty within ranges of $\\pm1\\sigma$ and $\\pm2\\sigma$ from the estimated $\\mu$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/f9e2c0aa43e3b3e28188f0024032a6fcc63a372f09b08e4528110315a10adc75.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: (a) Ego-Exo4D video frames, (b) the corresponding skeleton ground truth and our prediction results, and (c) qualitative results on AMASS data under different input conditions. green indicates the ground truth, blue indicates the predicted result, and red indicates the visible hands. Head only estimates body pose from head trajectories, whereas Ours estimates body pose from imputed hand and head trajectories. ", "page_idx": 8}, {"type": "text", "text": "hand information is available, compared to the Head Only results. Additionally, in the Ego-Exo4D results, the hands are more aligned compared to the lower body when hands are available. ", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Human Pose Estimation from Sparse Input A common capture setting in mixed reality involves using a head-mounted device and hand controllers. Estimating full-body motion from the sparse input of head and hand movements is challenging. Recently, several methods have been proposed to tackle this: AvatarPoser [13] is the first learning-based method to predict full-body poses in world coordinates using only head and hand motion inputs. It uses a Transformer encoder to extract deep features and decouples global motion from local joint orientations, refining arm positions with inverse kinematics for accurate full-body motion. BoDiffusion [3] employs a generative diffusion model for motion synthesis, addressing the under-constrained reconstruction problem. It uses a time and space conditioning scheme to leverage sparse tracking inputs, generating smooth and realistic full-body motion sequences. AvatarJLM [36] uses a two-stage framework where sparse signals are embedded into high-dimensional features and processed by an MLP to generate joint-level features. These features are then converted into tokens and fed into a transformer-based network to capture spatial and temporal dependencies, with an SMPL regressor transforming them into 3D full-body pose sequences. HMD-poser [6] combines a lightweight temporal-spatial feature learning network with regression layers and uses forward kinematics to achieve real-time human motion tracking. AGRoL [8] utilized conditional diffusion model to generate full body pose from sparse upper-body tracking signals. It is worth noting a concurrent work, EgoPoser [12], which also addresses ego body pose estimation from doubly sparse observations. Their focus lies in preparing training data through field-of-view (FoV) modeling rather than introducing new algorithms. Our work is orthogonal to theirs, providing algorithmic contributions through a multi-stage pipeline including an uncertainty-aware masked auto-encoder (MAE). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Human Body Pose Estimation from Egocentric Videos Estimating full 3D human body pose from egocentric videos is an ill-posed problem due to the partial visibility of wearer\u2019s body parts from the camera mounted on wearer\u2019s head. Recently, several approaches have been proposed to address this challenge. EgoEgo [16] integrates SLAM and a learned transformer to estimate head motion, then leverages estimated head pose to generate plausible full-body motions using diffusion models. [18] designs a kinematic policy to generate per-frame target motion from egocentric inputs, and leverages a pre-learned dynamics model to distill human dynamics information into the kinematic model. GIMO [37] integrates motion, 3D eye gaze, and 3D scene features to generate gaze informed long term intention-aware human motion prediction. [32] leverages external camera to generate pseudo labels to estimate full 3D body pose from single head mounted fish eye camera using weak supervision. [33] estimates geometry of surrounding objects and extracts 2D body pose features using EgoPW [32] to regress 3D body pose with a voxel-to-voxel network [20]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have addressed the problem of egocentric body pose estimation using temporally sparse observations from head-mounted displays (HMDs). By leveraging both temporal and spatial completion, our approach effectively utilizes intermittent hand pose detections from egocentric videos, alongside consistently available head pose data, to reconstruct full-body motions. Through comprehensive experiments on datasets such as AMASS and Ego-Exo4D, we have demonstrated the effectiveness of our framework. Our results indicate significant improvements over existing methods, particularly in scenarios where dense sensor data may not be available or practical. This advancement opens up new possibilities for beneficial augmented reality experiences in various applications, including sports training by providing feedback on body mechanics, and other scenarios where users need to move freely without additional sensors such as hand controllers. However, our method has not been explicitly tested for fairness across different demographic groups. Potential biases in the datasets used could result in uneven performance across various user populations. Careful curation of training datasets is necessary to prevent unfair failures for underrepresented groups. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While our proposed method for estimating the body movements of a camera wearer from sparse tracking signals shows promising results, several limitations should be acknowledged. Firstly, our method has been tested with only one type of sparse body part tracking signal, specifically the hand. Incorporating the detection of other body parts, such as feet and elbows, may improve overall body pose estimation. Additionally, variations in lighting, occlusions, and the quality of the egocentric video can impact the accuracy of hand pose detection, subsequently affecting the overall body pose estimation. ", "page_idx": 9}, {"type": "text", "text": "The effectiveness of our method was validated using the AMASS and Ego-Exo4D datasets. Although these datasets are comprehensive, they may not encompass the full spectrum of possible real-world variations. Our study focused on pose estimation within a window size of less than a few seconds, following standard settings from the literature. It remains unclear how our method will perform with larger window sizes. Furthermore, the scalability of our method with larger datasets has not been thoroughly evaluated. The use of diffusion models for pose estimation may limit its utility for real-time applications due to their inference speed. Additionally, using multiple models to compute epistemic uncertainty can be computationally intensive. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement We acknowledge Feddersen Chair Funds and the US National Science Foundation (FW-HTF 1839971, PFI-TT 2329804) for Professor Karthik Ramani. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency. We sincerely thank the reviewers for their constructive suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] K. Ahuja, E. Ofek, M. Gonzalez-Franco, C. Holz, and A. D. Wilson. Coolmoves: User motion accentuation in virtual reality. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5(2):1\u201323, 2021.   \n[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[3] A. Castillo, M. Escobar, G. Jeanneret, A. Pumarola, P. Arbel\u00e1ez, A. Thabet, and A. Sanakoyeu. Bodiffusion: Diffusing sparse observations for full-body human motion synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4221\u20134231, 2023.   \n[4] S. Chi, H. Chi, H. Ma, N. Agarwal, F. Siddiqui, K. Ramani, and K. Lee. M2d2m: Multi-motion generation from text with discrete diffusion models. In European conference on computer vision. Springer, 2024.   \n[5] CMU Graphics Lab. Cmu graphics lab motion capture database. http://mocap.cs.cmu. edu/, 2000.   \n[6] P. Dai, Y. Zhang, T. Liu, Z. Fan, T. Du, Z. Su, X. Zheng, and Z. Li. Hmd-poser: Ondevice real-time human motion tracking from scalable sparse observations. arXiv preprint arXiv:2403.03561, 2024.   \n[7] A. Dittadi, S. Dziadzio, D. Cosker, B. Lundell, T. J. Cashman, and J. Shotton. Full-body motion from a single head-mounted device: Generating smpl poses from partial observations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11687\u2013 11697, 2021. [8] Y. Du, R. Kips, A. Pumarola, S. Starke, A. Thabet, and A. Sanakoyeu. Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 481\u2013490, 2023.   \n[9] K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik, T. Afouras, K. Ashutosh, V. Baiyya, S. Bansal, B. Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. arXiv preprint arXiv:2311.18259, 2023.   \n[10] S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.   \n[11] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[12] J. Jiang, P. Streli, M. Meier, and C. Holz. Egoposer: Robust real-time ego-body pose estimation in large scenes. In European conference on computer vision. Springer, 2024.   \n[13] J. Jiang, P. Streli, H. Qiu, A. Fender, L. Laich, P. Snape, and C. Holz. Avatarposer: Articulated full-body pose tracking from sparse motion sensing. In European conference on computer vision, pages 443\u2013460. Springer, 2022.   \n[14] T. Jiang, P. Lu, L. Zhang, N. Ma, R. Han, C. Lyu, Y. Li, and K. Chen. Rtmpose: Real-time multi-person pose estimation based on mmpose. arXiv preprint arXiv:2303.07399, 2023.   \n[15] K. Lee, H. Chang, L. Jiang, H. Zhang, Z. Tu, and C. Liu. ViTGAN: Training GANs with vision transformers. In International Conference on Learning Representations, 2022.   \n[16] J. Li, K. Liu, and J. Wu. Ego-body pose estimation via ego-head pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17142\u201317151, 2023.   \n[17] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. Smpl: A skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851\u2013866. 2023.   \n[18] Z. Luo, R. Hachiuma, Y. Yuan, and K. Kitani. Dynamics-regulated kinematic policy for egocentric pose estimation. Advances in Neural Information Processing Systems, 34:25019\u2013 25032, 2021.   \n[19] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5442\u20135451, 2019.   \n[20] G. Moon, J. Y. Chang, and K. M. Lee. V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map. In Proceedings of the IEEE conference on computer vision and pattern Recognition, pages 5079\u20135088, 2018.   \n[21] M. M\u00fcller, T. R\u00f6der, M. Clausen, B. Eberhardt, B. Kr\u00fcger, and A. Weber. Documentation mocap database hdm05. Computer Graphics Technical Report CG-2007-2, Universit\u00e4t Bonn, 7:11, 2007.   \n[22] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman, D. Tzionas, and M. J. Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10975\u201310985, 2019.   \n[23] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[24] Y. Rong, T. Shiratori, and H. Joo. Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1749\u20131759, 2021.   \n[25] RootMotion. Final ik. https://assetstore.unity.com/packages/tools/animation/ final-ik-14290, 2018.   \n[26] M. Seitzer, A. Tavakoli, D. Antic, and G. Martius. On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks. arXiv preprint arXiv:2203.09168, 2022.   \n[27] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[28] B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch\u00f6lkopf, and G. Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:1517\u20131561, Apr. 2010.   \n[29] N. F. Troje. Decomposing biological motion: A framework for analysis and synthesis of human gait patterns. Journal of vision, 2(5):2\u20132, 2002.   \n[30] M. Valdenegro-Toro and D. S. Mori. A deeper look into aleatoric and epistemic uncertainty disentanglement. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1508\u20131516. IEEE, 2022.   \n[31] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[32] J. Wang, L. Liu, W. Xu, K. Sarkar, D. Luvizon, and C. Theobalt. Estimating egocentric 3d human pose in the wild with external weak supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13157\u201313166, 2022.   \n[33] J. Wang, D. Luvizon, W. Xu, L. Liu, K. Sarkar, and C. Theobalt. Scene-aware egocentric 3d human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13031\u201313040, 2023.   \n[34] D. Yang, D. Kim, and S.-H. Lee. Lobstr: Real-time lower-body pose prediction from sparse upper-body tracking signals. In Computer Graphics Forum, volume 40, pages 265\u2013275. Wiley Online Library, 2021.   \n[35] J. Zhang, Y. Zhang, X. Cun, Y. Zhang, H. Zhao, H. Lu, X. Shen, and Y. Shan. Generating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14730\u201314740, 2023.   \n[36] X. Zheng, Z. Su, C. Wen, Z. Xue, and X. Jin. Realistic full-body tracking from sparse observations via joint-level modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14678\u201314688, 2023.   \n[37] Y. Zheng, Y. Yang, K. Mo, J. Li, T. Yu, Y. Liu, C. K. Liu, and L. J. Guibas. Gimo: Gazeinformed human motion prediction in context. In European Conference on Computer Vision, pages 676\u2013694. Springer, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional Details ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "MHCnLo2QeA/tmp/7844d8d0f93c465588d58d86ebfb6709f1a01b4e3c11b8e528bbafba8337d40e.jpg", "table_caption": ["A.1 Architecture & Experimental Details "], "table_footnote": ["Table 9: Experimental results for different modules Table 10: Performance results for different inference and training step combinations "], "page_idx": 13}, {"type": "text", "text": "VQ-VAE We adhered to the architectural details and training protocol of Zhang et al. [35], with modifications including setting both the encoder and decoder stride to 1, and adjusting the window size to 40. For the Ego-Exo4D dataset, we employed wing loss with a width of 5 and a curvature of 4. For AMASS, we opted for L2 loss. Additionally, to generate smooth motion, we applied both velocity and acceleration losses, assigning weights of 10 for each in the AMASS dataset, and weights of 1 for each in the Ego-Exo4D dataset. The input shape is represented as $T_{w}\\times J\\times D_{\\mathrm{data}}$ , where $J$ is the number of joints, $T_{w}$ is time window, and $D_{\\mathrm{data}}$ is the data representation. For AMASS, which uses a 6D rotation representation, $J=22$ and $D_{d a t a}=6$ . For Ego-Exo4D, $J=17$ and $D_{d a t a}=3$ . We use $T_{w}=40$ for both AMASS and Ego-Exo4D. ", "page_idx": 13}, {"type": "text", "text": "VQ-Diffusion We employed the same hyperparameters and training specifications as Gu et al. [10] for training VQ-Diffusion. Additionally, we replaced the absolute positional encoding with relative positional encoding, following the implementation of VQ-Diffusion for human motion generation proposed by Chi et al. [4]. We replaced the text condition module with the uncertainty-aware MAE module to feed the imputed trajectory and uncertainty as a conditional input. ", "page_idx": 13}, {"type": "text", "text": "Masked Auto-Encoder We adapted the encoder to accommodate a variable number of visible hands and modified the last projection layer to guide the uncertainty, as detailed in the main paper. Otherwise, we followed the training details provided by He et al. [11]. We trained 4 models to measure the uncertainty, $M=4$ . ", "page_idx": 13}, {"type": "text", "text": "Dataloader We adopted the dataloader of AMASS and evaluation configurations from Bodiffusion [3] for our experiments. For the Ego-Exo4D dataset, we employed the dataset implementation from the Ego-Exo4D [9]. ", "page_idx": 13}, {"type": "text", "text": "Analaysis on Computaional Cost We evaluated the computational cost of our approach by analyzing the number of parameters, multiply-accumulate operations (MACs), and inference time for each module in our pipeline. The reported times in Table 10 represent the total measured time for the entire pipeline, whereas Table 9 measures the time only for the corresponding module, excluding overhead between modules. As shown in Table 9, the VQ-Diffusion model is responsible for the majority of MACs and inference time. To mitigate this, we conducted further experiments to explore ways of reducing the VQ-Diffusion model\u2019s inference time. Table 10 presents the trade-off between performance and inference time based on the number of diffusion steps, offering multiple options. Notably, training with 50 steps and inferring with 25 steps yields approximately a 4x faster inference time with only about $3\\%$ reduction in performance. ", "page_idx": 13}, {"type": "text", "text": "B Compute Resource ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We ran our experiments on one workstation, containing AMD Ryzen Threadripper PRO 7975WX, DDR5 RAM 256GB and 4 NVIDIA GeForce RTX 4090. AMASS dataset takes 512GB of storage and Ego-Exo4D dataset takes 11TB. One training run took around 18 hours on 1 GPU for AMASS and around 12 hours for Ego-Exo4D. Inference over AMASS validation set takes 40 minutes on 1 GPU and inference over Ego-Exo4D validation set takes 10 minutes on 1 GPU. In total, all experiments including preliminary or failed experiments took approximately 300 GPU-hours. ", "page_idx": 13}, {"type": "text", "text": "C Licenses for Assets Used in the Paper ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Code We use the code of BoDiffusion [3] which is available at https://github.com/BCVUniandes/BoDiffusion. Unfortunately, we could not locate the licensing terms for the source code. For the Masked Auto Encoder, we use the implementation available at https://github.com/pengzhiliang/MAE-pytorch, but we could not find the licensing terms for this source code. ", "page_idx": 14}, {"type": "text", "text": "We also employ VQ-Diffusion [10], available at https://github.com/cientgu/VQDiffusion/tree/main?tab $=$ readme-ov-file, which is licensed under Microsoft\u2019s Open Source Program. ", "page_idx": 14}, {"type": "text", "text": "For VQ-VAE, we use the implementation from T2M-GPT [35], which can be found at https://github.com/Mael-zys/T2M-GPT, and is licensed under the Apache License 2.0. ", "page_idx": 14}, {"type": "text", "text": "For 3D hand detection, we use the code of FrankMoCap [24]: https://github.com/facebookresearch/frankmocap, which is licensed under the CC BY-NC 4.0 license. We also used RTM-pose [14], which is available at https://github.com/open-mmlab/mmpose under the Apache License 2.0. ", "page_idx": 14}, {"type": "text", "text": "Data We use the Ego-Exo4D dataset [9] https://ego-exo4d-data.org, which is licensed under a custom (commercial or non-commercial) license. We also use AMASS [19] https://amass.is.tue.mpg.de, which is licensed under a custom (non-commercial scientific research) license. ", "page_idx": 14}, {"type": "text", "text": "D Preliminary ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Discrete Diffusion Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Discrete diffusion models [10] represent a category of diffusion models that progressively introduce noise into data while training to reverse this process. In contrast to continuous models, such as a latent diffusion model [23], which manipulate data in a continuous state space, discrete diffusion models operate within discrete state spaces. ", "page_idx": 14}, {"type": "text", "text": "VQ-VAE Vector Quantized-Variational Autoencoder (VQ-VAE) [31] is a generative model that extends the concept of Variational Autoencoders (VAEs) by incorporating discrete latent representations via vector quantization. The encoder $E(\\mathbf{x})$ compresses input data x into discrete latent vectors by mapping each encoded representation to the closest vector $\\mathbf{z}_{q}$ to the nearest codebook entry from a learned codebook of prototypes using the nearest-neighbor search: $\\begin{array}{r}{\\mathbf{z}_{q}\\!=\\!Q(\\mathbf{z})\\!=\\!\\operatorname*{argmin}_{\\mathbf{c}_{i}\\in\\mathcal{C}}||\\dot{\\mathbf{z}}-\\mathbf{c}_{i}||_{2}}\\end{array}$ . Here, $\\mathcal{C}=\\{\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{K}\\}$ , where $K$ is the total number of codebooks. The decoder $D(\\mathbf{z}_{q})$ reconstructs the input data $\\mathbf{x}$ from these quantized vectors, yielding a reconstructed output $\\hat{\\mathbf{x}}=D(\\mathbf{z}_{q})$ . The optimization process involves minimizing a combination of reconstruction loss and commitment loss. The reconstruction loss is expressed as $\\|\\mathbf{x}\\|-\\hat{\\mathbf{x}}\\|_{2}$ , while the commitment loss ensures the encoder commits to the nearest prototype in the codebook: $\\|\\mathbf{sg}[\\mathbf{z_{q}}]-\\mathbf{z}\\|_{2}$ , where sg is the stop-gradient operator. The overall loss function, which the VQ-VAE model minimizes, is: $\\mathcal{L}_{\\mathrm{VQ}}=\\|\\bar{\\mathbf{x}}-\\hat{\\mathbf{x}}\\|_{2}+\\|\\mathbf{z_{q}}-\\mathbf{sg}[\\mathbf{z}]\\|_{2}+\\lambda_{\\mathrm{VQ}}\\|\\mathbf{sg}[\\mathbf{z_{q}}]-\\mathbf{z}\\|_{2}$ . Here, $\\lambda_{\\mathrm{VQ}}$ is a coefficient for the commitment loss. ", "page_idx": 14}, {"type": "text", "text": "Forward Diffusion Process. Building on the foundation laid by the discrete diffusion models introduced by [27], VQ-Diffusion [10] refined the diffusion process with a mask-and-replace strategy. In VQ-Diffusion, during the forward diffusion process, tokens can either transition to other tokens or to a special $\\mathsf{\\Pi_{<M A S K>}}$ token. The transition probability from token $\\mathbf{z}^{i}$ to $\\mathbf{z}^{j}$ at diffusion step $t$ is defined by the matrix $\\mathbf{Q}_{t}[i,j]$ . The transition matrix $\\mathbf{Q}_{t}$ , structured in $\\mathbb{R}^{(K+1)\\times(K+1)}$ , follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{t}=\\left[\\frac{\\hat{\\mathbf{Q}}_{t}\\quad\\quad\\mathrm{~0~}}{\\gamma_{t}\\cdot\\mathbf{1}^{\\top}\\quad\\mathrm{~1~}}\\right],\\mathrm{where~}\\hat{\\mathbf{Q}}_{t}=\\alpha_{t}\\mathbf{I}+\\beta_{t}\\mathbf{1}\\mathbf{1}^{\\top}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $\\alpha_{t}$ adjusts to ensure conservation of probability, such that $\\alpha_{t}\\!=\\!1\\!-\\!K\\beta_{t}\\!-\\!\\gamma_{t}$ is the probability of transitioning between tokens, and $\\gamma_{t}$ governs transitions to the <MASK> token. The transition from step $t-1$ to $t$ is expressed as: $q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})=v^{\\top}(\\mathbf{z}_{t})Q_{t}v(\\mathbf{z}_{t-1})$ , where $\\mathbf{v}(\\mathbf{z}_{t})\\,\\in\\,\\mathbb{R}^{(K+1)\\times1}$ is an one-hot encoded vector representing the token index of $\\mathbf{z}_{t}$ . Using the Markov property, the probability of transitioning from any initial step 0 to step $t$ is $q(\\mathbf{z}_{t}|\\mathbf{z}_{0})=\\pmb{v}^{\\top}(\\mathbf{z}_{t})\\overline{{\\pmb{Q}}}_{t}\\pmb{v}(\\mathbf{z}_{0})$ , where $\\overline{{\\mathbf{Q}}}_{t}=\\mathbf{Q}_{t}\\mathbf{Q}_{t-1}\\cdot\\cdot\\cdot\\mathbf{Q}_{1}$ is the cumulative transition matrix. This defines the cumulative probabilities as $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i},\\bar{\\gamma}_{t}=1-\\prod_{i=1}^{t}(1-\\gamma_{i})}\\end{array}$ , and $\\bar{\\beta}_{t}=(1-\\alpha_{t}-\\gamma_{t})/K$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Conditional Denoising Process. In the conditional denoising process, a neural network denoted as $p_{\\theta}$ aims to predict the original, noiseless token $\\mathbf{z}_{\\mathrm{0}}$ given a corrupted token and the associated condition, such as a embedded hand trajectories. The posterior distribution for the discrete diffusion process can be defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{z}_{t-1}|\\mathbf{z}_{t},\\mathbf{z}_{0})=\\frac{q(\\mathbf{z}_{t-1}|\\mathbf{z}_{0})q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1},\\mathbf{z_{0}})}{q(\\mathbf{z}_{t}|\\mathbf{z}_{0})}}\\\\ &{\\qquad\\qquad=\\frac{\\left(v^{\\top}(\\mathbf{z}_{t})Q_{t}v(\\mathbf{z}_{t-1})\\right)\\left(v^{\\top}(\\mathbf{z}_{t-1})\\overline{{Q}}_{t-1}v(\\mathbf{z}_{0})\\right)}{v^{\\top}(\\mathbf{z}_{t})\\overline{{Q}}_{t}v(\\mathbf{z}_{0})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With this, the reverse transition distribution is determined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(\\mathbf{z}_{t-1}|\\mathbf{z}_{t},\\mathbf{y})=\\sum_{\\tilde{\\mathbf{z}}_{0}=1}^{K}\\!q(\\mathbf{z}_{t-1}|\\mathbf{z}_{t},\\tilde{\\mathbf{z}}_{0})p_{\\theta}(\\tilde{\\mathbf{z}}_{0}|\\mathbf{z}_{t},\\mathbf{y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the network iteratively denoises tokens from step $T$ down to 1, eventually generating the token $\\mathbf{z}_{\\mathrm{0}}$ conditioned on y. To train the network $p_{\\theta}$ , the training approach includes not only a denoising objective but also the standard variational lower bound objective [27], denoted as $\\mathcal{L}_{\\mathrm{vlb}}$ . The comprehensive training objective is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{vlb}}+\\lambda\\mathbb{E}_{\\mathbf{z}_{t}\\sim q(\\mathbf{z}_{t}|\\mathbf{z}_{0})}[-\\log p_{\\theta}(\\mathbf{z}_{0}|\\mathbf{z}_{t},\\mathbf{y})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\lambda$ is the coefficient for the denoising loss. ", "page_idx": 15}, {"type": "text", "text": "E Additional Experimental results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We illustrate additional uncertainty visualization on Fig. 5. In addition, we demonstrate the additional qualitative results with our method on Ego-Exo4D dataset and AMASS dataset in Fig. 6 and Fig. 7 & 8, respectively. ", "page_idx": 15}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/64b0b49ceb14cf39ac8ce8f8178517f66c4201ba1c860e058ba1865a3bac9b25.jpg", "img_caption": ["Frame "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 5: Additional uncertainty visualization of the right hand pose captured by the MAE. Gray areas represent frames where the hand is invisible, and white areas denote visible frames. We depict aleatoric uncertainty within ranges of $\\pm1\\sigma$ and $\\pm2\\sigma$ from the estimated $\\mu$ . ", "page_idx": 15}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/8eb90e2c62d2d5a99a0b734e75b7061a1ade8386ef87b958c232c07aaf8a5d6b.jpg", "img_caption": [], "img_footnote": ["Figure 6: Qualitative results showing the groundtruth in green and predicted human pose in blue using our method on Ego-Exo4D dataset. "], "page_idx": 16}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/80c09e1ff0288b62f76b832ea00685ecd0bee612aaeefa37ec921feff1164695.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 7: Qualitative results on AMASS dataset comparing DSPoser (Ours) against the baselines. Color gradient indicates an absolute positional error, with a higher error corresponding to higher blue intensity. Results demonstrate that motions generated by DSPoser exhibit greater similarity to the ground truth. Furthermore, it highlights higher errors (indicated with red circles) for baselines when the hand is occluded in the ground truth pose (indicated with a black circle). ", "page_idx": 17}, {"type": "image", "img_path": "MHCnLo2QeA/tmp/afdb7af3976186b2eaf862f59175b0e6f4a8f9447924402d0333eadfaba73a82.jpg", "img_caption": ["Figure 8: Qualitative results showing the groundtruth in Green and predicted human pose in blue using our method on AMASS dataset, with red indicating the visible hands. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The main claims accurately reflect this paper\u2019s empirical contributions. The introduction section discusses contributions made in the paper and important assumptions and limitations. Limitations of our work are described in Section 7. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Limitations of our work are described in Section 7. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Implementation details are explained in Section 3 and Section A.1 ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Code release is challenging due to our organization\u2019s policy. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We specified the experimental setting and details in Section A.1 and Section 4.1. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We report $95\\%$ confidence interval in the experimental tables in Section 4. The confidence interval was computed using closed form formula. The variability is due to random drawing of noise vectors in denoising diffusion and uncertainty guidance. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Compute resources used in this paper can be found in Section B of the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our research conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 22}, {"type": "text", "text": "Justification: Both potential positive societal impacts and negative societal impacts of the work are discussed in Section ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We believe this paper does not pose a high risk for misuse. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Citations and licenses of assets used in this paper can be found in Section C of the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]