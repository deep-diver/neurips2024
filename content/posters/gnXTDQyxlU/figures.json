[{"figure_path": "gnXTDQyxlU/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of PIVOT-R and other models. (a) Sequentially executed robot manipulation model. They sequentially execute each module in the model at each timestep to perform manipulation reasoning (e.g., RT-2 [64], RT-X [49], RT-H [5], VILA [20], Octo [36], etc.) or world modeling (e.g., Surfer [42], Daydreamer [56], 3D-VLA [60], etc.) This easily leads to model redundancy and weak key manipulation node prediction capabilities. (b) PIVOT-R is a primitive-driven waypoint-aware world model with asynchronous hierarchical executors. It only focuses on the prediction of waypoints related to the manipulation task, and it is easier to predict key nodes in the manipulation task than other methods. In addition, PIVOT-R sets different execution frequencies for different modules to have higher execution efficiency and lower redundancy.", "description": "This figure compares the architecture of PIVOT-R with traditional sequentially executed robot manipulation models.  Traditional models process visual input and instructions sequentially through multiple modules (VLM, world model, action head) at every timestep, leading to redundancy and difficulty in predicting key manipulation waypoints. In contrast, PIVOT-R uses an asynchronous hierarchical executor and focuses solely on predicting task-relevant waypoints, improving efficiency and reducing redundancy. The waypoints guide a lightweight action prediction module for low-level action decoding. ", "section": "1 Introduction"}, {"figure_path": "gnXTDQyxlU/figures/figures_1_2.jpg", "caption": "Figure 1: Comparison of PIVOT-R and other models. (a) Sequentially executed robot manipulation model. They sequentially execute each module in the model at each timestep to perform manipulation reasoning (e.g., RT-2 [64], RT-X [49], RT-H [5], VILA [20], Octo [36], etc.) or world modeling (e.g., Surfer [42], Daydreamer [56], 3D-VLA [60], etc.) This easily leads to model redundancy and weak key manipulation node prediction capabilities. (b) PIVOT-R is a primitive-driven waypoint-aware world model with asynchronous hierarchical executors. It only focuses on the prediction of waypoints related to the manipulation task, and it is easier to predict key nodes in the manipulation task than other methods. In addition, PIVOT-R sets different execution frequencies for different modules to have higher execution efficiency and lower redundancy.", "description": "This figure compares PIVOT-R with other robot manipulation models.  (a) shows a sequential model where modules are executed at every timestep, leading to redundancy and weak waypoint prediction. (b) illustrates PIVOT-R, which focuses on predicting task-relevant waypoints using an asynchronous hierarchical executor for improved efficiency and performance.", "section": "1 Introduction"}, {"figure_path": "gnXTDQyxlU/figures/figures_3_1.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure shows the overall architecture of PIVOT-R, highlighting its three main modules:  the asynchronous hierarchical executor (AHE), the waypoint-aware world model (WAWM), and the action prediction module.  The AHE coordinates the execution of the modules at different frequencies to optimize efficiency. The WAWM uses a pre-trained Vision-Language Model (VLM) for primitive action parsing and scene prediction, focusing on key waypoints in the manipulation task. The action prediction module then generates the low-level actions based on these waypoints.  The figure also provides a detailed breakdown of the scene and action prediction module architectures, illustrating the components used within each module (e.g., self-attention, cross-attention, feed-forward layers).", "section": "3 Architecture"}, {"figure_path": "gnXTDQyxlU/figures/figures_6_1.jpg", "caption": "Figure 3: Examples show the execution process of PIVOT-R. The text below the image describes the primitive actions to be performed next. Blue arrows indicate the direction of actions.", "description": "This figure demonstrates a step-by-step execution of a robotic manipulation task using the PIVOT-R model. Each image shows a different stage of the task, with text indicating the primitive action being performed at that stage. Blue arrows illustrate the movement direction. This visualization helps illustrate the model's ability to break down complex tasks into smaller, manageable primitive actions.", "section": "4.2 Benchmark and Baselines"}, {"figure_path": "gnXTDQyxlU/figures/figures_7_1.jpg", "caption": "Figure 4: We show demonstrations of real world evaluation. The first row is \"pick up the coke\", the second row is \"put the red bottle on the yellow block\", and the third row is \"push the object on the desk to the pink block\".", "description": "This figure presents three example scenarios from real-world robot manipulation experiments using PIVOT-R. Each row shows a different task: picking up a coke can, placing a red bottle on a yellow block, and pushing an object towards a pink block.  The images visually depict the robot's actions during the execution of these tasks, offering a qualitative assessment of the model's performance in real-world settings.", "section": "4.3 Results on Robotic Manipulation"}, {"figure_path": "gnXTDQyxlU/figures/figures_9_1.jpg", "caption": "Figure 5: Left: Example of retry and successful execution after a manipulation error occurred. Right: Example of retry still failing. After an object is knocked down and rolled a certain distance, it is difficult to successfully grab it again.", "description": "This figure shows three scenarios of robotic manipulation: a successful retry after an initial failure, a failed retry, and an unrecoverable failure. The successful retry illustrates the robot's ability to recover from minor errors, while the failed retry and unrecoverable failure highlight situations where the error is more significant and beyond the robot's recovery capabilities.", "section": "4.6 Failure and Retry"}, {"figure_path": "gnXTDQyxlU/figures/figures_15_1.jpg", "caption": "Figure 6: Scenes in SeaWave. The left column represents the first-perspective image, and the right column represents the third-perspective image. The scenes in the same row are the same.", "description": "This figure shows various scenes from the SeaWave benchmark dataset used in the paper.  The left column displays the scene from a first-person perspective (as if the robot were viewing it), while the right column offers a third-person perspective. Each row depicts the same scene, demonstrating different viewpoints of the simulation environment used for training and evaluating the robotic manipulation model. This helps showcase the visual complexity and variability in the environments the model has to handle.", "section": "4.2 Benchmark and Baselines"}, {"figure_path": "gnXTDQyxlU/figures/figures_16_1.jpg", "caption": "Figure 7: Feature analysis. The blue points D\u2081 represent the distance between F0t and FMt in the spatial dimension, and the red points D\u2082 represent the distance between FMt and FMt in the spatial dimension. We fit curves to these points and draw confidence intervals for better observation.", "description": "This figure shows a feature analysis that explores the spatial distance relationships between predicted features (FMt) and real-time observed features (F0t) relative to milestone features (FMt).  The L2 distance between F0t and FMt is shown to decrease as the task progresses, while the L2 distance between FMt and FMt shows smaller variances in long-term series analysis, indicating more stable predictions. This stability and convergence towards the milestones are highlighted as critical for the accuracy and reliability of model manipulation.", "section": "C.1 Feature Analysis"}, {"figure_path": "gnXTDQyxlU/figures/figures_17_1.jpg", "caption": "Figure 8: An example shows that when an out-of-distribution instruction is encountered, the VLM's reasoning ability is used to parse it into a learned task instruction, allowing the model to successfully complete the task.", "description": "This figure demonstrates an example where the model successfully completes a task despite receiving an instruction that is outside of its training data (out-of-distribution). The Vision-Language Model (VLM) successfully parses the instruction, transforming it into a learned task instruction, which enables the model to perform the task correctly.  This highlights the model's ability to generalize beyond its training data by leveraging the reasoning capabilities of the VLM.", "section": "C.2 Emergent Capabilities"}, {"figure_path": "gnXTDQyxlU/figures/figures_17_2.jpg", "caption": "Figure 9: An example shows that for an unseen task, based on the skills and actions that have been learned, PIVOT-R can break down the task and combine the actions to complete the task.", "description": "This figure shows an example of how PIVOT-R handles unseen tasks.  The instruction is to move a middle object to the left.  PIVOT-R decomposes this complex instruction into a sequence of learned sub-tasks (pick the middle object, push left, put down), which are then further broken down into primitive actions (close to, clamp, move up, push left, move down, unclamp). The images show the robot executing these actions sequentially to complete the task. This demonstrates the model's ability to generalize beyond its training data by combining known skills to solve novel problems.", "section": "C.2 Emergent Capabilities"}, {"figure_path": "gnXTDQyxlU/figures/figures_19_1.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure shows the overall architecture of the PIVOT-R model, which consists of a waypoint-aware world model (WAWM) and an action prediction module.  The WAWM uses a pre-trained vision-language model (VLM) for low-frequency primitive action parsing, providing waypoint indications for the scene prediction module. The scene prediction module then models world knowledge using waypoints and manipulation trajectories.  Finally, a lightweight action prediction module handles high-frequency action prediction and execution. The three modules operate asynchronously via an asynchronous hierarchical executor (AHE).", "section": "3 Architecture"}, {"figure_path": "gnXTDQyxlU/figures/figures_20_1.jpg", "caption": "Figure 10: Rollouts on multi-level tasks of the SeaWave benchmark.", "description": "This figure shows example rollouts of the PIVOT-R model on four different levels of tasks from the SeaWave benchmark. Each row represents a different level of complexity in the instructions, with Level 1 being the simplest and Level 4 being the most complex.  The images depict the robot's actions in attempting to complete the tasks based on the given natural language instructions.  The figure visually demonstrates the model's ability to handle various levels of instruction complexity.", "section": "More Results"}, {"figure_path": "gnXTDQyxlU/figures/figures_20_2.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure provides a detailed overview of the PIVOT-R architecture. It shows how the three main modules (primitive action parsing, scene prediction, and action prediction) work together asynchronously using an asynchronous hierarchical executor. The waypoint-aware world model uses a pre-trained vision-language model (VLM) for primitive action parsing and waypoint prediction to guide the scene and action prediction modules.", "section": "3 Architecture"}, {"figure_path": "gnXTDQyxlU/figures/figures_21_1.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure shows the overall architecture of PIVOT-R, a primitive-driven waypoint-aware world model for robotic manipulation.  It highlights the three main modules: a Waypoint-Aware World Model (WAWM), which uses a pre-trained Vision-Language Model (VLM) for primitive action parsing and scene prediction; an action prediction module for low-level action decoding; and an Asynchronous Hierarchical Executor (AHE) that manages the execution frequency of each module to optimize efficiency. The WAWM focuses on predicting task-relevant waypoints, which are then used by the action prediction module to generate the final robot actions.", "section": "3 Architecture"}, {"figure_path": "gnXTDQyxlU/figures/figures_23_1.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure shows the overall architecture of PIVOT-R, a primitive-driven waypoint-aware world model for robotic manipulation.  It highlights the three main modules: the Waypoint-Aware World Model (WAWM), the scene prediction module, and the action prediction module. The asynchronous hierarchical executor (AHE) coordinates the interaction between the modules, allowing them to operate at different execution frequencies for improved efficiency. WAWM utilizes a pretrained Vision-Language Model (VLM) to parse user instructions and predict waypoints which guide the scene and action prediction modules. ", "section": "3 Architecture"}, {"figure_path": "gnXTDQyxlU/figures/figures_23_2.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure presents a detailed overview of the PIVOT-R architecture, highlighting its key components: a Waypoint-Aware World Model (WAWM) and an Action Prediction module.  It illustrates how these modules interact asynchronously through a hierarchical executor (AHE). The WAWM uses a pre-trained Vision-Language Model (VLM) for primitive action parsing, providing waypoint cues to a scene prediction module that models world knowledge.  Finally, a lightweight action prediction module generates actions.", "section": "3 Architecture"}, {"figure_path": "gnXTDQyxlU/figures/figures_24_1.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure provides a detailed overview of the proposed PIVOT-R architecture, illustrating the interaction between its key components: the Waypoint-Aware World Model (WAWM) and the Action Prediction Module.  The diagram highlights the asynchronous nature of the system, with different modules operating at varying frequencies. The WAWM uses a pre-trained Vision-Language Model (VLM) for primitive action parsing and scene prediction guided by waypoints, while the Action Prediction Module focuses on high-frequency action prediction and execution.  Overall, the figure depicts a hierarchical, efficient system designed for robotic manipulation.", "section": "3 Architecture"}, {"figure_path": "gnXTDQyxlU/figures/figures_24_2.jpg", "caption": "Figure 2: PIVOT-R overview. It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction module. Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.", "description": "This figure shows the overall architecture of the PIVOT-R model, which consists of three main modules: a Waypoint-Aware World Model (WAWM), a scene prediction module, and an action prediction module.  These modules work asynchronously, with different frequencies, to improve efficiency. The WAWM parses user instructions using a pre-trained vision-language model (VLM) to identify key waypoints in the manipulation task, which are then used by the scene prediction module to predict the scene. Finally, the action prediction module uses these predictions to generate low-level actions for the robot.", "section": "3 Architecture"}]