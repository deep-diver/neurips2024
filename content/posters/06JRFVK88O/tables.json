[{"figure_path": "06JRFVK88O/tables/tables_7_1.jpg", "caption": "Table 1: Win-rates (percentage).", "description": "This table presents the win rates achieved by different multi-agent reinforcement learning algorithms across various scenarios in three benchmark environments: SMACv2, Google Research Football (GRF), and Gold Miner.  The algorithms compared include MAPPO, IPPO, QMIX, QPLEX, Sup-MAPPO, IMAX-PPO (GAIL), and IMAX-PPO (InQ).  The scenarios vary in terms of the number of agents involved and the complexity of the tasks. The table allows for a comparison of the performance of the proposed IMAX-PPO algorithm against state-of-the-art baselines.", "section": "6 Experiments"}, {"figure_path": "06JRFVK88O/tables/tables_19_1.jpg", "caption": "Table 1: Win-rates (percentage).", "description": "This table presents the win rates achieved by different multi-agent reinforcement learning algorithms across various tasks and scenarios.  The algorithms compared include MAPPO, IPPO, QMIX, QPLEX, Sup-MAPPO, IMAX-PPO (GAIL), and IMAX-PPO (InQ). The tasks are categorized into SMAC (with sub-categories for Protoss, Terran, and Zerg agents), Gold Miner, and GRF. Each task has multiple scenarios representing different numbers of agents and game complexities. The win rates are percentages, showing the success rate of each algorithm in each scenario.", "section": "6 Experiments"}, {"figure_path": "06JRFVK88O/tables/tables_20_1.jpg", "caption": "Table 1: Win-rates (percentage).", "description": "This table presents the win rates achieved by different multi-agent reinforcement learning algorithms across various scenarios in three game environments: SMACv2 (StarCraft Multi-Agent Challenge), GRF (Google Research Football), and Gold Miner.  The algorithms compared include MAPPO, IPPO, QMIX, QPLEX, Sup-MAPPO (MAPPO with supervised learning for opponent prediction), IMAX-PPO (GAIL) (our algorithm using Generative Adversarial Imitation Learning for opponent modeling), and IMAX-PPO (InQ) (our algorithm using Inverse Soft-Q Learning for opponent modeling).  Different scenarios within each game environment are tested, varying the number of agents (e.g., 5 vs 5, 10 vs 10) and the race (Protoss, Terran, Zerg for SMACv2).  The table showcases the superior performance of the IMAX-PPO algorithms, particularly IMAX-PPO (InQ), compared to the baselines across most scenarios. ", "section": "6 Experiments"}]