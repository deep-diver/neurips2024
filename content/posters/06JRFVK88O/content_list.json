[{"type": "text", "text": "Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Viet Bui Singapore Management University, Singapore theviet.bui.2023@phdcs.smu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Tien Mai Singapore Management University, Singapore atmai@smu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Thanh Hong Nguyen University of Oregon Eugene, Oregon, United States thanhhng@cs.uoregon.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training agents in multi-agent games presents significant challenges due to their intricate nature. These challenges are exacerbated by dynamics influenced not only by the environment but also by strategies of opponents. Existing methods often struggle with slow convergence and instability. To address these challenges, we harness the potential of imitation learning (IL) to comprehend and anticipate actions of the opponents, aiming to mitigate uncertainties with respect to the game dynamics. Our key contributions include: (i) a new multi-agent IL model for predicting next moves of the opponents \u2014 our model works with hidden actions of opponents and local observations; (ii) a new multi-agent reinforcement learning (MARL) algorithm that combines our IL model and policy training into one single training process; and (iii) extensive experiments in three challenging game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2). Experimental results show that our approach achieves superior performance compared to state-of-the-art MARL algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent works in MARL have made a significant progress in developing new effective algorithms that can perform well in complex multi-agent environments including SMAC [34, 25]. Among these works, centralized training and decentralized execution (CTDE) [5] has attracted a great attention from the RL community due to its advantage of leveraging global information to train a centralized critic (i.e., actor-critic methods [18]) or a joint Q-function (i.e., value-decomposition methods [23, 29]). This approach enables a more efficient and stable learning process while allowing agents to act in a decentralized manner. Under this CTDE framework, off-policy methods such as MADDPG [18] and QMIX [23] have become very popular due to their data efficiency and state-of-the-art (SOTA) results on a wide range of benchmarks. On the other hand, on-policy gradient methods have been under-explored in MARL due to their data consuming and difficulty in transferring knowledge from single-agent to multi-agent settings. However, a recent work shows that on-policy methods (such as MAPPO, a multi-agent version of proximal policy optimization) outperforms all other SOTA methods including MADDPG and QMIX in various multi-agent benchmarks, and especially works well in complex SMAC settings [34]. Motivated by this promising result, we focus on improving the performance of policy gradient methods in MARL. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We consider a partially observable MDP environment in which there are agents attempting to form an alliance to play against a team of opponents, where allied agents have to make decision independently without communicating with other members. We aim to enhance the performance of PPO in MARL with the introduction of a novel opponent-imitation component. This new component is then integrated into the MAPPO framework to enhance the policy learning of allied agents. A key challenge in our problem setting is that allied agents are unaware of actions taken by their opponents. In addition, each allied agent only has local observations of opponents locating in the current neighborhood of the agent \u2014 the locations and neighborhoods of all players are changing over time depending on actions taken by players and the dynamics of the environment. Lastly, learning to imitate opponents occurs during the policy learning process of the allied agents. The inter-dependency between these two learning components makes the entire learning process significantly challenging. ", "page_idx": 1}, {"type": "text", "text": "We address these challenges while providing the following key contributions. First, we convert the problem of imitating the opponent policy into predicting their next states. The outcome of this next state prediction is an indirect implication of the opponent policy. We then cast the problem of opponent next-state prediction as a new multi-agent imitation learning (IL) problem. We propose a new multi-agent IL algorithm, which is an adaptation of IQ-Learn [9] (a SOTA IL algorithm), that only considers local opponent-state-only observations. Especially, instead of imitating the opponents\u2019 policy, our IL algorithm targets the prediction of next states of the neighboring opponents. Second, we provide a comprehensive theoretical analysis which provides bounds on the impact of the changing policy of the allied agents (as a result of the policy learning process) on our IL outcomes. ", "page_idx": 1}, {"type": "text", "text": "Third, we present a unified MARL algorithmic framework in which we incorporate our IL component into MAPPO. Our idea is to combine each allied agent\u2019s local observations with the next-state prediction of neighboring opponents of that agent, creating an augmented input based on which to improve the decision making of the allied agent at every state. This novel integration results in a new MARL algorithm, which we name Imitation-enhanced Multi-Agent EXtended PPO (IMAX-PPO). ", "page_idx": 1}, {"type": "text", "text": "Finally, we conduct extensive experiments in several benchmarks ranging from complex to simple ones, including: SMACv2 (an advanced version of the Star-Craft multi-agent challenge) [4], Google research football (GRF) [15], and Gold Miner [7]. Our empirical results show that our new algorithm consistently outperforms SOTA algorithms significantly accross all these benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "MARL. The literature on MARL includes both centralized and decentralized algorithms. While centralized algorithms [2] learn a single joint policy to produce joint actions of all the agents, decentralized learning [17] optimizes each agent\u2019s local policy independently. There are also algorithms based on centralized training and decentralized execution (CTDE). For example, methods in [18, 5] adopt actor-critic structures and learn a centralized critic that takes global information as input. Value-decomposition (VD) is a class of methods that represent the joint Q-function as a function of agents\u2019 local Q-functions [29, 23]. Alternatively, the use of policy-gradient methods, such as PPO [26], has also been investigated in multi-agent RL. For example, [3] propose independent PPO (IPPO), a decentralized MARL, that can achieve high success rates in several hard SMAC maps. IPPO is, however, overall worse than QMIX [23], a method based on factorizing Q function to facilitate CTDE. Later methods based on factorized Q-learning include QTRAN [27] and QPLEX [30], where QPLEX has been shown to achive better perforamance than QMIX and QTRAN. Recently, [34] develop MAPPO, a PPO-based MARL algorithm that outperforms QMIX and QPLEX on some popular multi-agent environments such as SMAC [25, 4] and GRF [15]. To the best of our knowledge, MAPPO is currently a SOTA method for MARL. Our work integrates a new opponent imitation model into MAPPO, resulting in a new MARL algorithm that outperforms SOTA methods on various challenging game tasks. ", "page_idx": 1}, {"type": "text", "text": "Imitation Learning $(\\mathbf{IL})$ . In this study, we employ IL to anticipate the opponents\u2019 moves. IL is known as a compelling approach for sequential decision-making [20, 1]. In IL, a collection of expert trajectories is provided, with the objective of learning a policy that emulates behavior similar to the expert\u2019s policy. One of the simplest IL methods is Behavioral Cloning (BC), which aims to maximize the likelihood of the expert\u2019s actions under the learned policy. BC disregards environmental dynamics, rendering it suitable only for uncomplicated environments. Several advanced IL techniques, encompassing environmental dynamics, have been proposed [24, 8, 12]. While these methods operate in complex and continuous domains, they involve adversarial learning, making them prone to instability and sensitivity to hyperparameters. The IQ-learn [9] stands as a cutting-edge IL algorithm with distinct advantages, specifically its incorporation of dynamics awareness and non-adversarial training. It\u2019s important to note that all the aforementioned IL methods were designed for single-agent RL. In contrast, the literature on multi-agent RL is limited, with only a handful of studies addressing IL in multi-agent RL. For instance, [28] presents an adversarial training-based algorithm, named Multi-agent Generative Adversarial IL. It\u2019s worth noting that all the IL algorithms mentioned above are established on the premise that expert (aka. opponent in our case) actions are either observable or can be accessed via sampling, which implies that no existing algorithm can be directly applied to our multi-agent game settings with local state-only observations. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Opponent Modeling. Many existing works in MARL attempt to capture the learning process of opponents and incorporate it into the learning of the agent\u2019s policy [6, 14, 33]. For example, the LOLA algorithm [6] considers the impact of one agent\u2019s policy on the parameter update of other opponents while Meta-MAPG [14] combines LOLA with meta-learning, accounting for continuous adaptation. Another important line of research on opponent modelling follows hierarchical reasoning, considering each agent holds a belief about the other agents according to varying levels of reasoning ability [32, 31, 35, 19]. As an example, in [32], they introduce a probabilistic recursive reasoning framework in which variational Bayes methods are used to approximate the opponents\u2019 conditional policies. Depart from these two lines of research, there are many other works that attempt to learn a representation for the opponent\u2019s policy or to consider the prediction of opponents\u2019 actions as an an auxiliary task that can be trained simultaneously with the RL part [11, 13, 22, 21, 10]. For example, [21] use variational encoder to model the opponents\u2019 fixed policies while [10] apply behavioral cloning together with agent identification to learn a hybrid generative-discriminative representation for the opponents\u2019 policy. All the aforementioned related works require having access to opponent\u2019s observations and actions during training and/or execution. In our work, on the other hand, the opponent modeling can be only trained based on local observations of each agent in the allied team. These local observations contain limited information about current states of nearby opponents while opponents\u2019 actions are unobservable. As a result, existing methods on opponent modeling are not applicable in our multi-agent setting. ", "page_idx": 2}, {"type": "text", "text": "3 Multi-Agent POMDP Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a multi-player Markov game in which there are multiple agents forming an alliance to play against some opponent agents. We present the Markov game as a tuple $\\{S,\\bar{\\mathcal{N}}_{\\alpha},\\bar{\\mathcal{N}}_{e},\\mathcal{A}^{\\alpha},\\bar{\\mathcal{A}}^{e},\\bar{P_{\\nu}}\\bar{R}\\}$ , where $\\boldsymbol{S}$ is the set of global states shared by all the agents, $\\mathcal{N}_{\\alpha}$ and $\\mathcal{N}_{e}$ are the set of ally and enemy agents, $\\begin{array}{r}{\\mathcal{A}^{\\alpha}=\\prod_{i\\in\\mathcal{N}_{A}}\\mathcal{A}_{i}^{\\alpha}}\\end{array}$ is the set of joint actions and $\\begin{array}{r}{\\mathcal{A}^{e}=\\prod_{j\\in\\mathcal{N}_{e}}\\mathcal{A}_{j}^{e}}\\end{array}$ is the set of joint actions of all the ally agents, $P$ is the transition dynamics of the game environment, and $R$ is a reward function that takes inputs as states and actions of all agents and returns the corresponding rewards. At each time step where the global state is $S$ , each ally agent $i\\in\\mathcal N_{\\alpha}$ makes an action $a_{i}^{\\alpha}$ according to a policy $\\pi_{i}^{\\bar{\\alpha}}(a_{i}^{\\alpha}|o_{i}^{\\alpha})$ , where $o_{i}^{\\alpha}$ is the observation of ally agent $i$ given state $S$ . The joint action of allied agents can be now defined as $A^{\\alpha}=\\{a_{i}^{\\alpha}|\\;i\\in\\mathcal{N}_{\\alpha}\\}$ , and the joint policy is defined accordingly: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi^{\\alpha}(A^{\\alpha}|S)=\\prod_{i\\in\\mathcal{N}^{\\alpha}}\\pi_{i}^{\\alpha}(a_{i}^{\\alpha}|o_{i}^{\\alpha}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The enemy agents, at the same time, make a joint action $A^{e}=\\{a_{j}^{e}|\\;j\\in\\mathcal{N}_{e}\\}$ with the probability: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi^{e}(A^{e}|S)=\\prod_{j\\in\\mathcal{N}^{e}}\\pi_{j}^{e}(a_{j}^{e}|o_{j}^{e}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "After all agents make decisions, the global state transits to a new state $S^{\\prime}$ with the probability $P(S^{\\prime}|A^{e},\\bar{A}^{\\alpha},S)$ . In our setting, the enemies\u2019 policies $\\Pi^{e}$ are fixed and thus can be treated as a part of the environment dynamics, as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(S^{\\prime}|A^{\\alpha},S)=\\sum_{A^{e}}\\Pi(A^{e}|S)P(S^{\\prime}|A^{e},A^{\\alpha},S)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Our goal is to find a policy that optimizes the allies\u2019 expected joint reward, formulated as follows:1 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{max}_{\\Pi^{\\alpha}}\\operatorname{\\mathbb{E}}_{(A^{\\alpha},S)\\sim\\Pi^{\\alpha}}\\left[R^{\\alpha}(S,A^{\\alpha})\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The game dynamics involve both the environment dynamics and the joint policy of enemies, making the training costly to converge. We aim to migrate uncertainties associated with these game dynamics by first predicting the opponent policy based on the allies\u2019 past observations and leveraging this prediction into guiding the policy training for the allies. ", "page_idx": 3}, {"type": "text", "text": "4 Opponent Policy Imitation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The key challenge in our problem is that actions taken by opponents are hidden from allied agents. Moreover, each allied agent has limited observations of other agents; they can only obtain information about nearby opponents. For example, in the SMAC environment, for each allied agent, besides information about the agent itself, the allied agent is also aware of the relative position and health point, etc. of the neighboring opponents.2 Therefore, instead of directly predicting opponents\u2019 next moves, we focus on anticipating next states of opponents \u2014 this next-state prediction can be used as an implication of what actions have been taken by the neighboring opponents. Our key contributions include: (i) a novel representation of the opponent next-state prediction in the form of multi-agent IL; (ii) a new adaptation of IQ-Learn to solve our new IL problem; (iii) a comprehensive theoretical analysis on the influence of policy learning of allied agents on the next-state prediction outcomes; and (iv) a practical multi-agent IL algorithm which is tailored to local observations of allied agents. ", "page_idx": 3}, {"type": "text", "text": "Here, it is important to note that prior works on IL with state-only observations all assume that actions are not available in the expert demonstrations but can be accessed via sampling, which is not the case in our context. Alternatively, one could apply standard supervised learning for this opponent-next-state prediction task. However, a well-known drawback of this approach is that it disregards environment dynamics and often struggles with distribution shifts [16]. As shown later in our experiments, our IL approach significantly outperforms this supervised-learning approach. ", "page_idx": 3}, {"type": "text", "text": "4.1 Multi-Agent IL with Unobservable Actions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present our $\\mathrm{IL}$ formulation and our adaptation of IQ-Learn for solving our new IL problem.   \nFor the sake of theoretical analysis, this section focuses on IL with global state-only observations.   \nWe then introduce a new practical algorithm later which addresses local observations of allied agents. ", "page_idx": 3}, {"type": "text", "text": "Opponent Next-State Prediction as an IL. To formulate the problem as an $\\mathrm{IL}$ that accounts for the action-unobservable issue, we introduce a new notion of the \u201cexpert\u201d state in our $\\mathrm{IL}$ problem as a pair $W=(S,A_{-}^{\\alpha})$ which comprises of the original state $S$ and the joint action of the allies $A_{-}^{\\alpha}$ taken in the previous step that leads to state $S$ . The action space of the \u201cexpert\u201d is equivalent to the original state space $\\boldsymbol{S}$ . We then introduce a new notion of a reward function for the expert as $R^{e}(W,S^{\\prime})$ . This action $(S^{\\prime})$ of the expert is basically a resulting state of joint actions of the allies $A^{\\alpha}$ and hidden joint actions of the enemies $A^{e}$ taken at state $S$ . Altogether with the reward function $R^{e}(W,S^{\\prime})$ , we introduce a new notion of joint policy for the expert, $\\Pi^{e}(S^{\\prime}|W)$ (or $\\Pi^{e}(S^{\\prime}|S,A_{-}^{\\alpha}))$ , which is essentially the probability of ending up at a global state $S^{\\prime}$ from state $S$ . The dynamics in this $\\mathrm{IL}$ setting becomes $P(W^{\\prime}\\,|\\,W,S^{\\prime})=P((S^{\\prime},A)\\,|\\,(S,A_{-}^{\\alpha}),S^{\\prime})=\\Pi^{\\alpha}(A\\,|\\,S)$ (which is the allies\u2019 policy) where $W^{\\prime}=(S^{\\prime},\\dot{A})$ and $A$ is the action taken by the allies at state $S$ . ", "page_idx": 3}, {"type": "text", "text": "Let $\\begin{array}{r}{\\Pi=\\{\\Pi:S\\times S\\times A^{\\alpha}\\to[0,1],\\ \\sum_{S^{\\prime}\\in S}\\Pi(S^{\\prime}|S,A_{-}^{\\alpha})=1,\\ \\forall S,S^{\\prime}\\in S,A^{\\alpha}\\in A^{\\alpha}\\}}\\end{array}$ be the support set of the imitating policy. We  now introduce the maximum-entropy inverse RL framework [12] w.r.t the new notions of the expert\u2019s reward and policy $(R^{e}(S^{\\prime}|W),\\Pi^{\\bar{e}}(S^{\\prime}|W))$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{R^{\\epsilon}}\\operatorname*{min}_{\\Pi\\in\\Pi}\\Big\\{L(\\Pi,R^{e})=\\mathbb{E}_{\\rho^{e,\\alpha}}[R^{e}(W,S^{\\prime})-\\mathbb{E}_{\\rho^{\\Pi,\\alpha}}[R^{e}(W,S^{\\prime})]+\\mathbb{E}_{\\rho^{\\alpha,\\Pi}}[\\ln\\Pi(S^{\\prime}|W)]\\Big\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\rho^{e,\\alpha}$ is the occupancy measure of $(W,S^{\\prime})$ given by the expert policy $\\Pi^{e}$ and the ally joint policy $\\Pi^{\\alpha}$ , and $\\rho^{\\Pi,\\alpha}$ the occupancy measure of $(W,S^{\\prime})$ given by the imitation and ally policies. In particular, $\\rho^{e,\\alpha}$ can be computed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho^{e,\\alpha}(S^{\\prime},W)=(1-\\gamma)\\Pi^{e}(S^{\\prime}\\,|\\,W){\\prod}_{t=0}^{\\infty}\\gamma^{t}P(W_{t}=W\\,|\\,\\Pi^{e},\\Pi^{\\alpha})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "An Adaptation of IQ-Learn. Drawing inspiration from the SOTA $\\mathrm{IL}$ algorithm, IQ-learn, we construct our IL algorithm which is an adaptation of IQ-Learn tailored to our multi-agent environment. ", "page_idx": 3}, {"type": "text", "text": "The main idea of IQ-learn is to convert a reward learning problem into a Q-function learning one. To apply IQ-Learn to our setting, we present the following new soft and inverse soft Bellman operators, which is as adaption from the original ones introduced in [9]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}_{Q^{e}}^{\\Pi,R^{e}}(W,S^{\\prime})=R^{e}(W,S^{\\prime})+\\gamma\\mathbb{E}_{W^{\\prime}}[V_{\\Pi}^{e}(W^{\\prime})]}\\\\ {\\mathcal{T}_{Q^{e}}^{\\Pi}(W,S^{\\prime})=Q^{e}(W,S^{\\prime})-\\gamma\\mathbb{E}_{W^{\\prime}}[V_{\\Pi}^{e}(W^{\\prime})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the state value function is computed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{\\Pi}^{e}(W)\\!=\\!\\mathbb{E}_{S^{\\prime}\\sim\\Pi}\\bigl[Q^{e}(W,S^{\\prime})-\\ln(\\Pi(S^{\\prime}|W))\\bigr]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "First,e it is clear that $B_{Q^{e}}^{\\Pi,R^{e}}$ is contractive, thus defining a unique fixed point solution $Q^{*}$ such that $B_{Q^{*}}^{\\Pi,R^{e}}=Q^{*}$ . Let us further define the following function of $\\Pi$ and $Q^{e}$ for the Q-function learning: ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ(\\Pi,Q^{e})=\\mathbb{E}_{\\rho^{e,\\alpha}}[\\mathcal{T}_{Q^{e}}^{\\Pi}(W,S^{\\prime})]-\\mathbb{E}_{\\rho^{\\Pi,\\alpha}}[\\mathcal{T}_{Q^{e}}^{\\Pi}(W,S^{\\prime})]+\\mathbb{E}_{\\rho^{\\Pi,\\alpha}}[\\ln\\Pi(S^{\\prime}|W)]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We obtain a theoretical result on a connection between the learning reward and learning Q-functions:3 Proposition 1. For any reward function $R^{e}$ , let $Q^{*}$ be the unique fixed point solution to the soft Bellman equation $B_{Q^{*}}^{\\Pi,R^{e}}=Q^{*}$ , then: ${\\cal L}(\\Pi,R^{e})=J(\\Pi,Q^{*})$ , and for any $Q^{e}$ , $J(\\Pi,Q^{e})={\\cal L}(\\Pi,T_{Q^{e}}^{\\Pi})$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition (1) indicates that the $\\mathrm{IL}$ problem in (2) is equivalently to the $\\mathrm{^Q}$ -value-based $\\mathrm{IL}$ problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{max}_{Q^{e}}\\,\\mathrm{min}_{\\Pi}\\,J(\\Pi,Q^{e})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Suppose $Q^{*}$ is a solution to (5), then rewards can be recovered by taking $R^{e}(W,S^{\\prime})=Q^{*}(W,S^{\\prime})\\;-$ $\\gamma\\mathbb{E}_{W^{\\prime}}[V_{\\Pi}^{e}(W^{\\prime})]$ . Under this viewpoint, Prop. 2 shows that key properties of original IQ-learn still hold in our multi-agent setting with missing observations, making our $\\mathrm{IL}$ algorithm convenient to use. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. The IL problem (5) is equivalent to the maximization $\\operatorname*{max}_{Q^{e}}J(\\Pi^{Q},Q^{e})$ where the imitation policy can be computed based on $Q^{e}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Pi^{Q}(S^{\\prime}|W)=\\frac{\\exp(Q^{e}(W,S^{\\prime}))}{\\sum_{S^{\\prime\\prime}}\\exp(Q^{e}(W,S^{\\prime\\prime}))}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, the function $J(\\Pi^{Q},Q^{e})$ is concave in $Q^{e}$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Affect of Allies\u2019 Policies on Imitation Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The above $\\mathrm{IL}$ algorithm is trained during the training of the allies\u2019 policies, which means the dynamics $P(W^{\\prime}|\\bar{W},S^{\\prime})=\\Pi^{\\alpha}(A^{\\alpha}|S)$ change during the $\\mathrm{IL}$ process. Therefore, we aim to analyze the impact of these changes on the imitation policy. According to Proposition 2, given $Q^{e}$ , we can compute corresponding optimal imitation policy as $\\mathrm{i}\\mathrm{I}^{Q}$ . Therefore, the value function $V_{\\Pi}^{e}(W)$ can be alternatively write as $V_{Q}^{e}(W)$ . We now can denote the loss function of the imitation model (Eq. 8) as a function of the allies\u2019 joint policy explicitly: $\\Phi(\\Pi^{\\alpha}|Q^{e})\\equiv J(\\Pi,Q^{e})$ . ", "page_idx": 4}, {"type": "text", "text": "We first present our results about bounds on the impact of the allies\u2019 changing policies on the IL loss function (Prop. 3). Based on these results, we then provide a bound on the $\\mathrm{IL}$ learning outcome accordingly (Corollary 4). Let us denote by $\\overline{{Q}}=\\operatorname*{max}_{(W,S^{\\prime})}Q^{e}(W,S^{\\prime})$ an upper bound of $Q^{e}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. Given two allies\u2019 joint policies $\\Pi^{\\alpha}$ and $\\widetilde{\\Pi}^{\\alpha}$ such that $\\mathrm{KL}\\big(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde\\Pi^{\\alpha}(\\cdot|S)\\big)\\leq\\epsilon\\,f\\!o$ r any state $S\\in S$ , the following inequality holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left|\\Phi(\\Pi^{\\alpha}|Q^{e})-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e})\\right|\\leq\\left(\\alpha\\overline{{Q}}+\\beta\\ln|S|\\right)\\sqrt{2\\ln2\\epsilon}}}\\\\ {{\\left(\\frac{\\gamma}{(1-\\gamma)^{2}}+\\frac{\\gamma^{2}}{1-\\gamma}+(3-\\gamma),\\,a n d\\,\\beta=\\frac{\\gamma^{2}}{(1-\\gamma)}+3-\\gamma.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)$ be the objective of IL $J(\\Pi,Q^{e})$ , written as a function of the allies\u2019 joint policy $\\Pi^{\\alpha}$ . The following proposition establishes a bound for the variation of $|\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e},\\Pi)|$ as function of $\\bar{\\mathrm{KL}}(\\bar{\\Pi^{\\alpha}}||\\widetilde{\\Pi^{\\alpha}})$ , for any pair of allies\u2019 joint policies $(\\Pi^{\\alpha},\\widetilde{\\Pi}^{\\alpha})$ . ", "page_idx": 4}, {"type": "text", "text": "Prop. 3 allows us to establish an upper bound for the $\\mathrm{IL}$ when the allies\u2019 joint policy changes. ", "page_idx": 4}, {"type": "image", "img_path": "06JRFVK88O/tmp/e22a2a874d5c41433757850fe45e60c58e96458b5a444b3742b90dfaa1f4f484.jpg", "img_caption": ["Figure 1: An overview of our IMAX-PPO algorithm. Each local observation $o_{i}^{\\alpha}$ of an ally agent $i$ includes information about itself, as well as enemy and ally agents in its neighborhood (which changes over time). The output of the $\\mathrm{IL}$ component is the predicted next states of neighboring enemy agents (predictions for the non-neighbor enemies will be masked out). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Corollary 4. Given two allies\u2019 policies $\\Pi^{\\alpha}$ and $\\widetilde{\\Pi}^{\\alpha}$ with $\\mathrm{KL}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde\\Pi^{\\alpha}(\\cdot|S)\\leq\\epsilon,\\,\\forall S\\!\\in\\!S,$ , then: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\operatorname*{max}_{Q^{e}}\\big\\{\\Phi(\\Pi^{\\alpha}|Q^{e})\\big\\}-\\operatorname*{max}_{Q^{e}}\\big\\{\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e})\\big\\}\\Big|\\leq\\mathcal{O}(\\sqrt{\\epsilon})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the allies\u2019 joint policy $\\Pi^{\\alpha}$ will be changing during our policy learning process, the above result implies that the imitating policy will be stable if $\\Pi^{\\alpha}$ becomes stable, and if $\\Pi^{\\alpha}$ is converging to a target policy $\\Pi^{\\alpha*}$ , then the imitator\u2019s policy also converges to the one that is trained with the target ally policy with a rate of $\\sqrt{\\mathrm{KL}(\\Pi^{\\alpha}||\\widetilde\\Pi^{\\alpha})}$ . That is, if the actual policy is within a $O(\\epsilon)$ neighborhood of the\u221a target policy (i.e., $\\mathrm{KL}(\\Pi^{\\alpha}||\\widetilde{\\Pi}^{\\alpha})\\leq\\epsilon)$ then the expected return of the imitating policy is within a $\\mathcal{O}(\\sqrt{\\epsilon})$ neighborhood of the desired \u201cexpected return\u201d given by the target policy. ", "page_idx": 5}, {"type": "text", "text": "5 IMAX-PPO: Imitation-enhanced Multi-Agent EXtended PPO Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present our MARL algorithm for the competive game setting. We first focus on a practical implementation of an IL algorithm taking into account local observations. We then show how to integrate this into our MARL algorithm. We call our algorithm as IMAX-PPO, standing for Imitation-enhanced Multi-Agent EXtended PPO algorithm. ", "page_idx": 5}, {"type": "text", "text": "5.1 Imitation Learning with Local Observations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In previous section, we present our new $\\mathrm{IL}$ algorithm (which is an adaptation of IQ-Learn) to learn an expert policy $\\widehat{\\Pi}^{e}(S^{\\prime}|\\dot{W})=\\widehat{\\Pi}^{e}(S^{\\prime}|S,A^{\\alpha})$ that behaves similarly to the probabilities of ending up at state $S^{\\prime}$ wh en the current  global state is $S$ and the allies\u2019 joint action is $A^{\\alpha}$ . From the allies\u2019 perspective, to run this IL algorithm, it requires the allies to have access to the global state $S$ . However, each ally agent $i$ can only observe local states of its neighboring enemies (such as their locations, speeds, etc.). Therefore, we adapt our $\\mathrm{IL}$ algorithm in accordance with such local information. The goal is to predict next states of enemies in the neighborhood of each allied agent $i$ , denoted by $\\widehat{\\Pi}^{e}(S_{i}^{e,\\mathrm{next}}|w_{i}^{\\alpha})$ , where local information $w_{i}^{\\alpha}=(o_{i}^{\\alpha},a_{i,-}^{\\alpha})$ with $o_{i}^{\\alpha}$ is an observation vector of agent $i$ , containing the local states of the agent $i$ itself and of all the agents in the neighborhood that are observable by agent $i$ . In particular, given that $s_{i_{k}}^{\\alpha}$ is the local state of an ally agent $i_{k}$ and $s_{j_{k}}^{e}$ is of an enemy agent $j_{k}$ and $N(i)$ is the neighborhood of $i$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\no_{i}^{\\alpha}=\\{s_{i}^{\\alpha}\\}\\cup\\{s_{i_{k}}^{\\alpha}:i_{k}\\in N(i)\\cap\\mathcal{N}^{\\alpha}\\}\\cup\\{s_{j_{k}}^{e}:j_{k}\\in N(i)\\cap\\mathcal{N}^{e}\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To apply our $\\mathrm{IL}$ algorithm to this local observation setting, we build a common policy network $\\widehat{\\Pi}_{\\psi_{\\pi}}^{e}$ and Q network $\\widehat{Q}_{\\psi_{Q}}^{e}$ for all the agents where $\\psi_{\\pi}$ and $\\psi_{Q}$ are the network parameters. The $\\mathrm{IL}$ objective ", "page_idx": 5}, {"type": "text", "text": "function can be reformulated according to local observations of the allies as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\widehat{\\Pi}_{\\psi_{\\pi}}^{e},\\widehat{Q}_{\\psi_{Q}}^{e})=\\sum_{i\\in{\\mathcal N}^{\\alpha}}\\mathbb{E}_{(S_{i}^{e,\\mathrm{next}},w_{i}^{\\alpha})\\sim\\rho^{e,\\alpha}}\\Big[\\widehat{Q}^{e}(S_{i}^{e,\\mathrm{next}},w_{i}^{\\alpha})}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,\\gamma\\mathbb{E}_{w_{i}^{\\alpha,\\mathrm{next}}}[V_{\\Pi}^{e}(w_{i}^{\\alpha,\\mathrm{next}})]\\Big]-(1-\\gamma)\\mathbb{E}_{w_{i0}^{\\alpha}\\sim P^{0},\\Pi^{\\alpha}}V_{\\Pi}^{e}(w_{i0}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $w_{i}^{\\alpha}=(o_{i}^{\\alpha},a_{i,-}^{\\alpha})$ , $w_{i}^{\\alpha,\\mathrm{next}}=\\big(o_{i}^{\\alpha,\\mathrm{next}},a_{i}^{\\alpha}\\big)$ ( $\\overline{{a_{i}^{\\alpha}}}$ is the action taken by agent $i$ at observation $o_{i}^{\\alpha}$ , resulting in next observation $o_{i}^{\\alpha,\\mathrm{next}}$ ) $S_{i\\ \\_}^{e,\\mathrm{next}}=\\{s_{j_{k}}^{\\prime e}:j_{k}\\!\\in\\!N(i)\\cap\\!\\mathcal{N}^{e}\\}$ is the next states of enemies in the current neighborhood of the agent $i$ . In addition, the value functions are re-formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{\\Pi}^{e}(w_{i}^{\\alpha})=\\mathbb{E}_{(S_{i}^{e,\\mathrm{next}})\\sim\\hat{\\Pi}_{\\psi_{\\pi}}^{e}}[\\widehat{Q}_{\\psi_{Q}}^{e}(S_{i}^{e,\\mathrm{next}},w_{i}^{\\alpha})-\\ln(\\widehat{\\Pi}_{\\psi_{\\pi}}^{e}(S_{i}^{e,\\mathrm{next}}|w_{i}^{\\alpha}))]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the end, we can update $\\widehat{\\Pi}_{\\psi_{\\pi}}^{e}$ and $\\widehat{Q}_{\\psi_{Q}}^{e}$ by the following actor-critic rule: for a fixed $\\widehat{Q}_{\\psi_{Q}}^{e}$ , we update $\\psi_{Q}$ to maximize $J(\\widehat\\Pi_{\\psi_{\\pi}}^{e},\\widehat Q_{\\psi_{Q}}^{e})$ , and for a fixed $\\widehat{\\Pi}_{\\psi_{\\pi}}^{e}$ , we apply soft actor-critic (SAC) to update $\\psi_{\\pi}$ . ", "page_idx": 6}, {"type": "text", "text": "5.2 IMAX-PPO Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now combine the local observation $o_{i}^{\\alpha}$ of each allied agent $i$ with the next-state prediction $S_{i}^{e,\\mathrm{next}}$ of its neighboring enemies (obtained by our IL algorithm) to create an augmented input. This augmented input is used to improve the policy learning of the allied agent $i$ . That is, we aim to optimize the allies\u2019 policy $\\Pi_{\\theta}^{\\alpha}(a_{i}^{\\bar{\\alpha}}|o_{i}^{\\alpha},S_{i}^{e,\\mathrm{next}}),i\\bar{\\in}\\mathcal{N}^{\\alpha}\\}$ that optimizes the long-term expected joint reward: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{max}_{\\Pi_{\\theta}^{\\alpha}}\\operatorname{\\mathbb{E}}_{(a_{i}^{\\alpha},o_{i}^{\\alpha},S_{i}^{e,\\mathrm{next}})\\sim\\Pi_{\\theta}^{\\alpha}}\\Big[\\sum_{i\\in\\mathcal{N}^{\\alpha}}R_{i}^{\\alpha}(o_{i}^{\\alpha},a_{i}^{\\alpha})\\Big]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $o_{i}^{\\alpha}$ is an observation vector of agent $i$ , $S_{i}^{e,\\mathrm{next}}$ is the information derived from the imitator for agent $i$ , $a_{i}^{\\alpha}\\in\\mathcal{A}_{i}^{\\alpha}$ is a local action of agent $i$ . To facilitate the training and integration of the imitation learning policy into the MARL algorithm, for every ally agent $i$ , we gather game trajectories following the structure $\\mathbf{\\dot{(}}o_{i},a_{i}^{\\alpha},S_{i}^{e,\\mathrm{next}}\\mathbf{)}$ . These gathered observations are then stored in a replay buffer to train the imitation policy $\\widehat{\\Pi}_{\\psi_{\\pi}}^{e}(S_{i}^{e,\\mathrm{next}}|o_{i}^{\\alpha},a_{i}^{\\alpha})$ . ", "page_idx": 6}, {"type": "text", "text": "In the IMAX-PPO framework, at each game state $S$ , considering the current actor policy $\\Pi^{\\alpha}$ and the imitating policy $\\widehat{\\Pi}^{e}$ , for each agent $i\\;\\in\\;{\\cal N}^{\\alpha}$ , we draw a sample for the allied agents\u2019 joint action $\\widetilde{A}^{\\alpha}\\sim\\bar{\\Pi}^{\\alpha}$ . Corresponding local observation $o_{i}^{\\alpha}$ and action $\\widetilde{a}_{i}^{\\alpha}$ of each agent $i$ are then fed as inputs into the imitation policy to predict the subsequent state $S_{i}^{e,\\mathrm{next}}\\sim\\widehat{\\Pi}^{e}(\\cdot|o_{i}^{\\alpha},\\widetilde{a}_{i}^{\\alpha})$ . Once the predicted local states $\\{S_{i}^{e,\\mathrm{next}},\\ i\\in\\mathcal{N}^{\\alpha}\\}$ are available, it is used as input to  the acto r  policy $\\Pi_{\\theta}^{\\alpha}$ in order to generate new actions for the allied agents. In simpler terms, we select a next local action $a_{i}^{\\prime\\alpha}\\!\\sim\\!\\Pi_{\\theta}^{\\alpha}\\!\\left(\\cdot|o_{i}^{\\alpha},S_{i}^{e,\\mathrm{next}}\\right)$ . Beside the allies\u2019 policy network, we also use a centralized value network ${V}_{\\boldsymbol{\\theta}_{v}}^{\\alpha}(S)$ and update it together with the policy network in an actor-critic manner, similarly to MAPPO. The actor-network is trained by optimizing the following objective: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L^{\\alpha}(\\theta)=\\sum_{i\\in N^{\\alpha}}\\mathbb{E}_{o_{i}^{\\alpha},a_{i}^{\\alpha},S_{i}^{e,\\mathrm{next}}}\\big[\\operatorname*{min}\\{r_{i}(\\theta)\\widehat{A},\\mathrm{clip}(r_{i}(\\theta),1-\\epsilon,1+\\epsilon)\\widehat{A}\\}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{r_{i}(\\theta)=\\frac{\\Pi_{\\theta}^{\\alpha}(a_{i}^{\\alpha}|o_{i}^{\\alpha},S_{i}^{e,\\mathrm{next}})}{\\Pi_{\\theta_{o l d}}^{\\alpha}(a_{i}^{\\alpha}|o_{i}^{\\alpha},S_{i}^{e,\\mathrm{next}})}}\\end{array}$ and $\\widehat{A}$ is the advantage function, calculated by Generalized Advantage Estimation (GAE). The Critic network is trained by optimizing ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Phi^{\\alpha}(\\theta_{v})=\\mathbb{E}_{S}\\Big[\\operatorname*{max}\\Big\\{[V_{\\theta_{v}}^{\\alpha}(S)-\\widehat R(S)]^{2},[V_{\\theta_{v},\\theta_{v},_{o l d}}^{\\mathrm{clip}}(S)-\\widehat R(S)]^{2}\\Big\\}\\Big]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\widehat{R}(S)=\\widehat{A}+V_{\\theta_{v,o l d}}^{\\alpha}(S)$ and $V_{\\theta_{v},\\theta_{v,o l d}}^{\\mathrm{clip}}(S)=\\mathrm{clip}(V_{\\theta_{v}}^{\\alpha}(S),V_{\\theta_{v,o l d}}^{\\alpha}(S)-\\epsilon,V_{\\theta_{v,o l d}}^{\\alpha}(S)+\\epsilon)$ . We provide the key stages in Algorithm 1. Additionally, Fig. 1 serves as an illustration of our IMAX-PPO. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of our IMAX-PPO algorithm (Algo. 1) in comparison with some standard and SOTA multi-agent RL algorithms: IPPO, MAPPO, QMIX and QPLEX. In addition, to examine the impact of our multi-agent $\\mathrm{IL}$ model on the performance of IMAX-PPO, we include two versions of IMAX-PPO where opponent\u2019s next states are predicted by (i) an adaption of the GAIL ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 IMAX-PPO Algorithm ", "page_idx": 7}, {"type": "text", "text": "Input: Initial allies\u2019 policy network $\\Pi_{\\theta}^{\\alpha}$ , initial allies\u2019 value network $V_{\\theta_{v}}^{\\alpha}$ , initial imitator\u2019s policy network $\\widehat{\\Pi}_{\\psi_{\\pi}}^{e}$ , initial imitator\u2019s $\\mathrm{Q}$ network $Q_{\\psi_{Q}}^{e}$ , learning rates $\\kappa_{\\pi}^{e},\\kappa_{Q}^{e},\\kappa_{\\pi}^{\\alpha},\\kappa_{V}^{\\alpha}$ . ", "page_idx": 7}, {"type": "text", "text": "Output: Trained allies\u2019 policy $\\Pi_{\\theta}^{\\alpha}$ ", "page_idx": 7}, {"type": "text", "text": "1: for $t=0,1,\\dots\\mathbf{do}$   \n2: # Updating imitator:   \n3: $\\bar{\\psi_{Q,t+1}}=\\bar{\\psi}_{Q,t}+\\kappa_{Q}^{e}\\nabla_{\\psi_{Q}}[J(\\psi_{Q})]$ # Train Q function using the objective in (6)   \n4: $\\boldsymbol{\\psi}_{\\pi,t+1}=\\boldsymbol{\\psi}_{\\pi,t}-\\kappa_{\\pi}^{e}\\nabla_{\\boldsymbol{\\psi}_{\\pi}}\\mathbb{E}_{S_{i}^{e,\\mathrm{next}}}[V_{\\Pi}^{e}(S_{i}^{e,\\mathrm{next}})]$ # Update policy $\\widehat{\\Pi}_{\\psi_{\\pi}}^{e}$ (for continuous domains)   \n5: # Updating allies\u2019 policy:   \n6: $\\theta_{t+1}=\\theta_{t}+\\kappa_{\\pi}^{\\alpha}\\nabla_{\\theta}L^{\\alpha}(\\theta)$ # Update allies\u2019 actor by maximizing $L^{\\alpha}(\\theta)$   \n7: $\\theta_{v,t+1}=\\theta_{v,t}-\\kappa_{V}^{\\alpha}\\nabla_{\\theta_{v}}\\Phi^{\\alpha}(\\theta_{v})$ # Update allies\u2019 critic by minimizing $\\Phi^{\\alpha}(\\theta_{v})$   \n8: end for   \n9: return policy solution for allied agents ", "page_idx": 7}, {"type": "table", "img_path": "06JRFVK88O/tmp/bb00c1a569ea695a5bfb0723ef1c1b9dc167d9c89c7d0246cd181b47248c6e6c.jpg", "table_caption": ["Table 1: Win-rates (percentage). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "algorithm [12], denoted as IMAX-PPO (GAIL) and (ii) the IQ-learn adaption (i.e. Algorithm 1, denoted as IMAX-PPO (InQ). Moreover, to compare out IL-based approach with supervise learning, we include an approach based on MAPPO where the opponent\u2019s next states are learned and predicted by standard supervising learning. We denote this approach as Sup-MAPPO. ", "page_idx": 7}, {"type": "text", "text": "The details of Sup-MAPPO and IMAX-PPO (GAIL) are provided in the appendix. We run extensive experiments in three multi-agent competitive environments: SMACv2, Google Research Football (GRF), and Miner. Each reported value is computed based on 32 different rounds of game playing (each corresponds to a different random seed). ", "page_idx": 7}, {"type": "text", "text": "SMACv2. SMACv2 [4] is an advanced variant of SMAC, driven by the aim to present a more challenging setting for the assessment of cooperative MARL algorithms. In SMACv2, scenarios are procedurally generated, which require agents to generalize to previously unseen settings (from the same distribution) during evaluation. This benchmark consists of 15 sub-tasks where the number of agents varies from 5 to 20. The agents can play with opponents of different difficulty levels. In comparison to SMACv1 [25], SMACv2 stands apart by permitting randomized team compositions, varied starting positions, and an emphasis on augmenting diversity. ", "page_idx": 7}, {"type": "image", "img_path": "06JRFVK88O/tmp/27677a3219e7678e67fc29efebe0b7b37de8b2847ec5ddb508da33831fd38259.jpg", "img_caption": ["Figure 2: Win-rate curves on SMACv2 environment. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 2 shows the performance of the five algorithms during the training process across 15 sub-tasks. The ${\\bf X}$ -axis is the number of training steps and the y-axis is the winning rates averaged over 32 rounds of evaluations. In Figure 2, IMAX-PPO (InQ) consistently and significantly outperforms other baselines. Our algorithm frequently attains quicker convergence; it achieves high win rates at earlier training stages. This could be attributed to the incorporation of our IL component, which facilitates faster comprehension of opponents throughout the game. In particular, the IMAX-PPO $\\mathbf{(InQ)}$ outperforms the two other variants IMAX-PPO (GAIL) and Sup-MAPPO, indicating the advantage of our inverse-Q approach over other IL (i.e. GAIL) and traditional supervising learning methods. Details of win rates at the end of training are shown in Table 1. ", "page_idx": 8}, {"type": "text", "text": "Google Research Football (GRF). This is a challenge on Kaggle competitions made by Google Research team [15]. We focus on three main sub-tasks, sorted based on increasing difficulty levels: (i) academy-3-vs- $^{\\,l}$ -with-keeper: three allies try to score against a goal-keeping opponent; (ii) academycounterattack-easy: four allies versus a counter-attack opponent and a goal-keeping opponent; and (iii) academy-counterattack-hard: four allies versus two counter-attack opponents and a goalkeeper. ", "page_idx": 8}, {"type": "text", "text": "By default, the representations of all agents\u2019 observations are RGB pixels in GRF, so we pre-process this information by distilling some important features such as object positions, object directions, distances between objects, etc. The final win rates are in Table 1, which shows that IMAX-PPO $(\\mathbf{InQ})$ achieves nearly $100\\%$ win-rates, and significantly outperforms other baselines.4 ", "page_idx": 8}, {"type": "text", "text": "Gold Miner [7]. This is another competitive multi-agent game for evaluating our methods, originating from a MARL competition. Multiple miners navigate in a 2D terrain containing obstacles and repositories of gold. Players get points according to the volume of gold they successfully extract. This game is challenging to win as the agents have to learn playing against extremely well-designed heuristic-based enemies. In this game, the ally agents win if the allied team\u2019s average mined gold is higher than that of the enemy team. ", "page_idx": 8}, {"type": "text", "text": "We customized the original environment into three sub-tasks (between two allies against two enemies) of three difficulty levels: (i) Easy (easy_2_vs_2): The enemies\u2019 greedy strategy is to find the shortest way to the golds; (ii) Medium (medium_2_vs_2): One enemy is greedy, and the other follows the algorithm of the second-ranking team in the competition; and (iii) Hard (hard_2_vs_2): The enemies are the first- and second-ranking teams in the competition. ", "page_idx": 8}, {"type": "text", "text": "For this environment, the win rates are in Table 1. Again, IMAX-PPO $(\\mathbf{InQ})$ obtained superior win rates across all three tasks. Especially, in the hard-level task, our algorithm manages to win more than $50\\%$ of the time against the first and second-ranking teams in the competition. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a novel principled framework for enhancing agent training in multi-agent environments through IL. Our new IL model, adapted from IQ-learn, can predict opponents\u2019 policy using only local state observations. By integrating this model into a multi-agent PPO algorithm, our IMAX-PPO algorithm consistently outperforms previous SOTA algorithms such as QMIX and MAPPO. This improvement is observed across various challenging multi-agent tasks, including SMACv2 and GRF. A possible limitation of our work is that it relies on the assumption that the enemies do not update their policies during training (even though this is a standard setting in cooperative multi-agent reinforcement learning [4, 25]). A future direction would be to delve into these aspects to develop efficient MARL algorithms for cooperative-competitive multi-agent RL. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-017). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Abbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.   \n[2] Claus, C. and Boutilier, C. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998(746-752):2, 1998.   \n[3] de Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H., Sun, M., and Whiteson, S. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.   \n[4] Ellis, B., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J. N., and Whiteson, S. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2212.07489, 2022. [5] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[6] Foerster, J. N., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and Mordatch, I. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017.   \n[7] FPT. Fpt reinforcement learning competition. https://github.com/xphongvn/ rlcomp2020, 2020.   \n[8] Fu, J., Luo, K., and Levine, S. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.   \n[9] Garg, D., Chakraborty, S., Cundy, C., Song, J., and Ermon, S. Iq-learn: Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 34:4028\u20134039, 2021.   \n[10] Grover, A., Al-Shedivat, M., Gupta, J., Burda, Y., and Edwards, H. Learning policy representations in multiagent systems. In International conference on machine learning, pp. 1802\u20131811. PMLR, 2018.   \n[11] He, H., Boyd-Graber, J., Kwok, K., and Daum\u00e9 III, H. Opponent modeling in deep reinforcement learning. In International conference on machine learning, pp. 1804\u20131813. PMLR, 2016.   \n[12] Ho, J. and Ermon, S. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.   \n[13] Hong, Z.-W., Su, S.-Y., Shann, T.-Y., Chang, Y.-H., and Lee, C.-Y. A deep policy inference q-network for multi-agent systems. arXiv preprint arXiv:1712.07893, 2017.   \n[14] Kim, D. K., Liu, M., Riemer, M. D., Sun, C., Abdulhai, M., Habibi, G., Lopez-Cot, S., Tesauro, G., and How, J. A policy gradient algorithm for learning to learn in multiagent reinforcement learning. In International Conference on Machine Learning, pp. 5541\u20135550. PMLR, 2021.   \n[15] Kurach, K., Raichuk, A., Sta\u00b4nczyk, P., Zaj a\u02dbc, M., Bachem, O., Espeholt, L., Riquelme, C., Vincent, D., Michalski, M., Bousquet, O., et al. Google research football: A novel reinforcement learning environment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 4501\u20134510, 2020.   \n[16] Li, Z., Xu, T., Qin, Z., Yu, Y., and Luo, Z.-Q. Imitation learning from imperfection: Theoretical justifications and algorithms. Advances in Neural Information Processing Systems, 36, 2024.   \n[17] Littman, M. L. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pp. 157\u2013163. Elsevier, 1994.   \n[18] Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel, O., and Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.   \n[19] Ma, X., Isele, D., Gupta, J. K., Fujimura, K., and Kochenderfer, M. J. Recursive reasoning graph for multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 7664\u20137671, 2022.   \n[20] Ng, A. Y., Russell, S., et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, pp. 2, 2000.   \n[21] Papoudakis, G. and Albrecht, S. V. Variational autoencoders for opponent modeling in multiagent systems. arXiv preprint arXiv:2001.10829, 2020.   \n[22] Rabinowitz, N., Perbet, F., Song, F., Zhang, C., Eslami, S. A., and Botvinick, M. Machine theory of mind. In International conference on machine learning, pp. 4218\u20134227. PMLR, 2018.   \n[23] Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal of Machine Learning Research, 21(1):7234\u20137284, 2020.   \n[24] Reddy, S., Dragan, A. D., and Levine, S. Sqil: Imitation learning via reinforcement learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019.   \n[25] Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.   \n[26] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[27] Son, K., Kim, D., Kang, W. J., Hostallero, D. E., and Yi, Y. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International conference on machine learning, pp. 5887\u20135896. PMLR, 2019.   \n[28] Song, J., Ren, H., Sadigh, D., and Ermon, S. Multi-agent generative adversarial imitation learning. Advances in neural information processing systems, 31, 2018.   \n[29] Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.   \n[30] Wang, J., Ren, Z., Liu, T., Yu, Y., and Zhang, C. Qplex: Duplex dueling multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020.   \n[31] Wen, Y., Yang, Y., Luo, R., and Wang, J. Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning. arXiv preprint arXiv:1901.09216, 2019.   \n[32] Wen, Y., Yang, Y., Luo, R., Wang, J., and Pan, W. Probabilistic recursive reasoning for multi-agent reinforcement learning. arXiv preprint arXiv:1901.09207, 2019.   \n[33] Willi, T., Letcher, A. H., Treutlein, J., and Foerster, J. Cola: consistent learning with opponentlearning awareness. In International Conference on Machine Learning, pp. 23804\u201323831. PMLR, 2022.   \n[34] Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:24611\u201324624, 2022.   \n[35] Yu, X., Jiang, J., Zhang, W., Jiang, H., and Lu, Z. Model-based opponent modeling. Advances in Neural Information Processing Systems, 35:28208\u201328221, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Missing Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proposition 1: For any reward function $R^{e}$ , let $Q^{*}$ be the unique fixed point solution to the soft Bellman equation BQ\u03a0,\u2217Re $B_{Q^{*}}^{\\Pi,R^{\\bar{e}}}=Q^{*}$ , then we have ${\\cal L}(\\Pi,R^{e})=J(\\Pi,Q^{*})$ , and for any $Q^{e}$ , $J(\\Pi,Q^{e})=$ $L(\\Pi,{\\mathcal{T}}_{Q^{e}}^{\\Pi})$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. The proof is similar to that given in [9], as we can see that if $Q^{*}$ is a solution to the soft Bellman equation, then ", "page_idx": 12}, {"type": "equation", "text": "$$\nQ^{*}(W,S^{\\prime})=R^{e}(W,S^{\\prime})+\\gamma\\mathbb{E}_{W^{\\prime}}[V_{\\Pi}^{*}(W^{\\prime})]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where ", "page_idx": 12}, {"type": "equation", "text": "$$\nV_{\\Pi}^{*}(W)=\\mathbb{E}_{S^{\\prime}\\sim\\Pi}[Q^{*}(W,S^{\\prime})\\!-\\!\\ln(\\Pi(S^{\\prime}|W))]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which implies ", "page_idx": 12}, {"type": "equation", "text": "$$\nR^{e}(W,S^{\\prime})=Q^{*}(W,S^{\\prime})-\\gamma\\mathbb{E}_{W^{\\prime}}[V_{\\Pi}^{*}(W^{\\prime})]={\\mathcal T}_{Q^{*}}^{\\Pi}(W,S^{\\prime})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which validates ${\\cal L}(\\Pi,R^{e})=J(\\Pi,Q^{*})$ . The second inequality is just a trivial result from the definition of $J$ and $L$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "A.2 Proposition 5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our following Proposition 5 shows that the function $J(\\Pi,Q^{e})$ can be reformulated in a more compact form that is convenient for training. ", "page_idx": 12}, {"type": "text", "text": "Proposition 5. The function $J(\\cdot)$ can be written as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\Pi,Q^{e})=\\mathbb{E}_{(W,S^{\\prime})\\sim\\rho^{e,\\alpha}}\\left[Q^{e}(W,S^{\\prime})-\\gamma\\mathbb{E}_{W^{\\prime}}[V_{\\Pi}^{e}(W^{\\prime})]\\right]}\\\\ &{\\qquad\\qquad-\\left(1-\\gamma\\right)\\!\\mathbb{E}_{W_{0}\\sim(P^{0},\\Pi^{\\alpha})}V_{\\Pi}^{e}(W_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $S_{0}$ is an initial state and $P^{0}$ is the initial state distribution in the original MDP. ", "page_idx": 12}, {"type": "text", "text": "Proof. From the definition of $J(\\Pi,Q^{e})$ , we write function $J$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad I(\\Pi,Q^{e})=\\mathbb{E}_{\\rho^{e,\\alpha}}[T_{Q^{e}}^{\\Pi}(W,S^{\\prime})]-\\mathbb{E}_{\\rho^{o,\\mathtt{n}}}[T_{Q^{e}}^{\\Pi}(W,S^{\\prime})]+\\mathbb{E}_{\\rho^{\\alpha,\\mathtt{n}}}[\\ln\\Pi(S^{\\prime}|W)]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{(S,A_{-}^{\\alpha},S)\\sim\\rho^{e,\\alpha}}[Q^{e}(W,S^{\\prime})\\!-\\!\\gamma\\mathbb{E}_{W^{\\prime}}V_{\\Pi}^{e}(W^{\\prime})]-\\mathbb{E}_{(S,A_{-}^{\\alpha},S^{\\prime})\\sim\\rho^{\\alpha,\\mathtt{n}}}[Q^{e}(W,S^{\\prime})\\!-\\!\\gamma\\mathbb{E}_{W^{\\prime}}[V_{\\Pi}^{e}(W,S^{\\prime})]]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n+\\,\\mathbb{E}_{\\rho^{\\alpha,\\Pi}}[\\ln\\Pi(S^{\\prime}|W)]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We consider the second and third terms of (9) and write ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{(S,A_{-}^{m},S^{m})}\\mathord{\\left/{\\vphantom{\\left(E_{S,A_{-}^{m},S^{m}}\\right)}}\\right.\\kern-\\nulldelimiterspace}Q^{\\prime}(W,S^{\\prime})-\\mathord{\\left.\\gamma}\\mathord{\\mathbb{E}}_{W^{\\prime}}[V_{\\mathrm{tr}}^{\\prime}(W^{\\prime})]-\\mathord{\\mathbb{E}}_{\\rho^{\\prime\\prime}}\\thinspace\\mathord{\\left[\\mathrm{h}\\left[\\left(X^{\\prime}\\right)/\\left(W\\right)\\right]\\right.\\kern-\\nulldelimiterspace}\\right.}\\\\ &{=(1-\\gamma)\\left(\\mathbb{E}_{S,A_{-}^{m},S^{m}+\\cdots,S^{m},\\Pi}\\right)\\left[\\sum_{t=0}^{\\gamma^{\\prime}}\\gamma^{t}Q^{\\prime}(S_{t+1},W_{t})\\right]-\\gamma\\left(\\mathbb{E}_{W^{\\prime}\\sim\\Omega^{A_{1}},\\Pi}\\left[\\sum_{t=0}^{\\gamma^{\\prime}\\gamma^{\\prime}\\left(V_{T_{1}}^{\\prime}(W_{t+1})\\right)}\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\left.-\\mathbb{E}_{S,A_{-}^{m},S^{m}+\\cdots,\\Pi}\\left[\\sum_{t=0}^{\\gamma^{\\prime}\\gamma^{\\prime}\\left(\\ln\\left[S_{t+1}\\right]\\right)\\left[W_{t}\\right]}\\right]\\right)}\\\\ &{=(1-\\gamma)\\left(\\mathbb{E}_{S,A_{t-}^{m},S^{m}+\\cdots,S^{m},\\Pi}\\left[\\sum_{t=0}^{\\gamma^{\\prime}\\gamma^{\\prime}\\left(Z^{\\prime}(S_{t+1},W_{t})\\right)}-\\gamma\\left(\\mathbb{E}_{S+A_{1},S_{t},A_{-}^{m}\\mapsto\\Pi^{\\prime}\\sim\\Omega^{\\nu}},\\left[\\sum_{t=0}^{\\gamma-1-Q^{\\prime}(S_{t+1}-\\gamma)\\log\\left[\\left(S_{t+1}\\right)\\right]\\right]}-\\mathbb{E}_{S,\\left(X^{\\prime}\\right),\\left(X_{t-1}^{\\prime}\\right)\\right)}\\right)}\\\\ &{-\\gamma\\left(\\mathbb{E}_{S_{t+1},S_{t},A_{t-}^{m}\\sim\\Pi^{\\prime}\\sim\\Omega^{\\prime}},\\left[\\sum_{t=1}^{\\gamma-1}\\ln\\left[\\left(S_{t+1}\\right)\\mathbb{W}_{t}\\right] \n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ(\\Pi,Q^{e})=\\mathbb{E}_{(S,A_{-}^{\\alpha},S^{\\prime})\\sim\\rho^{e},\\alpha}[Q^{e}(W,S^{\\prime})-\\gamma\\mathbb{E}_{W^{\\prime}}V_{\\Pi}^{e}(W^{\\prime})]-(1-\\gamma)\\mathbb{E}_{W_{0}\\sim P^{0}}V_{\\Pi}^{e}(W_{0})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "as desired. ", "page_idx": 12}, {"type": "text", "text": "A.3 Proof of Proposition 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 2: The problem maxQe min\u03a0 $J(\\Pi,Q^{e})$ is equivalent to the maximization $\\operatorname*{max}_{Q^{e}}J(\\Pi^{Q},Q^{e})$ where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Pi^{Q}(S^{\\prime}|W)=\\frac{\\exp(Q^{e}(W,S^{\\prime}))}{\\sum_{S^{\\prime\\prime}}\\exp(Q^{e}(W,S^{\\prime\\prime}))}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, the function $J(\\Pi^{Q},Q^{e})$ is concave in $Q^{e}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. To simplify the proof, we first consider the following simpler optimization problem. Let $p_{1},p_{2},...,p_{N}\\in[0,1]$ and $x_{1},...,x_{N}$ are $\\mathbf{N}$ real numbers. Consider the maximization problem ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{p\\in[0,1]^{N}}}&{{}\\displaystyle\\sum_{i=1}^{N}\\alpha_{i}p_{i}-p_{i}\\ln p_{i}}\\\\ {\\mathrm{subject\\to}}&{{}\\displaystyle\\sum_{i}p_{i}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the KKT condition, if $p^{*}$ is an optimal solution to the above convex problem, $p^{*}$ needs to satisfy the following conditions: there exists $\\lambda$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{x_{i}-\\ln p_{i}^{*}-1-\\lambda=0,\\;\\forall i}\\\\ {\\sum_{i}p_{i}^{*}=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{i}^{*}=\\exp(x_{i}-1-\\lambda)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combine this with the condition $\\textstyle\\sum_{i}p_{i}^{*}=1$ we should have $\\begin{array}{r}{\\exp(-1\\,-\\,\\lambda)\\,=\\,\\sum_{i}\\exp(x_{i})}\\end{array}$ and pi\u2217 =  ej xepx(px(ix)j) (\\*). In addition, when p = p\u2217, the objective function of (P1) can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x)=\\displaystyle\\sum_{i}x_{i}p_{i}^{*}-p_{i}^{*}\\ln(p_{i}^{*})}\\\\ &{\\phantom{f(x)=\\sum_{i}x_{i}}p_{i}\\ (x_{i}-\\ln(p_{i}^{*}))}\\\\ &{\\phantom{f(x)=\\sum_{i}x_{i}}p_{i}\\ \\left(x_{i}-x_{i}+\\ln\\left(\\sum_{i}\\exp(x_{i})\\right)\\right)}\\\\ &{\\phantom{f(x)=\\sum_{i}x_{i}}=\\ln\\left(\\sum_{i}\\exp(x_{i})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We then see that $f(x)$ has a log-sum-exp form, thus it is convex in $x\\ (**)$ . ", "page_idx": 13}, {"type": "text", "text": "We now return to the minimax problem $\\mathrm{max}_{Q^{e}}\\,\\mathrm{min}_{\\Pi}\\,J(\\Pi,Q^{e})$ . For any fixed $Q^{e}$ , it can be seen that the problem min\u03a0 $J(\\Pi,Q^{e})$ is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\Pi}\\left\\{\\gamma\\mathbb{E}_{W}[V_{\\Pi}^{e}(W)]+(1-\\gamma)\\mathbb{E}_{W_{0}\\sim P^{0}}[V_{\\Pi}^{e}(W_{0})]\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We first consider the problem max\u03a0 $V_{\\Pi}^{e}(W)$ , recalling that $V_{\\Pi}^{e}(W)$ can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\nV_{\\Pi}^{e}(W)=\\sum_{S^{\\prime}}\\Pi(S^{\\prime}|W)\\times[Q^{e}(W,S^{\\prime})-\\log(\\Pi(S^{\\prime}|W))]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then it can be seen that each term $\\textstyle\\sum_{S^{\\prime}}\\Pi(S^{\\prime}|W)\\times[Q^{e}(W,S^{\\prime})-\\log(\\Pi(S^{\\prime}|W))]$ has the form of $(\\mathsf{P}1)$ , thus from $(^{**})$ we see that $V_{\\Pi}^{e}(S)$ is maximized at ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Pi^{Q}(S^{\\prime}|W)=\\frac{\\exp(Q^{e}(W,S^{\\prime}))}{\\sum_{S^{\\prime\\prime}}\\exp(Q^{e}(W,S^{\\prime\\prime}))}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, according to the result $({**})$ proved above, when $\\Pi\\,=\\,\\Pi^{Q}$ , $V_{\\Pi}^{e}(W)$ is convex in $Q^{e}$ . Consequently, the loss function of the IQ-learn ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\Pi,Q^{e})\\!=\\!\\mathbb{E}_{(S,A_{-}^{\\alpha},S^{\\prime})\\sim(\\rho^{e,\\alpha})}[Q^{e}(W,S^{\\prime})\\!-\\!\\gamma\\mathbb{E}_{W^{\\prime}}V_{\\Pi}^{e}(W^{\\prime})]+(1-\\gamma)\\mathbb{E}_{W^{0}\\sim P^{0}}V_{\\Pi}^{e}(W^{0})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "is concave in $Q^{e}$ , which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.4 Corollary 6 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Corollary 6. If $\\Pi=\\Pi^{Q}$ (defined in Proposition 2), then we can write $V_{\\Pi}^{e}(S)$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\Pi}^{e}(W)=V_{Q}^{e}(W)=\\ln\\left(\\sum_{S^{\\prime}}\\exp\\left(Q^{e}(W,S^{\\prime})\\right)\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that Corollary 6 is just a direct result from the proof of Proposition 2. ", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Proposition 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 3: Given two allies\u2019 joint policies $\\Pi^{\\alpha}$ and $\\widetilde{\\Pi}^{\\alpha}$ such that $\\mathrm{KL}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde\\Pi^{\\alpha}(\\cdot|S))\\leq\\epsilon$ for any state $S\\in S$ , the following inequality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\Phi(\\Pi^{\\alpha}|Q^{e})-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e})\\right|\\leq\\left(\\alpha\\overline{{Q}}+\\beta\\ln|S|\\right)\\sqrt{2\\ln2\\epsilon}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\overline{{Q}}=\\operatorname*{max}\\,Q^{e}(W,S^{\\prime})$ is an upper bound of $Q^{e}$ , $\\begin{array}{r}{\\alpha\\,=\\,\\frac{\\gamma}{(1-\\gamma)^{2}}+\\frac{\\gamma^{2}}{1-\\gamma}+(3-\\gamma)}\\end{array}$ , and $\\beta\\,=$ $\\mathrm{~\\boldmath~\\bar{~}{~\\bf~}}(W,S^{\\prime})\\mathrm{~\\boldmath~\\bar{~}{~\\bf~}}$   \n$\\begin{array}{r}{\\frac{\\gamma^{2}}{(1-\\gamma)}+3-\\gamma.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We first write the loss function as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi(\\Pi^{\\alpha}|Q^{e})\\!=\\!\\mathbb{E}_{(S,A_{-}^{\\alpha},S^{\\prime})\\sim(\\rho^{e,\\alpha})}\\!\\left[Q^{e}(W,S^{\\prime})\\!-\\!\\gamma\\mathbb{E}_{W^{\\prime}\\sim\\Pi^{\\alpha}}V_{Q}^{e}(W^{\\prime})\\right]-(1-\\gamma)\\mathbb{E}_{W^{0}\\sim P^{0},\\Pi^{\\alpha}}V_{Q}^{e}(W_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We first write the difference between two loss values as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(\\Pi^{\\alpha}|Q^{e})-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e})\\!=\\!\\mathbb{E}_{(S,A_{-}^{\\alpha},S^{\\prime})\\sim\\rho^{e,\\alpha}}\\![Q^{e}(W,S^{\\prime})]-\\mathbb{E}_{(S,A_{-}^{\\alpha},S^{\\prime})\\sim\\rho^{e,\\tilde{\\alpha}}}\\![Q^{e}(W,S^{\\prime})]}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\gamma\\left(\\mathbb{E}_{W^{\\prime}\\sim\\rho^{e,\\alpha}}\\![V_{Q}^{e}(W^{\\prime})]-\\mathbb{E}_{W^{\\prime}\\sim\\rho^{e,\\tilde{\\alpha}}}\\!\\left[V_{Q}^{e}(W^{\\prime})\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\left(1-\\gamma\\right)\\left(\\mathbb{E}_{W^{0}\\sim P^{0},\\Pi^{\\alpha}}\\!\\left(V_{Q}^{e}(W_{0})-V_{Q}^{e}(W_{0})\\right)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We first consider the first term of (12). Let us denote the following function ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Gamma(S^{\\prime}|W)=\\mathbb{E}_{(S_{t}^{\\prime},W_{t})\\sim\\Pi^{e},\\widetilde{\\Pi}^{\\alpha}}\\Big[\\sum_{t=0}\\gamma^{t}\\left(Q^{e}(S_{t}^{\\prime},W_{t})\\right)\\Big|(S_{0}^{\\prime},W_{0})=(W,S^{\\prime})\\Big]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and write the first term of (12) as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{1}-\\gamma)\\left(\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R})_{t}\\sim(\\mathbb{R}^{T},\\mathbb{R}^{T})}\\bigg[\\sum_{\\ell=0}^{\\gamma^{\\prime}}(\\mathcal{G}(S_{t}^{\\top},W_{t}))\\bigg]-\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R})_{t}\\sim(\\mathbb{R}^{T},\\mathbb{R}^{T})}\\bigg[\\sum_{\\ell=0}^{\\gamma^{\\prime}}(\\mathcal{G}(S_{t}^{\\top},W_{t}))\\bigg]\\right)}\\\\ &{=(1-\\gamma)\\left(\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R})_{t}\\sim(\\mathbb{R}^{T},\\mathbb{R}^{T})}\\bigg[\\sum_{\\ell=0}^{\\gamma^{\\prime}}(\\mathcal{G}(S_{t}^{\\top},W_{t}))\\bigg]-\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R})_{t}\\sim(\\mathcal{P}_{t}^{\\top},\\mathbb{R}^{T})}\\bigg[\\Gamma(S_{t}^{\\top}|\\mathbb{R}_{0})\\bigg]\\right)}\\\\ &{=(1-\\gamma)\\left(\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R})_{t}\\sim(\\mathbb{R}^{T},\\mathbb{R}^{T})}\\bigg[\\sum_{\\ell=0}^{\\gamma^{\\prime}}(\\mathcal{G}(S_{t}^{\\top},W_{t}))\\bigg]-\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R}^{T})_{t}\\sim(\\mathbb{R}^{T},\\mathbb{R}^{T})}\\bigg[\\Gamma(S_{t}^{\\top}|\\mathbb{R}_{0})\\bigg]\\right)}\\\\ &{\\qquad+(1-\\gamma)\\left(\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R})_{t}\\sim(\\mathbb{R}^{T},\\mathbb{R}^{T})}\\bigg[\\Gamma(S_{t}^{\\top}|\\mathbb{R}_{0})-\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R}^{T})}\\bigg]\\!\\!+\\!\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R}^{T})}\\bigg[\\Gamma(S_{t}^{\\top}|\\mathbb{R}_{0})\\bigg]\\right)}\\\\ &{=(1-\\gamma)\\Bigg(\\mathbb{E}_{(S_{t}^{\\top},\\mathbb{R})_{\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We first see that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Gamma(S|W)=\\mathbb{E}_{(S_{t}^{\\prime},W_{t})\\sim\\Pi^{\\mathrm{{e}}},\\widetilde{\\Pi}^{\\mathrm{{e}}}}\\Big[\\sum_{t=0}\\gamma^{t}\\left(Q^{e}(S_{t}^{\\prime},W_{t})\\right)\\Big|(S_{0}^{\\prime},W_{0})=(W,S^{\\prime})\\Big]\\le\\frac{\\overline{{Q}}}{1-\\gamma}\\le\\frac{\\tau}{\\Pi^{\\mathrm{{e}}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we can bound the second term of (13) as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\gamma)\\left(\\mathbb{E}_{(S_{0}^{\\prime},W_{0})\\sim(P^{0},\\Pi^{\\alpha})}\\left[\\Gamma(S_{0}^{\\prime}|W_{0})\\right]-\\mathbb{E}_{(S_{0}^{\\prime},W_{0})\\sim(P^{0},\\widetilde{\\Pi}^{\\alpha})}\\left[\\Gamma(S_{0}^{\\prime}|W_{0})\\right]\\right)}\\\\ &{\\leq(1-\\gamma)(\\underset{S,W}{\\operatorname*{max}}\\Gamma(S|W))||\\Pi^{\\alpha}(\\cdot|S_{t})-\\widetilde{\\Pi}^{\\alpha}(\\cdot|S_{t})||_{\\infty}}\\\\ &{\\leq\\overline{{Q}}\\underset{S}{\\operatorname*{max}}\\left\\{\\sqrt{2\\ln2\\mathrm{KL}}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S))\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We also see that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Gamma(S_{t}^{\\prime}|W_{t})=Q^{e}(S_{t}^{\\prime},W_{t})+\\gamma\\mathbb{E}_{(S_{t+1}^{\\prime},W_{t+1})\\sim\\Pi^{e},\\widetilde\\Pi^{\\alpha}|S_{t}^{\\prime},W_{t}}\\Big[\\Gamma(S_{t+1}^{\\prime}|W_{t+1})\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we further can bound the first term (13) as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{\\zeta}_{(S_{t+1}^{\\prime},W_{t+1})\\sim\\Pi^{\\epsilon},\\Pi^{\\alpha}|S_{t}^{\\prime},W_{t}}^{\\prime}\\left[\\left(Q^{\\epsilon}(S_{t}^{\\prime},W_{t})+\\gamma\\Gamma(S_{t+1}^{\\prime}|W_{t+1})-\\Gamma(S_{t}^{\\prime}|W_{t})\\right)\\right]}\\\\ &{=\\gamma\\mathbb{E}_{(S_{t+1}^{\\prime},W_{t+1})\\sim\\Pi^{\\epsilon},\\Pi^{\\alpha}|S_{t}^{\\prime},W_{t}}\\left[\\left(\\Gamma(S_{t+1}^{\\prime}|W_{t+1})\\right]-\\gamma\\mathbb{E}_{(S_{t+1}^{\\prime},W_{t+1})\\sim\\Pi^{\\epsilon},\\Pi^{\\alpha}|S_{t}^{\\prime},W_{t}}\\left[\\left(\\Gamma(S_{t+1}^{\\prime}|W_{t+1})\\right]-\\Gamma(S_{t+1}^{\\prime}|W_{t+1})\\right]\\right]}\\\\ &{\\leq\\gamma\\mathcal{H}||\\Pi^{\\alpha}(\\cdot|S_{t})-\\widetilde{\\Pi}^{\\alpha}(\\cdot|S_{t})||_{\\infty}}\\\\ &{\\leq\\gamma\\mathcal{H}\\operatorname*{max}\\left\\{\\sqrt{2\\ln{2\\mathrm{KL}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S))}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where: ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}\\displaystyle=\\operatorname*{max}_{(W,S^{\\prime})}\\left\\{\\Gamma(S^{\\prime}|W)\\right\\}}\\\\ &{\\,=\\displaystyle\\operatorname*{max}_{(W,S^{\\prime})}\\left\\{\\mathbb{E}_{(S_{t}^{\\prime},W_{t})\\sim(\\Pi^{e},\\widetilde\\Pi^{\\alpha})}\\left[\\sum_{t=0}\\gamma^{t}\\left(Q^{e}(S_{t}^{\\prime},W_{t})\\right)\\right]\\right\\}}\\\\ &{\\,\\le\\displaystyle\\frac{\\overline{{Q}}}{1-\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\overline{{Q}}$ is an upper bound of $Q^{e}(\\cdot|\\cdot)$ . Therefore, we can bound (13) as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{(S_{t}^{\\prime}\\mid S_{t},A_{t}^{\\alpha})\\sim(\\Pi^{\\epsilon},\\Pi^{\\alpha})}\\left[\\sum_{t=0}\\gamma^{t}\\left(Q^{e}(S_{t}^{\\prime},W_{t})+\\gamma\\Gamma(S_{t+1}^{\\prime}|W_{t+1})-\\Gamma(S_{t}^{\\prime}|W_{t})\\right)\\right]}\\\\ &{\\displaystyle\\qquad+\\left(1-\\gamma\\right)\\left(\\mathbb{E}_{(S_{0}^{\\prime},W_{0})\\sim(P^{0},\\Pi^{\\alpha})}\\left[\\Gamma(S_{0}^{\\prime}|W_{0})-\\mathbb{E}_{(S_{0}^{\\prime},W_{0})\\sim(P^{0},\\widetilde{\\Pi}^{\\alpha})}\\left[\\Gamma(S_{0}^{\\prime}|W_{0})\\right]\\right]\\right)}\\\\ &{\\displaystyle\\leq\\left(\\frac{\\gamma}{(1-\\gamma)^{2}}+1\\right)\\overline{{Q}}\\operatorname*{max}_{S}\\left\\{\\sqrt{2\\ln{2\\mathrm{KL}}\\left(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S)\\right)}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the second term of (12), we can bound it as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\left(\\mathbb{E}_{(W,S^{\\prime},W^{\\prime})\\sim\\rho^{\\ell},\\alpha}\\big[\\gamma V_{Q}^{e}(W^{\\prime})\\big]-\\gamma\\mathbb{E}_{(W,S^{\\prime},W^{\\prime})\\sim\\rho^{\\ell},\\tilde{\\alpha}}\\big[\\gamma V_{Q}^{e}(W^{\\prime})\\big]\\right)}\\\\ &{\\quad=(1-\\gamma)\\left(\\mathbb{E}_{W_{t}\\sim\\Pi^{e},\\Pi^{\\alpha}}\\Bigg[\\underset{t=1}{\\sum_{\\ell=1}}\\gamma^{t}V_{Q}^{e}(W_{t})\\Bigg]-\\mathbb{E}_{(W_{t})\\sim\\Pi^{e},\\Pi^{\\alpha}}\\Bigg[\\underset{t=1}{\\sum_{\\ell=1}}\\gamma^{t}V_{Q}^{e}(W_{t})\\Bigg]\\right)}\\\\ &{\\quad=(1-\\gamma)\\mathbb{E}_{(W_{t}\\sim\\Pi^{e},\\Pi^{\\alpha})}\\Bigg[\\underset{t=1}{\\sum_{\\ell=1}}\\gamma^{t}\\big(V_{Q}^{e}(W_{t})\\!+\\!\\gamma U(W_{t+1})\\!-\\!U(W_{t})\\big)\\Bigg]}\\\\ &{\\quad\\quad\\quad+\\left(1-\\gamma\\right)\\left(\\mathbb{E}_{(W_{1})\\sim(P^{0},\\Pi^{\\alpha})}\\big[U(W_{1})\\big]-\\mathbb{E}_{W_{1}\\sim(P^{0},\\tilde{\\Pi}^{\\alpha})}\\big[U(W_{1})\\big]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{{\\cal U}(W)\\!=\\!\\mathbb{E}_{(W_{t}\\sim\\Pi^{e},\\widetilde{\\Pi}^{\\alpha})}\\!\\left[\\underset{t=0}{\\sum}\\gamma^{t}\\!\\left(V_{Q}^{e}(W_{t})\\right)\\Bigm|W_{0}\\!=\\!W\\right]\\!.}\\end{array}$ . It then can be seen that ", "page_idx": 15}, {"type": "equation", "text": "$$\nU(W_{t})=V_{Q}^{e}(W_{t})+\\gamma\\mathbb{E}_{(W_{t+1})\\sim\\Pi^{e},\\widetilde{\\Pi}^{\\alpha}|W_{t}}[U(W_{t+1})]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{U(W)\\le\\frac{1}{1-\\gamma}\\operatorname*{max}_{W}V_{Q}^{e}(W)}}\\\\ &{\\le\\frac{1}{1-\\gamma}\\operatorname*{max}_{W}\\left\\{\\ln\\left(\\sum_{S^{\\prime}}\\exp(Q^{e}(W,S^{\\prime}))\\right)\\right\\}}\\\\ &{\\le\\frac{1}{1-\\gamma}(\\ln|S|+\\overline{{Q}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, given any $W_{t}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{W_{t+1}\\sim(\\Pi^{e},\\Pi^{\\alpha})|S_{t}}\\left[V_{Q}^{e}(W_{t})+\\gamma U(W_{t+1})-U(W_{t})\\right]}\\\\ &{=\\gamma\\mathbb{E}_{W_{t+1}\\sim(\\Pi^{e},\\Pi^{\\alpha})|W_{t}}\\left[U(W_{t+1})\\right]-\\gamma\\mathbb{E}_{W_{t+1}\\sim(\\Pi^{e},\\widetilde{\\Pi}^{\\alpha})|W_{t}}\\left[U(W_{t+1})\\right]}\\\\ &{\\leq\\gamma K||\\Pi^{\\alpha}(\\cdot|S_{t})-\\widetilde{\\Pi}^{\\alpha}(\\cdot|S_{t})||_{\\infty}}\\\\ &{\\leq\\gamma K\\operatorname*{max}_{S}\\left\\{\\sqrt{2\\ln{2\\mathrm{KL}}\\left(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S)\\right)}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{K}=\\frac{1}{1-\\gamma}(\\ln{|\\cal{S}|}+\\overline{{\\cal{Q}}})\\geq\\operatorname*{max}_{W}\\{U(W)\\}}\\end{array}$ . So, (17) can be bounded as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\gamma)\\mathbb{E}_{(W_{t}\\sim\\Pi^{e},\\Pi^{\\alpha})}\\Biggl[\\!\\sum_{t=1}\\!\\gamma^{t}\\!\\bigl(V_{Q}^{e}(W_{t})\\!+\\!\\gamma U(W_{t+1})\\!-\\!U(W_{t})\\!\\bigr)\\!\\Biggr]}\\\\ &{\\leq\\!\\frac{\\gamma^{2}(\\ln|S|+\\overline{{Q}})}{(1-\\gamma)}\\operatorname*{max}_{S}\\!\\left\\{\\sqrt{2\\ln2\\mathrm{KL}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S))}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\gamma)\\left(\\mathbb{E}_{(W_{1})\\sim(P^{0},\\Pi^{\\alpha})}[U(W_{1})]-\\mathbb{E}_{W_{1}\\sim(P^{0},\\widetilde\\Pi^{\\alpha})}[U(W_{1})]\\right)}\\\\ &{\\leq(1-\\gamma)\\underset{S,A^{\\alpha}}{\\operatorname*{max}}\\left((\\Pi^{\\alpha}(A^{\\alpha}|S))^{2}-(\\widetilde\\Pi^{\\alpha}(A^{\\alpha}|S))^{2}\\right)K}\\\\ &{\\leq2(1-\\gamma)K||\\Pi^{\\alpha}(\\cdot|S_{t})-\\widetilde\\Pi^{\\alpha}(\\cdot|S_{t})||_{\\infty}}\\\\ &{\\leq2(\\ln|S|+\\overline{Q})\\underset{S}{\\operatorname*{max}}\\left\\{\\sqrt{2\\ln2\\mathrm{KL}\\big(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde\\Pi^{\\alpha}(\\cdot|S)\\big)}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combine (17) and (20) and (21), we bound the second term of (12) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{(W,S^{\\prime},W^{\\prime})\\sim\\rho^{e,\\alpha}}\\!\\left[\\gamma V_{Q}^{e}(W^{\\prime})\\right]-\\mathbb{E}_{(W,S^{\\prime},W^{\\prime})\\sim\\rho^{e,\\tilde{\\alpha}}}\\!\\left[\\gamma V_{Q}^{e}(W^{\\prime})\\right]\\right|}\\\\ &{=\\left(\\frac{\\gamma^{2}}{(1-\\gamma)}+2\\right)(\\ln|S|+\\overline{{Q}})\\operatorname*{max}_{S}\\!\\left\\{\\sqrt{2\\ln2\\mathrm{KL}\\!\\left(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S)\\right)}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last term of (12) can be bounded simply as as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\gamma)\\left|\\mathbb{E}_{W_{0}\\sim P^{0},\\Pi^{\\alpha}}[V_{Q}^{e}(W^{0})]-\\mathbb{E}_{W_{0}\\sim P^{0},\\Pi^{\\alpha}}[V_{Q}^{e}(W^{0})])\\right|}\\\\ &{\\leq(1-\\gamma)\\underset{W}{\\operatorname*{max}}\\{V_{Q}^{e}(W)\\}\\operatorname*{max}_{S}\\left\\{\\sqrt{2\\ln2\\mathrm{KL}}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S))\\right\\}}\\\\ &{\\leq(1-\\gamma)(\\ln|S|+\\overline{{Q}})\\operatorname*{max}_{S}\\left\\{\\sqrt{2\\ln2\\mathrm{KL}}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S))\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combine (16), (22) and (23) we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(\\Pi^{\\alpha}|Q^{e})-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e})\\Big|}\\\\ &{\\leq\\left(\\frac{\\gamma\\overline{{Q}}}{(1-\\gamma)^{2}}+\\frac{\\gamma^{2}(\\ln|S|+\\overline{{Q}})}{(1-\\gamma)}+(3-\\gamma)(\\ln|S|+\\overline{{Q}})\\right)\\underset{S}{\\operatorname*{max}}\\left\\{\\sqrt{2\\ln2\\mathrm{KL}}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S))\\right\\}}\\\\ &{=\\left(\\left(\\frac{\\gamma}{(1-\\gamma)^{2}}+\\frac{\\gamma^{2}}{1-\\gamma}+(3-\\gamma)\\right)\\overline{{Q}}+\\left(\\frac{\\gamma^{2}}{(1-\\gamma)}+3-\\gamma\\right)\\ln|S|\\right)\\underset{S}{\\operatorname*{max}}\\left\\{\\sqrt{2\\ln2\\mathrm{KL}}(\\Pi^{\\alpha}(\\cdot|S)||\\mathcal{H}(\\cdot|S)|)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof. ", "page_idx": 16}, {"type": "text", "text": "B Continuous Action Space ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We extend the results in Section 4.2 to provide bounds for the case of continuous action space. In this scenario, an actor-critic method is used since the policy $\\Pi^{Q}$ cannot be computed exactly as in Prop. 2. In this case, we can use an explicit policy $\\Pi$ to approximate $\\Pi^{Q}$ instead. We then iteratively update $Q^{e}$ and $\\Pi$ alternatively using the loss function in Prop. 5. In particular, for a fixed $Q^{e}$ , a soft actor update max\u03a0 $\\mathbb{E}_{S^{\\prime}\\sim\\Pi(\\cdot|W)}[\\bar{Q^{e}}(W,S^{\\prime})-\\ln\\Pi(S^{\\prime}|W)]$ will bring the imitation policy $\\Pi$ closer to $\\Pi^{Q}$ . ", "page_idx": 17}, {"type": "text", "text": "Let $\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)$ be the objective of $\\mathrm{IL}$ in Eq. (8), written as a function of the allies\u2019 joint policy $\\Pi^{\\alpha}$ . The following proposition establishes a bound for the variation of $|\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e},\\Pi)|$ as function of ${\\mathrm{KL}}(\\Pi^{\\alpha}||\\widetilde{\\Pi}^{\\alpha})$ , for any pair of allies\u2019 joint policies $(\\Pi^{\\alpha},\\widetilde{\\Pi}^{\\alpha})$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition 7 (Continuous state space $\\boldsymbol{S}$ ). Given two allies\u2019 joint policies $\\Pi^{\\alpha}$ and\u03a0 \u03b1such that for every state $S\\in S$ , $\\mathrm{KL}\\big(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde\\Pi^{\\alpha}(\\cdot|S)\\big)\\leq\\epsilon,$ , the following inequality holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e},\\Pi)\\right|\\leq\\left(\\alpha\\overline{{Q}}-\\beta H\\right)\\sqrt{2\\ln2\\epsilon}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{H\\!=\\!\\operatorname*{inf}_{W}\\sum_{S^{\\prime}}\\Pi(S^{\\prime}|W)\\ln\\Pi(S^{\\prime}|W),}\\end{array}$ , is a lower bound of the entropy of the actor policy \u03a0. ", "page_idx": 17}, {"type": "text", "text": "Proof. We can follow the same arguments as in the proof of Proposition 3 above, with the only difference being when we bound $V_{Q}^{e}(W)$ . Here, $V_{Q}^{e}(W)$ is replaced by $V_{\\Pi}^{e}(W)$ and can be bounded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\Pi}^{e}(W)=\\displaystyle\\sum_{S^{\\prime}}\\Pi^{\\alpha}(S^{\\prime}|W)\\left[Q^{e}(W,S^{\\prime})-\\log(\\Pi(S^{\\prime}|W))\\right]}\\\\ &{\\qquad\\le\\displaystyle\\operatorname*{inf}_{W,S^{\\prime}}\\left\\{Q^{e}(W,S^{\\prime})\\right\\}-\\displaystyle\\operatorname*{inf}_{W}\\left\\{\\sum_{S^{\\prime}}\\Pi(S^{\\prime}|W)\\log(\\Pi(S^{\\prime}|W))\\right\\}}\\\\ &{\\qquad\\le\\overline{Q}-H}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now see that the entropy term $\\begin{array}{r}{\\sum_{S^{\\prime}}\\Pi(S^{\\prime}|W)\\log(\\Pi(S^{\\prime}|W))}\\end{array}$ is minimized when $\\Pi(S^{\\prime}|W)~=~1/|S|$ , where $H\\ =\\ \\operatorname*{inf}_{W}\\{H(\\tilde{W})\\}$ , and $H(W)$ is the entropy of $\\Pi(\\cdot|W)$ , i.e., $\\begin{array}{r}{\\sum_{S^{\\prime}}\\Pi(S^{\\prime}|W)\\ln\\Pi(S^{\\prime}|W)}\\end{array}$ . Therefore, the overall bound can be established by replacing the term $\\ln\\left|\\!\\boldsymbol{S}\\right|$ in the discrete case by $-H$ . We then obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e},\\Pi)\\right|\\leq\\left(\\alpha\\overline{{Q}}-\\beta H\\right)\\operatorname*{max}_{S}\\left\\{\\sqrt{2\\ln2\\mathrm{KL}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S))}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Prop. 3 & 7 allow us to establish an upper bound for the $\\mathrm{IL}$ when the allies\u2019 joint policy changes. ", "page_idx": 17}, {"type": "text", "text": "Corollary 8. Given two allies\u2019 policies $\\Pi^{\\alpha}$ and $\\widetilde{\\Pi}^{\\alpha}$ $[^{\\alpha}\\;w i t h\\;\\mathrm{KL}(\\Pi^{\\alpha}(\\cdot|S)||\\widetilde{\\Pi}^{\\alpha}(\\cdot|S)\\leq\\epsilon,\\,\\forall S\\!\\in\\!S,$ , then: $\\begin{array}{r}{\\Bigm|\\operatorname*{max}_{Q^{e}}\\operatorname*{min}_{\\Pi}\\bigm\\{\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)\\bigm\\}-\\operatorname*{max}_{Q^{e}}\\operatorname*{min}_{\\Pi}\\bigm\\{\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e},\\Pi)\\bigm\\}\\Bigm|\\leq\\mathcal{O}(\\sqrt{\\epsilon})}\\end{array}$ (continuous) ", "page_idx": 17}, {"type": "text", "text": "Proof. To simplify the notation, we first prove the following result: ", "page_idx": 17}, {"type": "text", "text": "Lemma 9. Given $\\epsilon>0$ , and two functions $f(x),\\,g(x)$ such that $|f(x)-g(x)|\\leq\\epsilon$ for any $x\\in\\mathscr{X}$ ( $\\chi$ is the feasible set of $x$ ). The following hold true ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\underset{x}{\\operatorname*{max}}\\,f(x)-\\underset{x}{\\operatorname*{max}}\\,g(x)|\\leq\\epsilon}\\\\ &{|\\operatorname*{min}_{x}f(x)-\\underset{x}{\\operatorname*{min}}\\,g(x)|\\leq\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To prove the above lemma, let $x^{f},x^{g}$ be optimal solutions to $\\operatorname*{max}_{x}f(x)$ and $\\operatorname*{min}_{x}g(x)$ , respectively. We consider 2 cases ", "page_idx": 17}, {"type": "text", "text": "(i) I $\\operatorname{f\\max}_{x}f(x)\\geq\\operatorname*{min}_{x}g(x)$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\operatorname*{max}_{x}f(x)-\\operatorname*{min}_{x}g(x)|=f(x^{f})-g(x^{g})\\leq f(x^{f})-g(x^{f})\\overset{(a)}{\\leq}\\epsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(a)$ is because $|f(x)-g(x)|\\leq\\epsilon$ for any $x\\in\\mathscr{X}$ ", "page_idx": 17}, {"type": "text", "text": "(ii) I $\\operatorname{f\\max}_{x}f(x)\\leq\\operatorname*{min}_{x}g(x)$ , then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\operatorname*{max}_{x}f(x)-\\operatorname*{min}_{x}g(x)|=g(x^{g})-f(x^{f})\\leq g(x^{g})-f(x^{g})\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus $|\\operatorname*{max}_{x}f(x)-\\operatorname*{max}_{x}g(x)|\\leq\\epsilon$ . The inequality $|\\operatorname*{min}_{x}f(x)-\\operatorname*{min}_{x}g(x)|\\leq\\epsilon$ can be validated in the same way. ", "page_idx": 18}, {"type": "text", "text": "We now get back to the main proof. Since $\\left|\\Phi(\\Pi^{\\alpha}|Q^{e})-\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e})\\right|\\;\\leq\\;\\mathcal{O}(\\sqrt{\\epsilon})\\;(\\mathrm{Proposition}\\;3),$ Lemma 9 above implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\operatorname*{max}_{Q^{e}}\\!\\big\\{\\Phi(\\Pi^{\\alpha}|Q^{e})\\big\\}-\\operatorname*{max}_{Q^{e}}\\!\\big\\{\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e})\\big\\}\\Big|\\leq\\mathcal{O}(\\sqrt{\\epsilon})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, from Proposition 7, applying Lemma 9 twice, we have the following ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Big|\\operatorname*{min}_{\\Pi}\\big\\{\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)\\big\\}-\\operatorname*{min}_{\\Pi}\\big\\{\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e},\\Pi)\\big\\}\\Big|\\le\\mathcal{O}(\\sqrt{\\epsilon})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\operatorname*{max}_{Q^{e}}\\operatorname*{min}_{\\Pi}\\left\\{\\Phi(\\Pi^{\\alpha}|Q^{e},\\Pi)\\right\\}-\\operatorname*{max}_{Q^{e}}\\operatorname*{min}_{\\Pi}\\left\\{\\Phi(\\widetilde{\\Pi}^{\\alpha}|Q^{e},\\Pi)\\right\\}\\right|\\leq\\mathcal{O}(\\sqrt{\\epsilon})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which completes the proof. ", "page_idx": 18}, {"type": "text", "text": "C Other Experimental Settings and Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 MAPPO with Supervised Learning to Predict Opponent\u2019s Next States ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this approach, instead of using IQ-learn to predict opponent\u2019s next states, we employ a simple supervised learning approach. Specifically, we create a neural network of parameters $\\delta$ : $\\bar{M}_{\\delta}^{i}(o_{i}^{\\alpha},\\bar{a_{i}^{\\alpha}})$ , taking inputs as an observation and an action of allied agent $i$ , and predict the next states of enemy agents in the observable area of agent $i$ . The loss function can be defined as to minimize the MSE between the actual next enemy states and predicted ones, as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{J_{S u p}(\\delta)=\\sum_{(\\tilde{S}_{i}^{e,\\mathrm{next}},\\tilde{\\sigma}_{i}^{\\alpha},\\tilde{a}_{i}^{\\alpha},\\forall i)\\sim\\mathrm{buffer}}\\sqrt{\\sum_{i}\\left(\\tilde{S}_{i}^{e,\\mathrm{next}}-M_{\\delta}^{i}(\\tilde{\\sigma}_{i}^{\\alpha},\\tilde{a}_{i}^{\\alpha})\\right)^{2}}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.2 IMAX-PPO with GAIL ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To predict enemies\u2019 next states, the GAIL algorithm by [12] can be adapted in a similar way as our main IMAX-PPO algorithm. Building upon our new $M D P$ framework, which is necessary to handle the unobservable-action issue, the aim is also to learn a policy $\\Pi(S^{\\prime}|W)$ by creating a discriminator $\\mathcal{D}(W,S^{\\prime})$ that distinguishes between generated transitions $(W,S^{\\prime})$ and those collected from interacting with the actual enemy agents. The GAIL loss function can be formulated as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Pi}\\operatorname*{max}_{D}\\left\\{J_{\\mathrm{GATL}}(\\Pi,D)=\\mathbb{E}_{(S^{\\prime},Q)\\sim\\rho^{\\Pi,\\alpha}}\\left[\\log D(S^{\\prime},W)\\right]+\\mathbb{E}_{\\rho_{\\epsilon,\\alpha}}\\left[\\log(1-D(W,S^{\\prime}))\\right]-\\lambda H(\\Pi)\\right\\}\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $H(\\Pi)$ is the entropy of $\\Pi$ . In practice with partial observations, we can model $\\mathcal{D}$ and $\\Pi$ as neutral networks $\\mathcal{D}_{w}(S_{i}^{e,\\mathrm{next}},o_{i}^{\\alpha},a_{i}^{\\alpha})$ and $\\Pi_{\\theta}\\big(S_{i}^{e,\\mathrm{next}},o_{i}^{\\alpha},a_{i}^{\\alpha}\\big)$ , where $w,\\ \\theta$ are trainable parameters. The objective can be formulated as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta}{\\operatorname*{min}}\\underset{w}{\\operatorname*{max}}\\Biggl\\{J_{\\mathrm{GAIL}}(\\theta,w)=\\mathbb{E}_{(S_{i}^{e,\\mathrm{next}},o_{i}^{\\alpha},a_{i}^{\\alpha})\\sim\\rho^{\\Pi,\\alpha}}\\left[\\log D(S_{i}^{e,\\mathrm{next}},o_{i}^{\\alpha},a_{i}^{\\alpha})\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}_{(S_{i}^{e,\\mathrm{next}},o_{i}^{\\alpha},a_{i}^{\\alpha})\\sim\\mathrm{buf}\\,\\mathrm{fer}}\\left[\\log(1-D(S_{i}^{e,\\mathrm{next}},o_{i}^{\\alpha},a_{i}^{\\alpha}))\\right]-\\lambda H(\\Pi_{\\theta})\\Biggr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.3 Experimental Settings ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For a fair comparison between our proposed algorithm and existing methods, we use the same model architecture and hyperparameters as shown in Tables 2 and 3 respectively. We also use ", "page_idx": 18}, {"type": "table", "img_path": "06JRFVK88O/tmp/60081ba502dda4acda56188303696e1ce2a8c9a8e38df806988629b567243164.jpg", "table_caption": [], "table_footnote": ["Table 2: Hyperparameters "], "page_idx": 19}, {"type": "text", "text": "Value Normalization method and Recurrent Neural Network mechanism proposed by MAPPO [34], which used as an improvement method to speed up the training process. All sub-tasks (SMACv2, GRF, Miner) are trained concurrently in a GPU-accelerated HPC (High Performance Computing). Therefore, running times reported might not be accurate. In average, each sub-task requires 4-7 days of training depending on its difficulty. As a result, due to the limitation of our computing resources, we do not test our method\u2019s performance with other settings such as disabling Value Normalization, using MLP instead of Recurrent Neural Network, tuning learning rate, tuning clip range, etc. ", "page_idx": 19}, {"type": "text", "text": "Regarding GRF tasks, there are 11 academy scenarios (depicted at https://github.com/ google-research/football/blob/master/gfootball/doc/scenarios.md). Although testing in all these sub-tasks is interesting, but we only evaluate on top three importance scenarios: academy_3_vs_1_with_keeper, academy_counterattack_easy, and academy_counterattack_hard. All are the hardest sub-tasks with the highest number of agents except the almost full football scenarios academy_single_goal_versus_lazy. We report the win-rate curves of our IMAX for the three tasks in Figure 4. We do not show the performance curves of the other baselines methods (QMIX, QPLEX, and MAPPO) as they are not available in their papers, noting that the final win-rates of all the methods considered are already reported in the main paper. Finally, Figure 5 shows the win-rate curves on the Gold Miner tasks. ", "page_idx": 19}, {"type": "image", "img_path": "06JRFVK88O/tmp/e72a1eb53c97cd80aacd75d91a6db6afd71ab6523cfa3c050c1b9a92fe19e99e.jpg", "img_caption": ["Figure 3: PPO model architecture "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "06JRFVK88O/tmp/8ad70c826d41d1129fae6f2a0abd482c329e42d908512393f58abbbac1819be4.jpg", "img_caption": ["Figure 4: Win-rate curves on GRF environment. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "06JRFVK88O/tmp/83468da8f9756ad4655e343f85b81780961af799b29378abfc559ab068d834e2.jpg", "img_caption": ["Figure 5: Win-rate curves on Gold Miner environment. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Ablation Study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present an ablation study to evaluate the effectiveness of our IMAX framework when integrated with traditional multi-agent reinforcement learning algorithms. Specifically, we introduce a variant called QMIX-IMAX, which incorporates our IMAX framework into the conventional QMIX algorithm. We conducted experiments using the SMACv2 benchmark, focusing on three different factions: Protoss, Terran, and Zerg. For each faction, we tested two team configurations: 5_vs_5 and 10_vs_10. ", "page_idx": 20}, {"type": "text", "text": "The performance of QMIX-IMAX was compared against the original, non-IMAX versions of popular algorithms such as MAPPO and QMIX. The results of these experiments are summarized in Table 3, which illustrates the winning rates achieved by each algorithm across the different scenarios. Additionally, Figure 6 displays the learning curves, providing a visual representation of the performance dynamics over the training period. ", "page_idx": 20}, {"type": "text", "text": "Our findings indicate that the QMIX-IMAX variant consistently outperforms the traditional QMIX algorithm across all tested scenarios. However, it does not achieve the same level of performance as our MAPPO-IMAX variant. This suggests that while the IMAX framework enhances the capabilities of QMIX, the integration with MAPPO yields superior results. These observations underscore the potential of the IMAX framework to improve baseline algorithms, offering a promising direction for future research in multi-agent reinforcement learning. ", "page_idx": 20}, {"type": "table", "img_path": "06JRFVK88O/tmp/37222ec7f7d223e7154288e21d32ecb24fd9c226257b9965cacb758332cec682.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 3: Wining rate (percentage) of two baseline methods MAPPO and QMIX and our improvement algorithm on SMACv2 tasks. ", "page_idx": 20}, {"type": "image", "img_path": "06JRFVK88O/tmp/acb5266e58480706672fb2490f15b00ba786b0af0f8ba58d8ef9fa57c09e3a37.jpg", "img_caption": ["Figure 6: Learning curves with different methods on SMACv2. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our abstract includes our main claims reflecting our main contributions and finding. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss limitations of the work in the conclusion section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All the proofs of the theorems and propositions stated in the main paper are provided in the appendix with clear references. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide details on the environments and hyper-parameter settings in the appendix. We also uploaded our source code for re-productivity purposes. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The data we used, along with our source code, has been uploaded with the main paper. We have also provided sufficient instructions for their use. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the details are provided in the main paper and appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Following standard practices, we reported win-rate scores, computed via several independent evaluations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provides all the details in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper develops a general algorithm for multi-agent RL, which we have tested only in simulated environments. As such, we do not foresee any direct societal impact. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided clear citations to the source code and data we used in the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our source code is submitted alongside the paper, accompanied by sufficient instructions. We will share the code publicly for re-producibility or benchmarking purposes. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]