[{"figure_path": "jfHkAEgKwH/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of LocCa. LocCa consists of a standard vision transformer and a transformer decoder. The vision transformer takes image pixel as input, produces visual tokens as cross attention input to the transformer decoder. The transformer decoder is trained to read out rich information from the visual tokens. We adopt the following three task for pretraining: Cap, AREF and GCAP.", "description": "The figure illustrates the architecture of LocCa, a novel visual pretraining paradigm.  LocCa uses a vision transformer (encoder) to process image pixels and generate visual tokens. These tokens are then fed into a transformer decoder, which performs three tasks: standard captioning (Cap), automatic referring expression (ARef), and grounded captioning (GCap). The multi-task learning approach allows the model to extract rich information from images and improve downstream performance on both holistic and location-aware tasks.", "section": "3 Location-aware Captioner"}, {"figure_path": "jfHkAEgKwH/figures/figures_7_1.jpg", "caption": "Figure 2: Result on COCO detection with a limit of 25 output boxes. For reward tuned models we show both the results before (dark blue and orange) and after (light blue and orange) reinforce tuning[88].", "description": "This figure shows the results of COCO detection experiments with a maximum of 25 output boxes.  It compares the performance of different models, including CLIP, Cap, CapPa, and LocCa, both before and after reinforcement learning is applied. The bars represent the mean average precision (mAP) and average recall (AR) metrics.  LocCa demonstrates superior performance compared to the other models, particularly after the reinforcement learning.", "section": "4 Experiments"}, {"figure_path": "jfHkAEgKwH/figures/figures_7_2.jpg", "caption": "Figure 1: Overview of LocCa. LocCa consists of a standard vision transformer and a transformer decoder. The vision transformer takes image pixel as input, produces visual tokens as cross attention input to the transformer decoder. The transformer decoder is trained to read out rich information from the visual tokens. We adopt the following three task for pretraining: Cap, AREF and GCAP.", "description": "This figure provides a high-level overview of the LocCa architecture.  It shows the image pixels as input to a vision transformer, which generates visual tokens. These tokens are then fed into a transformer decoder, which is trained to perform three tasks simultaneously: standard captioning (Cap), automatic referring expression generation (AREF), and grounded captioning (GCAP).  The decoder's multi-task capability is a key feature of LocCa.", "section": "3 Location-aware Captioner"}, {"figure_path": "jfHkAEgKwH/figures/figures_20_1.jpg", "caption": "Figure 1: Overview of LocCa. LocCa consists of a standard vision transformer and a transformer decoder. The vision transformer takes image pixel as input, produces visual tokens as cross attention input to the transformer decoder. The transformer decoder is trained to read out rich information from the visual tokens. We adopt the following three task for pretraining: Cap, AREF and GCAP.", "description": "This figure provides a high-level overview of the LocCa architecture.  It shows the model's components: a vision transformer (encoder) that processes image pixels and produces visual tokens; and a transformer decoder that takes these tokens as input along with task-specific prefixes and generates outputs (captions, bounding boxes). Three tasks are used during pretraining: Cap (standard captioning), AREF (automatic referring expression), and GCAP (grounded captioning).  These tasks encourage the model to learn richer image representations.", "section": "3 Location-aware Captioner"}, {"figure_path": "jfHkAEgKwH/figures/figures_20_2.jpg", "caption": "Figure 1: Overview of LocCa. LocCa consists of a standard vision transformer and a transformer decoder. The vision transformer takes image pixel as input, produces visual tokens as cross attention input to the transformer decoder. The transformer decoder is trained to read out rich information from the visual tokens. We adopt the following three task for pretraining: Cap, AREF and GCAP.", "description": "This figure provides a high-level overview of the LocCa architecture.  It shows the main components: a vision transformer (encoder) that processes image pixels and generates visual tokens, and a transformer decoder that uses these tokens along with task-specific prefixes to generate captions and bounding box coordinates for three different pretraining tasks: Cap (standard image captioning), AREF (automatic referring expressions), and GCAP (grounded captioning). The cross-attention mechanism between the encoder and decoder is highlighted, showing how visual information is integrated into the caption generation process.", "section": "3 Location-aware Captioner"}, {"figure_path": "jfHkAEgKwH/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of LocCa. LocCa consists of a standard vision transformer and a transformer decoder. The vision transformer takes image pixel as input, produces visual tokens as cross attention input to the transformer decoder. The transformer decoder is trained to read out rich information from the visual tokens. We adopt the following three task for pretraining: Cap, AREF and GCAP.", "description": "This figure shows the architecture of LocCa, a novel visual pretraining paradigm that incorporates location-aware tasks into captioners.  It consists of a vision transformer (encoder) that processes image pixels and produces visual tokens, and a transformer decoder that takes these tokens as input, along with task-specific prefixes, to generate captions and predict bounding boxes. Three tasks are used for pretraining: Cap (standard image captioning), AREF (automatic referring expression), and GCAP (grounded captioning). The multi-task decoder allows for efficient training on various tasks. ", "section": "3 Location-aware Captioner"}]