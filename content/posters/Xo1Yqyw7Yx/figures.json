[{"figure_path": "Xo1Yqyw7Yx/figures/figures_1_1.jpg", "caption": "Figure 1: Highly structured environment simulators assume access to parameterizations Es(0) for which random seeds Oi directly produce meaningfully diverse features (e.g. RACING tracks with challenging turns). Open-ended environments with flexible, unstructured parameterizations Eu(0)\u2014though enabling more complex emergent features\u2014lack direct control over high-level features of interest. We introduce DIVA, an approach that effectively creates a more workable parameterization EQD(0) by evolving levels beyond the minimally diverse population from Eu(\u03b8). By training on these discovered levels, DIVA enables superior performance on downstream tasks.", "description": "This figure illustrates the core idea of DIVA.  Highly structured simulators (left) have parameters that directly control the diversity of generated environments. Open-ended simulators (middle) have flexible, unstructured parameters, making it difficult to directly control diversity. DIVA (right) evolves a new parameterization by using quality diversity (QD) to discover diverse levels from the open-ended simulator, effectively creating a more workable representation of the environment space that allows for better agent training.", "section": "1 Introduction"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_2_1.jpg", "caption": "Figure 2: DIVA archive updates on ALCHEMY. The first stage (a) begins with bounds that encapsulate initial solutions, and the target region. As the first stage progresses (b), and QD discovers more of the solution space, the sampling region for the emitters gradually shrinks towards the target region. The second stage begins by redefining the archive bounds to be the target region and including some extra feature dimensions (c). QD fills out just the target region now (d), using sample weights from the target-derived prior (e), the same distribution used to sample levels during meta-training.", "description": "This figure shows the two stages of DIVA's archive update process.  Stage 1 starts with broad bounds encompassing both initial solutions and a target region defined by downstream task features.  As the QD algorithm explores the solution space, the sampling region narrows toward the target. Stage 2 resets the archive bounds to match the target region, adding extra dimensions.  The algorithm then populates this refined target region using weighted samples, mirroring the distribution of tasks used during meta-training.", "section": "3 Problem setting"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_4_1.jpg", "caption": "Figure 1: Highly structured environment simulators assume access to parameterizations  Es(\u03b8) for which random seeds \u03b8i directly produce meaningfully diverse features (e.g. RACING tracks with challenging turns). Open-ended environments with flexible, unstructured parameterizations Eu(\u03b8)\u2014though enabling more complex emergent features\u2014lack direct control over high-level features of interest. We introduce DIVA, an approach that effectively creates a more workable parameterization EQD(\u03b8) by evolving levels beyond the minimally diverse population from Eu(\u03b8). By training on these discovered levels, DIVA enables superior performance on downstream tasks.", "description": "The figure illustrates the difference between structured and unstructured environment simulators. In structured simulators, parameters directly translate to meaningful task diversity, while in unstructured simulators, such control is lacking. DIVA, a novel approach, is introduced to create a more workable parameterization by evolving levels, leading to enhanced performance in downstream tasks.", "section": "1 Introduction"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_5_1.jpg", "caption": "Figure 4: GRIDNAV analysis and results. (a) Target region coverage produced by DIVA and DR over different genotype complexities k. DR represents the average coverage of batches corresponding to the size of the QD archive. DR* represents the total number of unique levels discovered over the course of parameter randomization steps which equal in number to the additional environments PLR is provided for evaluation. DR* is thus an upper bound on the diversity that PLR+ can capture. 500k iterations (QD or otherwise) are used across all results. (b) The diversity produced by PLR+ and ACCEL over the course of training (later updates omitted due to no change in trend). (c) Final episode return curves for DIVA and baselines. (d) Final method success rates across each episode.", "description": "This figure presents an empirical evaluation of DIVA and three baselines on the GRIDNAV task.  Panel (a) compares the diversity of environments generated by DIVA and domain randomization (DR), showing DIVA's ability to maintain diversity even with increased task complexity. Panels (b), (c), and (d) show that DIVA outperforms the baselines in terms of diversity of explored environments, average episode return, and success rate, respectively.  This demonstrates DIVA's effectiveness in generating diverse and challenging training environments.", "section": "5 Empirical results"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_6_1.jpg", "caption": "Figure 5: ALCHEMY environment and results. (a) A visual representation of ALCHEMY's structured stone latent space. P\u2081 and P\u2082 represent potions acting on stones. Only P\u2081 results in a latent state change, because P\u2082 would push the stone outside of the valid latent lattice. (b) Marginal feature distributions for Es (the structured target distribution), DIVA, and Eu (the unstructured distribution used directly for DR, and to initialize DIVA's archive). (c) Final episode return curves for DIVA and baselines. (d) Number of unique genotypes used by each method over the course of meta-training.", "description": "This figure presents a visual representation of the ALCHEMY environment and the results of DIVA's performance against various baselines.  Subfigure (a) shows a diagram of the stone latent space and how potions affect stone states.  Subfigure (b) displays the marginal feature distributions for the target distribution (Es), DIVA's generated distribution, and the unstructured distribution (Eu). Subfigure (c) shows the final episode return curves for DIVA and the baselines, demonstrating DIVA's superior performance. Lastly, subfigure (d) illustrates the number of unique genotypes used by each method.", "section": "5.2 ALCHEMY"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_6_2.jpg", "caption": "Figure 6: ALCHEMY level diversity. Early on in DIVA's QD updates (left), the levels in the archive do not posses much latent stone diversity\u2014all are close to (1, 1, 1). As samples begin populating the target region in later QD updates (right), we see stone diversity is significantly increased.", "description": "This figure shows how DIVA improves the diversity of ALCHEMY levels over time.  The left panel displays the initial levels generated, which lack diversity in their latent stone states (all stones are close to (1,1,1)).  The center panel shows the archive after the first stage of DIVA's QD updates. The right panel shows levels sampled later in the process, demonstrating that DIVA successfully increased the diversity of latent stone states.", "section": "5.2 ALCHEMY"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_7_1.jpg", "caption": "Figure 7: RACING features and main results. Left: Marginal feature distributions for Es (target distribution), EF1 (human-designed F1 tracks), DIVA, and Eu (the unstructured distribution used for DR, the original levels that DIVA evolves)-cropped for readability. Center: Final episode return curves for DIVA and baselines on Es. Right: Track completion rates by method, evaluated on Es.", "description": "This figure displays the results of the RACING experiment. The left panel shows the marginal distributions of four features (AREATOLENGTHRATIO, CENTEROFMASSX, VARIANCEX, TOTALANGLECHANGES) for four different conditions: Es (target distribution), EF1 (human-designed tracks), DIVA, and Eu (unstructured distribution used by DR). The center panel shows the final episode return curves for DIVA and the baselines over training updates on the target distribution Es. The right panel shows the track completion rates for each method, evaluated on Es.", "section": "5.3 RACING"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_7_2.jpg", "caption": "Figure 8: RACING level diversity. We see that random Eu levels, used by DR, and which form the initial population of DIVA, are unable to produce qualitatively diverse tracks (left). After the two-stage QD-updates, DIVA is able to produce tracks of high qualitative diversity (right).", "description": "This figure shows the impact of DIVA's two-stage QD update process on the diversity of generated racing tracks.  The left side displays tracks generated using random parameters (Eu), demonstrating limited diversity and mostly uninteresting track shapes. The right side shows tracks generated after DIVA's QD updates, showcasing a significant increase in the variety and complexity of track layouts.", "section": "5.3 RACING"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_7_3.jpg", "caption": "Figure 9: Sample F1 levels (top), and track completion rates by methods targeting  E<sub>S</sub>, evaluated on  E<sub>F1</sub> (bottom).", "description": "This figure shows the transfer learning results of the trained agents on human-designed F1 tracks (E<sub>F1</sub>). The top panel displays example tracks from E<sub>F1</sub>. The bottom panel presents a bar chart illustrating the success rate (track completion rate) for each method (*ODS, DIVA, and DR) across various completion thresholds (80%, 90%, and 100%).  The results indicate that DIVA, although trained on a different track distribution (E<sub>S</sub>), exhibits considerable zero-shot generalization capabilities on E<sub>F1</sub>, outperforming the other baselines.", "section": "5.3 RACING"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_8_1.jpg", "caption": "Figure 10: DIVA+ results compared to DIVA, for (1) misspecified, and (2) well-specified archives, evaluated on Es.", "description": "This figure compares the performance of DIVA and DIVA+ on the RACING task, using two different archive configurations: one misspecified and one well-specified.  DIVA+ incorporates an additional level selection mechanism (PLR+), which is used to further refine the set of training levels from DIVA's output. The figure shows that for the misspecified archive, DIVA+ achieves significantly improved performance. However, for the well-specified archive, DIVA+ does not show significant gains over DIVA alone, suggesting that the supplemental level selection may be most beneficial when the initial level selection is suboptimal.", "section": "Combining DIVA and UED"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_15_1.jpg", "caption": "Figure 11: ALCHEMY all feature distributions.", "description": "This figure shows the distributions of all features used in the ALCHEMY environment.  It compares the distributions obtained from the structured environment parameterization (Es) with those from the unstructured parameterization (Eu). The comparison highlights the differences in diversity of the feature space captured by the two parameterizations. This is important because the objective is to use DIVA to improve the diversity of training tasks by exploring Eu in a way that targets the desired distribution, Es.", "section": "B Domain details"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_15_2.jpg", "caption": "Figure 12: ALCHEMY measure covariances.", "description": "This figure shows the covariance matrices for both the structured (Es) and unstructured (Eu) environment parameterizations in the ALCHEMY domain.  Each cell represents the covariance between a pair of features.  The color intensity indicates the strength and sign of the correlation, with darker shades of green representing stronger positive correlations and darker shades of orange representing stronger negative correlations.  Comparing the two matrices illustrates how the relationships between features differ depending on whether the environment parameters are structured (Es) or unstructured (Eu).", "section": "B Domain details"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_16_1.jpg", "caption": "Figure 13: RACING all feature distributions.", "description": "This figure displays the distributions of all features used in the RACING environment.  It shows separate distributions for the structured target environment (Es) and the unstructured environment used for domain randomization and initializing DIVA's archive (Eu).  The distributions reveal differences in the feature values produced by the structured versus the unstructured environment parameterizations, highlighting the challenge of generating diverse and representative training levels in open-ended environments.", "section": "B Domain details"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_17_1.jpg", "caption": "Figure 14: RACING measure covariances.", "description": "This figure displays the covariance matrices for the RACING features under the structured (Es) and unstructured (Eu) parameterizations.  The heatmaps visualize the correlation between pairs of features.  Strong positive correlations are shown in dark green, while strong negative correlations are in dark orange.  The color intensity reflects the strength of the correlation, with lighter colors indicating weak correlations. Comparing Es and Eu reveals differences in the relationships between features, suggesting that the unstructured parameterization does not inherently produce the same diverse feature combinations as the structured one.", "section": "B Domain details"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_17_2.jpg", "caption": "Figure 15: ALCHEMY sample mask ablation curves. This specific result is the result of two seeds instead of five, as we found the variance to be very low for this ablation (validated across other parameter settings).", "description": "This figure shows the result of ablating the sample mask used in the first stage of DIVA's QD optimization on the ALCHEMY environment.  Two curves are shown: one with the sample mask and one without.  The y-axis represents the number of target solutions (left) and archive solutions (right) found, while the x-axis represents the number of QD updates performed in the first stage. The figure demonstrates that using the sample mask significantly accelerates the discovery of target solutions and overall increases the number of solutions found in the archive.", "section": "Appendix C. Ablation analysis"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_18_1.jpg", "caption": "Figure 16: Effect of varying QD mutation rate in ALCHEMY. Left: The returns for the final episode by mutation rate, after training on archives produced with each mutation rate. Right: The final number of solutions in the archive after performing QD updates with each mutation rate. This result was produced by running three different seeds for each mutation rate.", "description": "This figure shows the results of an ablation study on the mutation rate used in the quality diversity (QD) optimization algorithm, specifically for the ALCHEMY environment. The left panel displays the final episode returns for different mutation rates, while the right panel shows the number of unique solutions found in the archive for each rate.  The results suggest that the performance is relatively insensitive to changes in mutation rate, within a certain range.", "section": "5.2 ALCHEMY"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_18_2.jpg", "caption": "Figure 17: Effect of varying the number of QD updates in ALCHEMY. Left: The returns for the final episode by number of QD updates in each stage (Ns1 = Ns2). Right: The final number of solutions in the archive after performing each number of QD updates. This result was produced by running three different seeds for each setting.", "description": "This figure analyzes the effect of varying the number of quality diversity (QD) updates on the performance of the DIVA algorithm in the ALCHEMY environment. The left panel shows the final episode returns, while the right panel displays the number of target solutions found in the archive.  It demonstrates that increasing the number of QD updates leads to better performance and more diverse solutions.", "section": "5.2 ALCHEMY"}, {"figure_path": "Xo1Yqyw7Yx/figures/figures_19_1.jpg", "caption": "Figure 18: Effect of varying number of samples in ALCHEMY. Left: DIVA evaluation returns for the final episode by number of downstream samples, after training on archives produced with by using each number of samples to produce archive bounds and prior. Center/Right: Errors for mean and variance parameters of the normal distribution based on number of samples used for computation; for MANHATTANTOOPTIMAL and LATENTSTATEDIVERSITY. For all plots, five seeds were used for each hyperparameter setting.", "description": "This figure shows the ablation study on the number of downstream samples used to generate the target distribution in ALCHEMY. The left plot shows the final episode returns for DIVA, ODS (oracle), and PLR+ baselines as a function of the number of samples. The center and right plots show the mean absolute error (MAE) and variance MAE for the features \"AVERAGE MANHATTAN TO OPTIMAL\" and \"LATENT STATE DIVERSITY\", respectively, as a function of the number of samples. The results indicate that DIVA is robust to the number of samples, even with as few as 5 samples. ", "section": "C Ablation analysis"}]