[{"figure_path": "L8h6cozcbn/tables/tables_16_1.jpg", "caption": "Table 1: Similarity of errors between algorithms. Transformers are more similar to full-observation methods such as Newton and GD; and LSTMs are more similar to online methods such as OGD.", "description": "This table presents the cosine similarity scores between the prediction errors of Transformers and LSTMs against those of three different linear regression algorithms: Iterative Newton's Method, Gradient Descent, and Online Gradient Descent.  Higher scores indicate greater similarity. The results show that Transformers' error patterns are much more closely aligned with those of Newton's Method and Gradient Descent (full-observation methods), whereas LSTMs are more similar to Online Gradient Descent (an online learning method).  This supports the paper's argument that Transformers learn to approximate second-order optimization methods.", "section": "A Additional Experimental Results"}]