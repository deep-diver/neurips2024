[{"type": "text", "text": "Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deqing Fu Tian-Qi Chen Robin Jia Vatsal Shara ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Southern California {deqingfu,tchen939,robinjia,vsharan}@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers excel at in-context learning (ICL)\u2014learning from demonstrations without parameter updates\u2014but how they do so remains a mystery. Recent work suggests that Transformers may internally run Gradient Descent (GD), a first-order optimization method, to perform ICL. In this paper, we instead demonstrate that Transformers learn to approximate second-order optimization methods for ICL. For in-context linear regression, Transformers share a similar convergence rate as Iterative Newton\u2019s Method; both are exponentially faster than GD. Empirically, predictions from successive Transformer layers closely match different iterations of Newton\u2019s Method linearly, with each middle layer roughly computing 3 iterations; thus, Transformers and Newton\u2019s method converge at roughly the same rate. In contrast, Gradient Descent converges exponentially more slowly. We also show that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. Finally, to corroborate our empirical findings, we prove that Transformers can implement $k$ iterations of Newton\u2019s method with $k+\\mathcal{O}(1)$ layers. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer neural networks [Vaswani et al., 2017] have become the default architecture for natural language processing [Devlin et al., 2019, Brown et al., 2020, OpenAI, 2023]. As first demonstrated by GPT-3 [Brown et al., 2020], Transformers excel at in-context learning (ICL)\u2014learning from prompts consisting of input-output pairs, without updating model parameters. Through in-context learning, Transformer-based Large Language Models (LLMs) can achieve state-of-the-art few-shot performance across a variety of downstream tasks [Rae et al., 2022, Smith et al., 2022, Thoppilan et al., 2022, Chowdhery et al., 2022]. ", "page_idx": 0}, {"type": "text", "text": "Given the importance of Transformers and ICL, many prior efforts have attempted to understand how Transformers perform in-context learning. Prior work suggests Transformers can approximate various linear functions well in-context [Garg et al., 2022]. Specifically to linear regression tasks, prior work has tried to understand the ICL mechanism, and the dominant hypothesis is that Transformers learn in-context by running optimization internally through gradient-based algorithms [von Oswald et al., 2022, 2023, Ahn et al., 2023, Dai et al., 2023, Mahankali et al., 2024]. ", "page_idx": 0}, {"type": "text", "text": "This paper presents strong evidence for a competing hypothesis: Transformers trained to perform in-context linear regression learn a strategy much more similar to a second-order optimization method than a first-order method like Gradient Descent (GD). In particular, Transformers approximately implement a second-order method with a convergence rate very similar to Newton-Schulz\u2019s Method, also known as the Iterative Newton\u2019s Method, which iteratively improves an estimate of the inverse of the data matrix to compute the optimal weight vector. Across many Transformer layers, subsequent layers approximately compute more and more iterations of Newton\u2019s Method, with increasingly better predictions; both eventually converge to the optimal minimum-norm solution found by ordinary least squares (OLS). Interestingly, this mechanism is specific to Transformers: LSTMs do not learn these same second-order methods, as their predictions do not even improve across layers. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We present both empirical and theoretical evidence for our claims. Empirically, Transformer layers demonstrate a similar rate of convergence to the OLS solution as second-order methods such as Iterative Newton, which is substantially faster than the rate of convergence of GD (Figure 2). The predictions made by the Transformer at successive layers closely match the predictions made by Iterative Newton after a proportional number of iterations, showing that they progress in similar ways at the same rate. In contrast, to match the Transformer\u2019s predictions after $k$ layers, GD would have to run for exponential in $k$ many steps (Figure 3). Some individual Transformer layers make progress equivalent to hundreds of GD steps: these layers must be doing something more sophisticated than GD. Furthermore, a crucial aspect of second-order methods is that they can handle ill-conditioned problems by correcting the curvature. We find that the convergence rate of Transformers is not significantly affected by ill-conditioning, which again matches Iterative Newton but not GD. To provide theoretical grounding to our empirical results, we show that Transformer circuits can efficiently implement Iterative Newton: one transformer layer can compute one Newton iteration (given $\\mathcal{O}(1)$ pre/post-processing layers), and requires hidden states of dimension $O(d)$ for a $d$ -dimensional linear regression problem. Overall, our work provides a mechanistic account of how Transformers perform ICL that explains model behavior better than previous hypotheses, and hints at why Transformers are well-suited for ICL relative to other architectures. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In-context learning by large language models. GPT-3 [Brown et al., 2020] first showed that Transformer-based large language models can \u201clearn\u201d to perform new tasks from in-context demonstrations (i.e., input-output pairs). Since then, a large body of work in NLP has studied in-context learning, for instance by understanding how the choice and order of demonstrations affects results [Lu et al., 2022, Liu et al., 2022, Rubin et al., 2022, Su et al., 2023, Chang and Jia, 2023, Nguyen and Wong, 2023], studying the effect of label noise [Min et al., 2022c, Yoo et al., 2022, Wei et al., 2023], and proposing methods to improve ICL accuracy [Zhao et al., 2021, Min et al., 2022a,b]. ", "page_idx": 1}, {"type": "text", "text": "In-context learning beyond natural language. Inspired by the phenomenon of ICL by large language models, subsequent work has studied how Transformers learn in-context beyond NLP tasks. Garg et al. [2022] first investigated Transformers\u2019 ICL abilities for various classical machine learning problems, including linear regression. We largely adopt their linear regression setup in this work. Li et al. [2023] formalize in-context learning as an algorithm learning problem. Han et al. [2023] suggests that Transformers learn in-context by performing Bayesian inference on prompts, which can be asymptotically interpreted as kernel regression. Other work has analyzed how Transformers do in-context classification [Tarzanagh et al., 2023a,b, Zhang et al., 2023], the role of pertaining data [Ravent\u00f3s et al., 2023], and the relationship between model architecture and ICL [Lee et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "Do Transformers implement Gradient Descent? A growing body of work has suggested that Transformers learn in-context by implementing gradient descent within their internal representations. Aky\u00fcrek et al. [2022] summarize operations that Transformers can implement, such as multiplication and affine transformations, and show that Transformers can implement gradient descent for linear regression using these operations. Concurrently, von Oswald et al. [2022] argue that Transformers learn in-context via gradient descent, where one layer performs one gradient update. In subsequent work, von Oswald et al. [2023] further argue that Transformers are strongly biased towards learning to implement gradient-based optimization routines. Ahn et al. [2023] extend the work of von Oswald et al. [2022] by showing Transformers can learn to implement preconditioned Gradient Descent, where the pre-conditioner can adapt to the data. Bai et al. [2023] provide detailed constructions for how Transformers can implement a range of learning algorithms via gradient descent. Finally, Dai et al. [2023] conduct experiments on NLP tasks and conclude that Transformer-based language models performing ICL behave similarly to models fine-tuned via gradient descent; however, concurrent work [Shen et al., 2023b] argues that real-world LLMs do not perform ICL via gradient descent. Mahankali et al. [2024] showed that implementing gradient descent is a global minima for single layer linear self-attention. However, we study deeper models in this work, which can behave differently from single-layer models. In this paper, we argue that Transformers actually learn to perform in-context learning by implementing a second-order optimization method, not gradient descent1. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Mechanistic interpretability for Transformers. Our work attempts to understand the mechanism through which Transformers perform in-context learning. Prior work has studied other aspects of Transformers\u2019 internal mechanisms, including reverse-engineering language models [Wang et al., 2022], the grokking phenomenon [Power et al., 2022, Nanda et al., 2023], manipulating attention maps [Hassid et al., 2022], and circuit finding [Conmy et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "Theoretical Expressivity of Transformers. Giannou et al. [2023] provide a construction of looped transformers to implement Iterative Newton\u2019s method for solving pseudo-inverse, and each Newton iteration can be implemented by 13 looped Transformer layers. In contrast, our construction needs only one Transformer layer to compute one Newton iteration. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we focus on the following linear regression task. The task involves n examples {xi, yi}in=1 where $\\pmb{x}_{i}~\\in~\\mathbb{R}^{d}$ and $y_{i}~\\in~\\mathbb{R}$ . The examples are generated from the following data generating distribution $P_{\\mathcal{D}}$ , parameterized by a distribution $\\mathcal{D}$ over $(d\\times d)$ positive semi-definite matrices. For each sequence of $n$ in-context examples, we first sample a ground-truth weight vector $\\boldsymbol{w^{\\star}}$ i.i\u223c.d. $\\mathcal{N}(\\mathbf{0},I)\\,\\in\\,\\mathbb{R}^{d}$ and a matrix $\\boldsymbol{\\Sigma}_{\\mathrm{~\\scriptsize~\\sim~}}^{\\mathrm{~i.i.d.~}}\\boldsymbol{D}$ . For $i\\in[n]$ , we sample each $\\pmb{x}_{i}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(\\mathbf{0},\\pmb{\\Sigma})$ . The label $y_{i}$ for each $\\pmb{x}_{i}$ is given by $y_{i}={\\pmb w}^{\\star\\top}{\\pmb x}_{i}$ . Note that for much of our experiments ", "page_idx": 2}, {"type": "image", "img_path": "L8h6cozcbn/tmp/3cff3e1590b20113207ca7b3baf60de04bdc83e0828516744685978a8e1e111b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Illustration of how Transformers are trained to do in-context linear regression. ", "page_idx": 2}, {"type": "text", "text": "$\\mathcal{D}$ is only supported on the identity matrix $\\boldsymbol{\\mathit{I}}$ and hence $\\Sigma=I$ , but we also consider some distributions over ill-conditioned matrices, which give rise to ill-conditioned regression problems. Most of our results are on this noiseless setup and results with the noisy setup are in Appendix A.3.2. ", "page_idx": 2}, {"type": "text", "text": "3.1 Standard Methods for Solving Linear Regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our central research question is: ", "page_idx": 2}, {"type": "text", "text": "What convergence rate does the algorithm Transformers learn for linear regression achieve? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To investigate this question, we first discuss various known algorithms for linear regression. We then compare them with Transformers empirically in $\\S4$ and theoretically in $\\S5$ , to evaluate if Transformers are more similar to first-order or second-order methods. We care particularly about algorithms\u2019 convergence rates (the number of steps required to reach an $\\epsilon$ error). ", "page_idx": 2}, {"type": "text", "text": "For any time step $t$ , let $X^{(t)}=\\left[\\pmb{x}_{1}\\quad\\cdot\\cdot\\quad\\pmb{x}_{t}\\right]^{\\intercal}$ be the data matrix and $\\pmb{y}^{(t)}=\\left[y_{1}\\quad\\cdot\\cdot\\quad y_{t}\\right]^{\\top}$ be the labels for all the datapoints seen so far. Note that since $t$ can be smaller than the data dimension $d$ , $X^{(t)}$ can be singular. We now consider various algorithms for making predictions for $x_{t+1}$ based on $X^{(t)}$ and $\\pmb{y}^{(t)}$ . When it is clear from context, we drop the superscript and refer to $\\pmb{X}^{(t)}$ and $\\pmb{y}^{(t)}$ as $\\mathbf{\\deltaX}$ and $\\textit{\\textbf{y}}$ , where $\\mathbf{\\deltaX}$ and $\\textit{\\textbf{y}}$ correspond to all the datapoints seen so far. ", "page_idx": 2}, {"type": "text", "text": "Ordinary Least Squares. This method finds the minimum-norm solution to the objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{w}\\mid\\pmb{X},\\pmb{y})=\\frac{1}{2n}\\|\\pmb{y}-\\pmb{X}\\pmb{w}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The Ordinary Least Squares (OLS) solution has a closed form given by the Normal Equations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\pmb w}^{\\mathrm{OLS}}=(\\pmb X^{\\top}\\pmb X)^{\\dagger}\\pmb X^{\\top}\\pmb y\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $S:=X^{\\top}X$ and $S^{\\dagger}$ is the pseudo-inverse [Moore, 1920] of $\\boldsymbol{S}$ . ", "page_idx": 3}, {"type": "text", "text": "Gradient Descent. Gradient descent (GD) is a first-order method which finds the weight vector $\\hat{w}^{\\mathrm{GD}}$ with initialization $\\hat{w}_{0}^{\\mathrm{GD}}=\\mathbf{0}$ using the iterative update rule: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{w}_{k+1}^{\\mathrm{GD}}=\\hat{w}_{k}^{\\mathrm{GD}}-\\eta\\nabla_{w}\\mathcal{L}\\big(\\hat{w}_{k}^{\\mathrm{GD}}\\mid X,y\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is known that GD requires ${\\mathcal{O}}\\left(\\kappa(S)\\log(1/\\epsilon)\\right)$ steps to converge to an $\\epsilon$ error where $\\begin{array}{r}{\\kappa(S)=\\frac{\\lambda_{\\mathrm{max}}(S)}{\\lambda_{\\mathrm{min}}(S)}}\\end{array}$ is the condition number. Thus, when $\\kappa(S)$ is large, GD converges slowly [Boyd and Vandenberghe, 2004]. ", "page_idx": 3}, {"type": "text", "text": "Online Gradient Descent. While GD computes the gradient with respect to the full data matrix $\\mathbf{\\deltaX}$ at each iteration, Online Gradient Descent (OGD) is an online algorithm that only computes gradients on the newly received data point $\\{x_{k},y_{k}\\}$ at step $k$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\pmb{w}}_{k+1}^{\\mathrm{OGD}}=\\hat{\\pmb{w}}_{k}^{\\mathrm{OGD}}-\\eta_{k}\\nabla_{\\pmb{w}}\\mathcal{L}(\\hat{\\pmb{w}}_{k}^{\\mathrm{OGD}}\\mid\\pmb{x}_{k},y_{k}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Picking \u03b7k = $\\begin{array}{r}{\\eta_{k}=\\frac{1}{\\|\\pmb{x}_{k}\\|_{2}^{2}}}\\end{array}$ 2ensures that the new weight vector w\u02c6kO+G1D makes zero error on $\\{x_{k},y_{k}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Iterative Newton\u2019s Method. This is a second-order method which finds the weight vector $\\hat{\\pmb w}^{\\mathrm{N}\\epsilon}$ wton by iteratively apply Newton\u2019s method to finding the pseudo inverse of $S=X^{\\daleth}X$ [Schulz, 1933, Ben-Israel, 1965]. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{0}=\\alpha{\\boldsymbol{S}},\\,\\mathrm{where}\\;\\alpha=\\cfrac{2}{\\|{\\boldsymbol{S}}{\\boldsymbol{S}}^{\\top}\\|_{2}},\\;\\;\\hat{w}_{0}^{\\mathrm{Newton}}=M_{0}{\\boldsymbol{X}}^{\\top}{\\boldsymbol{y}},}\\\\ &{M_{k+1}=2M_{k}-M_{k}{\\boldsymbol{S}}M_{k},\\;\\;\\hat{w}_{k+1}^{\\mathrm{Newton}}=M_{k+1}{\\boldsymbol{X}}^{\\top}{\\boldsymbol{y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This computes an approximation of the psuedo inverse using the moments of $\\boldsymbol{S}$ . In contrast to GD, the Iterative Newton\u2019s method only requires $\\mathcal{O}(\\log\\kappa(\\pmb{S})+\\log\\log(1/\\epsilon))$ steps to converge to an $\\epsilon$ error [Soderstrom and Stewart, 1974, Pan and Schreiber, 1991]. Note that this is exponentially faster than the convergence rate of GD. We discuss additional algorithms such as Conjugate Gradient, BFGS, and L-BFGS in the Appendix A.2.3. ", "page_idx": 3}, {"type": "text", "text": "3.2 Solving Linear Regression with Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will use neural network models such as Transformers to solve this linear regression task. As shown in Figure 1, at time step $t+1$ , the model sees the first $t$ in-context examples $\\{{x}_{i},{y}_{i}\\}_{i=1}^{t}$ , and then makes predictions for $\\pmb{x}_{t+1}$ , whose label $y_{t+1}$ is not observed by the Transformers model. ", "page_idx": 3}, {"type": "text", "text": "We randomly initialize our models and then train them on the linear regression task to make predictions for every number of in-context examples $t$ , where $t\\in[n]$ . Training and test data are both drawn from $P_{\\mathcal{D}}$ . To make the input prompts contain both $\\pmb{x}_{i}$ and $y_{i}$ , we follow same the setup as Garg et al. [2022]\u2019s to zero-pad $y_{i}$ \u2019s, and use the same GPT-2 model [Radford et al., 2019] with softmax activation and causal attention mask (discussed later in Definition 3.1). ", "page_idx": 3}, {"type": "text", "text": "We now present the key mathematical details for the Transformer architecture, and how they can be used for in-context learning. First, the causal attention mask enforces that attention heads can only attend to hidden states of previous time steps, and is defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Causal Attention Layer). A causal attention layer with $M$ heads and activation function $\\sigma$ is denoted as Attn on any input sequence $\\pmb{H}=[\\pmb{h}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{h}_{N}]\\in\\mathbb{R}^{D\\times N}$ , where $D$ is the dimension of hidden states and $N$ is the sequence length. In the vector form, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\boldsymbol{h}}_{t}=[\\mathrm{Attn}(\\boldsymbol{H})]_{t}=\\boldsymbol{h}_{t}+\\sum_{m=1}^{M}\\sum_{j=1}^{t}\\sigma\\left(\\langle Q_{m}\\boldsymbol{h}_{t},\\boldsymbol{K}_{m}\\boldsymbol{h}_{j}\\rangle\\right)\\cdot V_{m}\\boldsymbol{h}_{j}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Vaswani et al. [2017] originally proposed the Transformer architecture with the Softmax activation function for the attention layers. Later works have found that replacing Softmax $(\\cdot)$ with $\\begin{array}{r}{\\frac{1}{t}\\mathrm{ReLU}(\\cdot)}\\end{array}$ does not hurt model performance [Cai et al., 2022, Shen et al., 2023a, Wortsman et al., 2023]. The Transformers architecture is defined by putting together attention layers with feed forward layers: ", "page_idx": 3}, {"type": "image", "img_path": "L8h6cozcbn/tmp/2860f622994e542028fbff98bbbd41657f45efba4ad79dfaac6ddcc5d7e86886.jpg", "img_caption": ["Figure 2: Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer\u2019s performance improve over the layer index $\\ell$ . When $n\\,>\\,d$ , the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in $\\S\\mathrm{A}.4.2)$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Transformers). An $L$ -layer decoder-based transformer with Causal Attention Layers is denoted as $\\mathrm{TF}_{\\theta}$ and is a composition of a MLP Layer (with a skip connection) and a Causal Attention Layers. For input sequence $H^{(0)}$ , the transformers $\\ell_{}$ -th hidden layer is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TF}_{\\pmb\\theta}^{\\ell}(\\pmb H^{(0)}):=\\pmb H^{(\\ell)}=\\mathrm{MLP}_{\\pmb\\theta_{\\mathrm{mlp}}^{(\\ell)}}\\left(\\mathrm{Attn}_{\\pmb\\theta_{\\mathrm{attn}}^{(\\ell)}}(\\pmb H^{(\\ell-1)})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where \u03b8 = {\u03b8(\u2113), \u03b8(\u2113) }L mlp attn \u2113=1 and \u03b8attn $\\pmb{\\theta}_{\\mathrm{attn}}^{(\\ell)}=\\{\\pmb{Q}_{m}^{(\\ell)},\\pmb{K}_{m}^{(\\ell)},\\pmb{V}_{m}^{(\\ell)}\\}_{m=1}^{M}$ has $M$ heads at layer $\\ell$ ", "page_idx": 4}, {"type": "text", "text": "In particular for the linear regression task, Transformers perform in-context learning as follows ", "page_idx": 4}, {"type": "text", "text": "Definition 3.3 (Transformers for Linear Regression). Given in-context examples $\\{x_{1},y_{1},\\ldots,x_{t},y_{t}\\}$ , Transformers make predictions on a query example $x_{t+1}$ through a readout layer parameterized as $\\pmb{\\theta}_{\\mathrm{readout}}=\\{\\pmb{u},\\boldsymbol{v}\\}$ , and the prediction $\\hat{y}_{t+1}^{\\mathrm{TF}}$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{y}_{t+1}^{\\mathrm{TF}}:=\\mathrm{ReadOut}\\Big[\\underbrace{\\mathrm{TF}_{\\theta}^{L}\\big(\\{x_{1},y_{1},\\cdots,x_{t},y_{t},x_{t+1}\\}\\big)}_{H^{(L)}}\\Big]=\\boldsymbol{u}^{\\top}H_{:,2t+1}^{(L)}+\\boldsymbol{v}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To compare the rate of convergence of iterative algorithms to that of Transformers, we treat the layer index $\\ell$ of Transformers as analogous to the iterative step $k$ of algorithms discussed in $\\S3.1$ . Note that for Transformers, we need to re-train the ReadOut layer for every layer index $\\ell$ so that they can improve progressively (see $\\S4.1$ and for experimental details) for linear regression tasks. ", "page_idx": 4}, {"type": "text", "text": "3.3 Measuring Algorithmic Similarity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose two metrics to measure the similarity between linear regression algorithms. ", "page_idx": 4}, {"type": "text", "text": "Similarity of Errors. This metric aims to measure similarity of algorithms through comparing prediction errors. For a linear regression algorithm $\\boldsymbol{\\mathcal{A}}$ , let $\\mathcal{A}(\\pmb{x}_{t+1}\\mid\\{\\pmb{x}_{i},y_{i}\\}_{i=1}^{t})$ denote its prediction on the $(t+1)$ -th in-context example $x_{t+1}$ after observing the first $t$ examples (see Figure 1). We write $\\boldsymbol{\\mathcal{A}}(\\boldsymbol{x}_{t+1}):=\\boldsymbol{\\mathcal{A}}(\\boldsymbol{x}_{t+1}\\mid\\{\\boldsymbol{x}_{i},\\bar{\\boldsymbol{y}_{i}}\\}_{i=1}^{t})$ for brevity. Errors (i.e., residuals) on the sequence are:2 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{E}(A\\mid\\{\\pmb{x}_{i},y_{i}\\}_{i=1}^{n+1})=\\Big[\\pmb{\\mathcal{A}}(\\pmb{x}_{2})-y_{2},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{\\mathcal{A}}(\\pmb{x}_{n+1})-y_{n+1}\\Big]^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The similarity of errors for two algorithms $\\mathcal{A}_{a}$ and $\\mathcal{A}_{b}$ is the expected cosine similarity of their errors on a randomly sampled data sequence: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{SimE}(\\boldsymbol{A_{a}},\\boldsymbol{A_{b}})=\\underset{\\{\\boldsymbol{x_{i}},\\boldsymbol{y_{i}}\\}_{i=1}^{n+1}\\sim P_{D}}{\\mathbb{E}}\\left[\\mathcal{C}\\Big(\\mathcal{E}\\big(\\boldsymbol{A_{a}}|\\{\\boldsymbol{x_{i}},\\boldsymbol{y_{i}}\\}_{i=1}^{n+1}\\big),\\mathcal{E}(\\boldsymbol{A_{b}}|\\{\\boldsymbol{x_{i}},\\boldsymbol{y_{i}}\\}_{i=1}^{n+1})\\Big)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2the indices start from 2 to $n+1$ because we evaluate all cases where $t$ can choose from $1,\\cdots,n$ . ", "page_idx": 4}, {"type": "text", "text": "Here C(u, v) =\u2225u\u27e8\u2225u2,\u2225vv\u27e9\u22252 is the cosine similarity, $n$ is the total number of in-context examples, and $P_{\\cal D}$ is the data generation process discussed previously. ", "page_idx": 5}, {"type": "text", "text": "Similarity of Induced Weights. All standard algorithms for linear regression estimate a weight vector $\\hat{w}$ . While neural ICL models like Transformers do not explicitly learn such a weight vector, similar to Aky\u00fcrek et al. [2022], we can induce an implicit weight vector $\\tilde{w}$ learned by any algorithm $\\boldsymbol{\\mathcal{A}}$ by fitting a weight vector to its predictions. We can then measure similarity of algorithms by comparing the induced $\\tilde{\\pmb{w}}$ . To do this, for any fixed sequence of $t$ in-context examples $\\overline{{\\{x_{i},y_{i}\\}}}_{i=1}^{t}$ , we sample $T\\gg d$ query examples $\\widetilde{\\pmb{x}}_{k}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(\\mathbf{0},\\pmb{\\Sigma})$ , where $k\\,\\in\\,[T]$ . For this fixed sequence of in-context examples $\\overline{{\\{x_{i},y_{i}\\}}}_{i=1}^{t}$ , we create $T$ in-context prediction tasks and use the algorithm $\\boldsymbol{\\mathcal{A}}$ to make predictions $\\mathcal{A}(\\tilde{\\mathbf{x}}_{k}\\mid\\{\\boldsymbol{x}_{i},\\boldsymbol{y}_{i}\\}_{i=1}^{t})$ . We define the induced data matrix and labels as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{{\\cal X}}=\\left[\\begin{array}{c}{\\tilde{x}_{1}^{\\top}}\\\\ {\\vdots}\\\\ {\\tilde{x}_{T}^{\\top}}\\end{array}\\right]\\qquad\\tilde{{\\cal Y}}=\\left[\\begin{array}{c}{\\!\\!\\left[\\mathscr{A}(\\tilde{x}_{1}\\mid\\{x_{i},y_{i}\\}_{i=1}^{t})\\right]\\!\\!}\\\\ {\\vdots}\\\\ {\\!\\!\\left[\\mathscr{A}(\\tilde{x}_{T}\\mid\\{x_{i},y_{i}\\}_{i=1}^{t})\\right]\\!\\!}\\end{array}.\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The induced weight vector for $\\boldsymbol{\\mathcal{A}}$ and these $t$ examples is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{w}_{t}(\\boldsymbol{\\mathcal{A}}):=\\tilde{w}_{t}(\\boldsymbol{\\mathcal{A}}\\mid\\{\\boldsymbol{x}_{i},y_{i}\\}_{i=1}^{t})=(\\tilde{X}^{\\top}\\tilde{X})^{-1}\\tilde{X}^{\\top}\\tilde{Y}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The similarity of induced weights between two algorithms $\\mathcal{A}_{a}$ and $\\boldsymbol{A}_{b}$ is the expected average cosine similarity3 of induced weights $\\tilde{w}_{t}(\\mathcal{A}_{a})$ and $\\tilde{w}_{t}(\\mathcal{A}_{b})$ over all possible $1\\leq t\\leq n$ , on a randomly sampled data sequence: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{SimW}(A_{a},A_{b})=\\underset{\\{x_{i},y_{i}\\}_{i=1}^{n}\\sim P_{D}}{\\mathbb{E}}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\mathcal{C}\\Big(\\tilde{w}_{t}(A_{a}|\\{x_{i},y_{i}\\}_{i=1}^{t}),\\tilde{w}_{t}({\\mathcal A}_{b}|\\{x_{i},y_{i}\\}_{i=1}^{t})\\big)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Matching steps between algorithms. Each algorithm converges to its predictions after several steps \u2014 for example the number of iterations for Iterative Newton and GD, and the number of layers for Transformers (see Section 4.1). When comparing two algorithms, given a choice of steps for the first algorithm, we match it with the steps for the second algorithm that maximize similarity. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.4 (Best-matching Steps). Let $\\mathcal{M}$ be the metric for evaluating similarities between two algorithms $\\mathcal{A}_{a}$ and $\\mathcal{A}_{b}$ , which have steps $p_{a}\\,\\in\\,[0,T_{a}]$ and $p_{b}\\,\\in\\,[0,T_{b}]$ , respectively. For a given choice of $p_{a}$ , we define the best-matching number of steps of algorithm $\\mathcal{A}_{b}$ for $\\mathcal{A}_{a}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{b}^{\\mathcal{M}}(p_{a}):=\\underset{p_{b}\\in[0,T_{b}]}{\\arg\\operatorname*{max}}\\mathcal{M}(\\mathcal{A}_{a}(\\cdot\\mid p_{a}),\\mathcal{A}_{b}(\\cdot\\mid p_{b})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In our experiments, we chose $T_{a},T_{b}$ be large enough integers so the algorithms converge. The matching processes can be visualized as heatmaps as shown in Figure 3, where best-matching steps are highlighted. This enables us to compare the rate of convergence of algorithms. In particular, if two algorithms converge at the same rate, the best matching steps between the two algorithms should follow a linear trend. We will discuss these results in $\\S4$ . See Figure 26 on how best-matching steps help compare the convergence rates. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Evidence ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We primarily study the Transformers-based GPT-2 model with 12 layers and 8 heads per layer. Alternative configurations with fewer heads per layer, or with more layers, also support our findings; we defer them to $\\S\\mathrm{A.4.1}$ and $\\S\\mathrm{A}.4.2$ . We initially focus on isotropic cases where $\\Sigma=I$ and later consider ill-conditioned $\\Sigma$ in $\\S4.3$ . Our training setup is exactly the same as Garg et al. [2022]: models are trained with at most $n=40$ in-context examples for $d=20$ (with the same learning rate, batch size etc.). ", "page_idx": 5}, {"type": "text", "text": "We claim that Transformers learn high-order optimization methods in-context. We provide evidence that Transformers improve themselves with more layers in $\\S4.1$ ; Transformers share the same rate of convergence as Iterative Newton, exponentially faster than that of GD, in $\\S4.2$ ; and they also perform well on ill-conditioned problems in $\\S4.3$ . Finally, we contrast Transformers with LSTMs in $\\S4.5$ . ", "page_idx": 5}, {"type": "image", "img_path": "L8h6cozcbn/tmp/9ed0a1625dec45c2a19a0c22677a362f12b61003afe15bc11e1d7fef2060ffbe.jpg", "img_caption": ["Figure 3: Heatmaps of Similarity. The best matching steps are highlighted in yellow. Transformers layers show a linear trend with Iterative Newton steps but an exponential trend with GD. This suggests Transformers and Iterative Newton have the same convergence rate that is exponentially faster than GD. See Figure 10 for an additional heatmap where GD\u2019s steps are shown in log scale: on that plot there is a linear correspondence between Transformers and GD\u2019s steps. This further strengthens the claim that Transformers have an exponentially faster rate of convergence than GD. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Transformers improve progressively over layers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Many known algorithms for linear regression, including GD, OGD, and Iterative Newton, are iterative: their performance progressively improves as they perform more iterations, eventually converging to a final solution. How can a Transformer implement such an iterative algorithm? von Oswald et al. [2022] propose that deeper layers of the Transformer may correspond to more iterations; in particular, they show that there exist Transformer parameters such that each attention layer performs one step of GD. ", "page_idx": 6}, {"type": "text", "text": "Following this intuition, we first investigate whether the predictions of a trained Transformer improve as the layer index $\\ell$ increases. For each layer of hidden states $H^{(\\ell)}$ (see Definition 3.2), we re-train the ReadOut to predict $y_{t}$ for each $t$ ; the new predictions are given by $\\mathrm{ReadOut}^{(\\ell)}\\left[\\pmb{H}^{(\\ell)}\\right]$ . Thus for each input prompt, there are $L$ Transformer predictions parameterized by layer index $\\ell$ . All parameters besides the ReadOut layer parameters are kept frozen. ", "page_idx": 6}, {"type": "text", "text": "As shown in Figure 2(a) (and Figure 7(a) in the Appendix), as we increase the layer index $\\ell$ , the prediction performance improves progressively. Hence, Transformers progressively improve their predictions over layers $\\ell$ , similar to how iterative algorithms improve over steps. Such observations are consistent with language tasks where Transformers-based language models also improve their predictions along with layer progressions [Geva et al., 2022, Chuang et al., 2023]. ", "page_idx": 6}, {"type": "text", "text": "4.2 Transformers are more similar to second-order methods, such as Iterative Newton ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now test the more specific hypothesis that the iterative updates performed across Transformer layers are similar to the iterative updates for known iterative algorithms. First, Figure 2 shows that the middle layers of Transformers converge at a rate similar to Iterative Newton, and faster than GD. In particular, the Transformer and Iterative Newton both converge at a superlinear rate, while GD converges at a sublinear rate. ", "page_idx": 6}, {"type": "text", "text": "Next, we analyze whether each layer $\\ell$ of the Transformer corresponds to performing $k$ steps of some iterative algorithm, for some $k$ depending on $\\ell$ . We focus here on GD and Iterative Newton\u2019s Method; we will discuss online algorithms in Section 4.5, and additional optimization methods in Appendix A.2.3. We will discuss results on noisy linear regression tasks in Appendix A.3.2. ", "page_idx": 6}, {"type": "text", "text": "For each layer $\\ell$ of the Transformer, we measure the best-matching similarity (see Def. 3.4) with candidate iterative algorithms with the optimal choice of the number of steps $k$ . As shown in Figure 3, the Transformer has very high error similarity with Iterative Newton\u2019s method at all layers. Moreover, we see a clear linear trend between layer 3 and layer 9 of the Transformer, where each layer appears to compute roughly 3 additional iterations of Iterative Newton\u2019s method. This trend only stops at the last few layers because both algorithms converge to the OLS solution; Newton is known to converge to OLS (see $\\S3.1)$ , and we verify in Appendix A.2 that the last few layers of the Transformer also basically compute OLS (see Figure 14 in the Appendix). We observe the same trends when using similarity of induced weights as our similarity metric (see Figure 9 in the Appendix). Figure 11 in the Appendix shows that there is a similar linear trend between Transformer and BFGS, an alternative quasi-Newton method. This is perhaps not surprising, given that BFGS also gets a superlinear convergence rate for linear regression Nocedal and Wright [1999]. Thus, we do not claim that Transformers specifically implement Iterative Newton, only that they (approximately) implement some second-order method. ", "page_idx": 6}, {"type": "image", "img_path": "L8h6cozcbn/tmp/c5f391e84f450392fbb68ef327ba2917b937f737b46dd15a8ffeec0e18650b25.jpg", "img_caption": ["Figure 4: Transformers performance on ill-conditioned data. Given 40 in-context examples, Transformers and Iterative Newton converge similarly and they both can converge to the OLS solution quickly whereas GD suffers. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "L8h6cozcbn/tmp/dc1a7bf6bb93736b13deee80f76eddfa63ef28ee2f8bb5c9f5796ee29316b9cb.jpg", "img_caption": ["Figure 5: In the left figure, we measure model predictions with normalized MSE. Though LSTM is seemingly most similar to Newton\u2019s Method with only 5 steps, neither algorithm converges yet. OGD also has a similar trend as LSTM. In the right figure, we measure the model\u2019s error rate on example $x_{n-g}$ after seeing $n$ examples, for different values of the time stamp gap $g$ (see Appendix A.6), and find both Transformers and not-converged Newton have better memorization than LSTM and OGD. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In contrast, even though GD has a comparable similarity with the Transformers at later layers, their best matching follows an exponential trend. As discussed in the Section 3.1, for well-conditioned problems where $\\kappa\\approx1$ , to achieve $\\epsilon$ error, the rate of convergence of GD is ${\\mathcal{O}}(\\log(1/\\epsilon))$ while the rate of convergence of Iterative Newton is ${\\mathcal{O}}(\\log\\log(1/\\epsilon))$ . Therefore the rate of convergence of Iterative Newton is exponentially faster than GD. Transformer\u2019s linear correspondence with Iterative Newton and its exponential correspondence with GD provides strong evidence that the rate of convergence of Transformers is similar to Iterative Newton, i.e., ${\\mathcal{O}}(\\log\\log(1/\\epsilon))$ . We also note that it is not possible to significantly improve GD\u2019s convergence rate without using second-order methods: Nemirovski and Yudin [1983] showed a $\\Omega\\big(\\log(1/\\epsilon)\\big)$ lower bound on the convergence rate of gradient-based methods for smooth and strongly convex problems, and Arjevani et al. [2016] shows a similar lower bound specifically for quadratic problems. In the Appendix, we show that limited-memory BFGS Liu and Nocedal [1989] and conjugate gradient (see Figure 12), which do not use full-second order information, also converge slower than Transformers. This provides further evidence for the usage of second-order information by Transformers. We also show more evidence by investigating alternative function classes such as linear regression with noises in Appendix A.3.2 and 2-layer neural network with ReLU or Tanh activation function in Appendix A.3.3. ", "page_idx": 7}, {"type": "text", "text": "Overall, we conclude that a Transformer trained to perform in-context linear regression learns to implement an algorithm that is very similar to second-order methods, such as Iterative Newton\u2019s method, not GD. Starting at layer 3, subsequent layers of the Transformer compute more and more iterations of Iterative Newton\u2019s method. This algorithm successfully solves the linear regression problem, as it converges to the optimal OLS solution in the final layers. ", "page_idx": 7}, {"type": "text", "text": "4.3 Transformers perform well on ill-conditioned data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We repeat the same experiments with data $\\pmb{x}_{i}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(\\mathbf{0},\\pmb{\\Sigma})$ sampled from an ill-condition covariance matrix $\\Sigma$ with condition number $\\kappa(\\Sigma)=100$ , and eigenbasis chosen uniformly at random. The first $d/2$ eigenvalues of $\\Sigma$ are 100, and the last $d/2$ are 1. Note that choosing the eigenbasis uniformly at random for each sequence ensures that there is a different covariance matrix $\\Sigma$ for each sequence of datapoints. ", "page_idx": 7}, {"type": "image", "img_path": "L8h6cozcbn/tmp/acb35226bc9fbb0254aa89ec3a8c21be030de644be36bab1a417a976c9918c91.jpg", "img_caption": ["Figure 6: Ablation on Transformer\u2019s Hidden Size. For linear regression problems with $d\\,=\\,20$ , Transformers need $O(d)$ hidden dimension to mimic OLS solutions. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "As shown in Figure 4, the Transformer model\u2019s performance still closely matches Iterative Newton\u2019s Method with 21 iterations, same as when $\\Sigma=I$ (see layer 10-12 in Figure 3). The convergence of second-order methods has a mild logarithmic dependence on the condition number since they correct for the curvature. On the other hand, GD\u2019s convergence is affected polynomially by conditioning. As $\\kappa(\\Sigma)$ increase from 1 to 100, the number steps required for GD\u2019s convergence increases significantly (see Fig. 4 where GD requires 2,000 steps to converge), making it impossible for a 12-layer Transformers to implement these many gradient updates. We also note that preconditioning the data by $(X^{\\top}X)^{\\dagger}$ can make the data well-conditioned, but since the eigenbasis is chosen uniformly at random, with high probability there is no sparse pre-conditioner or any fixed pre-conditioner which works across the data distribution. Computing $\\mathbf{\\dot{\\Xi}}(X^{\\top}X)^{\\dagger}$ appears to be as hard as computing the OLS solution (Eq. 1)\u2014in fact Sharan et al. [2019] conjecture that first-order methods such as gradient descent and its variants cannot avoid polynomial dependencies in condition number in the ill-conditioned case.4 See Appendix A.3.1 for detailed experiments on ill-conditioned problems. These experiments further strengthen our thesis that Transformers learn to perform second-order optimization methods in-context, not first-order methods such as GD. ", "page_idx": 8}, {"type": "text", "text": "4.4 Transformers Require $O(d)$ Hidden Dimension ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We ablate 12-layer 1-head Transformers with various hidden sizes on $d=20$ problems. As shown in Figure 6, we observe that Transformers can mimic OLS solution when the hidden size is 32 or 64, but fail with smaller sizes. This resonates with our theoretical results on $O(d)$ hidden dimension in Theorem 5.1, and in this case, the theorem ensures a construction of transformers to implement Iterative Newton\u2019s method. ", "page_idx": 8}, {"type": "text", "text": "4.5 LSTM is more similar to OGD than Transformers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As discussed in $\\S\\mathrm{A.l}$ , LSTM is an alternative auto-regressive model widely used before the introduction of Transformers. Thus, a natural research question is: If Transformers can learn in-context, can LSTMs do so as well? If so, do they learn the same algorithms? To answer this question, we train a LSTM model in an identical manner to the Transformers studied in the previous sections. ", "page_idx": 8}, {"type": "text", "text": "Figure 5 plots the error of Transformers, LSTMs, and other standard methods as a function of the number of in-context (i.e., training) examples provided. While LSTMs can also learn linear regression in-context, they have much higher mean-squared error than Transformers. Their error rate is similar to Iterative Newton\u2019s Method after only 5 iterations, a point where it is far from converging to the OLS solution. Finally, we show that LSTMs behave more like an online learning algorithm than Transformers. In particular, its predictions are biased towards getting more recent training examples correct, as opposed to earlier examples, as shown in Figure 5. This property makes LSTMs similar to online GD. In contrast, five steps of Newton\u2019s method has the same error on average for recent and early examples, showing that the LSTM implements a very different algorithm from a few iterations of Newton. We hypothesize that since LSTMs have limited memory, they must learn in a roughly online fashion; in contrast, Transformer\u2019s attention heads can access the entire sequence of past examples, enabling it to learn more complex algorithms. See $\\S\\mathrm{A.l}$ for more discussions. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Theoretical Justification ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our empirical evidence demonstrates that Transformers behave much more similarly to Iterative Newton\u2019s than to GD. Iterative Newton is a second-order optimization method, and is algorithmically more involved than GD. We begin by first examining this difference in complexity. As discussed in Section 3, the updates for Iterative Newton are of the form, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\hat{w}_{k+1}^{\\mathrm{Newton}}=M_{k+1}X^{\\top}y\\qquad\\mathrm{where}~M_{k+1}=2M_{k}-M_{k}S M_{k}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "and $M_{0}=\\alpha S$ for some $\\alpha>0$ . We can express $M_{k}$ in terms of powers of $\\boldsymbol{S}$ by expanding iteratively, for example $M_{1}=2\\alpha S-4\\alpha^{2}S^{3},M_{2}\\stackrel{\\cdot}{=}4\\alpha S-12\\alpha^{2}S^{3}+\\stackrel{\\cdot}{16}\\alpha^{3}S^{5}-1\\dot{6}\\alpha^{4}\\dot{S}^{7}$ , and in general Mk = s2k=+11\u22121\u03b2sSs for some \u03b2s \u2208R (see Appendix B.3 for detailed calculations). Note that k steps of Iterative Newton\u2019s requires computing $\\Omega(2^{k})$ moments of $\\boldsymbol{S}$ . Let us contrast this with GD. GD updates for linear regression take the form, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\pmb{\\hat{w}}_{k+1}^{\\mathrm{GD}}=\\pmb{\\hat{w}}_{k}^{\\mathrm{GD}}-\\eta(\\pmb{S}\\pmb{\\hat{w}}_{k}^{\\mathrm{GD}}-\\pmb{X}^{\\top}\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Like Iterative Newton, we can express $\\hat{w}_{k}^{\\mathrm{GD}}$ in terms of powers of $\\boldsymbol{S}$ and $X^{\\top}y$ . However, after $k$ steps of GD, the highest power of $\\boldsymbol{S}$ is only $O(k)$ . This exponential separation is consistent with the exponential gap in terms of the parameter dependence in the convergence rate\u2014 ${\\mathcal{O}}\\left(\\kappa(S)\\log(1/\\epsilon)\\right)$ for GD vs. $\\bar{\\mathcal{O}}(\\log\\kappa(\\pmb{S})+\\log\\mathrm{\\bar{log}}(1/\\epsilon))$ for Iterative Newton. Therefore, a natural question is whether Transformers can actually as complicated of a method such as Iterative Newton with only polynomially many layers? Theorem 5.1 shows that this is indeed possible. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.1. For any $k$ , there exist Transformer weights such that on any set of in-context examples $\\{{x}_{i},{y}_{i}\\}_{i=1}^{n}$ and test point $\\pmb{x}_{\\mathrm{test}}$ , the Transformer predicts on $\\mathbf{\\deltax}_{\\mathrm{test}}$ using $\\bar{\\mathbf{x}}_{\\mathrm{test}}^{\\top}\\hat{w}_{k}^{\\mathrm{N}}$ w\u02c6Nwton. Here e $\\hat{\\pmb{w}}_{k}^{\\mathrm{Newton}}$ are the Iterative Newton updates given by $\\hat{\\pmb{w}}_{k}^{\\mathrm{Newton}}=M_{k}\\pmb{X}^{\\top}\\pmb{y}$ where $M_{j}$ is updated as ", "page_idx": 9}, {"type": "equation", "text": "$$\nM_{j}=2M_{j-1}-M_{j-1}S M_{j-1},1\\leq j\\leq k,\\quad M_{0}=\\alpha S,\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for some $\\alpha>0$ and $S=X^{\\top}X$ . The dimensionality of the hidden layers is $O(d)$ , and the number of layers is $k+8$ . One transformer layer computes one Newton iteration. 3 initial transformer layers are needed for initializing $M_{0}$ and $^{5}$ layers at the end are needed to read out predictions from the computed pseudo-inverse $M_{k}$ . ", "page_idx": 9}, {"type": "text", "text": "We note that our proof uses full attention instead of causal attention and ReLU activations for the self-attention layers. The definitions of these and the full proof appear in Appendix B. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we studied how Transformers perform in-context learning for linear regression. In contrast with the hypothesis that Transformers learn in-context by implementing gradient descent, our experimental results show that different Transformer layers match iterations of Iterative Newton linearly and Gradient Descent exponentially. This suggests that Transformers share a similar rate of convergence to Iterative Newton but not to Gradient Descent. Moreover, Transformers can perform well empirically on ill-conditioned linear regression, whereas first-order methods such as Gradient Descent struggle. This empirical evidence \u2014 when combined with existing lower bounds in optimization \u2014 suggests that Transformers use second-order information for solving linear regression, and we also prove that Transformers can indeed represent second-order methods. ", "page_idx": 9}, {"type": "text", "text": "An interesting direction is to explore a wider range of second-order methods that Transformers can implement. It also seems promising to extend our analysis to classification problems, especially given recent work showing that Transformers resemble SVMs in classification tasks [Li et al., 2023, Tarzanagh et al., 2023a]. Finally, a natural question is to understand the differences in the model architecture that make Transformers better in-context learners than LSTMs. Based on our investigations with LSTMs, we hypothesize that Transformers can implement more powerful algorithms because of having access to a longer history of examples. Investigating the role of this additional memory in learning appears to be an intriguing direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank the USC NLP Group and Center for AI Safety for providing compute resources. DF would like to thank Oliver Liu and Ameya Godbole for their extensive discussions. DF and RJ were supported by a Google Research Scholar Award. RJ was also supported by an Open Philanthropy research grant. VS was supported by NSF CAREER Award CCF-2239265 and an Amazon Research Award. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. ArXiv, abs/2306.00297, 2023. 1, 2 ", "page_idx": 10}, {"type": "text", "text": "Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. ArXiv, abs/2211.15661, 2022. 2, 3.3, B.1, B.4, B.2, B.5 ", "page_idx": 10}, {"type": "text", "text": "Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. On lower and upper bounds in smooth and strongly convex optimization. Journal of Machine Learning Research, 17(126):1\u201351, 2016. URL http://jmlr.org/papers/v17/15-106.html. 4.2 ", "page_idx": 10}, {"type": "text", "text": "Yu Bai, Fan Chen, Haiquan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. ArXiv, abs/2306.04637, 2023. 2 ", "page_idx": 10}, {"type": "text", "text": "Adi Ben-Israel. An iterative method for computing the generalized inverse of an arbitrary matrix. Mathematics of Computation, 19(91):452\u2013455, 1965. ISSN 00255718, 10886842. URL http: //www.jstor.org/stable/2003676. 3.1 ", "page_idx": 10}, {"type": "text", "text": "Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. 3.1 ", "page_idx": 10}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. 1, 2 ", "page_idx": 10}, {"type": "text", "text": "Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition. ArXiv, abs/2205.14756, 2022. 3.2 ", "page_idx": 10}, {"type": "text", "text": "Ting-Yun Chang and Robin Jia. Data curation alone can stabilize in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8123\u20138144, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.452. URL https://aclanthology.org/2023.acl-long. 452. 2 ", "page_idx": 10}, {"type": "text", "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan ", "page_idx": 10}, {"type": "text", "text": "Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. 1 ", "page_idx": 11}, {"type": "text", "text": "Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models, 2023. 4.1 ", "page_idx": 11}, {"type": "text", "text": "Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri\u00e0 GarrigaAlonso. Towards automated circuit discovery for mechanistic interpretability, 2023. 2 ", "page_idx": 11}, {"type": "text", "text": "Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. ArXiv, abs/2212.10559, 2023. 1, 2 ", "page_idx": 11}, {"type": "text", "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. 1 ", "page_idx": 11}, {"type": "text", "text": "Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. ArXiv, abs/2208.01066, 2022. 1, 2, 3.2, 4, A.3.3, D ", "page_idx": 11}, {"type": "text", "text": "Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30\u201345, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.3. URL https://aclanthology.org/2022.emnlp-main.3. 4.1 ", "page_idx": 11}, {"type": "text", "text": "Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 11398\u201311442. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/giannou23a.html. 2, B.5 ", "page_idx": 11}, {"type": "text", "text": "Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained as kernel regression, 2023. 2 ", "page_idx": 11}, {"type": "text", "text": "Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A. Smith, and Roy Schwartz. How much does attention actually attend? questioning the importance of attention in pretrained transformers, 2022. 2 ", "page_idx": 11}, {"type": "text", "text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9 (8):1735\u20131780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https: //doi.org/10.1162/neco.1997.9.8.1735. A.1 ", "page_idx": 11}, {"type": "text", "text": "Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Exploring the relationship between model architecture and in-context learning ability, 2023. 2 ", "page_idx": 11}, {"type": "text", "text": "Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, 2023. 2, 6 ", "page_idx": 11}, {"type": "text", "text": "Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503\u2013528, 1989. URL https://api.semanticscholar.org/ CorpusID:5681609. 4.2 ", "page_idx": 11}, {"type": "text", "text": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https: //aclanthology.org/2022.deelio-1.10. 2 ", "page_idx": 12}, {"type": "text", "text": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556. 2 ", "page_idx": 12}, {"type": "text", "text": "Arvind V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= 8p3fu56lKc. 1, 2 ", "page_idx": 12}, {"type": "text", "text": "Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330, Dublin, Ireland, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 365. URL https://aclanthology.org/2022.acl-long.365. 2 ", "page_idx": 12}, {"type": "text", "text": "Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States, July 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 201. URL https://aclanthology.org/2022.naacl-main.201. 2 ", "page_idx": 12}, {"type": "text", "text": "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates, December 2022c. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.759. URL https://aclanthology.org/2022. emnlp-main.759. 2 ", "page_idx": 12}, {"type": "text", "text": "E.H Moore. On the reciprocal of the general algebraic matrix. Bulletin of American Mathematical Society, 26:394\u2013395, 1920. 3.1 ", "page_idx": 12}, {"type": "text", "text": "Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability, 2023. 2 ", "page_idx": 12}, {"type": "text", "text": "A.S. Nemirovski and D.B Yudin. Problem complexity and method efficiency in optimization. 1983. 4.2 ", "page_idx": 12}, {"type": "text", "text": "Tai Nguyen and Eric Wong. In-context example selection with influences. arXiv preprint arXiv:2302.11042, 2023. 2 ", "page_idx": 12}, {"type": "text", "text": "Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999. 4.2, A.2.3 ", "page_idx": 12}, {"type": "text", "text": "OpenAI. Gpt-4 technical report, 2023. URL http://arxiv.org/abs/2303.08774v3. 1 ", "page_idx": 12}, {"type": "text", "text": "Victor Y. Pan and Robert S. Schreiber. An improved newton iteration for the generalized inverse of a matrix, with applications. SIAM J. Sci. Comput., 12:1109\u20131130, 1991. 3.1 ", "page_idx": 12}, {"type": "text", "text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. D ", "page_idx": 12}, {"type": "text", "text": "Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets, 2022. 2 ", "page_idx": 13}, {"type": "text", "text": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 3.2 ", "page_idx": 13}, {"type": "text", "text": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2022. 1 ", "page_idx": 13}, {"type": "text", "text": "Allan Ravent\u00f3s, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression, 2023. 2 ", "page_idx": 13}, {"type": "text", "text": "Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 191. URL https://aclanthology.org/2022.naacl-main.191. 2   \nG\u00fcnther Schulz. Iterative berechung der reziproken matrix. Zeitschrift f\u00fcr Angewandte Mathematik und Mechanik (Journal of Applied Mathematics and Mechanics), 13:57\u201359, 1933. 3.1   \nVatsal Sharan, Aaron Sidford, and Gregory Valiant. Memory-sample tradeoffs for linear regression with small error. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 890\u2013901, 2019. 4.3   \nKai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. A study on relu and softmax in transformer, 2023a. 3.2   \nLingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent?, 2023b. 2   \nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022. 1   \nTorsten Soderstrom and G. W. Stewart. On the numerical properties of an iterative method for computing the moore- penrose generalized inverse. SIAM Journal on Numerical Analysis, 11(1): 61\u201374, 1974. ISSN 00361429. URL http://www.jstor.org/stable/2156431. 3.1   \nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Selective annotation makes language models better few-shot learners. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=qY1hlv7gwg. 2   \nDavoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as ", "page_idx": 13}, {"type": "text", "text": "support vector machines. ArXiv, abs/2308.16898, 2023a. 2, 6 ", "page_idx": 13}, {"type": "text", "text": "Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism, 2023b. 2 ", "page_idx": 14}, {"type": "text", "text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022. 1   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. 1, 3.2   \nMax Vladymyrov, Johannes von Oswald, Mark Sandler, and Rong Ge. Linear transformers are versatile in-context learners, 2024. 1   \nJohannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, 2022. 1, 2, 4.1   \nJohannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise Ag\u00fcera y Arcas, Max Vladymyrov, Razvan Pascanu, and Joao Sacramento. Uncovering mesa-optimization algorithms in transformers. ArXiv, abs/2309.05858, 2023. 1, 2   \nKevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small, 2022. 2   \nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning differently, 2023. 2   \nMitchell Wortsman, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Replacing softmax with relu in vision transformers, 2023. 3.2   \nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2422\u20132437, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.155. URL https://aclanthology.org/2022.emnlp-main.155. 2   \nRuiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context. ArXiv, abs/2306.09927, 2023. 2   \nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR, 2021. 2 ", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Additional Experimental Results 16 ", "page_idx": 15}, {"type": "text", "text": "A.1 Contrast with LSTMs 16   \nA.2 Additional Results on Isotropic Data without Noise 17   \nA.2.1 Progression of Algorithms 17   \nA.2.2 Heatmaps 17   \nA.2.3 Comparison with Other Second-Order Methods 20   \nA.2.4 Additional Results on Comparison over Transformer Layers 22   \nA.2.5 Additional Results on Similarity of Induced Weights 22   \nA.3 Varying Data Distribution or Function Class 23   \nA.3.1 Experiments on Ill-Conditioned Problems 23   \nA.3.2 Experiments with Noisy Linear Regression 25   \nA.3.3 Experiments with a Non-Linear Function Class (2-Layer MLP) 25   \nA.4 Varying Transformer Architecture 27   \nA.4.1 Experiments on Transformers of Fewer Heads . 27   \nA.4.2 Experiments on Transformers with More Layers 28   \nA.5 Heatmaps with Best-Matching Steps Help Compare Convergence Rates 29   \nA.6 Definitions for Evaluating Forgetting 29   \nB Detailed Proofs for Section 5 30   \nB.1 Helper Results . . 30   \nB.2 Proof of Theorem 5.1 31   \nB.3 Iterative Newton as a Sum of Moments Method 34   \nB.4 Estimated weight vectors lie in the span of previous examples 35 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Computes 36 ", "page_idx": 15}, {"type": "text", "text": "D License 36 ", "page_idx": 15}, {"type": "text", "text": "E Limitations 36 ", "page_idx": 15}, {"type": "text", "text": "F Broader Impacts 36 ", "page_idx": 15}, {"type": "text", "text": "A Additional Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Contrast with LSTMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "While our primary goal is to analyze Transformers, we also consider LSTMs [Hochreiter and Schmidhuber, 1997] to understand whether Transformers learn different algorithms than other neural sequence models trained to do linear regression. In particular, we train a unidirectional $L$ -layer LSTM, which generates a sequence of hidden states $\\bar{H}^{(\\ell)}$ for each layer $\\ell$ , similarly to an $L$ -layer Transformer. As with Transformers, we add a readout layer that predicts the y\u02c6tL+S1T from the final hidden state at the final layer, H:(,2Lt)+1. ", "page_idx": 15}, {"type": "table", "img_path": "L8h6cozcbn/tmp/ec91fb926f7782eddd58dd79ffad4075663a744c6e6f2a247c76383c25e562e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 1: Similarity of errors between algorithms. Transformers are more similar to full-observation methods such as Newton and GD; and LSTMs are more similar to online methods such as OGD. ", "page_idx": 16}, {"type": "text", "text": "We train a 10-layer LSTM model, with 5.3M parameters, in an identical manner to the Transformers (with 9.5M parameters) studied in the previous sections.5 ", "page_idx": 16}, {"type": "text", "text": "LSTMs\u2019 inferior performance to Transformers can be explained by the inability of LSTMs to use deeper layers to improve their predictions. Figure 7 shows that LSTM performance does not improve across layers\u2014a readout head fine-tuned for the first layer makes equally good predictions as the full 10-layer model. Thus, LSTMs seem poorly equipped to fully implement iterative algorithms. Similarly, Table 1 shows that LSTMs are more similar to OGD than Transformers are, whereas Transformers are more similar to Newton and GD than LSTMs. ", "page_idx": 16}, {"type": "text", "text": "A.2 Additional Results on Isotropic Data without Noise ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.2.1 Progression of Algorithms ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "L8h6cozcbn/tmp/a14041bc8754c0740541aa3c723f93e2bbaead6136a3c4a0de6c6d585f39b886.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 7: Progression of Algorithms. (a) Transformer\u2019s performance improves over the layer index \u2113. (b) Iterative Newton\u2019s performance improves over the number of iterations $k$ , in a way that closely resembles the Transformer. We plot the best-matching $k$ to Transformer\u2019s $\\ell$ following Definition 3.4. (c) In contrast, LSTM\u2019s performance does not improve from layer to layer. ", "page_idx": 16}, {"type": "text", "text": "A.2.2 Heatmaps", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present heatmaps with all values of similarities. ", "page_idx": 16}, {"type": "image", "img_path": "L8h6cozcbn/tmp/95623239212dc9991b9bb823f9beb89f03e03faefd09748fdf740f910c21a6cc.jpg", "img_caption": ["Figure 8: Similarity of Errors. The best matching steps are highlighted in yellow. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "L8h6cozcbn/tmp/41c31567f069ec2556c11d9ad8b0475d16e50de4276b20765a66be026d80d499.jpg", "img_caption": ["Figure 9: Similarity of Induced Weight Vectors. The best matching steps are highlighted in yellow. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "L8h6cozcbn/tmp/b7a25a4dd3b03f8670d8927479ac58ce9841629170eca10f5d9e4bbfc2681dd2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 10: Similarity of Errors of Gradient Descent in Log Scale. The best matching steps are highlighted in yellow. Putting the number of steps of Gradient Descent in log scale further verifies the claim that Transformer\u2019s rate of covergence is exponentially faster than that of Gradient Descent. ", "page_idx": 18}, {"type": "text", "text": "A.2.3 Comparison with Other Second-Order Methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we ablate with alternative second-order methods, such as Conjugate Gradient, BFGS, and its limited memory variant, L-BFGS. ", "page_idx": 19}, {"type": "text", "text": "Conjugate Gradient Method. For linear regression problems, the Conjugate Gradient (CG) method solves the linear system ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underbrace{(\\boldsymbol{X}^{\\top}\\boldsymbol{X})}_{\\boldsymbol{s}}\\boldsymbol{w}-\\boldsymbol{X}^{\\top}\\boldsymbol{y}=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "CG finds the weight vector $\\hat{w}^{C G}$ with initialization $\\pmb{w}_{0}$ by maintain a set of conjugate gradient $\\left\\{\\Delta\\pmb{w}_{1},\\cdot\\cdot\\cdot\\,,\\Delta\\pmb{w}_{k}\\right\\}$ . It follows the iterative update rule ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{k}=-\\nabla{\\mathcal L}(w_{k})}\\\\ &{\\Delta w_{k}=d_{k}-\\displaystyle\\sum_{i=0}^{k-1}\\frac{d_{k}^{\\top}S\\Delta w_{i}}{\\Delta w_{i}^{\\top}S\\Delta w_{i}}\\Delta w_{i}}\\\\ &{\\alpha_{k}=\\arg\\displaystyle\\operatorname*{min}_{\\alpha}{\\mathcal L}(w_{k}+\\alpha\\Delta w_{k})}\\\\ &{w_{k+1}=w_{k}+\\alpha_{k}\\Delta w_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The conjugate Gradient method requires ${\\mathcal{O}}\\left({\\sqrt{\\kappa}}\\log(1/\\epsilon)\\right)$ steps to converge to an $\\epsilon$ error on quadratic objectives such as linear regression. ", "page_idx": 19}, {"type": "text", "text": "BFGS. Broyden\u2013 Fletcher\u2013Goldfarb\u2013Shanno (BFGS) is a Quasi-Newton method, designed to approximate the inverse Hessian $B_{k}:\\approx\\nabla^{2}\\mathcal{L}(\\pmb{w}_{k})^{-1}$ . The BFGS updates are given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{w}_{k+1}=\\pmb{w}_{k}-\\alpha_{k}\\pmb{B}_{k}\\nabla\\mathcal{L}(\\pmb{w}_{k})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbf{\\boldsymbol{s}}_{k}=\\boldsymbol{\\boldsymbol{w}}_{k+1}-\\boldsymbol{\\boldsymbol{w}}_{k}}\\\\ {\\displaystyle\\mathbf{\\boldsymbol{y}}_{k}=\\nabla{\\mathcal{L}}\\big(\\mathbf{\\boldsymbol{w}}_{k+1}\\big)-\\nabla{\\mathcal{L}}\\big(\\mathbf{\\boldsymbol{w}}_{k}\\big)}\\\\ {\\displaystyle B_{k+1}=B_{k}-\\frac{B_{k}\\boldsymbol{y}_{k}\\boldsymbol{y}_{k}^{\\top}B_{k}}{\\boldsymbol{y}_{k}^{\\top}B_{k}\\boldsymbol{y}_{k}}+\\frac{\\mathbf{\\boldsymbol{s}}_{k}\\mathbf{\\boldsymbol{s}}_{k}^{\\top}}{\\mathbf{\\boldsymbol{y}}_{k}^{\\top}s_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When $k$ is large, $B_{k}$ approximates the inverse Hessian well. ", "page_idx": 19}, {"type": "text", "text": "L(imited-memory)-BFGS. L-BFGS is a limited-memory version of BFGS. Instead of the inverse Hessian $B_{k}$ , L-BFGS maintains a history of past $m$ updates (where $m$ is usually small). Recall the iterative update rule of $B_{k}$ in BFGS ", "page_idx": 19}, {"type": "equation", "text": "$$\nB_{k+1}=B_{k}-{\\frac{B_{k}y_{k}y_{k}^{\\top}B_{k}}{y_{k}^{\\top}B_{k}y_{k}}}+{\\frac{s_{k}s_{k}^{\\top}}{y_{k}^{\\top}s_{k}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Unlike BFGS, which recursively unroll to an initialization $B_{0}$ , L-BFGS only unroll to $B_{k-m}$ but replacing $B_{k-m}$ with $B_{\\mathrm{init}}$ . In this regard, running $n$ steps of L-BFGS only requires $\\mathcal{O}(m n)$ memory, which is more memory-efficient than BFGS who requires $O(n^{2})$ memory. The trade-off is that LBFGS won\u2019t have a good estimate of the inverse Hessian when $m<d$ , where $d$ is the dimensionality of the quadratic problem. In this regard, it will converge slower than full BFGS. ", "page_idx": 19}, {"type": "text", "text": "In Figure 11 and Figure 12, we compare Transformers with BFGS, L-BFGS, and Conjugate Gradient method on the metric of similarity of errors. We find that Transformers have a similar linear correspondence with BFGS. This is perhaps not surprising, given that BFGS also gets a superlinear convergence rate for linear regression Nocedal and Wright [1999]. Meanwhile, Transformers show a substantially faster convergence rate than L-BFGS and CG. ", "page_idx": 19}, {"type": "image", "img_path": "L8h6cozcbn/tmp/29e287d118bdb59b78173c3adf6a0453e7222ece47e5a810b46783987a8fdac5.jpg", "img_caption": ["Figure 11: Similarity of Errors between Transformers and BFGS or L-BFGS. The best matching steps are highlighted in yellow. We find that Transformer, from layers 6 to 11, has a linear correspondence with BFGS. For L-BFGS, due to its limited memory, it approximates second-order information more slowly and results in a slower convergence rate than Transformers. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "L8h6cozcbn/tmp/ce9ff197c6c311dddb4ad27f969bca9a9c58a7bb06ea152ee71fedd0e7e862c5.jpg", "img_caption": ["Figure 12: Similarity of Errors between Transformers and Conjugate Gradient. Transformer\u2019s convergence rate is still faster than conjugate gradient methods. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "L8h6cozcbn/tmp/90a67ce43b19a602f2b75a37493a60378920fa4c03bd7df17fe62d09250a386a.jpg", "img_caption": ["Figure 13: Similarities between Transformer and candidate algorithms. Transformers resemble Iterative Newton\u2019s Method the most. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.2.5 Additional Results on Similarity of Induced Weights ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present more details line plots for how the similarity of weights changes as the models see more in-context observations $\\{\\pmb{x}_{i},y_{i}\\}_{i=1}^{n}$ , i.e., as $n$ increases. We fix the number of Transformers layers $\\ell$ and compare with other algorithms with their best-match steps to $\\ell$ in Figure 14. ", "page_idx": 21}, {"type": "image", "img_path": "L8h6cozcbn/tmp/1fbe22d81b28abf7985c450c5bb42ad78e6f22c6b0615e6d1b375f13468895cc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 14: Similarity of induced weights over varying number of in-context examples, on three layer indices of Transformers, indexed as 2, 3 and 12. We find that initially at layer 2, the Transformers model hasn\u2019t learned so it has zero similarity to all candidate algorithms. As we progress to the next layer number 3, we find that Transformers start to learn, and when provided few examples, Transformers are more similar to OLS but soon become most similar to the Iterative Newton\u2019s Method. Layer 12 shows that Transformers in the later layers converge to the OLS solution when provided more than 1 example. We also find there is a dip around $n\\,=\\,d$ for similarity between Transformers and OLS but not for Transformers and Newton, and this is probably because OLS has a more prominent double-descent phenomenon than Transformers and Newton. ", "page_idx": 21}, {"type": "text", "text": "A.3 Varying Data Distribution or Function Class ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "A.3.1 Experiments on Ill-Conditioned Problems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we repeat the same experiments as we did on isotropic data in the main text and in Appendix A.2, and we change the covariance matrix to be ill-conditioned such that $\\kappa(\\Sigma)=100$ . ", "page_idx": 22}, {"type": "image", "img_path": "L8h6cozcbn/tmp/94f944eb771e4ea62414c8e3687f57b400de44b5fc54e84463873831b80c9d53.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 15: Progression of Algorithms on Ill-Conditioned Data. Transformer\u2019s performance still improves over the layer index \u2113; Iterative Newton\u2019s Method\u2019s performance improves over the number of iterations $t$ and we plot the best-matching $t$ to Transformer\u2019s $\\ell$ following Definition 3.4. ", "page_idx": 22}, {"type": "text", "text": "We also present the heatmaps to find the best-matching steps and conclude that Transformers are similar to Newton\u2019s method than GD in ill-conditioned data. ", "page_idx": 22}, {"type": "image", "img_path": "L8h6cozcbn/tmp/9bf612f3214f7fb95820f142571f3f8a254a08e0d916499295481691b0b2d45b.jpg", "img_caption": ["Figure 16: Similarity of Errors on Ill-Conditioned Data. The best matching steps are highlighted in yellow. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "L8h6cozcbn/tmp/80127ef5608cc545ce4ac79bb589dcdba1230e637b90c10267608612eb77fcda.jpg", "img_caption": ["Figure 17: Similarity of Induced Weights on Ill-Conditioned Data. The best matching steps are highlighted in yellow. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "L8h6cozcbn/tmp/bb6317030628da20c2d686ef1bf47ae643ec3271743167f6d2ccffbb1cf66306.jpg", "img_caption": ["Figure 18: Similarity of Errors on Ill-Conditioned Data with Quasi-Newton Methods. The best matching steps are highlighted in yellow. Transformer also matches BFGS linearly, from layers 4 to 11. L-BFGS still suffers due to its limited memory but still better than Gradient Descentbecause L-BFGS also attempts to approximate second-order information. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.3.2 Experiments with Noisy Linear Regression ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We repeat the same experiments on noisy linear regression tasks with $y=\\pmb{w}^{\\top}\\pmb{x}+\\varepsilon$ where $\\varepsilon\\sim$ $\\mathcal{N}(0,\\dot{\\sigma}^{2})$ with noise level $\\sigma\\,=\\,0.1$ . As shown in Figure 19, Transformers still show superlinear convergence on noisy linear regression tasks. Since the predictor is $\\hat{w}=\\left(X^{\\top}X+\\lambda I\\right)^{\\dagger}X^{\\top}y$ for some $\\lambda$ , the iterative newton\u2019s method is applied to $S=X^{\\top}X+\\lambda I$ . Iterative Newton\u2019s method still keeps the same superlinear convergence rates. As it\u2019s also shown in Figure 19, Transformers and Iternative Newton\u2019s rates match linearly, as in the noiseless linear regression tasks. ", "page_idx": 24}, {"type": "image", "img_path": "L8h6cozcbn/tmp/346ddb7dba942e16ad84a9465eeb345d9c5f93df0775ab66c16ca39e4c6ccaf3.jpg", "img_caption": ["Figure 19: Experiment results on Noisy Linear Regression. (Top) Transformers have superlinear convergence rate. (Bottom) Transformers match Iterative Newton\u2019s rate and are exponentially faster than Gradient Descent. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "A.3.3 Experiments with a Non-Linear Function Class (2-Layer MLP) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To extend our experiments to non-linear cases, we adopt the same 2-layer ReLU neural network studied by Garg et al. [2022]: see Fig. 5(c) in their paper. For any prompt $(\\pmb{x}_{1},y_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{x}_{t},y_{t})$ , instead of generating labels $y_{k}=w^{\\star\\top}x$ as mainly studied in the paper, we study a 2-layer neural network function class parameterized by $W\\in\\mathbb{R}^{\\tilde{d}_{\\mathrm{hidden}}\\times d}$ , $\\pmb{v}\\in\\mathbb{R}^{\\pmb{\\lambda}_{d_{\\mathrm{hidden}}}}$ , $\\pmb{a}\\in\\mathbb{R}^{\\check{d}_{\\mathrm{hidden}}}$ , and $b\\in\\mathbb{R}$ , so that ", "page_idx": 24}, {"type": "equation", "text": "$$\ny_{k}=f_{W,v,a,b}(x_{k})=a^{\\top}\\mathrm{ReLU}\\Big(W x_{k}+v\\Big)+b\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we repeat the same probing experiments as in the main paper. As shown in Figure 20, even on 2-layer neural network tasks with ReLU activation, Transformer shows superlinear convergence rates. Transformer shows an exponentially faster convergence rate than Gradient Descent\u2019s, because ", "page_idx": 24}, {"type": "text", "text": "Gradient Descent\u2019s steps are shown in log scale and the trend is linear \u2013 similar to Figure 9 in the main paper. ", "page_idx": 25}, {"type": "image", "img_path": "L8h6cozcbn/tmp/3d47a1d4525fc6dd1c2335db4d3c6e124943a5039d8825ca83c875e136506154.jpg", "img_caption": ["Figure 20: Empirical Results on 2-Layer Neural Network Regression with ReLU activation function. Transformers have superlinear convergence rates and match Gradient Descent\u2019s convergence rate exponentially "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "It would be interesting to ablate the activation function used in Equation (16). We further consider the case when it\u2019s using the Tanh activation instead of ReLU, i.e. ", "page_idx": 25}, {"type": "equation", "text": "$$\ny_{k}=f_{W,v,a,b}(x_{k})=a^{\\top}\\mathrm{Tanh}\\Big(W x_{k}+v\\Big)+b\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Repeating the same experiments as before, as shown in Figure 21, we find that Transformers use the entire first 5 layers to pre-process and then only in the next few layers show exponentially faster convergence rate compared to Gradient Descent. We further note that in both Figure 20 and Figure 21, the cosine similarities between Transformers and Gradient Descent are significantly lower than the experiments with linear regression tasks. This might due to the over-parameterization of the function class and Transformers and Gradient Descent may arrive at different optima. ", "page_idx": 25}, {"type": "image", "img_path": "L8h6cozcbn/tmp/697809f7e93024c125e4610614359f3186827ce491f7306d2a8f04ef4dfdc779.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "", "img_caption": ["Figure 21: Empirical Results on 2-Layer Neural Network Regression with Tanh activation function. Transformers have superlinear convergence rates and match Gradient Descent\u2019s convergence rate exponentially "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "It would be interesting for future research to explore further this function class of 2-layer MLP to understand fully how Transformer solve the regression problem in-context and whether it achieves a different optimum compared to alternative algorithms such as (Stochastic) Gradient Descent. ", "page_idx": 25}, {"type": "text", "text": "A.4 Varying Transformer Architecture ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "A.4.1 Experiments on Transformers of Fewer Heads ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we present experimental results from an alternative model configurations than the main text. We show in the main text that Transformers learn second-order optimization methods in-context where the experiments are using a GPT-2 model with 12 layers and 8 heads per layer. In this section, we present experiments with a GPT-2 model with 12 layers but only 1 head per layer. ", "page_idx": 26}, {"type": "image", "img_path": "L8h6cozcbn/tmp/91aca7c977b0c424d40eca70e785e730e297db227aec7f2309c02a4b0381c41f.jpg", "img_caption": ["Figure 22: Similarity of Errors on an alternative Transformers Configuration. The best matching steps are highlighted in yellow. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "L8h6cozcbn/tmp/1a7c132b39049ef897b7a52a7f5eea2243688f41e7ebea89ba58f336ebd92fd7.jpg", "img_caption": ["Figure 23: Similarity of Induced Weights on an alternative Transformers Configuration. The best matching steps are highlighted in yellow. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "We conclude that our experimental results are not restricted to a specific model configurations, smaller models such as GPT-2 with 12 layers and 1 head each layer also suffice in implementing the Iterative Newton\u2019s method, and more similar than gradient descents, in terms of rate of convergence. ", "page_idx": 26}, {"type": "text", "text": "A.4.2 Experiments on Transformers with More Layers ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we investigate whether deeper models would behave similarly or differently. We work on Transformers with 24 layers and 8 heads each. ", "page_idx": 27}, {"type": "image", "img_path": "L8h6cozcbn/tmp/ea0f742c22d7770a29f357275c67c6e2ccfdd4b1a6fbb9a8581f9bc58fa6a249.jpg", "img_caption": ["Figure 24: Similarity of Errors on a 24-layer Transformers Configuration. The best matching steps are highlighted in yellow. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "L8h6cozcbn/tmp/691c2ba38583bf71781dad830be66632f2d811df295692a86e8164403adbc88a.jpg", "img_caption": ["Figure 25: Transformers with 24 layers also converge superlinearly, similar to Iterative Newton. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "A.5 Heatmaps with Best-Matching Steps Help Compare Convergence Rates ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we show the heatmaps with best-matching steps among known algorithms. ", "page_idx": 28}, {"type": "image", "img_path": "L8h6cozcbn/tmp/ccbb774df684efe954e0f0acc7eb818f506c5f7c0af28ba2c0237e86f46332bd.jpg", "img_caption": ["Figure 26: Best-Matching Steps on Similarity of Residuals Help Compare Convergence Rates. (a: top-left) When comparing Iterative Newton and Gradient Descent, there is an exponential trend \u2013 showing Iterative Newton converges exponentially faster than Gradient Descent. (b: top-right) When Iterative Newton is compared with itself in sub-figure, there is a linear trend \u2013 showing they have the same convergence rate. (c: bottom) When Iterative Newton is compared to BFGS in sub-figure, there a linear trend after there are enough steps for BFGS to approximate second-order information \u2013 showing Iterative Newton and BFGS share a similar convergence rate after sufficient BFGS steps. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "A.6 Definitions for Evaluating Forgetting ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We measure the phenomenon of model forgetting by reusing an in-context example within $\\{\\pmb{x}_{i},y_{i}\\}_{i=1}^{n}$ as the test example $\\pmb{x}_{\\mathrm{test}}$ . In experiments of Figure 5, we fix $n\\,=\\,20$ and reuse $\\pmb{x}_{\\mathrm{test}}=\\pmb{x}_{i}$ . We denote the \u201cTime Stamp Gap\u201d as the distance the reused example index $i$ from the current time stamp $n=20$ . We measure the forgetting of index $i$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Forgetting}(\\mathcal{A},i)=\\underset{\\{x_{i},y_{i}\\}_{i=1}^{n}\\sim P_{\\mathcal{D}}}{\\mathbb{E}}\\mathrm{MSE}\\Big(\\mathcal{A}(x_{i}\\mid\\{x_{i},y_{i}\\}_{i=1}^{n}),y_{i}\\Big)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note: the further away $i$ is from $n$ , the more possible algorithm $\\boldsymbol{\\mathcal{A}}$ forgets. ", "page_idx": 28}, {"type": "text", "text": "B Detailed Proofs for Section 5 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we work on full attention layers with normalized ReLU activation $\\begin{array}{r}{\\sigma(\\cdot)=\\frac{1}{n}\\mathrm{ReLU}(\\cdot)}\\end{array}$ given $n$ examples. ", "page_idx": 29}, {"type": "text", "text": "Definition B.1. A full attention layer with $M$ heads and ReLU activation is also denoted as Attn on any input sequence $H=[\\pmb{h}_{1},\\cdot\\cdot\\cdot\\dot{\\mathbf{\\Omega}},\\pmb{h}_{N}]\\in\\mathbb{R}^{D\\times N}$ , where $D$ is the dimension of hidden states and $N$ is the sequence length. In the vector form, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{\\boldsymbol{h}}_{t}=[\\mathrm{Attn}(\\boldsymbol{H})]_{t}=\\boldsymbol{h}_{t}+\\frac{1}{n}\\sum_{m=1}^{M}\\sum_{j=1}^{n}\\mathrm{ReLU}\\left(\\langle\\boldsymbol{Q}_{m}\\boldsymbol{h}_{t},\\boldsymbol{K}_{m}\\boldsymbol{h}_{j}\\rangle\\right)\\cdot\\boldsymbol{V}_{m}\\boldsymbol{h}_{j}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Remark B.2. This is slightly different from the causal attention layer (see Definition 3.1) in that at each time stamp $t$ , the attention layer in Definition B.1 has full information of all hidden states $j\\in[n]$ , unlike causal attention layer which requires $j\\in[t]$ . ", "page_idx": 29}, {"type": "text", "text": "B.1 Helper Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We begin by constructing a useful component for our proof, and state some existing constructions from Aky\u00fcrek et al. [2022]. ", "page_idx": 29}, {"type": "text", "text": "Lemma B.3. Given hidden states $\\{h_{1},\\cdot\\cdot\\cdot,h_{n}\\}$ , there exists query, key and value matrices $Q,K,V$ respectively such that one attention layer can compute $\\textstyle\\sum_{j=1}^{n}h_{j}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. We can pad each hidden state by 1 and 0\u2019s such that $h_{t}^{\\prime}\\;\\gets\\;\\left[\\!\\!{\\begin{array}{c}{h_{t}}\\\\ {1}\\\\ {0_{d}}\\end{array}}\\!\\!\\right]\\;\\in\\;\\mathbb{R}^{2d+1}$ . We construct two heads where $\\pmb{Q}_{1}\\,=\\,{K}_{1}\\,=\\,\\pmb{Q}_{2}\\,=\\,\\left[\\begin{array}{c c c}{\\pmb{O}_{d\\times d}}&{\\pmb{O}_{d\\times1}}&{\\pmb{O}_{d\\times d}}\\\\ {\\pmb{O}_{1\\times d}}&{1}&{\\pmb{O}_{1\\times d}}\\\\ {\\pmb{O}_{d\\times d}}&{\\pmb{O}_{d\\times1}}&{\\pmb{O}_{d\\times d}}\\end{array}\\right]$ and $K_{2}\\;=\\;-K_{1}$ . Then ${\\left[\\!\\!\\begin{array}{l l l}{\\!O_{d\\times d}}&{\\!O_{d\\times1}\\!}&{\\!O_{d\\times d}\\!}\\\\ {O_{1\\times d}}&{1}&{O_{1\\times d}\\!}\\\\ {O_{d\\times d}}&{O_{d\\times1}}&{O_{d\\times d}}\\end{array}\\!\\!\\right]}\\,h_{t}^{\\prime}={\\left[\\!\\!\\begin{array}{l}{\\![\\mathbf{0}_{d}]\\!}\\\\ {1}\\\\ {\\!0_{d}}\\end{array}\\!\\!\\right]}.$ Let $V_{1}=V_{2}=\\left[\\!\\!{\\begin{array}{c c}{\\pmb{{\\cal O}}_{(d+1)\\times d}}&{\\pmb{{\\cal O}}_{(d+1)\\times(d+1)}}\\\\ {n\\pmb{{\\cal I}}_{d\\times d}}&{\\pmb{{\\cal O}}_{d\\times(d+1)}}\\end{array}}\\!\\!\\!\\right]{\\mathrm{~so~that~}}V_{m}\\left[\\!\\!{\\begin{array}{c}{h_{j}}\\\\ {1}\\\\ {0_{d}}\\end{array}}\\!\\!\\right]=\\left[\\!\\!{\\begin{array}{c}{\\pmb{{\\cal0}}_{d+1}}\\\\ {n h_{j}}\\end{array}}\\!\\!\\right].$ ", "page_idx": 29}, {"type": "text", "text": "We apply one attention layer to these 1-padded hidden states and we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{h}_{t}=h_{t}^{\\prime}+\\frac{1}{n}\\sum_{m=1}^{2}\\sum_{j=1}^{n}\\mathrm{ReLU}\\left(\\left\\langle Q_{m}h_{t}^{\\prime},K_{m}h_{j}^{\\prime}\\right\\rangle\\right)\\cdot V_{m}h_{j}^{\\prime}}}\\\\ {~~}\\\\ {{\\displaystyle~=h_{t}^{\\prime}+\\frac{1}{n}\\sum_{j=1}^{n}\\left[\\mathrm{ReLU}(1)+\\mathrm{ReLU}(-1)\\right]\\cdot\\left[\\!\\!\\begin{array}{c}{{\\bf0}_{d+1}}\\\\ {{n h_{j}}}\\end{array}\\!\\!\\right]}}\\\\ {~~}\\\\ {{\\displaystyle~=\\left[\\!\\!\\begin{array}{c}{{h_{t}}}\\\\ {{\\bf1}}\\end{array}\\!\\!\\right]+\\left[\\sum_{j=1}^{0_{d+1}}\\!\\!\\!h_{j}\\right]=\\left[\\!\\!\\begin{array}{c}{{h_{t}}}\\\\ {{1}}\\\\ {{\\sum_{j=1}^{n}h_{j}}}\\end{array}\\!\\!\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proposition B.4 (Aky\u00fcrek et al., 2022). Each of mov, aff, mul, div can be implemented by a single transformer layer. These four operations are mappings $\\mathbb{R}^{D\\times N}\\rightarrow\\mathbb{R}^{D\\times N}$ , expressed as follows, $\\mathsf{m o v}(H;s,t,i,j,i^{\\prime},j^{\\prime})$ : selects the entries of the $s$ -th column of $\\pmb{H}$ between rows $i$ and $j$ , and copies them into the $t$ -th column $(t\\geq s,$ ) of $\\pmb{H}$ between rows $i^{\\prime}$ and $j^{\\prime}$ . ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "$\\mathtt{m u l}(H;a,b,c,(i,j),(i^{\\prime},j^{\\prime}),(i^{\\prime\\prime},j^{\\prime\\prime}))$ : in each column $^h$ of $\\pmb{H}$ , interprets the entries between i and $j$ as an $a\\times b$ matrix $A_{1}$ , and the entries between $i^{\\prime}$ and $j^{\\prime}$ as a $b\\times c$ matrix $A_{2}$ , multiplies these matrices together, and stores the result between rows $i^{\\prime\\prime}$ and $j^{\\prime\\prime}$ , yielding a matrix in which each column has the form $\\left[{{h}_{:i^{\\prime\\prime}-1}},{{A}_{1}}{{A}_{2}},{{h}_{j^{\\prime\\prime}:}}\\right]^{\\top}$ . This allows the layer to implement inner products. ", "page_idx": 29}, {"type": "text", "text": "$\\mathtt{d i v}(H;(i,j),i^{\\prime},(i^{\\prime\\prime},j^{\\prime\\prime}))$ : in each column $^h$ of $\\pmb{H}$ , divides the entries between $i$ and $j$ by the absolute value of the entry at $i^{\\prime}$ , and stores the result between rows $i^{\\prime\\prime}$ and $j^{\\prime\\prime}$ , yielding a matrix in which every column has the form $[h_{:i^{\\prime\\prime}-1},h_{i:j}/|h_{i^{\\prime}}|,h_{j^{\\prime\\prime}:}]^{\\top}$ . ", "page_idx": 30}, {"type": "text", "text": "${\\mathfrak{a f f}}(H;(i,j),(i^{\\prime},j^{\\prime}),(i^{\\prime\\prime},j^{\\prime\\prime}),W_{1},W_{2},\\mathbf{b})$ : in each column $^h$ of $\\pmb{H}$ , applies an affine transformation to the entries between $i$ and $j$ and $i^{\\prime}$ and $j^{\\prime}$ , then stores the result between rows $i^{\\prime\\prime}$ and $j^{\\prime\\prime}$ , yielding a matrix in which every column has the form $[h_{:i^{\\prime\\prime}-1},W_{1}h_{i:j}+W_{2}h_{i^{\\prime}:j^{\\prime}}+{\\bf b},h_{j^{\\prime\\prime}:}]^{\\top}$ . This allows the layer to implement subtraction by setting $W_{1}=I$ and $W_{2}=-I$ . ", "page_idx": 30}, {"type": "text", "text": "B.2 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Theorem 5.1. For any $k$ , there exist Transformer weights such that on any set of in-context examples $\\{{x}_{i},{y}_{i}\\}_{i=1}^{n}$ and test point $\\pmb{x}_{\\mathrm{test}}$ , the Transformer predicts on $\\mathbf{\\deltax}_{\\mathrm{test}}$ using $\\bar{\\mathbf{x}}_{\\mathrm{test}}^{\\top}\\hat{w}_{k}^{\\mathrm{N}}$ w\u02c6Newton. Here $\\hat{\\pmb{w}}_{k}^{\\mathrm{Newton}}$ are the Iterative Newton updates given by $\\hat{\\pmb{w}}_{k}^{\\mathrm{Newton}}=M_{k}\\pmb{X}^{\\top}\\pmb{y}$ where $M_{j}$ is updated as ", "page_idx": 30}, {"type": "equation", "text": "$$\nM_{j}=2M_{j-1}-M_{j-1}S M_{j-1},1\\leq j\\leq k,\\quad M_{0}=\\alpha S,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for some $\\alpha>0$ and $S=X^{\\top}X$ . The dimensionality of the hidden layers is $O(d)$ , and the number of layers is $k+8$ . One transformer layer computes one Newton iteration. 3 initial transformer layers are needed for initializing $M_{0}$ and 5 layers at the end are needed to read out predictions from the computed pseudo-inverse $M_{k}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. We break the proof into parts. ", "page_idx": 30}, {"type": "text", "text": "Transformers Implement Initialization $\\pmb{T}^{(0)}=\\alpha\\pmb{S}$ . Given input sequence $H:=\\{\\pmb{x}_{1},\\cdot\\cdot\\cdot,\\pmb{x}_{n}\\}$ , with $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{d}$ , we first apply the mov operations given by Proposition B.4 (similar to Aky\u00fcrek et al. [2022], we show only non-zero rows when applying these operations): ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\left[\\begin{array}{l l l}{x_{1}}&{\\cdot\\cdot\\cdot}&{x_{n}}\\\\ &&{}\\end{array}\\right]}\\,{\\xrightarrow{\\mathrm{mov}}}\\,\\left[x_{1}\\quad\\cdot\\cdot\\cdot\\quad x_{n}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We call each column after mov as $h_{j}$ . With an full attention layer, one can construct two heads with query and value matrices of the form $Q_{1}^{\\top}K_{1}\\,=\\,-Q_{2}^{\\top}K_{2}\\,=\\,\\left[\\!\\!\\begin{array}{c c}{{I_{d\\times d}}}&{{O_{d\\times d}}}\\\\ {{O_{d\\times d}}}&{{O_{d\\times d}}}\\end{array}\\!\\!\\right]$ such that for any $t\\in[n]$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{2}\\mathrm{ReLU}\\left(\\langle Q_{m}h_{t},K_{m}h_{j}\\rangle\\right)=\\mathrm{ReLU}(\\pmb{x}_{t}^{\\top}\\pmb{x}_{j})+\\mathrm{ReLU}(-\\pmb{x}_{t}^{\\top}\\pmb{x}_{j})=\\langle\\pmb{x}_{t},\\pmb{x}_{j}\\rangle\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let all value matrices $V_{m}=n\\alpha\\left[\\!\\!\\begin{array}{c c}{{I_{d\\times d}}}&{{O_{d\\times d}}}\\\\ {{O_{d\\times d}}}&{{O_{d\\times d}}}\\end{array}\\!\\!\\right]$ OOd\u00d7d for some \u03b1 \u2208R. Combining the skip connections, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\tilde{h}}_{t}=\\left[{\\pmb x}_{t}\\right]+\\frac{1}{n}\\sum_{j=1}^{n}\\left\\langle{\\pmb x}_{t},{\\pmb x}_{j}\\right\\rangle n\\alpha\\left[{\\pmb x}_{j}\\right]=\\left[{\\pmb x}_{t}\\right]+\\left[\\alpha\\left(\\sum_{j=1}^{n}x_{j}x_{j}^{\\top}\\right){\\pmb x}_{t}\\right]=\\left[{\\pmb x}_{t}+\\alpha S{\\pmb x}_{t}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now we can use the aff operator to make subtractions and then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left[{\\pmb x}_{t}+\\alpha{\\pmb S}{\\pmb x}_{t}\\right]\\,\\frac{\\mathrm{af}}{\\mathrm{\\pmbx}_{t}}\\,\\left[({\\pmb x}_{t}+\\alpha{\\pmb S}{\\pmb x}_{t})-{\\pmb x}_{t}\\right]=\\left[\\alpha{\\pmb S}{\\pmb x}_{t}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We call this transformed hidden states as $H^{(0)}$ and denote $\\pmb{T}^{(0)}=\\alpha\\pmb{S}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pmb{H}^{(0)}=\\left[\\pmb{h}_{1}^{(0)}\\quad.\\cdot\\cdot\\quad\\pmb{h}_{n}^{(0)}\\right]=\\left[\\pmb{T}^{(0)}\\pmb{x}_{1}\\quad.\\cdot\\cdot\\quad\\pmb{T}^{(0)}\\pmb{x}_{n}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Notice that $\\boldsymbol{S}$ is symmetric and thereafter ${\\pmb T}^{(0)}$ is also symmetric. ", "page_idx": 30}, {"type": "text", "text": "Transformers implement Newton Iteration. Let the input prompt be the same as Equation (24), ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pmb{H}^{(0)}=\\left[\\pmb{h}_{1}^{(0)}\\quad.\\cdot\\cdot\\quad\\pmb{h}_{n}^{(0)}\\right]=\\left[\\pmb{T}^{(0)}\\pmb{x}_{1}\\quad.\\cdot\\cdot\\quad\\pmb{T}^{(0)}\\pmb{x}_{n}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We claim that the $\\ell$ \u2019s hidden states can be of the similar form ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\pmb{H}^{(\\ell)}=\\left[\\pmb{h}_{1}^{(\\ell)}\\quad.\\cdot\\cdot\\quad\\pmb{h}_{n}^{(\\ell)}\\right]=\\left[\\pmb{T}^{(\\ell)}\\pmb{x}_{1}\\quad.\\cdot\\cdot\\quad\\pmb{T}^{(\\ell)}\\pmb{x}_{n}\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We prove by induction that assuming our claim is true for $\\ell$ , we work on $\\ell+1$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Let}\\,Q_{m}=\\tilde{Q}_{m}\\underbrace{\\left[\\!Q_{d}\\!\\!\\!}_{Q_{d}}\\!\\!\\!\\!}_{G}\\!\\!\\!\\!}&{-\\frac{n}{2}I_{d}\\right],K_{m}=\\tilde{K}_{m}\\underbrace{\\left[\\!L_{d}\\quad\\!\\!O_{d}\\right]}_{J}\\mathrm{where}\\ \\tilde{Q}_{1}^{\\top}\\tilde{K}_{1}:=I,\\tilde{Q}_{2}^{\\top}\\tilde{K}_{2}:=-I\\mathrm{\\ar}}\\\\ &{V_{1}=V_{2}=\\underbrace{\\left[\\!L_{d}\\quad\\!\\!O_{d}\\right]}_{J}.\\mathrm{\\A2-head\\self\\-attention\\layer,\\,with\\ReLU\\atentions,\\,can\\be\\writen\\,has}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\nh_{t}^{(\\ell+1)}=[\\mathrm{Attn}(H^{(\\ell)})]_{t}=h_{t}^{(\\ell)}+\\frac{1}{n}\\sum_{m=1}^{2}\\sum_{j=1}^{n}\\mathrm{ReLU}\\left(\\left\\langle Q_{m}h_{t}^{(\\ell)},K_{m}h_{j}^{(\\ell)}\\right\\rangle\\right)\\cdot V_{m}h_{j}^{(\\ell)}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{m=1}{\\overset{2}{\\sum}}\\operatorname{ReLU}\\left(\\left\\langle Q_{m}h_{t}^{(\\ell)},K_{m}h_{j}^{(\\ell)}\\right\\rangle\\right)\\cdot V_{m}h_{j}^{(\\ell)}}\\\\ &{=\\left[\\operatorname{ReLU}\\Big((G h_{t}^{(\\ell)})^{\\top}\\underset{I}{\\overset{\\sim}{\\underbrace{\\mathcal{Q}_{1}^{\\top}}}\\tilde{K}_{1}}(J h_{j}^{(\\ell)})\\Big)+\\operatorname{ReLU}\\Big((G h_{t}^{(\\ell)})^{\\top}\\underset{-I}{\\overset{\\sim}{\\underbrace{\\mathcal{Q}_{2}^{\\top}}}\\tilde{K}_{2}}(J h_{j}^{(\\ell)})\\Big)\\Big]\\cdot(J h_{j}^{(\\ell)})}\\\\ &{=\\left[\\operatorname{ReLU}((G h_{t}^{(\\ell)})^{\\top}(J h_{j}^{(\\ell)}))+\\operatorname{ReLU}(-(G h_{t}^{(\\ell)})^{\\top}(J h_{j}^{(\\ell)}))\\right]\\cdot(J h_{j}^{(\\ell)})}\\\\ &{=(G h_{j}^{(\\ell)})^{\\top}(J h_{j}^{(\\ell)})(J h_{j}^{(\\ell)})}\\\\ &{=(J h_{j}^{(\\ell)})(J h_{j}^{(\\ell)})^{\\top}(G h_{t}^{(\\ell)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plug in our assumptions that $\\pmb{h}_{j}^{(\\ell)}=\\left[\\pmb{T}_{\\pmb{x}_{j}}^{(\\ell)}\\pmb{x}_{j}\\right]$ , we have $J h_{j}^{(\\ell)}=\\left[\\mathbf{T}_{0_{d}}^{(\\ell)}\\mathbf{x}_{j}\\right]$ and $G h_{t}^{(\\ell)}=\\Biggl[-\\frac{n}{2}{\\pmb x}_{t}\\Biggr],$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{h_{t}^{(\\ell+1)}=\\displaystyle\\left[\\!\\!\\begin{array}{c}{{T^{(\\ell)}x_{t}}}\\\\ {{x_{t}}}\\end{array}\\!\\!\\right]{}+\\displaystyle\\frac1{n}\\sum_{j=1}^{n}\\left[\\!\\!\\begin{array}{c}{{T^{(\\ell)}x_{j}}}\\\\ {{0_{d}}}\\end{array}\\!\\!\\right]{T}^{(\\ell)}x_{j}\\right]^{\\top}\\left[\\!\\!\\begin{array}{c}{{-\\frac n2}x_{t}}\\\\ {{0_{d}}}\\end{array}\\!\\!\\right]}}\\\\ {{\\displaystyle=\\left[\\!\\!\\begin{array}{c}{{T^{(\\ell)}x_{t}-\\frac12\\sum_{j=1}^{n}(T^{(\\ell)}x_{j})(T^{(\\ell)}x_{j})^{\\top}x_{t}}}\\\\ {{x_{t}}}\\end{array}\\!\\!\\right]}}\\\\ {{\\displaystyle=\\left[\\!\\!\\begin{array}{c}{{T^{(\\ell)}x_{t}-\\frac12T^{(\\ell)}\\left(\\sum_{j=1}^{n}x_{j}x_{j}^{\\top}\\right)T^{(\\ell)}{}^{\\top}x_{t}}}\\\\ {{x_{t}}}\\end{array}\\!\\!\\right]}}\\\\ {{\\displaystyle=\\left[\\!\\!\\left(T^{(\\ell)}-\\frac12T^{(\\ell)}S T^{(\\ell)^{\\top}}\\right)x_{t}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now we pass over an MLP layer with ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t}^{(\\ell+1)}\\leftarrow h_{t}^{(\\ell+1)}+\\left[\\!\\!\\begin{array}{c c}{I_{d}}&{O_{d}}\\\\ {O_{d}}&{O_{d}}\\end{array}\\!\\!\\right]h_{t}^{(\\ell+1)}=\\left[\\!\\!\\begin{array}{c c}{\\left(2T^{(\\ell)}-T^{(\\ell)}S{T^{(\\ell)}}^{\\top}\\right)x_{t}}\\\\ {x_{t}}\\end{array}\\!\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now we denote the iteration ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\pmb{T}^{(\\ell+1)}=2\\pmb{T}^{(\\ell)}-\\pmb{T}^{(\\ell)}\\pmb{S}\\pmb{T}^{(\\ell)}^{\\top}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We find that $\\pmb{T}^{(\\ell+1)^{\\top}}=\\pmb{T}^{(\\ell+1)}$ since ${\\pmb T}^{(\\ell)}$ and $\\boldsymbol{S}$ are both symmetric. It reduces to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\pmb{T}^{(\\ell+1)}=2\\pmb{T}^{(\\ell)}-\\pmb{T}^{(\\ell)}\\pmb{S}\\pmb{T}^{(\\ell)}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This is exactly the same as the Newton iteration. ", "page_idx": 31}, {"type": "text", "text": "Transformers can implement $\\hat{\\pmb{w}}_{\\ell}^{\\mathrm{TF}}\\,=\\,{\\pmb{T}}^{(\\ell)}{\\pmb{X}}^{\\top}{\\pmb{y}}$ . Going back to the empirical prompt format $\\{x_{1},y_{1},\\cdot\\cdot\\cdot,x_{n},y_{n}\\}$ . We can let parameters be zero for positions of $y$ \u2019s and only rely on the skip ", "page_idx": 31}, {"type": "text", "text": "connection up to layer $\\ell$ , and the $H^{(\\ell)}$ is then $\\left[\\begin{array}{c c}{\\mathbf{\\boldsymbol{T}}^{(\\ell)}\\mathbf{\\boldsymbol{x}}_{j}}&{\\mathbf{\\boldsymbol{0}}}\\\\ {\\mathbf{\\boldsymbol{x}}_{j}}&{\\mathbf{\\boldsymbol{0}}}\\\\ {\\boldsymbol{0}}&{y_{j}}\\end{array}\\right]_{j=1}^{n}$ . We again apply operations from Proposition B.4: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left[\\!\\!{\\begin{array}{c c}{T^{(\\ell)}x_{j}}&{\\mathbf{0}}\\\\ {x_{j}}&{\\mathbf{0}}\\\\ {0}&{y_{j}}\\end{array}}\\!\\!\\right]_{j=1}^{n}{\\overset{\\mathrm{mov}}{\\longrightarrow}}\\left[\\!\\!{\\begin{array}{c c}{T^{(\\ell)}x_{j}}&{T^{(\\ell)}x_{j}}\\\\ {x_{j}}&{\\mathbf{0}}\\\\ {0}&{y_{j}}\\end{array}}\\!\\!\\right]_{j=1}^{n}\\xrightarrow{\\mathrm{mul}}\\left[\\!\\!{\\begin{array}{c c}{T^{(\\ell)}x_{j}}&{T^{(\\ell)}x_{j}}\\\\ {x_{j}}&{\\mathbf{0}}\\\\ {0}&{y_{j}}\\\\ {\\mathbf{0}}&{T^{(\\ell)}y_{j}x_{j}}\\end{array}}\\!\\!\\right]_{j=1}^{n}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now we apply Lemma B.3 over all even columns in Equation (33) and we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{Output}=\\sum_{j=1}^{n}{\\left[\\begin{array}{c}{T^{(\\ell)}x_{j}}\\\\ {\\mathbf{\\Delta}\\mathbf{}\\mathbf{{\\zeta}}^{\\mathbf{p}}}\\\\ {T^{(\\ell)}y_{j}x_{j}}\\end{array}\\right]}=\\left[\\mathbf{\\underline{{f}}}^{(\\ell)}\\sum_{j=1}^{n}y_{j}x_{j}\\right]=\\left[\\mathbf{\\underline{{f}}}^{\\ell}\\mathbf{\\underline{{\\Psi}}}^{\\mathbf{\\Xi}}\\mathbf{\\Xi}^{\\mathbf{\\mathbf{f}}}\\mathbf{\\Lambda}\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\xi$ denotes irrelevant quantities. Note that the resulting $\\pmb{T}^{(\\ell)}\\pmb{X}^{\\top}\\pmb{y}$ is also the same as Iterative Newton\u2019s predictor $\\hat{\\pmb w}_{k}=M_{k}\\pmb X^{\\top}\\pmb y$ after $k$ iterations. We denote $\\hat{\\pmb w}_{\\ell}^{\\mathrm{TF}}={\\pmb T}^{(\\ell)}{\\pmb X}^{\\top}{\\pmb y}$ . ", "page_idx": 32}, {"type": "text", "text": "Transformers can make predictions on $\\pmb{x}_{t e s t}$ by $\\langle\\hat{\\pmb{w}}_{\\ell}^{\\mathrm{TF}},\\pmb{x}_{\\mathrm{test}}\\rangle$ . ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Now we can make predictions on text query $\\mathbf{\\boldsymbol{x}}_{\\mathrm{test}}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c}{\\xi}&{x_{\\mathrm{test}}}\\\\ {\\hat{w}_{\\ell}^{\\mathrm{TF}}}&{x_{\\mathrm{test}}}\\end{array}\\right]\\stackrel{\\mathrm{mov}}{\\longrightarrow}\\left[\\begin{array}{c c}{\\xi}&{x_{\\mathrm{test}}}\\\\ {\\hat{w}_{\\ell}^{\\mathrm{TF}}}&{x_{\\mathrm{test}}}\\\\ {0}&{\\hat{w}_{\\ell}^{\\mathrm{TF}}}\\end{array}\\right]\\stackrel{\\mathrm{mul}}{\\longrightarrow}\\left[\\begin{array}{c c}{\\xi}&{x_{\\mathrm{test}}}\\\\ {\\hat{w}_{\\ell}^{\\mathrm{TF}}}&{x_{\\mathrm{test}}}\\\\ {0}&{\\hat{w}_{\\ell}^{\\mathrm{TF}}}\\\\ {0}&{\\left<\\hat{w}_{\\ell}^{\\mathrm{TF}},x_{\\mathrm{test}}\\right>\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, we can have an readout layer $\\beta_{\\mathrm{ReadOut}}\\,=\\,\\{\\pmb{u},\\boldsymbol{v}\\}$ applied (see Definition 3.3) with $\\textbf{\\em u}=$ $\\mathbf{\\Delta}[\\mathbf{0}_{3d}\\quad1]^{\\top}$ and $v=0$ to extract the prediction $\\langle\\hat{\\pmb{w}}_{\\ell}^{\\mathrm{TF}},\\pmb{x}_{\\mathrm{test}}\\rangle$ at the last location, given by $\\pmb{x}_{\\mathrm{test}}$ . This is exactly how Iterative Newton makes predictions. ", "page_idx": 32}, {"type": "text", "text": "To Perform $k$ steps of Newton\u2019s iterations, Transformers need $O(k)$ layers. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Let\u2019s count the layers: ", "page_idx": 32}, {"type": "text", "text": "\u2022 Initialization: mov needs $\\mathcal{O}(1)$ layer; gathering $\\alpha{\\cal S}$ needs $\\mathcal{O}(1)$ layer; and aff needs $\\mathcal{O}(1)$ layer. In total, Transformers need $\\mathcal{O}(1)$ layers for initialization.   \n\u2022 Newton Iteration: each exact Newton\u2019s iteration requires $\\mathcal{O}(1)$ layer. Implementing $k$ iterations requires $O(k)$ layers.   \n\u2022 Implementing $\\hat{\\pmb w}_{\\ell}^{\\mathrm{TF}}$ : We need one operation of mov and mul each, requiring ${\\mathcal O}(1)$ layer each. Apply Lemma B.3 for summation also requires ${\\mathcal O}(1)$ layer.   \n\u2022 Making prediction on test query: We need one operation of mov and mul each, requiring $\\mathcal{O}(1)$ layer each. ", "page_idx": 32}, {"type": "text", "text": "Hence, in total, Transformers can implement $k$ -step Iterative Newton and make predictions accordingly using $O(k)$ layers. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Remark B.5. We note that Giannou et al. [2023] used 13 layers to compute one Newton Iteration, and in our construction, we need only one Transformer layer (with one attention layer and one MLP layer) to compute one Newton Iteration. At the same time, we didn\u2019t use Aky\u00fcrek et al. [2022] for constructing Newton Iterations. Aky\u00fcrek et al. [2022] is applied to initialize Newton and for reading out the prediction. ", "page_idx": 32}, {"type": "text", "text": "In our construction, only the initialization and read-out prediction components use causal attention and softmax because Aky\u00fcrek et al. [2022]\u2019s construction is applied. To be more specific, those are the first 3 layers in initializing Iterative Newton and the last 5 layers in reading out the predictions from the computed pseudo-inverse. All the layers corresponding to the Iterative Newton updates are using full attention and normalized ReLU activations. ", "page_idx": 32}, {"type": "text", "text": "Remark B.6. We note that our proof can be extended to causal attention for $n$ sufficiently larger than $d$ . Under causal attention (see Definition 3.1) with normalized ReLU activation, Equation (29) can be rewritten as follows, given $t>d$ , we first choose $G=\\left[\\!\\!{\\begin{array}{r r}{O_{d}}&{-{\\frac{1}{2}}I_{d}}\\\\ {O_{d}}&{O_{d}}\\end{array}}\\!\\!\\right]$ , where the coefficient on the upper right block is $-\\,{\\frac{1}{2}}$ instead of $-\\,{\\frac{n}{2}}$ originally. Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t}^{(\\ell+1)}=\\left[\\begin{array}{l}{T^{(\\ell)}x_{t}}\\\\ {x_{t}}\\end{array}\\right]+\\frac{1}{t}\\displaystyle\\sum_{j=1}^{t}\\left[T^{(\\ell)}x_{j}\\right]\\left[T^{(\\ell)}x_{j}\\right]^{\\top}\\left[\\begin{array}{l}{-\\frac{1}{2}x_{t}}\\\\ {0_{d}}\\end{array}\\right]}\\\\ &{\\phantom{x x}=\\left[T^{(\\ell)}x_{t}-\\frac{1}{2}\\,\\frac{1}{t}\\sum_{j=1}^{t}(T^{(\\ell)}x_{j})(T^{(\\ell)}x_{j})^{\\top}x_{t}\\right]}\\\\ &{\\phantom{x x}x_{t}}\\\\ &{\\phantom{x x}=\\left[T^{(\\ell)}x_{t}-\\frac{1}{2}T^{(\\ell)}\\left(\\frac{1}{t}\\sum_{j=1}^{t}x_{j}x_{j}^{\\top}\\right){T^{(\\ell)}}^{\\top}x_{t}\\right]}\\\\ &{\\phantom{x x}x_{t}}\\\\ &{\\phantom{x x}=\\left[\\left(T^{(\\ell)}-\\frac{1}{2}T^{(\\ell)}\\dot{\\Sigma}T^{(\\ell)}\\right)x_{t}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{\\pmb{\\Sigma}}=\\frac{1}{t}\\sum_{j=1}^{t}{\\pmb{x}}_{j}\\pmb{x}_{j}^{\\top}}\\end{array}$ is the estimate of the covariance matrix given seen in-context examples $\\{{\\pmb x}_{j},{\\boldsymbol y}_{j}\\}_{j=1}^{t}$ so far. Since $t>d$ , $\\hat{\\Sigma}$ is an unbiased estimate for $\\begin{array}{r}{\\Sigma\\approx\\frac{1}{n}S}\\end{array}$ if $n$ is sufficiently large. The rest of the proof follows similarly, up to the perturbation introduced by the error in the estimate of \u02c6\u03a3. We also note when $t<d$ , the estimate $\\begin{array}{r}{\\hat{\\pmb{\\Sigma}}=\\frac{1}{t}\\sum_{j=1}^{t}{\\pmb{x}}_{j}\\pmb{x}_{j}^{\\top}}\\end{array}$ is no longer a valid covariance matrix since it\u2019s singular. Then this gives different $T^{(\\ell+1)}$ for different time stamp $t<d$ and such error may propagate in our proof. Hence, a formal extension to causal models requires extensive analysis of the error bounds and it is beyond the scope of this work. Nonetheless, we provide a plausible direction of such an extension. ", "page_idx": 33}, {"type": "text", "text": "B.3 Iterative Newton as a Sum of Moments Method ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Recall that Iterative Newton\u2019s method finds $S^{\\dagger}$ as follows ", "page_idx": 33}, {"type": "equation", "text": "$$\nM_{0}=\\underbrace{\\frac{2}{\\|S S^{\\top}\\|_{2}}}_{\\alpha}\\ S^{\\top},\\qquad M_{k}=2M_{k-1}-M_{k-1}S M_{k-1},\\forall k\\geq1.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can expand the iterative equation to moments of $\\boldsymbol{S}$ as follows. ", "page_idx": 33}, {"type": "equation", "text": "$$\nM_{1}=2M_{0}-M_{0}S M_{0}=2\\alpha S^{\\top}-4\\alpha^{2}S^{\\top}S S^{\\top}=2\\alpha S-4\\alpha^{2}S^{3}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let\u2019s do this one more time for $M_{2}$ . ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{2}=2M_{1}-M_{1}S M_{1}=2(2\\alpha S-4\\alpha^{2}S^{3})-(2\\alpha S-4\\alpha^{2}S^{3})S(2\\alpha S-4\\alpha^{2}S^{3})}\\\\ &{\\qquad=4\\alpha S-8\\alpha^{2}S^{3}-4\\alpha^{2}S^{3}+16\\alpha^{3}S^{5}-16\\alpha^{4}S^{7}}\\\\ &{\\qquad=4\\alpha S-12\\alpha^{2}S^{3}+16\\alpha^{3}S^{5}-16\\alpha^{4}S^{7}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can see that $M_{k}$ are summations of moments of $\\boldsymbol{S}$ , with respect to some pre-defined coefficients from the Newton\u2019s algorithm. Hence Iterative Newton is a special of an algorithm which computes an approximation of the inverse using second-order moments of the matrix, ", "page_idx": 33}, {"type": "equation", "text": "$$\nM_{k}=\\sum_{s=1}^{2^{k+1}-1}\\beta_{s}S^{s}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with coefficients $\\beta_{s}\\in\\mathbb{R}$ . ", "page_idx": 33}, {"type": "text", "text": "We note that Transformer circuits can represent other sum of moments other than Newton\u2019s method. We can introduce different coefficients $\\beta_{i}$ than in the proof of Theorem 5.1 by scaling the value matrices or through the MLP layers. ", "page_idx": 33}, {"type": "text", "text": "B.4 Estimated weight vectors lie in the span of previous examples ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "What properties can we infer and verify for the weight vectors which arise from Newton\u2019s method? A straightforward one arises from interpreting any sum of moments method as a kernel method. ", "page_idx": 34}, {"type": "text", "text": "We can expand $S^{s}$ as follows ", "page_idx": 34}, {"type": "equation", "text": "$$\nS^{s}=\\left(\\sum_{i=1}^{t}x_{i}x_{i}^{\\top}\\right)^{s}=\\sum_{i=1}^{t}\\left(\\sum_{j_{1},\\cdots,j_{s-1}}\\langle x_{i},x_{j_{1}}\\rangle\\prod_{v=1}^{s-2}\\langle x_{j_{v}},x_{j_{v+1}}\\rangle\\right)x_{i}x_{j_{s-1}}^{\\top}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\vec{w}_{t}=M_{i}{\\boldsymbol X}^{\\top}{\\boldsymbol y}=\\sum_{\\ s=1}^{2^{n+1}-1}\\beta_{s}{\\boldsymbol S}^{\\ast}{\\boldsymbol X}^{\\top}{\\boldsymbol y}}}\\\\ {{\\displaystyle~~}}\\\\ {{\\displaystyle~~~=\\sum_{s=1}^{2^{n+1}-1}\\beta_{s}\\left\\{\\sum_{i=1}^{r}\\left(\\sum_{i=1,\\atop i\\neq j,i=1}^{2}\\left({\\boldsymbol\\alpha}_{i},{\\boldsymbol x}_{j i}\\right)\\prod_{m=1}^{n-2}\\left({\\boldsymbol\\alpha}_{j},{\\boldsymbol x}_{j+1}\\right)\\right){\\boldsymbol x}_{i}{\\boldsymbol x}_{j-1}^{\\top}\\right\\}\\left\\{\\sum_{i=1}^{r}y_{i}{\\boldsymbol x}_{i}\\right\\}}}\\\\ {{\\displaystyle~~~}}\\\\ {{\\displaystyle~~~=\\sum_{s=1}^{2^{n+1}}\\beta_{s}\\left(\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{b}y_{j,\\cdot}\\left({\\boldsymbol x}_{i},{\\boldsymbol x}_{j}\\right)\\prod_{m=1}^{n}\\left({\\boldsymbol x}_{j},{\\boldsymbol x}_{j+1}\\right)\\right){\\boldsymbol x}_{i}\\right)}}\\\\ {{\\displaystyle~~~=\\sum_{i=1}^{2^{n+1}}\\left(\\sum_{s=1}^{2^{n+1}-1}\\sum_{j=i,i\\neq j,i\\neq i}^{2}\\left({\\boldsymbol x}_{i},{\\boldsymbol x}_{j}\\right)_{i}\\prod_{m=1}^{i-1}\\left({\\boldsymbol x}_{j},{\\boldsymbol x}_{j+1}\\right)\\right){\\boldsymbol x}_{i}}}\\\\ {{\\displaystyle~~~}}\\\\ {{\\displaystyle~~~~=\\sum_{i=1}^{2^{n}}\\phi_{i}(i\\mid{\\boldsymbol X},{\\boldsymbol y},{\\boldsymbol\\beta})\\alpha_{i}(i)\\times{\\boldsymbol y}^{\\top}\\left(\\sum_{m=1}^{n}y_{m},{\\boldsymbol x}_{j+1}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\mathbf{\\deltaX}$ is the data matrix, $\\beta$ are coefficients of moments given by the sum of moments method and $\\phi_{t}(\\cdot)$ is some function which assigns some weight to the $i$ -th datapoint, based on all other datapoints. Therefore if the Transformer implements a sum of moments method (such as Newton\u2019s method), then its induced weight vector $\\tilde{\\pmb{w}_{t}}$ (Transformers $\\mid\\{{\\pmb x}_{i},y_{i}\\}_{i=1}^{t})$ after seeing in-context examples $\\{{x}_{i},{y}_{i}\\}_{i=1}^{t}$ should lie in the span of the examples $\\{{\\pmb x}_{i}\\}_{i=1}^{t}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\tilde{w}}_{t}({\\mathrm{Transformers~}}|\\left\\{\\mathbf{\\boldsymbol{x}}_{i},y_{i}\\right\\}_{i=1}^{t})\\stackrel{?}{=}\\mathrm{Span}\\{\\mathbf{\\boldsymbol{x}}_{1},\\cdots,\\mathbf{\\boldsymbol{x}}_{t}\\}=\\sum_{t=1}^{t}a_{i}x_{i}\\qquad\\mathrm{for~coefficients~}a_{i}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We test this hypothesis. Given a sequence of in-context examples $\\{\\pmb{x}_{i},y_{i}\\}_{i=1}^{t}$ , we fit coefficients $\\{a_{i}\\}_{i=1}^{t}$ in Equation (43) to minimize MSE loss: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\{\\hat{a}_{i}\\}_{i=1}^{t}=\\underset{a_{1},a_{2},\\cdots,a_{t}\\in\\mathbb{R}}{\\arg\\operatorname*{min}}\\left\\|\\tilde{w}_{t}(\\mathrm{Transformers~}|\\left\\{\\boldsymbol{x}_{i},\\boldsymbol{y}_{i}\\right\\}_{i=1}^{t})-\\sum_{t=1}^{t}a_{i}\\boldsymbol{x}_{i}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We then measure the quality of this fti across different number of in-context examples $t$ , and visualize the residual error in Figure 27. We find that even when $t<d$ , Transformers\u2019 induced weights still lie close to the span of the observed examples $\\pmb{x}_{i}$ \u2019s. This provides an additional validation of our proposed mechanism. ", "page_idx": 34}, {"type": "image", "img_path": "L8h6cozcbn/tmp/030a61db174c5ac3f1ad65c77e2bdcfd288fb0ebf194e620faf196f0311afffd.jpg", "img_caption": ["Figure 27: Verification of hypothesis that the Transformers induced weight vector $\\pmb{w}$ lies in the span of observed examples $\\{x_{i}\\}$ . "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "C Computes ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "All experiments involving fine-tuning GPT2 models to learn in-context linear regressions are trained on one NVIDIA A6000. Linear probing experiments also used one NVIDIA A6000. ", "page_idx": 35}, {"type": "text", "text": "D License ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We used PyTorch Paszke et al. [2019] as our code framework and we used PyTorch implementation of LSTMs. PyTorch is licensed under the Modified BSD license. ", "page_idx": 35}, {"type": "text", "text": "We used GPT-2 Model as our backbone, and it\u2019s released under MIT License. We used trained GPT-2 checkpoints for linear regression by Garg et al. [2022] and it\u2019s released under MIT License. ", "page_idx": 35}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this work, our analyses of Transformers are mostly based on only one simple task: linear regression. It might not be able to extrapolate to any arbitrary algorithmic tasks. It would be interesting for future work to extend such analysis to an extensive class of problems. ", "page_idx": 35}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. Through a mechanistic understanding of Transformers, the backbone of modern large language models (LLMs), this work can help advance building safe and trustworthy models. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The limitations is discussed in Section E. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide a theorem in Section 5 with its proof in Appendix B. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide the details of experimental settings in $\\S4$ . ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We will release codes and data generation processes. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 38}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The detail of the computing resourse is provided at $\\S C$ Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The authors have read the NeurIPS Code of Ethics and made sure the paper follows the NeurIPS Code of Ethics in every aspect. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The potential societal impact is discussed in $\\S\\mathrm{F}$ . ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper works on simple linear regression tasks. We believe there is no such risk. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: See $\\S D$ for details. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We did not introduce any new assets in this paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]