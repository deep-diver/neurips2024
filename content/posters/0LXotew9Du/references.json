{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces the LLaMA model, which is the foundation of many experiments and analysis in the current paper."}, {"fullname_first_author": "Sehoon Kim", "paper_title": "Squeezellm: Dense-and-sparse quantization", "publication_date": "2023-06-07", "reason": "This paper introduces a weight-quantization method used as a baseline in several experiments in the current paper."}, {"fullname_first_author": "Ying Sheng", "paper_title": "Flexgen: High-throughput generative inference of large language models with a single gpu", "publication_date": "2023-00-00", "reason": "This paper proposes a method for efficient inference with large language models that is compared to the current work in this paper."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "LLM.int8 (): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-08-07", "reason": "This paper discusses low-bit matrix multiplication for transformers that is relevant to the current work."}, {"fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-08-14", "reason": "This paper introduces a benchmark for evaluating long-context models, which is used in this paper to demonstrate the efficiency of the proposed method."}]}