[{"type": "text", "text": "Progressive Entropic Optimal Transport Solvers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parnian Kassraie Aram-Alexandre Pooladian Michal Klein ETH Zurich, Apple New York University Apple pkassraie@ethz.ch aram-alexandre.pooladian@nyu.edu michalk@apple.com ", "page_idx": 0}, {"type": "text", "text": "James Thornton Jonathan Niles-Weed Marco Cuturi Apple New York University Apple jamesthornton@apple.com jnw@cims.nyu.edu cuturi@apple.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimal transport (OT) has profoundly impacted machine learning by providing theoretical and computational tools to realign datasets. In this context, given two large point clouds of sizes $n$ and $m$ in $\\mathbb{R}^{d}$ , entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovitch problem and output a $n\\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map. While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\\varepsilon$ . Setting $\\varepsilon$ can be difficult, as it simultaneously impacts various performance metrics, such as compute speed, statistical performance, generalization, and bias. In this work, we propose a new class of EOT solvers (PROGOT), that can estimate both plans and transport maps. We take advantage of several opportunities to optimize the computation of EOT solutions by dividing mass displacement using a time discretization, borrowing inspiration from dynamic OT formulations [McCann, 1997], and conquering each of these steps using EOT with properly scheduled parameters. We provide experimental evidence demonstrating that PROGOT is a faster and more robust alternative to EOT solvers when computing couplings and maps at large scales, even outperforming neural network-based approaches. We also prove the statistical consistency of PROGOT when estimating OT maps. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many problems in generative machine learning and natural sciences\u2014notably biology [Schiebinger et al., 2019, Bunne et al., 2023], astronomy [M\u00e9tivier et al., 2016] or quantum chemistry [Buttazzo et al., 2012]\u2014require aligning datasets or learning to map data points from a source to a target distribution. These problems stand at the core of optimal transport theory [Santambrogio, 2015] and have spurred the proposal of various solvers [Peyr\u00e9 et al., 2019] to perform these tasks reliably. In these tasks, we are given $n$ and $m$ points respectively sampled from source and target probability distributions on $\\mathbb{R}^{d}$ , with the goal of either returning a coupling matrix of size $n\\times m$ (which solves the so-called Kantorovitch problem), or a vector-valued map estimator that extends to out-of-sample data (solving the Monge problem). ", "page_idx": 0}, {"type": "text", "text": "In modern applications, where $n,m\\gtrsim10^{4}$ , a popular approach to estimating either coupling or maps is to rely on a regularization of the original Kantorovitch linear OT formulation using neg-entropy. This technique, referred to as entropic ${\\cal O}T$ , can be traced back to Schr\u00f6dinger and was popularized for ML applications in [Cuturi, 2013] (see Section 2). Crucially, EOT can be solved efficiently with Sinkhorn\u2019s algorithm (Algorithm 1), with favorable computational [Altschuler et al., 2017, Lin et al., 2022] and statistical properties [Genevay, 2019, Mena and Niles-Weed, 2019] compared to linear programs. Most couplings computed nowadays on large point clouds within ML applications are obtained using EOT solvers that rely on variants of the Sinkhorn algorithm, whether explicitly, or as a lower-level subroutine [Scetbon et al., 2021, 2022]. The widespread adoption of EOT has spurred many modifications of Sinkhorn\u2019s original algorithm (e.g., through acceleration [Thibault et al., 2021] or initialization [Thornton and Cuturi, 2023]), and encouraged its incorporation within neural-network OT approaches [Pooladian et al., 2023, Tong et al., 2023, Uscidda and Cuturi, 2023]. ", "page_idx": 0}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/9c1a4fffa59e2f3f33699f07e96179662dc597ff5ebd2c68a7f2f39ca9850955.jpg", "img_caption": ["Figure 1: (left) EOT solvers collapse when the value of $\\varepsilon$ is not properly chosen. This typically results in biased map estimators and in blurry couplings (see Fig. 2 for the coupling matrix obtained between $\\mathbf{x}_{\\mathrm{train}}$ and $\\mathbf{y}_{\\mathrm{train}},$ ). (middle) Debiasing the output of EOT solvers can prevent a collapse to the mean seen in EOT estimators, but computes the same coupling. PROGOT (right) ameliorates these problems in various ways: by decomposing the resolution of the OT problem into multiple time steps, and using various forms of progressive scheduling, we recover both a coupling whose entropy can be tuned automatically and a map estimator that is fast and reliable. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Though incredibly popular, Sinkhorn\u2019s algorithm is not without its drawbacks. While a popular tool due its scalability and simplicity, its numerical behavior is deeply impacted by the amount of neg-entropy regularization, driven by the hyperparameter $\\varepsilon$ . Some practitioners suggest to have the parameter nearly vanish [Xie et al., 2020, Schmitzer, 2019], others consider the case where it diverges, highlighting links with the maximum mean discrepancy [Ramdas et al., 2017, Genevay et al., 2019]. ", "page_idx": 1}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/8aeb113ac1970b62e14ea500c5faaf19a122f59a858c30b4ea71e764f62d030a.jpg", "img_caption": ["Figure 2: Coupling matrices between train points in Fig. 1. Comparison of EOT with a fairly large $\\varepsilon$ , and PROGOT which automatically tunes the entropy of its coupling according to the target point cloud\u2019s dispersion. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Several years after its introduction to the machine learning community [Cuturi, 2013], choosing a suitable regularization term for EOT remains a thorny pain point. Common approaches are setting $\\varepsilon\\ >\\ 0$ to a default value (e.g., the max [Flamary et al., 2021] or mean [Cuturi et al., 2022b] normalization of the transport cost matrix), incorporating a form of cross-validation or an unsupervised criterion [Vacher and Vialard, 2022, Van Assel et al., 2023], or scheduling $\\varepsilon$ [Lehmann et al., 2022, Feydy, 2020]. When $\\varepsilon$ is too large, the algorithm converges quickly, but yields severely biased maps (Figure 1, left), or blurry, uninformative couplings (Figure 2). Even theoretically and numerically debiasing the Sinkhorn solver (Figure 1, middle) does not seem to fully resolve the issue [Feydy et al., 2019, Pooladian et al., 2022]. To conclude, while strategies exist to alleviate this bias, there currently exists no one-size-ftis-all solution to this problem. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contribution: an EOT solver with a dynamic lens. Recent years have witnessed an explosion in neural-network approaches based on the so-called Benamou and Brenier dynamic formulation of OT [Lipman et al., 2022, Liu, 2022, Tong et al., 2023, Pooladian et al., 2023]. A benefit of this perspective is the ability to split the OT problem into simpler sub-problems that are likely better conditioned than the initial transport problem. With this observation, we propose a novel family of progressive EOT solvers, called PROGOT, that are meant to be sturdier and easier to parameterize than existing solvers. Our key idea is to exploit the dynamic nature of the problem, and vary parameters dynamically, such as $\\varepsilon$ and convergence thresholds, along the transport. We show that PROGOT ", "page_idx": 1}, {"type": "text", "text": "\u2022 can be used to recover both Kantorovitch couplings and Monge map estimators, \u2022 gives rise to a novel, provably statistically consistent map estimator under standard assumptions. \u2022 strikes the right balance between computational and statistical tradeoffs, \u2022 can outperform other (including neural-based) approaches on real datasets, ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Optimal transport. For domain $\\Omega\\subseteq\\mathbb{R}^{d}$ , let ${\\mathcal{P}}_{2}(\\Omega)$ denote the space of probability measures over $\\Omega$ with a finite second moment, and let $\\mathcal{P}_{2,\\mathrm{ac}}(\\Omega)$ be those with densities. Let $\\mu,\\dot{\\nu}\\in\\mathscr{P}_{2}(\\Omega)$ , and let $\\Gamma(\\mu,\\nu)$ be the set of joint probability measures with left-marginal $\\mu$ and right-marginal $\\nu$ . We consider a translation invariant cost function $c(x,y):=h(x-y)$ , with $h$ a strictly convex function, and define the Wasserstein distance, parameterized by $h$ , between $\\mu$ and $\\nu$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nW(\\mu,\\nu):=\\operatorname*{inf}_{\\pi\\in\\Gamma(\\mu,\\nu)}\\int\\!\\!\\!\\int h(x-y)\\mathrm{d}\\pi(x,y)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This formulation is due to Kantorovitch [1942], and we call the minimizers to (1) OT couplings or OT plans, and denote it as $\\pi_{0}$ . A subclass of couplings are those induced by pushforward maps. We say that $T:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ pushes $\\mu$ forward to $\\nu$ if $T(X)\\sim\\nu$ for $X\\sim\\mu$ , and write $T_{\\#}\\mu=\\nu$ . Given a choice of cost, we can define the Monge [1781] formulation of OT ", "page_idx": 2}, {"type": "equation", "text": "$$\nT_{0}:=\\underset{T:T_{\\#}\\mu=\\nu}{\\arg\\operatorname*{min}}\\int h(x-T(x))\\mathrm{d}\\mu(x)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the minimizers are referred to as Monge maps, or OT maps from $\\mu$ to $\\nu$ . Unlike OT couplings, OT maps are not always guaranteed to exist. Though, if $\\mu$ has a density, we obtain significantly more structure on the OT map: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (Brenier\u2019s Theorem [1991]). Suppose $\\mu\\in\\mathscr{P}_{2,a c}(\\Omega)$ and $\\nu\\in\\mathscr{P}_{2}(\\Omega)$ . Then there exists $a$ unique solution to (2) that is of the form $T_{0}=\\mathrm{{Id}}-\\nabla h^{*}\\circ\\nabla f_{0}$ , where $h^{*}$ is the convex-conjugate of $h,$ i.e. $h^{\\ast}(y):=\\operatorname*{max}_{x}\\langle x,y\\rangle-h(x)$ , and ", "page_idx": 2}, {"type": "equation", "text": "$$\n(f_{0},g_{0})\\in\\underset{(f,g)\\in\\mathcal F}{\\arg\\operatorname*{max}}\\int f\\mathrm{d}\\mu+\\int g\\mathrm{d}\\nu\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{F}:=\\{(f,g)\\in L^{1}(\\mu)\\times L^{1}(\\nu):\\;f(x)+g(y)\\leq h(x-y),\\forall x,y\\in\\Omega.\\}$ . Moreover, the OT plan is given by $\\pi_{0}(\\mathrm{d}x,\\mathrm{d}y)=\\delta_{T_{0}(x)}(y)\\mu(\\mathrm{d}x)$ . ", "page_idx": 2}, {"type": "text", "text": "Importantly, (3) is the dual problem to (1) and the pair of functions $(f_{0},g_{0})$ are referred to as the optimal Kantorovich potentials. Lastly, we recall the notion of geodesics with respect to the Wasserstein distance. For a pair of measures $\\mu$ and $\\nu$ with OT map $T_{0}$ , the McCann interpolation between $\\mu$ and $\\nu$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\alpha}:=\\left((1-\\alpha)\\operatorname{Id}+\\alpha T_{0}\\right)\\!\\#\\mu\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha\\in[0,1]$ . Equivalently, $\\mu_{\\alpha}$ is the law of $X_{\\alpha}=(1-\\alpha)X+\\alpha T_{0}(X)$ , where $X\\sim\\mu$ . In the case where $h=\\|\\cdot\\|^{p}$ for $p>1$ , the McCann interpolation is in fact a geodesic in the Wasserstein space [Ambrosio et al., 2005]. While this equivalence may not hold for general costs, the McCann interpolation still provides a natural path of measures between $\\mu$ and $\\nu$ [Liu, 2022]. ", "page_idx": 2}, {"type": "text", "text": "Entropic OT. Entropic regularization has become the de-facto approach to estimate all three variables $\\pi_{0}$ , $f_{0}$ and $T_{0}$ using samples $\\left(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\right)$ and $\\left({\\bf y}_{1},\\ldots,{\\bf y}_{m}\\right)$ , both weighted by probability weight vectors $\\mathbf{a}\\in\\mathbb{R}_{+}^{n}$ , $\\mathbf{b}\\in\\mathbb{R}_{+}^{m}$ $\\begin{array}{r l r}{\\lefteqn{\\sum_{i=1}^{n}\\mathbf{a}_{i}\\bar{\\delta}_{\\mathbf{x}_{i}}}}\\end{array}$ atainodn $\\begin{array}{r}{\\hat{\\nu}_{m}\\,=\\,\\bar{\\sum_{i=1}^{m}}\\,\\mathbf{b}_{j}\\delta_{\\mathbf{y}_{j}}}\\end{array}$ . i s At $\\hat{\\mu}_{n}\\,=$ lowing $\\varepsilon$ -strongly concave program: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{f}^{\\star},\\mathbf{g}^{\\star}=\\underset{\\mathbf{f}\\in\\mathbb{R}^{n}}{\\arg\\operatorname*{max}}\\left\\langle\\mathbf{f},\\mathbf{a}\\right\\rangle+\\left\\langle\\mathbf{g},\\mathbf{b}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad-\\varepsilon\\big\\langle e^{\\mathbf{f}/\\varepsilon},\\mathbf{K}e^{\\mathbf{g}/\\varepsilon}\\big\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/a766e042964d4ccd21df7d6dc105d2051ff92a7cb2a531b1e9f9afb9df84bcb7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "where $\\varepsilon>0$ and $\\mathbf{K}_{i,j}=[\\exp(-(\\mathbf{x}_{i}-\\mathbf{y}_{j})/\\varepsilon)]_{i,j}\\in\\mathbb{R}_{+}^{n\\times m}$ . We can verify that (5) is a regularized version of (3) when applied to empirical measures [Peyr\u00e9 et al., 2019, Proposition 4.4]. Sinkhorn\u2019s algorithm presents an iterative scheme for obtaining $(\\mathbf{f}^{\\star},\\mathbf{g}^{\\star})$ , and we recall it in Algorithm 1, where for a matrix $\\mathbf{S}=[\\mathbf{S}_{i,j}]$ we use the notation $\\begin{array}{r}{\\operatorname*{min}_{\\boldsymbol{\\varepsilon}}(\\mathbf{S}):=[-\\varepsilon\\log\\left(\\mathbf{1}^{\\top}e^{-\\mathbf{S}_{i,\\cdot}/\\varepsilon}\\right)]_{i}}\\end{array}$ , and $\\bigoplus$ is the tensor sum of two vectors, i.e. f $\\oplus\\mathbf{g}:=[\\mathbf{f}_{i}+\\mathbf{g}_{j}]_{i j}$ . Note that solving (5) also outputs a valid coupling $\\mathbf{P}_{i,j}^{\\star}=\\mathbf{K}_{i,j}\\exp(-(\\mathbf{f}_{i}^{\\star}+\\mathbf{g}_{j}^{\\star})\\bar{/}\\varepsilon)$ , which approximately solves the finite-sample counterpart of (1). Additionally, the optimal potential $f_{0}$ can be approximated by the entropic potential ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{f}_{\\varepsilon}(x):=\\operatorname*{min}_{\\varepsilon}([\\mathbf{g}_{j}^{\\star}-h(x-\\mathbf{y}_{j})]_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where an analogous expression can be written for $\\hat{g}_{\\varepsilon}$ in terms of $\\mathbf{f}^{\\star}$ . Using the entropic potential, we can also approximate the optimal transport map $T_{0}$ by the entropic map ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{T}_{\\varepsilon}(x)=x-\\nabla h^{*}\\circ\\nabla\\hat{f}_{\\varepsilon}(x)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This connection is shown in Pooladian and Niles-Weed [2021, Proposition 2] for $h=\\textstyle{\\frac{1}{2}}\\|\\cdot\\|^{2}$ and [Cuturi et al., 2023] for more general functions. ", "page_idx": 3}, {"type": "text", "text": "3 Progressive Estimation of Optimal Transport ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the problem of estimating the OT solutions $\\pi_{0}$ and $T_{0}$ , given empirical measures $\\hat{\\mu}$ and $\\hat{\\nu}$ from $n$ i.i.d. samples. Our goal is to design an algorithm which is numerically stable, computationally light, and yields a consistent estimator. The entropic map (7) is an attractive option to estimate OT maps compared to other consistent estimators [e.g., H\u00fctter and Rigollet, 2021, Manole et al., 2021]. In contrast to these methods, the entropic map is tractable since it is the output of Sinkhorn\u2019s algorithm. While Pooladian and Niles-Weed [2021] show that the entropic map is a bona fide estimator of the optimal transport map, it hides the caveat that the estimator is always biased. For any pre-set $\\varepsilon>0$ , the estimator is never a valid pushforward map i.e., $(\\hat{T}_{\\varepsilon})_{\\#}\\mu\\neq\\nu$ , and this holds true as the number of samples tends to infinity. In practice, the presence of this bias implies that the performance of $\\hat{T}_{\\varepsilon}$ is sensitive to the choice of $\\varepsilon$ , e.g. as in Figure 1. Instead of having Sinkhorn as the end-all solver, we propose to use it as a subroutine. Our approach is to iteratively move the source closer to the target, thereby creating a sequence of matching problems that are increasingly easier to solve. As a consequence, the algorithm is less sensitive to the choice of $\\varepsilon$ for the earlier EOT problems, since it has time to correct itself at later steps. To move the source closer to the target, we construct a McCann-type interpolator which uses the entropic map $\\hat{T}_{\\varepsilon}$ of the previous iterate, as outlined in the next section. ", "page_idx": 3}, {"type": "text", "text": "3.1 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As a warm-up, consider $T_{0}$ the optimal transport map from $\\mu$ to $\\nu$ . We let $T^{(0)}:=T_{0}$ and define $S^{(0)}:=\\left(1-\\alpha_{0}\\right)\\mathrm{Id}+\\alpha_{0}T^{(0)}$ . This gives rise to the measure $\\mu^{(1)}=S_{\\#}^{(0)}\\mu$ S(#0 )\u00b5, which traces out the McCann interpolation between $(\\mu,\\nu)$ as $\\alpha$ varies in the interval $(0,1)$ . Then, letting $T^{(1)}$ be the optimal transport map for the pair $(\\mu^{(1)},\\nu)$ , it is straightforward to show that $T^{(1)}\\circ S^{(0)}=T^{(0)}$ . In other words, in the idealized setting, composing the output of a progressive sequence of Monge problems along the McCann interpolation path recovers the solution to the original Monge problem. ", "page_idx": 3}, {"type": "text", "text": "Building on this observation, we set up a progressive sequence of entropic optimal transport problems, along an estimated interpolation path, between the empirical counterparts of $(\\mu,\\nu)$ . We show that, as long as we remain close to the true interpolation path (by not allowing $\\alpha$ to be too large), the final output is close to $\\nu$ . Moreover, as the algorithm progresses, choosing the parameters $\\varepsilon_{i}$ becomes a less arduous task, and computation of $\\hat{T}_{\\varepsilon}$ becomes a more stable numerical problem. ", "page_idx": 3}, {"type": "text", "text": "At step zero, we set $\\hat{\\mu}_{\\varepsilon}^{\\left(0\\right)}=\\hat{\\mu}$ and calculate the entropic map $\\mathcal{E}^{(0)}:=\\hat{T}_{\\varepsilon_{0}}$ from samples $(\\hat{\\mu}_{\\varepsilon}^{(0)},\\hat{\\nu})$ with a regularization parameter $\\varepsilon_{0}>0$ . To set up the next EOT problem, we create an intermediate distribution via the McCann-type interpolation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{\\varepsilon}^{(1)}:=\\mathcal{S}_{\\#}^{(0)}\\hat{\\mu}_{\\varepsilon}^{(0)},\\;\\mathcal{S}^{(0)}:=(1-\\alpha_{0})\\,\\mathrm{Id}+\\alpha_{0}\\mathcal{E}^{(0)}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\alpha_{0}\\in(0,1)$ . In doing so, we are mimicking a step along the interpolation path for the pair $(\\mu,\\nu)$ . In fact, we can show that $\\hat{\\mu}_{\\varepsilon}^{\\left(1\\right)\\cdot}$ is close to $\\mu_{\\alpha_{0}}$ as defined in (4) (see Lemma 12). For the next iteration of the algorithm, we choose $\\varepsilon_{1}$ and $\\alpha_{1}$ , compute $\\mathcal{E}^{(1)}$ the entropic map for the pair $\\big(\\hat{\\mu}_{\\varepsilon}^{\\left(1\\right)},\\hat{\\nu}\\big)$ with regularization $\\varepsilon_{1}$ , and move along the estimated interpolation path by computing the distribution $\\hat{\\mu}_{\\varepsilon}^{\\left(2\\right)}$ . We repeat the same process for $K$ steps. The algorithm then outputs the progressive entropic map ", "page_idx": 3}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/6941946a9612cb7343d020f6c4f53f192c1d4eda07d1a9e4b8b384b9aa86212a.jpg", "img_caption": ["Figure 3: Intuition of PROGOT: By iteratively fitting to the interpolation path, the final transport step is less likely to collapse, resulting in more stable solver. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{Prog}}^{(K)}:=\\mathcal{E}^{(K)}\\circ\\mathcal{S}^{(K-1)}\\circ\\cdot\\cdot\\cdot\\circ\\mathcal{S}^{(0)}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\S^{(k)}=(1-\\alpha_{k})\\operatorname{Id}+\\alpha_{k}\\mathcal{E}^{(k)}$ is the McCann-type interpolator at step $k$ . Figure 3 visualizes the one-step algorithm, and Definition 2 formalizes the construction of our progressive estimators. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (PROGOT). For two empirical measures $\\hat{\\mu},\\hat{\\nu}$ , and given step and regularization schedules $(\\alpha_{k})_{k=0}^{K}$ and $(\\varepsilon_{k})_{k=0}^{K}$ , the PROGOT map estimator $\\upgamma_{\\mathrm{Prog}}^{(K)}$ is defined as the composition ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{J}_{\\mathrm{Prog}}^{(K)}:=\\mathcal{E}^{(K)}\\circ\\mathcal{S}^{(K-1)}\\circ\\cdot\\cdot\\circ\\mathcal{S}^{(0)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where these maps are defined recursively, starting from $\\hat{\\mu}_{\\varepsilon}^{\\left(0\\right)}:=\\hat{\\mu}$ , and then at each iteration: ", "page_idx": 4}, {"type": "text", "text": "\u2022 $\\mathcal{E}^{(k)}$ is the entropic map $\\hat{T}_{\\varepsilon_{k}}$ , computed between samples $\\big(\\hat{\\mu}_{\\varepsilon}^{\\left(k\\right)},\\hat{\\nu}\\big)$ with regularization $\\varepsilon_{k}$ .   \n\u2022 $\\mathcal{S}^{(k)}:=\\left({1-\\alpha_{k}}\\right)\\operatorname{Id}+\\alpha_{k}\\mathcal{E}^{(k)}$ , is a McCann-type interpolating map at time $\\alpha_{k}$ . $\\hat{\\mu}_{\\varepsilon}^{(k+1)}:=\\mathcal{S}_{\\#}^{(k)}\\hat{\\mu}_{\\varepsilon}^{(k)}$ the updated measure used in the next iteration. ", "page_idx": 4}, {"type": "text", "text": "Additionally, the PROGOT coupling matrix $\\mathbf{P}$ between $\\hat{\\mu}$ and $\\hat{\\nu}$ is identified with the matrix solving the discrete EOT problem between \u00b5\u02c6(\u03b5K) and $\\hat{\\nu}$ . ", "page_idx": 4}, {"type": "text", "text": "The sequence of $(\\alpha_{k})_{k=0}^{K}$ characterizes the speed of movement along the path. By choosing $\\alpha_{k}=$ $\\alpha(k)$ we can recover a constant-speed curve, or an accelerating curve which initially takes large steps and as it gets closer to the target, the steps become finer, or a decelerating curve which does the opposite. This is discussed in more detail in Section 4 and visualized in Figure (4-C). Though our theoretical guarantee requires a particular choice for the sequence $(\\varepsilon_{k})_{k=0}^{K}$ and $(\\alpha_{k})_{k=0}^{K}$ , our experimental results reveal that the performance of our estimators is not sensitive to this choice. We hypothesize that this behavior is due to the fact that PROGOT is \u201cself-correcting\u201d\u2014by steering close to the interpolation path, later steps in the trajectory can correct the biases introduced in earlier steps. ", "page_idx": 4}, {"type": "text", "text": "3.2 Theoretical Guarantees ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "By running PROGOT, we are solving a sequence of EOT problems, each building on the outcome of the previous one. Since error can potentially accumulate across iterations, it leads us to ask if the algorithm diverges from the interpolation path and whether the ultimate progressive map estimator $T_{\\mathrm{Prog}}^{\\mathcal{(}K)}$ is consistent, focusing on the squared-Euclidean cost of transport, i.e., $h\\,=\\,\\textstyle{\\frac{1}{2}}\\|\\cdot\\|^{2}$ . . To answer this question, we assume ", "page_idx": 4}, {"type": "text", "text": "(A1) $\\mu,\\nu\\,\\in\\,\\mathscr{P}_{2,\\mathrm{ac}}(\\Omega)$ with $\\Omega\\subseteq\\mathbb{R}^{d}$ convex and compact, with $0\\,<\\,\\nu_{\\mathrm{min}}\\,\\leq\\,\\nu(\\cdot)\\,\\leq\\,\\nu_{\\mathrm{max}}$ and $\\mu(\\cdot)\\leq\\mu_{\\mathrm{max}},$ ", "page_idx": 4}, {"type": "text", "text": "(A2) the inverse mapping $x\\mapsto(T_{0}(x))^{-1}$ has at least three continuous derivatives, (A3) there exists $\\lambda,\\Lambda>0$ such that $\\lambda I\\preceq D T_{0}(x)\\preceq\\Lambda I$ , for all $x\\in\\Omega$ ( $D$ denotes Jacobian) ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "and prove that PROGOT yields a consistent map estimator. Our error bound depends on the number of iterations $K$ , via a constant multiplicative factor. Implying that $T_{\\mathrm{{Prog}}}^{(K)}$ is consistent as long as $K$ does not grow too quickly as a function of $n$ the number of samples. In experiments, we set $K\\ll n$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 (Consistency of Progressive Entropic Maps). Let $h=\\textstyle{\\frac{1}{2}}\\|\\cdot\\|^{2}$ . Suppose $\\mu,\\nu$ and their optimal transport map $T_{0}$ satisfy (A1)-(A3), and further suppose we have n i.i.d. samples from both $\\mu$ and $\\nu$ . Let $\\upgamma_{\\mathrm{Prog}}^{(k)}$ be as defined in Definition 2, with parameters $\\varepsilon_{k}\\asymp n^{-\\frac{1}{2d}},\\alpha_{k}\\asymp n^{\\frac{-1}{d}}$ for all $k\\in[K]$ . Then, the progressive entropic map is consistent and converges to the optimal transport map as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big\\|\\mathcal{T}_{\\mathrm{Prog}}^{(K)}-T_{0}\\Big\\|_{L^{2}(\\mu)}^{2}\\lesssim_{\\log(n),K}n^{-\\frac{1}{d}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the notation implies that the inequality ignores terms of rate $\\operatorname{Poly}(\\log(n),K)$ . ", "page_idx": 4}, {"type": "text", "text": "The rate of convergence for PROGOT is slower than the convergence of entropic maps shown by Pooladian and Niles-Weed [2021] under the same assumptions, with the exception of convexity of $\\Omega$ . However, the rates that Theorem 3 suggests for the parameters $\\alpha_{k}$ and $\\varepsilon_{k}$ are set merely to demonstrate convergence and do not reflect how each parameter should be chosen as a function of $k$ when executing the algorithm. We will present practical schedules for $(\\alpha_{k})_{k=1}^{K}$ and $(\\varepsilon_{k})_{k=1}^{K}$ in Section 4. The proof is deferred to Appendix $\\textrm{C}$ ; here we present a brief sketch. ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. In Lemma 11, we show that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big\\|\\mathcal{T}_{\\mathrm{Prog}}^{(K)}-T_{0}\\Big\\|_{L^{2}(\\mu)}^{2}\\lesssim\\sum_{k=0}^{K}\\Delta_{k}:=\\sum_{k=0}^{K}\\mathbb{E}\\|\\mathcal{E}^{(k)}-T^{(k)}\\|_{L^{2}(\\mu^{(k)})}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu^{(k)}$ is a point on the true interpolation path, and $T^{(k)}$ is the optimal transport map emanating from it. Here, $\\bar{\\mathcal{E}}^{(k)}$ is the entropic map estimator between the final target points and the data that has been pushed forward by earlier entropic maps. It suffices to control the term $\\Delta_{k}$ . Since ${\\mathcal E}^{(k)}$ and $T^{(k)}$ are calculated for different measures, we prove a novel stability property (Proposition 4) to show that along the interpolation path, these intermediate maps remain close to their unregularized population counterparts, if $\\alpha_{k}$ and $\\varepsilon_{k}$ are chosen as prescribed. This result is based off the recent work by Divol et al. [2024] and allows us to recursively relate the estimation at the $k$ -th iterate to the estimation at the previous ones, down to $\\Delta_{0}$ . Thus, Lemma 12 tells us that, under our assumptions and parameter choices $\\alpha_{k}\\asymp n^{-1/d}$ and $\\varepsilon_{k}\\asymp n^{-1/2d}$ , it holds that for all $k\\geq0$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta_{k}\\lesssim_{\\log(n)}n^{-1/d}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the stability bound allows us to relate $\\Delta_{k}$ to $\\Delta_{0}$ , combined with the above, we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{k}\\lesssim_{\\log(n)}\\Delta_{0}\\lesssim_{\\log(n)}n^{-1/d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the penultimate inequality uses the existing estimation rates from Pooladian and Niles-Weed [2021], with our parameter choice for $\\varepsilon_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 4 (Stability of entropic maps with variations in the source measure). Let $h=\\textstyle{\\frac{1}{2}}\\|\\cdot\\|^{2}$ . Let $\\mu,\\mu^{\\prime},\\rho$ be probability measures over a compact domain with radius $R$ . Suppose $T_{\\varepsilon},T_{\\varepsilon}^{\\prime}$ are, respectively, the entropic maps from $\\mu$ to $\\rho$ and $\\mu^{\\prime}$ to $\\rho_{:}$ , both with the parameter $\\varepsilon>0$ . Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|T_{\\varepsilon}-T_{\\varepsilon}^{\\prime}\\right\\|_{L^{2}(\\mu)}^{2}\\leq3R^{2}\\varepsilon^{-1}W_{2}^{2}(\\mu,\\mu^{\\prime})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Computing Couplings and Map Estimators with PROGOT ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following the presentation and motivation of PROGOT in Section 3, here we outline a practical implementation. Recall that $\\begin{array}{r}{\\hat{\\mu}_{n}\\,=\\,\\sum_{i=1}^{n}{\\bf a}_{i}\\delta_{{\\bf x}_{i}}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\nu}_{m}\\,=\\,\\sum_{j=1}^{m}\\mathbf{b}_{j}\\delta_{\\mathbf{y}_{j}}}\\end{array}$ , and we summarize the locations of these measures to the matrices $\\mathbf{X}=(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})$ and $\\mathbf{Y}=(\\mathbf{y}_{1},\\hdots,\\mathbf{y}_{m})$ , which are of size $n\\times d$ and $m\\times d$ , respectively. Our PROGOT solver, concretely summarized in Algorithm 2, takes as input two weighted point clouds, step-lengths $(\\alpha_{k})_{k}$ , regularization parameters $\\bar{(\\varepsilon_{k})}_{k}$ , and threshold parameters $(\\tau_{k})_{k}$ , to output two objects of interest: the final coupling matrix $\\mathbf{P}$ of size $n\\times m$ , as illustrated in Figure 2, and the entities needed to instantiate the $\\mathcal{T}_{\\mathrm{Prog}}$ map estimator, where an implementation is detailed in Algorithm 3. We highlight that Algorithm 2 incorporates a warm-starting method when instantiating Sinkhorn solvers (Line 3). This step may be added to improve the runtime. ", "page_idx": 5}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/924310baab458f718d3d803512ebedc3286279429d8f8bf3de4ddc6119cbc3e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Setting step lengths. We propose three scheduling schemes for $(\\alpha_{k})_{k}$ : decelerated, constant-speed and accelerated. Let $t_{k}\\in[0,1]$ denote the progress along the interpolation path at iterate $k$ . At step zero, $t_{0}=\\alpha_{0}$ . Then at the next step, we progress by a fraction $\\alpha_{1}$ of the remainder, and therefore $t_{1}=t_{0}+\\alpha_{1}(1-\\alpha_{0})$ . It is straightforward to show that $\\begin{array}{r}{t_{k}=1-\\prod_{\\ell=1}^{k}(1-\\alpha_{\\ell})}\\end{array}$ . We call a schedule constant speed, if $t_{k+1}-t_{k}$ is a constant function of $k$ , whereas an accelerated (resp. decelerated) ", "page_idx": 5}, {"type": "text", "text": "schedule has $t_{k+1}-t_{k}$ increasing (resp. decreasing) with $k$ . Table 6 presents the specific choices of $\\alpha_{k}$ for each of these schedules. By convention, we set the last step to be a complete step, i.e., $\\alpha_{K}=1$ . ", "page_idx": 6}, {"type": "text", "text": "Setting regularization schedule. To set the regularization parameters $(\\varepsilon_{k})_{k=0}^{K}$ , we propose Algorithm 4. To set $\\varepsilon_{0}$ , we use the average of the values in the cost matrix $[h(\\mathbf{\\ddot{x}}_{i}-\\mathbf{y}_{j})]_{i j}$ between source and target, multiplied by a small factor, as implemented in [Cuturi et al., 2022b]. Then for $\\varepsilon_{K}$ , we make the following key observation. As the last iteration, the algorithm is computing $\\mathcal{E}^{(k)}$ , an entropic map roughly between the target measure and itself. For this problem, we know trivially that the OT map should be the identity. Therefore, given a set of values to choose from, we pick $\\varepsilon_{K}$ to be that which minimizes this error over a hold-out evaluation set of $\\mathbf{Y}_{\\mathrm{test}}=(\\tilde{\\mathbf{y}}_{j})_{j=1}^{m}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{error}(\\varepsilon;Y_{\\mathrm{test}}):=\\sum_{j=1}^{m}\\Big\\|\\tilde{\\mathbf{y}}_{j}-\\mathbf{\\xi}^{(K)}(\\tilde{\\mathbf{y}}_{j})\\Big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The intermediate values are then set by interpolating between $\\beta_{0}\\varepsilon_{0}$ and $\\varepsilon_{K}$ , according to the times $t_{k}$ . Figure 4-C visualizes the effect of applying Algorithm 4 for scheduling, as opposed to choosing default values for $\\varepsilon_{k}$ . ", "page_idx": 6}, {"type": "text", "text": "Setting threshold schedule. By setting the Sinkhorn stopping threshold $\\tau_{k}$ as a function of time $k$ , one can modulate the amount of compute effort spent by the Sinkhorn subroutine at each step. This can be achieved by decreasing $\\tau_{k}$ linearly w.r.t. the iteration number, from a loose initial value, e.g., 0.1, to a final target value $\\tau_{K}\\ll1$ . Doing so results naturally in sub-optimal couplings $\\mathbf{P}$ and dual variables $g^{(k)}$ at each step $k$ , which might hurt performance. However, two comments are in order: $(i)$ Because the last threshold $\\tau_{K}$ can be set independently, the final coupling matrix $\\mathbf{P}$ returned by PROGOT can be arbitrarily feasible, in the sense that its marginals can be made arbitrarily close to a, $\\mathbf{b}$ by setting $\\tau_{K}$ to a small value. This makes it possible to compare in a fair way to a direct application of the Sinkhorn algorithm. $(i i)$ Because the coupling is normalized by its own marginal in line (5) of Algorithm 2, we ensure that the barycentric projection computed at each step remains valid, i.e., the matrix $\\mathbf{Q}$ is a transition kernel, with line vectors in the probability simplex. ", "page_idx": 6}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/1cca3248f2d3428409b9fc50ceaa850fbff1f492bca064d60f9a717faa730073.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We run experiments to evaluate the performance of PROGOT across various datasets, on its ability to act as a map estimator, and to produce couplings between the source and target points. The code for PROGOT, is included in the OTT-JAX package [Cuturi et al., 2022b]. ", "page_idx": 6}, {"type": "text", "text": "5.1 PROGOT as a Map Estimator ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In map experiments, unless mentioned otherwise, we run PROGOT for $K\\,=\\,16$ steps, with a constant-speed schedule for $\\alpha_{k}$ , and the regularization schedule set via Algorithm 4 with $\\beta_{0}=5$ and $s_{p}\\in\\{2^{-\\hat{3}},\\ldots,2^{3}\\}$ . In these experiments, we fti the estimators on training data using the $\\ell_{2}^{2}$ transport cost, and report their performance on test data in Figure 4 and Table 1. To this end, we quantify the distance between two test point clouds $(\\mathbf{X},\\mathbf{Y})$ with the Sinkhorn divergence [Genevay et al., 2018, Feydy et al., 2019], always using the $\\ell_{2}^{2}$ transport cost. Writing $\\mathrm{OT}_{\\varepsilon}(\\mathbf{X},\\mathbf{Y})$ for the objective value of Equation (5), the Sinkhorn divergence reads ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\varepsilon_{D}}(\\mathbf{X},\\mathbf{Y}):=\\mathrm{OT}_{\\varepsilon_{D}}(\\mathbf{X},\\mathbf{Y})-\\frac{1}{2}\\big(\\mathrm{OT}_{\\varepsilon_{D}}(\\mathbf{X},\\mathbf{X})+\\mathrm{OT}_{\\varepsilon_{D}}(\\mathbf{Y},\\mathbf{Y})\\big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\varepsilon_{D}$ is $5\\%$ of the mean (intra) cost seen within the target distribution (see Appendix B). ", "page_idx": 6}, {"type": "text", "text": "Exploratory Experiments on Synthetic Data. We consider a synthetic dataset where $\\mathbf{X}$ is a $d\\!.$ - dimensional point cloud sampled from a 3-component Gaussian mixture. The ground-truth $T_{0}$ is the gradient of an input convex neural network (ICNN) previously ftited to push roughly $\\mathbf{X}$ to a mixture of 10 Gaussians [Korotin et al., 2021]. From this map, we generate the target point cloud $\\mathbf{Y}$ . Unless stated otherwise, we use $n_{\\mathrm{train}}=7000$ samples to train a progressive map between the source and target point clouds and visualize some of its properties in Figure 4 using $n_{\\mathrm{test}}=500$ test points. ", "page_idx": 6}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/15f0cec2d8f670dc485f46dcf957a40882f96c5f91494a417a25850154a20fff.jpg", "img_caption": ["Figure 4: (A) Convergence of $\\mathrm{{\\mathcal{T}}_{P r o g}}$ to the ground-truth map w.r.t. the empirical L2 norm, for $d=4$ . (B) Effect of scheduling $\\alpha_{k}$ , for $d=64$ . (C) Effect of scheduling $\\varepsilon_{k}$ using Algorithm 4, for $d=64$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4-(A) demonstrates the convergence of $\\mathrm{\\mathcal{T}}_{\\mathrm{Prog}}$ to the true map as the number of training points grows, in empirical L2 norm, that is, $\\begin{array}{r}{\\mathrm{MSE}=\\frac{\\ddot{\\mathrm{I}}}{n_{\\mathrm{test}}}\\sum_{i=1}^{n_{\\mathrm{test}}}\\lVert T_{0}(\\dot{\\mathbf{x}}_{i})-\\mathcal{T}_{\\mathrm{Prog}}(\\mathbf{x}_{i})\\rVert_{2}^{2}}\\end{array}$ . Figure 4-(B) shows the progression of PROGOT from source to target as measured by $D_{\\varepsilon_{D}}(\\mathbf{\\bar{X}}^{(k)},\\mathbf{Y})$ where $\\mathbf{X}^{(k)}$ are the intermediate point clouds corresponding to $\\hat{\\mu}_{\\varepsilon}^{\\left(k\\right)}$ . The curves reflect the speed of movement for three different schedules, e.g., the decelerated algorithm takes larger steps in the first iterations, resulting in $D_{\\varepsilon_{D}}(\\mathbf{X}^{(k)},\\mathbf{Y})$ to initially drop rapidly. Across multiple evaluations, we observe that the $\\alpha_{k}$ schedule has little impact on performance and settle on the constant-speed schedule. Lastly, Figure 4-(C) plots $D_{\\varepsilon_{D}}(\\mathbf{X}^{(\\bar{k})},\\mathbf{Y})$ for the last 6 steps of the progressive algorithm under two scenarios. PROGOT uses regularization parameters set according to Algorithm 4, and PROGOT without scheduling, sets every $\\varepsilon_{k}$ as $5\\%$ of the mean of the cost matrix between the point clouds of $(\\mathbf{X}^{(k)},\\mathbf{Y})$ . This experiment shows that Algorithm 4 can result in displacements $\\mathbf{X}^{(k)}$ that are \u201ccloser\u201d to the target $\\mathbf{Y}$ , potentially improving the overall performance. ", "page_idx": 7}, {"type": "text", "text": "Comparing Map Estimators on Single-Cell Data. We consider the sci-Plex single-cell RNA sequencing data from [Srivatsan et al., 2020] which contains the responses of cancer cell lines to 188 drug perturbations, as reflected in their gene expression. Visualized in Figure 8, we focus on 5 drugs (Belinostat, Dacinostat, Givinostat, Hesperadin, and Quisinostat) which have a significant impact on the cell population as reported by Srivatsan et al. [2020]. We remove genes which appear in less than 20 cells, and discard cells which have an incomplete gene expression of less than 20 genes, obtaining $n\\approx10^{4}$ source and $m\\approx500$ target cells, depending on the drug. We whiten the data, take it to $\\log(1+x)$ scale and apply PCA to reduce the dimensionality to $d=\\{16,64,256\\}$ This procedure repeats the pre-processing steps of Cuturi et al. [2023]. ", "page_idx": 7}, {"type": "text", "text": "We consider four baselines: (1) training an input convex neural network (ICNN) [Amos et al., 2017] using the objective of Amos [2022] (2) training a feed-forward neural network regularized with the Monge Gap [Uscidda and Cuturi, 2023], (3) instantiating the entropic map estimator [Pooladian and Niles-Weed, 2021] and (4) its debiased variant [Feydy et al., 2019, Pooladian et al., 2022]. The first two algorithms use neural networks, and we follow hyper-parameter tuning in [Uscidda and Cuturi, 2023]. We choose the number of hidden layers for both as [128, 64, 64]. For the ICNN we use a learning rate $\\eta=10^{-3}$ , batch size $b=256$ and train it using the Adam optimizer [Kingma and Ba, 2014] for 2000 iterations. For the Monge Gap we set the regularization constant $\\lambda_{\\mathrm{MG}}=10$ , $\\lambda_{\\mathrm{cons}}\\,=\\,0.1$ and the Sinkhorn regularization to $\\varepsilon\\,=\\,0.01$ . We train the Monge Gap in a similar setting, except that we set $\\eta=0.01$ . To choose $\\varepsilon$ for entropic estimators, we split the training data to get an evaluation set and perform 5-fold cross-validation on the grid of $\\{2^{-3},\\stackrel{\\bullet}{\\dots},2^{3}\\}\\times\\varepsilon_{0}$ , where $\\varepsilon_{0}$ is computed as in line 2 of Algorithm 4. ", "page_idx": 7}, {"type": "text", "text": "We compare the algorithms by their ability to align the population of control cells, to cells treated with a drug. We randomly split the data into $80\\%-20\\%$ train and test sets, and report the mean and standard error of performance over the test set, for an average of 5 runs. Detailed in Table 1, PROGOT outperforms the baselines consistently with respect to $D_{\\varepsilon_{D}}((\\mathcal{\\sf T}_{\\mathrm{Prog}})_{\\#}\\mathbf{X},\\mathbf{Y})$ . The table shows complete results for 3 drugs, and the overall ranking based on performance across all 5 drugs. Table 5 presents the synthetic counterpart to Table 1, using Gaussian Mixture data for $d=128$ , 256. ", "page_idx": 7}, {"type": "text", "text": "5.2 PROGOT as a Coupling Solver ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we benchmark the ability of PROGOT to return a coupling, and compare it to that of the Sinkhorn algorithm. Comparing coupling solvers is rife with challenges, as their time performance must be compared comprehensively by taking into account three crucial metrics: $(i)$ the cost of transport according to the coupling $\\mathbf{P}$ , that is, $\\langle\\mathbf{P},\\mathbf{C}\\rangle$ , $(i i)$ the entropy $E(\\mathbf{P})$ , and (iii) satisfaction of marginal constraints $\\|\\mathbf{P1}_{m}-\\mathbf{\\dot{a}}\\|_{1}+\\|\\mathbf{P}^{T}\\mathbf{1}_{n}-\\mathbf{\\dot{b}}\\|_{1}$ . Due to our threshold schedule $(\\tau_{k})_{k}$ , as detailed ", "page_idx": 7}, {"type": "text", "text": "Table 1: Performance of PROGOT compared to baselines, w.r.t $\\mathrm{D}_{\\varepsilon_{D}}$ between source and target of the sci-Plex dataset. Reported numbers are the average of 5 runs, together with the standard error. ", "page_idx": 8}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/f79a8631850d0c922e889e5fd21a90e8d4b8fbcfa31e1d65ea9fb7c799c86a6f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/a0240c73cb92976e8c5fe3175f524f8bf3e52065ddd1e9769ac8eb23b9b3fe96.jpg", "img_caption": ["Figure 5: Performance as a coupling solver on the 4i dataset. PROGOT returns better couplings, in terms of the OT cost and the entropy, for a fraction of Sinkhorn iterations, while still returning a coupling that has the same deviation to the original marginals. The (top) row is computed using $h=\\|.\\|_{2}^{2}$ , the (bottom) row shows results for the cost $\\begin{array}{r}{h=\\frac{\\daleth}{p}\\|\\cdot\\|_{p}^{p}}\\end{array}$ where $p=1.5$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "in Section 4, both approaches are guaranteed to output couplings that satisfy the same threshold for criterion $(i i i)$ , leaving us only three quantities to monitor: compute effort here quantified as total number of Sinkhorn iterations, summed over all $K$ steps for PROGOT), transport cost and entropy. While compute effort and transport cost should, ideally, be as small as possible, certain applications requiring, e.g., differentiability [Cuturi et al., 2019] or better sample complexity [Genevay et al., 2019], may prefer higher entropies. ", "page_idx": 8}, {"type": "text", "text": "To monitor these three quantities, and cover an interesting space of solutions that, we run Sinkhorn\u2019s algorithm for a logarithmic grid of $\\varepsilon=\\beta\\varepsilon_{0}$ values (here $\\varepsilon_{0}$ is defined in Line 2 of Algorithm 4), and compare it to constant-speed PROGOT with $K=\\{2,4,8\\}$ . Because one cannot directly compare regularizations, we explore many choices to schedule $\\varepsilon$ within PROGOT. Following the default strategy used in OTT-JAX [Cuturi et al., 2022a], we set at every iterate $k$ , $\\varepsilon_{k}=\\theta\\bar{c}_{k}$ , where $\\bar{c}_{k}$ is $5\\%$ of the the mean of the cost matrix at that iteration, as detailed in Appendix B. We do not use Algorithm 4 since it returns a regularization schedule that is tuned for map estimation, while the goal here is to recover couplings that are comparable to those outputted by Sinkhorn. We set the threshold for marginal constraint satisfaction for both algorithms as $\\tau_{K}=\\tau=0.001$ and run all algorithms to convergence, with infinite iteration budget. For the coupling experiments, we use the single-cell multiplex data of Bunne et al. [2023], reflecting morphological features and protein intensities of melanoma tumor cells. The data describes $d=47$ features for $n\\approx11$ , 000 control cells, and $m\\approx2$ , 800 treated cells, for each of 34 drugs, of which we use only 6 at random. To align the cell populations, we consider two ground costs: the squared-Euclidean norm $\\|\\cdot\\|^{2}$ as well as $\\begin{array}{r}{\\bar{h}=\\frac{1}{p}\\|\\cdot\\|_{p}^{\\bar{p}}}\\end{array}$ , with $p=1.5$ . ", "page_idx": 8}, {"type": "text", "text": "Results for 6 drugs are displayed in Figure 5. The area of the marker reflects the total number of Sinkhorn iterations needed for either algorithm to converge to a coupling with a threshold $\\tau=10^{-3}$ . The values for $\\beta$ and $K$ displayed in the legend are encoded using colors. The global scaling parameter for PROGOT is set to $\\theta=\\overline{{2}}^{-4}$ . Figure 10 and 11 visualize other choices for $\\theta$ . These results prove that PROGOT provides a competitive alternative to Sinkhorn, to compute couplings that yield a small entropy and cost at a low computational effort, while satisfying the same level of marginal constraints. ", "page_idx": 8}, {"type": "text", "text": "Figure 6: We consider the optimal assignment problem Table 2: Coupling recovery, quantified between all CIFAR images and their blurry CIFAR as trace, and KL divergence from counterparts using the $\\ell_{2}^{2}$ loss. A small subset of 3 original identity matrix, for coupling matrices images on the left can be compared with their blurred obtained with PROGOT and Sinkhorn, counterpart on the right, with $\\sigma=4$ . The optimal coupling and blur strengths $\\sigma=2,4$ . PROGOT for this task is the identity, which we compare with is run for $K=4$ and with the constantcouplings recovered by our methods at large scales. speed schedule. ", "page_idx": 9}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/757d0f56da60783512272ae05efd1b96c336a4e72408ca6be306f177d7c80ce0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/aaf2be135a421b6b6518ec042e09c217bbb262a3683badb9ae76cff4da208bb7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Scalability of PROGOT ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Real-world experiments on pre-processed single-cell data are often run with limited sample sizes $(n\\simeq10^{3})$ ) and medium data dimensionality $(d\\leq200)$ . As a result they are not suitable to benchmark OT solvers at larger scale. To address this limitation, we design a challenging large-scale (large $n$ , large $d$ ) OT problem on real data, for which the ground-truth is known. We believe our approach can be replicated to create benchmarks for OT solvers. We consider the entire grayscale CIFAR10 dataset [Krizhevsky et al., 2009] for which $n=60,000$ and $d=32\\times32=1024$ . We consider the task of matching these $n$ images to their blurred counterparts, using Gaussian blurs of varying width. To blur an image $U\\in\\mathbb{R}^{\\breve{N}\\times N}$ we use the isotropic Gaussian kernel $K=[\\exp{\\left(-(i-\\stackrel{\\cdot}{j})^{2}\\right/(\\sigma N^{2})\\right)}]_{i j}$ for $i,j\\ \\leq\\ N$ [c.f. Remark 4.17, Peyr\u00e9 et al., 2019], and define the Gaussian blur operator as $G(U)^{\\backslash}:=K U\\bar{K}\\in\\mathbb{R}^{N\\times N}$ . The crucial observation we make in Proposition 5 is that, when using the squared Euclidean ground cost $\\ell_{2}^{2}$ , the optimal matching is necessarily equal to the identity (i.e. each image must be necessarily matched to its blurred counterpart), as pictured in (Figure 6). ", "page_idx": 9}, {"type": "text", "text": "Proposition 5. Let $\\begin{array}{r}{\\hat{\\mu}=\\frac{1}{n}\\sum_{s\\leq n}\\delta_{U_{s}}}\\end{array}$ be the empirical distribution over n images and define $\\hat{\\nu}:=$ $G_{\\#}\\hat{\\mu}$ where $G$ is the Gaussian blur operator with $\\sigma<\\infty$ . Then $P^{\\star}$ the optimal coupling between $(\\hat{\\mu},\\hat{\\nu})$ with the $\\begin{array}{r}{h=\\frac{1}{2}\\left\\Vert\\cdot\\right\\Vert_{2}^{2}}\\end{array}$ cost is the normalized $n$ -dimensional identity matrix $\\operatorname{Id}/n$ . ", "page_idx": 9}, {"type": "text", "text": "In light of Proposition 5, we generate two blurred datasets using a Gaussian kernel with $\\sigma=2$ and $\\sigma=4$ (see Figure 7). We then use PROGOT with and Sinkhorn\u2019s Algorithm to match the blurred dataset back to the original CIFAR10 (de-blurring). The hyper-parameter configurations are the same as Section 5.2, with $\\bar{\\beta}=\\theta=2^{-4}$ . We evaluate the performance of the OT solvers by checking how close the trace of the recovered coupling $\\operatorname{Tr}(\\hat{P})$ is to 1.0, or with the KL divergence from the ground-truth, that is, $\\begin{array}{r}{\\mathrm{KL}(P^{\\star}||\\hat{P})=-\\log n-n\\sum_{i\\leq n}\\log(\\hat{P}_{i i}).}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "Table 2 compares the performance of PROGOT and Sinkhorn, along with the number of iterations needed to achieve this performance. Both algorithms scale well and show high accuracy, while requiring a similar amount of computation. We highlight that at this scale, simply storing the cost or coupling matrices would require about 30Gb. The experiment has to happen across multiple GPUs. Thanks to its integration in JAX and OTT-JAX [Cuturi et al., 2022b], PROGOT supports sharding by simply changing a few lines of code. The algorithms scales seamlessly and each run takes about 15 minutes, on a single node of 8 A100 GPUs. This experiment sets a convincing example on how PROGOT scales to much larger (in $n$ and $d$ ) problems than considered previously. ", "page_idx": 9}, {"type": "text", "text": "Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we proposed PROGOT, a new family of EOT solvers that blend dynamic and static formulations of OT by using the Sinkhorn algorithm as a subroutine within a progressive scheme. PROGOT aims to provide practitioners with an alternative to the Sinkhorn algorithm that $(i)$ does not fail when instantiated with uninformed or ill-informed $\\varepsilon$ regularization, thanks to its self-correcting behavior and our simple $\\varepsilon$ -scheduling scheme that is informed by the dispersion of the target distribution, (ii) performs at least as fast as Sinkhorn when used to compute couplings between point clouds, and (iii) provides a reliable out-of-the-box OT map estimator that comes with a non-asymptotic convergence guarantee. We believe PROGOT can be used as a strong baseline to estimate Monge maps. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "J. Altschuler, J. Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, 2017.   \nL. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.   \nB. Amos. On amortizing convex conjugates for optimal transport. In The Eleventh International Conference on Learning Representations, 2022.   \nB. Amos, L. Xu, and J. Z. Kolter. Input convex neural networks. In Proceedings of the 34th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2017.   \nJ.-D. Benamou and Y. Brenier. A computational fluid mechanics solution to the Monge\u2013Kantorovich mass transfer problem. Numerische Mathematik, 2000.   \nY. Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Comm. Pure Appl. Math., 1991.   \nC. Bunne, S. G. Stark, G. Gut, J. S. del Castillo, M. Levesque, K.-V. Lehmann, L. Pelkmans, A. Krause, and G. R\u00e4tsch. Learning single-cell perturbation responses using neural optimal transport. Nature Methods, 2023.   \nG. Buttazzo, L. De Pascale, and P. Gori-Giorgi. Optimal-transport formulation of electronic densityfunctional theory. Phys. Rev. A, 2012.   \nG. Carlier, L. Chizat, and M. Laborde. Displacement smoothness of entropic optimal transport. ESAIM: COCV, 2024.   \nS. Chewi and A.-A. Pooladian. An entropic generalization of Caffarelli\u2019s contraction theorem via covariance inequalities. Comptes Rendus. Math\u00e9matique, 2023.   \nI. Csisz\u00e1r. $I$ -divergence geometry of probability distributions and minimization problems. Ann. Probability, 3:146\u2013158, 1975.   \nM. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural Information Processing Systems, 26, 2013.   \nM. Cuturi, O. Teboul, and J.-P. Vert. Differentiable ranking and sorting using optimal transport. Advances in neural information processing systems, 32, 2019.   \nM. Cuturi, L. Meng-Papaxanthos, Y. Tian, C. Bunne, G. Davis, and O. Teboul. Optimal transport tools (ott): A JAX toolbox for all things Wasserstein. arXiv preprint, 2022a.   \nM. Cuturi, L. Meng-Papaxanthos, Y. Tian, C. Bunne, G. Davis, and O. Teboul. Optimal transport tools (OTT): A JAX toolbox for all things Wasserstein. CoRR, 2022b.   \nM. Cuturi, M. Klein, and P. Ablin. Monge, Bregman and occam: Interpretable optimal transport in high-dimensions with feature-sparse maps. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2023.   \nV. Divol, J. Niles-Weed, and A.-A. Pooladian. Tight stability bounds for entropic Brenier maps. arXiv preprint, 2024.   \nS. Eckstein and M. Nutz. Quantitative stability of regularized optimal transport and convergence of Sinkhorn\u2019s algorithm. SIAM Journal on Mathematical Analysis, 2022.   \nJ. Feydy. Geometric data analysis, beyond convolutions. Applied Mathematics, 2020.   \nJ. Feydy, T. S\u00e9journ\u00e9, F.-X. Vialard, S.-i. Amari, A. Trouv\u00e9, and G. Peyr\u00e9. Interpolating between optimal transport and MMD using Sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.   \nR. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos, K. Fatras, N. Fournier, L. Gautheron, N. T. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko, A. Rolet, A. Schutz, V. Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 2021.   \nN. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability theory and related fields, 162(3):707\u2013738, 2015.   \nA. Genevay. Entropy-regularized optimal transport for machine learning. PhD thesis, Paris Sciences et Lettres (ComUE), 2019.   \nA. Genevay, G. Peyr\u00e9, and M. Cuturi. Learning generative models with Sinkhorn divergences. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, 2018.   \nA. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyr\u00e9. Sample complexity of sinkhorn divergences. In The 22nd international conference on artificial intelligence and statistics, pages 1574\u20131583. PMLR, 2019.   \nP. Ghosal, M. Nutz, and E. Bernton. Stability of entropic optimal transport and Schr\u00f6dinger bridges. Journal of Functional Analysis, 283(9):109622, 2022.   \nG. Gut, M. D. Herrmann, and L. Pelkmans. Multiplexed protein maps link subcellular organization to cellular states. Science, 2018.   \nJ.-C. H\u00fctter and P. Rigollet. Minimax estimation of smooth optimal transport maps. The Annals of Statistics, 2021.   \nL. Kantorovitch. On the translocation of masses. C. R. (Doklady) Acad. Sci. URSS (N.S.), 1942.   \nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nA. Korotin, L. Li, A. Genevay, J. M. Solomon, A. Filippov, and E. Burnaev. Do neural optimal transport solvers work? A continuous Wasserstein-2 benchmark. Advances in neural information processing systems, 2021.   \nA. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nT. Lehmann, M.-K. von Renesse, A. Sambale, and A. Uschmajew. A note on overrelaxation in the Sinkhorn algorithm. Optimization Letters, 2022.   \nT. Lin, N. Ho, and M. I. Jordan. On the efficiency of entropic regularized algorithms for optimal transport. Journal of Machine Learning Research, 2022.   \nY. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. arXiv preprint, 2022.   \nQ. Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint, 2022.   \nT. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasserman. Plugin estimation of smooth optimal transport maps. arXiv preprint, 2021.   \nR. J. McCann. A convexity principle for interacting gases. Advances in mathematics, 128(1):153\u2013179, 1997.   \nG. Mena and J. Niles-Weed. Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem. Advances in neural information processing systems, 2019.   \nL. M\u00e9tivier, R. Brossier, Q. M\u00e9rigot, E. Oudet, and J. Virieux. Measuring the misfti between seismograms using an optimal transport distance: Application to full waveform inversion. Geophysical Supplements to the Monthly Notices of the Royal Astronomical Society, 2016.   \nG. Monge. M\u00e9moire sur la th\u00e9orie des d\u00e9blais et des remblais. Histoire de l\u2019Acad\u00e9mie Royale des Sciences, 1781.   \nG. Peyr\u00e9, M. Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 2019.   \nA.-A. Pooladian and J. Niles-Weed. Entropic estimation of optimal transport maps. arXiv preprint, 2021.   \nA.-A. Pooladian, M. Cuturi, and J. Niles-Weed. Debiaser beware: Pitfalls of centering regularized transport maps. In International Conference on Machine Learning. PMLR, 2022.   \nA.-A. Pooladian, H. Ben-Hamu, C. Domingo-Enrich, B. Amos, Y. Lipman, and R. T. Q. Chen. Multisample flow matching: Straightening flows with minibatch couplings. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2023.   \nA. Ramdas, N. Garc\u00eda Trillos, and M. Cuturi. On Wasserstein two-sample testing and related families of nonparametric tests. Entropy, 2017.   \nF. Santambrogio. Optimal transport for applied mathematicians. Springer, 2015.   \nM. Scetbon, M. Cuturi, and G. Peyr\u00e9. Low-rank Sinkhorn factorization. In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research, 2021.   \nM. Scetbon, G. Peyr\u00e9, and M. Cuturi. Linear-time Gromov Wasserstein distances using low rank couplings and costs. In Proceedings of the 39th International Conference on Machine Learning. PMLR, 2022.   \nG. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin, P. Berube, et al. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. Cell, 2019.   \nB. Schmitzer. Stabilized sparse scaling algorithms for entropy regularized transport problems. SIAM Journal on Scientific Computing, 2019.   \nE. Schr\u00f6dinger. \u00dcber die umkehrung der naturgesetze. Verlag der Akademie der Wissenschaften in Kommission bei Walter De Gruyter u ..., 1931.   \nR. Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The Annals of mathematical statistics, 1964.   \nS. R. Srivatsan, J. L. McFaline-Figueroa, V. Ramani, L. Saunders, J. Cao, J. Packer, H. A. Pliner, D. L. Jackson, R. M. Daza, L. Christiansen, et al. Massively multiplex chemical transcriptomics at single-cell resolution. Science, 2020.   \nA. Thibault, L. Chizat, C. Dossal, and N. Papadakis. Overrelaxed Sinkhorn\u2013Knopp algorithm for regularized optimal transport. Algorithms, 2021.   \nJ. Thornton and M. Cuturi. Rethinking initialization of the sinkhorn algorithm. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. PMLR, 2023.   \nA. Tong, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y. Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint, 2023.   \nT. Uscidda and M. Cuturi. The Monge gap: A regularizer to learn all transport maps. In International Conference on Machine Learning. PMLR, 2023.   \nA. Vacher and F.-X. Vialard. Parameter tuning and model selection in optimal transport with semi-dual Brenier formulation. Advances in Neural Information Processing Systems, 2022.   \nH. Van Assel, T. Vayer, R. Flamary, and N. Courty. Optimal transport with adaptive regularisation. In NeurIPS 2023 Workshop Optimal Transport and Machine Learning, 2023.   \nC. Villani et al. Optimal transport: old and new. Springer, 2009.   \nY. Xie, X. Wang, R. Wang, and H. Zha. A fast proximal point method for computing exact Wasserstein distance. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, Proceedings of Machine Learning Research. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Section 5.1, we demonstrated the performance of PROGOT for map estimation on the sci-Plex dataset. Here, we present a similar experiment on the 4i data, and extend all map experiments to the case of a general translation invariant cost function, $h=1.5\\|\\cdot\\|_{1.5}$ . In Table 3 and Table 4 we show $D_{\\varepsilon_{D}}((\\mathcal{\\bar{T}}_{\\mathrm{Prog}})_{\\#}\\mathbf{X},\\mathbf{Y};h)$ the sinkhorn divergence using the cost function $h$ . ", "page_idx": 13}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/de670d170cafb7a3730063c2d4a6480edd322d4085c9a1e548b394132d935666.jpg", "table_caption": ["Table 3: Performance of algorithms on sci-Plex data, w.r.t $D_{\\varepsilon_{D}}((\\mathcal{\\mathrm{T}}_{\\mathrm{Prog}})_{\\#}\\mathbf{X},\\mathbf{Y};h)$ with the 1.5-norm cost. Reported numbers are the average of 5 runs, together with the standard error. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 4: Performance of algorithms on 4i data, w.r.t $D_{\\varepsilon_{D}}((\\mathcal{\\ T}_{\\mathrm{Prog}})_{\\#}\\mathbf{X},\\mathbf{Y};h)$ where $h$ is reported in the table. Reported numbers are the average of 10 runs, together with the standard error. ", "page_idx": 13}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/52aa63e35987db418876a930b44b623783a10aee3606884727a96eb7665e07d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "GMM Benchmark. To provide further evidence on performance of PROGOT, we benchmark the map estimation methods on high-dimensional Gaussian Mixture data, using the dataset of Korotin et al. [2021] for $d=128$ , 256. In this experiment the ground truth maps are known and allow us to compare the algorithms more rigorously using the empirical $\\ell_{2}$ distance of the maps, as defined in section Section 5.1. Shown in Table 5, we consider $n_{\\mathrm{test}}=500$ test points and $n_{\\mathrm{train}}=8000$ and 9000 training points, respectively for each dimension. We have also included a variant of the entropic estimator which uses the default value of the OTT-JAX library for $\\varepsilon$ , and unlike other EOT algorithms, is not cross-validated. This is to demonstrate the significant effect that $\\varepsilon$ has on Sinkhorn solvers. ", "page_idx": 13}, {"type": "text", "text": "Table 5: GMM benchmark. The Table shows the MSE, average of $\\lVert\\hat{\\mathbf{y}}-\\mathbf{y}_{\\mathrm{test}}\\rVert_{2}^{2}$ for $n_{\\mathrm{test}}=500$ points, where $\\hat{\\mathbf{y}}=\\hat{T}(\\mathbf{x}_{\\mathrm{test}})$ and the ground truth is $\\mathbf{y}_{\\mathrm{test}}$ . ", "page_idx": 13}, {"type": "table", "img_path": "7WvwzuYkUq/tmp/4183b01e8b6d0161eaeae9ed07c56ed47aba1b48a39d6df7a49c9164f114ec55.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Details of Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Generation of Figure 1 and Figure 2. We consider a toy example where the target and source clouds are as shown in Figure 1. We visualize the entropic map [Pooladian and Niles-Weed, 2021], its debiased variant [Pooladian et al., 2022] and PROGOT, where we consider a decelerated schedule with 6 steps, and only visualize steps $k=3,5$ to avoid clutter. The hyperparameters of the algorithms are set as described in Section 5. Figure 2 shows the coupling matrix corresponding to the same data, resulting from the same solvers. ", "page_idx": 13}, {"type": "text", "text": "Sinkhorn Divergence and its Regularization Parameter. In some of the experiments, we calculate the Sinkhorn divergence between two point clouds as a measure of distance. In all experiments we set the value of $\\varepsilon_{D}$ and according to the geometry of the target point cloud. In particular, we set $\\varepsilon$ to be default value of the OTT-JAX Cuturi et al. [2022b] library for this point cloud via ott.geometry.pointcloud.PointCloud(Y).epsilon, that is, $5\\%$ of the average cost matrix, within the target points. ", "page_idx": 13}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/1c6dde2ca7fee0813b3704b2f1e65c3a297708ccce333689ea1e1d8e4c48d66d.jpg", "img_caption": ["Figure 7: Example of a CIFAR10 image and blurred variant. We match blurry images to the originals. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/e34a0ac8171bd970c1275e3afd1bb72a28594bb716086d833d146919c63a92b5.jpg", "img_caption": ["Figure 8: Overview of the single cell dataset Srivatsan et al. [2020]. We show the first two PCA dimensions performed on the training data, and limit the figure to 50 samples. The point cloud $(\\mathbf{x}_{\\mathrm{train}},\\mathbf{x}_{\\mathrm{test}})$ shows the control cells and $({\\bf y}_{\\mathrm{train}},{\\bf y}_{\\mathrm{test}})$ are the perturbed cells using a specific drug. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Details of Scheduling $(\\alpha_{k})_{k}$ . Table 6 specifies our choices of $\\alpha_{k}$ for the three schedules detailed in Section 4. Figure 9 compares the performance of PROGOT with constant-speed schedule in red, with the decelerated (Dec.) schedules in green. The figure shows results on the sci-Plex data (averaged across 5 drugs) and the Gaussian Mixture synthetic data. We observe that the algorithms perform roughly on par, implying that in practice PROGOT is robust to choice of $\\alpha$ . ", "page_idx": 14}, {"type": "table", "img_path": "", "table_caption": ["Table 6: Scheduling Schemes for $\\alpha_{k}$ . "], "table_footnote": [], "page_idx": 14}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\frac{\\operatorname{schedule}}{\\operatorname{Decelerated}}}{\\Bigg|}{\\frac{\\operatorname{\\rho}_{\\alpha\\times\\times\\times\\times\\operatorname{sums}}\\operatorname{sums}\\cup\\operatorname{sums}\\subseteq\\operatorname{sums}\\subseteq\\operatorname{\\rho}_{\\alpha\\times\\dots}}{\\left(k+1\\right)^{2}-\\left(k+1\\right)}}}\\\\ &{{\\frac{\\operatorname{Constant-Speed}}{\\operatorname{Constant-Speed}}}{\\Bigg|}{\\frac{\\frac{1}{K-k+2}}{\\frac{2k-1}{\\left(K+1\\right)^{2}-\\left(k-1\\right)^{2}}}}{\\Bigg|}{\\frac{\\left({\\frac{k}{K}}\\right)^{2}}{\\left({\\frac{k}{K}}\\right)^{2}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Details of Scheduling $(\\varepsilon_{k})_{k}$ . For map experiments on the sci-Plex data [Srivatsan et al., 2020], we schedule the regularization parameters via Algorithm 4. We set $\\beta_{0}\\,=\\,5$ and consider the set $s_{p}\\,=\\,\\{0.1,0.5,1,5,10,20\\}$ . For coupling experiments on the 4i data [Gut et al., 2018] we set the regularizations as follows. Let $\\mathbf{x}_{1}^{\\prime(k)},\\ldots,\\mathbf{x}_{n}^{(k)}$ denote the interpolated point cloud at iterate $k$ (according to Line 7, Algorithm 2) and recall that ${\\bf y}_{1},\\ldots,{\\bf y}_{m}$ is the target point cloud. The scaled average cost at this iterate is $\\begin{array}{r}{\\bar{c}_{k}=\\sum_{i,j}h(\\mathbf{x}_{i}^{(k)}-\\mathbf{\\bar{y}}_{j})/(20m n)}\\end{array}$ , which is the default value of $\\varepsilon$ typically used for running Sinkhorn. T hen for every $k\\in[K]$ , we set $\\varepsilon_{k}=\\theta\\bar{c}_{k}$ to make PROGOT compatible to the $\\beta$ regularization levels of the bench-marked Sinkhorn algorithms. For Figure 5, we have set $\\theta=2^{-4}$ . In Figure 10 and Figure 11, we visualize the results for $\\theta\\in\\{2^{-7},2^{-\\overline{{4}}},2^{-1}\\}$ to give an overview of the results using small and larger scaling values. ", "page_idx": 14}, {"type": "text", "text": "Compute Resources. Experiments were run on a single Nvidia A100 GPU for a total of 24 hours.   \nSmaller experiments and debugging was performed on a single MacBook M2 Max. ", "page_idx": 14}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/ec2b59f38114bb855d36363df861d305104a7c6112b30c6147c8cb32a8659cdc.jpg", "img_caption": ["Figure 9: Comparison of constant speed vs decelerated PROGOT. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/49caf69dc64fd111313ae2a24381265af7d5987b58676fd2fbc2ac04117a3fcc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 10: Comparison of PROGOT and Sinkhorn as coupling solvers for $h(\\cdot)=\\|\\cdot\\|_{2}^{2}$ on the 4i dataset. Rows show different choices of regularization $\\theta$ for PROGOT as detailed in Appendix B. ", "page_idx": 15}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Preliminaries. Before proceeding with the proofs, we collect some basic definitions and facts. First, we write the the $p$ -Wasserstein distance for any $p\\geq1$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{p}(\\mu,\\nu):=\\left(\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\iint\\|x-y\\|^{p}\\mathrm{d}\\pi(x,y)\\right)^{1/p}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, it is well-known that $p$ -Wasserstein distances are ordered for $p\\geq1$ : for $1\\leq p\\leq q$ , it holds that $W_{p}(\\mu,\\nu)\\leq W_{q}(\\mu,\\nu)$ [cf. Remark 6.6, Villani et al., 2009]. ", "page_idx": 15}, {"type": "text", "text": "For the special case of the 1-Wasserstein distance, we have the following dual formulation ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(\\mu,\\nu)=\\operatorname*{sup}_{f\\in\\mathrm{Lip}_{1}}\\int f\\mathrm{d}(\\mu-\\nu)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathrm{Lip_{1}}$ is the space of 1-Lipschitz functions [cf. Theorem 5.10, Villani et al., 2009]. ", "page_idx": 15}, {"type": "text", "text": "Returning to the 2-Wasserstein distance, we will repeatedly use the following two properties of optimal transport maps. First, for any two measures $\\mu,\\nu$ and an $L$ -Lipschitz map $T$ , it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{2}(T_{\\#}\\mu,T_{\\#}\\nu)\\leq L W_{2}(\\mu,\\nu)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This follows from a coupling argument. In a similar vein, we will use the following upper bound on the Wasserstein distance between the pushforward of a source measure $\\mu$ by two different optimal transport maps $T_{a}$ and $T_{b}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{2}^{2}((T_{a})_{\\#}\\mu,(T_{b})_{\\#}\\mu)\\leq\\|T_{a}-T_{b}\\|_{L^{2}(\\mu)}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/cbaf3b8c4d61b02ed2f409d34babb8fec88d4d4a8f4a1aa8fcbfb509c6acbe7a.jpg", "img_caption": ["Figure 11: Comparison of PROGOT and Sinkhorn as coupling solvers for $h(\\cdot)=\\|\\cdot\\|_{1.5}^{1.5}/1.5$ on the 4i dataset. Rows show different choices of regularization $\\theta$ for PROGOT as detailed in Appendix B. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/f949b46a125a56e5d422228aa7a7a8e9fbf5ac1b8ff4d023860d0807a18779a8.jpg", "img_caption": ["Figure 12: Comparison of PROGOT and Sinkhorn as coupling solvers for $h(\\cdot)=\\|\\cdot\\|_{2}^{2}$ on the sci-Plex dataset. Rows show different choices of regularization $\\theta$ for PROGOT as detailed in Appendix B. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/455053046c7eb3b71c2e6a6cb4562fd02a221680d70cece1b6423ef140424c84.jpg", "img_caption": ["Figure 13: Comparison of PROGOT and Sinkhorn as coupling solvers for $h(\\cdot)=\\|\\cdot\\|_{1.5}^{1.5}/1.5$ on the sci-Plex dataset. Rows show different choices of regularization $\\theta$ for PROGOT as detailed in Appendix B. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "7WvwzuYkUq/tmp/e9ee055906d0735067d88f3540fbf05ca2147323f926d7fb548b1ff36024dc17.jpg", "img_caption": ["Figure 14: Comparison of PROGOT and Sinkhorn as coupling solvers for $h(\\cdot)=\\|\\cdot\\|_{1.5}^{1.5}/1.5$ on the sci-Plex dataset. Rows show different choices of regularization $\\theta$ for PROGOT as detailed in Appendix B. The threshold for marginals is also scheduled here, starting from $\\tau=0.01$ and reaching $\\tau=0.001$ to match Sinkhorn. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Notation conventions. For an integer $K\\in\\mathbb N$ , $[K]:=\\{0,\\ldots,K\\}$ . We write $a\\lesssim b$ to mean that there exists a constant $C>0$ such that $a\\leq C b$ . A constant can depend on any of the quantities present in (A1) to (A3), as well as the support of the measures, and the number of iterations in Algorithm 2. The notation $a\\lesssim_{\\log(n)}b$ means that $a\\leq C_{1}(\\log n)^{C_{2}}b$ for positive constants $C_{1}$ and $C_{2}$ . ", "page_idx": 18}, {"type": "text", "text": "C.1 Properties of entropic maps ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before proving properties of the entropic map, we first recall the generalized form of (5), which holds for arbitrary measures (cf. Genevay [2019]): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(f,g)\\in L^{1}(\\mu)\\times L^{1}(\\nu)}\\int f\\mathrm{d}\\mu+\\int g\\mathrm{d}\\nu-\\varepsilon\\iint e^{(f(x)+g(y)-\\frac{1}{2}\\|x-y\\|^{2})/\\varepsilon}\\mathrm{d}\\mu(x)\\mathrm{d}\\nu(y)+\\varepsilon\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When the entropic dual formulation admits maximixers, we denote them by $(f_{\\varepsilon},g_{\\varepsilon})$ and refer to them as optimal entropic Kantorovich potentials [e.g., Genevay, 2019, Theorem 7]. Such potentials always exist if $\\mu$ and $\\nu$ have compact support. ", "page_idx": 18}, {"type": "text", "text": "We can express an entropic approximation to the optimal transport coupling $\\pi_{0}$ as a function of the dual maximizers [Csisz\u00e1r, 1975]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi_{\\varepsilon}(x,y):=\\gamma_{\\varepsilon}(x,y)\\mathrm{d}\\mu(x)\\mathrm{d}\\nu(y):=\\exp\\Big(\\frac{f_{\\varepsilon}(x)+g_{\\varepsilon}(y)-\\frac{1}{2}\\|x-y\\|^{2}}{\\varepsilon}\\Big)\\mathrm{d}\\mu(x)\\mathrm{d}\\nu(y)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When necessary, we will be explicit about the measures that give rise to the entropic coupling. For example, in place of the above, we would write ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi_{\\varepsilon}^{\\mu\\to\\nu}(x,y)=\\gamma_{\\varepsilon}^{\\mu\\to\\nu}(x,y)\\mathrm{d}\\mu(x)\\mathrm{d}\\nu(y)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The population counterpart to (7), the entropic map from $\\mu$ to $\\nu$ , is then expressed as ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{\\varepsilon}^{\\mu\\to\\nu}(x):=\\mathbb{E}_{\\pi_{\\varepsilon}}[Y|X=x]\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and similarly the entropic map from $\\nu$ to $\\mu$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{\\varepsilon}^{\\nu\\to\\mu}(y):=\\mathbb{E}_{\\pi_{\\varepsilon}}[X|Y=y]\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We write the forward (resp. backward) entropic Brenier potentials as $\\textstyle\\varphi_{\\varepsilon}:=\\,{\\frac{1}{2}}\\|\\cdot\\|^{2}\\,-\\,f_{\\varepsilon}$ (resp. $\\begin{array}{r}{\\psi_{\\varepsilon}:=\\frac{1}{2}\\|\\cdot\\|^{2}-g_{\\varepsilon})}\\end{array}$ . By dominated convergence, one can verify that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla\\varphi_{\\varepsilon}(x)=T_{\\varepsilon}^{\\mu\\to\\nu}(x)\\,,\\quad\\nabla\\psi_{\\varepsilon}(y)=T_{\\varepsilon}^{\\nu\\to\\mu}(y)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now collect some general properties of the entropic map, which we state over the ball but can be readily generalized. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6. Let $\\mu,\\nu$ be probability measures over $B(0;R)$ in $\\mathbb{R}^{d}$ . Then for a fixed $\\varepsilon>0$ , it holds that both $T_{\\varepsilon}^{\\mu\\to\\nu}$ and $T_{\\varepsilon}^{\\nu\\to\\mu}$ are Lipschitz with constant upper-bounded by $\\mathit{\\dot{R}}^{2}/\\varepsilon$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma $^{6,}$ . We prove only the case for $T_{\\varepsilon}^{\\mu\\to\\nu}$ as the proof for the other map is completely analogous. It is well-known that the Jacobian of the map is a symmetric positive semi-definite matrix: $\\nabla T_{\\varepsilon}^{\\mu\\overline{{{\\to}}}\\nu}(x)=\\varepsilon^{-1}\\mathrm{Cov}_{\\pi_{\\varepsilon}}(Y|X=x)$ (see e.g., Chewi and Pooladian [2023, Lemma 1]). Since the probability measures are supported in a ball of radius $R$ , it holds that $\\begin{array}{r}{\\operatorname*{sup}_{x}\\|\\mathbf{Cov}_{\\pi_{\\varepsilon}}(Y|X=x)\\|_{\\mathrm{op}}\\leq}\\end{array}$ $R^{2}$ , which completes the claim. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "We also require the following results from Divol et al. [2024], as well as the following object: for three measures $\\rho,\\mu,\\nu$ with finite second moments, write ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{I}:=\\int\\!\\!\\!\\int\\!\\!\\!\\int\\log\\Big(\\frac{\\gamma_{\\varepsilon}^{\\rho\\to\\mu}(x,y)}{\\gamma_{\\varepsilon}^{\\rho\\to\\nu}(x,z)}\\Big)\\gamma_{\\varepsilon}^{\\rho\\to\\mu}(x,y)\\mathrm{d}\\pi(y,z)\\mathrm{d}\\rho(x)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pi$ is an optimal transport coupling for the 2-Wasserstein distance between $\\mu$ and $\\nu$ , and $\\gamma_{\\varepsilon}$ is the density defined in (13). ", "page_idx": 18}, {"type": "text", "text": "Lemma 7. [Divol et al., 2024, Proposition 3.7 and Proposition 3.8] Suppose $\\rho,\\mu,\\nu$ have finite second moments, then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varepsilon\\bar{I}\\le\\int\\!\\!\\int\\langle T_{\\varepsilon}^{\\mu\\to\\rho}(y)-T_{\\varepsilon}^{\\nu\\to\\rho}(z),y-z\\rangle\\mathrm{d}\\pi(y,z)\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\iint\\|T_{\\varepsilon}^{\\mu\\to\\rho}(y)-T_{\\varepsilon}^{\\nu\\to\\rho}(z)\\|^{2}\\mathrm{d}\\pi(y,z)\\leq2\\bar{I}\\operatorname*{sup}_{v\\in\\mathbb{R}^{d}}\\|\\mathrm{Cov}_{\\pi_{\\varepsilon}^{\\rho\\to\\nu}}(X|Y=v)\\|_{\\mathrm{op}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We are now ready to prove Proposition 4. We briefly note that stability of entropic maps and couplings has been investigated by many [e.g., Ghosal et al., 2022, Eckstein and Nutz, 2022, Carlier et al., 2024]. These works either present qualitative notions of stability, or give bounds that depend exponentially on $1/\\varepsilon$ . In contrast, the recent work of Divol et al. [2024] proves that the entropic maps are Lipschitz with respect to variations of the target measure, where the underlying constant is linear in $\\bar{1/\\varepsilon}$ . We show that their result also encompasses variations in the source measure, which is of independent interest. ", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition $^{4}$ . Let $\\pi\\in\\Gamma(\\mu,\\mu^{\\prime})$ be the optimal transport coupling from $\\mu$ to $\\mu^{\\prime}$ . By disintegrating and applying the triangle inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu\\prime\\to\\rho}(x)\\|\\mathrm{d}\\mu(x)=\\int\\int\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu\\prime\\to\\rho}(x)\\|\\mathrm{d}\\pi(x,x^{\\prime})}}\\\\ &{\\leq\\int\\!\\int\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu\\prime\\to\\rho}(x^{\\prime})\\|\\mathrm{d}\\pi(x,x^{\\prime})}\\\\ &{\\qquad\\qquad+\\|T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(x)-T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(x^{\\prime})\\|\\mathrm{d}\\pi(x,x^{\\prime})}\\\\ &{\\leq\\int\\!\\int\\!\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(x^{\\prime})\\|\\mathrm{d}\\pi(x,x^{\\prime})}\\\\ &{\\qquad\\qquad+\\frac{R^{2}}{\\varepsilon}\\iint\\|x-x^{\\prime}\\|\\mathrm{d}\\pi(x,x^{\\prime})}\\\\ &{\\leq\\int\\!\\int\\!\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(x^{\\prime})\\|\\mathrm{d}\\pi(x,x^{\\prime})+\\frac{R^{2}}{\\varepsilon}W_{2}(\\mu,\\mu^{\\prime})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the penultimate inequality follows from Lemma 6, and the last step is due to Jensen\u2019s inequality. To bound the remaining term, recall that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{v\\in\\mathbb{R}^{d}}\\|\\mathrm{Cov}_{\\pi_{\\varepsilon}^{\\rho\\to\\nu}}(X|Y=v)\\|_{\\infty}\\le R^{2}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and by the two inequalities in Lemma 7, we have (replacing $\\nu$ with $\\mu^{\\prime}$ ) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int\\!\\!\\int\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(x^{\\prime})\\|^{2}\\mathrm{d}\\pi(x,x^{\\prime})\\le2\\bar{I}R^{2}}}\\\\ &{}&{\\qquad=\\frac{2R^{2}}{\\varepsilon}(\\varepsilon\\bar{I})}\\\\ &{}&{\\qquad\\le\\frac{2R^{2}}{\\varepsilon}\\displaystyle\\iint\\langle T_{\\varepsilon}^{\\mu\\to\\rho}(y)-T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(z),y-z\\rangle\\mathrm{d}\\pi(y,z)}\\\\ &{}&{\\qquad\\le\\frac{2R^{2}}{\\varepsilon}\\displaystyle\\left(\\iint\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(x^{\\prime})\\|\\mathrm{d}\\pi(x,x^{\\prime})\\right)W_{2}(\\mu,\\mu^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used Cauchy-Schwarz in the last line. An application of Jensen\u2019s inequality and rearranging results in the bound: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int\\!\\!\\int\\|T_{\\varepsilon}^{\\mu\\to\\rho}(x)-T_{\\varepsilon}^{\\mu^{\\prime}\\to\\rho}(x^{\\prime})\\|\\mathrm{d}\\pi(x,x^{\\prime})\\leq\\frac{2R^{2}}{\\varepsilon}W_{2}(\\mu,\\mu^{\\prime})\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the claim. ", "page_idx": 19}, {"type": "text", "text": "Finally, we require the following results from Pooladian and Niles-Weed [2021], which we restate for convenience but under our assumptions. ", "page_idx": 19}, {"type": "text", "text": "Lemma 8. [Two-sample bound: Pooladian and Niles-Weed, 2021, Theorem 3] Consider i.i.d. samples of size n from each distribution $\\mu$ and $\\nu$ , resulting in the empirical measures $\\hat{\\mu}$ and $\\hat{\\nu}$ , with the corresponding. Let $\\hat{T}_{\\varepsilon}$ be the entropic map between $\\hat{\\mu}$ and $\\hat{\\nu}$ . Under (A1)-(A3), it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|\\hat{T}_{\\varepsilon}-T_{0}\\right\\|_{L^{2}(\\mu)}^{2}\\lesssim_{\\log(n)}\\varepsilon^{-d/2}n^{-1/2}+\\varepsilon^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, i $f\\varepsilon=\\varepsilon(n)\\asymp n^{-1/(d+4)}$ , then the overall rate of convergence is $n^{-2/(d+4)}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 9. [One-sample bound: Pooladian and Niles-Weed, 2021, Theorem 4] Consider i.i.d. samples of size n from $\\nu$ , resulting in the empirical measure $\\hat{\\nu}$ , with full access to a probability measure $\\mu$ . Let $\\hat{R}_{\\varepsilon}$ be the entropic map from $\\mu$ to $\\hat{\\nu}$ . Under (A1)-(A3), it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|\\hat{R}_{\\varepsilon}-T_{0}\\right\\|_{L^{2}(\\mu)}^{2}\\lesssim_{\\log(n)}\\varepsilon^{1-d/2}n^{-1/2}+\\varepsilon^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, i ${\\mathfrak{c}}={\\varepsilon}(n)\\asymp n^{-1/(d+2)}$ , then the overall rate of convergence is $n^{-2/(d+2)}$ . ", "page_idx": 20}, {"type": "text", "text": "C.2 Remaining ingredients for the proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We start by analyzing our Progressive OT map estimator between the iterates. We will recurse on these steps, and aggregate the total error at the end. We introduce some concepts and shorthand notations. ", "page_idx": 20}, {"type": "text", "text": "First, the ideal progressive Monge problem: Let $T^{(0)}$ be the optimal transport map from $\\mu$ to $\\nu$ , and write $\\boldsymbol{\\mu}^{(0)}:=\\boldsymbol{\\mu}$ . Then write ", "page_idx": 20}, {"type": "equation", "text": "$$\nS^{(0)}:=(1-\\alpha_{0})\\,\\mathrm{Id}+\\alpha_{0}T^{(0)}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and consequently $\\mu^{(1)}:=(S^{(0)})_{\\#}\\mu^{(0)}$ . We can iteratively define $T^{(i)}$ to be the optimal transport map from $\\mu^{(i)}$ to $\\nu$ , and consequently ", "page_idx": 20}, {"type": "equation", "text": "$$\nS^{(i)}:=(1-\\alpha_{i})\\,\\mathrm{Id}+\\alpha_{i}T^{(i)}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and thus $\\boldsymbol{\\mu}^{(i+1)}:=(\\boldsymbol{S}^{(i)})_{\\#}\\boldsymbol{\\mu}^{(i)}$ . The definition of McCann interpolation implies that these iterates all lie on the geodesic between $\\mu$ and $\\nu$ . This ideal progressive Monge problem precisely mimicks our progressive map estimator, though (1) these quantities are defined at the population level, and (2) the maps are defined a solutions to the Monge problem, rather than its entropic analogue. Recall that we write $\\hat{\\mu}$ and $\\hat{\\nu}$ as the empirical measures associated with $\\mu$ and $\\nu$ , and recursively define $\\mathcal{E}^{(i)}$ to be the entropic map from $\\hat{\\mu}_{\\varepsilon}^{\\left(i\\right)}$ to $\\hat{\\nu}$ , where $\\hat{\\mu}_{\\varepsilon}^{\\left(0\\right)}:=\\hat{\\mu}$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{\\varepsilon}^{(i+1)}:=(\\mathcal{S}^{(i)})_{\\#}\\hat{\\mu}_{\\varepsilon}^{(i)}:=((1-\\alpha_{i})\\operatorname{Id}+\\alpha_{i}\\mathcal{E}^{(i)})_{\\#}\\hat{\\mu}_{\\varepsilon}^{(i)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We also require $\\hat{R}_{\\varepsilon}^{\\left(i\\right)}$ , defined to be the the entropic map between $\\mu^{(i)}$ and $\\hat{\\nu}$ using regularization $\\varepsilon_{i}$ . This map can also be seen as a \u201cone-sample\" estimator, which starts from iterates of the McCann interpolation, and maps to an empirical target distribution. ", "page_idx": 20}, {"type": "text", "text": "To control the performance of $\\hat{R}_{\\varepsilon}^{\\left(i\\right)}$ below, we want to use Lemma 9. To do so, we need to verify that $\\mu^{(i)}$ also satisfies the key assumptions (A1) to (A3). This is accomplished in the following lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma 10 (Error rates for $\\hat{R}_{\\varepsilon}^{\\left(i\\right)}$ ). For any $i\\geq0$ , the measures $\\mu^{(i)}$ and $\\nu$ continue to satisfy (A1) to (A3), and thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\hat{R}_{\\varepsilon}^{(i)}-T_{0}\\|_{L^{2}(\\mu^{(i)})}^{2}\\lesssim\\varepsilon_{i}^{1-d/2}n^{-1/2}+\\varepsilon_{i}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We verify that the conditions (A1) to (A3) hold for the pair $(\\mu^{(1)},\\nu)$ ; repeating the argument for the other iterates is straightforward. ", "page_idx": 20}, {"type": "text", "text": "First, we recall that for two measures $\\mu_{0}:=\\mu,\\mu_{1}:=\\nu$ with support in a convex subset $\\Omega\\subseteq\\mathbb{R}^{d}$ , the McCann interpolation $(\\mu_{\\alpha})_{\\alpha\\in[0,1]}$ remains supported in $\\Omega$ ; see Santambrogio [2015, Theorem 5.27]. Moreover, by Proposition 7.29 in Santambrogio [2015], it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mu_{\\alpha}\\|_{L^{\\infty}(\\Omega)}\\leq\\operatorname*{max}\\{\\|\\mu_{0}\\|_{L^{\\infty}(\\Omega)},\\|\\mu_{1}\\|_{L^{\\infty}(\\Omega)}\\}\\leq\\operatorname*{max}\\{\\mu_{\\operatorname*{max}},\\nu_{\\operatorname*{max}}\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $\\alpha\\in[0,1]$ , recall that the quantities $\\mu_{\\mathrm{max}},\\nu_{\\mathrm{max}}$ are from (A1). Thus, the density of $\\mu^{(1)}=$ $\\mu_{\\alpha_{0}}=\\left(\\left(1-\\alpha_{0}\\right)\\mathrm{Id}+\\alpha_{0}T\\right)\\!\\#\\mu$ is uniformly upper bounded on $\\Omega$ ; altogether this covers (A1). For ", "page_idx": 20}, {"type": "text", "text": "(A2) and (A3), note that we are never leaving the geodesic. Rather than study the \u201cforward\u201d map, we can therefore instead consider the \u201creverse\" map ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{T}:=\\left(\\alpha_{0}\\,\\mathrm{Id}+(1-\\alpha_{0})T^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which satisfies $\\bar{T}_{\\#}\\nu=\\mu^{(1)}$ and hence $T^{(1)}=\\bar{T}^{-1}$ . We now verify the requirements of (A2) and (A3). For (A2), since $(T^{(1)})^{-1}=\\bar{T}=\\alpha_{0}\\,\\mathrm{Id}\\,{+}(1-\\alpha_{0})T^{-1}$ , and $T^{-1}$ is three-times continuously differentiable by assumption, the map $(T^{(1)})^{-1}$ is also three times continuously differentiable, with third derivative bounded by that of $T^{-1}$ . For (A3), we use the fact that $T=\\nabla\\varphi_{0}$ for some function $\\varphi_{0}$ which is $\\Lambda$ -smooth and $\\lambda$ -strongly convex. Basic properties of convex conjugation then imply that $T^{(1)}=\\nabla\\varphi_{1}$ , where $\\begin{array}{r}{\\varphi_{1}=(\\alpha_{0}\\frac{\\|\\cdot\\|^{2}}{2}+(1-\\alpha_{0})\\varphi_{0}^{*})^{*}}\\end{array}$ . Since the conjugate of a $\\lambda$ -strongly convex function is $\\lambda^{-1}$ -smooth, and conversely, we obtain that the function $\\varphi_{1}$ is $(\\alpha_{0}+(1-\\alpha_{0})\\lambda^{-1})^{-1}$ strongly convex and $(\\alpha_{0}+(1-\\alpha_{0})\\Lambda^{-1})^{-1}$ . In particular, since $(\\alpha_{0}+(1-\\alpha_{0})\\lambda^{-1})^{-1}\\geq\\operatorname*{min}(1,\\lambda)$ and $(\\alpha_{0}+(1-\\alpha_{0})\\Lambda^{-1})^{-1}\\leq\\operatorname*{max}(1,\\Lambda)$ , we obtain that $D T^{(1)}$ is uniformly bounded above and below. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We define the following quantities which we will recursively bound: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{i}:=\\mathbb{E}\\Vert\\mathsf{E}^{(i)}-T^{(i)}\\Vert_{L^{2}(\\mu^{(i)})}^{2}\\,,\\Delta_{R_{i}}:=\\Vert\\hat{R}_{\\varepsilon}^{(i)}-T^{(i)}\\Vert_{L^{2}(\\mu^{(i)})}^{2}\\,,\\mathbb{W}_{i}:=\\mathbb{E}W_{2}^{2}(\\hat{\\mu}_{\\varepsilon}^{(i)},\\mu^{(i)})\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as well as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{i}:=1-\\alpha_{i}+R^{2}\\frac{\\alpha_{i}}{\\varepsilon_{i}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where recall $R$ is the radius of the ball $B(0;R)$ in $\\mathbb{R}^{d}$ . ", "page_idx": 21}, {"type": "text", "text": "First, the following lemma: ", "page_idx": 21}, {"type": "text", "text": "Lemma 11. If the support of $\\mu$ and $\\nu$ is contained in $B(0;R)$ and $\\alpha_{i}\\lesssim\\varepsilon_{i}$ for $i=0,\\ldots,k,$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{T}_{\\mathrm{Prog}}^{(k)}-T_{0}\\right\\|_{L^{2}(\\mu)}^{2}\\lesssim\\sum_{i=0}^{k}\\Delta_{i}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We prove this lemma by iterating over the quantity defined by ", "page_idx": 21}, {"type": "equation", "text": "$$\nE_{j}:=\\Big\\|\\mathsf{E}^{(k)}\\circ\\mathsf{S}^{(k-1)}\\circ\\cdots\\circ\\mathsf{S}^{(j)}-T^{(k)}\\circ S^{(k-1)}\\circ\\cdots\\circ S^{(j)}\\Big\\|_{L^{2}(\\mu^{(j)})}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "when $j\\leq k-1$ and $E_{k}=\\Delta_{k}$ . By adding and subtracting $S^{(j)}$ and $\\hat{S}_{\\varepsilon}^{\\left(j\\right)}$ appropriately, we obtain for $j\\le k-1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{j}\\lesssim\\left\\|\\boldsymbol{\\mathcal{E}}^{(k)}\\circ\\boldsymbol{\\mathcal{S}}^{(k-1)}\\circ\\cdots\\circ\\boldsymbol{\\mathcal{S}}^{(j)}-\\boldsymbol{\\mathcal{E}}^{(k)}\\circ\\boldsymbol{\\mathcal{S}}^{(k-1)}\\circ\\cdots\\circ\\boldsymbol{\\mathcal{S}}^{(j+1)}\\circ\\boldsymbol{S}^{(j)}\\right\\|_{L^{2}(\\mu^{(j)})}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\left\\|\\boldsymbol{\\mathcal{E}}^{(k)}\\circ\\boldsymbol{\\mathcal{S}}^{(k-1)}\\circ\\cdots\\circ\\boldsymbol{\\mathcal{S}}^{(j+1)}\\circ\\boldsymbol{S}^{(j)}-T^{(k)}\\circ\\boldsymbol{S}^{(k-1)}\\circ\\cdots\\circ\\boldsymbol{S}^{(j)}\\right\\|_{L^{2}(\\mu^{(j)})}^{2}}\\\\ &{\\quad\\leq\\left(\\alpha_{j}\\mathrm{Lip}(\\boldsymbol{\\mathcal{E}}^{(k)})\\mathrm{Lip}(\\boldsymbol{\\mathcal{S}}^{(k-1)})\\cdots\\mathrm{Lip}(\\boldsymbol{\\mathcal{S}}^{(j+1)})\\right)^{2}\\Delta_{j}+E_{j+1}}\\\\ &{\\lesssim\\left(\\frac{\\alpha_{j}}{\\varepsilon_{k}}\\prod_{l=j+1}^{k-1}A_{\\ell}\\right)^{2}\\Delta_{j}+E_{j+1}}\\\\ &{\\lesssim\\Delta_{j}+E_{j+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in the last inequality we have used the fact that $\\alpha_{j}\\lesssim\\varepsilon_{j}$ , so that $A_{k}\\lesssim1$ for all $k$ . Repeating this process yields $\\begin{array}{r}{\\left\\|\\mathcal{T}_{\\mathrm{Prog}}^{(k)}-T_{0}\\right\\|_{L^{2}(\\mu)}^{2}=E_{0}\\lesssim\\sum_{i=0}^{k}\\dot{\\Delta}_{k}}\\end{array}$ , which completes the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "To prove Theorem 3, it therefore suffices to bound $\\Delta_{k}$ . We prove the following lemma by induction, which gives the proof. ", "page_idx": 21}, {"type": "text", "text": "Lemma 12. Assume $d\\geq4.$ . Suppose (A1) to (A3) hold, and $\\alpha_{i}\\asymp n^{-1/d}$ and $\\varepsilon_{i}\\asymp n^{-1/2d}$ for all $i\\in[k]$ . Then it holds that for $k\\geq0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{W}_{k}\\lesssim_{\\log(n)}n^{-2/d}\\,,\\quad a n d\\quad\\Delta_{k}\\lesssim_{\\log(n)}n^{-1/d}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We proceed by induction. For the base case $k\\,=\\,0$ , the bounds of Fournier and Guillin [2015] imply that $\\mathbb{W}_{0}\\,=\\,\\mathbb{E}[W_{2}^{2}(\\hat{\\mu},\\mu)]\\,\\lesssim\\,n^{-2/d}$ . Similarly, by Lemma 8, we have $\\Delta_{0}\\,\\lesssim_{\\log(n)}$ $\\varepsilon_{0}^{-d/2}n^{-1/2}+\\varepsilon_{0}^{2}\\lesssim_{\\log(n)}n^{-1/d}$ ", "page_idx": 22}, {"type": "text", "text": "Now, assume that the claimed bounds hold for $\\mathbb{W}_{k}$ and $\\Delta_{k}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{W}_{k+1}=\\mathbb{E}[W_{2}^{2}((\\S^{(k)})_{\\#}\\hat{\\mu}_{\\varepsilon}^{(k)},(S^{(k)})_{\\#}\\mu^{(k)})]}\\\\ &{\\quad\\lesssim\\mathbb{E}\\,W_{2}^{2}\\left((\\S^{(k)})_{\\#}\\hat{\\mu}_{\\varepsilon}^{(k)},(\\S^{(k)})_{\\#}\\mu^{(k)}\\right)+\\mathbb{E}\\,W_{2}^{2}\\left((\\S^{(k)})_{\\#}\\mu^{(k)},(S^{(k)})_{\\#}\\mu^{(k)}\\right)}\\\\ &{\\quad\\quad\\leq\\mathbb{E}[\\mathrm{Lip}(\\S^{(k)})^{2}W_{2}^{2}(\\hat{\\mu}_{\\varepsilon}^{(k)},\\mu^{(k)})]+\\mathbb{E}\\Big\\|\\mathbb{S}^{(k)}-S^{(k)}\\Big\\|_{L^{2}(\\mu^{(k)})}^{2}}\\\\ &{\\quad\\lesssim A_{k}^{2}\\mathbb{W}_{k}+\\alpha_{k}^{2}\\Delta_{k}}\\\\ &{\\quad\\lesssim\\mathbb{W}_{k}+\\alpha_{k}^{2}\\Delta_{k}}\\\\ &{\\quad\\lesssim n^{-2/d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last step follows by the induction hypothesis and the choice of $\\alpha_{k}$ . By Proposition 4 and the preceding bound, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Delta_{k+1}\\lesssim\\|\\mathcal{E}^{(k+1)}-\\hat{R}_{\\varepsilon}^{(k+1)}\\|_{L^{2}(\\mu^{(k+1)})}^{2}+\\Delta_{R_{k+1}}}}\\\\ &{\\lesssim\\varepsilon_{k+1}^{-2}\\mathbb{W}_{k+1}+\\Delta_{R_{k+1}}}\\\\ &{\\lesssim\\varepsilon_{k+1}^{-2}n^{-2/d}+\\Delta_{R_{k+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 10 implies that $\\Delta_{R_{k+1}}\\lesssim_{\\log n}n^{-1/d}$ . The choice of $\\varepsilon_{k+1}$ therefore implies $\\Delta_{k+1}\\lesssim_{\\log(n)}$ $n^{-1/d}$ , completing the proof. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "C.3 Proofs for the CIFAR Benchmark ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Proposition $5$ . By Proposition 13, the Gaussian blur map $G$ is a linear positive-definite operator. Considering the $\\begin{array}{r}{\\dot{h}=\\frac{1}{2}\\lVert\\cdot\\rVert_{2}^{2}}\\end{array}$ cost, then $G$ acts as a Monge map between from distribution $\\hat{\\mu}$ over a finite set of images, onto their blurred counterparts $\\hat{\\nu}=G_{\\#}\\hat{\\mu}$ . This is a direct corollary of Brenier\u2019s Theorem, and follows the fact that $G$ is the gradient of $\\begin{array}{r}{\\ddot{h}(U)=\\frac{1}{2}\\langle U,G(U)\\rangle}\\end{array}$ the convex potential, and therefore a Monge map [c.f. Section 1.3.1, Santambrogio, 2015]. Therefore, and again following Brenier, the optimal assignment between $\\hat{\\mu}$ and their blurred counterparts $\\hat{\\nu}$ , is necessarily that which maps an image to its blurred version regardless of the value of $\\sigma<\\infty$ and the optimal permutation is the identity. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Proposition 13. The gaussian blur operator $G:U\\rightarrow K U K$ is a linear positive-definite operator, where $U,K\\in\\mathbb{R}^{N\\times N}$ and the kernel $K$ is defined via ", "page_idx": 22}, {"type": "equation", "text": "$$\nK=\\left[\\exp\\left(-(i-j)^{2}/(\\sigma N^{2})\\right)\\right]_{i j},\\quad\\forall i,j\\leq N.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The linearity of the operator is implied by the linearity of matrix multiplication. As for the positive-definiteness, we show that the kernel matrix corresponding to the operator $G$ is positivedefinite. Consider $s$ images $U_{1},\\dots,U_{s}$ in $\\mathbb{R}^{N\\times N}$ and the corresponding kernel matrix $A\\in\\mathbb{R}^{s\\times s}$ defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\nA_{i j}:=\\langle U_{i},\\,G(U_{j})\\rangle=\\langle U_{i},\\,K U_{j}K\\rangle=\\langle K U_{i},\\,K U_{j}\\rangle.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is a dot-product matrix (of all elements $K U_{i}$ ), and is therefore always positive definite. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have linked the numerical and theoretical evidence for the claims made in the abstract and introduction in the text. We have provided a theorem showing consistency of our method. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The limitation of theory are clearly stated in Assumptions A1-A3. We report the error and runtime of the algorithm, showing the numerical limitations, over numerous runs and 3 different datasets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 23}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: This is provided in A1-A3 and in the statement of the Theorem and lemmas in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All experimental details are reported in the beginning of the experiment section. If a hyper-parameter is chosen via cross-validation, the CV method is described. Otherwise, the exact values are reported. We provide a detailed algorithm pseudo-code which allows the reader to re-implement the method without using our code. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not share new dataset. We include the base implementation of our main algorithm in Jax as supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is detailed in first paragraph of Section 5 and in a later paragraph titled \"Comparing Map Estimators on Single Cell Data\". ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We run every experiment for 5 (or 10 depending on the experiment) random seeds. All results are reported with error bars indicating the standard error. In our Table, we do the same, reporting only numbers that have statistical significance. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our experiments are light and ran on a single GPU node. Details mentioned in Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We fully adhere to the NeurIPS code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper is of theoretical and methodological nature. We do not foresee an application of our proposed algorithm which may have a direct social impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not release a model or a dataset. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We use open sourced data and code. The reference packages and papers are cited. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not introduce a new asset. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not have involve crowdsourcing or human feedback. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not have involve crowdsourcing or human feedback. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]