{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper is foundational to the field of vision transformers, introducing the architecture that the current paper builds upon and improves."}, {"fullname_first_author": "Mostafa Dehghani", "paper_title": "Scaling vision transformers to 22 billion parameters", "publication_date": "2023-00-00", "reason": "This paper demonstrates the current state-of-the-art in vision transformers, providing a benchmark for comparison and context for the improvements proposed in the current paper."}, {"fullname_first_author": "Xiaofeng Mao", "paper_title": "Towards robust vision transformer", "publication_date": "2022-00-00", "reason": "This paper directly addresses the issue of robustness in vision transformers, which is the central focus of the current paper, providing related work and a baseline for comparison."}, {"fullname_first_author": "Alexander Mordvintsev", "paper_title": "Growing neural cellular automata", "publication_date": "2020-00-00", "reason": "This paper introduces neural cellular automata, the core component that the current paper adapts and integrates into vision transformers."}, {"fullname_first_author": "Yong Guo", "paper_title": "Robustifying token attention for vision transformers", "publication_date": "2023-00-00", "reason": "This paper presents a state-of-the-art approach to improving the robustness of vision transformers, offering another relevant comparison point for the current work."}]}