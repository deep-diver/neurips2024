{"importance": "This paper is crucial for researchers working with large language models (LLMs) because **it introduces a novel method to reliably detect and quantify data contamination**, a significant problem affecting model evaluation and comparison.  It **challenges the traditional definition of contamination**, offering a more practical and robust approach that is less susceptible to evasion.  The findings will **improve benchmark reliability** and help establish more trustworthy model comparisons, driving advancements in the field.", "summary": "ConStat: Exposing hidden LLM contamination!", "takeaways": ["A novel performance-based definition of contamination in LLMs is proposed, focusing on artificially inflated and non-generalizing benchmark performance.", "ConStat, a statistical method, effectively detects and quantifies contamination by comparing performance across primary and reference benchmarks and models.", "Extensive evaluation reveals high contamination levels in various popular LLMs (e.g., MISTRAL, LLAMA, YI), highlighting the critical need for robust contamination detection methods."], "tldr": "Large language models (LLMs) are evaluated using public benchmarks, but data contamination inflates their performance, hindering reliable comparisons.  Current detection methods are easily bypassed, failing to quantify contamination's impact. This paper addresses these issues.  The proposed approach, ConStat, uses a statistical method that effectively identifies and quantifies contamination by comparing performance across multiple benchmarks and reference models. It's robust against evasion techniques, offering a reliable solution for evaluating and comparing LLMs. \nConStat's key contribution lies in its novel definition of contamination as artificially inflated performance that doesn't generalize to rephrased samples, synthetic data, or similar tasks.  The method directly compares the model's actual performance to its expected performance (based on reference models and a secondary benchmark) and uses bootstrapping to quantify the contamination's magnitude with a p-value. This work was thoroughly evaluated on several model architectures, contamination scenarios, and benchmarks, showcasing its effectiveness in an extensive evaluation and highlighting significant levels of contamination in several prominent LLMs. This offers a valuable improvement over existing methods for evaluating LLMs.", "affiliation": "ETH Zurich", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "ALISPmDPCq/podcast.wav"}