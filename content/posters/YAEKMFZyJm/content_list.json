[{"type": "text", "text": "Finding NeMo : Localizing Neurons Responsible For Memorization in Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dominik Hintersdorf \u22171,2 Lukas Struppek\u22171,2 Kristian Kersting1,2,3,4 Adam Dziedzic5 Franziska Boenisch5 ", "page_idx": 0}, {"type": "text", "text": "1German Research Center for Artificial Intelligence (DFKI)   \n2Computer Science Department, Technical University of Darmstadt 3Hessian Center for AI (Hessian.AI)   \n4Centre for Cognitive Science, Technical University of Darmstadt 5CISPA Helmholtz Center for Information Security ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models (DMs) produce very detailed and high-quality images. Their power results from extensive training on large amounts of data\u2014usually scraped from the internet without proper attribution or consent from content creators. Unfortunately, this practice raises privacy and intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples during inference, or removing the memorized data from training altogether. While those are viable solutions when the DM is developed and deployed in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released. To solve the problem, we introduce NEMO, the first method to localize memorization of individual data samples down to the level of neurons in DMs\u2019 cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, single neurons are responsible for memorizing particular training samples. By deactivating these memorization neurons, we can avoid the replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data. In this way, our NEMO contributes to a more responsible deployment of DMs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, diffusion models (DMs) have made remarkable advances in image generation. In particular, text-to-image DMs, such as Stable Diffusion [32], DALL-E [30], or Deep Floyd [38] enable the generation of complex images given a textual input prompt. Yet, DMs carry a significant risk to privacy and intellectual property, as the models have been shown to generate verbatim copies of their potentially sensitive or copyrighted training data at inference time [8, 36]. This ability has often been linked to their memorization of training data [48, 12, 1, 5]. Memorization in DMs recently received a lot of attention [16, 47, 49, 8], and several mitigations have been proposed [46, 31, 36]. Those mitigations usually focus on either identifying potentially highly memorized samples and excluding them from training, monitoring inference and preventing their generation, or altering the inputs to prevent the verbatim output of training data [31, 36, 46]. While mitigations that rely on preventing the generation of memorized samples are effective when the DM is developed and deployed in a secure environment, they hold the inherent risk of adversaries circumventing them. Additionally, they are not effective when the DMs are publicly released, such that users can freely interact with them. ", "page_idx": 0}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/fc6b45252566f54ee9d34b9ade09d4f9214ac7c820d56cf405961efe9b48b2ea.jpg", "img_caption": ["Figure 1: Overview of NeMo. For memorized prompts, we observe that the same (original training) image is constantly generated independently of the initial random seed. This yields severe privacy and copyright concerns. In the initial stage, NEMO $\\clubsuit$ first identifies candidate neurons potentially responsible for the memorization based on out-of-distribution activations. In a refinement step, NEMO $\\clubsuit$ detects the memorization neurons from the candidate set by leveraging the noise similarities during the first denoising step. Deactivating memorization neurons prevents unintended memorization behavior and induces diversity in the generated images. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As a first step to solving this problem, we propose FINDING NEURON MEMORIZATION (NEMO), a new method for localizing where individual data samples are memorized inside the DMs. NEMO\u2019s localization tracks the memorization of training data samples down to the level of individual neurons in the DMs\u2019 cross-attention layers. To achieve this, NEMO relies on analyzing the different activation patterns of individual neurons on memorized and non-memorized data samples and identifying memorization neurons by outlier activation detection (as visualized in Fig. 2b). We empirically assess the success of NEMO on the publicly available DM Stable Diffusion [32]. Our findings indicate that most memorization happens in the value mappings of the cross-attention layers of DMs. Furthermore, they highlight that most training data samples are memorized by just a few or even a single neuron, which is surprising given the high resolution and complexity of the training data. ", "page_idx": 1}, {"type": "text", "text": "Based on the insights about where within the DMs individual data samples are memorized, we can prevent their verbatim output by deactivating the identified memorization neuron(s). We demonstrate the effect of NEMO in Fig. 1. Without our approach, the image generated for the memorized input prompt is the same, independent of the random seed for generation. By localizing the neuron responsible for the memorization through NEMO and deactivating it, we prevent the verbatim output of the training data and instead cause the generation of various non-memorized related samples. Hence, by relying on NEMO to localize and deactivate memorization neurons, we can limit memorization, which mitigates the privacy and copyright issues while keeping the overall performance intact. ", "page_idx": 1}, {"type": "text", "text": "In summary, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose NEMO, the first method to localize where memorization happens within DMs down to the level of individual neurons.   \n\u2022 Our extensive empirical evaluation of localizing memorization within Stable Diffusion reveals that few or even single neurons are responsible for the memorization.   \n\u2022 We limit the memorization in DMs by deactivating the highly memorizing neurons and further show that this leads to a higher diversity in the generated outputs. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Text-to-Image Synthesis with Diffusion Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion models (DMs) [37, 19] are generative models trained by progressively adding random Gaussian noise to training images and having the model learn to predict the added noise. After the training is finished, new samples can be generated by sampling an initial noise image $x_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and then iteratively removing portions of the predicted noise $\\epsilon_{\\theta}(x_{t},t,y)$ at each time step $t=T,\\dots,1$ . ", "page_idx": 1}, {"type": "text", "text": "This denoising process is formally defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(x_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x_{t},t,y)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with variance scheduler $\\beta_{t}\\in(0,1)$ , $\\alpha_{t}=1-\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{t}}\\end{array}$ . The noise predictor $\\epsilon_{\\theta}(x_{t},t,y)$ , usually a U-Net [33], receives an additional input $y$ for conditional image generation. ", "page_idx": 2}, {"type": "text", "text": "Common text-to-image DMs [32, 30, 34] are conditioned on text embeddings $y$ computed by pretrained text encoders like CLIP [29]. The typical way to incorporate the conditioning $y$ into the denoising process is the cross-attention mechanism [41]. (Cross-)Attention consists of three main components: query matrices $Q=z_{t}W_{Q}$ , key matrices $K=y W_{K}$ , and value matrices $V=y W_{V}$ . All three matrices are computed by applying learned linear projections $W_{Q},W_{K}$ , and $W_{V}$ to the hidden image representation $z_{t}$ and the text embeddings $y$ . The attention outputs are computed by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right)\\cdot V\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with scaling factor $d$ . Importantly, in most text-to-image models, the noise predictor receives guidance only through the cross-attention layers, which renders them particularly relevant for memorization. ", "page_idx": 2}, {"type": "text", "text": "2.2 Memorization in Deep Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Memorization. Memorization was extensively studied in supervised models and with respect to data labels [48, 1, 9]. Recently, studies have been extended to unlabeled self-supervised learning [25, 43]. In both setups, it was shown that memorization is required for generalization [12, 13, 43]. However, memorization also yields privacy risks [4, 7, 14, 40] since it can expose sensitive training data. In particular for generative models, including DMs, it was shown that memorization enables the extraction of training data points [4\u20136, 8, 36]. ", "page_idx": 2}, {"type": "text", "text": "Localizing Memorization. Early work on localizing where inside machine learning (ML) models memorization happens focuses on small neural networks. Initial findings suggested that in supervised models, memorization happens in the deeper layers [2, 39]. However, more fine-grained analyses contradict these findings and identify that individual units, i.e., individual neurons or convolutional channels throughout the entire model, are responsible for memorization [24]. To identify these, Maini et al. [24] deactivate units throughout the network until a label flip on the memorized training input image occurs. However, due to the unavailability of labels, this approach does not transfer to DMs. ", "page_idx": 2}, {"type": "text", "text": "Memorization in Diffusion Models. Recent empirical studies connect the model architecture, training data complexity, and the training procedure to the expected level of DM memorization [16], while others connect memorization to the generalization of the generation process [47]. Two types of memorization are usually distinguished: Verbatim memorization that replicates the training image exactly. And template memorization that reproduces the general composition of the training image while having some non-semantic variations at fixed image positions [45]. Existing approaches for detecting memorized training samples are based on statistical differences in the model behavior when queried with memorized prompts. These approaches explore differences in predicted noise magnitudes [46], the distribution of attention scores [31], the amount of noise modification in one-step synthesis [45], and the edge consistency in generated images [45]. Our work is orthogonal to these detection methods, focusing on the exact localization of memorization in the DM\u2019s U-Net rather than detecting memorized samples. ", "page_idx": 2}, {"type": "text", "text": "Previously proposed methods for mitigating memorization during inference either rescale the attention logits [31] or adjust the text embeddings with a gradient-based approach to minimize the magnitude of noise predictions [46]. However, these inference time mitigation strategies are easy to deactivate in practice and provide no permanent mitigation strategies for publicly released models. In contrast, related training-based mitigation strategies [46, 31] require re-training an already trained model like Stable Diffusion, which is time- and resource-intensive. We show that NEMO can reliably identify individual neurons responsible for memorizing specific training samples. Pruning these neurons effectively mitigates memorization, does not harm the general model performance, and provides a more permanent solution to avoid training data replication. ", "page_idx": 2}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/49816e76b66608d047868a17c0c781e7728e41af94e0eb679857c2b303a69cb3.jpg", "img_caption": ["Figure 2: Differences Between Memorized and Non-memorized Prompts. (a) depicts the distribution of pairwise SSIM scores between initial noise differences starting from different seeds. Since the noise trajectories are more consistent for memorized samples, the score reflects the degree of memorization. (b) shows the distribution of the $z$ -scores of each neuron in the first cross-attention value layer. Memorization neurons produce considerably higher activations, here depicted as standardized $z$ -scores, for memorized prompts, allowing them to be identified by outlier detection. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 NeMo: Localizing and Removing Memorization in Diffusion Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "NEMO, our method for detecting memorization neurons, consists of a two-step selection process: ", "page_idx": 3}, {"type": "text", "text": "(1) Initial Selection: We first identify a broad set of candidate neurons that might be responsible for memorizing a specific training sample. This initial selection is coarse-grained to speed up the computation among the many neurons in DMs. Consequently, it might select false positives, i.e., neurons not directly responsible for memorization. ", "page_idx": 3}, {"type": "text", "text": "(2) Refinement: In the refinement step, we fliter neurons out to reduce the size of the initial candidate set. After refinement, we deactivate the remaining memorization neurons to remove memorization. ", "page_idx": 3}, {"type": "text", "text": "In our study, we apply this two-step approach of NEMO to detect memorization neurons in the DM\u2019s cross-attention layers, the only components that directly process the text embeddings. Image editing research [17, 42, 10] shows that cross-attention layers highly influence the generated content, so we expect them to be the driving force behind memorization. We analyze the impact of blocking individual key and value layers in Appx. C.8 and the influence of blocking neurons in the convolutional layers in Appx. C.9. Our results show that the value layers in the down- and mid-blocks of the UNet indeed have the highest memorization effect, whereas value layers in the up-blocks barely affect the memorization. Deactivating the outputs of neurons in value layers completely blocks the information flow of the guidance signal and, hence, potential memorization triggers. Deactivating the key layers in cross-attention also impacts memorization but often impedes the image-prompt alignment. Similarly, deactivating neurons in the convolutional layers of the U-Net did not mitigate memorization. Therefore, we limit our search on memorization neurons to the value layers of the U-Net\u2019s down- and mid-blocks. While the identified neurons effectively mitigate the data replication problem, we emphasize that other parts of the U-Net might also play a crucial role in memorizing training data. Specifically, the identified neurons trigger the data replication, which is then executed by other parts of the U-Net, such as convolutional and fully connected layers. Deactivating the memorization neurons in value layers effectively interrupts the memorization chain and replication process. Before detailing the two steps of NEMO\u2019s selection process in Sec. 3.2 and Sec. 3.3, we first introduce how we quantify memorization strength in the next section. We provide detailed algorithmic descriptions for each of NEMO\u2019s components in Appx. D. ", "page_idx": 3}, {"type": "text", "text": "3.1 Quantifying the Memorization Strength ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Intuitively, the denoising process of DMs for memorized prompts follows a rather consistent trajectory to reconstruct the corresponding training image, yielding image generations with little diversity. Conversely, the denoising trajectory highly depends on the initially sampled noise for non-memorized prompts [46]. We measure the similarity between the first denoising steps for different initial seeds as a proxy to compare the denoising trajectories and to quantify the memorization strength. The higher the similarity, the more consistent the denoising trajectories, which indicates stronger memorization. Let $x_{T}\\sim\\dot{\\mathcal{N}}(\\mathbf{0},\\mathbf{I})$ be the initial noise following the denoising process described in Sec. 2.1. ", "page_idx": 3}, {"type": "text", "text": "Let $\\epsilon_{\\theta}(x_{T},T,y)$ further denote the initial noise prediction. We found that the normalized difference between the initial noise and the first noise prediction $\\delta=\\epsilon_{\\theta}(x_{T},T,y)-x_{T}$ for memorized prompts is more consistent for different seeds than for non-memorized prompts. We visualize this phenomenon for some initial noise differences in Appx. C.6. ", "page_idx": 4}, {"type": "text", "text": "To detect the grade of memorization, we, therefore, use the similarity between the noise differences $\\delta^{(i)}$ and $\\delta^{(j)}$ generated with seeds $i$ and $j$ as a proxy. We measure the similarity with the common structural similarity index measure (SSIM) [44]. A formal description of the $\\mathrm{SSIM}\\in[-1,1]$ score and an additional experiment outlining how the SSIM score can be used to detect memorization in the first place is provided in Appx. B.4. ", "page_idx": 4}, {"type": "text", "text": "A higher SSIM indicates higher similarity between the noise differences, reflecting a higher degree of memorization. Notably, the SSIM computation only requires a single denoising step per seed, which makes the process fast. To set a memorization threshold $\\tau_{\\mathrm{mem}}$ , starting from which we define a sample as memorized, we first compute the mean SSIM on a holdout set of non-memorized prompts. We compute the pairwise SSIM between ten different initial noise samples for each prompt and take the maximum score. After that, we average the scores across all prompts and set the threshold $\\tau_{\\mathrm{mem}}$ to the mean plus one standard deviation. We consider the current image generation non-memorized if the maximum pairwise SSIM scores are below this threshold $\\tau_{\\mathrm{mem}}$ . Fig. 2a shows the distribution of SSIM scores for memorized and non-memorized prompts, demonstrating that memorized prompts lead to a substantially higher score. ", "page_idx": 4}, {"type": "text", "text": "3.2 Initial Candidate Selection for Memorization Neurons ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With our measure for quantifying the strength of memorization defined, we move on to detail the first step of our NEMO\u2019 localization. Our initial neuron selection procedure is based on the observation that activation patterns of memorized prompts differ from the ones of non-memorized prompts on the neuron level. Fig. 2b underlines this observation by plotting the standardized activation scores for memorized and non-memorized samples in the first value layer. Leveraging this insight, we identify memorization neurons as the ones that exhibit an out-of-distribution (OOD) activation behavior. We first compute the standard activation behavior of neurons on a separate hold-out set of non-memorized prompts. Then, we compare the activation pattern of the neurons for memorized prompts and identify neurons with OOD behavior. Let the cross-attention value layers of a DM be $\\bar{l}\\in\\{1,\\ldots,L\\}$ . We denote the activation of the $i$ -th neuron in the $l$ -th layer for prompt $y$ as $a_{i}^{l}(y)$ . The activation values are averaged across the absolute neuron activations for each token vector in the text embedding. Let $\\mu_{i}^{l}$ be the pre-computed mean activation and $\\sigma_{i}^{l}$ the corresponding standard deviation for this neuron. To detect neurons potentially responsible for the memorization of a memorized prompt $y$ , we compute the standardized $z$ -score [20], defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{i}^{l}(y)=\\frac{a_{i}^{l}(y)-\\mu_{i}^{l}}{\\sigma_{i}^{l}}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The $z$ -score quantifies the number of standard deviations $\\sigma_{i}^{l}$ by which the activation $a_{i}^{l}(y)$ is above or below the mean activation $\\mu_{i}^{l}$ . Here, the activation $a_{i}^{l}(y)$ is calculated by taking the mean over the absolute token activations. To identify a neuron as exhibiting an OOD activation behavior, we set a threshold $\\theta_{\\mathrm{act}}$ and assume that neuron $i$ in layer $l$ has OOD behavior if $|z_{i}^{l}(y)|>\\theta_{\\mathrm{act}}$ . The lower the threshold $\\theta_{\\mathrm{act}}$ , the more neurons are labeled as OOD and added to the memorization neuron candidate set. An algorithmic description of the OOD detection step is provided in Alg. 2 in Appx. D.2. ", "page_idx": 4}, {"type": "text", "text": "Fig. 2a shows that the pairwise SSIM score can be used to measure the generated sample\u2019s degree of memorization. Hence, to get an initial selection of memorization neurons, we calculate the standardized $z$ -scores for all neurons and start with a relatively high value of $\\theta_{\\mathrm{act}}=5$ . We deactivate all neurons with OOD activations given the current threshold $\\theta_{\\mathrm{act}}$ , i.e., setting the output of a neuron to 0 if $|z_{i}^{l}(y)|\\,>\\,\\theta_{\\mathrm{act}}$ , to reduce the memorization strength. If, after deactivating these neurons, the memorization score is not below the threshold $\\tau_{\\mathrm{mem}}$ , we then iteratively decrease the activation threshold $\\theta_{\\mathrm{act}}$ by 0.25 and update the candidate set until the target memorization score $\\tau_{\\mathrm{mem}}$ is reached. ", "page_idx": 4}, {"type": "text", "text": "The activation patterns of some neurons in the network show high variance, even on non-memorized prompts. Such neurons can also be memorization neurons, but due to their high activation variance, they might not be detected by our OOD approach based solely on the $z$ -scores. Therefore, we also add the top- $k$ neurons of each layer with the highest absolute activation on the memorized prompt $y$ to our current candidate set to account for such high-variance neurons. We start by setting $k=0$ and increase $k$ at each iteration by one if the memorization score is still above the threshold $\\tau_{\\mathrm{mem}}$ . We detail our initial selection process in Alg. 3 in Appx. D.3. All neurons identified by our OOD approach and the neurons with the $k$ highest activations are then collected in the neuron set $S_{\\mathrm{initial}}$ . Since not all neurons in set $S_{\\mathrm{initial}}$ might be memorization neurons, we refine this set in the next step. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3 Refinement of the Candidate Set ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this step, we take the set of identified neurons $S_{\\mathrm{refined}}=S_{\\mathrm{initial}}$ and remove the neurons that are actually not responsible for memorization. To speed up this process, we first group the identified neurons layer-wise, leading to the neuron set $S_{\\mathrm{refined}}^{l}$ for layer $l$ . We iterate over the individual layers $l\\in\\{1,\\ldots,L\\}$ and re-activate all identified neurons $S_{\\mathrm{refined}}^{l}$ from a single layer $l$ while keeping the identified neurons in the remaining layers deactivated. ", "page_idx": 5}, {"type": "text", "text": "We then compute the SSIM-based memorization score and check if it is still below the threshold $\\tau_{\\mathrm{mem}}$ . If the memorization score does not increase above the threshold $\\tau_{\\mathrm{mem}}$ , we consider the candidate neurons $S_{\\mathrm{refined}}^{l}$ of layer $l$ as not memorizing and remove them from our set of neurons $S_{\\mathrm{refined}}$ . After iterating over all layers, the set $S_{\\mathrm{refined}}$ only contains neurons from layers that substantially influence the memorization score. ", "page_idx": 5}, {"type": "text", "text": "Next, we individually check each remaining neuron in the set $S_{\\mathrm{refined}}$ by re-activating this particular neuron while keeping all other neurons in the set $S_{\\mathrm{refined}}$ deactivated. Again, if the memorization score computed on the remaining deactivated neurons does not exceed the memorization threshold $\\tau_{\\mathrm{mem}}$ , we remove this neuron from the set $S_{\\mathrm{refined}}$ . After iterating over all neurons in $S_{\\mathrm{refined}}$ , we consider the remaining neurons as memorizing and denote the final set of memorization neurons as $S_{\\mathrm{final}}$ . We detail this refinement approach in Alg. 4 in Appx. D.4. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now empirically evaluate NEMO\u2019s localization in text-to-image DMs. ", "page_idx": 5}, {"type": "text", "text": "Models and Datasets: We follow current research on memorization in DMs [46, 31] and investigate memorization in Stable Diffusion v1.4 [32]. Our set of memorized prompts consists of 500 LAION prompts [35] provided by Wen et al. [46]. We analyzed the prompts using the Self-Supervised Descriptor (SSCD) score [28], a model designed to detect and quantify copying in DMs. The lower the score, the less similar the contents in the image pairs. Additionally, we split the dataset into verbatim(VM) and template-memorized (TM) samples to enable a more detailed analysis of results. The hyperparameter selection and experimental conduction are independent of the type of memorization. If not further specified, we used the same hyperparameters for all the experiments in the paper. ", "page_idx": 5}, {"type": "text", "text": "Images generated by VM prompts match the training image exactly, i.e., pixel-wise, independent of the chosen seed. TM prompts, on the other hand, reproduce the general composition of the training image while having some non-semantic variations at fixed image positions. Details about the analysis and the annotation can be found in Appx. C.1. ", "page_idx": 5}, {"type": "text", "text": "Other publicly available models, like Stable Diffusion v2 and Deep Floyd [38], are trained on more carefully curated and deduplicated datasets. We thoroughly checked for memorized prompts using the tools by Webster [45] and our SSIM-based memorization score but could not identify any properly memorized prompts. This result aligns with related research on memorization in DMs [46, 31]. ", "page_idx": 5}, {"type": "text", "text": "Metrics: We split our metrics into memorization, diversity, and quality metrics. The memorization metrics measure the degree of memorization still present in the generated images. We generate ten images for each memorized prompt with activated/deactivated memorization neurons and measure the cosine similarities between image pairs using SSCD embeddings to quantify the memorization. We denote this metric by $\\mathrm{SSCD}_{\\mathrm{Gen}}$ . Since the generated images without deactivated neurons also differ in their degree of memorization from the original training images, we additionally measure the degree of memorization towards the original training images and denote this metric as $\\mathrm{SSCD_{\\mathrm{Orig}}}$ . Higher SSCD scores indicate a higher degree of memorization. ", "page_idx": 5}, {"type": "text", "text": "Our diversity metric assesses the variety of images generated for the same memorized prompt with different seeds. Diversity is usually low for memorized samples, and generated images almost always depict the same image. Deactivating memorization neurons increases the diversity in the generations. ", "page_idx": 5}, {"type": "table", "img_path": "YAEKMFZyJm/tmp/6ac98f2c9d8a8c71e62b521cc30eb50106c82b77c0cc8f1a086ee35abfc14209.jpg", "table_caption": ["Table 1: Impact of Deactivating the Memorization Neurons. Keeping all neurons active (1st row) and randomly deactivating neurons (3rd row) has no impact on memorization. However, deactivating the memorization neurons located by NEMO (8th row) successfully mitigates memorization, increases diversity, and maintains prompt alignment. These results are comparable to the gradient-based mitigation strategies adjusting the prompt embeddings (2nd row) and the attention scaling (4th row). Adding random tokens also reduces memorization. However, for 1 or 4 tokens, the memorization, as quantified by the SSCD scores, is still higher than with deactivated memorization neurons. Adding 10 random tokens leads to comparable mitigation but also reduces the prompt alignment score. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We quantify the sample diversity by computing the pairwise cosine similarity between the SSCD embeddings of different images generated for the same prompt. We refer to this metric as $D_{\\mathrm{SSCD}}$ , for which lower values indicate more image diversity. ", "page_idx": 6}, {"type": "text", "text": "To assess the overall image quality of a DM with activated/deactivated neurons, we compute the Fr\u00e9chet Inception Distance (FID) [18], the CLIP-FID [21], and the Kernel Inception Distance (KID) [3] on COCO [22] prompts. All quality computations follow Parmar et al. [26] to avoid biased results. Additionally, we compute the similarities between the generated images and the input prompts using CLIP scores [29] to ensure the alignment $A_{\\mathrm{CLIP}}$ between generated images and their prompts. The higher the alignment, the better the generated images represent the concepts described in the prompt. ", "page_idx": 6}, {"type": "text", "text": "Importantly, we use different seeds for detecting memorization neurons with NEMO and the metric computations to avoid undesired biases during the evaluation due to seed overftiting. We always state each metric\u2019s median value and absolute deviation across ten seeds, except the quality metrics (FID, CLIP-FID, KID, and $A_{\\mathrm{CLIP}})$ ), for which we used five different seeds. ", "page_idx": 6}, {"type": "text", "text": "Memorization Threshold: We set the memorization score threshold to $\\tau_{\\mathrm{mem}}\\,=\\,0.428$ , which corresponds to the mean plus one standard deviation of the pairwise SSIM score between initial noise differences measured on a holdout dataset of 50,000 LAION [35] prompts. ", "page_idx": 6}, {"type": "text", "text": "Baselines: As a baseline, we repeated the image generations five times but replaced the deactivated memorization neurons with random neurons from the same layer. We also generated images using the inference mitigation strategy proposed by Wen et al. [46], which performs a gradient-based adjustment of the text embeddings. Importantly, gradient-based mitigation strategies are memoryintensive, particularly for larger batch sizes. NEMO, however, computes no gradients, which enables the approach to also work on machines with limited computing resources. Additionally, we also selected the attention scaling method by Ren et al. [31] and the addition of random tokens, proposed by Somepalli et al. [36], as baselines. ", "page_idx": 6}, {"type": "text", "text": "4.1 Localizing Memorization Down To Individual Neurons ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We begin by demonstrating the effectiveness of our memorization localization method. Tab. 1 presents the quantitative results for images generated with the identified memorization neurons deactivated. NEMO detected a median of 4 and 21 memorization neurons for VM and TM prompts, respectively. For VM prompts, deactivating these memorization neurons significantly decreases memorization, as reflected by low memorization metrics $\\mathrm{SSCD_{\\mathrm{Orig}}}$ and $\\mathrm{SSCD}_{\\mathrm{Gen}}$ , while increasing the image diversity in terms of pairwise similarity $D_{\\mathrm{SSCD}}$ . However, the $\\mathrm{SSCD_{\\mathrm{Orig}}}$ does not change noticeably for TM prompts. ", "page_idx": 6}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/aaa7d1fb5850067ba0393684416b5fe7b90d0fd3c7680b7e7c7c1dfe3bcd6af6.jpg", "img_caption": ["Figure 3: Impact of Deactivating Memorization Neurons. The top row shows images generated with memorized prompts, closely replicating the training images. The bottom row demonstrates that deactivating memorization neurons increases diversity and mitigates memorization. Notably, only a few neurons (counts indicated by digits in the boxes) are responsible for these memorizations. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/12fdc8ecbe843df75f46340930f198deed6a6f7f23162b2bed31580dd371768e.jpg", "img_caption": ["(a) Memorization neurons per prompt. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/e47fd405b9da9d95408b97b88eefee96d87fd5547a69987480169d305d714245.jpg", "img_caption": ["Figure 4: Distribution of Memorization Neurons. (a) shows the number of prompts that are memorized by a fixed number of neurons, e.g., the verbatim memorization of 28 prompts is located in single neurons. (b) depicts the average number of memorization neurons per layer and prompt. ", "(b) Memorization neurons per layer. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "This behavior results from the fact that TM prompts typically memorize specific parts of the original training image, such as objects or compositions, rendering the $\\mathrm{SSCD_{\\mathrm{Orig}}}$ metric less informative. In contrast, the $\\mathrm{SSCD}_{\\mathrm{Gen}}$ score, which compares similarities between images generated with and without the deactivated neurons, provides a more accurate measure. This score highlights that deactivating the identified neurons effectively alters the images and mitigates memorization. Importantly, the imageprompt alignment $A_{\\mathrm{CLIP}}$ remains constant in all cases, indicating that deactivating memorization neurons does not result in misguided image generations. We visualize examples of deactivating memorization neurons to avoid data replication and increase diversity in Fig. 3. ", "page_idx": 7}, {"type": "text", "text": "Comparing the results of deactivating the neurons identified by NEMO with those obtained from randomly deactivated neurons highlights that only a specific subset of neurons is actually responsible for memorizing a prompt. While deactivating the identified memorization neurons significantly impacts both memorization and the diversity of the generated images, randomly deactivating neurons has no noticeable effect. ", "page_idx": 7}, {"type": "text", "text": "Moreover, the mitigation effect of deactivating memorization neurons is comparable to the stateof-the-art method of adjusting the prompt embeddings [46]. Yet, adjusting the prompt embeddings requires gradient computations for each seed and prompt, which are time- and memory-expensive, especially with large batch sizes. In contrast, once the memorization neurons are identified using our gradient-free NEMO, no additional computations are needed during image generations, thus adding no overhead to the generation process. ", "page_idx": 7}, {"type": "text", "text": "4.2 Analyzing the Distribution of Memorization Neurons ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we analyze the distribution of the memorization neurons. Fig. 4a shows the total number of neurons responsible for memorizing specific prompts. Typically, a small set of neurons is responsible for verbatim memorization. For instance, $28~\\mathrm{VM}$ prompts from our dataset are memorized by a single neuron. Additionally, five or fewer neurons replicate two-thirds of VM images, indicating that verbatim memorization can often be precisely localized within the model. Template memorization can also frequently be pinpointed to a small set of neurons, with about $30\\%$ of TV replication triggered by five or fewer neurons. ", "page_idx": 7}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/7d7c4be910ea17465c9ee2ab052a7ead69f480501f621156e8de6f05b8879d34.jpg", "img_caption": ["(a) Assessing image quality using FID and KID. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/c46ec16c7a3db4fa85817ebae28a95d22317139ef496d8640e5068a46515b28e.jpg", "img_caption": ["(b) Scaling the activations of memorization neurons. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Image Quality and Sensitivity to Scaling Factor. (a) assesses the generated images\u2019 quality when blocking an increasing number of neurons. As can be seen, the FID and KID values vary only slightly, indicating that blocking neurons identified by NEMO does not negatively affect image generation quality. Gray lines indicate the baseline without any neurons blocked. (b) investigates the effect of scaling the memorization neurons\u2019 activations by a scaling factor instead of deactivating them (scaling by zero). Whereas positively scaling memorization neuron activations only slightly reduces memorization, negative scaling reduces the memorization not any further. ", "page_idx": 8}, {"type": "text", "text": "However, approximately one-third of TM prompts are distributed across 50 or more neurons. We hypothesize that this broader distribution results from the higher variation in generated images for TM prompts, where memorization spread across multiple neurons leads to increased image diversity. In contrast, VM prompts, often memorized by a small group of neurons, consistently produce the same image without variation. More detailed plots of the identified neurons can be found in Appx. C.2. ", "page_idx": 8}, {"type": "text", "text": "Interestingly, we identify two neurons in the first cross-attention value layer responsible for the verbatim memorization of multiple prompts. Neuron $\\#25$ in this layer is associated with depicting people, while neuron $\\#221$ is responsible for memorizing multiple podcast covers. Together, these neurons account for memorizing $17\\%$ of our dataset\u2019s VM prompts. Similarly, neurons $\\#507$ and #517 in the third value layer are responsible for multiple TM prompts describing iPhone cases. The impact of deactivating these neurons on the image generation of memorized prompts is visualized in Appx. C.5. We also plot the distribution of the average layer-wise number of memorization neurons per prompt in Fig. 4b. Neurons responsible for VM prompts are primarily located in the value mappings of the first cross-attention layers within the U-Net\u2019s down-blocks (each block contains two cross-attention layers). A similar pattern appears for TM prompts, although value layers located deeper in the U-Net seem to play a more crucial role for TM prompts than for VM prompts. ", "page_idx": 8}, {"type": "text", "text": "4.3 Memorization Neurons Hardly Influence Non-Memorized Prompts ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Until now, our focus has been on the impact of deactivating memorization neurons on memorized prompts. In this part, we investigate how these neurons influence non-memorized prompts and the overall image quality of the DM. To assess their impact, we deactivate varying numbers of memorization neurons, ordered by their frequency of occurrence as identified in our experiments, and compute the FID and KID scores on the COCO dataset. We also repeat the generations by deactivating the same number of randomly selected neurons that are not among the identified memorization neurons. As shown in Fig. 5a, there is no significant degradation in the image quality when blocking either the random neurons or the memorization neurons, even with up to 750 blocked neurons. This finding underscores the potential for pruning memorization neurons in DMs without compromising the overall image quality. The plot for the CLIP-FID metric and more detailed plots of the other two metrics, as well as an additional experiment measuring the disentanglement of the neurons in the value layers, can be found in Appx. C.3. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study and Sensitivity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further analyze the impact of each component of NEMO and its sensitivity to hyperparameter selection. We discuss the most crucial insights here, with the complete study included in Appx. C.7. First, we evaluate the impact of different memorization thresholds $\\theta_{\\mathrm{{mem}}}$ . Lowering this threshold slightly increases the number of identified neurons but has a negligible effect on performance metrics. ", "page_idx": 8}, {"type": "text", "text": "Selecting this threshold based on statistics computed on a holdout set provides a simple yet effective way for hyperparameter selection. ", "page_idx": 9}, {"type": "text", "text": "Additionally, we compare the results of using both stages of NEMO versus a setting where we only perform the initial candidate selection, skipping the refinement process. Although memorization is successfully mitigated by deactivating the initially selected neurons, the number of identified memorization neurons increases substantially, resulting in a median of 26.5 neurons $+22.5$ neurons) for VM prompts and 674.5 neurons ( $+653.5$ neurons) for TM prompts. This highlights the importance of the refinement stage in reducing the number of neurons necessary to mitigate memorization efficiently. To further test whether the assumptions about the statistics of memorization neurons hold, we applied NEMO to a set of 500 non-memorized prompts not used to calibrate our thresholds. As we further show, NEMO does not identify any neurons for most of the non-memorized prompts, underscoring the validity of our assumptions. ", "page_idx": 9}, {"type": "text", "text": "We also compared the effect of completely deactivating the identified memorization neurons to down-scaling their activations by a fixed factor. The SSCD scores in Fig. 5b, computed for different scaling factors, demonstrate that memorization is not fully mitigated when using a positive scaling factor. Conversely, negative scaling factors do not provide any additional mitigation compared to our default setting of deactivating the neurons (i.e., using a factor of zero). ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Outlook ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "DMs have rapidly become a cornerstone of computer vision. Yet, problems like memorization of training samples can lead to undesired replication of potentially sensitive or copyrighted training images. Previous research has primarily focused on identifying memorized prompts and proposing mitigation strategies by adjusting the DM\u2019s input. However, there has been a lack of understanding regarding the precise location of memorization within the model. ", "page_idx": 9}, {"type": "text", "text": "Our research provides novel insights into the memorization mechanisms in text-to-image DMs. Unlike previous studies that focused on identifying memorized prompts, our approach, NEMO, is the first to localize memorization within the model and pinpoint individual neurons responsible for it. Traditional pruning methods [11] are orthogonal to our approach by pruning only structures to reduce the total parameter count of the model. Our memorization localization algorithm enables model providers to prune these memorization neurons, effectively mitigating memorization permanently without additional model training, which can be costly in terms of data and resources. Our mitigation can be executed without compromising the model\u2019s overall performance or the quality of the generated images, allowing model providers to deploy the resulting models without additional safeguards to prevent memorization. ", "page_idx": 9}, {"type": "text", "text": "There are several directions to expand and build upon our method for detecting neurons responsible for memorization. One intriguing avenue is to investigate whether an adjusted version of NEMO can detect concept neurons [23]. These neurons are not responsible for memorizing a certain prompt but for generating a particular concept. Such an approach could enable model providers and users to perform knowledge editing [15] and remove undesired concepts like violence and nudity. Another exciting application for NEMO is in large language models, also known for memorizing training samples [5]. Identifying neurons responsible for memorizing text from the training data could lead to new mitigation strategies. ", "page_idx": 9}, {"type": "text", "text": "Additionally, our insights could be interesting for developing new pruning algorithms for DMs to reduce the number of parameters while eliminating unintended memorization. As demonstrated in our experiments, pruning memorization neurons does not significantly impact the model\u2019s overall performance, which is crucial for effective pruning strategies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work has been financially supported by the German Research Center for Artificial Intelligence (DFKI) project \u201cSAINT\u201d. The project also received funding from the Initiative and Networking Fund of the Helmholtz Association in the framework of the Helmholtz AI project call under the name \u201cPAFMIM\u201d with funding number ZT-I-PF-5-227. Responsibility for the content of this publication lies with the author. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Devansh Arpit, Stanis\u0142aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning, pages 233\u2013242. PMLR, 2017.   \n[2] Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of example difficulty. Advances in Neural Information Processing Systems, 34:10876\u201310889, 2021.   \n[3] Mikolaj Binkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD gans. In International Conference on Learning Representations (ICLR), 2018.   \n[4] Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19), pages 267\u2013284, 2019.   \n[5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650, 2021.   \n[6] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[7] Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer. The privacy onion effect: Memorization is relative. Advances in Neural Information Processing Systems, 35:13263\u201313276, 2022.   \n[8] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 5253\u20135270, 2023.   \n[9] Satrajit Chatterjee. Learning and memorization. In International conference on machine learning, pages 755\u2013763. PMLR, 2018.   \n[10] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In Winter Conference on Applications of Computer Vision (WACV), pages 5331\u20135341, 2024.   \n[11] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[12] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954\u2013959, 2020.   \n[13] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems, 33: 2881\u20132891, 2020.   \n[14] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322\u20131333, 2015.   \n[15] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in diffusion models. In Winter Conference on Applications of Computer Vision (WACV), pages 5099\u20135108, 2024.   \n[16] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On Memorization in Diffusion Models. arXiv preprint, arXiv:2310.02664, 2023.   \n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel CohenOr. Prompt-to-prompt image editing with cross-attention control. In Eleventh International Conference on Learning Representations (ICLR), 2023.   \n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Conference on Neural Information Processing Systems (NeurIPS), page 6629\u20136640, 2017.   \n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Conference on Neural Information Processing Systems (NeurIPS), pages 6840\u20136851, 2020.   \n[20] Erwin Kreyszig, Herbert Kreyszig, and E. J. Norminton. Advanced Engineering Mathematics. Wiley, tenth edition, 2011.   \n[21] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in fr\u00e9chet inception distance. In Eleventh International Conference on Learning Representations (ICLR), 2023.   \n[22] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740\u2013755, 2014.   \n[23] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept Neurons in Diffusion Models for Customized Generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning (ICML), 2023.   \n[24] Pratyush Maini, Michael Curtis Mozer, Hanie Sedghi, Zachary Chase Lipton, J Zico Kolter, and Chiyuan Zhang. Can neural network memorization be localized? In International Conference on Machine Learning, pages 23536\u201323557. PMLR, 2023.   \n[25] Casey Meehan, Florian Bordes, Pascal Vincent, Kamalika Chaudhuri, and Chuan Guo. Do ssl models have d\u00e9j\u00e0 vu? a case of unintended memorization in self-supervised learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 11400\u201311410, 2022.   \n[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Conference on Neural Information Processing Systems (NeurIPS), pages 8024\u20138035, 2019.   \n[28] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised descriptor for image copy detection. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 14512\u201314522, 2022.   \n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 8748\u20138763, 2021.   \n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint, arXiv:2204.06125, 2022.   \n[31] Jie Ren, Yaxin Li, Shenglai Zeng, Han Xu, Lingjuan Lyu, Yue Xing, and Jiliang Tang. Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention. arXiv preprint, arXiv:2403.11052, 2024.   \n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022.   \n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), pages 234\u2013241, 2015.   \n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Conference on Neural Information Processing Systems (NeurIPS), 2022.   \n[35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Conference on Neural Information Processing Systems (NeurIPS), 2022.   \n[36] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. Advances in Neural Information Processing Systems, 36:47783\u201347803, 2023.   \n[37] Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models. In Conference on Neural Information Processing Systems (NeurIPS), pages 12438\u2013 12448, 2020.   \n[38] StabilityAI. DeepFloyd IF: a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. https://www.deepfloyd.ai/ deepfloyd-if, 2023. Retrieved on 2023-11-08.   \n[39] Cory Stephenson, Abhinav Ganesh, Yue Hui, Hanlin Tang, SueYeon Chung, et al. On the geometry of generalization and memorization in deep neural networks. In International Conference on Learning Representations, 2020.   \n[40] Lukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, and Kristian Kersting. Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. In International Conference on Machine Learning (ICML), pages 20522\u201320545, 2022.   \n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Conference on Neural Information Processing Systems (NeurIPS), pages 5998\u20136008, 2017.   \n[42] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. In Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[43] Wenhao Wang, Muhammad Ahmad Kaleem, Adam Dziedzic, Michael Backes, Nicolas Papernot, and Franziska Boenisch. Memorization in Self-Supervised Learning Improves Downstream Generalization. In International Conference on Learning Representations (ICLR), 2024.   \n[44] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13:600\u2013 612, 2004.   \n[45] Ryan Webster. A reproducible extraction of training images from diffusion models. arXiv preprint, arXiv:2305.08694, 2023.   \n[46] Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, Explaining, and Mitigating Memorization in Diffusion Models. In International Conference on Learning Representations (ICLR), 2024.   \n[47] TaeHo Yoon, Joo Young Choi, Sehyun Kwon, and Ernest K. Ryu. Diffusion Probabilistic Models Generalize when They Fail to Memorize. In ICML Workshop on Structured Probabilistic Inference & Generative Modeling, 2023.   \n[48] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2016.   \n[49] Xiaosen Zheng, Tianyu Pang, Chao Du, Jing Jiang, and Min Lin. Intriguing properties of data attribution on diffusion models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=vKViCoKGcB. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Highly Memorized Prompts: For certain memorized prompts, our method identifies a set of over 100 neurons. Upon closer examination, we found that in these cases, memorization is distributed across many neurons in various layers, rather than being concentrated in a small group. Even deactivating a substantial number of neurons in the network does not eliminate memorization for these prompts. However, such instances, primarily TM prompts, were rare in our experiments. We provide examples of highly memorized prompts in Appx. C.4. ", "page_idx": 14}, {"type": "text", "text": "Mitigating Template Memorization Is Harder Than Verbatim Memorization: Deactivating memorization neurons is effective for mitigating verbatim memorization, and often requires only a small number of neurons to be deactivated. In contrast, mitigating template memorization often requires deactivating more neurons, and even then, complete removal of memorization is not always possible in some rare cases. This difficulty arises because template memorization frequently results in diverse generations, with the memorized content corresponding to only certain aspects of the image. Distinguishing between these memorized parts and the remaining image parts is not always clearly achievable with our SSIM-based memorization strength. However, we emphasize that for the majority of prompts, deactivating the identified memorization neurons successfully removes template memorization. ", "page_idx": 14}, {"type": "text", "text": "Runtime: For most memorized prompts, NEMO detects the memorization neurons in a few seconds. We timed the runtime of NEMO and found that on average NEMO can identify memorization neurons for verbatim memorized prompts within 14.2 seconds, while for template memorized prompts memorization neurons are identified in 43.7 seconds on average. As discussed in the previous paragraph, mitigating and localizing memorization for template memorized prompts is harder. We suspect this is one reason why NEMO\u2019s runtime is slightly longer for template memorized prompts. To get further insight into how long the runtime of each part of our algorithm is, we also timed the runtime for the algorithms D1-D4 separately. The results can be seen in Tab. 2. ", "page_idx": 14}, {"type": "text", "text": "Table 2: NEMO can localize the neurons responsible for memorization efficiently. The average runtime (in seconds) for Alg. 1 and Alg. 2 is below one second, while the runtime for Alg. 3 is below 10 seconds. While the runtime for Alg. 4 is longer than for the other parts of NEMO, the runtime is still only 45 seconds for TM. Alg. 4 has the longest runtime for TM, since the initial candidate set is larger than for verbatim memorized samples. ", "page_idx": 14}, {"type": "table", "img_path": "YAEKMFZyJm/tmp/54bb42e04736a9a2dbbadb088b684cbfe3e0bf4b52055fdca21dc25e4ab1dfa8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Hard- and Software Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We performed all our experiments on NVIDIA DGX machines running NVIDIA DGX Server Version 5.2.0 and Ubuntu $20.04.5\\,\\mathrm{LTS}$ . The machines have 1.5 TB (machine 1) and 2 TB (machine 2) of RAM and contain NVIDIA Tesla V100 SXM3 32GB (machine 1) NVIDIA A100-SXM4-40GB (machine 2) GPUs with Intel(R) Xeon(R) Platinum 8174 (machine 1) and AMD EPYC 7742 64-core (machine 2) CPUs. We further relied on CUDA 12.1, Python 3.10.13, and PyTorch 2.2.2 with Torchvision 0.17.2 [27] for our experiments. All investigated models are publicly available on Hugging Face. For access, we used the Hugging Face diffusers library with version 0.27.1. ", "page_idx": 14}, {"type": "text", "text": "We provide a Dockerflie with our code to make reproducing our results easier. In addition, all training and configuration files are available to reproduce the results stated in this paper. ", "page_idx": 14}, {"type": "text", "text": "B.2 Model and Dataset Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Experiments were mainly conducted on Stable Diffusion v1-4 [32], publicly available at https: //huggingface.co/CompVis/stable-diffusion-v1-4. All details regarding the data, training parameters, limitations, and environmental impact are available at that URL. The model is available under the CreativeML OpenRAIL M license. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "The investigated prompts originate from the LAION2B-en [35] dataset used to train the DM. The set of memorized prompts is taken from Wen et al. $[46]^{2}$ , who collected the prompts by using the tool of Webster [45]. The LAION dataset itself is licensed under the Creative Common CC-BY 4.0. The images of the LAION dataset might be under copyright, so we do not include them in our code base; we only provide URLs to retrieve the images directly from their source. ", "page_idx": 15}, {"type": "text", "text": "B.3 Experimental Details and Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All images depicted throughout the paper are generated with fixed seeds, 50 inference steps, and a classifier-free guidance strength of 7 using the default DDIM scheduler. Notably, the seeds used for generating the images and computing the evaluation metrics differ from those used for our detection method NEMO to avoid seed overfitting. ", "page_idx": 15}, {"type": "text", "text": "During detection with NEMO, no classifier-free guidance was used, which speeds up the detection since only a single forward pass per seed is required, compared to an additional forward pass on the null-text embedding with classifier-free guidance. We always used ten different seeds for each prompt. The threshold on the SSIM memorization score was set to $\\tau_{\\mathrm{mem}}=0.428$ during the experiments in the main paper. We vary this threshold and analyze its impact in our sensitivity analysis in Appx. C.7. ", "page_idx": 15}, {"type": "text", "text": "We run all experiments \u2013 detection with NEMO and the generations for the metric computations \u2013 with half-precision (float16) to reduce the memory consumption and speed up the computations. ", "page_idx": 15}, {"type": "text", "text": "B.4 Structural Similarity Index Measure (SSIM) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We quantify the memorization strength during our experiments using the structural similarity index measure commonly used in the computer vision domain to assess the similarity between image pairs. Our memorization score is computed as follows: Let $x_{T}\\sim N(\\mathbf{0},\\mathbf{I})$ be the initial noisy image. Let $\\epsilon_{\\theta}(x_{T},T,y)$ further denote the initial noise prediction without any scaling by the scheduler. We found that the normalized difference between the initial noise and the first noise prediction $\\delta=\\epsilon_{\\theta}(x_{T},T,y)-x_{T}$ for memorized prompts is substantially more consistent for different seeds than for non-memorized prompts. To detect the grade of memorization, we, therefore, use the similarity between the noise differences $\\delta^{(i)}$ and $\\bar{\\delta^{(j)}}$ generated with seeds $i$ and $j$ as a proxy. We measure the similarity with the common structural similarity index measure (SSIM) [44]. The $\\mathrm{SSIM}\\in[0,1]$ between two noise differences ${\\delta^{(i)}}$ and $\\delta^{(j)}$ is defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{SSIM}(\\delta^{(i)},\\delta^{(j)})=\\frac{(2\\mu_{i}\\mu_{j}+C_{1})(2\\sigma_{i j}+C_{2})}{(\\mu_{i}^{2}+\\mu_{j}^{2}+C_{1})(\\sigma_{i}^{2}+\\sigma_{y}^{2}+C_{2})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The parameters $\\mu_{i},\\,\\mu_{j}$ and $\\sigma_{i}^{2},\\;\\sigma_{j}^{2}$ denote the mean and variance of the pixels in $\\delta^{(i)}$ an $\\delta^{(j)}$ , respectively. Likewise, $\\sigma_{i j}$ denotes the covariance between the images. Following the original paper, $C_{1}$ and $C_{2}$ are small constants added for numerical stability. ", "page_idx": 15}, {"type": "text", "text": "A higher SSIM indicates higher similarity between the noise differences, reflecting a higher degree of memorization. Notably, the SSIM computation only requires a single denoising step per seed, which makes the process fast. ", "page_idx": 15}, {"type": "text", "text": "Indeed, the SSIM score itself can be used to detect memorization. We visualized the different SSIM score distributions for non-memorized and memorized prompts in Fig. 2a. To underline this observation with quantitative metrics, we ran additional experiments to explore the detection capabilities of our SSIM-based method. We measured the efficiency of the SSIM score for memorization detection on our dataset of memorized and non-memorized prompts. Without extensive hyperparameter tuning, this detection method achieves an AUROC of $98\\%$ , an accuracy of $94.2\\%$ (using a naive threshold of 0.5 on the SSIM similarity), and a $\\mathrm{TPR}@1\\%\\mathrm{FPR}$ of $87.6\\%$ . Since the amount of memorization of template memorized prompts varies significantly, we repeated the computation for detecting the verbatim memorized prompts. Here, the SSIM approach even achieves an AUROC of $99.64\\bar{\\%}$ , an accuracy of $97.39\\%$ (with a threshold of 0.5), and a TPR $.\\mathcal{O}1\\%\\mathrm{FPR}$ of $98.21\\%$ . These results indicate that the SSIM score can also be used to detect memorization reliably in the first place. However, detection is not the focus of the paper but the localization on the neuron level. ", "page_idx": 15}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Distinguishing Between Different Types of Memorization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Webster [45] distinguished between verbatim and template memorized prompts. Verbatim memorized prompts lead to the exact reconstruction of training samples, while template-memorized prompts replicate the composition and structure of the training image. To provide a more fine-grained analysis of our results, we classify the prompts in our dataset into these two categories. We distinguish between both types by computing the SSCD [28] scores between the original training image and ten generations with different seeds. We then classify a prompt as verbatim memorized if the maximum SSCD score computed as cosine similarity exceeds a threshold of 0.7 and as template memorized otherwise. Fig. 6 plots the distribution of SSCD scores for both datasets. We manually inspected and classified the prompts where the original training image is no longer available (16 out of 500). ", "page_idx": 16}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/b46be380932829642483b4e7f23f78adf542758d8c2b9083b6802aee10a888a4.jpg", "img_caption": ["Figure 6: We compare the maximum SSCD score between ten generated images and the original training sample. We categorize the memorized prompts into verbatim memorized if the SSCD score exceeds 0.7 and into template memorized prompts otherwise. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.2 Detailed Analysis of the Distribution of Memorization Neurons ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/e61cab4d9cc325f83336065789a0e8f6a7ea7b26a30bd2d093fbc4d88f882cf2.jpg", "img_caption": ["(a) Number of initial neurons per prompt. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/185493632469aeac88a7dd9439bca913e2da7ca9914699b3ec77061176f7d813.jpg", "img_caption": ["(b) Number of neurons per prompt after refinement. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 7: Distribution of Memorization Neurons. We show the number of prompts that are memorized by a fixed number of neurons. (a) plots the number of neurons found in the initial neuron selection. (b) shows the number of neurons after refinement. As we can observe, the refinement step drastically reduces the number of found memorization neurons for both the template and the verbatim memorized prompts. ", "page_idx": 16}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/623dc3fa9b553e043817594ed2e598234ea07b40ff414472813524f512ab1347.jpg", "img_caption": ["(c) Assessing image quality using KID. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 8: Image Quality Does Not Degrade When Deactivating Memorization Neurons. Depicted are the generated images\u2019 FID, CLIP-FID (FID calculated using a CLIP model), and KID scores when blocking an increasing number of neurons. For all three metrics, smaller values are better. As can be seen, the FID, KID, and CLIP-FID values vary only slightly, indicating that blocking neurons identified by NEMO does not negatively affect image generation quality. Gray lines indicate baselines without any neurons blocked. We repeated the experiment with five different seeds. Depicted are the mean values and the standard deviation. ", "page_idx": 17}, {"type": "text", "text": "Table 3: We measured the disentanglement of neurons for deactivating top-k memorization neurons or randomly selected neurons. More specifically, we collected and compared the attention layer outputs for 500 non-memorized prompts with and without neurons deactivated and 3 seeds. We then computed the average cosine similarity between corresponding outputs. The high similarities show that blocking neurons only has a negligible impact on the outputs of the attention layer, suggesting that other neurons can substitute the functionality of blocked neurons on non-memorized prompts. ", "page_idx": 17}, {"type": "table", "img_path": "YAEKMFZyJm/tmp/d1e2417e75f7f2dec43241247b3fbdf66a8214b7b1e1b0334d27df4e700269c8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We ran additional experiments to analyze the impact and effects of deactivating neurons in the value layers. The result can be seen in Tab. 3. We deactivated a varying number of neurons, either randomly selected or the top memorization neurons, and measured the similarity between the outputs of the cross-attention blocks with deactivated neurons and all neurons activated. Our results, stated in the rebuttal PDF, show that even deactivating a large number of neurons only impacts the attention outputs marginally. We, therefore, conclude that other neurons can replace the functionality of specific neurons on non-memorized prompts and that neurons in the value layers act rather independently. Yet, for neurons memorizing specific prompts, this memorizing functionality is not replaced by other neurons, explaining the mitigation effect of deactivating these neurons. ", "page_idx": 17}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/de1fc1a78fabebcc2bcf592ea877b2dfc54ed6e193b02e58c9f0a5f34026f1f8.jpg", "img_caption": ["Figure 9: Memorization of highly memorized prompts is distributed across many neurons in various layers, rather than concentrated in a small group of neurons. We show examples of such prompts and the impact of deactivating the identified memorization neurons. The number of memorization neurons in each case is stated behind each prompt. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "To illustrate that some single neurons are responsible for memorizing multiple training prompts, we generated images with and without these specific neurons deactivated. In Fig. 10 and Fig. 11, we only deactivate a single neuron each in the first value layer, whereas in Fig. 12, we deactivate two neurons in the third value layer. ", "page_idx": 19}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/002ab8ab14eed6c33840f064cca3148ecf5d18e08fdb409fb8e6f0d404d1e3ad.jpg", "img_caption": ["Figure 10: We found that neuron $\\#25$ in the first cross-attention layer\u2019s value mapping is responsible for verbatim memorization of multiple prompts, all associated with depicting people. Deactivating this single neuron mitigates the memorization and introduces diversity into the images (right columns) compared to images generated with all neurons active (left columns). Generations were conducted with seeds different from the seeds used for the neuron localization process. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/d202acbb547625bbafc1aeecba19656f300afabcf69060fd6a12aa110386aeaf.jpg", "img_caption": ["Figure 11: We found that neuron $\\#25$ in the first cross-attention layer\u2019s value mapping is responsible for verbatim memorization of multiple prompts, all associated with depicting people. Deactivating this single neuron mitigates the memorization and introduces diversity into the images (right columns) compared to images generated with all neurons active (left columns). Generations were conducted with seeds different from the seeds used for the neuron localization process. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/78e12a299b07e60757e9471a461ce5336a01156ce373fc6e0c60b06b2f8a8a42.jpg", "img_caption": ["Figure 12: We found that neurons #507 and #517 in the third cross-attention layer\u2019s value mapping is responsible for template memorization of multiple prompts describing iPhone cases. Deactivating these two neurons mitigates the template memorization and introduces diversity into the images (right columns) compared to images generated with all neurons active (left columns). Image generations were conducted with a fixed seed different from the seed used for the neuron localization process. ", "\"\"Bliss of Being\"\" Zen Meditation iPhone 6 6s Plus case - iPhone 6 6s Plus Tough Case - 1\" "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/251f10009c383d57048c825fa90872d19a922f13634063dde4fbf2ec843540bc.jpg", "img_caption": ["Figure 13: Visualizations for generated images and the noise differences between the predicted noise after the first denoising step and the initial Gaussian noise. Noise differences for memorized prompts (left column) have low diversity and are already structurally similar to the final image. The noise differences for non-memorized prompts (right column) show no clear structure and differ substantially for different noise initializations. Deactivating the memorization neurons detected with NEMO (middle column) removes the structure in the initial noise differences and adds more diversity, leading to diverse image generations. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.7 Ablation Study and Sensitivity Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We conduct an ablation study to investigate the impact of the individual components of NEMO. Additionally, we analyze the sensitivity of the memorization threshold $\\tau_{\\mathrm{mem}}$ and explore alternatives to deactivating neurons by setting their activations to zero. The results for the various settings are presented in Tab. 4. ", "page_idx": 23}, {"type": "text", "text": "The first two rows provide evaluation results for the model with all neurons active and with randomly deactivated neurons. Both scenarios exhibit strong memorization. The third row shows the results of blocking the neurons identified as memorizing by NEMO, using the threshold $\\tau_{\\mathrm{mem}}=0.428$ as specified in the main paper. This threshold corresponds to the mean SSIM memorization plus one standard deviation, calculated on a holdout set of 50,000 non-memorized LAION prompts. In row four, we repeat this setting using classifier-free guidance (CFG) with a guidance strength of 7.0, as opposed to our default setting without CFG. Detection with CFG further reduces the number of detected memorization neurons. However, the SSCD scores indicate slightly increased memorization after deactivating the identified neurons. Additionally, running NEMO with CFG doubles the number of forward passes in the U-Net since a separate noise prediction is generated for each initial seed. ", "page_idx": 23}, {"type": "text", "text": "Rows five to seven display the results of varying the memorization threshold $\\tau_{\\mathrm{mem}}$ . Specifically, we adjust the threshold to one standard deviation below the mean SSIM score, to the mean, and two standard deviations above the mean. A lower threshold identifies more neurons. However, for lower thresholds, the metrics are comparable to those obtained with our default threshold value $(\\tau_{\\mathrm{mem}}\\,=\\,0.428)$ . Increasing the threshold reduces the number of identified neurons but slightly increases memorization, as measured by the SSCD scores. Thus, a trade-off exists between reducing the number of identified memorization neurons and their memorization mitigation effect. In addition, we provide heat maps to directly compare the impact of different thresholds $\\tau_{\\mathrm{mem}}$ used during the initial selection and the refinement step in Fig. 14. Fig. 15 further compares the SSCD scores for varying the threshold values. ", "page_idx": 23}, {"type": "text", "text": "Rows eight and nine examine the impact of removing the refinement step or incorporating no neurons with top- $.k$ activations during the initial selection. As anticipated, without refinement, the number of identified neurons increases substantially. Despite this, the various metrics remain comparable to those obtained after the refinement step, even with more neurons deactivated. This underscores the robustness of image generations against pruning out-of-distribution (OOD) neurons. Without the top-k selection, NEMO identifies a larger set of neurons. However, deactivating these neurons does not mitigate memorization as effectively as with a top-k search. Notably, for template verbatim prompts, the ${\\mathrm{SSCD}}_{\\mathrm{Gen}}$ is substantially higher without top-k, indicating increased memorization. ", "page_idx": 23}, {"type": "text", "text": "In the remaining rows, we explore the impact of scaling the activations of memorization neurons instead of deactivating them. With negative scaling factors, the results are comparable to those of completely deactivating the neurons. For positive scaling factors, however, the generated images demonstrate higher degrees of memorization, with a scaling factor of 0.75 having almost no influence on memorization. ", "page_idx": 23}, {"type": "text", "text": "We also apply NEMO to a set of 500 LAION non-memorized prompts, different from the 50,000 prompts used to set the memorization threshold. For 442 of these prompts, NEMO identified no memorization neurons, which is to be expected since these prompts show no memorization behavior. For the remaining prompts, a median of $62\\pm27$ neurons was found. ", "page_idx": 23}, {"type": "table", "img_path": "YAEKMFZyJm/tmp/6bf42f7f4623588ecd23ab7e7d7f17a0ef2341684fc0af2b0a9e049383af5736.jpg", "table_caption": ["Table 4: Quantitative Results of Our Ablation Study and Sensitivity Analysis. "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/2eef479069e14d0bde752b4d56633f8fb359a2b28b63a4d3146a467f4c7e1ac3.jpg", "img_caption": ["(a) Number of initial neurons found for VM. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/574b8967b4514ffd3d40b52fa1f61952017e6b89af17d65a76a2b4d8647c2277.jpg", "img_caption": ["(b) Number of initial neurons found for TM. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/40f72376439f5fed58d463481f5b08b871894258e836b8c783a25a14ee3fff72.jpg", "img_caption": ["(c) Number of refined neurons found for VM. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/c502d5a855803c9e02a2b9888ad6797facfc1a08bf3b961700d945fecec9def5.jpg", "img_caption": ["(d) Number of refined neurons found for TM. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 14: Number of neurons found with different initial and refinement thresholds $\\tau_{\\mathbf{mem}}$ . The left plots show the results for verbatim memorization prompts, while the right plots show the results for template memorization prompts. The refinement step significantly reduces the number of identified neurons across all threshold combinations. Notably, using 0.428 for both the initial selection and refinement thresholds results in the smallest set of identified neurons. ", "page_idx": 25}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/3e5eaf2dba0f001212e0889002f00fcab77cfc9e0d81c67cbb20063db60c273f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "", "img_caption": ["Figure 15: SSCD memorization scores with different initial and refinement thresholds $\\tau_{\\mathbf{mem}}$ . The left plots show the results for verbatim memorization prompts, while the right plots show the results for template memorization prompts. The value of the thresholds does not seem to have a high impact on the memorization scores. Since higher thresholds identify much less memorization neurons, choosing a threshold of $\\tau_{\\mathrm{mem}}=0.428$ is a valid choice. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "C.8 Ablation of Individual Key and Value Layers ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "During our experiments in the main paper, we limit our search with NEMO for memorization neurons to cross-attention value layers in the down- and mid-blocks of the U-Net. To motivate this decision, we perform an analysis of the influence of neurons in the individual key and value layers of different cross-attention blocks. Let us first recall the computation performed in attention layers: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right)\\cdot V\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The computed key and query matrices $K$ and $Q$ are used to calculate the attention scores, i.e., the weighting of the components in the value matrix $V$ . In the cross-attention layers, the query matrix $Q$ is computed by linearly mapping the current feature maps from the previous U-Net layer. Therefore, information from the textual guidance is only indirectly contained, i.e., of earlier layers or the U-Nets input feature map after the first denoising step. Therefore, we can exclude neurons in the query mapping layers since we aim to identify neurons directly responsible for memorization. The neurons in the key mapping layers directly process the text embeddings to compute the attention scores. However, strong interdependencies exist between the activations of different neurons through the nature of the softmax function. The impact each neuron\u2019s activation has on the computed attention score also depends on the activations of all other neurons from the same layer. Removing a single neuron, i.e., setting its activation to zero, does not necessarily imply substantial changes in the attention scores and the corresponding weighting of features from the value mapping layer. ", "page_idx": 27}, {"type": "text", "text": "The value mapping layers, however, also directly process the text embeddings, but there is no direct interdependence between the activations of different neurons. Consequently, setting the activations of individual neurons in value layers to zero directly blocks the information flow from the text embeddings. We hypothesize that the neurons in the value layers are mainly responsible for memorizing the text embeddings of specific prompts. ", "page_idx": 27}, {"type": "text", "text": "We evaluate this assumption by taking a set of 100 memorized prompts, generating ten samples for each prompt, and comparing the impact of removing neurons from different layers. More specifically, we remove all activations of individual key and value mapping layers, i.e., setting the output vectors of these layers to zero while keeping all other parts of the model untouched. We then compare the generated images with removed activations to the original training images. Fig. 16 plots the resulting SSCD similarity scores for deactivating individual value (top row) and key (bottom row) layers. We distinguish between verbatim (left column) and template (right column) prompts. The plots show the maximum and median SSCD scores and the deviations for the median scores. We decided not to plot deviations for the maximum score to improve readability. However, deviations are comparable to the median scores. Baselines computed without any deactivated neurons are plotted as dashed lines. ", "page_idx": 27}, {"type": "text", "text": "Stable Diffusion contains six cross-attention layers in the down-blocks, one in the mid-block, and nine in the up-blocks. The vertical lines indicate the separation between the different blocks. For the value layers, the layers with indexes 1 (down-lock) and 7 (mid-block) have the highest impact, whereas layers later in the network hardly change the SSCD scores. Also, the effect of the remaining layers in the down-blocks is small on their own. However, we expect there to be entwined effects between deactivating neurons in different layers, which is why we also searched for memorization neurons in these down-block layers. ", "page_idx": 27}, {"type": "text", "text": "For the key layers, particularly layers 4 and 6 in the down-blocks have the strongest impact on the generated images. However, removing these layers often produces images that no longer align with the concepts in the prompt or degrades the image quality, both leading to lower SSCD scores. We quantify this behavior by computing the alignments between the generated images and the corresponding input prompts in Fig. 17. While deactivating individual value layers only slightly decreases the alignment scores, deactivating some key layers substantially reduces the alignment. To further illustrate this fact, we plot some of the generated images for deactivating individual value layers in Fig. 18 and for key layers in Fig. 19. ", "page_idx": 27}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/38a83c4c1b108ec4ce79426ee1a259567948c20c86136450e24f5f265822dcf7.jpg", "img_caption": ["Figure 16: SSCD similarity scores between memorized generations and the corresponding training samples. Scores are computed for 100 prompts and ten different seeds per generation. We then take the maximum and median scores of each prompt. During the generation, we deactivated individual value and key layers of the cross-attention blocks in the network. A lower SSCD score indicates a lower similarity between generated and training images. Dashed lines denote the median and the maximum SSCD baselines for images generated without deactivating any neurons. For verbatim memorized prompts, both baselines are close, which is why we only plot the median SSCD baseline. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/42e50c5d146a0f2e42d7ff12515e8164a4247ce64329febac6cf57ba3859a3fc.jpg", "img_caption": ["Figure 17: CLIP alignment scores between memorized generations and the corresponding input prompt. Scores are computed for 100 prompts and ten different seeds per generation. We then take the median alignment scores of each prompt. During the generation, we deactivated individual value and key layers of the cross-attention blocks in the network. A higher alignment score indicates a better representation of the prompt concepts in the generated images. Dashed lines denote the median alignment scores for images generated without deactivating any neurons. For both types of memorization, deactivating value layers decreases the alignment only slightly, whereas deactivating some key layers substantially reduces the alignment. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/cd9759c97bcd06fc0cdacfd473056cb277589dd37ec7f1248016d3210427784c.jpg", "img_caption": ["Figure 18: Images generated with memorized prompts with deactivated individual value layers. Whereas the standard row shows generations with keeping all neurons active, the following rows depict results for deactivating all neurons in a specific value layer. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/e5188e6e919fefc9577607f632b82fa126e9940f034cd86bc80044717d957041.jpg", "img_caption": ["Figure 19: Images generated with memorized prompts with deactivated individual key layers. Whereas the standard row shows generations with keeping all neurons active, the following rows depict results for deactivating all neurons in a specific key layer. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "C.9 Ablation of Individual Convolutional Layers ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We also experimented with deactivating neurons in other U-Net layers, including both convolutional and fully connected layers, but we did not find any indicators of memorization in these units. Even when deactivating numerous neurons in the convolutional and fully connected layers of the U-Net, the memorized training images were still faithfully reproduced. However, the quality of the images degraded, particularly when deactivating neurons in early layers, which are responsible for defining the image structure. We showcase in Fig. 20 various examples of memorized images generated with $50\\%$ deactivated neurons in the convolutional layers to illustrate that these neurons have no noticeable impact on the memorization behavior. However, deactivating too many neurons in early layers can negatively affect the overall generation process. So, to conclude, deactivating other neurons in the U-Net didn\u2019t seem to impact memorization, which is why our choice to focus on the value layers seems reasonable. ", "page_idx": 31}, {"type": "image", "img_path": "YAEKMFZyJm/tmp/d464cf13041da0d59c0993f0433abfa4e31227a6de7544dde00b6d0a877d3db6.jpg", "img_caption": ["Figure 20: For each convolutional layer in the U-Net\u2019s down blocks, we randomly deactivated $50\\%$ of the neurons. The results demonstrate that blocking neurons in the convolutional layers does not mitigate memorization. Instead, deactivating the neurons reduces the quality of the generated images and, in some cases, causes the entire generation process to collapse, especially when neurons in the early layers are deactivated. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "D Algorithmic Description of NeMo ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "D.1 Computing Noise Differences ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Alg. 1 defines our algorithm to compute the differences between the initial noise samples and the noise predicted during the first denoising step. The resulting noise differences are used to compute our SSIM-based memorization score during the initial neuron selection and the refinement step. We compute the noise differences always for $n=10$ different seeds to avoid undesired biases due to the random sampling process. We further remove noise differences from the set, which have low similarity with other noise differences. By this step, we remove noise differences for seeds that do not lead to memorization and might mislead the algorithm. ", "page_idx": 32}, {"type": "text", "text": "Algorithm 1 Compute Noise Differences ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Input: Prompt embedding $y$ Neuron set Sneurons Noise predictor $\\epsilon_{\\theta}$ Memorization threshold (SSIM) \u03c4mem ", "page_idx": 32}, {"type": "text", "text": "\u25b7Text prompt (embedding) \u25b7Set of neurons to deactivate \u25b7Diffusion model \u25b7Target memorization score ", "page_idx": 32}, {"type": "text", "text": "Output: Noise differences $\\Delta$ ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Set $\\Delta$ as empty list $\\tilde{\\epsilon}_{\\theta}\\gets$ deactivate_neurons $\\left(\\epsilon_{\\theta},S_{\\mathrm{Neurons}}\\right)$ ", "page_idx": 32}, {"type": "text", "text": "$\\triangleright$ Initialize list of noise differences \u25b7Set activations of neurons in $S_{\\mathrm{neurons}}$ to zero // Compute noise differences for each random seed ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "for $i=1,\\ldots,10\\,\\mathbf{d}$ o set_seed(i) sample $x_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ $x_{T-1}\\leftarrow\\tilde{\\epsilon}_{\\theta}(x_{T},T,y)$ \u03b4 \u2190xT \u22121 \u2212xT $\\begin{array}{r}{\\delta\\leftarrow\\frac{\\delta-\\operatorname*{min}(\\delta)}{\\operatorname*{max}(\\delta)-\\operatorname*{min}(\\delta)}}\\end{array}$ append $\\delta$ to $\\Delta$   \nend for ", "page_idx": 32}, {"type": "text", "text": "$\\triangleright$ Iterate over 10 seeds $\\triangleright$ Set random seed to $i$ \u25b7Randomly initialize noise image $\\triangleright$ Compute noise prediction \u25b7Compute noise difference \u25b7Normalize differences by min-max scaling $\\triangleright$ Add current noise difference to list // Remove noise differences not leading to memorization for $\\delta\\in\\Delta$ do $\\bar{\\Delta}\\leftarrow\\Delta\\setminus\\delta$ $d\\leftarrow$ compute_memorization $(\\delta,\\bar{\\Delta})$ if max(d) < \u03c4mem then \u2206\u2190\u2206\\ \u03b4 end if ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "$\\triangleright$ Iterate over noise differences \u25b7Get set of noise differences without $\\delta$   \n\u25b7Compute pairwise memorization scores (SSIM)   \n$\\triangleright$ Highest memorization score is below threshold \u25b7Remove noise difference from set ", "page_idx": 32}, {"type": "text", "text": "\u25b7Return list of noise differences ", "page_idx": 32}, {"type": "text", "text": "D.2 Detecting Neurons with Out-of-Distribution Activations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Alg. 2 describes our method to detect neurons with out-of-distribution (OOD) activations. Our method detects OOD neurons based on their activation distance for a memorized prompt to a neuron\u2019s mean activation computed on a hold-out dataset of non-memorized prompts. In addition, we also add the $k$ neurons with the highest absolute activations within each layer to the set. ", "page_idx": 33}, {"type": "text", "text": "Algorithm 2 Get OOD Neurons ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Input: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Prompt embedding $y$ $\\triangleright$ Text prompt (embedding) Activation threshold $\\theta_{\\mathrm{act}}$ \u25b7Threshold for the OOD detection Top $k$ \u25b7Value of top-k detection Activation mean $\\mu$ and standard deviation $\\sigma$ $\\triangleright$ Activation statistics of hold-out dataset Output: Set of neurons with OOD activations $S_{\\mathrm{initial}}$ $\\underbrace{S_{\\mathrm{activations}}\\leftarrow\\mathrm{collect\\_activations}(y)}_{\\mathrm{{initial}}}$ \u25b7Collect activations on prompt $\\triangleright$ Initialize empty neuron set // Check each neuron in each layer for OOD activation for $l\\in\\{1,\\ldots,L\\}$ do $\\triangleright$ Iterate over all layers for $i\\in\\{1,\\ldots,N\\}$ do $\\triangleright$ Iterate over all $N$ neurons in layer $l$ zl(y) = ali(y)l\u2212\u00b5li $\\triangleright$ Compute z-score for current neuron if $z_{i}^{l}(y)>\\theta_{\\mathrm{act}}$ then \u25b7Activation above OOD threshold $S_{\\mathrm{initial}}\\leftarrow S_{\\mathrm{initial}}\\cup\\{\\mathrm{neuron}_{i}^{l}\\}$ $\\triangleright$ Add OOD neuron to set end if end for // Add $k$ neurons of layer $l$ with the highest absolute activations to the candidate set $S_{\\mathrm{topk}}\\leftarrow$ top_k_activations $(S_{\\mathrm{activations}},l,k)$ $\\triangleright$ Get neurons with highest absolute activations $S_{\\mathrm{initial}}\\leftarrow S_{\\mathrm{initial}}\\cup S_{\\mathrm{topk}}$ $\\triangleright$ Add top-k neurons to set end for ", "page_idx": 33}, {"type": "text", "text": "return Sinitial ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u25b7Return set with OOD neurons ", "page_idx": 33}, {"type": "text", "text": "D.3 Selecting Initial Candidates of Memorization Neurons ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Alg. 3 defines our algorithm to compute the initial set of memorization neurons. The resulting initial set of selected memorization neurons is then refined in a second step, shown in Alg. 4. ", "page_idx": 34}, {"type": "text", "text": "Algorithm 3 Initial Neuron Selection ", "text_level": 1, "page_idx": 34}, {"type": "table", "img_path": "YAEKMFZyJm/tmp/be2e0e80f93a4ab9186b3e8d4fc1e9d006d5556c4a28a1de90dba1fd496f2f02.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "D.4 Neuron Selection Refinement ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Alg. 4 defines our algorithm to refine the set of candidate neurons identified from NEMO\u2019s initial selection step. ", "page_idx": 35}, {"type": "text", "text": "Algorithm 4 Neuron Selection Refinement ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "YAEKMFZyJm/tmp/a4053e5e41905796885ad2ad7c0c65931d23f22dc02c4e56362086f975a42822.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All claims and promises made in the abstract and introduction are accurately reflected in the paper by a clear formal description (Section 3) and an extensive experimental evaluation (Section 4). ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We discuss limitations in Appx. A. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not include any theoretical proof but offers a practical method (Section 3) to detect and localize memorization in diffusion models. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We describe all settings, models, datasets, and hyperparameters used during our experiments in Section 4 and in Appx. B. Appx. D further provides detailed algorithmic descriptions for all steps of our method. Therefore, we expect reproduction to be possible regardless of whether the code is provided. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The code is publicly available, including all datasets required for reproduction. The README flie within the code base explains each step, including the dataset, models, localization algorithm, and the computation of metrics, in detail. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We stated all experimental details in Section 4 and provide additional details in Appx. B. We also state all used hyperparameters within our code base. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We provide results computed on a sufficiently large dataset, including the variability for all experimental results in the paper (Section 4). All experiments were performed with ten seeds to provide statistically stable results. We also provide an ablation and sensitivity analysis to add further insights into the robustness and stability of our method and the results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We state all hardware and software details in Appx. B.1. We also provide our complete source code and a docker image for reproduction. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Our research identifies neurons in text-to-image systems that are responsible for memorizing training samples. We strongly believe that our research has no negative societal impact or potentially harmful consequences. In stark contrast, our research tackles the memorization problem in diffusion models and might support the development of models and methods to mitigate such undesired memorization. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We discuss our method\u2019s broader impact and advantages in localizing neurons responsible for memorization in diffusion models (Section 5). Localizing such memorization neurons is an effective way of mitigating memorization. We do not expect any potential malicious or unintended uses of our method. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do not develop or release any kind of new model or dataset. Instead, we only investigate existing, publicly available models and datasets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We explicitly state the origin of data and models in the paper and properly credit the source of these assets (Appx. B.2). The authors wrote the code for conducting the experiments by themselves. Any code snippets taken from external sources are explicitly marked. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our paper releases no new assets. All models and datasets used in our experiments are publicly available (Appx. B.2). ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We did not conduct any kind of crowdsourcing experiments or research with human subjects at any point. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We did not conduct any kind of crowdsourcing experiments or research with human subjects at any point. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 41}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]