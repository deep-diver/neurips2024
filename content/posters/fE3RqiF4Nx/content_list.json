[{"type": "text", "text": "Metric Flow Matching for Smooth Interpolations on the Data Manifold ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kacper Kapus\u00b4niak1,\u2217 Peter Potaptchik1, Teodora $\\mathbf{Reu^{1}}$ , Leo Zhang1, Alexander Tong2,3, Michael Bronstein1,4, Avishek Joey Bose1,2, Francesco Di Giovanni1 1University of Oxford, 2Mila, 3Universit\u00e9 de Montr\u00e9al, 4AITHYRA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matching objectives underpin the success of modern generative models and rely on constructing conditional paths that transform a source distribution into a target distribution. Despite being a fundamental building block, conditional paths have been designed principally under the assumption of Euclidean geometry, resulting in straight interpolations. However, this can be particularly restrictive for tasks such as trajectory inference, where straight paths might lie outside the data manifold, thus failing to capture the underlying dynamics giving rise to the observed marginals. In this paper, we propose METRIC FLOW MATCHING (MFM), a novel simulationfree framework for conditional flow matching where interpolants are approximate geodesics learned by minimizing the kinetic energy of a data-induced Riemannian metric. This way, the generative model matches vector fields on the data manifold, which corresponds to lower uncertainty and more meaningful interpolations. We prescribe general metrics to instantiate MFM, independent of the task, and test it on a suite of challenging problems including LiDAR navigation, unpaired image translation, and modeling cellular dynamics. We observe that MFM outperforms the Euclidean baselines, particularly achieving SOTA on single-cell trajectory prediction. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A central task in many natural and scientific domains entails the inference of system dynamics of an underlying (physical) process from noisy measurements. A core challenge, in these application domains such as biomedical ones\u2014e.g. tracking health metrics [Oeppen and Vaupel, 2002] or diseases [Hay et al., 2021]\u2014is that one typically lacks access to entire time-trajectories and can only leverage cross-sectional samples. An even more poignant example is the case of single-cell RNA sequencing [Macosko et al., 2015, Klein et al., 2015], where measurements are sparse and static, due to the procedure being expensive and destructive. Consequently, the nature of these tasks demands the design of frameworks capable of reconstructing the temporal dynamics of a system (e.g. cells) from observed time marginals that contain finite samples. This overarching problem specification is referred to as trajectory inference [Hashimoto et al., 2016, Lavenant et al., 2021]. ", "page_idx": 0}, {"type": "text", "text": "To address this challenge, we rely on matching objectives, a powerful generative modeling paradigm encompassing successful approaches including diffusion models [Sohl-Dickstein et al., 2015, Song et al., 2021], flow matching [Lipman et al., 2023, Liu et al., 2022, Albergo and Vanden-Eijnden, 2022], and finding a Schr\u00f6dinger Bridge [Schr\u00f6dinger, 1932, L\u00e9onard, 2013]. Specifically, to reconstruct the unknown dynamics $t\\mapsto p_{t}^{*}$ between observed time marginals $p_{\\mathrm{0}}$ and $p_{1}$ , we leverage Conditional Flow Matching (CFM) [Tong et al., 2023b], a simulation-free framework which constructs a probability path $p_{t}$ through interpolants $x_{t}$ connecting samples of $p_{0}$ to samples of $p_{1}$ . In general, $x_{t}$ is designed under the assumption of Euclidean geometry, resulting in straight trajectories. However, in light of the widely accepted \u201cmanifold hypothesis\u201d [Tenenbaum et al., 2000, Belkin and Niyogi, 2003], the target time-evolving density $p_{t}^{*}$ is supported on a curved low-dimensional manifold $\\mathcal{M}\\subset\\mathbb{R}^{d}.$ \u2014a condition satisfied by cells in the space of gene expressions [Moon et al., 2018]. As such, straight interpolants stray away from the data manifold $\\mathcal{M}$ , leading to reconstructions $p_{t}$ that fail to model the nonlinear dynamics generated by the underlying process. ", "page_idx": 0}, {"type": "image", "img_path": "fE3RqiF4Nx/tmp/eb8f95581587adee1cf9708309d60f54530d1059ac831ec7959ef1c756e14333.jpg", "img_caption": ["Figure 1: In orange and violet the source and target distributions. On the left, straight interpolations vs interpolations following a data-dependent Riemannian metric. On the right, densities of reconstructed marginals at time $\\scriptstyle t\\;=\\;{\\frac{\\tau}{2}}$ , using Conditional Flow Matching and METRIC FLOW MATCHING (MFM), respectively. MFM provides a more meaningful reconstruction supported on the data manifold. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Present work. We aim to design interpolants $x_{t}$ that stay on the data manifold $\\mathcal{M}$ associated with the underlying dynamics. Nonetheless, parameterizing the lower-dimensional manifold $\\mathcal{M}$ is prone to instabilities [Loaiza-Ganem et al., 2022] and may require multiple coordinate systems [Salmona et al., 2022]. Accordingly, we adopt the \u201cmetric learning\u201d approach [Xing et al., 2002, Hauberg et al., 2012], where we still work in the ambient space $\\mathbb{R}^{d}$ , but equip it with a data-dependent Riemannian metric $g$ whose shortest-paths (geodesics) stay close to the data points, and hence to $\\mathcal{M}$ [Arvanitidis et al., 2021]. We introduce METRIC FLOW MATCHING (MFM), a simulation-free generalization of CFM where interpolants $x_{t}$ are learned by minimizing a geodesic loss that penalizes the velocity measured by the metric $g$ . As a result, $x_{t}$ approximates the geodesics of $g$ and hence tends towards the data, leading to the evaluation of the matching objective in regions of lower uncertainty. Therefore, the resulting probability path $p_{t}$ is supported on the data manifold for all $t\\in[0,1]$ , giving rise to a more natural reconstruction of the underlying dynamics in the trajectory inference task, as depicted in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "(1) We prove that given a dataset $\\mathcal{D}\\subset\\mathbb{R}^{d}$ , one can always construct a metric $g$ on $\\mathbb{R}^{d}$ such that the geodesics connecting $x_{0}$ sampled from $p_{0}$ to $x_{1}$ sampled from $p_{1}$ , always lie close to $\\mathcal{D}$ (\u00a73.1).   \n(2) We propose METRIC FLOW MATCHING, a novel framework generalizing CFM to the Riemannian manifold associated with any data-dependent metric $g$ . In MFM, before training the matching objective, we learn interpolants that stay close to the data by minimizing a cost induced by $g$ (\u00a73). MFM is simulation-free and stays relevant when geodesics lack closed form and hence Riemannian Flow Matching [Chen and Lipman, 2023] is not easily applicable (\u00a73.2).   \n(3) We prescribe a universal way of designing a data-dependent metric, independent of the specifics of the task, which enforces interpolants $x_{t}$ to stay supported on the data manifold (\u00a74.1). Through the proposed metric, $x_{t}$ depends on the entire data manifold and not just the endpoints $x_{0}$ and $x_{1}$ sampled from the marginals. By accounting for the Riemannian geometry induced by the data, MFM generalizes existing approaches that construct $x_{t}$ by minimizing energies (\u00a74.2).   \n(4) We propose OT-MFM, an instance of MFM that relies on Optimal Transport to draw samples from the marginals (\u00a74). Empirically, we show that OT-MFM attains SOTA results for reconstructing single-cell dynamics (\u00a75). Additionally, we validate the versatility of the framework through tasks such as 3D navigation with LiDAR point clouds and unpaired translation of images. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We review Conditional Flow Matching, which forms the basis of METRIC FLOW MATCHING $\\S3$ .   \nNext, we recall basic notions of Riemannian geometry, with emphasis on constructing geodesics. ", "page_idx": 1}, {"type": "text", "text": "Notation and convention. We let $\\mathbb{R}^{d}$ be the ambient space where data points are embedded. A random variable $x$ with a distribution $p$ is denoted as $x\\sim p(x)$ . A function $\\varphi$ depending on time $t$ , space $x$ and learnable parameters $\\theta$ , is denoted by $\\varphi_{t,\\theta}(x)$ , and its time derivative by $\\dot{\\varphi}_{t,\\theta}$ . We also let $\\delta_{x_{0}}(x)$ be the Dirac function centered at $x_{0}$ and assume that all distributions are absolutely continuous, which allows us to use densities. We denote the space of symmetric, positive definite $d\\times d$ matrices as ${\\mathrm{SPD}}(d)$ , and let $\\mathbf{G}(x)\\in\\mathrm{SPD}(d)$ be the coordinate representation of a Riemannian metric at some point $x$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Conditional Flow Matching. We consider a source distribution $p_{0}$ and a target distribution $p_{1}$ defined on $\\mathbb{R}^{d}$ . We are interested in finding a map $f$ that pushes forward $p_{0}$ to $p_{1}$ , i.e. $f_{\\#}p_{0}=p_{1}$ . In line with Continuous Normalizing Flows [Chen et al., 2018], we look for a map of the form $f=\\psi_{1}$ , where the time-dependent diffeomorphism $\\psi_{t}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is the flow generated by a vector field $u_{t}$ , i.e. $d\\psi_{t}(x)/d t=u_{t}(\\psi_{t}(x))$ , with $\\psi_{0}(x)=x$ , for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . If we define the density path $p_{t}=[\\psi_{t}]_{\\#}p_{0}$ , then $p_{t}$ and $u_{t}$ satisfy the continuity equation and we say that $p_{t}$ is generated by $u_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "If density path $p_{t}$ and vector field $u_{t}$ are known, we could regress a vector field $v_{t,\\theta}$ , modeled as a neural network, to $u_{t}$ by minimizing $\\mathcal{L}_{\\mathrm{FM}}(\\theta)=\\mathbb{E}_{t,p_{t}(x)}\\|v_{t,\\theta}(x)-u_{t}(x)\\|^{2}$ . Since $p_{t}$ and $u_{t}$ are typically intractable though, Conditional Flow Matching (CFM) [Lipman et al., 2023, Albergo and Vanden-Eijnden, 2022, Liu et al., 2022] simplifies the problem by assuming that $p_{t}$ is a mixture of conditional paths: $\\begin{array}{r}{p_{t}(x)=\\int p_{t}(x|z)q(z)d z}\\end{array}$ , where ${\\boldsymbol{z}}=\\left(x_{0},x_{1}\\right)$ is sampled from a joint distribution $q$ with marginals $p_{0}$ and $p_{1}$ , and $p_{t}(x|z)$ satisfy $p_{0}(x|z)\\,\\approx\\,\\dot{\\delta}_{x_{0}}(x)$ and $p_{1}(x|z)\\;\\approx\\;\\delta_{x_{1}}(x)$ . If $\\psi_{t}(x|x_{0},x_{1})$ denotes the flow generating $p_{t}(x|z)$ , then the CFM objective is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CFM}}(\\theta)=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left\\lVert v_{t,\\theta}(x_{t})-\\dot{x}_{t}\\right\\rVert^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{t}:=\\psi_{t}(x_{0}|x_{0},x_{1})$ are the interpolants from $x_{0}$ to $x_{1}$ . Since ${\\mathcal{L}}_{\\mathrm{FM}}$ and ${\\mathcal{L}}_{\\mathrm{CFM}}$ have same gradients [Lipman et al., 2023, Tong et al., 2023b], we can use the tractable conditional paths to learn $v_{\\theta}$ . As in $\\S3$ we design $x_{t}$ to approximate geodesics, we review key notions from Riemannian geometry. ", "page_idx": 2}, {"type": "text", "text": "Riemannian manifolds. A Riemannian manifold $(\\mathcal{M},g)$ is a smooth orientable manifold $\\mathcal{M}$ equipped with a smooth map $g$ assigning to each point $x$ an inner product $\\langle\\cdot,\\cdot\\rangle_{g}$ defined on the tangent space of $\\mathcal{M}$ at $x$ . We let $\\bar{\\mathbf{G}(x)}\\in\\mathrm{SPD}(d)$ be the matrix representing $g$ in coordinates, at any point $x$ , with $d$ the dimension of $\\mathcal{M}$ . Integration is taken with respect to the volume form $d\\mathrm{vol}$ (see Appendix $\\S\\mathrm{A}$ for details). A continuous, positive function $p$ is then a probability density on $(\\mathcal{M},g)$ , i.e. $p\\in\\mathbb{P}(\\mathcal{M})$ , if $\\begin{array}{r}{\\int p(x)d\\mathrm{vol}(x)=1}\\end{array}$ . Naturally, it is possible to define curves $\\gamma_{t}$ , indexed by time $t$ . A geodesic is then the curve $\\gamma_{t}^{*}$ that minimizes the distance with respect to $g$ . Specifically, the geodesic connecting $x_{0}$ to $x_{1}$ in $\\mathcal{M}$ , can be found by minimizing the length functional: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\gamma_{t}^{*}=\\operatorname*{arg\\,min}_{\\gamma_{t}:\\gamma_{0}=x_{0},\\gamma_{1}=x_{1}}\\int_{0}^{1}\\|\\dot{\\gamma}_{t}\\|_{g(\\gamma_{t})}\\,d t,\\quad\\|\\dot{\\gamma}_{t}\\|_{g(\\gamma_{t})}:=\\sqrt{\\langle\\dot{\\gamma}_{t},\\mathbf{G}(\\gamma_{t})\\dot{\\gamma}_{t}\\rangle},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\dot{\\gamma}_{t}$ is velocity. From eq. (2), we see that geodesics tend towards regions where $\\Vert\\mathbf{G}(x)\\Vert$ is small.   \nIn Euclidean geometry (i.e., $\\mathcal{M}=\\mathbb{R}^{d}$ and $\\mathbf{G}(x)=\\mathbf{I}{\\mathrm{,}}$ , $\\gamma_{t}^{*}$ is a straight line with constant speed. ", "page_idx": 2}, {"type": "text", "text": "3 Metric Flow Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce METRIC FLOW MATCHING (MFM), a new simulation-free framework that generalizes CFM by constructing probability paths supported on the data manifold. MFM learns interpolants $x_{t}$ in eq. (1) whose velocity minimizes a data-dependent Riemannian metric assigning a lower cost to regions with high data concentration. Consequently, the CFM objective is evaluated in areas of low data uncertainty, and the corresponding vector field, $v_{\\theta}$ in eq. (1), learns to pass through these regions. ", "page_idx": 2}, {"type": "text", "text": "We structure this section as follows. In $\\S3.1$ we discuss the trajectory inference problem and how straight interpolants $x_{t}$ in CFM result in undesirable probability paths whose support is not defined on the data manifold. We remedy this problem by choosing to represent the data manifold via a Riemannian metric in $\\mathbb{R}^{d}$ , such that geodesics avoid straying away from the samples in the training set. In $\\S3.2$ we introduce MFM and compare it with Riemannian Flow Matching [Chen and Lipman, 2023]. ", "page_idx": 2}, {"type": "text", "text": "3.1 Metric learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume that $p_{0}$ and $p_{1}$ are empirical distributions and that we have access to a dataset of samples $\\mathcal{D}=\\{x_{i}\\}_{i=1}^{N}$ \u2014in practice $\\mathcal{D}$ is constructed concatenating samples from both the source and target distributions. We are interested in the problem of trajectory inference [Lavenant et al., 2021], where we need to reconstruct an unknown dynamics $t\\mapsto p_{t}^{*}$ , with observed time marginals $p_{0}^{*}=p_{0}$ and $p_{1}^{*}=p_{1}$ \u2014the extension to multiple timepoints is easy. In many realistic settings, including single-cell RNA sequencing [Macosko et al., 2015], time measurements are sparse, and leveraging biases from the data is hence key to achieving a faithful reconstruction. For this reason, we invoke the \u201cmanifold hypothesis\u201d [Bengio et al., 2013], where the data arises from a low-dimensional manifold $\\mathcal{M}\\subset\\mathbb{R}^{d}.$ \u2014 a property satisfied by cells embedded in the space of gene expressions [Moon et al., 2018]: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{supp}(p_{t}^{*}):=\\{x\\in\\mathbb{R}^{d}:p_{t}^{*}(x)>0\\}\\subset\\mathcal{M},\\quad t\\geq0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since any regular time dynamics can be described using the continuity equation generated by some vector field $v_{t}^{*}$ [Ambrosio et al., 2005, Theorem 8.3.1], [Neklyudov et al., 2023a], we rely on CFM, and approximate $p_{t}^{*}$ via the probability path $p_{t}$ associated with $v_{t,\\theta}$ in eq. (1). From eq. (3), it follows that a valid reconstruction entails $\\operatorname{supp}(p_{t})\\subset\\mathcal{M}$ . As the support of $p_{t}$ is determined by the interpolants $x_{t}$ in eq. (1), we need $x_{t}$ to be constrained to stay on $\\mathcal{M}$ . However, in the classical CFM setup, this condition is violated since interpolants are often straight lines, agnostic of the data\u2019s support: $x_{t}=t x_{1}+(1-t)x_{0}$ [Tong et al., 2023b, Shaul et al., 2023], with $x_{0},x_{1}$ sampled from the marginals. In this case, if $p_{t}(x|x_{0},x_{1})\\approx\\delta_{x_{t}}(x)$ , then the support of $p_{t}$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{supp}(p_{t})\\subset\\{y\\in\\mathbb{R}^{d}:\\exists\\ (x_{0},x_{1})\\sim q:\\ y=t x_{1}+(1-t)x_{0}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, the dynamics $t\\mapsto p_{t}^{*}$ is often nonlinear, as for single-cells [Moon et al., 2018], meaning that $\\operatorname{supp}(p_{t})\\ \\mathbb{Z}\\ M$ . Straight interpolants are hence too restrictive and should instead be supported on $\\mathcal{M}$ so to replicate actual trajectories from $x_{0}$ to $x_{1}$ , which are generated by the underlying process. ", "page_idx": 3}, {"type": "text", "text": "Operating on a lower-dimensional $\\mathcal{M}$ is challenging though, since it requires different coordinate systems [Schonsheck et al., 2019, Salmona et al., 2022] and may incur overftiting [Loaiza-Ganem et al., 2022, 2024]. Nonetheless, a key property posited by the \u201cmanifold hypothesis\u201d is that $\\mathcal{M}$ concentrates around the data points $\\mathcal{D}$ [Arvanitidis et al., 2022, Chadebec and Allassonni\u00e8re, 2022]. As such, interpolants $x_{t}$ should remain close to $\\mathcal{D}$ . Therefore, instead of changing the dimension, we design $x_{t}$ to minimize a cost in $\\mathbb{R}^{d}$ that is lower on regions close to $\\mathcal{D}$ . We achieve this following the \u201cmetric learning\u201d approach, [Hauberg et al., 2012] where we equip $\\mathbb{R}^{d}$ with a suitable Riemannian metric $g$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1. A data-dependent metric $g$ on $\\mathbb{R}^{d}$ is a smooth map $g:\\mathbb{R}^{d}\\rightarrow\\operatorname{SPD}(d)$ parameterized by the dataset ${\\mathcal{D}}=\\{x_{i}\\}_{i=1}^{N}\\subset\\mathbb{R}^{d}$ , i.e. $g(x)=\\mathbf{G}(x;D)\\in\\mathrm{SPD}(d),\\forall x\\in\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "We describe a specific metric $g$ in $\\S4$ , and note that $g$ can also enforce constraints from the task $(\\S7)$ . Naturally, if $\\mathbf{G}(x;\\mathcal{D})=\\mathbf{I}$ , we recover the Euclidean metric. We show that we can always construct $g$ so that the geodesics $\\gamma_{t}^{*}$ stay close to the data $\\mathcal{D}$ . For details, we refer to Theorem B.1 in Appendix $\\S B$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Informal). Given a dataset $\\mathcal{D}\\subset\\mathbb{R}^{d}$ , let g be any metric such that: (i) The eigenvalues of $\\mathbf{G}(x;\\mathcal{D})$ do not approach zero when $x$ is distant from $\\mathcal{D}$ ; (ii) $\\|\\mathbf{G}(x;\\mathcal{D})\\|$ is sufficiently small if $x$ is close to $\\mathcal{D}$ . Then for each $(x_{0},x_{1})\\sim q$ , the geodesic $\\gamma_{t}^{*}$ connecting $x_{0}$ to $x_{1}$ stays close to $\\mathcal{D}$ . ", "page_idx": 3}, {"type": "text", "text": "Given $g$ as in the statement, if $x_{t}\\,=\\,\\gamma_{t}^{*}$ and $p_{t}(x|x_{0},x_{1})\\,\\approx\\,\\delta_{x_{t}}(x)$ , then the probability path $p_{t}$ generated by $v_{\\theta}$ in eq. (1) has support near $\\mathcal{D}$ , i.e. $\\operatorname{supp}(p_{t})$ lies close to the data manifold $\\mathcal{M}$ , which is our goal. Unfortunately, for all but the most trivial metrics $g$ on $\\mathbb{R}^{d}$ , it is not possible to obtain closedform expressions for the geodesics $\\gamma_{t}^{*}$ . As such, finding the geodesic $\\gamma_{t}^{*}$ necessitates the expensive simulation of second-order nonlinear Euler-Lagrange equations [Hennig and Hauberg, 2014]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Parameterization and optimization of interpolants ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a metric $g$ on $\\mathbb{R}^{d}$ as in Definition 1 whose geodesics $\\gamma_{t}^{*}$ lie close to $\\mathcal{D}$ as per Proposition 1. We propose a simulation-free approximation to paths $\\gamma_{t}^{*}$ by introducing interpolants of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{t,\\eta}=(1-t)x_{0}+t x_{1}+t(1-t)\\varphi_{t,\\eta}(x_{0},x_{1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta$ are the parameters of a neural network $\\varphi_{t,\\eta}$ acting as a nonlinear \u201ccorrection\u201d for straight interpolants. Note that the boundary conditions are met, i.e. that the path $x_{t,\\eta}$ recovers both $x_{0}$ and $x_{1}$ at times $t=0$ and $t=1$ , respectively. In fact, $x_{t,\\eta}$ reduces to the convex combination between $x_{0}$ and $x_{1}$ if $\\varphi_{t,\\eta}=0$ , meaning that $x_{t,\\eta}$ strictly generalize the straight paths used in [Lipman et al., 2023, Liu et al., 2022]. Towards the goal of learning $\\eta$ so that $x_{t,\\eta}$ approximates the geodesic $\\gamma_{t}^{*}$ , we note that $\\gamma_{t}^{*}$ can be characterized as the path minimizing the convex functional $\\mathcal{E}_{g}$ below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma_{t}^{*}=\\underset{\\gamma_{t}:\\,\\gamma_{0}=x_{0},\\,\\gamma_{1}=x_{1}}{\\arg\\operatorname*{min}}\\mathcal{E}_{g}(\\gamma_{t}),\\quad\\mathcal{E}_{g}(\\gamma_{t}):=\\mathbb{E}_{t}\\left[\\dot{\\gamma}_{t}^{\\top}\\mathbf{G}(\\gamma_{t};\\mathcal{D})\\dot{\\gamma}_{t}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since $\\gamma_{t}^{*}$ minimizes $\\mathcal{E}_{g}$ over all paths connecting $x_{0}$ to $x_{1}$ , and $x_{t,\\eta}$ in eq. (4) satisfies these boundary conditions, we can estimate $\\eta$ by simply minimizing the convex functional $\\mathcal{E}_{g}$ over $x_{t,\\eta}$ , which leads to the following geodesic objective (the training procedure is reported in Algorithm 1): ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Pseudocode for training of geodesic interpolants ", "page_idx": 4}, {"type": "text", "text": "Require: coupling $q$ , initialized network $\\varphi_{t,\\eta}$ , data-dependent metric $\\mathbf{G}(\\cdot;\\mathcal{D})$ 1: while Training do ", "page_idx": 4}, {"type": "text", "text": "return (approximate) geodesic interpolants parametrized by $\\varphi_{t,\\eta}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g}(\\eta):=\\mathbb{E}_{(x_{0},x_{1})\\sim q}\\left[\\mathcal{E}_{g}(x_{t,\\eta})\\right]=\\mathbb{E}_{(x_{0},x_{1})\\sim q,t}\\,\\left[(\\dot{x}_{t,\\eta})^{\\top}\\mathbf{G}(x_{t,\\eta};\\mathcal{D})\\dot{x}_{t,\\eta}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given interpolants that approximate $\\gamma_{t}^{*}$ and hence stay close to the data manifold, we can then rely on the CFM objective in eq. (1) to regress the vector field $v_{\\theta}$ . Since $g$ makes the ambient space $\\mathbb{R}^{d}$ into a Riemannian manifold $(\\bar{\\mathbb{R}}^{d},g)$ , we need to replace the norm $\\lVert\\cdot\\rVert$ in eq. (1) with the one $\\|\\bar{\\cdot}\\|_{g}$ induced by the metric, and rescale the marginals $p_{0},p_{1}$ using the volume form induced by $g$ , so to extend $p_{0},p_{1}$ to densities in $\\mathbb{P}(\\mathbb{R}^{d},g)$ (see Appendix $\\S\\mathrm{A}$ ). Similar arguments work for the joint distribution $q$ . We can finally introduce our framework METRIC FLOW MATCHING that generalizes Conditional Flow Matching (1) to leverage any data-dependent metric $g$ , by using interpolants $x_{t,\\eta}$ (4), whose parameters $\\eta$ are obtained from minimizing the geodesic cost $\\mathcal{E}_{g}$ . The MFM objective can be stated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MFM}}(\\theta)=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left[\\Vert v_{t,\\theta}(x_{t,\\eta^{*}})-\\dot{x}_{t,\\eta^{*}}\\Vert_{g(x_{t,\\eta^{*}})}^{2}\\right],\\quad\\eta^{*}=\\arg\\operatorname*{min}_{\\eta}\\mathcal{L}_{g}(\\eta).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A description of METRIC FLOW MATCHING is given in Algorithm 2. As the interpolants $x_{t,\\eta}$ approximate geodesics of $g$ , in MFM the vector field $v_{t,\\theta}$ is regressed on the data manifold $\\mathcal{M}$ , where the underlying dynamics $p_{t}^{*}$ is supported, resulting in better reconstructions. Crucially, eq. (6) only depends on time derivatives of $x_{t,\\eta}$ . Therefore, MFM avoids simulations and simply requires training an additional (smaller) network $\\varphi_{t,\\eta}$ in eq. (4), which can be done prior to training $v_{t,\\theta}$ . ", "page_idx": 4}, {"type": "text", "text": "MFM versus Riemannian Flow Matching. While CFM has already been extended to Riemannian manifolds in the Riemannian Flow Matching (RFM) framework of Chen and Lipman [2023], MFM crucially differs from RFM in two ways. To begin with, MFM relies on the data or task inducing a Riemannian metric on the ambient space which is then accounted for in the matching objective. This is in sharp contrast to RFM, which instead assumes that the metric of the ambient space is given and is independent of the data points. Secondly, RFM does not incorporate conditional paths that are learned. In fact, in the scenario above where $g$ is a metric whose geodesics $\\gamma_{t}^{*}$ stay close to the data support, adopting RFM would entail replacing the MFM objective ${\\mathcal{L}}_{\\mathrm{MFM}}$ in (7) with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{RFM}}(\\theta)=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left\\lVert v_{t,\\theta}(\\gamma_{t}^{*})-\\dot{\\gamma}_{t}^{*}\\right\\rVert_{g(\\gamma_{t}^{*})}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, as argued above, for almost any metric $g$ on $\\mathbb{R}^{d}$ , geodesics $\\gamma_{t}^{*}$ can only be found via simulations, which in high dimensions inhibits the easy application of RFM. Conversely, MFM designs interpolants that minimize a geodesic cost (6) and hence approximate $\\gamma_{t}^{*}$ without incurring simulations. ", "page_idx": 4}, {"type": "text", "text": "4 Learning Riemannian Metrics in Ambient Space ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we focus on a concrete choice of $g$ , which can easily be used within MFM (\u00a74.1). We also introduce OT-MFM, a variant of MFM that leverages Optimal Transport to find a coupling $q$ between $p_{0}$ and $p_{1}$ . Next, in $\\S4.2$ we discuss how MFM generalizes recent works that find interpolants that minimize energies by accounting for the Riemannian geometry induced by the data. ", "page_idx": 4}, {"type": "text", "text": "4.1 A family of diagonal metrics: LAND and RBF ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider a family of metrics $g_{\\mathrm{LAND}}$ as in Definition 1, independent of specifics of the data type or task. For the ease of exposition, we omit to write the explicit dependence on the dataset $\\mathcal{D}=\\{\\dot{x_{i}}\\}_{i=1}^{N}$ . ", "page_idx": 4}, {"type": "text", "text": "Given $\\varepsilon>0$ , we let $x\\mapsto g_{\\mathrm{LAND}}(x)\\equiv\\mathbf{G}_{\\varepsilon}(x)=(\\mathrm{diag}(\\mathbf{h}(x))+\\varepsilon\\mathbf{I})^{-1}$ be the \u201cLAND\u201d metric, where ", "page_idx": 5}, {"type": "equation", "text": "$$\nh_{\\alpha}(x)=\\sum_{i=1}^{N}(x_{i}^{\\alpha}-x^{\\alpha})^{2}\\exp\\Big(-\\frac{\\|x-x_{i}\\|^{2}}{2\\sigma^{2}}\\Big),\\quad1\\le\\alpha\\le d,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\sigma$ the kernel size. We emphasize that $\\mathbf{G}_{\\varepsilon}(x)$ was introduced by Arvanitidis et al. [2016]\u2014from which we borrow the name\u2014but its algorithmic use in MFM is fundamentally different. In line with Proposition 1, we see that $\\|\\mathbf{G}_{\\varepsilon}(x)\\|$ is larger away from $\\mathcal{D}$ , thus pushing geodesics (2) to stay close to the data support, as desired. While $g_{\\mathrm{LAND}}$ is flexible and directly accounts for all the samples in $\\mathcal{D}$ in high-dimension selecting $\\sigma$ in eq. (9) can be challenging. For these reasons, we follow Arvanitidis et al. [2021] and introduce a variation of $g_{\\mathrm{LAND}}$ of the form $\\mathbf{\\bar{G}}_{\\mathrm{RBF}}(x)=(\\mathrm{diag}(\\tilde{\\mathbf{h}}(x))+\\varepsilon\\mathbf{I})^{-1}$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{h}_{\\alpha}(\\boldsymbol{x})=\\sum_{k=1}^{K}\\omega_{\\alpha,k}(\\boldsymbol{x})\\exp\\Big(-\\frac{\\lambda_{\\alpha,k}}{2}\\|\\boldsymbol{x}-\\boldsymbol{\\hat{x}}_{k}\\|^{2}\\Big),\\quad1\\le\\alpha\\le d,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $K$ the number of clusters with centers $\\hat{x}_{k}$ and $\\lambda_{\\alpha,k}$ the bandwidth of cluster $k$ for channel $\\alpha$ (see Appendix $\\S C$ for details). In particular, $h_{\\alpha}$ is realized via a Radial Basis Function (RBF) network [Que and Belkin, 2016], where $\\omega_{\\alpha,k}>0$ are learned to enforce the behavior $h_{\\alpha}(x_{i})\\approx1$ for each data point $x_{i}$ so that the resulting metric $g_{\\mathrm{RBF}}$ assigns lower cost to regions close to the centers $\\hat{x}_{k}$ . In our experiments, we then rely on $g_{\\mathrm{LAND}}$ in low dimensions, and instead use $g_{\\mathrm{RBF}}$ in high dimensions. We also note that all metrics considered are diagonal, which makes MFM more efficient. Explicitly, given the interpolants in eq. (4), the geometric loss in eq. (6) with respect to $g_{\\mathrm{RBF}}$ can be written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g_{\\mathrm{RBF}}}(\\eta)=\\mathbb{E}_{(x_{0},x_{1})\\sim q}[\\mathcal{E}_{g_{\\mathrm{RBF}}}(x_{t,\\eta})]=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left[\\sum_{\\alpha=1}^{d}\\frac{(\\dot{x}_{t,\\eta})_{\\alpha}^{2}}{\\widetilde{h}_{\\alpha}(x_{t,\\eta})+\\varepsilon}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We see that the loss acts as a geometric regularization, with the velocity $\\dot{x}_{t,\\eta}$ penalized more in regions away from the support of the dataset $\\mathcal{D}$ , i.e. when $\\lambda_{\\alpha,k}\\|x_{t,\\eta}-\\hat{x}_{k}\\|$ is large for all centers $\\hat{x}_{k}$ in eq. (10). ", "page_idx": 5}, {"type": "text", "text": "OT-MFM. In eq. (7), samples $x_{0},x_{1}$ follow a joint distribution $q$ , with marginals $p_{0}$ and $p_{1}$ . Since we are interested in the problem of trajectory inference, with emphasis on single-cell applications where the principle of least action holds [Schiebinger, 2021], we focus on a coupling $q$ that minimizes the distance in probability space between the source and target distributions. Namely, we consider the case where $q$ is the 2-Wasserstein optimal transport plan $\\pi^{*}$ from $p_{0}$ to $p_{1}$ [Villani et al., 2009]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi^{*}=\\underset{\\pi\\in\\Pi}{\\arg\\operatorname*{min}}\\,\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}c^{2}(x,y)d\\pi(x,y),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Pi$ are the probability measures on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ with marginals $p_{0}$ and $p_{1}$ and $c$ is any cost. While we could choose $c$ based on $g_{\\mathrm{RBF}}$ , we instead select $c$ to be the $L_{2}$ distance in $\\mathbb{R}^{d}$ to avoid additional computations and so we can study the role played by $x_{t,\\eta}$ even when $q=\\pi^{*}$ is agnostic of the data manifold. We then propose the OT-MFM objective, where $\\eta^{*}$ minimizes the geodesic loss in eq. (11): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{OT-MFM_{RBF}}}(\\theta)=\\mathbb{E}_{t,(x_{0},x_{1})\\sim\\pi^{*}}\\left[\\Vert v_{t,\\theta}(x_{t,\\eta^{*}})-\\dot{x}_{t,\\eta^{*}}\\Vert_{g_{\\mathrm{RBF}}(x_{t,\\eta^{*}})}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We note that the case of $g_{\\mathrm{LAND}}$ is dealt with similarly. Additionally, different choices of the joint distribution $q$ , beyond Optimal Transport, can be adapted from CFM [Tong et al., 2023b] to MFM in eq. (7). ", "page_idx": 5}, {"type": "text", "text": "4.2 Understanding MFM through energies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In MFM we learn interpolants $x_{t}$ that are optimal according to eq. (6). Previous works have studied the \u201coptimality\u201d of interpolants, but have ignored the data manifold. Shaul et al. [2023] proposed to learn interpolants $x_{t}$ that minimize the kinetic energy $K({\\dot{x}}_{t})=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\,\\|{\\dot{x}}_{t}\\|^{2}$ . However, $\\kappa$ assigns each point in space the same cost, independent of the data, and leads to straight interpolants that may stray away from $\\mathcal{D}$ . Conversely, our objective in eq. (6) can equivalently be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g}(\\eta)=\\mathbb{E}_{(x_{0},x_{1})\\sim q}\\left[\\mathcal{E}_{g}(x_{t,\\eta})\\right]\\equiv\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}[\\left\\lVert\\dot{x}_{t,\\eta}\\right\\rVert_{g(x_{t,\\eta})}^{2}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As a result, the geodesic loss $\\mathcal{L}_{g}(\\eta)\\equiv\\mathcal{K}_{g}(\\dot{x}_{t,\\eta})$ is precisely the kinetic energy of vector fields with respect to $g$ and hence accounts for the cost induced by the data. In fact, choosing $\\mathbf{G}(x)=\\mathbf{I}$ , recovers $\\kappa$ . We note that the parameterization $x_{t,\\eta}$ in eq. (4) is more expressive than $x_{t}=a(t)x_{1}+b(t)x_{0}$ , which is studied in Albergo and Vanden-Eijnden [2022], Shaul et al. [2023]. Besides, $x_{t,\\eta}$ not only depends on the endpoints $x_{0},x_{1}$ but, implicitly, on all the data points $\\mathcal{D}$ through metrics such as $g_{\\mathrm{RBF}}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Data-dependent potentials. Energies more general than the kinetic one $\\kappa$ have been considered in Neklyudov et al. [2023a], Liu et al. [2024], Neklyudov et al. [2023b]. In particular, adapting GSBM [Liu et al., 2024] from the stochastic Schr\u00f6dinger bridge setting to CFM, entails designing interpolants $x_{t}$ that minimize $\\mathcal{U}(x_{t},\\dot{x}_{t})=\\mathcal{K}(\\dot{x}_{t})+V_{t}(x_{t})$ , with $V_{t}$ a potential enforcing additional constraints. However, GSBM does not prescribe a general recipe for $V_{t}$ and instead leaves to the modeler the task of constructing $V_{t}$ , based on applications. Conversely, MFM relies on a Riemannian approach to propose an explicit objective, i.e. eq. (11), that holds irrespective of the task and can be learned prior to regressing the vector field $v_{t,\\theta}$ . In fact, eq. (11) can be rewritten as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g_{\\mathrm{RBF}}}(\\eta)=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left[\\Vert\\dot{x}_{t,\\eta}\\Vert^{2}+V_{t,\\eta}(x_{t,\\eta},x_{0},x_{1})\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $V_{t,\\eta}$ is parametric function depending on the boundary points $x_{0},x_{1}$ (see Appendix $\\S C.1$ for an expression). In contrast to GSBM, MFM designs interpolants $x_{t,\\eta}$ that jointly minimize $\\kappa$ and a potential $V_{t,\\eta}$ that is not fixed but also updated with the same parameters $\\eta$ to bend paths towards the data. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We test METRIC FLOW MATCHING on different tasks: artificial dynamic reconstruction and navigation through LiDAR surfaces $\\S5.1$ ; unpaired translation between classes in images $\\S5.2$ ; reconstruction of cell dynamics. Further results and experimental details can be found in Appendices $\\S D$ , $\\S E$ and $\\S\\mathrm{F}$ . ", "page_idx": 6}, {"type": "text", "text": "The model. In all the experiments, we test the OT-MFM method detailed in $\\S4.1$ . As argued in $\\S4.1$ , for high-dimensional data, we leverage the RBF metric (10) and hence train with the objective (13). We refer to this model as $\\mathrm{OT-MFM_{\\mathrm{RBF}}}$ . Conversely, for low-dimensional data we rely on the LAND metric (9), replacing $g_{\\mathrm{RBF}}$ with $g_{\\mathrm{LAND}}$ in eq. (11) and eq. (13). We denote this model as $\\mathrm{OT-MFM_{\\mathrm{LAND}}}$ . Both metrics are task-independent and do not require further manipulation from the modeler. To assess the impact of metric learning in MFM even without using Optimal Transport for the coupling $q$ , we tested MFM with independent coupling on the Arch task and the single cell datasets, denoted as $\\mathbf{I}{\\mathrm{-}}\\mathbf{M}\\mathbf{F}\\mathbf{M}_{\\mathrm{LAND}}$ and $\\mathbf{I-MFM}_{\\mathrm{RBF}}$ . ", "page_idx": 6}, {"type": "text", "text": "Baselines. MFM generalizes CFM by learning interpolants that account for the geometry of the data. As such, in $\\S5.1$ and $\\S5.2$ , we focus on validating that OT-MFM leads to more meaningful matching than its Euclidean counterpart OT-CFM [Tong et al., 2023b]. For single-cell experiments instead, we also compare with a variety of baselines, including models specific to the single-cell domain [Tong et al., 2020, Koshizuka and Sato, 2023], Schr\u00f6dinger Bridge models [De Bortoli et al., 2021, Shi et al., 2023, Tong et al., 2023a], and ODE-flow methods [Tong et al., 2023b, Neklyudov et al., 2023b]. ", "page_idx": 6}, {"type": "image", "img_path": "fE3RqiF4Nx/tmp/613597396b5208a41a478f319607950d405984498ece6bd3e4babc47efcf60ca.jpg", "img_caption": ["Figure 2: Interpolants for the Arch dataset (top row), Sphere dataset (middle row) and over LiDAR scans of Mt. Rainier (bottom row). In all cases, learning interpolants that minimize the LAND metric (9) leads to more meaningful matchings. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Synthetic experiments and LiDAR ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our first goal is to validate that MFM and specifically the family of interpolants $x_{t,\\eta}$ in eq. (4), are expressive enough to model matching of distributions when the data induce a nonlinear geometry. ", "page_idx": 6}, {"type": "text", "text": "To begin with, we consider the Arch dataset in Tong et al. [2020], where we have access to the underlying paths. In Table 1 we report the Wasserstein distance (EMD) between the interpolation according to true dynamics and the marginal at time $1/2$ obtained using the Euclidean baseline OT-CFM and our Riemannian model OT- $\\mathbf{-MFM}_{\\mathrm{LAND}}$ . We see that $\\mathrm{OT-MFM_{\\mathrm{LAND}}}$ is able to faithfully reconstruct the dynamics by learning interpolants that minimize the geodesic cost induced by the metric in eq. (9). Conversely, straight interpolants fail to stay on the manifold generated by the dynamics (see Figure 2). ", "page_idx": 7}, {"type": "table", "img_path": "fE3RqiF4Nx/tmp/34e58f7a4eb3002fed5279008e20433d523c68a3f6913078fbf6b20c19d49eae.jpg", "table_caption": ["Table 1: Wasserstein distance between reconstructed marginal at time $1/2$ and ground-truth. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "To further showcase the ability of MFM to learn trajectories that stay close to an unknown underlying manifold without any added noise, we conduct an experiment on a 2D sphere embedded in $\\mathbb{R}^{5}$ . We consider source and target distributions that are Gaussians centered at the sphere\u2019s poles, with samples lying exactly on the sphere. MFM significantly improves over the Euclidean baseline, CFM, with samples at intermediate times being much closer to the underlying sphere than those from the Euclidean counterpart. This improvement is achieved without explicitly parameterizing the lower-dimensional space, relying solely on the LAND metric. Quantitative results, including EMD and the distance of the interpolating paths to the sphere, are reported in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Finally, to highlight the ability of MFM to generate meaningful matching, we also compare the interpolants of OT-CFM and OT- $\\mathbf{\\cal{MFM}}_{\\mathrm{LAND}}$ for navigations through surfaces scanned by LiDAR [Legg and Anderson, 2013]. From Figure 2 we find that straight paths result in unnatural trajectories, whereas OT-MFM manages to construct meaningful interpolations that better navigate the complex surface. While OT-MFM can be further enhanced using potentials specific to the task, similar to [Liu et al., 2024], here we focus on its ability to provide meaningful matching even with a task-agnostic metric. ", "page_idx": 7}, {"type": "text", "text": "5.2 Unpaired translation in latent space ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To test the advantages of MFM for more meaningful generation, we consider the task of unpaired image translation between dogs and cats in AFHQ [Choi et al., 2020]. Specifically, we perform unpaired translation in the latent space of the Stable Diffusion v1 VAE [Rombach et al., 2022]. Figure 3 reports a qualitative comparison between OT-CFM and $\\mathrm{OT-MFM_{\\mathrm{RBF}}}$ . Additionally, a quantitative comparison can be found in Table 2, where we measure both the quality of images via Fr\u00e9chet Inception Distance (FID) [Heusel et al., 2017] and the perceptual similarity of generated cats to source dogs via Learned Perceptual Image Patch Similarity (LPIPS) [Zhang et al., 2018]. We see that OT-MFM improves upon the Euclidean baseline. Our results using MFM further highlight the role played by the nonlinear geometry associated with latent representations, a topic studied extensively $\\S6$ . ", "page_idx": 7}, {"type": "image", "img_path": "fE3RqiF4Nx/tmp/f998983a9d496683d57d1fe1fcbe418e07affb9d101fcac15a99d4efd640fabb.jpg", "img_caption": ["Figure 3: Qualitative comparison for image translation. By designing interpolants on the data manifold, OT- $\\mathbf{MFM}_{\\mathrm{RBF}}$ better preserves input features. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Trajectory inference for single-cell data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We finally test MFM for reconstructing cell dynamics, a central problem in biomedical applications [L\u00e4hnemann et al., 2020], which holds great promise thanks to the advancements of single-cell RNA sequencing (scRNA-seq) [Macosko et al., 2015, Klein et al., 2015]. ", "page_idx": 8}, {"type": "text", "text": "Since in scRNA-seq trajectories cannot be tracked, we only assume access to $K$ unpaired distributions describing cell populations at $K$ time points. We then apply the matching objective in eq. (7) between every consecutive time points, sharing parameters for both the vector field $v_{t,\\theta}$ and the interpolants $x_{t,\\eta}$ \u2014see eq. (20) for how to extend $x_{t,\\eta}$ to multiple timepoints. Following the setup of Schiebinger et al. [2019], Tong et al. [2020, 2023a] we perform leave-one-out interpolation, where we measure the Wasserstein-1 distance between the $k$ -th left-out density and the one reconstructed after training on the remaining timepoints. We compare OT-MFM and baselines over Embryoid ", "page_idx": 8}, {"type": "table", "img_path": "fE3RqiF4Nx/tmp/dfed948bab64de9e7521727afb9f3c0a94a6d442dfa712ee8a1ed3288bf6ea1d.jpg", "table_caption": ["Table 3: Wasserstein-1 distance averaged over leftout marginals for 100-dim PCA single-cell data for corresponding datasets. Results averaged over 5 runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "body (EB) [Moon et al., 2019], and CITE-seq (Cite) and Multiome (Multi) data from [Lance et al., 2022]. In Table 4 and Table 3 we consider the first 5 and 100 principal components of the data, respectively\u2014results with 50 principal components can be found in Appendix D. We observe that OT-MFM significantly improves upon its Euclidean counterpart OT-CFM, which resonates with the manifold hypothesis for single-cell data [Moon et al., 2018]. In fact, OT-MFM surpasses all baselines, including those that add biases such as stochasticity ( $\\mathrm{\\DeltaSF^{2}}$ Tong et al. [2023a]) or mass teleportation (WLF-UOT Neklyudov et al. [2023b]). OT-MFM instead relies on metrics such as LAND and RBF to favor interpolations that remain close to the data. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Geometry-aware generative models. The manifold hypothesis [Bengio et al., 2013] has been studied in the context of manifold learning [Tenenbaum et al., 2000, Belkin and Niyogi, 2003] and metric learning [Xing et al., 2002, Weinberger and Saul, 2009, Hauberg et al., 2012]. Recently, this has also been analyzed in relation to generative models for obtaining meaningful interpolations [Arvanitidis et al., 2017, 2021, Chadebec and Allassonni\u00e8re, 2022], diagnosing model instability [Cornish et al., 2020, Loaiza-Ganem et al., 2022, 2024], assessing the ability to perform dimensionality reduction [Stanczuk et al., 2022, Pidstrigach, 2022] and for the improved learning and representation of curved, low-dimensional data manifolds [Dupont et al., 2019, Schonsheck et al., 2019, Horvat and Pfister, 2021, Yonghyeon et al., 2021, De Bortoli, 2022, Jang et al., 2022, Lee et al., 2022, Lee and Park, 2023, Nazari et al., 2023]. Closely related is the extension of generative models to settings where the ambient space itself is a Riemannian manifold [Mathieu and Nickel, 2020, Lou et al., 2020, Falorsi, 2021, De Bortoli et al., 2022, Huang et al., 2022, Rozen et al., 2021, Ben-Hamu et al., 2022, Chen and Lipman, 2023, Jo and Hwang, 2023]. Particularly, Maoutsa [2023] utilized a data-dependent geodesic solver [Arvanitidis et al., 2019] to refine drift estimation in stochastic differential equations. Several studies extended beyond the standard linear matching process to meet specific task requirements, but have not accounted for the geometry that the data naturally forms [Liu et al., 2024, Bartosh et al., 2024, Neklyudov et al., 2023b]. ", "page_idx": 8}, {"type": "table", "img_path": "fE3RqiF4Nx/tmp/e2b4cd86fb0b1c22f745298faf709416ec6dde44982bc91615ab060e3e16f94e.jpg", "table_caption": ["Table 4: Wasserstein-1 distance (\u2193) averaged over left-out marginals for 5-dim PCA representation of single-cell data for corresponding datasets. Results are averaged over 5 independent runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Trajectory inference. Reconstructing dynamics from cross-sectional distributions [Hashimoto et al., 2016, Lavenant et al., 2021] is an important problem within the natural sciences, especially in the context of single-cell analysis [Macosko et al., 2015, Moon et al., 2018, Schiebinger et al., 2019]. Recently, diffusion and Continuous Normalizing Flow (CNFs) based methods have been proposed [Tong et al., 2020, Bunne et al., 2022, 2023, Huguet et al., 2022, Koshizuka and Sato, 2023] but require simulations, whereas Tong et al. [2023a], Neklyudov et al. [2023a], Palma et al. [2023] allow for simulation-free training. In particular, Huguet et al. [2022], Palma et al. [2023] regularize CNFs in a latent space to enforce that straight paths correspond to interpolations on the original data manifold. Finally, Scarvelis and Solomon [2023] propose a solution to the trajectory inference problem for cellular data, which depends on a regularization of vector fields with respect to a learned metric similar to eq. (6). Crucially though, their framework requires simulations in training and is not immediately extended to more general matching objectives and applications as for MFM. In this regard, we observe that one can also adopt the metric-learning scheme of Scarvelis and Solomon [2023] in MFM, replacing $g_{\\mathrm{LAND}}$ and $g_{\\mathrm{RBF}}$ introduced in $\\S4$ . ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented METRIC FLOW MATCHING, a simulation-free framework that generalizes Conditional Flow Matching to design probability paths whose support lies on the data manifold. In MFM, this is achieved via interpolants that minimize the geodesic cost of a data-dependent Riemannian metric. We have empirically shown that instances of MFM using prescribed task-agnostic metrics, surpass Euclidean baselines, with emphasis on single-cell dynamics reconstruction. While the universality of the metrics proposed in $\\S4$ is a benefit, we have not investigated how to further encode biases into the metric that are specific to the downstream task\u2014a topic reserved for future work. Additionally, the principle of learning interpolants that minimize a geodesic cost can also be adapted to score-based generative models such as diffusion models, beyond CFM. When relying on the OT coupling, standard limitations of using OT for high-dimensional problems with large datasets may arise. Lastly, our approach requires the data to be embedded in Euclidean space for the interpolants to be defined; it is an interesting direction to explore how one can learn interpolants that minimize a data-dependent metric even when the ambient space itself is not Euclidean. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "KK is supported by the EPSRC CDT in Health Data Science (EP/S02428X/1). PP and LZ are supported by the EPSRC CDT in Modern Statistics and Statistical Machine Learning (EP/S023151/1) and acknowledge helpful discussions with Yee Whye Teh. AJB is partially supported by an NSERC Post-doc fellowship and an EPSRC Turing AI World-Leading Research Fellowship. FDG is supported by an EPSRC Turing AI World-Leading Research Fellowship. MB is partially supported by EPSRC Turing AI World-Leading Research Fellowship No. EP/X040062/1 and EPSRC AI Hub on Mathematical Foundations of Intelligence: An \"Erlangen Programme\" for AI No. EP/Y028872/1 ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. (Cited on pages 1, 3, and 7) L. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005. (Cited on page 4) G. Arvanitidis, L. K. Hansen, and S. Hauberg. A locally adaptive normal distribution. Advances in Neural Information Processing Systems, 29, 2016. (Cited on pages 6 and 20) G. Arvanitidis, L. K. Hansen, and S. Hauberg. Latent space oddity: on the curvature of deep generative models. arXiv preprint arXiv:1710.11379, 2017. (Cited on page 10) G. Arvanitidis, S. Hauberg, P. Hennig, and M. Schober. Fast and robust shortest paths on manifolds learned from data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1506\u20131515. PMLR, 2019. (Cited on page 10) G. Arvanitidis, S. Hauberg, and B. Sch\u00f6lkopf. Geometrically enriched latent spaces. In International Conference on Artificial Intelligence and Statistics, pages 631\u2013639. PMLR, 2021. (Cited on pages 2,   \n6, 10, and 20) G. Arvanitidis, M. Gonz\u00e1lez-Duque, A. Pouplin, D. Kalatzis, and S. Hauberg. Pulling back information geometry. In 25th International Conference on Artificial Intelligence and Statistics, 2022. (Cited on page 4) G. Bartosh, D. Vetrov, and C. A. Naesseth. Neural flow diffusion models: Learnable forward process for improved diffusion modelling. arXiv preprint arXiv:2404.12940, 2024. (Cited on page 10) M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373\u20131396, 2003. (Cited on pages 2 and 9) H. Ben-Hamu, S. Cohen, J. Bose, B. Amos, A. Grover, M. Nickel, R. T. Chen, and Y. Lipman. Matching normalizing flows and probability paths on manifolds. arXiv preprint arXiv:2207.04711,   \n2022. (Cited on page 10) Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013. (Cited on pages 4 and 9) C. Bunne, L. Papaxanthos, A. Krause, and M. Cuturi. Proximal optimal transport modeling of population dynamics. In International Conference on Artificial Intelligence and Statistics, pages   \n6511\u20136528. PMLR, 2022. (Cited on page 10) C. Bunne, Y.-P. Hsieh, M. Cuturi, and A. Krause. The schr\u00f6dinger bridge between gaussian measures has a closed form. In International Conference on Artificial Intelligence and Statistics, pages   \n5802\u20135833. PMLR, 2023. (Cited on page 10) C. Chadebec and S. Allassonni\u00e8re. A geometric perspective on variational autoencoders. Advances in Neural Information Processing Systems, 35:19618\u201319630, 2022. (Cited on pages 4 and 10) R. T. Chen and Y. Lipman. Riemannian flow matching on general geometries. arXiv preprint arXiv:2302.03660, 2023. (Cited on pages 2, 3, 5, 10, and 19) R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. (Cited on page 3) Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages   \n8188\u20138197, 2020. (Cited on pages 8 and 22) R. Cornish, A. Caterini, G. Deligiannidis, and A. Doucet. Relaxing bijectivity constraints with continuously indexed normalising flows. In International conference on machine learning, pages   \n2133\u20132143. PMLR, 2020. (Cited on page 10)   \nV. De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint arXiv:2208.05314, 2022. (Cited on page 10)   \nV. De Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34: 17695\u201317709, 2021. (Cited on pages 7 and 9)   \nV. De Bortoli, E. Mathieu, M. Hutchinson, J. Thornton, Y. W. Teh, and A. Doucet. Riemannian scorebased generative modelling. Advances in Neural Information Processing Systems, 35:2406\u20132422, 2022. (Cited on page 10)   \nP. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021. (Cited on page 21)   \nE. Dupont, A. Doucet, and Y. W. Teh. Augmented neural odes. Advances in neural information processing systems, 32, 2019. (Cited on page 10)   \nL. Falorsi. Continuous normalizing flows on manifolds. arXiv preprint arXiv:2104.14959, 2021. (Cited on page 10)   \nC. Finlay, J.-H. Jacobsen, L. Nurbekyan, and A. Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In International conference on machine learning, pages 3154\u20133164. PMLR, 2020. (Cited on page 9)   \nT. Hashimoto, D. Gifford, and T. Jaakkola. Learning population-level diffusions with generative rnns. In International Conference on Machine Learning, pages 2417\u20132426. PMLR, 2016. (Cited on pages 1 and 10)   \nS. Hauberg, O. Freifeld, and M. Black. A geometric take on metric learning. Advances in Neural Information Processing Systems, 25, 2012. (Cited on pages 2, 4, and 9)   \nJ. A. Hay, L. Kennedy-Shaffer, S. Kanjilal, N. J. Lennon, S. B. Gabriel, M. Lipsitch, and M. J. Mina. Estimating epidemiologic dynamics from cross-sectional viral load distributions. Science, 373 (6552):eabh0635, 2021. (Cited on page 1)   \nP. Hennig and S. Hauberg. Probabilistic solutions to differential equations and their application to riemannian statistics. In Artificial Intelligence and Statistics, pages 347\u2013355. PMLR, 2014. (Cited on page 4)   \nM. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. (Cited on pages 8 and 22)   \nC. Horvat and J.-P. Pfister. Denoising normalizing flow. Advances in Neural Information Processing Systems, 34:9099\u20139111, 2021. (Cited on page 10)   \nC.-W. Huang, M. Aghajohari, J. Bose, P. Panangaden, and A. C. Courville. Riemannian diffusion models. Advances in Neural Information Processing Systems, 35:2750\u20132761, 2022. (Cited on page 10)   \nG. Huguet, D. S. Magruder, A. Tong, O. Fasina, M. Kuchroo, G. Wolf, and S. Krishnaswamy. Manifold interpolating optimal-transport flows for trajectory inference. Advances in neural information processing systems, 35:29705\u201329718, 2022. (Cited on page 10)   \nC. Jang, Y. Lee, Y.-K. Noh, and F. C. Park. Geometrically regularized autoencoders for non-euclidean data. In The Eleventh International Conference on Learning Representations, 2022. (Cited on page 10)   \nJ. Jo and S. J. Hwang. Generative modeling on manifolds through mixture of riemannian diffusion processes. arXiv preprint arXiv:2310.07216, 2023. (Cited on page 10)   \nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. (Cited on pages 21 and 22)   \nD. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014. (Cited on page 22)   \nA. M. Klein, L. Mazutis, I. Akartuna, N. Tallapragada, A. Veres, V. Li, L. Peshkin, D. A. Weitz, and M. W. Kirschner. Droplet barcoding for single-cell transcriptomics applied to embryonic stem cells. Cell, 161(5):1187\u20131201, 2015. (Cited on pages 1 and 9)   \nT. Koshizuka and I. Sato. Neural lagrangian schr\u00f6dinger bridge: Diffusion modeling for population dynamics. In International Conference on Learning Representations, 2023. (Cited on pages 7, 9, and 10)   \nD. L\u00e4hnemann, J. K\u00f6ster, E. Szczurek, D. J. McCarthy, S. C. Hicks, M. D. Robinson, C. A. Vallejos, K. R. Campbell, N. Beerenwinkel, A. Mahfouz, et al. Eleven grand challenges in single-cell data science. Genome biology, 21:1\u201335, 2020. (Cited on page 9)   \nC. Lance, M. D. Luecken, D. B. Burkhardt, R. Cannoodt, P. Rautenstrauch, A. Laddach, A. Ubingazhibov, Z.-J. Cao, K. Deng, S. Khan, et al. Multimodal single cell data integration challenge: results and lessons learned. BioRxiv, pages 2022\u201304, 2022. (Cited on pages 9 and 22)   \nH. Lavenant, S. Zhang, Y.-H. Kim, and G. Schiebinger. Towards a mathematical theory of trajectory inference. arXiv preprint arXiv:2102.09204, 2021. (Cited on pages 1, 3, and 10)   \nJ. M. Lee. Smooth manifolds. Springer, 2012. (Cited on page 16)   \nY. Lee and F. C. Park. On explicit curvature regularization in deep generative models. In Topological, Algebraic and Geometric Learning Workshops 2023, pages 505\u2013518. PMLR, 2023. (Cited on page 10)   \nY. Lee, S. Kim, J. Choi, and F. Park. A statistical manifold framework for point cloud data. In International Conference on Machine Learning, pages 12378\u201312402. PMLR, 2022. (Cited on page 10)   \nN. Legg and S. Anderson. Southwest flank of Mt.Rainier, wa, 2013. URL https://doi.org/10. 5069/G9PZ56R1.Accessed:2024-05-21. (Cited on pages 8 and 21)   \nC. L\u00e9onard. A survey of the schr\\\" odinger problem and some of its connections with optimal transport. arXiv preprint arXiv:1308.0215, 2013. (Cited on page 1)   \nY. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. International Conference on Learning Representations (ICLR), 2023. (Cited on pages 1, 3, and 4)   \nG.-H. Liu, Y. Lipman, M. Nickel, B. Karrer, E. Theodorou, and R. T. Chen. Generalized schr\u00f6dinger bridge matching. In ICLR, 2024. (Cited on pages 7, 8, 10, 20, and 21)   \nX. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. (Cited on pages 1, 3, and 4)   \nG. Loaiza-Ganem, B. L. Ross, J. C. Cresswell, and A. L. Caterini. Diagnosing and fixing manifold overftiting in deep generative models. arXiv preprint arXiv:2204.07172, 2022. (Cited on pages 2, 4, and 10)   \nG. Loaiza-Ganem, B. L. Ross, R. Hosseinzadeh, A. L. Caterini, and J. C. Cresswell. Deep generative models through the lens of the manifold hypothesis: A survey and new connections. arXiv preprint arXiv:2404.02954, 2024. (Cited on pages 4 and 10)   \nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. (Cited on pages 21 and 22)   \nA. Lou, D. Lim, I. Katsman, L. Huang, Q. Jiang, S. N. Lim, and C. M. De Sa. Neural manifold ordinary differential equations. Advances in Neural Information Processing Systems, 33:17548\u2013 17558, 2020. (Cited on page 10)   \nE. Z. Macosko, A. Basu, R. Satija, J. Nemesh, K. Shekhar, M. Goldman, I. Tirosh, A. R. Bialas, N. Kamitaki, E. M. Martersteck, et al. Highly parallel genome-wide expression profiling of individual cells using nanoliter droplets. Cell, 161(5):1202\u20131214, 2015. (Cited on pages 1, 3, 9, and 10)   \nD. Maoutsa. Geometric constraints improve inference of sparsely observed stochastic dynamics. arXiv preprint arXiv:2304.00423, 2023. (Cited on page 10)   \nE. Mathieu and M. Nickel. Riemannian continuous normalizing flows. Advances in Neural Information Processing Systems, 33:2503\u20132515, 2020. (Cited on page 10)   \nM. G. Monera, A. Montesinos-Amilibia, and E. Sanabria-Codesal. The taylor expansion of the exponential map and geometric applications. Revista de la Real Academia de Ciencias Exactas, Fisicas y Naturales. Serie A. Matematicas, 108:881\u2013906, 2014. (Cited on page 19)   \nK. R. Moon, J. S. Stanley III, D. Burkhardt, D. van Dijk, G. Wolf, and S. Krishnaswamy. Manifold learning-based methods for analyzing single-cell rna-sequencing data. Current Opinion in Systems Biology, 7:36\u201346, 2018. (Cited on pages 2, 4, 9, and 10)   \nK. R. Moon, D. Van Dijk, Z. Wang, S. Gigante, D. B. Burkhardt, W. S. Chen, K. Yim, A. v. d. Elzen, M. J. Hirn, R. R. Coifman, et al. Visualizing structure and transitions in high-dimensional biological data. Nature biotechnology, 37(12):1482\u20131492, 2019. (Cited on pages 9 and 22)   \nP. Nazari, S. Damrich, and F. A. Hamprecht. Geometric autoencoders\u2013what you see is what you decode. arXiv preprint arXiv:2306.17638, 2023. (Cited on page 10)   \nK. Neklyudov, R. Brekelmans, D. Severo, and A. Makhzani. Action matching: Learning stochastic dynamics from samples. In International Conference on Machine Learning, pages 25858\u201325889. PMLR, 2023a. (Cited on pages 4, 7, and 10)   \nK. Neklyudov, R. Brekelmans, A. Tong, L. Atanackovic, Q. Liu, and A. Makhzani. A computational framework for solving wasserstein lagrangian flows. arXiv preprint arXiv:2310.10649, 2023b. (Cited on pages 7, 9, 10, and 23)   \nJ. Oeppen and J. W. Vaupel. Broken limits to life expectancy, 2002. (Cited on page 1)   \nA. Palma, S. Rybakov, L. Hetzel, and F. J. Theis. Modelling single-cell rna-seq trajectories on a flat statistical manifold. In NeurIPS 2023 AI for Science Workshop, 2023. (Cited on page 10)   \nJ. Pidstrigach. Score-based generative models detect manifolds. Advances in Neural Information Processing Systems, 35:35852\u201335865, 2022. (Cited on page 10)   \nQ. Que and M. Belkin. Back to the future: Radial basis function networks revisited. In Artificial intelligence and statistics, pages 1375\u20131383. PMLR, 2016. (Cited on page 6)   \nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022. (Cited on pages 8 and 22)   \nN. Rozen, A. Grover, M. Nickel, and Y. Lipman. Moser flow: Divergence-based generative modeling on manifolds. Advances in Neural Information Processing Systems, 34:17669\u201317680, 2021. (Cited on page 10)   \nA. Salmona, V. De Bortoli, J. Delon, and A. Desolneux. Can push-forward generative models fit multimodal distributions? Advances in Neural Information Processing Systems, 35:10766\u201310779, 2022. (Cited on pages 2 and 4)   \nC. Scarvelis and J. Solomon. Riemannian metric learning via optimal transport. In ICLR, 2023. (Cited on page 10)   \nG. Schiebinger. Reconstructing developmental landscapes and trajectories from single-cell data. Current Opinion in Systems Biology, 27:100351, 2021. (Cited on page 6)   \nG. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin, P. Berube, et al. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. Cell, 176(4):928\u2013943, 2019. (Cited on pages 9 and 10)   \nS. Schonsheck, J. Chen, and R. Lai. Chart auto-encoders for manifold structured data. arXiv preprint arXiv:1912.10094, 2019. (Cited on pages 4 and 10)   \nE. Schr\u00f6dinger. Sur la th\u00e9orie relativiste de l\u2019\u00e9lectron et l\u2019interpr\u00e9tation de la m\u00e9canique quantique. In Annales de l\u2019institut Henri Poincar\u00e9, volume 2, pages 269\u2013310, 1932. (Cited on page 1)   \nN. Shaul, R. T. Chen, M. Nickel, M. Le, and Y. Lipman. On kinetic optimal probability paths for generative models. In International Conference on Machine Learning, pages 30883\u201330907. PMLR, 2023. (Cited on pages 4, 6, and 7)   \nY. Shi, V. De Bortoli, A. Campbell, and A. Doucet. Diffusion schr\u00f6dinger bridge matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. (Cited on pages 7 and 9)   \nJ. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. International Conference on Machine Learning (ICML), 2015. (Cited on page 1)   \nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations (ICLR), 2021. (Cited on page 1)   \nJ. Stanczuk, G. Batzolis, T. Deveney, and C.-B. Sch\u00f6nlieb. Your diffusion model secretly knows the dimension of the data manifold. arXiv preprint arXiv:2212.12611, 2022. (Cited on page 10)   \nJ. B. Tenenbaum, V. d. Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323, 2000. (Cited on pages 2 and 9)   \nA. Tong, J. Huang, G. Wolf, D. Van Dijk, and S. Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In International conference on machine learning, pages 9526\u20139536. PMLR, 2020. (Cited on pages 7, 8, 9, 10, 21, and 22)   \nA. Tong, N. Malkin, K. Fatras, L. Atanackovic, Y. Zhang, G. Huguet, G. Wolf, and Y. Bengio. Simulation-free schr\u00f6dinger bridges via score and flow matching, 2023a. (Cited on pages 7, 9, 10, and 22)   \nA. Tong, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y. Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023b. (Cited on pages 1, 3, 4, 6, 7, 9, and 22)   \nC. Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. (Cited on page 6)   \nK. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classification. Journal of machine learning research, 10(2), 2009. (Cited on page 9)   \nE. Xing, M. Jordan, S. J. Russell, and A. Ng. Distance metric learning with application to clustering with side-information. Advances in neural information processing systems, 15, 2002. (Cited on pages 2 and 9)   \nL. Yonghyeon, S. Yoon, M. Son, and F. C. Park. Regularized autoencoders for isometric representation learning. In International Conference on Learning Representations, 2021. (Cited on page 10)   \nR. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018. (Cited on pages 8 and 22) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Outline of Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Appendix A we give a brief overview of relevant notions from differential and Riemannian geometry. Appendix B provides more details for Section 3 including the formal statement and proof of Proposition 1. We also derive how to rigorously extend flow matching to the Riemannian manifold induced by a data-dependent metric. We report a pseudocode for MFM in Algorithm 2. Appendix $\\mathrm{C}$ provides more details regarding the data-dependent Riemannian metrics we use, and relevant training procedures. In Appendix $\\mathrm{D}$ we supply more details regarding the various experiments. Appendix E, Appendix F, and Appendix $\\mathrm{H}$ contain supplementary figures and tables for single-cell reconstruction, unpaired translation, and LiDAR tasks, respectively. Finally, in Appendix G, we outline the quantitative evaluation results for MFM on the sphere experiment. ", "page_idx": 15}, {"type": "text", "text": "All exact hyperparameters used and reproducible code can be found at https://github.com/ kksniak/metric-flow-matching.git. ", "page_idx": 15}, {"type": "text", "text": "A Primer on Riemannian Geometry ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Riemannian geometry is the study of smooth manifolds $\\mathcal{M}$ equipped with a (Riemannian) metric $g$ . Intuitively, this corresponds to spaces that can be considered locally Euclidean, and which allow for a consistent notion of measuring distances, angles, curvature, shortest paths, etc. on abstract geometric objects. We provide a primer on the relevant concepts of Riemannian geometry for our work, covering smooth manifolds and their tangent spaces, Riemannian metrics and geodesics, and integration on Riemannian manifolds. For a comprehensive introduction, see Lee [2012]. ", "page_idx": 15}, {"type": "text", "text": "Smooth manifolds. Formally, we say a topological space2 $\\mathcal{M}$ is a $d_{\\cdot}$ -dimensional smooth manifold if we have a collection of charts $(U_{i},\\varphi_{i})$ where $U_{i}$ are open subsets of $\\mathcal{M}$ and $\\cup_{i}U_{i}\\ =\\ M$ , $\\varphi_{i}:U_{i}\\to\\mathbb{R}^{d}$ are homeomorphisms onto their image, and when $U_{i}\\cap U_{j}\\neq\\emptyset$ , the transition maps $\\varphi_{j}\\circ\\varphi_{i}^{-1}:\\varphi_{i}(U_{i}\\cap U_{j})\\to\\varphi_{j}(U_{i}\\cap U_{j})$ are diffeomorphisms. These charts allow us to represent quantities on $\\mathcal{M}$ through the local coordinates obtained by mapping back to Euclidean space via $\\varphi_{i}$ , as well as allowing us to define smooth maps between manifolds. ", "page_idx": 15}, {"type": "text", "text": "In particular, we define a smooth path in $\\mathcal{M}$ passing through $x\\in{\\mathcal{M}}$ as a function $\\gamma:(-\\epsilon,\\epsilon)\\rightarrow\\mathcal{M}$ , for some $\\epsilon>0$ and $\\gamma(0)=x$ , such that the local coordinate representation $\\varphi_{i}\\circ\\gamma$ of $\\gamma$ is smooth in the standard sense (for any suitable chart $(U_{i},\\varphi_{i}))$ . The derivatives ${\\dot{\\gamma}}(0)$ of smooth paths $\\gamma$ passing through $x\\ \\in\\ M$ form a $d$ -dimensional vector space called the tangent space $T_{x}\\mathcal{M}$ at $x$ . We can represent $T_{x}\\mathcal{M}$ in local coordinates with $\\varphi_{i}$ by the identification of $\\dot{\\gamma}(0)\\,\\in\\,T_{x}\\mathcal{M}$ to $(a_{1}^{\\prime}(0),\\dots,a_{d}^{\\prime}(0))^{\\top}\\in\\mathbb{R}^{d}$ where $\\boldsymbol{(a_{1}(t),\\cdot\\cdot\\cdot,a_{d}(t))^{\\intercal}=\\varphi_{i}\\circ\\gamma(t)}$ . ", "page_idx": 15}, {"type": "text", "text": "Riemannian structure. To introduce geometric notions of length and distances to smooth manifolds, we define a Riemannian metric3 $g$ on $\\mathcal{M}$ as a map providing a smooth assignment of points $x\\in\\mathcal{M}$ on the manifold to a positive definite inner product $\\langle\\cdot,\\cdot\\rangle_{g(x)}$ defined on the corresponding tangent space $T_{x}\\mathcal{M}$ . In local coordinates about $x\\in\\mathcal{M}$ given by a suitable chart $(U_{i},\\varphi_{i})$ , we have the local representation $\\langle v,w\\rangle_{g(x)}\\,=\\,v^{\\top}{\\mathbf G}(x)w$ where $\\mathbf{G}(x)\\,\\in\\,\\mathbb{R}^{d\\times d}$ is a positive definite matrix (which implicitly depends on the choice of chart). We call the pair $(\\mathcal{M},g)$ a Riemannian manifold. ", "page_idx": 15}, {"type": "text", "text": "Through the Riemannian metric, we can now define the norm of tangent vectors by $\\|v\\|_{g(x)}=$ $\\left\\langle v,v\\right\\rangle_{g(x)}^{1/2}$ for $v\\in T_{x}\\M$ , as well as the length of a smooth path $\\gamma:[0,1]\\rightarrow\\mathcal{M}$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Len}(\\gamma)=\\int_{0}^{1}\\|{\\dot{\\gamma}}(t)\\|_{g(\\gamma(t))}d t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can then define a geodesic $\\gamma^{*}$ between $x_{0}$ and $x_{1}$ in $\\mathcal{M}$ as the shortest possible path between the two points - i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma^{*}=\\arg\\operatorname*{min}_{\\gamma}\\int_{0}^{1}\\|\\dot{\\gamma}(t)\\|_{g(\\gamma(t))}\\,d t,\\quad\\gamma(0)=x_{0},\\ \\gamma(1)=x_{1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We assume that all Riemannian manifolds being considered are (geodesically) complete, meaning that geodesics can be extended indefinitely. In particular, for any pair of points $x_{0},x_{1}$ , there exists a unique geodesic $\\gamma_{t}^{*}$ starting at $x_{0}$ and ending at $x_{1}$ . ", "page_idx": 16}, {"type": "text", "text": "Integration on Riemannian manifolds. To introduce integration on Riemannian manifolds, we use the fact that under the technical assumption that $\\mathcal{M}$ is orientable, the Riemannian manifold $(\\mathcal{M},g)$ has a canonical volume form dvol. This can be used to define a measure on $\\mathcal{M}$ , where in local coordinates, we have that ${d\\mathrm{vol}}(x)=\\sqrt{|{\\bf G}(x)|}d x$ where $d x$ denotes the Lebesgue measure in $\\mathbb{R}^{d}$ and $|\\cdot|$ denotes the determinant. Hence, for a chart $(U_{i},\\varphi_{i})$ and a continuous function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ which is compactly supported in $U_{i}$ , we define the integral ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{M}}f d\\mathrm{vol}=\\int_{\\varphi_{i}(U_{i})}f\\circ\\varphi_{i}^{-1}(x)\\sqrt{|\\mathbf{G}(x)|}d x.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This definition can be extended to more general functions through the use of partitions of unity and the Riesz\u2013Markov\u2013Kakutani representation theorem. ", "page_idx": 16}, {"type": "text", "text": "The Riemannian geometry of Metric Flow Matching. Finally, we note that in our work, we take our smooth manifold to be $\\mathcal{M}=\\mathbb{R}^{d}$ with the trivial chart $U_{i}=\\mathbb{R}^{d},\\varphi_{i}=\\mathrm{id}$ . This means we can work in the usual Euclidean coordinates instead of requiring charts and local coordinates, simplifying our framework. For example, we can define $g$ through $\\mathbf{G}$ directly in Definition 1 without the need to check consistency across the choice of local coordinates, and we have the trivial identification of $T_{x}\\mathcal{M}$ to $\\mathbb{R}^{d}$ . In addition, for a continuous $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ , we have the simplified definition of the Riemannian integral (when the integral exists) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{M}}f d\\mathrm{vol}=\\int_{\\mathbb{R}^{d}}f(x)\\sqrt{|\\mathbf{G}(x)|}d x.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We use this to define probability densities on $(\\mathcal{M},g)$ to extend CFM to our setting in $\\S B.1$ . ", "page_idx": 16}, {"type": "text", "text": "B Additional Details for Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To begin with, we provide a formal statement covering Proposition 1 and report a proof below. We introduce some notation. We write $\\gamma_{t}\\subset U$ when the image set of $\\gamma:[0,1]\\stackrel{\\Sigma}{\\rightarrow}\\mathbb{R}^{d}$ is contained in $U$ , i.e. $\\gamma_{t}\\in U$ for each $t\\in[0,1]$ . Moreover, we let ", "page_idx": 16}, {"type": "text", "text": "denote the set of points in $\\mathbb{R}^{d}$ whose distance from the dataset $\\mathcal{D}$ is at most $r>0$ . ", "page_idx": 16}, {"type": "text", "text": "Theorem B.1 (Formal statement of Proposition 1). Consider a closed dataset $\\mathcal{D}\\subset\\mathbb{R}^{d}$ (e.g. finite). Assume that for each $(x_{0},x_{1})\\;\\sim\\;q,$ , there exists at least a path $\\gamma_{t}$ connecting $x_{0}$ to $x_{1}$ whose length is at most $\\Gamma$ and such that $\\gamma_{t}\\,\\subset\\,B_{\\delta}(\\mathcal{D})$ , with $\\delta\\:>\\:0$ . Let $\\kappa\\;>\\;0,\\rho\\;>\\;\\delta$ and $g$ be any data-dependent metric satisfying: ( $i)\\;v^{\\top}\\mathbf{G}(x;D)v\\geq\\kappa\\|v\\|^{2}$ for each $x\\in\\mathbb{R}^{d}\\setminus B_{\\rho}(\\mathcal{D})$ and $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ ; (ii) $\\|\\mathbf{G}(x;D)\\|\\leq\\kappa(\\rho/\\Gamma)^{2}$ for each $x\\in B_{\\delta}(\\mathcal{D})$ . Then for any $(x_{0},x_{1})\\sim q$ , the geodesic $\\gamma_{t}^{*}$ of $g$ connecting $x_{0}$ to $x_{1}$ satisfies $\\gamma_{t}^{*}\\subset B_{2\\rho}(\\mathcal{D})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem B.1. We argue by contradiction and assume that there exist $x\\in\\mathbb{R}^{d}\\setminus B_{2\\rho}(\\mathcal{D})$ and $(x_{0},x_{1})\\sim q$ such that the geodesic $\\gamma_{t}^{*}$ of $g$ connecting $x_{0}$ to $x_{1}$ passes through $x$ , i.e. there is a time $t_{0}\\in(0,1)$ such that $\\gamma_{t_{0}}^{*}=x$ . ", "page_idx": 16}, {"type": "text", "text": "By assumption, there exists a path $\\gamma_{t}$ that connects $x_{0}$ to $x_{1}$ and stays within $B_{\\delta}(\\mathcal{D})$ , which means that $x_{0}=\\gamma_{0}=\\gamma_{0}^{*}\\in B_{\\delta}(\\mathcal{D})$ . Since $\\mathcal{D}$ is closed, the function $x\\mapsto d_{\\mathrm{E}}(x,\\mathcal{D}):=\\operatorname*{inf}_{y\\in\\mathcal{D}}\\|x-y\\|$ , i.e. the Euclidean distance of $x$ from the dataset $\\mathcal{D}$ , is continuous. In particular, $t\\mapsto d_{\\mathrm{E},\\mathcal{D}}(t):=d_{\\mathrm{E}}(\\gamma_{t}^{*},\\mathcal{D})$ is also continuous, due to the geodesic being a smooth function in the interval $[0,1]$ . From the continuity of $d_{\\mathrm{E},\\mathcal{D}}$ and the fact that $d_{\\mathrm{E},\\mathcal{D}}(0)\\leq\\delta$ and $d_{\\mathrm{E},\\mathcal{D}}(t_{0})>2\\rho$ , it follows that there must be a time $0<t^{\\prime}<t_{0}$ such that $d_{\\mathrm{E},\\mathcal{D}}(t)\\geq\\rho$ for all $t\\in(t^{\\prime},t_{0}]$ and $d_{\\mathrm{E},\\mathcal{D}}(t^{\\prime})=\\rho$ . ", "page_idx": 16}, {"type": "text", "text": "If we unpack the definition of $d_{\\mathrm{E},\\mathcal{D}}$ , we have just shown that $\\gamma_{t}^{*}\\in\\mathbb{R}^{d}\\setminus B_{\\rho}(\\mathcal{D})$ for all $t\\in(t^{\\prime},t_{0}]$ Accordingly, we can estimate the length of $\\gamma_{t}^{*}$ with respect to the Riemannian metric $g$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Len}_{g}(\\gamma_{t}^{*})=\\displaystyle\\int_{0}^{1}\\|\\dot{\\gamma}_{t}^{*}\\|_{g(\\gamma_{t}^{*})}\\,d t>\\displaystyle\\int_{t^{\\prime}}^{t_{0}}\\|\\dot{\\gamma}_{t}^{*}\\|_{g(\\gamma_{t}^{*})}\\,d t}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{t^{\\prime}}^{t_{0}}\\sqrt{(\\dot{\\gamma}_{t}^{*})^{\\top}{\\bf G}(\\gamma_{t}^{*};\\mathcal{D})\\dot{\\gamma}_{t}^{*}}\\,d t\\ge\\sqrt{\\kappa}\\displaystyle\\int_{t^{\\prime}}^{t_{0}}\\|\\dot{\\gamma}_{t}^{*}\\|\\,d t\\ge\\sqrt{\\kappa}\\|\\gamma_{t^{\\prime}}^{*}-\\gamma_{t_{0}}^{*}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where in the\u221a second-to-last inequality we have used the assumption (i) on the metric having minimal eigenvalue $\\sqrt{\\kappa}$ in $\\mathbb{R}^{d}\\setminus B_{\\rho}(\\mathcal{D})$ , while the final inequality simply follows from the Euclidean length of any curve between the points $\\gamma_{t^{\\prime}}^{*}$ and $\\gamma_{t_{0}}^{*}$ being larger than their Euclidean distance. ", "page_idx": 17}, {"type": "text", "text": "We claim that $\\|\\gamma_{t^{\\prime}}^{*}-\\gamma_{t_{0}}^{*}\\|\\geq\\rho$ . To validate the latter point, take $\\epsilon>0$ . It follows that there exists $x_{i}\\in\\mathcal{D}$ such that $\\lVert x_{i}-\\gamma_{t^{\\prime}}^{*}\\rVert\\leq\\rho+\\epsilon.$ , because $d_{\\mathrm{E},\\mathcal{D}}(t^{\\prime})=\\rho$ , i.e. $\\gamma_{t^{\\prime}}^{*}\\in B_{\\rho}(\\mathcal{D})$ . If $\\|\\gamma_{t^{\\prime}}^{*}-\\gamma_{t_{0}}^{*}\\|<\\rho$ , then we could apply the triangle inequality and derive that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\gamma_{t_{0}}^{*}-x_{i}\\|\\leq\\|\\gamma_{t_{0}}^{*}-\\gamma_{t^{\\prime}}^{*}\\|+\\|\\gamma_{t^{\\prime}}^{*}-x_{i}\\|<\\rho+\\rho+\\epsilon=2\\rho+\\epsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\epsilon>0$ was arbitrary, it would follow that $\\gamma_{t_{0}}^{*}\\equiv x\\in B_{2\\rho}(D)$ , which is a contradiction to our starting point. Therefore, $\\|\\gamma_{t^{\\prime}}^{*}-\\gamma_{t_{0}}^{*}\\|\\geq\\rho$ , and hence ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Len}_{g}(\\gamma_{t}^{*})>\\sqrt{\\kappa}\\rho.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, the assumptions guarantee the existence of another path $\\gamma_{t}$ connecting $x_{0}$ to $x_{1}$ whose image is always contained in $B_{\\delta}(\\mathcal{D})$ . It follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{Len}_{g}(\\gamma_{t})=\\displaystyle\\int_{0}^{1}\\|\\dot{\\gamma}_{t}\\|_{g(\\gamma_{t})}\\,d t=\\displaystyle\\int_{0}^{1}\\sqrt{(\\dot{\\gamma}_{t}^{*})^{\\top}\\mathbf{G}(\\gamma_{t}^{*};\\mathcal{D})\\dot{\\gamma}_{t}^{*}}\\,d t}\\\\ {\\le\\sqrt{\\kappa}\\displaystyle\\frac{\\rho}{\\Gamma}\\int_{0}^{1}\\|\\dot{\\gamma}_{t}\\|\\,d t=\\sqrt{\\kappa}\\displaystyle\\frac{\\rho}{\\Gamma}\\Gamma=\\sqrt{\\kappa}\\rho,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have used the assumption (ii) on $g$ and that the length of $\\gamma_{t}$ is at most $\\Gamma$ . We can then combine the last inequality and eq. (19), and conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Len}_{g}(\\gamma_{t})\\leq\\sqrt{\\kappa}\\rho<\\mathrm{Len}_{g}(\\gamma_{t}^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which means that we have found a path connecting $x_{0}$ to $x_{1}$ , whose length with respect to the metric $g$ is shorter than the one of the geodesic $\\gamma_{t}^{*}$ from $x_{0}$ to $x_{1}$ . This is a contradiction and concludes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.1 Extending CFM to the manifold $(\\mathbb{R}^{d},g)$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this subsection, we demonstrate how to generalize the CFM objective in eq. (1) to the Riemannian manifold $(\\mathbb{R}^{d},g)$ , defined by a data-dependent metric $g$ as per Definition 1. ", "page_idx": 17}, {"type": "text", "text": "Densities on the manifold. To begin with, we extend the densities $p_{0},p_{1}$ and the joint density $q$ to valid densities on the manifold $(\\mathbb{R}^{\\tilde{d}},g)$ . First, we recall that the Riemannian volume form induced by $g$ can be written in coordinates as ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\mathrm{vol}(x)=\\sqrt{|\\mathbf{G}(x;\\mathcal{D})|}d x,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\big|\\cdot\\big|$ denotes the determinant of the matrix $\\mathbf{G}(x;\\mathcal{D})$ , while $d x$ is the standard Lebesgue measure on $\\mathbb{R}^{d}$ . Accordingly, given a density $p\\in\\mathbb{P}(\\mathbb{R}^{d})$ , we can derive the associated density $\\hat{p}\\in\\mathbb{P}(\\mathbb{R}^{d},g)$ by rescaling: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{p}(x):=\\frac{p(x)}{\\sqrt{|\\mathbf{G}(x;D)|}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which guarantees that $\\hat{p}$ is continuous, positive, and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\hat{p}(x)d\\mathrm{vol}(x)=\\int_{\\mathbb{R}^{d}}p(x)d x=1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This in particular applies to the marginals $p_{0},p_{1}$ given by our problem. A similar argument applies to the joint density $q$ whose marginals are $p_{0}$ and $p_{1}$ , respectively. In fact, we can now define a density $\\hat{q}$ on the product manifold, i.e. $\\hat{q}\\in\\mathbb{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d},g\\times g)$ , by simply taking ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{q}(x_{0},x_{1}):=\\frac{q(x_{0},x_{1})}{\\sqrt{|\\mathbf{G}(x_{0};\\mathcal{D})\\cdot\\mathbf{G}(x_{1};\\mathcal{D})|}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the denominator is exactly the pointwise volume form induced by the product metric $g\\times g$ . Therefore, the MFM objective in (7) is interpreted as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{MFM}}(\\theta)=\\mathbb{E}_{t,(x_{0},x_{1})\\sim\\hat{q}}\\left[\\|v_{t,\\theta}(x_{t,\\eta^{*}})-\\dot{x}_{t,\\eta^{*}}\\|_{g(x_{t,\\eta^{*}})}^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{t}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\|v_{t,\\theta}(x_{t,\\eta^{*}})-\\dot{x}_{t,\\eta^{*}}\\|_{g(x_{t,\\eta^{*}})}^{2}\\hat{q}(x_{0},x_{1})\\sqrt{|\\mathbf{G}(x_{0};\\mathcal{D})\\cdot\\mathbf{G}(x_{1};\\mathcal{D})|}d x_{0}d x_{1}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{t}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\|v_{t,\\theta}(x_{t,\\eta^{*}})-\\dot{x}_{t,\\eta^{*}}\\|_{g(x_{t,\\eta^{*}})}^{2}q(x_{0},x_{1})d x_{0}d x_{1}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left[\\|v_{t,\\theta}(x_{t,\\eta^{*}})-\\dot{x}_{t,\\eta^{*}}\\|_{g(x_{t,\\eta^{*}})}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is exactly eq. (7). We highlight that for this reason, we slightly abuse notation, and write an expectation with respect to $\\hat{q}$ , regarded as a density on the manifold $(\\mathbb{R}^{d},g)$ , the same as an expectation over the density $q$ with respect to Lebesgue measure. ", "page_idx": 18}, {"type": "text", "text": "The matching objective. We also emphasize that, similarly to Riemannian Flow Matching [Chen and Lipman, 2023], we normalize the regression objective by $\\mathbf{G}^{-1/2}(\\boldsymbol{x};\\mathcal{D})$ to avoid the need for initialization schemes that are specific to the metric. In fact, introducing a high-dimensional, nontrivial norm $\\|\\cdot\\|_{g}$ in the CFM objective could introduce instabilities in the optimization of CFM. As such, the objective in $\\mathcal{L}_{\\mathrm{MFM}}$ effectively reduces to ${\\mathcal{L}}_{\\mathrm{CFM}}$ with interpolants $x_{t,\\eta^{*}}$ minimizing the geodesic loss $\\mathcal{L}_{g}$ . Equivalently, METRIC FLOW MATCHING consists of a 2-step procedure: we first learn interpolants that approximate the geodesics of a data-dependent metric (Algorithm 1), and then we regress the vector field $v_{\\theta}$ using the interpolants from the first stage as per the standard CFM objective. A pseudocode for the MFM-pipeline is reported in Algorithm 2. ", "page_idx": 18}, {"type": "text", "text": "Inference. We finally note that, in principle, the differential equation generated by $v_{\\theta}$ is defined over the tangent bundle of $(\\mathbb{R}^{d},g)$ and hence solving it through Euler discretization, would require adopting the exponential map associated with the metric $g$ to project the tangent vector to the manifold. However, since the underlying space is still $\\mathbb{R}^{d}$ , the Euclidean Euler-step integration provides a firstorder approximation of the Riemannian exponential associated with $g$ (see for example Monera et al. [2014]). Accordingly, at inference, we can simply rely on the canonical Euler-discrete integration to approximate the exponential map associated with the data-dependent metric $g$ , provided that the step size is small. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 Pseudocode for METRIC FLOW MATCHING ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Require: coupling $q$ , initialized network $v_{t,\\theta}$ , trained network $\\varphi_{t,\\eta}$ , data-dependent metric $g(\\cdot)$ 1: while Training do ", "page_idx": 18}, {"type": "text", "text": "2: Sample $(x_{0},x_{1})\\sim q$ and $t\\sim\\mathcal{U}(0,1)$   \n3: $x_{t,\\eta}\\gets(1-t)x_{0}+t x_{1}+t(1-t)\\varphi_{t,\\eta}(x_{0},x_{1})$ \u25b7eq. (4)   \n4: $\\dot{x}_{t,\\eta}\\gets x_{1}-x_{0}+t(1-t)\\dot{\\varphi}_{t,\\eta}(x_{0},x_{1})+(1-2t)\\varphi_{t,\\eta}(x_{0},x_{1})$   \n5: \u2113(\u03b8) \u2190\u2225vt,\u03b8(xt,\u03b7) \u2212x\u02d9t,\u03b7\u2225g2(xt,\u03b7) \u25b7Estimate of objective ${\\mathcal{L}}_{\\mathrm{MFM}}(\\theta)$ from eq. (7) ", "page_idx": 18}, {"type": "text", "text": "B.1.1 Support of $p_{t}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conclude this section by justifying our claims about the relation of the support of the probability path $p_{t}$ and the interpolants. Consider a family of conditional paths $p_{t}(x|x_{0},x_{1})$ such that $p_{t}(x|x_{0},x_{1})\\approx\\delta(x{-}x_{t})$ , where $x_{t}$ are the interpolants connecting $x_{0}$ to $x_{1}$ , and $\\delta$ is the Dirac distribution. Assume that $x_{t}$ are straight interpolants, i.e. for any $(x_{0},x_{1})\\sim q$ , we define $x_{t}=t x_{1}\\!+\\!(1\\!-\\!t)x_{0}$ . Let us now consider $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ such that there is no $(x_{0},x_{1})\\sim q:\\;y=t x_{1}+(1-t)x_{0}$ . Accordingly: ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{t}(y):=\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}p_{t}(y|x_{0},x_{1})q(x_{0},x_{1})d x_{0}d x_{1}=\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\delta(y-x_{t})q(x_{0},x_{1})d x_{0}d x_{1}=0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since there is no $(x_{0},x_{1})$ in the support of $q$ , such that $y=x_{t}$ . We have then shown that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{supp}(p_{t})\\subset\\{y\\in\\mathbb{R}^{d}:\\exists\\,(x_{0},x_{1})\\sim q:\\;y=t x_{1}+(1-t)x_{0}\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can be limiting for tasks where the data induce a nonlinear geometry, as validated in Section 5. ", "page_idx": 18}, {"type": "text", "text": "C Additional Details on the Riemannian Metrics used ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this Section, we provide further details on the diagonal metrics introduced in Section 4, which we adopt in METRIC FLOW MATCHING. In particular, we comment on important differences between the LAND metric and the RBF metric. We finally derive an explicit connection between MFM and recent methods that learn interpolant minimizing generalized energies. ", "page_idx": 19}, {"type": "text", "text": "Learning the RBF metric $g_{\\mathrm{RBF}}$ . For the RBF metric in (10), we follow the metric design of Arvanitidis et al. [2021] and find the centroids by performing $\\boldsymbol{\\mathrm{k}}$ -means clustering. Similarly, we define the bandwidth $\\lambda_{k}$ associated with cluster $C_{k}$ as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{k}=\\frac{1}{2}\\left(\\frac{\\kappa}{|C_{k}|}\\sum_{x\\in C_{k}}\\|x-\\hat{x}_{k}\\|^{2}\\right)^{-2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\kappa$ is a tunable hyperparameter. We note that the bandwidth is chosen to assign smaller decay in (10) to the centroids of clusters that have high spread to better enable attraction of trajectories. Finally, the weights $\\omega_{\\alpha,k}$ are determined by training the loss function, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{RBF}}(\\{\\omega_{\\alpha,k}\\})=\\sum_{x_{i}\\in\\mathcal{D}}\\left(1-\\tilde{h}_{\\alpha}(x_{i})\\right)^{2}=\\sum_{x_{i}\\in\\mathcal{D}}\\bigg(1-\\sum_{k=1}^{K}\\omega_{\\alpha,k}\\exp\\Big(-\\frac{\\lambda_{\\alpha,k}}{2}\\|x_{i}-\\hat{x}_{k}\\|^{2}\\Big)\\bigg)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Two important comments are in order. First, by learning the RBF metric, the framework MFM effectively entails jointly learning a data-dependent Riemannian metric and a suitable matching objective along approximate geodesics $x_{t,\\eta}$ . Second, we note that more general training objectives can be adopted for $\\tilde{h}$ , to enforce specific properties for the metric that are task-aware. We reserve the exploration of this topic for future work. ", "page_idx": 19}, {"type": "text", "text": "LAND vs RBF metrics. While similar in spirit, since they both assign a lower cost to regions of space close to the support of the data points $\\mathcal{D}$ , the LAND metric [Arvanitidis et al., 2016] and the RBF metric [Arvanitidis et al., 2021] differ in two fundamental aspects. Crucially, RBF is learned based on the data points, requiring less tuning than LAND, which is a key advantage in high-dimensions and the main motivation for why we resort to RBF for experiments on images and single-cell data with more than 50 principal components. Additionally, the RBF metric assigns similar cost to regions with data and is, in principle, more robust than the LAND metric to variations in the concentration of samples in $\\mathcal{D}$ . While this is neither a benefit nor a downside in general, we observe that if a metric such as LAND consistently assigns much lower cost to regions of space with higher data concentration, then the geodesic objective in eq. (6) could always bias to learn interpolants $x_{t,\\eta}$ moving through these regions independent of the starting point\u2014this is a consequence of the fact that we never compute the actual length of the paths to avoid simulations. Note though that this has not been observed as an issue in practice whenever we adopted the LAND metric for low-dimensional data. ", "page_idx": 19}, {"type": "text", "text": "C.1 Connection between Riemmanian approach and data potentials ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We detail how the geometric loss (6) used to learn interpolants $x_{t,\\eta}$ can be recast as a generalization of the GSBM framework in Liu et al. [2024]. In general, for any data-dependent metric $g$ , we can indeed rewrite $\\mathcal{L}_{g}(\\eta)$ in eq. (6) as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{g}(\\eta)=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\big[\\langle\\dot{x}_{t,\\eta},\\mathbf{G}(x_{t,\\eta};\\mathcal{D})\\dot{x}_{t,\\eta}\\rangle\\big]}\\\\ &{\\qquad=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left[\\|\\dot{x}_{t,\\eta}\\|^{2}+\\langle\\dot{x}_{t,\\eta},(\\mathbf{G}(x_{t,\\eta};\\mathcal{D})-\\mathbf{I})\\dot{x}_{t,\\eta}\\rangle\\right]}\\\\ &{\\qquad=\\mathbb{E}_{t,(x_{0},x_{1})\\sim q}\\left[\\|\\dot{x}_{t,\\eta}\\|^{2}+V_{t,\\eta}(x_{t,\\eta},x_{0},x_{1})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the parametric potential $V_{t,\\eta}$ has the form ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t,\\eta}(x_{t,\\eta},x_{0},x_{1})=\\langle\\dot{x}_{t,\\eta},(\\mathbf G(x_{t,\\eta};\\mathcal D)-\\mathbf I)\\dot{x}_{t,\\eta}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\dot{x}_{t,\\eta}=x_{1}-x_{0}+t(1-t)\\dot{\\varphi}_{t,\\eta}(x_{0},x_{1})+(1-2t)\\varphi_{t,\\eta}(x_{0},x_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have used the parameterization of $x_{t,\\eta}$ in eq. (4). By replacing $\\mathbf{G}(x;\\mathcal{D})$ with the explicit expression given by the RBF metric in eq. (10), this provides a concrete formulation for eq. (14). We also note that an equivalent perspective amounts to replacing the parametric potential with a fixed function $\\mathcal{U}_{t}(x_{t,\\eta},\\dot{x}_{t,\\eta})$ that depends not only on $x_{t,\\eta}$ as the potentials in Liu et al. [2024], but also on the velocities $\\dot{x}_{t,\\eta}$ . Once again, we emphasize that our framework prescribes explicit parametric potentials $V_{t,\\eta}$ via the diagonal Riemannian metrics in Section 3.1 and hence differs from Liu et al. [2024] which leave to the user the task of designing potentials based on applications. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We parameterize the models $\\varphi_{t,\\eta}(x_{0},x_{1})$ in eq. (4) and $v_{t,\\theta}(x_{t})$ in eq. (7) as neural networks, and train them separately and sequentially. We start with $\\varphi_{t,\\eta}(x_{0},x_{1})$ , so that interpolants are learned prior to regressing $v_{t,\\theta}(x_{t})$ in the matching objective. Synthetic Arch, Sphere, LiDAR, and single-cell experiments were run on a single CPU. A single run of the LiDAR architecture and 5-dimensional single-cell experiments trains in under 10 minutes, while higher-dimensional single-cell experiments typically train in under an hour. The unpaired translation experiment on AFHQ was trained on a GPU cluster with NVIDIA A100 and V100 GPUs. Training time for AFHQ varies by GPU, ranging from 12 hours (A100) to 1 day (V100). ", "page_idx": 20}, {"type": "text", "text": "D.1 Synthetic Arch, Sphere and LiDAR experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the synthetic Arch, Sphere and LiDAR experiments, we parameterized both $\\varphi_{t,\\eta}(x_{0},x_{1})$ and $v_{t,\\theta}(x_{t})$ as 3-layer MLP networks with a width of 64 and SeLU activation. The networks were trained for up to 1000 epochs with early stopping (patience of 3 based on validation loss). We employed the Adam optimizer Kingma and Ba [2014] with a learning rate of 0.0001 for $\\varphi_{t,\\eta}(x_{0},x_{1})$ and the AdamW optimizer Loshchilov and Hutter [2017] with a learning rate of $10^{-3}$ and a weight decay of $10^{-5}$ for $v_{t,\\theta}(x_{t})$ . We used a $90\\%/10\\%$ train/validation split. Training samples served as source and target distributions and for calculating the LAND metric, while validation samples were used for early stopping. We used $\\sigma=0.125$ and $\\epsilon=0.001$ in the LAND metric. During inference, we solved for $p_{t}$ using the Euler integrator for 100 steps. ", "page_idx": 20}, {"type": "text", "text": "Synthetic Arch To generate the Arch dataset, we follow the experimental setup from [Tong et al., 2020]. We sampled 5000 points from two half Gaussians $\\textstyle N(0,{\\frac{1}{2\\pi}})$ and $N(1,{\\frac{1}{2\\pi}})$ . The exact optimal transport interpolant at $\\textstyle t={\\frac{1}{2}}$ was used as the test distribution. We then embedded the points on a half circle of radius 1 with noise $N(0,0.1)$ added to the radius. The Earth Mover\u2019s Distance between the sampled and test sets was averaged across five independent runs. ", "page_idx": 20}, {"type": "text", "text": "Synthetic Sphere To generate the Sphere dataset, we sampled 5,000 points. We sampled the latitudes from two half Gaussians $\\textstyle N(0,{\\frac{1}{2\\pi}})$ and $N(1,{\\frac{1}{2\\pi}})$ , and then scaled them by $\\pi$ . We sampled longitudes uniformly. The exact optimal transport interpolant at $\\begin{array}{r}{t=\\frac{1}{2}}\\end{array}$ was used as the test distribution. We then embedded the points on a sphere of radius 1 without any noise. The Earth Mover\u2019s Distance between the sampled and test sets, and the mean distance of the middle points of trajectories to the sphere (both reported in Appendix G) were averaged across five independent runs. ", "page_idx": 20}, {"type": "text", "text": "LiDAR For LiDAR, the data consists of point clouds within $[-5,5]^{3}\\subset\\mathbb{R}^{3}$ , representing scans of Mt. Rainier [Legg and Anderson, 2013]. The source and target distributions are generated as Gaussian Mixture Models and projected onto the LiDAR manifold as in Liu et al. [2024]. We then standardized all the LiDAR, source, and target points. ", "page_idx": 20}, {"type": "text", "text": "D.2 Unpaired translation in latent space ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the unpaired translation experiments, we utilized the U-Net architecture setup from Dhariwal and Nichol [2021] for both $\\varphi_{t,\\eta}(x_{0},x_{1})$ and $\\boldsymbol{v}_{t,\\theta}(\\boldsymbol{x}_{t})$ . The exact hyperparameters are reported in Table 5. We used the Adam optimizer Kingma and Ba [2014] for both networks and applied early stopping only for $\\varphi_{t,\\eta}(x_{0},x_{1})$ based on training loss. During inference, we solved for $p_{t}$ at $t=1$ using the adaptive step-size solver Tsit5 with 100 steps. We trained the RBF metric with $\\kappa=0.5$ and $\\epsilon=0.0001$ with k-means clustering, as described in Appendix C. For this experiment, we enforce stronger bending by using $(\\tilde{h}_{\\alpha}(x))^{\\bar{8}}$ , in the loss function $\\mathcal{L}_{\\mathrm{{g}_{\\mathrm{RBF}}}}(\\eta)$ in (11). Note that higher powers of $\\tilde{h}$ ensures that regions away from the support of $\\mathcal{D}$ , where $\\tilde{h}<1$ , are penalized even more in the geodesic objective, which highlights the role played by the biases introduced via the metric. ", "page_idx": 20}, {"type": "text", "text": "Our method operates in the latent space of the Stable Diffusion v1 VAE Kingma and Welling [2014], Rombach et al. [2022], except for the k-means clustering step in RBF metric pretraining, which was performed in the ambient space. ", "page_idx": 21}, {"type": "table", "img_path": "fE3RqiF4Nx/tmp/11a9205d838e9c76c4fce5fb2e2b7ad9096611fd8a87b46e955a0d90afb06d60.jpg", "table_caption": ["Table 5: U-Net architecture hyperparameters for unpaired image translation on AFHQ. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Dataset We used the Animal Face dataset from Choi et al. [2020], adhering to the splitting predefined by dataset authors for train and validation sets, with validation treated as the test set. Standard preprocessing was applied: upsizing to $313\\mathrm{x}256$ , center cropping to $256\\mathrm{x}256$ , resizing to 128x128, and using VAE encoders for preprocessing. Finally, we computed all embeddings using pretrained Stable Diffusion v1 VAE Rombach et al. [2022]. FID Heusel et al. [2017] and LPIPS Zhang et al. [2018] were computed using the validation sets. FID was measured with respect to the cat validation set, while LPIPS Zhang et al. [2018] between pairs of source dogs and generated cats. ", "page_idx": 21}, {"type": "text", "text": "D.3 Trajectory inference for single-cell data ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We performed both low-dimensional and high-dimensional single-cell experiments following the setups in Tong et al. [2023a,b]. For each experiment, the single-cell datasets were partitioned by excluding an intermediary timepoint, resulting in multiple subsets. Independent models were then trained on each subset. Test metrics were calculated on the left-out marginals and averaged across all model predictions. ", "page_idx": 21}, {"type": "text", "text": "We employed the Adam optimizer Kingma and Ba [2014] with a learning rate of $10^{-4}$ for $\\varphi_{t,\\eta}(x_{0},x_{1})$ and the AdamW optimizer Loshchilov and Hutter [2017] with a learning rate of $10^{-3}$ and a weight decay of $10^{-5}$ for $v_{t,\\theta}(x_{t})$ . During inference, we solved for $p_{t}$ at $t$ being left-out marginal using the Euler integrator for 100 steps. ", "page_idx": 21}, {"type": "text", "text": "We used a $90\\%/10\\%$ train/validation split, excluding left-out marginals from both sets. Training samples served as source and target distributions and for calculating the metrics, while validation samples were used for early stopping. We note that these settings are slightly more restrictive (and realistic) than those reported for $\\mathrm{\\bar{S}F^{2}M}.$ -Geo Tong et al. [2023a], where the left-out timepoint was also included in the validation set. ", "page_idx": 21}, {"type": "text", "text": "Embryoid Body dataset We used the Embryoid Body (EB) data Moon et al. [2019] preprocessed by Tong et al. [2020], focusing on the first five whitened dimensions. The dataset, consisting of five time points over 30 days, was used to train separate models across the full-time scale, each time leaving out one of the time points 1, 2, or 3. ", "page_idx": 21}, {"type": "text", "text": "Cite and Multi datasets We utilized the Cite and Multi datasets from the Multimodal Single-cell Integration Challenge at NeurIPS 2022 Lance et al. [2022], preprocessed by Tong et al. [2023a]. These datasets include single-cell measurements from $\\mathrm{CD4+}$ hematopoietic stem and progenitor cells for 1000 highly variable genes across four time points (days 2, 3, 4, and 7). We trained separate models each time leaving out one of the time points 3 or 4. The data was whitened only for 5-dimensional experiments. ", "page_idx": 21}, {"type": "text", "text": "Multiple constraints setting Following a similar approach to Neklyudov et al. [2023b], we modified our sampling procedure to interpolate between two intermediate dataset marginals, with neural network parameters $\\eta$ shared across timesteps. The interpolation is defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{t}=\\frac{t_{i+1}-t}{t_{i+1}-t_{i}}x_{t_{i}}+\\frac{t-t_{i}}{t_{i+1}-t_{i}}x_{t_{i+1}}+\\left(1-\\left(\\frac{t_{i+1}-t}{t_{i+1}-t_{i}}\\right)^{2}-\\left(\\frac{t-t_{i}}{t_{i+1}-t_{i}}\\right)^{2}\\right)\\varphi_{t,\\eta}(x_{0},x_{1}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Low-Dimensional experiments We parameterized both $\\varphi_{t,\\eta}(x_{0},x_{1})$ and $v_{t,\\theta}(x_{t})$ as 3-layer MLP networks with a width of 64 and SeLU activation. In the LAND metric, we used $\\sigma=0.125$ and $\\epsilon=0.001$ for all EB sets and the first leave-out time point in the Cite and Multi datasets. For the second leave-out time point in Cite and Multi, we used $\\sigma=0.25$ and $\\epsilon=0.001$ . ", "page_idx": 22}, {"type": "text", "text": "High-Dimensional experiments Both $\\varphi_{t,\\eta}(x_{0},x_{1})$ and $v_{t,\\theta}(x_{t})$ were parameterized as 3-layer MLP networks with a width of 1024 and SeLU activation. We set $\\kappa=1.5$ and $\\epsilon$ to be the complement of the final metric pretraining loss to maintain consistent regularization across datasets and leave-out timesteps. ", "page_idx": 22}, {"type": "text", "text": "Baselines We reproduced the results reported by Neklyudov et al. [2023b] to ensure consistent reporting of standard deviations and the same versions of EB dataset used across all experiments. The standard deviations were calculated across all leave-out timesteps and seeds for each dataset and dimension. We used the provided code and hyperparameters for training, averaging the results across 5 seeds. ", "page_idx": 22}, {"type": "text", "text": "E Supplementary Single-Cell Reconstruction ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We report additional results for single-cell reconstruction using 50 principal components in Table 6. Again, we note that OT-MFM performs strongly, and it is marginally surpassed only by $\\mathrm{SF^{2}}$ on Multi(50D) (results have not been reproduced). Two important remarks are in order. First, the baseline $\\mathrm{SF^{2}}$ M-Geo leverages a geodesic cost in the formulation of the Optimal Transport coupling, which enforces similar biases to OT-MFM. In fact, as argued in Section 4, OT-MFM can similarly consider data-aware costs in the formulation of the optimal transport coupling. In this work though, we wanted to focus on the ability of interpolants to lead to meaningful matching in settings where the optimal transport coupling is agnostic of the data support. Additionally, we highlight that as we move to more realistic high-dimensional settings (100 principal components, shown in Table 3) the advantages of our framework become even more apparent. ", "page_idx": 22}, {"type": "table", "img_path": "fE3RqiF4Nx/tmp/bf41faafcbd09d94d986c8a632ec14901415a6358c439732a1dca9a2548773a1.jpg", "table_caption": ["Table 6: Wasserstein-1 distance averaged over left-out marginals ( $\\downarrow$ better) for 50-dim PCA representation of single-cell data for corresponding datasets. Results are averaged over 5 independent runs. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "F Supplementary Unpaired Translation Results ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "fE3RqiF4Nx/tmp/3bb86b061e460145fd589000a7651eacbd4ec6d644e5a64b35467f0ad0d218f8.jpg", "img_caption": ["Figure 4: Additional qualitative comparison for the task of unpaired translation between OT-CFM and OT- $\\mathbf{MFM}_{\\mathrm{RBF}}$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "G Supplementary Sphere Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We visualize the problem setting in Figure 5. We report the Earth Mover\u2019s Distance between the sampled and test sets, as well as the mean distance of the middle points of trajectories to the sphere, to quantitatively compare CFM and MFM on the task. MFM improves significantly over the Euclidean baseline, CFM (Table 8). Furthermore, the samples generated by MFM at intermediate times are much closer to the underlying sphere than the Euclidean counterparts (Table 7). ", "page_idx": 24}, {"type": "text", "text": "Table 7: Mean Distance of reconstructed trajectories at time $1/2$ from the sphere. Results averaged over 5 runs. ", "page_idx": 24}, {"type": "table", "img_path": "fE3RqiF4Nx/tmp/3b7026241306cc1d3d6d8f56db7a552b8beb10c28e022c8bb3d74dd493e6c6c1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "fE3RqiF4Nx/tmp/3aeee7d526b8fdd8ecf039d771211d9c4b2677835ddf7f052ff797626f79fde8.jpg", "img_caption": ["Figure 5: Problem Setup "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 8: Wasserstein-1 distance between reconstructed marginal at time $1/2$ and ground-truth. Results averaged over 5 runs. ", "page_idx": 24}, {"type": "table", "img_path": "fE3RqiF4Nx/tmp/b3f2c8cfc0e905ace44a9527701eeb23eb2d35daaa623c91fba4bd53cd8843f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "H Supplementary LiDAR Visualizations ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "fE3RqiF4Nx/tmp/5d95b6edb99956af97a5e966860c409096040b05d7ddb5e6063450ecc847cfb7.jpg", "img_caption": ["Figure 6: Supplementary Visualizations of MFM Interpolants on LiDAR "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 25}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 25}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 25}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 25}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 25}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The abstract and introduction clearly state our claims, assumptions, and outline contributions (see the end of the Introduction). The claims are supported by theoretical and experimental results found in the main body and the provided appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The limitations of this work can be found in the final section of the main body. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All assumptions in theoretical results are clearly stated (both main body and appendix), and proofs are complete and correct. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The main section of experiments alongside the Experimental Details section in the Appendix clearly outlines all the technical information, hyperparameters, and hardware needed to reproduce the results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The anonymous repository provided contains the code, python environment requirements, a full set of hyperparameters for each result, and links to the datasets. The link to the repository can be found in the Experimental Details appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 27}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The experimental Details section in the Appendix clearly outlines all the data splits, hyperparameters, and optimizers used. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All single-cell and synthetic experiments have been independently run across 5 seeds, and standard deviations have been reported. For single-cell experiments, the standard deviation is measured both across seeds and leave-out timepoints. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes, such information can be found at the beginning of the Experimental Details appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The authors reviewed the NeurIPS Code of Ethics, and the research conducted in this paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Introduction outlines the context of the trajectory inference problem, and its potential positive impact to biomedical applications. We cannot foresee immediate negative societal impacts associated with our work. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Datasets used were already publicly available. We do not provide pretrained models with a high risk for misuse. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We cite and acknowledge any assets used, both in the paper and the repository provided. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets apart form the provided anonymous code with hyperparameters for reproducibility. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]