[{"type": "text", "text": "Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recent research has uncovered that federated learning (FL) systems are vulnera  \n2 ble to various security threats. Although various defense mechanisms have been   \n3 proposed, they are typically non-adaptive and tailored to specific types of attacks,   \n4 leaving them insufficient in the face of adaptive or mixed attacks. In this work,   \n5 we formulate adversarial federated learning as a Bayesian Stackelberg Markov   \n6 game (BSMG) to tackle poisoning attacks of unknown/uncertain types. We further   \n7 develop an efficient meta-learning approach to solve the game, which provides a   \n8 robust and adaptive FL defense. Theoretically, we show that our algorithm provably   \n9 converges to the first-order $\\varepsilon$ -equilibrium point in $O(\\varepsilon^{-2})$ gradient iterations with   \n10 $O(\\varepsilon^{-4})$ samples per iteration. Empirical results show that our meta-Stackelberg   \n11 framework obtains superb performance against strong model poisoning and back  \n12 door attacks with unknown/uncertain types. ", "page_idx": 0}, {"type": "text", "text": "13 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "14 Federated learning (FL) allows multiple devices with private data to jointly train a model without   \n15 sharing their local data [39]. However, FL systems are vulnerable to various adversarial attacks   \n16 such as untargeted model poisoning attacks (e.g., IPM [68], LMP [15]) and backdoor attacks (e.g.,   \n17 BFL [2], DBA [71]). To address these vulnerabilities, various robust aggregation rules such as   \n18 Krum [7], coordinate-wise trimmed mean [69], and FLTrust [10] have been proposed to defend against   \n19 untargeted attacks, and both training-stage and post-training defenses such as Norm bounding [57],   \n20 NeuroClip [62], and Prun [64] have been proposed to mitigate backdoor attacks. Further, dynamic   \n21 defenses that myopically adapt parameters such as learning rate [45], norm clipping threshold [21],   \n22 and regularizer [1] have been proposed. However, state-of-the-art defenses remain inadequate in   \n23 countering advanced adaptive attacks (e.g., the reinforcement learning (RL)-based attacks [31, 32])   \n24 that dynamically adjust the attack strategy to obtain long-term advantages. Further, current defenses   \n25 are typically designed to counter specific types of attacks, rendering them ineffective in the presence   \n26 of mixed attacks. As shown in Table 1 (Section 4), simply combining existing defenses with manual   \n27 tuning proves ineffective due to the interference between defense methods, the defender\u2019s lack of   \n28 information about adversaries, and the dynamic nature of FL.   \n29 In this work, we propose a meta-Stackelberg game (meta-SG) framework that obtains superb defense   \n30 performance even in the presence of strong adaptive attacks and a mix of attacks of the same or   \n31 different types (e.g., the coexistence of model poisoning and backdoor attacks). Our meta-SG defense   \n32 framework is built upon the following key observations. First, when the attack type (to be defined in   \n33 Section 2.1) is known as priori, the defender can utilize the limited amount of local data at the server   \n34 and publicly available information to build an approximate world model of the FL system. This   \n35 allows the defender to identify a robust defense policy offline by solving either a Markov decision   \n36 process (MDP) when the attack is non-adaptive or a Markov game when the attack is adaptive. This   \n37 approach naturally applies to both a single attack and the coexistence of multiple attacks and can   \n38 potentially produce a (nearly) optimal defense. Second, when the attacks are unknown or uncertain,   \n39 as in more realistic settings, the problem can be formulated as a Bayesian Stackelberg Markov game   \n40 (BSMG) [52], which provides a general model for adversarial FL. However, the standard solution   \n41 concept for BSMG, namely, the Bayesian Stackelberg equilibrium, targets the expected case and does   \n42 not adapt to the actual attack with an unknown/uncertain type.   \n43 Motivated by this limitation, we propose a novel solution concept called meta-Stackelberg equilibrium   \n44 (meta-SE) for BSMG as a principled way of developing robust and adaptive defenses for FL. By   \n45 integrating meta-learning and Stackelberg reasoning, meta-SE offers a computationally efficient   \n46 approach to address information asymmetry in adversarial FL and enables strategic adaptation in   \n47 online execution in the presence of multiple (adaptive) attackers. Before training an FL model,   \n48 a meta policy is learned by solving the BSMG using experiences sampled from a set of possible   \n49 attacks. When facing an actual attacker during FL training, the meta-policy is quickly adapted   \n50 using a relatively small number of samples collected on the fly. The proposed meta-SG framework   \n51 only requires a rough estimate of possible worst-case attacks during meta-training, thanks to the   \n52 generalization ability brought by meta-learning.   \n53 To solve the BSMG in the pre-training phase, we propose a meta-Stackelberg learning (meta-SL)   \n54 algorithm based on the debiased meta-reinforcement learning approach in [14]. The meta-SL   \n55 provably converges to the first-order $\\varepsilon$ -approximate meta-SE in $\\bar{O}(\\varepsilon^{-2})$ iterations, and the associated   \n56 sample complexity per iteration is of $O(\\varepsilon^{-4})$ . Even though meta- $\\mathrm{{SL}}$ achieves state-of-the-art sample   \n57 efficiency presented in [24], its operation involves the Hessian of the defender\u2019s value function. To   \n58 obtain a more practical solution (to bypass the Hessian computation), we further propose a fully   \n59 first-order pre-training algorithm, called Reptile meta-SL, inspired by Reptile [43]. Reptile meta-SL   \n60 only utilizes the first-order stochastic gradients from the attacker\u2019s and the defender\u2019s problem to   \n61 solve for the approximate equilibrium. The numerical results in Table 1 demonstrate its effectiveness   \n62 in handling various types of non-adaptive attacks, including mixed attacks , while Figure 2 and   \n63 Figure 9 highlight its efficiency in coping with uncertain or unknown attacks, including adaptive   \n64 attacks. Due to the space limit, we move related work section to Appendix A. Our contributions are   \n65 summarized as follows:   \n66 \u2022 We address critical security problems in FL in the face of attacks that may be adaptive or   \n67 mixed with multiple types.   \n68 \u2022 We develop a Bayesian Stackelberg game model (Section 2.2) to capture the information   \n69 asymmetry in the adversarial FL under multiple uncertain/unknown attacks.   \n70 \u2022 To create a strategically adaptable defense, we propose a new equilibrium concept: meta  \n71 Stackelberg equilibrium (meta-SE), where the defender (the leader) commits to a meta   \n72 policy and an adaptation strategy, leading to a data-driven approach to tackle information   \n73 asymmetry.   \n74 \u2022 To learn the meta equilibrium defense in the pre-training phase, we develop meta-Stackelberg   \n75 learning (Algorithm 1), an efficient first-order meta RL algorithm, which provably converges   \n76 to $\\varepsilon$ -approximate equilibrium in $O(\\varepsilon^{-2})$ gradient steps with $O(\\varepsilon^{-4})$ samples per iteration,   \n77 matching the state-of-the-art efficiency in stochastic bilevel optimization.   \n78 \u2022 We conduct extensive experiments in real-world settings to demonstrate the superb perfor  \n79 mance of our proposed method. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "80 2 Meta Stackelberg Defense Framework ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "81 2.1 Framework Overview ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "82 As shown in Figure 1, the meta-learning framework includes two stages: pre-training, online   \n83 adaptation. The pre-training stage is implemented in a simulated environment, which allows   \n84 sufficient training using trajectories generated from the interactions between the defender and the   \n85 attacker with its type randomly sampled from a set of potential attacks. Both adaptive and non  \n86 adaptive attacks could be considered for pre-training. After obtaining a meta-policy, the defender will   \n87 interact with the real FL environment in the online adaptation stage to tune its defense policy using   \n88 feedback (i.e., model updates and environment parameters) received in the face of real attacks that   \n89 are not necessarily in the pre-training attack set. Finally, at the last round of FL training, the defender   \n90 will perform a post-training defense on the global model, which may or may not be considered in the   \n91 design of intelligent attacks. Pre-training and online adaptation are indispensable in the proposed   \n92 framework. Table 5 in Appendix $\\mathrm{D}$ indicate that directly applying defense learned from pre-training   \n93 without online adaptation, as well as adaptation from a randomly initialized defense policy without   \n94 pre-training, both fail to address malicious attacks.   \n95 FL objective. Consider a learning system that includes one server and $n$ clients, each client possesses   \n96 its own private dataset $D_{i}\\,=\\,(\\bar{x_{i}^{j}},\\bar{y_{i}^{j}})_{j=1}^{|D_{i}|}$ where $|D_{i}|$ is the size of the dataset for the $i$ -th client.   \n97 Let $U=\\{D_{1},D_{2},...,D_{n}\\}$ denote the collection of all client datasets. The objective of federated   \n98 learning is to obtain a model $w$ that minimizes the average loss across all the devices: $\\operatorname*{min}_{w}F(w):=$   \n99 $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}f(w,D_{i})$ , where $\\begin{array}{r}{f(w,D_{i})\\;:=\\;\\frac{1}{|D_{i}|}\\sum_{j=1}^{|D_{i}|}\\ell(w,(x_{i}^{j},y_{i}^{j}))}\\end{array}$ is the local empirical loss with   \n100 $\\ell(\\cdot,\\cdot)$ being the loss function.   \n101 Attack objective. We consider two major categories of attacks: untargeted model poisoning attacks   \n102 and backdoor attacks. An untargeted model poisoning attack aims to maximize the average model loss,   \n103 i.e., $\\mathrm{min}_{w}-F(w)$ , while a targeted one strives to cause misclassification of poisoned test inputs to   \n104 one or more target labels (e.g., backdoor attacks). A malicious client $i$ employing targeted attack first   \n105 produces a poisoned dataset $D_{i}^{\\prime}$ by altering a subset of data samples $(x_{i}^{j},y_{i}^{j})\\in\\bar{D_{i}}$ to $\\bar{(x_{i}^{j},c^{*})}$ . Here ${\\hat{x}}_{i}^{j}$   \n106 is the tainted sample with a backdoor trigger inserted, and $c^{*}\\neq y_{i}^{j},c^{*}\\in C$ is the targeted label. Let   \n107 $\\rho_{i}=|D_{i}^{\\prime}|/|D_{i}|$ denote the poisoning ratio, which is typically unknown to the defender. To simplify   \n108 the notation, we assume that among the $M=M_{1}+M_{2}$ malicious clients, the first $M_{1}$ malicious   \n109 clients carry out a targeted attack, and the following $M_{2}$ malicious clients undertake an untargeted   \n110 attack. Note that clients in the same category may use different attack methods. Then, the joint   \n111 objective of these malicious clients is $\\begin{array}{r}{\\operatorname*{min}_{w}\\bar{F}^{\\prime}(w):=\\frac{1}{M_{1}}\\sum_{i=1}^{M_{1}}f(w,D_{i}^{\\prime})\\!-\\!\\frac{1}{M_{2}}\\sum_{i=M_{1}+1}^{M}f(w,D_{i})}\\end{array}$   \n112 FL process. At each round $t$ out of $H$ rounds of $\\mathrm{FL}$ training, the server randomly selects a subset of   \n113 clients $S^{t}$ and sends them the most recent global model $w_{g}^{t}$ . Every benign client $k\\in S^{t}$ updates the   \n114 model using their local data via one or more iterations of stochastic gradient descent and returns the   \n115 model update $g_{k}^{t}$ to the server. In contrast, an adversary $j\\in S^{t}$ creates a malicious model update   \n116 $\\widetilde{\\boldsymbol g}_{j}^{t}$ and sends it back. The server then collects the set of model updates $\\{\\widetilde{g}_{i}^{t}\\cup\\widetilde{g}_{j}^{t}\\cup g_{k}^{t}\\}_{i,j,k\\in S^{t}}$ , for   \n117 $i\\stackrel{*}{\\in}\\{1,\\dots,M_{1}\\},j\\in\\{M_{1}+1,\\dots,M\\},k\\in\\mathcal{S}^{t}\\backslash[M]$ , utilizes an aggregation rule Aggr to combine   \n118 them, and updates the global model: $\\overset{\\cdot}{w}_{g}^{t+1}=w_{g}^{t}-\\overset{\\cdot}{\\eta}^{t}A g g r(\\widetilde{g}_{i}^{t}\\cup\\widetilde{g}_{j}^{t}\\cup g_{k}^{t})$ , which is then sent to   \n119 clients in round $t+1$ . At the end of each round, the defender  will p erform a post-training defense   \n120 $h(\\cdot)$ on the global model $\\widehat{w}_{g}^{t}=h(w_{g}^{t})$ to evaluate the current defense performance. Only at the final   \n121 round $H$ or whenever a c li ent is leaving the $\\mathrm{FL}$ systems, the global model with post-training defense   \n122 $\\widehat{w}_{g}^{t}$ will be sent to all (leaving) clients.   \n123 Attack types. To simplify the exposition, we assume that a single mastermind attacker controls all   \n124 malicious clients within the FL system and employs diverse attack strategies on each controlled client.   \n125 We introduce the concept of attack type to differentiate various attack scenarios, which typically   \n126 include the following three aspects. The first aspect is the attack objective chosen by a malicious   \n127 client. Let $\\Omega_{1}$ be the set of all possible attack objectives from the defender\u2019s knowledge base. We set   \n128 $\\Omega_{1}=\\{$ {untargeted, targeted $\\}$ in this work. The second aspect specifies the attack method (i.e., the   \n129 algorithm used to generate the actual attack policy) adopted by a malicious client. Let $\\Omega_{2}$ be the set   \n130 of all possible attack methods from the defender\u2019s knowledge base. The third aspect captures the   \n131 configuration associated with an attack method, including its hyperparameters and other attributes   \n132 (e.g., triggers implanted in backdoor attacks, labels used in targeted attacks, and attacker\u2019s knowledge   \n133 about the FL system). Let $\\Omega_{3}$ denote the set of all possible configurations. For each malicious client   \n134 $i$ , the tuple $(\\omega_{1},\\omega_{2},\\omega_{3})_{i}$ where $\\omega_{k}\\,\\in\\,\\Omega_{k}$ for each $k$ fully specifies its particular attack type. Let   \n135 $\\xi\\,=\\,\\{(\\omega_{1},\\omega_{2},\\omega_{3})_{i}\\}_{i=1}^{M}$ be the joint attack type. Further, let $\\Xi=({\\Omega_{1}}^{\\star}\\times\\Omega_{2}\\times\\Omega_{3})^{M}$ denote the   \n136 domain of attacks that the defender is aware of. Table 2 in Appendix $\\mathbf{C}$ gives the types of all the   \n137 attacks considered in this work. However, the actual attack type encountered during $\\mathrm{FL}$ training is   \n138 not necessary in $\\Xi$ , although it is presumably similar to a known type in $\\Xi$ . ", "page_idx": 1}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/184f922e02a2607ca7035ca9b66a5f41325f2c75ca60d37602affa039ad1bc78.jpg", "img_caption": ["Figure 1: A graphical abstract of meta-Stackelberg defense. In the pertaining stage, a simulated environment is constructed using generated data and the attack domain. The defender utilizes metaStackelberg learning (Algorithm 1) to obtain the meta policy to be online adapted in the real FL. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "139 2.2 Pre-training as a Bayesian Stackelberg Markov game ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 From the discussion above, the global model updates and the final output are jointly influenced by the   \n141 defender (through aggregation) and the malicious clients (through corrupted gradients). Hence, the   \n142 FL process in an adversarial environment can be formulated as a two-player discrete time Bayesian   \n143 Stackelberg Markov game (BSMG) defined by a tuple $\\langle S,A_{\\mathcal{D}},A_{\\xi},\\mathcal{T},r,\\gamma,H\\rangle$ . Using discrete time   \n144 index $t$ (one step corresponds to one FL round), we have the following.   \n45 \u2022 $S$ is the state space, and its elements represent the global model at each round $s^{t}=w_{g}^{t}$ .   \n46 \u2022 $A_{\\mathcal{D}}$ is the defender\u2019s action set. Each action $a_{\\mathcal{D}}^{t}$ represents a combination of the robust   \n47 aggregation and post-training defenses: $a_{\\mathcal{D}}^{t}=\\{\\bar{A}g g r(\\cdot),h(\\cdot)\\}$ .   \n4498 \u2022 $A_{\\xi}$ liisc itohues  tcylpieen- $\\xi$ s :a $a_{\\mathcal{A}}^{t}=\\{\\widetilde{g}_{i}^{t}\\}_{i=1}^{M_{1}}\\cup\\{\\widetilde{g}_{i}^{t}\\}_{i=M_{1}+1}^{M}$ .on includes the joint model updates of all   \n50 \u2022 $\\tau(s^{t+1}|s^{t},A g g r(\\cdot),a_{\\mathcal{A}}^{t})$ specifies the distribution of the next state given the current state   \n51 atnd joint tactiotns at $t$ , which is determined by the global model update: $w_{g}^{t+1}\\,=\\,w_{g}^{t}\\,-$   \n52 $\\eta^{t}A g g r(\\widetilde{g}_{i}^{t}\\cup\\widetilde{g}_{j}^{t}\\cup g_{k}^{t})$ .   \n53 \u2022 $r_{\\mathcal{D}},r_{\\xi}$ are the defender\u2019s and the attacker\u2019s reward functions (to be maximized), respectively.   \n54 The defender aims to minimize the loss after the post-training: $r_{\\mathcal{D}}^{t}:=-F(\\widehat{w}_{g}^{\\bar{t}})$ where   \n55 $\\widehat{w}_{g}^{t}=h(w_{g}^{t})$ . The attacker\u2019s $r_{\\xi}^{t}$ is given by the joint attack objective: $-F^{\\prime}(\\widehat{w}_{g}^{t})$ .   \n156 Remark 2.1. The post-training defense is only applied in the final round or to a client leaving the   \n157 FL system and does not interfere with the model updates on $w_{g}^{t}$ . The defender\u2019s reward function is   \n158 crafted to encompass post-training, as we prioritize a practical, long-term average reward within an   \n159 online process, which enables clients to seamlessly join and depart from the FL system. This design   \n160 enables us to incorporate a post-training defense along with techniques for modifying the model   \n161 structure, such as drop-off and pruning.   \n162 Simulated environment in the white-box setting. With the game model defined above, the defender   \n163 (i.e., the server) can, in principle, identify a strong defense by solving the game (we discuss different   \n164 solution concepts in Section 3). Due to efficiency and privacy concerns in FL, however, it is often   \n165 infeasible to solve the game in real time when facing the actual attacker. Instead, the defender can   \n166 create a simulated environment to approximate the actual FL system during the pre-training stage.   \n167 The main challenge, however, is that the defender often lacks information about the individual devices   \n168 in FL. We first consider the white-box setting where the defender is aware of the number of malicious   \n169 devices in each category (i.e., $M_{1}$ and $M_{2}$ ) and their actual attack types, as well as the non-i.i.d. level   \n170 (to be defined in Section 4.1) of local data distributions across devices. However, it does not have   \n171 access to individual devices\u2019 local data and random seeds, making it difficult to simulate clients\u2019 local   \n172 training and evaluate rewards. To this end, we assume that the server has a small amount of root data   \n173 randomly sampled from the the collection of all client dataset $U$ as in previous work [10, 40]. We   \n174 then use generative model (e.g., conditional GAN model [41] for MNIST and diffusion model [55]   \n175 for CIFAR-10 in our experiments) to generate as much data as necessary to mimic the local training   \n176 (see details in Appendix C). We give an ablation study (Table 6) in Appendix D to evaluate the   \n177 influence of limited/biased root data. We remark that the purpose of pre-training is to derive a defense   \n178 policy rather than the model itself. Directly using the shifted data (root or generated) to train the FL   \n179 model will result in low model accuracy (see Table 5 in Appendix D).   \n180 Handling the black-box setting. We then consider the more realistic black-box setting, where   \n181 the defender has no access to the number of malicious devices and their actual attack types,   \n182 nor the non-i.i.d. level of local data distributions. To obtain a robust defense, we assume the   \n183 server considers the worst-case scenario based on a rough estimate of the missing information   \n184 (see our ablation study in the experiment section) and adopts the RL-based attacks to simulate   \n185 the worst-case attacks (see Section 3.1) when the attack is unknown or adaptive. In the face of   \n186 an unknown backdoor attack, the defender does not know the backdoor triggers and targeted la  \n187 bels. To simulate a backdoor attacker\u2019s behavior, we first implement multiple GAN-based attack   \n188 models as in [12] to generate worst-case triggers (which maximizes attack performance given the   \n189 backdoor objective) in the simulated environment. Since the defender does not know the poi  \n190 soning ratio $\\rho_{i}$ and the target label of the attacker\u2019s poisoned dataset (needed to determine the   \n191 attack objective $F^{\\prime}$ ), we approximate the attacker\u2019s reward function by $r_{\\mathcal{A}}^{t}=-F^{\\prime\\prime}(\\widehat{w}_{g}^{t+1})$ , where   \n192 $\\begin{array}{r}{F^{\\prime\\prime}(w)\\,:=\\,\\operatorname*{min}_{c\\in C}[\\frac{1}{M_{1}}\\sum_{i=1}^{M_{1}}\\frac{1}{|D_{i}^{\\prime}|}\\sum_{j=1}^{|D_{i}^{\\prime}|}\\ell(w,(\\hat{x}_{i}^{j},c))]-\\frac{1}{M_{2}}\\sum_{i=M_{1}+1}^{M}f(\\omega,D_{i}).}\\end{array}$ . $F^{\\prime\\prime}$ differs $F^{\\prime}$   \n193 only in the first $M_{1}$ clients, where we use a strong target label (that minimizes the expected loss) as a   \n194 surrogate to the true label $c^{*}$ . We compare the defense performance against white-box and black-box   \n195 backdoor attacks ( see Figure 10 in Appendix D). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "196 3 Meta Stackelberg Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "197 Since the pre-training is modeled by a Bayesian Markov Stackelberg game, solving the game   \n198 efficiently is crucial to a successful defense. This work\u2019s main contribution includes the formulation   \n199 of a new solution concept to the game, meta-Stackelberg equilibrium (meta-SE), and a learning   \n200 algorithm to approximate such equilibrium in finite time. To motivate the proposed concept, we begin   \n201 by addressing the defense against non-adaptive attacks.   \n202 Consider the attacker employing a non-adaptive attack of type $\\xi$ ; in other words, the attack action at   \n203 each iteration is determined by a fixed attack strategy $\\pi_{\\xi}$ , where $\\pi_{\\xi}(a)$ gives the probability of taken   \n204 action $a\\in A_{\\xi}$ , independent of the FL training and the defense strategy. In this case, BSMG reduces   \n205 to an MDP, where the transition kernel is $\\begin{array}{r}{\\mathcal{T}_{\\xi}(\\cdot|s,a_{\\mathcal{D}})\\triangleq\\int_{A_{\\xi}}\\mathcal{T}(\\cdot|s,a_{A},a_{\\mathcal{D}})d\\pi_{\\xi}(a_{A})}\\end{array}$ . Parameterizing   \n206 the defender\u2019s policy $\\pi_{D}\\big(a_{D}^{t}|s^{t};\\theta\\big)$ by a neural network with model weights $\\theta\\in\\Theta$ , the solution   \n207 to the following optimization problem $\\begin{array}{r}{\\operatorname*{max}_{\\theta\\in\\Theta}\\mathbb{E}_{a_{\\mathcal{D}}^{t}\\sim\\pi_{\\mathcal{D}},s^{t}\\sim\\mathcal{T}_{\\xi}}[\\sum_{t=1}^{H}\\gamma^{t}r_{\\mathcal{D}}^{t}]\\triangleq J_{\\mathcal{D}}(\\theta,\\xi)}\\end{array}$ gives the   \n208 optimal defense against the non-adaptive attack. When the actual attack in the online stage falls   \n209 within \u039e, which the defender is uncertain of, one can consider the defense against the expected attack:   \n210 max\u03b8 $\\mathbb{E}_{\\xi\\sim Q}J_{\\mathcal{D}}(\\theta,\\xi)$ , where $Q$ is a distribution over the attack domain to be designed by the defender.   \n211 One intuitive design is to include all reported attack methods in history as the attack domain and their   \n212 empirical frequency as the $Q$ distribution.   \n213 In stark contrast to non-adaptive attacks, an adaptive attack can adjust attack actions to the FL   \n214 environment and the defense mechanism [31, 32]. Most existing attacks are history-independent [50,   \n215 65]. Hence, we assume that an adaptive attack takes the current state (global model) as input, i.e., the   \n216 attack policy is a Markov policy denoted by $\\pi_{\\mathcal{A}}(a_{\\mathcal{A}}^{t}|s^{t})$ . Denoted by $\\xi$ the attack type; then, an optimal   \n217 adaptive attack policy, parameterized by $\\phi$ , is the best response to the existing defense $\\pi_{\\mathcal{D}}(\\cdot|s^{t};\\theta)$ :   \n218 $\\begin{array}{r}{\\phi\\in\\arg\\operatorname*{max}\\mathbb{E}_{a_{A}^{t}\\sim\\pi_{\\xi},a_{D}^{t}\\sim\\pi_{\\mathcal{D}}}[\\sum_{t=1}^{H}\\gamma^{t}r_{\\xi}^{t}]\\triangleq J_{A}(\\theta,\\phi,\\xi).}\\end{array}$ . Denote by $\\phi_{\\xi}^{*}$ the maximizer, and then, the   \n219 defender\u2019s cumulative rewards under such attack is $\\begin{array}{r}{J_{\\mathcal{D}}(\\theta,\\phi_{\\xi}^{*},\\xi)\\triangleq\\mathbb{E}_{a_{\\mathcal{A}}^{t}\\sim\\pi_{\\xi},a_{\\mathcal{D}}^{t}\\sim\\pi_{\\mathcal{D}}}[\\sum_{t=1}^{H}\\gamma^{t}r_{\\mathcal{D}}^{t}].}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "220 3.1 RL-based attacks and defenses ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "221 The actual attack type (which could be either adaptive or non-adaptive) encountered in the online   \n222 phase may be not in $\\Xi$ and thus unknown to the defender. To prepare for these unknown attacks,   \n223 we propose to use multiple RL-based attacks with different objectives, adapted from RL-based   \n224 untargeted model poising attack [31] and RL-based backdoor attack [32], as surrogates for unknown   \n225 attacks, which are added to the attack domain for pre-training. The rationale behind the RL surrogates   \n226 includes: (1) they achieve strong attack performance by optimizing long-term objectives; (2) they   \n227 adopt the most general action space (i.e., model updates), which allows them to mimic any adaptive   \n228 or non-adaptive attacks given the corresponding objectives; (3) they are flexible enough to incorporate   \n229 multiple attack methods by using RL to tune the hyper-parameters of a mixture of attacks. A similar   \n230 argument applies to RL-based defenses. We remark that in this paper, an RL-based attack (defense)   \n231 is not a single attack (defense) as in [31, 32] but a systematically synthesized combination of existing   \n232 attacks (defenses). In the simulated environment, we train our defense against the strongest white-box   \n233 RL attacks in [31, 32] with different objectives (e.g., untargeted or targeted), which is considered the   \n234 optimal attack strategy. The \u201cworst-case\u201d scenario is commonly used in security scenarios to ensure   \n235 the associated defense has performance guarantees under \u201cweaker\u201d attacks with similar objectives.   \n236 Such a robust defense policy gives us a good starting point to further adapt to uncertain or unknown   \n237 attacks. Our defense is generalizable to other adaptive attacks (see Table 8 in Appendix D). The key   \n238 novelty of our RL-based defense is that instead of using a fixed and hand-crafted algorithm as in   \n239 existing approaches, we use RL to optimize the policy network $\\pi_{D}\\big(a_{D}^{t}|s^{t};\\theta\\big)$ . Similar to RL-based   \n240 attacks, the most general action space could be the set of global model parameters. However, the   \n241 high dimensional action space will lead to an extremely large search space that is prohibitive in terms   \n242 of training time and memory space. Thus, we apply compression techniques (see Appendix C) to   \n243 reduce the action from high-dimensional space to a 3-dimensional space. Note that the execution   \n244 of our defense policy is lightweight, without using any extra data for evaluation/validation. See the   \n245 discussion in Appendix C on how we apply our RL-based defense during online adaptation. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "246 3.2 Meta-Stackelberg equilibrium ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "247 As discussed in Section 2.2, one of the key challenges to solving the BSMG is the defender\u2019s   \n248 incomplete information on attack types. Prior works have explored a Bayesian equilibrium approach   \n249 to address this issue [52]. Given the set of possible attacks $\\Xi$ that the defender is aware of and a   \n250 prior distribution $Q$ over the domain, the Bayesian Stackelberg equilibrium (BSE) is given by the   \n251 following bi-level optimization: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta}\\mathbb{E}_{\\xi\\sim Q}[J_{\\mathcal{D}}(\\theta,\\phi_{\\xi}^{*},\\xi)]\\quad\\mathrm{~s.t.~}\\phi_{\\xi}^{*}\\in\\arg\\operatorname*{max}J_{\\mathcal{A}}(\\theta,\\phi,\\xi).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "252 In (BSE), unaware of the exact attacker type, the defender (the leader) aims to maximize the defense   \n253 performance against an average of all attack types, anticipating their best responses.   \n254 From a game-theoretic viewpoint, the Bayesian equilibrium in (BSE) is of ex-ante. The defender   \n255 determines its equilibrium strategy only knowing the type distribution $Q$ . However, as the Markov   \n256 game proceeds, the attacker\u2019s moves (e.g., malicious global model updates) during the interim stage   \n257 (online stage) reveal additional information on the attacker\u2019s private type. This Bayesian equilibrium   \n258 defense strategy fails to handle the emerging information on the attacker\u2019s hidden type in the interim   \n259 stage, as the policy obtained from (BSE) remains fixed throughout the online stage without adaptation.   \n260 To address the limitation of Bayesian equilibrium, we introduce the novel solution concept, meta  \n261 Stackelberg equilibrium (meta-SE), to equip the defender with online responsive intelligence under   \n262 incomplete information. As a synthesis of meta-learning and Stackelberg equilibrium, the meta-SE   \n263 aims to pre-train a meta policy on a variety of attack types sampled from the attack domain $\\Xi$ such   \nthat online gradient adaption applied to the base produces a decent defense against the actual attack   \n265 in the online environment. Using mathematical terms, we denote by $\\tau_{\\xi}\\,:=\\,(s^{k},a_{\\mathcal{D}}^{k},a_{\\xi}^{k})_{k=1}^{H}$ the   \n266 trajectory of the FL system under type- $\\xi$ attacker up to round $H$ , which is subject to the distribution   \n267 $\\begin{array}{r}{q(\\theta,\\xi):=\\prod_{t=1}^{H}\\pi_{\\mathcal{D}}(a_{\\mathcal{D}}^{t}|s^{t};\\theta)\\pi_{\\xi}(a_{\\mathcal{A}}^{t}|s^{t})\\mathcal{T}(s^{t+1}|s^{t},a_{\\mathcal{D}}^{t},a_{\\mathcal{A}}^{t})}\\end{array}$ . Let $\\hat{\\nabla}_{\\boldsymbol{\\theta}}J_{\\mathcal{D}}(\\tau)$ be the unbiased estimate   \n268 of the policy gradient $\\nabla_{\\theta}J_{\\mathcal{D}}$ using the sample trajectory $\\tau_{\\xi}$ (see Appendix E). Then, a one-step   \n269 gradient adaptation using the sample trajectory is given by $\\bar{\\theta^{\\dag}}+\\eta\\nabla_{\\theta}J_{\\mathcal{D}}$ . Incorporating this gradient   \n270 adaptation into (BSE) leads to the proposed meta-SE. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta\\in\\Theta}{\\operatorname*{max}}\\mathbb{E}_{\\xi\\sim Q}\\mathbb{E}_{\\tau\\sim q}[J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi_{\\xi}^{*},\\xi)],}\\\\ &{\\mathrm{~s.t.~}\\phi_{\\xi}^{*}\\in\\arg\\operatorname*{max}\\mathbb{E}_{\\tau\\sim q}J_{A}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(meta-SE) ", "page_idx": 5}, {"type": "text", "text": "271 The idea of adding the gradient adaptation to the equilibrium is inspired by the recent developments   \n272 in gradient-based meta-learning [16, 43]. When the attack is non-adaptive, the BSMG reduces to   \n273 MDP problem, as delineated at the beginning of this section. Consequently, (meta-SE) turns into   \n274 the standard form of meta-learning [16]. Unlike the conventional (BSE), the solution to (meta-SE   \n275 gives the defender a decent defense initialization after pre-training whose gradient adaptation in the   \n276 online stage is tailored to type $\\xi$ , since the online trajectory follows the distribution $\\overline{{q(\\theta,\\xi)}}$ . The   \n277 novelty of (meta-SE) lies in that the leader (defender) determines an optimal adaptation scheme   \n278 rather than a policy, which is computed using an online trajectory without knowing the actual type,   \n279 creating a data-driven strategic adaptation after the pre-training. Besides equation BSE, Appendix G   \n280 also compares the perfect Bayesian equilibrium with the proposed meta-SE, highlighting the latter\u2019s   \n281 scalability to complex FL systems.   \n283 Unlike finite Stackelberg Markov games that   \n284 can be solved (approximately) using mixed  \n285 integer programming [59] or Q-learning [52],   \n286 our BSMG admits high-dimensional continu  \n287 ous state and action spaces, posing a more chal  \n288 lenging computation issue. Hence, we resort   \n289 to a two-timescale policy gradient (PG) algo  \n290 rithm, referred to as meta-Stackelberg learning   \n291 (meta-SL) presented in Algorithm 1, to solve   \n292 for the meta-SE in a similar vein to [33]. In   \n293 plain words, meta-SL first learns the attacker\u2019s   \n294 best response at a fast scale (lines 13-15), based   \n295 on which it updates the defender\u2019s meta pol  \n296 icy at a slow scale at each iteration using ei  \n297 ther debiased meta-learning [14] or reptile [43].   \n298 The two-timescale meta-SL alleviates the non  \n299 stationarity caused by concurrent policy updates   \n300 from both players [70]. Of particular note is   \n301 that the debiased meta-learning involves Hes  \n302 sian computation when evaluating the gradient   \n303 of the defender\u2019s objective function since the   \n304 attacker\u2019s best response $\\phi_{\\xi}^{\\ast}(\\theta)$ also depends on   \n305 $\\theta$ . In contrast, reptile uses a first-order approx  \n306 imation to avoid Hessian. The mathematical   \n307 subties between two policy gradient estimations   \n308 are deferred to the Appendix E.   \n309 The rest of this subsection addresses the com  \n310 putation expense of the proposed meta-SL. We begin with an alternative solution concept for   \n311 our first-order gradient algorithm, which is slightly weaker than (meta-SE). Let $\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi,\\xi)\\ \\triangleq$   \n312 $\\mathbb{E}_{\\tau\\sim q}J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)$ , $\\underline{{\\mathcal{L}_{A}}}(\\theta,\\phi,\\xi)\\triangleq\\mathbb{E}_{\\tau\\sim q}J_{A}(\\theta+\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)$ , for a fixed type $\\xi\\in\\Xi$ .   \n313 In the sequel, we will assume $\\mathcal{L}_{\\mathcal{D}}$ and $\\mathcal{L}_{\\mathcal{A}}$ to be continuously twice differentiable and Lipschitz  \n314 smooth with respect to both $\\theta$ and $\\phi$ as in [33], see Appendix F.   \n315 Definition 3.1. For $\\varepsilon\\in(0,1)$ , a pair $(\\theta^{\\ast},\\{\\phi_{\\xi}^{\\ast}\\}_{\\xi\\in\\Xi})\\in\\Theta\\times\\Phi^{\\mid\\Xi\\mid}$ is a $\\varepsilon$ -meta First-Order Stackelbeg   \n316 Equilibrium $\\bar{\\varepsilon}$ -meta-FOSE) of the meta-SG if it satisfies the following conditions: for $\\xi\\:\\in\\:\\Xi$ ,   \n317 $\\begin{array}{r}{\\operatorname*{max}_{\\theta\\in B(\\theta^{*})}\\langle\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}\\big(\\theta^{*},\\phi_{\\xi}^{*},\\xi\\big),\\theta-\\theta^{*}\\rangle\\,\\le\\,\\varepsilon.}\\end{array}$ , $\\begin{array}{r}{\\operatorname*{max}_{\\phi\\in B(\\phi_{\\xi}^{*})}\\langle\\nabla_{\\phi}\\mathcal{L}_{A}\\big(\\theta^{*},\\phi_{\\xi}^{*},\\xi\\big),\\phi-\\phi_{\\xi}^{*}\\rangle\\,\\le\\,\\varepsilon}\\end{array}$ , where   \n318 $B(\\theta^{*}):=\\{\\theta\\in\\Theta:\\|\\theta-\\theta^{*}\\|\\leq1\\}$ , and $B(\\phi_{\\xi}^{*}):=\\{\\phi\\in\\bar{\\Phi}:\\|\\phi-\\phi_{\\xi}^{*}\\|\\leq1\\}$ .   \n319 Definition 3.1 contains the necessary equilibrium condition for meta-SE in (meta-SE), which can be   \n320 reduced to $\\|\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}(\\theta^{*},\\phi_{\\xi},\\xi)\\|\\leq\\varepsilon$ and $\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta^{*},\\phi_{\\xi},\\xi)\\|\\leq\\varepsilon$ in the unconstraint settings. Since   \n321 we utilize stochastic gradient in practice, all inequalities mentioned above shall be considered in   \n322 expectation. The existence of meta-FOSE is guaranteed Theorem F.1 in Appendix F.   \n323 Since the value functions $J_{A},J_{D}$ are nonconvex, we impose a regularity assumption adapted from   \n324 the Polyak-\u0141ojasiewicz (PL) condition [26], which is customary in nonconvex analysis. Despite the   \n325 lack of theoretical justifications for the PL condition in the literature, [33] empirically demonstrates   \n326 that the cumulative rewards in meta-reinforcement learning satisfy the PL condition, see Figure 4   \n327 Appendix D therein. Assumption 3.2 subsequently leads to the main result in Theorem 3.3   \n328 Assumption 3.2 (Stackelberg Polyak-\u0141ojasiewicz condition). There exists a positive constant $\\mu$ such   \n329 that for any $(\\theta,\\phi)\\in\\Theta\\times\\Phi$ and $\\xi\\in\\Xi$ , the following inequalities hold: $\\begin{array}{r}{\\frac{\\dot{\\mathrm{\\scriptsize~1}}}{2\\mu}\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi,\\dot{\\xi})\\|^{2}\\geq}\\end{array}$   \n330 $\\begin{array}{r}{\\operatorname*{max}_{\\phi}\\mathcal{L}_{D}(\\theta,\\phi,\\xi)-\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi,\\xi),~\\frac{1}{2\\mu}\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)\\|^{2}\\geq\\operatorname*{max}_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)-\\mathcal{L}_{A}(\\theta,\\phi,\\xi)}\\end{array}$ .   \n331 Theorem 3.3. Under assumption 3.2 and other regularity assumptions in Appendix $F$ , for any   \n332 given $\\varepsilon\\in(0,1)$ , let the learning rates $\\kappa_{\\mathcal{A}}$ and $\\kappa_{\\mathcal{D}}$ be properly chosen; let $N_{A}\\sim\\mathcal{O}(\\log\\epsilon^{-1})$ and   \n333 $N_{b}\\sim\\mathcal{O}(\\epsilon^{-4})$ be properly chosen (Appendix $F_{.}$ ), then, Algorithm $^{\\,l}$ finds a $\\varepsilon$ -meta-FOSE within   \n334 $N_{\\mathcal{D}}\\sim O(\\varepsilon^{-2})$ iterations.   \n335 Finally, we conclude this section by analyzing the meta-SG defense\u2019s generalization ability when   \n336 the learned meta policy is exposed to attacks unseen in the pre-training. Proposition 3.4 asserts that   \n337 meta-SG is generalizable to the unseen attacks, given that the unseen is not distant from those seen.   \n338 The formal statement is deferred to Appendix F.   \n339 Proposition 3.4. Consider sampled attack types $\\xi_{1},\\allowbreak\\dots,\\allowbreak\\xi_{m}$ during the pre-training and the unseen   \n340 attack type $\\xi_{m+1}$ in the online stage. The generalization error is upper-bounded by the \u201cdiscrepancy\u201d   \n341 between the unseen and the seen attacks $\\displaystyle{\\cal C}(\\xi_{m+1},\\{\\xi_{i}\\}_{i=1}^{m})$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/fd165795ac0bb063e5b4385cb67d3236e84fb68eff689bf225c9ba5cd41a0b99.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "342 4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "343 4.1 Experiment Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "344 Dataset. Our experiments are conducted on MNIST [30] and CIFAR-10 [28] datasets with a CNN   \n345 classifier and ResNet-18 model respectively (see Appendix C for details). We consider horizontal FL   \n346 and adopt the approach introduced in [15] to measure the diversity of local data distributions among   \n347 clients. Let the dataset encompass $C$ classes, such as $C=10$ for datasets like MNIST and CIFAR-10.   \n348 Client devices are divided into $C$ groups (with $M$ attackers evenly distributed among these groups).   \n349 Each group is allocated $1/C$ of the training samples in the following manner: a training instance   \n350 labeled as $c$ is assigned to the $c$ -th group with a probability of $q\\geq1/C$ , while being assigned to   \n351 every other group with a probability of $\\bar{(1-q)}/\\bar{(C-1)}$ . Within each group, instances are evenly   \n352 distributed among clients. A higher value of $q$ signifies a greater non-i.i.d. level. By default, we   \n353 set $q\\,=\\,0.5$ as the standard non-i.i.d. level. We assume the server holds a small amount of root   \n354 data randomly sampled from the the collection of all client dataset $U$ . (100 for MNIST and 200 for   \n355 CIFAR-10).   \n356 Baseline. We evaluate our meta-RL and meta-SG defenses under the following untargeted model   \n357 poisoning attacks including IPM [68] (with scaling factor 2), LMP [15], RL [31], and backdoor   \n358 attacks including BFL [2] (with poisoning ratio 1), DBA [67] (with 4 sub-triggers evenly distributed   \n359 to malicious clients and poisoning ratio 0.5), BRL [32], and a mix of attacks from the two categories   \n360 (see Table 2 for all attacks\u2019 categories in Appendix C). We consider various strong defenses as   \n361 baselines, including training-stage defenses such as Coordinate-wise trimmed mean/median [69],   \n362 Norm bounding [57], FLTrust [10], Krum [7], and post-training stage defenses such as NeuroClip [62]   \n363 and Prun [64] and the selected combination of them. We utilize the Twin Delayed DDPG (TD3) [18]   \n364 algorithm to train both attacker\u2019s and defender\u2019s policies. We use the following default parameters:   \n365 number of devices $=100$ , number of malicious clients for untargeted model poisoning attack $=10$ ,   \n366 number of malicious clients for backdoor attack $=5$ (20 for DBA), client subsampling rate $=10\\%$ ,   \n367 number of FL epochs $=\\,500$ (1000) for MNIST (CIFAR-10). We fix the initial model and the   \n368 random seeds for client subsampling and local data sampling for fair comparisons. The details of the   \n369 experiment setup and additional results are provided in Appendices C and D.   \n371 Effectiveness against single/multiple type of attacks. We examine the defense performance of   \n372 our meta-RL compared with other defense combinations in Table 1 based on average global model   \n373 accuracy after $500\\,\\mathrm{{FL}}$ rounds on CIFAR-10, which measures the success of defense and learning   \n374 speed ignoring the randomness influence (corner-case updates, bias data, etc.) at the bargaining stage   \n375 of FL. The meta-RL first learns a meta-defense policy from the attack domain involving {NA, IPM,   \n376 LMP, BFL, DBA}, then adapts it to the real single/mixed attack. We observe that multiple types   \n377 of attacks may intervene with each other (e.g., $\\scriptstyle\\mathrm{IPM+BFL}$ , ${\\mathrm{LMP}}{+}{\\mathrm{DBA}}$ ), which makes it impossible   \n378 to manually address the entangled attacks. It is not surprising to see FedAvg [39] and defenses   \n379 specifically designed for untargeted attacks (i.e., Trimmed mean, FLTrust) fail to defend backdoor   \n380 attacks (i.e., BFL, DBA) due to the huge deviation of defense objective from the optimum. For   \n381 a fair comparison, we further manually tune the norm threshold (more results in Appendix D)   \n382 from $[0.01,0.02,0.05,0.1,0.2,0.5,1]$ for ClipMed (i.e., Norm bounding $^+$ Coordinate-wise Median)   \n383 and clipping range from $[2:2:10]$ for FLTrust $^+$ NeuroClip to achieve the best performance to   \n384 balance the global model and backdoor accuracy in linear form (i.e., Acc - Bac). Intuitively, a tight   \n385 threshold/range has better performance in defending against backdoor attacks, yet will hinder or even   \n386 damage the FL progress. On the other hand, a loose threshold/range fails to defend backdoor injection.   \n387 Nevertheless, manually tuning in real-world FL scenarios is nearly impossible due to the limited   \n388 knowledge of the ongoing environment and the presence of asymmetric adversarial information.   \n389 Instead of suffering from the above concerns and exponential growth of parameter combination   \n390 possibilities, our data-driven meta-RL approach can automatically tune multiple parameters at each   \n391 round. Targeting the cumulative defense rewards, the RL approach naturally holds more flexibility   \n392 than myopic optimization.   \n393 Adaptation to uncertain/unknown attacks. To evaluate the necessity and efficiency of adaptation   \n394 from the meta-SG policy in the face of unknown attacks, we plot the global model accuracy graph   \n395 over FL epochs. The meta-RL pre-trained from non-adaptive attack domain $\\{{\\mathrm{NA}}$ , IPM, LMP, BFL,   \n396 DBA} (RL attack is unknown), while meta-SG pre-train from interacting with a group of RL attacks   \n397 initially target on {FedAvg, Coordinate-wise Median, Norm bounding, Krum, FLTrust $\\}$ (LMP is   \n398 unknown). The meta-SG plus (i.e., meta- $S G+$ ) is a pre-trained model from the combined attack   \n399 domain of the above two. All three defenses then adapt to the real FL environments under LMP or RL   \n400 attacks. As shown in Figure 2, the meta-SG can quickly adapt to both uncertain RL-based adaptive   \n401 attack (attack action is time-varying during FL) and unknown LMP attack, while meta-RL can only   \n402 slowly adapt to or fail to adapt to the unseen RL-based adaptive attacks on MNIST and CIFAT-10   \n403 respectively. In addition, the first and the third Figures in Figure 2 demonstrate the power of meta-SG   \n404 against unknown LMP attacks, even if LMP is not directly used during its pre-training stage. The   \n405 results are only slightly worse than meta-SG plus, where LMP is seen during pre-training. Similar   \n406 observations are given under IPM in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/0daf35c9da266beccb4afa7889df116303bf2657b8dc0580738a42d522cc6ec1.jpg", "table_caption": ["370 4.2 Experiment Results "], "table_footnote": ["Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10. All parameters are set as default and random seeds are fixed. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/793bb770ac37c82347dfd10b0e370746dc93447d7309e15bff92869a200a8c6d.jpg", "img_caption": ["Figure 2: Comparisons of defenses against untargeted model poisoning attacks (i.e., LMP and RL) on MNIST and CIFAR-10. All parameters are set as default and random seeds are fixed. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "407 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "408 We have proposed a meta-Stackelberg framework to tackle attacks of uncertain or unknown types in   \n409 federated learning through data-driven adaptation. The proposed meta-Stackelberg learning approach   \n410 is computationally tractable and strategically adaptable, targeting mixed and adaptive attacks under   \n411 incomplete information. The major limitation of our current approach pertains to privacy concerns.   \n412 Our current simulation necessitates that the defender either accesses a small portion of root data or   \n413 learns clients\u2019 data through inversion, which slightly violates the privacy principles of FL. To minimize   \n414 privacy risks, we train our meta-policy in a simulated environment and apply data augmentation to   \n415 blur the learned data. In our experiments, the current \u201cblack-box\u201d setting operates under certain   \n416 conditions: we test only one or a few agnostic variables at a time while leaving other information   \n417 known to the defender (see Appendix D). In our future work, we plan to incorporate additional   \n418 state-of-the-art defense algorithms to counter more potent attacks, such as edge-case attacks [63], as   \n419 well as other attack types, such as privacy-leakage attacks [37]. We will also explore new application   \n420 scenarios, including NLP and large generative models. Our framework could be further improved by   \n421 including a client-side defense mechanism that closely mirrors real-world scenarios, replacing the   \n422 current processes of self-data generation. ", "page_idx": 8}, {"type": "text", "text": "423 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "424 [1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and   \n425 Venkatesh Saligrama. Federated learning based on dynamic regularization. In International   \n426 Conference on Learning Representations, 2020.   \n427 [2] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How   \n428 to backdoor federated learning. In International Conference on Artificial Intelligence and   \n429 Statistics, pages 2938\u20132948. PMLR, 2020.   \n430 [3] Pierre Bernhard and Alain Rapaport. On a theorem of Danskin with an application to a theorem   \n431 of Von Neumann-Sion. Nonlinear Analysis: Theory, Methods & Applications, 24(8):1163\u20131181,   \n432 1995.   \n433 [4] Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd   \n434 with majority vote is communication efficient and fault tolerant. In International Conference on   \n435 Learning Representations(ICLR), 2018.   \n436 [5] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing   \n437 federated learning through an adversarial lens. In International Conference on Machine Learn  \n438 ing(ICML), 2019.   \n439 [6] Umang Bhaskar, Yu Cheng, Young Kun Ko, and Chaitanya Swamy. Hardness results for   \n440 signaling in bayesian zero-sum and network routing games. In Proceedings of the 2016 ACM   \n441 Conference on Economics and Computation, pages 479\u2013496, 2016.   \n442 [7] Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries:   \n443 Byzantine tolerant gradient descent. In Advances in Neural Information Processing Sys  \n444 tems(NeurIPS), 2017.   \n445 [8] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir   \n446 Ivanov, Chlo\u00e9 Kiddon, Jakub Kone\u02c7cn\u00fd, Stefano Mazzocchi, Brendan McMahan, Timon   \n447 Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated   \n448 learning at scale: System design. In Proceedings of Machine Learning and Systems, 2019.   \n449 [9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,   \n450 and Wojciech Zaremba. Openai gym, 2016.   \n451 [10] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust   \n452 federated learning via trust bootstrapping. In Network and Distributed System Security (NDSS)   \n453 Symposium, 2021.   \n454 [11] Katherine Crowson. Trains a diffusion model on cifar-10 (version 2).   \n455 https://colab.research.google.com/drive/1IJkrrV-D7boSCLVKhi7t5docRYqORtm3, 2018.   \n456 [12] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust   \n457 backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer   \n458 Vision, pages 11966\u201311976, 2021.   \n459 [13] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2:   \n460 Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,   \n461 2016.   \n462 [14] Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence   \n463 theory of debiased model-agnostic meta-reinforcement learning, 2021.   \n464 [15] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to   \n465 byzantine-robust federated learning. In 29th USENIX Security Symposium, 2020.   \n466 [16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap  \n467 tation of deep networks. In International conference on machine learning, pages 1126\u20131135.   \n468 PMLR, 2017.   \n469 [17] Drew Fudenberg and Jean Tirole. Game Theory. MIT Press, Cambridge, MA, 1991.   \n470 [18] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error   \n471 in actor-critic methods. In International conference on machine learning, pages 1587\u20131596.   \n472 PMLR, 2018.   \n473 [19] Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and Michael Moeller. Inverting gradients  \n474 how easy is it to break privacy in federated learning? Advances in Neural Information Processing   \n475 Systems, 33:16937\u201316947, 2020.   \n476 [20] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A   \n477 client level perspective. arXiv preprint arXiv:1712.07557, 2017.   \n478 [21] Yifan Guo, Qianlong Wang, Tianxi Ji, Xufei Wang, and Pan Li. Resisting distributed backdoor   \n479 attacks in federated learning: A dynamic norm clipping approach. In 2021 IEEE International   \n480 Conference on Big Data (Big Data), pages 1172\u20131182. IEEE, 2021.   \n481 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im  \n482 age recognition. In Proceedings of the IEEE conference on computer vision and pattern   \n483 recognition(CVPR), 2016.   \n484 [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances   \n485 in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n486 [24] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and   \n487 enhanced design. In International conference on machine learning, pages 4882\u20134892. PMLR,   \n488 2021.   \n489 [25] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Ar  \n490 jun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings,   \n491 et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine   \n492 Learning, 14(1\u20132):1\u2013210, 2021.   \n493 [26] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal  \n494 gradient methods under the polyak-\u0142ojasiewicz condition. In Machine Learning and Knowledge   \n495 Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy,   \n496 September 19-23, 2016, Proceedings, Part I 16, pages 795\u2013811. Springer, 2016.   \n497 [27] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.   \n498 Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n499 [28] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.   \n500 2009.   \n501 [29] Artur Lacerda. Pytorch conditional gan. https://github.com/arturml/mnist-cgan, 2018.   \n502 [30] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning   \n503 applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n504 [31] Henger Li, Xiaolin Sun, and Zizhan Zheng. Learning to attack federated learning: A model  \n505 based reinforcement learning attack framework. In Advances in Neural Information Processing   \n506 Systems, 2022.   \n507 [32] Henger Li, Chen Wu, Senchun Zhu, and Zizhan Zheng. Learning to backdoor federated learning.   \n508 arXiv preprint arXiv:2303.03320, 2023.   \n509 [33] Tao Li, Haozhe Lei, and Quanyan Zhu. Sampling attacks on meta reinforcement learning: A   \n510 minimax formulation and complexity analysis. arXiv preprint arXiv:2208.00081, 2022.   \n511 [34] Tao Li, Guanze Peng, Quanyan Zhu, and Tamer Baar. The Confluence of Networks, Games,   \n512 and Learning a Game-Theoretic Framework for Multiagent Decision Making Over Networks.   \n513 IEEE Control Systems, 42(4):35\u201367, 2022.   \n514 [35] Tao Li, Yuhan Zhao, and Quanyan Zhu. The role of information structures in game-theoretic   \n515 multi-agent learning. Annual Reviews in Control, 53:296\u2013314, 2022.   \n516 [36] Tao Li and Quanyan Zhu. On the price of transparency: A comparison between overt persuasion   \n517 and covert signaling. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages   \n518 4267\u20134272, 2023.   \n519 [37] Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and S Yu   \n520 Philip. Privacy and robustness in federated learning: Attacks and defenses. IEEE transactions   \n521 on neural networks and learning systems, 2022.   \n522 [38] Mohammad Hossein Manshaei, Quanyan Zhu, Tansu Alpcan, Tamer Bac\u00b8sar, and Jean-Pierre   \n523 Hubaux. Game theory meets network security and privacy. ACM Comput. Surv., 45(3), jul 2013.   \n524 [39] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.   \n525 Communication-efficient learning of deep networks from decentralized data. In Artificial   \n526 intelligence and statistics (AISTATS), pages 1273\u20131282. PMLR, 2017.   \n527 [40] Yinbin Miao, Ziteng Liu, Hongwei Li, Kim-Kwang Raymond Choo, and Robert H Deng.   \n528 Privacy-preserving byzantine-robust federated learning via blockchain systems. IEEE Transac  \n529 tions on Information Forensics and Security, 17:2848\u20132861, 2022.   \n530 [41] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint   \n531 arXiv:1411.1784, 2014.   \n532 [42] Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen M\u00f6llering, Hossein   \n533 Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, et al.   \n534 Flame: Taming backdoors in federated learning. Cryptology ePrint Archive, 2021.   \n535 [43] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms.   \n536 arXiv preprint arXiv:1803.02999, 2018.   \n537 [44] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with   \n538 auxiliary classifier gans. In International conference on machine learning, pages 2642\u20132651.   \n539 PMLR, 2017.   \n540 [45] Mustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R Gel. Defending against backdoors in   \n541 federated learning with robust learning rate. In Proceedings of the AAAI Conference on Artificial   \n542 Intelligence, volume 35, pages 9268\u20139276, 2021.   \n543 [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,   \n544 Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative   \n545 style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.   \n546 [47] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated   \n547 learning. IEEE Transactions on Signal Processing, 70:1142\u20131154, 2022.   \n548 [48] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah   \n549 Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of   \n550 Machine Learning Research, 22(268):1\u20138, 2021.   \n551 [49] Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, and Ahmad-Reza Sadeghi. Deepsight:   \n552 Mitigating backdoor attacks in federated learning through deep model inspection. arXiv preprint   \n553 arXiv:2201.00763, 2022.   \n554 [50] Nuria Rodr\u00edguez-Barroso, Daniel Jim\u00e9nez-L\u00f3pez, M Victoria Luz\u00f3n, Francisco Herrera, and   \n555 Eugenio Mart\u00ednez-C\u00e1mara. Survey on federated learning threats: Concepts, taxonomy on   \n556 attacks and defences, experimental study and challenges. Information Fusion, 90:148\u2013173,   \n557 2023.   \n558 [51] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.   \n559 arXiv preprint arXiv:2202.00512, 2022.   \n560 [52] Sailik Sengupta and Subbarao Kambhampati. Multi-agent Reinforcement Learning in Bayesian   \n561 Stackelberg Markov Games for Adaptive Moving Target Defense. arXiv, 2020.   \n562 [53] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep   \n563 learning. Journal of big data, 6(1):1\u201348, 2019.   \n564 [54] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness   \n565 with principled adversarial training. In International Conference on Learning Representations,   \n566 2018.   \n567 [55] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper  \n568 vised learning using nonequilibrium thermodynamics. In International Conference on Machine   \n569 Learning, pages 2256\u20132265. PMLR, 2015.   \n570 [56] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv   \n571 preprint arXiv:2010.02502, 2020.   \n572 [57] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really   \n573 backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.   \n574 [58] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient   \n575 methods for reinforcement learning with function approximation. In Advances in Neural   \n576 Information Processing Systems 12, pages 1057\u20141063. MIT press, 2000.   \n577 [59] Yevgeniy Vorobeychik and Satinder Singh. Computing stackelberg equilibria in discounted   \n578 stochastic games. Proceedings of the AAAI Conference on Artificial Intelligence, 26(1):1478\u2013   \n579 1484, Sep. 2021.   \n580 [60] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and   \n581 Ben Y. Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.   \n582 In 2019 IEEE Symposium on Security and Privacy $(S P)$ , pages 707\u2013723, 2019.   \n583 [61] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and   \n584 Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.   \n585 In 2019 IEEE Symposium on Security and Privacy $(S P)$ , pages 707\u2013723. IEEE, 2019.   \n586 [62] Hang Wang, Zhen Xiang, David J Miller, and George Kesidis. Mm-bd: Post-training detection   \n587 of backdoor attacks with arbitrary backdoor pattern types using a maximum margin statistic. In   \n588 2024 IEEE Symposium on Security and Privacy $(S P)$ , pages 15\u201315. IEEE Computer Society,   \n589 2023.   \n590 [63] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal,   \n591 Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you   \n592 really can backdoor federated learning. Advances in Neural Information Processing Systems,   \n593 33:16070\u201316084, 2020.   \n594 [64] Chen Wu, Xian Yang, Sencun Zhu, and Prasenjit Mitra. Mitigating backdoor attacks in federated   \n595 learning. arXiv preprint arXiv:2011.01767, 2020.   \n596 [65] Geming Xia, Jian Chen, Chaodong Yu, and Jun Ma. Poisoning attacks in federated learning: A   \n597 survey. IEEE Access, 11:10708\u201310722, 2023.   \n598 [66] Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. Crfl: Certifiably robust federated   \n599 learning against backdoor attacks. In International Conference on Machine Learning, pages   \n600 11372\u201311382. PMLR, 2021.   \n601 [67] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against   \n602 federated learning. In International conference on learning representations, 2019.   \n603 [68] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine  \n604 tolerant sgd by inner product manipulation. In Uncertainty in Artificial Intelligence (UAI),   \n605 pages 261\u2013270. PMLR, 2020.   \n606 [69] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed   \n607 learning: Towards optimal statistical rates. In International Conference on Machine Learning,   \n608 pages 5650\u20135659. PMLR, 2018.   \n609 [70] Bora Yongacoglu, G\u00fcrdal Arslan, and Serdar Y\u00fcksel. Asynchronous Decentralized Q-Learning:   \n610 Two Timescale Analysis By Persistence. arXiv, 2023.   \n611 [71] Xianyang Zhang, Chen Hu, Bing He, and Zhiguo Han. Distributed reptile algorithm for meta  \n612 learning over multi-agent systems. IEEE Transactions on Signal Processing, 70:5443\u20135456,   \n613 2022.   \n614 [72] Chen Zhao, Yu Wen, Shuailou Li, Fucheng Liu, and Dan Meng. Federatedreverse: A detection   \n615 and defense method against backdoor attacks in federated learning. In Proceedings of the 2021   \n616 ACM Workshop on Information Hiding and Multimedia Security, IH&MMSec \u201921, page 51\u201362,   \n617 New York, NY, USA, 2021. Association for Computing Machinery. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "618 A Related Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "619 Poisoning/backdoor attacks and defenses in FL Several defensive strategies against model   \n620 poisoning attacks broadly fall into two categories. The first category includes robust-aggregation  \n621 based defenses encompassing techniques such as dimension-wise filtering. These methods treat   \n622 each dimension of local updates individually, as explored in studies by [4, 69]. Another strategy is   \n623 client-wise filtering, aiming to limit or entirely eliminate the influence of clients who might harbor   \n624 malicious intent. This approach has been examined in the works of [7, 47, 57]. Some defensive   \n625 methods necessitate the server having access to a minimal amount of root data, as detailed in the   \n626 study by [10]. Naive backdoor attacks are limited by even simple defenses like norm-bounding   \n627 [57] and weak differential private [20] defenses. Despite the sophisticated design of state-of-the-art   \n628 non-addaptive backdoor attacks against federated learning, post-training stage defenses [64, 42, 49]   \n629 can still effectively erase suspicious neurons/parameters in the backdoored model.   \n630 Incomplete Information in Adversarial Machine Learning Prior works have attempted to tackle   \n631 the challenge of incomplete information through two distinct approaches. The first approach is the   \n632 \u201cinfer-then-counter\u201d approach, where the hidden information regarding the attacks is first inferred   \n633 through observations. For example, one can infer the backdoor triggers through reverse engineering   \n634 using model weights [60], based on which the backdoor attacks can be mitigated [72]. The inference   \n635 helps adapt the defense to the present malicious attacks. However, this inference-based adaptation   \n636 requires prior knowledge of the potential attacks (i.e., backdoor attacks) and does not directly lend   \n637 itself to mixed/adaptive attacks. Moreover, the inference and adaptation are offilne, unable to counter   \n638 online adaptive backdoor attack [31]. The other approach explored the notion of robustness that   \n639 prepares the defender for the worst case [54, 52], which often leads to a Stackelberg game (SG)   \n640 between the defender and the attacker. Yet, such a Stackelberg approach often leads to conservative   \n641 defense, lacking adaptability. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "642 B Broader Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "643 Towards Universal Robust Federated Learning. Our goal is to establish a comprehensive frame  \n644 work for universal federated learning defense against all kinds of attacks. This framework ensures   \n645 that the server remains oblivious to any details pertaining to the environment or potential attackers.   \n646 Still, it possesses the ability to swiftly adapt and respond to uncertain or unknown attackers during   \n647 the actual federated learning process. Nevertheless, achieving this universal defense necessitates an   \n648 extensive attack set through pre-training, which often results in a protracted convergence time toward   \n649 a meta-policy. Moreover, the effectiveness and efficiency of generalizing from a wide range of diverse   \n650 distributions pose additional challenges. Considering these, we confine our experiments in this paper   \n651 to specifically address a subset of uncertainties and unknowns. This includes variables such as the   \n652 method of attacker, the number of attackers, the level of independence and identically distributed data,   \n653 backdoor triggers, backdoor targets, and other relevant aspects. However, we acknowledge that our   \n654 focus is not all-encompassing, and there may be other factors that remain unexplored in our research.   \n655 Meta Equilibrium and Information Asymmetry. Information asymmetry is a prevailing phe  \n656 nomenon arising in a variety of contexts, including adversarial machine learning (e.g. FL discussed in   \n657 this work), cyber security [38], and large-scale network systems [34]. Our proposed meta-equilibrium   \n658 offers a data-driven approach tackling asymmetric information structure in dynamic games without   \n659 Bayesian-posterior beliefs. Achieving the strategic adaptation through stochastic gradient descent,   \n660 the meta-equilibrium is computationally superior to perfect Bayesian equilibrium and better suited   \n661 for real-world engineering systems involving high-dimensional continuous parameter spaces. It is   \n662 expected that the meta-equilibrium can also be relevant to other adversarial learning contexts, cyber   \n663 defense, and decentralized network systems. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "664 C Experiment Setup ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "665 Datasets. We consider two datasets: MNIST [30] and CIFAR-10 [28], and default i.i.d. local data   \n666 distributions, where we randomly split each dataset into $n$ groups, each with the same number of   \n667 training samples. MNIST includes 60,000 training examples and 10, 000 testing examples, where   \n668 each example is a $28\\!\\times\\!28$ grayscale image, associated with a label from 10 classes. CIFAR-10 consists   \n669 of 60,000 color images in 10 classes of which there are 50, 000 training examples and 10,000 testing   \n670 examples. For the non-i.i.d. setting (see Figure 11(d)), we follow the method of [15] to quantify the   \n671 heterogeneity of the data. We split the workers into $C=10$ (for both MNIST and CIFAR-10) groups   \n672 and model the non-i.i.d. federated learning by assigning a training instance with label $c$ to the $c$ -th   \n673 group with probability $q$ and to all the groups with probability $1-q$ . A higher $q$ indicates a higher   \n674 level of heterogeneity.   \n675 Federated Learning Setting. We use the following default parameters for the FL environment:   \n676 local minibatch size $=128$ , local iteration number $=1$ , learning rate $=0.05$ , number of workers   \n677 $=100$ , number of backdoor attackers $=5$ , number of untargeted model poisoning attackers $=20$ ,   \n678 subsampling rate $=10\\%$ , and the number of FL training rounds $=500$ (resp. 1000) for MNIST (resp.   \n679 CIFAR-10). For MNIST, we train a neural network classifier of $8{\\times}8$ , $_{6\\times6}$ , and $5{\\times}5$ convolutional   \n680 filter layers with ReLU activations followed by a fully connected layer and softmax output. For   \n681 CIFAR-10, we use the ResNet-18 model [22]. We implement the FL model with PyTorch [46] and   \n682 run all the experiments on the same 2.30GHz Linux machine with 16GB NVIDIA Tesla P100 GPU.   \n683 We use the cross-entropy loss as the default loss function and stochastic gradient descent (SGD) as   \n684 the default optimizer. For all the experiments except Figures 11(c) and 11(d), we fix the initial model   \n685 and random seeds of subsampling for fair comparisons.   \n686 Baselines. We evaluate our defense method against various state-of-the-art attacks, including non  \n687 adaptive and adaptive untargeted model poison attacks (i.e., IPM [68], LMP [15], RL [31]), as well as   \n688 backdoor attacks (BFL [2] without model replacement, BRL [32], with tradeoff parameter $\\lambda=0.5$ ,   \n689 DBA [67] where each selected attacker randomly chooses a sub-trigger as shown in Figures 6, PGD   \n690 attack [63] with a projection norm of 0.05), and a combination of both types. To establish the   \n691 effectiveness of our defense, we compare it with several strong defense techniques. These baselines   \n692 include defenses implemented during the training stage, such as Krum [7], ClipMed [69, 57, 31] (with   \n693 norm bound 1), FLTrust [10] with 100 root data samples and bias $q=0.5$ , training stage CRFL [66]   \n694 with norm bound of 0.02 and noise level $1e-3$ as well as post-training defenses like NeuroClip [62]   \n695 and Prun [64]. We use the original clipping thresholds 7 in [62] and set the default Prun number to   \n696 256.   \n697 Reinforcement Learning Setting. In our RL-based defense, since both the action space and state   \n698 space are continuous, we choose the state-of-the-art Twin Delayed DDPG (TD3) [18] algorithm to   \n699 individually train the untargeted defense policy and the backdoor defense policy. We implement our   \n700 simulated environment with OpenAI Gym [9] and adopt OpenAI Stable Baseline3 [48] to implement   \n701 TD3. The RL training parameters are described as follows: the number of FL rounds $=300$ rounds,   \n702 policy learning rate $=0.001$ , the policy model is MultiInput Policy, batch size $=256$ , and $\\gamma=0.99$ for   \n703 updating the target networks. The default $\\lambda=0.5$ when calculating the backdoor rewards.   \n704 Meta-learning Setting. The attack domains (i.e., potential attack sets) are built as following: For   \n705 meta-RL, we consider IPM [68], LMP [15], EB [5] as three possible attack types. For meta-SG against   \n706 untargeted model poisoning attack, we consider RL-based attacks [31] trained against Krum [7] and   \n707 ClipMed [31, 69, 57] as initial attacks. For meta-SG against backdoor attack, we consider RL-based   \n708 backdoor attacks [32] trained against Norm-bounding [57] and NeuroClip [62] (Prun [64]) as initial   \n709 attacks. For meta-SG against mix type of attacks, we consider both RL-based attacks [31] and   \n710 RL-based backdoor attacks [32] described above as initial attacks.   \n711 At the pre-training stage, we set the number of iterations $T=100$ . In each iteration, we uniformly   \n712 sample $K=10$ attacks from the attack type domain (see Algorithm 2 and Algorithm 1). For each   \n713 attack, we generate a trajectory of length $H=200$ for MNIST $H=500$ for CIFAR-10), and update   \n714 both attacker\u2019s and defender\u2019s policies for 10 steps using TD3 (i.e., $l=N_{\\cal A}=N_{\\mathscr D}=10^{\\circ}$ ). At the   \n715 online adaptation stage, the meta-policy is adapted for 100 steps using TD3 with $T=10$ , $H=100$   \n716 for MNIST $H=200$ for CIFAR-10), and $l=10$ . Other parameters are described as follows: single   \n717 task step size $\\kappa=\\kappa_{A}=\\kappa_{\\mathcal{D}}=0.001$ , meta-optimization step size $=1$ , adaptation step size $=0.01$ .   \n718 Space Compression. Following the BSMG model, it is most generally to use $w_{g}^{t}$ or $(w_{g}^{t},\\mathbf{I}^{t})$ as   \n719 $\\{\\widetilde{g}_{k}^{t}\\}_{k=1}^{M_{1}+M_{2}}$ $w_{g}^{t+1}$   \n720 the federated le arning model is small. However, when we use federated learning to train a high  \n721 dimensional model (i.e., a large neural network), the original state/action space will lead to an   \n722 extremely large search space that is prohibitive in terms of training time and memory space. We   \n723 adopt the RL-based attack in [31] to simulate an adaptive model poisoning attack and the RL-based   \n724 local search in [32] to simulate an adaptive backdoor attack, both having a 3-dimensioanl real action   \n725 spaces after space comparison (see ). We further restrict all malicious devices controlled by the same   \n726 attacker to take the same action. To compress the state space, we reduce $w_{g}^{t}$ to only include its last   \n727 two hidden layers for both attacker and defender and reduce ${\\bf{I}}^{t}$ to the number of malicious clients   \n728 sampled at round $t$ .   \n729 Our approach rests on an RL-based synthesis of existing specialized defense methods against mixed   \n730 attacks, where multiple defenses can be selected at the same time and combined with dynamically   \n731 tuned hyperparameters. The following specialized defenses are selected in our implementation. For   \n732 training stage aggregation-based defenses, we first normalize the magnitude of all gradients to a   \n733 threshold $\\bar{\\alpha^{*}}\\in(\\bar{0_{,}}\\operatorname*{mix}_{i\\in\\mathcal{S}^{t}}\\{\\|g_{i}^{t}\\|\\}]$ , then apply coordinate-wise trimmed mean [69] with trimmed   \n734 rate $\\beta\\in[0,1)$ . For post-training defense, NeuroClip [62] with clip range $\\varepsilon$ or Prun [64] with mask   \n735 rate $\\sigma$ is applied. The concrete approach used in each of the above defenses can be replaced by other   \n736 defense methods. The key novelty of our approach is that instead of using a fixed and hand-crafted   \n737 algorithm as in existing approaches, we use RL to optimize the policy network $\\pi_{\\mathcal{D}}\\big(a_{\\mathcal{D}}^{t}|s^{t};\\boldsymbol{\\theta}\\big)$ . Similar   \n738 to RL-based attacks, the most general action space could be the set of global model parameters.   \n739 However, the high dimensional action space will lead to an extremely large search space that is   \n740 prohibitive in terms of training time and memory space. Thus, we apply reduce the action space to   \n741 $\\stackrel{\\triangledown}{a}_{\\mathcal{D}}^{t}:=(\\alpha^{t},\\beta^{t},\\varepsilon^{t}/\\sigma^{t})$ . Note that the execution of our defense policy is lightweight, without using   \n742 any extra data for evaluation/validation.   \n743 Self-generated Data. We begin by acknowledging that the server only holds a small amount of   \n744 initial data (200 samples with $q=0.1$ in this work) learned from first $20\\,\\mathrm{FL}$ rounds using inverting   \n745 gradient [19], to simulate training set with 60,000 images (for both MNIST and CIFAR-10) for FL.   \n746 This limited data is augmented using several techniques such as normalization, random rotation, and   \n747 color jittering to create a larger and more varied dataset, which will be used as an input for generative   \n748 models. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/82ac9fd7d1ff4f0f7e6230572f8af8c145dca5f6e13f80a9c3be977887454ad4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/24737851a051e6782d8e479cc110e066f6182d2df7790d3810224264fc31c4aa.jpg", "table_caption": ["Table 3: A table showcasing the attacks and defenses employed during pre-training and onlineadaptation, with links to the relevant figures or tables. RL and BRL are initially target on {FedAvg, ClipMed, Krum, FLTrust $\\mathbf{\\mathcal{+NC}}$ } during pre-training. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/f1e1edb79453bf71ce821daaf0d2ad0962b1918ce1c920fdeb0c0a4e01fb7c18.jpg", "img_caption": ["Figure 3: Self-generated MNIST images using conditional GAN [41] (second row) and CIFAR-10 images using a diffusion model [55] (fourth row). "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/48b7c2df9a3456baf2705cb4e063ae9793f68bcc0fa447f7f837e144ae9ddb03.jpg", "img_caption": ["Figure 4: Generated backdoor triggers using GAN-based models [12]. Original image (first row). Backdoor image (second row). Residual (third row). "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/9f8114e0c43310b152fb4f64069dd87e3616feeb79bf8b0771afd2ee67158b9b.jpg", "img_caption": ["Figure 5: MNIST backdoor trigger patterns. The global trigger is considered the default poison pattern and is used for backdoor accuracy evaluation. The sub-triggers are used by pre-training and DBA only. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/81ea659acd66fdb6f3e60dadbbfcfcd8b53b5ece61da9c8e1eb4a2c7af34964e.jpg", "img_caption": ["Figure 6: CIFAR-10 fixed backdoor trigger patterns. The global trigger is considered the default poison pattern and is used for online adaptation stage backdoor accuracy evaluation. The sub-triggers are used by pre-training and DBA only. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Noisy Denoised 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "749 For MNIST, we use the augmented dataset to train a Conditional Generative Adversarial Network   \n750 (cGAN) model [41, 44] built upon the codebase in [29]. The cGAN model for the MNIST dataset   \n751 comprises two main components - a generator and a discriminator, both of which are neural networks.   \n752 Specifically, we use a dataset with 5,000 augmented data as the input to train cGAN, keep the network   \n753 parameters as default, and set the training epoch as 100.   \n754 For CIFAR-10, we leverage a diffusion model implemented in [11] that integrates several recent   \n755 techniques, including a Denoising Diffusion Probabilistic Model (DDPM) [23], DDIM-style deter  \n756 ministic sampling [56], continuous timesteps parameterized by the log SNR at each timestep [27] to   \n757 enable different noise schedules during sampling. The model also employs the \u2018v\u2019 objective, derived   \n758 from Progressive Distillation for Fast Sampling of Diffusion Models [51], enhancing the conditioning   \n759 of denoised images at high noise levels. During the training process, we use a dataset with 50,000   \n760 augmented data samples as the input to train this model, keep the parameters as default, and set the   \n761 training epoch as 30.   \n762 Simulated Environment. To further improve efficiency and privacy, the defender simulate a smaller   \n763 FL system when solving the game. In our experiments, we include 10 clients in pre-training while   \n764 using 100 clients in the online FL system. The simulation relies on a smaller dataset (generated from   \n765 root data) and endures a shorter training time (100 (500) FL rounds for MINST (CIFAR-10) v.s. 1000   \n766 rounds in online FL experiments). Although the offline simulated Markov game deviates from the   \n767 ground truth, the learned meta-defense policy can quickly adapt to the real FL during the online   \n768 adaptation, as shown in our experiment section.   \n769 Backdoor Attacks. We consider the trigger patterns shown in Figure 4 and Figure 6 for backdoor   \n770 attacks. For triggers generated using GAN (see Figure 4), the goal is to classify all images of different   \n771 classes to the same target class (all-to-one). For fixed patterns (see Figure 6), the goal is to classify   \n772 images of the airplane class to the truck class (one-to-one). The default poisoning ratio is 0.5 in   \n773 both cases. The global trigger in Figure 6 is considered the default poison pattern and is used for the   \n774 online adaptation stage for backdoor accuracy evaluation. In practice, the defender (i.e., the server)   \n775 does not know the backdoor triggers and targeted labels. To simulate a backdoor attacker\u2019s behavior,   \n776 we first implement multiple GAN-based attack models as in [12] to generate worst-case triggers   \n777 (which maximizes attack performance given backdoor objective) in the simulated environment.   \n778 Since the defender does not know the poisoning ratio $\\rho_{i}$ and target label of the attacker\u2019s poisoned   \n779 dataset (involved in the attack objective $F^{\\prime}$ ), we approximate the attacker\u2019s reward function as $r_{A}^{t}=$   \n780 $\\begin{array}{r}{-F^{\\prime\\prime}(\\widehat{w}_{g}^{t+1}),F^{\\prime\\prime}(w):=\\operatorname*{min}_{c\\in\\mathcal{C}}[\\frac{1}{M_{1}}\\sum_{i=1}^{M_{1}}\\frac{1}{|D_{i}^{\\prime}|}\\sum_{j=1}^{|D_{i}^{\\prime}|}\\ell(w,(\\widehat{x}_{i}^{j},c))]-\\frac{1}{M_{2}}\\sum_{i=M_{1}+1}^{M}f(\\omega,D_{i}).\\overset{\\because}{F}_{\\mathrm{and}}^{\\prime}\\sum_{i=1}^{M_{1}+1}f(\\omega,D_{i}).}\\end{array}$ $F^{\\prime\\prime}$   \n781 differs $F^{\\prime}$ only in the first $M_{1}$ clients, where we use a strong target label (the minimizer) as a surrogate   \n782 to the true label $c^{*}$ .   \n783 Inverting Gradient/Reverse Engineering. In invert gradient, we set the step size for inverting   \n784 gradients $\\eta^{\\prime}=0.05$ , the total variation parameter $\\beta\\,=\\,0.02$ , optimizer as Adam, the number of   \n785 iterations for inverting gradients $m a x\\_i t e r=10,00\\$ 0, and learn the data distribution from scratch.   \n786 The number of steps for distribution learning is set to $\\tau_{E}=100$ . 32 images are reconstructed (i.e.,   \n787 $B^{\\prime}=32$ ) and denoised in each FL epoch. If no attacker is selected in the current epoch, the aggregate   \n788 gradient estimated from previous model updates is reused for reconstructing data. To build the   \n789 denoising autoencoder, a Gaussian noise sampled from $0.3\\mathcal{N}(0,1)$ is added to each dimension of   \n790 images in $D_{r e c o n s t r u c t e d}$ , which are then clipped to the range of [0,1] in each dimension. The result   \n791 is shown in Figure 7.   \n792 In the process of reverse engineering, we use Neural Cleanse [61] to find hidden triggers (See   \n793 Figure 8) connected to backdoor attacks. This method is essential for uncovering hidden triggers   \n794 and for preventing such attacks. In particular, we use the global model, root generated data and   \n795 inverted data as inputs to reverse backdoor triggers. The Neural Cleanse class from ART is used for   \n796 this purpose. The reverse engineering process in this context involves using the generated backdoor   \n797 method from the Neural Cleanse defense to find the trigger pattern that the model is sensitive to. The   \n798 returned pattern and mask can be visualized to understand the nature of the backdoor.   \n799 Online Adaptation and Execution. During the online adaptation stage, the defender starts by   \n800 using the meta-policy learned from the pre-training stage to interact with the true FL environment,   \n801 while collecting new samples $\\{s,a,\\widetilde{r},\\bar{s}^{\\prime}\\}$ . Here, the estimated reward $\\widetilde{r}$ is calculated using the   \n802 self-generated data and simulated triggers from the pertaining stage, as well as new data inferred   \n803 online through methods such as inverting gradient [19] and reverse engineering [61]. Inferred data   \n804 samples are blurred using data augmentation [53] while protecting clients\u2019 privacy. For a fixed   \n805 number of FL rounds (e.g., 50 for MNIST and 100 for CIFAR-10 in our experiments), the defense   \n806 policy will be updated using gradient ascents from the collected trajectories. Ideally, the defender\u2019s   \n807 adaptation time (including the time for collecting new samples and that for updating the policy)   \n808 should be significantly less than the whole FL training period so that the defense execution will not   \n809 be delayed. In real-world FL training, the server typically waits for up to 10 minutes before receiving   \n810 responses from the clients [8, 25], enabling defense policy\u2019s online update with enough episodes. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/0c2b55c0c4c3a693afbed942f0dc3e6d92a360144ed785214f9c1488141c8f99.jpg", "img_caption": ["Figure 8: Reversed MNIST backdoor trigger patterns. Original triggers (first row). Reversed triggers (second row) "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/e973df528bb7a2ef13f7c978f7fc2183438e69f46564319b49041f5a1cec51ab.jpg", "img_caption": ["Figure 9: Comparisons of defenses against untargeted model poisoning attacks (i.e., IPM and RL) on MNIST and CIFAR-10. RL-based attacks are trained before FL round 0 against the associate defenses (i.e., Krum and meta-policy of meta-RL/meta-SG). All parameters are set as default and all random seeds are fixed. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "811 D Additional Experiment Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "812 More untargetd model poisoning/backdoor results. As shown in Figure 9, similar to results   \n813 in Figure 2 as described in Section 4, meta-SG plus achieves the best performance (slightly better   \n814 than meta-SG) under IPM attacks for both MNIST and CIFAR-10. On the other hand, meta-SG   \n815 performs the best (significantly better than meta-RL) against RL-based attacks for both MNIST   \n816 and CIFAR-10. Notably, Krum can be easily compromised by RL-based attacks by a large margin.   \n817 In contrast, meta-RL gradually adapts to adaptive attacks, while meta-SG displays near-immunity   \n818 against RL-based attacks. In addition, we illustrate results under backdoor attacks and defenses on   \n819 MNIST in Table 4.   \n820 Defender\u2019s knowledge of backdoor attacks. We consider two settings: 1) the server knows the   \n821 backdoor trigger but is uncertain about the target label, and 2) the server knows the target label but   \n822 not the backdoor trigger. In the former case, the meta-SG first pre-trains the defense policy with RL   \n823 attacks using a known fixed global pattern (see Figure 6) targeting all 10 classes in CIFAR-10, then   \n824 adapts with an RL-based backdoor attack using the same trigger targeting class 0 (airplane), with   \n825 results shown in the third figure of Figure 10. In the latter case where the defender does not know the   \n826 true backdoor trigger used by the attacker, we implement the GAN-based model [12] to generate the   \n827 worst-case triggers (see Figure 4) targeting one known label (truck). The meta-SG will train a defense   \n828 policy with the RL-based backdoor attacks using the worst-case triggers targeting the known label,   \n829 then adapt with a RL-based backdoor attack using a fixed global pattern (see Figure 6) targeting the   \n830 known label in the real FL environment (results shown in the fourth graph in Figure 10. We call the   \n831 two above cases blackbox settings since the defender misses key backdoor information and solely   \n832 depends on their own generated data/triggers w/o inverting/reversing during online adaptation. In   \n833 the whitebox setting, the server knows the backdoor trigger pattern (global) and the targeted label   \n834 (truck), and is trained by true clients\u2019 data. The corresponding results are in the first two graphs of   \n835 Figures 10, which show the upper bound performance of meta-SG and may not be practical in a real   \n836 FL environment. Post-training defenses alone (i.e., NeuroClip and Prun) and combined defenses   \n837 (i.e., ClipMed and FLTrust+NC) are susceptible to RL-based attacks once the defense mechanism   \n838 is known. On the other hand, as depicted in Figure 10, we demonstrate that our whitebox meta-SG   \n839 approach is capable of effectively eliminating the backdoor influence while preserving high main   \n840 task accuracy simultaneously, while blackbox meta-SG against uncertain labels is unstable since   \n841 the meta-policy will occasionally target a wrong label, even with adaptation and blackbox meta-SG   \n842 against unknown trigger is not robust enough as its backdoor accuracy still reaches nearly $50\\%$ at the   \n843 end of FL training. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/18cf4592844b7ec925de6a8caf2fe3a79566411c3267da429624cdb451551ae3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/607946abe8f080cbca692c16f0381cde9aade7507b154629ef48f7b0b3b13649.jpg", "img_caption": ["Table 4: Comparisons of average backdoor accuracy (lower the better) after $250\\,\\mathrm{FL}$ rounds under backdoor attacks and defenses on MNIST. All parameters are set as default and all random seeds are fixed. ", "Figure 10: Comparisons of baseline defenses, i.e., NeuroClip, Prun, ClipMed, FLTrust+NeuroClip (from left to right) and whitebox/blackbox meta-SG under RL-based backdoor attack (BRL) on CIFAR-10. The BRLs are trained before FL round 0 against the associate defenses (i.e., NeuroClip, Prun, ClipMed, FLTrust+NC and meta-policy of meta-SG). Other parameters are set as default and all random seeds are fixed. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/ac56c728728100be86faea750a80196c65273771766727e1526d575c5d987773.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 5: Ablation studies of only using root data/generated dataset in simulated environment to learn the FL model and the defense performance under IPM of directly applying meta-policy learned from pre-training without adaptation/starting online adaptation from a randomly initialized defense policy. Results are average globel model accuracy after 250 (500) FL rounds on MNIST (CIFAR-10). All parameters are set as default and all random seeds are fixed.. ", "page_idx": 19}, {"type": "text", "text": "844 Importance of inverting/reversing methods. In the ablation study, we examine a practical and   \n845 relatively well-performed graybox meta-SG. The graybox meta-SG has the same setting as blackbox   \n846 meta-SG during pre-training as describe in Section 2.2, but utilizes inverting gradient [19] and reverse   \n847 engineering [61] during online adaptation to learn clients\u2019 data and backdoor trigger in a way without   \n848 breaking the privacy condition in FL. The graybox approach only learns ambiguous data from clients,   \n849 then applies data augmentation (e.g., noise, distortion) and combines them with previously generated   \n850 data before using. Figure 11(a) illustrates that graybox meta-SG exhibits a more stable and robust   \n851 mitigation of the backdoor attack compared to blackbox meta-SG. Furthermore, in Figure 11(b),   \n852 graybox meta-SG demonstrates a significant reduction in the impact of the backdoor attack, achieving   \n853 nearly a $70\\%$ mitigation, outperforming blackbox meta-SG.   \n854 Number of malicious clients/Non-i.i.d. level. Here we apply our meta-RL to study the impact of   \n855 inaccurate knowledge of the number of malicious clients and the non-i.i.d. level of clients\u2019 local data   \n856 distribution. With rough knowledge that the number of malicious clients is in the range of $5\\%{-}50\\%$ ,   \n857 the meta-SG will pre-train on LMP attacks with malicious clients $[5:5:50]$ , and adapt to three cases   \n858 with $20\\%$ , $30\\%$ , and $40\\%$ malicious clients in online adaptation, respectively. Similarly, when the   \n859 non-i.i.d. level is between 0.1-1, the meta-SG will pre-train on LMP attacks with non-i.i.d. level   \n860 $[0.1\\,:\\,0.1\\,:\\,1]$ and adapt to $\\mathrm{q}\\mathrm{=~}0.5,0.6,0.7$ in online adaptation. As illustrated in Figures 11(c)   \n861 and 11(d), meta-SG reaches the highest model accuracy for all numbers of malicious clients and   \n862 non-i.i.d. levels under LMP.   \n863 Importance of pre-training and online adaptation As shown in Table 5, the pre-training is to   \n864 derive defense policy rather than the model itself. Directly using those shifted data (root or generated)   \n865 to train the FL model will result in model accuracy as low as 0.2-0.3 (0.4-0.5) for CIFAR-10 (MNIST)   \n866 in our setting. Pre-training and online adaptation are indispensable in the proposed framework. Our   \n867 experiments in Table 5 indicate that directly applying defense learned from pre-training w/o online   \n868 adaptation and adaptation from randomly initialized defense policy w/o pre-training both fail to   \n869 address malicious attacks, resulting in global model accuracy as low as 0.3-0.6 (0.1-0.4) on MNIST   \n870 (CIFAR-10). In the absence of adaptation, meta policy itself falls short of the distribution shift   \n871 between the simulated and the real environment. Likewise, the online adaptation fails to attain the   \n872 desired defense policy without the pre-trained policy serving as a decent initialization.   \n873 Biased/Limited root data We evaluate the average model accuracy after $250\\,\\mathrm{FL}$ epochs under the   \n874 meta-SG framework against the IPM attack, using root data with varying i.i.d. levels (as defined in   \n875 the experiment setting section). Here, $\\mathrm{q}=0.1$ (indicating the root data is i.i.d.) serves as our baseline   \n876 meta-SG, as presented in the paper. We designate class 0 as the reference class. For instance, when q   \n877 $=0.4$ , it indicates a $40\\%$ probability for each data labeled as class 0 within the root data, while the   \n878 remaining $60\\%$ are distributed equally among the other classes. We observe that when q is as high   \n879 as 0.7, there is one class (i.e., 3) missing in the root data. Although, through inverting methods in   \n880 online adaptation, the defender can learn the missing data in the end, it suffered the slower adaptation   \n881 compared with a good initial defense policy. In addition, we test the average model accuracy after   \n882 $250\\,\\mathrm{FL}$ epochs under meta-SG against IPM attack using different numbers of root data (i.e., 100, 60,   \n883 20), where 100 root data is our original meta-SG setting in the rest of paper. We overserve that when   \n884 number of root data is 20, two classes of data are missing (i.e., 1 and 5).   \n885 Generalization to unseen adaptive attacks We thoroughly search related works considering   \n886 adaptive attacks in FL and find very limited works (with solid and lightweight open-source implemen  \n887 tation) that can be used as our benchmark. As a result, we introduce two new benchmark adaptive   \n888 attack methods in the testing stage as unseen adaptive attacks: (1) adaptive LMP![15], which requires   \n889 access to normal clients\u2019 updates in each FL round, and (2) RL attack [31] restricted 1-dimensional   \n890 action space (i.e., adaptive scalar factor) compared with the baseline 3-dimensional RL attack [31]   \n891 showing in our paper. The defender in pre-training only interacts with the 3-dimensional RL attack.   \n892 We test the average model accuracy after 250 FL epochs under meta-SG against different (unseen)   \n893 adaptive attacks. What is interesting here is that meta-SG can achieve even better performance against   \n894 unseen attacks. ", "page_idx": 19}, {"type": "image", "img_path": "UTwuDTpdNO/tmp/fc8b0af6b296495f3649706b00b3d61fa61e1fb7f7291adae00c0304362a4505.jpg", "img_caption": ["Figure 11: Ablation studies. (a)-(b): uncertain backdoor target and unknown backdoor triggers, where the meta-policies are trained by worst-case triggers generated from GAN-based models [12] or targeting multiple labels on CIFAR-10 during pre-training and utilizing inverting gradient [19] and reverse engineering [61] during online adaptation. (c)-(d): meta-RL tested by the number of malicious clients in $20\\%,30\\%,40\\%$ and non-i.i.d. level in $q\\bar{=}\\,[0.5,0.6,0.7]$ on MNIST compared with Krum and ClipMed under LMP attack. Other parameters are set as default. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/76c7fd1e74cc8afe0bebfe9c3bb0166886df9908a58d6b744c408da66d0966d4.jpg", "table_caption": ["(b) Ablation study of limited root data. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/4dd6b75e00e71fd6742afdf26efdd992a3fd2e88df9cf3b4abf43aa2184d13c2.jpg", "table_caption": ["Table 6: Results of the average model accuracy on MNIST after $250\\ \\mathrm{FL}$ epochs under meta-SG against IPM attack using root data with (a) different i.i.d levels and (b) different numbers of root data. All random seeds are fixed and all other parameters are set as default. "], "table_footnote": ["Table 7: Results of manually tuning norm threshold [57] and clipping range [62]. All other parameters are set as default and all random seeds are fixed. "], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "UTwuDTpdNO/tmp/e21681613e2f4f271c10caff0d8f5f8bf0b3d4debcfe489593cfeb86801b1dbd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 8: Comparisons of average model accuracy after 250 FL rounds under different adaptive attacks on MNIST. All parameters are set as default and all random seeds are fixed. ", "page_idx": 21}, {"type": "text", "text": "895 E Algorithms ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "896 This section elaborates on meta-learning defense and meta-Stackelberg defense in equation meta-SE.   \n897 To begin with, we first review the policy gradient method [58] in RL and its Monte-Carlo estimation.   \n898 To simplify our exposition, we fix the attacker\u2019s policy $\\phi$ , and then the Markov game reduces to a   \n899 single-agent MDP, where the optimal policy to be learned is the defender\u2019s $\\theta$ .   \n900 Policy Gradient The idea of the policy gradient method is to apply gradient ascent to the   \n901 value function $J_{D}$ . Following [58], we obtain $\\nabla_{\\theta}J_{\\mathcal{D}}~:=~\\mathbb{E}_{\\tau\\sim q(\\theta)}[g(\\tau;\\theta)]$ , where $g(\\tau;\\theta)\\;=\\;$   \n902 $\\begin{array}{r}{\\sum_{t=1}^{H}\\nabla_{\\theta}\\log\\pi(a_{\\mathcal{D}}^{t}|s^{t};\\theta)R(\\tau)}\\end{array}$ and $\\begin{array}{r}{R(\\tau)\\,=\\,\\sum_{t=1}^{H}\\gamma^{t}r(s^{t},a_{\\mathcal{D}}^{t})}\\end{array}$ . Note that for simplicity, we sup  \n903 press the parameter $\\phi,\\xi$ in the trajectory dis tribution $q$ , and instead view it as a function of . In   \n904 numerical implementations, the policy gradient $\\nabla_{\\theta}J_{\\mathcal{D}}$ is replaced by its Monte-Carlo (MC) estima  \n905 tion using sample trajectory. Suppose a batch of trajectories $\\{\\tau_{i}\\}_{i=1}^{N_{b}}$ , and $N_{b}$ denotes the batch size,   \n906 then the MC estimation is ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\nabla}_{\\boldsymbol{\\theta}}J_{\\mathcal{D}}(\\boldsymbol{\\theta},\\tau):=1/N_{b}\\sum_{\\tau_{i}}g(\\tau_{i};\\boldsymbol{\\theta}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "907 The same deduction also holds for the attacker\u2019s problem when fixing the defense $\\theta$ . ", "page_idx": 21}, {"type": "text", "text": "908 Meta-Learning FL Defense As discussed in Section 3, meta-learning-based defense (meta defense)   \n909 mainly targets non-adaptive attack methods, where $\\pi_{\\mathcal{A}}(\\cdot;\\phi,\\xi)$ is a pre-fixed attack strategy following   \n910 some rulebook, such as IPM [68] and LMP [15]. In this case, the BSMG reduces to single-agent MDP   \n911 for the defender, where the transition kernel is determined by the attack method. Mathematically, the   \n912 meta-defense problem is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta,\\Psi}\\mathbb{E}_{\\xi\\sim Q(\\cdot)}[J_{\\mathcal{D}}(\\Psi(\\theta,\\tau),\\phi,\\xi)].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "913 Since the attack type is hidden from the defender, the adaptation mapping $\\Psi$ is usually defined in a   \n914 data-driven manner. For example, $\\Psi(\\theta,\\tau)$ can be defined as a one-step stochastic gradient update   \n915 with learning rate $\\eta$ : $\\Psi(\\theta,\\tau)\\,=\\,\\theta+\\eta\\hat{\\nabla}J_{\\mathcal{D}}(\\tau_{\\xi})$ [16] or a recurrent neural network in [13]. This   \n916 work mainly focuses on gradient adaptation for the purpose of deriving theoretical guarantees in   \n917 Appendix F. ", "page_idx": 22}, {"type": "text", "text": "918 With the one-step gradient adaptation, the meta-defense problem in equation E2 can be simplified as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\xi\\sim Q(\\cdot)}\\mathbb{E}_{\\tau\\sim q(\\theta)}[J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "919 Recall that the attacker\u2019s strategy is pre-determined, $\\phi,\\xi$ can be viewed as fixed parameters, and   \n920 hence, the distribution $q$ is a function of $\\theta$ . To apply the policy gradient method to equation E3, one   \n921 needs an unbiased estimation of the gradient of the objective function in equation E3. Consider the   \n922 gradient computation using the chain rule: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\mathbb{E}_{\\tau\\sim q(\\theta)}[J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)]}\\\\ &{=\\mathbb{E}_{\\tau\\sim q(\\theta)}\\{\\underbrace{\\nabla_{\\theta}J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)(I+\\eta\\hat{\\nabla}_{\\theta}^{2}J_{D}(\\tau))}_{\\Phi}}\\\\ &{\\quad+\\underbrace{J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau))\\nabla_{\\theta}\\displaystyle\\sum_{t=1}^{H}\\pi(a^{t}|s^{t};\\theta)\\}}_{\\mathcal{D}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "923 The first term results from differentiating the integrand $J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)$ (the expectation is   \n924 taken as integration), while the second term is due to the differentiation of $q(\\theta)$ . One can see from   \n925 the first term that the above gradient involves a Hessian ${\\hat{\\nabla}}^{2}J_{\\mathcal{D}}$ , and its sample estimate is given by   \n926 the following. For more details on this Hessian estimation, we refer the reader to [14]. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\nabla}^{2}J_{\\mathcal{D}}(\\tau)=\\frac{1}{N_{b}}\\sum_{i=1}^{N_{b}}[g(\\tau_{i};\\boldsymbol{\\theta})\\nabla_{\\boldsymbol{\\theta}}\\log q(\\tau_{i};\\boldsymbol{\\theta})^{\\top}+\\nabla_{\\boldsymbol{\\theta}}g(\\tau_{i};\\boldsymbol{\\theta})]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "927 Finally, to complete the sample estimate of $\\nabla_{\\theta}\\mathbb{E}_{\\tau\\sim q(\\theta)}[J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)]$ , one still needs to   \n928 estimate $\\nabla_{\\theta}J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)$ in the first term. To this end, we need to first collect a batch   \n929 of sample trajectories $\\tau^{\\prime}$ using the adapted policy $\\theta^{\\prime}=\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{D}(\\tau)$ . Then, the policy gradient   \n930 estimate of $\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\theta^{\\prime})$ proceeds as in equation E1. To sum up, constructing an unbiased estimate of   \n931 equation E4 takes two rounds of sampling. The first round is under the meta policy $\\theta$ , which is used   \n932 to estimate the Hessian equation E5 and to adapt the policy to $\\theta^{\\prime}$ . The second round aims to estimate   \n933 the policy gradient $\\nabla_{\\theta}J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi,\\xi)$ in the first term in equation E4.   \n934 In the experiment, we employ a first-order meta-learning algorithm called Reptile [43] to avoid the   \n935 Hessian computation. The gist is to simply ignore the chain rule and update the policy using the   \n936 gradient $\\nabla_{\\theta}\\bar{J}_{\\mathcal{D}}(\\theta^{\\prime},\\phi,\\xi)\\vert_{\\theta^{\\prime}=\\bar{\\theta}+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau)}$ . Naturally, without the Hessian term, the gradient in this   \n937 update is biased, yet it still points to the ascent direction as argued in [43], leading to effective meta   \n938 policy. The advantage of Reptile is more evident in multi-step gradient adaptation. Consider a $l_{\\cdot}$ -step   \n939 gradient adaptation, the chain rule computation inevitably involves multiple Hessian terms (each   \n940 gradient step brings a Hessian term) as shown in [14]. In contrast, Reptile only requires first-order   \n941 information, and the meta-learning algorithm $l$ -step adaptation) is given by Algorithm 2.   \n942 Meta-Stackelberg Learning Recall that in meta-SE, the attacker\u2019s policy $\\phi_{\\xi}^{*}$ is not pre-fixed.   \n943 Instead, it is the best response to the defender\u2019s adapted policy as shown in equation meta-SE. To ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 Reptile Meta-Reinforcement Learning with $l$ -step adaptation ", "page_idx": 23}, {"type": "text", "text": "1: Input: the type distribution $Q(\\xi)$ , step size parameters $\\kappa,\\eta$   \n2: Output: $\\theta^{T}$   \n3: randomly initialize $\\theta^{0}$   \n4: for iteration $t=1$ to $T$ do   \n5: Sample a batch $\\hat{\\Xi}$ of $K$ attack types from $Q(\\xi)$ ;   \n6: for each $\\boldsymbol{\\xi}\\in\\hat{\\Xi}\\,\\mathbf{do}$   \n7: $\\theta_{\\xi}^{t}(0)\\gets\\theta^{t}$   \n8: for $k=0$ to $l-1$ do   \n9: Sample a batch trajectories $\\tau$ of the horizon length $H$ under $\\theta_{\\xi}^{t}(k)$ ;   \n10: Evaluate $\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\theta_{\\xi}^{t}(k),\\tau)$ using MC in equation E1;   \n11: $\\theta_{\\xi}^{t}(k+1)\\leftarrow\\theta_{\\xi}^{t}(k)+\\kappa\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\theta^{t},\\tau)$   \n12: end for   \n13: end for   \n14: Update $\\begin{array}{r}{\\theta^{t+1}\\leftarrow\\theta^{t}+1/K\\sum_{\\xi\\in\\hat{\\Xi}}(\\theta_{\\xi}^{t}(l)-\\theta^{t});}\\end{array}$   \n15: end for   \n944 obtain this best response, one needs alternative training: fixing the defense policy, and applying   \n945 gradient ascent to the attacker\u2019s problem until convergence. It should be noted that the proposed   \n946 meta-SL utilizes the unbiased gradient estimation in equation E5, which paves the way for theoretical   \n947 analysis in Appendix F. Yet, we turn to the Reptile to speed up pre-straining in the experiments. We   \n948 present both algorithms in Algorithm 3, and only consider one-step adaptation for simplicity. The   \nmulti-step version is a straightforward extension of Algorithm 3. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Algorithm 3 (Reptile) Meta-Stackelberg Learning with one-step adaptation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1: Input: the type distribution $Q(\\xi)$ , initial defense meta policy $\\theta^{0}$ , pre-trained attack policies   \n$\\{\\bar{\\phi_{\\xi}^{0}}\\}_{\\xi\\in\\Xi}$ , step size parameters $\\kappa_{\\mathcal{D}}$ , $\\kappa_{A},\\,\\eta$ , and iterations numbers $N_{A},N_{D}$ ;   \n2: Output: \u03b8ND   \n3: for iteration $t=0$ to $N_{D}-1$ do   \n4: Sample a batch $\\hat{\\Xi}$ of $K$ attack types from $Q(\\xi)$ ;   \n5: for each $\\boldsymbol{\\xi}\\in\\hat{\\Xi}\\,\\mathbf{do}$   \n6: Sample a batch of trajectories using $\\phi^{t}$ and $\\phi_{\\xi}^{t}$ ;   \n7: Evaluate $\\hat{\\nabla}_{\\theta}J_{D}(\\theta^{t},\\phi_{\\xi}^{t},\\xi)$ using equation E1;   \n8: Perform one-step adaptation $\\theta_{\\xi}^{t}\\gets\\theta^{t}+\\eta\\hat{\\nabla}_{\\theta}J_{D}(\\theta_{\\xi}^{t}(k),\\phi_{\\xi}^{t},\\xi)$ ;   \n9: $\\phi_{\\xi}^{t}(0)\\leftarrow\\phi_{\\xi}^{t}$ ;   \n10: for $k=0,\\bar{\\ldots},N_{A}-1$ do   \n11: Sample a batch of trajectories using $\\theta_{\\xi}^{t}$ and $\\phi_{\\xi}^{t}(k)$ ;   \n12: $\\phi_{\\xi}^{t}(k+1)\\leftarrow\\phi_{\\xi}^{t}(k)+\\kappa_{A}\\hat{\\nabla}_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\bar{\\phi}_{\\xi}^{t}(k),\\xi);$   \n13: end for   \n14: if Reptile then   \n15: Sample a batch of trajectories using $\\theta_{\\xi}^{t}$ and $\\phi_{\\xi}^{t}(N_{A})$ ;   \n16: Evaluate $\\begin{array}{r}{\\hat{\\nabla}J_{D}(\\xi):=\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\theta,\\phi_{\\xi}^{t}(N_{\\mathcal{A}}),\\xi)\\vert_{\\theta=\\theta_{\\xi}^{t}}}\\end{array}$ using equation E1;   \n17: else   \n18: Sample a batch of trajectories using $\\theta^{t}$ and $\\phi_{\\xi}^{t}(N_{A})$ ;   \n19: Evaluate the Hessian using equation E5;   \n20: Sample a batch of trajectories using $\\theta_{\\xi}^{t}$ and $\\phi_{\\xi}^{t}(N_{A})$ ;   \n21: Evaluate $\\hat{\\nabla}J_{D}(\\xi):=\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(\\bar{N_{A}}),\\xi)$ using equation E4;   \n22: end if   \n23: $\\bar{\\theta}_{\\xi}^{t}\\leftarrow\\theta^{t}+\\kappa_{\\mathcal{D}}\\hat{\\nabla}J_{D}(\\xi)$ ;   \n24: end for   \n25: $\\begin{array}{r}{\\theta^{t+1}\\leftarrow\\theta^{t}+1/K\\sum_{\\xi\\sim\\hat{\\Xi}}(\\bar{\\theta}_{\\xi}^{t}-\\theta_{t}),\\phi_{\\xi}^{t+1}\\leftarrow\\phi_{\\xi}^{t}(N_{\\cal A});}\\end{array}$   \n26: end for ", "page_idx": 23}, {"type": "text", "text": "949 F Theoretical Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "950 F.1 Existence of Meta-SG ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "951 Theorem F.1. Under the conditions that $\\Theta$ and $\\Phi$ are compact and convex, the meta-SG admits at   \n952 least one meta-FOSE.   \n953 Proof. Clearly, $\\Theta\\times\\Phi^{|\\Xi|}$ is compact and convex, let $\\phi\\,\\in\\,\\Phi^{|\\Xi|},\\phi_{\\xi}\\,\\in\\,\\Phi$ be the (type-aggregated)   \n954 attacker\u2019s strategy, since the consider twice continuously differentiable utility functions $\\ell_{\\cal D}(\\theta,\\phi):=$   \n955 $\\mathbb{E}_{\\xi\\sim Q}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi_{\\xi},\\xi)$ and $\\ell_{\\xi}(\\theta,\\phi):=\\mathcal{L}_{A}(\\theta,\\phi_{\\xi},\\xi)$ for all $\\xi\\in\\Xi$ . Then, there exists a constant $\\gamma_{c}>0$ ,   \n956 such that the auxiliary utility functions: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\ell}_{\\mathcal{D}}\\big(\\theta;(\\theta^{\\prime},\\phi^{\\prime})\\big)\\equiv\\ell_{\\mathcal{D}}\\big(\\theta,\\phi\\big)-\\frac{\\gamma_{c}}{2}\\|\\theta-\\theta^{\\prime}\\|^{2}}\\\\ &{\\widetilde{\\ell}_{\\xi}\\big(\\phi_{\\xi};(\\theta^{\\prime},\\phi^{\\prime})\\equiv\\ell_{\\xi}(\\theta^{\\prime},(\\phi_{\\xi},\\phi_{-\\xi}^{\\prime}))-\\frac{\\gamma_{c}}{2}\\|\\phi_{\\xi}-\\phi_{\\xi}^{\\prime}\\|^{2}\\quad\\forall\\xi\\in\\Xi}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "957 are $\\gamma_{c}$ -strongly concave in spaces $\\theta\\in\\Theta$ , $\\phi_{\\xi}\\in\\Phi$ for all $\\xi\\in\\Xi$ , respectively for any fixed $(\\theta^{\\prime},\\phi^{\\prime})\\in$   \n958 \u0398 \u00d7 \u03a6|\u039e|. ", "page_idx": 24}, {"type": "text", "text": "959 Define the self-map $h:\\Theta\\times\\Phi^{|\\Xi|}\\to\\Theta\\times\\Phi^{|\\Xi|}$ with $h(\\theta^{\\prime},\\phi^{\\prime})\\equiv(\\bar{\\theta}(\\theta^{\\prime},\\phi^{\\prime}),\\bar{\\phi}(\\theta^{\\prime},\\phi^{\\prime}))$ , where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bar{\\theta}(\\theta^{\\prime},\\phi^{\\prime})=\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{max}}\\,\\tilde{\\ell}_{\\mathcal{D}}(\\theta,\\phi^{\\prime}),\\qquad\\bar{\\phi}_{\\xi}(\\theta^{\\prime},\\phi^{\\prime})=\\underset{\\phi_{\\xi}\\in\\Phi}{\\arg\\operatorname*{max}}\\,\\tilde{\\ell}_{\\xi}(\\theta^{\\prime},(\\phi_{\\xi},\\phi_{-\\xi}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "960 Due to compactness, $h$ is well-defined. By strong concavity of $\\tilde{\\ell}_{\\mathcal{D}}(\\cdot;(\\theta^{\\prime},\\phi^{\\prime}))$ and $\\tilde{\\ell}_{\\xi}(\\cdot;(\\theta^{\\prime},\\phi^{\\prime}))$ , it   \n961 follows that $\\bar{\\theta},\\bar{\\phi}$ are continuous self-mapping from $\\Theta\\times\\Phi^{|\\Xi|}$ to itself. By Brouwer\u2019s fixed point   \n962 theorem, there exists at least one $(\\theta^{*},\\phi^{*})\\in\\Theta\\times\\Phi^{|\\Xi|}$ such that $h(\\theta^{\\ast},\\phi^{\\ast})=(\\theta^{\\ast},\\phi^{\\ast})$ . Then, one can   \n963 verify that $(\\theta^{*},\\phi^{*})$ is a meta-FOSE of the meta-SG with utility function $\\ell_{\\mathcal{D}}$ and $\\ell_{\\xi},\\xi\\in\\Xi$ , in view of   \n964 the following inequality ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\nabla_{\\theta}\\tilde{\\ell}_{\\mathcal{D}}(\\theta^{*};(\\theta^{*},\\phi^{*})),\\theta-\\theta^{*}\\rangle=\\langle\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta^{*},\\phi^{*}),\\theta-\\theta^{*}\\rangle}\\\\ &{\\langle\\nabla_{\\phi_{\\xi}}\\tilde{\\ell}_{\\xi}(\\theta^{*};(\\theta^{*},\\phi^{*})),\\phi_{\\xi}-\\phi_{\\xi}^{*}\\rangle=\\langle\\nabla_{\\phi_{\\xi}}\\ell_{\\xi}(\\theta^{*},\\phi^{*}),\\phi_{\\xi}-\\phi_{\\xi}^{*}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "965 therefore, the equilibrium conditions for meta-SG with utility functions $\\tilde{\\ell}_{\\mathcal{D}}$ and $\\{\\tilde{\\ell}_{\\xi}\\}_{\\xi\\in\\Xi}$ are the same   \n966 as with utility functions $\\ell_{\\mathcal{D}}$ and $\\{\\ell_{\\xi}\\}_{\\xi\\in\\Xi}$ , hence the claim follows. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "967 F.2 Proofs: Non-Asymptotic Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "968 In the sequel, we make the following smoothness assumptions for every attack type $\\xi\\,\\in\\,\\Xi$ . In   \n969 addition, we assume, for analytical simplicity, that all types of attackers are unconstrained, i.e., $\\Phi$ is   \n970 the Euclidean space with proper finite dimension.   \n971 Assumption F.2 ( $(\\xi$ -wise) Lipschitz smoothness). The functions $\\mathcal{L}_{\\mathcal{D}}$ and $\\mathcal{L}_{\\mathcal{A}}$ are continuously   \n972 diffrentiable in both $\\theta$ and $\\phi$ . Furthermore, there exists constants $L_{11}$ $\\phantom{}_{1},\\,L_{12},L_{21}$ , and $L_{22}$ such that   \n973 for all $\\theta,\\theta_{1},\\theta_{2}\\in\\Theta$ and $\\phi,\\phi_{1},\\phi_{2}\\in\\Phi$ , we have, for any $\\xi\\in\\Xi$ , ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta_{1},\\phi,\\xi\\right)-\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta_{2},\\phi,\\xi\\right)\\|\\leq L_{11}\\left\\|\\theta_{1}-\\theta_{2}\\right\\|}\\\\ &{\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta,\\phi_{1},\\xi\\right)-\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta,\\phi_{2},\\xi\\right)\\|\\leq L_{22}\\left\\|\\phi_{1}-\\phi_{2}\\right\\|}\\\\ &{\\|\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta,\\phi_{1},\\xi\\right)-\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta,\\phi_{2},\\xi\\right)\\|\\leq L_{12}\\left\\|\\phi_{1}-\\phi_{2}\\right\\|}\\\\ &{\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta_{1},\\phi,\\xi\\right)-\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}\\left(\\theta_{2},\\phi,\\xi\\right)\\|\\leq L_{12}\\left\\|\\theta_{1}-\\theta_{2}\\right\\|}\\\\ &{\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{A}}\\left(\\theta,\\phi_{1},\\xi\\right)-\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{A}}(\\theta,\\phi_{2},\\xi)\\|\\leq L_{21}\\|\\phi_{1}-\\phi_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "974 We also make the following strict-competitiveness assumption. This notion can be treated as a   \n975 generalization of zero-sum games: if one joint action $(a_{\\mathscr D},a_{\\mathscr A})$ leads to payoff increases for one   \n976 player, it must decrease the other\u2019s payoff.   \n977 Assumption F.3 (Strict-Competitiveness). The BSMG is strictly competitive, i.e., there exist con  \n978 stants $c<0$ , $d$ such that $\\forall\\xi\\in\\Xi$ , $s\\in S$ , $a_{\\mathcal{D}},a_{\\mathcal{A}}\\in A_{\\mathcal{D}}\\times A_{\\xi}$ , $r_{\\mathcal{D}}(s,a_{\\mathcal{D}},a_{\\mathcal{A}})=c r_{\\mathcal{A}}(s,a_{\\mathcal{D}},a_{\\mathcal{A}})+d$ .   \n979 In adversarial $\\mathrm{FL}$ , the untargeted attack naturally makes the game zero-sum (hence, SC). The purpose   \n980 of introducing Assumption F.3 is to establish the Danskin-type result [3] for the Stackelberg game   \n981 with nonconvex value functions (see Lemma F.5), which spares us from the Hessian inversion.   \n982 Lemma F.4 (Implicit Function Theorem (IFT) for Meta-SG). Suppose for $(\\bar{\\theta},\\bar{\\phi})\\;\\in\\;\\Theta\\times\\,\\Phi^{|\\Xi|}$ ,   \n983 $\\xi\\in\\Xi$ we have $\\dot{\\nabla}_{\\phi}\\mathcal{L}_{A}(\\bar{\\theta},\\bar{\\phi},\\xi)=0$ the Hessian $\\nabla_{\\phi}^{2}\\mathcal{L}_{A}(\\bar{\\theta},\\bar{\\phi},\\xi)$ is non-singular. Then, there exists   \n984 a neighborhood $B_{\\varepsilon}(\\bar{\\theta}),\\varepsilon>0$ centered around $\\bar{\\theta}$ and a $C^{1}$ -function $\\phi(\\cdot):B_{\\varepsilon}(\\bar{\\theta})\\rightarrow\\Phi^{|\\Xi|}$ such that   \n985 near $(\\bar{\\theta},\\bar{\\phi})$ the solution set $\\{(\\theta,\\phi)\\in\\Theta\\times\\Phi^{|\\Xi|}:\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)=0\\}$ is a $C^{1}$ -manifold locally near   \n986 $(\\bar{\\theta},\\bar{\\phi})$ . The gradient $\\nabla_{\\theta}\\phi(\\theta)$ is given by $-(\\nabla_{\\phi}^{2}\\mathcal{L}_{A}(\\theta,\\phi,\\xi))^{-1}\\nabla_{\\phi\\theta}^{2}\\mathcal{L}_{A}(\\theta,\\phi,\\xi).$ . ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Lemma F.5. Under assumptions $F.2$ , 3.2, there exists $\\{\\phi_{\\xi}:\\phi_{\\xi}\\in\\arg\\operatorname*{max}_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)\\}_{\\xi\\in\\Xi}$ , such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}V(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{\\xi\\sim Q,\\tau\\sim q}J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi_{\\xi},\\xi).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "987 Moreover, the function V (\u03b8) is L-Lipschitz-smooth, where L = L11 + L12\u00b5L21 ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lVert\\nabla_{\\theta}V(\\theta_{1})-\\nabla_{\\theta}V(\\theta_{2})\\rVert\\leq L\\lVert\\theta_{1}-\\theta_{2}\\rVert.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "988 Proof of Lemma F.5. First, we show that for any $\\theta_{1},\\theta_{2}\\quad\\in\\quad\\Theta,\\xi\\quad\\in\\quad\\Xi,$ , and $\\phi_{1}\\quad\\in$   \n989 arg $\\operatorname*{max}_{\\phi}\\mathcal{L}_{A}(\\theta_{1},\\phi,\\xi)$ , there exists $\\phi_{2}\\in\\arg\\operatorname*{max}_{\\phi}\\mathcal{L}_{A}(\\theta_{2},\\phi,\\xi)$ such that $\\begin{array}{r}{\\|\\phi_{1}-\\phi_{2}\\|\\leq\\frac{L_{12}}{\\mu}\\|\\theta_{1}-}\\end{array}$   \n990 $\\theta_{2}\\|$ . Indeed, based on smoothness assumption equation F11 and equation F10, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta_{1},\\phi_{1},\\xi)-\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta_{2},\\phi_{1},\\xi)\\|\\leq L_{21}\\|\\theta_{1}-\\theta_{2}\\|,}\\\\ &{\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta_{1},\\phi_{1},\\xi)-\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta_{2},\\phi_{1},\\xi)\\|\\leq L_{12}\\|\\theta_{1}-\\theta_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "991 Since $\\begin{array}{r}{\\phi_{2}\\in\\arg\\operatorname*{max}_{\\phi}\\mathcal{L}_{A}(\\theta_{2},\\phi,\\xi),\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta_{2},\\phi_{2},\\xi)=0}\\end{array}$ . Apply $\\mathrm{PL}$ condition to $\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi_{2},\\xi)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\phi}\\mathcal{L}_{A}(\\theta_{1},\\phi,\\xi)-\\mathcal{L}_{A}(\\theta_{1},\\phi_{2},\\xi)\\leq\\displaystyle\\frac{1}{2\\mu}\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta_{1},\\phi_{2},\\xi)\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{2\\mu}\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta_{1},\\phi_{2},\\xi)-\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta_{2},\\phi_{2},\\xi)\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{L_{21}^{2}}{2\\mu}\\|\\theta_{1}-\\theta_{2}\\|^{2}\\qquad\\mathrm{by~equation~F11}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "992 Since PL condition implies quadratic growth, we also have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A}(\\theta_{1},\\phi_{1},\\xi)-\\mathcal{L}_{A}(\\theta_{1},\\phi_{2},\\xi)\\geq\\frac\\mu2\\|\\phi_{1}-\\phi_{2}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the two inequalities above we obtain the Lipschitz stability for $\\phi_{\\xi}^{*}(\\cdot)$ , i.e., ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\phi_{1}-\\phi_{2}\\|\\leq\\frac{L_{21}}{\\mu}\\|\\theta_{1}-\\theta_{2}\\|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "993 Second, show that $\\nabla_{\\theta}V(\\theta)$ can be directly evaluated at $\\{{\\phi}_{\\xi}^{*}\\}_{\\xi\\in\\Xi}$ . Inspired by Danskin\u2019s theorem, we   \n994 first made the following argument, consider the definition of directional derivative. Let $\\ell(\\theta,\\phi):=$   \n995 $\\nabla_{\\theta}\\mathbb{E}_{\\xi,\\tau}J_{\\mathcal{D}}(\\theta+\\eta\\hat{\\nabla}J_{\\mathcal{D}}(\\tau),\\xi)$ . For a constant $\\tau$ and an arbitrary direction $d$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ell(\\theta+\\tau d,\\phi^{*}(\\theta+\\tau d))-\\ell(\\theta,\\phi^{*}(\\theta)))}\\\\ &{=\\ell(\\theta+\\tau d,\\phi^{*}(\\theta+\\tau d))-\\ell(\\theta+\\tau d,\\phi^{*}(\\theta))+\\ell(\\theta+\\tau d,\\phi^{*}(\\theta))-\\ell(\\theta,\\phi^{*}(\\theta))}\\\\ &{=\\nabla_{\\phi}\\ell(\\theta+\\tau d,\\phi^{*}(\\theta))^{\\top}\\underbrace{[\\phi^{*}(\\theta+\\tau d)-\\phi^{*}(\\theta))]}_{\\Delta\\phi}+o(\\Delta\\phi^{2})}\\\\ &{\\quad+\\tau\\nabla_{\\theta}\\ell(\\theta,\\phi^{*}(\\theta))^{T}d+o(d^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "996 Hence, a sufficient condition for the first equation is $\\nabla_{\\phi}\\ell(\\theta+\\tau d,\\phi^{\\ast}(\\theta))=0$ , meaning that $\\ell_{D}(\\theta,\\phi)$   \n997 and $\\mathcal{L}_{\\mathcal{A}}(\\theta,\\phi,\\xi)$ share the first-order stationarity at every $\\phi$ when fixing $\\theta$ . Indeed, by Lemma F.4, we   \n998 have, the gradient is locally determined by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}V=\\mathbb{E}_{\\xi\\sim Q}[\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi_{\\xi},\\xi)+(\\nabla_{\\theta}\\phi_{\\xi}(\\theta))^{\\top}\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi_{\\xi},\\xi)]}\\\\ &{\\qquad=\\mathbb{E}_{\\xi\\sim Q}\\left[\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi_{\\xi},\\xi)-[(\\nabla_{\\phi}^{2}\\mathcal{L}_{A}(\\theta,\\phi,\\xi))^{-1}\\nabla_{\\phi\\theta}^{2}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)]^{\\top}\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi_{\\xi},\\xi)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "999 Given a trajectory $\\tau:=(s^{1},a_{\\mathcal{D}}^{t},a_{\\mathcal{A}}^{t},\\dots,a_{\\mathcal{D}}^{H},a_{\\mathcal{A}}^{H},s^{H+1})$ , let $\\begin{array}{r}{R_{\\mathcal{D}}(\\tau,\\xi):=\\sum_{t=1}^{H}\\gamma^{t-1}r_{\\mathcal{D}}(s_{t},a_{t},\\xi)}\\end{array}$   \n1000 and $\\begin{array}{r}{R_{\\mathcal{D}}(\\tau,\\xi):=\\sum_{t=1}^{H}\\gamma^{t-1}r_{\\mathcal{D}}(s_{t},a_{t},\\xi)}\\end{array}$ . Denote by $\\mu(\\tau;\\theta,\\phi)$ the trajectory distribution, that the   \n1001 log probability of $\\mu$ is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\log\\mu(\\tau;\\theta,\\phi)=\\sum_{t=1}^{H}(\\log\\pi_{\\mathcal{D}}(a_{\\mathcal{D}}^{t}|s^{t};\\theta+\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau))+\\log\\pi_{A}(a_{A}^{t}|s^{t};\\phi)+\\log P(s^{t+1}|a_{\\mathcal{D}}^{t},a_{A}^{t},s^{t})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "1002 According to the policy gradient theorem, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi,\\xi)=\\mathbb{E}_{\\mu}[R_{\\mathcal{D}}(\\tau,\\xi)\\displaystyle\\sum_{t=1}^{H}\\nabla_{\\phi}\\log(\\pi_{A}(a_{A}^{t}|s^{t};\\phi))],}\\\\ &{\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)=\\mathbb{E}_{\\mu}[R_{A}(\\tau,\\xi)\\displaystyle\\sum_{t=1}^{H}\\nabla_{\\phi}\\log(\\pi_{A}(a_{A}^{t}|s^{t};\\phi))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "1003 By SC Assumption F.3, when $\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)=0$ , there exists $c<0,d_{!}$ , such that $\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi,\\xi)=$   \n1004 $\\begin{array}{r l}&{\\mathfrak{L}_{\\mu}[c R_{A}(\\tau,\\xi)\\sum_{t=1}^{H}\\nabla_{\\phi}\\log(\\pi_{A}(a_{A}^{t}|s_{\\cdot}^{t};\\phi))]+\\mathbb{E}_{\\mu}[\\sum_{t=1}^{H}\\gamma^{t-1}d\\sum_{t=1}^{H}\\nabla_{\\phi}\\log(\\pi_{A}(a_{A}^{t}|s^{t};\\phi))]=0.}\\end{array}$   \n1005 Hence $\\nabla_{\\theta}V=\\mathbb{E}_{\\xi\\sim Q}[\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi_{\\xi},\\xi)]$ .   \n1006 Third, ${\\cal V}(\\theta)$ is also Lipschitz smooth. As we notice that, $\\ell_{\\mathcal{D}}$ is Lipschitz smooth since $\\mathbb{E}_{\\xi\\sim Q}$ is a   \n1007 linear operator, we have, ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ \\|\\nabla_{\\theta}V(\\theta_{1})-\\nabla_{\\theta}V(\\theta_{2})\\|}\\\\ &{\\le\\|\\nabla_{\\theta}\\mathbb{E}_{\\xi\\sim Q}\\mathcal{L}_{\\mathcal{D}}(\\theta_{1},\\phi_{1},\\xi)-\\nabla_{\\theta}\\mathbb{E}_{\\xi\\sim Q}\\mathcal{L}_{\\mathcal{D}}(\\theta_{2},\\phi_{2},\\xi)\\|}\\\\ &{=\\|\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{1},\\phi_{1})-\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{2},\\phi_{1})+\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{2},\\phi_{1})-\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{2},\\phi_{2})\\|}\\\\ &{\\le\\|\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{1},\\phi_{1})-\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{2},\\phi_{1})\\|+\\|\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{2},\\phi_{1})-\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta_{2},\\phi_{2})\\|}\\\\ &{\\le L_{11}\\|\\theta_{1}-\\theta_{2}\\|+L_{12}\\|\\phi_{1}-\\phi_{2}\\|}\\\\ &{\\le(L_{11}+\\frac{L_{12}L_{21}}{\\mu})\\|\\theta_{1}-\\theta_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "1008 which implies the Lipschitz constant $\\begin{array}{r}{L=L_{11}+\\frac{L_{12}L_{21}}{\\mu}}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "1009 It is impossible to present the convergence theory without the assistance of some standard assumptions   \n1010 in batch reinforcement learning, of which the justification can be found in [14]. We also require some   \n1011 additional information about the parameter space and function structure. These assumptions are all   \n1012 stated in Assumption F.6. ", "page_idx": 26}, {"type": "text", "text": "1013 Assumption F.6. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1014 (a) The policy gradients are bounded, $\\|\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{D}}(\\theta,\\phi,\\xi)\\|\\leq G^{2}$ , $\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta,\\phi,\\xi)\\|\\le G^{2}$ for all   \n1015 $\\theta,\\phi\\in\\Theta\\times\\Phi$ and $\\xi\\in\\Xi$ . ", "page_idx": 26}, {"type": "text", "text": "1016 (b) The policy gradient estimations are unbiased, i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\hat{\\nabla}_{\\phi}J_{A}(\\theta^{t},\\phi_{\\xi}^{t},\\xi)-\\nabla_{\\phi}J_{A}(\\theta^{t},\\phi_{\\xi}^{t},\\xi)]=0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "1017 (c) The variances for the stochastic gradients are bounded, i.e., for all $\\theta^{t},\\phi_{\\xi}^{t},\\xi$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\hat{\\nabla}_{\\phi}J_{A}(\\theta^{t},\\phi_{\\xi}^{t},\\xi)-\\nabla_{\\phi}J_{A}(\\theta^{t},\\phi_{\\xi}^{t},\\xi)\\|^{2}]\\leq\\frac{\\sigma^{2}}{N_{b}}.}\\\\ &{\\mathbb{E}[\\|\\hat{\\nabla}_{\\phi}J_{D}(\\theta^{t},\\phi_{\\xi}^{t},\\xi)-\\nabla_{\\theta}J_{D}(\\theta^{t},\\phi_{\\xi}^{t},\\xi)\\|^{2}]\\leq\\frac{\\sigma^{2}}{N_{b}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "1018 (d) The parameter space $\\Theta$ has diameter $D_{\\Theta}:=\\operatorname*{sup}_{\\theta_{1},\\theta_{2}\\in\\Theta}\\|\\theta_{1}-\\theta_{2}\\|$ ; the initialization $\\theta^{0}$   \n019 admits at most $D_{V}$ function gap, i.e., $D_{V}:=\\operatorname*{max}_{\\theta\\in\\Theta}V(\\theta)-V(\\theta^{0})$ . ", "page_idx": 26}, {"type": "text", "text": "(e) It holds that the parameters satisfy $0<\\mu<-c L_{22}$ . ", "page_idx": 26}, {"type": "text", "text": "1021 Equipped with Assumption F.6 we are able to unfold our main result Theorem 3.3, before which   \n1022 we show in Lemma F.7 that $\\phi_{\\xi}^{\\ast}$ can be efficiently approximated by the inner loop in the sense that   \n1023 $\\nabla_{\\theta}\\mathbb{E}_{\\xi\\sim Q}\\mathcal{L}_{\\mathcal{D}}(\\theta^{t},\\phi_{\\xi}^{t}(N_{A}),\\xi)\\approx\\dot{\\nabla}_{\\theta}V(\\theta^{t})$ , where $\\phi_{\\xi}^{t}(N_{A})$ is the last iterate output of the attacker   \n1024 policy.   \n1025 Lemma F.7. Under Assumption F.6, 3.2, $F.3$ , and $F.2$ , let $\\rho\\ :=\\ 1\\,+\\,\\textstyle\\frac{\\mu}{c L_{22}}\\ \\in\\ (0,1)$ , $\\bar{L}\\ =$   \n1026 $\\operatorname*{max}\\{L_{11},L_{12},L_{22},L_{21},V_{\\infty}\\}$ where $V_{\\infty}:=\\operatorname*{max}\\{\\operatorname*{max}\\|\\nabla V(\\theta)\\|,1\\}$ . For all $\\varepsilon>0,$ , if the attacker   \n1027 learning iteration $N_{A}$ and batch size $N_{b}$ are large enough such that ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "1028 then, for ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N_{A}\\geq\\frac{1}{\\log{\\rho^{-1}}}\\log{\\frac{32D_{V}^{2}(2V_{\\infty}+L D_{\\Theta})^{4}\\bar{L}|c|G^{2}}{L^{2}\\mu^{2}\\varepsilon^{4}}}}\\\\ {N_{b}\\geq\\frac{32\\mu L_{21}^{2}D_{V}^{2}(2V_{\\infty}+L D_{\\Theta})^{4}}{|c|L_{22}^{2}\\sigma^{2}\\bar{L}L\\varepsilon^{4}},~~~~~~~~~~~~~~}\\\\ {z_{t}:=\\nabla_{\\theta}\\mathbb{E}_{\\xi\\sim Q}\\mathcal{L}_{D}(\\theta^{t},\\phi_{\\xi}^{t}(N_{A}),\\xi)-\\nabla_{\\theta}V(\\theta^{t}),~~~~~~~~~~~~~~~~}\\\\ {\\mathbb{E}[\\|z_{t}\\|]\\leq\\frac{L\\varepsilon^{2}}{4D_{V}(2V_{\\infty}+L D_{\\Theta})^{2}},~~~~~~~~~~~~~~~~}\\\\ {\\mathbb{E}[\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{t}(N),\\xi)\\|]\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1029 and ", "page_idx": 27}, {"type": "text", "text": "1030 Proof of Lemma $F7$ . Fixing a $\\xi\\in\\Xi$ , due to Lipschitz smoothness, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}_{\\mathcal{D}}(\\theta^{t},\\phi_{\\xi}^{t}(N),\\xi)-\\mathcal{L}_{\\mathcal{D}}(\\theta^{t},\\phi_{\\xi}^{t}(N-1),\\xi)}\\\\ &{\\leq\\langle\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta^{t},\\phi_{\\xi}^{t}(N-1),\\xi),\\phi_{\\xi}^{t}(N)-\\phi_{\\xi}^{t}(N-1)\\rangle+\\displaystyle\\frac{L_{22}}{2}\\|\\phi_{\\xi}^{t}(N)-\\phi_{\\xi}^{t}(N-1)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1031 The inner loop updating rule ensures that when $\\begin{array}{r l r}{\\kappa_{A}}&{{}=}&{\\frac{1}{L_{21}}}\\end{array}$ , $\\phi_{\\xi}^{t}(N)\\ -\\ \\phi_{\\xi}^{t}(N\\ -\\ 1)\\ \\ =$   \n1032 $\\begin{array}{r}{\\frac{1}{L_{21}}\\hat{\\nabla}_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)}\\end{array}$ . Plugging it into the inequality, we arrive at ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}_{D}(\\theta^{t},\\phi_{\\xi}^{t}(N),\\xi)-\\mathcal{L}_{D}(\\theta^{t},\\phi_{\\xi}^{t}(N-1),\\xi)}\\\\ &{\\leq\\displaystyle\\frac{1}{L_{21}}\\langle\\nabla_{\\phi}\\mathcal{L}_{D}(\\theta^{t},\\phi_{\\xi}^{t}(N-1),\\xi),\\hat{\\nabla}_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\rangle+\\frac{L_{22}}{2L_{21}^{2}}\\|\\hat{\\nabla}_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1033 Therefore, we let $(\\mathcal{F}_{n}^{t})_{0\\leq n\\leq N}$ be the filtration generated by $\\sigma(\\{\\phi_{\\xi}^{t}(\\tau)\\}_{\\xi\\in\\Xi}|\\tau\\leq n)$ and take condi  \n1034 tional expectations on ${\\mathcal{F}}_{n}^{t}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N))|\\mathcal{F}_{N-1}^{t}]\\leq V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N-1))}\\\\ &{\\mathbb{E}_{\\xi}\\left[\\frac{1}{L_{21}}\\langle\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}},\\nabla_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\rangle+\\frac{L_{22}}{2L_{21}^{2}}\\|\\hat{\\nabla}_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1035 By variance-bias decomposition, and Assumption F.6 (b) and (c), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[\\|\\hat{\\nabla}_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}|\\mathcal{F}_{N-1}^{t}]}\\\\ &{=\\mathbb{E}[\\|\\hat{\\nabla}_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)-\\nabla_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)+\\nabla_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}|\\mathcal{F}_{N-1}^{t}]}\\\\ &{=\\mathbb{E}[\\|(\\hat{\\nabla}_{\\phi}-\\nabla_{\\phi})J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}|\\mathcal{F}_{N-1}^{t}]+\\mathbb{E}[\\|\\nabla_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}|\\mathcal{F}_{N-1}^{t}]}\\\\ &{\\quad+\\mathbb{E}[2\\langle(\\hat{\\nabla}_{\\phi}-\\nabla_{\\phi})J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi),\\nabla_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\rangle|\\mathcal{F}_{N-1}^{t}]}\\\\ &{\\le\\frac{\\sigma^{2}}{N_{b}}+\\|\\nabla_{\\phi}J_{A}(\\theta_{\\xi}^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1036 Applying the PL condition (Assumption 3.2), and Assumption F.6 (a) we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta,\\phi^{t}(N))|\\phi^{N-1}]-V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta,\\phi^{t}(N-1))}\\\\ &{\\leq\\mathbb{E}_{\\xi}\\left[\\frac{1}{L_{21}}\\langle\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}},\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\rangle+\\frac{L_{22}}{2L_{21}^{2}}(\\frac{\\sigma^{2}}{N_{b}}+\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2})\\right]}\\\\ &{=\\mathbb{E}_{\\xi}\\left[-\\frac{1}{2L_{22}}\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}\\|^{2}+\\frac{1}{2L_{22}}\\|\\nabla_{\\phi}(\\mathcal{L}_{\\mathcal{D}}+\\frac{L_{22}}{L_{21}}\\mathcal{L}_{A})(\\theta^{t},\\phi_{\\xi}^{t}(N-1),\\xi)\\|^{2}+\\frac{L_{22}\\sigma^{2}}{2L_{21}^{2}N_{b}}\\right]}\\\\ &{\\leq\\frac{\\mu}{c L_{21}}(\\underset{\\phi}{\\operatorname*{max}}\\ell_{\\mathcal{D}}(\\theta^{t},\\phi)-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N-1)))+\\frac{L_{22}\\sigma^{2}}{2L_{21}^{2}N_{b}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1037 rearranging the terms yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N))|\\mathcal{F}_{n}^{t}]\\le\\rho(V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N-1)))+\\frac{L_{22}\\sigma^{2}}{2L_{21}^{2}N_{b}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1038 where we use the fact that \u2212 $\\begin{array}{r}{\\operatorname*{max}_{\\phi}\\ell_{\\mathcal{D}}(\\theta^{t},\\phi)\\leq-V(\\theta^{t})}\\end{array}$ . Telescoping the inequalities from $\\tau=0$ to   \n1039 $\\tau=N$ , we arrive at ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N))]\\leq\\rho^{N}(V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(0)))+\\frac{1-\\rho^{N}}{1-\\rho}\\left(\\frac{L_{22}\\sigma^{2}}{2L_{21}^{2}N_{b}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1040 PL-condition implies quadratic growth, we also know that $V(\\theta^{t})~-~\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N))\\quad\\le$   \n1041 $\\begin{array}{r}{\\mathbb{E}_{\\xi}\\frac{1}{2\\mu}\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{D}}(\\theta^{t},\\dot{\\phi}_{\\xi}^{t}(N),\\dot{\\xi})\\|^{2}\\leq\\frac{1}{2\\mu}G^{2}}\\end{array}$ , by Assumption F.3, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\phi_{\\xi}^{*}(\\theta^{t})-\\phi_{\\xi}^{t}(N)\\|^{2}\\leq\\frac{2}{\\mu}(\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{*},\\xi)-\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{t}(N),\\xi))}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{2|c|}{\\mu}|\\mathcal{L}_{\\mathcal{D}}(\\theta^{t},\\phi_{\\xi}^{*},\\xi)-\\mathcal{L}_{\\mathcal{D}}(\\theta^{t},\\phi_{\\xi}^{t}(N),\\xi)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1042 Hence, with Jensen inequality and choice of $N_{A}$ and $N_{b}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|z_{t}\\|]=\\mathbb{E}[\\|\\nabla_{\\theta}V(\\theta^{t})-\\mathbb{E}_{\\xi}\\nabla_{\\theta}\\mathcal{L}_{D}(\\theta^{t},\\phi_{\\xi}^{t}(N_{A}),\\xi)\\|]}\\\\ &{\\quad\\quad\\quad\\leq L_{12}\\mathbb{E}[\\|\\phi_{\\xi}^{t}(N_{A})-\\phi_{\\xi}^{*}\\|]}\\\\ &{\\quad\\quad\\quad\\leq L_{12}\\sqrt{\\frac{2|c|}{\\mu}\\mathbb{E}[V(\\theta^{t})-\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N_{A}))]}}\\\\ &{\\quad\\quad\\quad\\leq L_{12}\\sqrt{\\frac{|c|}{\\mu^{2}}\\rho^{N_{A}}G^{2}+(1-\\rho^{N_{A}})\\frac{|c|\\displaystyle L_{22}^{2}\\sigma^{2}}{\\mu L_{21}^{2}N_{b}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1043 Now we adjust the size of $N_{A}$ and $N_{b}$ to make $\\mathbb{E}[\\Vert z_{t}\\Vert]$ small enough, to this end, we set ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\rho^{N_{A}}\\displaystyle\\frac{|c|G^{2}}{\\mu^{2}}\\leq\\displaystyle\\frac{\\varepsilon^{4}L^{2}}{32D_{V}^{2}(2V_{\\infty}+L D_{\\Theta})^{4}\\bar{L}}}}\\\\ {{\\displaystyle\\frac{|c|L_{22}^{2}\\sigma^{2}}{L_{21}^{2}N_{b}}\\leq\\displaystyle\\frac{\\varepsilon^{4}L^{2}\\mu^{2}}{32D_{V}^{2}(2V_{\\infty}+L D_{\\Theta})^{4}\\bar{L}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1044 which further indicates that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{A}\\geq\\frac{1}{\\log\\rho^{-1}}\\log\\frac{32D_{V}^{2}(2V_{\\infty}+L D_{\\Theta})^{4}\\bar{L}|c|G^{2}}{L^{2}\\mu^{2}\\varepsilon^{4}}}\\\\ &{N_{b}\\geq\\frac{32\\mu L_{21}^{2}D_{V}^{2}(2V_{\\infty}+L D_{\\Theta})^{4}}{|c|L_{22}^{2}\\sigma^{2}\\bar{L}L\\varepsilon^{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1045 In the setting above, it is not hard to verify that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Vert z_{t}\\Vert]\\leq\\frac{L\\varepsilon^{2}}{4D_{V}(2V_{\\infty}+L D_{\\Theta})^{2}}\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1046 Also note that $\\begin{array}{r}{\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{t}(N_{A}),\\xi)\\|=\\|\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{t}(N_{A}),\\xi)-\\nabla_{\\phi}\\mathcal{L}_{A}(\\theta^{t},\\phi_{\\xi}^{*},\\xi)\\|}\\end{array}$ , given the   \n1047 proper choice of $N_{A}$ and $N_{b}$ , one has ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\|\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{A}}(\\theta^{t},\\phi_{\\xi}^{t}(N_{\\mathcal{A}}),\\xi)-\\nabla_{\\phi}\\mathcal{L}_{\\mathcal{A}}(\\theta^{t},\\phi_{\\xi}^{*},\\xi)\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\leq L_{21}\\mathbb{E}[\\|\\phi_{\\xi}^{t}(N_{\\mathcal{A}})-\\phi_{\\xi}^{*}\\|]\\leq\\frac{L\\varepsilon^{2}}{4D_{V}(2V_{\\infty}+L D_{\\Theta})^{2}}\\leq\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "1048 which indicates the $\\xi$ -wise inner loop stability. ", "page_idx": 28}, {"type": "text", "text": "1050 Theorem F.8. Under Assumption $F.6_{;}$ , Assumption $F.3$ , and Assumption $F.2$ , let the stepsizes be,   \n1051 $\\begin{array}{r}{\\kappa_{A}=\\frac{1}{L_{22}}}\\end{array}$ , $\\begin{array}{r}{\\kappa_{D}=\\frac{1}{L}}\\end{array}$ , $i f N_{D},N_{A}$ , and $N_{b}$ are large enough, ", "page_idx": 29}, {"type": "equation", "text": "$$\nN_{\\mathcal{D}}\\geq N_{\\mathcal{D}}(\\varepsilon)\\sim\\mathcal O(\\varepsilon^{-2})\\quad N_{A}\\geq N_{A}(\\varepsilon)\\sim\\mathcal O(\\log\\varepsilon^{-1}),\\quad N_{b}\\geq N_{b}(\\varepsilon)\\sim\\mathcal O(\\varepsilon^{-4})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1052 then there exists $t\\in\\mathbb{N}$ such that $(\\theta^{t},\\{\\phi_{\\xi}^{t}(N_{A})\\}_{\\xi\\in\\Xi})$ is $\\varepsilon$ -meta-FOSE. ", "page_idx": 29}, {"type": "text", "text": "1053 Proof. According to the update rule of the outer loop, (here we omit the projection analysis for   \n1054 simplicity) ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\theta^{t+1}-\\theta^{t}=\\frac{1}{L}\\hat{\\nabla}_{\\theta}\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N_{\\ A})),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1055 one has, due to unbiasedness assumption, let $(\\mathcal{F}_{t})_{0\\leq t\\leq N_{D}}$ be the flitration generated by $\\sigma(\\theta^{t}|k\\leq t)$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\langle\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N_{A})),\\theta^{t+1}-\\theta^{t}\\rangle|\\mathcal{F}_{t}]=\\displaystyle\\frac{1}{L}\\mathbb{E}[\\|\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N_{A}))\\|^{2}|\\mathcal{F}_{t}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=L\\mathbb{E}\\|\\theta^{t+1}-\\theta^{t}\\|^{2}|\\mathcal{F}_{t}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1056 which leads to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\langle\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{*}),\\theta^{t+1}-\\theta^{t}\\rangle|\\mathcal{F}_{t}]=\\mathbb{E}[\\langle z_{t},\\theta^{t}-\\theta^{t+1}\\rangle|\\mathcal{F}_{t}]+L\\mathbb{E}[\\|\\theta^{t+1}-\\theta^{t}\\|^{2}\\|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1057 Since $V(\\cdot)$ is $L$ -Lipschitz smooth, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[V(\\theta^{t})-V(\\theta^{t+1})]\\leq\\mathbb{E}[\\langle\\nabla_{\\theta}V(\\theta^{t}),\\theta^{t}-\\theta^{t+1}\\rangle]+\\displaystyle\\frac{L}{2}\\mathbb{E}[\\|\\theta^{t+1}-\\theta^{t}\\|^{2}]}\\\\ &{\\quad\\quad\\leq\\mathbb{E}[\\langle z_{t},\\theta^{t+1}-\\theta^{t}\\rangle]-\\mathbb{E}[\\langle\\nabla_{\\theta}\\ell_{D}(\\theta^{t},\\phi^{t}(N_{A})),\\theta^{t+1}-\\theta^{t}\\rangle]+\\displaystyle\\frac{L}{2}\\mathbb{E}[\\|\\theta^{t+1}-\\theta^{t}\\|^{2}]}\\\\ &{\\quad\\quad\\leq\\mathbb{E}[\\langle z_{t},\\theta^{t+1}-\\theta^{t}\\rangle]-\\displaystyle\\frac{L}{2}\\mathbb{E}[\\|\\theta^{t+1}-\\theta^{t}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1058 Fixing a $\\theta\\in\\Theta$ , let $\\begin{array}{r}{e_{t}:=\\langle\\nabla_{\\theta}\\ell_{\\mathcal{D}}(\\theta^{t},\\phi^{t}(N_{A})),\\theta-\\theta^{t}\\rangle}\\end{array}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[e_{t}|{\\mathcal F}_{t}]=L\\mathbb{E}[\\langle\\theta^{t+1}-\\theta^{t},\\theta-\\theta^{t}\\rangle|{\\mathcal F}_{t}]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[\\langle\\nabla_{\\theta}\\ell_{D}(\\theta^{t},\\phi^{t}(N_{A}))-\\nabla_{\\theta}V(\\theta^{t}),\\theta^{t+1}-\\theta^{t}\\rangle+\\langle\\nabla_{\\theta}V(\\theta^{t}),\\theta^{t+1}-\\theta^{t}\\rangle]}\\\\ &{\\quad\\quad\\quad\\quad+\\ L\\mathbb{E}[\\langle\\theta^{t+1}-\\theta^{t},\\theta-\\theta^{t+1}\\rangle]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\mathbb{E}[(\\|z_{t}\\|+V_{\\infty}+L D_{\\Theta})\\|\\theta^{t+1}-\\theta^{t}\\|]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1059 By the choice of $N_{b}$ , we have, since $V_{\\infty}=\\operatorname*{max}\\{\\operatorname*{max}_{\\theta}\\,\\lVert\\nabla V(\\theta)\\rVert,1\\}.$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|z_{t}\\|]\\le L_{12}\\mathbb{E}[\\|\\phi^{N}-\\phi^{*}\\|]\\le\\frac{L\\varepsilon^{2}}{4D_{V}(2V_{\\infty}+L D_{\\Theta})}\\le V_{\\infty}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1060 Thus, the relation equation F13 can be reduced to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[e_{t}]\\leq(2V_{\\infty}+L D_{\\Theta})\\mathbb{E}[\\|\\theta^{t+1}-\\theta^{t}\\|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1061 Telescoping equation F12 yields ", "page_idx": 29}, {"type": "equation", "text": "$$\n-D_{V}\\leq\\mathbb{E}[V(\\theta^{0})-V(\\theta^{N_{D}})]\\leq D_{\\Theta}\\sum_{t=0}^{T-1}\\mathbb{E}[\\|z_{t}\\|]-{\\frac{L}{2(2V_{\\infty}+L D_{\\Theta})^{2}}}\\mathbb{E}[\\sum_{t=0}^{T-1}\\mathbb{E}[e_{t}^{2}|\\mathcal{F}_{t}].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1062 Thus, setting ND \u22654DV (2VL\u221e\u03b52+LD\u0398) , and then by Lemma F.7, we obtain that, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{N_{\\mathcal{D}}}\\sum_{t=0}^{N_{\\mathcal{D}}-1}\\mathbb{E}[e_{t}^{2}]\\leq\\frac{\\varepsilon^{2}}{2}+\\frac{2D_{V}(2V_{\\infty}+L D_{\\Theta})^{2}}{L N_{\\mathcal{D}}}\\leq\\varepsilon^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1063 which implies there exists $t\\in\\{0,\\ldots,N_{\\mathcal{D}}-1\\}$ such that $\\mathbb{E}[e_{t}^{2}]\\le\\varepsilon^{2}$ . ", "page_idx": 29}, {"type": "text", "text": "1065 F.3 Generalization to Unseen Attacks ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1066 In the online adaptation phase, the pre-trained meta-defense may be exposed to attacks unseen in the   \n1067 pre-training phase, which poses an out-of-distribution (OOD) generalization issue to the proposed   \n1068 meta-SG framework. Yet, Proposition F.9 and Proposition F.13 assert that meta-SG is generalizable   \n1069 to the unseen attacks, given that the unseen is not distant from those seen. The formal statement is   \n1070 deferred to Appendix F, and the proof mainly targets those unseen non-adaptive attacks for simplicity.   \n1071 Proposition F.9 (OOD Generalization Informal Statement). Consider sampled attack types   \n1072 $\\xi_{1},\\allowbreak\\dots,\\allowbreak\\xi_{m}$ during the pre-training and the unseen attack type $\\xi_{m+1}$ in the online stage. The gen  \n1073 eralization error is upper-bounded by the \u201cdiscrepancy\u201d between the unseen and the seen attacks   \n1074 $C(\\xi_{m+1},\\{\\xi_{i}\\}_{i=1}^{m})$ .   \n1075 Our main goal is to quantify the value discrepancy under an attack type that is out of empirical   \n1076 distribution. We consider attack types $\\xi_{1},\\allowbreak\\dots,\\allowbreak\\xi_{m}$ to be empirically sampled from distribution $Q(\\cdot)$   \n1077 during the pre-training stage, and an unseen attack type $\\xi_{m+1}$ in the online stage. The quantification   \n1078 of distance $C(\\xi_{m+1},\\bar{\\{\\xi_{i}\\}}_{i=1}^{\\bar{m}})$ relies on the total variation,   \n1079 Definition F.10 (total variation). For two distributions $P$ and $Q$ , defined over the sample space $\\Omega$   \n1080 and $\\sigma$ -field $\\mathcal{F}$ , the total variation between $P$ and $Q$ is $\\|P-Q\\|_{T V}:=\\operatorname*{sup}_{U\\in{\\mathcal{F}}}|P(U)-Q({\\bar{U}})|$ . ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "1081 The celebrated result shows the following characterization of total variation, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|P-Q\\|_{T V}=\\operatorname*{sup}_{f:0\\leq f\\leq1}\\mathbb{E}_{x\\sim P}[f(x)]-\\mathbb{E}_{x\\sim Q}[f(x)].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1082 Let the fixed attack policies $\\phi_{i}$ , $i=1,\\dotsc,m+1$ corresponding to each attack type. To formalize   \n1083 the generalization error, for each $\\theta\\in\\Theta$ , we define populational values ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\hat{V}(\\theta):=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{\\tau\\sim q_{i}^{\\theta}}J_{\\mathcal{D}}(\\theta-\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi_{i},\\xi_{i})}}\\\\ {{\\displaystyle\\hat{V}_{m+1}(\\theta):=\\mathbb{E}_{\\tau\\sim q_{m+1}^{\\theta}}J_{\\mathcal{D}}(\\theta-\\eta\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau),\\phi_{m+1},\\xi_{m+1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1084 where $q_{i}^{\\theta}(\\cdot)\\,:\\,(S\\times A\\times S)^{H-1}\\times S\\to\\,[0,1]$ is the trajectory distribution determined by state   \n1085 dependent policies $\\pi_{\\mathcal{D}}(\\cdot|s;\\theta),\\,\\pi_{A}(\\cdot|s;\\phi_{i},\\xi_{i})$ and transition kernel $\\tau$ . Since $q_{i}^{\\theta}$ is factorizable, we   \n1086 have Lemma F.11 to eliminate $\\lVert q_{i}^{\\theta}-q_{m+1}^{\\theta}\\rVert_{T V}$ dependence on $\\theta$ by upper bounding it using another   \n1087 pair of mariginal distributions.   \n1088 Lemma F.11. For any $\\theta\\in\\Theta$ , there exist marginals $d_{i},d_{m+1}:(S\\times A_{A}\\times S)^{H-1}\\times S\\to[0,1]$   \n1089 total variation $\\lVert q_{i}^{\\theta}-q_{m+1}^{\\theta}\\rVert_{T V}$ can be bounded by $\\|d_{i}-d_{m+1}\\|_{T V}$ . ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "1090 Proof. By factorization, for a trajectory $\\tau$ , any $\\theta\\in\\Theta$ , and any type index $i=1,\\dotsc,m+1$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nq_{i}^{\\theta}(\\tau)=\\prod_{t=1}^{H-1}\\pi_{\\mathcal{D}}(a_{\\mathcal{D}}^{t}|s_{t};\\theta)\\prod_{t=1}^{H-1}\\pi_{A}(a_{A}^{t}|s_{t},\\phi_{i},\\xi_{i})\\prod_{t=1}^{H-1}\\mathcal{T}(s_{t+1}|s_{t},a_{t}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1091 thus, by the inequality of product measure, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|q_{i}^{\\theta}-q_{m+1}^{\\theta}\\|_{T V}\\leq\\sum_{t=1}^{H-1}\\underbrace{\\|\\pi_{\\mathcal{D}}(\\cdot|s_{t};\\theta)-\\pi_{\\mathcal{D}}(\\cdot|s_{t};\\theta)\\|_{T V}}_{0}+\\|d_{i}-d_{m+1}\\|_{T V},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1092 where $d_{i}$ and $d_{m+1}$ are the residue factors after removing $\\pi_{\\mathcal{A}}\\big(\\cdot|s_{t};\\theta\\big)$ . ", "page_idx": 30}, {"type": "text", "text": "1093 Assumption F.12. For any $\\xi\\in\\Xi$ and $\\phi_{\\xi}$ , the function $J_{D}(\\theta,\\phi_{\\xi},\\xi)$ is $G$ -Lipschitz continuous w.r.t.   \n1094 $\\theta\\in\\Theta$ ;   \n1095 Proposition F.13. Under assumption 3.2 and certain regularity conditions, fixing a policy $\\theta\\in\\Theta$ , we   \n1096 have, there exist some marginal distribution of ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\hat{V}_{m+1}(\\theta)-\\hat{V}(\\theta)|\\leq C(d_{m+1},\\{d_{i}\\}_{i=1}^{m}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1097 where the constant $C$ depending on the total variation between $d_{m+1}$ and $\\{d_{i}\\}_{i=1}^{m}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nC(d_{m+1},\\{d_{i}\\}_{i=1}^{m}):=\\frac{2\\eta G^{2}}{m}\\sum_{i=1}^{m}\\|d_{m+1}-d_{i}\\|_{T V}+\\frac{1-\\gamma^{H}}{1-\\gamma}\\|d_{m+1}-\\frac{1}{m}\\sum_{i=1}^{m}d_{i}\\|_{T V},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1098 here, $G$ is the Lipschitz parameter of $J_{D}$ w.r.t. both $\\theta$ . ", "page_idx": 30}, {"type": "text", "text": "1099 Proof. We start with the decomposition of the generalization error, for an arbitrary attack type $\\xi_{i}$ ,   \n1100 $i=1,\\hdots,m$ , fixing a policy $\\theta\\in\\Theta$ determines jointly with each $\\phi_{i}$ the trajectory distribution $q_{i}^{\\theta}$ .   \n1101 Denoting the one-step adaptation policy $\\theta^{\\prime}(\\tau)=\\theta-\\eta\\nabla J_{\\mathcal{D}}(\\tau)$ as a function of trajectory $\\tau$ , we have   \n1102 the following decomposition, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{V}_{m+1}(\\theta)-\\hat{V}(\\theta)=\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{m+1},\\xi_{m+1})-\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{\\tau_{i}\\sim q_{i}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{i}),\\phi_{i},\\xi_{i})}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{m+1},\\xi_{m+1})-\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{i},\\xi_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n+\\underbrace{\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{i},\\xi_{i})}_{(i i)}-\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{\\tau_{i}\\sim q_{i}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{i}),\\phi_{i},\\xi_{i})}_{(i i)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1103 We assume $(\\tau_{m+1},\\tau_{i})$ is drawn from a joint distribution which has marginals $q_{m+1}^{\\theta}$ and $q_{i}^{\\theta}$ and is   \n1104 corresponding to the maximal coupling of these two. Then, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tau_{m+1}\\sim q_{m+1}^{\\theta},\\quad\\tau_{i}\\sim q_{i}^{\\theta},\\quad\\mathbb{P}(\\tau_{m+1}\\neq\\tau_{i})=\\|q_{i}^{\\theta}-q_{m+1}^{\\theta}\\|_{T V},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1105 if $\\tau_{m+1}$ disagrees with $\\tau_{i}$ , for $(i i)$ , we have, since $J_{\\mathcal{D}}^{\\theta}$ is Lipschitz with respect to $\\theta$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|J_{\\mathcal{D}}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{i},\\xi_{i})-J_{\\mathcal{D}}(\\theta^{\\prime}(\\tau_{i}),\\phi_{i},\\xi_{i})\\|}\\\\ &{\\leq\\eta G\\|\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau_{m+1})-\\hat{\\nabla}_{\\theta}J_{\\mathcal{D}}(\\tau_{i})\\|}\\\\ &{\\leq2\\eta G^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1106 as a result, denoting the maximal coupling of $q_{m+1}^{\\theta}$ and $q_{i}^{\\theta}$ as gives, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad[\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}J_{\\mathcal{D}}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{i},\\xi_{i})-\\mathbb{E}_{\\tau_{i}\\sim q_{i}^{\\theta}}J_{\\mathcal{D}}(\\theta^{\\prime}(\\tau_{i}),\\phi,\\xi_{i})]}\\\\ &{=\\mathbb{E}_{(\\tau_{m+1},\\tau_{i})\\sim\\prod(q_{m+1}^{\\theta},q_{i}^{\\theta})}[J_{\\mathcal{D}}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{i},\\xi_{i})-J_{\\mathcal{D}}(\\theta^{\\prime}(\\tau_{i}),\\phi,\\xi_{i})]}\\\\ &{\\leq2\\eta G^{2}\\|q_{m+1}^{\\theta}-q_{i}^{\\theta}\\|_{T V}\\leq2\\eta G^{2}\\|d_{i}-d_{m+1}\\|_{T V},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1107 where the last inequality is due to Lemma F.11. Averaging the $m$ empirical $\\xi_{i}$ \u2019s yeilds the result: ", "page_idx": 31}, {"type": "equation", "text": "$$\n(i i)\\leq\\frac{2\\eta G^{2}}{m}\\sum_{i=1}^{m}\\|d_{i}-d_{m+1}\\|_{T V}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1108 Since the trajectory distribution is a product measure, the difference between $q_{i}^{\\theta}$ and $q_{m+1}^{\\theta}$ only lies   \n1109 by attacker\u2019s type, \u2225q\u03b8m(+\u03c41m $\\begin{array}{r}{\\|q_{m+1}^{\\theta^{\\prime}(\\tau_{m+1})}-q_{i}^{\\theta^{\\prime}(\\tau_{m+1})}\\|_{T V}=\\|q_{m+1}^{\\theta}-q_{i}^{\\theta}\\|_{T V}\\leq\\|d_{m+1}-d_{i}\\|_{T V}.}\\end{array}$   \n1110 Now we bound $(i)$ , for ease of exposition we let $q^{\\prime\\prime}=q_{m+1}^{\\theta^{\\prime}(\\tau_{m+1})}$ and qi\u2032 := $q_{i}^{\\prime}:=q_{i}^{\\theta^{\\prime}(\\tau_{m+1})}$ qi\u03b8(\u03c4m+1). By the finiteness   \n1111 of total trajectory reward R(\u03c4) for any trajectory \u03c4, R(\u03c4) \u226411\u2212\u2212\u03b3\u03b3H , hence, ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(i)=\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{m+1},\\xi_{m+1})-\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}J_{D}(\\theta^{\\prime}(\\tau_{m+1}),\\phi_{i},\\xi_{i})}\\\\ &{\\quad=\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}\\left[\\mathbb{E}_{\\tau^{\\prime\\prime}\\sim q^{\\prime\\prime}}R_{D}(\\tau^{\\prime\\prime})-\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{\\tau_{i}^{\\prime}\\sim q_{i}^{\\prime}}R_{D}(\\tau_{i}^{\\prime})\\right]}\\\\ &{\\quad\\le\\mathbb{E}_{\\tau_{m+1}\\sim q_{m+1}^{\\theta}}\\displaystyle\\frac{1-\\gamma^{H}}{1-\\gamma}\\|q_{m+1}^{\\prime\\prime}-\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}q_{i}^{\\prime}\\|_{T V}}\\\\ &{\\quad\\le\\displaystyle\\frac{1-\\gamma^{H}}{1-\\gamma}\\|d_{m+1}-\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}d_{i}\\|_{T V}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1114 This section offers further justification for the meta-equilibrium in (meta-SE), and we argue that meta  \n1115 equilibrium provides a data-driven approach to address incomplete information in dynamic games.   \n1116 Note that information asymmetry is prevalent in the adversarial machine learning context, where the   \n1117 attacker enjoys an information advantage (e.g., the attacker\u2019s type). The proposed meta-equilibrium   \n1118 notion can shed light on these related problems beyond the adversarial FL context.   \n1119 We begin with the insufficiency of Bayesian Stackelberg equilibrium defined as the solution to the   \n1120 bilevel optimization in equation BSE in handling information asymmetry, a customary solution   \n1121 concept in security studies [35]. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta}\\mathbb{E}_{\\xi\\sim Q(\\cdot)}[J_{\\mathcal{D}}(\\theta,\\phi_{\\xi}^{*},\\xi)]\\quad\\mathrm{s.t.~}\\phi_{\\xi}^{*}\\in\\arg\\operatorname*{max}J_{\\mathcal{A}}(\\theta,\\phi,\\xi),\\forall\\xi\\in\\Xi.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "1122 One can see from equation BSE that such an equilibrium is of ex-ante type: the defender\u2019s strategy is   \n1123 determined before the game starts. It targets a \u201crepresentative\u201d attacker (an average of all types). As   \n1124 the game unfolds, new information regarding the attacker\u2019s private type is revealed (e.g., through   \n1125 the global model updates). However, this ex-ante strategy does not enable the defender to adjust its   \n1126 strategy as the game proceeds. Using game theory language, the defender fails to handle the emerging   \n1127 information in the interim stage.   \n1128 To create interim adaptability in this dynamic game of incomplete information, one can consider   \n1129 introducing the belief system to capture the defender\u2019s learning process on the hidden type. Let $I^{t}$   \n1130 be the defender\u2019s observations up to time $t$ , i.e., $I^{t}:=\\,(s^{k},a_{\\mathcal{D}}^{\\overline{{k}}}\\big)_{k=1}^{t}s^{t+1}$ . Denote by $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ the belief   \n1131 generation operator $b^{t+1}(\\xi)=B[I^{t}]$ . With the Bayesian equilibrium framework, the belief generation   \n1132 can be defined recursively as below ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "equation", "text": "$$\nb^{t+1}(\\xi)=\\mathcal{B}[s^{t},a_{\\mathcal{D}}^{t},b^{t}]:=\\frac{b^{t}(\\xi)\\pi_{A}(a_{A}^{t}|s^{t};\\xi)\\mathcal{T}(s^{t+1}|s^{t},a_{A}^{t},a_{\\mathcal{D}}^{t})}{\\sum_{\\xi^{\\prime}}b^{t}(\\xi^{\\prime})\\pi_{A}(a_{A}^{t}|s^{t};\\xi^{\\prime})\\mathcal{T}(s^{t+1}|s^{t},a_{A}^{t},a_{\\mathcal{D}}^{t})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "1133 Since $b^{t}$ is the defender\u2019s belief on the hidden type at time $t$ , its belief-dependent Markovian strategy   \n1134 is defined as $\\pi_{\\mathcal{D}}(s^{t},b^{t})$ . Therefore, the interim equilibrium, also called Perfect Bayesian Equilibrium   \n1135 (PBE) [17] is given by a tuple $(\\pi_{\\mathcal{D}}^{*},\\pi_{\\mathcal{A}}^{*},\\{b^{t}\\}_{t=1}^{H})$ satisfying ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\pi_{\\mathcal{D}}^{*}=\\arg\\operatorname*{max}\\mathbb{E}_{\\xi\\sim Q}\\mathbb{E}_{\\pi_{\\mathcal{D}},\\pi_{\\mathcal{A}}^{*}}[\\displaystyle\\sum_{t=1}^{H}r_{\\mathcal{D}}(s^{t},a_{\\mathcal{D}}^{t},a_{\\mathcal{A}}^{t})b^{t}(\\xi)]}}\\\\ {{\\displaystyle\\pi_{\\mathcal{A}}^{*}=\\arg\\operatorname*{max}\\mathbb{E}_{\\pi_{\\mathcal{D}},\\pi_{\\mathcal{A}}}[\\displaystyle\\sum_{t=1}^{H}r_{\\mathcal{A}}(s^{t},a_{\\mathcal{D}}^{t},a_{\\mathcal{A}}^{t})],\\forall\\xi,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "1136 In contrast with (BSE), this perfect Bayesian equilibrium notion (PBE) enables the defender to make   \n1137 good use of the information revealed by the attacker, and subsequently adjust its actions according to   \n1138 the revealed information through the belief generation. From a game-theoretic viewpoint, both (PBE)   \n1139 and (meta-SE) create strategic online adaptation: the defender can infer and adapt to the attacker\u2019s   \n1140 private type through the revealed information since different types aim at different objectives, hence,   \n1141 leading to different actions. Compared with PBE, the proposed meta-equilibrium notion is better   \n1142 suited for large-scale complex systems where players\u2019 decision variables can be high-dimensional   \n1143 and continuous, as argued in the ensuing paragraph.   \n1144 To achieve the strategic adaptation, PBE relies on the Bayesian-posterior belief updates, which soon   \n1145 become intractable as the denominator in equation G1 involves integration over high-dimensional   \n1146 space and discretization inevitably leads to the curse of dimensionality. Despite the limited practicality,   \n1147 PBE is inherently difficult to solve, even in finite-dimensional cases. It is shown in [6] that the   \n1148 equilibrium computation in games with incomplete information is NP-hard, and how to solve for   \n1149 PBE in dynamic games remains an open problem. Even though there have been encouraging attempts   \n1150 at solving PBE in two-stage games [36], it is still challenging to address PBE computation in generic   \n1151 Markov games. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "1152 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Please refer to conclusion section and Appendix B. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "74 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n75 the paper has limitations, but those are not discussed in the paper.   \n76 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n77 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n78 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n79 model well-specification, asymptotic approximations only holding locally). The authors   \n80 should reflect on how these assumptions might be violated in practice and what the   \n81 implications would be.   \n82 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n83 only tested on a few datasets or with a few runs. In general, empirical results often   \n84 depend on implicit assumptions, which should be articulated.   \n85 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n86 For example, a facial recognition algorithm may perform poorly when image resolution   \n87 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n88 used reliably to provide closed captions for online lectures because it fails to handle   \n89 technical jargon.   \n90 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n91 and how they scale with dataset size.   \n92 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n93 address problems of privacy and fairness.   \n94 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n95 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n96 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n97 judgment and recognize that individual actions in favor of transparency play an impor  \n98 tant role in developing norms that preserve the integrity of the community. Reviewers   \n99 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "0 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1201 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n1202 a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "204 Justification: Please refer to Appendix F.   \n205 Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "1216 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "17 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n18 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n19 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "21 Justification: Please refer to Appendix C.   \n22 Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes]   \nJustification: The datasets (MNIST and CIFAR-10) are open source, and we will publish the   \ncodes during the final revision stage.   \nGuidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Please check Appendix C. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The error bars are added to Figure 11 (c) and (d), while random seeds are fixed for other figures/tables. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors). ", "page_idx": 35}, {"type": "text", "text": "1310 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1311 of the mean.   \n1312 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1313 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1314 of Normality of errors is not verified.   \n1315 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1316 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1317 error rates).   \n1318 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1319 they were calculated and reference the corresponding figures or tables in the text.   \n1320 8. Experiments Compute Resources   \n1321 Question: For each experiment, does the paper provide sufficient information on the com  \n1322 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1323 the experiments?   \n1324 Answer: [Yes]   \n1325 Justification: Please see Appendix C.   \n1326 Guidelines:   \n1327 \u2022 The answer NA means that the paper does not include experiments.   \n1328 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1329 or cloud provider, including relevant memory and storage.   \n1330 \u2022 The paper should provide the amount of compute required for each of the individual   \n1331 experimental runs as well as estimate the total compute.   \n1332 \u2022 The paper should disclose whether the full research project required more compute   \n1333 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1334 didn\u2019t make it into the paper).   \n1335 9. Code Of Ethics   \n1336 Question: Does the research conducted in the paper conform, in every respect, with the   \n1337 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1338 Answer: [Yes]   \n1339 Justification: We stick to the NeurIPS Code of Ethics.   \n1340 Guidelines:   \n1341 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1342 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1343 deviation from the Code of Ethics.   \n1344 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1345 eration due to laws or regulations in their jurisdiction).   \n1346 10. Broader Impacts   \n1347 Question: Does the paper discuss both potential positive societal impacts and negative   \n1348 societal impacts of the work performed?   \n1349 Answer: [Yes]   \n1350 Justification: Please see Appendix B.   \n1351 Guidelines:   \n1352 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1353 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1354 impact or why the paper does not address societal impact.   \n1355 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1356 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1357 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1358 groups), privacy considerations, and security considerations.   \n1359 \u2022 The conference expects that many papers will be foundational research and not tied   \n1360 to particular applications, let alone deployments. However, if there is a direct path to   \n1361 any negative applications, the authors should point it out. For example, it is legitimate   \n1362 to point out that an improvement in the quality of generative models could be used to   \n1363 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1364 that a generic algorithm for optimizing neural networks could enable people to train   \n1365 models that generate Deepfakes faster.   \n1366 \u2022 The authors should consider possible harms that could arise when the technology is   \n1367 being used as intended and functioning correctly, harms that could arise when the   \n1368 technology is being used as intended but gives incorrect results, and harms following   \n1369 from (intentional or unintentional) misuse of the technology.   \n1370 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1371 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1372 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1373 feedback over time, improving the efficiency and accessibility of ML).   \n1374 11. Safeguards   \n1375 Question: Does the paper describe safeguards that have been put in place for responsible   \n1376 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1377 image generators, or scraped datasets)?   \n1378 Answer: [NA]   \n1379 Justification: The paper poses no safeguards risks.   \n1380 Guidelines:   \n1381 \u2022 The answer NA means that the paper poses no such risks.   \n1382 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1383 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1384 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1385 safety filters.   \n1386 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1387 should describe how they avoided releasing unsafe images.   \n1388 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1389 not require this, but we encourage authors to take this into account and make a best   \n1390 faith effort.   \n1391 12. Licenses for existing assets   \n1392 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1393 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1394 properly respected?   \n1395 Answer: [Yes]   \n1396 Justification: We credited all assets (e.g., code, data, models) used in the paper.   \n1397 Guidelines:   \n1398 \u2022 The answer NA means that the paper does not use existing assets.   \n1399 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1400 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1401 URL.   \n1402 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1403 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1404 service of that source should be provided.   \n1405 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1406 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1407 has curated licenses for some datasets. Their licensing guide can help determine the   \n1408 license of a dataset.   \n1409 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1410 the derived asset (if it has changed) should be provided.   \n1411 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1412 the asset\u2019s creators.   \n1413 13. New Assets   \n1414 Question: Are new assets introduced in the paper well documented and is the documentation   \n1415 provided alongside the assets?   \n1416 Answer: [NA]   \n1417 Justification: We didn\u2019t release new assets at this stage.   \n1418 Guidelines:   \n1419 \u2022 The answer NA means that the paper does not release new assets.   \n1420 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1421 submissions via structured templates. This includes details about training, license,   \n1422 limitations, etc.   \n1423 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1424 asset is used.   \n1425 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1426 create an anonymized URL or include an anonymized zip file.   \n1427 14. Crowdsourcing and Research with Human Subjects   \n1428 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1429 include the full text of instructions given to participants and screenshots, if applicable, as   \n1430 well as details about compensation (if any)?   \n1431 Answer: [NA]   \n1432 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1433 Guidelines:   \n1434 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1435 human subjects.   \n1436 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1437 tion of the paper involves human subjects, then as much detail as possible should be   \n1438 included in the main paper.   \n1439 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1440 or other labor should be paid at least the minimum wage in the country of the data   \n1441 collector.   \n1442 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1443 Subjects   \n1444 Question: Does the paper describe potential risks incurred by study participants, whether   \n1445 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1446 approvals (or an equivalent approval/review based on the requirements of your country or   \n1447 institution) were obtained?   \n1448 Answer: [NA]   \n1449 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1450 Guidelines:   \n1451 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1452 human subjects.   \n1453 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1454 may be required for any human subjects research. If you obtained IRB approval, you   \n1455 should clearly state this in the paper.   \n1456 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1457 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1458 guidelines for their institution.   \n1459 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1460 applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}]