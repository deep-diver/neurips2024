[{"figure_path": "KT6F5Sw0eg/tables/tables_3_1.jpg", "caption": "Table 1: Per-task test performance of each finetuned model and block efficiency over language modeling (LM), extractive question answering (QA), and both long and short summarization (L-Sum & S-Sum).", "description": "This table presents the performance and block efficiency of different language models across various tasks. The tasks include Language Modeling (LAMBADA), Question Answering (SQuAD v1), and Summarization (CNN/DailyMail, SAMSUM, MultiNews, XSUM, Newsroom). For each task, the table shows the performance metric (perplexity for LM, exact match for QA, ROUGE-L for summarization) and the block efficiency. Block efficiency indicates the average number of tokens decoded per serial call to the blockwise parallel language model.", "section": "4 Analysis setup"}, {"figure_path": "KT6F5Sw0eg/tables/tables_3_2.jpg", "caption": "Table 1: Per-task test performance of each finetuned model and block efficiency over language modeling (LM), extractive question answering (QA), and both long and short summarization (L-Sum & S-Sum).", "description": "This table presents the performance of different models on seven tasks: language modeling, question answering, and summarization.  The \"Performance\" column indicates how well each model performs on the task, while the \"Block Efficiency\" column shows the average number of tokens decoded per serial call to the blockwise parallel Language Model, indicating the potential speedup compared to traditional decoding. Lower block efficiency means higher speedup potential.  The tasks are further divided into subcategories (long vs. short summarization).", "section": "4 Analysis setup"}, {"figure_path": "KT6F5Sw0eg/tables/tables_4_1.jpg", "caption": "Table 3: Consecutive token repetition in block drafts before and after C4-trained 2-gram rescoring of the top-16 lattice. \u201c% Consec\u201d is the percentage of consecutive identical draft tokens out of all pairs of consecutive tokens. \u201cMax run\u201d is the average maximum repeated subsequence length in tokens (upper bound of 9, the number of block draft heads). Higher values correspond to more egregious repetition in drafts.", "description": "This table presents the results of an analysis of consecutive token repetition in block drafts, both before and after applying a 2-gram language model for rescoring. It shows the percentage of consecutive tokens that are identical and the average maximum length of repeated sequences for various tasks and datasets. Lower values indicate less repetition and better draft quality.", "section": "5 Exploration of block drafts"}, {"figure_path": "KT6F5Sw0eg/tables/tables_5_1.jpg", "caption": "Table 2: Sample outputs from blockwise parallel LMs finetuned per task. Black indicates standard decoded output, blue indicates accepted draft tokens, and brown is the prompt.", "description": "This table displays example outputs from blockwise parallel language models (LMs) fine-tuned for different tasks (language modeling, question answering, summarization).  Each row shows the drafts generated by different prediction heads in parallel, indicating how the models simultaneously predict multiple tokens. The \"accepted\" tokens (in blue) are those that match the output of the standard autoregressive LM. The \"rejected\" tokens (in red) are those which do not match the standard autoregressive LM.  The prompt for each example is in brown. This illustrates how the blockwise parallel decoding process works and the differences in draft quality across different tasks.", "section": "2.1 Observations on block drafts"}, {"figure_path": "KT6F5Sw0eg/tables/tables_6_1.jpg", "caption": "Table 4: Block efficiency of rescoring methods over the top-16 lattice. '16-best 0-gram BPD' indicates performance of 16-best draft verification over the original lattice without n-gram rescoring. Relative percent improvement over BPD (Baseline) is indicated in parentheses. Green circles (0) indicate improvement over the Baseline, while red circles () denote no improvement.", "description": "This table presents the block efficiency results for different rescoring methods (local rescoring with neural networks, global rescoring with n-gram language models) compared to the baseline BPD method.  The results are shown for various tasks (language modeling, question answering, summarization) and different lattice sizes. The relative improvement over the baseline is shown in parentheses.  Green and red circles indicate whether there was an improvement or not over the baseline.", "section": "6 Lattice rescoring with lightweight rescorers"}, {"figure_path": "KT6F5Sw0eg/tables/tables_7_1.jpg", "caption": "Table 4: Block efficiency of rescoring methods over the top-16 lattice. '16-best 0-gram BPD' indicates performance of 16-best draft verification over the original lattice without n-gram rescoring. Relative percent improvement over BPD (Baseline) is indicated in parentheses. Green circles (0) indicate improvement over the Baseline, while red circles () denote no improvement.", "description": "This table shows the block efficiency results for different rescoring methods on seven tasks using a top-16 lattice.  It compares the baseline blockwise parallel decoding (BPD) method with several rescoring approaches:  local neural rescoring (using different sized models), global n-gram rescoring (using 4-gram language models), and variations on the 16-best draft verification.  The table shows percentage improvement or decrease in block efficiency over the baseline for each method on each task. Green circles indicate improvement while red indicates no improvement or negative improvement. The results highlight that local neural rescoring often provides the best performance, especially when the initial block efficiency was low.", "section": "6.3 Empirical evaluation"}, {"figure_path": "KT6F5Sw0eg/tables/tables_9_1.jpg", "caption": "Table 4: Block efficiency of rescoring methods over the top-16 lattice. '16-best 0-gram BPD' indicates performance of 16-best draft verification over the original lattice without n-gram rescoring. Relative percent improvement over BPD (Baseline) is indicated in parentheses. Green circles (0) indicate improvement over the Baseline, while red circles () denote no improvement.", "description": "This table presents the block efficiency results for various rescoring methods (local neural and global n-gram) and baselines (BPD, 16-best 0-gram BPD) across multiple tasks (LM, QA, S-SUM, L-SUM).  It shows the relative improvement of each method compared to the standard BPD method, highlighting which methods yield significant gains. Green circles indicate performance improvements, while red circles indicate no improvement.  The results reveal the effectiveness of the different rescoring strategies in enhancing the quality of block drafts and improving decoding speed.", "section": "6 Lattice rescoring with lightweight rescorers"}, {"figure_path": "KT6F5Sw0eg/tables/tables_9_2.jpg", "caption": "Table 7: Speedup ratio of efficient LLM inference methods during greedy decoding.", "description": "This table compares the speedup ratios achieved by various efficient large language model (LLM) inference methods, including the proposed method, during greedy decoding.  It shows the speedup relative to standard autoregressive decoding across several benchmark datasets (MT-Bench, S-Sum, QA, GSM8K, RAG) and for different model sizes (Vicuna 7B and 13B). The table highlights the consistent speed improvements provided by the local rescoring method, particularly when compared to other methods whose performance is less consistent.", "section": "7 Lattice rescoring on open-source blockwise parallel LLMs"}, {"figure_path": "KT6F5Sw0eg/tables/tables_15_1.jpg", "caption": "Table 8: Architecture hyperparameters for each of the transformer-based neural language models.", "description": "This table lists the architecture hyperparameters for the various transformer-based neural language models used in the paper.  It shows the model size, the number of layers, the embedding dimension, and the hidden dimension for both the blockwise parallel decoder and the autoregressive decoder models of different sizes (1.5B, 32M, 61M, and 94M parameters). These details are crucial for understanding the experimental setup and the computational resources required for the different models.", "section": "D.2 Neural model details"}, {"figure_path": "KT6F5Sw0eg/tables/tables_16_1.jpg", "caption": "Table 9: Block efficiency from rescoring with in-domain trained rescoring models for 2-gram and 61M parameter neural rescorer.", "description": "This table shows the block efficiency for different rescoring models (2-gram and neural-61M) using both C4-trained and in-domain-trained models across six downstream tasks. It compares the block efficiency when using models trained on the general-purpose C4 dataset versus those trained specifically on the target task's dataset. This helps to analyze whether using task-specific rescoring models leads to improved block efficiency compared to using general-purpose models.", "section": "E Rescoring with in-domain language models"}, {"figure_path": "KT6F5Sw0eg/tables/tables_17_1.jpg", "caption": "Table 10: Tuned interpolation weight per task for neural and n-gram rescoring.", "description": "This table shows the tuned interpolation weight (alpha) for both neural and n-gram rescoring methods across different datasets.  The weights were tuned to maximize block efficiency on a held-out set of examples before evaluating on the remaining data.  The weights vary widely by dataset, indicating that optimal rescoring strategies depend on the specific characteristics of the dataset.  Lower weights suggest that the base model's predictions were more accurate and require less adjustment, while higher weights indicate that the rescorer makes a larger contribution.", "section": "F. Rescoring with in-domain language models"}, {"figure_path": "KT6F5Sw0eg/tables/tables_18_1.jpg", "caption": "Table 4: Block efficiency of rescoring methods over the top-16 lattice. '16-best 0-gram BPD' indicates performance of 16-best draft verification over the original lattice without n-gram rescoring. Relative percent improvement over BPD (Baseline) is indicated in parentheses. Green circles (0) indicate improvement over the Baseline, while red circles () denote no improvement.", "description": "This table presents the block efficiency results for different rescoring methods applied to the top-16 lattice.  The baseline is standard blockwise parallel decoding (BPD).  It shows the relative improvement in block efficiency achieved by using local and global rescoring techniques, compared to the baseline.  Green circles indicate a positive improvement, while red circles indicate no improvement or a negative impact compared to the baseline.", "section": "6 Lattice rescoring with lightweight rescorers"}, {"figure_path": "KT6F5Sw0eg/tables/tables_18_2.jpg", "caption": "Table 1: Per-task test performance of each finetuned model and block efficiency over language modeling (LM), extractive question answering (QA), and both long and short summarization (L-Sum & S-Sum).", "description": "This table presents the performance and block efficiency of different models across seven tasks (language modeling, question answering, and summarization). For each task, it shows the performance metric (perplexity for LM, exact match for QA, ROUGE-L for summarization) and the block efficiency, representing the theoretical speedup compared to standard greedy decoding.", "section": "4 Analysis setup"}, {"figure_path": "KT6F5Sw0eg/tables/tables_19_1.jpg", "caption": "Table 13: Comparative analysis of per decoded token efficiency metrics across block rescoring methods and the standard autoregressive LM (batch size=1). This table shows the average block efficiency, parameter I/O, key-value (KV) cache I/O at varying sequence lengths, and FLOPS-evaluated on a per-token basis with batch size 1.", "description": "This table compares the hardware utilization of different decoding methods: autoregressive, base BPD, 4-gram BPD, neural-61M BPD, 16-best 0-gram BPD, and 16-best 4-gram BPD.  Metrics include average block efficiency, parameter I/O (in GB), key-value cache I/O (in GB) at different sequence lengths (128, 512, 1024, and 2048), and floating-point operations (FLOPS in trillions). It shows how different methods impact resource usage for decoding.", "section": "I Practical efficiency of rescoring block drafts"}, {"figure_path": "KT6F5Sw0eg/tables/tables_20_1.jpg", "caption": "Table 1: Per-task test performance of each finetuned model and block efficiency over language modeling (LM), extractive question answering (QA), and both long and short summarization (L-Sum & S-Sum).", "description": "This table presents the results of experiments evaluating the performance of different language models on various tasks.  The \"Performance\" column shows the scores achieved by the models, while the \"Block Efficiency\" column indicates the efficiency gains of blockwise parallel decoding (BPD) compared to standard autoregressive decoding for each task.  Block efficiency is a key metric for evaluating the speed improvement offered by BPD, with higher values suggesting greater efficiency gains.", "section": "4 Analysis setup"}, {"figure_path": "KT6F5Sw0eg/tables/tables_20_2.jpg", "caption": "Table 1: Per-task test performance of each finetuned model and block efficiency over language modeling (LM), extractive question answering (QA), and both long and short summarization (L-Sum & S-Sum).", "description": "This table presents the results of experiments on seven different tasks: language modeling, extractive question answering, and summarization (both long and short).  For each task, it shows the performance of the fine-tuned model and the block efficiency. Block efficiency measures how many tokens are decoded per serial call to the blockwise parallel LM, representing a speedup compared to standard greedy decoding.  Higher block efficiency indicates faster decoding.", "section": "4 Analysis setup"}]