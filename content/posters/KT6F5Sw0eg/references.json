{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is foundational for the field of large language models and their few-shot learning capabilities, which is directly related to the current paper's focus on improving inference speeds for LLMs."}, {"fullname_first_author": "Mitchell Stern", "paper_title": "Blockwise parallel decoding for deep autoregressive models", "publication_date": "2018-00-00", "reason": "This paper introduces the Blockwise Parallel Decoding (BPD) method, which is the core method improved upon in the current paper."}, {"fullname_first_author": "Tianle Cai", "paper_title": "Medusa: Simple llm inference acceleration framework with multiple decoding heads", "publication_date": "2024-00-00", "reason": "This paper presents Medusa, a related method that also aims to improve LLM decoding speed, which serves as a benchmark and comparison for the current paper's methods."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-00-00", "reason": "This paper introduces the T5 model, a unified text-to-text transformer architecture, which is relevant to the current work as many LLMs use similar architectures."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-00-00", "reason": "This paper highlights the multitask learning capabilities of LLMs, a property that is relevant to the evaluation of the proposed methods across diverse tasks in the current paper."}]}