[{"figure_path": "ZIpdu0cHYu/tables/tables_7_1.jpg", "caption": "Table 1: Main Experiment Results. Avg represents the average pass rate or win rate of the 6 test scenarios. A win rate higher than 50% means the model performs better than ChatGPT+DFSDT.", "description": "This table presents the main experimental results comparing the performance of TP-LLaMA against several baseline models (ChatGPT, Davinci, ToolLLaMA, and LLaMA with SFT) across six different test scenarios.  The scenarios vary in the complexity of the tasks, involving single tools, intra-category multi-tools, and inter-category multi-tools.  Performance is measured using two metrics: pass rate (the probability of the model successfully providing an answer within a limited number of steps) and win rate (the likelihood that an evaluator would prefer the model's response over a reference solution).  The \"Avg\" column shows the average performance across all six scenarios, providing a summary of the overall performance of each model.", "section": "5.2 Main results"}, {"figure_path": "ZIpdu0cHYu/tables/tables_8_1.jpg", "caption": "Table 2: Efficiency Results of TP-LLaMA. Imp denotes the improvement of TP-LLaMA over LLaMA with SFT in terms of the average steps.", "description": "This table presents the average number of steps taken by two models, TP-LLaMA and LLaMA with SFT, during the DFSDT inference process across six test scenarios.  Each scenario is categorized by instruction type (instruction, tool, category) and tool usage (single-tool, intra-category multi-tool, inter-category multi-tool)  The 'Imp' column shows the percentage improvement in the average number of steps achieved by TP-LLaMA compared to LLaMA with SFT.", "section": "5.3 Efficiency Evaluation"}, {"figure_path": "ZIpdu0cHYu/tables/tables_9_1.jpg", "caption": "Table 3: Ablation Performance Experiment Results. Avg represents the average pass rate or win rate of the 6 test scenarios. A win rate higher than 50% means the model performs better than ChatGPT+DFSDT.", "description": "This table presents the results of ablation experiments, where the base language model in the TP-LLAMA framework was replaced with three different models: Mistral-7B, Qwen-1.5-7B, and Gemma-7B.  The table shows the pass rate and win rate for each model on six test scenarios, comparing the performance of the models trained only with successful trajectories (SFT) against the models further trained with preference data obtained from failed explorations (TP-LLAMA). The results demonstrate the consistent effectiveness of the proposed framework across different base models.", "section": "5.4 Ablation experiments"}, {"figure_path": "ZIpdu0cHYu/tables/tables_9_2.jpg", "caption": "Table 4: Ablation Efficiency Experiment Results. Imp denotes the improvement of TP-LLaMA over LLaMA with SFT in terms of the average steps.", "description": "This table presents the results of ablation experiments conducted to evaluate the impact of using different base LLMs on the efficiency of the proposed TP-LLaMA model.  It compares the average number of steps in one successful path for six different test scenarios (G1-Ins, G1-Tool, G1-Cat, G2-Ins, G2-Cat, G3-Ins) across three different base LLMs (Mistral, Qwen, and Gemma) both with and without preference learning (TP-LLaMA). The \"Imp\" column shows the percentage improvement in efficiency achieved by TP-LLaMA over the baseline SFT model for each base LLM.  This demonstrates the robustness of TP-LLaMA's efficiency gains across various LLMs.", "section": "5.4 Ablation experiments"}, {"figure_path": "ZIpdu0cHYu/tables/tables_15_1.jpg", "caption": "Table 1: Main Experiment Results. Avg represents the average pass rate or win rate of the 6 test scenarios. A win rate higher than 50% means the model performs better than ChatGPT+DFSDT.", "description": "This table presents the main experimental results, comparing the performance of TP-LLaMA against several baseline models (ChatGPT, Davinci, ToolLLaMA, and LLaMA with SFT) across six test scenarios.  The scenarios vary in difficulty, testing the models' abilities with single tools, intra-category multi-tools, and inter-category multi-tools, as well as their generalization to unseen APIs and instructions. Performance is measured using two metrics: pass rate (probability of successful completion within a limit of steps) and win rate (preference for the model's answer compared to the ChatGPT+DFSDT approach).  A higher pass rate indicates better performance and a win rate above 50% shows superiority to the ChatGPT+DFSDT method.", "section": "5.2 Main results"}, {"figure_path": "ZIpdu0cHYu/tables/tables_17_1.jpg", "caption": "Table 5: New Pass Rate Experiment Results", "description": "This table presents the results of experiments using a new pass rate definition based on GPT evaluation, comparing ToolLLaMA, LLaMA with SFT, and TP-LLaMA across six test scenarios.  The pass rate is calculated based on whether the model successfully provides an answer, and whether GPT evaluates that answer as correct and solvable with the available APIs.  Two different GPT versions (gpt-3.5-turbo-16k and gpt-3.5-turbo-1106) were used in the evaluation, highlighting the impact of GPT version on the results.", "section": "B Supplementary experiment results"}]