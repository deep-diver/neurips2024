[{"figure_path": "ZIpdu0cHYu/figures/figures_1_1.jpg", "caption": "Figure 1: Our Inference Trajectory Optimization Framework.", "description": "This figure illustrates the workflow of the proposed inference trajectory optimization framework. It starts with user queries interacting with a real-world environment via RapidAPI Hub APIs.  A depth-first search-based decision tree explores multiple reasoning paths, some leading to success (green checkmark) and others to failure (red X).  Successful trajectories are collected and used for supervised fine-tuning (SFT) of a pre-trained LLM.  The framework then introduces a novel method for generating preference data from both successful and failed steps of the decision tree, resulting in a ToolPreference dataset. This dataset is used with direct preference optimization (DPO) to further refine the LLM, leading to the final ToolPrefer-LLaMA (TP-LLaMA) model.", "section": "4 Our method"}, {"figure_path": "ZIpdu0cHYu/figures/figures_6_1.jpg", "caption": "Figure 2: Depth-first search-based decision tree and two preference data construction methods", "description": "This figure illustrates the structure of depth-first search-based decision trees used in the ToolBench dataset and demonstrates two methods for constructing preference data from these trees: path-wise and step-wise.  The left side shows a sample decision tree, where nodes represent LLM decisions about API calls, and edges represent the transitions between decisions. Successful and failed paths are highlighted in green and red, respectively. The right side illustrates the two preference data construction methods. The path-wise method uses an entire success path and an entire failure path to form a preference pair. The step-wise method uses branch nodes along the success path and its corresponding child nodes to create preference pairs.  This figure visually explains how the authors leverage both successful and failed paths within the decision trees to create a more comprehensive preference dataset.", "section": "4.2 Preference data construction"}, {"figure_path": "ZIpdu0cHYu/figures/figures_14_1.jpg", "caption": "Figure 1: Our Inference Trajectory Optimization Framework.", "description": "This figure illustrates the overall framework of the proposed inference trajectory optimization method. It starts with a pre-trained LLM and utilizes a depth-first search-based decision tree to explore tool usage.  Successful and failed trajectories are collected to create a preference dataset (ToolPreference). This dataset is then used in a direct preference optimization (DPO) step to fine-tune the LLM, resulting in the ToolPrefer-LLaMA model (TP-LLaMA).", "section": "1 Introduction"}, {"figure_path": "ZIpdu0cHYu/figures/figures_16_1.jpg", "caption": "Figure 1: Our Inference Trajectory Optimization Framework.", "description": "This figure illustrates the proposed inference trajectory optimization framework. It shows the process starting from user queries, then depth-first search-based decision trees exploring different API calls.  Successful and failed steps are collected to create a ToolPreference dataset used in direct preference optimization (DPO) with a pre-trained LLM. This results in the final TP-LLaMA model. The framework highlights the incorporation of insights from failed paths to improve LLM performance.", "section": "1 Introduction"}]