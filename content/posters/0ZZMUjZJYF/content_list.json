[{"type": "text", "text": "Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuefei $\\mathbf{Ning}^{*1}$ , Zifu Wang\u22172, Shiyao $\\mathbf{L}\\mathbf{i}^{*1,3}$ , Zinan $\\mathbf{Lin}^{*4}$ , Peiran $\\mathbf{Yao^{*3,5}}$ , Tianyu $\\mathbf{F}\\mathbf{u}^{1,3}$ , Matthew B. Blaschko2, Guohao Dai6,3, Huazhong $\\mathbf{Yang^{1}}$ , Yu Wang1 ", "page_idx": 0}, {"type": "text", "text": "1Tsinghua University $^{2}\\mathrm{KU}$ Leuven 3Infinigence-AI 4Microsoft Research 5University of Alberta 6Shanghai Jiao Tong University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Teaching to improve student models (e.g., knowledge distillation) is an extensively studied methodology in LLMs. However, in human education, teaching enhances not only the students but also the teachers by fostering more rigorous and clearer reasoning, as well as deeper knowledge building. We ask: Can LLMs also learn by teaching (LbT) for better reasoning? If the answer is yes, we can potentially unlock the possibility of continuously advancing the models without solely relying on human-produced data or stronger models. In this paper, we provide a preliminary exploration of this question. We show that LbT ideas can be incorporated into existing LLM training/prompting pipelines and bring improvements. Specifically, we design three methods, each mimicking one of the three levels of LbT: observing students\u2019 feedback, learning from the feedback, and learning iteratively, with the goal of improving answer accuracy without training or improving models\u2019 inherent capability with fine-tuning. We reveal some findings: (1) Teaching materials that make it easier for students to learn (via in-context learning) have clearer and more accurate logic; (2) Weak-to-strong generalization: LbT might help improve strong models by teaching weak models; (3) Diversity in students might help: teaching multiple students could be better than teaching a single student or the teacher alone. We hope that our exploration can inspire future research on LbT and, more broadly, the adoption of advanced education techniques to improve LLMs. The code and website are at https://github.com/imagination-research/lbt and https://sites.google.com/view/llm-learning-by-teaching. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "I couldn\u2019t reduce it to the freshman level. That means we really don\u2019t understand it. ", "page_idx": 0}, {"type": "text", "text": "\u2013 Richard Feynman ", "page_idx": 0}, {"type": "text", "text": "\u201cLearning from teachers (LfT)\u201d is a common pipeline in machine learning, especially in the realm of Large Language Models (LLMs). For example, knowledge distillation [24, 44, 75] and distillation via synthetic data [1, 28, 41] focus on transferring the knowledge from teacher LLMs to student LLMs by letting teacher models teach student models through token logits, features, or synthetic data [85]. They become the go-to methods for closing the performance gap between open-source and proprietary LLMs, as well as for maintaining performance during model compression. ", "page_idx": 0}, {"type": "text", "text": "In fact, in human learning, teaching not only benefits students but can also improve the teachers themselves. \u201cLearning by teaching (LbT)\u201d, also known as the Feynman learning method, is proven to improve human learning by fostering rigorous and clear reasoning as well as knowledge building [5, 18, 22, 33, 62\u201365]. Fig. 1 illustrates the conceptual comparison of the LfT and LbT pipelines. ", "page_idx": 0}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/8dc26881cb5b838a0effe149d8a3b2bbafbc071cd22f7395aa68aaaba4f0fc79.jpg", "img_caption": ["Figure 1: Left: Learning from teacher aims at improving student LLMs with knowledge from the teacher LLMs. It is the essential idea behind common approaches including knowledge distillation and distillation via synthetic data. Right: In contrast, Learning by teaching aims at improving teacher LLMs through the teaching process using feedback from student LLMs. "], "img_footnote": [], "page_idx": 1}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/34cd3526318ef7075e3f71e1957c8b191fe82edf0eb034f353ad2578ce020855.jpg", "table_caption": ["Table 1: The explored M1, M2, M3 methods. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Motivated by this insight, in order to improve one of the most crucial abilities of LLMs \u2013 the reasoning ability, we want to ask: Can LLMs also learn by teaching for better reasoning? In addition to improving reasoning, as one can imagine, LbT could open exciting opportunities for the models to continuously evolve by teaching other (potentially weaker) models, rather than solely relying on human-produced data or stronger teacher models. More broadly, we hope that this exploration could provide insights on borrowing advanced education techniques to improve LLMs [16, 32, 53]. ", "page_idx": 1}, {"type": "text", "text": "To explore this question, we draw on learning science literature that connects LbT in human learning with reflection [5, 13, 48] and knowledge-building [62\u201364], summarizing three levels of LbT: ", "page_idx": 1}, {"type": "text", "text": "\u2022 L1: Observing students\u2019 feedback. The teacher instructs the students, who then provide feedback (e.g., taking exams and reporting the scores, asking questions about unclear logic).   \n\u2022 L2: Learning from the feedback. Based on the feedback, the teacher can analyze which logic and concepts the students might have (mis)understood. This information is useful for the teachers to improve their teaching strategy, and further enhance the teacher\u2019s own understanding of the concepts.   \n\u2022 L3: Learning from the feedback iteratively. The teacher can teach the students, observe the feedback (L1), and learn from the feedback (L2) iteratively. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we study the viability of instantiating these LbT ideas in LLMs. There is a range of possibilities in terms of the objective, the pipeline, and the implementation ( $\\S\\,2$ and Tab. 1). As an initial exploration, we study three methods, each for one of the three LbT levels. ", "page_idx": 1}, {"type": "text", "text": "\u2022 M1 aims at improving LLMs\u2019 answer quality by directly utilizing students\u2019 feedback (L1). More specifically, given a set of generated answers, we score each rationale based on its ability to teach student models using in-context learning (ICL) to correctly answer similar problems. We show that aggregating multiple rationales [79] with LbT-based scores can improve the answer accuracy. Notably, M1 improves GPT-4o\u2019s accuracy on the MATH dataset [27] from $87.84\\%$ to $96.69\\%$ . \u2022 M2 aims at improving LLMs\u2019 inherent ability by learning from students\u2019 feedback (L2). We use the approach in M1 to score teacher-generated rationales. Then, we apply direct preference optimization (DPO) [59] to fine-tune the teacher model with the rationale-score pairs. We show that M2 is better than using DPO with correctness scores. \u2022 M3 aims at improving LLMs\u2019 answer quality by iteratively learning from students\u2019 feedback (L3). Specifically, we prompt the LLM to reflect on the failure cases of multiple students and devise new positive and negative exemplars. We show that the LLM can improve the exemplars based on feedback from multiple students. These improved exemplars used in prompts not only improve the learning outcomes for multiple students but also enhance the teacher\u2019s performance. ", "page_idx": 1}, {"type": "text", "text": "We reveal some interesting or promising findings related to LbT: ", "page_idx": 1}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/852eb3ed0f7d29f602702d6b8a11e96f9315d737a415060c307f849ceaed1ffa.jpg", "img_caption": ["(a) The \u201csearch-based output generation pipeline\u201d for improving the answer quality. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/c76c9fb795d0c09753c91eb75a3403f46a389b5e93939c75dc66e941428062c2.jpg", "img_caption": ["(b) The \u201cgenerating-scoring-finetuning pipeline\u201d for improving model capability. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Two general pipelines for improving the answer quality and model capability. $^{\\bullet\\bullet}$ stands for \u201cProblem\u201d; $\\mathbf{\\nabla}^{*}\\mathbf{R}^{*}$ stands for \u201cRationale\u201d; \u201cA\u201d stands for \u201cAnswer\u201d. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Teaching materials that make it easier for students to learn have clearer and more accurate logic (LbT-TMQ1assumption) when using ICL as the student\u2019s \u201clearning\u201d method: Our LbTbased scoring relies on this assumption, and our results and inspection support this assumption. \u2022 Weak-to-strong generalization: Strong teachers can improve even when teaching weaker students, suggesting some promise of using LbT to improve superhuman models [6]. \u2022 Diversity in students might help: Rather than teaching the teacher itself, teaching other students and multiple students might help. This suggests the feasibility of using LbT to synergize the capability and knowledge from multiple models. ", "page_idx": 2}, {"type": "text", "text": "To summarize, with appropriate pipelines and teacher-student settings, LbT can help improve LLMs\u2019 answer quality and inherent capability. We believe that these preliminary case studies are only scratching the surface of the potential of LbT. As LLMs are becoming increasingly powerful, more advanced approaches in pedagogy can potentially help with the inference and training of LLMs. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work of Our Learning by Teaching Implementations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As shown in Tab. 1, we study two types of objectives: improving answer quality without training and improving the inherent ability of the model with training. $\\S\\,2.1$ and $\\S\\,\\,2.2$ describe how M1, M2, and M3 relate to prior work on these two objectives, respectively. See App. D.5 for more discussion. ", "page_idx": 2}, {"type": "text", "text": "2.1 Improving the Answer Quality without Training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Existing literature has incorporated various insights from the human reasoning process to develop prompting-based methods, including writing down the thinking process [36, 81], subproblem decomposition [52, 56, 94], fetching the abstract principles and answering based on them [93], selfreflection-based answer refinement [47, 67], and so on. We explore two ways of incorporating the LbT insight to implement two prompting-based methods: ", "page_idx": 2}, {"type": "text", "text": "\u2022 M1 relates to the popular \u201csearch-based output generation pipeline\u201d shown in Fig. 2a [4, 42, 45, 47, 67, 79, 84, 87]. This pipeline iteratively samples and evaluates new rationales or rationale steps for searching the optimal output, and ultimately derives the final rationale or answer from the search history. One essential component in this pipeline is an evaluator who evaluates the quality of each rationale or rationale step. We design an LbT evaluator that scores each generated rationale based on its ability to teach student models to correctly answer similar problems. \u2022 M3 relates to existing prompt optimization methods [57, 71, 95] that iteratively improve the prompts based on their performance (e.g., accuracy, failure cases). The key innovation in M3 is how it evaluates the \u201cperformance\u201d: instead of evaluating with the same model that produced the prompts (i.e., the teacher model), we test how the prompt works with other student models and show that this change benefits the prompt tuning outcome. ", "page_idx": 2}, {"type": "text", "text": "2.2 Improving the Inherent Model Capability with Training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To improve the inherent model capability, M2 incorporates the LbT insight into the \u201cgeneratingscoring-finetuning pipeline\u201d. Fig. 2b illustrates the three steps in the pipeline: (1) Letting the target ", "page_idx": 2}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/a11b3920f5836495f57469b6424974c65f1566e94e80cfd825b1def3ba0f85b9.jpg", "img_caption": ["Figure 3: M1. The goal is to derive the best TA from the TR-TA pairs generated by the teacher LLM. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "LLM or a teacher LLM generate multiple rationales for a given problem; (2) Scoring the rationales using an evaluator; (3) Utilizing the rationales and scores to (optionally) train a verifier [14, 45, 77], and finetune the target LLM by reinforcement learning [77], DPO [59, 88] or its variant [49, 55], filtering and supervised finetuning (SFT) [31, 89, 90], or score-conditioned SFT [43, 46]. ", "page_idx": 3}, {"type": "text", "text": "In these works, the rationale scoring is usually achieved through manual labeling [45, 46, 99], groundtruth (GT) answer matching [89, 90], agreement-based scoring [31], or self-evaluation [88]. In contrast, M2 scores the rationale based on its ability to teach student models to correctly answer similar problems. In this way, M2 can provide automatic and fine-grained quality evaluation for rationales, which helps automate and improve the continual evolution of models\u2019 capability. ", "page_idx": 3}, {"type": "text", "text": "3 Method (M1) for LbT Level 1: Observing Students\u2019 Feedback ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One common teaching strategy in education is that the teacher first teaches students how to solve a class of problems by giving them the example rationale (named Teaching Rationale, or $T R$ in short) and the answer (named Teaching Answer, or TA in short) to a particular question (named Teaching Problem, or $T P$ in short). Then, the teacher asks students to solve other similar problems (named Exam Problem, or $E P$ in short) to test if the students understand the concepts. The teacher can also learn from this process by observing the feedback (i.e., LbT level 1): if the students can answer EPs well, then it likely means that the TR-TA pair is of high-quality. ", "page_idx": 3}, {"type": "text", "text": "Our idea is to implement this strategy in LLMs to select high-quality TR-TA pairs. As depicted in Fig. 3 and Alg. A1, we first instruct the teacher model to solve a given TP multiple times, resulting in multiple TR-TA pairs. Then, each TR-TA pair is used as an in-context learning (ICL) example to guide the student model in solving a series of EPs. With the produced Exam Rationales (ERs) and Exam Answers (EAs), each student will then receive an exam score (i.e., the accuracy of EAs), denoted as the LbT score. The LbT score can be used as a quality assessment of the corresponding TR-TA pair. We consider two ways to select the final TA [79]: (1) We select the TR-TA pair with the highest LbT score. We denote this approach as \u201cM1 (MAX)\u201d. (2) For TAs that can be aggregated via exact matching, such as mathematical reasoning, we can take the sum of the LbT scores for each TA separately and find the TA with the maximum sum. We denote this approach as \u201cM1 (SUM)\u201d. ", "page_idx": 3}, {"type": "text", "text": "The following subsections present the evaluation of M1 on mathematical reasoning and code synthesis tasks. Please refer to App. D for the rationale behind the task selection. ", "page_idx": 3}, {"type": "text", "text": "3.2 Evaluation on Mathematical Reasoning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.2.1 Experimental Setups ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use the extension MATH() [72] of the MATH dataset [27], where each problem has variants with different values. Following the train-test split specified by [45], among the 500 test problems, 181 problems are provided with 3 functional variants each. We use these 181 problems as TPs. For each TP, we sample 256 TR-TA pairs. Then, using each TR-TA pair as the ICL exemplar, we use the 3 functional variants of TP as EPs. Each exam is repeated 3 times with randomized student decoding, resulting in 9 ER-EA pairs. Each TA is scored based on the correctness of the 9 EAs. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We show the results in Tab. 2 and provide analyses as follows. More results are in App. A. ", "page_idx": 3}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/e2e774ab50d1fae8cbaf43a6f8e3b33e758babb2e9c00363a6c9f2aa735cda29.jpg", "table_caption": ["Table 2: Results on 181 MATH test problems with 256 TR-TA pairs. The best results of each row are highlighted in green. The \u201cImprov\u201d column calculates the improvements of average performance achieved by M1 (SUM) over SC. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "M1 is effective with various model settings and surpasses baselines. M1 exceeds self-consistency (SC) [79] with various model settings: strong-teach-weak (e.g., GPT-4o teaches GPT-4o mini), weakteach-strong (e.g., Mistral-7B teaches LLaMA3-8B), and self-teaching (e.g., LLaMA3-8B teaches itself). M1 (SUM) outperforms M1 (MAX) in most cases. We also show that LbT-based scoring surpasses self-evaluation scoring [35, 74, 84, 88] in Tab. A7. Since M1 incurs higher inference cost than SC when using the same number of TR-TA pairs, we also conduct an experiment in Tab. A6, showing that with comparable or much lower compute, M1 with just 24 TR-TA pairs achieves a $0.17\\%{\\sim}8.29\\%$ accuracy improvement over SC with 256 TR-TA pairs. ", "page_idx": 4}, {"type": "text", "text": "M1 can further benefit from multiple students. Using GPT-3.5 to teach both LLaMA3-8B and Mistral-7B achieves a significant improvement than teaching LLaMA3-8B or Mistral-7B separately. ", "page_idx": 4}, {"type": "text", "text": "M1 can identify infrequent but correct TAs. M1 can efficiently discover the correct answer from many teacher samples, whereas SC requires the correct answer to be in the majority to derive it. Fig. 4 (left, middle) shows the improvements of M1 over SC across different numbers of TR-TA pairs and difficulty levels. The relative improvement of M1 over SC increases as the number of TR-TA pairs or the difficulty levels grow within the experimental range. ", "page_idx": 4}, {"type": "text", "text": "The TP and the corresponding EPs should be similar. It is crucial to choose EPs similar to a TP such that the student can apply the logic from TR to solve EPs. We use the functional variants as EPs, which are very similar to TPs. To verify the necessity of TP-EPs similarity, we conduct an experiment that selects similar EPs from the original MATH training set. We calculate the embedding of each TP using the \u201call-mpnet-base-v2\u201d sentence embedding model [60], and select the 2 closest problems from the training set as EPs. We sort TPs by the cosine distance to the corresponding EPs and calculate the relative improvements over SC on a fraction of TPs. Fig. 4 (right) shows that M1 only provides improvements for TPs that have similar problems in the training set. ", "page_idx": 4}, {"type": "text", "text": "3.3 Evaluation on Competition-Level Code Synthesis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.3.1 Experimental Setups ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use the Grandmaster Dynamic Programming (DP) study plan on LeetCode.2 Each dataset in the study plan has $5\\!\\sim\\!10$ problems, and each problem has $2{\\sim}3$ visible test cases and many hidden test cases. We assign a visible score (V-score) of 1 and 0 to the code that passes all or fails any visible cases [43]. To evaluate the actual correctness of a code, we submit the code to LeetCode, and record the pass rate on the hidden cases as the submit score (S-score). For a TP, we sample 8 TR-TA pairs from the teacher, where TR is a rationale in natural language, and TA is a Python code (See Ex. 1 for an example). Each TR-TA pair is assigned an LbT score by teaching a student to solve the remaining problems in the dataset. M1 calculates the exam V-score as the LbT score to avoid additional LeetCode submissions. Check App. A.3.2 for additional setups. ", "page_idx": 4}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/bfcb13c5b6b0cf24d0ca00d1014880eb0fc799152a6d493fbcb7fac7dd791ec0.jpg", "table_caption": ["Table 3: S-score results on Game Theory dataset in LeetCode Grandmaster DP study plan. \u201cSG-1\u201d- \u201cSG-4\u201d and \u201cPW\u201d are abbreviations of individual questions in the dataset; see Tab. A8 for details. The results of M1 that improve (degrade) by more than 0.01 are highlighted in green (red). "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3.2 Results Here, we analyze the results on the Game Theory dataset. Check App. A.3 for additional results. ", "page_idx": 5}, {"type": "text", "text": "M1 can be more general than agreement-based methods such as SC. M1 (MAX) does not require an oracle to assess the equivalence of two answers, which is challenging for codes. Therefore, we only use the average pass rate (with or without V-score $^{=1}$ filtering) as the baseline. Nevertheless, when such an oracle is provided [8, 43, 66], we can use M1 (SUM) which was shown to be better than M1 (MAX) in $\\S\\ 3.2$ . We defer this exploration to future work. ", "page_idx": 5}, {"type": "text", "text": "M1 selects better TR-TA than the baseline in most cases. If the student closely follows the strategies in TR-TA to solve EPs, the student exam score can indicate the quality of TR-TA. (1) When the TR-TA has high quality (Ex. 1), the student mimics the teacher\u2019s strategy to solve the EP with a correct DP code. (2) When the TR-TA is logically incorrect, e.g., DP code with wrong recurrences (Ex. 2) or a non-DP wrong code (Ex. 3), the student also follows the wrong TR-TA with a wrong ER and EA. (3) When the TR-TA is logically correct but has high complexity, e.g. recursive re-computation instead of DP (Ex. 4), the student also writes a recursion with high complexity. ", "page_idx": 5}, {"type": "text", "text": "As shown in Tab. 3, using the V-score on the few visible test cases can filter out some low-quality code, but M1 can identify better TA in most cases. This is because LbT-based scoring can leverage student scores on similar EPs, providing a more informative evaluation of TA. Note that M1 shows the largest improvements on TPs with medium difficulty. For very simple (e.g., SG-4 for GPT-3.5) or challenging (e.g., SG-2) problems, M1 shows marginal or no improvements. ", "page_idx": 5}, {"type": "text", "text": "Self-Debugging (SD) is both complementary to and beneficial for M1. We experiment with applying one-iteration SD [11] using Prompt 5. Applying SD on TAs can provide S-score benefits complementary to M1, since SD fixes simple non-logical bugs, such as missing imports, miswritten variable names, and incorrect usage of library functions (an example is shown in Ex. 6), whereas M1 mainly assess the quality of the logic. In addition, applying SD on EAs leads to more informative LbT score, as fixing non-logical bugs can make the students\u2019 exam V-score more indicative of quality of the TR-TA. Tab. 3 shows that after incorporating SD for both M1 and the baselines, M1 achieves consistent improvements. ", "page_idx": 5}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/f96bf010521256b8414c90d2183e6c74fb7e45166355b500d99eb4085dc5418c.jpg", "img_caption": ["Figure 5: Baseline vs. M2. Both approaches use scores of TRs to craft preference data and finetune the teacher LLM with DPO. Left: The correctness score of TA. Right: The LbT score of TR and TA. ", "", ""], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/4e8c9a07ad7a0d460c3c8fe3958643c582d4b00210dea5623943a0fa1a3a4184.jpg", "table_caption": ["Table 4: Results on 500 MATH test problems with greedy decoding. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "For competition-level code synthesis task, M1 is more effective when the teacher and student come from the same family, as shown by Tabs. 3 and A10. We find that this is because the student can follow a teacher from the same model family better, making the feedback more informative. A failure case of student-following when GPT-3.5 teaches LLaMA3-8B is shown in Ex. 5. ", "page_idx": 6}, {"type": "text", "text": "TPs and EPs should be similar. Most failure cases in Tabs. 3 and A10 occur when solving the \u201cPW\u201d TP. We find that this is because the solving of \u201cPW\u201d involves 2D DP, which differs from other problems that can be solved with 1D DP. Consequently, the student cannot follow TA to solve EPs. ", "page_idx": 6}, {"type": "text", "text": "4 Method (M2) for LbT Level 2: Learning from the Feedback 4.1 Method ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In education, after identifying which teaching materials (e.g., TR-TA pairs) can enhance student performance $(\\S\\ 3.1)$ , teachers can use this information to improve their knowledge or teaching strategies. For example, if students perform poorly due to unclear or inaccurate teaching materials, teachers can correct their knowledge and avoid generating similar TR-TA pairs in the future. ", "page_idx": 6}, {"type": "text", "text": "We use this idea to train the LLMs to improve its reasoning ability. As depicted in Fig. 5, since the LbT-based scoring provides informative feedback on the quality of a TR-TA pair (verified in $\\S\\ 3$ ), we collect the LbT scores of many TR-TA pairs and use them to finetune the teacher with DPO [59]. ", "page_idx": 6}, {"type": "text", "text": "4.2 Experimental Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use 1564 training problems from MATH() [72] as TPs. For each TP, we sample 32 TR-TA pairs from the teacher. For each TR-TA pair, we calculate $0.5\\;\\times$ correctness score + $\\cdot\\,0.5\\times\\mathrm{LbT}$ score as its final score, where the correctness score is 1 or 0 when the corresponding TA is correct or wrong, respectively. For running DPO, we select pairs from the 32 TR-TA pairs whose score difference exceeds a threshold of 0.3, and keep at most 8 pairs of TR-TA pairs for each TP. ", "page_idx": 6}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tab. 4 shows that M2 achieves better results compared to solely using the correctness scores in DPO. This improvement is because LbT provides more informative scores than those purely based on correctness. One example is shown in Ex. 10. Although both TRs produce a correct TA, the losing TR is unnecessarily verbose and cannot be generalized to other similar problems. Another example is in Ex. 11. Although both TRs produce a wrong TA, the winning TR is logically better than the loser. LbT can discern the correct preference between these TR-TA pairs, thereby improving DPO results. ", "page_idx": 6}, {"type": "text", "text": "5 Method (M3) for LbT Level 3: Learning from the Feedback Iteratively 5.1 Method ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have shown that the students\u2019 exam scores can serve as an indicator of the reasoning quality of the teaching rationales. This indicator can be leveraged to aggregate better answers in M1 and to further fine-tune the teacher in M2. In M3, we explore whether reflecting on students\u2019 detailed exam responses can help the teacher iteratively refine its teaching materials. Notably, we aim to verify whether these refinements can enhance the teacher\u2019s own performance by providing more effective knowledge. If so, we can assert that the iterative process of teaching, reflection, and material refinement facilitates some form of \u201cknowledge building\u201d [62\u201364] for the teacher. Additionally, we are interested in whether having multiple and diverse LLMs as students offers further benefits. ", "page_idx": 6}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/8a99e038d8636b4a5f8624d5463cbb61a1f23de31e5e179f96daa73e9933d10b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/9120bfd9b57db6ee8e9b768614e48853174d67ae45fc4e0c70a53b6f353038f6.jpg", "table_caption": ["Figure 6: Overview of M3. The teacher teaches the students through a set of positive and negative ICL examples. These examples are iteratively refined by the teacher according to students\u2019 feedback. Table 5: Teacher\u2019s $F_{1}$ score of M3 on combined Liar dev and test set at the end of iteration $T$ , where LLaMa3-70B is used as the teacher for all settings. The best results are in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Specifically, we guide the teacher to iteratively improve teaching materials in the form of a set of positive and negative exemplars, based on the student and teacher performance when the set is used as the ICL examples. As depicted in Fig. 6 and Alg. A2, given a classification task, we first sample $K=8$ positive and negative exemplars from the teacher, and then run multiple refinement iterations. Finally, we report the teacher performance on the test set when using the resulting ICL examples. ", "page_idx": 7}, {"type": "text", "text": "Each iteration contains the following steps: (1) The current exemplars are used as the ICL examples to teach students to answer a set of $E P s$ . The EPs are randomly sampled from the training data in each iteration. (2) We select the EPs that students answered incorrectly and prompt the teacher to reflect on why the current exemplars might have misled students in these instances. (3) Based on the reflection, the teacher generates multiple updated exemplar sets. (4) We keep the exemplar set that achieves the best teacher performance on the training data when the set is used as the ICL examples. ", "page_idx": 7}, {"type": "text", "text": "5.2 Experimental Setups ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate M3 on two binary text classification tasks: Liar [78] and Logical Fallacy [34]. Liar is a dataset for false statement detection in political media, with 4,574 statements with speaker and context information. Logical Fallacy is a dataset of 2,449 samples of 13 logical fallacy types, which we adapt to classify the most common type faulty generalization against the rest of the types. We report the teacher $F_{1}$ score on the dev and test splits combined. Within an iteration, we choose the exemplar set with the highest teacher $F_{1}$ score on the training set. Across 14 random experiments, we report the mean $F_{1}$ and the standard deviation. We run a total of five refinement iterations. ", "page_idx": 7}, {"type": "text", "text": "5.3 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As Tabs. 5 and A17 shows, it is feasible to apply LbT on iterative prompt optimization: LLMs are able to reflect on the failure cases of students and propose revised exemplars that improve the teacher\u2019s performance, similar to the case of iteratively optimizing task descriptions as in previous work [57]. ", "page_idx": 7}, {"type": "text", "text": "More importantly, we observe a performance gain brought by having dedicated students (as opposed to using a single LLM in prompt optimization as in previous work). Comparing to the scenario where the teacher and student are the same, having one or multiple LLMs different to the teacher as the student improves the quality of the teaching material faster. This demonstrates LbT as a case of weak-to-strong generalization. We speculate that the beneftis are brought by more diverse error types made by a different (weaker) student model; see App. C.3 for more examples and analyses. ", "page_idx": 7}, {"type": "text", "text": "6 Broader Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Insights into In-Context Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Currently, we conduct student \u201clearning\u201d with ICL, based on the assumption that students can effectively \u201clearn\u201d from ICL examples and apply similar strategies to solve EPs. Interestingly, prior work [50] found that a correct input-output pairing in ICL examples does not matter much. At first glance, this finding seems to challenge our design, as it suggests that the TA accuracy may not affect the EA accuracy, which means the LbT score cannot reflect the quality of $\\mathrm{TR+TA}$ . However, we find that, as opposed to only providing labels in the ICL examples [50], providing rationales is important. LLMs can follow the problem-solving logic in the detailed rationale in the ICL examples well. This may be because the rationale provides more information, making the ICL examples easier to follow. Consequently, we see that students can use similar logic as the TR when solving the EP. This means that better $\\mathrm{TR}+\\mathrm{TA}$ can indeed lead to improved ER and thus higher EA accuracy (i.e., LbT score). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Our findings highlight two key factors for successful ICL following and for establishing a positive correlation between EA accuracy and TR quality/TA accuracy: (1) similarity between TP and EP, and (2) the use of Chain-of-Thought (i.e., detailed rationale). See App. A.3.5 for the effect of natural language rationale on ICL following in code synthesis. Note that we tried explicitly instructing the student to follow the ICL example, but it did not work well. We hope these findings can complement the current understanding of ICL [26, 40, 50, 83] and offer insights for improving ICL. ", "page_idx": 8}, {"type": "text", "text": "6.2 Weak-to-Strong Generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Improving models with human-generated/annotated data or synthetic data from stronger models is the dominant paradigm. However, how can we continuously improve the strongest model without relying on human-generated and annotated data? A recent work [6] conducts an exploration on using weak model supervision to train a larger model. Our work is another attempt towards the \u201cweak-to-strong generalization\u201d prospect by drawing from how humans continuously acquire new knowledge without direct instruction. We demonstrate that stronger models can further improve their own results (M1), parameters (M2), and prompt (M3) by utilizing the feedback of weaker models. ", "page_idx": 8}, {"type": "text", "text": "6.3 Limitations and Near-Term Extensions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "M1 and M2 rely on generating/selecting similar EPs. We verify that LbT-based scoring can help select high-quality TR-TAs but require the TP and EPs having similar problem-solving strategies. In our experiments, suitable EPs are selected according to human-provided information in the dataset. One extension is to let a model automatically identify EPs similar to a TP from a large pool (Fig. 4). Another direction is to synthesize similar problems based on a group of problems and exploit the LbT principle to score many rationales for the new problems. Specifically, as a \u201cself-instruct\u201d [80] extension to M2, we can generate a new problem $P$ based on a group of problems $S=\\{P_{1},\\cdots,P_{k}\\}$ that are already known to be similar. The generating-scoring pipeline can then be applied to $P$ to obtain rationale-score pairs, where the LbT score can be easily obtained using $S$ as the EPs. ", "page_idx": 8}, {"type": "text", "text": "Additional inference cost. LbT-based scoring in M1 and M2 requires additional inference cost, which aligns with recent studies that show that increasing inference cost might be a promising way to improve models\u2019 reasoning capabilities [54, 69]. Nevertheless, designing efficient inference algorithms and systems [96] is needed to make these approaches more usable. ", "page_idx": 8}, {"type": "text", "text": "See App. E for other extensions and App. F for the discussion on potential risks of bias perpetuation. ", "page_idx": 8}, {"type": "text", "text": "6.4 Borrowing Education Strategies to Improve LLMs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Borrowing the design strategies of teaching materials. We show an LbT pipeline in Fig. 7. Each iteration involves six steps: (1) The teacher generates the Teaching Material (TM). (2) The student learns from the TM. Our work uses in-context learning for all student learning, but exploring other learning strategies is an interesting future direction. (3) The student provides feedback. The feedback can take many forms as listed in the figure. Our work mainly explored feedback in the form of exam details and scores. (4) The teacher reflects on the feedback and identifies the knowledge gaps in the TM or in the teacher\u2019s own knowledge. (5) The teacher can optionally refer to some external data source to address its own knowledge gaps. (6) The teacher improves the rigorousness, clarity, and completeness of their knowledge and updates the TM for the next iteration. ", "page_idx": 8}, {"type": "text", "text": "On one hand, updating TM can improve the teacher\u2019s own knowledge. For example, M3 saves the updated exemplars as the teacher\u2019s prompt to improve the teacher\u2019s reasoning. On the other hand, a high-quality TM helps students learn better, so that the students can provide more meaningful feedback for the teacher. To create high-quality TM, it might be beneficial to borrow from TM design strategies in human education. Fig. 7 summarizes various TM design strategies, among which our work has explicitly explored three types (marked in black). ", "page_idx": 8}, {"type": "text", "text": "Borrowing the pipelines. We can borrow insights from education pipelines to design LLM inference and training pipelines. For example: (1) Task-oriented collaborative multi-agent learning [51]: ", "page_idx": 8}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/789c77098159c079c202606115a69418cb9fd2d2b946c973cf8fc7f9e98a776f.jpg", "img_caption": ["Figure 7: LbT pipeline and the summary of knowledge types, TM design strategies, and feedback. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Multiple LLM agents can form a collaborative study group to learn difficult topics in a task-oriented manner (see some discussions in App. D.2). Similar multi-agent collaboration ideas have been leveraged by LLM agent research [12, 17, 29, 39, 58, 91]. (2) Better LbT by configuring proper teacher&student: Literature on \u201cteachable agents\u201d finds that configuring a student\u2019s knowledge level appropriately can lead to more useful feedback for the teacher [5, 65]. This suggests the intriguing possibility of prompting the student LLM to \u201cconfine\u201d its knowledge level [33], thereby amplifying the beneftis of LbT for the teacher model. Furthermore, a junior model $\\mathcal{M}$ can first teach a \u201cstudent\u201d that is stronger than itself, who can understand and critique mistakes and ambiguity. As $\\mathcal{M}$ becomes stronger, it might become better at evolving its knowledge by teaching weaker students. This can be seen as a form of easy-to-hard task progression in curriculum learning [3]. (3) Flexible teaching quality evaluation: In human learning, feedback can take many forms [25, 82] beyond traditional exams [62\u201364], such as peer recommendations [48] and satisfaction questionnaires [37, 61]. Such mechanisms can be adapted to LLMs, potentially useful for open-ended tasks. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Aiming to improve LLM reasoning, we conduct a preliminary exploration of whether LLMs can \u201clearn by teaching\u201d\u2014 a well-known paradigm in human learning. We implement the LbT idea into well-established pipelines to develop three methods, and evaluate whether they improve reasoning performance on complex tasks such as mathematical reasoning and competition-level code synthesis: ", "page_idx": 9}, {"type": "text", "text": "\u2022 M1 is based on the LbT-TMQ assumption. Specifically, we adopt ICL as the instructional method and measure the students\u2019 success in grasping the logic embedded in ICL examples by evaluating their performance on similar EPs. M1 is implemented as a standard \u201csearch-based output generation\u201d pipeline with an LbT-based rationale scoring component. Results: In mathematical reasoning, M1 achieves a $3.31\\%{\\sim}18.23\\%$ accuracy improvement over the competitive SC baseline on 181 MATH test problems with 256 TR-TA pairs (Tab. 2). Note that M1 achieves a $3.32\\%{\\sim}4.98\\%$ improvement on the powerful GPT-4o, reaching a high accuracy of $96.69\\%$ . Using comparable or much lower compute, M1 with 24 TR-TA pairs achieves a $0.17\\%{\\sim}8.29\\%$ accuracy improvement over SC with 256 TR-TA pairs (Tab. A6). In code synthesis, M1 achieves notable improvements in submission score in most scenarios, particularly when the teacher and student belong to the same model family (Tabs. 3, A11 and A13).   \n\u2022 M2 uses the LbT scores from M1 to fine-tune the teacher with DPO. M2 is implmented as a standard \u201cgenerating-scoring-finetuning\u201d pipeline with an LbT-based rationale scoring component. Results: In the experiment of fine-tuning LLaMA3-8B, the M2-tuned model achieves a $1.8\\%$ accuracy improvement over the model tuned with correctness-based DPO, evaluated on 500 MATH test problems with standard greedy decoding (Tab. 4).   \n\u2022 M3 lets the LLM iteratively refine ICL examples by analyzing the students\u2019 feedback. Results: For two binary text classification tasks requiring common-sense and logic reasoning, M3 can craft better ICL examples through multiple refinement rounds, and the feedback from students other than the teacher itself is beneficial (Tabs. 5 and A17). ", "page_idx": 9}, {"type": "text", "text": "In summary, the LbT idea is implemented as a scoring method in M1 and M2, and as an iterative refining pipeline in M3 (Tab. 1). Our results suggest LbT\u2019s potential for harnessing the diversity offered by different students and facilitating weak-to-strong generalization in improving reasoning. ", "page_idx": 9}, {"type": "text", "text": "We believe our work only scratches the surface of leveraging educational principles to improve LLMs. Will these approaches find greater use as LLMs grow more intelligent? We discuss our research rationale and roadmap for exploring this intriguing question in App. D and $\\S\\ 6$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China (No. 62325405, 62104128, U19B2019, U21B2031, 61832007, 62204164), Tsinghua EE Xilinx AI Research Fund, and Beijing National Research Center for Information Science and Technology (BNRist). We thank for all the supports from Infinigence-AI. Z.W. and M.B.B. acknowledge support from the Research Foundation - Flanders (FWO) through project numbers G0A1319N and S001421N, and funding from the Flemish Government under the Onderzoeksprogramma Artifici\u00eble Intelligentie (AI) Vlaanderen programme. Z.W. and M.B.B. acknowledge LUMI-BE for awarding this project access to the LUMI supercomputer, owned by the EuroHPC JU, hosted by CSC (Finland) and the LUMI consortium, and EuroHPC JU for awarding this project access to the Leonardo supercomputer, hosted by CINECA. We thank Sergey Yekhanin from Microsoft Research for their support and suggestions for the work. We thank Zixuan Zhou, Chao Yu, Boxun Li for their discussions. We thank the anonymous reviewers for their insightful suggestions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.   \n[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \n[3] Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348, 2009.   \n[4] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682\u201317690, 2024.   \n[5] Gautam Biswas, Krittaya Leelawong, Daniel Schwartz, Nancy Vye, and The Teachable Agents Group at Vanderbilt. Learning by teaching: A new agent paradigm for educational software. Applied Artificial Intelligence, 19(3-4):363\u2013392, 2005.   \n[6] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.   \n[7] Xiaofeng Cao, Yaming Guo, Tieru Wu, and Ivor W Tsang. Black-box generalization of machine teaching. arXiv preprint arXiv:2206.15205, 2022.   \n[8] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations, 2023.   \n[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[10] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023.   \n[11] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024.   \n[12] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et al. Exploring large language model based intelligent agents: Definitions, methods, and prospects. arXiv preprint arXiv:2401.03428, 2024.   \n[13] Michelene TH Chi, Stephanie A Siler, Heisawn Jeong, Takashi Yamauchi, and Robert G Hausmann. Learning from human tutoring. Cognitive science, 25(4):471\u2013533, 2001.   \n[14] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[15] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 113\u2013123, 2019.   \n[16] Aniket Rajiv Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy P Lillicrap, Danilo Jimenez Rezende, Yoshua Bengio, Michael Curtis Mozer, and Sanjeev Arora. Metacognitive capabilities of LLMs: An exploration in mathematical problem solving. In AI for Math Workshop @ ICML 2024, 2024.   \n[17] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. ACM Trans. Softw. Eng. Methodol., jun 2024.   \n[18] David Duran. Learning-by-teaching. evidence and implications as a pedagogical mechanism. Innovations in education and teaching international, 54(5):476\u2013484, 2017.   \n[19] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. Journal of Machine Learning Research, 20(55):1\u201321, 2019.   \n[20] Matthias Feurer and Frank Hutter. Hyperparameter optimization. Automated machine learning: Methods, systems, challenges, pages 3\u201333, 2019.   \n[21] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[22] Alan Gartner et al. Children teach children: Learning by teaching. 1971.   \n[23] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789\u20131819, 2021.   \n[24] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[25] John Hattie and Helen Timperley. The power of feedback. Review of educational research, 77(1):81\u2013112, 2007.   \n[26] Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.   \n[27] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.   \n[28] Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14852\u201314882, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[29] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J\u00fcrgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024.   \n[30] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149\u20135169, 2021.   \n[31] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1051\u20131068, Singapore, December 2023. Association for Computational Linguistics.   \n[32] Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, and Marco Pavone. Learning from teaching regularization: Generalizable correlations should be easy to imitate. arXiv preprint arXiv:2402.02769, 2024.   \n[33] Hyoungwook Jin, Seonghee Lee, Hyungyu Shin, and Juho Kim. Teach ai how to code: Using large language models as teachable agents for programming education. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI \u201924, New York, NY, USA, 2024. Association for Computing Machinery.   \n[34] Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schoelkopf. Logical fallacy detection. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7180\u20137198, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.   \n[35] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.   \n[36] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022.   \n[37] Janet Leckey and Neville Neill. Quantifying quality: the importance of student feedback. Quality in Higher Education, 7(1):19\u201332, 2001.   \n[38] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843\u20133857, 2022.   \n[39] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36, 2023.   \n[40] Jiaoda Li, Yifan Hou, Mrinmaya Sachan, and Ryan Cotterell. What do language models learn in context? the structured task hypothesis. arXiv preprint arXiv:2406.04216, 2024.   \n[41] Shiyang Li, Jianshu Chen, yelong shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan. Explanations from large language models make small reasoners better. In 2nd Workshop on Sustainable AI, 2024.   \n[42] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315\u20135333, 2023.   \n[43] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.   \n[44] Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less is more: Task-aware layer-wise distillation for language model compression. In International Conference on Machine Learning, pages 20852\u201320867. PMLR, 2023.   \n[45] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.   \n[46] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. In The Twelfth International Conference on Learning Representations, 2024.   \n[47] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.   \n[48] Lynn McAlpine, Cynthia Weston, Catherine Beauchamp, C Wiseman, and Jacinthe Beauchamp. Building a metacognitive model of reflection. Higher education, 37:105\u2013131, 1999.   \n[49] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. arXiv preprint arXiv:2405.14734, 2024.   \n[50] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.   \n[51] Joel M Moskowitz, Janet H Malvin, Gary A Schaeffer, and Eric Schaps. Evaluation of jigsaw, a cooperative learning technique. Contemporary educational psychology, 10(2):104\u2013112, 1985.   \n[52] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeletonof-thought: Prompting llms for efficient parallel generation. In The Twelfth International Conference on Learning Representations, 2024.   \n[53] Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Sch\u00f6lkopf, Abulhair Saparov, and Mrinmaya Sachan. Do language models exhibit the same cognitive biases in problem solving as human learners? In International Conference on Machine Learning, pages 38762\u201338778. PMLR, 2024.   \n[54] OpenAI. Learning to reason with llms, Sep 2024.   \n[55] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024.   \n[56] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687\u20135711, Singapore, December 2023. Association for Computational Linguistics.   \n[57] Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with \u201cgradient descent\u201d and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7957\u20137968, Singapore, December 2023. Association for Computational Linguistics.   \n[58] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.   \n[59] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[60] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China, November 2019. Association for Computational Linguistics.   \n[61] John TE Richardson. Instruments for obtaining student feedback: A review of the literature. Assessment & evaluation in higher education, 30(4):387\u2013415, 2005.   \n[62] Rod D Roscoe and Michelene TH Chi. The influence of the tutee in learning by peer tutoring. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 26, 2004.   \n[63] Rod D Roscoe and Michelene TH Chi. Understanding tutor learning: Knowledge-building and knowledge-telling in peer tutors\u2019 explanations and questions. Review of educational research, 77(4):534\u2013574, 2007.   \n[64] Rod D Roscoe and Michelene TH Chi. Tutor learning: The role of explaining and responding to questions. Instructional science, 36:321\u2013350, 2008.   \n[65] Tasmia Shahriar and Noboru Matsuda. \u201ccan you clarify what you said?\u201d: Studying the impact of tutee agents\u2019 follow-up questions on tutors\u2019 learning. In Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14\u201318, 2021, Proceedings, Part I 22, pages 395\u2013407. Springer, 2021.   \n[66] Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3533\u20133546, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.   \n[67] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[68] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755\u2013 759, 2024.   \n[69] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.   \n[70] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. Advances in neural information processing systems, 25, 2012.   \n[71] Alessandro Sordoni, Eric Yuan, Marc-Alexandre C\u00f4t\u00e9, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux. Joint prompt optimization of stacked llms using variational inference. In Advances in Neural Information Processing Systems, volume 36, pages 58128\u201358151. Curran Associates, Inc., 2023.   \n[72] Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, et al. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. arXiv preprint arXiv:2402.19450, 2024.   \n[73] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472, 2024.   \n[74] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5433\u20135442, Singapore, December 2023. Association for Computational Linguistics.   \n[75] Inar Timiryasov and Jean-Loup Tastet. Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 279\u2013289, Singapore, December 2023. Association for Computational Linguistics.   \n[76] Thiemo Wambsganss, Vinitra Swamy, Roman Rietsche, and Tanja K\u00e4ser. Bias at a second glance: A deep dive into bias for german educational peer-review data modeling. arXiv preprint arXiv:2209.10335, 2022.   \n[77] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, $\\mathrm{Yu}\\ \\mathrm{Wu}$ , and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426\u20139439, 2024.   \n[78] William Yang Wang. \u201cliar, liar pants on fire\u201d: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422\u2013426, Vancouver, Canada, July 2017. Association for Computational Linguistics.   \n[79] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[80] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484\u201313508, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[82] Benedikt Wisniewski, Klaus Zierer, and John Hattie. The power of feedback revisited: A meta-analysis of educational feedback research. Frontiers in psychology, 10:487662, 2020.   \n[83] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \n[84] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2024.   \n[85] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. A survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024.   \n[86] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[87] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[88] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.   \n[89] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.   \n[90] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.   \n[91] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[92] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4320\u20134328, 2018.   \n[93] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[94] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[95] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023.   \n[96] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. A survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024.   \n[97] Xiaojin Zhu. Machine teaching: An inverse problem to machine learning and an approach toward optimal education. In Proceedings of the AAAI conference on artificial intelligence, volume 29, 2015.   \n[98] Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N Rafferty. An overview of machine teaching. arXiv preprint arXiv:1801.05927, 2018.   \n[99] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "M1 19   \nA.1 Workflow 19   \nA.2 Mathematical Reasoning 19   \nA.2.1 Prompt Design 19   \nA.2.2 Additional Experimental Setups 20   \nA.2.3 Additional Results 20   \nA.3 Competition-Level Code Synthesis 21   \nA.3.1 Prompt Design 21   \nA.3.2 Additional Experimental Setups 22   \nA.3.3 Additional Results . . 23   \nA.3.4 Examples 26   \nA.3.5 Natural Language Rationale has Positive Effect on Code Following in ICL 34   \nB M2 38   \nB.1 Additional Experimental Setups 38   \nB.2 Examples 38   \nM3 40   \nC.1 Workflow 40   \nC.2 Prompt Design 40   \nC.3 Examples 41   \nD Discussion on Research Rationale 42   \nD.1 Target (the \u201cnail\u201d in the high level) 42   \nD.2 Idea (the \u201chammer\u201d in the high level) 43   \nD.3 Task (the \u201cnail\u201d in the low level) . 43   \nD.4 Implementation (the \u201chammer\u201d in the low level) 44   \nD.5 Relation to other machine learning techniques 45   \nE More Extension Possibilities 45   \nF Potential Risks of Bias Perpetuation 46 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A M1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.1 Workflow ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The pseudo-code for M1 is illustrated in Alg. A1. In each iteration of the for loop, we calculate the LbT score of one TA in the following three steps: ", "page_idx": 18}, {"type": "text", "text": "1. Use teacher LLM to sample TR-TA pairs: As shown in line 4 of the Alg. A1, given a TP, the teacher LLM samples diverse TR-TA pairs as the teaching materials. The prompts of the teacher are shown in Prompt 1 (MATH) and Prompt 3 (Coding).   \n2. Use student LLM to take the exam: As shown in lines 7-11 in Alg. A1, the sampled TPTR-TA pair serves as an in-context demonstration and we prompt the student LLM to solve related EPs. The prompts of the student LLM are shown in Prompt 2 (MATH) and Prompt 4 (Coding). Finally, we evaluate each EA and store its exam score to calculate the LbT score.   \n3. Calculate the LbT score for TA: As illustrated in lines 14-18 in Alg. A1, we use the exam scores of EPs to calculate the LbT score for each TA. For MATH, we employ both the MAX and SUM modes, while for Coding, we use only the MAX mode. ", "page_idx": 18}, {"type": "text", "text": "Finally, after getting the LbT score for all TAs, we select the TA with the largest LbT score. ", "page_idx": 18}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/e89499b691a7c3682436081619d212773eb8f1930d8b901ba9589ccafa0f74a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.2 Mathematical Reasoning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.2.1 Prompt Design ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For MATH, we use 4-shot examples from Minerva [38]. The prompt for the student is the same as the teacher, except that an additional shot from the teacher (i.e. a TP-TR-TA pair) is appended after the original 4-shot examples. ", "page_idx": 18}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/a77825d5d44ca103d726267124086ce1004af1a16ec18bcc09838b1baebc38a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.2.2 Additional Experimental Setups ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In $\\S\\ 3.2.1$ , both the teacher and the student use the same sampling parameters. Following [73], we use top-K sampling with $K{=}20$ and a temperature of 0.7. ", "page_idx": 19}, {"type": "text", "text": "In Tab. A7, we provide additional results in small-scale experiments. Specifically, for each of the 7 categories in MATH, we select 10 problems, resulting in a total of $70\\,\\mathrm{TPs}$ . We then sample 16 TR-TA pairs for each TP. The rest of the procedure follows $\\S\\ 3.2.1$ . ", "page_idx": 19}, {"type": "text", "text": "A.2.3 Additional Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "M1 requires additional inference costs for the student, raising concerns about whether M1 can surpass the baseline within the same budget. We provide additional results in Tab. A6. Notably, using only 24 TR-TA pairs in M1 still outperforms SC with 256 TR-TA pairs, especially on the most recent LLaMA3 models. Under this setting, M1 has a lower inference cost than SC, particularly when comparing M1 and SC on LLaMA3-70B and using LLaMA3-8B as the student (the first row of the table). ", "page_idx": 19}, {"type": "text", "text": "Furthermore, as shown in $\\S\\ 3.2.2$ , the improvement over the baseline does not saturate and actually increases, indicating that M1 could achieve a higher upper-bound performance. ", "page_idx": 20}, {"type": "text", "text": "In Tab. A7, we compare LbT-based scoring with self-evaluation scoring [35, 74, 84, 88]. Our method consistently outperforms the self-evaluation baseline. Additionally, the table reveals that the performance gap between the strong-teach-strong and strong-teach-weak settings is relatively small. ", "page_idx": 20}, {"type": "text", "text": "Table A6: Results on 181 MATH test problems. SC is with 256 TR-TA pairs, while M1 is with 24 TR-TA pairs. Standard deviations are calculated using the bootstrap sampling technique [43], where size-24 subsets are sampled from the 256 TR-TA pairs as the TR-TA set for M1, and standard deviations are computed across 10 sets. The \u201cImprov\u201d column calculates the improvements of average performance achieved by M1 (SUM) over SC. ", "page_idx": 20}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/724bb61249ed83665971b9ac73b15d0eea3f9eaed64bffb88b2901c524177c95.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/70a0eb061face163a138273034471f06acab04b4bd65865969fd655990db2a6c.jpg", "table_caption": ["Table A7: Results on 70 MATH problems with 16 TR-TA pairs. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.3 Competition-Level Code Synthesis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.3.1 Prompt Design ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/e92ed73f14b9b4a562cb271fb705b57a391189c9afba21b1149cca6e66be0bd0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/02bec064497fa2c946f6fbfc9b02a077bab0e2ed76757fee615d220597eab3b5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/90a4c2ce20ea7cd87b2d7aa67de3045af0e4074714bcc5702939058fb209cbe0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.3.2 Additional Experimental Setups ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To get the problems from LeetCode, we follow the approach of Reflexion [67] to use LeetCode\u2019s official API to obtain all datasets from the Grandmaster dynamic programming study plan. We also employ GPT-3.5 to extract all visible examples for offline evaluation. Additionally, we use the LeetCode Python API to submit the Python code and evaluate the code on invisible examples. ", "page_idx": 21}, {"type": "text", "text": "As shown in Alg. A1, within the for loop, we first have the teacher LLM sample TR and TA. To validate LbT, we introduce randomness at this step to generate a variety of TR-TA pairs. For GPT-3.5- 0613, we set the temperature and top-P to 1, while for the LLaMA3 family, we set the temperature to 0.6 and top-P to 0.9 (default setting). Next, we prompt the student LLM with the sampled TR-TA as ICL examples to solve EPs. In this step, we use greedy sampling. ", "page_idx": 21}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/a640b2aca6d28996fa9680ce31910641f20ba61522bfd40b01daba7d5b0d0658.jpg", "table_caption": ["Table A9: S-score results with standard deviation on Game Theory dataset in LeetCode Grandmaster DP study plan. \u201cSG-1\u201d-\u201cSG-4\u201d and \u201cPW\u201d are abbreviations of individual questions in the Game Theory dataset. The standard deviations are calculated using the bootstrap sampling technique [43], where size-4 subsets are sampled from the 8 TR-TA pairs as the TR-TA set for M1, and the standard deviation of performance is computed across 20 sets. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/de74c861e8bcaca1cf5f109525e575a5e059bd12021f299582677ad7b6b0532e.jpg", "table_caption": ["Table A8: The question IDs and the question title names of problems in Game Theory, Bitmasking, General-1D, and Tricky Invariant datasets. Note that only the Tricky Invariant dataset is related to the binary search, while the remaining three datasets focus on dynamic programming. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.3.3 Additional Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As shown in Table A9, we use bootstrap sampling (choose 4 TR-TAs from 8 TR-TAs, 20 sets are sampled) to produce the results in Table 3 with standard deviations. ", "page_idx": 22}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/75809197e6c5fd8c3a8d2d87af259f72b62949a3b2d1506de4f6c300b2a5c369.jpg", "table_caption": ["Table A10: Ablation of model settings on Game Theory dataset in LeetCode Grandmaster DP study plan: The teacher and studnet belong to different model families. The results of M1 that improve (degrade) by more than 0.01 are highlighted in green (red). "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/0c04b7f6bba8a451f2120ae6bfa4034a736c7888ba381fda5c0e9d3f298a8c78.jpg", "table_caption": ["Table A11: S-score results on Bitmasking dataset in LeetCode Grandmaster DP study plan. Here, the teacher and student are the same. This dataset is too difficult for LLaMA3-8B, which can hardly solve these coding problems. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/70a270722d7e8fc75dd63334f02823e90916946c28dfeab1ab75783e8a7daaef.jpg", "table_caption": ["Table A12: S-score results on Bitmasking dataset in LeetCode Grandmaster DP study plan. Here, the teacher and student are different. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/c47280554caf3e5d70cdc6217c0fbce0e7867109a7a9b9b776195c1ddfbedbca.jpg", "table_caption": ["Table A13: S-score results on General-1D dataset in LeetCode Grandmaster DP study plan. Here, the teacher and student are the same. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/31789c99d8058da2712c58e342dcffd5b0d2d5cadc1a587cb06459534cc3f6b0.jpg", "table_caption": ["Table A14: S-score results on General-1D dataset in LeetCode Grandmaster DP study plan. Here, the teacher and student are different. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "A.3.4 Examples ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Student models can make both logical and non-logical errors. The non-logical errors \u2013 such as missing imports, miswritten variable names, improper handling of simple boundary conditions, or incorrect usage of library functions \u2013 are mainly related to the knowledge required by specific EPs and the robustness of the student model, rather than the quality of the TR-TA pair. Thus, these errors do not provide helpful feedback to the teacher. To address this, we apply self-debugging to correct non-logical bugs, making the students\u2019 exam V-score more reflective of the TR-TA quality and therefore more helpful to the teacher. We show an example of how self-debugging resolves a simple boundary condition bug in Ex. 6. ", "page_idx": 25}, {"type": "text", "text": "Regarding logical errors, our experiments show that students generally follow the TR-TA pair in making logical errors or writing inefficient logic. In such cases, the students\u2019 exam V-score provides valuable feedback for the teacher. We have discussed the examples Exs. 1 to 4 in $\\S\\ 3.3.2$ . Nevertheless, there exist cases where the student fails to follow the logic of the TR-TA pair to solve the EPs. We find that this undesirable situation is more likely to occur when the student and teacher come from different model families. An example of a failure case is shown in Ex. 5. This discrepancy is highlighted by the comparisons between Tabs. 3 and A10, between Tabs. A11 and A12, and between Tabs. A13 and A14, where negative cases are more frequent in Tabs. A10, A12 and A14 when the teacher and student belong to different model families. ", "page_idx": 25}, {"type": "text", "text": "Example 1. M1 on Code Synthesis 1 (Teacher=Student=GPT-3.5-turbo-0613) ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/25e68cd16c33aa418e4c265d3936fa6f955fbbac2d1bd2e44e38528c25909daf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "b) Taking the pile at index \u2018j\u2018 and the maximum number of stones from piles \u2018i\u2018 to \u2018j-1\u2018 (Bob\u2019s turn). ", "page_idx": 26}, {"type": "text", "text": "4. Return the winner: After the dynamic programming iteration, check if the maximum number of stones Alice can collect (from piles $\\mathrm{^\\circ[0,n-1]^{\\ast}}$ ) is greater than the maximum number of stones Bob can collect. If so, return True; otherwise, return False. ", "page_idx": 26}, {"type": "text", "text": "1 from typing import List   \n2   \n3 class Solution:   \n4 def stoneGame(self , piles: List[int ]) -> bool:   \n5 n = len(piles)   \n6 dp $=$ [[0] \\* n for _ in range(n)]   \n7   \n8 # Base case: diagonal elements   \n9 for i in range(n):   \n10 dp[i][i] $=$ piles[i]   \n11   \n12 # Dynamic programming iteration   \n13 for length in range (2, n+1):   \n14 for i in range(n - length + 1):   \n15 j = i + length 1   \n16 dp[i][j] $=$ max(piles[i] - dp[i+1][j], piles[j] - dp[i][j -1])   \n17   \n18 # Return the winner   \n19 return dp [0][n-1] > 0 ", "page_idx": 26}, {"type": "text", "text": "EP: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "(Leetcode Question ID: 486) Write a python code \"\"\"class Solution(): def predictTheWinner(self, nums: List[int]) $->$ bool:\"\"\" to solve the following problem: You are given an integer array \u2018nums\u2018. Two players are playing a game with this array: player 1 and player 2. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "ER and EA: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To determine if Player 1 can win the game, you can use a dynamic programming approach. Let\u2019s think step by step: ", "page_idx": 26}, {"type": "text", "text": "1. Create a 2D array: Create a 2D array of size $\\mathbf{\\acute{n}}\\,^{\\circ}$ by $\\mathrm{\\Omega^{\\ast}}$ , where $\\mathbf{\\dot{n}}^{*}$ is the length of the \u2018nums\u2018 list. This array will store the maximum score difference between Player 1 and Player 2 when considering the subarray from index \u2018i\u2018 to index \u2018j\u2018. ", "page_idx": 26}, {"type": "text", "text": "2. Base case: Initialize the diagonal elements of the 2D array with the values of the numbers in the \u2018nums\u2018 list, as Player 1 and Player 2 can only choose one number each. ", "page_idx": 26}, {"type": "text", "text": "3. Dynamic programming iteration: Iterate over the 2D array diagonally, dynamically computing the maximum score difference between Player 1 and Player 2 when considering the subarray from index \u2018i\u2018 to index \u2018j\u2018. The maximum score difference is the maximum of two options: ", "page_idx": 26}, {"type": "text", "text": "a) The number at index \u2018i\u2018 minus the maximum score difference between Player 1 and Player 2 when considering the subarray from index $\\omega_{1+1}\\,^{\\ast}$ to index \u2018j\u2018 (Player 1\u2019s turn), or ", "page_idx": 26}, {"type": "text", "text": "b) The number at index \u2018j\u2018 minus the maximum score difference between Player 1 and Player 2 when considering the subarray from index \u2018i\u2018 to index \u2018j-1\u2018 (Player 2\u2019s turn). ", "page_idx": 26}, {"type": "text", "text": "4. Return the result: After the dynamic programming iteration, check if the maximum score difference between Player 1 and Player 2 when considering the entire array (from index $\"0\"$ to index \u2018n-1\u2018) is greater than or equal to $\"0\"$ . If so, return True; otherwise, return False. ", "page_idx": 26}, {"type": "text", "text": "12345678910111213141516171819 ", "page_idx": 26}, {"type": "text", "text": "from typing import List   \nclass Solution: def predictTheWinner (self , nums: List[int ]) -> bool: n = len(nums) $\\begin{array}{r c c c l}{\\mathbf{dp}}&{=}&{[\\,[\\,0\\,]}&{*}&{\\mathbf{n}}\\end{array}$ for _ in range(n)] # Base case: diagonal elements for i in range(n): dp[i][i] $=$ nums[i] # Dynamic programming iteration for length in range (2, $\\mathtt{n}+1\\,;$ ): for i in range(n - length + 1): j = i + length - 1 dp[i][j] $=$ max(nums[i] - dp[i+1][j], nums[j] - dp[i][j -1]) # Return the result return dp [0][n-1] >= 0 ", "page_idx": 26}, {"type": "text", "text": "Example 2. M1 on Code Synthesis 2 (Teacher=Student=GPT-3.5-turbo-0613) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "TP: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "(Leetcode Question ID: 877) Write a python code \"\"\"class Solution(): def stoneGame(self, piles: List[int]) $->$ bool:\"\"\" to solve the following problem: Alice and Bob play a game with piles of stones. There are an even number of piles arranged in a row, ", "page_idx": 27}, {"type": "text", "text": "TR and TA: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To solve this problem, we can use a dynamic programming approach. Let\u2019s break down the problem-solving strategy step by step: ", "page_idx": 27}, {"type": "text", "text": "1. Initialize Variables: The code initializes two variables $\\mathrm{~\\hat{n}~}^{\\epsilon}$ and $\\cdot\\mathbf{m}^{\\ast}$ to keep track of the total number of piles and an empty cache dictionary. ", "page_idx": 27}, {"type": "text", "text": "2. Recursive Function: The code defines a recursive function called \u2018aliceWins\u2018 which takes two parameters: \u2018start\u2018 and \u2018end\u2018. This function represents the state of the game - the range of piles currently available. ", "page_idx": 27}, {"type": "text", "text": "3. Base Case: The base case of the recursive function is when there are no more piles left, i.e., when \u2018start $>$ end\u2018. In this case, we return True if Alice\u2019s total number of stones is greater than Bob\u2019s total number of stones, otherwise return False. ", "page_idx": 27}, {"type": "text", "text": "4. Memoization: Before calculating the result for the current state, we check if the result is already present in the cache dictionary. If so, we return the cached result instead of recalculating it. ", "page_idx": 27}, {"type": "text", "text": "5. Maximum Stones Calculation: We calculate the maximum number of stones Alice can obtain if she starts from the \u2018start\u2018 pile and Bob plays optimally. We have two options: ", "page_idx": 27}, {"type": "text", "text": "- Alice takes the \u2018start\u2018 pile: In this case, Alice\u2019s total number of stones will be the \u2018start\u2018 pile minus the maximum number of stones Bob can obtain from the remaining piles. Bob\u2019s maximum number of stones can be obtained using the \u2018aliceWins\u2018 function recursively by changing the \u2018start\u2018 index to \u2018start+1\u2018 and keeping the \u2018end\u2018 index the same. - Alice takes the \u2018end\u2018 pile: In this case, Alice\u2019s total number of stones will be the \u2018end\u2018 pile minus the maximum number of stones Bob can obtain from the remaining piles. Bob\u2019s maximum number of stones can be obtained using the \u2018aliceWins\u2018 function recursively by changing the \u2018end\u2018 index to \u2018end- $\\cdot1^{\\,\\cdot}$ and keeping the \u2018start\u2018 index the same. ", "page_idx": 27}, {"type": "text", "text": "6. Update Cache and Return Result: After calculating the maximum number of stones Alice can obtain, we update the cache dictionary with the result for the current state \u2018(start, end)\u2018 and return the result. ", "page_idx": 27}, {"type": "text", "text": "7. Main Function: The code defines the main \u2018stoneGame\u2018 function that calls the \u2018aliceWins\u2018 function with the initial state $\\left(0,\\mathfrak{n}\\mathrm{-}1\\right)$ \u2018 and returns the result. ", "page_idx": 27}, {"type": "text", "text": "from typing import List ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "class Solution (): def stoneGame(self , piles: List[int ]) -> bool: n = len(piles) cache $=$ {} # Cache dictionary to store results def aliceWins(start: int , end: int) -> bool: # Base case - no more piles left if start $>$ end: return sum(piles) $\\%\\ \\ 2\\ \\ =\\ \\ 1$ # Check if result is already cached if (start , end) in cache: return cache [(start , end)] # Option 1: Alice takes the start pile option1 $=$ piles[start] - aliceWins(start + 1, end) # Option 2: Alice takes the end pile option2 $=$ piles[end] - aliceWins(start , end - 1) # Update cache with the result and return cache [(start , end)] $=$ max(option1 , option2) > 0 return cache [(start , end)] # Call the recursive function with initial state (0, n -1) return aliceWins (0, n -1) ", "page_idx": 27}, {"type": "text", "text": "EP: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "(Leetcode Question ID: 486) Write a python code \"\"\"class Solution(): def predictTheWinner(self, nums: List[int]) $->$ bool:\"\"\" to solve the following problem: You are given an integer array \u2018nums\u2018. Two players are playing a game with this array: player 1 and player 2. ", "page_idx": 27}, {"type": "text", "text": "ER and EA: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To solve this problem, we can use a dynamic programming approach. Let\u2019s break down the problem-solving strategy step by step: ", "page_idx": 28}, {"type": "text", "text": "1. Initialize Variables: The code initializes two variables $\\mathbf{\\dot{n}}^{\\epsilon}$ and $\\mathbf{\\omega}^{\\ast}\\mathbf{m}^{\\ast}$ to keep track of the total number of elements in the array and an empty cache dictionary. ", "page_idx": 28}, {"type": "text", "text": "2. Recursive Function: The code defines a recursive function called \u2018player1Wins\u2018 which takes two parameters: \u2018start\u2018 and \u2018end\u2018. This function represents the state of the game - the range of elements currently available. ", "page_idx": 28}, {"type": "text", "text": "3. Base Case: The base case of the recursive function is when there are no more elements left, i.e., when \u2018start $>$ end\u2018. In this case, we return True if Player 1\u2019s score is greater than or equal to Player 2\u2019s score, otherwise return False. ", "page_idx": 28}, {"type": "text", "text": "4. Memoization: Before calculating the result for the current state, we check if the result is already present in the cache dictionary. If so, we return the cached result instead of recalculating it. ", "page_idx": 28}, {"type": "text", "text": "5. Maximum Score Calculation: We calculate the maximum score Player 1 can obtain if they start from the \u2018start\u2018 element and both players play optimally. We have two options: ", "page_idx": 28}, {"type": "text", "text": "- Player 1 takes the \u2018start\u2018 element: In this case, Player 1\u2019s score will be the \u2018start\u2018 element plus the minimum score Player 2 can obtain from the remaining elements. Player 2\u2019s minimum score can be obtained using the \u2018player1Wins\u2018 function recursively by changing the \u2018start\u2018 index to \u2018start+1\u2018 and keeping the \u2018end\u2018 index the same. ", "page_idx": 28}, {"type": "text", "text": "- Player 1 takes the \u2018end\u2018 element: In this case, Player 1\u2019s score will be the \u2018end\u2018 element plus the minimum score Player 2 can obtain from the remaining elements. Player 2\u2019s minimum score can be obtained using the \u2018player1Wins\u2018 function recursively by changing the \u2018end\u2018 index to \u2018end-1\u2018 and keeping the \u2018start\u2018 index the same. ", "page_idx": 28}, {"type": "text", "text": "6. Update Cache and Return Result: After calculating the maximum score Player 1 can obtain, we update the cache dictionary with the result for the current state \u2018(start, end)\u2018 and return the result. ", "page_idx": 28}, {"type": "text", "text": "7. Main Function: The code defines the main \u2018predictTheWinner\u2018 function that calls the \u2018player1Wins\u2018 function with the initial state $^{\\ast}(0,\\mathfrak{n}{-}1)^{\\ast}$ and returns the result. ", "page_idx": 28}, {"type": "text", "text": "from typing import List ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "class Solution (): def predictTheWinner (self , nums: List[int ]) -> bool: n = len(nums) cache $=$ {} # Cache dictionary to store results def player1Wins(start: int , end: int) -> bool: # Base case - no more elements left if start > end: return True # Check if result is already cached if (start , end) in cache: return cache [(start , end)] # Option 1: Player 1 takes the start element option1 $=$ nums[start] + min( player1Wins (start $^+$ 1, end), player1Wins (start , end - 1)) # Option 2: Player 1 takes the end element option2 $=$ nums[end] $^+$ min( player1Wins (start + 1, end), player1Wins (start , end - 1)) # Update cache with the result and return cache [(start , end)] $=$ option1 $>=$ option2 return cache [(start , end)] # Call the recursive function with initial state (0, n -1) return player1Wins (0, n -1) ", "page_idx": 28}, {"type": "text", "text": "Example 3. M1 on Code Synthesis 3 (Teacher=Student=GPT-3.5-turbo-0613) ", "page_idx": 28}, {"type": "text", "text": "TP: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "(Leetcode Question ID: 877) Write a python code \"\"\"class Solution(): def stoneGame(self, piles: List[int]) $->$ bool:\"\"\" to solve the following problem: Alice and Bob play a game with piles of stones. There are an even number of piles arranged in a row, ", "page_idx": 28}, {"type": "text", "text": "TR and TA: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The problem is asking to determine if Alice can win the game given a list of piles of stones. Here\u2019s a step-by-step strategy to solve the problem: ", "page_idx": 28}, {"type": "text", "text": "1. Determine Alice\u2019s optimal strategy: - Since Alice starts first, she has the advantage of making the first move. - Alice\u2019s goal is to end with the most stones, so she will try to maximize her score. - In each turn, Alice can choose to take the entire pile from either the beginning or the end of the row. 2. Determine the total number of stones: - The total number of stones is odd, which means there will be no ties. - Alice can always take the largest pile in each turn. - Calculate the total number of stones by summing all the elements in the piles list. 3. Check if Alice can always win: - If Alice takes all the piles from either the beginning or the end in each turn, she will always end up with more stones than Bob. - Check if the total number of stones is greater than half of the total stones. 4. Return the result: - If Alice can always win, return True. Otherwise, return False. 1 from typing import List 2 3 class Solution (): 4 def stoneGame(self , piles: List[int ]) -> bool: 5 total_stones $=$ sum(piles) 6 half_total $=$ total_stones // 2 7 8 if half_total $<=$ max(piles): 9 return True 10 else: 11 return False ", "page_idx": 29}, {"type": "text", "text": "EP: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "(Leetcode Question ID: 486) Write a python code \"\"\"class Solution(): def predictTheWinner(self, nums: List[int]) $->$ bool:\"\"\" to solve the following problem: You are given an integer array \u2018nums\u2018. Two players are playing a game with this array: player 1 and player 2. ", "page_idx": 29}, {"type": "text", "text": "ER and EA: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The problem is asking to determine if Player 1 can win the game given an array of numbers. Here\u2019s a step-by-step strategy to solve the problem: ", "page_idx": 29}, {"type": "text", "text": "1. Determine Player 1\u2019s optimal strategy: - Player 1 starts first and wants to maximize their score. - In each turn, Player 1 can choose a number from either end of the array. ", "page_idx": 29}, {"type": "text", "text": "Example 4. M1 on Code Synthesis 4 (Teacher=Student=GPT-3.5-turbo-0613) ", "page_idx": 29}, {"type": "text", "text": "TP: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "(Leetcode Question ID: 1406) Write a python code \"\"\"class Solution(): def stoneGameIII(self, stoneValue: List[int]) $->$ str:\"\"\" to solve the following problem: ", "page_idx": 29}, {"type": "text", "text": "Alice and Bob continue their games with piles of stones. There are several stones arranged in a row, and each stone has an associated value which is an integer given in the array \u2018stoneValue\u2018. ", "page_idx": 30}, {"type": "text", "text": "TR and TA: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To solve this problem, we can use dynamic programming to determine the optimal strategy for each player. Let\u2019s think step by step: ", "page_idx": 30}, {"type": "text", "text": "1. Create a helper function called \u2018dfs\u2018, which takes in the current position \u2018i\u2018 and the list of stone values. This function will recursively calculate the maximum score that can be achieved starting from position \u2018i\u2018 and returning the difference of Alice\u2019s score and Bob\u2019s score. ", "page_idx": 30}, {"type": "text", "text": "2. In the \u2018dfs\u2018 function, we need a base case for the end of the game. If the current position \u2018i\u2018 is greater than or equal to the length of the stone values list, we\u2019ve reached the end of the game, and the difference in scores is 0. ", "page_idx": 30}, {"type": "text", "text": "3. If the base case is not met, we need to calculate the maximum score difference achievable starting from the current position. We can iterate through the next 3 positions, taking 1, 2, or 3 stones each time, and recursively calculate the score difference for the opponent. We take the maximum score difference among the opponent\u2019s choices and subtract the current stone value to get the score difference for the current player. ", "page_idx": 30}, {"type": "text", "text": "4. We return the maximum score difference calculated from the previous step. If the current player is Alice (starting player), a positive score difference means Alice wins. If the current player is Bob, a negative score difference means Bob wins. If the score difference is 0, it\u2019s a tie. ", "page_idx": 30}, {"type": "text", "text": "5. Finally, in the \u2018stoneGameIII\u2018 function, we can call the \u2018dfs\u2018 function with the initial position 0 and the stone values list.   \nWe compare the result of \u2018dfs\u2018 with 0 to determine the winner. ", "page_idx": 30}, {"type": "text", "text": "class Solution (): def stoneGameIII (self , stoneValue : List[int ]) -> str: def dfs(i: int , stoneValues : List[int ]) -> int: if i $>=$ len( stoneValues ): return 0 maxScore $=$ float(\u2019-inf \u2019) for j in range(i, min( $_{\\textrm{1+3}}$ , len( stoneValues ))): currScore $=$ sum( stoneValues $[\\,\\mathrm{i}:\\,\\mathrm{j}+1\\,]$ ) opponentScoreDiff $=$ dfs $(\\mathrm{~j~}+\\mathrm{~1~}$ , stoneValues ) maxScore $=$ max(maxScore , currScore - opponentScoreDiff ) return maxScore scoreDiff $=$ dfs(0, stoneValue ) if scoreDiff $>\\ 0$ : return \u2019Alice \u2019 elif scoreDiff $<\\ 0$ : return \u2019Bob \u2019 else: return \u2019Tie \u2019 ", "page_idx": 30}, {"type": "text", "text": "EP: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "(Leetcode Question ID: 486) Write a python code \"\"\"class Solution(): def predictTheWinner(self, nums: List[int]) $->$ bool:\"\"\" to solve the following problem: ", "page_idx": 30}, {"type": "text", "text": "You are given an integer array \u2018nums\u2018. Two players are playing a game with this array: player 1 and player 2. ", "page_idx": 30}, {"type": "text", "text": "ER and EA: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To solve this problem, we can use dynamic programming to determine the optimal strategy for each player. Let\u2019s think step by step: ", "page_idx": 30}, {"type": "text", "text": "1. Create a helper function called \u2018dfs\u2018, which takes in the current start and end indices of the array. This function will recursively calculate the maximum score that can be achieved starting from the current indices and returning the difference of Player 1\u2019s score and Player 2\u2019s score. ", "page_idx": 30}, {"type": "text", "text": "2. In the \u2018dfs\u2018 function, we need a base case for the end of the game. If the start index is greater than the end index, we\u2019ve reached the end of the game, and the difference in scores is 0. ", "page_idx": 30}, {"type": "text", "text": "3. If the base case is not met, we need to calculate the maximum score difference achievable starting from the current indices. We have two options: either Player 1 chooses the number at the start index or Player 1 chooses the number at the end index. We recursively calculate the score difference for the opponent for both options and take the maximum score difference among the opponent\u2019s choices. We subtract the opponent\u2019s score difference from the current player\u2019s chosen number to get the score difference for the current player. ", "page_idx": 30}, {"type": "text", "text": "4. We return the maximum score difference calculated from the previous step. If the score difference is positive or equal to 0, it means Player 1 wins. 5. Finally, in the \u2018predictTheWinner\u2018 function, we can call the \u2018dfs\u2018 function with the initial start and end indices (0 and len(nums)-1) of the array. We compare the result of \u2018dfs\u2018 with 0 to determine if Player 1 can win. 1 from typing import List 2 3 class Solution (): 4 def predictTheWinner (self , nums: List[int ]) -> bool: 5 def dfs(start: int , end: int) -> int: 6 if start $>$ end: 7 return 0 8 9 chooseStart $=$ nums[start] - dfs(start + 1, end) 10 chooseEnd $=$ nums[end] - dfs(start , end - 1) 11 12 return max(chooseStart , chooseEnd) 13 14 scoreDiff $=$ dfs(0, len(nums) - 1) 15 return scoreDiff $>=~~0$ ", "page_idx": 31}, {"type": "text", "text": "Example 5. M1 on Code Synthesis 5 (Teacher=GPT-3.5-turbo-0613 Student=LLaMA3-8B) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "TP:   \n(Leetcode Question ID: 486) Write a python code \"\"\"class Solution(): def predictTheWinner(self, nums: List[int]) $->$ bool:\"\"\" to solve the following problem:   \nYou are given an integer array \u2018nums\u2018. Two players are playing a game with this array: player 1 and player 2. ", "page_idx": 31}, {"type": "text", "text": "TR and TA: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To solve this problem, we can use the concept of dynamic programming. Let\u2019s think step by step: ", "page_idx": 31}, {"type": "text", "text": "1. Define a function to determine the winner:   \n- The function should take the array of integers \u2018nums\u2018 as input and return a boolean value indicating whether Player 1 can win the game.   \n- We will create a 2D memoization table, \u2018dp\u2018, to store the scores difference between Player 1 and Player 2 at each subgame. \u2018dp[i][j]\u2018 represents the maximum score difference when considering the subarray \u2018nums[i:j+1]\u2018.   \n- The function will call a helper function \u2018calculateScores\u2018 to calculate the maximum scores at each subgame. - The \u2018calculateScores\u2018 function will take the start and end indices, \u2018start\u2018 and \u2018end\u2018, as input and return the maximum score difference between Player 1 and Player 2 for the subarray \u2018nums[start:end+1]\u2018.   \n- We will use a recursive approach to consider all possible moves that Player 1 can make.   \n- Within the recursive function, we will check if the score difference for the current subarray has already been calculated and stored in the memoization table \u2018dp\u2018. If so, we will return the stored value.   \n- If the start index is greater than the end index, it means that all elements in the subarray have been chosen, and we will return a score difference of 0.   \n- Otherwise, we will consider two options:   \n- If Player 1 chooses the first element (\u2018nums[start]\u2018), the score difference will be the value of \u2018nums[start]\u2018 minus the maximum score difference that Player 2 can obtain in the remaining subarray, calculated recursively.   \n- If Player 1 chooses the last element (\u2018nums[end]\u2018), the score difference will be the value of \u2018nums[end]\u2018 minus the maximum score difference that Player 2 can obtain in the remaining subarray, calculated recursively.   \n- We will take the maximum of these two options and store the result in the memoization table \u2018dp\u2018.   \n- Finally, we will return the calculated score difference. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "3. In the main function \u2018predictTheWinner\u2018, we will initialize the memoization table \u2018dp\u2018 with zeros and call the helper function \u2018calculateScores\u2018 to determine the maximum score difference between Player 1 and Player 2 for the entire array \u2018nums\u2018. ", "page_idx": 31}, {"type": "text", "text": "- If the score difference is greater than or equal to 0, it means that Player 1 can win the game, and we will return \u2018True\u2018.   \n- Otherwise, we will return \u2018False\u2018. ", "page_idx": 31}, {"type": "text", "text": "class Solution (): def predictTheWinner (self , nums: List[int ]) -> bool: n $=$ len(nums) $\\begin{array}{r c c c l}{\\mathbf{dp}}&{=}&{[\\,[\\,0\\,]}&{*}&{\\mathbf{n}}\\end{array}$ for _ in range(n)] def calculateScores (start: int , end: int) $->$ int: if start $>$ end: return 0 if dp[start ][end] $!=~0$ : ", "page_idx": 31}, {"type": "text", "text": "return dp[start ][ end] chooseStart $=$ nums[start] - calculateScores (start + 1, end) chooseEnd $=$ nums[end] - calculateScores (start , end - 1) dp[start ][end] $=$ max(chooseStart , chooseEnd) return dp[start ][ end] return calculateScores $\\left(\\!\\!\\!\\begin{array}{c c c c c}{0\\,,}&{{\\mathrm n}}&{-}&{1\\,)}&{>=}&{0}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "12131415161718192021 ", "page_idx": 32}, {"type": "text", "text": "EP: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "(Leetcode Question ID: 1510) Write a python code \"\"\"class Solution(): def winnerSquareGame(self, n: int) $->$ bool:\"\" to solve the following problem:   \nAlice and Bob take turns playing a game, with Alice starting first.   \nER and EA: To solve this problem, we can use a similar approach as before. Let\u2019s think step by step:   \n1. Define a function to determine the winner:   \n- The function should take the integer $\\mathrm{\\Omega^{\\ast}}$ as input and return a boolean value indicating whether Alice wins the game.   \n- We will create a set \u2018squares\u2018 to store all the square numbers less than or equal to $\\mathbf{\\dot{n}}^{*}$ .   \n- The function will call a helper function \u2018canWin\u2018 to determine if Alice can win the game.   \n2. Define the \u2018canWin\u2018 function:   \n- The \u2018canWin\u2018 function will take the current number of stones $\\mathrm{\\Delta}^{\\ast}\\mathrm{n}^{\\ast}$ as input and return a boolean value indicating whether Alice can win the game.   \n- We will use a recursive approach to consider all possible moves that Alice can make.   \n- Within the recursive function, we will check if the current number of stones is less than or equal to 0. If so, it means that Alice has lost the game, and we will return \u2018False\u2018.   \n- Otherwise, we will consider all the square numbers in the set \u2018squares\u2018 that are less than or equal to $\\mathbf{\\dot{n}}^{*}$ . For each square   \nnumber, we will recursively call the \u2018canWin\u2018 function with the updated number of stones.   \n- If any of the recursive calls return \u2018True\u2018, it means that Alice can win the game, and we will return \u2018True\u2018.   \n- If none of the recursive calls return \u2018True\u2018, it means that Alice cannot win the game, and we will return \u2018False\u2018.   \n3. In the main function \u2018winnerSquareGame\u2018, we will create the set \u2018squares\u2018 and call the helper function \u2018canWin\u2018 to determine if Alice can win the game.   \n- If the result is \u2018True\u2018, we will return \u2018True\u2018.   \n- Otherwise, we will return \u2018False\u2018. class Solution (): def winnerSquareGame (self , n: int) -> bool: squares $=$ set(i \\* i for i in range(int(math.sqrt(n)) + 1)) memo $\\begin{array}{r l}{=}&{{}\\{\\}}\\end{array}$ def canWin(n: int) -> bool: if n $<=~~0$ : return False if n in memo: return memo[n] square in squares: if square > n: break if not canWin(n - square): memo[n] $=$ True return True memo[n] $=$ False return False return canWin(n) ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Example 6. Code Synthesis with Self-Debugging (Debugger=GPT-3.5-turbo-0613) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Problem: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "(Leetcode Question ID: 1406) Write a python code \"\"\"class Solution(): def stoneGameIII(self, stoneValue: List[int]) $->$ str:\"\"\" to solve the following problem:   \nAlice and Bob continue their games with piles of stones. There are several stones arranged in a row, and each stone has an associated value which is an integer given in the array \u2018stoneValue\u2018. ", "page_idx": 32}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/6d33fcebcc5d773d2b0ad1bbc7cc13c481c4f78c0b69e419498a36e988282d05.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "A.3.5 Natural Language Rationale has Positive Effect on Code Following in ICL ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In the code synthesis task, the answer (TA and EA) refers to the resulting code. We find that having models produce a natural language rationale (TR and ER) before writing the code is crucial for ", "page_idx": 33}, {"type": "text", "text": "ICL and ultimately the LbT-based scoring to function as expected. Specifically, this approach allows the student\u2019s output code EA to align more closely with the ICL example code TA, thus the exam score (i.e., LbT score) can better reflect the quality of TA. ", "page_idx": 34}, {"type": "text", "text": "As shown in Ex. 8, when we only use $\\mathrm{TP+TA}$ as the ICL example to teach the student, the EA does not follow the DP coding style in TA and instead employs a recursive method to solve EP. This aligns with the finding in [50] that labels in the ICL examples have limited effect on the LLM output. In contrast, as shown in Ex. 9, when we use $\\mathrm{TP+TR+TA}$ as the ICL example, the student first follows the TR to create an ER, which subsequently leads to the successful generation of an EA that aligns with the coding style of TA. ", "page_idx": 34}, {"type": "text", "text": "Example 7. Code Synthesis without In-context example (without TP, TR, and TA) ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "EP: (Leetcode Question ID: 877) Write a python code \"\"\"class Solution(): def stoneGame(self, piles: List[int]) $->$ bool:\"\"\" to ", "page_idx": 34}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/9f84da83904c1c95ad62c8d0cd258218dc67fa4ee489a409d2439e54b6ef632f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Example 8. Code Synthesis without TR ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "TP: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "(Leetcode Question ID: 486) Write a python code \"\"\"class Solution(): def predictTheWinner(self, nums: List[int]) $->$ bool:\"\"\" to solve the following problem: You are given an integer array \u2018nums\u2018. Two players are playing a game with this array: player 1 and player 2. ", "page_idx": 34}, {"type": "text", "text": "TA: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1 from typing import List   \n2   \n3 class Solution:   \n4 def predictTheWinner (self , nums: List[int ]) -> bool:   \n5 n = len(nums)   \n6 dp1 , dp2 = [0] \\* n, [0] \\* n   \n7 dp1 [0], dp2 [0] $=$ nums [0], 0   \n8 for i in range (1, n):   \n9 dp1[i] $=$ max(dp1[i -1] + nums[i], dp2[i -1])   \n0 dp2[i] = max(dp1[i-1], dp2[i -1] $^+$ nums[i])   \n1 return dp1[-1] $>$ dp2 [-1] ", "page_idx": 34}, {"type": "text", "text": "(Leetcode Question ID: 877) Write a python code \"\"\"class Solution(): def stoneGame(self, piles: List[int]) $->$ bool:\"\"\" to solve the following problem: Alice and Bob play a game with piles of stones. There are an even number of piles arranged in a row, ", "page_idx": 34}, {"type": "text", "text": "EA: ", "page_idx": 34}, {"type": "text", "text": "12345678910111213141516 from typing import List class Solution:   \ndef stoneGame(self , piles: List[int ]) -> bool:   \nn = len(piles)   \nprefix_sum $=$ [0] \\* (n + 1)   \nfor i in range(n):   \nprefix_sum[i + 1] $=$ prefix_sum [i] $^+$ piles[i]   \ndef dp(i, j): if i > j: return 0 return max(prefix_sum[j + 1] - prefix_sum [i] - dp(i + 1, j - 1) prefix_sum [j $^+$ 1] - prefix_sum[i] - dp(i, j - 1))   \nreturn dp(0, n - 1) \\% 2 == 1 ", "page_idx": 35}, {"type": "text", "text": "Example 9. Code Synthesis with TR ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "TP:   \n(Leetcode Question ID: 486) Write a python code \"\"\"class Solution(): def predictTheWinner(self, nums: List[int]) $->$ bool:\"\"\" to solve the following problem:   \nYou are given an integer array \u2018nums\u2018. Two players are playing a game with this array: player 1 and player 2. ", "page_idx": 35}, {"type": "text", "text": "TR and TA: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The problem can be solved by using a dynamic programming approach. The idea is to maintain two arrays, \u2018dp1\u2018 and \u2018dp2\u2018, where \u2018dp1[i]\u2018 and \u2018dp2[i]\u2018 represent the maximum score that player 1 and player 2 can get respectively after considering the first \u2018i\u2018 elements in the array.   \nLet\u2019s think step by step:   \n1. Initialize \u2018dp1\u2018 and ${\\mathrm{~\\hat{dp}}}2^{*}$ with zeros. \u2018dp1[0]\u2018 and \u2018dp2[0]\u2018 are set to \u2018nums[0]\u2018 because the first element is the starting point for both players.   \n2. For each element \u2018nums[i]\u2018 in the array, calculate \u2018dp1[i]\u2018 as the maximum of two options: - \u2018dp1[i-1] $^+$ nums[i]\u2018: Player 1 chooses \u2018nums[i]\u2018 and leaves \u2018nums[i-1]\u2018 for player 2. - \u2018dp2[i-1]\u2018: Player 1 chooses \u2018nums[i-1]\u2018 and leaves \u2018nums[i]\u2018 for player 2.   \n3. Calculate \u2018dp2[i]\u2018 as the maximum of two options: - \u2018dp1[i-1]\u2018: Player 2 chooses \u2018nums[i-1]\u2018 and leaves \u2018nums[i]\u2018 for player 1. - \u2018dp2[i-1] $^+$ nums[i]\u2018: Player 2 chooses \u2018nums[i]\u2018 and leaves \u2018nums[i-1]\u2018 for player 1.   \n4. If \u2018dp1[-1] $>$ dp2[-1]\u2018, return \u2018True\u2018 because player 1 can win. Otherwise, return \u2018False\u2018. ", "page_idx": 35}, {"type": "text", "text": "2   \n3 class Solution:   \n4 def predictTheWinner (self , nums: List[int ]) -> bool:   \n5 n = len(nums)   \n6 dp1 , $\\mathsf{d p}2\\;\\;=\\;\\;\\;[0]\\;\\;*\\;\\;{\\bf n}$ , [0] \\* n   \n7 dp1 [0], dp2 [0] $=$ nums [0], 0   \n8 for i in range (1, n):   \n9 dp1[i] $=$ max(dp1[i -1] + nums[i], dp2[i -1])   \n10 dp2[i] $=$ max(dp1[i-1], dp2[i -1] $^+$ nums[i])   \n1 return dp1[-1] $>$ dp2 [-1] ", "page_idx": 35}, {"type": "text", "text": "EP: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "(Leetcode Question ID: 877) Write a python code \"\"\"class Solution(): def stoneGame(self, piles: List[int]) $->$ bool:\"\"\" to solve the following problem: Alice and Bob play a game with piles of stones. There are an even number of piles arranged in a row, ", "page_idx": 35}, {"type": "text", "text": "ER and EA: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The problem can be solved by using a dynamic programming approach. The idea is to maintain two arrays, \u2018dp1\u2018 and \u2018dp2\u2018, where \u2018dp1[i]\u2018 and \u2018dp2[i]\u2018 represent the maximum number of stones that Alice and Bob can get respectively after considering the first \u2018i\u2018 piles.   \nLet\u2019s think step by step:   \n1. Initialize \u2018dp1\u2018 and ${\\mathrm{}}^{\\prime}{\\mathrm{d}}{\\mathrm{p}}2^{\\prime}$ with zeros. \u2018dp1[0]\u2018 and \u2018dp2[0]\u2018 are set to \u2018piles[0]\u2018 because the first pile is the starting point for both players.   \n2. For each pile \u2018piles[i]\u2018 in the array, calculate \u2018dp1[i]\u2018 as the maximum of two options: - \u2018dp1[i-1] $^+$ piles[i]\u2018: Alice chooses \u2018piles[i]\u2018 and leaves \u2018piles[i-1]\u2018 for Bob. - \u2018dp2[i-1]\u2018: Alice chooses \u2018piles[i-1]\u2018 and leaves \u2018piles[i]\u2018 for Bob. 3. Calculate \u2018dp2[i]\u2018 as the maximum of two options: - \u2018dp1[i-1]\u2018: Bob chooses \u2018piles[i-1]\u2018 and leaves \u2018piles[i]\u2018 for Alice. - \u2018dp2[i-1] $^+$ piles[i]\u2018: Bob chooses \u2018piles[i]\u2018 and leaves \u2018piles[i-1]\u2018 for Alice.   \n4. If \u2018dp1[-1] $>$ dp2[-1]\u2018, return \u2018True\u2018 because Alice wins. Otherwise, return \u2018False\u2018. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "from typing import List ", "page_idx": 35}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/61ffb86f8c2afc69718cc8fe14c89d4d74fffab5a2866e48a2b7aa064638eb8e.jpg", "img_caption": ["In the following, we quantitatively assess the following rate of the ICL example and the resulting positive correlation between TA accuracy and the EA accuracy (i.e., LbT score). "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "\u2022 ICL Example Following Rate (Coding Style). To quantitatively assess the positive effect of the natural language rationale on ICL following, we define the Style Follow Rate, which measures whether EA matches the coding style of TA. Specifically, if the two codes are of the same algorithmic style and raise the same type of exception when an error occurs, we regard them as of the same style. We conduct the judgment manually. For example, if the teacher employs dynamic programming as the problem-solving strategy while the student resorts to a recursive strategy, we classify this as not following. Tab. A15 shows the Style Follow Rate on the Game Theory dataset with LLaMA3-8B. Using both TR and TA as an ICL example yields a $12.5\\%$ higher follow rate than using TA alone, illustrating the importance of natural language rationale for enhancing ICL following. ", "page_idx": 36}, {"type": "text", "text": "\u2022 ICL Example Ignore Rate (Coding Style). Moreover, we observe that when using only the $\\mathrm{TP+TA}$ as the ICL example, without including TR, LLMs often ignore the ICL example and produce responses that are nearly identical or exactly the same as those generated without any ICL example, as shown in Ex. 8 and Ex. 7. To quantitatively assess this tendency to ignore ICL example in the absence of a natural language rationale, we define the ICL Ignore Rate, which measures whether EA\u2019s style matches that of the code generated by the student without any ICL example. As shown in Tab. A15, when TR is not used, the ICL Ignore Rate is as high as $43.13\\%$ , but using TR reduces this rate significantly to only $1.88\\%$ . This further illustrates that using natural language rationale in the ICL example helps the student more effectively learn from the teacher\u2019s code, rather than ignoring it. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Ranking Correlation between TA Accuracy and EPs Accuracy. We compute Kendall\u2019s Tau ranking correlation between the TA\u2019s S-score and the average S-score of EAs, as well as between the TA\u2019s S-score and the average V-score of EAs (i.e., the LbT score). As shown in Tab. A16, we can see that the correlation of \u201cTeach w/ $\\mathrm{TR+TA^{\\circ}}$ is consistently higher than the corresponding correlation of \u201cTeach w/ TA\u201d. When teaching without TR, the correlation between the TA\u2019s S-score and EAs\u2019 S-score or V-score is zero or even negative. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Selected TA Ranking with the Highest EPs Accuracy. We show the S-score ranking of the TA among 8 TAs with the highest average EA score. We observe that: (1) When TPs and EPs are similar and TR is provided in the ICL example, we can select high-accuracy TAs based on the S-score or V-score of EAs. For example, for questions SG1 and SG4, the TA with the highest EAs\u2019 V-score achieves the best S-score among the 8 TA. (2) When TR is not provided in the ICL example, the V-scores of EAs with different TAs as the ICL example are usually not discriminative, i.e., are of exactly the same value. Therefore, when TRs is not provided in the ICL example, using EAs\u2019 V-score (i.e., LbT score) cannot help select a high-accuracy TA. ", "page_idx": 36}, {"type": "text", "text": "Table A15: The Style Follow Rate and ICL Ignore Rate on the Game Theory dataset in LeetCode Grandmaster DP study plan with LLaMA3-8B. The Game Theory dataset contains 5 problems, and 8 pairs of $\\mathrm{TR+TA}$ are sampled for each problem. Then, we used the $5\\times8=40$ pairs of $\\mathrm{TP+TR+TA}$ or $\\mathrm{TP+TA}$ only to teach students to solve the remaining problems other than TP, resulting in a total of $5\\times8\\times(5-1)=160\\,\\mathrm{{F}}$ As. ", "page_idx": 36}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/fcc5ee90a3c799454f9aacf9e810a248b1771a7ae842d417f837e39a3096372c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "Table A16: The Kendall\u2019s Tau ranking correlation between the TA\u2019s S-score and the average V-score of EAs (i.e., LbT score), and between the TA\u2019s S-score and the average S-score of EAs, and the S-score ranking of the TA among 8 TAs $(\\in\\{1,\\cdots\\,,8\\}$ , 1 is the best) with the highest average EA score (either LbT score, or the TA\u2019s S-score). All EPs come from the Game Theory dataset. For the \u201cSimilar TP and $\\mathrm{EPs^{\\circ}}$ section, we select TP from the Game Theory dataset to ensure high similarity between TPs and EPs. For the \u201cDissimilar TP and EPs\u201d section, we select TP from the Tricky Invariant dataset related to binary search. Details of the teacher IDs can be found in Tab. A8. For each TP, we sampled 8 pairs of $\\mathrm{TR+TA}$ as the ICL examples. ", "page_idx": 37}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/980ea3e139f88b884bff8eba8d56b0116c498543fe2b94348b7be8b6b1748d7c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "B M2 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "B.1 Additional Experimental Setups ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We use a learning rate of 5e-7, a batch size of 16 and 1 training epoch. We set $\\beta=0.1$ and add an additional NLL term [55] weighted by 50. ", "page_idx": 37}, {"type": "text", "text": "B.2 Examples ", "text_level": 1, "page_idx": 37}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/a49c165ed6a2ecb03c5dadc20a4827489f72b16310d42e395bf8b11c356f4cbe.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "TP and TP-GT: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "A positive integer $X$ is 2 more than a multiple of 3. Its units digit is the same as the units digit of a number that is 4 more than a multiple of 5. What is the smallest possible value of $X$ ? ", "page_idx": 37}, {"type": "text", "text": "If a positive integer is 4 more than a multiple of 5, then its units digit must be 4 or 9. We check positive integers ending in 4 or 9 until we find one which is 2 more than a multiple of 3: 4 is 1 more than a multiple of 3, 9 is a multiple of 3, and 14 is 2 more than a multiple of 3. ", "page_idx": 38}, {"type": "text", "text": "Winning TR-TA: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Let\u2019s think step by step. ", "page_idx": 38}, {"type": "text", "text": "We know that $X$ is 2 more than a multiple of 3, so $X=3k+2$ for some integer $k$ .   \nThe units digit of $X$ is the same as the units digit of a number that is 4 more than a multiple of 5. The numbers that are 4 more than a multiple of 5 are 4, 9, 14, 19,..., so the units digit of such a number is either 4 or 9. Since $X$ has units digit 4, we have that $X$ has units digit 4.   \nTherefore, we can write $X=3k+2=4$ , or $k=1$ . The smallest possible value of $X$ is 5 . ", "page_idx": 38}, {"type": "text", "text": "Losing TR-TA: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Let\u2019s think step by step. ", "page_idx": 38}, {"type": "text", "text": "The units digit of $X$ is the same as the units digit of a number that is 4 more than a multiple of 5. Since the units digit of a multiple of 5 is either 0 or 5, the units digit of $X$ is either 6 or 1.   \nAlso, $X$ is 2 more than a multiple of 3, so $X$ is either 2 more than a multiple of 3, or 1 more than a multiple of 3. If the units digit of $X$ is $6$ , then $X$ must be 8 or 11 or... and so on. However, if $X$ is 2 more than a multiple of 3, then $X$ cannot be 8 or 11 or... because the units digit of 8 or 11 or... is not the same as the units digit of a number that is 4 more than a multiple of 5.   \nTherefore, the units digit of $X$ must be 1. In this case, $X$ must be 1 or 4 or 7 or... and so on. Since $X$ is 2 more than a multiple of 3, the smallest possible value of $X$ is 4 . ", "page_idx": 38}, {"type": "text", "text": "C M3 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Algorithm A2 The Workflow of M3 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Require:   \nTeacher and (one or multiple) Student LLM: T, $\\mathbf{S}_{i}$   \nTask description $t a s k$ , and instances of the task TPs   \nExam Problems: EPs   \nGround-truth of Exam Problems: EPs_GT   \nNumber of iterations: $T$   \n1: # Generate initial exemplars in a zero-shot manner.   \n2: exemplars $=\\mathbf{T}_{i n i t}(t a s k)$   \n3: # Iteratively search for better exemplars.   \n4: for $\\mathfrak{t}=1$ to $T$ do   \n5: failures $=[]$   \n6: # Find student failures when using current exemplars.   \n7: for EP, EP_GT in zip(EPs, EPs_GT) do   \n8: for $\\mathbf{S}_{i}$ in S do   \n9: $\\mathrm{EA}=\\mathbf{S}_{i}$ (exemplars, EP)   \n10: if $\\mathrm{EA}\\neq\\mathrm{EP_{-}G T}$ then   \n11: failures.append((EP, EA, EP_GT))   \n12: end if   \n13: end for   \n14: end for   \n15:   \n16: # Reflect on student failures   \n17: reflections $=\\mathbf{T}_{\\mathit{r e f l e c t}}$ (exemplars, failures)   \n18: # Generate new exemplars based on reflections   \n19: for $\\mathrm{i}=1$ to N do   \n20: exemplars ${\\bf\\nabla}_{i}={\\bf T}$ improve(exemplars, failures, reflections)   \n21: # Evaluate new exemplars   \n22: ${\\mathrm{score}}_{i}=$ EVALUATE(EPs_GT, T(exemplars $_i$ , EPs))   \n23: end for   \n24: # Update exemplars   \n25: best $\\mathbf{\\mu}=\\mathbf{ArgMax}(\\mathbf{score}_{i})$   \n26: exemplars $=$ exemplarsbest   \n27: end for   \n28: # Teacher uses the optimized exemplar to solve Teaching Problem.   \n29: return T(exemplars, TPs) ", "page_idx": 39}, {"type": "text", "text": "C.2 Prompt Design ", "text_level": 1, "page_idx": 39}, {"type": "image", "img_path": "0ZZMUjZJYF/tmp/83bc3abb77341756c47ce009aa4065ee3a29ebffb8f2a208c726f577b8e38746.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Give {num_feedbacks} reasons why these examples could have gotten these cases wrong. ", "page_idx": 40}, {"type": "text", "text": "[Assistant:] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Prompt 8. Teacher Prompt (improve): Improve Exemplars Based on Reflections ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "[User:]   \nI\u2019m trying to write $\\{k\\}$ in-context learning examples for a few-shot classifier. The classifier will answer this question: \"{task}\"   \nBut with these examples, the classifier got the following cases wrong: {failure_cases}   \nBased on these failure cases, the problem with the current in-context examples is that {reflection}.   \nBased on the above information, I need a new list of $\\{k\\}$ improved positive and negative learning examples. ", "page_idx": 40}, {"type": "text", "text": "C.3 Examples ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Table A17: Teacher\u2019s $F_{1}$ score of M3 on Logical Fallacy test set at the end of iteration $T$ , where LLaMa3-70B is used as the teacher for all settings. The best results are in bold. ", "page_idx": 40}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/b62c33ebfa3e2a29e1660ba29faa570653ee9e64b67d252618983077ef53a144.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "Ex. 12 provides an instance of how the teacher makes verbalized reflections on why the current in-context learning exemplars are causing students to make mistakes, and how the reflections help guide the teacher to revise the exemplars to address the mistakes. ", "page_idx": 40}, {"type": "text", "text": "Example 12. An Example Round of Exemplar Revision ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Problem: Is this text an instance of the logical fallacy of faulty generalization? ", "page_idx": 40}, {"type": "text", "text": "Example 1: My father told me that the sky is green. So it must be the truth. [No]   \nExample 2: Which rhetorical fallacy is a broad statement about people on the basis of gender, ethnicity, race, or political,   \nsocial, professional, or religious group? [Yes]   \nExample 3: But, Dad, I know you had no curfew when you were my age, so how can you give me a curfew? [No]   \nExample 4: Some tall people recently vandalized the park, tall people are irresponsible. [Yes]   \n(some examples omitted) ", "page_idx": 40}, {"type": "text", "text": "Student Failures: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Failure Case 4: Input: We haven\u2019t proven aliens didn\u2019t create life on earth, so aliens created life on earth. ", "page_idx": 40}, {"type": "text", "text": "Label: No ", "page_idx": 40}, {"type": "text", "text": "Prediction: Yes (some cases omitted) ", "page_idx": 40}, {"type": "text", "text": "Reflection: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The classifier might be having trouble with texts that involve negation or indirect reasoning. In Failure Case 4, the text presents a classic example of an argument from ignorance, but the classifier might have been thrown off by the double negation (\"haven\u2019t proven...didn\u2019t create\") or the indirect way of stating the conclusion. ", "page_idx": 40}, {"type": "text", "text": "Revised Exemplars: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Example 3: I\u2019ve never met a person from France who doesn\u2019t love cheese, so the French must have a genetic predisposition   \nto loving cheese. [Yes]   \nExample 7: The fact that some people claim to have seen ghosts doesn\u2019t prove that ghosts exist. [No]   \n(some examples omitted) ", "page_idx": 40}, {"type": "table", "img_path": "0ZZMUjZJYF/tmp/7d141ad8646b3d3c107fb4ee8e370e2a39c34e1d61aadc5bc5d38d070934b30e.jpg", "table_caption": ["Table A18: Causes of errors identified by the teacher (LLaMa3-70B) in M3, and analysis of whether they also caused teacher mistakes and are mitigated by LbT. "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "We further analyze the types and proportions of errors made by the students (Tab. A18), and verify that having diverse students help discover a diverse set of errors that the teacher could make, and that our LbT pipeline M3 could indeed generate exemplars that help correct those errors. More specifically, during one round of exemplar revision for the Logical Fallacy task, we identify the main error types (numbered a,b,c) from teacher\u2019s reflections of mistakes of different students (numbered 1,2,3). ", "page_idx": 41}, {"type": "text", "text": "We observe that the teacher identifies diverse and complementary causes of errors from different students, which help interpret why having diverse students is better. We verify that the causes of students\u2019 mistakes (1a, 1b, ...) are indeed also causing teachers\u2019 mistakes. Specifically, for each mistake the teacher makes on the test set, we prompt LLaMa-3-70B to judge which cause categories (1a, 1b,...) does this mistake fall into. Note that one mistake can be caused by multiple causes simultaneously. Then, we report the percentage of teacher mistakes of that cause in the third column of Tab. A18. By choosing a student model different from the teacher model, we identify more types of valid causes of teacher mistakes. ", "page_idx": 41}, {"type": "text", "text": "Finally, these causes in Tab. A18 indeed help the teacher improve the ICL examples. After the teacher revises the ICL examples by learning from student 1, the teacher\u2019s mistakes caused by 1a, 1b & 1c are reduced by $6.0\\%$ , $11.6\\%$ , and $13.3\\%$ . Based on this, we conjecture that LbT methods like M3 could lead to a more interpretable way to understand the flaws of in-context learning models. ", "page_idx": 41}, {"type": "text", "text": "D Discussion on Research Rationale ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "This section discusses our research rationale, and how it leads to specific choices made in our work, including the ultimate target, the LbT idea, the chosen tasks, and the concrete LbT implementation. We have already covered the roadmap \u2013 the next steps for further developing the LbT idea \u2013 in $\\S\\ 6$ , so it is omitted here. Finally, we discuss related machine learning techniques in App. D.5. ", "page_idx": 41}, {"type": "text", "text": "D.1 Target (the \u201cnail\u201d in the high level) ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In recent years, we have been impressed by the emergence of powerful AI models with extensive knowledge, strong planning skills, and good intuitions. However, the ability of current LLMs to provide accurate knowledge and reasoning appears to lag behind these other abilities. We consider accurate knowledge and reasoning to be among one of the most crucial factors for advancing LLM capabilities. ", "page_idx": 41}, {"type": "text", "text": "D.2 Idea (the \u201chammer\u201d in the high level) ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "LLMs demonstrate an impressive ability in grasping solution formats and strategies. However, they often struggle with accurately grasping the underlying logic, differentiating between concepts, and consistently adhering to correct logical reasoning. Actually, these errors mirror some difficulties encountered in human learning. Therefore, we speculate that integrating human educational strategies may help pave the way toward logically accurate AI. ", "page_idx": 42}, {"type": "text", "text": "The LbT methodology has been shown to effectively promote human learning, especially in terms of accurate knowledge building and reasoning. Drawing on literature that demystifies the working mechanism of LbT in human learning [5, 18, 22, 33, 62\u201365] and based on our own teaching and learning experience, we summarize three major ways in which LbT improves human learning: ", "page_idx": 42}, {"type": "text", "text": "(a) Increased self-accountability: The task of teaching introduces social pressure and incentives, encouraging individuals to raise their standards and work harder. ", "page_idx": 42}, {"type": "text", "text": "(b) Explicit articulation of implicit and vague thoughts: During the preparation of teaching materials, the teacher needs to use clear and organized language to convey its inner thoughts to ensure the student\u2019s comprehension. This process requires translating implicit concepts into precise terms, describing subtle distinctions, organizing information logically, and so on. This \u201cslow\u201d verbalization and organization not only aids communication, but also helps the teacher identify gaps in its own understanding and discover new connections. As for the teaching material themselves, intuitively, teaching materials that make it easier for students to learn have clearer and more accurate logic (we refer to this as the \u201cLbT assumption on teaching material quality\u201d or the LbT-TMQ assumption). Especially when the students are unable to solve a set of problems independently, if teaching material 1 enables them to solve the problems more effectively than teaching material 2, it is reasonable to infer that teaching material 1 may have a better logic. We expect M1 and M2 to benefit the teacher in a similar way. ", "page_idx": 42}, {"type": "text", "text": "(c) Iterative feedback from diverse students: In the teaching process, interaction with students of varying ability levels and knowledge backgrounds offers valuable feedback. The teacher can check if or not students misunderstand the teaching material or struggle with certain problems, and analyze why, in a multi-round discussion. In this process, the teacher might (1) recognize gaps in the teaching material \u2013 some conditions and logic may be straightforward to the teacher but require further elaboration and supporting information for students of different backgrounds, to ensure they actually understand and are able to utilize this logic in solving new problems; (2) identify gaps in their own knowledge; (3) discover novel connections when addressing students\u2019 misconceptions and erroneous associations. ", "page_idx": 42}, {"type": "text", "text": "We expect M3 to benefit the teacher in a similar way. ", "page_idx": 42}, {"type": "text", "text": "We are interested in whether LbT-inspired methods could become a standard prompting or fine-tuning practice to enhance the reasoning abilities of AI models. As a first step, this work investigates whether simple implementations of LbT, focusing on its beneftis (b) and (c), can improve the reasoning abilities of contemporary LLMs. For future work, implanting incentives into the LLM learning process to mimic benefit (a) could be a promising pathway, such as setting up a collaborative multi-agent learning framework with proper rewards and communication restrictions. ", "page_idx": 42}, {"type": "text", "text": "Additionally, LbT has the potential to improve stronger models by having them teach weaker ones (i.e., weak-to-strong generalization). This might offer exciting opportunities for continuous model evolution, especially as scaling the size of high-quality data faces challenges. ", "page_idx": 42}, {"type": "text", "text": "D.3 Task (the \u201cnail\u201d in the low level) ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Tasks. In this work, we choose mathematical reasoning, competition-level code synthesis, and verbal logical reasoning, because compared to other task domains, these tasks require accurate knowledge and reasoning and cannot be easily solved through vague logic or simple memorization. Moreover, all three tasks are popular and have attracted considerable attention from the community. ", "page_idx": 42}, {"type": "text", "text": "Datasets. For mathematical reasoning, we choose the challenging MATH dataset [27]. First, we experiment with using the all-mpnet-base-v2 sentence embedding model [60] to calculate the embeddings for all problems. Then, to score the rationale and answer for a given problem, we select the 2 closest problems based on embedding proximity as the EPs to calculate the LbT score. The results, presented in $\\S\\ 3.3$ (right), demonstrate that LbT-based scoring provides substantial relative improvement over SC for problems with close problems. For example, those problems within the smallest $5\\%$ cosine distance from their closest problems achieve a $24\\%$ relative improvement over SC. However, for problems lacking similar counterparts in the training set, LbT-based scoring may even have a negative effect, which is expected, as the student\u2019s score on irrelevant EPs cannot reliably reflect the quality of TRs. MATH() [72] is an extension of the MATH dataset, which offers 3 functional variants for each problem. These variants share a similar solution logic, making the dataset suitable for verifying the LbT-TMQ assumption. Therefore, we choose to conduct the main mathematical reasoning experiments of M1 on the MATH() dataset. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "For code synthesis, we select competition-level problems that require heavy reasoning. Instead of using popular datasets like HumanEval [9] or MBPP [2], which contain mostly basic Python programming problems, we extract more challenging competition-level problems directly from the LeetCode platform. To verify the LbT-TMQ assumption, it is important for the EPs and TPs to be similar. Fortunately, LeetCode offers curated \u201cstudy plans\u201d (e.g., programming skills, dynamic programming, etc.), each comprising several datasets with problems that exhibit similar underlying structures and solution strategies, making them suitable for our evaluation. ", "page_idx": 43}, {"type": "text", "text": "In M3, we select the verbal logical reasoning task focused on analyzing and identifying fallacious arguments. In line with our criteria of task selection, determining fallacious arguments also requires accurate reasoning, while the reasoning here takes a verbal natural-language form, complementing the symbolic reasoning in mathematical reasoning and code synthesis. Specifically, we choose Logic [34], a dedicated dataset for logical fallacy detection. In addition, we use Liar [78], a misinformation detection dataset. In misinformation detection, many claims can be factually correct but logically fallacious and it requires LLM\u2019s logical reasoning ability on top of factual memorization. For example, the claim \u201cif we allow students to use calculators in math exams, soon they won\u2019t be able to perform basic arithmetic without them\u201d is based on the fact that calculators are allowed in some exams, but it is misinformation as it has the logical fallacy of \u201cslippery slope\u201d: allowing calculators doesn\u2019t necessarily mean students will lose all arithmetic skills, and there is no evidence suggesting such a causal relationship. Logic and Liar are multi-class classification datasets and we cast them as One-vs-Rest binary classification problems following the practice of previous work on prompt optimization [57]. For Logic, we choose the largest class \u201cfaulty generalization\u201d as positive and the rest as negative. For Liar, we categorize the class \u201ctrue\u201d as positive and the rest as negative. ", "page_idx": 43}, {"type": "text", "text": "D.4 Implementation (the \u201chammer\u201d in the low level) ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "As a preliminary investigation into the potential of LbT in improving AI, we aim for simple implementations and easily controllable experiments to clearly understand the effects of LbT. ", "page_idx": 43}, {"type": "text", "text": "\u2022 M1 and M2: We implement the LbT idea into a single component in well-established pipelines. Specifically, we design an LbT-based scoring component, leveraging the LbT-TMQ assumption \u201cteaching materials that make it easier for students to learn have clearer and more accurate logic\u201d. This allows for simple ablation experiments to isolate and understand the beneftis derived from this particular LbT-TMQ assumption. Baselines: Existing studies have proposed various rationale scoring methods based on GT answer matching [89, 90], answer agreement [31, 79], generation likelihood [79, 84], and self-evaluation [74, 84, 87, 88]. GT answer matching and answer agreement-based methods assume that a more correct or consensus answer indicates a better logic in the corresponding rationale. However, an unclear or wrong rationale can still lead to a correct answer by chance, and a high-quality rationale may result in an incorrect answer due to a simple computational error. In such cases, LbT-based scoring has the potential to better reflect the reasoning quality of the rationale compared to using the correctness and consensus of the final answer. Regarding generation likelihood, as [79] found that using the likelihood as the score for weighted self-consistency even degrades the performance, we did not experiment with this method. For self-evaluation, we provide a comparison in Tab. A7, which demonstrates clear improvements with LbT-based scoring over self-evaluation methods.   \n\u2022 M3: In addition to implementing the LbT-TMQ assumption as a \u201cstatic\u201d scoring mechanism ", "page_idx": 43}, {"type": "text", "text": "for teaching materials, M3 takes a step forward towards exploring the broader potential of LbT by implementing an \u201cactive\u201d and \u201citerative\u201d feedback process. ", "page_idx": 43}, {"type": "text", "text": "D.5 Relation to other machine learning techniques ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Knowledge distillation. In traditional task domains, knowledge distillation [23] is a well-known and widely adopted technique. Most knowledge distillation methods aim at improving the student model by transferring knowledge from a fixed stronger teacher. As a special branch in knowledge distillation, mutual learning [92] allows multiple models to learn from each other in an alternating manner. In the realm of LLMs, researchers have explored knowledge distillation [24, 44, 75] and distillation via synthetic data [1, 28, 41] techniques to transfer knowledge from teacher LLMs to student LLMs by letting teacher models teach student models through token logits, features, and synthetic data [85]. All of these work follows the \u201clearning from teachers (LfT)\u201d instead of our \u201clearning by teaching (LbT)\u201d paradigm. ", "page_idx": 44}, {"type": "text", "text": "Meta learning. Our work also relates to meta-learning [30], a broadly defined term encompassing various methods within the \u201cbetter learning by learning to learn\u201d paradigm. These approaches typically aim to optimize different aspects of the learning process, such as initial weights [21], training hyperparameters (e.g., learning rate, regularization coefficient) [20, 70], neural architectures [19], data augmentations [15], and so on. Many approaches formulate meta-learning as a bilevel optimization problem, where the outer optimization targets the configuration of interest, and the inner optimization is the learning process itself. In the era of increasingly intelligent LLMs, our work introduces a novel, human cognition-inspired paradigm \u2013 \u201cbetter learning by learning to teach\u201d (LbT) \u2013 to continuously evolve the LLMs, which uses the \u201ctask\u201d of teaching (weaker) models to challenge the LLM. Initially, we considered a straightforward implementation involving explicit bilevel optimization, where the inner optimization is the student\u2019s learning process, and the outer optimization can be conducted with reinforcement learning techniques. However, by leveraging the in-context learning abilities of LLMs, we were able to explore the LbT paradigm in a simpler and controllable manner. For example, we optimize the ICL examples for the teacher itself by observing students\u2019 results in M3. ", "page_idx": 44}, {"type": "text", "text": "Machine teaching. Another related area is machine teaching [7, 97, 98], which is defined as an inverse problem of machine learning: finding the optimal teaching examples if the teacher already knows the parameters, and aims to make the student learn the target concept with a minimal set of labeled examples. In contrast, our work focuses on enhancing the performance of the teacher, as measured by the teacher\u2019s accuracy on a test set. In addition, while machine teaching literature typically focuses on small-scale tasks using analytical models, our focus is on the reasoning abilities of state-of-the-art LLMs. ", "page_idx": 44}, {"type": "text", "text": "Other methods related to the LbT idea. It is worth noting that an independent work [32] designs a regularization loss to improve model generalization. This regularization loss can help filter out extraneous details that are difficult for auxiliary students to imitate. This approach is based on the assumption that \u201cgeneralizable correlations should be easy to imitate\u201d, which shares a conceptual similarity with our LbT-TMQ assumption. Another recent work, Math-Shepherd [77], proposes an automatic scoring method for partial rationales within the generating-scoring-finetuning pipeline. It evaluates each partial rationale by measuring how often another \u201ccompleter\u201d model arrives at the correct answer by continuing from the partial rationale. Although this paper does not frame the method from an LbT perspective, the method can be interpreted as LbT-based scoring, based on the assumption that \u201cpartial rationales which make it easier for students to continue and reach the correct answer have clearer and more accurate logic\u201d. In Math-Shepherd, the students (i.e., the completer) are examined by extending the partial teaching rationale for the same problem, whereas in our M2, students are examined on similar problems, using the full rationale from the teaching problem as an exemplar. We hope that framing this method together with our methods within the LbT framework could help enlighten a broader roadmap for drawing insights from human education. ", "page_idx": 44}, {"type": "text", "text": "E More Extension Possibilities ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Based on the framework presented in $\\S\\ 1$ and 2, we can explore other possibilities to incorporate LbT insights into the inference and training of LLMs. For example, to improve answer quality for a TP, we can incorporate the idea of having the teacher reflect on multiple students\u2019 feedback into the search-based output generation pipeline, as follows: for each student, the teacher iteratively reflects on the student\u2019s feedback and updates its TR. The final answer is then derived by aggregating the updated TR for each student. Intuitively, different students might require distinct teaching materials, so generating TR for multiple students can encourage TR diversity. Increasing diversity has been shown to benefti the accuracy of agreement-based aggregation methods [79]. Compared to increasing the sampling temperature of TR, this LbT-based sampling is guided towards high-quality TRs and might yield better results. Besides, LbT can potentially be extended to open-ended problems, such as dialogue, writing, and open-ended math problems [86]. A natural extension could involve using a teacher LLM to evaluate a student\u2019s answer [10] and provide the LbT score. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "F Potential Risks of Bias Perpetuation ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In open-domain problems where no GT judgment exists and LLM-based judgment is needed, it is possible that teaching materials deemed \u201cwell accepted\u201d by students are not necessarily more accurate or closer to the truth. Instead, they may align with the existing biases of teachers or students, posing a risk of the teacher perpetuating their own biases [68] or indirectly learning the students\u2019 biases. ", "page_idx": 45}, {"type": "text", "text": "Furthermore, while this work primarily focuses on utilizing the LbT idea to improve reasoning in mathematical and code domains, considering the potential bias perpetuation effect becomes even more critical in domains where societal bias and fairness are of paramount concern [76]. ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: We discuss the necessary design that makes the method work as expected and the cases it doesn\u2019t achieve improvements compared to baselines. We also discuss possibilities to address the limitations in $\\S\\ 6.3$ . ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 47}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide clear description and pseudo code of the methods, as well as all the experimental settings. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We have open-sourced all the code. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 48}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 48}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: For most experiments, we run multiple random experiments. But note that for some experiments, due to high LLM inference costs and evaluation overheads of LeetCode platform, we only conduct a single experiment. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [No] ", "page_idx": 49}, {"type": "text", "text": "Justification: As this project has been carried out loosely over a long time span (about 10 months), we did not record this information. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 49}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: As our work focuses on enhancing reasoning abilities and exploring a potentially continuously evolving method for LLMs, it could facilitate the deployment of LLMs in critical applications requiring precise reasoning and address the data scaling bottleneck. Regarding the negative societal impacts, a very helpful reviewer also pointed out a potential risk of bias perpetuation, and we have added the discussion to App. F. Although we made efforts to keep this discussion into the main paper as planned in the rebuttal, due to the page limit, we finally include it as a standalone section in the appendix, with a reference to it in $\\S\\ 6$ . ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 49}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 50}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We cite all the dataset providers. For LeetCode, we carefully check their term of service in https://leetcode.com/terms/, and won\u2019t reveal any problems or visible test cases in our paper and code. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 51}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 51}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 51}]