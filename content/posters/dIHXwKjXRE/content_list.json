[{"type": "text", "text": "Towards the Dynamics of a DNN Learning Symbolic Interactions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qihan Ren1\u2217, Junpeng Zhang1\u2217, Yang $\\mathbf{X}\\mathbf{u}^{2}$ , Yue $\\mathbf{Xin^{1}}$ , Dongrui Liu1,3, Quanshi Zhang1 ", "page_idx": 0}, {"type": "text", "text": "1Shanghai Jiao Tong University 2Zhejiang University 3Shanghai Artificial Intelligence Laboratory {renqihan, zhangjp63, zqs1022}@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This study proves the two-phase dynamics of a deep neural network (DNN) learning interactions. Despite the long disappointing view of the faithfulness of post-hoc explanation of a DNN, a series of theorems have been proven [27] in recent years to show that for a given input sample, a small set of interactions between input variables can be considered as primitive inference patterns that faithfully represent a DNN\u2019s detailed inference logic on that sample. Particularly, Zhang et al. [41] have observed that various DNNs all learn interactions of different complexities in two distinct phases, and this two-phase dynamics well explains how a DNN changes from under-fitting to over-fitting. Therefore, in this study, we mathematically prove the two-phase dynamics of interactions, providing a theoretical mechanism for how the generalization power of a DNN changes during the training process. Experiments show that our theory well predicts the real dynamics of interactions on different DNNs trained for various tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Background: mathematically guaranteeing that the inference score of a DNN can be faithfully explained as symbolic interactions. Explaining the detailed inference logic hidden behind the output score of a DNN is considered one of the core issues for the post-hoc explanation of a DNN. However, after a comprehensive survey of various explanation methods, many studies [28, 1, 12] have unanimously and empirically arrived at a disappointing view of the faithfulness of almost all post-hoc explanation methods. Fortunately, the recent progress [27] has mathematically proven that given a specific input sample $\\pmb{x}=[x_{1},\\therefore\\,,x_{n}]^{\\top}$ , a $\\bar{\\mathrm{DNN}^{3}}$ for a classification task usually only encodes a small set of interactions between input variables in the sample. It is proven that these interactions act like primitive inference patterns and can accurately predict all network outputs, no matter how we randomly mask the input sample4. An interaction refers to a non-linear relationship encoded by the DNN between a set of input variables in $S$ . For example, as Figure 1 shows, a DNN may encode a non-linear relationship between the three image patches in $S=\\{x_{1},x_{2},x_{3}\\}$ to form a dog-snout pattern, which makes a numerical effect $I(S)$ on the network output. The complexity (or order) of an interaction is defined as the number of input variables in the set $S$ , i.e., $\\mathrm{order}(S)\\ {\\stackrel{\\mathrm{def}}{=}}\\ |S|$ . ", "page_idx": 0}, {"type": "text", "text": "Our task. Since Zhou et al. [44] found that high-order (complex) interactions usually have a much higher risk of over-ftiting than low-order (simple) interactions, in this study, we hope to further track the change in the complexity of interactions during training, so as to explain the change of the DNN\u2019s generalization power during training. In particular, the time when the DNN starts to learn high-order (complex) interactions indicates the starting point of over-fitting. ", "page_idx": 0}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/8c3a5a2cc64005f28a5a872a0b9f4eb91c9a681b8bf6da1b88ee17dca8dd9563.jpg", "img_caption": ["Figure 1: (a) It is proven that the DNN\u2019s inference on a certain sample is equivalent to a logical model that uses a small number of AND-OR interactions for inference. Each interaction corresponds to a non-linear (AND or OR) relationship between a set $S$ of input variables (e.g., image patches). (b) Sparsity of interactions. We show the strength $|I(S|x)|$ of all $2^{n}$ interactions sorted in descending order. (c) Illustration of the two-phase dynamics of a DNN learning interactions of different orders. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Specifically, we focus on the two-phase dynamics of interaction complexity which was empirically observed by [41], and we aim to mathematically prove this dynamics. First, before training, a DNN with randomly initialized parameters mainly encodes interactions of medium complexities. As Figure 2 shows, the distribution of interactions appears spindle-shaped. Then, in the first phase, the DNN eliminates interactions of medium and high complexities, thereby mainly encoding interactions of low complexity. In the second phase, the DNN gradually learns interactions of increasing complexities. We have conducted experiments to train DNNs with various architectures for different tasks. It shows that our theory can well predict the learning dynamics of interactions in real DNNs. ", "page_idx": 1}, {"type": "text", "text": "The proven two-phase dynamics explain hidden factors that push the DNN from under-fitting to over-fitting. (1) In the first phase, the DNN mainly removes noise interactions, (2) In the second phase, the DNN gradually learns more complex and non-generalizable interactions toward over-ftiting. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Long-standing disappointment on the faithfulness of existing post-hoc explanation of DNNs. Many studies [30, 40, 29, 2, 15] have explained the inference score of a DNN, but how to mathematically formulate and guarantee the faithfulness of the explanation is still an open problem. For example, using an interpretable surrogate model to approximate the output of a DNN [3, 11, 35, 34] is a classic explanation technique. However, the good matching between the DNN\u2019s output and the surrogate model\u2019s output cannot fully guarantee that the two models use exactly the same inference patterns and/or use the same attention. Therefore, many studies [28, 12, 1] have unanimously and empirically arrived at a disappointing view of the faithfulness of current explanation methods. Rudin [28] pointed out that inaccurate post-hoc explanations of DNNs would be harmful to high-stakes applications. Ghassemi et al. [12] showed various failure cases of current explanation methods in the healthcare field and argued that using these methods to aid medical decisions was a false hope. ", "page_idx": 1}, {"type": "text", "text": "New progress towards proving the faithfulness of symbolic explanation of a DNN. Despite the disappointing view of post-hoc explanation methods, we have established a theory system of interactions within three years, which includes more than 30 papers, to quantify the symbolic concepts encoded by a DNN and explain the hidden factors that determine the generalization power and robustness of a DNN. We revisit this theory system as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Proving interactions act as faithful primitives inference patterns encoded by the DNN. Recent achievements in the theory system of interactions have provided a new perspective to formulate primitive inference patterns encoded by a DNN. We discovered [23] and proved [27] that a DNN\u2019s inference logic on a certain sample can be explained by only a small number of interactions. Furthermore, we discovered that salient interactions usually represented common inference patterns shared by different samples (sample-wise transferability of interactions) [21], and proposed a method to extract generalizable interactions shared by different DNNs (model-wise transferability of interactions) [4]. ", "page_idx": 1}, {"type": "text", "text": "The above studies indicated that salient interactions could be considered primitive inference patterns encoded by a DNN, which served as the theoretical foundation of this study. Based on interactions, we also defined and learned the optimal baseline value for the Shapley value [25], and explained the encoding of different types of visual patterns in DNNs for image classification [5, 6]. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Using interactions to explain the representation power of DNNs. Our recent studies showed that interactions well explained the hidden factors that determine the adversarial robustness [24], adversarial transferability [37], and generalization power [44] of a DNN. We also discovered and proved the representation bottleneck of a DNN in encoding middle-complexity interactions [7]. In addition, we proved that compared to a standard DNN, a Bayesian neural network (BNN) tended to avoid encoding complex interactions [26], thus explaining the good adversarial robustness of BNNs. We discovered and explained the phenomenon that DNNs tended to learn simple interactions more easily than complex interactions [22]. We found that complex interactions were less generalizable than simple interactions [44], and further discovered the two-phase dynamics of a DNN learning interactions of different complexities [41]. To this end, this study aims to theoretically prove the discovery in [41] to better understand the two-phase dynamics of interactions. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Using interactions to unify the common mechanism of various empirical deep learning methods. We proved that fourteen attribution methods could all be explained as a re-allocation of interaction effects [8]. We proved that twelve existing methods to improve adversarial transferability all shared the common utility of suppressing the interactions between adversarial perturbation units [42]. ", "page_idx": 2}, {"type": "text", "text": "3 Dynamics of interactions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary: interactions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let us consider a DNN $v$ and an input sample $\\pmb{x}=[x_{1},\\cdots\\,,x_{n}]^{\\top}$ with $n$ input variables indexed by $N=\\{1,\\cdot\\cdot\\cdot,n\\}$ . In different tasks, one can define different input variables, e.g., each input variable may represent an image patch for image classification or a word/token for text classification. Let us consider a scalar output5 of a DNN, denoted by $v(\\pmb{x})\\in\\mathbb{R}$ . Previous studies [4, 43] show that the output score $v(x)$ can be decomposed into the sum of AND interactions and $O R$ interactions. ", "page_idx": 2}, {"type": "equation", "text": "$$\nv(\\pmb{x})=v(\\pmb{x}_{\\emptyset})+\\sum_{\\emptyset\\neq\\pmb{S}\\subseteq N}I_{\\mathrm{and}}(S|\\pmb{x})+\\sum_{\\emptyset\\neq S\\subseteq N}I_{\\mathrm{or}}(S|\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the computation of $I_{\\mathrm{and}}(S|x)$ and $I_{\\mathrm{or}}(S|x)$ will be introduced later in Eq. (2). ", "page_idx": 2}, {"type": "text", "text": "How to understand the physical meaning of AND-OR interactions. Suppose that we are given an input sample $\\textbf{\\em x}$ . According to Theorem 2, a non-zero interaction effect $I_{\\mathrm{and}}(S|x)$ indicates that the entire function of the DNN must equivalently encode an AND relationship between input variables in the set $S\\subseteq N$ , although the DNN does not use an explicit neuron to model such an AND relationship. As Figure 1 shows, when the image patchs in the set $S_{2}\\!=\\!\\left\\{x_{1}\\!=\\!n o s e,x_{2}\\!=\\!t o n g u e,x_{3}\\!=\\!c h e e k\\right\\}$ are all present (i.e., not masked), the three regions form a dog-snout pattern, and make a numerical effect $\\bar{I}_{\\mathrm{and}}(S_{2}|x)$ to push the output score $v(x)$ towards the dog category. Masking any image patch in $S_{2}$ will deactivate the AND interaction and remove $I_{\\mathrm{and}}(S_{2}|\\pmb{x})$ from $v(x)$ . This will be shown by Theorem 2. Likewise, $I_{\\mathrm{or}}(S|x)$ can be considered as the numerical effect of the OR relationship encoded by the DNN between input variables in the set $S$ . As Figure 1 shows, when one of the patches in $S_{1}=\\{x_{4}=s p o t t y\\ r e g i o n I,x_{5}=s p o t t y\\ r e g i o n2\\}$ is present, a speckles pattern is used by the DNN to make a numerical effect $I_{\\mathrm{or}}(S_{1}|x)$ on the network output $v(x)$ . ", "page_idx": 2}, {"type": "text", "text": "Definition and computation. Given a DNN and an input $\\textbf{\\em x}$ , the AND-OR interactions between each specific set of input variables $S\\subseteq N(S\\neq\\emptyset)$ are computed as follows [4, 43]. ", "page_idx": 2}, {"type": "equation", "text": "$$\nI_{\\mathrm{and}}(S|x)=\\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{\\mathrm{and}}\\left(x_{T}\\right),\\quad I_{\\mathrm{or}}(S|x)=-\\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{\\mathrm{or}}\\left(x_{N\\setminus T}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{T}$ denotes the sample in which input variables in $N\\setminus T$ are masked6, while input variables in $T$ are unchanged. The network output on each masked sample $v(\\pmb{x}_{T}),T\\subseteq N$ , is decomposed into two components: (1) the component $v_{\\mathrm{and}}(\\pmb{x}_{T})=0.5v(\\pmb{x}_{T})+\\gamma T$ that exclusively contains AND interactions, and (2) the component $v_{\\mathrm{or}}(\\pmb{x}_{T})=0.5v(\\pmb{x}_{T})-\\gamma_{T}$ that exclusively contains OR interactions, subject to $v(\\pmb{x}_{T})=v_{\\mathrm{and}}(\\pmb{x}_{T})+v_{\\mathrm{or}}(\\pmb{x}_{T})$ . Appendix F.1 shows that $\\begin{array}{r}{{v}_{\\mathrm{and}}({\\pmb x}_{T})={v}({\\pmb x}_{\\varnothing})+\\sum_{\\varnothing\\neq S^{\\prime}\\subseteq T}I_{\\mathrm{and}}(S^{\\prime}|{\\pmb x})}\\end{array}$ and $\\begin{array}{r}{v_{\\mathrm{or}}\\bigl(\\pmb{x}_{T}\\bigr)=\\sum_{S^{\\prime}\\subseteq N:S^{\\prime}\\cap T\\neq\\emptyset}I_{\\mathrm{or}}\\bigl(S^{\\prime}|\\pmb{x}\\bigr)}\\end{array}$ . The sparsest AND-OR interactions are extracted by minimizing the follow ing objective [20]: $\\begin{array}{r}{\\operatorname*{min}_{\\{\\gamma_{T}\\}}\\sum_{S\\subset N}|I_{\\mathrm{and}}(S|x)|+|I_{\\mathrm{or}}(S|x)|}\\end{array}$ . Please see Appendix C for details about the computation and Appendix D for mathematical support of the coefficient in Eq. (2). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Salient interactions and noisy patterns. Let us enumerate all $2^{n}$ combinations of variables $S\\subseteq N$ , and compute the interaction effects $I_{\\mathrm{and}}(S|x)$ and $I_{\\mathrm{or}}(S|x)$ . We can identify a few salient interactions from all these interactions, i.e., interactions whose absolute value exceeds a threshold $(|I_{\\mathrm{and}}(S|x)|\\geq\\tau$ or $|I_{\\mathrm{or}}(S|{\\pmb x})|\\geq\\tau)$ . Other interactions have small effects and are termed noisy patterns. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Sparsity property, proven by [27], and discussed in Appendix B). Given a DNN $v$ and an input sample $\\textbf{\\em x}$ with $n$ input variables, let $\\Omega\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{S\\subseteq N:|I_{\\mathrm{and}}(S|x)|\\geq\\tau\\}$ denote the set of salient AND interactions whose absolute value exceeds a threshold $\\tau$ . If the DNN can generate relatively stable inference outputs $v({\\bf x}_{S})$ on masked samples7, then the size of the set $|\\Omega|$ has an upper bound of ${\\mathcal{O}}(n^{\\xi}/\\tau)$ , where $\\xi$ is an intrinsic parameter for the smoothness of the network function $\\boldsymbol{v}(\\cdot)$ . Empirically, $\\xi$ is usually within the range of [1.9,2.2]. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Universal matching property, proven in [4] and Appendix F.1). Given an input sample $\\hat{\\pmb{x}}$ , let us construct the following surrogate logical model $f(\\cdot)$ to use AND-OR interactions for inference, which are extracted from the DNN $v(\\cdot)$ on the sample $\\hat{\\pmb{x}}$ . Then, the output of the surrogate logical model $f(\\cdot)$ can always match the output of the DNN $v(\\cdot)$ , no matter how the input sample is masked. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle S\\subseteq N,\\,f(\\hat{x}_{S})=v(\\hat{x}_{S}),\\,f(\\hat{x}_{S})=\\underbrace{v(\\hat{x}_{\\theta})+\\sum_{I\\b{\\operatorname{mad}}}(T|\\hat{x})\\cdot\\mathbb{I}\\left(\\frac{\\hat{\\alpha}_{S}\\,v(\\mathrm{iggers})}{\\mathrm{AND~relation}\\,T}\\right)}_{T\\subseteq N}+\\sum_{I\\b{\\operatorname{mad}}(\\Omega_{S})}+\\sum_{I\\b{\\operatorname{mad}}(I)}\\cdot\\mathbb{I}\\left(\\frac{\\hat{\\alpha}_{S}\\,v(\\mathrm{iggers})}{\\mathrm{v}_{\\infty}(\\Omega_{S})}\\right)}\\\\ &{\\qquad=v(\\pmb{x}=\\hat{x}_{\\theta})+\\sum_{\\vartheta\\neq T\\subset S}I_{\\mathrm{and}}\\,(T|\\pmb{x}=\\hat{x})+\\sum_{T\\subset N:T\\cap S\\neq\\vartheta}I_{\\mathrm{or}}\\,(T|\\pmb{x}=\\hat{x})}\\\\ &{\\qquad\\approx v(\\pmb{x}=\\hat{x}_{\\theta})+\\sum_{T\\in\\Omega_{\\mathrm{ad}}:\\vartheta\\neq T\\subset S}I_{\\mathrm{and}}\\,(T|\\pmb{x}=\\hat{x})+\\sum_{T\\in\\Omega_{\\mathrm{ac}}:T\\cap S\\neq\\vartheta}I_{\\mathrm{or}}\\,(T|\\pmb{x}=\\hat{x})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Omega_{\\mathrm{and}}$ is the set of all salient AND interactions, and $\\Omega_{\\mathrm{or}}$ is the set of all salient $O R$ interactions. ", "page_idx": 3}, {"type": "text", "text": "What makes the interaction-based explanation faithful. The following four properties guarantee that the inference score of a DNN can be faithfully explained by symbolic interactions. ", "page_idx": 3}, {"type": "text", "text": "$\\bullet$ Sparsity property. The sparsity property means that a DNN for a classification task usually only encodes a small number of AND interactions with salient effects, i.e., for most of all $2^{n}$ subsets of input variables $S\\subseteq N$ , $I_{\\mathrm{and}}(S|\\pmb{x})$ has almost zero interaction effect. Specifically, the sparsity property has been widely observed on various DNNs for different tasks [21], and it is also theoretically proven (see Theorem 1). The number of AND interactions whose absolute value exceeds the threshold $\\tau$ $(|I_{\\mathrm{and}}(S|x)|\\geq\\tau)$ , is ${\\mathcal{O}}(n^{\\xi}/\\tau)$ , where $\\xi$ is empirically within the range of [1.9, 2.2]. This indicates that the number of salient interactions is much less than $2^{n}$ . Furthermore, the sparsity property also holds for OR interactions, because an OR interaction can be viewed as a special kind of AND interaction8. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Universal matching property. The universal matching property means that the output of the DNN on a masked sample $\\pmb{x}_{S}$ can be well matched by the sum of interaction effects, no matter how we randomly mask the sample and obtain $\\pmb{x}_{S}$ . This property is proven in Theorem 2. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Transferability property. The transferability property means that salient interactions extracted from one input sample can usually be extracted from other input samples as well. If so, these interactions are considered transferable across different samples. This property has been widely observed by [21] on various DNNs for different tasks. ", "page_idx": 3}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/13980b2ef2132cc4ec0e9d085ce82435b1e48e90d50d75674609b10541bebdec.jpg", "img_caption": ["Figure 2: The distribution of interaction strength $I_{\\mathrm{real}}^{(k)}$ over different orders $k$ . Each row shows the change in the distribution during the training process. Experiments showed that the two-phase phenomenon widely existed on different DNNs trained on various datasets. It also verified the finding in [41] that the beginning of the 2nd phase was temporally aligned with the time point when the loss gap increased. Please see Appendix J.1 for results on the other six DNNs trained for 3D point cloud/image/sentiment classification. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "\u2022 Discrimination property. This property means that the same interaction extracted from different samples consistently contributes to the classification of a certain category. This property has been observed on various DNNs [21], and it implies that interactions are discriminative for classification. ", "page_idx": 4}, {"type": "text", "text": "Complexity/order of interactions. The complexity (or order) of an interaction is defined as the number of input variables in the set $S$ , i.e., $\\mathrm{order}(S)\\ {\\stackrel{\\mathrm{def}}{=}}\\ |S|$ . In this way, a high-order interaction represents a complex non-linear relationship among many input variables. ", "page_idx": 4}, {"type": "text", "text": "3.2 Two-phase dynamics of learning interactions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Zhang et al. [41] have discovered the following two-phase dynamics of interaction complexity during the training process. (1) As Figure 2 shows, before the training process, the DNN with randomly initialized parameters mainly encodes interactions of medium orders. (2) In the first phase, the DNN removes initial interactions of medium and high orders, and mainly encodes low-order interactions. (3) In the second phase, the DNN gradually learns interactions of increasing orders. ", "page_idx": 4}, {"type": "text", "text": "To better illustrate this phenomenon, we followed [41] to conduct experiments on different DNNs, including AlexNet [17], VGG [31], BERT [9], DGCNN [38], and on various datasets, including image data (MNIST [19], CIFAR-10 [16], CUB-200-2011 [36], and Tiny-ImageNet [18]), natural language data (SST-2 [32]), and point cloud data (ShapeNet [39]). For image data, we followed [41] to select a random set of ten image patches as input variables. For natural language data, we set the entire embedding vector of each token as an input variable. For point cloud data, we took point clusters as input variables. Please see Appendix G.3 for the detailed settings. We set $v(\\pmb{x})\\,\\,{=}\\,\\mathrm{log}\\,\\left(p(y^{\\mathrm{truth}}|\\pmb{x})/[1\\stackrel{\\cdot}{-}p(y^{\\mathrm{truth}}|\\pmb{x})]\\right)$ by following [7], where $p(y^{\\mathrm{truth}}|x)$ denotes the probability of classifying the input sample $\\textbf{\\em x}$ to the ground-truth category. We followed [41] to define the interaction whose absolute value is greater than or equal to $\\tau=0.03\\ \\mathbb{E}_{\\mathbf{x}}[|v(\\mathbf{x})-v(\\mathbf{x}_{\\emptyset})|]$ as salient interaction. For interactions of each $k$ -th order, we normalized the strength of salient interactions as $\\begin{array}{r}{I_{\\mathrm{real}}^{(k)}\\!=\\!\\mathbb{E}_{\\mathbf{x}}\\!\\left[\\sum_{\\mathrm{rype}\\in\\{\\mathrm{and},\\mathrm{or}\\}}\\sum_{S:|S|=k,|I_{\\mathrm{vpe}}(S|\\mathbf{x})|\\geq\\tau}|I_{\\mathrm{type}}(S|\\mathbf{x})|\\right]\\!/Z}\\end{array}$ to enable fair comparison between different training epochs9, where $Z\\!=\\!\\mathbb{E}_{1\\leq k^{\\prime}\\leq n}\\mathbb{E}_{\\mathbf{x}}\\!\\left[\\sum_{{\\mathrm{type}}\\in\\{{\\mathrm{and}},{\\mathrm{or}}\\}}\\!\\sum_{S:|S|=k^{\\prime},|I_{{\\mathrm{type}}}(S|\\mathbf{x})|\\geq\\tau}\\!\\left|I_{{\\mathrm{type}}}(S|\\mathbf{x})\\right|\\right]$ denotes the normalizing constant. ", "page_idx": 4}, {"type": "text", "text": "Figure 2 shows how the distribution of interaction strength $I_{\\mathrm{real}}^{(k)}$ of different orders changed throughout the entire training process, and it demonstrates that the two-phase dynamics widely existed on different DNNs trained on various datasets. Before training, the interaction strength of medium orders dominated, and the distribution of interaction strength of different orders looked like a spindle. In the first phase (from the 2nd column to the 3rd column in the figure), the strength of medium-order and high-order interactions gradually shrank to zero, while the strength of low-order interactions increased. In the second phase (from the 3rd column to the 6th column in the figure), the DNN learned interactions of increasing orders (complexities). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "How to understand the two-phase phenomenon. Previous studies [44, 26] have observed and partially proved that the complexity/order of an interaction can reflect the generalization ability10 of the interaction. Let us consider an interaction that is frequently extracted by a DNN from training samples (see the transferability property in Section 3.2). If this interaction also frequently appears in testing samples, then this interaction is considered generalizable10; otherwise, non-generalizable. To this end, Zhou et al. [44] have discovered that high-order (complex) interactions are less generalizable between training and testing samples than low-order (simple) interactions. Furthermore, Ren et al. [26] have proved that high-order (complex) interactions are more unstable than low-order (simple) interactions when input variables or network parameters are perturbed by random noises. ", "page_idx": 5}, {"type": "text", "text": "Therefore, the two-phase dynamics enable us to revisit the change of generalization power of a DNN: ", "page_idx": 5}, {"type": "text", "text": "1. Before training, the interactions extracted from an initialized DNN exhibited a spindle-shaped distribution of interaction strength over different orders. These interactions could be considered random patterns irrelevant to the task, and such patterns were mostly of medium orders.   \n2. In the first phase, the DNN mainly removed the irrelevant patterns caused by the randomly initialized parameters. At the same time, the DNN shifted its attention to low-order interactions between very few input variables. These low-order interactions usually represented relatively simple and generalizable10inference patterns, without encoding complex inference patterns.   \n3. In the second phase, the DNN gradually learned interactions of increasing orders (increasing complexities). Although there was no clear boundary between under-fitting and over-fitting in mathematics, the learning of very complex interactions had been widely considered as a typical sign of over-fitting10 [44]. ", "page_idx": 5}, {"type": "text", "text": "3.3 Proving of the two-phase dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.3.1 Analytic solution to interaction effects ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As the foundation of proving the dynamics of the two phases, let us first derive the analytic solution to interaction effects at a specific time point during the training process. Then, Sections 3.3.2 and 3.3.3 will use this analytic solution to further explain detailed dynamics in the second phase and the first phase, respectively. Later experiments show that our theory can well predict the true dynamics of all AND-OR interactions during the learning of real DNNs. ", "page_idx": 5}, {"type": "text", "text": "The proof in this subsection can be divided into three steps. (1) We first rewrite a DNN\u2019s inference on an input sample as a weighted sum of triggering functions of different interactions. (2) Then, we can reformulate the learning of the DNN on an input sample as a linear regression problem. (3) Thus, the interactions at an intermediate time point during training can be obtained as the optimal solution to the linear regression problem under a certain level of parameter noises. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Step 1: Rewriting a DNN\u2019s inference on an input sample as a weighted sum of triggering functions of different interactions. For simplicity, let us only focus on the dynamics of AND interactions, because OR interactions can also be represented as a specific kind of AND interactions8 (see Appendix E for details). In this way, without loss of generality, let us just analyze the learning of AND interactions w.r.t. $\\begin{array}{r}{v_{\\mathrm{and}}(\\pmb{x})=v(\\pmb{x}_{\\emptyset})+\\sum_{\\emptyset\\neq\\cal S\\subseteq\\cal N}I_{\\mathrm{and}}(\\cal S|\\pmb{x}),}\\end{array}$ , and simplify the notation as $v(\\pmb{x})=$ $\\begin{array}{r}{{v}({\\pmb x}_{\\varnothing})\\,+\\,\\sum_{\\varnothing\\neq S\\subseteq N}I(S|{\\pmb x})}\\end{array}$ in the following  proof. Our conclusions can also be extended to OR interactions, as mentioned above. ", "page_idx": 5}, {"type": "text", "text": "Given a DNN, we follow [26, 22] to rewrite the inference function of the network $v(x)$ . This is inspired by the universal matching property of interactions in Theorem 2, i.e., given any arbitrarily masked input sample $\\hat{\\b{x}}_{S}~\\boldsymbol{w}.\\boldsymbol{r}t$ . a random subset $S\\subseteq N$ , the network output can always be represented as a linear sum of different interaction effects $\\begin{array}{r}{v(\\pmb{x}=\\hat{\\pmb{x}}_{S})=\\sum_{T\\subset S}I(T|\\pmb{x}=\\hat{\\pmb{x}})}\\end{array}$ . In this way, the following equation rewrites the inference function of the DN N $v(\\bar{\\pmb{x}}=\\hat{\\pmb{x}}_{S})$ as the weighted sum of triggering functions of interactions (see Appendix F.2 for proof). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall\\,S\\subseteq N,\\,\\,v(x\\!=\\!\\hat{x}_{S})=f(x\\!=\\!\\hat{x}_{S}),\\,\\,\\,\\mathrm{subject}\\,\\,\\mathrm{to}\\,\\,f(x)\\overset{\\mathrm{def}}{=}\\sum_{T\\subseteq N}w_{T}\\,\\,J_{T}(x),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the interaction triggering function $J_{T}(x)$ is a real-valued approximation of the binary indicator function $\\mathbb{1}(\\hat{\\pmb{x}}_{S}$ triggers the AND relation $T$ ) in Eq. (3) and returns the triggering value of the interaction pattern $T$ . In particular, we set $w_{\\pmb{\\varnothing}}\\;=\\;v(\\pmb{x}=\\hat{\\pmb{x}}_{\\varnothing})$ , $J_{\\emptyset}(\\pmb{x})\\;=\\;1$ . $J_{T}\\overline{{(x)}}$ is computed as a sum of compositional terms in the Taylor expansion of $v(x)$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\nJ_{T}(\\pmb{x})=\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\Big|_{\\pmb{x}=\\pmb{x}_{\\emptyset}}\\prod_{i\\in T}\\left(x_{i}-b_{i}\\right)^{\\pi_{i}}/w_{T},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the scalar weight $w_{T}$ should be computed as $w_{T}=I(T|\\pmb{x}\\!=\\!\\hat{\\pmb{x}})$ to satisfy the equality in Eq. (6), and $Q_{T}=\\{[\\pi_{1},\\ldots,\\bar{\\pi_{n}}]^{\\top}:\\forall i\\in T,\\pi_{i}\\in\\mathbb{N}^{+};\\forall i\\notin T,\\pi_{i}=0\\}$ . See Appendix F.2 for proof. ", "page_idx": 6}, {"type": "text", "text": "Understanding $J_{T}({\\pmb x})$ and $w_{T}$ . Let us consider a masked sample $\\hat{\\pmb{x}}_{S}$ in which input variables in $N\\setminus S$ are masked. If $T\\subseteq S$ , which means all input variables in $T$ are not masked in $\\hat{\\pmb{x}}_{S}$ , then $J_{T}(\\hat{\\pmb{x}}_{S})=1$ , indicating the interaction pattern is triggered; otherwise, $J_{T}(\\hat{\\pmb{x}}_{S})=0$ , indicating the interaction pattern is not triggered. $w_{T}$ is a scalar weight. Particularly, let $I_{f}(T|x)$ denote the interaction extracted from the function $\\begin{array}{r}{f(\\pmb{x})=\\sum_{T\\subseteq N}w_{T}J_{T}(\\pmb{x})}\\end{array}$ , then we have $I_{f}(T|\\pmb{x})=w_{T}$ . ", "page_idx": 6}, {"type": "text", "text": "\u2022 Step 2: Based on Eq. (6), the learning of the DNN on an input sample can be reformulated as learning the scalar weight $w_{T}$ for each interaction triggering function $J_{T}(x)$ , under a linear regression setting. We can roughly consider the learning problem as a linear regression to a set of potentially true interactions, because it has been discovered by [21, 4] that different DNNs for the same task usually encode similar sets of interactions. Therefore, the learning of a DNN can be considered as training a model to fti a set of pre-defined interactions. In spite of the above simplifying settings, subsequent experiments in Figure 4 still verify that our theoretical results can well predict the learning dynamics of interactions in real DNNs. ", "page_idx": 6}, {"type": "text", "text": "Specifically, let the DNN be trained on a set of samples $\\textit{\\textbf{D}}=\\{(x,y)\\}$ . According to Theorem 2, given each training sample $\\textbf{\\em x}$ , output scores of the finally converged DNN on all $2^{n}$ randomly masked samples $\\{\\pmb{x}_{S}\\,:\\,S\\,\\subseteq\\,N\\}$ can be written in the form of $y_{S}\\;\\stackrel{\\mathrm{def}}{=}\\;y({\\pmb x}_{S})\\;=\\;v({\\pmb x}_{\\emptyset})+$ $\\sum_{\\emptyset\\neq T\\subseteq N}\\mathbb{1}(\\mathbf{x}_{S}$ triggers interaction $\\begin{array}{r}{T)\\cdot w_{T}^{*}=v(\\pmb{x}_{\\emptyset})+\\sum_{\\emptyset\\neq T\\subseteq S}w_{T}^{*}}\\end{array}$ , which is determined by parameters $\\{w_{T}^{*}:T\\subseteq N\\}^{11}$ . $\\{w_{T}^{*}:T\\subseteq N\\}$ can be taken as a set of true interactions that the DNN needs to learn. Therefore, the learning of the converged interactions on the training sample $\\textbf{\\em x}$ can be represented as the regression towards the converged function $y(x_{S})$ on all masked samples $\\{({\\pmb x}_{S},y_{S}):S\\subseteq N\\}$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\pmb{w})=\\mathbb{E}_{S\\subseteq N}[(y_{S}-\\pmb{w}^{\\top}\\pmb{J}(\\pmb{x}_{S}))^{2}].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we simplify the notation as follows. $w\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\operatorname{vec}(\\{w_{T}\\,:\\,T\\subseteq N\\})\\,\\in\\,\\mathbb{R}^{2^{n}}$ denotes the weight vector of $2^{n}$ different interactions, and $J(x_{S})\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\operatorname{vec}(\\{J_{T}(x_{S}):T\\subseteq N\\})\\in\\mathbb{R}^{2^{n}}$ denotes the vector of triggering values of $2^{n}$ different interactions $\\{T\\subseteq N\\}$ on the masked sample $\\pmb{x}_{S}$ . ", "page_idx": 6}, {"type": "text", "text": "\u2022 Step 3: Directly optimizing Eq. (8) gives the interactions of the finally converged DNN $w_{T}\\gets w_{T}^{*}$ , but how do we estimate the interactions in an intermediate time point during the training process? To this end, we assume that the training process of the DNN is subject to parameter noises (see Lemma 1). In fact, this assumption is common. Before training, randomly initialized parameters in the DNN are pure noises without clear meanings. In this way, the DNN\u2019s training process can be viewed as a process of gradually reducing the noise on its parameters. This is also supported by the lottery ticket hypothesis [10], i.e., the learning process actually penalizes most noisy parameters and learns a very small number of meaningful parameters. Therefore, as training proceeds, the noise on the network parameters can be considered to gradually diminish. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 (Noisy triggering function, proven in Appendix F.3). If the inference score of the DNN contains an unlearnable noise, i.e., $\\forall S\\;\\subseteq\\;N,\\widetilde{v}(\\pmb{x}_{S})\\;=\\;v(\\pmb{x}_{S})\\,+\\,\\Delta v_{S}$ , $\\Delta v_{S}\\,\\sim\\,{\\mathcal{N}}(0,\\sigma^{2})$ , then the interaction between input variables w.r.t. $\\varnothing\\neq T\\subseteq N$ , extracted from inference scores $\\{\\widetilde{v}(\\pmb{x}_{S})\\}$ can be written as $\\widetilde{I}(T|\\pmb{x})=I(T|\\pmb{x})+\\Delta I_{T}$ , where $\\Delta I_{T}$ denotes the noise in the interaction caused by the noise in th e output $\\Delta v_{S}$ , and we have $\\mathbb{E}[\\Delta I_{T}]=0$ and $\\mathrm{Var}[\\Delta I_{T}]=2^{|T|}\\sigma^{2}$ . In this way, given an input sample $\\hat{\\pmb{x}}$ , we can consider the scalar weight $w_{T}=I(T|\\pmb{x}=\\hat{\\pmb{x}})$ , and consider the interaction triggering function $\\widetilde{J}_{T}(\\pmb{x})=J_{T}(\\pmb{x})+\\epsilon_{T}$ , where $J_{T}({\\pmb x})$ is defined in Eq. (7). $\\epsilon_{T}=\\Delta I_{T}/w_{T}$ represents the noise term on the triggering function. We have $\\mathbb{E}[\\epsilon_{T}]=0$ and $\\mathrm{Var}[\\epsilon_{T}]\\propto2^{|T|}\\sigma^{2}\\;w.r.t$ . noises. ", "page_idx": 6}, {"type": "text", "text": "Therefore, the learned interactions under unavoidable parameter noises can be represented as minimizing the following loss, where we vectorize the noise $\\begin{array}{r}{\\epsilon\\!=\\!\\mathrm{vec}\\big(\\{\\epsilon_{T}:T\\subseteq N\\}\\big)\\!\\in\\!\\mathbb{R}^{\\hat{2}^{n}}}\\end{array}$ for simplicity. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widetilde{L}(\\pmb{w})=\\mathbb{E}_{\\epsilon}\\mathbb{E}_{S\\subseteq N}[(y_{S}-\\pmb{w}^{\\top}\\widetilde{J}(\\pmb{x}_{S}))^{2}]=\\mathbb{E}_{\\epsilon}\\mathbb{E}_{S\\subseteq N}[(y_{S}-\\pmb{w}^{\\top}(\\pmb{J}(\\pmb{x}_{S})+\\epsilon))^{2}].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark. The minimizer to Eq. (9) does not represent the end of training, but represents the intermediate state of interactions after a certain epoch in the training process. We formulate the training process as a process of gradually reducing the noise on the DNN\u2019s parameters, and the minimizer $\\hat{w}$ to Eq. (9) represents the optimal interaction state when the training is subject to certain parameter noises. We will show later that the minimizer $\\hat{w}$ computed under different noise levels can accurately predict the dynamics of interactions during the training process (see Figures 4 and 8). ", "page_idx": 7}, {"type": "text", "text": "Assumption 1. To simplify the proof, we assume that different noise terms $\\epsilon_{T}$ on the triggering function are independent, and uniformly set the variance as $\\forall\\,T\\subseteq N$ , $\\mathrm{Var}[\\epsilon_{T}]=2^{|T|}\\sigma^{2}$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 1 is made according to two findings in Lemma 1: (1) the interaction triggering function $\\widetilde{J}_{T}(\\pmb{x})$ is real-valued subject to the noise on the DNN\u2019s parameters, (2) the variance of the interaction triggering function $\\widetilde{J}_{T}(\\pmb{x})$ increases exponentially along with the order $|T|$ . More importantly, the assumed exponential increase of the variance in the above finding (2) has been widely observed in various DNNs trained for different tasks in previous experiments [26, 22]. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Proven in Appendix F.4). Let $\\begin{array}{r}{\\pmb{\\hat{w}}=\\arg\\operatorname*{min}_{\\pmb{w}}\\widetilde{L}(\\pmb{w})}\\end{array}$ denote the optimal solution to the minimization of the loss function $\\widetilde L(\\boldsymbol{w})$ . Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}=(J^{\\top}J+2^{n}\\mathrm{diag}(c))^{-1}J^{\\top}y=(J^{\\top}J+2^{n}\\mathrm{diag}(c))^{-1}J^{\\top}J w^{*}=\\hat{M}w^{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $J\\stackrel{\\mathrm{def}}{=}[J(\\pmb{x}_{S_{1}}),\\pmb{J}(\\pmb{x}_{S_{2}}),\\cdot\\cdot\\cdot\\;,J(\\pmb{x}_{S_{2^{n}}})]^{\\top}\\in\\mathbb{R}^{2^{n}\\times2^{n}}$ is a matrix to represent the triggering values of $2^{n}$ interactions (w.r.t. $2^{n}$ columns) on $2^{n}$ masked samples (w.r.t. $2^{n}$ rows). ${\\pmb x}_{S_{1}},{\\pmb x}_{S_{2}},\\cdots\\,,{\\pmb x}_{S_{2^{n}}}$ enumerate all masked samples. $\\pmb{y}\\stackrel{\\mathrm{def}}{=}[y(\\pmb{x}_{S_{1}}),y(\\pmb{x}_{S_{2}}),\\cdot\\cdot\\cdot\\mathrm{~,~}y(\\pmb{x}_{S_{2^{n}}})]^{\\top}\\in\\mathbb{R}^{2^{n}}$ enumerates the finallyconverged outputs on $2^{n}$ masked samples. $c\\stackrel{\\mathrm{def}}{=}\\mathrm{vec}(\\{\\mathrm{Var}[\\epsilon_{T}]:T\\subseteq N\\})=\\mathrm{vec}(\\{2^{|T|}\\sigma^{2}:T\\subseteq N\\})\\in$ $\\mathbb{R}^{2^{n}}$ denotes the vector of variances of the triggering values of $2^{n}$ interactions. The matrix $\\hat{M}$ is defined as $\\hat{M}\\overset{\\mathrm{def}}{=}(J^{\\top}J+2^{n}\\mathrm{diag}(c))^{-1}J^{\\top}J,$ , and $w^{*}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathrm{vec}(\\{w_{T}^{*}:T\\subseteq N\\})$ . ", "page_idx": 7}, {"type": "text", "text": "In this way, Theorem 3 provides an analytic solution to the minimization of $\\widetilde L(\\boldsymbol{w})$ under parameter noises. Experiments in Figure 4 will show that the learning dynamics of interactions derived from our simplifying assumption can still predict the real distribution of interactions over different orders. ", "page_idx": 7}, {"type": "text", "text": "3.3.2 Explaining the dynamics in the second phase ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Based on the above analytic solution, this subsection aims to prove that in the second phase, the DNN first encodes interactions of low orders and then gradually encodes interactions of higher orders. ", "page_idx": 7}, {"type": "text", "text": "\u2022 The second phase can be viewed as a process of gradually reducing the noise level $\\sigma^{2}$ . The analytic solution $\\hat{w}$ in Theorem 3 under different noise levels $\\sigma^{\\check{2}}$ enables us to analyze the dynamics of interactions during the second phase. This is because the noise on the network parameters can be considered to gradually diminish during the training process, as we assume in Section 3.3.1. Then accordingly, the noise level $\\sigma^{2}$ of the noise term $\\epsilon_{T}$ on the interaction triggering function also gradually diminishes during training. At the start of the second phase, the noise level $\\sigma^{2}$ is large, and the interaction triggering function $\\widetilde{J}_{T}(\\pmb{x})$ is dominated by the noise term $\\epsilon_{T}$ . Later, as training proceeds in the second phase, the noise level $\\sigma^{2}$ gradually decreases, making less effect on the interaction triggering function. ", "page_idx": 7}, {"type": "text", "text": "\u2022 The change of the analytic solution $\\hat{w}$ along with the decreasing noises $\\sigma^{2}$ explains the dynamics in the second phase. We prove that as $\\sigma^{2}$ decreases, the ratio of low-order interaction strength to high-order interaction strength in the analytic solution $\\hat{w}$ decreases. This means that the DNN gradually learns higher-order interactions in the second phase, which can be verified by our observation in Figure 2. The detailed results are derived as follows. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2 (Proven in Appendix F.5). The compositional term $J_{T}(x)$ in the Taylor expansion in Eq. (7) always has fixed values on $2^{n}$ masked samples $\\{\\pmb{x}_{S}:S\\subseteq N\\}$ , i.e., $\\forall S\\subseteq N.$ , $J_{T}(\\pmb{x}_{S})=\\mathbb{1}(T\\subseteq S)$ . It means that the matrix $J=[J(\\pmb{x}_{S_{1}}),J(\\pmb{x}_{S_{2}}),\\cdot\\cdot\\cdot\\;,J(\\pmb{x}_{S_{2^{n}}})]^{\\top}\\;\\in\\;\\{0,1\\}^{2^{n}\\times2^{n}}$ in $E q$ . $(I O)$ is a fixed binary matrix, no matter how we change the DNN $\\boldsymbol{v}(\\cdot)$ or the input sample $\\textbf{\\em x}$ . ", "page_idx": 7}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/2d978de2184f96e053dc8d00e95262d1337a5daa3cd8fe98f3ab0d4265425421.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Monotonic increase of $r^{(k)}$ along with $\\sigma^{2}$ mentioned in Proposition 1. We show the curves of $\\bar{r}^{(k)}$ when we set different numbers of input variables $n$ and different orders $k=1,\\cdot\\cdot\\cdot\\,,n-1$ . ", "page_idx": 8}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/bc7ea95ca6b95e9966fb174ccb920ea38532a64e282fc53d424a0acc76690ceb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Comparison between the theoretical distribution of interaction strength $I_{\\mathrm{theo}}^{(k)}$ and the real distribution of interaction strength $I_{\\mathrm{real}}^{(k)}$ in the second phase. Please see Appendix J.3 for the comparison on the other six DNNs trained for 3D point cloud/image/sentiment classification. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Proven in Appendix F.6). According to Theorem 3, we can write the analytic solution of the interaction effect $\\hat{w}_{T}$ w.r.t. a subset $T$ as $\\hat{w}_{T}=\\hat{m}_{T}^{\\top}{w}^{*}$ , where $\\hat{m}_{T}^{\\top}\\in\\mathbb{R}^{1\\times2^{n}}$ denotes a row vector of the matrix $\\hat{M}=[\\hat{m}_{T_{1}},\\hat{m}_{T_{2}}\\cdot\\cdot\\cdot\\,,\\hat{m}_{T_{2^{n}}}]^{\\top}$ , indexed by $T$ . Combining with Lemma 2, for any two subsets $T,T^{\\prime}\\subseteq N$ of the same order, i.e., $|T|=|T^{\\prime}|,$ , we have $\\Vert\\hat{m}_{T}\\Vert_{2}=\\Vert\\hat{m}_{T^{\\prime}}\\Vert_{2}$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 1. For any two subsets $T,T^{\\prime}\\subseteq N$ with $|T|<|T^{\\prime}|,\\,\\|\\hat{m}_{T}\\|_{2}/\\|\\hat{m}_{T^{\\prime}}\\|_{2}$ is greater than $^{\\,l}$ and decreases monotonically as $\\sigma^{2}$ decreases throughout training. The norm $\\Vert\\hat{m}_{T}\\Vert_{2}$ is only determined by n, $\\sigma^{2}$ , and the order $|T|$ , but is agnostic to finally-converged interactions $\\{w_{T}^{*}:T\\subseteq N\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 1 shows a monotonic decrease of $\\|\\hat{m}_{T}\\|_{2}/\\|\\hat{m}_{T^{\\prime}}\\|_{2}$ along with the decrease of $\\sigma^{2}$ . The physical meaning of $\\lVert\\hat{\\pmb{m}}_{T}\\rVert_{2}/\\lVert\\hat{\\pmb{m}}_{T^{\\prime}}\\rVert_{2}$ can be understood as follows. According to $\\hat{w}_{T}\\,=\\,\\hat{m}_{T}^{\\top}{w}^{*}$ , $\\Vert\\hat{m}_{T}\\Vert_{2}$ reflects the strength of the DNN encoding the interaction $T$ . In this way, $\\|\\hat{m}_{T}\\|_{2}/\\|\\hat{m}_{T^{\\prime}}\\|_{2}$ measures the relative strength of encoding a low-order interaction $T\\,w.r.t.$ . that of encoding a highorder interaction $T^{\\prime}$ . ", "page_idx": 8}, {"type": "text", "text": "Conclusions from Theorem 4 and Proposition 1: Because the second phase is viewed as a process of gradually reducing the noise level $\\sigma^{2}$ , Theorem 4 and Proposition 1 explain why the DNN mainly encodes low-order interactions and suppresses high-order interactions at the start of the second phase (when $\\sigma^{2}$ is large). They also explain why the DNN learns interactions of increasing orders during the second phase (when $\\sigma^{\\bar{2}}$ gradually decreases). ", "page_idx": 8}, {"type": "text", "text": "Experimental verification of Proposition $^{\\,l}$ : We measured the relative strength $r^{(k)}\\stackrel{\\mathrm{def}}{=}\\|\\hat{m}_{T}\\|_{2}/\\|\\hat{m}_{T^{\\prime}}\\|_{2}$ subject to $|T|=k$ and $\\left|T^{\\prime}\\right|=k+1$ , for $k=1,\\cdots\\,,n-1$ , under different values of $\\sigma^{2}$ . Figure 3 shows that when $\\sigma^{2}$ decreased, $r^{(k)}$ monotonically decreased for all orders $k=1,\\cdot\\cdot\\cdot\\,,n-1$ , which verified the proposition. The experiment was conducted using different numbers of input variables $n$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 5 (Proven in Appendix F.7). When $\\sigma=0$ , $\\hat{w}$ satisfies $\\forall\\,\\varnothing\\neq T\\subseteq N,\\,\\,\\hat{w}_{T}=w_{T}^{*}.$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 5 shows a special case when there is no noise on the network parameters. Then, the DNN learns the finally converged interactions $\\{w_{T}^{*}\\;:\\;T\\;\\subseteq\\;N\\}$ . Note that the finally converged DNN probably encodes some interactions of high orders, which correspond to over-fitted patterns. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Experiments on real datasets. We conducted experiments to examine whether our theory could predict the real dynamics of interaction strength of different orders when we trained DNNs in practice. We trained AlexNet and VGG on the MNIST dataset, the CIFAR-10 dataset, the CUB-200-2011 ", "page_idx": 8}, {"type": "text", "text": "dataset, and the Tiny-ImageNet dataset, trained BERT-Tiny and BERT-Medium on the SST-2 dataset, and trained DGCNN on the ShapeNet dataset. Then, we computed the real distribution of interaction strength over different orders on each DNN, and tracked the change of the distribution throughout the training process. As mentioned in Section 3.2, the real interaction strength of each $k$ -th order was quantified as $\\begin{array}{r}{I_{\\mathrm{real}}^{(k)}=\\mathbb{E}_{\\mathbf{\\boldsymbol{x}}}[\\sum_{S:|S|=k,|I(S|\\mathbf{\\boldsymbol{x}})|\\ge\\tau}|I(S|\\mathbf{\\boldsymbol{x}})|]\\,/\\,Z^{\\,12}}\\end{array}$ . Accordingly, we defined the metric $I_{\\mathrm{theo}}^{(k)}=$ $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}}\\!\\left[\\sum_{S:|S|=k,|\\hat{w}_{S}|\\ge\\tau_{\\mathrm{theo}}}|\\hat{w}_{S}|\\right]/Z_{\\mathrm{theo}}}\\end{array}$ in the same way of $I_{\\mathrm{real}}^{(k)}$ to measure the theoretical distribution of the interaction strength, where $\\begin{array}{r}{Z_{\\mathrm{theo}}=\\mathbb{E}_{1\\leq k^{\\prime}\\leq n}\\mathbb{E}_{\\pmb{x}}[\\sum_{S:|S|=k^{\\prime},|\\hat{w}_{S}|\\geq\\tau_{\\mathrm{theo}}}|\\hat{w}_{S}|]}\\end{array}$ , $\\tau_{\\mathrm{theo}}=0.03\\cdot|v_{\\mathrm{theo}}(\\pmb{x})-\\hat{w}_{\\emptyset}|$ and $\\begin{array}{r}{v_{\\mathrm{theo}}({\\pmb x})\\stackrel{\\mathrm{def}}{=}\\sum_{S\\subseteq N}\\hat{w}_{S}}\\end{array}$ . To compute the theoretical solution $\\hat{\\pmb w}=\\hat{\\pmb M}\\pmb w^{*}$ in Eq. (10), given an input sample $\\textbf{\\em x}$ , we used the set of salient interactions $\\Omega=\\{S\\subseteq N:|I(S|x)|\\geq\\tau\\}$ ) extracted from the finally converged DNN to construct the set of true interactions $\\pmb{w}^{*}$ . ", "page_idx": 9}, {"type": "text", "text": "Figure 4 shows that the theoretical distribution $I_{\\mathrm{theo}}^{(k)}$ could well match the real distribution $I_{\\mathrm{real}}^{(k)}$ at different training epochs. Particularly, we used a sequence of theoretical distributions of $I_{\\mathrm{theo}}^{(k)}$ with ddeetcerremasiinnegd $\\sigma^{2}$ avcahliueevse  ttoh em baetscth  mthatec rhe able tdwiseterin $I_{\\mathrm{theo}}^{(k)}$ na nodf $I_{\\mathrm{real}}^{(k)}$ $I_{\\mathrm{real}}^{(k)}$ at different epochs. The $\\sigma^{2}$ value was ", "page_idx": 9}, {"type": "text", "text": "3.3.3 Explaining the dynamics in the first phase ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Because the spindle-shaped distribution of interaction strength in a randomly initialized DNN has already been proven by [41], in this subsection, let us further explain the DNN\u2019s dynamics in the first phase based on Eq. (9). As previously shown in Figure 2, in the first phase, the DNN removes initial interactions of medium and high orders, and mainly encodes low-order interactions. ", "page_idx": 9}, {"type": "text", "text": "Therefore, the first phase is explained as the process of removing chaotic initial interactions and converging to the optimal solution to Eq. (9) under large parameter noise (i.e., large $\\sigma^{2}$ ). In sum, the first phase is a process of pushing initial random interactions to the optimal solution, while the second phase corresponds to the change of the optimal solution as $\\sigma^{2}$ gradually decreases. ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion and discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we have proven the two-phase dynamics of a DNN learning interactions of different orders. Specifically, we have followed [26, 22] to reformulate the learning of interactions as a linear regression problem on a set of interaction triggering functions. In this way, we have successfully derived an analytic solution to interaction effects when the DNN was learned with unavoidable parameter noises. This analytic solution has successfully predicted a DNN\u2019s two-phase dynamics of learning interactions in real experiments. Considering a series of recent theoretical guarantees of taking interactions as faithful primitive inference patterns encoded by the DNN [44, 27], our study has first mathematically explained why and how the learning process gradually shifts attention from generalizable (low-order) inference patterns to probably over-fitted (high-order) inference patterns. ", "page_idx": 9}, {"type": "text", "text": "Practical implications. A theoretical understanding of the two-phase dynamics of interactions offers a new perspective to monitor the overfitting level of the DNN on different training samples throughout training. The two-phase dynamics enables us to evaluate the overfitting level of each specific sample, making overfitting no longer a problem w.r.t. the entire dataset. We can track the change of the interaction complexity for each training sample, and take the time point when high-order interactions increase as a sign of overfitting. In this way, the two-phase dynamics of interactions may help people remove overfitted samples from training and guide the early stopping on a few \"hard samples.\" ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. This work is partially supported by the National Science and Technology Major Project (2021ZD0111602), the National Nature Science Foundation of China (92370115, 62276165). This work is also partially supported by Huawei Technologies Inc. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [2] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.   \n[3] Zhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu outcome prediction. In AMIA annual symposium proceedings, volume 2016, page 371. American Medical Informatics Association, 2016.   \n[4] Lu Chen, Siyu Lou, Benhao Huang, and Quanshi Zhang. Defining and extracting generalizable interaction primitives from DNNs. In The Twelfth International Conference on Learning Representations, 2024.   \n[5] Xu Cheng, Chuntung Chu, Yi Zheng, Jie Ren, and Quanshi Zhang. A Game-Theoretic Taxonomy of Visual Concepts in DNNs. arXiv preprint arXiv:2106.10938, 2021.   \n[6] Xu Cheng, Xin Wang, Haotian Xue, Zhengyang Liang, and Quanshi Zhang. A Hypothesis for the Aesthetic Appreciation in Neural Networks. arXiv preprint arXiv::2108.02646, 2021. [7] Huiqi Deng, Qihan Ren, Xu Chen, Hao Zhang, Jie Ren, and Quanshi Zhang. Discovering and Explaining the Representation Bottleneck of DNNs. ICLR, 2021.   \n[8] Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, Ziwei Yang, Zheyang Li, and Quanshi Zhang. Unifying Fourteen Post-hoc Attribution Methods with Taylor Interactions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[10] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019.   \n[11] Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784, 2017.   \n[12] Marzyeh Ghassemi, Luke Oakden-Rayner, and Andrew L Beam. The false hope of current approaches to explainable artificial intelligence in health care. The Lancet Digital Health, 3(11):e745\u2013e750, 2021.   \n[13] Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among players in cooperative games. International Journal of game theory, 28(4):547\u2013565, 1999.   \n[14] John C. Harsanyi. A simplified bargaining model for the n-person cooperative game. International Economic Review, 4(2):194\u2013220, 1963.   \n[15] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2668\u20132677. PMLR, 10\u201315 Jul 2018.   \n[16] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, volume 25, pages 1097\u20131105, 2012.   \n[18] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[19] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[20] Mingjie Li and Quanshi Zhang. Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. arXiv preprint arXiv:2304.13312, 2023.   \n[21] Mingjie Li and Quanshi Zhang. Does a Neural Network Really Encode Symbolic Concepts? International Conference on Machine Learning, 2023.   \n[22] Dongrui Liu, Huiqi Deng, Xu Cheng, Qihan Ren, Kangrui Wang, and Quanshi Zhang. Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[23] Jie Ren, Mingjie Li, Qirui Chen, Huiqi Deng, and Quanshi Zhang. Defining and Quantifying the Emergence of Sparse Concepts in DNNs. In The IEEE/CVF Computer Vision and Pattern Recognition Conference, 2023.   \n[24] Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi, and Quanshi Zhang. Towards a Unified Game-Theoretic View of Adversarial Perturbations and Robustness. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 3797\u20133810. Curran Associates, Inc., 2021.   \n[25] Jie Ren, Zhanpeng Zhou, Qirui Chen, and Quanshi Zhang. Can We Faithfully Represent Absence States to Compute Shapley Values on a DNN? In International Conference on Learning Representations, 2023.   \n[26] Qihan Ren, Huiqi Deng, Yunuo Chen, Siyu Lou, and Quanshi Zhang. Bayesian Neural Networks Tend to Ignore Complex and Sensitive Concepts. International Conference on Machine Learning, 2023.   \n[27] Qihan Ren, Jiayang Gao, Wen Shen, and Quanshi Zhang. Where We Have Arrived in Proving the Emergence of Sparse Interaction Primitives in DNNs. In The Twelfth International Conference on Learning Representations, 2024.   \n[28] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence, 1(5):206\u2013215, 2019.   \n[29] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.   \n[30] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In International Conference on Learning Representations, 2014.   \n[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2014.   \n[32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.   \n[33] Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The shapley taylor interaction index. In International Conference on Machine Learning, pages 9259\u20139268. PMLR, 2020.   \n[34] Sarah Tan, Giles Hooker, Paul Koch, Albert Gordo, and Rich Caruana. Considerations when learning additive explanations for black-box models. arXiv preprint arXiv:1801.08640, 2018.   \n[35] Joel Vaughan, Agus Sudjianto, Erind Brahimi, Jie Chen, and Vijayan N Nair. Explainable neural networks based on additive index models. arXiv preprint arXiv:1806.01933, 2018.   \n[36] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The CaltechUCSD Birds-200-2011 Dataset. 2011.   \n[37] Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, and Quanshi Zhang. A Unified Approach to Interpreting and Boosting Adversarial Transferability. In International Conference on Learning Representations, 2021.   \n[38] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Trans. Graph., 38(5), oct 2019.   \n[39] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):1\u201312, 2016.   \n[40] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.   \n[41] Junpeng Zhang, Qing Li, Liang Lin, and Quanshi Zhang. Two-phase dynamics of interactions explains the starting point of a dnn learning over-fitted features. arXiv preprint arXiv:2405.10262v1, 2024.   \n[42] Quanshi Zhang, Xin Wang, Jie Ren, Xu Cheng, Shuyun Lin, Yisen Wang, and Xiangming Zhu. Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. arXiv preprint arXiv:2207.11694, 2022.   \n[43] Huilin Zhou, Huijie Tang, Mingjie Li, Hao Zhang, Zhenyu Liu, and Quanshi Zhang. Explaining how a neural network play the go game and let people learn. arXiv preprint arXiv:2310.09838, 2023.   \n[44] Huilin Zhou, Hao Zhang, Huiqi Deng, Dongrui Liu, Wen Shen, Shih-Han Chan, and Quanshi Zhang. Explaining Generalization Power of a DNN using Interactive Concepts. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024. AAAI Press, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Properties of the AND interaction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Harsanyi interaction [14] (i.e., the AND interaction in this paper) was a standard metric to measure the AND relationship between input variables encoded by the network. In this section, we present several desirable properties/axioms that the Harsanyi AND interaction $I_{\\mathrm{and}}(S|\\pmb{x})$ satisfies. These properties further demonstrate the faithfulness of using Harsanyi AND interaction to explain the inference score of a DNN. ", "page_idx": 13}, {"type": "text", "text": "(1) Efficiency axiom (proven by [14]). The output score of a model can be decomposed into interaction effects of different patterns, i.e. $\\begin{array}{r}{{\\boldsymbol{v}}({\\pmb x})=\\sum_{S\\subseteq N}I_{\\mathrm{and}}(S|{\\pmb x})}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "(2) Linearity axiom. If we merge output scores of two models $v_{1}$ and $v_{2}$ as the output of model $v$ , i.e. $\\forall S\\subseteq N$ , $v(\\pmb{x}_{S})=v_{1}(\\pmb{x}_{S})+v_{2}(\\pmb{x}_{S})$ , then their interaction effects $I_{\\mathrm{and}}^{v_{1}}(S|x)$ and $I_{\\mathrm{and}}^{v_{2}}(S|x)$ can also be merged as $\\forall S\\subseteq N,I_{\\mathrm{and}}^{v}(S|x)=I_{\\mathrm{and}}^{v_{1}}(S|x)+I_{\\mathrm{and}}^{v_{2}}(S|x)$ .   \n(3) Dummy axiom. If a variable $\\textit{i}\\in\\textit{N}$ is a dummy variable, i.e. $\\forall S\\;\\subseteq\\;N\\;\\backslash\\;\\{i\\},v(x_{S\\cup\\{i\\}})\\;=$ $v(\\pmb{x}_{S})+v(\\pmb{x}_{\\{i\\}})$ , then it has no interaction with other variables, $\\forall\\,\\emptyset\\neq S\\subseteq N\\setminus\\{i\\}$ , $I_{\\mathrm{and}}(S\\cup\\{i\\}|x)=0$ . (4) Symmetry axiom. If input variables $i,j\\in N$ cooperate with other variables in the same way, $\\forall S\\subseteq N\\setminus\\{i,j\\},v({\\pmb x}_{S\\cup\\{i\\}})=v({\\pmb x}_{S\\cup\\{j\\}})$ , then they have same interaction effects with other variables, $\\forall S\\subseteq N\\setminus\\{i,j\\},I_{\\mathrm{and}}(S\\cup\\{i\\}|x)=I_{\\mathrm{and}}(S\\cup\\{j\\}|x).$ .   \n(5) Anonymity axiom. For any permutations $\\pi$ on $N$ , we have $\\forall S\\!\\subseteq\\!N,I_{\\mathrm{and}}^{v}(S|x)=I_{\\mathrm{and}}^{\\pi v}(\\pi S|x)$ , where $\\pi S\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{\\pi(i)|i\\in S\\}$ , and the new model $\\pi v$ is defined by $(\\pi v)({\\pmb x}_{\\pi S})\\,=\\,v({\\pmb x}_{S})$ . This indicates that interaction effects are not changed by permutation.   \n(6) Recursive axiom. The interaction effects can be computed recursively. For $\\textit{i}\\in\\textit{N}$ and $S\\subseteq$ $N\\setminus\\{i\\}$ , the interaction effect of the pattern $S\\cup\\{i\\}$ is equal to the interaction effect of $S$ with the presence of $i$ minus the interaction effect of $S$ with the absence of $i$ , i.e. $\\forall S\\subseteq N\\setminus\\{i\\}$ , $I_{\\mathrm{and}}(S\\cup$ $\\{i\\}|\\pmb{x})=I_{\\mathrm{and}}(S|\\pmb{x},i$ is always present) \u2212 $\\cdot\\ I_{\\mathrm{and}}(S|x)$ . $I_{\\mathrm{and}}(S|{\\textbf x},i$ is always present) denotes the interaction effect when the variable $i$ is always present as a constant context, i.e. $I_{\\mathrm{and}}(S|{\\textbf x},i$ is always present) $=$ $\\begin{array}{r}{\\sum_{L\\subseteq S}(-1)^{|S|-|L|}\\cdot v(\\pmb{x}_{L\\cup\\{i\\}})}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "(7) Interaction distribution axiom. This axiom characterizes how interactions are distributed for \u201cinteraction functions\u201d [33]. An interaction function $v_{T}$ parameterized by a subset of variables $T$ is defined as follows. $\\forall S\\subseteq N$ , if $T\\subseteq S$ , $v_{T}(\\pmb{x}_{S})=c$ ; otherwise, $v_{T}(\\pmb{x}_{S})=0$ . The function $v_{T}$ models pure interaction among the variables in $T$ , because only if all variables in $T$ are present, the output value will be increased by $c$ . The interactions encoded in the function $v_{T}$ satisfies $I_{\\mathrm{and}}(T|x)=c$ , and $\\forall S\\neq T$ , $I_{\\mathrm{and}}(S|{\\pmb x})=0$ . ", "page_idx": 13}, {"type": "text", "text": "B Common conditions for sparse interactions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Ren et al. [27] have formulated three mathematical conditions for the sparsity of AND interactions, as follows. ", "page_idx": 13}, {"type": "text", "text": "Condition 1. The DNN does not encode interactions higher than the M-th order: $\\prime\\;S\\in\\{S\\subseteq N\\mid}$ $|S|\\geq M+1\\}$ , $I_{a n d}(S|{\\pmb x})=0$ . ", "page_idx": 13}, {"type": "text", "text": "Condition 1 implies that the DNN does not encode extremely high-order interactions. This is because extremely high-order interactions usually represent very complex and over-ftited patterns, which are unnecessary and unlikely to be learned by the DNN in real applications. ", "page_idx": 13}, {"type": "text", "text": "Condition 2. Let us consider the average network output $\\bar{\\boldsymbol{u}}^{(k)}\\ \\stackrel{\\mathrm{def}}{=}\\mathbb{E}_{|S|=k}[\\boldsymbol{v}(\\pmb{x}_{S})-\\boldsymbol{v}(\\pmb{x}_{\\emptyset})]$ over all masked samples $\\pmb{x}_{S}$ with $k$ unmasked input variables. This average network output monotonically increases when $k$ increases: $\\forall\\:k^{\\prime}\\leq k$ , we have $\\bar{u}^{(k^{\\prime})}\\leq\\bar{u}^{(k)}$ . ", "page_idx": 13}, {"type": "text", "text": "Condition 2 implies that a well-trained DNN is likely to have higher classification confidence for input samples that are less masked. ", "page_idx": 13}, {"type": "text", "text": "Condition 3. Given the average network output $\\bar{u}^{(k)}$ of samples with $k$ unmasked input variables, there is a polynomial lower bound for the average network output of samples with $k^{\\prime}(k^{\\prime}\\,\\leq\\,k)$ unmasked input variables: $\\forall\\,k^{\\prime}\\leq k$ , $\\begin{array}{r}{\\bar{u}^{(k^{\\prime})}\\geq(\\frac{k^{\\prime}}{k})^{p}\\;\\bar{u}^{(k)}}\\end{array}$ , where $p>0$ is a positive constant. ", "page_idx": 13}, {"type": "text", "text": "Condition 3 implies that the classification confidence of the DNN does not significantly degrade on masked input samples. The classification/detection of masked/occluded samples is common in real ", "page_idx": 13}, {"type": "text", "text": "scenarios. In this way, a well-trained DNN usually learns to classify a masked input sample based on local information (which can be extracted from unmasked parts of the input) and thus should not yield a significantly low confidence score on masked samples. ", "page_idx": 14}, {"type": "text", "text": "C Details of optimizing $\\{\\gamma_{T}\\}$ to extract the sparsest AND-OR interactions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A method is proposed [20, 4] to simultaneously extract AND interactions $I_{\\mathrm{and}}(S|x)$ and OR interactions $I_{\\mathrm{or}}(S|x)$ from the network output. Given a masked sample $x_{T}$ , [20] proposed to learn a decomposition $\\begin{array}{r}{{\\dot{v}}({\\dot{\\pmb x}}_{T})=v_{\\mathrm{and}}({\\pmb x}_{T})\\!+\\!v_{\\mathrm{or}}({\\pmb x}_{T})}\\end{array}$ towards the sparsest interactions. The component $v_{\\mathrm{and}}(x_{T})$ was explained by AND interactions, and the component $v_{\\mathrm{or}}(x_{T})$ was explained by OR interactions. Specifically, they decomposed $v(x_{T})$ into $v_{\\mathrm{and}}(\\pmb{x}_{T})=0.5\\;v(\\pmb{x}_{T})+\\gamma_{T}$ and $v_{\\mathrm{and}}(\\pmb{x}_{T})=0.5\\cdot v(\\pmb{x}_{T})-\\gamma_{T}$ , where $\\{\\dot{\\gamma}_{T}~:~T~\\stackrel{\\cdot}{\\subseteq}~N\\}$ is a set of learnable variables that determine the decomposition. In this way, the AND interactions and OR interactions can be computed according to Eq. (2), i.e., $\\begin{array}{r}{I_{\\mathrm{and}}(S|\\dot{\\mathbf{x}})=\\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{\\mathrm{and}}(\\mathbf{x}_{T})}\\end{array}$ , and $\\begin{array}{r}{I_{\\mathrm{or}}(S|\\pmb{x})=-\\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{\\mathrm{or}}(\\pmb{x}_{N\\backslash T})}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "The parameters $\\{\\gamma_{T}\\}$ were learned by minimizing the following LASSO-like loss to obtain sparse interactions: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\{\\gamma_{T}\\}}\\sum_{S\\subseteq N}|I_{\\mathrm{and}}(S|x)|+|I_{\\mathrm{or}}(S|x)|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Removing small noises. A small noise $\\delta_{S}$ in the network output may significantly affect the extracted interactions, especially for high-order interactions. Thus, [20] proposed to learn to remove a small noise term $\\delta_{S}$ from the computation of AND-OR interactions. Specifically, the decomposition was rewritten as $v_{\\mathrm{and}}(\\pmb{x}_{T})=0.5\\bar{(\\upsilon(\\pmb{x}_{T})-\\delta_{T})}+\\gamma_{T}$ and $v_{\\mathrm{or}}(\\pmb{x}_{T})=\\bar{0}.5(v(\\pmb{x}_{T})-\\delta_{T})+\\bar{\\gamma_{T}}$ . Thus, the parameters $\\{\\delta_{T}\\}$ , and $\\{\\gamma_{T}\\}$ are simultaneously learned by minimizing the loss function in Eq. (11). The values of $\\{\\delta_{T}\\}$ were constrained in $[-\\zeta,\\zeta]$ where $\\zeta=0.02\\cdot|v(\\pmb{x})-v(\\pmb{x}_{\\emptyset})|$ . ", "page_idx": 14}, {"type": "text", "text": "D Where does the coefficient $(-1)^{|S|-|T|}$ in Eq. (2) come from? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In fact, it is proven in [13] and [23] that the coefficient $(-1)^{|S|-|T|}$ in Eq. (2) is the unique coefficient to ensure that the interaction satisfies the universal matching property. Recall that the universal matching property means that no matter how we randomly mask an input sample $\\textbf{\\em x}$ , the network output on the masked sample $\\pmb{x}_{S}$ can always be accurately mimicked by the sum of interaction effects within $S$ . An extension of this property for AND-OR interactions is also mentioned in Theorem 2. ", "page_idx": 14}, {"type": "text", "text": "E OR interactions can be considered a special kind of AND interactions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The OR interaction can be considered a specific kind of AND interaction, when we flip the masked state and presence (unmasked) state of each input variable. ", "page_idx": 14}, {"type": "text", "text": "Given an input sample $\\pmb{x}\\in\\mathbb{R}^{n}$ , let $x_{T}$ denote the masked sample obtained by masking input variables in $N\\setminus T$ , while leaving variables in $T$ unchanged. Specifically, the baseline values $\\pmb{b}\\in\\mathbb{R}^{n}$ are used to mask the input variables, which represent the masked states of the input variables. The definition of ${\\mathbf{}}x_{T}$ is given as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n({\\pmb x}_{T})_{i}=\\left\\{\\!\\!\\begin{array}{l l}{x_{i},}&{i\\in T}\\\\ {b_{i},}&{i\\in N\\setminus T}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Based on the above definition, the AND interaction is computed as $\\begin{array}{r l}{I_{\\mathrm{and}}(S|x)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{T\\subset S}(-1)^{|S|-|T|}v_{\\mathrm{and}}\\left(\\pmb{x}_{T}\\right)}\\end{array}$ , while the OR interaction is computed as $\\begin{array}{r l}{I_{\\mathrm{or}}(S|x)}&{{}=}\\end{array}$ $\\begin{array}{r}{-\\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{\\mathrm{or}}\\left({\\pmb x}_{N\\setminus T}\\right)}\\end{array}$ . To simplify the analysis, let us assume $v_{\\mathrm{and}}(\\cdot)=v_{\\mathrm{or}}(\\cdot)=0.5v(\\cdot)$ . ", "page_idx": 14}, {"type": "text", "text": "Then, let us consider a masked sample $\\tilde{x}_{T}$ , where we filp the masked state and presence (unmasked) state of each input variable. In this way, $\\tilde{\\pmb{x}}_{T}$ is defined as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\tilde{\\pmb{x}}_{T})_{i}=\\left\\{\\begin{array}{l l}{x_{i},}&{i\\in N\\setminus T}\\\\ {b_{i},}&{i\\in T}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the OR interaction $I_{\\mathrm{or}}(S|x)$ in Eq. 2 in main paper can be represented as an AND interaction $I_{\\mathrm{or}}(S|\\tilde{\\pmb x})$ , as follows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\mathrm{or}}(S|\\pmb{x})=-\\displaystyle\\sum_{T\\subseteq S}(-1)^{|S|-|T|}v(\\pmb{x}_{N\\setminus T}),}\\\\ &{\\qquad\\qquad=-\\displaystyle\\sum_{T\\subseteq S}(-1)^{|S|-|T|}v(\\tilde{\\pmb{x}}_{T}),}\\\\ &{\\qquad=-I_{\\mathrm{and}}\\,(S|\\tilde{\\pmb{x}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In this way, the proof of the sparsity of AND interactions in [27] can also extend to OR interactions. Furthermore, we can simplify our analysis of the DNN\u2019s learning of interactions by only focusing on AND interactions. ", "page_idx": 15}, {"type": "text", "text": "F Proof of theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. (1) Universal matching theorem of AND interactions. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We will prove that output component $v_{\\mathrm{and}}(x_{S})$ on all $2^{n}$ masked samples $\\{x_{S}\\,:\\,S\\,\\subseteq\\,N\\}$ could be universally explained by the all interactions in $S~\\subseteq~N$ , i.e., $\\forall\\emptyset\\ \\neq\\ S\\ \\subseteq\\ N,v_{\\mathrm{and}}(\\pmb{x}_{S})\\ =$ $\\begin{array}{r}{\\sum_{\\emptyset\\neq T\\subseteq S}I_{\\mathrm{and}}(T|\\bar{\\mathbf{x}})+v(\\mathbf{x}_{\\emptyset})}\\end{array}$ . In particular, we define $v_{\\mathrm{and}}(\\mathbf{\\boldsymbol{x}}_{\\emptyset})=v(\\mathbf{\\boldsymbol{x}}_{\\emptyset})$ (i.e., we attribute output on an empty sample to AND interactions). ", "page_idx": 15}, {"type": "text", "text": "Specifically, the AND interaction is defined as $\\begin{array}{r}{I_{\\mathrm{and}}(T|\\pmb{x})=\\sum_{L\\subseteq T}(-1)^{|T|-|L|}v_{\\mathrm{and}}(\\pmb{x}_{L})}\\end{array}$ in 2. To compute the sum of AND interactions $\\begin{array}{r}{\\sum_{\\emptyset\\neq T\\subseteq S}I_{\\mathrm{and}}(T|x)=\\sum_{\\emptyset\\neq T\\subseteq S}\\sum_{L\\subseteq T}(-1)^{|T|-|L|}v_{\\mathrm{and}}(x_{L})}\\end{array}$ , we first exchange the order of summation of the set $L\\ \\subseteq\\ T\\ \\subseteq\\ {\\bar{S}}$ and the set $T\\supseteq L$ . That is, we compute all linear combinations of all sets $T$ containing $L$ with respect to the model outputs $v_{\\mathrm{and}}(\\boldsymbol{x}_{L})$ given a set of input variables $L$ , i.e., $\\begin{array}{r}{\\sum_{T:L\\subseteq T\\subseteq S}(-1)^{|T|-|L|}v_{\\mathrm{and}}(\\pmb{x}_{L})}\\end{array}$ . Then, we compute all summations over the set $L\\subseteq S$ . ", "page_idx": 15}, {"type": "text", "text": "In this way, we can compute them separately for different cases of $L\\subseteq T\\subseteq S$ . In the following, we consider the cases (1) $L=S=T$ , and (2) $L\\subseteq T\\subseteq S,L\\neq S$ , respectively. ", "page_idx": 15}, {"type": "text", "text": "(1) When $L=S=T$ , the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{\\mathrm{and}}(\\boldsymbol{x}_{L})$ is $(-1)^{|S|-|S|}v_{\\mathrm{and}}(\\mathbf{x}_{L})=v_{\\mathrm{and}}(\\mathbf{x}_{L}).$ . ", "page_idx": 15}, {"type": "text", "text": "(2) When $L\\subseteq T\\subseteq S,L\\neq S$ , the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{\\mathrm{and}}(\\boldsymbol{x}_{L})$ is $\\begin{array}{r}{\\sum_{T:L\\subseteq T\\subseteq S}(-1)^{|T|-|L|}v_{\\mathrm{and}}(\\pmb{x}_{L})}\\end{array}$ . For all sets $T:S\\supseteq T\\supseteq L$ , let us consider the linear combinations of all sets $T$ with number $|T|$ for the model output $v_{\\mathrm{and}}(\\boldsymbol{x}_{L})$ , respectively. Let $m:=|T|-|L|$ , $(0\\leq m\\leq|S|-|L|)$ , then there are a total of C|mS|\u2212|L| combinations of all sets $T$ of order $|T|$ . Thus, given $L$ , accumulating the model outputs $v_{\\mathrm{and}}(\\boldsymbol{x}_{L})$ corresponding to all $T\\supseteq L$ , then $\\begin{array}{r}{\\sum_{T:L\\subseteq T\\subseteq S}(-1)^{|T|-|L|}v_{\\mathrm{and}}(x_{L})=v_{\\mathrm{and}}(x_{L})\\cdot\\sum_{m=0}^{|S|-|L|}C_{|S|-|L|}^{m}(-1)^{m}=0.}\\end{array}$ = 0 ", "page_idx": 15}, {"type": "text", "text": "Please see the complete derivation of the following formula. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\sum_{\\emptyset\\neq T\\subsetneq S}I_{\\mathrm{and}}(T|x)}\\\\ &{=\\sum_{\\emptyset\\neq T\\subsetneq S}\\sum_{L\\subseteq T}(-1)^{|T|-|L|}v_{\\mathrm{and}}(x_{L})}\\\\ &{=\\sum_{L\\subseteq S}\\sum_{T:L\\subseteq T\\subsetneq S}(-1)^{|T|-|L|}v_{\\mathrm{and}}(x_{L})-v_{\\mathrm{and}}(x_{\\emptyset})}\\\\ &{=\\underbrace{v_{\\mathrm{and}}(x_{S})}_{L=S}+\\sum_{L\\subseteq S,L\\not=S}v_{\\mathrm{and}}(x_{L})\\cdot\\underbrace{\\sum_{m=0}^{|S|-|L|}C_{|S|-|L|}^{m}(-1)^{m}}_{=0}-v_{\\mathrm{and}}(x_{\\emptyset})}\\\\ &{=v_{\\mathrm{and}}(x_{S})-v_{\\mathrm{and}}(x_{\\emptyset})}\\\\ &{=v_{\\mathrm{and}}(x_{S})-v(x_{\\emptyset})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we have $\\begin{array}{r}{\\forall\\emptyset\\neq S\\subseteq N,v_{\\mathrm{and}}(\\mathbf{\\boldsymbol{x}}_{S})=\\sum_{\\emptyset\\neq T\\subseteq S}I_{\\mathrm{and}}(T|\\mathbf{\\boldsymbol{x}})+v(\\mathbf{\\boldsymbol{x}}_{\\emptyset}).}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "(2) Universal matching theorem of OR interactions. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "According to the definition of OR interactions, we will derive that $\\forall S\\ \\ \\subseteq\\ \\ N,v_{\\mathrm{or}}(\\pmb{x}_{S})\\ \\ =$ $\\textstyle\\sum_{T:T\\cap S\\neq\\emptyset}I_{\\mathrm{or}}(S|\\pmb{x})$ , where we define $v_{\\mathrm{or}}(\\mathbf{\\boldsymbol{x}}_{\\emptyset})\\,=\\,0$ (recall that in Step (1), we attribute the output on empty input to AND interactions). ", "page_idx": 16}, {"type": "text", "text": "Specifically, the OR interaction is defined as $\\begin{array}{r}{I_{\\mathrm{or}}(T|\\pmb{x})~=~-\\sum_{L\\subseteq T}(-1)^{|T|-|L|}v_{\\mathrm{or}}(\\pmb{x}_{N\\setminus L})}\\end{array}$ in 2. Similar to the above derivation of the universal matching theorem of AND interactions, to compute the sum of OR interactions $\\begin{array}{r}{\\sum_{T:T\\cap S\\neq\\emptyset}I_{\\mathrm{or}}(T|x)=\\sum_{T:T\\cap S\\neq\\emptyset}^{-}\\biggl[-\\sum_{L\\subseteq T}(-1)^{|T|-|L|}v_{\\mathrm{or}}(x_{N\\setminus L})\\biggr],}\\end{array}$ we first exchange the order of summation of the set $L\\subseteq T\\subseteq N$ and the set $T:T\\cap S\\neq\\emptyset$ . That is, we compute all linear combinations of all sets $T$ containing $L$ with respect to the model outputs $v_{\\mathrm{or}}(x_{N\\setminus L})$ given a set of input variables $L$ , i.e., $\\begin{array}{r}{\\sum_{T:T\\cap S\\neq\\emptyset,T\\supseteq L}(-1)^{|T|-|L|}v_{\\mathrm{or}}({\\pmb x}_{N\\setminus L})}\\end{array}$ . Then, we compute all summations over the set $L\\subseteq N$ . ", "page_idx": 16}, {"type": "text", "text": "In this way, we can compute them separately for different cases of $L\\subseteq T\\subseteq N,T\\cap S\\neq\\emptyset$ . In the following, we consider the cases (1) $L=N\\setminus S$ , (2) $L=N$ , (3) $L\\cap S\\neq\\emptyset,L\\neq N$ , and (4) $L\\cap S=\\emptyset,\\bar{L}\\neq N\\setminus S$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "(1) When $L=N\\setminus S$ , the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{\\mathrm{or}}(x_{N\\setminus L})$ is $\\begin{array}{r}{\\sum_{T:T\\cap S\\neq\\emptyset,T\\supset L}(-1)^{|T|-|L|}v_{\\mathrm{or}}(x_{N\\setminus L})=\\sum_{T:T\\cap S\\neq\\emptyset,T\\supset L}(-1)^{|T|-|L|}v_{\\mathrm{or}}(x_{S})}\\end{array}$ For all sets $T\\ :\\ T\\ \\supseteq\\ L,T\\cap S\\ \\neq\\ \\emptyset$ (then $T\\ \\ne\\ N\\ \\backslash\\ S,T\\ \\ne\\ L)$ , let us consider the linear combinations of all sets $T$ with number $|T|$ for the model output $v_{\\mathrm{or}}(x_{S})$ , respectively. Let |T \u2032| := |T| \u2212|L|, (1 \u2264|T \u2032| \u2264|S|), then there are a total of C||ST |\u2032| combinations of all sets $T^{\\prime}$ of order $\\left|T^{\\prime}\\right|$ . Thus, given $L$ , accumulating the model outputs $v_{\\mathrm{or}}(x_{S})$ corresponding to all $T\\supseteq L$ , then $\\begin{array}{r}{\\sum_{T:T\\cap S\\neq\\emptyset,T\\supset L}(-1)^{|T|-|L|}v_{\\mathrm{or}}({\\pmb x}_{N\\setminus L})=v_{\\mathrm{or}}({\\pmb x}_{S})\\cdot\\underbrace{\\overset{\\cdot}{\\sum}_{|T^{\\prime}|=1}^{|S|}C_{|S|}^{|T^{\\prime}|}(-1)^{|T^{\\prime}|}}_{=-1}=-v_{\\mathrm{or}}({\\pmb x}_{S}).}\\end{array}$ (2) When $L=N$ (then $T=N)$ ), the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{\\mathrm{or}}(x_{N\\setminus L})$ is $\\begin{array}{r}{\\sum_{T:T\\cap S\\neq\\emptyset,T\\supset L}(-1)^{|T|-|L|}v_{\\mathrm{or}}(\\pmb{x}_{N\\setminus L})=(-1)^{|\\bar{N}|-|N|}v_{\\mathrm{or}}(\\pmb{x}_{\\emptyset}^{\\prime})=}\\end{array}$ $v_{\\mathrm{or}}(x_{\\emptyset})$ . ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "(3) When $L\\cap S\\ \\neq\\ \\emptyset,L\\ \\neq\\ N$ , the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{\\mathrm{or}}(x_{N\\setminus L})$ is $\\begin{array}{r}{\\sum_{T:T\\cap S\\not=\\emptyset,T\\supseteq L}(-1)^{|T|-|L|}v_{\\mathrm{or}}(x_{N\\setminus L})}\\end{array}$ . For all sets $T\\ :\\ T\\ \\supseteq\\ L,T\\cap S\\ \\neq\\ \\emptyset$ , let us consider the linear combinations of all sets $T$ with number $|T|$ for the model output $v_{\\mathrm{or}}(x_{S})$ , respectively. Let us split $|T|-|L|$ into $\\left|T^{\\prime}\\right|$ and $|T^{\\prime\\prime}|$ , $i.e.,\\stackrel{.}{|T|}-|L|=|T^{\\prime}|+|T^{\\prime\\prime}|,$ , where $T^{\\prime}=\\{i|\\bar{i}\\in T,i\\notin L,i\\in N\\,\\backslash\\,S\\}$ , $T^{\\prime\\prime}=\\{i|i\\in T,i\\notin L,i\\in S\\}$ (then $0\\,\\leq\\,|T^{\\prime\\prime}|\\,\\leq\\,|S|-|S\\cap L|)$ and $|T^{\\prime}|+|T^{\\prime\\prime}|+|L|\\,=\\,|T|$ . In this way, there are a total of $C_{|S|-|S\\cap L|}^{|T^{\\prime\\prime}|}$ $T^{\\prime\\prime}$ $|T^{\\prime\\prime}|$ $L$   \noutputs $v_{\\mathrm{or}}(x_{N\\setminus L})$ corresponding to all $T\\ \\supseteq\\ L$ , then $\\begin{array}{r l r}{\\sum_{T:T\\cap S\\neq\\emptyset,T\\supset L}(-1)^{|T|-|L|}v_{\\mathrm{or}}(\\pmb{x}_{N\\setminus L})\\!\\!\\!\\!}&{{}=}&{\\!\\!\\!\\!\\!}\\end{array}$ $\\begin{array}{r}{v_{\\mathrm{or}}({\\pmb x}_{N\\setminus L})\\cdot\\sum_{T^{\\prime}\\subseteq N\\setminus S\\setminus L}\\underbrace{\\sum_{|T^{\\prime\\prime}|=0}^{|S|-|S\\cap L|}C_{|S|-|S\\cap L|}^{|T^{\\prime\\prime}|}(-1)^{|T^{\\prime}|+|T^{\\prime\\prime}|}}_{=0}=0.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "(4) When $L\\cap S=\\emptyset,L\\neq N\\setminus S$ , the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{\\mathrm{or}}(x_{N\\setminus L})$ is $\\begin{array}{r}{\\sum_{T:T\\cap S\\not=\\emptyset,T\\supseteq L}(-1)^{|T|-|L|}v_{\\mathrm{or}}({\\pmb x}_{N\\setminus L})}\\end{array}$ . Similarly, let us split $|T|-|L|$ into $\\left|T^{\\prime}\\right|$ and $|T^{\\prime\\prime}|,i.e.,|T|-|L|=|T^{\\prime}|^{-}\\!+|T^{\\prime\\prime}|$ , where $T^{\\prime}=\\{i|i\\in T,i\\notin L,i\\in N\\setminus S\\}$ , $T^{\\prime\\prime}=\\{i|i\\in T,i\\in S\\}$ (then $0\\leq|T^{\\prime\\prime}|\\leq|S|)$ and $|T^{\\prime}|+|T^{\\prime\\prime}|+|L|=|T|$ . In this way, there are a total of $C_{|S|}^{|T^{\\prime\\prime}|}$ combinations of all sets $T^{\\prime\\prime}$ of order $\\left|T^{\\prime\\prime}\\right|$ . Thus, given $L$ , accumulating the model outputs $v_{\\mathrm{or}}(x_{N\\setminus L})$ corresponding to all $T\\ \\supseteq\\ L$ , then $\\begin{array}{r l r}{\\sum_{T:T\\cap S\\neq\\emptyset,T\\supset L}(-1)^{|T|-|L|}v_{\\mathrm{or}}(\\pmb{x}_{N\\setminus L})\\!\\!\\!\\!}&{{}=}&{\\!\\!\\!\\!\\!}\\end{array}$ $\\begin{array}{r}{v_{\\mathrm{or}}(\\pmb{x}_{N\\setminus L})\\cdot\\sum_{T^{\\prime}\\subseteq N\\setminus S\\setminus L}\\underbrace{\\sum_{|T^{\\prime\\prime}|=0}^{|S|}C_{|S|}^{|T^{\\prime\\prime}|}(-1)^{|T^{\\prime}|+|T^{\\prime\\prime}|}}_{=0}=0.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Please see the complete derivation of the following formula. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{T\\in T\\times S\\times\\mu}E_{T}(\\Gamma_{S})=\\sum_{T\\in T\\times S^{\\prime}}\\left[-\\sum_{k\\geq\\nu_{T}\\in T}(-1)^{\\nu_{T}(T)-1}e^{-\\nu_{T}(T)}\\right]}\\\\ {=}&{-\\sum_{k\\geq\\nu_{T}\\in T}\\sum_{T\\cap T\\times S^{\\prime}}x_{k\\geq\\nu_{T}}(-1)^{\\nu_{T}(T)-1}e^{-\\nu_{T}(T)}}\\\\ {=}&{-\\left[\\frac{1}{\\nu\\geq\\nu_{T}}C_{\\nu_{T}}^{(\\nu_{T})}(1\\!\\!-\\!1)^{\\nu_{T}}\\right]\\frac{1}{\\sqrt{T-\\nu_{T}}}\\frac{1}{\\sqrt{\\nu_{T}}\\times T}\\frac{1}{\\sqrt{\\nu_{T}}}\\frac{1}{\\sqrt{\\nu_{T}}}}\\\\ &{-\\underbrace{\\sum_{k\\geq\\nu_{T}\\in T}\\left[\\sum_{\\nu_{T}\\in T}\\sum_{\\nu_{T}\\in T}\\left(\\frac{\\sum_{\\nu_{T}\\in T}\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\nu_{T}(T)}{\\nu_{T}\\int\\nu_{T}\\int_{T}\\nu_{T}}\\right)\\right]}_{\\geq0}\\cdot\\operatorname{suc}(x_{T}(x_{T}))}\\\\ &{-\\underbrace{\\sum_{k\\geq\\nu_{T}\\in T}\\left[\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\left(\\frac{\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\nu_{T}(T)}{\\nu_{T}\\int_{T}\\nu_{T}}\\right)\\right]}_{\\geq0}\\cdot\\operatorname{suc}(x_{T}(x_{T}))}\\\\ {=}&{-(-1)\\cdot\\operatorname{suc}(x_{T})-u_{T}(x_{T})}\\\\ &{-\\underbrace{\\sum_{k\\geq\\nu_{T}\\in T}\\sum_{\\nu_{t}\\in T}\\left[\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\right]}_{\\geq0}\\cdot\\operatorname{suc}(x_{T}(x_{T}))}\\\\ &{-\\underbrace{\\sum_{k\\geq\\nu_{T}\\in T}\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\sum_{\\nu_{t}\\in T}\\sum_{\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(3) Universal matching theorem of AND-OR interactions. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "With the universal matching theorem of AND interactions and the universal matching theorem of OR interactions, we can easily get $\\begin{array}{r}{v(\\pmb{x}_{S})=v_{\\mathrm{and}}(\\pmb{x}_{S})+v_{\\mathrm{or}}(\\pmb{x}_{S})=v(\\pmb{x}_{\\emptyset})+\\sum_{\\emptyset\\neq T\\subseteq S}I_{\\mathrm{and}}(T|\\pmb{x})+}\\end{array}$ $\\scriptstyle\\sum_{T:T\\cap S\\not=\\emptyset}I_{\\mathrm{or}}(T|x)$ , thus, we obtain the universal matching theorem of AND-OR interactions. ", "page_idx": 17}, {"type": "text", "text": "F.2 Proof of Eq. (6) and Eq. (7) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Before we give the derivation of Eq. (6) and Eq. (7), we first prove the following lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. The effect $I(T|x)$ of an AND interaction w.r.t. subset $T$ on sample $\\textbf{\\em x}$ can be rewritten as ", "page_idx": 17}, {"type": "equation", "text": "$$\nI(T|x)=\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\left.\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdot\\cdot\\cdot\\partial x_{n}^{\\pi_{n}}}\\right|_{x=x_{\\theta}}\\prod_{i\\in T}(x_{i}-b_{i})^{\\pi_{i}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$Q_{T}=\\{[\\pi_{1},\\ldots,\\pi_{n}]^{\\top}\\;|\\;\\forall\\;i\\in T,\\pi_{i}\\in\\mathbb{N}^{+};\\forall\\;i\\notin T,\\pi_{i}=0\\}.$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that a similar proof was first introduced in [26]. ", "page_idx": 17}, {"type": "text", "text": "Proof. Let us denote the function on the right of Eq. (19) by $K(T|x)$ , i.e., for $S\\ne\\emptyset$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nK(T|x)\\stackrel{\\mathrm{def}}{=}\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\left.\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\right|_{x=x_{0}}\\prod_{i\\in T}(x_{i}-b_{i})^{\\pi_{i}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Actually, it has been proven in [13] and [23] that the AND interaction $I(T|x)$ (see definition in Eq. (2)) is the unique metric satisfying the following property (an extension of the property for AND-OR interactions is mentioned in Theorem 2), i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall\\,S\\subseteq N,\\,v(\\pmb{x}_{S})=\\sum_{\\emptyset\\neq T\\subseteq S}I(T|\\pmb{x})+v(\\pmb{x}_{\\emptyset}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, as long as we can prove that $K(T|x)$ also satisfies the above universal matching property, we can obtain $\\bar{I(T|\\pmb{x})}=K\\bar{(T|\\pmb{x})}$ . ", "page_idx": 17}, {"type": "text", "text": "To this end, we only need to prove $K(T|x)$ also satisfies the property in Eq. (21). Specifically, given an input sample $\\pmb{x}\\in\\mathbb{R}^{n}$ , let us consider the Taylor expansion of the network output $v(x_{S})$ of an arbitrarily masked sample $_{x_{s}}$ , which is expanded at $\\pmb{x}_{\\varnothing}=\\pmb{b}=[b_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}b_{n}]^{\\intercal}$ . Then, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall\\,S\\subseteq N,\\;v(x_{S})=\\sum_{\\pi_{1}=0}^{\\infty}\\cdot\\cdot\\cdot\\sum_{\\pi_{n}=0}^{\\infty}{\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}}\\left.{\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdot\\cdot\\cdot\\partial x_{n}^{\\pi_{n}}}}\\right|_{x=x_{0}}\\prod_{i=1}^{n}((x_{S})_{i}-b_{i})^{\\pi_{i}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $b_{i}$ denotes the baseline value to mask the input variable $x_{i}$ . ", "page_idx": 18}, {"type": "text", "text": "According to the definition of the masked sample $\\pmb{x}_{S}$ , we have that all variables in $S$ keep unchanged and other variables are masked to the baseline value. That is, $\\forall\\;i\\;\\in\\;S,\\;({\\pmb x}_{S})_{i}\\;=\\;\\bar{x}_{i};\\;\\forall\\;i\\;\\notin\\;S$ , $(x_{S})_{i}\\,=\\,b_{i}$ . Hence, we obtain $\\forall i\\,\\,\\notin\\,{\\cal S},\\,\\,(({\\pmb x}_{S})_{i}\\,-\\,b_{i})^{\\pi_{i}}\\,=\\,0$ if $\\pi_{i}\\,>\\,0$ . Then, among all Taylor expansion terms, only terms corresponding to degrees $\\pi$ in the set $P_{S}\\,=\\,\\{[\\pi_{1},\\cdot\\cdot\\cdot\\,,\\pi_{n}]^{\\intercal}\\enspace|\\enspace\\forall i\\enspace\\in$ $S,\\pi_{i}\\in\\mathbb{N};\\forall i\\notin S,\\pi_{i}=0\\}$ may not be zero (we consider the value of $((\\pmb{x}_{S})_{i}-b_{i})^{\\pi_{i}}$ to be always equal to 1 if $\\pi_{i}=0$ ). Therefore, Eq. (22) can be re-written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall\\,S\\subseteq N,\\quad v(\\pmb{x}_{S})=\\sum_{\\pi\\in P_{S}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\left.\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\right|_{\\substack{x=\\pmb{x}_{\\theta}\\,i\\in S}}\\prod_{i\\in S}(x_{i}-b_{i})^{\\pi_{i}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We find that the set $P_{S}$ can be divided into multiple disjoint sets as $P_{S}\\;=\\;\\cup_{T\\subseteq S}\\;Q_{T}$ , where $Q_{T}=\\{[\\pi_{1},\\cdot\\cdot\\cdot~,\\pi_{n}]^{\\top}~|~\\forall i\\in T,\\pi_{i}\\in\\mathbb{N}^{+};\\forall i\\notin T,\\pi_{i}=0\\}$ . Then, we can further write Eq. (23) as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall\\,S\\subseteq N,\\quad v(\\mathbf{x}_{S})=\\sum_{T\\subseteq S}\\sum_{\\pi\\in Q_{T}}{\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}}\\left.{\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdot\\cdots\\partial x_{n}^{\\pi_{n}}}}\\right|_{\\mathbf{x}=x_{\\emptyset}}\\prod_{i\\in T}(x_{i}-b_{i})^{\\pi_{i}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last step is obtained as follows. When $T=\\emptyset$ , $Q_{T}$ only has one element $\\pmb{\\pi}=[0,\\cdots\\,,0]^{\\top}$ , which corresponds to the term $v(\\pmb{x}_{\\emptyset})$ . ", "page_idx": 18}, {"type": "text", "text": "Thus, $K(T|x)$ satisfies the property in Eq. (21), and this means $I(T|x)\\;\\;=\\;\\;K(T|x)\\;\\;=$ $\\begin{array}{r}{\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\left.\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\right|_{x=x_{\\emptyset}}\\prod_{i\\in T}(x_{i}-b_{i})^{\\pi_{i}}.}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Then, let us continue the proof of Eq. (6) and Eq. (7). ", "page_idx": 18}, {"type": "text", "text": "Proof. Given a specific sample $\\hat{\\pmb{x}}$ , let us consider the following function defined in Eq. (6) and Eq. (7). ", "page_idx": 18}, {"type": "equation", "text": "$$\nf({\\pmb x})=\\sum_{T\\subseteq N}w_{T}\\ J_{T}({\\pmb x}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the scalar weight $\\begin{array}{r l r}{w_{T}}&{{}=}&{I(T|\\pmb{x}_{}}&{{}=}&{\\hat{\\pmb{x}})}\\end{array}$ , and the function $\\begin{array}{r l r}{J_{T}(\\pmb{x})}&{{}=}&{}\\end{array}$ $\\begin{array}{r}{\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\left.\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\right|_{x=x_{\\emptyset}}\\prod_{i\\in T}(x_{i}-b_{i})^{\\pi_{i}}/w_{T}.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "We will then prove that $\\forall S\\subseteq N,\\ f({\\hat{\\mathbf{x}}}_{S})=v({\\hat{\\mathbf{x}}}_{S})$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{x}s)=\\displaystyle\\sum_{T\\subseteq N}w_{T}\\,J_{T}(\\hat{x}s)}\\\\ &{\\qquad=\\displaystyle\\sum_{T\\subseteq N}\\displaystyle\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\,\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots+\\partial x_{n}^{\\pi_{n}}}\\bigg\\vert_{x=x_{0}}\\prod_{i\\in T}((\\hat{x}s)_{i}-b_{i})^{\\pi_{i}}\\,\\,//\\,\\,w_{T}\\,\\mathrm{cancels~out}}\\\\ &{\\qquad=\\displaystyle\\sum_{T\\subseteq S}\\displaystyle\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\,\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\bigg\\vert_{x=x_{0}}\\prod_{i\\in T}((\\hat{x}s)_{i}-b_{i})^{\\pi_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n=\\sum_{T\\subseteq S}\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\left.\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\right|_{x=x_{\\theta}}\\prod_{i\\in T}(\\hat{x}_{i}-b_{i})^{\\pi_{i}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$//$ when $T\\subseteq S$ , we have $\\forall i\\in T,(\\hat{\\pmb{x}}_{S})_{i}=\\hat{x}_{i}$ (3 $=\\sum_{\\varnothing\\neq T\\subseteq S}I(T|x=\\hat{x})+v(x_{\\varnothing})$ // the inverse direction of Lemma 3 we have just proven ", "page_idx": 19}, {"type": "text", "text": "Remark. The function $f(x)$ essentially provides a continuous implementation of Eq. (3) in the universal matching theorem (Theorem 2). The weight $w_{T}=I(T|\\bar{\\pmb{x}}=\\hat{\\pmb{x}})$ is the interaction effect $w.r t$ to subset $T$ on the unmasked sample $\\hat{\\pmb x}$ , while the function $J_{T}({\\pmb x})$ is a continuous extension of the indicator function $\\mathbb{1}(\\hat{\\pmb{x}}_{S}$ triggers the AND relation $T$ ) (thus we call $J_{T}(x)$ a triggering function and the value of this function triggering strength). ", "page_idx": 19}, {"type": "text", "text": "F.3 Proof of Lemma 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Given the inference scores on masked samples $\\{\\widetilde{v}(\\mathbf{x}_{S}):S\\subseteq N\\}$ , the interaction between input variables w.r.t. $T\\subseteq N$ can be computed as $\\begin{array}{r}{\\widetilde I(T|\\pmb{x})=\\sum_{\\pmb{S}\\subseteq T}(-1)^{|T|-|\\cal S|}\\,\\widetilde v(\\pmb{x}_{S})}\\end{array}$ (the computation of AND interactions in Eq. (2)). ", "page_idx": 19}, {"type": "text", "text": "Since we assume that $\\forall S\\subseteq N,\\widetilde{v}(\\pmb{x}_{S})=v(\\pmb{x}_{S})+\\Delta v_{S},\\Delta v_{S}\\sim\\mathcal{N}(0,\\sigma^{2}),\\widetilde{I}(T|\\pmb{x})$ can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widetilde{I}(T|x)=\\sum_{S\\subseteq T}(-1)^{|T|-|S|}\\,\\widetilde{v}(x_{S})}\\\\ &{\\quad\\quad=\\sum_{S\\subseteq T}(-1)^{|T|-|S|}\\,\\left(v(x_{S})+\\Delta v_{S}\\right)}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{S\\subseteq T}(-1)^{|T|-|S|}\\,\\,v(x_{S})+\\sum_{S\\subseteq T}(-1)^{|T|-|S|}\\Delta v_{S}}\\\\ &{\\quad\\quad=I(T|x)+\\Delta I_{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{I(T|\\pmb{x})=\\sum_{S\\subseteq T}(-1)^{|T|-|S|}v(\\pmb{x}_{S})}\\end{array}$ is a noiseless component (not a random variable), and $\\begin{array}{r}{\\Delta I_{T}=\\sum_{S\\subseteq T}(-1)^{|T|-|S|}\\Delta v_{S}}\\end{array}$ is the noise component on the interaction. ", "page_idx": 19}, {"type": "text", "text": "Since each Gaussian noise $\\Delta v_{S}\\sim\\mathcal{N}(0,\\sigma^{2}),\\forall S\\subseteq N,$ is independent and identically distributed, it is easy to see $\\begin{array}{r}{\\mathbb{E}[\\Delta I_{T}]=\\sum_{S\\subseteq T}(-1)^{|T|-|S|}\\mathbb{E}[\\Delta v_{S}]=0}\\end{array}$ . The variance of $\\Delta I_{T}$ is computed as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}[\\Delta I_{T}]=\\mathrm{Var}(\\displaystyle\\sum_{S\\subseteq T}(-1)^{|T|-|S|}\\Delta v_{S})}\\\\ &{\\qquad\\qquad=\\mathrm{Var}(\\Delta v_{S_{1}})+\\mathrm{Var}(\\Delta v_{S_{2}})+\\cdot\\cdot\\cdot+\\mathrm{Var}(\\Delta v_{S_{2}|T|})}\\\\ &{\\qquad\\qquad=2^{|T|}\\cdot\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "because there are a total of $2^{|T|}$ subsets for $S\\subseteq T$ . ", "page_idx": 19}, {"type": "text", "text": "Furthermore, according to the analytic form of interaction effect in Eq. (19), we note that the values of $\\widetilde I(T|x)$ and $\\widetilde{J}_{T}(\\pmb{x})$ have a ratio of $w_{T}$ . Therefore, if we write $\\tilde{J_{T}}({\\pmb x})\\stackrel{\\sim}{=}J_{T}({\\pmb x})+\\epsilon_{T}$ , then the noise term satisfies $\\epsilon_{T}=\\Delta I_{T}/w_{T}$ , and thus $\\mathbb{E}[\\epsilon_{T}]=0,\\mathrm{Var}[\\epsilon_{T}]\\propto2^{|T|}\\sigma^{2}$ . ", "page_idx": 20}, {"type": "text", "text": "F.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. We concatenate all $J({\\bf{\\sigma}}_{{\\bf{\\sigma}}_{S})}$ (w.r.t. all $2^{n}$ masked samples $\\pmb{x}_{S}$ , $S\\ \\subseteq\\ N)$ into a matrix $J\\,=\\,[J(x_{S_{1}}),J(x_{S_{2}}),\\cdot\\cdot\\cdot\\,,J(x_{S_{2^{n}}})]^{\\top}\\,\\in\\,\\{0,1\\}^{2^{n}\\times2^{n}}$ to represent the triggering strength of $2^{n}$ interactions on $2^{n}$ masked samples We also concatenate all noise terms on all $2^{n}$ masked samples into a matrix $\\pmb{\\mathcal{E}}=[\\epsilon^{(1)},\\epsilon^{(2)},\\bar{\\epsilon}\\bar{\\cdot}\\bar{\\cdot}\\;,\\epsilon^{(2^{n})}]^{\\top}$ to represent the noise term over $_{J}$ . We concatenate the output score vector $y\\ {\\stackrel{\\mathrm{def}}{=}}\\ [y(x_{S_{1}}),y(x_{S_{2}}),\\cdot\\cdot\\cdot\\ ,y(x_{S_{2^{n}}})]^{\\top}\\in\\mathbb{R}^{2^{n}}$ to represent the finally converged outputs on all $2^{n}$ masked samples. ", "page_idx": 20}, {"type": "text", "text": "The optimal weights $\\hat{w}$ can be solved by minimizing the loss function $\\widetilde L(\\boldsymbol{w})$ in Eq. (9). The loss function can be rewritten as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{w}=\\underset{\\omega}{\\arg\\operatorname*{min}}\\,\\tilde{L}(w)}\\\\ &{\\tilde{L}(w)=\\mathbb{E}_{\\epsilon}\\mathbb{E}_{S\\subseteq N}\\left[\\left(y_{S}-w^{\\top}(J(x_{S})+\\epsilon)\\right)^{2}\\right],}\\\\ &{\\,\\,\\,=\\mathbb{E}_{\\varepsilon}\\left[\\frac{1}{2^{n}}\\|y-(J+\\varepsilon)w\\|_{2}^{2}\\right],}\\\\ &{\\,\\,\\,=\\frac{1}{2^{n}}\\mathbb{E}_{\\varepsilon}\\left[(y-(J+\\varepsilon)w)^{\\top}(y-(J+\\varepsilon)w)\\right],}\\\\ &{\\,\\,\\,=\\frac{1}{2^{n}}\\left(y^{\\top}y-2y^{\\top}\\mathbb{E}_{\\varepsilon}\\left[(J+\\varepsilon)\\right]w+w^{\\top}\\mathbb{E}_{\\varepsilon}\\left[(J+\\varepsilon)^{\\top}(J+\\varepsilon)\\right]w\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking the derivative with respect to $\\pmb{w}$ and setting it to zero, we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\widetilde{L}}{\\partial w}=-2\\mathbb{E}_{\\mathcal{E}}\\left[(J+\\mathcal{E})^{\\top}\\pmb{y}\\right]+2\\mathbb{E}_{\\mathcal{E}}\\left[(J+\\mathcal{E})^{\\top}(J+\\mathcal{E})w\\right]=0,}\\\\ &{\\Rightarrow\\mathbb{E}_{\\mathcal{E}}\\left[(J+\\mathcal{E})^{\\top}(J+\\mathcal{E})\\right]w=\\mathbb{E}_{\\mathcal{E}}\\left[(J+\\mathcal{E})^{\\top}\\pmb{y}\\right],}\\\\ &{\\Rightarrow(J^{\\top}J+\\mathbb{E}_{\\mathcal{E}}[\\mathcal{E}^{\\top}J]+J^{\\top}\\mathbb{E}_{\\mathcal{E}}[\\mathcal{E}]+\\mathbb{E}_{\\mathcal{E}}[\\mathcal{E}^{\\top}\\mathcal{E}])w=J^{\\top}\\pmb{y},}\\\\ &{\\Rightarrow(J^{\\top}J+\\mathbb{E}_{\\mathcal{E}}[\\mathcal{E}^{\\top}\\mathcal{E}])w=J^{\\top}\\pmb{y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that the sample covariance matrix $\\begin{array}{r}{\\frac{1}{m}\\pmb{\\mathcal{E}}^{\\top}\\pmb{\\mathcal{E}}}\\end{array}$ converges to the true covariance matrix $\\operatorname{Cov}(\\pmb{\\mathscr{E}})$ , when $m=2^{n}$ is large. Therefore, $\\mathbb{E}_{\\pmb\\varepsilon}[\\pmb{\\mathscr{E}}^{\\top}\\pmb{\\mathscr{E}}])=\\mathbb{E}_{\\pmb\\varepsilon}[2^{n}\\mathrm{Cov}(\\pmb{\\mathscr{E}})])=2^{n}\\mathrm{Cov}(\\pmb{\\mathscr{E}})$ . Because we assume noises on different interactions are independent, it is a diagonal matrix, denoted by $\\operatorname{Cov}(\\pmb{\\mathscr{E}})=\\mathrm{diag}(\\pmb{c})$ , where $c\\,=\\,\\mathrm{vec}(\\{\\mathrm{Var}[\\epsilon_{T}]\\,:\\,T\\subseteq N\\})\\,=\\,\\mathrm{vec}(\\{2^{|T|}\\sigma^{2}\\,:\\,T\\subseteq N\\})\\,\\in\\,\\mathbb{R}^{2^{n}}$ denotes the vector of variances of the triggering strength of $2^{n}$ interactions. ", "page_idx": 20}, {"type": "text", "text": "Thus, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(J^{\\top}J+2^{n}\\mathrm{diag}(c))w=J^{\\top}y.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we can prove that the matrix $J^{\\top}J+2^{n}\\mathrm{diag}(c)$ is always invertible, as follows. (1) We can prove that $J^{\\top}J$ is positive semi-definite, because $\\forall\\dot{\\boldsymbol{u}}\\neq\\mathbf{0},\\dot{\\boldsymbol{u}}^{\\top}\\boldsymbol{J}^{\\top}\\boldsymbol{J}\\boldsymbol{u}=\\|\\boldsymbol{J}\\boldsymbol{u}\\|_{2}^{2}\\geq0.$ . (2) We can further prove that $J^{\\top}J$ is positive definite. Let us denote the eigenvalues of $J^{\\top}J$ as $\\lambda_{1},\\cdot\\cdot\\cdot\\,,\\lambda_{2^{n}}\\in\\mathbb{R}$ (because $J^{\\top}J$ is real symmetric, its eigenvalues must be real). Note that the diagonal elements of $J^{\\top}J$ are all positive, so we have $\\begin{array}{r}{\\bar{\\prod}_{i=1}^{2^{n}}\\lambda_{i}\\;=\\;\\prod_{i=1}^{2^{n}}(J^{\\top}J)_{i i}\\;>\\;0}\\end{array}$ . Combining the positive semi-definiteness, we know that the eigenvalues of $J^{\\top}J$ must be all positive, without having a zero eigenvalue. It means that $J^{\\top}J$ is positive definite. (3) We can prove that $J^{\\top}J+2^{n}\\mathrm{diag}\\breve{(c)}$ is positive definite. The diagonal matrix $2^{n}\\mathrm{diag}(c)$ is positive definite, because all its diagonal elements are positive. The sum of two positive definite matrices is still positive definite. (4) Since $J^{\\top}J+2^{n}\\mathrm{diag}(c)$ is positive definite, it cannot have a zero eigenvalue, and is thus invertible. ", "page_idx": 20}, {"type": "text", "text": "So the optimal weights can be solved as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{\\hat{w}}=(\\pmb{J}^{\\top}\\pmb{J}+2^{n}\\mathrm{diag}(\\pmb{c}))^{-1}\\pmb{J}^{\\top}\\pmb{y}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next we will show that $\\boldsymbol{y}=\\boldsymbol{J}^{\\top}\\boldsymbol{w}^{*}$ . Recall that definition of $y(x_{S})$ is given by $y(\\pmb{x}_{S})=v(\\pmb{x}_{\\emptyset})+$ $\\sum_{\\emptyset\\neq T\\subseteq S}w_{T}^{*}$ in the main paper. According to the Lemma 2, we have $J_{T}(\\pmb{x})=\\mathbb{1}(T\\subseteq S)$ . Therefore, $y(x_{S})$ can be rewritten as $\\begin{array}{r}{y(\\pmb{x}_{S})=\\sum_{T\\subseteq N}J_{T}(\\pmb{x}_{S})w_{T}^{*}}\\end{array}$ , where we define $w_{\\emptyset}^{*}\\ {\\stackrel{\\mathrm{def}}{=}}\\ v(x_{\\emptyset})$ for simplicity of notation. Writing the sum in vector norm, we obtain $y(\\pmb{x}_{S})\\,=\\,\\pmb{J}(\\pmb{x}_{S})^{\\top}\\pmb{w}^{*}$ . Furthermore, the whole vector $\\textit{\\textbf{y}}$ can be written as $y=J^{\\top}w^{*}$ . ", "page_idx": 21}, {"type": "text", "text": "With $y=J^{\\top}w^{*}$ , we have $\\begin{array}{r}{\\hat{\\pmb w}=({\\pmb J}^{\\top}{\\pmb J}+2^{n}\\mathrm{diag}({\\pmb c}))^{-1}{\\pmb J}^{\\top}{\\pmb J}{\\pmb w}^{*}=\\hat{M}{\\pmb w}^{*}.}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "F.5 Proof of Lemma 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. According to Eq. (7), the interaction triggering function on an arbitrarily given sample $\\hat{\\pmb{x}}$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ_{T}(\\pmb{x})=\\sum_{\\pmb{\\tau}\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\Big|_{\\pmb{x}=\\pmb{x}_{\\theta}}\\prod_{i\\in T}\\left(x_{i}-b_{i}\\right)^{\\pi_{i}}/w_{T}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $w_{T}=I(T|\\pmb{x}=\\hat{\\pmb{x}})$ , and $Q_{T}=\\{[\\pi_{1},\\ldots,\\pi_{n}]^{\\top}:\\forall i\\in T,\\pi_{i}\\in\\mathbb{N}^{+};\\forall i\\notin T,\\pi_{i}=0\\}.$ ", "page_idx": 21}, {"type": "text", "text": "Specifically, now we consider a masked sample $\\hat{\\pmb{x}}_{S}$ , and we will prove that $J_{T}(\\hat{\\pmb{x}}_{S})=\\mathbb{1}(T\\subseteq S)$ .   \nWe consider the following two cases. ", "page_idx": 21}, {"type": "text", "text": "Case 1: $T\\not\\subseteq S$ . Then, there exists some $j\\in T\\backslash S$ . Since $j\\not\\in S$ , according to the masking rule of the sample $\\hat{\\pmb{x}}_{S}$ , we have $(\\hat{\\pmb{x}}_{S})_{j}-b_{j}=0$ . Since $j\\in T$ , we have $\\pi_{j}\\in\\mathbb{N}^{+}$ . Therefore, $((\\hat{\\pmb{x}}_{S})_{j}-b_{j})^{\\pi_{j}}=0$ In this way, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall\\pmb{\\pi}\\in Q_{T},\\quad\\prod_{i\\in T}\\big((\\pmb{\\hat{x}}_{S})_{i}-b_{i}\\big)^{\\pi_{i}}=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since each term in the summation equals zero, we have $J_{T}(\\hat{\\pmb{x}}_{S})=0$ . ", "page_idx": 21}, {"type": "text", "text": "Case 2: $T\\subseteq S$ . In this case, $\\forall i\\in T$ , we have $i\\in S$ . Therefore, according to the masking rule, we have $\\forall i\\in T\\Rightarrow i\\in S\\Rightarrow(\\hat{\\pmb{x}}_{S})_{i}=\\hat{x}_{i}$ . ", "page_idx": 21}, {"type": "text", "text": "According to the analytic form of $I(T|x)$ in Eq. (19) in the proof in Appendix F.2, we can derive the value of $w_{T}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{T}=I(T|x=\\hat{x})=\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\Big|_{x=x_{\\theta}}\\prod_{i\\in T}\\left(\\hat{x}_{i}-b_{i}\\right)^{\\pi_{i}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we can derive the value of $J_{T}(\\hat{\\pmb{x}}_{S})$ as follows. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{J_{T}(\\hat{x}_{S})=\\displaystyle\\sum_{\\pi\\in Q_{T}}\\frac{1}{\\prod_{i=1}^{n}\\pi_{i}!}\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}}v}{\\partial x_{1}^{\\pi_{1}}\\cdots\\partial x_{n}^{\\pi_{n}}}\\Big\\lvert_{\\alpha=x_{0}}\\prod_{i\\in T}\\big((\\hat{x}_{S})_{i}-b_{i}\\big)^{\\pi_{i}}/\\upsilon_{T}}&{}\\\\ {=\\displaystyle\\frac{\\sum_{\\pi\\in Q_{T}}\\prod_{i=1}^{n}\\frac{1}{\\pi^{i}}\\partial^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}{\\sum_{\\pi\\in Q_{T}}\\prod_{i=1}^{n}\\pi_{i}!}\\Big\\lvert_{\\alpha=x_{1}^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}\\prod_{i=x_{0}}\\prod_{i\\in T}\\big((\\hat{x}_{S})_{i}-b_{i}\\big)^{\\pi_{i}}}&{}\\\\ {=\\displaystyle\\frac{\\sum_{\\pi\\in Q_{T}}\\prod_{i=1}^{n}\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}{\\partial x_{1}^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}\\Big\\lvert_{\\alpha=x_{0}}\\prod_{i\\in T}\\big(\\hat{x}_{i}-b_{i}\\big)^{\\pi_{i}}}{\\sum_{\\pi\\in Q_{T}}\\prod_{i=1}^{n}\\pi_{i}!}}&{}\\\\ {=\\displaystyle\\frac{\\sum_{\\pi\\in Q_{T}}\\prod_{i=1}^{1}\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}{\\partial x_{1}^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}\\Big\\lvert_{\\alpha=x_{0}}\\prod_{i\\in T}\\big(\\hat{x}_{i}-b_{i}\\big)^{\\pi_{i}}}{\\sum_{\\pi\\in Q_{T}}\\prod_{i=1}^{n}\\frac{\\partial^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}{\\partial x_{1}^{\\pi_{1}+\\cdots+\\pi_{n}v_{i}}}\\Big\\lvert_{\\alpha=x_{0}}\\prod_{i\\in T}\\big(\\hat{x}_{i}-b_{i}\\big)^{\\pi\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining the two cases, we can conclude that $J_{T}(\\hat{\\pmb{x}}_{S})=\\mathbb{1}(T\\subseteq S)$ . ", "page_idx": 21}, {"type": "text", "text": "In this way, no matter how we change the DNN $\\boldsymbol{v}(\\cdot)$ or the input sample $\\textbf{\\em x}$ , the matrix $\\textbf{\\emph{J}}=$ $[{\\pmb J}({\\pmb x}_{S_{1}}),{\\pmb J}({\\pmb x}_{S_{2}}),\\cdot\\cdot\\cdot\\,,{\\pmb J}({\\pmb x}_{S_{2^{n}}})]^{\\intercal}\\in\\{0,1\\}^{2^{n}\\times2^{n}}$ in Eq. (10) is a always a fixed binary matrix. ", "page_idx": 21}, {"type": "text", "text": "F.6 Proof of Theorem 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We prove that for any two subsets $T,T^{\\prime}\\subseteq N$ of the same order, the vector $\\hat{\\pmb{m}}_{T}$ is a permutation of the vector $\\hat{\\pmb{m}}_{T^{\\prime}}$ . ", "page_idx": 22}, {"type": "text", "text": "The proof consists of two steps. First, we show that there exists a symmetric matrix transformation $\\bar{T}(\\cdot)^{'}=P_{k}P_{k-1}\\cdot\\cdot\\cdot P_{1}(\\cdot)P_{1}\\bar{P}_{2}\\cdot\\cdot\\cdot P_{k-1}P_{k}$ , where $P_{i}$ is a permutation matrix, that maps both $J^{\\top}J$ and $J^{\\top}J+2^{n}\\mathrm{diag}(c)$ to themselves, i.e., $\\mathcal{T}(J^{\\top}J)\\,=\\,J^{\\top}J$ , $\\mathcal{T}(J^{\\top}J+2^{n}\\mathrm{diag}(c))\\,=\\,J^{\\top}J+$ $2^{n}\\mathrm{diag}(c)$ . We will show that this transformation $\\tau(\\cdot)$ applies permutation to the rows and columns ", "page_idx": 22}, {"type": "text", "text": "of the same order. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Second, we show that this transformation also maps $\\hat{M}$ to itself, i.e., $\\mathcal{T}(\\hat{M})=\\hat{M}$ , implying that row vectors of the same order in $\\hat{M}$ are permutations of each other. ", "page_idx": 22}, {"type": "text", "text": "From Theorem 3, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(J^{\\top}J+2^{n}\\mathrm{diag}(c))\\hat{M}=J^{\\top}J\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To simplify the notation, we denote $B:=J^{\\top}J$ and $D:=2^{n}\\mathrm{diag}(c)$ . Then, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(B+D)\\hat{M}=B\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Step 1: We construct a transformation $\\tau(\\cdot)$ which permutes the rows and columns of a $2^{n}\\times2^{n}$ matrix based on element selection. Let us first consider the matrix $_B$ . For the matrix $_{D}$ , the analysis is similar because its diagonal elements $2^{|T|+n}\\sigma^{2}$ are the same for each order. Thus, if $\\tau(\\cdot)$ maps $_B$ to itself, it also maps $_{D}$ to itself. ", "page_idx": 22}, {"type": "text", "text": "Given the set $N\\;\\;=\\;\\;\\{1,2,\\cdots\\;,n\\}$ , the subsets $S_{1},S_{2},\\cdots\\,,S_{2^{n}}$ can be regarded as selections from the power set of $N$ , denoted as $2^{N}$ . Consider a permutation $\\mathcal{P}$ acting on $N$ . Under this permutation, the selections $S_{1},S_{2},\\cdot\\cdot\\cdot,S_{2^{n}}$ transform correspondingly. For example, if $\\begin{array}{r l r}{\\bar{N}}&{{}=}&{\\{1,2,3\\}}\\end{array}$ is mapped to $\\textit{\\textbf{N}}=\\:\\{3,2,1\\}$ under the permutation $\\mathcal{P}$ , the list of subsets $\\begin{array}{r c l}{\\left[S_{1},S_{2},\\dot{\\cdot}\\cdot\\cdot,\\dot{S_{2^{n}}}\\right]}&{=}&{\\left[\\varnothing,\\{1\\},\\{2\\},\\{3\\},\\{1,2\\},\\{1,3\\},\\{2,3\\},\\{1,2,3\\}\\right]}\\end{array}$ is mapped to $[\\emptyset,\\{3\\},\\{2\\},\\{1\\},\\{3,2\\},\\{3,1\\},\\{2,1\\},\\{3,2,1\\}].$ . ", "page_idx": 22}, {"type": "text", "text": "This permutation induces a transformation $T(\\cdot)=P_{k}P_{k-1}\\cdot\\cdot\\cdot P_{1}(\\cdot)P_{1}P_{2}\\cdot\\cdot\\cdot P_{k-1}P_{k}$ on the matrix $B=J^{\\top}J$ by permuting its rows and columns. ", "page_idx": 22}, {"type": "text", "text": "Since the permutation acts on $N$ and preserves the inclusion relation, the transformation $\\tau(\\cdot)$ is invariant, meaning $\\boldsymbol{\\mathcal{T}}(\\boldsymbol{B})=\\boldsymbol{B}$ . Similarly, we have $\\mathcal{T}(B+D)=B+D$ . ", "page_idx": 22}, {"type": "text", "text": "Step 2: We apply $\\tau(\\cdot)$ to the matrices $B+D$ and $_B$ in Eq. (61). Since the transformation is invariant, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{T}}(B+D)\\hat{\\boldsymbol{M}}=\\boldsymbol{\\mathcal{T}}(B)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus: ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{k}P_{k-1}\\cdot\\cdot\\cdot P_{1}(B+D)P_{1}P_{2}\\cdot\\cdot\\cdot P_{k-1}P_{k}\\hat{M}=P_{k}P_{k-1}\\cdot\\cdot\\cdot P_{1}(B)P_{1}P_{2}\\cdot\\cdot\\cdot P_{k-1}P_{k}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can easily see that if $\\hat{M}$ is a solution to this equation, then $\\begin{array}{r l}{\\boldsymbol{\\mathcal{T}}(\\hat{\\boldsymbol{M}})}&{{}=}\\end{array}$ $P_{k}P_{k-1}\\cdot\\cdot\\cdot P_{1}\\hat{M}P_{1}P_{2}\\cdot\\cdot\\cdot P_{k-1}P_{k}$ is also a solution, since $P_{i}^{2}\\;=\\;I,i\\;=\\;1,\\cdots\\;,k$ , where $I$ is the identity matrix. In addition, because $B+D$ is invertible (as shown in Appendix F.4), this solution is unique. Therefore: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{T}}(\\hat{\\boldsymbol{M}})=\\hat{\\boldsymbol{M}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This shows that the transformation $\\tau(\\cdot)$ also maps $\\hat{M}$ to itself. ", "page_idx": 22}, {"type": "text", "text": "Conclusion: We have shown that, under the transformation $\\tau(\\cdot)$ , the affected rows of $\\hat{M}$ are permutations of each other. Note that only the rows with the same order will be permuted to each other because $\\tau(\\cdot)$ is derived from the permutation of the power set of $N$ , so the order of the rows is preserved. ", "page_idx": 22}, {"type": "text", "text": "For any two subsets $T,T^{\\prime}\\subseteq N$ of the same order, we can construct a permutation of indices from $T$ to $T^{\\prime}$ that maps $\\hat{\\pmb{m}}_{T}$ to $\\hat{\\pmb{m}}_{T^{\\prime}}$ . Therefore, $\\hat{m}_{T}$ is a permutation of $\\hat{\\pmb{m}}_{T^{\\prime}}$ . \u53e3 ", "page_idx": 22}, {"type": "table", "img_path": "dIHXwKjXRE/tmp/8054fb42ff688535ba951e6e24497763cf6a252c8d2823d3715c0c3c24391e43.jpg", "table_caption": [], "table_footnote": ["Table 1: Mathematical setting of hyper-parameters for interactions. "], "page_idx": 23}, {"type": "text", "text": "F.7 Proof of Theorem 5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. From Eq. (10), when there is no noise (i.e., $\\begin{array}{r l r}{\\sigma}&{{}=}&{0)}\\end{array}$ ), it is obvious that $\\hat{\\pmb w}^{\\mathrm{~\\,~}}=$ $(J^{\\top}\\bar{J})^{-1}J^{\\top}J w^{*}=w^{*}$ , which means that the optimal weights $\\hat{w}$ are the same as the true weights $\\pmb{w}^{*}$ . $\\operatorname{So}\\forall\\,\\varnothing\\neq T\\subseteq N$ , $\\hat{w}_{T}=w_{T}^{*}$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "G Experimental details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "G.1 Models and datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We trained various DNNs on different datasets. Specifically, for image data, we trained VGG-11 on the MNIST dataset (Creative Commons Attribution-Share Alike 3.0 license), VGG-11/VGG-16 on the CIFAR-10 dataset (MIT license), AlexNet/VGG-16 on the CUB-200-2011 dataset (license unknown), and VGG-16 on the Tiny ImageNet dataset (license unknown). For natural language data, we trained BERT-Tiny and BERT-Medium on the SST-2 dataset (license unknown). For point cloud data, we trained DGCNN on the ShapeNet dataset (Custom (non-commerical) license). ", "page_idx": 23}, {"type": "text", "text": "For the CUB-200-2011 dataset, we cropped the images to remove the background regions, using the bounding box provided by the dataset. These cropped images were resized to $224\\!\\times\\!224$ and fed into the DNN. For the Tiny ImageNet dataset, due to the computational cost, we selected 50 classes from the total 200 classes at equal intervals (i.e., the 4th, 8th,..., 196th, 200th classes). All these images were resized to $224\\!\\times\\!224$ . For the MNIST dataset, all images were resized to $32\\!\\times\\!32$ for classification. To better demonstrate that the learning of higher-order interactions in the second phase was closely related to overfitting, we added a small ratio of label noise to the MNIST dataset, the CIFAR-10 dataset, and the CUB-200-2011 dataset to boost the significance of over-fitting of the DNNs. Specifically, we randomly selected $1\\%$ training samples in the MNIST dataset and the CIFAR-10 dataset, and randomly reset their labels. We randomly selected $5\\%$ training samples in the CUB-200-2011 dataset and randomly reset their labels. ", "page_idx": 23}, {"type": "text", "text": "G.2 Training settings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We trained all DNNs using the SGD optimizer with a learning rate of 0.01 and a momentum of 0.9. No learning rate decay was used. We trained VGG models, AlexNet models, and BERT models for 256 epochs, and trained the DGCNN model for 512 epochs. The batchsize was set to 128 for all DNNs on all datasets. ", "page_idx": 23}, {"type": "text", "text": "G.3 Details on computing interactions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "First, we provide a summary of the mathematical settings of the hyper-parameters for interactions in Table 1, including the scalar output function of the DNN $\\boldsymbol{v}(\\cdot)$ , the baseline value $^{b}$ for masking, and the threshold $\\tau$ . These settings are uniformly applied to all DNNs. More detailed settings for different datasets can be found below. ", "page_idx": 23}, {"type": "text", "text": "Image data. For image data, we considered image patches as input variables to the DNN. To generate a masked sample $\\pmb{x}_{S}$ , we followed [41] to mask the patch on the intermediate-layer feature map corresponding to each image patch in the set $N\\setminus S$ . Specifically, we considered the feature map after the second ReLU layer for VGG-11/VGG-16 and the feature map after the first ReLU layer for AlexNet. For the VGG models and the AlexNet model, we uniformly partitioned the feature map into $8\\!\\times\\!8$ patches, randomly selected 10 patches from the central $6\\!\\times\\!6$ region (i.e., we did not select patches that were on the edges), and considered each of the 10 patches as an input variable in the set $N$ to calculate interactions. We considered each of the 10 patches as an input variable in the set $N$ to calculate interactions. We used a zero baseline value to mask the input variables in the set $N\\setminus S$ to obtain the masked sample $\\pmb{x}_{S}$ . ", "page_idx": 23}, {"type": "text", "text": "Natural language data. We considered the input tokens as input variables for each input sentence. Specifically, we randomly selected 10 words that are meaningful (i.e., not including stopwords, special characters, and punctuations) as input variables in the set $N$ to calculate interactions. We used the \u201cmask\u201d token with the token id 103 to mask the tokens in the set $N\\setminus S$ to obtain the masked sample $\\pmb{x}_{S}$ . ", "page_idx": 24}, {"type": "text", "text": "Point cloud data. We clustered all the points into 30 clusters using K-means clustering, and randomly selected 10 clusters as the input variables in the set $N$ to calculate interactions. We used the average coordinate of the points in each cluster to mask the corresponding cluster in $N\\setminus S$ and obtained the masked sample $\\pmb{x}_{S}$ . ", "page_idx": 24}, {"type": "text", "text": "For all DNNs and datasets, we randomly selected 50 samples from the testing set to compute interactions, and averaged the interaction strength of the $k$ -th order on each sample to obtain $I_{\\mathrm{real}}^{(\\bar{k})}$ . ", "page_idx": 24}, {"type": "text", "text": "G.4 Compute resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "All DNNs can be trained within 12 hours on a single NVIDIA GeForce RTX 3090 GPU (with 24G GPU memory). Computing all interactions on a single input sample usually takes 35-40 seconds, which is acceptable in real applications. ", "page_idx": 24}, {"type": "text", "text": "H Potential limitations of the theoretical proof ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this study, we have assumed that during the training process, the noise on the parameters gradually decreased ( $\\sigma^{2}$ gradually became smaller). Although experiments in Figure 4 and Figure 8 have verified that the theoretical distribution of interaction strength can well match the real distribution by using a set of decreasing $\\sigma^{2}$ values, it is not exactly clear how the value of $\\sigma^{2}$ is related to the training process. The value of $\\sigma^{\\frac{\\smile}{2}}$ probably does not decrease linearly along with the training epochs/iterations, which needs more precise formulations. ", "page_idx": 24}, {"type": "text", "text": "I More discussions about the two-phase dynamics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "I.1 Does the model re-learn the initial interactions during the second phase? ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our theory does not claim that in the second phase, a DNN will not re-encode an interaction that is removed in the first phase. Instead, Theorem 4 and Proposition 1 collectively indicate the possibility of a DNN gradually re-encoding a few higher-order interactions in the second phase along with the decrease of the parameter noise. ", "page_idx": 24}, {"type": "text", "text": "The key point to this question is that the massive interactions in a fully initialized DNN are all chaotic and meaningless patterns caused by randomly initialized network parameters. Therefore, the crux of the matter is not whether the DNN re-learns the initially removed interactions, but the fact that the DNN mainly removes chaotic and meaningless initial interactions in the first phase, and learns potential target interactions in the second phase. In this way, although a few interactions may be re-encoded later in the second phase, we do not consider this as a problem with the training of a DNN. ", "page_idx": 24}, {"type": "text", "text": "I.2 About extending the theoretical analysis to specific network architectures ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our current analysis is agnostic to the network architecture, and aims to explain the common twophase dynamics of interactions that is shared by different network architectures for various tasks. Fig. 2 and Fig. 5 demonstrate this shared two-phase dynamics. ", "page_idx": 24}, {"type": "text", "text": "On the other hand, although DNNs with different architectures all exhibit the two-phase dynamics of interactions, the length of the two phases and the finally converged state of the DNN are influenced by the network architecture and can slightly vary among different architectures. Eq. (10) shows that our current formulation is to use the finally converged state of a DNN to accurately predict the DNN\u2019s learning dynamics of interactions. Therefore, the learning dynamics predicted by our theory also exhibits slight differences among different DNN architectures and datasets accordingly, but it still matches well with the empirical dynamics of interactions. To this end, studying how the network architecture affects the finally converged state of a DNN may be a good future direction. ", "page_idx": 24}, {"type": "text", "text": "J More experimental results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "J.1 More results for the two-phase phenomenon ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this subsection, we show the two-phase dynamics of learning interactions on more DNNs and datasets. See Figure 5 and Figure 6 for details. ", "page_idx": 25}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/3da52bb8cb2e0ef4f43afea4d9dc8b7a2ab8c747a9b4b43689adc7878143c476.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 5: The distribution of interaction strength $I_{\\mathrm{real}}^{(k)}$ over different orders $k$ . Each row shows the change of the distribution during the training process. Experiments showed that the two-phase phenomenon widely existed on different DNNs trained on various datasets. ", "page_idx": 25}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/a36f921a900e221e7609b2be2ff6b308d9759f398d65fbc8bc8ce869469289c4.jpg", "img_caption": ["Figure 6: Demonstration of the two-phase dynamics of interactions on more textual datasets. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Besides the loss gap, in Figure 7, we also show the training loss and the testing loss separately. In fact, instead of considering underfitting (or learning useful features) and overfitting (or learning overfitted features) as two separate processes, the DNN simultaneously learns both useful features and overftited features during training. The learning of useful features decreases the training loss and the testing loss, which alleviates underftiting. Meanwhile, the learning of overftited features gradually increases the loss gap. ", "page_idx": 26}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/b8b79bea1b48acb5c8a852b60aa59cb57a9de33c762696aca684d70c17923f48.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 7: Demonstration of the training loss and the testing loss (the last column) in addition to the two-phase dynamics of interactions (1st column to 6th column) and the loss gap (7th column). ", "page_idx": 26}, {"type": "text", "text": "J.3 More results for the experimental verification of our theory ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this subsection, we show results of using the theoretical distribution of interaction strength $I_{\\mathrm{theo}}^{(k)}$ to match the real distribution of interaction strength $I_{\\mathrm{real}}^{(k)}$ on more DNNs and datasets, as shown in ", "page_idx": 26}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/cea41ef2657187fc4f36057ea8b3ceda3af5308da2e383af1790173edb67ba88.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 8: Comparison between the theoretical distribution of interaction strength $I_{\\mathrm{theo}}^{(k)}$ and the real distribution of interaction strength Ir(ekal) i n the second phase on more DNNs and datasets. ", "page_idx": 27}, {"type": "text", "text": "J.4 Using the theoretical distribution $I_{\\mathrm{theo}}^{(k)}$ to predict the real distribution of AND interactions ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this subsection, we show results of using the theoretical distribution of interaction strength $I_{\\mathrm{theo}}^{(k)}$ to match the real distribution of AND interactions (rather than the AND-OR interactions), as shown in Figure 9. ", "page_idx": 27}, {"type": "image", "img_path": "dIHXwKjXRE/tmp/8c164363e799e3796deaad014f91648c5b566f29049902ff9c49bdb18300da33.jpg", "img_caption": ["Figure 9: Comparison between the theoretical distribution of interaction strength $I_{\\mathrm{theo}}^{(k)}$ and the real distribution of interaction strength of AND interactions. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect our paper\u2019s contributions and scope. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Although we have no room for a separate Limitations section in the main paper, we provide discussion of potential limitations in Appendix G. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 29}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide the assumptions in the main paper, and the proof for all theorems in Appendix E. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The contribution of this paper is mainly theoretical. Nevertheless, we provide the detailed experimental settings in Appendix F to reproduce the experiment results. The code will be released when the paper is accepted. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: The code will be released when the paper is accepted. All datasets used in this paper are publicly available. Nevertheless, to enhance reproducibility, we provide the detailed experimental settings in Appendix F. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Details on dataset preprocessing can be found in Appendix F.1. Details on training settings can be found in Appendix F.2. Details on how to compute interactions can be found in Appendix F.3. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: The main contribution of this study is to provide theoretical proof for the twophase dynamics phenomenon discovered in previous studies. The experiments in this study are mainly to reproduce the two-phase dynamics phenomenon for better illustration and to verify that our theory can predict the trend of the interaction dynamics on real DNNs. This study does not propose new methods to boost performance or discover a new phenomenon, so we refrain from reporting error bars for clarity. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the compute resources needed in Appendix F.4, including the type of GPU and the approximate amount of time for training DNNs and computing interactions. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The contribution of this paper is mainly theoretical, which has not yet been applied to real applications. The social impact could be little, for now. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: All models and datasets used in this paper are already publicly available. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We cite the original paper for all datasets. The name of the license is included for each dataset in Appendix F.1, although some licenses are unknown. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]