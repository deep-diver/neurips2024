[{"figure_path": "8x48XFLvyd/figures/figures_6_1.jpg", "caption": "Figure 1: Negative log-likelihood across gradient steps, for network widths 64, 256, and 1024 neurons. NLL for the exact posterior is denoted by the red line.", "description": "This figure shows the negative log-likelihood (NLL) across gradient descent steps for three different network widths (64, 256, and 1024 neurons).  The NLL measures how well the model approximates the true posterior distribution. Lower NLL indicates better approximation.  The figure compares the performance of the full neural network and its linearization around initialization, demonstrating that with increasing width, the linearized model closely approximates the full model's performance, suggesting convergence to an asymptotic regime.", "section": "5.1 Toy Example"}, {"figure_path": "8x48XFLvyd/figures/figures_7_1.jpg", "caption": "Figure 2: Mode of q(S; f1(x; \u03c61)) across experimental replicates.", "description": "This figure displays kernel-smoothed frequency plots for the posterior mode estimates of the shift parameter S, obtained from 100 independent experimental replications, using both ELBO-based and Lp-based optimization methods. Two different parameterizations of the Gaussian variational distributions, mean-only and natural parameterization, are used for each method. The results illustrate that Lp-based optimization consistently recovers the correct value of S, while ELBO-based optimization exhibits label switching, converging to different permutations of the true cluster centers.", "section": "5.2 Label Switching in Amortized Clustering"}, {"figure_path": "8x48XFLvyd/figures/figures_8_1.jpg", "caption": "Figure 3: 100 of the N = 1000 data observations with counterclockwise rotation of \u03b8 = 260 degrees.", "description": "This figure shows 100 MNIST digits that have been rotated counterclockwise by 260 degrees.  These digits were generated as part of a generative model used in the Rotated MNIST Digits experiment (Section 5.3) to test the ability of the expected forward KL divergence minimization approach to variational inference compared to likelihood-based methods for inferring a shared rotation angle for a set of MNIST digits.", "section": "5.3 Rotated MNIST Digits"}, {"figure_path": "8x48XFLvyd/figures/figures_8_2.jpg", "caption": "Figure 4: Estimate of angle \u03b8 across gradient steps, with fitting performed to maximize the IWBO.", "description": "This figure shows the results of fitting a variational distribution to maximize the importance weighted bound (IWBO) for the rotated MNIST digits experiment. The IWBO is a likelihood-based approach that is prone to getting stuck in shallow local optima due to the multimodality of the likelihood in this particular problem. Different initializations of the rotation angle converge to different local optima. This is in contrast to the expected forward KL approach which consistently converges to the global optimum (shown in Figure 5).", "section": "5.3 Rotated MNIST Digits"}, {"figure_path": "8x48XFLvyd/figures/figures_8_3.jpg", "caption": "Figure 1: Negative log-likelihood across gradient steps, for network widths 64, 256, and 1024 neurons. NLL for the exact posterior is denoted by the red line.", "description": "This figure shows the negative log-likelihood (NLL) curves for three different network widths (64, 256, and 1024 neurons) during the training process.  The NLL measures the quality of the variational posterior approximation to the true posterior distribution.  Lower NLL values indicate better approximations. The red line represents the NLL of the exact posterior, serving as a baseline for comparison.  As the network width increases, the NLL of the fitted network approaches the NLL of the exact posterior, demonstrating that wider networks perform better.", "section": "5.1 Toy Example"}, {"figure_path": "8x48XFLvyd/figures/figures_8_4.jpg", "caption": "Figure 6: Zoomed-in trajectories across the first 2000 gradient steps, showing similar estimates regardless of initialization.", "description": "This figure shows the trajectories of the optimization of the expected forward KL divergence across multiple initializations. It zooms in on the first 2000 gradient steps to show the behavior of the optimization algorithm. The plot demonstrates that regardless of the starting point, the algorithm converges to the same solution. This illustrates that the algorithm is not trapped by local minima and consistently finds the global optimum.", "section": "5.2 Label Switching in Amortized Clustering"}, {"figure_path": "8x48XFLvyd/figures/figures_9_1.jpg", "caption": "Figure 7: Forward and reverse KL divergences to the true posterior across fitting for minimization of the expected forward KL (blue) or the negative ELBO (green). We also plot the negative log likelihood of the true angle, as well as the variational mode (true angle \u03b8true is plotted in red.)", "description": "This figure compares the performance of minimizing the expected forward KL divergence against maximizing the evidence lower bound (ELBO) for variational inference on a simple rotated MNIST digit problem.  The plot shows the forward and reverse KL divergences between the true posterior and the variational approximations obtained by both methods, along with the negative log-likelihood and the estimated angle. The results indicate that minimizing the expected forward KL leads to better approximations of the true posterior than maximizing the ELBO, which suffers from getting stuck in shallow local optima.", "section": "5.4 Local Optima vs. Global Optima"}]