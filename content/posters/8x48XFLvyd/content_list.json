[{"type": "text", "text": "Globally Convergent Variational Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Declan McNamara Jackson Loper Jeffrey Regier Department of Statistics University of Michigan {declan, jaloper, regier}@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a local optimum can be guaranteed. In this work, we instead establish the global convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In variational inference (VI), the parameters $\\eta$ of an approximation to the posterior $Q(\\Theta;\\eta)$ are selected to optimize an objective function, typically the evidence lower bound (ELBO) (Blei et al., 2017). However, the ELBO is generally nonconvex in $\\eta$ , even for simple variational families such as the family of Gaussian distributions, and so only convergence to a local optimum of the ELBO can be guaranteed (Ghadimi and Lan, 2015; Ranganath et al., 2014; Hoffman et al., 2013). As the number of such optima and the degree of suboptimality of each are generally unknown, the lack of global convergence guarantees constitutes a significant complication for practitioners and a longstanding barrier to the broader adoption of VI. ", "page_idx": 0}, {"type": "text", "text": "In this work, we present the first global convergence result for variational inference. We accomplish this in the context of an increasingly popular alternative objective for variational inference, the expected forward $\\mathrm{KL}$ divergence: ", "page_idx": 0}, {"type": "equation", "text": "$$\nL_{P}(\\phi):=\\mathbb{E}_{P(X)}{\\mathrm{KL}}\\left[P(\\Theta\\mid X)\\mid\\mid Q(\\Theta;f(X;\\phi))\\right].\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here, $P(X)$ denotes a marginal of the model and $P(\\Theta\\mid X)$ denotes the posterior. For each $x\\in\\mathscr{X}$ , the approximation $Q(\\Theta;\\eta)$ to $P(\\Theta\\mid X=x)$ is indexed by the distributional parameters $\\eta\\in\\mathcal{D}\\subseteq\\mathbb{R}^{q}$ , which are themselves the output of a neural network $f(x;\\phi)$ with weights $\\phi\\in\\Phi$ . Approximating a posterior distribution by minimizing $L_{P}$ has a long history (Section 2.1), and is sometimes known as neural posterior estimation (NPE) (Papamakarios and Murray, 2016) and the \u201csleep\u201d objective of reweighted wake-sleep (Bornschein and Bengio, 2015; Le et al., 2019). Minimization of this objective is straightforward: computing unbiased gradients requires only sampling $\\theta,x\\sim P(\\Theta,X)$ from the joint model (Section 2.1), which is readily accomplished by ancestral sampling. This approach is \u201clikelihood-free\u201d in that the density of $P(X\\mid\\Theta)$ need not be evaluated, and therefore expected forward KL minimization is more widely applicable than ELBO-based optimization, which requires a tractable likelihood function. Analysis of the amortized problem (i.e., optimizing an objective that averages over $P(X))$ is beneficial when considering the forward KL; for the non-amortized problem in which a single observation $x$ is considered, only biased estimates of the gradient of the forward KL can be obtained using self-normalized importance sampling, making convergence difficult to establish (Bornschein and Bengio, 2015; Le et al., 2019; Owen, 2013). Our analysis considers a functional form of variational objective $L_{P}$ , given by ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{F}(f):=\\mathbb{E}_{P(X)}{\\mathrm{KL}}\\left[P(\\Theta\\mid X)\\mid\\mid Q(\\Theta;f(X))\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $L_{F}:\\mathcal{H}\\to\\mathbb{R}$ is defined over a general reproducing kernel Hilbert space of functions $\\mathcal{H}$ . We refer to (1) as the \u201cparametric objective\u201d, as its argument is the parameters $\\phi\\in\\Phi$ , and we refer to (2) as the \u201cfunctional objective\u201d as its argument is a function $f\\in\\mathcal H$ . These objectives are closely related: under a given network parameterization, provided $f(\\cdot;\\phi)\\in\\mathcal{H}$ , we have $\\bar{L_{P}}(\\phi)=L_{F}(f(\\cdot;\\bar{\\phi}))$ . The objective $L_{P}$ has been considered in several related works (see Section 2.1). The formulation of $L_{F}$ and the analysis of its minimizer relative to those of $L_{P}$ is our main contribution. ", "page_idx": 1}, {"type": "text", "text": "We first demonstrate strict convexity of the functional objective $L_{F}$ when $Q$ is parameterized as an exponential family distribution (Section 3). This implies the existence of a unique global optimizer $f^{*}$ of $L_{F}$ for a large class of variational families. Afterward, we analyze kernel gradient flow dynamics using the neural tangent kernel to show that minimization of $L_{P}$ results (asymptotically) in an empirical mapping $f$ that is at most $\\epsilon$ -suboptimal relative to $f^{*}$ , provided a sufficiently flexible neural network is used to parameterize $f$ (Section 4). Together, these results imply that in the infinite-width limit, optimization of $L_{P}$ by gradient descent recovers a unique global solution. ", "page_idx": 1}, {"type": "text", "text": "Our analysis relies on fairly mild conditions, the most important of which are the positive definiteness of the neural tangent kernel and the structure of the variational family (e.g., an exponential family) (Section 6). Our proofs further assume a two-layer ReLU network architecture, but we conjecture that this assumption can be relaxed, and our experiments (Section 5) demonstrate global convergence for a wide variety of architectures. We illustrate that the minimization of $L_{P}$ converges to a global solution for problems with both synthetic and semi-synthetic data, and that finite network widths exhibit the behavior of the asymptotic regime (i.e., that of an infinitely wide network). ", "page_idx": 1}, {"type": "text", "text": "We further show that optimizing $L_{P}$ can produce better posterior approximations than likelihoodbased ELBO methods, which suffer from convergence to shallow local optima. These results suggest, surprisingly, that a likelihood-free approach to inference can outperform likelihood-based approaches. Further, even for practitioners interested in inference for a single observation $x_{0}$ , for whom amortization is not needed for computational efficiency, our approach may still be preferable to traditional ELBO-based inference due to the convergence guarantees of the former. ", "page_idx": 1}, {"type": "text", "text": "Related work. There is a large body of literature that analyzes the convergence of variational inference methods that target the ELBO. These works typically prove rates of convergence to the posterior (Zhang and Gao, 2020) or to a local optimum of the objective, as the ELBO is not amenable to global minimization because it is nonconvex (Domke, 2020; Domke et al., 2023; Kim et al., 2023). Liu et al. (2023) demonstrated nonconvexity of the ELBO in the context of small-object detection, and showed empirically that the expected forward KL was more robust to the pitfalls of numerical optimization. The nonconvexity of the variational objective has previously been addressed through workarounds such as convex relaxations (Fazelnia and Paisley, 2018) or iterated runs of the optimization routine to improve the quality of the local optimum (Altosaar et al., 2018). Our work differs from previous work in that our convergence result is global. Additionally, our approach is novel compared to related analyses because we consider the (arguably) more complicated problem of amortized inference, where the variational parameters are the weights of a neural network and are shared among observations. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 The Expected Forward KL Divergence ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The expected forward KL objective is equivalent to the sleep-phase objective of Reweighted WakeSleep (RWS) (Bornschein and Bengio, 2015), and to the objective optimized by forward amortized variational inference (FAVI) (Ambrogioni et al., 2019). It is also a special case of the thermodynamic variational objective (TVO) (Masrani et al., 2019). Similar objectives have been referred to as neural posterior estimation (NPE) (Papamakarios and Murray, 2016; Papamakarios et al., 2019), though in these works the prior distribution, and thus the marginal $P(X)$ , mutates during training. ", "page_idx": 2}, {"type": "text", "text": "Objectives based on the forward KL divergence generally result in variational posteriors that are overdispersed, a desirable property compared to reverse KL-based optimization (Le et al., 2019; Domke and Sheldon, 2018). ", "page_idx": 2}, {"type": "text", "text": "Unbiased gradient estimation for the parametric objective $L_{P}$ is straightforward. The outer expectation over $P(X)$ allows for gradients to be computed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}\\mathbb{E}_{P(X)}\\mathrm{KL}\\left[P(\\Theta\\mid X)\\mid\\mid Q(\\Theta;f(X;\\phi))\\right]=-\\mathbb{E}_{P(\\Theta)}\\mathbb{E}_{P(X\\mid\\Theta)}\\nabla_{\\phi}\\log q(\\Theta;f(X;\\phi)),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $q$ is the density of $Q$ with respect to Lebesgue measure (see Appendix B for details). Rewriting the left-hand side as an expectation over $P(\\Theta)$ and $P(X\\mid\\Theta)$ by Bayes\u2019 rule illustrates that samples can be drawn from this model by ancestral sampling of $\\Theta$ followed by $X$ . ", "page_idx": 2}, {"type": "text", "text": "Other methods targeting the forward $\\mathrm{KL}$ , such as the wake-phase of RWS, often optimize over a different expectation, typically $\\mathbb{E}_{X\\sim\\mathcal{D}}\\mathrm{KL}\\left[P(\\Theta\\mid X)\\mid\\mid Q(\\Theta;\\bar{f}(X;\\phi))\\right]$ . Here, the outer expectation is over an empirical dataset $\\mathcal{D}$ rather than $P(X)$ . In this case, approximation techniques such as importance sampling are required to estimate the gradient, as sampling from $P(\\Theta\\mid X=x)$ is intractable for any $x$ . Relying on importance sampling results in biased gradient estimates, with which stochastic gradient descent (SGD) may not converge (Bornschein and Bengio, 2015; Le et al., 2019). ", "page_idx": 2}, {"type": "text", "text": "2.2 The Neural Tangent Kernel ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A neural network architecture and the parameter space $\\Phi$ of its weights together define a family of functions $\\{f(\\cdot;\\phi):\\phi\\in\\Phi\\}$ . Let $\\ell(x,\\bar{f}(x))$ denote a general real-valued loss function and consider selecting the parameters $\\phi$ to minimize $\\Dot{\\mathbb{E}}_{P(X)}\\ell(X,f(X;\\phi))$ , where $P(X)$ is a distribution on the data space $\\mathcal{X}$ . The neural tangent kernel (NTK) (Jacot et al., 2018) analyzes the evolution of the function $f(\\cdot;\\phi)$ while $\\phi$ is fitted by gradient descent to minimize the above objective. Continuoustime dynamics are used in the formulation; $\\phi(t)$ and $f(\\cdot;\\phi(t))$ are defined for continuous time $t$ . The parameters $\\phi$ thus follow the ODE ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\dot{\\phi}(t)=-\\nabla_{\\phi}\\mathbb{E}_{P(X)}\\ell(X,f(X;\\phi(t))).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\dot{\\phi}$ denotes the derivative with respect to $t$ , and by the chain rule, the function values $f(x;\\phi(t))$ evolve via ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{f}(x;\\phi(t))=-\\mathbb{E}_{P(X)}\\underbrace{J_{\\phi}f(x;\\phi(t))J_{\\phi}f(X;\\phi(t))^{\\top}}_{\\mathrm{NTK}}\\ell^{\\prime}(X,f(X;\\phi(t))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We define $\\ell^{\\prime}(X,f(X)):=\\nabla_{f}\\ell(X,f(X))$ to simplify the notation. The product of Jacobians above is known as the neural tangent kernel (NTK): ", "page_idx": 2}, {"type": "equation", "text": "$$\nK_{\\phi}(x,x^{\\prime})=J_{\\phi}f(x;\\phi)J_{\\phi}f(x^{\\prime};\\phi)^{\\top}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The seminal work of Jacot et al. (2018) defined and studied this kernel and established the convergence of $K_{\\phi}$ to a limiting kernel for certain neural network architectures as the layer width grows large. ", "page_idx": 2}, {"type": "text", "text": "2.3 Vector-Valued Reproducing Kernel Hilbert Spaces ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Most existing NTK-based analyses consider neural networks with scalar outputs and squared error loss. Instead, we consider neural networks with multivariate outputs to accommodate the multidimensional distributional parameter $\\eta$ , which parameterizes our variational distribution $Q(\\Theta;\\eta)$ . Furthermore, we consider the objective functions $L_{P}$ and $L_{F}$ . Consequently, we rely on results from the vector-valued reproducing kernel Hilbert space (RKHS) literature, as these spaces contain vector-valued functions such as the network function $f(\\cdot;\\phi)$ . Carmeli et al. (2008, 2006) provide a detailed review, and in the following, we summarize the key properties. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Recall that $\\eta=f(\\cdot;\\phi)$ and $\\eta\\in\\mathbb{R}^{q}$ . An $\\mathbb{R}^{q}$ -valued kernel on $\\mathcal{X}$ is a map $K:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}^{q\\times q}$ . The neural tangent kernel (4) is precisely such a kernel. If the $\\mathbb{R}^{q}$ -valued kernel $K$ is positive definite, then $K$ defines a unique Hilbert space of functions $\\mathcal{H}$ whose elements are maps from $\\mathcal{X}$ to $\\mathbb{R}^{q}$ (Carmeli et al., 2006) . This kernel $K$ is called the reproducing kernel, and the corresponding space of functions is the RKHS associated with the kernel $K$ . ", "page_idx": 3}, {"type": "text", "text": "In an $\\mathbb{R}^{q}$ -valued RKHS, the reproducing property takes on a more general form. For any $x\\in\\mathscr{X}$ , $f\\in\\mathcal H$ , and $\\boldsymbol{v}\\in\\mathbb{R}^{q}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(x)^{\\top}v=\\langle f(x),v\\rangle_{\\mathbb{R}^{q}}=\\langle f(\\cdot),K(\\cdot,x)v\\rangle_{\\mathcal{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle_{\\mathcal{H}}$ is the inner product of the Hilbert space $\\mathcal{H}$ . For any fixed $x\\in\\mathscr{X}$ and $v\\in\\mathbb{R}^{q},K(\\cdot,x)v$ is a function mapping from $\\mathcal{X}\\mapsto\\mathbb{R}^{q}$ , as is required to be an element of $\\mathcal{H}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Convexity of the Functional Objective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now turn to the analysis of the functional objective $L_{F}$ given in Equation (2). We fix an RKHS $\\mathcal{H}$ over which to minimize $L_{F}$ for now, specializing to the particular choice of $\\mathcal{H}$ based on the neural tangent kernel subsequently. Let $\\ell(x,f(x))=\\mathrm{KL}\\,[P(\\Theta\\mid X=x)\\mid\\mid Q(\\Theta;f(x))].$ . The functional $L_{F}$ then has the form $L_{F}=\\mathbb{E}_{P(X)}\\ell(X,f(X))$ ; we will use this form subsequently for our neural tangent kernel analysis. Our first result shows that targeting $L_{F}$ is highly desirable theoretically: $L_{F}$ admits a unique global minimizer if the variational family $Q$ is an exponential family, as is common practice in VI. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. Suppose that $Q(\\Theta;\\eta)$ is an exponential family distribution in minimal representation with natural parameters $\\eta,$ , sufficient statistics $T(\\theta)$ , and density $q(\\theta;\\eta)$ with respect to Lebesgue measure $\\lambda(\\Theta)$ . Then, for any observation $x\\in\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , the loss function ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell(x,\\eta)=\\mathrm{KL}\\left[P(\\Theta\\mid X=x)\\mid\\mid Q(\\Theta;\\eta)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is strictly convex in $\\eta,$ provided that $P(\\Theta\\mid X=x)\\ll Q(\\Theta;\\eta)\\ll\\lambda(\\Theta)$ for all $\\eta\\in\\mathcal{Y}\\subseteq\\mathbb{R}^{q}$ . ", "page_idx": 3}, {"type": "text", "text": "A proof of Lemma 1, which follows quickly from the convexity of the log partition function in the natural parameter (Wainwright and Jordan, 2008, Proposition 3.1), is provided in Appendix A. Lemma 1 shows the strict convexity of the function $\\ell$ in $\\eta$ . This implies the strict convexity of the functional $L(f)=\\mathbb{E}_{P(X)}\\ell(X,f(X))$ in $f$ by the linearity of expectation, which in turn implies the existence of at most one global minimizer. ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. Suppose that $Q(\\Theta;\\eta)$ is an exponential family distribution. Then, under the conditions of Lemma $^{\\,I}$ , the functional objective ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{F}(f):=\\mathbb{E}_{P(X)}{\\mathrm{KL}}\\left[P(\\Theta\\mid X)\\mid\\mid Q(\\Theta;f(X))\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is strictly convex in $f$ . Consequently, the set of global minimizers of $L_{F}$ is either a singleton set or empty. ", "page_idx": 3}, {"type": "text", "text": "We will assume that the set of global minimizers is nonempty (so that the minimization of $L_{F}$ is well-posed) and let $f^{*}$ denote the global minimizer. We also assume that $\\vert\\vert f^{*}\\vert\\vert_{\\mathcal{H}}<\\infty$ so that $f^{*}\\in\\mathcal{H}$ . Hereafter, we use the term \u201cunique\u201d to mean unique almost everywhere with respect to $P(X)$ . Furthermore, in a slight abuse of notation, $f^{*}$ will denote the unique equivalence class of functions that minimize $L(f)$ . ", "page_idx": 3}, {"type": "text", "text": "Whereas Lemma 1 establishes the convexity of the (non-amortized) forward KL divergence, Corollary 1 establishes the convexity of $L_{F}$ , an amortized objective, in function space. The convexity of the functional objective $L_{F}$ holds regardless of the distribution chosen for the outer expectation by the same linearity argument. Choices other than $P(X)$ , however, may not permit unbiased gradient estimation, as is the case for the wake-phase updates of RWS (Section 2.1). ", "page_idx": 3}, {"type": "text", "text": "4 Global Optima of the Parametric Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In practice, we must directly minimize $L_{P}$ rather than $L_{F}$ , as optimizing the latter over the infinitedimensional space $\\mathcal{H}$ directly is not tractable. Thus, in the second phase of our analysis, we consider convergence to $f^{*}$ by minimizing the parametric objective $L_{P}$ with gradient descent. We define $\\phi$ across continuous time as in Equation (3). Continuous-time dynamics simplify theoretical analysis; SGD with unbiased gradients follows a (noisy) Euler discretization of the continuous ODE (Santambrogio, 2017; Yang et al., 2021). Considering $X\\sim P(X)$ for the outer expectation in both $L_{P}$ and $L_{F}$ is key in this context: this choice enables unbiased stochastic gradient estimation for $L_{P}$ (see Appendix B), whereas other choices require approximations that result in biased gradient estimates (see Section 2.1) and thus follow different gradient dynamics. ", "page_idx": 4}, {"type": "text", "text": "Analysis of the trajectories of the parametric objective $L_{P}$ throughout its minimization initially seems infeasible: the argument of this objective is the neural network parameters $\\phi$ , and even well-behaved loss functions such as the mean squared error (MSE) are nonconvex in these parameters. Nevertheless, neural tangent kernel (NTK)-based results enable analysis of $L_{P}$ . We bridge the divide between the minimizers of the convex functional $L_{F}$ and the nonconvex objective $L_{P}$ using the limiting kernel, and show that in the large-width limit, the optimization path of $L_{P}$ converges arbitrarily close to $f^{*}$ , the unique minimizer of the functional objective $L_{F}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Consider the width- $p$ scaled 2-layer ReLU network, evolving via the flow ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{f}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\phi(t)}^{p}(x,X)\\ell^{\\prime}(X,f_{t}(X)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{t}$ denotes $f(\\cdot,\\phi(t))$ . Let $f^{*}$ denote the unique minimizer of $L=L_{F}$ from Lemma $^{\\,l}$ , and fix $\\epsilon>0$ . Then, under conditions $(C l){-}(C4),(D I){-}(D4),$ , and $(E I)\u2013(E5),$ , there exists $T>0$ such that almost surely ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left[\\operatorname*{lim}_{p\\to\\infty}L(f_{T})\\right]\\leq L(f^{*})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Regularity conditions (C1)\u2013(C4), (D1)\u2013(D4), and (E1)\u2013(E5) are provided in Appendices C, D, and E, respectively. We consider a scaled two-layer ReLU network architecture (further detailed in Appendix C) and use this simple architecture to prove the results as the network width $p$ tends to infinity. Our results may also be extended to multilayer perceptrons with other activation functions. Below, we briefly sketch the key ingredients needed to prove Theorem 1. ", "page_idx": 4}, {"type": "text", "text": "Recall the NTK $K_{\\phi}^{p}$ from Equation (4), where we now let $p$ denote the network width. For certain neural network architectures, Jacot et al. (2018) show that as the network width $p$ tends to infinity, the neural tangent kernel becomes stable and tends (pointwise) towards a fixed, positive-definite limiting neural tangent kernel $K_{\\infty}$ . ", "page_idx": 4}, {"type": "text", "text": "Under suitable positivity conditions on the limiting kernel, we take the domain $\\mathcal{H}$ of $L_{F}$ to be the RKHS with kernel $K_{\\infty}$ (Section 2.3). Because $L_{F}$ has a unique minimizer $f^{*}$ , under mild conditions on $K_{\\infty}$ , $f^{*}$ may be characterized as the solution obtained by following kernel gradient flow dynamics in $\\mathcal{H}$ , that is, the ODE given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{f}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\infty}(x,X)\\ell^{\\prime}(X,f_{t}(X)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, starting from some function $f_{0}$ , following the limiting NTK gradient flow dynamics above minimizes the functional objective $L_{F}$ for sufficiently large $T$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2. Let $f^{*}$ denote the minimizer of $L_{F}$ from Lemma $^{\\,l}$ , and $\\epsilon>0$ . Fix $f_{0}$ , and let $K_{\\infty}$ denote the limiting neural tangent kernel. Let $f_{0}$ evolve according to the dynamics ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{f}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\infty}(x,X)\\ell^{\\prime}(X,f_{t}(X)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Suppose that the conditions of Lemma $^{\\,l}$ and $(E I)\u2013(E3)$ hold. Then, there exists $T>0$ such that $L(\\bar{f}_{T})\\leq L(f^{*})+\\epsilon$ , where $L$ is the loss functional of $L_{F}$ . ", "page_idx": 4}, {"type": "text", "text": "Appendix E enumerates regularity conditions (E1)\u2013(E3) and provides a proof of Lemma 2. The characterization of $f^{*}$ in Lemma 2 clarifies how the analysis of the parametric objective $L_{P}$ will proceed. The gradient flow $L_{P}$ causes the network function to similarly evolve according to a kernel ", "page_idx": 4}, {"type": "text", "text": "gradient flow via the empirical neural tangent kernel, that is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\dot{f}(x;\\phi(t))=-\\mathbb{E}_{P(X)}K_{\\phi(t)}^{p}(x,X)\\ell^{\\prime}(X,f(X;\\phi(t))),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "as derived in Section 2.2. Comparison of the minimizers of $L_{P}$ and $L_{F}$ can be accomplished by comparing the two gradient flows above, i.e. kernel gradient flow dynamics that follow $\\bar{K}_{\\phi(t)}^{p}$ and $K_{\\infty}$ , respectively. As $K_{\\phi(t)}^{p}\\to K_{\\infty}$ (Appendix D), these trajectories should not differ greatly: for any fixed $T$ , the functions obtained by following the kernel gradient dynamics with K\u03d5(t) and K\u221ecan be made arbitrarily close to one another, provided $p$ is sufficiently large. The proof of Theorem 1 first selects a $T$ using Lemma 2, and then bounds the difference in the trajectories on $[0,T]$ for sufficiently large width $p$ by convergence of the kernels K\u03d5p(t) \u2192K\u221e. Our proof differs from previous results in that it relies on uniform convergence of kernels (cf. Appendices $\\mathbf{C}$ and D), enabling the analysis of population quantities such as $\\bar{\\mathbb{E}_{P(X)}}\\ell(X,f(X))$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 proves convergence to an $\\epsilon$ -neighborhood of the global solution when optimizing $L_{P}$ despite the highly nonconvex nature of this optimization problem in the network parameters $\\phi$ . For sufficiently flexible network architectures, optimization of $L_{P}$ thus behaves similarly to that of $L_{F}$ , which we have shown is a convex problem in the function space $\\mathcal{H}$ in Section 3. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having established conditions under which global convergence is guaranteed, the main aim of our experiments is to demonstrate approximate global convergence in practice, even for scenarios where the conditions assumed in our proofs are not satisfied exactly. Section 5.1 demonstrates that finite-neuron layer widths used in practice approximate the limiting behavior well, while Section 5.2 and Section 5.3 utilize problem-specific network architectures for amortized inference. Our results suggest that there may exist weaker assumptions under which global convergence is still guaranteed. ", "page_idx": 5}, {"type": "text", "text": "5.1 Toy Example ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first assess whether the asymptotic regime of Theorem 1 is relevant to practice with layers of finite width. We use a diagnostic motivated by the lazy training framework of Chizat et al. (2019), which provides the intuition that in the limiting NTK regime, the function $f$ behaves much like its linearization around the initial weights $\\phi_{0}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x;\\phi)\\approx f(x;\\phi_{0})+J_{\\phi}f(x;\\phi_{0})(\\phi-\\phi_{0}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Liu et al. (2020) prove that equality holds exactly in the equation above if and only if $f(x;\\phi)$ has a constant tangent kernel (i.e., $K_{\\infty},$ ). Therefore, similarity between $f$ and its linearization indicates that the asymptotic regime of the limiting NTK is approximately achieved. Note that even if $f$ is linear in $\\phi$ , as in the above expression, it may still be highly nonlinear in $x$ . ", "page_idx": 5}, {"type": "text", "text": "We consider a toy example for which $||x||_{2}=1$ . The generative model first draws a rotation angle $\\Theta$ uniformly between 0 and $2\\pi$ , and then a rotation perturbation $Z\\sim\\mathcal{N}(0,\\sigma^{2})$ , where we take $\\sigma=0.5$ . Conditional on $\\Theta$ and $Z$ , the data $x$ is deterministic: $x=\\left[\\cos(\\theta+z),\\sin(\\theta+z)\\right]^{\\top}$ . This construction ensures that the data lie on the sphere $\\mathbb{S}^{1}\\subset\\mathbb{R}^{2}$ , which guarantees the positivity of the limiting NTK for certain architectures (Jacot et al., 2018). We aim to infer $\\Theta$ given a realization $x$ , marginalizing over the nuisance latent variable $Z$ . Our variational family $Q(\\Theta;{\\bar{f}}(x))$ is a von Mises distribution, whose support is the interval $[0,2\\pi]$ . This family is an exponential family distribution, allowing for the application of Lemma 1. The encoder network $f(\\cdot;\\phi)$ is given by a dense two-layer network (that is, one hidden layer) with rectified linear unit (ReLU) activation, which we study as the network width $p$ grows. The network outputs $f(x;\\phi)$ parameterize the natural parameter $\\eta$ . ", "page_idx": 5}, {"type": "text", "text": "We fit the neural network $f(x;\\phi)$ in two ways. First, we use SGD to fit the network parameters $\\phi$ to minimize $L_{P}$ . Second, we fit the linearization $f_{\\mathrm{lin}}(x;\\phi)\\,=\\,f(x;\\phi_{0})+J_{\\phi}f(x;\\phi_{0}\\bar{)}(\\phi-\\phi_{0})$ in $\\phi$ . We perform both of these fitting procedures for various widths $p$ . For both settings, stochastic gradient estimation was performed by following the procedure in Appendix B. For evaluation, we fix $N\\,=\\,1000$ independent realizations $x_{1}^{*},\\ldots,x_{N}^{*}$ from the generative model with underlying ground-truth latent parameter values $\\theta_{1}^{*},\\ldots,\\theta_{N}^{*}$ , and evaluate the held-out negative log-likelihood (NLL), $\\begin{array}{r}{-\\frac{1}{N}\\sum_{i=1}^{N}\\log q\\left(\\theta_{i}^{*}\\mid f(x_{i}^{*};\\phi)\\right)}\\end{array}$ , for both functions: $f(x;\\phi)$ and $f_{\\mathrm{lin}}(x;\\phi)$ . Figure 1 shows the evolution of the held-out NLL across the fitting procedure for three different network widths p: 64, 256 and 1024. The difference in quality between the linearizations and the true functions at convergence diminishes as the width $p$ grows; for $p=1024$ , the two are nearly identical, providing evidence that the asymptotic regime is achieved. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "8x48XFLvyd/tmp/4fe75f4cef2165e059b639ed646926aec4e44a90f4cd8174f42c9de19ac02fe7.jpg", "img_caption": ["Figure 1: Negative log-likelihood across gradient steps, for network widths 64, 256, and 1024 neurons. NLL for the exact posterior is denoted by the red line. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Label Switching in Amortized Clustering ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this experiment and the subsequent one, we consider a more diverse set of network architectures. Although Theorem 1 assumes a shallow, dense network architecture, these experimental results show that empirically, global convergence can be obtained for many other architectures as well. We consider the difficult problem of amortizing clustering. In this problem, we are given an unordered collection (set) of data $x\\,=\\,\\{x_{1},\\ldots,x_{n}\\}$ , $x_{i}\\in\\mathbb{R}$ and our objective is to infer the locations and labels of the cluster centers from which the data were generated. The generative model first draws a shift parameter $S\\sim\\mathcal{N}(0,100^{2})$ followed by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{Z\\mid(S=s)\\sim\\mathcal{N}\\left(\\left[\\mu_{1}+s,\\ldots,\\mu_{d}+s\\right]^{\\top},\\sigma^{2}I_{d}\\right)}}}\\\\ {{\\displaystyle{X_{i}\\mid(Z=z)\\stackrel{i i d}{\\sim}\\sum_{j=1}^{d}p_{j}\\mathcal{N}(z_{j},\\tau^{2})}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the above, $\\sigma^{2},\\tau^{2},\\mu\\in\\mathbb{R}^{d}$ and $p\\in\\mathbb{R}^{d}$ are known variances, locations, and proportions, respectively. The global parameter $s\\in\\mathbb{R}$ shifts the $d$ locations $\\mu_{1},\\ldots,\\mu_{d}$ to $\\mu_{1}+s,\\ldots,\\mu_{d}+s$ . The cluster centers $Z$ are then obtained by adding noise to these locations. An implicit labeling is imposed on the centers by assigning each a dimension in $\\mathbb{R}^{d}$ via the prior. Finally, the data are drawn independently from a mixture of $d$ univariate Gaussians with centers $Z_{i}$ , $i=1,\\ldots,d$ . We consider the tasks of inferring the scalar shift $S$ and the vector of cluster centers $Z$ . We fix $\\mu=[-20,-10,0,10,20]^{\\top}$ with $d=5$ . We fix the hyperparameters $\\sigma=0.5$ and $\\tau=0.1$ , and artificially fix the shift as $S=100$ to generate $n=1000$ independent realizations $X=x$ from the generative model. ", "page_idx": 6}, {"type": "text", "text": "Inferring $S$ should be straightforward because the vector $\\mu$ is known and fixed, ensuring that the joint likelihood $p(x,s)$ (marginalizing over $z$ ) is unimodal in $s$ . However, inference on $Z$ may be difficult for likelihood-based methods because the order of the entries of $Z$ matters: the joint density $p(x,z,s)\\neq p(x,\\pi(z),s)$ for a permutation $\\pi$ , even though $p(x\\mid z)=p(x\\mid\\pi(z))$ . \u201cLabel switching\u201d can thus pose a significant obstacle for likelihood-based methods, as any permutation of the cluster centers $Z$ will still explain the data well. This problem formulation thus results in a likelihood, and hence a posterior density, with many local optima but a single global optimum (i.e., where the cluster centers have the correct labeling). ", "page_idx": 6}, {"type": "text", "text": "Now we show that likelihood-based approaches to variational inference, such as maximizing the evidence lower-bound (ELBO), result in suboptimal solutions compared to the minimization of $L_{P}$ . We take the variational distributions $q(S;f_{1}(x;\\phi_{1}))$ and $q(Z;f_{2}(x;\\phi_{2}))$ to be Gaussian. We fit the networks $f_{1}$ and $f_{2}$ . Due to the exchangeability of the observations $\\dot{x}=\\{x_{1},\\ldots,x_{n}\\}$ , we parameterize each as permutation-invariant neural networks, ftiting both $\\phi_{1}$ and $\\phi_{2}$ to either minimize $L_{P}$ or to maximize the ELBO (see Appendix F). We perform 100 replications of this experiment across different random seeds, and consider two different parameterizations of the Gaussian variational distribution: a mean-only parameterization with fixed unit variance and a natural parameterization with an unknown mean and unknown variance. Both of these variational families are exponential families, and so convexity (in the sense of Corollary 1) holds. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Figure 2 plots kernel-smoothed frequencies of point estimates of $S$ , where each point estimate is the mode of the variational posterior for an experimental replicate. Both ELBO- and $L_{P}$ -based training estimate $S=100$ well. However, the limitations of ELBO-based training are evident in the fidelity of the posterior approximation of $Z$ . Table 2 plots the average $\\ell_{1}$ distance $||\\hat{Z}-Z||_{1}$ between the variational mode $\\hat{Z}$ and the true latent draw $Z$ . Optimization of the ELBO converges to a local optimum that is a permutation of the entries of $Z$ , resulting in a large $\\ell_{1}$ distance on average. Minimization of $L_{P}$ , on the other hand, converges to the global optimum without any label switching. Table 1 indicates the degree of label switching, showing the proportion of trials in which the entries $\\hat{Z}$ were correctly ordered. ", "page_idx": 7}, {"type": "table", "img_path": "8x48XFLvyd/tmp/49e3b9083daee06658a3bb3adae32f1365a15476bfb888446e7860408e1f329e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "8x48XFLvyd/tmp/6f49b91e4aad9795d1bb877d7e1ba7cf20c6095c2e3e6410f9dfc3382ba54c95.jpg", "table_caption": ["Table 1: Proportion out of one hundred replicates where posterior mode of $q(Z;f_{2}(x;\\bar{\\phi_{2}}))$ was a vector in increasing order. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Average $\\ell_{1}$ distance $||\\hat{Z}-Z||_{1}$ (and std. deviations) across one hundred replicates. ", "page_idx": 7}, {"type": "image", "img_path": "8x48XFLvyd/tmp/f209e496b10b67322add49058c7542b7a7bf2bcf6e24c85e0ec58addf54c68c4.jpg", "img_caption": ["Figure 2: Mode of $q(S;f_{1}(x;\\phi_{1}))$ across experimental replicates. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Rotated MNIST Digits ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We consider the task of inferring a shared rotation angle $\\Theta$ for a set of $N$ MNIST digits. The generative model is as follows. The rotation angle is drawn as $\\Theta\\sim\\operatorname{Uniform}(0,2\\pi)$ , and for all $\\bar{i}\\in[N]$ a noise term is drawn as $Z_{i}\\sim\\mathcal{N}(0_{p},\\sigma^{2}\\tilde{I_{p}})$ . Finally, each image is drawn as ", "page_idx": 7}, {"type": "equation", "text": "$$\nX_{i}\\mid(Z_{i}=z_{i},\\Theta=\\theta)\\sim\\mathcal{N}\\left(\\mathtt{R o t a t e M N I S T}(z_{i},\\theta),\\tau^{2}\\right)\\ i\\in[N].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here, RotateMNIST is fitted ahead of time and fixed throughout this experiment. Given a latent representation $z$ and angle $\\theta$ , RotateMNIST returns a $28\\times28$ MNIST digit image rotated counterclockwise by $\\theta$ degrees. (See Appendix F for additional experimental details.) We aim to fit the variational posterior $q(\\Theta;f(x_{1},\\dots,x_{N};\\phi))$ , implicitly marginalizing over the nuisance latent variables $Z_{1},\\ldots,Z_{N}$ . The true data $\\{x_{i}\\}_{i=1}^{N}$ are generated from the above model, with $N=1000$ digits and an underlying counterclockwise rotation angle of $\\theta=260$ degrees. We provide visualizations of some of these digits in Figure 3. The variational distribution $q(\\Theta;f(x_{1},\\dots,\\bar{x}_{N};\\phi))$ is taken to be a von Mises distribution with natural parameterization, as in Section 5.1, although for this example the architecture of the encoder network makes $f$ invariant to permutations of the inputs $\\{x_{i}\\}_{i=1}^{N}$ , to reflect the exchangeability of the data. We fit $f$ to minimize $L_{P}$ using 100,000 iterations of SGD. ", "page_idx": 7}, {"type": "text", "text": "We compare to ftiting $\\theta$ to directly maximize the likelihood of the data $p(\\theta,\\{x_{i}\\}_{i=1}^{N})$ . As this quantity is intractable, we maximize the importance-weighted bound (IWBO), which is a generalization of the ELBO (Burda et al., 2016), by maximizing the joint likelihood $p(\\stackrel{\\cdot}{\\theta},\\{z_{i}\\}_{i=1}^{N},\\overleftarrow{x_{i}}\\}_{i=1}^{N})$ in $\\theta$ while fitting a Gaussian variational distribution on the variables $Z$ . ", "page_idx": 7}, {"type": "text", "text": "The likelihood function is multimodal in $\\theta$ due to the approximate rotational symmetry of several handwritten digits: zero, one, and eight are approximately invariant under rotations of 180 degrees. Additionally, the digits six and nine are similar following 180-degree rotations. These symmetries yield a multimodal posterior distribution on $\\Theta$ . Likelihood-based fitting procedures, such as maximizing the IWBO, often get stuck in these shallow local optima, while fitting $f$ to minimize the parametric objective $L_{P}$ finds a unique global solution. Figure 4 shows estimates of the angle $\\theta$ conditional on the data $\\{x_{i}\\}_{i=1}^{N}$ during training with the IWBO objective, with $\\theta$ initialized to a variety of values. For some initializations, the IWBO optimization converges quickly to near the correct value of 260 degrees, but in many others, it converges to a shallow local optimum. ", "page_idx": 7}, {"type": "image", "img_path": "8x48XFLvyd/tmp/a932ab02385523c981b958d44a58639e6f7afb761635d07fdf409b00dcb73311.jpg", "img_caption": ["Figure 3: 100 of the ${\\cal N}\\ =$ 1000 data observations with counterclockwise rotation of 260 degrees. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "8x48XFLvyd/tmp/7ef4c62de68c375dc9528e8f72bc05808db9fbad5c4e229096fb667d21191722.jpg", "img_caption": ["Figure 4: Estimate of angle $\\theta$ across gradient steps, with fitting performed to maximize the IWBO. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We perform the same routine, fitting $q(\\Theta;f(x_{1},\\dots,x_{N};\\phi))$ to minimize the expected forward KL divergence $L_{P}$ . Figure 5 shows that across a variety of initializations of the angles, this approach always converges to a unique solution and ftis the posterior mode to the correct value of 260 degrees. $L_{P}$ minimization converges rapidly, and so Figure 6 zooms in on the initial few thousand gradient steps to show the various trajectories among initializations. ", "page_idx": 8}, {"type": "image", "img_path": "8x48XFLvyd/tmp/e1b95cdb0b6760cb4e8c55cdfbb4990e0443eb29f3d403f98eec20e5619a1536.jpg", "img_caption": ["Figure 5: Mode of $q(\\Theta;f(x_{1},\\dots,x_{N};\\phi))$ across training (starting at different initializations) when minimizing objective $L_{P}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "8x48XFLvyd/tmp/e9b428e6e8852276bb35f7ed464b6fb6e6e93354613901000e0f0dbb09466216.jpg", "img_caption": ["Figure 6: Zoomed-in trajectories across the first 2000 gradient steps, showing similar estimates regardless of initialization. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Local Optima vs. Global Optima ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this experiment, we directly compare the quality of the variational approximation minimizing the expected forward KL objective to that of the local optima found by optimizing the ELBO. We consider an adapted version of the rotated MNIST digit problem outlined above. Each digit $x_{i}$ is drawn from the model $x_{i}\\sim\\mathcal{N}$ Rotate $\\left(x_{i}^{0},\\theta\\right),\\tau^{2})$ for $i=1,\\dots,50$ with angle set to $\\theta_{\\mathrm{true}}=260$ degrees. The unrotated digits $\\boldsymbol{x}_{i}^{0}$ are fixed a priori, eliminating the nuisance latent variables $Z$ in this setting. This allows us to directly perform ELBO-based variational inference on $\\Theta$ , instead of merely maximizing the likelihood (or a bound thereof) as in Section 5.3. We fit an amortized Gaussian variational distribution with fixed variance $\\sigma^{2}=0.5^{2}$ for inference on $\\Theta$ , and use the same prior as above. This is an exponential family with only one location parameter to be learned. ", "page_idx": 8}, {"type": "text", "text": "Fitting $q(\\Theta;f(x_{1},\\dots,x_{50};\\phi))$ is performed by minimizing either the negative ELBO or the expected forward KL divergence. The only differences between the two approaches are their objective functions: the data, network architecture, learning rates, and all other hyperparameters are the same. We optimize each objective function over 10,000 gradient steps, and measure the quality of the obtained variational approximations by a variety of metrics, including: the (non-expected) forward KL divergence; the reverse KL divergence; negative log-likelihood; and the angle point estimate. Intuition suggests that a global optimum should outperform local ones, and we find this to be the case. By any of the performance metrics we consider, the global solution of the expected forward KL minimization outperforms the variational approximations found by optimizing the ELBO. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "8x48XFLvyd/tmp/eca037e1165ca85924a3d811a74a8ba012ef39c459f939503f589d3932b548f1.jpg", "img_caption": ["Figure 7: Forward and reverse KL divergences to the true posterior across fitting for minimization of the expected forward KL (blue) or the negative ELBO (green). We also plot the negative log likelihood of the true angle, as well as the variational mode (true angle $\\theta_{\\mathrm{true}}$ is plotted in red.) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we showed that in the asymptotic limit of an infinitely wide neural network, gradient descent dynamics on the expected forward KL objective $L_{P}$ converge to an $\\epsilon_{}$ -neighborhood of a unique function $f^{*}$ , a global minimizer. Our results depend on several regularity conditions, the most important of which is the positive definiteness of the limiting neural tangent kernel, the compactness of the data space $\\mathcal{X}$ , and the specific architecture considered, a single hidden-layer ReLU network. We conjecture and show experimentally that global convergence holds more generally. First, we illustrate that the asymptotic regime describes practice with a finite number of neurons (see Section 5.1). Second, our conditions allow for a wide variety of activation functions beyond ReLU (see Appendix D). Third, empirically, global convergence can be achieved with many network architectures. Beyond multilayer perceptrons, we illustrate global convergence for convolutional neural networks (CNNs) and permutation-invariant architectures in Section 5.2 and Section 5.3. ", "page_idx": 9}, {"type": "text", "text": "Expected forward KL minimization is a likelihood-free inference (LFI) method. For Bayesian inference, likelihood-based and likelihood-free methods are not typically viewed as competitors, but as different tools for different settings. In a setting where the likelihood is available, the prevailing wisdom suggests utilizing it. However, our results suggest that likelihood-free approaches to inference may be preferable even when the likelihood function is readily available. We find likelihood-based methods are prone to suffer the shortcomings of numerical optimization, often converging to shallow local optima of ELBO-based variational objectives. Expected forward KL minimization instead converges to a unique global optimum of its objective function. ", "page_idx": 9}, {"type": "text", "text": "There are situations in which ELBO optimization may nevertheless be preferable. First, if the likelihood function of the model is approximately convex and well-conditioned in the region of interest, ELBO optimization should recover a nearly global optimizer. Second, in certain situations, the ELBO can be optimized using deterministic optimization methods, which can be much faster than SGD. Third, if the generative model has free model parameters, with the ELBO they can be fitted while simultaneously fitting the variational approximation, with a single objective function for both tasks. Fourth, the ELBO can be applied with non-amortized variational distributions, which can have computational beneftis in settings with few observations. Many important inference problems do not fall into any of these four categories. Even for those that do, the benefits of expected forward KL minimization, including global convergence, may outweigh the benefits of ELBO optimization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the reviewers for their helpful comments and suggestions. This material is based on work supported by the National Science Foundation under Grant Nos. 2209720 (OAC) and 2241144 (DGE), and the U.S. Department of Energy, Office of Science, Office of High Energy Physics under Award Number DE-SC0023714. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Altosaar, J., Ranganath, R., and Blei, D. (2018). Proximity variational inference. In International Conference on Artificial Intelligence and Statistics.   \nAmbrogioni, L., G\u00fc\u00e7l\u00fc, U., Berezutskaya, J., van den Borne, E., G\u00fc\u00e7l\u00fct\u00fcrk, Y., Hinne, M., Maris, E., and van Gerven, M. (2019). Forward amortized inference for likelihood-free variational marginalization. In International Conference on Artificial Intelligence and Statistics.   \nBa, J., Erdogdu, M., Suzuki, T., Wu, D., and Zhang, T. (2020). Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations.   \nBlei, D. M., Kucukelbir, A., and McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877.   \nBornschein, J. and Bengio, Y. (2015). Reweighted wake-sleep. In International Conference on Learning Representations.   \nBurda, Y., Grosse, R. B., and Salakhutdinov, R. (2016). Importance weighted autoencoders. In International Conference on Learning Representations.   \nCarmeli, C., De Vito, E., and Toigo, A. (2006). Vector valued reproducing kernel Hilbert spaces of integrable functions and Mercer theorem. Analysis and Applications, 4(4):377\u2013408.   \nCarmeli, C., Vito, E. D., Toigo, A., and Umanit\u00e0, V. (2008). Vector valued reproducing kernel Hilbert spaces and universality.   \nChizat, L., Oyallon, E., and Bach, F. (2019). On lazy training in differentiable programming. In Neural Information Processing Systems.   \nDomke, J. (2020). Provable smoothness guarantees for black-box variational inference. In International Conference on Machine Learning.   \nDomke, J., Gower, R. M., and Garrigos, G. (2023). Provable convergence guarantees for black-box variational inference. In Neural Information Processing Systems.   \nDomke, J. and Sheldon, D. R. (2018). Importance weighting and variational inference. In Neural Information Processing Systems.   \nDragomir, S. S. (2003). Some Gronwall Type Inequalities and Applications. Nova Science Publishers.   \nFazelnia, G. and Paisley, J. (2018). CRVI: Convex relaxation for variational inference. In International Conference on Machine Learning.   \nGhadimi, S. and Lan, G. (2015). Stochastic approximation methods and their finite-time convergence properties. In Handbook of Simulation Optimization, pages 149\u2013178. Springer.   \nHoffman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference. Journal of Machine Learning Research, 14(40):1303\u20131347.   \nJacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural networks. In Neural Information Processing Systems.   \nKim, K., Oh, J., Wu, K., Ma, Y., and Gardner, J. R. (2023). On the convergence of black-box variational inference. In Neural Information Processing Systems.   \nLe, T. A., Kosiorek, A. R., Siddharth, N., Teh, Y. W., and Wood, F. (2019). Revisiting reweighted wake-sleep for models with stochastic control flow. In International Conference on Uncertainty in Artificial Intelligence.   \nLee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. (2019). Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning.   \nLiu, C., Zhu, L., and Belkin, M. (2020). On the linearity of large non-linear models: when and why the tangent kernel is constant. In Neural Information Processing Systems.   \nLiu, R., McAuliffe, J. D., Regier, J., and the LSST Dark Energy Science Collaboration (2023). Variational inference for deblending crowded starfields. Journal of Machine Learning Research, 24(179):1\u201336.   \nMasrani, V., Le, T. A., and Wood, F. (2019). The thermodynamic variational objective. In Neural Information Processing Systems.   \nOwen, A. B. (2013). Monte Carlo Theory, Methods and Examples.   \nPapamakarios, G. and Murray, I. (2016). Fast $\\epsilon$ -free inference of simulation models with bayesian conditional density estimation. In Neural Information Processing Systems.   \nPapamakarios, G., Sterratt, D., and Murray, I. (2019). Sequential neural likelihood: Fast likelihoodfree inference with autoregressive flows. In International Conference on Artificial Intelligence and Statistics.   \nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., et al. (2019). PyTorch: An imperative style, high-performance deep learning library.   \nRanganath, R., Gerrish, S., and Blei, D. (2014). Black box variational inference. In International Conference on Artificial Intelligence and Statistics.   \nSantambrogio, F. (2017). Euclidean, metric, and Wasserstein gradient flows: an overview. Bulletin of Mathematical Sciences, 7(1):87\u2013154.   \nShapiro, A. (2003). Monte Carlo sampling methods. In Stochastic Programming, volume 10 of Handbooks in Operations Research and Management Science, pages 353\u2013425. Elsevier.   \nSrivastava, M. K., Khan, A. H., and Srivastava, N. (2014). Statistical Inference: Theory of Estimation. PHI Learning.   \nWainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1\u2013305.   \nYang, J., Hu, W., and Li, C. J. (2021). On the fast convergence of random perturbations of the gradient flow. Asymptotic Analysis, 122:371\u2013393.   \nZhang, F. and Gao, C. (2020). Convergence rates of variational posterior distributions. The Annals of Statistics, 48(4):2180 \u2013 2207. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Convexity of the Functional Objective ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We prove Lemma 1 from the manuscript below. ", "page_idx": 12}, {"type": "text", "text": "Proof. Let the log-density be given by $\\log q(\\theta;\\eta)=\\log h(\\theta)+\\eta^{\\top}T(\\theta)-A(\\eta)$ . First, observe that under the conditions given, the function $\\ell$ is equivalent (up to additive constants) to a much simpler expression, the expected log-density of $q$ , via ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ell(x,\\eta)=\\mathrm{KL}\\bigg[P(\\Theta|X=x)\\;\\vert\\vert\\;Q(\\Theta;\\eta)\\bigg]}&{}\\\\ {=H_{x}-\\mathbb{E}_{P(\\Theta|X=x)}\\log q(\\theta;\\eta)}&{}\\\\ {=-\\mathbb{E}_{P(\\Theta|X=x)}\\log q(\\theta;\\eta)+C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $C$ is a constant that does not depend on $\\eta$ . Now, the mapping $\\eta\\rightarrow-\\log q(\\theta;\\eta)$ is convex in $\\eta$ because its Hessian is $\\begin{array}{r}{\\frac{\\partial A}{\\partial\\eta\\partial\\eta^{\\top}}=\\mathrm{Var}(T(\\theta))\\succ0}\\end{array}$ (cf. Chapter 6.6.3 of Srivastava et al. (2014)). We can show $\\ell$ is convex in $\\eta$ by applying linearity of expectation. We have for any $\\lambda\\in[0,1]$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(x,\\lambda\\eta_{1}+(1-\\lambda)\\eta_{2})=-\\mathbb{E}_{P(\\Theta|X=x)}\\log q(\\Theta;\\lambda\\eta_{1}+(1-\\lambda)\\eta_{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq-\\mathbb{E}_{P(\\Theta|X=x)}\\bigl(\\lambda\\log q(\\Theta;\\eta_{1})-(1-\\lambda)\\log q(\\Theta;\\eta_{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\lambda\\ell(x,\\eta_{1})+(1-\\lambda)\\ell(x,\\eta_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the second line follows from the convexity of the map $\\eta\\mapsto-\\log q(\\theta;\\eta)$ above for any value of $\\theta$ . So the function $\\ell(x,\\eta)$ is strictly convex in $\\eta$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "B Unbiased Stochastic Gradients for the Parametric Objective ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Computation of unbiased estimates of the gradient of the loss function $L_{P}(\\phi)$ with respect to the parameters $\\phi$ is all that is needed to implement SGD for $L_{P}$ . Under mild conditions (see Proposition 1), the loss function $L(\\phi)$ may be equivalently written as ", "page_idx": 12}, {"type": "equation", "text": "$$\nL(\\phi)=\\mathbb{E}_{P(X)}\\mathbb{E}_{P(\\Theta|X)}\\log\\frac{p(\\Theta\\mid X)}{q(\\Theta;f(X;\\phi))}=\\mathbb{E}_{P(\\Theta,X)}\\log\\frac{p(\\Theta\\mid X)}{q(\\Theta;f(X;\\phi))}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for density functions $p,q$ , where $f(\\cdot;\\phi)$ denotes a function parameterized by $\\phi$ . Under the conditions of Proposition 1 differentiation and integration may be interchanged, so that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}L(\\phi)=\\mathbb{E}_{P(\\Theta,X)}\\nabla_{\\phi}\\log\\frac{p(\\Theta\\mid X)}{q(\\Theta;f(X;\\phi))}=-\\mathbb{E}_{P(\\Theta,X)}\\nabla_{\\phi}\\log q(\\Theta;f(X;\\phi))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and unbiased estimates of the quantity can be easily attained by samples drawn $(\\theta,x)\\sim P(\\Theta,X)$ . ", "page_idx": 12}, {"type": "text", "text": "Proposition 1. Let $(\\Omega_{1},\\mathcal{B}_{1})$ , $\\left(\\Omega_{2},B_{2}\\right)$ be measurable spaces on which the random variables $X$ : $\\Omega_{1}\\rightarrow\\mathcal{X}$ and $\\Theta:\\Omega_{2}\\rightarrow{\\mathcal{O}}$ are defined, respectively. Suppose that for all $x\\in\\mathscr{X}$ and all $\\phi\\in\\Phi$ we have $P(\\Theta\\mid X=x)\\ll Q(\\Theta;f(x;\\phi))\\ll\\lambda(\\Theta)$ , with $\\lambda(\\Theta)$ denoting Lebesgue measure and $\\ll$ denoting absolute continuity. Further, suppose that log q(\u0398p;(f\u0398(|XX;)\u03d5)) is measurable with respect to the product space $\\left(\\Omega_{1}\\times\\Omega_{2},B_{1}\\times B_{2}\\right)$ for each $\\phi\\in\\Phi$ , and $\\nabla_{\\phi}\\log\\,q(\\theta\\mid f(x;\\phi))$ exists for almost all $(\\theta,x)\\in\\mathcal{O}\\times\\mathcal{X}$ . Finally, assume there exists an integrable $Y$ dominating $\\nabla_{\\phi}\\log q(\\theta;f(x;\\phi))$ for all $\\phi\\in\\Phi$ and almost all $(\\theta,x)\\in\\mathcal{O}\\times\\mathcal{X}$ . Then for any $B\\in\\mathbb{N}$ and any $\\phi\\in\\Phi$ the quantity ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{\\nabla}(\\phi)=-\\frac{1}{B}\\sum_{i=1}^{B}\\nabla_{\\phi}\\log q(\\theta_{i};f(x_{i};\\phi)),\\;\\;\\;(\\theta_{i},x_{i})\\overset{i i d}{\\sim}P(\\Theta,X)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "is an unbiased estimator of the gradient of the objective $L_{P}$ , evaluated at $\\phi\\in\\Phi$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. By the absolute continuity assumptions, for any $x\\in\\mathscr{X}$ the distributions $P(\\Theta\\mid X=x)$ and $Q(\\Theta;f(\\bar{x};\\phi))$ admit densities with respect to Lebesgue measure denoted $p(\\theta\\mid x)$ and $q(\\theta;f(x;\\phi))$ , ", "page_idx": 12}, {"type": "text", "text": "respectively. We may then rewrite the KL divergence from Equation (1) as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{KL}\\Bigg[P(\\Theta\\mid X=x)\\mid\\mid Q(\\Theta;f(x;\\phi))\\Bigg]:=\\mathbb{E}_{P(\\Theta\\mid X=x)}\\log\\Bigg(\\frac{d P(\\Theta\\mid X=x)}{d Q(\\Theta;f(x;\\phi))}\\Bigg)}}\\\\ &{}&{=\\mathbb{E}_{P(\\Theta\\mid X=x)}\\log\\Bigg(\\frac{p(\\Theta\\mid x)}{q(\\Theta;f(x;\\phi))}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "because the Radon-Nikodym derivative $d P/d Q$ is given by the ratio of these densities. Equation (1) is thus equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P(X)}\\mathbb{E}_{P(\\Theta|X)}\\log\\left(\\frac{p(\\Theta\\mid X)}{q(\\Theta;f(X;\\phi))}\\right)=\\mathbb{E}_{P(\\Theta,X)}\\log\\left(\\frac{p(\\Theta\\mid X)}{q(\\Theta;f(X;\\phi))}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This expectation is well-defined by the measurability assumption on log q(\u0398p;(f\u0398(|XX;)\u03d5)) . To interchange differentiation and integration, it suffices by Leibniz\u2019s rule that the gradient of this quantity with respect to $\\phi$ is dominated by a measurable r.v. $Y$ . More precisely, there exists an integrable $Y(\\theta,x)$ defined on the product space ${\\mathcal{O}}\\times{\\mathcal{X}}$ such that $\\begin{array}{r}{\\left|\\left|\\nabla_{\\phi}\\log\\left(\\frac{p(\\theta|x)}{q(\\theta;f(x;\\phi))}\\right)\\right|\\right|\\leq Y(\\theta,x)}\\end{array}$ for all $\\phi\\in\\Phi$ and almost everywhere- $P(\\Theta,X)$ . This is assumed in the statement of the proposition, and so we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla_{\\phi}\\mathbb{E}_{P(\\Theta,X)}\\log\\left(\\frac{p(\\Theta\\mid X)}{q(\\Theta;f(X;\\phi))}\\right)=\\mathbb{E}_{P(\\Theta,X)}\\nabla_{\\phi}\\log\\left(\\frac{p(\\Theta\\mid X)}{q(\\Theta;f(X;\\phi))}\\right)}}\\\\ &{}&{=-\\mathbb{E}_{P(\\Theta,X)}\\nabla_{\\phi}\\log q(\\Theta;f(X;\\phi))~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the result follows by sampling. ", "page_idx": 13}, {"type": "text", "text": "The variance of the gradient estimator can be reduced at the standard Monte Carlo rate, and for any $B$ Equation (12) can be used for SGD. ", "page_idx": 13}, {"type": "text", "text": "C The Limiting NTK ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Before proceeding, we introduce the architecture specific to our analysis, a scaled two-layer network, and several theorems that we will use throughout the analysis. ", "page_idx": 13}, {"type": "text", "text": "The first result from Shapiro (2003) concerns optimization of the objective $f(x)=\\mathbb{E}_{\\xi\\sim P}F(x;\\xi)$ in $x$ via its empirical approximation $\\begin{array}{r}{\\hat{f}_{n}(x)=\\frac{1}{n}\\sum_{i=1}^{n}F(x;\\xi_{i}),\\xi_{i}\\overset{i i d}{\\sim}P.}\\end{array}$ . We reproduce this result below. ", "page_idx": 13}, {"type": "text", "text": "Theorem 2 (Proposition 7 of Shapiro (2003)). Let $C$ be a nonempty compact subset of $\\mathbb{R}^{n}$ and suppose thats (i) for almost every $\\xi\\in\\Xi$ the function $F(\\cdot,\\xi)$ is continuous on $C$ , (ii) $F(x,\\xi)$ , $x\\in C$ , is dominated by an integrable function, (iii) the sample $\\xi_{1},\\allowbreak\\dots,\\allowbreak\\xi_{n}$ is iid. Then the expected value function $f(x)$ is finite-valued and continuous on $C$ , and $\\hat{f}_{n}(x)$ converges to $f(x)$ with probability $^{\\,l}$ uniformly on $C$ . ", "page_idx": 13}, {"type": "text", "text": "The next two results are integral forms of Gronwall\u2019s inequality that we use in subsequent analysis.   \nWe refer to Dragomir (2003) for a detailed review and summarize the results therein below. ", "page_idx": 13}, {"type": "text", "text": "Theorem 3 (Gronwall\u2019s Inequality, Corollary 3 of Dragomir (2003)). Let $u(t)\\in\\mathbb{R}$ be such that $\\begin{array}{r}{u(t)\\le c_{1}+c_{2}\\int_{0}^{t}u(s)d s}\\end{array}$ for $t>0$ and nonnegative $c_{1},c_{2}$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\nu(t)\\leq c_{1}\\exp[c_{2}t].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem 4 (Theorem 57 of Dragomir (2003)). Let $u(t)~\\in~\\mathbb{R}$ be such that $u(t)~\\leq~c_{1}~+$ $\\begin{array}{r}{c_{2}\\int_{0}^{t}\\int_{0}^{s}u(v)d v d s}\\end{array}$ for $t>0$ and nonnegative $c_{1},c_{2}$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\nu(t)\\leq c_{1}\\exp[c_{2}t^{2}/2].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now we turn to specifics of the architecture we consider. Assume the function $f$ has the architecture of a (scaled) two-layer (single hidden layer) network mapping $f:\\mathcal X\\to\\mathcal Y$ with $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $\\mathcal{V}\\subseteq\\mathbb{R}^{q}$ . We consider this network architecture for a given width $p$ , and study each of the $i\\,=\\,1,\\ldots,q$ ", "page_idx": 13}, {"type": "text", "text": "coordinate functions of $f$ . For a scaled two-layer network, the $i$ th such function is ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\boldsymbol{x};\\boldsymbol{\\phi})_{i}:=\\frac{1}{\\sqrt{p}}\\sum_{j=1}^{p}a_{i j}\\sigma\\left(\\boldsymbol{x}^{\\top}\\boldsymbol{w}_{j}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $i\\,=\\,1,.\\,.\\,,q$ , where $\\sigma$ denotes an activation function. The scaling depends on the width of the network $p$ . The parameters $\\phi$ are thus $\\phi\\,=\\,\\{w_{j},a_{(\\cdot),j}\\}_{j=1}^{p}$ where $a_{(\\cdot),j}$ denotes the vector $[a_{1j},\\dotsc,a_{q j}]^{\\top}$ (i.e. the $j$ th coefficient for each component function $i$ ). The individual parameters have dimensions as follows: $w_{j}\\in\\mathbb{R}^{d}$ and $a_{(\\cdot),j}\\in\\mathbb{R}^{q}$ , for all $j=1,\\hdots,p$ , where again $p$ denotes the network width and $d$ the data dimension dim $\\mathcal{X}$ . For ease hereafter, we write $a_{j}\\,=\\,a_{(\\cdot),j}$ to refer to the entire $j$ th vector of second layer network coefficients when the context is clear. As is standard, the first layer parameters are initialized as independent standard Gaussian random variables, i.e. $w_{j}\\overset{i i d}{\\sim}\\mathcal{N}(0,I_{d})$ for all $j=1,\\hdots,p$ . The weight $a_{i j}$ can also be drawn $a_{i j}\\overset{i i d}{\\sim}\\mathcal{N}(0,1)$ for all $i\\,=\\,1,\\ldots,q,j\\,=\\,1,\\ldots,p$ , but in this work, we initialize these second-layer weights to zero for simplicity to ensure that at initialization, $f(\\cdot;\\phi)=0$ . A zero-initialized network function is used for analysis in several related works, e.g. Chizat et al. (2019) and Ba et al. (2020). For now, notationally we denote weights to be initialized as draws from an arbitrary distribution $D$ , and we introduce specificity to $D$ as required. ", "page_idx": 14}, {"type": "text", "text": "The neural tangent kernel (Equation (4)) can be computed explicitly for this architecture and is given in the lemma below, which proves pointwise convergence to the limiting NTK at initialization as the width $p$ tends to infinity. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Pointwise Convergence At Initialization). For the architecture above, consider any $p$ . Let $a,\\in\\mathbb{R}^{q},w\\in\\mathbb{R}^{d}$ be distributed according to $a,w\\sim D$ for some distribution $D$ such that $a,w$ are integrable $(L_{1})$ random variables. Assume $\\mathcal{X}$ is compact, and $\\sigma^{\\prime}$ is bounded. Then, provided condition $(C4)$ holds (see below), we have for any $x,\\tilde{x}\\in\\mathcal{X}$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\nK_{\\phi(0)}^{p}(x,\\tilde{x})\\stackrel{a.s.}{\\rightarrow}\\mathbb{E}_{D}K(x,\\tilde{x};w,a)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as $p\\rightarrow\\infty$ where $K_{\\phi(0)}^{p}$ denotes the NTK at initialization constructed from draws $a_{j},w_{j}\\stackrel{i i d}{\\sim}D$ and $K(x,\\tilde{x};w,a)\\in\\mathbb{R}^{q\\times{\\dot{q}}}$ is the $q\\times q$ matrix whose $k$ , lth entry is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[\\mathbf{1}_{k=l}\\sigma\\left(x^{\\top}w\\right)\\sigma\\left(\\tilde{x}^{\\top}w\\right)+a_{k}a_{l}\\sigma^{\\prime}\\left(x^{\\top}w\\right)\\sigma^{\\prime}\\left(\\tilde{x}^{\\top}w\\right)\\left(x^{\\top}\\tilde{x}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $k,l=1,\\dots,q$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Consider the $k$ th coordinate function of $f$ . For any choice of $p$ , the gradient is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}f_{k}(x;\\phi)=\\left[\\begin{array}{c}{\\frac{\\partial f_{k}(x;\\phi)}{\\partial a_{k1}}}\\\\ {\\vdots}\\\\ {\\frac{\\partial f_{k}(x;\\phi)}{\\partial a_{k p}}}\\\\ {\\frac{\\partial f_{k}(x;\\phi)}{\\partial w_{1}}}\\\\ {\\vdots}\\\\ {\\frac{\\partial f_{k}(x;\\phi)}{\\partial w_{p}}}\\end{array}\\right]=\\frac{1}{\\sqrt{p}}\\left[\\begin{array}{c}{\\sigma\\left(x^{\\top}w_{1}\\right)}\\\\ {\\vdots}\\\\ {\\sigma\\left(x^{\\top}w_{p}\\right)}\\\\ {a_{k1}\\sigma^{\\prime}\\left(x^{\\top}w_{1}\\right)x}\\\\ {\\vdots}\\\\ {a_{k p}\\sigma^{\\prime}\\left(x^{\\top}w_{p}\\right)x}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have imposed an arbitrary ordering on the parameters. In the above, we omitted partial derivatives $\\frac{\\partial f_{k}}{\\partial a_{l j}}$ for $l\\neq k,j=1,\\ldots,p$ because these are all identically zero. From this, it follows that for any fixed $x,\\tilde{x}\\in\\mathcal{X}$ , the $k,l$ -th entry of $K_{\\phi(0)}^{p}(x,\\tilde{x})$ is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla_{\\phi}f_{k}(x;\\phi(0))^{\\top}\\nabla_{\\phi}f_{l}(\\tilde{x};\\phi(0))=\\frac{1}{p}\\sum_{j=1}^{p}\\mathbf{1}_{k=l}\\sigma\\left(x^{\\top}w_{j}\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}\\right)+}}\\\\ &{}&{\\frac{1}{p}\\sum_{j=1}^{p}a_{k j}a_{l j}\\sigma^{\\prime}\\left(x^{\\top}w_{j}\\right)\\sigma^{\\prime}\\left(\\tilde{x}^{\\top}w_{j}\\right)\\left(x^{\\top}\\tilde{x}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The existence of the limiting NTK follows immediately: for each of the two terms above, each term is integrable by the compactness of $\\mathcal{X}$ and domination (see (C4)). It follows that $K_{\\infty}(x,\\tilde{x})$ is the $q\\times q$ matrix whose $k$ , lth entry is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{w,a\\sim D}\\left[\\mathbf{1}_{k=l}\\sigma\\left(x^{\\top}w\\right)\\sigma\\left(\\tilde{x}^{\\top}w\\right)+a_{k}a_{l}\\sigma^{\\prime}\\left(x^{\\top}w\\right)\\sigma^{\\prime}\\left(\\tilde{x}^{\\top}w\\right)\\left(x^{\\top}\\tilde{x}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $w,a\\sim D$ . Convergence in probability pointwise follows from the weak law of large numbers, and almost sure convergence holds by the strong law of large numbers. $K(x,x;a,w)$ is integrable by the assumption (C4) (see below), so the expectation is well-defined. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "The proof of the existence and pointwise convergence to the limiting NTK $K_{\\infty}$ above is rather straightforward, and this result has been previously established in other works (Jacot et al., 2018). For our analysis of kernel gradient flows in Theorem 1 for the expected forward KL objectives $L_{P}$ and $L_{F}$ , however, we require uniform convergence to $K_{\\infty}$ over the entire data space $\\mathcal{X}$ . ", "page_idx": 15}, {"type": "text", "text": "We establish conditions under which this uniform convergence holds in two results, Proposition 2 and Proposition 3. Proposition 2, given below, concerns convergence at initialization to the limiting neural tangent kernel $K_{\\infty}$ (i.e. before beginning gradient descent). Proposition 3, proven in Appendix D, demonstrates that across a finite training interval $[0,T]$ , the NTK changes minimally from its initial value in a large width regime. Generally, we refer to the first result as \u201cdeterministic initialization\u201d and the second as \u201clazy training\u201d following related works (Jacot et al., 2018; Chizat et al., 2019). ", "page_idx": 15}, {"type": "text", "text": "Below, we give suitable regularity conditions and state and prove Proposition 2. ", "page_idx": 15}, {"type": "text", "text": "(C1) The data space is $\\mathcal{X}$ is compact.   \n(C2) The distribution $D$ is such that $w\\sim\\mathcal{N}(0,I_{d})$ and $a=0$ w.p. 1. For $j=1,\\dots,p$ iid draws from this distribution, we thus have $w_{j}\\overset{i i d}{\\sim}\\mathcal{N}(0,I_{d})$ and $a_{i j}=0$ w.p 1 for all $i,j$ .   \n(C3) The activation function $\\sigma$ is continuous. Under (C2), this implies that the function $K(\\cdot,\\cdot;a,w)$ from Lemma 3 with $a,w\\sim D$ is almost surely continuous.   \n(C4) The function $K(x,\\tilde{x};a,w)$ is dominated by some integrable random variable $G$ , i.e. for all $x,\\tilde{x}\\in\\mathcal{X}\\times\\mathcal{X}$ we have $||K(x,\\tilde{x};a,w)||_{F}\\leq G(a,w)$ almost surely for integrable $G(a,w)$ ", "page_idx": 15}, {"type": "text", "text": "Proposition 2. Fix a scaled two-layer network architecture of width $p$ , and let $\\Phi$ denote the corresponding parameter space. Initialize $\\phi(0)$ as independent, identically distributed random variables drawn from the distribution $D$ in $(C2)$ . Let $K_{\\phi(0)}^{p}:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}^{q\\times q}$ be the mapping defined by $(x,x^{\\prime})\\mapsto K_{\\phi(0)}(x,x^{\\prime})=J_{\\phi}f(x;\\phi(0))J_{\\phi}f(x^{\\prime};\\phi(0))^{\\top}$ . Then, provided conditions $(C I)\u2013(C4)$ hold, we have as $p\\rightarrow\\infty$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x,\\tilde{x}\\in\\mathcal{X}}||K_{\\phi(0)}^{p}(x,\\tilde{x})-K_{\\infty}(x,\\tilde{x})||_{F}\\overset{a.s.}{\\to}0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{K_{\\infty}(x,\\tilde{x}):=\\mathrm{plim}_{p\\to\\infty}\\,K_{\\phi(0)}^{p}(x,\\tilde{x})}\\end{array}$ is a fixed, continuous kernel. ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof follows by direct application of Proposition 7 of Shapiro (2003). Precisely, we satisfy i) almost-sure continuity of $K(\\cdot,\\cdot;a,w)$ by (C3), ii) domination by (C4), and iii) the draws comprising $K_{\\phi(0)}^{p}$ are iid by assumption. By this proposition, then, we have uniform convergence of $K_{\\phi(0)}^{p}$ to $K_{\\infty}$ and obtain continuity of $K_{\\infty}$ as well. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "D Lazy Training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Below, we prove several results that will aid in proving the \u201clazy training\u201d result of Proposition 3 (see below). Given the same architecture as above in Appendix $\\mathbf{C}$ and a fixed width $p$ and time $T>0$ , we will begin by bounding $||w_{j}(T)-w_{j}(0)||$ and $\\lvert|a_{k j}(T)-a_{k j}(0)\\rvert|,\\lvert|a_{l j}(T)-a_{l j}(0)\\rvert|$ for all $k,l=1,\\dots,q$ and all $j=1,\\hdots,p$ . As in Appendix C, there are several conditions that we impose and use in the following results. (D1)\u2013(D2) are identical to (C1)\u2013(C2), repeated for clarity. ", "page_idx": 15}, {"type": "text", "text": "(D1) The data space is $\\mathcal{X}$ is compact. ", "page_idx": 15}, {"type": "text", "text": "(D2) The distribution $D$ is such that $w\\sim\\mathcal{N}(0,I_{d})$ and $a=0$ w.p. 1. For $j=1,\\dots,p$ iid draws from this distribution, we thus have $w_{j}\\overset{i i d}{\\sim}\\mathcal{N}(0,I_{d})$ and $a_{i j}=0$ w.p 1 for all $i,j$ . ", "page_idx": 15}, {"type": "text", "text": "(D3) The function $\\ell(x,\\eta)\\,=\\,\\mathrm{KL}\\left[P(\\Theta\\mid X=x)\\mid\\mid Q(\\Theta;\\eta)\\right]$ is such that $\\ell^{\\prime}(x;\\eta)$ is bounded uniformly for all $x$ and for all $\\eta\\in\\{f(x;\\phi(t)):t>0\\}$ by a constant $\\tilde{M}$ , uniformly over the width $p$ . We recall that this notation is shorthand for $\\nabla_{\\eta}\\ell(x,\\eta)$ . ", "page_idx": 16}, {"type": "text", "text": "(D4) The activation function $\\sigma(\\cdot)$ is non-polynomial and is Lipschitz with constant $C$ . Note that the Lipschitz condition implies $\\sigma$ has bounded first derivative i.e. $|\\sigma^{\\prime}(r)|\\le C$ for all $r\\in\\mathbb{R}$ . ", "page_idx": 16}, {"type": "text", "text": "With these conditions in hand, we now prove several lemmas for individual parameters. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Lazy Training of $w$ ). For the width $p$ scaled two-layer architecture above, assume that conditions $(D I)\u2013(D4)$ hold. Let $\\phi$ evolve according to the gradient flow of the objective $L_{P}$ , i.e. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\dot{\\phi}(t)=-\\nabla_{\\phi}L(\\phi).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Fix any $T>0$ . Then for all $j=1,\\cdot\\cdot,p,$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n||w_{j}(T)-w_{j}(0)||_{2}\\leq||w_{j}(0)||_{2}\\cdot D_{p,T}+E_{p,T}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "almost surely, where $D_{p,T},E_{p,T}$ are constants depending on $p,T_{\\mathrm{i}}$ , and $\\begin{array}{r}{\\operatorname*{lim}_{p\\to\\infty}D_{p,T}\\,=\\,0}\\end{array}$ and $\\mathrm{lim}_{p\\rightarrow\\infty}\\,E_{p,T}=0$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. First note that for any fixed $j$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ_{w_{j}}f(x;\\phi)=\\left[\\begin{array}{c}{\\nabla_{w_{j}}f_{1}(x;\\phi)^{\\top}}\\\\ {\\vdots}\\\\ {\\nabla_{w_{j}}f_{q}(x;\\phi)^{\\top}}\\end{array}\\right]=\\frac{1}{\\sqrt{p}}\\left[\\begin{array}{c}{a_{1j}\\sigma^{\\prime}\\left(x^{\\top}w_{j}\\right)x^{\\top}}\\\\ {\\vdots}\\\\ {a_{q j}\\sigma^{\\prime}\\left(x^{\\top}w_{j}\\right)x^{\\top}}\\end{array}\\right]\\in\\mathbb{R}^{q\\times d}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as required, where $a_{i j}\\in\\mathbb{R}$ for $i=1,\\dots,q$ and $x\\in\\mathbb{S}^{d-1}$ from (D1). We can bound the operator 2-norm of this matrix by observing that for any $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle||J_{w_{j}}f(x;\\phi)y||_{2}^{2}=\\frac{1}{p}\\cdot\\left(\\sum_{i=1}^{q}a_{i j}^{2}\\right)\\cdot\\sigma^{\\prime}\\left(x^{\\top}w_{j}\\right)^{2}(x^{\\top}y)^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\leq\\frac{C^{2}}{p}||a_{j}||_{2}^{2}\\cdot||x||_{2}^{2}\\cdot||y||_{2}^{2}\\mathrm{~by~(D4)~and~Cauchy-Schwarz}}\\\\ &{\\displaystyle\\qquad\\implies||J_{w_{j}}f(x;\\phi)||_{2}\\leq\\frac{C}{\\sqrt{p}}||a_{j}||_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by observing $||x||_{2}^{2}=1$ . By similar computations, we also have ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ_{a_{j}}f(x;\\phi)=\\left[\\begin{array}{c}{\\nabla_{a_{j}}f_{1}(x;\\phi)^{\\top}}\\\\ {\\vdots}\\\\ {\\nabla_{a_{j}}f_{q}(x;\\phi)^{\\top}}\\end{array}\\right]=\\frac{1}{\\sqrt{p}}\\mathrm{diag}\\left[\\begin{array}{c}{\\sigma\\left(x^{\\top}w_{j}\\right)}\\\\ {\\vdots}\\\\ {\\sigma\\left(x^{\\top}w_{j}\\right)}\\end{array}\\right]\\in\\mathbb{R}^{q\\times q}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using condition (D4), it follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|J_{a_{j}}f(x;\\phi)\\|_{2}\\leq\\frac{|\\sigma(x^{\\top}w_{j})|}{\\sqrt{p}}}\\\\ &{\\phantom{\\|}\\leq\\frac{|\\sigma(0)|+C|x^{\\top}w_{j}|}{\\sqrt{p}}}\\\\ &{\\frac{d e f}{\\sqrt{p}}\\frac{K+C|x^{\\top}w_{j}|}{\\sqrt{p}}}\\\\ &{\\leq\\frac{K+C||w_{j}||_{2}}{\\sqrt{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by Cauchy-Schwarz and (D1),(D4), where throughout the following we let $K:=|\\sigma(0)|$ . Now we will use these computations to bound the variation on $w_{j}$ across the interval $(0,T]$ . Fix any $t\\in(0,T]$ . ", "page_idx": 16}, {"type": "text", "text": "Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|w_{1}(t)-w_{2}(t)|||\\leq\\int_{\\Gamma}\\bigg||w_{3}(t)||d t\\leq\\int_{\\Gamma}\\bigg||w_{4}(t)||\\leq\\int_{\\Gamma}\\bigg|(\\Delta_{t})\\int_{\\Gamma}\\big(|X_{t}(x_{4}(t)|)|\\mathrm{d}t\\bigg)d t}\\\\ {\\leq\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int_{\\Gamma}\\big(|X_{t}(x_{4})|\\big)|d t\\leq\\int_{\\Gamma}\\bigg||w_{5}(t)||\\leq\\int_{\\Gamma}\\big(|X_{t}(x_{4})|\\big)|d t}\\\\ {\\leq\\int_{\\Gamma}\\bigg||\\int_{\\Gamma}\\big(|X_{t}(x_{4})|\\big)|d t\\leq\\int_{\\Gamma}\\bigg||w_{6}(t)||\\leq\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\big(|X_{t}(x_{4})|\\big)|d t}\\\\ {\\leq\\frac{C_{2}\\int_{\\Gamma}\\big|w_{7}(t)\\big||d t+\\int_{\\Gamma}\\big(|X_{t}(t)|\\big)|d t+\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\big(|X_{t}(x_{4})|\\big)|d t+\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\big(|X_{t}(x_{4})|\\big)|d t+\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\big(|X_{t}(x_{4})|\\big)|d t}\\\\ {\\leq\\int_{\\Gamma}\\bigg||\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\big(|X_{t}(x_{4})|\\big)|d t+\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\big(|X_{t}(x_{4})|\\big)|d t+\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\big(|X_{t}(x_{4})|\\big)|d t+\\int_{\\Gamma}\\bigg||\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t}\\int\\bigg|\\Delta_{t \n$$$$\n\\begin{array}{r l}&{\\mathbb{E}\\{M\\int_{\\mathbb{Z}}\\sum_{t=1}^{T}|\\sum_{\\ell=1}^{T}|\\mathcal{Z}_{t}(x,\\ell)||\\mathcal{Z}_{t}(x,\\ell)||\\mathcal{H}(\\mathcal{R})=P(0)\\}}\\\\ &{\\le\\frac{C\\eta^{2}\\eta^{2}}{\\eta}\\int_{\\mathbb{Z}}\\Big|\\mu_{0}(1)\\eta+\\eta\\Big|\\eta\\sin{\\theta}\\cos{\\theta}\\Big\\}}\\\\ &{=\\frac{C\\eta^{3}}{\\eta}\\int_{\\mathbb{Z}}\\Big|\\Big|\\eta\\sin{\\theta}-\\eta\\Big|(1)d\\theta+V\\eta(2)}\\\\ &{\\le\\frac{C\\eta^{3}}{\\eta}\\int_{\\mathbb{Z}}\\Big|\\int_{\\mathbb{Z}}\\Big|\\frac{\\int_{X}}{\\theta}\\Big|\\eta\\sin{\\theta}\\Big\\rangle\\Big|\\sqrt{\\theta}\\Big|\\eta\\sin{\\theta}\\Big\\rangle}\\\\ &{\\le\\frac{C\\eta^{3}}{\\sqrt{\\eta}}\\int_{\\mathbb{Z}}\\Big|\\sum_{t=1}^{T}|\\sum_{\\ell=1}^{T}|\\theta_{0}|\\Big\\rangle\\Big|\\sqrt{\\eta}\\Big|\\sqrt{\\theta}\\Big|\\int_{\\mathbb{Z}}\\Big|(X\\varphi_{0}^{T}(x)\\Big|)|d\\theta\\,d\\theta\\Big\\rangle}\\\\ &{\\le\\frac{C\\eta^{3}}{\\sqrt{\\eta}}\\int_{\\mathbb{Z}}\\Big|\\int_{\\mathbb{Z}}\\sum_{t=1}^{T}|\\sum_{\\ell=1}^{T}|\\theta_{0}|\\Big\\rangle\\Big|\\sqrt{\\eta}\\Big|\\sin{\\theta}\\Big\\rangle\\cos{\\theta}\\Big\\rangle}\\\\ &{\\le\\frac{C\\eta^{2}\\eta^{2}R^{2}}{\\eta}+C\\frac{C\\eta^{3}\\eta^{2}}{\\eta}\\int_{\\mathbb{Z}}\\Big|\\int_{\\mathbb{Z}}\\Big|\\sin{\\theta}\\Big\\rangle\\Big|\\sin{\\theta}\\Big\\rangle\\sin{\\theta}\\cos{\\theta}\\Big\\rangle}\\\\ &{\\le\\frac{C\\eta^{3}R^{2}}{\\eta}R^{3}+\\frac{C\\eta^{3}\\eta^{2}}{\\eta}\\int_{\\mathbb{Z}}\\Big|\\int_{\\mathbb{Z}}\\Big|\\eta\\sin{\\theta}\\Big\\rangle-\\eta\\Big|(1)\\eta+\\eta\\sin{\\theta}\\sin{\\theta}}\\\\ &{=\\frac{C\\eta^{3}R^{2}}{\\eta}\\frac{C\\eta^{3}}{\\eta}+\\frac{C\\eta^{3}\\eta^{ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\begin{array}{r}{c_{1}=\\frac{C\\tilde{M}^{2}K T^{2}}{2p}+\\frac{C^{2}\\tilde{M}^{2}T^{2}}{2p}||w_{j}(0)||_{2}}\\end{array}$ and $\\begin{array}{r}{c_{2}=\\frac{C^{2}\\tilde{M}^{2}}{p}}\\end{array}$ . Note that even though $c_{1}$ depends on $T$ , this is constant as $T$ is fixed. We write these quantities in this way to recognize a Gronwall-type inequality that we can use to bound the left hand side. Indeed, by direct application of Theorem 57 of Dragomir (2003) (see Theorem 4) we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w_{j}(t)-w_{j}(0)\\|_{2}\\leq c_{1}\\exp\\left[\\displaystyle\\int_{0}^{t}\\displaystyle\\int_{0}^{s}c_{2}d v d s\\right]}\\\\ &{\\qquad\\qquad\\qquad=c_{1}\\exp\\displaystyle\\frac{c_{2}t^{2}}{2}}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\frac{C\\tilde{M}^{2}K T^{2}}{2p}+\\frac{C^{2}\\tilde{M}^{2}T^{2}}{2p}||w_{j}(0)||_{2}\\right)\\exp\\left[\\frac{C^{2}\\tilde{M}^{2}t^{2}}{2p}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "giving the result for $t\\ \\ =\\ \\ T$ if we take $\\begin{array}{r l r}{D_{p,T}}&{=}&{\\frac{C^{2}\\tilde{M}^{2}T^{2}}{2p}\\exp\\left[\\frac{C^{2}\\tilde{M}^{2}T^{2}}{2p}\\right]}\\end{array}$ and $\\begin{array}{r l}{E_{p,T}}&{{}=}\\end{array}$ $\\frac{C\\tilde{M}^{2}K T^{2}}{2p}\\exp\\left[\\frac{C^{2}\\tilde{M}^{2}T^{2}}{2p}\\right]$ C2 M2\u02dcp 2T 2 . Clearly these constants satisfy limp\u2192\u221eDp,T = 0 and limp\u2192\u221eEp,T = 0 for any fixed $\\bar{T}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 (Lazy Training of $a$ ). Under the same conditions as Lemma $^{4}$ , let $\\phi$ evolve according to the gradient flow of problem $L_{P}$ , i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\dot{\\phi}(t)=-\\nabla_{\\phi}L(\\phi).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Fix any $T>0$ . Then we have for any $j$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n||a_{j}(T)||_{2}\\leq||w_{j}(0)||_{2}\\cdot F_{p,T}+G_{p,T}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "almost surely, where $E_{p,T}$ and $F_{p,T}$ are constants depending on $p,T$ satisfying $\\mathrm{lim}_{p\\rightarrow\\infty}\\,E_{p,T}=0$ and $\\textstyle\\operatorname*{lim}_{p\\to\\infty}F_{p,T}=0$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. We will use much of the same work as in Lemma 4. Namely, $||a_{j}(t)||_{2}=||a_{j}(t)-a_{j}(0)||_{2}$ almost surely by (D2), and thereafter for any $t\\in(0,T]$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|u_{j}(t)-u_{j}(0)\\right\\|\\leq\\frac{\\int_{0}^{T}\\left||u_{j}(s)|||d s}{\\leq\\frac{1}{\\eta}\\int_{0}^{T}\\mathbf{g}_{r\\in\\mathcal{V}_{n}}\\||u_{j}(s,\\cdot)\\,d u||^{p}(X,f(X;\\phi(e)))\\|d s}}\\\\ &{\\leq\\frac{3}{\\sqrt{\\eta}}\\int_{0}^{T}\\int_{\\mathbb{R}^{n}\\cap\\mathcal{V}_{n}}\\|u_{j}(s,\\cdot)\\||d s}\\\\ &{\\leq\\frac{M}{\\eta}\\int_{0}^{T}\\int_{\\mathbb{R}^{n}\\cap\\mathcal{V}_{n}}\\|u_{j}(s,\\cdot)\\||d s}\\\\ &{\\leq\\frac{K M}{\\eta}+\\frac{\\hat{M}C}{\\eta}\\int_{0}^{T}\\int_{\\mathbb{R}^{n}\\cap\\mathcal{V}_{n}}\\|u_{j}(s)\\||d s\\;\\Psi\\mathrm{weth}\\,\\mathrm{Lemma}\\,4}\\\\ &{\\leq\\frac{K M}{\\eta}+\\frac{\\hat{M}C}{\\eta}\\int_{0}^{T}\\left||u_{n}(s)-u_{j}(s)|||+\\|u_{n}(0)\\||d s}\\\\ &{\\leq\\frac{K M}{\\eta}+\\frac{\\hat{M}C}{\\eta}\\int_{0}^{T}\\|u_{n}(0)\\|_{L^{1}}+\\frac{\\hat{M}C}{\\eta}\\int_{0}^{T}\\int_{\\mathbb{R}^{n}\\cap\\mathcal{V}_{n}}\\|u_{j}(s)\\||d s+E_{n}\\|_{L^{2}}\\mathrm{d}y\\log\\log4}\\\\ &{\\leq\\frac{K M}{\\eta}+\\frac{\\hat{M}C1}{\\eta}\\|_{L^{1}(\\mathbb{R}^{n})}\\|\\|\\tilde{u}_{n}(\\cdot)\\frac{\\hat{M}C}{\\eta}\\int_{0}^{T}E_{n}d s+\\frac{\\hat{M}C}{\\eta}\\int_{\\mathbb{R}^{n}\\cap\\mathcal{V}_{n}}\\|\\exp\\int_{\\mathbb{R}^{n}\\mathcal{V}_{n}}d s}\\\\ &{=\\|\\omega_{0}(s)\\|_{L^{1}}\\underbrace{\\hat{M}C}_{\\mathcal{V}}+\\frac{\\hat{M}C}{\\eta}\\int_{0}^{T}E_{n}\\int_{\\mathbb{R}^{n}\\setminus\\mathcal{V}_{n}}d s\\Big)+\\left(\\frac{K M}{\\eta} \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Clearly, these constants satisfy $\\textstyle\\operatorname*{lim}_{p\\to\\infty}F_{p,t}\\to0$ and $\\begin{array}{r}{\\operatorname*{lim}_{p\\to\\infty}G_{p,t}\\to0}\\end{array}$ (to see this, simply plug in the forms of $D_{p,s}$ and $E_{p,s}$ from Lemma 4 above) and we have the result by taking $t=T$ . ", "page_idx": 18}, {"type": "text", "text": "Now with these results in hand, we may state and prove Proposition 3, given below. ", "page_idx": 18}, {"type": "text", "text": "Proposition 3. Under the same conditions as Proposition 2, fix any $T\\ >\\ 0.$ . For any $t\\ \\in$ $(0,\\bar{T}]$ let $K_{\\phi(t)}^{p}\\ :\\ \\mathcal{X}\\,\\times\\,\\mathcal{X}\\ \\to\\ \\mathbb{R}^{q\\times q}$ be the mapping defined by $(x,x^{\\prime})\\;\\mapsto\\;K_{\\phi(t)}(x,x^{\\prime})\\;=$ $J_{\\phi}f(x;\\phi(t))J_{\\phi}f(x^{\\prime};\\phi(t))^{\\top}$ . Then provided conditions $(D I)\u2013(D4)$ hold, we have as $p\\rightarrow\\infty$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x,\\tilde{x}\\in\\mathcal{X},t\\in(0,T]}||K_{\\phi(t)}^{p}(x,\\tilde{x})-K_{\\phi(0)}^{p}(x,\\tilde{x})||_{F}\\stackrel{a.s.}{\\to}0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let us examine the $k,l$ th term of the $q\\times q$ matrix given by $K_{\\phi(t)}^{p}(x,\\tilde{x})-K_{\\phi(0)}^{p}(x,\\tilde{x})$ for fixed $x,\\tilde{x}$ , and some $t\\in(0,T]$ . The $k$ , lth term is given by (see the work in Appendix C): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{1}{p}\\sum_{j=1}^{p}\\mathbf{1}_{k=l}\\bigg(\\sigma\\left(x^{\\top}w_{j}(t)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(t)\\right)\\bigg)-}\\\\ {\\displaystyle\\bigg(\\sigma\\left(x^{\\top}w_{j}(0)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)\\bigg)}\\\\ {+\\displaystyle\\frac{1}{p}\\sum_{j=1}^{p}\\bigg(a_{k j}(t)a_{l j}(t)\\sigma^{\\prime}\\left(x^{\\top}w_{j}(t)\\right)\\sigma^{\\prime}\\left(\\tilde{x}^{\\top}w_{j}(t)\\right)\\left(x^{\\top}\\tilde{x}\\right)\\bigg)-}\\\\ {\\displaystyle\\bigg(a_{k j}(0)a_{l j}(0)\\sigma^{\\prime}\\left(x^{\\top}w_{j}(0)\\right)\\sigma^{\\prime}\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)\\left(x^{\\top}\\tilde{x}\\right)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Above, we have explicitly made clear the dependence of the parameters on time, e.g. $w_{j}(t)$ vs. $w_{j}(0)$ . We aim to show that the quantity above tends to zero as $p\\rightarrow\\infty$ . We first prove this holds pointwise, and will consider the red and blue terms one at a time for a fixed $x,\\tilde{x}$ . ", "page_idx": 19}, {"type": "text", "text": "First consider the $j$ th summand of the red term. We will bound its absolute value. If $k\\neq l$ , we\u2019re done, so assume $k=l$ . We have for any $j$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma\\left(x^{\\top}w_{j}(t)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(t)\\right)-\\sigma\\left(x^{\\top}w_{j}(0)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)\\bigg|}\\\\ &{=\\left|\\sigma\\left(x^{\\top}w_{j}(t)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(t)\\right)-\\sigma\\left(x^{\\top}w_{j}(t)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)+}\\\\ &{\\phantom{=}\\sigma\\left(x^{\\top}w_{j}(t)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)-\\sigma\\left(x^{\\top}w_{j}(0)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)\\bigg|}\\\\ &{\\leq|\\sigma\\left(x^{\\top}w_{j}(t)\\right)|\\cdot|\\sigma\\left(\\tilde{x}^{\\top}w_{j}(t)\\right)-\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)|+|\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)|\\cdot|\\sigma\\left(x^{\\top}w_{j}(t)\\right)-\\sigma\\left(x^{\\top}w_{j}(0)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and by the Lipschitz assumption on $\\sigma(\\cdot)$ and Cauchy-Schwarz, we can bound the quantity above as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq(K+C||x||_{2}||w_{j}(t)||_{2})\\cdot C||\\bar{x}||_{2}||w_{j}(t)-w_{j}(0)||_{2}+(K+C||x||_{2}||w_{j}(0)||_{2})\\cdot C||x||_{2}||w_{j}(t)-C||w_{j}(0)||_{2}}\\\\ &{=C^{2}||w_{j}(t)-w_{j}(0)||_{2}\\left(2\\frac{K}{C}+||w_{j}(t)||_{2}+||w_{j}(0)||_{2}\\right)\\ \\mathrm{since}\\ ||x||_{2}=||\\bar{x}||_{2}=1}\\\\ &{\\leq C^{2}||w_{j}(t)-w_{j}(0)||_{2}\\left(2\\frac{K}{C}+||w_{j}(t)-w_{j}(0)||_{2}+2||w_{j}(0)||_{2}\\right)\\ \\mathrm{~by~triangle~inequality}}\\\\ &{=2C K||w_{j}(t)-w_{j}(0)||_{2}+C^{2}||w_{j}(t)-w_{j}(0)||_{2}^{2}+2C^{2}||w_{j}(0)||_{2}||w_{j}(t)-w_{j}(0)||_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and using Lemma 4, we can bound all terms above using $||w_{j}(0)||_{2}$ as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq2C K\\left(D_{p,t}||w_{j}(0)||_{2}+E_{p,t}\\right)+C^{2}\\left(D_{p,t}||w_{j}(0)||_{2}+E_{p,t}\\right)^{2}+2C^{2}\\left(D_{p,t}||w_{j}(0)||_{2}^{2}+E_{p,t}||w_{j}(0)||_{2}^{2}\\right)}\\\\ &{=\\left(2C^{2}D_{p,t}+C^{2}D_{p,t}^{2}\\right)||w_{j}(0)||_{2}^{2}+\\left(2C K D_{p,t}+2C^{2}D_{p,t}E_{p,t}+2C^{2}E_{p,t}\\right)||w_{j}(0)||_{2}+\\left(2C K E_{p,t}+2E_{p,t}\\right)\\left(\\left(\\left(\\left|\\mathcal{D}_{p,t}^{2}\\right|_{2}+\\left(2C E_{p,t}\\right)\\right)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recalling that $w_{j}(0)\\overset{i i d}{\\sim}\\mathcal{N}(0,I_{d})$ , we have that $||w_{j}(0)||_{2}$ and $||w_{j}(0)||_{2}^{2}$ are integrable with expectations denoted $\\mu$ and $\\nu$ , respectively. All our work has allowed us to show that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\vert\\frac{1}{p}\\sum_{j=1}^{p}\\left(\\sigma\\left(x^{\\top}w_{j}(t)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(t)\\right)-\\sigma\\left(x^{\\top}w_{j}(0)\\right)\\sigma\\left(\\tilde{x}^{\\top}w_{j}(0)\\right)\\right)\\right\\vert}\\\\ &{\\le\\displaystyle\\frac{1}{p}\\sum_{j=1}^{p}\\left(2C^{2}D_{p,t}+C^{2}D_{p,t}^{2}\\right)\\|w_{j}(0)\\|_{2}^{2}+\\left(2C K D_{p,t}+2C^{2}D_{p,t}E_{p,t}+2^{2}C E_{p,t}\\right)\\|w_{j}(0)\\|_{2}}\\\\ &{\\quad\\,\\,+\\left(2C K E_{p,t}+C^{2}E_{p,t}^{2}\\right)}\\\\ &{\\displaystyle\\xrightarrow{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\,\\,\\,\\,\\,}\\\\ &{\\displaystyle\\quad\\to\\left(\\operatorname*{lim}_{p\\to\\infty}2C^{2}D_{p,t}+C^{2}D_{p,t}^{2}\\right)\\nu+\\left(\\operatorname*{lim}_{p\\to\\infty}2C K D_{p,t}+2C^{2}D_{p,t}E_{p,t}+2C^{2}E_{p,t}\\right)\\mu}\\\\ &{\\quad\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by conditions on $D_{p,t}$ and $E_{p,t}$ from Lemma 4, the strong law of large numbers, and the classic result from analysis that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\stackrel{}{a}_{n}b_{n}=(\\operatorname*{lim}_{n\\to\\infty}a_{n})\\left(\\operatorname*{lim}_{n\\to\\infty}b_{n}\\right.}\\end{array}$ , provided both limits on the right hand side exist. Lastly, we can achieve the same result for the blue term quickly. Because $a_{i j}(0)\\,{\\bar{=}}\\,0$ w.p. 1 by (D2), we have almost surely that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{p}\\displaystyle\\sum_{j=1}^{p}\\left(a_{k j}(t)a_{j}(t)\\sigma^{\\prime}\\left(x^{\\top}w_{j}(t)\\right)\\sigma^{\\prime}\\left(\\bar{x}^{\\top}w_{j}(t)\\right)\\left(x^{\\top}\\bar{x}\\right)\\right)-}\\\\ &{\\left(a_{k j}(0)a_{j}(0)\\sigma^{\\prime}\\left(x^{\\top}w_{j}(\\theta)\\right)\\sigma^{\\prime}\\big(\\bar{x}^{\\top}w_{j}(0)\\big)\\left(x^{\\top}\\bar{x}\\right)\\right)}\\\\ &{\\overbrace{\\leq\\frac{1}{p}\\displaystyle\\sum_{j=1}^{p}|a_{k j}(t)||a_{j}(t)||\\sigma^{\\prime}\\left(x^{\\top}w_{j}(0)\\right)||\\sigma^{\\prime}\\left(\\bar{x}^{\\top}w_{j}(0)\\right)||x||_{2}||\\bar{x}||_{2}}^{2}}\\\\ &{\\leq\\frac{C^{2}}{p}\\displaystyle\\sum_{j=1}^{p}|a_{k j}||a_{j}|}\\\\ &{\\leq\\frac{C^{2}}{p}\\displaystyle\\sum_{j=1}^{p}|a_{j}(t)||_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "because for all $j$ , we have $|a_{k j}|,|a_{k j}|$ are dominated by $||a_{j}||_{2}$ . From here, we have by Lemma 5 that we can bound each term in the sum above by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\le\\frac{C^{2}}{p}\\sum_{j=1}^{p}(\\|w_{j}(0)\\|_{2}F_{p,t}+G_{p,t})^{2}}}\\\\ &{=\\displaystyle\\frac{C^{2}}{p}\\sum_{j=1}^{p}F_{p,t}^{2}||w_{j}(0)||_{2}^{2}+2F_{p,t}G_{p,t}||w_{j}(0)||_{2}+G_{p,t}^{2}}\\\\ &{\\overset{a.s.}{\\rightarrow}0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as $p\\rightarrow\\infty$ by similar logic to the above. Together, these results combine to show that $|K_{\\phi(t)}^{p}(x,\\tilde{x})_{k l}-$ $K_{\\phi(0)}^{p}(x,\\tilde{x})_{k l}|\\overset{a.s.}{\\rightarrow}\\ 0$ as $p\\rightarrow\\infty$ . As $k,l$ were arbitrary $k,l\\,\\in\\,1,\\ldots,q$ , we have $||K_{\\phi(t)}^{p}(x,\\tilde{x})-$ K\u03d5p(0)(x, x\u02dc)||F a.\u2192s.0. This establishes pointwise convergence for some fixed t \u2208(0, T]. Uniform convergence over all of $\\mathcal X\\times\\mathcal X$ and all $t\\in(0,T]$ follows easily in this case. Firstly, the numbers $D_{p,t},E_{p,t},F_{p,t}$ , and $G_{p,t}$ are monotonic in $t$ , so we can bound uniformly for all $t\\in(0,T]$ by taking $t\\,=\\,T$ in the expressions above. Secondly, in our work above, our bounds on the red and blue terms were independent of the choice of point $(x,\\tilde{x})$ . More precisely, the supremum over $x,\\tilde{x}$ can accounted for in the bounds easily by observing that $\\begin{array}{r}{\\operatorname*{sup}_{x,\\tilde{x}\\in\\dot{\\mathcal{X}_{\\cdot}},t\\in(0,T]}\\|K_{\\phi(t)}^{p}(\\bar{x},\\tilde{x})-K_{\\phi(0)}^{p}(x,\\tilde{x})\\|_{F}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "can be bounded above by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\underset{x\\leq t\\leq t}{\\operatorname*{sup}}\\left\\|\\frac{1}{p_{j-1}}\\sum_{k=i}^{p}\\bigg(\\sigma\\left(x^{\\top}w_{j}(t)\\right)\\sigma\\left(\\bar{x}^{\\top}w_{j}(t)\\right)\\bigg)}\\\\ &{\\qquad\\qquad-\\left(\\sigma\\left(x^{\\top}w_{j}(0)\\right)\\sigma\\left(\\bar{x}^{\\top}w_{j}(0)\\right)\\right)\\bigg\\|}\\\\ &{+\\underset{x\\leq t\\leq t}{\\operatorname*{sup}}\\left\\|\\frac{1}{p_{j-1}}\\sum_{i=1}^{p}\\bigg(a_{j+1}(t)a_{j}(t)\\sigma^{\\prime}\\left(x^{\\top}w_{j}(t)\\right)\\sigma^{\\prime}\\left(\\bar{x}^{\\top}w_{j}(t)\\right)\\left(x^{\\top}\\bar{x}\\right)\\bigg)}\\\\ &{\\qquad\\qquad-\\left(a_{j}(0)a_{j}(0)\\sigma^{\\prime}\\left(x^{\\top}w_{j}(0)\\right)\\sigma^{\\prime}\\left(\\bar{x}^{\\top}w_{j}(0)\\right)(x^{\\top}\\bar{x})\\right)\\right\\|}\\\\ &{\\leq\\underset{x\\leq t\\leq t}{\\operatorname*{sup}}\\frac{1}{p_{j-1}}\\sum_{i=1}^{p}(2C^{2}D_{p,T}+C^{2}D_{p,T}^{2})\\left\\|w_{j}(0)\\right\\|_{2}^{2}+\\left(2C K D_{p,t}+2C^{2}D_{p,T}E_{p,T}+2C^{2}E_{p,T}\\right)\\left\\|w_{j}(t)\\right.}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\left(2C K E_{p,T}+C^{2}E_{p,T}^{2}\\right)}\\\\ &{\\qquad\\qquad+\\underset{x\\leq t\\leq t}{\\operatorname*{sup}}\\frac{C^{2}}{p_{j-1}}\\sum_{i=1}^{p}P_{p,T}^{2}\\left\\|w_{j}(0)\\right\\|_{2}^{2}+2E_{p,T}G_{p,T}\\left\\|w_{j}(0)\\right\\|_{2}+G_{p,T}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "by the same work as above. ", "page_idx": 21}, {"type": "text", "text": "E Kernel Gradient Flow Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We rely on additional regularity conditions outlined below. We will consider the following three flows in our proof of Theorem 1 (for some choice of $p$ ): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{f}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\phi(t)}^{p}(x,X)\\ell^{\\prime}(X,f_{t}(X))}\\\\ &{\\dot{g}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\infty}(x,X)\\ell^{\\prime}(X,g_{t}(X))}\\\\ &{\\dot{h}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\phi(0)}^{p}(x,X)\\ell^{\\prime}(X,h_{t}(X))}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $f_{t}$ is shorthand for $f(\\cdot;\\phi(t))$ . The three flows above can be thought of as corresponding to $L_{P},L_{F}$ , and a \u201clazy\u201d variant, respectively. The flow of $h_{t}$ is \u201clazy\u201d because it follows the dynamics of a fixed kernel, the kernel at initialization. The flow of $g_{t}$ also follows a fixed kernel, but the limiting NTK $K_{\\infty}$ instead. The flow of $f_{t}$ is that obtained in practice, where the kernel $K_{\\phi(t)}^{p}$ changes continuously as the parameters $\\phi(t)$ evolve in time. The flow in $h_{t}$ is used to bound the differences between $f_{t}$ and $g_{t}$ in the proof of Theorem 1. We now enumerate the regularity conditions. ", "page_idx": 21}, {"type": "text", "text": "(E1) The functional $L(f)$ in $L_{F}$ satisfies $\\operatorname*{inf}_{f}L(f)>-\\infty.$ .   \n(E2) The limiting NTK $K_{\\infty}$ is positive definite (so that the RKHS $\\mathcal{H}$ with kernel $K_{\\infty}$ is welldefined).   \n(E3) Under (E1) and (E2), the function $f^{*}$ minimizing $L_{F}$ satisfies $\\vert\\vert f^{*}\\vert\\vert_{\\mathcal{H}}<\\infty$ , so that $f^{*}\\in\\mathcal{H}$ .   \n(E4) For any choice of $p$ , we have for all $t,x$ that $\\ell^{\\prime}(x;f_{t}(x)),\\ell^{\\prime}(x;g_{t}(x))$ , and $\\ell^{\\prime}(x;h_{t}(x))$ are bounded by a constant $\\tilde{M}$ .   \n(E5) The function $\\ell$ is $\\tilde{L}$ -smooth in its second argument, i.e. $||\\ell^{\\prime}(x,\\eta_{1})\\!-\\!\\ell^{\\prime}(x,\\eta_{2})||\\leq\\tilde{L}||\\eta_{1}\\!-\\!\\eta_{2}||.$ . ", "page_idx": 21}, {"type": "text", "text": "We first prove Lemma 2 from the manuscript. ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $f^{*}\\in\\mathrm{argmin}L_{F}(f)$ , where $L_{F}(f)$ is the functional objective. Then $L(f^{*})>-\\infty$ by (E1). Then if $f_{t}$ evolves according to the kernel gradient flow above, we have (from the chain rule for ", "page_idx": 21}, {"type": "text", "text": "Frechet derivatives) that ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\dot{L}}(f_{t})={\\frac{\\partial L}{\\partial f_{t}}}\\circ{\\frac{\\partial f_{t}}{\\partial t}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By definition, $\\begin{array}{r}{\\frac{\\partial f_{t}}{\\partial t}=\\dot{f}_{t}}\\end{array}$ . We also have $\\begin{array}{r}{\\frac{\\partial L}{\\partial f_{t}}:h\\mapsto\\mathbb{E}_{P(X)}\\ell^{\\prime}(X,f_{t}(X))^{\\top}h(X)}\\end{array}$ . Plugging this in yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{L}(f_{t})=\\mathbb{E}_{X\\sim P(X)}\\ell^{\\prime}(X,f_{t}(X))^{\\top}\\left[-\\mathbb{E}_{X^{\\prime}\\sim P(X)}K_{\\infty}(X,X^{\\prime})\\ell^{\\prime}(X^{\\prime},f_{t}(X^{\\prime}))\\right]}\\\\ &{\\qquad=-\\mathbb{E}_{X,X^{\\prime}\\sim P}\\ell^{\\prime}(X,f_{t}(X))^{\\top}K_{\\infty}(X,X^{\\prime})\\ell^{\\prime}(X^{\\prime},f_{t}(X^{\\prime}))\\le0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "by the positiveness of the kernel $K_{\\infty}$ (from (E2)). Now define $\\begin{array}{r}{\\Delta_{t}=\\frac{1}{2}||f_{t}-f^{*}||_{\\mathcal{H}}^{2}}\\end{array}$ , where $\\mathcal{H}$ is the vector-valued reproducing kernel Hilbert space corresponding to the kernel $K_{\\infty}$ (see Carmeli et al. (2006) for a detailed review). It follows that \u2202\u2202f\u2206tt : $\\begin{array}{r}{\\frac{\\partial\\Delta_{t}}{\\partial f_{t}}:h\\mapsto\\bar{\\langle f_{t}-\\bar{f}^{*},h\\rangle}}\\end{array}$ . Then by the chain rule we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\dot{\\Delta}_{t}=-\\langle f_{t}-f^{*},\\dot{f}_{t}\\rangle}\\\\ &{\\quad\\quad=-\\langle f_{t}-f^{*},-\\mathbb{E}_{P(X)}K_{\\infty}(\\cdot,X)\\ell^{\\prime}(X,f_{t}(X))\\rangle}\\\\ &{\\quad\\quad=\\mathbb{E}_{P(X)}\\ell^{\\prime}(X,f_{t}(X))^{\\top}\\left[f_{t}(X)-f^{*}(X)\\right]}\\\\ &{\\quad\\quad\\geq\\mathbb{E}_{P(X)}\\ell(X,f_{t}(X))-\\ell(X,f^{*}(X))}\\\\ &{\\quad\\quad=L(f_{t})-L(f^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To go from the second to the third line, we used the reproducing property of the vector-valued kernel, the definition of inner product, and the linearity of integration. More precisely, the reproducing property (cf. Eq. (2.2) of Carmeli et al. (2006)) tells us for any functions $g,h$ and fixed $x$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle g,K_{\\infty}(\\cdot,x)h(x)\\rangle=g(x)^{\\top}h(x)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and so the third line results from the second by exchanging the integral and inner product. In the second-to-last line, we used the convexity of $\\ell$ in its second argument (from Lemma 1 of the manuscript). Now consider the Lyapunov functional given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}(t)=t\\left[L(f_{t})-L(f^{*})\\right]+\\Delta_{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Differentiating, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{\\mathcal{E}}(t)=L(f_{t})-L(f^{*})+t\\dot{L}(f_{t})+\\dot{\\Delta}_{t}\\le0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "by the above work because i) $t\\dot{L}(f_{t})\\leq0$ and ii) $L(f_{t})-L(f^{*})+\\dot{\\Delta}_{t}\\le0$ , implying that $\\mathcal{E}(t)\\leq\\mathcal{E}(0)$ for all $t$ . Evaluating at $t=0$ , thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t\\left[L(f_{t})-L(f^{*})\\right]+\\Delta_{t}\\le\\Delta_{0}}\\\\ {t\\left[L(f_{t})-L(f^{*})\\right]\\le\\Delta_{0}-\\Delta_{t}}\\\\ {t\\left[L(f_{t})-L(f^{*})\\right]\\le\\Delta_{0}\\ \\ \\mathrm{since}\\ \\Delta_{t}\\ge0}\\\\ {\\left[L(f_{t})-L(f^{*})\\right]\\le\\displaystyle\\frac{1}{t}\\Delta_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and so we have that there exists a sufficiently large $T$ such that $|L(f_{T})-L(f^{*})|\\leq\\epsilon$ as desired. ", "page_idx": 22}, {"type": "text", "text": "Using this result and our previous results, we are now able to prove Theorem 1 from the manuscript. ", "page_idx": 22}, {"type": "text", "text": "Proof. We will examine the three gradient flows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{f}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\phi(t)}^{p}(x,X)\\ell^{\\prime}(X,f_{t}(X))}\\\\ &{\\dot{g}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\infty}(x,X)\\ell^{\\prime}(X,g_{t}(X))}\\\\ &{\\dot{h}_{t}(x)=-\\mathbb{E}_{P(X)}K_{\\phi(0)}^{p}(x,X)\\ell^{\\prime}(X,h_{t}(X))}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and establish the result by the triangle inequality, i.e. ", "page_idx": 22}, {"type": "equation", "text": "$$\n|L(f_{T})-L(f^{*})|\\leq|L(f_{T})-L(g_{T})|+|L(g_{T})-L(f^{*})|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The flow in $h_{t}$ will be used to help bound the first term, but we begin with the second term. By Lemma 2, pick $T>0$ sufficiently large such that $|L(g_{T})-L(f^{*})|\\leq\\bar{\\epsilon}/2$ . Fix this $T$ . This provides a suitable bound on the second term. ", "page_idx": 23}, {"type": "text", "text": "Turning to the first term, by continuity of $L(f)$ from $L_{F}$ in $f$ , there exists $\\delta\\,>\\,0$ such that $y\\in$ $B(g_{T},\\bar{\\delta})\\implies|L(y)-L(\\bar{g}_{T})|\\leq\\epsilon/2$ . We will show that there exists $P$ sufficiently large such that $p>P$ implies $\\lvert|f_{T}-g_{T}\\rvert|\\leq\\delta$ almost surely, yielding the desired bound on the first term of the decomposition above. Throughout, $||\\cdot||$ denotes the $L^{2}$ norm of a function with respect to probability measure $P$ (i.e. the marginal distribution of $P(X)$ from our joint model $P(\\Theta,X))$ ). ", "page_idx": 23}, {"type": "text", "text": "To show that there exists sufficiently large $P$ such that $\\lvert|f_{T}-g_{T}\\rvert|\\leq\\delta$ , we use another application of the triangle inequality ", "page_idx": 23}, {"type": "equation", "text": "$$\n||f_{T}-g_{T}||\\leq||f_{T}-h_{T}||+||h_{T}-g_{T}||\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and construct bounds on the two terms on the right hand side using Proposition 2 and Proposition 3. Observe first that by (C2)/(D2), at initialization we have almost surely that $f_{0}=g_{0}=h_{0}=0$ . Also note that by continuity of $K_{\\infty}$ (established in Lemma 3) on the compact domain $\\mathcal X\\times\\mathcal X$ we have $\\operatorname*{sup}_{x,\\tilde{x}}||K\\dot{(x,\\tilde{x})}||_{2}<\\dot{M}$ for some $M$ . Finally, note that by (E5) the function $\\ell^{\\prime}(x,\\eta)$ is Lipschitz in its second argument with constant $\\tilde{L}$ . Below, we let $||\\cdot||_{2}$ denote the 2-norm of a vector or matrix, depending on the argument, and $||\\cdot||_{F}$ the Frobenius norm of a matrix. For functions, as stated $||\\cdot||$ denotes the $L^{2}$ norm with respect to measure $P(X)$ , i.e. $\\begin{array}{r}{||f||^{2}=\\int f(X)^{\\top}f(X)d P(X)}\\end{array}$ . From here, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|g_{T}-h_{T}\\right\\|^{q_{\\mathcal{S}}}}&{\\left\\|(g_{T}-g_{0})-(h_{T}-h_{0})\\right\\|}\\\\ &{\\qquad\\qquad=\\left\\|\\displaystyle\\int_{0}^{T}\\mathbb{E}_{P(\\mathcal{X})}\\left[K_{\\infty}(\\cdot,X)\\ell^{\\prime}(\\boldsymbol{X},g_{t}(X))-K_{\\phi(0)}^{p}(\\cdot,X)\\ell^{\\prime}(X,h_{t}(X))\\right]d t\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\int_{0}^{T}\\mathbb{E}_{P(\\mathcal{X})}\\|K_{\\infty}(\\cdot,X)\\ell^{\\prime}(\\boldsymbol{X},g_{t}(X))-K_{\\phi(0)}^{p}(\\cdot,X)\\ell^{\\prime}(X,h_{t}(X))\\|d t}\\\\ &{\\qquad=\\displaystyle\\int_{0}^{T}\\mathbb{E}_{P(\\mathcal{X})}\\|K_{\\infty}(\\cdot,X)\\ell^{\\prime}(\\boldsymbol{X},g_{t}(X))-K_{\\infty}(\\cdot,X)\\ell^{\\prime}(X,h_{t}(X))+}\\\\ &{\\qquad\\qquad K_{\\infty}(\\cdot,X)\\ell^{\\prime}(X,h_{t}(X))-K_{\\phi(0)}^{p}(\\cdot,X)\\ell^{\\prime}(X,h_{t}(X))\\|d t}\\\\ &{\\qquad\\leq\\displaystyle\\int_{0}^{T}\\mathbb{E}_{P(\\mathcal{X})}\\|K_{\\infty}(\\cdot,X)\\left[\\ell^{\\prime}(X,g_{t}(X))-\\ell^{\\prime}(X,h_{t}(X))\\right]\\|+}\\\\ &{\\qquad\\qquad\\qquad\\|K_{\\infty}(\\cdot,X)\\ell^{\\prime}(X,h_{t}(X))-K_{\\phi(0)}^{p}(\\cdot,X)\\ell^{\\prime}(X,h_{t}(X))\\||d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, we note the following facts. Firstly, for any kernel $K$ that is uniformly bounded (i.e. $||K(x,y)||_{2}\\,\\leq\\,M$ for any $x,y)$ , the $L^{2}$ norm of the function $||K(\\cdot,X)v||$ for fixed $X,v$ can be bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|K(\\cdot,X)v\\|^{2}=\\int v^{\\top}K(Y,X)^{\\top}K(Y,X)v d P(Y)\\leq\\int\\|K(Y,X)\\|_{2}^{2}\\|v\\|_{2}^{2}d P(Y)\\leq M^{2}\\|v\\|_{2}^{2}}\\\\ &{\\implies\\|K(\\cdot,X)v\\|\\leq M\\|v\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Secondly, we have again for any fixed $v$ and $X$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\left[K_{\\infty}(\\cdot,X)-K_{\\phi(0)}^{p}(\\cdot,X)\\right]v\\Vert^{2}=\\int v^{\\top}\\left[K_{\\infty}(Y,X)-K_{\\phi(0)}^{p}(Y,X)\\right]^{\\top}\\left[K_{\\infty}(Y,X)-K_{\\phi(0)}^{p}(X,X)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\int||K_{\\infty}(Y,X)-K_{\\phi(0)}^{p}(Y,X)||_{2}^{2}||v||_{2}^{2}d P(y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\left(\\operatorname*{sup}_{x,y}||K_{\\infty}(y,x)-K_{\\phi(0)}^{p}(y,x)||_{F}\\right)^{2}||v||_{2}^{2}}\\\\ &{\\implies||\\left[K_{\\infty}(\\cdot,X)-K_{\\phi(0)}^{p}(\\cdot,X)\\right]v||\\leq\\operatorname*{sup}_{x,y}||K_{\\infty}(y,x)-K_{\\phi(0)}^{p}(y,x)||_{F}\\cdot||v||_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since the matrix (spectral) 2-norm is dominated by the Frobenius norm. Plugging these facts into Equation (28) above, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\int_{0}^{T}\\mathbb{E}_{P(X)}M\\cdot\\|\\ell^{\\prime}(X,g_{\\ell}(X))-\\ell^{\\prime}(X,h_{t}(X))\\|_{2}+\\underset{s,y}{\\operatorname*{sup}}\\,|{\\boldsymbol K}_{\\infty}(x,y)-{\\boldsymbol K}_{\\varphi(0)}^{p}(x,y)|}\\\\ &{\\leq\\int_{0}^{T}\\mathbb{E}_{P(X)}M\\tilde{L}|g_{\\ell}(X)-h_{t}(X)||_{2}+\\tilde{M}\\underset{s,y}{\\operatorname*{sup}}\\,|{\\boldsymbol K}_{\\infty}(x,y)-{\\boldsymbol K}_{\\varphi(0)}^{p}(x,y)||d t\\,\\mathrm{by~(E4)}}\\\\ &{\\leq\\tilde{M}T\\underset{s,y}{\\operatorname*{sup}}\\,|{\\boldsymbol K}_{\\infty}(x,y)-{\\boldsymbol K}_{\\varphi(0)}^{p}(x,y)||_{F}+M\\tilde{L}\\int_{0}^{T}\\mathbb{E}_{P(X)}\\sqrt{\\|g_{\\ell}(X)-h_{t}(X)\\|_{2}^{2}}d t}\\\\ &{\\leq\\tilde{M}T\\underset{s,y}{\\operatorname*{sup}}\\,|{\\boldsymbol K}_{\\infty}(x,y)-{\\boldsymbol K}_{\\varphi(0)}^{p}(x,y)||_{F}+M\\tilde{L}\\int_{0}^{T}\\sqrt{\\mathbb{E}_{P(X)}\\|g_{\\ell}(X)-h_{t}(X)\\|_{2}^{2}}d t\\,\\mathrm{b}}\\\\ &{=\\tilde{M}T\\underset{s,y}{\\operatorname*{sup}}\\,|{\\boldsymbol K}_{\\infty}(x,y)-{\\boldsymbol K}_{\\varphi(0)}^{p}(x,y)||_{F}+M\\tilde{L}\\int_{0}^{T}\\|g_{\\ell}-h_{t}||d t}\\\\ &{\\leq\\tilde{M}T\\underset{s,y}{\\operatorname*{sup}}\\,||{\\boldsymbol K}_{\\infty}(x,y)-{\\boldsymbol K}_{\\varphi(0)}^{p}(x,y)||_{F}\\,\\mathrm{exp}(M\\tilde{L}T)\\,\\mathrm{by~Gromwall}^{\\gamma}\\,\\mathrm{inequality}\\,(\\mathrm{Theor}(\\Omega)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Proposition 2, there thus exists $P_{1}$ such that for all $p>P_{1}$ we have $\\begin{array}{r}{||g_{T}-h_{T}||\\leq\\frac{\\delta}{2}}\\end{array}$ almost surely. We proceed nearly identically for the term $||h_{T}-f_{T}||$ . We need only note that K\u03d5p(0) for sufficiently large $p$ , say $p>P_{2}$ , we can bound K\u03d5p(0) uniformly (almost surely) by a constant A > M. To see this, observe that by Proposition 2 we have that there exists almost surely a sufficiently large $P$ such that $\\mathrm{sup}_{x,y}\\,||K_{\\infty}(\\bar{x},y)\\bar{-K}_{\\phi(0)}^{p}(x,y)||_{F}<A-M$ and so by triangle inequality we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{sup}_{x,y}||K_{\\phi(0)}^{p}||_{F}\\leq\\operatorname*{sup}_{x,y}||K_{\\phi(0)}^{p}(x,y)-K_{\\infty}(x,y)||_{F}+||K_{\\infty}(x,y)||_{F}}}\\\\ &{\\leq\\operatorname*{sup}_{x,y}||K_{\\phi(0)}^{p}(x,y)-K_{\\infty}(x,y)||_{F}+\\operatorname*{sup}_{x,y}||K_{\\infty}(x,y)||_{F}}\\\\ &{\\leq A-M+M=A}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thereafter, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|h_{T}-f\\|_{0}^{\\alpha}\\|\\|\\alpha-h_{0})-(f_{T}-f_{0})\\|}&{}\\\\ {=\\bigg\\|\\int_{0}^{T}\\mathbb{E}_{\\mathcal{P}(\\mathcal{X})}\\left[K_{\\phi_{0}(\\cdot)}^{\\alpha}(\\cdot,X)^{\\ell}(X,h_{t}(X))-K_{\\phi_{0}(\\cdot)}^{\\nu}(\\cdot,X)^{\\ell}(X,f_{t}(X))\\right]d t\\bigg\\|}&{}\\\\ {\\leq\\int_{0}^{T}\\mathbb{E}_{\\mathcal{P}(\\mathcal{X})}\\|K_{\\phi(0)}^{\\alpha}(\\cdot,X)^{\\ell}(X,h_{t}(X))-K_{\\phi(0)}^{\\beta}(\\cdot,X)^{\\ell}(X,f_{t}(X))\\|d t}\\\\ &{\\leq\\int_{0}^{T}\\mathbb{E}_{\\mathcal{P}(\\mathcal{X})}\\|K_{\\phi(0)}^{\\alpha}(\\cdot,X)^{\\ell}(X,h_{t}(X))-K_{\\phi(0)}^{\\beta}(\\cdot,X)^{\\ell}(X,f_{t}(X))\\|1+}\\\\ {\\|K_{\\phi(0)}^{\\alpha}(\\cdot,X)^{\\ell}(X,f_{t}(X))-K_{\\phi(0)}^{\\beta}(\\cdot,X)^{\\ell}(X,f_{t}(X))\\||d t}&{}\\\\ {\\leq\\int_{0}^{T}\\mathbb{E}_{\\mathcal{P}(\\mathcal{X})}\\mathcal{A}\\|\\ell^{\\prime}(X,h_{t}(X))-\\ell^{\\prime}(X,f_{t}(X))\\|_{2}+}&{}\\\\ {\\underset{x\\in\\mathcal{Y}(0),T}{\\operatorname*{sup}}\\|K_{\\phi(0)}^{\\alpha}(x,y)-K_{\\phi(0)}^{\\beta}(x,y)\\|_{F}\\cdot|\\ell^{\\prime}(X,f_{t}(X))\\|d t}&{}\\\\ {\\leq\\hat{M}^{\\mathcal{T}}\\underset{x,y\\in\\{0,T\\}}{\\operatorname*{sup}}\\|K_{\\phi(0)}^{\\beta}(x,y)-K_{\\phi(0)}^{\\beta}(x,y)\\|_{F}+}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and we can similarly switch from $\\mathbb{E}_{P(X)}||h_{t}(X)-f_{t}(X)||_{2}$ to the $L_{2}$ norm $||h_{t}-f_{t}||$ as above using Jensen\u2019s inequality, yielding ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\leq\\tilde{M}T\\displaystyle\\operatorname*{sup}_{x,y,t\\in(0,T]}||K_{\\phi(0)}^{p}(x,y)-K_{\\phi(t)}^{p}(x,y)||_{F}+A\\tilde{L}\\displaystyle\\int_{0}^{T}||h_{t}-f_{t}||d t}}\\\\ {{\\leq\\tilde{M}T\\displaystyle\\operatorname*{sup}_{x,y,t\\in(0,T]}||K_{\\phi(0)}^{p}(x,y)-K_{\\phi(t)}^{p}(x,y)||_{F}\\exp\\Big(A\\tilde{L}T\\Big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Clearly, by the same logic as the above there exists $P_{3}$ such that $\\mathrm{~\\textit~{~p~}~}{}>\\mathrm{~\\textit~{~P~}~}$ implies $\\begin{array}{r}{\\tilde{M}T\\operatorname*{sup}_{x,y,t\\in(0,T]}||K_{\\phi(0)}^{p}(\\bar{x},y)-K_{\\phi(0)}^{p}(x,y)||\\exp(A\\tilde{L}T)\\leq\\delta/2}\\end{array}$ by Proposition 3. Then for all $p>\\operatorname*{max}(P_{1},P_{2},P_{3})$ , we have almost surely that $||h_{T}-f_{T}||\\leq\\delta/2$ . This completes the proof, as in this case we have by the triangle inequality that $||f_{T}-g_{T}||\\leq\\delta$ and so $[L(\\bar{f_{T}})-L(g_{T})|\\leq\\epsilon/2$ by construction. ", "page_idx": 25}, {"type": "text", "text": "F Experimental Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our code is publicly available at https://github.com/declanmcnamara/gcvi_neurips. We used PyTorch (Paszke et al., 2019) for our experiments in accordance with its license, and NVIDIA GeForce RTX 2080 Ti GPUs. ", "page_idx": 25}, {"type": "text", "text": "F.1 Toy Example ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Recall the generative model for this problem, given by the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Theta\\sim\\operatorname{Unif}[0,2\\pi]}}\\\\ {{Z\\sim\\mathcal{N}(0,\\sigma^{2})}}\\\\ {{X\\mid(\\Theta=\\theta,Z=z)\\sim\\delta\\left([\\cos(\\theta+z),\\sin(\\theta+z)]^{\\top}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The variable $\\sigma$ is a hyperparameter of the model that we take to be $\\sigma=0.5$ . The model is constructed in such a way that $\\bar{x}\\in\\mathring{\\mathbb{S}}^{1}$ satisfies assumptions (C1) and (D1), respectively. One thousand pairs of data points $\\bar{\\{\\theta_{i},x_{i}\\}}_{i=1}^{1000}$ were generated independently from the model above and fixed as the dataset for which the ground-truth latent parameter values are known. ", "page_idx": 25}, {"type": "text", "text": "We constructed scaled, dense single hidden-layer ReLU networks of varying widths, with $2^{j}$ neurons for $j=6,\\ldots,12$ with the same architecture as in Appendix C and the initialization described in condition (C2). All networks were trained to minimize the expected forward KL objective; stochastic gradients were estimated using batches of 16 independent simulated $(\\theta,x)$ pairs from the generative model, and SGD was performed using the Adam optimizer with a learning rate of $\\rho\\,=\\,0.0001$ . We employ a learning rate scheduler that scales the learning rate as $O(1/I)$ , where $I$ denotes the number of iterations. All models were ftited for 200,000 stochastic gradient steps, and the execution time is less than one hour. The natural parameter for the von Mises distribution is parameterized as $\\eta=f(x;\\phi)+0.0001$ . This small perturbation must be added because $f(\\cdot;\\phi)=0$ at initialization and because the value of $\\eta=0$ lies outside the natural parameter space for this variational family. ", "page_idx": 25}, {"type": "text", "text": "For the linearized neural network models, all training settings were the same except for the architecture. For these models, we first constructed neural networks as above for each width to compute the Jacobian evaluated at the initial weights $J_{\\phi}(x;\\phi_{0})$ . Thereafter, the model in $\\phi$ is fixed as ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(x;\\phi)=f(x;\\phi_{0})+J_{\\phi}(x;\\phi_{0})(\\phi-\\phi_{0})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\phi,\\phi_{0}$ are flattened vectors of parameters from the neural network architectures. Using this linearized model above, the parameter $\\phi$ is fitted by SGD as above. ", "page_idx": 25}, {"type": "text", "text": "The plots in Figure 1 of the manuscript are constructed by evaluating the average negative loglikelihood on the dataset at each iteration, i.e. for the fixed $n=1000$ pairs of observations above, we evaluate the finite-sample loss for the expected forward KL divergence. Up to fixed constants, this ", "page_idx": 25}, {"type": "text", "text": "quantity is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\frac{1}{n}\\sum_{i=1}^{n}\\log q(\\theta_{i};f(x_{i};\\phi))\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\phi$ is the current iterate of the parameters (either the neural network parameters or the flattened vector of parameters of the same size for the linearized model). The horizontal red line in Figure 1 is set at the value $\\begin{array}{r}{-{\\frac{1}{n}}\\sum_{i=1}^{n}\\log p(\\theta_{i}\\mid x_{i})}\\end{array}$ , where $p$ denotes the exact posterior distribution (computed using numerical integration over a fine grid of evaluation points). ", "page_idx": 26}, {"type": "text", "text": "F.2 Label Switching in Amortized Clustering ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Recall the amortized clustering model from the manuscript, given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{S\\sim\\mathcal{N}(0,100^{2})}}\\\\ {{Z\\mid(S=s)\\sim\\mathcal{N}\\left(\\left[\\begin{array}{c}{{\\mu_{1}+s}}\\\\ {{\\vdots}}\\\\ {{\\mu_{d}+s}}\\end{array}\\right],\\sigma^{2}I_{d}\\right)}}\\\\ {{X_{i}\\mid(Z=z)\\sim\\displaystyle\\sum_{j=1}^{d}p_{j}\\mathcal{N}(z_{j},\\tau^{2}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We take $\\sigma=1,\\tau=0.1$ and artificially fix $s=100$ as our realization from the prior on $S$ . Thereafter, we generate $N=1000$ independent, identically distributed samples $\\{x_{i}\\}_{i=1}^{1000}$ from the model above, conditional on observing . The other non-random hyperparameters are the cluster centers $\\mu\\,=\\,[-20,-10,0,10,\\bar{20}]^{\\top}$ , as well as the mixture weights for the mixture of Gaussians, which are uniform, i.e. $\\begin{array}{r}{p_{j}\\,=\\,\\frac{1}{d}}\\end{array}$ for all $j$ . These draws constitute the \u201cdata\u201d for this problem for which inference on $S$ is desired conditionally on the draws $x_{1},\\dots,x_{1000}$ ; separately, we seek to infer the random variable $Z$ conditional on the data as well. Although $Z$ and $S$ are closely related, we impose a mean-field variational family $q(S,Z\\mid X_{1}=x_{1},\\ldots,\\bar{X}_{N}=x_{n})=q(S\\mid\\dot{X}_{1}=x_{1},\\ldots,\\dot{X_{N}}=$ $x_{N})q(Z\\mid X_{1}=x_{1},\\ldots,X_{N}=x_{N})$ for ease, with both components taken to be isotropic Gaussian distributions. ", "page_idx": 26}, {"type": "text", "text": "We use two different parameterizations for each of $q(S)$ and $q(Z)$ . The mean parameterization fixes the variance at unity and aims to learn only the mean \u2013 in this case, the natural parameter of this family of distributions is simply the mean, so the network parameterizes the variational means $\\mu_{S},\\mu_{Z}$ for each of these distributions. We also use a natural parameterization with an unknown mean and variance, and in this case, the natural parameterizations $\\eta_{S},\\eta_{Z}$ are output by the networks, which we denote $f(\\cdot;\\phi)$ and $f(\\cdot;\\psi)$ for $S$ and $Z$ , respectively. Both amortized encoder networks $f(\\cdot,\\phi),f(\\cdot;\\psi)$ take the entire set of points $\\{x_{1},\\dots,x_{1000}\\}$ as input. The architecture should thus be permutation-invariant, as the order of these points does not matter for inference on the latent quantities of interest. We accomplish this by using a set transformer architecture for the networks, which achieves permutation invariance using self-attention blocks (Lee et al., 2019). ", "page_idx": 26}, {"type": "text", "text": "Parameters are ftited to minimize $L_{P}$ using SGD with stochastic gradients estimated using simulated draws of the same size as the observed data. We use a learning rate of 0.001 for fitting both $q(Z)$ and $q(S)$ , with schedules that decrease these as training progresses across 50,000 total gradient steps. We perform one hundred replicates of this experiment across different random seeds, each time generating a new dataset and reftiting the networks. Accordingly, there is a high computational cost: using 10 parallel processes, the experiment takes about 8 hours to run. We compare $L_{P}$ -based minimization to ftiting both networks to maximize an evidence lower bound (ELBO), using the IWBO with $K=10$ importance samples as the objective, but otherwise keeping all other hyperparameters the same. Figure 2 in the manuscript plots the mode of $q(S;f(x_{1},\\dots,x_{N};\\phi))$ across one hundred replicates of the experiment with different random seeds. The estimate should be approximately 100 to match the ground truth, and both ELBO-based and $L_{P}$ -based training perform well. ", "page_idx": 26}, {"type": "text", "text": "For inference on $Z$ , we extract the mode $\\hat{Z}\\in\\mathbb{R}^{5}$ as the point estimate and compute the $\\ell_{1}$ distance to the true draw $Z=z$ for our dataset (which is known a priori because the data were generated synthetically). ELBO-based training illustrates label-switching behavior, converging to a vector which is a permutation of the entries of the true draw $z$ , resulting in a large $\\ell_{1}$ distance. $L_{P}$ -based fitting does not suffer from this pathological behavior and instead converges rapidly to the global optimum. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "F.3 Rotated MNIST ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We use the MNIST dataset, freely available from torchvision under the BSD-3 License1 to fit a GAN generative model of MNIST digits. The simplistic GAN uses dense networks for both the discriminator and generator and is fti to the data with binary cross-entropy loss. Training was stopped when the generator produced realistic images, sufficient for our problem (see Figure 3). ", "page_idx": 27}, {"type": "text", "text": "The variational distribution $q(\\Theta;f(x_{1},\\dots,x_{N};\\phi))$ on the shared rotation angle is taken to be von Mises for minimization of $L_{P}$ . We parameterize $q$ using its natural parameterization as in Appendix F.1. The encoder for $\\Theta$ uses three Conv2D layers to increase the number of channels to 64 (with a kernel size of 3 and a stride length of 2), followed by a flattening layer and a three-layer dense network with LeakyReLU activations. Permutation invariance is imposed by a naive averaging step across all observations in the dataset $\\{x_{i}\\}_{i=1}^{N}$ that are provided to the network. The neural network architecture for this example thus differs significantly from the simple two-layer ReLU network, yet still demonstrates the same global convergence behavior. ", "page_idx": 27}, {"type": "text", "text": "We compare to a semi-amortized approach for maximizing the IWBO for this example. To compute the IWBO, the practitioner must posit a variational distribution on $z_{i}$ for each image $x_{i}$ . This is because only the full joint likelihood $p(\\theta,\\{z_{i}\\}_{i=1}^{N},\\{x_{i}\\}_{i=1}^{N})$ is readily available. Accordingly, for this method we construct a second network $f(\\cdot;\\psi)$ used to construct a variational distribution $q(Z_{i};f(X_{i};\\psi))$ on $Z_{i}$ for a given $X_{i}$ . This network is amortized over all images, yielding $q(Z_{i};f(x_{i};\\psi))$ for any image $x_{i}$ in the dataset $\\{x_{i}\\}_{i=1}^{N}$ . We parameterize these distributions on the latent representation as multivariate (isotropic) Gaussian with mean and log-scale parameters for each dimension the outputs of an encoder network. The architecture for this encoder consists of three Conv2D layers, each with kernel size 3 and stride of length 2. Across the three layers we increase the number of channels from a single channel (the input) up to 8 channels. After the convolutional layers, we perform a flattening layer followed a 2-layer dense network with ReLU activations. As the latent dimension for the data is known to be 64, the outputs of the encoder are 128-dimensional to parameterize the mean and log-scale across each dimension. ", "page_idx": 27}, {"type": "text", "text": "As there is only one rotation angle of interest we directly maximize the joint likelihood $p(\\theta,\\{z_{i}\\}_{i=1}^{N},\\{x_{i}\\}_{i=1}^{N})$ in $\\theta$ by targeting the IWBO. Conceptually, this approach is the same as placing a point mass variational family on $\\Theta$ , i.e., we simply initialize $\\theta_{0}$ prior to training and update its value directly to maximize the IWBO. This approach thus fits the parameters $\\{\\psi,\\bar{\\theta}_{0}\\}$ of $q(Z_{i};f(x_{i};\\psi))$ and $\\delta_{\\theta_{0}}$ , the distributions on the latent representations and the angle, respectively. These parameters are optimized using the Adam optimizer with initial learning rate 0.01 and square summable learning rate schedule. The angle parameter value is initialized at a variety of angles in different trials to produce Figure 4. ", "page_idx": 27}, {"type": "text", "text": "The advantageous marginalization properties of $L_{P}$ -based fitting allow it to perform inference on $\\theta$ without performing inference on the latent representations $Z_{i}$ ; thus, the distributions $q(Z_{i};f(x_{i};\\psi))$ need not be fitted when using this approach. We use the Adam optimizer for all parameter updates with initial learning rate of 0.0001 (and a square summable learning rate schedule) for $L_{P}$ -based training, which only fits the parameters $\\phi$ of $\\displaystyle\\overline{{q}}(\\Theta;f(x_{1},\\ldots,x_{N};\\phi))$ . ", "page_idx": 27}, {"type": "text", "text": "Minimization of $L_{P}$ trains for 100,000 gradient steps, although convergence is rapid, and so trajectories are truncated to produce Figure 4. Runs were performed in parallel across multiple processes for both methods and completed in less than one hour. ", "page_idx": 27}, {"type": "text", "text": "F.4 Local Optima vs. Global Optima ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For this experiment, the setting above is modified slightly to remove the nuisance latent variables $\\{Z_{i}\\}$ and put ELBO-based optimization on more equal footing with expected forward KL minimization. ", "page_idx": 27}, {"type": "text", "text": "The data are generated as follows: for $i\\,=\\,1,\\ldots,50$ , we fix latents $z_{i}\\overset{i i d}{\\sim}p(z)$ , and $\\theta_{\\mathrm{true}}\\,=\\,260$ degrees is set artificially. The digits $\\boldsymbol{x}_{i}^{0}$ are computed via the generative adversarial network and thereafter fixed for the rest of the problem (the superscript denotes a rotation angle of zero degrees). ", "page_idx": 27}, {"type": "text", "text": "With the unrotated digits fixed, the only random variables in the model are $\\Theta$ and the rotated versions of the digits. ", "page_idx": 28}, {"type": "text", "text": "Conditioning on the observed data $\\{x_{i}\\}_{i=1}^{50}$ , $\\Theta$ is thus the only latent to be inferred. In this setting, the ELBO can compute the full likelihood function $p(\\theta,\\{x_{i}\\}_{i=1}^{50})$ for any value of $\\Theta=\\theta$ (the same $\\mathrm{Uniform}(0,2\\pi)$ is placed on the angle). The simulation-based approach of expected forward KL minimization can proceed as usual: we draw $\\Theta\\sim\\mathrm{Uniform}(0,2\\pi)$ and simulated digits are obtained by rotating the digits according to the drawn angle and adding noise. ", "page_idx": 28}, {"type": "text", "text": "The variational distribution, as stated in the text, is Gaussian with fixed variance instead of the von Mises distribution used in the local optima section. This is necessary for a one-to-one comparison between the two methods, as the von Mises distribution is not reparameterizable for ELBO-based training. The mean of the Gaussian is parameterized via the same neural network architecture above for ftiting by both the ELBO and the expected forward KL. We use the Adam optimizer with learning rate 0.001 for 10,000 gradient steps. ", "page_idx": 28}, {"type": "text", "text": "For computing some of the metrics in Figure 7, we rely on approximations as these quantities are difficult to compute exactly. Letting $x_{\\mathrm{{true}}}$ denote the observed data $\\{x_{i}\\}_{i=1}^{50}$ , the forward KL divergence $\\mathrm{KL}(p(\\stackrel{\\bullet}{\\theta}|\\stackrel{\\bullet}{x}_{\\mathrm{true}})\\stackrel{\\bullet}{\\theta}||\\stackrel{\\bullet}{q}(\\theta\\stackrel{\\bullet}{\\theta}|\\stackrel{\\bullet}{x_{\\mathrm{true}}}))$ is estimated using self-normalized importance sampling with $K\\,=\\,1,000$ samples drawn from the prior as a proposal. The reverse KL divergence $\\mathrm{KL}(\\dot{q}(\\theta\\mid x_{\\mathrm{true}}))\\mid\\mid p(\\theta\\mid x_{\\mathrm{true}})$ is computed as the difference $\\bar{\\log{p(x)}}-\\mathrm{ELBO}(q)$ ; the log-evidence is approximated via the importance weighted bound (IWBO) with $K=1,000$ . ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The abstract and introduction outline the main contributions of our work relative to preceding work, making clear that our results are primarily theoretical while still having implications for practitioners. We make clear that for our results to hold, regularity conditions must be satisfied. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In Section 6, we articulate the main conditions necessary for results to hold. These are enumerated even more explicitly in Appendices C, D, and E. We acknowledge in Section 6 that in practice these assumptions may not be met, and in our experiments we anticipate several settings where the assumptions are not met exactly, but our results still approximately hold. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We include several lemmas and/or theorems in the main text. Due to space constraints, we do not enumerate all assumptions in the statements, but we do reference the assumptions numerically and list them explicitly in the relevant appendices. Our appendices contain detailed proofs of all lemmas and theorems provided. As the proof of Theorem 1 is rather involved, we devote a large portion of Section 4 to sketching the proof and providing intuition for the reader. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed information in Appendix F to allow for reproduction of results. If accepted, we will publicly release code to reproduce our experiments exactly. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 30}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the appendix, we include a link to the GitHub repository with all code necessary to reproduce experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide detailed information on all experiments in-text, with additional implementation details in the relevant appendices. We also plan to publicly release code for this paper on GitHub, where experiments can be replicated and various details (e.g., network architectures) can be examined to understand the results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Where reasonable, we show statistical significance by aggregating runs across different random seeds. We do this in Section 5.2. In our other experiments, which illustrate optimization trajectories, we omit error bars as this type of data does not readily admit uncertainty estimates. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Appendix F, we detail the hardware used (GPUs), the memory requirements, and the approximate runtime for each of our experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We have read the Code of Ethics and verify that the paper complies. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work is primarily theoretical in nature, although we hope practitioners will utilize it to improve inference in probabilistic modeling. While the actions of individual practitioners have the potential for positive or negative societal impacts, our work is several stages removed from direct paths to applications yielding undesirable outcomes for society. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We do not release any models with this paper that carry risk for misuse. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We credit and cite PyTorch in our appendices, used with license to implement experiments. Our other main asset used is the MNIST dataset, whose license we name in Appendix F. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No new assets are released with this work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: There are no human participants or crowdsourcing used in this work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: There are no human participants or crowdsourcing used in this work. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]