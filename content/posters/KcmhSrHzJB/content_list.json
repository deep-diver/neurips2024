[{"type": "text", "text": "On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Boyao Li\u2217 Department of Biostatistics and Bioinformatics Duke University boyao.li@duke.edu ", "page_idx": 0}, {"type": "text", "text": "Alexandar J. Thomson\u2217   \nDepartment of Computer Science Duke University   \nalexander.thomson@duke.edu ", "page_idx": 0}, {"type": "text", "text": "Houssam Nassif Meta Inc. houssamn@meta.com ", "page_idx": 0}, {"type": "text", "text": "Matthew M. Engelhard\u2020 Department of Biostatistics and Bioinformatics Duke University m.engelhard@duke.edu ", "page_idx": 0}, {"type": "text", "text": "David Page\u2020 Department of Biostatistics and Bioinformatics Duke University david.page@duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs), including large language models, offer state-of-the-art performance on many tasks, but they are difficult to interpret due to their complex structure, large number of latent variables, and the presence of nonlinear activation functions [Buhrmester et al., 2021]. To gain a precise statistical interpretation for DNNs, much progress has been made in linking them to probabilistic graphical models (PGMs). Variational autoencoders (VAEs) [Kingma and Welling, 2014] are an early example; more recent examples include probabilistic dependency graphs [Richardson, 2022] as well as work that relates recurrent neural networks (RNNs) with hidden Markov models (HMMs) [Choe et al., 2017] and convolutional neural networks (CNNs) with Gaussian processes (GPs) [Garriga-Alonso et al., 2018]. When such a connection is possible, potential benefits include: ", "page_idx": 0}, {"type": "text", "text": "\u2022 Clear statistical semantics for a trained DNN model, beyond merely providing the conditional distribution over output variables given input variables. Instead, PGMs provide a joint distribution over all variables including the latent variables.   \n\u2022 Ability to import any PGM algorithm into DNNs, such as belief propagation or MCMC, for example to reverse any DNN to use output variables as evidence and variables anywhere earlier as query variables. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Improved calibration, i.e., improved predictions of probabilities at the output nodes by incorporation of PGM algorithms. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we establish such a correspondence between DNNs of any structure and PGMs. Given an arbitrary DNN architecture, we first construct an infinite-width tree-structured PGM. We then demonstrate that during training, the DNN executes approximations of precise inference in the PGM during the forward propagation step. We prove our result exactly in the case of sigmoid activations. We indicate how it can be extended to ReLU activations by building on a prior result of Nair and Hinton [2010]. Because the PGM in our result is a Markov network, the construction can extend even further, to all nonnegative activation functions provided that proper normalization is employed. We argue that modified variants of layer normalization and batch normalization could be viewed as approximations to proper MN normalization in this context, although formal analyses of such approximations are left for future work. Finally, in the context of sigmoid activations we empirically evaluate how the second and third benefits listed above follow from the result, as motivated and summarized now in the next paragraph. ", "page_idx": 1}, {"type": "text", "text": "A neural network of any architecture with only sigmoid activations satisfies the definition of a Bayesian network over binary variables\u2014it is a directed acyclic graph with a conditional probability distribution at each node, conditional on the values of the node\u2019s parents\u2014and thus is sometimes called a Bayesian belief network (BBN). Nevertheless, it can be shown that the standard gradient used in neural network training (whether using cross-entropy or other common error functions) is inconsistent with the BBN semantics, that is, with the probability distribution defined by this BBN. On the other hand, Gibbs sampling is a training method consistent with the BBN semantics, and hence effective for calibration and for reversing any neural network, but it is woefully inefficient for training. Hamiltonian Monte Carlo (HMC) is more efficient than Gibbs and better ftis BBN semantics than SGD. We demonstrate empirically that after training a network quickly using SGD, calibration can be improved by fine-tuning using HMC. The specific HMC algorithm employed here follows directly from the theoretical result, being designed to approximate Gibbs-sampling in the theoretical, infinite-width tree structured Markov network. The degree of approximation is controlled by the value of a single hyperparameter that is also defined based on the theoretical result. ", "page_idx": 1}, {"type": "text", "text": "The present paper stands apart from many other theoretical analyses of DNNs that view DNNs purely as function approximators and prove theorems about the quality of function approximation. Here we instead show that DNNs may be viewed as statistical models, specifically PGMs. This work is also different from the field of Bayesian neural networks, where the goal is to seek and model a probability distribution over neural network parameters. In our work, the neural network itself defines a joint probability distribution over its variables (nodes). Our work therefore is synergistic with Bayesian neural networks but more closely related to older work on learning stochastic neural networks via expectation maximization (EM) [Amari, 1995] or approximate EM [Song et al., 2016]. ", "page_idx": 1}, {"type": "text", "text": "Although the approach is different, our motivation is similar to that of Dutordoir et al. [2021] and Sun et al. [2020] in their work to link DNNs to deep Gaussian processes (GPs) [Damianou and Lawrence, 2013]. By identifying the forward pass of a DNN with the mean of a deep GP layer, they aim to augment DNNs with advantages of GPs, notably the ability to quantify uncertainty over both output and latent nodes. What distinguishes our work from theirs is that we make the DNN-PGM approximation explicit and include all sigmoid DNNs, not just unsupervised belief networks or other specific cases. ", "page_idx": 1}, {"type": "text", "text": "All code needed to reproduce our experimental results may be found at https://github.com/ engelhard-lab/DNN_TreePGM. ", "page_idx": 1}, {"type": "text", "text": "Syntactically a Bayesian network (BN) is a directed acyclic graph, like a neural network, whose nodes are random variables. Here we use capital letters to stand for random variables, and following Russell and Norvig [Russell and Norvig, 2020] and others, we take a statement written using such variables to be a claim for all specific settings of those variables. Semantically, a BN represents a full joint probability distribution over its variables as $\\begin{array}{r}{P(\\vec{V})=\\prod_{i}P(V_{i}|p a(V_{i}))}\\end{array}$ , where $p a(V_{i})$ denotes the parents of variable $V_{i}$ . If the conditional probability d istributions (CPDs) $P(V_{i}|p a(V_{i}))$ are all logistic regression models, we refer to the network as a sigmoid BN. ", "page_idx": 2}, {"type": "text", "text": "It is well known that given sigmoid activation and a cross-entropy error, training a single neuron by gradient descent is identical to training a logistic regression model. Hence, a neural network under such conditions can be viewed as a \u201cstacked logistic regression model\u201d, and also as a Bayesian network with logistic regression CPDs at the nodes. Technically, the sigmoid BN has a distribution over the input variables (variables without parents), whereas the neural network does not, and all nodes are treated as random variables. These distributions are easily added, and distributions of the input variables can be viewed as represented by the joint sample over them in our training set. ", "page_idx": 2}, {"type": "text", "text": "A Markov network (MN) syntactically is an undirected graph with potentials $\\phi_{i}$ on its cliques, where each potential gives the relative probabilities of the various settings for its variables (the variables in the clique). Semantically, it defines the full joint distribution on the variables as $\\begin{array}{r}{P(\\vec{V})=\\frac{1}{Z}\\prod_{i}\\phi_{i}(\\vec{V})}\\end{array}$ where the partition function $Z$ is defined as $\\textstyle\\sum_{\\vec{V}}\\prod_{i}\\phi_{i}(\\vec{V})$ . It is common to use a loglinear form of the same MN, which can be obtained by tr eating a setting of the variables in a clique as a binary feature $f_{i}$ , and the natural log of the corresponding entry for that setting in the potential for that clique as a weight $w_{i}$ on that feature; the equivalent definition of the full joint is then $\\begin{array}{r}{\\bar{P}(\\vec{V})=\\frac{1}{Z}e^{\\sum_{i}w_{i}f_{i}(\\vec{V})}}\\end{array}$ For training and prediction at this point the original graph itself is superfluous. ", "page_idx": 2}, {"type": "text", "text": "The potentials of an MN may be on subsets of cliques; in that case we simply multiply all potentials on subsets of a clique to derive the potential on the clique itself. If the MN can be expressed entirely as potentials on edges or individual nodes, we call it a \u201cpairwise\u201d MN. An MN whose variables are all binary is a binary MN. ", "page_idx": 2}, {"type": "text", "text": "A DNN of any architecture is, like a Bayesian network, a directed acyclic graph. A sigmoid activation can be understood as a logistic model, thus giving a conditional probability distribution for a binary variable given its parents. Thus, there is a natural interpretation of a DNN with sigmoid activations as a Bayesian network (e.g., Bayesian belief network). Note, however, that when the DNN has multiple, stacked hidden nodes, the values calculated for those nodes in the DNN by its forward pass do not match the values of the corresponding hidden nodes in a Bayesian network. Instead, for the remainder of this paper, we adopt the view that the DNN\u2019s forward pass might serve as an approximation to an underlying PGM and explore how said approximation can be precisely characterized. As reviewed in Appendix A, this Bayes net in turn is equivalent to (represents the same probability distribution) as a Markov network where every edge of weight $w$ from variable $A$ to variable $B$ has a potential of the following form: ", "page_idx": 2}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/56a3e6f0b87865a1b14395381d567392b2db3886e846c13aad95ba0d67fd37f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "For space reasons, we assume the reader is already familiar with the Variable Elimination (VE) algorithm for computing the probability distribution over any query variable(s) given evidence (known values) at other variables in the network. This algorithm is identical for Bayes nets and Markov nets. It repeatedly multiplies together all the potentials (in a Bayes net, conditional probability distributions) involving the variable to be eliminated, and then sums that variable out of the resulting table, until only the query variable(s) remain. Normalization of the resulting table yields the final answer. VE is an exact inference algorithm, meaning its answers are exactly correct. ", "page_idx": 2}, {"type": "text", "text": "3 The Construction of Tree-structured PGMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Although both a binary pairwise Markov network (MN) and a Bayesian network (BN) share the same sigmoid functional structure as a DNN with sigmoid activations, it can be shown that the DNN does not in general define the same probability for the output variables given the input variables: forward propagation in the DNN is very fast but yields a different result than VE in the MN or BN, which can be much slower because the inference task is NP-complete. Therefore, if we take the distribution $\\mathcal{D}$ defined by the BN or MN to be the correct meaning of the DNN, the DNN must be using an approximation $\\mathcal{D}^{\\prime}$ to $\\mathcal{D}$ . Procedurally, the approximation can be shown to be exactly the following: the DNN repeatedly treats the expectation of a variable $V$ , given the values of $V$ \u2019s parents, as if it were the actual value of $V$ . Thus previously binary variables in the Bayesian network view and binary features in the Markov network view become continuous. This procedural characterization of the approximation of $\\mathcal{D}^{\\prime}$ to $\\mathcal{D}$ yields exactly the forward pass in the neural network within the space of a similarly structured PGM, yet, on its own, does not yield a precise joint distribution for said PGM. We instead prefer in the PGM literature to characterize approximate distributions such as $\\mathcal{D}^{\\prime}$ with an alternative PGM that precisely corresponds to $\\mathcal{D}^{\\prime}$ ; for example, in some variational methods we may remove edges from a PGM to obtain a simpler PGM in which inference is more efficient. Treewidth-1 (tree-structured or forest-structured) PGMs are among the most desirable because in those models, exact inference by VE or other algorithms becomes efficient. We seek to so characterize the DNN approximation here. ", "page_idx": 3}, {"type": "text", "text": "This approach aligns somewhat with the idea of the computation tree that has been used to explore the properties of belief propagation by expressing the relevant message passing operations in the form of a tree [Tatikonda and Jordan, 2002, Ihler et al., 2005, Weitz, 2006]. Naturally the design of the tree structured PGM proposed here differs from the computation trees for belief propagation as we instead aim to capture the behavior of the forward pass of the neural network. Nonetheless, both methods share similar general approaches, the construction of a simpler approximate PGM, and aims, to better understand the theoretical behavior of an approximation to a separate original PGM. ", "page_idx": 3}, {"type": "text", "text": "To begin, we consider the Bayesian network view of the DNN. Our first step in this construction is to copy the shared parents in the network into separate nodes whose values are not tied. The algorithm for this step is as follows: ", "page_idx": 3}, {"type": "text", "text": "a. Consider the observed nodes in the Bayesian network that correspond to the input of the neural network and their outgoing edges.   \nb. At each node, for each outgoing edge, create a copy of the current node that is only connected to one of the original node\u2019s children with that edge. Since these nodes are observed at this step, these copies do all share the same values. The weights on these edges remain the same.   \nc. Consider then the children of these nodes. Again, for each outgoing edge, make a copy of this node that is only connected to one child with that edge. In this step, for each copied node, we then also copy the entire subgraph formed by all ancestor nodes of the current node. Note that while weights across copies are tied, the values of the copies of any node are not tied. However, since we also copy the subtree of all input and intermediary hidden nodes relevant to a forward pass up to each copy, the probability of any of these copied nodes being true remains the same across copies (ignoring the influence of any information passed back from their children).   \nd. We repeat this process across each layer until we have separate trees for each output node in the original deep neural network graph. ", "page_idx": 3}, {"type": "text", "text": "This process ultimately creates a graph whose undirected structure is a tree or forest. In the directed structure, trees converge at the output nodes. The probability of any copy of a latent node given the observed input (and ignoring any information passed back through a node\u2019s descendant) is the same across all the copies, but when sampling, their values may not be. ", "page_idx": 3}, {"type": "text", "text": "The preceding step alone is still not sufficient to accurately express the deep neural network as a PGM. Recall that in the probabilistic graphical model view of the approximation made by the DNN\u2019s forward pass, the neural network effectively takes a local average, in place of its actual value, from the immediately previous nodes and passes that information only forward. The following additional step in the construction yields this same behavior. This next step of the construction creates $L$ copies of every non-output node in the network (starting at the output and moving backward) while also copying the entire ancestor subtrees of each of these nodes, as was done in step 1. The weight of a ", "page_idx": 3}, {"type": "image", "img_path": "KcmhSrHzJB/tmp/58f83b5a7adbf2415e563d39b9c249b6826a722e2df64f033738b9715045bdd0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "KcmhSrHzJB/tmp/7f8fd73099bad940f20550694549f81a1b52330cc9b601b2d972318e33f25249.jpg", "img_caption": ["(a) The neural network\u2019s graphical structure before (b) The neural network\u2019s graphical structure after applying the first step of this PGM construction. applying the first step of this PGM construction. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: The first step of the PGM construction where shared latent parents are separated into copies along with the subtree of their ancestors. Copies of nodes H1 and H2 are made in this example. ", "page_idx": 4}, {"type": "text", "text": "copied edges is then set to its original value divided by $L$ . Note that this step results in a number of total copies that grows exponentially in the number of layers (i.e. $L$ copies in the 2nd to last layer, $L^{2}$ copies in the layer before, etc). Detailed algorithms for the two steps in the construction of the infinite tree-structured PGM are presented in Appendix B. As $L$ approaches infinity, we show that both inference and the gradient in this PGM construction matches the forward pass and gradient in the neural network exactly. ", "page_idx": 4}, {"type": "text", "text": "This second step in the construction can be thought of intuitively by considering the behavior of sampling in the Bayesian network view. Since we make $L$ copies of each node while also copying the subgraph of its ancestors, these copied nodes all share the same probabilities. As $L$ grows large, even if we sampled every copied node only once, we would expect the average value across these $L$ copies to match the probability of an individual copied node being true. Given that we set the new weights between these copies and their parents as the original weights divided by $L$ , the sum of products (new weights times parent values) yields the average parent value multiplied by the original weight. As $L$ goes to infinity, we remove sampling bias and the result exactly matches the value of the sigmoid activation function of the neural network, where this expectation in the PGM view is passed repeatedly to the subsequent neurons. The formal proof of this result, based on variable elimination, is found in Appendix C. There, we show the following: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. In the PGM construction, as $L\\rightarrow\\infty$ , $\\begin{array}{r}{P(H=1|\\vec{x})\\to\\sigma(\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\theta_{i}\\sigma(p_{i})),}\\end{array}$ for an arbitrary latent node $H$ in the DNN that has observed parents $g_{1},...,g_{M}$ and latent parents $h_{1},...,h_{N}$ that are true with probabilities $\\sigma(p_{1}),...,\\sigma(p_{N})$ . Here, $\\sigma(\\cdot)$ is the logistic sigmoid function and $w_{1},...,w_{M}$ and $\\theta_{1},...,\\theta_{N}$ are the weights on edges between these nodes and $H$ . ", "page_idx": 4}, {"type": "text", "text": "The PGM of our construction is a Markov network that always has evidence at the nodes $\\vec{X}$ corresponding to the input nodes of the neural network. As such, it is more specifically a conditional random field (CRF). Theorem 1 states the probability that a given node anywhere in the CRF is true given $\\vec{X}$ equals the output of that same node in the neural network given input $\\vec{X}$ . The CRF may also have evidence at the nodes $\\vec{Y}$ that correspond to the output nodes of the neural network. Given the form of its potentials, as illustrated at the end of Section 2, the features in the CRF\u2019s loglinear form correspond exactly to the edges and are true if and only if the nodes on each end of the edge are true. It follows that the gradient of this CRF can be written as a vector with one entry for each feature $f$ corresponding to each weight $w$ of the neural network, of the form $P(f|\\vec{X})-P(f|\\vec{X},\\vec{Y})$ . Building on Theorem 1, this gradient of the CRF can be shown to be identical to the gradient of the cross-entropy loss in the neural network: the partial derivative of the cross-entropy loss with respect to the weight $w$ on an edge, or feature $f$ of the CRF, is $P(f|\\vec{X})-P(f|\\vec{X},\\vec{Y})$ . This result is more precisely stated below in Theorem 2 below, which is proven in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. In the PGM construction, as $L\\to\\infty,$ , the derivative of the marginal log-likelihood, where all hidden nodes have been summed out, with respect to a given weight exactly matches the derivative of the cross entropy loss in the neural network with respect to the equivalent weight in its structure. ", "page_idx": 5}, {"type": "text", "text": "4 Implications and Extensions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We are not claiming that one should actually carry out the PGM construction used in the preceding section, since that PGM is infinite. Rather, its contribution is to give precise semantics to an entire neural network, as a joint probability distribution over all its variables, not merely as a machine computing the probability of its output variables given the input variables. Any Markov network, including any CRF, precisely defines a joint probability distribution over all its variables, hidden or observed, in a standard, well-known fashion. The particular CRF we constructed is the right one in the very specific sense that it agrees with the neural network exactly in the gradients both use for training (Theorem 2). While the CRF is infinite, it is built using the original neural network as a template in a straightforward fashion and is tree-structured, and hence it is easy to understand. Beyond these contributions to pedagogy and comprehensibility, are there other applications of the theoretical results? ", "page_idx": 5}, {"type": "text", "text": "One application is an ability to use standard PGM algorithms such as Markov chain Monte Carlo (MCMC) to sample latent variables given observed values of input and output variables, such as for producing confidence intervals or understanding relationships among variables. One could already do so using Gibbs sampling in the BN or MN directly represented by the DNN itself (which we will call the \u201cdirect PGM\u201d), but then one wouldn\u2019t be using the BN or MN with respect to which SGD training in the DNN is correct. For that, our result has shown that one instead needs to use Gibbs sampling in the infinite tree-structured PGM, which is impractical. Nevertheless, for any variable $V$ in the original DNN, on each iteration a Gibbs sampler takes infinitely many samples of $V$ given infinitely many samples of each of the members of $V$ \u2019s Markov blanket in the original DNN. By treating the variables of the original DNN as continuous, with their values approximating their sampled probabilities in the Gibbs sampler, we can instead apply Hamiltonian Monte Carlo or other MCMC methods for continuous variables in the much smaller DNN structure. We explore this approach empirically rather than theoretically in the next section. Another, related application of our result is that one could further fine-tune the trained DNN using other PGM algorithms, such as contrastive divergence. We also explore this use in the next section. ", "page_idx": 5}, {"type": "text", "text": "One might object that most results in this paper use sigmoid activation functions. Nair and Hinton showed that rectified linear units (ReLU) might be thought of as a combination of infinitely many sigmoid units with varying biases [Nair and Hinton, 2010]. Hence our result in the previous section can be extended to ReLU activations by the same argument. More generally, with any non-negative activation function that can yield values greater than one, while our BN argument no longer holds, the MN version of the argument can be extended. An MN already requires normalization to represent a probability distribution. While Batch Normalization and Layer Normalization typically are motivated procedurally, to keep nodes from \u201csaturating,\u201d and consequently to keep gradients from \u201cexploding\u201d or \u201cvanishing,\u201d as the names suggest, they might also be used to bring variables into the range $[0,1]$ and hence to being considered as probabilities. Consider an idealized variant of these that begins by normalizing all the values coming from a node $h$ of a neural network, over a given minibatch, to sum to 1.0; the argument can be extended to a set of $h$ and all its siblings in a layer (or other portion of the network structure) assumed to share their properties. It is easily shown that if the parents of any node $h$ in the neural network provide to $h$ approximate probabilities that those parent variables are true in the distribution defined by the Markov network given the inputs, then $h$ in turn provides to its children an approximate probability that $h$ is true in the distribution defined by the Markov network given the inputs. Use of a modified Batch or Layer Normalization still would be only approximate and hence adds an additional source of approximation to the result of the preceding section. Detailed consideration of other activation functions is left for further work; in the next section we return to the sigmoid case. ", "page_idx": 5}, {"type": "text", "text": "To illustrate the potential utility of the infinite tree-structured PGM view of a DNN, in this section we pursue one of its implications in greater depth; other implications for further study are summarized in the Conclusion. We have already noted we can view forward propagation in an all-sigmoid DNN as exact inference in a tree-structured PGM, such that the CPD of each hidden variable is a logistic regression. In other words, each hidden node is a Bernoulli random variable, with parameter $\\lambda$ being a sigmoid activation (i.e. logistic function) applied to a linear function of the parent nodes. This view suggests alternative learning or fine-tuning algorithms such as contrastive divergence (CD) [Carreira-Perpinan and Hinton, 2005, Bengio and Delalleau, 2009, Sutskever and Tieleman, 2010]. CD in a CRF uses MCMC inference with many MCMC chains to estimate the joint probability over the hidden variables given the evidence (the input and output variables in a standard DNN), and then takes a gradient step based on the results of this inference. But to increase speed, CD- $^{\\cdot n}$ advances each MCMC chain only $n$ steps before the next gradient step, with CD-1 often being employed. CD has a natural advantage over SGD, which samples the values of the hidden variables using only the evidence at the input values; instead, MCMC in CD uses all the available evidence, both at input and output variables. Unfortunately, if the MCMC algorithm employed is Gibbs sampling on the many hidden variables found in a typical neural network, then it suffers from high cost in computational resources. MCMC has now advanced far beyond Gibbs sampling with methods such as Hamiltonian Monte Carlo (HMC), but HMC samples values in [0, 1] rather than $\\{0,1\\}$ . Our theoretical results for the first time justify the use of HMC rather than Gibbs sampling in a DNN, as follows. ", "page_idx": 6}, {"type": "text", "text": "Recall that the DNN is itself a BN with sigmoid CPDs, but if we take the values of the hidden variables to be binary then DNN training is not correct with respect to this BN. Instead, based on the correctness of our infinite tree-structured PGM, the probabilistic behavior of one hidden node in the BN is the result of sampling values across its $L$ copies in the PGM. Within any copy, the value of the hidden node follows the Bernoulli distribution with the same probability distribution as the other copies, determined by the parent nodes. Since all the copies share the same parent nodes by the construction and are sampled independently, the sample average follows a normal distribution as the asymptotic distribution when $L\\rightarrow\\infty$ by the central limit theorem. In practice, $L$ is finite and this normal distribution is a reasonable approximation to the distribution of the hidden node. Thus in the BN, whose variables correspond exactly with those of the DNN, the variables have domain [0,1] rather than $\\{0,1\\}$ , as desired. We precisely define this BN and resulting HMC algorithm next. ", "page_idx": 6}, {"type": "text", "text": "5.1 Learning via Contrastive Divergence with Hamiltonian Monte Carlo Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Consider a Bayesian network composed of input variables $\\pmb{x}=\\pmb{h}_{0}$ , a sequence of layers of hidden variables $h_{1},...,h_{K}$ , and output variables $\\textit{\\textbf{y}}$ . Each pair of consecutive layers forms a bipartite subgraph of the network as a whole, and the variables $\\pmb{h}_{i}=(h_{i1},...,h_{i M_{i}})$ follow a multivariate normal distribution with parameters $\\pmb{p}_{i}=(p_{i1},...,p_{i M_{i}})$ that depend on variables in the previous layer $\\displaystyle h_{i-1}$ as follows: ", "page_idx": 6}, {"type": "text", "text": "where $\\sigma:\\mathbb{R}\\rightarrow(0,1)$ is a non-linearity \u2013 here the logistic function \u2013 that is applied element-wise, and $\\pmb{\\theta}_{i}=(\\pmb{W}_{i},\\pmb{b}_{i})$ are parameters to be learned. The distribution in equation (1) is motivated by supposing that $h_{i j}$ is the average of $L$ copies of the corresponding node in the PGM, each of which is 1 with probability $p_{i j}$ and zero otherwise, then applying the normal approximation to the binomial distribution. Importantly, this approximation is valid only for large $L$ . ", "page_idx": 6}, {"type": "text", "text": "For a complete setting of the variables $\\{x,h,y\\}$ , where $\\pmb{h}=\\{h_{1},...,h_{K}\\}$ , and parameters $\\pmb\\theta=$ $\\{\\pmb{\\theta}_{i}\\}_{i=0}^{K}$ , the likelihood $p(\\pmb{y},\\pmb{h}|\\pmb{x};\\pmb{\\theta})$ may be decomposed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\np(\\pmb{y},\\pmb{h}|\\pmb{x};\\pmb{\\theta})=p(\\pmb{y}|\\pmb{h}_{K};\\pmb{\\theta}_{K})\\cdot\\prod_{i=1}^{K}\\prod_{j=1}^{M_{i}}p_{N}(h_{i j}|p_{i j}(\\pmb{h}_{i-1};\\pmb{\\theta}_{i-1})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p_{\\mathcal{N}}(\\cdot|\\cdot)$ denotes the normal density, and a specific form for $p(y|h_{K};\\theta_{K})$ has been omitted to allow variability in the output variables. In our experiments, $\\textit{\\textbf{y}}$ is a Bernoulli(binary) or categorical random variable parameterized via the logistic(sigmoid) or softmax function, respectively. ", "page_idx": 6}, {"type": "text", "text": "Let ${\\pmb h}^{(0)},{\\pmb h}^{(1)},{\\pmb h}^{(2)},\\dots$ denote a chain of MCMC samples of the complete setting of hidden variables in the neural network. As previously noted, we allow hidden variables $h_{i j}\\in(0,1)$ for $i\\in\\{1,...,K\\}$ ", "page_idx": 6}, {"type": "text", "text": "and $j\\in\\{1,...,M_{i}\\}$ , and use Hamiltonian Monte Carlo (HMC) to generate the next state due to its fast convergence. Since HMC samples are unbounded, we sample the logit associated with $h_{i j}\\in(0,1)$ , i.e. $\\sigma^{-1}(h_{i j})\\in(-\\infty,\\infty)$ , rather than sampling the $h_{i j}$ directly. ", "page_idx": 7}, {"type": "text", "text": "The HMC trajectories are defined by Hamilton\u2019s Equations: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{d\\rho_{i}}{d t}=\\frac{\\partial H}{\\partial\\mu_{i}}\\qquad\\qquad\\qquad\\qquad\\frac{d\\mu_{i}}{d t}=-\\frac{\\partial H}{\\partial\\rho_{i}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\rho_{i},\\mu_{i}$ are the $i$ th component of the position and momentum vector. They are intermediate variables used to generate a new state for the MCMC chain. The Hamiltonian $H$ is ", "page_idx": 7}, {"type": "equation", "text": "$$\nH=H(\\pmb\\rho,\\pmb\\mu)=U(\\pmb\\rho)+\\frac12\\pmb\\mu^{T}M^{-1}\\pmb\\mu\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $M^{-1}$ is a positive definite convariance matrix and acts as a metric to rotate and scale the target distribution, which is usually set to identity matrix in practice. Defining the position $\\rho=h$ , the complete set of hidden variables of the network, we have that the potential energy $U$ is the negative log-likelihood associated with equation (2): ", "page_idx": 7}, {"type": "equation", "text": "$$\nU(h)=-\\log p(\\pmb{y},h|\\pmb{x};\\pmb{\\theta})=-\\log p(\\pmb{y}|h_{K};\\pmb{\\theta}_{K})-\\sum_{i=1}^{K}\\sum_{j=1}^{M_{i}}\\log p_{N}(h_{i j}|p_{i j}(\\pmb{h}_{i-1};\\pmb{\\theta}_{i-1})).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We set the leap frog size $l\\,>\\,0$ , step size $\\Delta t\\,>\\,0$ . A description of the HMC trajectories (i.e., evolution of $^h$ ) is provided in Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 7}, {"type": "text", "text": "The initial state of the chain $\\pmb{h}^{(0)}$ is drawn with a simple forward pass through the network, ignoring the output variables; in other words, we have hi(j0) \u223cN(\u03c3(W i(\u22120)1hi(0\u2212)1 + bi(0\u2212)1)j) for i \u2208{1, ...K}, where $h_{0}=x$ are the input variables, and the values of $\\pmb{W}_{i}^{(0)}$ and $\\boldsymbol{b}_{i}^{(0)}$ are manually set or drawn from a standard normal or uniform distribution. We update $^h$ through a number of burn-in steps before beginning to update our parameters to ensure that $^h$ is first consistent with evidence from the output variables. After $k$ steps, corresponding to $\\mathrm{CD-}k$ , we define the loss based on equation (2): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\pmb{\\theta}^{(n)})=-\\log p(\\pmb{y},h|\\pmb{x};\\pmb{\\theta}^{(n)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We then apply the following gradients to update the parameters $\\{W_{i}^{(n)}\\}_{i=0}^{K}$ and $\\{b_{i}^{(n)}\\}_{i=0}^{K}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nW_{i}^{(n+1)}=W_{i}^{(n)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial W_{i}^{(n)}}\\qquad\\qquad\\qquad b_{i}^{(n+1)}=b_{i}^{(n)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial b_{i}^{(n)}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\eta$ is the learning rate. Algorithm 3 (see Appendix F) summarizes this procedure. ", "page_idx": 7}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The previous section discussed the modeling of the HMC learning algorithm inspired by the construction of tree-structured PGMs. This section compares the proposed algorithm to Gibbs sampling and SGD training with DNNs in both the synthetic experiments and experiments on Covertype dataset [Blackard, 1998], which is a real-world dataset for classification. They are designed to illustrate how the HMC-based algorithm could fine-tune and improve the calibration of DNNs. The experimental setup and additional experiments are described in Appendix. An internal cluster of GPUs was employed for all experiments, and part of run-times are provided in the appendix; as anticipated, SGD is faster than HMC, which is faster than Gibbs. ", "page_idx": 7}, {"type": "text", "text": "5.2.1 Synthetic experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The synthetic datasets are generated by simple BNs and MNs with their weights in different ranges, which are used to define the conditional probabilistic distributions for BNs and potentials for MNs. Each dataset contains 1000 data points $\\{(\\mathbf{\\bar{X}}_{i},y_{i})\\},i=1,2,...,1000$ , where each input $X_{i}\\in\\{0,1\\}^{n}$ is a binary vector with $n$ dimension and each output $y_{i}~\\in~\\{0,1\\}$ is a binary value. The true probabilistic distribution $P(y|X)$ of the corresponding BN/MN is calculated by sampling or applying the VE algorithm on it. ", "page_idx": 7}, {"type": "text", "text": "To explore how the proposed algorithm performs in model calibration, a DNN is first trained with SGD for 100 or 1000 epochs, and then fine-tuned by Gibbs or HMC with different $L$ \u2019s for 20 epochs based on the trained DNN model. Here $L$ defines the normal distribution for hidden nodes in Eqn. 1 and is explored across the set of values: $\\{10,100,1000\\}$ . The calibration is assessed by mean absolute error (MAE) in all the synthetic experiments and compared between non-extra fine-tuning (shown in the \"DNN\" column in Table 1) and fine-tuning with Gibbs or HMC. Since the ground truth of $P(y|X)$ in the synthetic dataset can be achieved from the BN/MN, the MAE is calculated by comparing the predicted $P(y|X)$ from the finetuned network and the true probability. ", "page_idx": 8}, {"type": "text", "text": "Table 1 shows that in general, DNN results tend to get worse with additional training, particularly with smaller weights, and the HMC-based fine-tuning approaches can mitigate this negative impact of additional training on the model calibration. Across all the HMC with different $L$ \u2019s, HMC $(L{=}10)$ performs better than the others and DNN training itself for BNs and MNs with smaller weights. Additionally, the MAE of HMC $(L{=}10)$ tends to be similar to Gibbs but runs much faster, especially in the BN simulations, whereas HMC $\\scriptstyle{L=1000})$ is more similar to the NN. This is consistent with what we have argued in the theory that when $L$ goes smaller, the number of the sampled copies for each hidden node decreases in our tree-PGM construction and HMC sampling performs more similar to Gibbs sampling; and as $L$ increases, the probability of each hidden node given the input approaches the result of the DNN forward propagation and thus HMC performs more similar to DNN training. ", "page_idx": 8}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/fc7af7704b5c7550685beb4e6fdadcd5a4cad9ded7914e53a50e6bd6af1e0051.jpg", "table_caption": ["Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2.2 Covertype Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Similar experiments are also run on the Covertype dataset to compare the calibration of SGD in DNNs, Gibbs and the HMC-based algorithm. Since the ground truth for the distribution of $P(y|X)$ cannot be found, the metric for the calibration used in this experiment is the expected calibration error (ECE), which is a common metric for model calibration. To simplify the classification task, we choose the data with label 1 and 2 and build two binary subsets, each of which contains 1000 data points. Similarly, the number of training epochs is also 100 or 1000, while the fine-tuning epochs shown in Table 2 is 20. ", "page_idx": 8}, {"type": "text", "text": "Table 2 shows that HMC with $L=10$ fine-tuning generally performs better than DNN results, and HMC with $L\\,=\\,1000$ has the similar ECE as that in DNN. It meets the conclusion made in the synthetic experiments. Gibbs sampling, however, could perform worse than just using DNN. It could be because Gibbs may be too far removed from the DNN, whereas our proposed HMC is more in the middle. This suggests perhaps future work testing the gradual shift from DNN to HMC to Gibbs. ", "page_idx": 8}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/f01d1213c98abc386d8590a854491f383aba4b3e3a7f88ebee863dbf189cd04e.jpg", "table_caption": ["Table 2: Calibration performance on Covertype datasets. Highlighted cells show the best calibrations among each row. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion, Limitations, and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have established a new connection between DNNs and PGMs by constructing an infinite-width tree-structured PGM corresponding to any given DNN architecture, then showing that inference in this PGM corresponds exactly to forward propagation in the DNN given sigmoid activation functions. This theoretical result is valuable in its own right, as it provides new perspective that may help us understand and explain relationships between PGMs and DNNs. Moreover, we anticipate it will inspire new algorithms that merge strengths of PGMs and DNNs. We have explored one such algorithm, a novel HMC-based algorithm for DNN training or fine-tuning motivated by our PGM construction, and we illustrated how it can be used to improve to improve DNN calibration. ", "page_idx": 9}, {"type": "text", "text": "Limitations of the present work and directions for future work include establishing formal results about how closely batch- and layer-normalization can be modified to approximate Markov network normalization when using non-sigmoid activations, establishing theoretical results relating HMC in the neural network to Gibbs sampling in the large treewidth-1 Markov network, and obtaining empirical results for HMC with non-sigmoid activations. Also of great interest is comparing HMC and other PGM algorithms to Shapley values, Integrated Gradients, and other approaches for assessing the relationship of some latent variables to each other or to inputs and/or outputs in a neural network. We note that the large treewidth-1 PGM is a substantial approximation to the direct PGM of a DNN \u2013 in other words, the PGM whose structure exactly matches that of the DNN. In future work, we will explore other DNN fine-tuning methods, perhaps based on loopy belief propagation or other approximate algorithms often used in PGMs, that may allow us to more closely approximate inference in this direct PGM. ", "page_idx": 9}, {"type": "text", "text": "Another direction for further work is in the original motivation for this work. Both DNNs and PGMs are often used to model different components of very large systems, such as the entire gene regulatory network in humans. For example, in the National Human Genome Research Institute (NHGRI) program Impact of Genetic Variation on Function (IGVF), different groups are building models of different parts of gene regulation, from genotypic variants or CRISPRi perturbations of the genome, to resulting changes in transcription factor binding or chromatin remodeling, to post-translational modifications, all the way to phenotypes characterized by changes in the expression of genes in other parts of the genome IGVF Consortium [2024]. Some of these component models are DNNs and others are PGMs. As a community we know from years of experience with PGMs that passing the outputs of one model to the inputs of another model is typically less effective than concatenating them into a larger model and fine-tuning and using this resulting model. But this concatenation and fine-tuning and usage could not be done with a mixture of PGM and DNN components until now. Having an understanding of DNN components as PGMs enables their combination with PGM components, and then performing fine-tuning and inference in the larger models using algorithms such as the new HMC algorithm theoretically justified, developed, and then evaluated in this paper. Furthermore, the same HMC approach can be employed to reason just as easily from desired gene expression changes at the output nodes back to variants or perturbations at the input nodes that are predictive of the desired changes. Ordinarily, to reason in reverse in this way in a DNN would require special invertible architectures or training of DNNs that operate only in the other direction such as diffusion. Experiments evaluating all these uses of HMC (or other approximate algorithms in PGMs such as loopy belief propagation or other message passing methods) are left for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Sayan Mukherjee, Samuel I. Berchuck, Youngsoo Baek, David Dunson, Andrew Allen, William H. Majoros, Jude Shavlik, Sriraam Natarajan, David Carlson, Kouros Owzar, and Juan Restrepo for their helpful discussion about the theoretical work. We are also grateful to Mengyue Han and Jinyi Zhou for their technical support. ", "page_idx": 10}, {"type": "text", "text": "This project is in part supported by Impact of Genomic Variation on Function (IGVF) Consortium of the National Institutes of Health via grant U01HG011967. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Vanessa Buhrmester, David M\u00fcnch, and Michael Arens. Analysis of explainers of black box deep neural networks for computer vision: A survey. Machine Learning and Knowledge Extraction, 3 (4):966\u2013989, 2021.   \nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.   \nOliver E Richardson. Loss as the inconsistency of a probabilistic dependency graph: Choose your model, not your loss function, 2022. URL https://arxiv.org/abs/2202.11862.   \nYo Joong Choe, Jaehyeok Shin, and Neil Spencer. Probabilistic interpretations of recurrent neural networks. Probabilistic Graphical Models, 2017.   \nAdri\u00e0 Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional networks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.   \nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814, 2010.   \nShun-ichi Amari. Information geometry of the em and em algorithms for neural networks. Neural networks, 8(9):1379\u20131408, 1995.   \nZhao Song, Ricardo Henao, David Carlson, and Lawrence Carin. Learning sigmoid belief networks via monte carlo expectation maximization. In Artificial Intelligence and Statistics, pages 1347\u2013 1355. PMLR, 2016.   \nVincent Dutordoir, James Hensman, Mark van der Wilk, Carl Henrik Ek, Zoubin Ghahramani, and Nicolas Durrande. Deep neural networks as point estimates for deep gaussian processes. Advances in Neural Information Processing Systems, 34, 2021.   \nShengyang Sun, Jiaxin Shi, and Roger Baker Grosse. Neural networks as inter-domain inducing points. In Third Symposium on Advances in Approximate Bayesian Inference, 2020.   \nAndreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and statistics, pages 207\u2013215. PMLR, 2013.   \nStuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach (4th Edition). Pearson, 2020. ISBN 9780134610993. URL http://aima.cs.berkeley.edu/.   \nSekhar C. Tatikonda and Michael I. Jordan. Loopy belief propagation and gibbs measures. In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201902, page 493\u2013500, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc. ISBN 1558608974.   \nAlexander T. Ihler, John W. Fisher III, and Alan S. Willsky. Loopy belief propagation: Convergence and effects of message errors. Journal of Machine Learning Research, 6(31):905\u2013936, 2005. URL http://jmlr.org/papers/v6/ihler05a.html.   \nDror Weitz. Counting independent sets up to the tree threshold. In Proceedings of the Thirty-Eighth Annual ACM Symposium on Theory of Computing, STOC \u201906, page 140\u2013149, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595931341. doi: 10.1145/1132516.1132538. URL https://doi.org/10.1145/1132516.1132538.   \nMiguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In Aistats, volume 10, pages 33\u201340. Citeseer, 2005.   \nYoshua Bengio and Olivier Delalleau. Justifying and generalizing contrastive divergence. Neural computation, 21(6):1601\u20131621, 2009.   \nIlya Sutskever and Tijmen Tieleman. On the convergence properties of contrastive divergence. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9, pages 789\u2013795. JMLR Workshop and Conference Proceedings, 2010. URL http://proceedings.mlr.press/v9/sutskever10a. html.   \nJock Blackard. Covertype. UCI Machine Learning Repository, 1998. DOI: https://doi.org/10.24432/C50K5N.   \nIGVF Consortium. Deciphering the impact of genomic variation on function. Nature, 633(8028): 47\u201357, 2024.   \nCharles Sutton and Andrew McCallum. An introduction to conditional random fields. Found. Trends Mach. Learn., 4(4):267\u2013373, April 2012. ISSN 1935-8237. doi: 10.1561/2200000013. URL https://doi.org/10.1561/2200000013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Bayesian Belief Net and Markov Net Equivalence ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We don\u2019t claim the following theorem is new, but we provide a proof because it captures several components of common knowledge to which we couldn\u2019t find a single reference. ", "page_idx": 12}, {"type": "text", "text": "Theorem 3. Let $N$ be a Bayesian belief network whose underlying undirected graph has treewidth $^{\\,l}$ , and let $w_{A B}$ denote the coefficient of variable $A$ in the logistic CPD for its child B. Let M be a binary pairwise Markov random field with the same nodes and edges (now undirected) as N. Let $M$ \u2019s potentials all have the value $e^{w_{A B}}$ if the nodes $A$ and $B$ on either side of edge $A B$ are true, and the value 1 otherwise. $M$ and $N$ represent the same joint probability distribution over their nodes. ", "page_idx": 12}, {"type": "text", "text": "Proof. According to $M$ the probability of a setting $\\vec{V}$ of its variables is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{Z}\\Pi_{i}\\phi_{i}(\\vec{V})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\phi_{i}$ are the potentials in $M$ , and $Z$ is the partition function, defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\nZ=\\Sigma_{\\vec{V}}\\Pi_{i}\\phi_{i}(\\vec{V})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We use $D O M(\\phi)$ to designate the variables in a potential $\\phi$ . Because the nodes and structures of $M$ and $N$ agree, we will refer to the parents, children, ancestors, and descendants of any node in $M$ to designate the corresponding nodes in $N$ . Likewise we will refer to the input and output variables of $M$ as those nodes of $N$ that have no parents and no children, respectively. Because $M$ has treewidth 1, each node of $M$ d-separates its set of ancestors from its set of descendants and indeed from all other nodes in $M$ . As a result, it is known that the partition function can be computed efficiently in treewidth-1 Markov networks, for example by the following recursive procedure $f$ defined below. Let $V_{0}$ be the empty set of variables, and let $V_{1}$ be the input variables of $M$ . Let $C h(V)$ denote the children of any set $V$ of variables in $M$ , and similarly let $P a(V)$ denote the parents of $V$ . For convenience, when $V$ is a singleton we drop the set notation and let $V$ denote the variable itself. For all natural numbers $i\\geq0$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(V_{i})=\\Pi_{N\\in C h(V_{i})}\\Sigma_{N=0,1}\\Pi_{\\phi_{j}:D O M(\\phi_{j})\\subseteq V_{i},D O M(\\phi_{j})\\subseteq V_{i-1}}\\phi_{j}(V_{i})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nf(V_{m+1})=1\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $V_{m}$ is not the full set of variable in $M$ but $V_{m+1}$ is the full set. Then $Z=f(V_{1})$ . ", "page_idx": 12}, {"type": "text", "text": "For each variable $v\\in\\vec{V}$ , we can multiply the potentials on the edges between $v$ and its parents, to get a single potential $\\phi_{\\{v,P_{2}(v)\\}}$ over $\\{v,P a(v)\\}$ . For a given setting of the parents of $v$ in $\\vec{V}$ , let $\\phi_{v\\mid P a(v)}$ denote the result of conditioning on this setting of the parents, and let $\\phi_{v,\\lnot v|P a(v)}$ denote the result of summing out variable $v$ . Using these product potentials of $M$ , and given the method above for computing $Z$ for a tree-structured Markov network, we can define the probability of a particular setting $\\vec{V}$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\nP(\\vec{V})=\\Pi_{v\\in\\vec{V}}\\frac{\\phi_{v\\mid P a(v)}}{\\phi_{v,\\lnot v\\mid P a(v)}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "These terms are exactly the terms of the logistic conditional probabilities of the Bayesian belief network $N$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nP(\\vec{V})=\\Pi_{v\\in\\vec{V}}P(v|P a(v))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that in general when converting a Bayes net structure to a Markov net structure, to empower the Markov net to represent any probability distribution representable by the Bayes net we have to moralize. A corollary of the above theorem is that in the special case where the Bayes net uses only sigmoid activations, and its underlying undirected graph is tree-structured, moralization is not required. ", "page_idx": 12}, {"type": "text", "text": "B Step 1 and Step 2 Construction Algorithms ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the following, calls to add to sets $V$ or $E$ (or to check if either set contains a vertex or edge) immediately edits/checks the respective set object that they reference. ", "page_idx": 13}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/b4a90df190316565544b7f6d37794426d4e91d81cd54afd94f0e574e190cb3ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Algorithm 2 Step 2 of the PGM Construction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Require: A graph $G$ of the vertices and edges created in Step 1, a list $H$ of the DNN\u2019s output vertices, an   \ninteger l   \nEnsure: Vertices $V$ and edges $E$ of the final tree structure graph with $l$ copies of each parent node from Step 1   \n1: let $V$ be an empty set of vertices   \n2: let $E$ be an empty set of edges   \n3: for each vertex in $H$ do   \n4: $V^{\\prime},E^{\\prime}\\gets\\mathrm{DNN\\_COPY}(v e r t e x,G,v e$ ertex, $V,E,l)$   \n5: $V\\leftarrow V$ .union $(V^{\\prime})$ , $E\\leftarrow E$ .union $\\!\\left[E^{\\prime}\\right)$   \n6: end for   \n7: procedure DNN_COPY(current, $G$ , copy, V , E, l)   \n8: $V$ .add(copy)   \n9: for each parent in $G$ .parents(current) do   \n10: weight $\\leftarrow G$ .getWeight(parent, current)   \n11: for $i\\gets1$ to $l$ do   \n12: $n\\gets p a r e n t+\\cdots+\\dot{1}$ $\\triangleright$ create the ith copy in the current subtree   \n13: while $_n$ in $V$ do \u25b7create a unique label so the graph retains its tree-structure   \n14: $n\\leftarrow n+^{\\prime}r^{\\prime}$   \n15: end while   \n16: let $e$ be an edge between copy and $n$ $\\triangleright$ connect this new copied node to the previous   \n17: E.add(e), $E$ .addWeight(e, weight/l)   \n18: DNN_COPY(parent, $G,n,V,E,l)$ \u25b7create copies in the remaining subtrees   \n19: end for   \n20: end for   \n21: return V , $E$   \n22: end procedure ", "page_idx": 13}, {"type": "text", "text": "C A Proof Using Variable Elimination ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In order to prove that as $L$ goes to infinity, this PGM construction does indeed match the neural network\u2019s forward propagation, we consider an arbitrary latent node $H$ with $N$ unobserved parents $h_{1},...,h_{N}$ , and $M$ observed parents $g_{1},...,g_{M}$ . The edges between these parents and $H$ then have weights $\\theta_{i}$ , $1\\leq i\\leq N$ , for the unobserved nodes, and weights $w_{j}$ , $1\\leq j\\leq M$ , for the observed nodes. The network as a whole has observed evidence $\\vec{x}$ . For the rest of this problem we use a Markov network view of the neural network. The relevant potential passed from the unobserved parent nodes of $H$ , $\\phi(h_{i})$ (as per the directed version of the graph) forward to $H$ have the following form: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/46c41babf2f8359b121c5247a9407e4dde150e73cdeb7df819facf6690275a50.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Since $g_{j}$ are observed, their values are found in $\\vec{x}$ . Finally, the potentials between each of these nodes and the central node $H$ are as follows: ", "page_idx": 14}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/b620a9ccbbec7f8fd5dd04e2e61031ba2a099c476a409bde4d1f04e25a61f774.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Suppose, then, using the second step of our construction, we make $L$ copies of all the nodes that were parents of $H$ in the directed version of the tree, $h_{1}^{1},...,h_{1}^{L}$ , ..., $h_{N}^{1},...,h_{N}^{L}$ and $g_{1}^{1},...,g_{1}^{L}$ , ..., $g_{M}^{1},...,g_{M}^{L}$ with weights $\\theta_{1}/L,...,\\theta_{N}/L$ and $w_{1}/L,...,w_{M}/L$ respectively. The potentials between $H$ and these copied nodes is then: ", "page_idx": 14}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/cf7e8aa373e9728103150bead0469df523be7bbae03ececdba0a30af28a18d85.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "where $1\\leq i\\leq N$ , $1\\leq j\\leq M$ , and $1\\leq k\\leq L$ . The relevant potentials for each of the copied nodes to be passed forward are the same as the nodes they were originally copied from. We then have that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\phi(H,h_{1}^{1},...,h_{1}^{L},...,h_{N}^{1},...,h_{N}^{L},g_{1}^{1},...,g_{1}^{L},...,g_{M}^{L},...,g_{M}^{L}|\\vec{x})}}\\\\ {{\\displaystyle=\\prod_{j=1}^{M}\\prod_{k=1}^{L}e^{(w_{j}/L)H\\times g_{j}^{k}}\\times\\prod_{i=1}^{N}\\prod_{k=1}^{L}e^{(\\theta_{i}/L)H\\times h_{i}^{k}}\\phi(h_{i}^{k})}}\\\\ {{\\displaystyle=e^{\\sum_{j=1}^{M}w_{j}g_{j}\\times H}\\times\\prod_{i=1}^{N}\\prod_{k=1}^{L}e^{(\\theta_{i}/L)H\\times h_{i}^{k}}\\phi(h_{i}^{k}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing out an arbitrary, copied latent node, $h_{\\alpha}^{\\beta}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{i=1}^{n}\\phi(L|\\lambda_{1}|\\lambda_{-1}^{i},\\ldots,\\lambda_{N-1}^{i},\\ldots,\\lambda_{N}^{i}|\\lambda_{j}^{i})}\\\\ &{\\quad\\displaystyle=e^{\\sum_{i=1}^{N}\\phi(x_{i})\\cdot x_{i}}\\sum_{s=1}^{N}\\frac{L}{\\prod_{i=1}^{N}e^{\\phi_{i}(x_{i})/2\\lambda_{i}\\lambda_{j}\\lambda_{i}}e^{\\phi_{i}(k_{i}^{\\prime})}}}\\\\ &{\\quad\\displaystyle=e^{\\sum_{i=1}^{N}\\phi_{i}\\cdot x_{i}}\\sum_{s=1}^{N}\\frac{L}{e^{\\phi_{i}(x_{i})/2\\lambda_{j}\\lambda_{i}}e^{\\psi_{i}(k_{i}^{\\prime})}}}\\\\ &{\\quad\\displaystyle\\left[\\frac{e^{\\sum_{j=1}^{N}\\phi_{j}}}{(1+\\sum_{i=1}^{N}e^{\\sum_{i=1}^{N}\\phi_{j}})^{\\frac{1}{2}}}\\times\\frac{e^{\\sum_{j=1}^{N}\\phi_{j}(1/2\\lambda_{j}\\lambda_{j}\\lambda_{j}^{i})}e^{\\phi_{i}(k_{i}^{\\prime})}}{(1+\\sum_{i=1}^{N}e^{\\sum_{i=1}^{N}\\phi_{j}})^{\\frac{1}{2}}}\\right]}\\\\ &{\\quad\\displaystyle\\qquad\\qquad+\\prod_{i=1}^{N}\\frac{L}{e^{\\left(\\sum_{j=1}^{N}\\phi_{j}\\right)\\lambda_{i}\\lambda_{j}\\lambda_{j}^{i}}e^{\\left(\\sum_{j=1}^{N}\\phi_{j}\\right)}}}\\\\ &{\\quad\\displaystyle=e^{\\sum_{j=1}^{N}\\phi_{j}\\cdot x_{i}}\\sum_{s=1}^{N}\\left(e^{\\sum_{j=1}^{N}\\phi_{j}}\\right)^{\\prod_{i=1}^{N}}+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Summing out all $L$ copies of $h_{\\alpha}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\ne^{\\sum_{j=1}^{M}w_{j}g_{j}\\times H}\\times(e^{p_{\\alpha}}e^{(\\theta_{\\alpha}/L)H}+1)^{L}\\times\\left[\\prod_{\\stackrel{i=1,\\ldots,N}{i\\not=\\alpha}}\\prod_{k=1,\\ldots L}e^{(\\theta_{i}/L)H\\times h_{i}^{k}}\\phi(h_{i}^{k})\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then summing out the $L$ copies of each latent parent: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne^{\\sum_{j=1}^{M}w_{j}g_{j}\\times H}\\times\\prod_{i}^{N}(e^{p_{i}}e^{(\\theta_{i}/L)H}+1)^{L}\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Normalizing this message locally, $\\phi(H=1|\\vec{x})$ becomes $\\phi(H=1|\\vec{x})/\\phi(H=0|\\vec{x})$ and $\\phi(H=0|\\vec{x})$ becomes 1. This then gives us: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(H=1|\\vec{x})}\\\\ &{=\\left[e^{\\sum_{j=1}^{M}w_{j}g_{j}\\times1}\\times\\prod_{i}^{N}(e^{p_{i}}e^{(\\theta_{i}/L)\\times1}+1)^{L}\\right]\\Bigg/\\left[e^{\\sum_{j=1}^{M}w_{j}g_{j}\\times0}\\times\\prod_{i}^{N}(e^{p_{i}}e^{(\\theta_{i}/L)\\times0}+1)^{L}\\right]}\\\\ &{=\\frac{e^{\\sum_{j=1}^{M}w_{j}g_{j}}\\times\\prod_{i}^{N}(e^{p_{i}}e^{(\\theta_{i}/L)}+1)^{L}}{\\prod_{i}^{N}(e^{p_{i}}+1)^{L}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then consider: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad}&{\\displaystyle\\operatorname*{lim}_{L\\rightarrow\\infty}\\frac{e^{\\sum_{j=1}^{M}w_{j}g_{j}}\\times\\prod_{i}^{N}\\left(e^{p_{i}}e^{(\\theta_{i}/L)}+1\\right)^{L}}{\\prod_{i}^{N}\\left(e^{p_{i}}+1\\right)^{L}}=}\\\\ &{\\quad}&{\\displaystyle\\operatorname*{lim}_{L\\rightarrow\\infty}\\exp\\left(\\sum_{j=1}^{M}w_{j}g_{j}-\\sum_{i=1}^{N}L\\times\\log(e^{p_{i}}+1)\\right.}\\\\ &{\\quad}&{\\displaystyle\\left.+\\sum_{i=1}^{N}L\\times\\log(e^{p_{i}}e^{(\\theta_{i}/L)}+1)\\right)\\right)_{,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the logarithm of this limit is, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{L\\to\\infty}\\left[\\sum_{j=1}^{M}w_{j}g_{j}-\\sum_{i=1}^{N}L\\times\\log(e^{p_{i}}+1)\\right.}\\\\ &{\\quad\\quad\\quad\\left.+\\sum_{i=1}^{N}L\\times\\log(e^{p_{i}}e^{(\\theta_{i}/L)}+1)\\right]}\\\\ &{=\\displaystyle\\sum_{j=1}^{M}w_{j}g_{j}\\quad+\\operatorname*{lim}_{L\\to\\infty}\\frac{\\sum_{i=1}^{N}\\Big(\\log(e^{p_{i}}e^{(\\theta_{i}/L)}+1)-\\log(e^{p_{i}}+1)\\Big)}{1/L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The limit in the previous expression clearly has the indeterminate form of $\\frac{0}{0}$ . Let $\\begin{array}{r}{G=\\sum_{j=1}^{M}w_{j}g_{j}}\\end{array}$ and consider the following change of variables, , and subsequent use of l\u2019H\u00f4spital\u2019s rule. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G+\\underset{S\\rightarrow0}{\\operatorname*{lim}}\\frac{\\sum_{i=1}^{N}\\left(\\log(e^{\\gamma_{i}(\\theta,S)}+1)-\\log(e^{\\gamma_{i}}+1)\\right)}{S}}\\\\ &{=G+\\underset{s\\rightarrow0}{\\operatorname*{lim}}\\frac{\\frac{\\beta_{S}}{\\Delta}\\sum_{i=1}^{N}\\left(\\log(e^{\\gamma_{i}(\\theta,S)}+1)-\\log(e^{\\gamma_{i}}+1)\\right)}{\\frac{\\beta_{S}}{\\Delta}S}}\\\\ &{=G+\\underset{s\\rightarrow0}{\\operatorname*{lim}}\\frac{\\sum_{i=1}^{N}\\frac{1}{e^{\\gamma_{i}(\\theta,S)}+1}\\times e^{\\gamma_{i}(\\theta,S)}\\times\\theta_{i}}{\\operatorname*{sup}}}\\\\ &{=G+\\underset{s\\rightarrow0}{\\operatorname*{lim}}\\frac{\\sum_{i=1}^{N}e^{\\gamma_{i}(\\theta,S)}\\times e^{\\beta_{i}}}{\\sum_{i=1}^{N}\\frac{1}{e^{\\gamma_{i}(\\theta,S)}+1}}}\\\\ &{=G+\\underset{s\\rightarrow0}{\\operatorname*{lim}}\\frac{e^{\\gamma_{i}}}{\\sum_{i=1}^{N}\\frac{1}{e^{\\gamma_{i}}+1}}\\times\\theta_{i}}\\\\ &{=\\,\\dots\\,\\underset{s\\rightarrow0}{\\operatorname*{lim}}\\,g_{i}\\geq\\underset{s\\rightarrow0}{\\operatorname*{lim}}\\,g_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\operatorname*{lim}_{L\\to\\infty}\\frac{e^{\\sum_{j=1}^{M}w_{j}g_{j}}\\times\\prod_{i}^{N}(e^{p_{i}}e^{(\\theta_{i}/L)}+1)^{L}}{\\prod_{i}^{N}(e^{p_{i}}+1)^{L}}}\\\\ {\\displaystyle=\\exp(\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The collected potential at node $H$ from summing out its ancestors (from the directed view) then has the form: ", "page_idx": 16}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/2cd1dd9d10e162d6f6852d1237608b15165f7d06d65fa43a15f216238308a5a1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "This is exactly the form of messages that we assumed were originally passed to node $H$ . Suppose then that $z$ is a hidden node whose parents in the original deep neural network\u2019s DAG are all observed. By our PGM construction, we have that node $z$ collects potential $e^{\\sum_{x\\in\\vec{x}}w_{z x}x}$ for $z$ true and 1 for $z$ false from our initial forward step. Here $w_{z x}$ is the weight between nodes $z$ and $x$ . Consider, then, the nodes whose parents in the DNN\u2019s DAG are either one of these first layer hidden nodes, or an observed node. By our PGM construction, we have shown that so long as the nodes in the previous layer are either observed or have this exponential message product, as is the case here, the message product of the nodes that immediately follow will have the same form. ", "page_idx": 17}, {"type": "text", "text": "Note that in order to calculate probability, $P(\\boldsymbol{H}|\\vec{x})$ , we must also consider the influence that the child of node $H$ , call this $C$ , has on node $H$ itself (this would be information passed through and collected at later nodes in the tree network that are then passed back through this node $C$ to node $H$ ) or may not exist at all in the case of output nodes. Suppose the weight between this child $C$ and node $H$ is $\\gamma$ . Suppose also that the message coming from node $C$ to $H$ has the following form, where $c$ is a non-negative real value that can be arbitrarily large. ", "page_idx": 17}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/8a0291a279f3a3fa7520f93d41cbeca3d14221365d2e07ff9fcfe672a01cc508.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Finally, note that with the $L$ copies made in this graph, the potential between node $H$ and $C$ has the form: ", "page_idx": 17}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/23a0ad2c61a1a39f2cbfff3a67011c9c3a62eef404a382127bd32ba6a40ad4d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Using the potential collected at $H$ from the forward pass and this addition information from $C$ , we can then calculate the probability of node $H$ given the input evidence $\\vec{x}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{P(H|\\vec{x})=\\displaystyle\\frac{1}{Z}\\times e^{(\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i})\\times H}\\times\\sum_{C}e^{\\gamma/L\\times C\\times H}\\phi(C)}}\\\\ {{\\mathrm{}=\\displaystyle\\frac{1}{Z}\\times e^{(\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i})\\times H}\\times(c e^{\\gamma/L\\times H}+1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From this we have that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(H=1|\\vec{x})}\\\\ &{=\\frac{e^{(\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i})\\times1}\\times(c e^{\\gamma/L\\times C\\times1}+1)}{e^{(\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i})\\times1}\\times(c e^{\\gamma/L\\times C\\times1}+1)+e^{0}\\times(c e^{0}+1)}}\\\\ &{=\\frac{e^{\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i}}\\times(c e^{\\gamma/L\\times C\\times1}+1)}{e^{\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i}}\\times(c e^{\\gamma/L\\times C\\times1}+1)+(c+1)}}\\\\ &{=\\frac{e^{\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i}}}{e^{\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i}}+(c+1)/(c e^{\\gamma/L\\times C\\times1}+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\operatorname*{lim}_{L\\to\\infty}(c+1)/(c e^{\\gamma/L\\times C\\times1}+1)=1}\\end{array}$ , i.e. as $L$ grows increasingly large the informa$\\begin{array}{r}{\\sigma(\\sum_{j=1}^{M}w_{j}g_{j}+\\sum_{i}^{N}\\sigma(p_{i})\\theta_{i})}\\end{array}$ n, cwe heivcehr yi s neoxdaec tilny t thhise  nsiegtwmoorikd  hacatviev atthieo ns avmaleu feo fromu n $\\bar{P}(H=1|\\vec{x})=$ sages from their parents and child, we have that the conditional probability in this PGM construction and the activation values of the DNN match for any node in any layer of the DNN/PGM. ", "page_idx": 17}, {"type": "text", "text": "D A Proof of the Gradient ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "From An Introduction to Conditional Random Fields Sutton and McCallum [2012] we have the gradient of weight $w_{p k}$ for this form of CRF can be written as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial l}{\\partial w_{p k}}=\\sum_{\\Psi_{c}\\in C_{p}}\\sum_{h_{c}^{\\prime}}P(h_{c}^{\\prime}|y,x)f_{k}(y_{c},x_{c},h_{c}^{\\prime})-\\sum_{\\Psi_{c}\\in C_{p}}\\sum_{h_{c}^{\\prime},y_{c}^{\\prime}}P(h_{c}^{\\prime},y_{c}^{\\prime}|x)f_{k}(y_{c}^{\\prime},x_{c},h_{c}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the features on the cliques of the proposed infinite width-PGM structure are non-zero $(1/L$ , which is a minor change from instead dividing each weight by $L$ ) only when the two adjacent nodes are both 1 (true), this expression simplifies. The update on a specific edge in the PGM then takes the following form and the complete weight update would be the summation over the updates on each edge the weight of interest appears. ", "page_idx": 18}, {"type": "equation", "text": "$$\n(1/L)[P(h_{n}=1,h_{n+1}=1|x,y)-P(h_{n}=1,h_{n+1}=1|x)],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $h_{n}$ and $h_{n+1}$ are the nodes that the edge connects. ", "page_idx": 18}, {"type": "text", "text": "Consider a single branch in the infinite width PGM structure. Let $h_{n}$ be the node closer to the output node and let it be exactly $n$ nodes separated from said output (node $h_{n+1}$ is then naturally the next node in this branch). Due to the infinite-width structure of the proposed PGM there are $L^{n+1}$ copies of this exact path ending in the weight of interest. These copied paths are entirely equivalent to one another. ", "page_idx": 18}, {"type": "text", "text": "Suppose now we sum out the influence output $y$ has on the weight update of these paths. Note that this could be written equivalently as summing out node $h_{0}$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(h_{n}=1,h_{n+1}=1|x,y)-P(h_{n}=1,h_{n+1}=1|x)}\\\\ &{=P(h_{n}=1,h_{n+1}=1|x,y)-\\displaystyle\\sum_{y}P(h_{n}=1,h_{n+1}=1,y|x)}\\\\ &{=P(h_{n}=1,h_{n+1}=1|x,y)-\\displaystyle\\sum_{y}P(h_{n}=1,h_{n+1}=1|x,y)P(y|x)}\\\\ &{=y P(h_{n}=1,h_{n+1}=1|x,y=1)+(1-y)P(h_{n}=1,h_{n+1}=1|x,y=0)}\\\\ &{\\quad-P(h_{n}=1,h_{n+1}=1|x,y=1)P(y=1|x)}\\\\ &{\\quad-P(h_{n}=1,h_{n+1}=1|x,y=0)P(y=0|x)}\\\\ &{=(y-\\hat{y})P(h_{n}=1,h_{n+1}=1|x,y=1)}\\\\ &{\\quad+(1-y-(1-\\hat{y}))P(h_{n}=1,h_{n+1}=1|x,y=0)}\\\\ &{=(y-\\hat{y})[P(h_{n}=1,h_{n+1}=1|x,y=1)-P(h_{n}=1,h_{n+1}=1|x,y=0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that if we are considering the weight update on the connections between the output and its neighbors, this update can be used immediately as $y$ would be $h_{0}$ . The update across all $L$ copies would then have the form: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=1}^{L}(1/L)(y-\\hat{y})[P(h_{0}=1,h_{1}=1|x,h_{0}=1)-P(h_{0}=1,h_{1}=1|x,h_{0}=0)]}}\\\\ &{=\\displaystyle\\sum_{k=1}^{L}(1/L)(y-\\hat{y})P(h_{1}=1|x,h_{0}=1)}\\\\ &{=(y-\\hat{y})P(h_{1}=1|x,h_{0}=1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can easily calculate $P(h_{1}\\,=\\,1|x,h_{0}\\,=\\,1)$ used in this update as follows. Let node $h_{i}$ have conditional probability $\\sigma(S_{h_{i}})$ , which corresponds to a forward product of messages of the form: ", "page_idx": 18}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/7709b072429c7df9a07228952f6cb338f7416a4c9f130041b3208bbbe359b836.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Let the weight between nodes $h_{i}$ and $h_{i-1}$ in the path of interest be $w_{i-1}$ . The product of messages at $h_{i}$ then including the fact that we know $h_{i-1}=1$ , then has the form: ", "page_idx": 19}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/13febb86236bd9babef9c00858e3788427add797cad69f0fb436cf471c3e6c5c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "From this, we can then calculate: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP(h_{i}=1|h_{i-1}=1)=\\sigma(S_{h_{i}}+w_{i-1}/L).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the update of the weights surrounding the output, the overall update must then have the form $(1/L)(y\\bar{-y})\\sigma(S_{h_{1}}+w_{0}/\\bar{L})$ . Note that there are $L$ such copies of this update and $L$ is infinitely large. The overall update of $w_{0}$ is then finally, $\\begin{array}{r}{\\operatorname*{lim}_{L\\to\\infty}\\sum_{k=1}^{L}(1/L)(y{-}\\hat{y})\\sigma(S_{h_{1}}{+}w_{0}/L)=(y{-}\\hat{y})\\sigma(S_{h_{1}})}\\end{array}$ This exactly matches the update of $w_{0}$ in the gradient descent step of the neural network. ", "page_idx": 19}, {"type": "text", "text": "We now consider the updates of weights with further depth away from the output. This update take the form (1/L)(y \u2212y\u02c6)[P(hn = 1, hn $\\begin{array}{r}{\\mathbf{\\Phi}_{-1}=1|x,y=1)-P(h_{n}=1,h_{n+1}=1|x,y=0)}\\end{array}$ )]. For the remainder of this proof we focus on this difference of probabilities. Note however, the complete update does include this $(1/L)(y-\\hat{y})$ term. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{=\\widetilde{P}_{\\mathrm{{R}}}\\left(n_{1},\\ln_{1}=1,\\ln_{1}=1\\right)}&&{\\ n_{2}\\in\\mathcal{V}}\\\\ &{=P_{\\mathrm{{R}}}\\left(n_{2},\\ln_{2}=1\\right)}&&{1\\times10\\,,}\\\\ &{=P_{\\mathrm{R}}\\left(n_{3},\\ln_{3}=1,\\ln_{4}=1\\right)}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(n_{4},\\ln_{4}=1\\right)\\times10\\,,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(n_{5},\\ln_{4}=1,\\ln_{4}=1\\right)\\times0\\,,}&&{\\ n_{6}\\in\\mathcal{V}}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R}}\\left(\\ln_{4}=1\\right)\\times10^{-1}\\,}&&{1}\\\\ &{=P_{\\mathrm{R \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $P(h_{n}=1,h_{n+1}=1|x,h_{1}=1)-P(h_{n}=1,h_{n+1}=1|x,h_{1}=0)$ has the exact same form as the difference we started with, but $y$ (or written equivalently $h_{0}$ ) has been effectively replaced with $h_{1}$ . Due to the tree structure of the PGM, the conditional independence relationships are also entirely equivalent. If $h_{2}$ then also existed in this branch and preceded $h_{n}$ , we could then apply the exact same operations to \u2019sum out\u2019 $h_{1}$ in this expression and replace it with $h_{2}$ . This can be repeated until $h_{n}$ itself is reached and we condition on the node one position closer to the output $h_{n+1}$ . Note that at for step we must multiply the entire expression by $\\begin{array}{r}{P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=}\\end{array}$ $1|x,h_{i-1}=0)$ ) terms. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(h_{n}=1,h_{n+1}=1|x,y=1)-P(h_{n}=1,h_{n+1}=1|x,y=0)}\\\\ &{\\ \\ \\ \\times\\prod_{i=1}^{k}(P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))]}\\\\ &{\\ \\ \\ \\times\\left[P(h_{n}=1,h_{n+1}=1|x,h_{k}=1)-P(h_{n}=1,h_{n+1}=1|x,h_{k}=0)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $n>k$ . We have already shown that this holds for the base case of $k=1$ . We then consider the case of $(k+1)$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{*}(\\mathbf{k}_{1},\\cdots,\\mathbf{k}_{N+1}|\\mathcal{S}_{p},\\mathbf{x}_{0})-P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N+1}=1)\\times\\mathcal{F}^{*}\\otimes\\theta(\\mathbf{k}^{*}\\mathrm{babab}\\theta(\\mathbf{k}^{*}\\mathrm{bab}\\theta(\\mathbf{k}_{1}\\times\\mathbf{x})))}\\\\ &{=\\frac{1}{N}\\Big\\{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N+1}=1),\\cdots,P^{*}(\\mathbf{k}_{0},\\cdots)\\Big\\}}\\\\ &{\\quad\\times\\frac{1}{N}[\\mathcal{F}(\\{r_{k},\\cdots,\\mathbf{k}_{N}},\\alpha_{0},\\cdots|\\mathcal{F}_{p},\\theta_{0})-P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N+1}=1),\\theta_{0}]}\\\\ &{=\\frac{1}{N}\\Big\\{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N+1}=1),\\cdots,P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N}=1)\\Big\\}}\\\\ &{\\quad\\times\\frac{1}{N}\\Big\\{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N+1}=1),\\cdots,P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N}=1)\\Big\\}}\\\\ &{\\quad\\times\\frac{1}{N}\\Big\\{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N+1}=1),\\cdots,P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N}=1),\\theta_{0}\\Big\\}}\\\\ &{=\\frac{1}{N}\\Big\\{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N+1}=1),\\cdots,P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N}=1)\\Big\\}}\\\\ &{\\quad\\times\\frac{1}{N}\\Big\\{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N}=1),\\cdots,\\theta_{0}\\Big\\}}\\\\ &{\\quad-\\frac{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N}=1),\\theta_{0}}{P^{*}(\\mathbf{k}_{0},\\cdots,\\mathbf{k}_{N}=1)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{I}\\Big[\\|\\Gamma(h_{n}+1,h_{n-1}-1)-P(h_{n}-1)\\|_{\\mathcal{S}_{n}}h_{n-1}-0\\big)\\Big\\|}\\\\ &{\\qquad\\times\\bigg[\\mathcal{P}(h_{n}-1,h_{n+1}-1)\\|\\mathcal{P}(h_{n})\\|_{\\mathcal{S}_{n}}h_{n-1}\\big]\\nu_{n}-\\mathbb{I}\\Big[h_{n}+1\\Big]\\nu_{n}-1\\big]}\\\\ &{\\qquad-P(h_{n}-1,h_{n+1}-1)\\|\\mathcal{P}(h_{n+1})\\|_{\\mathcal{S}_{n}}h_{n}-0\\big]}\\\\ &{\\qquad-\\mathbb{I}\\Big[P(h_{n}-1)\\|_{\\mathcal{S}_{n}}h_{n-1}-1\\big]P(h_{n}-1)\\|_{\\mathcal{S}_{n}}h_{n-1}\\Big]}\\\\ &{=\\underbrace{\\prod_{s=1}^{n}P(h_{n}-1)\\|\\mathcal{S}_{n}h_{n}-1\\big]P(h_{n}-1)\\big[P(h_{n+1})\\big]}_{\\textnormal{S p e l f}(h_{n})=0}}\\\\ &{\\qquad\\times\\bigg[\\mathcal{P}(h_{n}-1)\\big]\\nu_{n}+\\underbrace{1\\nu_{n}}_{\\textnormal{S p e l f}(h_{n})}\\big]\\big[P(h_{n+1})\\big]\\nu_{n}-\\mathbb{I}P(h_{n+1})\\big[\\nu_{n}-0\\big]}\\\\ &{=\\underbrace{\\prod_{s=1}^{n}\\big[P(h_{n}-1)\\big]\\nu_{n}-1\\big]P(h_{n}-1)\\big[P(h_{n}-1)\\big]}_{\\textnormal{S p e l f}(h_{n})=0}}\\\\ &{\\qquad\\times\\bigg[P(h_{n}-1)\\big]\\nu_{n+1}-1\\big]P(h_{n+1})\\big[P(h_{n+1})\\big]\\nu_{n}-\\mathbb{I}P(h_{n+1}-1)\\big[\\nu_{n}h_{n}-0\\big]}\\\\ &{\\qquad+P(h_{n}-1)\\big[P(h_{n}+1)\\big]\\nu_{n}+0\\big[P(h_{n+1}-0)\\big]\\nu_{n}-\\mathbb{I}\\big[\\nu_{n+1}-0(\\nu_{n},h_{n}-0)\\big]}\\\\ &{=\\prod_{s=1}^{n}\\big[P(h_{n}-1)\\\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=[\\prod_{i=1}^{k}(P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))]}\\\\ {\\displaystyle\\qquad\\times\\left[P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=1)(P(h_{k+1}=1|x,h_{k}=1)-P(h_{k+1}=1|x,h_{k}=0))\\right.}\\\\ {\\displaystyle\\qquad-\\left.P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=0)(P(h_{k+1}=1|x,h_{k}=1)-P(h_{k+1}=1|x,h_{k}=0))\\right]}\\\\ {\\displaystyle=[\\prod_{i=1}^{k+1}(P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))]}\\\\ {\\displaystyle\\qquad\\times\\left[P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=1)-P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=0)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We therefore have tha $\\textsf{t}P(h_{n}=1,h_{n+1}=1|x,y=1)-P(h_{n}=1,h_{n+1}=1|x,y$ = 0) can be written as a product of the differences $P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0)$ multiplied by the difference $P(h_{n}=1,h_{n+1}=1|x,h_{k}=1)-P(h_{n}=1,h_{n+1}=1|x,h_{k}=0$ ), for any $k<n$ . ", "page_idx": 21}, {"type": "text", "text": "Note that from D.1 we have that $P(h_{i}=1|x,h_{i-1}=1)=\\sigma(S_{h_{i}}+w_{i-1}/L)$ where $w_{i-1}$ is the weight between the two nodes and $S_{h_{i}}$ is the internal summation at $h_{i}$ from the forward pass of the neural network prior to applying the sigmoid activation function $\\sigma(\\cdot)$ . It similarly follows that $P(h_{i}=1|x,h_{i-1}\\,\\bar{=}\\,0)=\\sigma(\\bar{S}_{h_{i}})$ . We then divide and multiply this difference by $w_{i-1}/L$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))}\\\\ &{=(w_{i-1}/L)\\frac{P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))}{w_{i-1}/L}}\\\\ &{=(w_{i-1}/L)\\frac{\\sigma(S_{h_{i}}+w_{i-1}/L)-\\sigma(S_{h_{i}})}{w_{i-1}/L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the right hand term of the above expression has the same form as the definition of a derivative when placed within the limit as $L$ approaches infinity, i.e.: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{L\\to\\infty}\\frac{\\sigma(S_{h_{i}}+w_{i-1}/L)-\\sigma(S_{h_{i}})}{w_{i-1}/L}=\\frac{\\partial}{\\partial S_{h_{i}}}\\sigma(S_{h_{i}}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "At each step away from the output of the network along this branch, we are therefore multiplying by derivative of the local sigmoid per step (this exactly matches the behaviour of the backward pass of the neural network) multiplied by the relevant weight. There is naturally the concern of the remaining $\\scriptstyle{\\frac{1}{L}}$ term both per step and in the $(y-\\hat{y})/L$ term. Recall that there are ${\\boldsymbol{L}}^{n+1}$ copies of the weight of interest in branches with the exact same structure. We sum over these equivalent weight updates and so have iL=1 Ln1+1 = 1. The summation of all these equivalent weight updates effectively cancels this repeated division (note that this is including the $1/L$ term shown in the final step of this proof). ", "page_idx": 21}, {"type": "text", "text": "We finally consider the difference involving nodes $n$ and $n+1$ conditioned on node $n-1$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(h_{n}=1,h_{n+1}=1|x,h_{n-1}=1)-P(h_{n}=1,h_{n+1}=1|x,h_{n-1}=0)}\\\\ &{\\quad=P(h_{n+1}=1|x,h_{n-1}=1,h_{n}=1)P(h_{n}=1|x,h_{n-1}=1)}\\\\ &{\\quad-P(h_{n+1}=1|x,h_{n-1}=0,h_{n}=1)P(h_{n}=1|x,h_{n-1}=0)\\qquad(\\mathrm{Since}\\;h_{n+1}\\perp h_{n-1}|h_{n})}\\\\ &{\\quad=P(h_{n+1}=1|x,h_{n}=1)P(h_{n}=1|x,h_{n-1}=1)}\\\\ &{\\quad-P(h_{n+1}=1|x,h_{n}=1)P(h_{n}=1|x,h_{n-1}=0)}\\\\ &{\\quad=\\sigma(S_{n+1}+w_{n}/L)\\times(P(h_{n}=1|x,h_{n-1}=1)-P(h_{n}=1|x,h_{n-1}=0))}\\\\ &{\\quad=\\sigma(S_{n+1}+w_{n}/L)\\times(w_{n-1}/L)(\\frac{\\sigma\\big(S_{h_{n}}+w_{n-1}/L\\big)-\\sigma\\big(S_{h_{n}}\\big)}{w_{n-1}/L})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The right hand term becomes the derivative of $\\sigma(S_{h_{n}})$ as in earlier steps and the left $\\sigma(\\cdot)$ term becomes $\\sigma(S_{n+1})$ i.e. the forward pass value at node $n+1$ , the last node relevant to this weight calculation on this branch. ", "page_idx": 21}, {"type": "text", "text": "The entire weight update then has the form: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(y-\\hat{y})\\times[\\prod_{i=1}^{n}w_{i-1}\\frac{\\partial}{\\partial S_{h_{i}}}\\sigma(S_{h_{i}})]\\times\\sigma(S_{n+1})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is exactly the gradient update in the neural network when considering a specific path/weight combination in the network. The infinite width PGM naturally has branches for each possible path in the neural network and as such the complete weight updates of both views will align. ", "page_idx": 22}, {"type": "text", "text": "E HMC Sampling Trajectories ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Suppose the current chain state is $\\pmb{h}^{(n)}=\\pmb{\\rho}_{n}(0)$ . We then draw a momentum $\\pmb{\\mu}_{n}(0)\\sim\\mathcal{N}(0,M)$ . The HMC trajectories imply that after $\\Delta t$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mu_{n}(t+\\frac{\\Delta t}{2})=\\mu_{n}(t)-\\frac{\\Delta t}{2}\\nabla U(\\rho)\\Big\\vert_{\\rho=\\rho_{n}(t)}}\\\\ {\\displaystyle\\rho_{n}(t+\\Delta t)=\\rho_{n}(t)+\\Delta t M^{-1}\\mu_{n}(t+\\frac{\\Delta t}{2})}\\\\ {\\displaystyle\\mu_{n}(t+\\Delta t)=\\mu_{n}(t+\\frac{\\Delta t}{2})-\\frac{\\Delta t}{2}\\nabla U(\\rho)\\Big\\vert_{\\rho=\\rho_{n}(t+\\Delta t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We may then apply these equations to $\\rho_{n}(0)$ and $\\pmb{\\mu_{n}}(0)\\,L$ times to get $\\rho_{n}(L\\Delta t)$ and $\\pmb{\\mu_{n}}(L\\Delta t)$ . Thus, the transition from $\\pmb{h}_{(n)}=\\pmb{\\rho}_{n}$ to the next state $\\boldsymbol{h}^{(n+1)}$ is given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\nh^{(n+1)}\\Big|(h^{(n)}=\\rho_{n}(0))=\\binom{\\rho_{n}(L\\Delta t)}{\\rho_{n}(0)}\\quad\\mathrm{with~probability}\\;\\alpha(\\rho_{n}(0),\\rho_{n}(L\\Delta t))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha(\\rho_{n}(0),\\rho_{n}(L\\Delta t))=\\operatorname*{min}\\bigg(1,\\exp\\big(H(\\rho_{n}(0),\\mu_{n}(0)\\big)-H(\\rho_{n}(L\\Delta t),\\mu_{n}(L\\Delta t))\\big)\\bigg).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "F CD-k Algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Algorithm 3 CD- $k$ Learning for the Deep Belief Network ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Input: Initialized $\\pmb{h}^{(0)}$ , $W^{(0)}=\\{{W}_{i}^{(0)}|i=1,2,...,K\\}$ , $\\pmb{b}^{(0)}=\\{\\pmb{b}_{i}^{(0)}|i=1,2,...,K\\}$   \nOutput: $W^{(n)},b^{(n)}$ when loss converges.   \n1: procedure BURN- $\\cdot\\mathrm{IN}(N)$   \n2: for $i\\gets0$ to $N-1$ do   \n3: $h^{(i+1)}\\gets\\mathrm{Sampling}(h^{(i)},W^{(0)},\\pmb{b}^{(0)})$   \n4: end for   \n5: end procedure   \n6: procedure TRAINING $M)$   \n7: for $i\\gets0$ to $M-1$ do   \n8: $\\begin{array}{r l}&{\\Dot{W}^{(i+1)},b^{(i+1)}\\overleftarrow{\\leftarrow}\\mathrm{Weight-updating}(h^{(N+i k)},W^{(i)},b^{(i)})}\\\\ &{\\partial{\\mathbf{r}}\\,j\\gets0\\mathrm{~to~}k-1\\mathrm{~d}\\mathbf{o}}\\\\ &{\\quad h^{(N+i k+j+1)}\\gets\\mathrm{Sampling}(h^{(N+i k+j)},W^{(i+1)},b^{(i+1)})}\\end{array}$   \n9: f   \n10:   \n11: end for   \n12: end for   \n13: end procedure ", "page_idx": 22}, {"type": "text", "text": "In our experiments, $k=1$ , i.e. we do one sampling step before weight updating. We mainly use HMC sampling method for the sampling step, where the detailed trajectories are defined in Appendix E. The weight-updating step is defined in equations (2), (6) and (7). ", "page_idx": 22}, {"type": "text", "text": "We also run Gibbs method for comparison, where results are shown in Table 1. Both Gibbs and HMC are sampling methods to generate new states for all the hidden nodes and they both apply CD-1 or CD- $\\cdot\\mathbf{k}$ to learn the weight. While HMC samples real values in the range of [0, 1] under the normal distribution, Gibbs samples values in $\\{0,1\\}$ under Bernoulli distribution. For node $h_{i j}$ , i.e., the $j$ th node in $h_{i}$ layer, the Bernoulli distribution used to sample its new state is determined by its Markov blanket, which includes all the nodes in $h_{i-1},h_{i}$ , and $\\pmb{h}_{i+1}$ in this case. It could be calculated by normalizing the joint probability distribution of the blanket when $h_{i j}=1$ versus $h_{i j}=0$ , which is written as following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\small\\begin{array}{r}{\\L(h_{i j}=1)=\\frac{p\\left(h_{i j}=1|h_{i-1}\\right)\\cdot p\\left(h_{i+1}|h_{i}\\backslash\\{h_{i j}\\},h_{i j}=1\\right)}{p\\left(h_{i j}=0|h_{i-1}\\right)\\cdot p\\left(h_{i+1}|h_{i}\\backslash\\{h_{i j}\\},h_{i j}=0\\right)+p\\left(h_{i j}=1|h_{i-1}\\right)\\cdot p\\left(h_{i+1}|h_{i}\\backslash\\{h_{i j}\\},h_{i j}=1\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We then sample a new state for $h_{i j}$ from $\\mathrm{Bern}(p(h_{i j}=1))$ ). In Gibbs sampling, the node is sampled one by one since a newly sampled node will affect the sampling of subsequent nodes in its Markov blanket. After sampling new values for all the hidden nodes, the weight is similarly updated using equations (2), (6) and (7). The Gibbs sampling does not depend on $L$ . ", "page_idx": 23}, {"type": "text", "text": "G Experimental Setup ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For all the experiments, the train-test split ratio is 80:20. For the training and finetuning, Adam optimizer is used with learning rate being $1\\times10^{-4}$ . To get the predicted probabilities for fine-tuned network, 1000 output probabilities are sampled and averaged. In synthetic experiments, both BNs and MNs have the structure with the input dimension being 4, two latent layers with 4 nodes in each one, and one binary output. ", "page_idx": 23}, {"type": "text", "text": "H Running time ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Running time of one BN experiment in the main text are shown in Table 3. ", "page_idx": 23}, {"type": "text", "text": "Table 3: Average running time of different methods on synthetic datasets. For DNN, table 3 shows the time of the training for 100 and 1000 epochs. For Gibbs and HMC, it shows the time of the finetuning for 20 epochs. ", "page_idx": 23}, {"type": "table", "img_path": "KcmhSrHzJB/tmp/09e4b1ca45f16b6e233354555a3df71789ce5a3b2d60c8721451a4912f9516d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "[Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide the theorem, proof (in supplement), and empirical results promised in the abstract and introduction. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "[Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The introduction, theorem, and last section (\u201cConclusion, Limitations...\u201d) note that the theorem and experiments are limited to sigmoid activations and that formal results for other activations are left for future work. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Note that for space reasons, the detailed proof had to be moved to the supplement. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Note that a reader will have to download our provided software and data to reproduce experiments, although as much detail was included in the paper as space permits. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, the data and code are all available. We provide the link for them at the end of the introduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All details are in the paper, appendix, or code link. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We conducted each individual synthetic experiment 100 times and performed paired t-tests to assess statistical significance. The p-values are reported in the results section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We say the work was on an internal GPU cluster, and that all experiments are reported. The precise individual run-times are in Appendix H; in the text we simply summarize that SGD is the fastest, Gibbs is the slowest, and HMC is in between. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We read the Code, and the research here conforms fully. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The most important motivation and selling point for this work is the ability to better understand a trained neural network as a proper statistical model. This ability is key to trustworthy AI. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The data and models do not have high risk of misuse. The data are either synthetic, themselves from synthetic target models, or are publicly available. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All software and dataq sets are described and cited. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Only assets are algorithms, theoretical results, and code. The first two are described in detail, and code is available (currently anonymized for double-blind review). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No human subjects or crowdsourcing. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: see above justification ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]