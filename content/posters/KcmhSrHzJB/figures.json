[{"figure_path": "KcmhSrHzJB/figures/figures_4_1.jpg", "caption": "Figure 1: The first step of the PGM construction where shared latent parents are separated into copies along with the subtree of their ancestors. Copies of nodes H1 and H2 are made in this example.", "description": "This figure illustrates the first step in constructing a tree-structured probabilistic graphical model (PGM) that corresponds to a given deep neural network (DNN).  The original DNN is shown in (a), illustrating its structure with shared latent variables (H1 and H2). In (b), the first step of the PGM construction is shown, where these shared latent parents are separated into copies.  Each copy is only connected to one child of the original latent variable, replicating the entire ancestral subtree for each copy.  This process continues for each layer of the network until separate trees are formed for each output node.", "section": "3 The Construction of Tree-structured PGMs"}, {"figure_path": "KcmhSrHzJB/figures/figures_4_2.jpg", "caption": "Figure 1: The first step of the PGM construction where shared latent parents are separated into copies along with the subtree of their ancestors. Copies of nodes H1 and H2 are made in this example.", "description": "This figure illustrates the first step of the proposed algorithm for constructing a tree-structured probabilistic graphical model (PGM) that corresponds to a given deep neural network (DNN). The original DNN is shown in (a). In this step, shared latent parents (i.e., nodes with more than one child) are separated into multiple copies, one for each child. Each copy is connected to only one child, creating separate subtrees. The subtrees preserve the ancestor nodes, ensuring consistency with the DNN's information flow. The weights on the edges remain the same but are divided equally across all copies.", "section": "3 The Construction of Tree-structured PGMs"}]