[{"figure_path": "KcmhSrHzJB/tables/tables_2_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the results of calibration performance experiments on synthetic datasets using different training methods (DNN, Gibbs sampling, and HMC with varying L values).  The mean absolute error (MAE) is reported for each method, along with p-values from t-tests comparing the MAE of Gibbs and HMC to SGD. Highlighted cells indicate statistically significant improvements in MAE for Gibbs and/or HMC over SGD.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_8_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the results of calibration experiments on synthetic datasets generated using Bayesian networks (BNs) and Markov networks (MNs) with varying weight parameters.  The table compares the mean absolute error (MAE) of predictions from a deep neural network (DNN) trained using stochastic gradient descent (SGD), fine-tuned with Gibbs sampling and Hamiltonian Monte Carlo (HMC) methods.  The p-values from t-tests comparing the MAE of Gibbs sampling and HMC methods to SGD are also reported, highlighting statistically significant improvements in calibration accuracy for some methods.", "section": "5.2 Synthetic experiments"}, {"figure_path": "KcmhSrHzJB/tables/tables_9_1.jpg", "caption": "Table 2: Calibration performance on Covertype datasets. Highlighted cells show the best calibrations among each row.", "description": "This table presents the results of calibration experiments on the Covertype dataset.  It compares the performance of three methods: SGD (DNN), Gibbs sampling, and the proposed Hamiltonian Monte Carlo (HMC) algorithm.  The table shows the test expected calibration error (ECE) for different training epochs (100 and 1000) and different HMC parameters (L=10, L=100, L=1000).  Highlighted cells indicate the lowest ECE for each row (dataset/training epoch combination). The results demonstrate the impact of the proposed HMC algorithm for improving model calibration.", "section": "5.2 Covertype Experiments"}, {"figure_path": "KcmhSrHzJB/tables/tables_13_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the mean absolute error (MAE) in model calibration for different training methods (DNN, Gibbs sampling, and Hamiltonian Monte Carlo with different numbers of copies) on synthetic datasets (Bayesian networks and Markov networks with varying weights).  The table shows MAE values and p-values from t-tests comparing the different training methods. Highlighted cells indicate statistically significant improvements in MAE compared to standard DNN training.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_14_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the results of calibration experiments using synthetic datasets generated from Bayesian Networks (BNs) and Markov Networks (MNs).  The experiments compare the performance of three methods:  DNN (deep neural network) training with SGD (stochastic gradient descent), Gibbs sampling, and HMC (Hamiltonian Monte Carlo) with different values of hyperparameter L. The table shows the mean absolute error (MAE) for each method on various datasets with different weight settings. Highlighted p-values indicate statistical significance when comparing Gibbs and HMC to SGD.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_14_2.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the mean absolute error (MAE) in model calibration for different training methods (DNN, Gibbs, and HMC with various L values) across several synthetic datasets.  Each dataset is trained for 100 or 1000 epochs using SGD and then fine-tuned with Gibbs or HMC for 20 epochs. The MAE is calculated by comparing the predicted probabilities from the fine-tuned networks with the true probabilities.  Statistical significance is assessed via t-tests. Highlighted cells show statistically significant improvements over SGD.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_14_3.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the mean absolute error (MAE) in model calibration for different training methods (DNN, Gibbs sampling, and HMC with different L values) on synthetic datasets.  The MAE measures the difference between predicted probabilities and true probabilities.  The results are based on 100 runs per experiment for each model. Statistical significance tests (t-tests) are performed to compare different training methods, highlighting when the difference is statistically significant.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_17_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "The table presents the results of calibration experiments on synthetic datasets using three methods: DNN, Gibbs sampling, and Hamiltonian Monte Carlo (HMC).  For each method and dataset, the mean absolute error (MAE) is reported, along with p-values from t-tests comparing the MAE of Gibbs and HMC to SGD. Highlighted values indicate statistically significant differences (p<0.05). The experiments are designed to assess the impact of fine-tuning a pre-trained DNN using Gibbs sampling and HMC on model calibration performance, particularly with variations in network weights (0.3, 1, 3, 10). The results demonstrate that fine-tuning with HMC can improve model calibration.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_17_2.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the mean absolute error (MAE) results for different models (DNN, Gibbs sampling, and HMC with different L values) on synthetic datasets.  The results are averaged over 100 runs to reduce the impact of randomness. Statistical significance tests (t-tests) are performed to determine if Gibbs sampling and HMC yield lower MAE values than standard SGD training. Highlighted cells in the table indicate statistically significant improvements (p<0.05).  The table shows results for both Bayesian Networks (BNs) and Markov Networks (MNs) with varying weight scales to examine the influence of weight magnitude on model calibration.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_17_3.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the results of calibration experiments on synthetic datasets generated by Bayesian Networks (BNs) and Markov Networks (MNs).  Different network weights and training epochs are used, and performance is measured by Mean Absolute Error (MAE).  The table compares the MAE of models trained with Stochastic Gradient Descent (SGD), Gibbs sampling, and Hamiltonian Monte Carlo (HMC). Highlighted cells indicate statistically significant improvement of Gibbs or HMC compared to SGD.  The number of copies (L) used in the HMC method is also varied.", "section": "5.2 Synthetic experiments"}, {"figure_path": "KcmhSrHzJB/tables/tables_18_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the results of calibration experiments on synthetic datasets using three different methods: DNN, Gibbs, and HMC with various values of L. The table compares the mean absolute error (MAE) of each method in predicting probabilities, and highlights statistically significant results showing when Gibbs or HMC outperforms SGD. The results demonstrate that HMC, particularly with L=10, generally performs better in terms of calibration, especially for networks with smaller weights.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_19_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the mean absolute error (MAE) results for different models (DNN, Gibbs sampling, and HMC with varying L values) on synthetic datasets (BNs and MNs with varying weights). The MAE is a measure of the model's calibration, indicating how well the predicted probabilities match the true probabilities. The table compares the performance of the models after training for 100 and 1000 epochs, with additional fine-tuning using Gibbs or HMC for 20 epochs. The statistical significance of the results, comparing Gibbs and HMC to SGD, is indicated using p-values; highlighted cells represent statistically significant improvements.", "section": "5.2 Experimental Results"}, {"figure_path": "KcmhSrHzJB/tables/tables_23_1.jpg", "caption": "Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.", "description": "This table presents the results of calibration experiments on synthetic datasets generated from Bayesian Networks (BNs) and Markov Networks (MNs).  It compares the Mean Absolute Error (MAE) of three different fine-tuning methods (Gibbs sampling, Hamiltonian Monte Carlo with L=10, L=100, and L=1000) against the performance of a Deep Neural Network (DNN) trained with Stochastic Gradient Descent (SGD). The table shows the MAE for each method, along with p-values from t-tests comparing the fine-tuning methods to SGD. Highlighted p-values indicate statistically significant improvements in MAE.", "section": "5.2 Experimental Results"}]