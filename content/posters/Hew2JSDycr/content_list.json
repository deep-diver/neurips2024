[{"type": "text", "text": "BISCOPE: AI-generated Text Detection by Checking Memorization of Preceding Tokens ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanxi Guo Purdue University guo778@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Siyuan Cheng Purdue University cheng535@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Xiaolong Jin Purdue University jin509@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Zhuo Zhang Purdue University zhan3299@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Kaiyuan Zhang Purdue University zhan4057@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Guanhong Tao University of Utah guanhong.tao@utah.edu ", "page_idx": 0}, {"type": "text", "text": "Guangyu Shen Purdue University shen447@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Xiangyu Zhang Purdue University xyzhang@cs.purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Detecting text generated by Large Language Models (LLMs) is a pressing need in order to identify and prevent misuse of these powerful models in a wide range of applications, which have highly undesirable consequences such as misinformation and academic dishonesty. Given a piece of subject text, many existing detection methods work by measuring the difficulty of LLM predicting the next token in the text from their prefix. In this paper, we make a critical observation that how well the current token\u2019s output logits memorizes the closely preceding input tokens also provides strong evidence. Therefore, we propose a novel bi-directional calculation method that measures the cross-entropy losses between an output logits and the ground-truth token (forward) and between the output logits and the immediately preceding input token (backward). A classifier is trained to make the final prediction based on the statistics of these losses. We evaluate our system, named BISCOPE, on texts generated by five latest commercial LLMs across five heterogeneous datasets, including both natural language and code. BISCOPE demonstrates superior detection accuracy and robustness compared to nine existing baseline methods, exceeding the state-of-the-art non-commercial methods\u2019 detection accuracy by over 0.30 F1 score, achieving over 0.95 detection F1 score on average. It also outperforms the best commercial tool GPTZero that is based on a commercial LLM trained with an enormous volume of data. Code is available at https://github.com/MarkGHX/BiScope. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given the superior performance of Large Language Models (LLMs) in understanding and generating text, they have become an integral part of human society, assisting people with daily activities such as summarizing articles, polishing emails, and more. However, the widespread use of LLMs also raises concerns about the misuse of AI-generated text. For instance, students and academics may utilize LLMs to produce content for their assignments and research [10, 37, 30], compromising academic integrity. Adversarial individuals could leverage LLMs to efficiently create inflammatory and fraudulent content on social media [22]. Additionally, the development of LLMs themselves faces challenges related to the quality of existing datasets, which may be compromised by the significant inclusion of AI-generated text [49, 27]. All of these issues underscore the urgent need to distinguish AI-generated text from human-written text [5, 14, 18, 50, 3, 12]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite this urgency, current AI-generated text detection techniques fall short as LLMs become increasingly diverse and advanced. Our experiments demonstrate that most of the existing approaches cannot achieve an F1 score exceeding $80\\%$ on the Yelp dataset [32] when using the latest LLMs (e.g., Claude-3-Opus) to generate content. A close examination of these approaches reveals inherent limitations by design. Specifically, there are three kinds of methods that do not need pre-training or additional information on the LLMs that generate the data (e.g., watermarking [7, 19, 44, 46, 20]). The first kind [5] directly prompts another LLM or NLP model to classify whether the subject text is AI-generated. While intuitive, the inevitable model hallucination [55, 26] consistently prevents it from achieving a high accuracy. The second kind of methods [28, 31, 9, 54] examines the linguistic features of the subject text, which are increasingly susceptible to deception as LLMs become more sophisticated and human-like in their responses. The third approach [11, 41, 17, 40, 16, 32, 35, 4, 53, 48] feeds partial or entire text to a surrogate model and checks how well the output text aligns with the surrogate model\u2019s preference via various metrics or downstream classifiers. While this method outperforms the first two approaches, it only examines the next token information in the output logits, representing just part of model behaviors, thereby naturally limiting its performance. ", "page_idx": 1}, {"type": "text", "text": "In this work, we explore the potential of leveraging internal model states to detect AI-generated text. Like existing methods, we hypothesize that since LLMs are trained on vast corpora of data from the Internet, their training data likely exhibit significant similarities, leading to similar behaviors across models. Therefore, we use a surrogate model to approximate the behaviors of the one used to generate the subject text. We also make a critical observation that, in causal language models (e.g., GPT), the current token\u2019s output logits encode information about both the next token (i.e., prediction) and its preceding input tokens (i.e., memorization), indicating a bidirectional relationship between the output logits and the input text. Specifically, when these causal language models encounter human data, they tend to memorize more preceding token information while predicting less next token information in their output logits. ", "page_idx": 1}, {"type": "text", "text": "To reveal this relationship, we calculate two kinds of cross-entropy losses by feeding different portions of the subject text into the surrogate model. One is the forward information, calculated as the cross-entropy loss between the output logits and the expected next token in the subject text. The other is the backward information, calculated as the cross-entropy loss between the output logits and the most preceding input token. We then train a binary classifier on the collected statistical loss features to make the final prediction. We also introduce several novel improvements in the prototype, such as providing a summary of the subject text to better guide the surrogate model, thereby enhancing its practical effectiveness and robustness, and using parallel model inference to enhance efficiency. As such, we propose BISCOPE, an effective and efficient AI-generated text detector by harnessing both the prediction and memorization features of the LLM through its output logits. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel AI-generated text detection algorithm that exploits both the preceding token information (i.e., memorization) and the next token information (i.e., prediction) via an innovative bi-directional cross-entropy loss calculation method. Additionally, we are the first to utilize text summaries to guide the detection, further enhancing its effectiveness and robustness toward heterogeneous data.   \n\u2022 We extend existing datasets and craft a large-scale public dataset for more challenging AI-generated texts, consisting of 25 distinct groups and more than 22, 000 samples. The dataset is sourced from five different text domains (both natural language and code) and generated by the five latest commercial LLMs. This dataset presents more challenging scenarios compared to existing datasets, which are typically sourced from open-source LLMs with fewer parameters and capabilities. We also craft a paraphrased version of our dataset.   \n\u2022 We develop a prototype named BISCOPE, a detection pipeline without any fine-tuning needed for the detection LLM. We evaluate it on our dataset and compare it with nine stateof-the-art baseline techniques. Our results show that BISCOPE can achieve an average F1 score of over 0.95, taking less than 200 milliseconds to detect a sample (when the summary procedure is disabled), while the baseline techniques achieve only 0.70-0.85 F1 scores and take up to 27 seconds per sample. BISCOPE also outperforms the best commercial tool, ", "page_idx": 1}, {"type": "text", "text": "GPTZero, in $72\\%$ of cases. Additionally, we conduct a comprehensive ablation study to verify the effectiveness and robustness of each component of BISCOPE. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In addition to various watermarking techniques [7, 19, 44, 46, 20, 24, 15, 52] that require fine-tuning or additional information about the LLMs generating the text, several efforts have been directed towards the detection of AI-generated texts with minimal prior knowledge about the generative models. These efforts broadly fall into three categories. As stated in $\\S\\ 1$ , the first two categories perform worse than the last category, hence we mainly focus on the methods in the third category that use a surrogate LLM in this paper. The methods in the third category can be further divided into two types: statistical methods and training-based methods. ", "page_idx": 2}, {"type": "text", "text": "Statistical Methods. These techniques [45, 34, 21, 33] primarily utilize pre-trained LLMs to simulate the generation process of the target generative AI, analyzing the statistical differences between AIgenerated texts and human-written ones. These methods commonly serve as zero-shot approaches, assigning scores to indicate the probability of texts being AI-generated. For example, Zero-shot Query [32, 48] prompts a pre-trained LLM to score the input text. LogRank [11, 35] calculates the average probability rank of each token in a text processed through a pre-trained LLM, where higher ranks suggest the text is AI-generated. LRR [41] improves on LogRank by incorporating token confidence. DetectGPT [35] involves masking parts of the text to see how an LLM reconstructs them, and Raidar [32] employs the LLM to rewrite the text. Both methods assume that AI-generated texts are more likely to be preserved accurately in the process. Binoculars [13] analyzes the cross-entropy between the output logits from two surrogate models with different fine-tuning configurations. ", "page_idx": 2}, {"type": "text", "text": "Training-based Methods. This type includes methods train an NLP model to distinguish between AI-generated and human-written texts. For example, OpenAI [40] uses a RoBERTa-based model for training an AI-text classifier. Such methods can be susceptible to adversarial attacks or paraphrasing. The state-of-the-art technique RADAR [16] leverages adversarial training to improve the robustness of the classifier. There are also commercial services for the detection of AI-generated texts. For example, GPTZero [43] employs a multi-step statistical detection process and utilizes a pre-trained commercial LLM to deliver the prediction. ", "page_idx": 2}, {"type": "text", "text": "Our approach, BISCOPE, is a statistical method that uniquely incorporates meticulous statistical feature extraction via a bi-directional calculation method to ensure its general effectiveness and robustness against paraphrasing across five text domains and five of the latest commercial LLMs. To evaluate BISCOPE, we compare it with nine existing detection methods, including Zero-shot Query [48, 32], LogRank [11], LRR [41], DetectGPT [35], RADAR [16], Raidar [32], OpenAI Detector [40], Binoculars [13], and GhostBuster [48], as well as with the most renowned commercial detection API, GPTZero. We surpass these baseline methods in both effectiveness and efficiency. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology of BISCOPE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Design Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Although existing AI-generated text detection methods have been proven to be robust against texts generated by various open-source LLMs, their performance degrades in more complex and real-world scenarios, especially when dealing with the latest commercial LLMs and heterogeneous text genres. The degradation can be attributed to the following two reasons: feature insufficiency and contextual heterogeneity. ", "page_idx": 2}, {"type": "text", "text": "Feature Insufficiency. Existing methods focus on analyzing the difficulty a surrogate LLM experiences in predicting the next token given the preceding input text. For example, these methods use the rank or the probability of the next token as a metric or compare the discrepancy between the input text and the surrogate model\u2019s generation. Figure 1(a) presents the detection F1 score of a toy example that uses the average next token rank from Llama-2-7B as the feature and a random forest model as the classification model on both human text (in blue) and GPT-4-Turbo\u2019s text (in orange). The detection F1 score only reaches 0.55, which is just slightly better than random guesses. This indicates the lack of a clear separation using only next token ranks. One may argue that the random forest may not be powerful enough. However, we will show later that using additional features proposed in the paper, the same random forest configuration could achieve much better results. ", "page_idx": 2}, {"type": "image", "img_path": "Hew2JSDycr/tmp/6e4b2eaf612d86fc6f1fd84f2023d0297c815edf8dd111bad1a7969d85c22746.jpg", "img_caption": ["Figure 1: Comparison of detection F1 scores when utilizing the rankFigure 2: Comparison and cross-entropy loss regarding next token, preceding token or both.of output logits informaThe surrogate detection model is Llama-2-7B. tion utilization. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We observe that the internals of the surrogate LLM when used to predict the subject text have much richer information that can be used. Figure 2 illustrates how LLM encodes information. The arrows and texts in green illustrate the information related to the next token, while the arrows and texts in gray show the information related to the preceding tokens. In the auto-regressive generation mode, the LLM receives the tokens preceding to the current position as the input and outputs the logits that contains its prediction for the next token. During this procedure, its internal states encode the preceding tokens (i.e., memorization) [47] while implicitly \u201cplanning\u201d for the next token [51], namely, as observed by researchers in [51], the internal states show similarities to the encodings of future tokens. The output logits, which can be considered as a reduced representation of the model\u2019s internal states, also contains both the information to predict the next token and the information of preceding tokens. Existing methods focus only on the former by comparing the output logits with the expected next token. In this paper, we propose to consider the preceding token information as well. In particular, we hypothesize the following: for human-written text, the surrogate LLM has a poor prediction for the next token and a strong memory of the previous token, reflected in the output logits, whereas the behaviors for LLM-generated text are the opposite. Intuitively, it\u2019s like when we humans are unsure of what to say next, and the last word tends to stay in our minds. ", "page_idx": 3}, {"type": "text", "text": "To validate our hypothesis, we conducted an experiment in which we compared the current output logits with the preceding token for a piece of a given text, leveraging the same random forest as before. Figure 1(b) and (c) present the detection F1 score when only using the preceding token\u2019s rank and when using both the preceding and next tokens\u2019 ranks (to distinguish human and LLM texts), reaching 0.73 and 0.78 F1 scores, respectively, denoting a $0.2\\,\\mathrm{F}1$ score improvement compared to using the next token information alone. Observe that in (d) for next token prediction, the AI texts (in orange) tend to have a smaller CE loss than human texts (in blue), indicating the LLM has a better prediction for the AI texts. In (e) for previous token memorization, the human texts tend to have a smaller loss than the AI texts, indicating the LLM has poorer memory for AI texts. Figures (a) and (b) and an additional example in Appendix B show a similar trend. These support our hypothesis. ", "page_idx": 3}, {"type": "text", "text": "Thus, in BISCOPE, we design a novel bi-directional cross-entropy loss computation method that computes the cross-entropy losses between the output logits and the expected next token, and between the output logits and the preceding token. Figure 1(d)-(f) illustrate the F1 scores when using the cross-entropy losses for next token, previous token, and both. Observe they achieve better results compared to using plain ranks, due to the more wealthy information encoded. An additional example using GPT-Neo-2.7B in Appendix B shows a similar trend. ", "page_idx": 3}, {"type": "text", "text": "Contextual Heterogeneity. In addition to insufficient feature utilization, we also observe that contextual heterogeneity significantly influences detection accuracy. Existing methods directly use surrogate LLMs to generate the given text in an auto-regressive manner, without incorporating any additional information of the context (for the text). As such, given a prefix part of the text, the LLM may have a diverse set of possible completions, limiting the ability to separate human and LLM texts. ", "page_idx": 3}, {"type": "image", "img_path": "Hew2JSDycr/tmp/79bc4cc35535d5d68eb32677de3a113e9fa07638f0453408498a2ffc1d6acb03.jpg", "img_caption": ["Figure 3: Overview of BISCOPE. Arrows and texts in brown indicate text summarization. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To alleviate this problem, we formalize our detection as a guided completion task, using a surrogate LLM to first summarize the entire input text. These text summaries are then used to guide the completion, providing complementary contextual information and making the features more robust. ", "page_idx": 4}, {"type": "text", "text": "3.2 Overview of BISCOPE ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The entire workflow of BISCOPE can be summarized in four key steps, shown in Figure 3. ", "page_idx": 4}, {"type": "text", "text": "Step $\\textcircled{1}$ : Completion Prompt Generation. In the first step, we initialize the detection as a guided text completion task. We use a surrogate LLM to summarize the input text and generate a text summary as a guidance. We then divide the input text into two segments. The first segment, along with a completion request, is utilized to construct a text completion request. The text summary guidance and the text completion request form a completion prompt. Details are presented in $\\S\\ 3.3$ . ", "page_idx": 4}, {"type": "text", "text": "Step $\\circled{2}$ : Loss Computation In Text Completion. Given the completion prompt and the second segment of the input text from Step $\\textcircled{1}$ , we then calculate our novel bi-directional cross-entropy losses for the tokens in the second text segment using multiple open-source LLMs in parallel. Details are presented in $\\S\\ 3.4$ . The use of multiple LLMs is to reduce the uncertainty. ", "page_idx": 4}, {"type": "text", "text": "Step $\\circled{3}$ : Statistical Feature Extraction. We vary the separation of the two segments at different positions of the subject text (e.g., one-fourth, half, and three-fourth of the whole length). For each setup, we collect the statistics of the bi-directional cross-entropy loss values. The statistics are concatenated to form a feature vector. More details and justifications are presented in $\\S\\ 3.5$ . ", "page_idx": 4}, {"type": "text", "text": "Step $\\circled{4}$ : Feature Classification. In the final step, we use the concatenated feature vector to train a binary classifier, which determines whether the input text is human-generated or AI-generated. Further details are presented in $\\S\\ 3.6$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Completion Prompt Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In BISCOPE, we calculate the bi-directional cross-entropy losses within a guided text completion scenario. This scenario involves providing a text summary guidance and a short sub-string of the input text to force LLMs to generate the remainder of the text. Specifically, to alleviate the impact of contextual heterogeneity during text generation, we first utilize a surrogate LLM to summarize the entire input text and obtain a summary as guidance. We then divide the input text into two segments (e.g., the first $10\\%$ and the remaining $90\\%$ ). The first segment, referred to as Input Text Segment 1, serves as the sub-string in a text completion request, while the second segment, referred to as Input Text Segment 2, is used as the completion ground-truth in $\\S\\ 3.4$ . By appending the text completion request after the summary guidance, we construct a completion prompt shown as follows: ", "page_idx": 4}, {"type": "text", "text": "Given the summary: {Text Summary Guidance} Complete the following text: {Input Text Segment 1} ", "page_idx": 4}, {"type": "text", "text": "The text in black indicates the text completion request, while the text in brown indicates the guidance, which is summarized using the following prompt: ", "page_idx": 5}, {"type": "text", "text": "The summary contains the aggregated contextual information of the entire text, providing complementary guidance to the LLM completion, in addition to the first segment. To balance the detection accuracy and efficiency, BISCOPE can also disable this text summary procedure to achieve faster AI-generated text detection with satisfactory accuracy. ", "page_idx": 5}, {"type": "text", "text": "3.4 Loss Computation In Text Completion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After crafting the completion prompt, we then feed it into multiple open-source LLMs in parallel to obtain multiple output logits that correspond to the Input Text Segment 2 from $\\S\\ 3.3$ using the teacher forcing pattern [25], which feeds the ground-truth token prefixes (Input Text Segment 2) to compute the output logits at each token position. These output logits can be used to measure how likely the LLMs predict the next token given its prefix in the subject text and how well the LLMs memorize the preceding token, according to our discussion in $\\S\\ 3.1$ . We hence propose a bi-directional cross-entropy calculation method in BISCOPE, which consists of both forward and backward cross-entropy calculations. The forward cross-entropy $({\\mathcal{F}}C{\\mathcal{E}})$ calculation is identical to the commonly used cross-entropy in most LLM training processes, utilizing the output logits and the next ground-truth token to capture the output logits\u2019 next-token-related information. In contrast, the backward cross-entropy $(B C E)$ is calculated between the output logits and the immediate preceding input token, capturing how much the logits memorizes the preceding token. The detailed $\\mathcal{F}\\mathcal{C}\\mathcal{E}$ and $B C\\mathcal{E}$ calculations at token position $i$ with LLM $\\mathcal{M}$ are shown in Equation 1: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F C E}_{i}=-\\sum_{z=1}^{||\\mathcal{V}||}\\tilde{\\mathcal{P}}_{i+1}^{z}\\cdot\\log(\\mathcal{P}_{i}^{z}),\\quad\\mathcal{B C E}_{i}=-\\sum_{z=1}^{||\\mathcal{V}||}\\tilde{\\mathcal{P}}_{i}^{z}\\cdot\\log(\\mathcal{P}_{i}^{z})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\nu$ indicates the vocabulary of the LLM, $\\mathcal{P}_{i}$ denotes the soft-maxed output logits from $\\mathcal{M}$ at position $i$ of the generated text (when given the preceding tokens from the completion prompt). $\\tilde{\\mathcal{P}}_{i}$ indicates the ground-truth token encoding at the same position. ", "page_idx": 5}, {"type": "text", "text": "3.5 Statistical Feature Extraction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The aforementioned bi-directional cross-entropy loss values may have different characteristics when the text is partitioned at different positions. Intuitively, when the text is partitioned at a ratio of 1:9, meaning that we use the first $10\\%$ of the text to perform the completion, there tends to be a lot more uncertainty compared to a partition of 9:1. A naive design is to fix a partition ratio. However, finding the most effective partition is difficult. Another design is to use the loss values computed at all positions. However, it can hardly deal with length variations of input texts. Therefore, our design is to partition the whole text into $n$ segments $n=10$ in our implementation). For each segment, we collect the bidirectional loss value statistics over all the positions with the segment, including the mean, maximum, minimum, and standard deviation values. This allows us to align texts of various lengths and leverage the later classification to figure out the best partition positions (through learning). Additionally, this multi-segment analysis requires only a one-time inference of the input text, as the loss calculation at each position is independent of the others via teacher forcing. This allows BISCOPE to obtain various and sufficient features with high efficiency. ", "page_idx": 5}, {"type": "text", "text": "3.6 Feature Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the final step, we concatenate all the statistical features of both the $\\mathcal{F C E}$ and $B C\\mathcal{E}$ vectors from all the detection LLMs into a one-dimensional feature vector, which is then used to train a binary classifier to perform the classification. Due to the generality of these features, the binary classifier can be directly used to detect unseen data, whether from unknown LLMs or unfamiliar text domains. ", "page_idx": 5}, {"type": "image", "img_path": "Hew2JSDycr/tmp/fa45a30cd937a06ad86a591eda89ab174691656b1fc41ae6475d929dd7fe8fe8.jpg", "img_caption": ["Figure 4: ROC curves of BISCOPE and all the baselines. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Evaluation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use five datasets in our evaluation, including two short natural language datasets (Arxiv [32] and Yelp [32]), two long natural language datasets (Creative [48] and Essay [48]), and one code dataset [8]. For all datasets, we reuse their human-generated data and craft AI-generated text using five of the latest commercial LLMs. More details are presented in Appendix C. To the best of our knowledge, we are the first to craft a comprehensive AI-generated text dataset using five of the latest commercial LLMs from three leading AI corporations. For the metrics, we use two metrics in our evaluation. To assess the effectiveness of the detection, we use a 5-fold cross-validation F1 score. To evaluate the efficiency of the detection, we use the time cost per sample as the metric. We present more hyper-parameter settings in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "4.2 Detection Performance Comparison with Existing Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first evaluate BISCOPE (with and without the text summary guidance) and nine detection methods on the normal dataset under both in-distribution and out-of-distribution (OOD) settings. Under the in-distribution setting, the test data and training data are from the same source. While under the OOD setting, the test data is from an unknown source. The results are presented in Table 1. Our BISCOPE outperforms existing AI-generated text detection methods, achieving a 0.25 average detection F1 score increase on the normal dataset under in-distribution setting. In the OOD setting, our BISCOPE still surpasses existing detection methods with over a 0.16 average detection F1 score increase. Detailed analysis is shown as follows. ", "page_idx": 6}, {"type": "text", "text": "In-Distribution Results. For the in-distribution setting, we report a 5-fold cross-validation F1 score using one piece of human data and one piece of AI-generated data from the five latest commercial LLMs, as shown in Table 1. BISCOPE \u2217indicates our method with text summary guidance, while BISCOPE represents our method without it. Our method outperforms all nine baselines across all five datasets generated by the latest LLMs. Specifically, our method achieves a 0.29 average F1 score increase on the two short natural language datasets, where existing methods\u2019 best F1 score is around 0.90, while BISCOPE reaches over 0.95 in most cases. On the two long natural language datasets, existing methods reach up to 0.97, while BISCOPE achieves over 0.99 in all cases, resulting in a 0.23 average improvement. On the code dataset, existing methods achieve a 0.60-0.70 average, whereas BISCOPE reaches 0.84, achieving a 0.21 average increase. We further present the TPR-FPR (ROC) curves of BISCOPE and 8 baselines in Figure 4 on the Yelp dataset. We observe that our BISCOPE reaches over 0.8 detection TPR on average when the FPR is only 0.01, outperforming all the baselines on all the five generative models\u2019 data. ", "page_idx": 6}, {"type": "text", "text": "Out-of-Distribution Results. We use two out-of-distribution (OOD) evaluation settings on the normal dataset, cross-model (CM) and cross-dataset (CD), based on previous studies [48, 32]. In CM setting, we assess detection performance on AI-generated data from unknown LLMs within the same text domain by training classifiers on human data and one piece of AI-generated data, then testing on four others from the same dataset. In CD setting, we evaluate detection transferability across text domains by training on data from one dataset and testing on data from four others, all generated by the same LLM. Results across five LLMs are reported in Table 1. In the CM setting, the highest baseline F1 score is 0.84 (average 0.77), while our BISCOPE achieves 0.93, showing a 0.16 average increase. This indicates superior generality of BISCOPE in detecting unseen LLM-generated texts. In the CD setting, BISCOPE\u2019s average F1 score is 0.05 lower than RADAR that utilizes a model pre-trained on multiple datasets. However, compared to the other four baselines, BISCOPE still shows a 0.12 average improvement. Detailed OOD results are in Appendix D. ", "page_idx": 6}, {"type": "table", "img_path": "Hew2JSDycr/tmp/69060a02a1ef0d0ce9b11ee4fab14123556b4bc0c7ee6f571ae862ff13005dae.jpg", "table_caption": ["Table 1: Detection performance of BISCOPE and nine baselines on both normal and paraphrased datasets with in-distribution and out-of-distribution (OOD) settings. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Robustness against Intentional Paraphrasing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Existing studies [23, 39] show that intentional paraphrasing can effectively evade AI-generated text detection. To verify the robustness of BISCOPE against paraphrasing, we utilize five of the latest commercial LLMs to paraphrase their own data using the paraphrasing prompt from previous work [16], and compare BISCOPE with nine baselines on this paraphrased dataset under both indistribution and out-of-distribution settings, as shown in Table 1. Under the in-distribution setting, we test all the methods using the same configuration as on the normal dataset, exploring whether paraphrasing reduces the discriminative power of existing methods. BISCOPE outperforms existing baselines with a 0.29 average F1 score increase. Additionally, existing baselines experience an overall 0.03 F1 detection score drop compared to their performance on the normal dataset. In contrast, our BISCOPE performs even better on the paraphrased dataset, with an overall 0.02 average F1 score increase. Under the out-of-distribution setting, we train the classifiers for all the methods on the normal dataset, while testing them on the paraphrased dataset, exploring the generality of the detection against unseen paraphrased data. Our BISCOPE outperforms existing baselines with an average 0.29 F1 detection score increase. ", "page_idx": 7}, {"type": "image", "img_path": "Hew2JSDycr/tmp/37a08b06dcfd378128879f65eeab4479cce4c1381074ca50e13c2d08dc3b3ee5.jpg", "img_caption": ["Figure 5: Comparison with GPTZero on five datasets using five latest commercial generative AI models. The metric used in the figure is the detection F1 score. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Hew2JSDycr/tmp/5ce0e3fc030f4377545665ccc7588ca0963511879a58d5bbe469494b9f2546ee.jpg", "img_caption": ["Figure 6: Comparison of time Figure 7: Comparison of BIS- Figure 8: Comparison of the costs for processing a single COPE\u2019s effectiveness and ef- contributions of $\\mathcal{F C E}$ and $B C\\mathcal{E}$ sample between BISCOPE and ficiency when using different to detection effectiveness in nine baselines. open-source LLMs. BISCOPE. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4 Comparison with The Latest Commercial Detection Method ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We also compare BISCOPE with the latest version (2024-01-09) of the most renowned commercial AI-generated text detection API, GPTZero [43], across all five datasets, as shown in Figure 5. BISCOPE outperforms GPTZero in $72\\%$ of the cases. Specifically, BISCOPE achieves a 0.02, 0.01, and 0.01 average F1 detection score increase on the Arxiv, Essay, and Creative datasets, respectively. On the Yelp dataset, BISCOPE\u2019s F1 detection score is 0.04 lower than GPTZero\u2019s. However, on the code dataset, BISCOPE performs significantly better than GPTZero, achieving a 0.19 average F1 score improvement, demonstrating BISCOPE\u2019s superior generality from natural language to code. Note that GPTZero\u2019s detection model is pre-trained on millions of data points, while our BISCOPE\u2019s classifier is trained on at most 4, 000 test samples for each case. ", "page_idx": 8}, {"type": "text", "text": "4.5 Efficiency Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To address real-world challenges, efficiency is crucial for detection methods. We compared BISCOPE with nine baselines across five datasets, detailing average processing times for a single sample in Figure 6. RADAR is the fastest at 0.01s per sample, while its adversarial training overhead may be considerable. Processing times for the other baselines are as follows: zero-shot query at 0.32s, LogRank at 0.05s, LRR at 0.10s, DetectGPT at 15.95s, Raidar at 27.42s, OpenAI Detector at 0.03s, Binoculars at 0.19s, and GhostBuster at 0.37s. Zero-shot query, LogRank, LRR, and OpenAI Detector are much quicker than DetectGPT, Raidar, Binoculars, and GhostBuster but offer lower detection performance and robustness. Our BISCOPE processes a sample in 0.14s without summary guidance, matching the real-time levels of zero-shot query, LogRank, and LRR, while improving detection F1 score by over 0.30. With summary guidance, BISCOPE\u2019s processing time increases to 1.35s per sample, still 12 to 20 times faster than DetectGPT and Raidar, and achieves the highest detection score. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.6 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We further evaluate the importance of each component in BISCOPE with two categories of ablation experiments. Since BISCOPE utilizes six open-source LLMs in parallel and ensemble their features, we first investigate the contribution of each LLM\u2019s feature in Figure 7 individually. Then, we further explore the contribution of BISCOPE\u2019s $\\mathcal{F C E}$ and $B C\\mathcal{E}$ losses respectively, compared with their aggregated performance, shown as Figure 8. More detailed results are shown in Appendix E. We also present more detailed ablation study on the impact of different segmentation strategies in multi-point splitting (Appendix E.3), and the impact of the completion prompt (Appendix E.4), in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "BISCOPE\u2019s Performance with Different Base Models. We individually test all the detection base models in BISCOPE both with and without the summary procedure, including Gemma-2B, Gemma7B, Llama-2-7B, Mistral-7B, Llama-3-8B, and Llama-2-13B. All the detection models help BISCOPE achieve over a 0.84 overall F1 detection score across all five datasets, demonstrating high consistency across different detection base models. As the size of the detection base model increases, BISCOPE performs progressively better, with F1 scores improving from 0.84 to 0.95. ", "page_idx": 9}, {"type": "text", "text": "Importance of $\\mathcal{F C E}$ and $B C\\mathcal{E}$ in BISCOPE\u2019s Detection. To demonstrate the rationality of the proposed bi-directional cross-entropy losses, we also evaluate BISCOPE by only using either $\\mathcal{F C E}$ or $B C\\mathcal{E}$ losses, and using both of them with the Llama-2-7B model, as shown in Figure 8. When using both the $\\mathcal{F C E}$ and $B C\\mathcal{E}$ losses, BISCOPE achieves the best average F1 detection score across the five datasets, which is 0.94, highlighting the necessity of combining both $\\mathcal{F C E}$ and $B C\\mathcal{E}$ loss features. When only using $\\mathcal{F C E}$ or $B C\\mathcal{E}$ , BISCOPE reaches 0.86 and 0.93 average F1 detection scores, respectively, indicating that $B C\\mathcal{E}$ is more discriminative than $\\mathcal{F}\\mathcal{C}\\mathcal{E}$ . ", "page_idx": 9}, {"type": "text", "text": "5 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our BISCOPE can achieve over a $0.95\\,\\mathrm{F1}$ detection score across five datasets generated by the five latest commercial LLMs, both with and without intentional paraphrasing, illustrating the importance of the preceding token information in the output logits. However, in the OOD cross-dataset setting, there is a noticeable $>0.10$ detection F1 score drop, highlighting the challenges in this setting. Thus, there is still room for future research to achieve more effective and robust detection in few-shot and cross-dataset settings, where the training set contains fewer samples and the test set comes from different text domains. Additionally, exploring ways to further exploit preceding token information and combine it with next token information should also be a future direction in the field of AIgenerated text detection. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Existing methods to differentiate AI-generated texts from human-generated texts often analyze the difficulty for a surrogate LLM to generate the next token based on previous tokens from the text. We propose a more discriminative approach via a novel bi-directional cross-entropy calculation method, leveraging both the preceding token information and the next token information in the output logits. We integrate this method into a four-step detection pipeline, BISCOPE, which consists of Completion Prompt Generation, Loss Computation in Text Completion, Statistical Feature Extraction, and Feature Classification. We evaluate BISCOPE on five datasets, including both natural language and code, against six existing detection methods. BISCOPE surpasses all these methods, improving the average F1 detection score by 0.30 and also outperforming the well-known commercial API \u2013 GPTZero in $72\\%$ cases, while maintaining a real-time processing speed of less than 200ms per sample. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are grateful to the Center for AI Safety for providing computational resources. This work was funded in part by the National Science Foundation (NSF) Awards SHF-1901242, SHF-1910300, Proto-OKN 2333736, IIS-2416835, DARPA VSPELLS - HR001120S0058, IARPA TrojAI W911NF19-S0012, ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. Claude 3 api. https://www.anthropic.com/news/claude-3-family, 2023.   \n[3] Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc\u2019Aurelio Ranzato, and Arthur Szlam. Real or fake? learning to discriminate machine from human generated text. arXiv preprint arXiv:1906.03351, 2019. [4] Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. In International Conference on Learning Representations (ICLR), 2024. [5] Amrita Bhattacharjee and Huan Liu. Fighting fire with fire: can chatgpt detect ai-generated text? ACM SIGKDD Explorations Newsletter, 25(2):14\u201321, 2024. [6] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021.   \n[7] Jack T Brassil, Steven Low, Nicholas F Maxemchuk, and Lawrence O\u2019Gorman. Electronic marking and identification techniques to discourage document copying. IEEE Journal on Selected Areas in Communications (JSAC), 13(8):1495\u20131504, 1995. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Xiuyuan Cheng and Alexander Cloninger. Classification logit two-sample testing by neural networks for differentiating near manifold densities. IEEE Transactions on Information Theory, 68(10):6631\u20136662, 2022.   \n[10] Debby RE Cotton, Peter A Cotton, and J Reuben Shipway. Chatting and cheating: Ensuring academic integrity in the era of chatgpt. Innovations in Education and Teaching International, 61(2):228\u2013239, 2024.   \n[11] Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and visualization of generated text. In Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL), 2019.   \n[12] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597, 2023.   \n[13] Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Spotting llms with binoculars: Zero-shot detection of machine-generated text. In International Conference on Machine Learning (ICML), 2024.   \n[14] Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. Mgtbench: Benchmarking machine-generated text detection. arXiv preprint arXiv:2303.14822, 2023.   \n[15] Abe Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, and Yulia Tsvetkov. Semstamp: A semantic watermark with paraphrastic robustness for text generation. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2024.   \n[16] Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Radar: Robust ai-text detection via adversarial learning. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[17] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. In Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL), 2020.   \n[18] Ganesh Jawahar, Muhammad Abdul Mageed, and VS Laks Lakshmanan. Automatic detection of machine generated text: A critical survey. In International Conference on Computational Linguistics (COLING), 2020.   \n[19] Mohan S Kankanhalli and KF Hau. Watermarking of electronic text documents. Electronic Commerce Research, 2:169\u2013187, 2002.   \n[20] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. In International Conference on Machine Learning (ICML), 2023.   \n[21] Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples. In AAAI Conference on Artificial Intelligence (AAAI), 2024.   \n[22] Sarah Kreps, R Miles McCain, and Miles Brundage. All the news that\u2019s fti to fabricate: Ai-generated text as a tool of media misinformation. Journal of Experimental Political Science, 9(1):104\u2013117, 2022.   \n[23] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[24] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models. arXiv preprint arXiv:2307.15593, 2023.   \n[25] Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. Advances in Neural Information Processing Systems (NeurIPS), 2016.   \n[26] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale hallucination evaluation benchmark for large language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.   \n[27] Jiayi Liang, Xi Zhang, Yuming Shang, Sanchuan Guo, and Chaozhuo Li. Clean-label poisoning attack against fake news detection models. In IEEE International Conference on Big Data (BigData), 2023.   \n[28] Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J Sutherland. Learning deep kernels for non-parametric two-sample tests. In International Conference on Machine Learning (ICML), 2020.   \n[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[30] Chung Kwan Lo. What is the impact of chatgpt on education? a rapid review of the literature. Education Sciences, 13(4):410, 2023.   \n[31] David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. In International Conference on Learning Representations (ICLR), 2016.   \n[32] Chengzhi Mao, Carl Vondrick, Hao Wang, and Junfeng Yang. Raidar: generative ai detection via rewriting. International Conference on Learning Representations (ICLR), 2024.   \n[33] Hope McGovern, Rickard Stureborg, Yoshi Suhara, and Dimitris Alikaniotis. Your large language models are leaving fingerprints. arXiv preprint arXiv:2405.14057, 2024.   \n[34] Niloofar Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick. Smaller language models are better zero-shot machine-generated text detectors. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2024.   \n[35] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning (ICML), 2023.   \n[36] OpenAI. Gpt-3.5 turbo api. https://platform.openai.com/docs/models/gpt-3-5-turbo, 2023.   \n[37] Mike Perkins. Academic integrity considerations of ai large language models in the post-pandemic era: Chatgpt and beyond. Journal of University Teaching & Learning Practice, 20(2):07, 2023.   \n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 21(140):1\u201367, 2020.   \n[39] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? arXiv preprint arXiv:2303.11156, 2023.   \n[40] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203, 2019.   \n[41] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.   \n[42] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[43] Edward Tian and Alexander Cui. Gptzero: Towards detection of ai-generated text using zero-shot and supervised methods\", 2023.   \n[44] Umut Topkara, Mercan Topkara, and Mikhail J Atallah. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In The 8th Workshop on Multimedia and Security, 2006.   \n[45] Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Sergey Nikolenko, Evgeny Burnaev, Serguei Barannikov, and Irina Piontkovskaya. Intrinsic dimension estimation for robust detection of ai-generated texts. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[46] Honai Ueoka, Yugo Murawaki, and Sadao Kurohashi. Frustratingly easy edit-based linguistic steganography with a masked language model. In The North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2021.   \n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[48] Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting text ghostwritten by large language models. arXiv preprint arXiv:2305.15047, 2023.   \n[49] Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on nlp models. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2021.   \n[50] Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, et al. M4: Multi-generator, multidomain, and multi-lingual black-box machine-generated text detection. arXiv preprint arXiv:2305.14902, 2023.   \n[51] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan ahead for future tokens? arXiv preprint arXiv:2404.00859, 2024.   \n[52] Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, and Nenghai Yu. Watermarking text generated by black-box language models. arXiv preprint arXiv:2305.08883, 2023.   \n[53] Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. International Conference on Learning Representations (ICLR), 2024.   \n[54] Shuhai Zhang, Yiliao Song, Jiahao Yang, Yuanqing Li, Bo Han, and Mingkui Tan. Detecting machinegenerated texts by multi-population aware optimization for maximum mean discrepancy. In International Conference on Learning Representations (ICLR), 2024.   \n[55] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "To further illustrate our BISCOPE with more details, we present the following materials in the Appendix, shown as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix A: Hyper-parameter settings for our method and all the baselines.   \n\u2022 Appendix B: An additional motivation example using a smaller LLM.   \n\u2022 Appendix C: More statistical details of the dataset crafted and utilized in the paper.   \n\u2022 Appendix D: More detailed results on both normal and paraphrased data under OOD setting.   \n\u2022 Appendix E: More details of the ablation study. ", "page_idx": 13}, {"type": "text", "text": "A Hyper-parameter Settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use the default best-performance settings for all the baselines. Specifically, we use GPT-Neo2.7B [6] as the surrogate model in LogRank and LRR, and as the scoring model in DetectGPT, where we use T5-3B [38] as the mask-filling model. For RADAR, we use its officially released detection model, which is a pre-trained RoBERTa-Large [29] model. For Zero-shot Query and Raidar, we use GPT-3.5-Turbo as the query model and the rewriting model. For other baselines, we strictly follow their official implementations with their best configurations. For our BISCOPE, we utilize six opensource LLMs in parallel, including Gemma-2B, Gemma-7B, Llama-2-7B, Mistral-7B, Llama-3-8B, and Llama-2-13B. We also test BISCOPE with (BISCOPE \u2217) and without (BISCOPE) text summary guidance. For better reproducibility, we recommend using either Llama2-7B or Llama2-13B as the surrogate model, while the ensemble of more surrogate models is always welcomed. For the text split method, we recommend splitting the text at every $10\\%$ length, as used in our paper. ", "page_idx": 13}, {"type": "text", "text": "B Additional Motivation Example ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "Hew2JSDycr/tmp/b8a057db4f68f1851da3fd9c4fb09052ae79f8412eecb980e9059a23372a8e61.jpg", "img_caption": ["Figure 9: Comparison between the detection F1 scores when utilizing the rank and cross-entropy loss regarding next token, last token or both. The detection sarrogate model is GPT-Neo 2.7B. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 9 presents an additional example using a smaller language model: GPT-Neo-2.7B. Specifically, Figure 9(a) and (d) show the detection F1 score when only using the next token\u2019s rank and crossentropy loss for detection. The F1 detection scores are less than 0.60 in both cases. However, when we use the preceding token\u2019s rank or cross-entropy loss as the feature, shown in Figure 9(b) and (e), the detection F1 scores increase to 0.63 and 0.76 respectively, indicating the higher reference value of the preceding token information. Figure 9(c) and (f) also show that using both the preceding token information and the next token information in the output logits achieves the best detection performance, with overall detection F1 scores of 0.67 and 0.82. ", "page_idx": 13}, {"type": "text", "text": "Moreover, when considering the preceding token information, AI text has a smaller rank score or higher cross-entropy loss, representing worse memorization of the AI text in the output logits. ", "page_idx": 13}, {"type": "text", "text": "Conversely, when considering the next token information, the result is the opposite, showing better prediction for AI text. This trend aligns with the results in Figure 1, demonstrating the generality of our observation. ", "page_idx": 14}, {"type": "text", "text": "C Additional Details about Datasets ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "Hew2JSDycr/tmp/989b929f830f3aa4de1f84be7d43c2f8037763e137aa795c4637096e4da8a142.jpg", "table_caption": ["Table 2: Statistical details of the five datasets used in our paper. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "For all datasets, we reuse their human-generated data and craft AI-generated text using five of the latest commercial LLMs: GPT-3.5-Turbo [36], GPT-4-Turbo [1], Claude-3-Sonnet [2], Claude-3- Opus [2], and Gemini-1.0-Pro [42]. Thus, for each dataset, we have one piece of human-generated data and five pieces of AI-generated data. Additionally, we also create a corresponding paraphrased dataset using similar paraphrasing prompts from previous studies [32] on our newly crafted dataset to evaluate the robustness of the detection methods. ", "page_idx": 14}, {"type": "text", "text": "Table 2 presents more detailed information about the datasets used in our paper, for both the normal dataset and the paraphrased dataset. For the short natural language datasets (i.e., Arxiv and Yelp), the average text lengths are 187 and 477, respectively, with minimal lengths of 101 and 10. In contrast, the average text lengths of the long natural language datasets (i.e., Creative and Essay) are 2860 and 3899, which are 6-15 times larger than the short natural language dataset\u2019s sample length. The code dataset contains approximately 164 pieces of Python code for human-generated text and AI-generated text from each LLM, with an average length of 983, a minimum length of 41, and a maximum length of 1993. For all the datasets, the human-generated data and AI-generated data share similar lengths, preventing simple length-based detection. ", "page_idx": 14}, {"type": "text", "text": "D Additional Comparison Results under OOD Setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 3 illustrates more detailed OOD results on both the normal and paraphrased datasets across five datasets. Under the cross-model setting on the normal dataset, BISCOPE outperforms existing baselines in 21 out of 25 cases. Specifically, BISCOPE reaches over a 0.95 detection F1 score against five generative LLMs on the Arxiv, Creative, and Essay datasets, while the detection F1 scores of existing baselines are usually less than 0.90. On the Yelp dataset, BISCOPE achieves over a 0.90 detection F1 score against all five generative models, while the five baselines reach less than a 0.85 detection F1 score in most cases. On the code dataset, our BISCOPE outperforms all baselines except for Raidar, achieving more than a 0.15 detection F1 score increase. Under the cross-dataset setting on the normal dataset, our BISCOPE performs worse than under the cross-model setting, outperforming existing methods in only 9 out of 25 cases. However, in most cases, BISCOPE is the second-best detection method, only trailing RADAR, which uses a detection model pre-trained on texts from multiple domains. As for the OOD setting on the paraphrased dataset, BISCOPE outperforms the five baselines in 17 out of 20 cases, achieving over a 0.90 detection F1 score compared to the $<0.75$ average F1 score of the baselines. ", "page_idx": 14}, {"type": "table", "img_path": "Hew2JSDycr/tmp/b211e9f77df5fcf5c31e73b1a2e0ec814570e8ccc2363533068daba48d7b6be9.jpg", "table_caption": ["Table 3: Detailed performance comparison on both normal and paraphrased dataset under OOD setting. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Additional Details of Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Results with Different Open-source LLMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 4 shows additional details when different detection models are used by BISCOPE on both the normal and paraphrased datasets. In most cases, the ensemble results are better than the F1 scores derived using a single detection model. However, at least $90\\%$ of the detection performance can be preserved in most single-detection-model settings. Additionally, the detection F1 score consistently increases with the increasing model size of the detection model. ", "page_idx": 15}, {"type": "text", "text": "Among all the detection models, the Llama-2 series performs the best, outperforming its updated version, Llama-3. In contrast, the Gemma series performs the worst. A possible reason is that the Gemma and Llama-3 series models are still in their early development stages and are not fully capable of handling the AI-generated text detection task. ", "page_idx": 15}, {"type": "table", "img_path": "Hew2JSDycr/tmp/5eb5a53fdf0dff98b5589b8225f44666346f104ebebb529fb1183334fe5ed297.jpg", "table_caption": ["Table 4: Detailed performance comparison with different detection base models in BISCOPE. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Hew2JSDycr/tmp/5ef43cbee26452a01bde31fc3942f32f35dd999f87e7614ea8a3d9b718a91ca5.jpg", "table_caption": ["Table 5: Detailed contribution comparison between $\\mathcal{F}\\mathcal{C}\\mathcal{E}$ and $B{\\mathcal{C}}{\\mathcal{E}}.$ . "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Hew2JSDycr/tmp/a657db99e689eae13111a4a779fe4153d0ee4ab511766fb41e31526612dc6efe.jpg", "table_caption": ["Table 6: Ablation results of different segmentation methods in multi-point splitting in BISCOPE. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Hew2JSDycr/tmp/5f3a35f802e798c6f292719aed88dcd623cdb6388c03fc8fd9f317a077dd5eba.jpg", "table_caption": ["Table 7: Performance results when not using the completion prompt in BISCOPE. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Additionally, when using only a single base model, BISCOPE can achieve a processing speed as fast as 0.03s per sample, outperforming all baseline methods except for RADAR. ", "page_idx": 17}, {"type": "text", "text": "Table 5 shows a more concrete comparison between the contributions of $\\mathcal{F C E}$ and $B C\\mathcal{E}$ to BISCOPE\u2019s detection effectiveness. In 21 of 25 cases, $B C\\mathcal{E}$ is more discriminative compared with $\\mathcal{F C E}$ (especially on code dataset), providing sufficient justifications for BISCOPE that introduce the preceding token information into the detection. Besides, In 16 of the 25 cases, the combination of $\\mathcal{F C E}$ and $B C\\mathcal{E}$ outperforms the $\\mathcal{F C E}$ only and $B C\\mathcal{E}$ only versions, supporting the necessity to use the bi-directional cross-entropy loss calculation method. ", "page_idx": 18}, {"type": "text", "text": "E.3 Impact of Different Segmentation Strategies in Multi-point Splitting in BISCOPE ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 6 presents the ablation results with different segmentation strategies in the multi-point splitting of BISCOPE. We tested three strategies: splitting at every $50\\%$ text length, every $25\\%$ text length, and every $10\\%$ text length (as used in our paper). The results indicate that a more fine-grained splitting interval generally improves BISCOPE\u2019s performance. However, in a small number of cases, a smaller splitting interval may degrade performance. We choose to use $10\\%$ as it achieves the highest detection scores in most cases while reaching low degradation in corner cases. ", "page_idx": 18}, {"type": "text", "text": "E.4 Impact of The Completion Prompt in BISCOPE ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 7 presents the comparison results when using and not using the completion prompt in BISCOPE. The results show that in 25 of 45 cases, using the completion prompt performs better. Additionally, the completion prompt is more compatible with the summary procedure. Thus, we chose to use the completion prompt in BISCOPE. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We summarize the procedure of our method and highlight both of our method and dataset contribution in the abstract and introduction sections. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We present the limitation of our work and potential future research directions in $\\S\\ S$ . ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We use empirical examples and results to justify our method. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We introduce the details of our method at each step in $\\S\\ 3$ , and the hyperparameters we used in Appendix A. The code will be available at https://github.com/ MarkGHX/BiScope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We will open-source our data and code at https://github.com/MarkGHX/ BiScope. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We detailed all the dataset, metrics, and evaluation settings in $\\S\\ 4$ . We also list the hyper-parameters we used for our methods and all the baselines in Appendix A. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not use error bars, but we use the F1 score to measure our method and all the baselines instead. Additionally, we also conduct a detailed ablation study for each specific case in our paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide detailed hyper-parameter settings and present the execution time for our method and all the baselines. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper is aligned with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We present the advantages and limitations of our method, and introduce future directions at $\\S5$ . We also compare our method with the latest commercial method, showing the positive impacts on the open-source community. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Not involved in misusing. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We cite all the baseline methods and data sources in our paper. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We present a detailed information about our new dataset in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]