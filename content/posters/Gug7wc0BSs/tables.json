[{"figure_path": "Gug7wc0BSs/tables/tables_2_1.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against several other sparse training methods across various tasks.  It shows the sparsity level, total number of parameters (normalized to the dense model), and FLOPs (training and inference, also normalized to the dense model) for each algorithm and task.  It highlights MAST's superior performance and efficiency in achieving high sparsity levels while maintaining performance close to the dense model.", "section": "4 Experiments"}, {"figure_path": "Gug7wc0BSs/tables/tables_6_1.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of the proposed Multi-Agent Sparse Training (MAST) framework against several other sparse training baselines across four different StarCraft II environments and four different multi-agent reinforcement learning algorithms.  The table shows the sparsity level, total model size, training FLOPs, testing FLOPs, and the win rate for each method.  The results are normalized with respect to the dense model's performance.  It highlights MAST's superior performance and efficiency in sparse MARL training.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_21_1.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against several baseline methods for different sparsity levels on various StarCraft II Multi-Agent Challenge (SMAC) environments.  The baselines include using a tiny dense network, static sparse networks, other dynamic sparse training methods such as SET and RigL, and a single-agent dynamic sparse training method (RLx2). The table shows the total size, training FLOPs, and inference FLOPs of each method, along with their test win rates (normalized against the dense network) on various SMAC maps.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_22_1.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against several baseline methods across various tasks in the StarCraft Multi-Agent Challenge (SMAC) benchmark.  The table shows the sparsity level (percentage of parameters pruned), the total number of parameters (normalized to the dense model), and the final performance (win rate) achieved by each method, including MAST, Tiny (a small dense model), SS (static sparsity), SET, RigL, RLx2 (single-agent dynamic sparsity).  Results demonstrate the superior performance of MAST over other baseline methods in different scenarios, and across different MARL algorithms like QMIX, WQMIX, and RES.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_23_1.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of the proposed MAST framework against various baseline methods for different sparsity levels across several environments from the StarCraft Multi-Agent Challenge (SMAC) benchmark.  It shows the total size, training FLOPS, inference FLOPS, and win rates for each algorithm and environment.  The sparsity levels tested are shown in the Sp. column, and the results are normalized to the dense model's performance.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_27_1.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against various baseline sparse training methods across multiple SMAC environments and different value-based MARL algorithms.  The \"Sp.\" column indicates the sparsity level used, while \"Total Size\" represents the total number of model parameters.  The FLOPs (floating point operations) for training and inference are also shown, normalized against the dense model. The results highlight MAST's significant performance improvement over other methods, even with high sparsity levels.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_27_2.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against several baseline methods (Tiny, SS, SET, RigL, RLx2) across four different SMAC environments (3m, 2s3z, 3s5z, 64zg) and three different MARL algorithms (QMIX, WQMIX, RES).  For each algorithm and environment, the table shows the sparsity level achieved (Sp.), the total size of the sparse model (Total Size), and the win rate achieved in percentage for each method.  The results demonstrate the superior performance of MAST compared to the other methods, especially in terms of achieving high win rates while maintaining a high level of sparsity.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_27_3.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against several baseline methods for different sparsity levels on various SMAC benchmark environments.  The baselines include using tiny dense networks, static sparse networks, and dynamic sparse training methods like SET and RigL.  The table shows the total size (number of parameters), training FLOPs, and inference FLOPs, all normalized relative to the dense model. The \"Sp.\" column indicates the sparsity level used for each method and environment.  The results demonstrate that MAST outperforms baseline methods, achieving minimal performance degradation with significantly reduced FLOPs.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_28_1.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of the proposed Multi-Agent Sparse Training (MAST) framework against several baseline methods across various tasks in the StarCraft Multi-Agent Challenge (SMAC) benchmark.  The performance metric is the win rate, with sparsity levels ranging from 85% to 95%.  The table shows the total model size (in terms of parameters), and training and inference FLOPs, all normalized relative to a dense model.  This demonstrates the model compression and computational efficiency gains achieved by MAST compared to other approaches such as static sparse networks (SS), SET, RigL, and RLx2.", "section": "4.1 Comparative Evaluation"}, {"figure_path": "Gug7wc0BSs/tables/tables_28_2.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against various baseline methods for sparse training in multi-agent reinforcement learning.  It shows the sparsity level (percentage of parameters retained), total number of parameters (normalized to the dense model), and performance (win rate) for different algorithms and environments in the StarCraft Multi-Agent Challenge (SMAC) benchmark. The baseline methods include Tiny (a small dense model), SS (static sparse network), SET (Sparse Evolutionary Training), RigL (a dynamic sparse training method), and RLx2 (a dynamic sparse training method for single-agent RL).  The results demonstrate that MAST outperforms other sparse training methods by achieving high win rates while maintaining a high level of sparsity.", "section": "4 Experiments"}, {"figure_path": "Gug7wc0BSs/tables/tables_28_3.jpg", "caption": "Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model.", "description": "This table compares the performance of MAST against various baseline methods for different sparsity levels across four different StarCraft II environments.  The baselines include training tiny dense networks, using static sparse networks, employing dynamic sparse training with SET and RigL, and using a dynamic sparse training framework from single agent RL (RLx2). The table shows the total size (number of parameters), training FLOPs, and testing FLOPs of the models, normalized by the dense model.  The \"Sp.\" column shows the sparsity level (%) of each method and environment. The results demonstrate that MAST achieves significantly higher performance compared to the baseline methods, while maintaining a high level of sparsity.", "section": "4.1 Comparative Evaluation"}]