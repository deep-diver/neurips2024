[{"type": "text", "text": "Fundamental Convergence Analysis of Sharpness-Aware Minimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pham Duy Khanh Ho Chi Minh City University of Education khanhpd@hcmue.edu.vn ", "page_idx": 0}, {"type": "text", "text": "Hoang-Chau Luong VNU-HCM University of Science lhchau20@apcs.fitus.edu.vn ", "page_idx": 0}, {"type": "text", "text": "Boris S. Mordukhovich Wayne State University boris@math. wayne.edu ", "page_idx": 0}, {"type": "text", "text": "Dat Ba Tran Wayne State University tranbadat@wayne.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method [Foret et al., 2021] that significantly improves the generalization of deep neural networks. The convergence properties including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks [Khanh et al., 2023b] allowsits extensions toefficientnormalizedversions ofSAM such as F-SAM[Li et al., 2024], VaSSO [Li and Giannakis, 2023], RSAM [Liu et al., 2022], and to the unnormalized versions of SAM such as USAM [Andriushchenko and Flammarion, 2022]. Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper concentrates on optimization methods for solving the standard optimization problem where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is a continuously differentiable ( $\\mathcal{C}^{1}$ -smooth) function. We study the convergence behavior of the gradient-based optimization algorithm Sharpness-Aware Minimization [Foret et al., 2021] together with its efficient practical variants [Liu et al., 2022, Li and Giannakis, 2023, Andriushchenko and Flammarion, 2022]. Given an initial point $x^{1}\\,\\in\\,\\mathbb{R}^{n}$ , the original iterative procedure of SAM is designed as follows ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-t\\nabla f\\left(x^{k}+\\rho{\\frac{\\nabla f(x^{k})}{\\|\\nabla f(x^{k})\\|}}\\right)\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "for all $k\\in\\mathbb{N}$ where $t>0$ and $\\rho>0$ are respectively the stepsize (in other words, the learning rate) and perturbation radius. The main motivation for the construction algorithm is that by making the backward step zk + pVf() , it avoids minimizers with large sharpness, which is usually poor for the generalization of deep neural networks as shown in Keskar et al. [2017]. ", "page_idx": 0}, {"type": "text", "text": "1.1  Lack of convergence properties for SAM due to constant stepsize ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The consistently high efficiency of SAM has driven a recent surge of interest in the analysis of the method. The convergence analysis of SAM is now a primary focus on its theoretical understanding with several works being developed recently (e.g., Ahn et al. [2024], Andriushchenko and Flammarion [2022], Dai et al. [2023], Si and Yun [2023]). However, none of the aforementioned studies have addressed the fundamental convergence properties of SAM, which are outlined below where the stationary accumulation point in (2) means that every accumulation point $\\bar{x}$ of the iterative sequence $\\{x^{k}\\}$ satisfies the condition $\\nabla f({\\bar{x}})=0$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/7b8f0af89129efc0348722573b24e5bb6afd67e0674fb42f99b5dfce282ca0ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "The relationship between the properties above is summarized as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n(1)^{\\{\\|x_{\\ll}^{k}\\|\\}\\cdots}\\overset{\\rightharpoonup}{\\longleftarrow}(2)\\Longleftarrow(3)\\Longleftarrow(5)\\Longrightarrow(4).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The aforementioned convergence properties are standard and are analyzed by various smooth optimization methods including gradient descent-type methods, Newton-type methods, and their accelerated versions together with nonsmooth optimization methods under the usage of subgradients. The readers are referred to Bertsekas [2016], Nesterov [2018], Polyak [1987] and the references therein for those results. The following recent publications have considered various types of convergence rates for the sequences generated by SAM as outlined below: ", "page_idx": 1}, {"type": "text", "text": "(i) Dai et al. [2023, Theorem 1] ", "page_idx": 1}, {"type": "equation", "text": "$$\nf(x^{k})-f^{*}\\leq(1-t\\mu(2-L t))^{k}(f(x^{0})-f^{*})+{\\frac{t L^{2}\\rho^{2}}{2\\mu(2-L t)}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $L$ is the Lipschitz constant of $\\nabla f$ , and where $\\mu$ is the constant of strong convexity constant for $f$ ", "page_idx": 1}, {"type": "text", "text": "(ii) Si and Yun [2023, Theorems 3.3, 3.4] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{1}{k}\\sum_{i=1}^{k}\\left\\|\\nabla f(x^{i})\\right\\|^{2}=\\mathcal{O}\\left(\\frac{1}{k}+\\frac{1}{\\sqrt{k}}\\right)\\quad\\mathrm{~and~}\\quad\\frac{1}{k}\\sum_{i=1}^{k}\\left\\|\\nabla f(x^{i})\\right\\|^{2}=\\mathcal{O}\\left(\\frac{1}{k}\\right)+L^{2}\\rho^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $L$ is the Lipschitz constant of $\\nabla f$ .We emphasize that none of the results mentioned above achieve any fundamental convergence properties listed in Table 1. The estimation in (i) only gives us the convergence of the function value sequence to a value close to the optimal one, not the convergence to exactly the optimal value. Additionally, it is evident that the results in (i) do not imply the convergence of $\\{\\nabla f(x^{k})\\}$ to 0. To the best of our knowledge, the only work concerning the fundamental convergence properties listed in Table 1 is Andriushchenko and Flammarion [2022]. However, the method analyzed in that paper is unnormalized SAM (USAM), a variant of SAM with the norm being removed in the iterative procedure (2c). Recently, Dai et al. [2023] suggested that USAM has different effects in comparison with SAM in both practical and theoretical situations, and thus, they should be addressed separately. This observation once again highlights the necessity for a fundamental convergence analysis of SAM and its normalized variants. ", "page_idx": 1}, {"type": "text", "text": "Note that, using exactly the iterative procedure (2), SAM does not achieve the convergence for either $\\{x^{k}\\}$ $\\smash{\\{f(x^{k})\\}}$ $\\{\\boldsymbol{\\nabla}f(\\boldsymbol{x}^{k})\\}$ to the optimal solution,the optimal value, and the origin, respectively. It is illustrated by Example 3.1 below dealing with quadratic functions. This calls for the necessity of employing an alternative stepsize rule for SAM. Scrutinizing the numerical experiments conducted for SAM and its variants (e.g., Foret et al. [2021, Subsection C1], Li and Giannakis [2023, Subsection 4.1l), we can observe that in fact the constant stepsize rule is not a preferred strategy. Instead, the cosine stepsize scheduler from Loshchilov and Hutter [2016], designed to decay to zero and then restart after each fixed cycle, emerges as a more favorable approach. This observation motivates us to analyze the method under diminishing stepsize, which is standard and employed in many optimization methods including the classical gradient descent methods together with its incremental and stochastic counterparts [Bertsekas and Tsitsiklis, 2000]. Diminishing step sizes also converge to zero as the number of iterations increases, a condition satisfied by the practical cosine step size scheduler in each cycle. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2  Our Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Convergence of SAM and normalized variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We establish fundamental convergence properties of SAM in various settings. In the convex case, we consider the perturbation radi to be variable and bounded. This analysis encompasses the practical implementation of SAM with a constant perturbation radius. The results in this case are summarized in Table 2. ", "page_idx": 2}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/c884b0d5956cf01cb0f2131bcb86dd8a3724b509a9b93a8f11369a17b6e51362.jpg", "table_caption": ["Table 2: Convergence properties of SAM for convex functions in Theorem 3.2 "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "In the nonconvex case, we present a unified convergence analysis framework that can be applied to most variants of SAM, particularly recent efficient developments such as VaSSO [Li and Giannakis, 2023], F-SAM [Li et al., 2024], and RSAM [Liu et al., 2022]. We observe that all these methods can be viewed as inexact gradient descent (IGD) methods with absolute error. This version of IGD has not been previously considered, and its convergence analysis is significantly more complex than the one in Khanh et al. [2023b, 2024a, 2023a, 2024b], as the function value sequence generated by the new algorithm may not be decreasing. This disrupts the convergence framework for monotone function value sequences used in the aforementioned works. To address this challenge, we adapt the analysis for algorithms with nonmonotone function value sequences from Li et al. [2023], which was originally developed for random reshuffling algorithms, a context entirely different from ours. ", "page_idx": 2}, {"type": "text", "text": "We establish the convergence of IGD with absolute error when the perturbation radi decrease at an arbitrarily slow rate. Although the analysis of this general framework does not theoretically cover the case of a constant perturbation radius, it poses no issues for the practical implementation of these methods, as discussed in Remark 3.6. A summary of our results in the nonconvex case is provided in the first part of Table 3. ", "page_idx": 2}, {"type": "text", "text": "Convergence of USAM and unnormalized variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our last theoretical contribution in this paper involves a refined convergence analysis of USAM in Andriushchenko and Flammarion [2022]. In the general seting, we address functions satisfying the $L$ -descent condition (4), which is even weaker than the Lipschitz continuity of $\\nabla f$ as considered in Andriushchenko and Flammarion [2022]. The summary of the convergence analysis for USAM is given in the second part of Table 3. ", "page_idx": 2}, {"type": "text", "text": "As will be discussed in Remark G.4, our convergence properties for USAM use weaker assumptions and cover a broader range of applications in comparison with those analyzed in [Andriushchenko and Flammarion, 2022]. Furthermore, the universality of the conducted analysis allows us to verify all the convergence properties for the extragradient method [Korpelevich, 1976] that has been recently applied in Lin et al. [2020] to large-batch training in deep learning. ", "page_idx": 2}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/2f3f334b29aaa8ca86ef6fc02dca87eefd43403108e30fc45f55aab2198b24f4.jpg", "table_caption": [], "table_footnote": ["Table 3: Convergence properties of SAM together with normalized variants (Corollary 3.5, Appendix D), and USAM together with unnormalized variants (Theorem 4.2) "], "page_idx": 2}, {"type": "text", "text": "1.3  Importance of Our Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our work develops, for the first time in the literature, a fairly comprehensive analysis of the fundamental convergence properties of SAM and its variants. The developed approach addresses general frameworks while being based on the analysis of the newly proposed inexact gradient descent methods. Such an approach can be applied in various other circumstances and provides useful insights into the convergence understanding of not only SAM and related methods but also many other numerical methods in smooth, nonsmooth, and derivative-free optimization. ", "page_idx": 3}, {"type": "text", "text": "1.4Related Works ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Variants of SAM. There have been several publications considering some variants to improve the performance of SAM. Namely, Kwon et al. [2021] developed the Adaptive Sharpness-Aware Minimization (ASAM) method by employing the concept of normalization operator. Du et al. [2022] proposed the Efficient Sharpness-Aware Minimization (ESAM) algorithm by combining stochastic weight perturbation and sharpness-sensitive data selection techniques. Liu et al. [2022] proposed a novel Random Smoothing-Based SAM method called RSAM that improves the approximation quality in the backward step. Quite recently, Li and Giannakis [2023] proposed another approach called Variance Suppressed Sharpness-aware Optimization (VaSSO), which perturbed the backward step by incorporating information from the previous iterations. As Li et al. [2024] identified noise in stochastic gradient as a crucial factor in enhancing SAM's performance, they proposed Friendly Sharpness-Aware Minimization (F-SAM) which perturbed the backward step by extracting noise from the difference between the stochastic gradient and the expected gradient at the current step. Two efficient algorithms, AE-SAM and AE-LookSAM, are also proposed in Jiang et al. [2023], by employing adaptive policy based on the loss landscape geometry. ", "page_idx": 3}, {"type": "text", "text": "Theoretical Understanding of SAM. Despite the success of SAM in practice, a theoretical understanding of SAM was absent until several recent works. Barlett et al. [2023] analyzed the convergence behavior of SAM for convex quadratic objectives, showing that for most random initialization, it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature. Ahn et al. [2024] introduces the notion of $\\varepsilon$ -approximate flat minima and investigates the iteration complexity of optimization methods to find such approximate fat minima. As discussed in Subsection 1.1, Dai et al. [2023] considers the convergence of SAM with constant stepsize and constant perturbation radius for convex and strongly convex functions, showing that the sequence of iterates stays in a neighborhood of the global minimizer while Si and Yun [2023] considered the properties of the gradient sequence generated by SAM in different settings. ", "page_idx": 3}, {"type": "text", "text": "Theoretical Understanding of USAM. This method was first proposed by Andriushchenko and Flammarion [2022] with fundamental convergence properties being analyzed under different settings of convex and nonconvex and optimization. Analysis of USAM was further conducted in Behdin and Mazumder [2023] for a linear regression model, and in Agarwala and Dauphin [2023] for a quadratic regression model. Detailed comparison between SAM and USAM, which indicates that they exhibit different behaviors, was presented in the two recent studies by Compagnoni et al. [2023] and Dai et al. [2023]. During the final preparation of the paper, we observed that the convergence of USAM can also be deduced from Mangasarian and Solodov [1994], though under some additional assumptions, including the boundedness of the gradient sequence. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "First we recall some notions and notations frequently used in the paper. All our considerations are given in the space $\\mathbb{R}^{n}$ with the Euclidean norm $\\Vert\\cdot\\Vert$ . As always, $\\mathbb{N}:=\\{1,2,\\ldots\\}$ signifies the collections of natural numbers. The symbol $x^{k}\\xrightarrow{J}\\bar{x}$ means that $x^{k}\\rightarrow\\bar{x}$ as $k\\rightarrow\\infty$ with $k\\in J\\subset\\mathbb{N}$ Recall that $\\bar{x}$ is a stationary point of a $\\mathcal{C}^{1}$ -smooth function $f\\colon\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ if $\\nabla f({\\bar{x}})=0$ . A function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is said to posses a Lipschitz continuous gradient with the uniform constant $L>0$ ,or equivalently it belongs to the class $\\stackrel{\\triangledown}{\\mathcal{C}}^{1,L}$ , if we have the estimate ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\left\\|x-y\\right\\|\\,{\\mathrm{~for~all~}}\\,x,y\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This class of function enjoys the following property called the $L$ -descent condition (see, e.g., Izmailov and Solodov [2014, Lemma A.11] and Bertsekas [2016, Lemma A.24]): ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\left\\langle\\nabla f(x),y-x\\right\\rangle+\\frac{L}{2}\\left\\Vert y-x\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all $x,y\\,\\in\\,\\mathbb{R}^{n}$ . Conditions (3) and (4) are equivalent to each other when $f$ is convex,while the equivalence fails otherwise. A major class of functions satisfying the $L$ -descentcondition but not having the Lipschitz continuous gradient is given by Khanh et al. [2023b, Section 2] as $f(x)\\;:=\\;{\\frac{1}{2}}\\,\\langle\\bar{A}x,x\\rangle\\,+\\,\\langle b,x\\rangle\\,+\\,c\\,-\\,h(x)$ , where $A$ is an $n\\,\\times\\,n$ matrix, $b\\;\\in\\;\\mathbb{R}^{n}$ $c~\\in~\\mathbb{R}$ and $h:\\mathbb{R}^{n}\\to\\mathbf{\\bar{\\mathbb{R}}}$ is a smooth convex function whose gradient is not Lipschitz continuous. There are also circumstances where a function has a Lipschitz continuous gradient and satisfies the descent condition at the same time, but the Lipschitz constant is larger than the one in the descent condition. ", "page_idx": 4}, {"type": "text", "text": "Our convergence analysis of the methods presented in the subsequent sections benefits from the Kurdyka-Lojasiewicz. $(K L)$ property taken from Attouch et al. [2010]. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.1 (Kurdyka-Lojasiewicz property). We say that a smooth function $f:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ enjoys the $K L$ property at $\\bar{x}\\,\\in\\,\\mathrm{dom}\\,\\partial f$ if there exist $\\eta\\,\\in\\,(0,\\infty]$ , a neighborhood $U$ of $\\textstyle{\\bar{x}}$ , and a desingularizing concave continuous function $\\varphi:[0,\\eta)\\rightarrow[0,\\infty)$ such that $\\varphi(0)=0$ $\\varphi$ is $\\mathcal{C}^{1}$ -smooth on $(0,\\bar{\\eta})$ \uff0c $\\varphi^{\\prime}>0$ on $(0,\\eta)$ , and for all $x\\in U$ with $0<f(\\bar{x})-\\dot{f}(\\bar{x})<\\eta$ we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varphi^{\\prime}(f(x)-f({\\bar{x}}))\\left\\|\\nabla f(x)\\right\\|\\geq1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 2.2. It has been realized that the KL property is satisfied in broad settings. In particular, it holds at every nonstationary point of $f$ ; see Attouch et al. [2010, Lemma 2.1 and Remark 3.2(b)]. Furthermore, it is proved at the seminal paper [Lojasiewicz, 1965] that any analytic function $f:$ $\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ satisfies the KL property on $\\mathbb{R}^{n}$ with $\\bar{\\varphi}(t)~=~M t^{1-q}$ for some $q\\,\\in\\,[0,1)$ . Typical functions that satisfy the KL property are semi-algebraic functions and in general, functions definable in $o$ -minimal structures; see Attouch et al. [2010, 2013],Kurdyka [1998]. ", "page_idx": 4}, {"type": "text", "text": "We utilize the following assumption on the desingularizing function in Definition 2.1, which is employed in Li et al. [2023]. The satisfaction of this assumption for a general class of desingularizing functions is discussed in Remark G.1. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.3. There is some $C>0$ such that whenever $x,y\\in(0,\\eta)$ with $x+y<\\eta$ , it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\nC[\\varphi^{\\prime}(x+y)]^{-1}\\leq(\\varphi^{\\prime}(x))^{-1}+(\\varphi^{\\prime}(y))^{-1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3 SAM and normalized variants ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1  Convex case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin this subsection with an example that illustrates SAM's inability to achieve the convergence of the sequence of iterates to an optimal solution of strongly convex quadratic functions by using a constant stepsize. This emphasizes the necessity of avoiding this type of stepsize in our subsequent analysis. ", "page_idx": 4}, {"type": "text", "text": "Example 3.1 (SAM with constant stepsize and constant perturbation radius does not converge). Let the sequence $\\left\\{x^{k}\\right\\}$ be generated by SAM in (2) applied to the strongly convex quadratic function $\\begin{array}{r}{f(x)=\\frac{1}{2}\\left<A x,x\\right>-\\left<b,x\\right>}\\end{array}$ ,where $A$ is an $n\\times n$ symmetric, positive-definite matrix and $b\\in\\mathbb{R}^{n}$ Then for any fixed small constant perturbation radius and for some small constant stepsize together with an initial point close to the solution, the sequence $\\{x^{k}\\}$ generated by this algorithm does not converge to the optimal solution. ", "page_idx": 4}, {"type": "text", "text": "The details of the above example are presented in Appendix A.1. Figure 1 gives an empirical illustration for Example 3.1. The graph shows that, while the sequence generated by GD converges to O, the one generated by SAM gets stuck at a different point. ", "page_idx": 4}, {"type": "text", "text": "As the constant stepsize does not guarantee the convergence of SAM, we consider another well-known stepsize called diminishing (see (7)). The following result provides the convergence properties of SAM in the convex case for that type of stepsize. ", "page_idx": 4}, {"type": "image", "img_path": "PuXYI4HOQU/tmp/1bdeeb8ce8805accb480bd8a55b39cfb54f751c5a31c898162d4b3f4ef52f24f.jpg", "img_caption": ["Figure 1: SAM with constant stepsize does not converge to optimal solution "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a smooth convex function whose gradient is Lipschitz continuous with constant $L>0$ . Given any initial point $x^{1}\\in\\dot{\\mathbb{R}}^{n}$ ,let $\\{x^{k}\\}$ be generated by the SAM method with theiterative procedure ", "page_idx": 5}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-t_{k}\\nabla f\\left(x^{k}+\\rho_{k}{\\frac{\\nabla f(x^{k})}{\\|\\nabla f(x^{k})\\|}}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "forall $k\\in\\mathbb{N}$ with nonnegative stepsizes and perturbation radi satisfying the conditions ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}t_{k}^{2}<\\infty,\\,\\sum_{k=1}^{\\infty}t_{k}=\\infty,\\,\\operatorname*{sup}_{k\\in\\mathbb{N}}\\rho_{k}<\\infty.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assume that $\\nabla f(x^{k})\\neq0$ forall $k\\in\\mathbb{N}$ and that $\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})>-\\infty$ Then the following assertions hold: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{liminf}_{k\\to\\infty}\\nabla f(x^{k})=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(i) If $f$ has a nonempty bounded level set, then $\\{x^{k}\\}$ is bounded, every accumulation point of $\\{x^{k}\\}$ is a global minimizer of $f$ and $\\{f(x^{k})\\}$ converges to the optimal value of $f$ If in addition $f$ has $a$ unique minimizer, then the sequence $\\{x^{k}\\}$ converges to that minimizer. ", "page_idx": 5}, {"type": "text", "text": "Due to the space limit, the proof of the theorem is presented in Appendix C.1. ", "page_idx": 5}, {"type": "text", "text": "3.2  Nonconvex case ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we study the convergence of several versions of SAM from the perspective of the inexact gradient descent methods. ", "page_idx": 5}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/97fd0962fb112452c3e3ef8040e33a9120cbfdeecbdd9b63f4c393570c22f06a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "This algorithm is motivated by while being different from the Inexact Gradient Descent methods proposed in [Khanh et al., 2023a,b, 2024b,a]. The latter constructions consider relative errors in gradient calculation, while Algorithm 1 uses the absolute ones. This approach is particularly suitable for the constructions of SAM and its normalized variants. The convergence properties of Algorithm 1 are presented in the next theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a smooth function whose gradient is Lipschitz continuous with someconstant $L>0$ and let $\\{x^{k}\\}$ be generated by the IGD method in Algorithm 1 with stepsizes and errors satisfying the conditions ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}t_{k}=\\infty,\\;t_{k}\\downarrow0,\\sum_{k=1}^{\\infty}t_{k}\\varepsilon_{k}<\\infty,\\;\\operatorname*{lim}\\operatorname*{sup}\\varepsilon_{k}<2.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumethat $\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})>-\\infty$ Then the following convergence properties hold: ", "page_idx": 6}, {"type": "text", "text": "(i $\\nabla f(x^{k})\\to0$ , and thus every accumulation point of $\\{x^{k}\\}$ is stationary for $f$ ", "page_idx": 6}, {"type": "text", "text": "(i) f $\\bar{x}$ is an accumulation point of the sequence $\\{x^{k}\\}$ then $f(x^{k})\\to f({\\bar{x}})$ ", "page_idx": 6}, {"type": "text", "text": "(ii) Suppose that $f$ satisfies the $K L$ property at some accumulation point $\\bar{x}$ of $\\{x^{k}\\}$ with the desingularizing function $\\varphi$ satisfying Assumption 2.3. Assume in addition that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}t_{k}\\left(\\varphi^{\\prime}\\left(\\sum_{i=k}^{\\infty}t_{k}\\varepsilon_{k}\\right)\\right)^{-1}<\\infty,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and that $f(x^{k})>f({\\bar{x}})$ for suffciently large $k\\in\\mathbb{N}$ Then $x^{k}\\rightarrow\\bar{x}$ as $k\\rightarrow\\infty$ In particular if $\\textstyle{\\bar{x}}$ . $a$ global minimizer of $f$ theneither $f(x^{k})=f({\\bar{x}})$ for some $k\\in\\mathbb{N},$ or $x^{k}\\rightarrow{\\bar{x}}$ ", "page_idx": 6}, {"type": "text", "text": "The proof of the theorem is presented in Appendix C.2. The demonstration that condition (9) is satisfiedwhen $\\varphi(t)=M t^{1-q}$ with some $M>0$ and $q\\in(0,1)$ , and when $\\begin{array}{r}{t_{k}=\\frac{1}{k}}\\end{array}$ and $\\begin{array}{r}{\\varepsilon_{k}=\\frac{1}{k^{p}}}\\end{array}$ with $p\\geq0$ for all $k\\in\\mathbb{N}$ , is provided in Remark G.3. ", "page_idx": 6}, {"type": "text", "text": "The next example discusses the necessity of the last two conditions in (8) in the convergence analysis of IGD while demonstrating that employing a constant error leads to the convergence to a nonstationary point of the method. ", "page_idx": 6}, {"type": "text", "text": "Example 3.4 (IGD with constant error converges to a nonstationary point). Let $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be defined by $f({\\dot{x}})=x^{2}$ for $x\\in\\mathbb{R}$ . Given a perturbation radius $\\rho>0$ and an initial point $x^{1}>\\rho$ consider the iterative sequence ", "page_idx": 6}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-2t_{k}\\left(x^{k}-\\rho\\frac{x^{k}}{|x^{k}|}\\right)\\ \\ \\mathrm{for}\\;k\\in\\mathbb{N},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\{t_{k}\\}\\subset[0,1/2],t_{k}\\downarrow0$ , and $\\textstyle\\sum_{k=1}^{\\infty}t_{k}=\\infty$ . This algorithm is in fact the IGD applied to $f$ with $\\begin{array}{r}{g^{k}=\\nabla f\\left(x^{k}-\\rho\\frac{f^{\\prime}(x^{k})}{|f^{\\prime}(x^{k})|}\\right)}\\end{array}$ Then $\\{x^{k}\\}$ convergesto $\\rho$ whichis ot statorary poin of $f$ ", "page_idx": 6}, {"type": "text", "text": "The details of the example are presented in Appendix A.2. We now propose a general framework that encompasses SAM and all of its normalized variants including RSAM [Liu et al., 2022], VaSSO [Li and Giannakis, 2023] and F-SAM [Li et al., 2024]. Due to the page limit, we refer readers to Appendix $\\mathrm{D}$ for the detailed constructions of those methods. Remark D.1 in Appendix D also shows that all of these methods are special cases of Algorithm 1a, and thus all the convergence properties presented in Theorem 3.3 follow. ", "page_idx": 6}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/ac61a12789547b8623d3fecf0a8cdd469e5b1514d580d24924989285675573fb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Corollary 3.5. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a $\\mathcal{C}^{1,L}$ function, and let $\\{x^{k}\\}$ be generated by Algorithm la with the parameters ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}t_{k}=\\infty,\\;t_{k}\\downarrow0,\\sum_{k=1}^{\\infty}t_{k}\\rho_{k}<\\infty,\\;\\operatorname*{lim}\\operatorname*{sup}\\rho_{k}<\\frac{2}{L}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumethat $\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})>-\\infty$ Then all convergence properties presented in Theorem 3.3 hold. ", "page_idx": 6}, {"type": "text", "text": "The proof of this result is presented in Appendix C.5. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.6. Note that the conditions in (11) do not pose any obstacles to the implementation of a constant perturbation radius for SAM in practical circumstances. This is due to the fact that a possible selection of $t_{k}$ and $\\rho_{k}$ satisfying (11) is $\\textstyle t_{k}={\\frac{1}{k}}$ and $\\begin{array}{r}{\\rho_{k}=\\frac{C}{k^{0.001}}}\\end{array}$ forall $k\\in\\mathbb{N}$ (almost constant), where $C>0$ . Then the initial perturbation radius is $C$ , while after $C$ million iterations, it remains greater than $0.99C$ . This phenomenon is also confirmed by numerical experiments in Appendix $\\boldsymbol{\\mathrm E}$ onnonconvexfunctionsThenumericalrsutshowthat SAMwithalmotconstatrai $\\begin{array}{r}{\\dot{\\rho}_{k}=\\frac{C}{k^{p}}}\\end{array}$ has a similar convergence behavior to SAM with a constant radius $\\rho=C$ . As SAM with a constant perturbation radius has sufficient empirical evidence for its efficiency in Foret et al. [2021], this also supports the practicality of our almost constant perturbation radi. ", "page_idx": 6}, {"type": "text", "text": "4  USAM and unnormalized variants ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we study the convergence of various versions of USAM from the perspective of the following Inexact Gradient Descent method with relative errors. ", "page_idx": 7}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/cd63bc8d647d83e5b6719948e15d7a4cd43309a368fd0b0cd969c8de2ddd02db.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "This algorithm was initially introduced in Khanh et al. [2023b] in a different form, considering a different selection of error. The form of IGDr closest to Algorithm 2 was established in Khanh et al. [2024a] and then further studied in Khanh et al. [2024a, 2023a, 2024b]. In this paper, we extend the analysis of the method to a general stepsize rule covering both constant and diminishing cases, which was not considered in Khanh et al. [2024a]. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a smooth function satisfying the descent condition for some constant $L>0$ andlet $\\{x^{k}\\}$ be the sequence generatedbyAlgorithm2withtherelative error $\\nu\\in[0,1)$ , and the stepsizes satisfying ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}t_{k}=\\infty\\ a n d\\ t_{k}\\in\\left[0,\\frac{2-2\\nu-\\delta}{L(1+\\nu)^{2}}\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for sufficiently large $k\\,\\in\\,\\mathbb{N}$ and for some $\\delta\\:>\\:0$ \uff1aThen either $f(x^{k})\\,\\rightarrow\\,-\\infty,$ .or we have the assertions: ", "page_idx": 7}, {"type": "text", "text": "(i) Every accumulation point of $\\{x^{k}\\}$ is a stationary point of the cost function $f$ ", "page_idx": 7}, {"type": "text", "text": "(i) If the sequence $\\{x^{k}\\}$ has any accumulation point $\\textstyle{\\bar{x}}$ then $f(x^{k})\\downarrow f({\\bar{x}})$ (ii) If $f\\in\\mathcal{C}^{1,L}$ ,then $\\nabla f(x^{k})\\to0$ (iv) If $f$ satisfies the $K L$ property at some accumulation point $\\bar{x}$ of $f_{;}$ then $\\{x^{k}\\}\\to{\\bar{x}}$ ", "page_idx": 7}, {"type": "text", "text": "(v) Assume in addition to $(i\\nu)$ that the stepsizes are bounded away from O, and the $K L$ property in (iv) holds with the desingularizing function $\\varphi(t)=M t^{1-q}$ with $M>0$ and $q\\in(0,1)$ Then either $\\{x^{k}\\}$ stops fnitely at a stationary point, or the following convergence rates are achieved: ", "page_idx": 7}, {"type": "text", "text": "\u00b7If $\\dot{\\boldsymbol{q}}\\in(1/2,1)$ ,then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|x^{k}-\\bar{x}\\right\\|=\\mathcal{O}\\left(k^{-\\frac{1-q}{2q-1}}\\right),\\ \\left\\|\\nabla f(x^{k})\\right\\|=\\mathcal{O}\\left(k^{-\\frac{1-q}{2q-1}}\\right),\\ f(x^{k})-f(\\bar{x})=\\mathcal{O}\\left(k^{-\\frac{2-2q}{2q-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Although the ideas for proving this result is similar to the one given in Khanh et al. [2024a], we do provide the full proof in the Appendix C.3 for the convenience of the readers. We now show that using this approach, we derive more complete convergence results for USAM in Andriushchenko and Flammarion [2022] and also the extragradient method by Korpelevich [1976], Lin et al. [2020]. ", "page_idx": 7}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/a625166aa0babb1ca791e6f0ff9989a063ecf468f4bb2ab66f3258795df79493.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/511b82d0a258a6f475c543a813e5bacf25a9fb229c3a87c95503c1d1b17abb4a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We are ready now to derive convergence of the two algorithms above. The proof of the theorem is given in Appendix C.4 ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a $\\mathcal{C}^{1}$ -smoothfunction satisfying thedescent conditionwith some constant $L>0$ . Let $\\{x^{k}\\}$ be the sequencegenerated by either Algorithm 2a, or by Algorithm 2b with $\\rho_{k}\\le\\frac{\\nu}{L}$ for some $\\nu\\in[0,1)$ and with the stepsize satisfying (12). Then allthe convergence properties in Theorem 4.1 hold. ", "page_idx": 8}, {"type": "text", "text": "5  Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate the practical aspect of our theory, this section compares the performance of SAM employing constant and diminishing stepsizes in image classification tasks. All the experiments are conducted on a computer with NVIDIA RTX 3090 GPU. The three types of diminishing stepsizes considered in the numerical experiments are $\\eta_{1}/n$ (Diminish 1), $\\bar{\\eta_{1}}/n^{0.5001}$ (Diminish 2), and $\\eta_{1}/m\\log m$ (Diminish 3), where $\\eta_{1}$ is the initial stepsize, $n$ represents the number of epochs performed, and $m=\\lfloor n/5\\rfloor+2$ . The constant stepsize in SAM is selected through a grid search over $\\{0.1,0.01,0.001\\}$ to ensure a fair comparison with the diminishing ones. The algorithms are tested on two widely used image datasets: CIFAR-10 [Krizhevsky et al., 2009] and CIFAR-100 [Krizhevsky et al., 2009]. ", "page_idx": 8}, {"type": "text", "text": "CIFAR-10. We train well-known deep neural networks including ResNet18 [He et al., 2016], ResNet34 [He et al., 2016], and WideResNet28-10 [Zagoruyko and Komodakis, 2016] on this dataset by using $10\\%$ of the training set as a validation set. Basic transformations, including random crop, random horizontal flip, normalization, and cutout [DeVries and Taylor, 2017], are employed for data augmentation. All the models are trained by using SAM with SGD Momentum as the base optimizer for 200 epochs and a batch size of 128. This base optimizer is also used in the original paper [Foret et al., 2021] and in the recent works on SAM [Ahn et al., 2024, Li and Giannakis, 2023]. Following the approach by Foret et al. [2021], we set the initial stepsize to 0.1, momentum to 0.9, the $\\ell_{2}$ -regularization parameter to 0.001, and the perturbation radius $\\rho$ to 0.05. Setting the perturbation radius to be a constant here does not go against our theory, since by Remark 3.6, SAM with a constant radius and our almost constant radius have the same numerical behavior. We also conduct the numerical experiment with an almost constant radius and got the same results. Therefore, for simplicity of presentation, a constant perturbation radius is chosen. The algorithm with the highest accuracy, corresponding to the best performance, is highlighted in bold. The results in Table 5 report the mean and $95\\%$ confidence interval across the three independent runs. The training loss in several tests is presented in Figure 2. ", "page_idx": 8}, {"type": "text", "text": "CIFAR-100. The training configurations for this dataset are similar to CIFAR10. The accuracy results are presented in Table 5, while the training loss results are illustrated in Figure 2. ", "page_idx": 8}, {"type": "image", "img_path": "PuXYI4HOQU/tmp/d9c5dc9fc302136f0a045486145cc0bb343a171e273a9d2508058ad5fc25c733.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/850c35cc29b38ee500f7c74a6b2a8b34d473baecc62db346e2606a2680ae58ec.jpg", "table_caption": ["Figure 2: Training loss on CIFAR-10 (first two graphs) and CIFAR-100 (last two graphs) "], "table_footnote": ["Table 4: Test accuracy on CIFAR-10 and CIFAR-100 "], "page_idx": 8}, {"type": "text", "text": "The results on CIFAR-10 and CIFAR-100 indicate that SAM with Diminish 3 stepsize usually achieves the best performance in both accuracy and training loss among all tested stepsizes. In all the architectures used in the experiment, the results consistently show that diminishing stepsizes outperform constant stepsizes in terms of both accuracy and training loss measures. Additional numerical results on a larger data set and without momentum can be found in Appendix F. ", "page_idx": 9}, {"type": "text", "text": "6 Discusison ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "6.1  Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we provide a fundamental convergence analysis of SAM and its normalized variants together with a refined convergence analysis of USAM and its unnormalized variants. Our analysis is conducted in deterministic settings under standard assumptions that cover a broad range of applications of the methods in both convex and nonconvex optimization. The conducted analysis is universal and thus can be applied in different contexts other than SAM and its variants. The performed numerical experiments show that our analysis matches the efficient implementations of SAM and its variants that are used in practice. ", "page_idx": 9}, {"type": "text", "text": "6.2 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our analysis is only conducted in deterministic settings, which leaves the stochastic and random reshuffling developments to our future research. The analysis of SAM coupling with momentum methods is also not considered in this paper. Another limitation pertains to numerical experiments, where only SAM was tested on three different architectures of deep learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Pham Duy Khanh, Research of this author is funded by the Ministry of Education and Training Research Funding under the project B2024-SPS-07. Boris S. Mordukhovich, Research of this author was partly supported by the US National Science Foundation under grants DMS-1808978 and DMS2204519, by the Australian Research Council under grant DP-190100555, and by Project 111 of China under grant D21024. Dat Ba Tran, Research of this author was partly supported by the US National Science Foundation under grants DMS-1808978 and DMS-2204519. ", "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Professor Mikhail V. Solodov for his fruitful discussions on the convergence of variants of SAM. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "A. Agarwala and Y. N. Dauphin. Sam operates far from home: eigenvalue regularization as a dynamical phenomenon. arXiv preprint arXiv:2302.08692, 2023.   \nK. Ahn, A. Jadbabaie, and S. Sra. How to escape sharp minima with random perturbations. Proceedings of the 38th International Conference on Machine Learning, 2024.   \nM. Andriushchenko and N. Flammarion. Towards understanding sharpness-aware minimization. Proceedings of International Conference on Machine Learning, 2022.   \nH. Attouch, J. Bolte, P. Redont, and A. Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems. Mathematics of Operations Research, pages 438-457, 2010.   \nH. Attouch, J. Bolte, and B. F. Svaiter. Convergence of descent methods for definable and tame problems: Proximal algorithms, forward-backward splitting, and regularized gauss-seidel methods. Mathematical Programming,137:91-129, 2013.   \nP. L. Barlett, P. M. Long, and O. Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. Journal of Machine Learning Research, 24:1-36, 2023.   \nK. Behdin and R. Mazumder. On statistical properties of sharpness-aware minimization: Provable guarantees. arXiv preprint arXiv:2302.11836, 2023. ", "page_idx": 9}, {"type": "text", "text": "D. Bertsekas. Nonlinear programming, 3rd edition. Athena Scientific, Belmont, MA, 2016. ", "page_idx": 10}, {"type": "text", "text": "D. Bertsekas and J. N. Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization, 10:627-642, 2000.   \nE. M. Compagnoni, A. Orvieto, L. Biggio, H. Kersting, F. N. Proske, and A. Lucchi. An sde for modeling sam: Theory and insights. Proceedings of International Conference on Machine Learning, 2023.   \nY. Dai, K. Ahn, and K. Sra. The crucial role of normalization in sharpness-aware minimization. Advances in Neural Information Processing System, 2023.   \nTerrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. https://arxiv.org/abs/1708.04552, 2017.   \nJ. Du, H. Yan, J. Feng, J. T. Zhou, L. Zhen, R. S. M. Goh, and V. Y. F.Tan. Effcient sharpness-aware minimization for improved training of neural networks. Proceedings of International Conference on Learning Representations, 2022.   \nP. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. Proceedings of International Conference on Learning Representations, 2021.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.   \nA. F. Izmailov and M. V. Solodov. Newton-type methods for optimization and variational problems. Springer, 2014.   \nW. Jiang, H. Yang, Y. Zhang, and J. Kwok. An adaptive policy to employ sharpnessaware minimization. Proceedings of International Conference on Learning Representations, 2023.   \nN. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep leaning: Generalization gap and sharpminnma.Procedingsof Intrnational Confrence on Learning Representations, 2017.   \nP. D. Khanh, B. S. Mordukhovich, V. T. Phat, and D. B. Tran. Inexact proximal methods for weakly convex functions. https://arxiv.org/abs/2307.15596, 2023a.   \nP D. Khanh, B. S. Mordukhovich, and D. B. Tran. Inexact reduced gradient methods in smooth nonconvex optimization. Journal of Optimization Theory and Applications, doi.org/10.1007/s10957- 023-02319-9, 2023b.   \nP. D. Khanh, B. S. Mordukhovich, and D. B. Tran. A new inexact gradient descent method with applications to nonsmooth convex optimization. Optimization Methods and Software https://doi.org/10.1080/10556788.2024.2322700, pages 1-29, 2024a.   \nP. D. Khanh, B. S. Mordukhovich, and D. B. Tran. Globally convergent derivative-free methods in nonconvex optimization with and without noise. https://optimization-online.org/ $\\scriptstyle7p=26889$ , 2024b.   \nG. M. Korpelevich. An extragradient method for finding saddle points and for other problems. Ekon. Mat. Metod., page 747-756, 1976.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. technical report, 2009.   \nK. Kurdyka. On gradients of functions definable in o-minimal structures. Annales de l'institut Fourier, pages 769-783, 1998.   \nJ. Kwon, J. Kim, H. Park, and I. K. Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. Proceedings of the 38th International Conference on Machine Learning, pages 5905-5914, 2021.   \nYa Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. In CS 231N: Convolutional Neural Networks for Visual Recognition, Stanford, 2015.   \nB. Li and G. B. Giannakis. Enhancing sharpness-aware optimization through variance suppression. Advances in Neural Information Processing System, 2023.   \nTao Li, Pan Zhou, Zhengbao He, Xinwen Cheng, and Xiaolin Huang. Friendly sharpness-aware minimization. Proceedings of Conference on Computer Vision and Pattern Recognition, 2024.   \nX. Li, A. Milzarek, and J. Qiu. Convergence of random reshuffing under the kurdyka-lojasiewicz inequality. SIAM Journal on Optimization, 33:1092-1120, 2023.   \nT. Lin, L. Kong, S. U. Stich, and M. Jaggi. Extrapolation for large-batch training in deep learning. Proceedings of International Conference on Machine Learning, 2020.   \nY. Liu, S. Mai, M. Cheng, X. Chen, C-J. Hsieh, and Y. You. Random sharpness-aware minimization. Advances in Neural Information Processing System, 2022.   \nS. Lojasiewicz. Ensembles semi-analytiques. Institut des Hautes Etudes Scientifiques, pages 438-457, 1965.   \nI. Loshchilov and F. Hutter. Sgdr: stochastic gradient descent with warm restarts. Proceedings of International Conference on Learning Representations, 2016.   \nO. Mangasarian and M. V. Solodov. Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. Optimzation Methods and Software, 4, 1994.   \nY. Nesterov. Lectures on convex optimization, 2nd edition, Springer, Cham, Switzerland, 2018.   \nB. Polyak. Introduction to optimization. Optimization Software, New York, 1987.   \nA. Ruszczynski. Nonlinear optimization. Princeton university press, 2006.   \nD. Si and C. Yun. Practical sharpness-aware minimization cannot converge all the way to optima. Advances in Neural Information Processing System, 2023.   \nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv: 1605.07146, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1.1 Lack of convergence properties for SAM due to constant stepsize . . 1   \n1.2 Our Contributions . 3   \n1.3 Importance of Our Work. 4   \n1.4 Related Works . . . 4 ", "page_idx": 12}, {"type": "text", "text": "2   Preliminaries 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "3 SAM and normalized variants ", "page_idx": 12}, {"type": "text", "text": "3.1   Convex case 5   \n3.2 Nonconvex case . 6 ", "page_idx": 12}, {"type": "text", "text": "4   USAM and unnormalized variants 8 ", "page_idx": 12}, {"type": "text", "text": "5  Numerical Experiments 9 ", "page_idx": 12}, {"type": "text", "text": "6Discusison 10 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "6.1 Conclusion 10   \n6.2Limitations 10 ", "page_idx": 12}, {"type": "text", "text": "A  Counterexamples ilustrating the Insufficiency of Fundamental Convergence Properties 14 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1  Proof of Example 3.1 14   \nA.2 Proof of Example 3.4 . 14 ", "page_idx": 12}, {"type": "text", "text": "B  Auxiliary Results for Convergence Analysis 15 ", "page_idx": 12}, {"type": "text", "text": "C  Proof of Convergence Results 17 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Proof of Theorem 3.2 17   \nC.2Proof of Theorem 3.3 19   \nC.3 Proof of Theorem 4.1 . 22   \nC.4 Proof of Theorem 4.2 . 24   \nC.5 Proof of Corollary 3.5 . . 24 ", "page_idx": 12}, {"type": "text", "text": "D  Efficient normalized variants of SAM 24 ", "page_idx": 12}, {"type": "text", "text": "E  Numerical experiments on SAM constant and SAM almost constant 25   \nF Numerical experiments on SAM with SGD without momentum as base optimizer 27   \nG  Additional Remarks 27 ", "page_idx": 12}, {"type": "text", "text": "A Counterexamples illustrating the Insufficiency of Fundamental Convergence Properties ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Example 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Since $f(x)={\\textstyle{\\frac{1}{2}}}\\left\\langle A x,x\\right\\rangle-\\left\\langle b,x\\right\\rangle$ , the gradient of $f$ and the optimal solution are given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla f(x)=A x-b\\quad{\\mathrm{~and~}}\\quad x^{*}=A^{-1}b.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}>0$ be the minimum, maximum eigenvalues of $A$ , respectively and assume that ", "page_idx": 13}, {"type": "equation", "text": "$$\nt\\in\\left(\\frac{1}{\\lambda_{\\operatorname*{min}}}-\\frac{1}{\\lambda_{\\operatorname*{max}}+\\lambda_{\\operatorname*{min}}},\\frac{1}{\\lambda_{\\operatorname*{min}}}\\right),\\ \\rho>0,\\ \\mathrm{and}\\ 0<\\left\\|x^{1}-x^{*}\\right\\|<\\frac{t\\rho\\lambda_{\\operatorname*{min}}}{1-t\\lambda_{\\operatorname*{min}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The iterative procedure of (6) can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-t\\nabla f\\left(x^{k}+\\rho{\\frac{\\nabla f(x^{k})}{\\left\\|\\nabla f(x^{k})\\right\\|}}\\right)=x^{k}-t\\left[A\\left(x^{k}+\\rho{\\frac{A x^{k}-b}{\\left\\|A x^{k}-b\\right\\|}}\\right)-b\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then $\\{x^{k}\\}$ satisfies the inequalities ", "page_idx": 13}, {"type": "equation", "text": "$$\n0<\\left\\|x^{k}-A^{-1}b\\right\\|<{\\frac{t\\rho\\lambda_{\\operatorname*{min}}}{1-t\\lambda_{\\operatorname*{min}}}}~{\\mathrm{for}}~{\\mathrm{all}}~~k\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is obvious that (15) holds for $k=1$ . Assuming that this condition holds for any $k\\in\\mathbb{N}$ , let us show that it holds for $k+1$ . We deduce from the iterative update (14) that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|x^{k+1}-A^{-1}b\\right\\|=\\left\\|x^{k}-t\\left[A\\left(x^{k}+\\rho\\frac{A x^{k}-b}{\\|A x^{k}-b\\|}\\right)-b\\right]-A^{-1}b\\right\\|}\\\\ {\\quad}\\\\ {\\displaystyle=\\left\\|(I-t A)(x^{k}-A^{-1}b)-t\\rho\\frac{A(A x^{k}-b)}{\\|A x^{k}-b)\\|}\\right\\|}\\\\ {\\displaystyle\\geq\\left\\|t\\rho\\frac{A(A x^{k}-b)}{\\|A x^{k}-b\\|}\\right\\|-\\left\\|(I-t A)(x^{k}-A^{-1}b)\\right\\|}\\\\ {\\displaystyle\\geq t\\rho\\lambda_{\\mathrm{min}}-(1-t\\lambda_{\\mathrm{min}})\\left\\|x^{k}-A^{-1}b\\right\\|>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In addition, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|x^{k+1}-A^{-1}b\\right\\|\\leq\\left\\|t\\rho\\frac{A(A x^{k}-b)}{\\|A x^{k}-b\\|}\\right\\|+\\left\\|(I-t A)(x^{k}-A^{-1}b)\\right\\|}\\\\ &{\\qquad\\qquad\\leq t\\rho\\lambda_{\\operatorname*{max}}+(1-t\\lambda_{\\operatorname*{min}})\\left\\|x^{k}-A^{-1}b\\right\\|}\\\\ &{\\qquad\\qquad\\leq t\\rho\\lambda_{\\operatorname*{max}}+t\\rho\\lambda_{\\operatorname*{min}}<t\\rho\\frac{\\lambda_{\\operatorname*{min}}}{1-t\\lambda_{\\operatorname*{min}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality follows from $\\begin{array}{r}{t>\\frac{1}{\\lambda_{\\operatorname*{min}}}-\\frac{1}{\\lambda_{\\operatorname*{max}}+\\lambda_{\\operatorname*{min}}}}\\end{array}$ from (13). Thus, (15) is verified. It follows from (16) that $x^{k}\\nrightarrow x^{*}$ \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Example 3.4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Observe that $x^{k}>\\rho$ for all $k\\in\\mathbb{N}$ . Indeed, this follows from $x^{1}>\\rho,t_{k}<1/2$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\nx^{k+1}-\\rho=x^{k}-\\rho-2t_{k}(x^{k}-\\rho)=(1-2t_{k})(x^{k}-\\rho).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In addition, we readily get ", "page_idx": 13}, {"type": "equation", "text": "$$\n0\\le x^{k+1}-\\rho=(1-2t_{k})(x^{k}-\\rho)=\\ldots=(x^{1}-\\rho)\\prod_{i=1}^{k}(1-2t_{i}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Furthermore, deduce from $\\textstyle\\sum_{k=1}^{\\infty}2t_{k}=\\infty$ that $\\begin{array}{r}{\\prod_{k=1}^{\\infty}(1-2t_{k})=0}\\end{array}$ Indeed, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n0\\leq\\prod_{k=1}^{\\infty}(1-2t_{k})\\leq{\\frac{1}{\\prod_{k=1}^{\\infty}(1+2t_{k})}}\\leq{\\frac{1}{1+\\sum_{k=1}^{\\infty}2t_{k}}}=0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This tells us by (17) and the classical squeeze theorem that $x^{k}\\to\\rho$ as $k\\rightarrow\\infty$ ", "page_idx": 13}, {"type": "text", "text": "B  Auxiliary Results for Convergence Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first establish the new three sequences lemma, which is crucial in the analysis of both SAM, USAM, and their variants. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1 (three sequences lemma). Let $\\{\\alpha_{k}\\},\\{\\beta_{k}\\},\\{\\gamma_{k}\\}$ be sequences of nonnegative numbers satisfying the conditions ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\{\\beta_{k}\\right\\}\\;{\\mathrm{is~bounded,}}\\sum_{k=1}^{\\infty}\\beta_{k}=\\infty,\\;\\sum_{k=1}^{\\infty}\\gamma_{k}<\\infty,\\;\\;{\\mathrm{and}}\\;\\;\\sum_{k=1}^{\\infty}\\beta_{k}\\alpha_{k}^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we have that $\\alpha_{k}\\rightarrow0$ as $k\\rightarrow\\infty$ ", "page_idx": 14}, {"type": "text", "text": "Proof. First we show that $\\operatorname*{lim}\\operatorname*{inf}_{k\\to\\infty}\\alpha_{k}=0$ Supposing the contrary gives us some $\\delta>0$ and $N\\in\\mathbb{N}$ such that $\\alpha_{k}\\geq\\delta$ for all $k\\geq N$ . Combining this with the second and the third condition in (b) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\infty>\\sum_{k=N}^{\\infty}\\beta_{k}\\alpha_{k}^{2}\\geq\\delta^{2}\\sum_{k=N}^{\\infty}\\beta_{k}=\\infty,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is a contradiction verifying the claim. Let us now show that in fact $\\dim_{k\\to\\infty}\\alpha_{k}=0$ Indeed, by the boundedness of $\\{\\beta_{k}\\}$ define $\\bar{\\beta}:=\\operatorname*{sup}_{k\\in\\mathbb{N}}\\beta_{k}$ and deduce from (a) that there exists $K\\in\\mathbb N$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha_{k+1}-\\alpha_{k}\\leq\\beta_{k}\\alpha_{k}+\\gamma_{k}{\\mathrm{~for~all~}}k\\geq K.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Pick $\\varepsilon>0$ and find by $\\operatorname*{lim}\\operatorname*{inf}_{k\\to\\infty}\\alpha_{k}=0$ and the two last conditions in (b) some $K_{\\varepsilon}\\in\\mathbb{N}$ with $K_{\\varepsilon}\\ge K,\\alpha_{K_{\\varepsilon}}\\le\\varepsilon$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{k=K_{\\varepsilon}}^{\\infty}\\gamma_{k}<\\frac{\\varepsilon}{3},\\sum_{k=K_{\\varepsilon}}^{\\infty}\\beta_{k}\\alpha_{k}^{2}<\\frac{\\varepsilon^{2}}{3},\\;\\;\\mathrm{and}\\;\\;\\bar{\\beta}\\beta_{k}\\alpha_{k}^{2}\\leq\\frac{\\varepsilon^{2}}{9}\\;\\;\\mathrm{as}\\;\\;k\\geq K_{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It suffices to show that $\\alpha_{k}\\leq2\\varepsilon$ for all $k\\geq K_{\\varepsilon}$ . Fix $k\\geq K_{\\varepsilon}$ and observe that for $\\alpha_{k}\\leq\\varepsilon$ the desired inequality is obviously satisfied. If $\\alpha_{k}>\\varepsilon$ , we use $\\alpha_{K_{\\varepsilon}}\\le\\varepsilon$ and find some $k^{\\prime}<k$ such that $k^{\\prime}\\geq K_{\\varepsilon}$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha_{k^{\\prime}}\\leq\\varepsilon{\\mathrm{~}}{\\mathrm{and~}}\\;\\alpha_{i}>\\varepsilon{\\mathrm{~}}{\\mathrm{for~}}\\;i=k,k-1,\\ldots,k^{\\prime}+1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we deduce from (18) and (19) that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{k}-\\alpha_{k^{\\prime}}=\\displaystyle\\sum_{i=k^{\\prime}}^{k-1}(\\alpha_{i+1}-\\alpha_{i})\\le\\displaystyle\\sum_{i=k^{\\prime}}^{k-1}(\\beta_{i}\\alpha_{i}+\\gamma_{i})}\\\\ {=\\displaystyle\\sum_{i=k^{\\prime}+1}^{k}\\beta_{i}\\alpha_{i}+\\displaystyle\\sum_{i=k^{\\prime}}^{k-1}\\gamma_{i}+\\beta_{k^{\\prime}}\\alpha_{k^{\\prime}}}\\\\ {\\le\\displaystyle\\frac1{\\varepsilon}\\displaystyle\\sum_{i=k^{\\prime}+1}^{k}\\beta_{i}\\alpha_{i}^{2}+\\displaystyle\\sum_{i=k^{\\prime}}^{k-1}\\gamma_{i}+\\sqrt{\\beta_{k^{\\prime}}}\\sqrt{\\beta_{k^{\\prime}}}\\alpha_{k^{\\prime}}}\\\\ {\\le\\displaystyle\\frac1{\\varepsilon}\\displaystyle\\sum_{i=K_{\\varepsilon}}^{\\infty}\\beta_{i}\\alpha_{i}^{2}+\\displaystyle\\sum_{i=K_{\\varepsilon}}^{\\infty}\\gamma_{i}+\\sqrt{\\beta}\\beta_{k^{\\prime}}\\alpha_{k^{\\prime}}^{2}}\\\\ {\\le\\displaystyle\\frac1{\\varepsilon}\\displaystyle\\frac{\\varepsilon^{2}}3+\\frac{\\varepsilon}3+\\frac{\\varepsilon}3=\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As a consequence, we arrive at the estimate ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\alpha_{k^{\\prime}}+\\alpha_{k}-\\alpha_{k^{\\prime}}\\leq\\varepsilon+\\varepsilon=2\\varepsilon\\,\\mathrm{~for~all~}\\,k\\geq K_{\\varepsilon},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which verifies that $\\alpha_{k}\\rightarrow0$ as $k\\rightarrow\\infty$ sand thus completes the proof of the lemma. ", "page_idx": 14}, {"type": "text", "text": "Next we recall some auxiliary results from Khanh et al. [2023b]. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. Let $\\{x^{k}\\}$ and $\\{d^{k}\\}$ be sequences in $\\mathbb{R}^{n}$ satisfying the condition ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\left\\|x^{k+1}-x^{k}\\right\\|\\cdot\\left\\|d^{k}\\right\\|<\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\textstyle{\\bar{x}}$ is an accumulation point of the sequence $\\{x^{k}\\}$ and O is an accumulation points of the sequence $\\{d^{k}\\}$ , then there exists an infinite set $J\\subset\\mathbb{N}$ such that we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nx^{k}\\stackrel{J}{\\rightarrow}\\bar{x}\\;\\;a n d\\;\\;d^{k}\\stackrel{J}{\\rightarrow}0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition B.3. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ bea $\\mathcal{C}^{1}$ -smooth function, and let the sequence $\\{x^{k}\\}\\subset\\mathbb{R}^{n}$ satisfy the conditions: ", "page_idx": 15}, {"type": "text", "text": "(H1) (primary descent condition). There exists $\\sigma>0$ such that for sufficiently large $k\\in\\mathbb{N}\\,w e$ have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf({\\boldsymbol{x}}^{k})-f({\\boldsymbol{x}}^{k+1})\\geq\\sigma\\left\\|\\nabla f({\\boldsymbol{x}}^{k})\\right\\|\\cdot\\left\\|{\\boldsymbol{x}}^{k+1}-{\\boldsymbol{x}}^{k}\\right\\|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\left(\\mathbf{H}2\\right)$ (complementary descent condition). For sufficiently large $k\\in\\mathbb{N}$ wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n[f(x^{k+1})=f(x^{k})]\\Longrightarrow[x^{k+1}=x^{k}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "f $\\textstyle{\\bar{x}}$ is an accumulation point of $\\{x^{k}\\}$ and $f$ satisfies the KL property at $\\textstyle{\\bar{x}}$ then $x^{k}\\rightarrow{\\bar{x}}$ as $k\\rightarrow\\infty$ ", "page_idx": 15}, {"type": "text", "text": "When the sequence under consideration is generated by a linesearch method and satisfies some conditions stronger than (H1) and (H2) in Proposition B.3, its convergence rates are established in Khanh et al. [2023b, Proposition 2.4] under the KL property with $\\psi(t)=M t^{1-q}$ asgivenbelow. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.4. Let $f\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ be a $\\mathcal{C}^{1}$ -smooth function, and let the sequences $\\{x^{k}\\}~\\subset$ $\\mathbb{R}^{n},\\{\\tau_{k}\\}\\subset[0,\\infty)$ \uff0c $\\{d^{k}\\}\\subset\\mathbb{R}^{n}$ satisfy the iterative condition $x^{k+1}=x^{k}+\\tau_{k}d^{k}$ for all $k\\in\\mathbb{N}$ Assume that for all suffciently large $k\\in\\mathbb{N}$ we have $x^{k+1}\\neq x^{k}$ and the estimates ", "page_idx": 15}, {"type": "equation", "text": "$$\nf({\\boldsymbol{x}}^{k})-f({\\boldsymbol{x}}^{k+1})\\geq\\beta\\tau_{k}\\left\\|{\\boldsymbol{d}}^{k}\\right\\|^{2}~a n d~\\left\\|\\nabla f({\\boldsymbol{x}}^{k})\\right\\|\\leq\\alpha\\left\\|{\\boldsymbol{d}}^{k}\\right\\|,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\alpha,\\beta\\,>0$ \uff1aSuppose in addition that the sequence $\\{\\tau_{k}\\}$ is bounded away from O (i.e., there is some $\\bar{\\tau}>0$ such that $\\tau_{k}\\geq\\bar{\\tau}$ for large $k\\in\\mathbb{N}$ ),that $\\textstyle{\\bar{x}}$ is an accumulation point of $\\left\\{x^{k}\\right\\}$ and that $f$ satisfiesthe $K L$ property at $\\textstyle{\\bar{x}}$ with $\\psi(t)=M t^{1-q}$ for some $M>0$ and $q\\in(0,1)$ . Then the following convergence rates are guaranteed: ", "page_idx": 15}, {"type": "text", "text": "(i) If $q\\in(0,1/2]$ then the sequence $\\{x^{k}\\}$ converges linearly to $\\bar{x}$ (ii) If $q\\in(1/2,1)$ ,then we have the estimate ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|x^{k}-{\\bar{x}}\\right\\|={\\mathcal{O}}\\left(k^{-{\\frac{1-q}{2q-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Yet another auxiliary result needed below is as follows. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.5. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a $\\mathcal{C}^{1}$ -smooth function satisfying the descent condition (4) with some constant $L>0$ Let $\\{x^{k}\\}$ be a sequence in $\\mathbb{R}^{n}$ that converges to $\\bar{x}$ ,and let $\\alpha>0$ be such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\leq f(x^{k})-f(x^{k+1})\\ f o r\\,s u f\\!f\\!c i e n t l y\\ l a r g e}&{\\in\\mathbb{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider the following convergence rates of $\\{x^{k}\\}$ ", "page_idx": 15}, {"type": "text", "text": "(i) $x^{k}\\rightarrow\\bar{x}$ linearly. ", "page_idx": 15}, {"type": "text", "text": "Then (i) ensures the linear convergences of $f(\\boldsymbol x^{k})$ to $f({\\bar{x}})$ and $\\nabla f(x^{k})$ to $0$ while (i) yields $\\left|f(x^{k})-f(\\bar{x})\\right|={\\mathcal{O}}(m^{2}(k))$ and $\\left\\|\\nabla f(x^{k})\\right\\|=\\mathcal{O}(m(k))$ as $k\\rightarrow\\infty$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Condition (22) tells us that there exists some $N\\in\\mathbb{N}$ such that $f(x^{k+1})\\,\\leq\\,f(x^{k})$ for all $k\\geq\\mathbb{N}$ .As $x^{k}\\rightarrow\\bar{x}$ ,we deduce that $f(x^{k})\\to f({\\bar{x}})$ with $f(x^{k})\\geq f({\\bar{x}})$ for $k\\geq N$ Letting $k\\rightarrow\\infty$ in (22) and using the squeeze theorem together with the convergence of $\\{x^{k}\\}$ to $\\textstyle{\\bar{x}}$ and the continuity of $\\nabla f$ lead us to $\\nabla f({\\bar{x}})=0$ . It follows from the descent condition of $f$ with constant $L>0$ and from (4) that ", "page_idx": 16}, {"type": "equation", "text": "$$\n0\\leq f(x^{k})-f(\\bar{x})\\leq\\left\\langle\\nabla f(\\bar{x}),x^{k}-\\bar{x}\\right\\rangle+\\frac{L}{2}\\left\\Vert x^{k}-\\bar{x}\\right\\Vert^{2}=\\frac{L}{2}\\left\\Vert x^{k}-\\bar{x}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This verifes the desired convergence rates of $\\{f(x^{k})\\}$ . Employing fnally (22) and $f(x^{k+1})\\geq f({\\bar{x}})$ we also get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\leq f(x^{k})-f({\\bar{x}}){\\mathrm{~for~all~}}k\\geq N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This immediately gives us the desired convergence rates for $\\{\\nabla f(x^{k})\\}$ and completes the proof. ", "page_idx": 16}, {"type": "text", "text": "C Proof of Convergence Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Prof Toverify () frst foray INdee g =f(r+ p) and get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|=\\left\\|\\nabla f\\left(x^{k}+\\rho_{k}\\frac{\\nabla f(x^{k})}{\\left\\|\\nabla f(x^{k})\\right\\|}\\right)-\\nabla f(x^{k})\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq L\\left\\|x^{k}+\\rho_{k}\\frac{\\nabla f(x^{k})}{\\left\\|\\nabla f(x^{k})\\right\\|}-x^{k}\\right\\|=L\\rho_{k}\\leq L\\rho,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\rho:=\\operatorname*{sup}_{k\\in\\mathbb{N}}\\rho_{k}$ . Using the monotonicity of $\\nabla f$ due to the convexity of $f$ ensures that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{k},\\nabla f(x^{k})\\rangle=\\left\\langle\\nabla f\\left(x^{k}+\\rho_{k}\\frac{\\nabla f(x^{k})}{\\left\\|\\nabla f(x^{k})\\right\\|}\\right)-\\nabla f(x^{k}),\\nabla f(x^{k})\\right\\rangle+\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{\\left\\|\\nabla f(x^{k})\\right\\|}{\\rho_{k}}\\left\\langle\\nabla f\\left(x^{k}+\\rho_{k}\\frac{\\nabla f(x^{k})}{\\left\\|\\nabla f(x^{k})\\right\\|}\\right)-\\nabla f(x^{k}),\\rho_{k}\\frac{\\nabla f(x^{k})}{\\left\\|\\nabla f(x^{k})\\right\\|}\\right\\rangle+\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With the definition of $g^{k}$ , the iterative procedure (6) can also be rewritten as $x^{k+1}=x^{k}-t_{k}g^{k}$ for all $k\\in\\mathbb{N}$ The first condition in (7) yields $t_{k}\\downarrow0$ , which gives us some $K\\in\\mathbb{N}$ such that $L t_{k}<1$ for all $k\\geq K$ . Take some such $k$ . Since $\\nabla f$ is Lipschitz continuous with constant $L>0$ , it follows from the descent condition in (4) and the estimates in (23), (24) that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(x^{k+1})\\leq f(x^{k})+\\left\\langle\\nabla f(x^{k}),x^{k+1}-x^{k}\\right\\rangle+\\frac{L}{2}\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}}&{}\\\\ {=f(x^{k})-t_{k}\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle+\\frac{L t_{k}^{2}}{2}\\left\\Vert g^{k}\\right\\Vert^{2}}&{}\\\\ {=f(x^{k})-t_{k}(1-L t_{k})\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle+\\frac{L t_{k}^{2}}{2}\\left(\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}-\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right)}&{}\\\\ {\\leq f(x^{k})-t_{k}(1-L t_{k})\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}-\\frac{L t_{k}^{2}}{2}\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\frac{L^{3}t_{k}^{2}\\rho^{2}}{2}}&{}\\\\ {=f(x^{k})-\\frac{t_{k}}{2}\\left(2-L t_{k}\\right)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\frac{L^{3}t_{k}^{2}\\rho^{2}}{2}}&{}\\\\ {\\leq f(x^{k})-\\frac{t_{k}}{2}\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\frac{L^{3}t_{k}^{2}\\rho^{2}}{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rearranging the terms above gives us the estimate ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{t_{k}}{2}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\leq f(x^{k})-f(x^{k+1})+\\frac{L^{3}t_{k}^{2}\\rho^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$M>K$ $\\begin{array}{r}{S:=\\frac{L^{3}\\rho^{2}}{2}\\sum_{k=1}^{\\infty}t_{k}^{2}<\\infty}\\end{array}$ $\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})>$ $-\\infty$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{2}\\sum_{k=K}^{M}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\leq\\sum_{k=K}^{M}\\left(f(x^{k})-f(x^{k+1})\\right)+\\sum_{k=K}^{M}\\frac{L^{3}t_{k}^{2}\\rho^{2}}{2}}}\\\\ &{\\leq f(x^{K})-f(x^{M+1})+S}\\\\ &{\\leq f(x^{K})-\\displaystyle\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})+S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Letting $M\\to\\infty$ yields $\\begin{array}{r}{\\sum_{k=K}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}<\\infty}\\end{array}$ Let us now show that lim inf $\\left\\|\\nabla f(x^{k})\\right\\|=0$ Supposing the contrary gives us $\\varepsilon>0$ and $N\\geq K$ such that $\\left\\|\\nabla f(x^{k})\\right\\|\\geq\\varepsilon$ for all $k\\geq N$ , which tells us that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\infty>\\sum_{k=N}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\geq\\varepsilon^{2}\\sum_{k=N}^{\\infty}t_{k}=\\infty.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This clearly contradicts the second condition in (7) and this justifies (i). ", "page_idx": 17}, {"type": "text", "text": "To verify (i) defne us := for all $k\\in\\mathbb{N}$ anddedcfm te frst condionn ) that $u_{k}\\downarrow0$ as $k\\rightarrow\\infty$ With the usage of $\\{u_{k}\\}$ , estimate (26) is written as ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x^{k+1})+u_{k+1}\\leq f(x^{k})+u_{k}-{\\frac{t_{k}}{2}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\ {\\mathrm{~for~all~}}\\ k\\geq K,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which means that $\\left\\{f(x^{k})+u_{k}\\right\\}_{k\\geq K}$ is nonincreasing. It follows from $\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})>-\\infty$ and $u_{k}\\downarrow0$ that $\\left\\{f(x^{k})+u_{k}\\right\\}$ is convergent, which means that the sequence $\\{f(x^{k})\\}$ is convergent as well. Assume now $f$ has some nonempty and bounded level set. Then every level set of $f$ is bounded by Ruszczynski [2006, Exercise 2.12]. By (26), we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x^{k+1})\\leq f(x^{k})+{\\frac{L^{3}\\rho^{2}}{2}}t_{k}^{2}-{\\frac{t_{k}}{2}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\leq f(x^{k})+{\\frac{L^{3}\\rho^{2}}{2}}t_{k}^{2}{\\mathrm{~}}{\\mathrm{~for~all~}}\\,k\\geq K.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proceeding by induction leads us to ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x^{k+1})\\leq f(x^{1})+{\\frac{L^{3}\\rho^{2}}{2}}\\sum_{i=1}^{k}t_{i}^{2}\\leq f(x^{1})+S{\\mathrm{~}}\\;{\\mathrm{for~all~}}\\;k\\geq K,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which means that $x^{k+1}\\;\\in\\;\\left\\{x\\in\\mathbb{R}^{n}\\;|\\;f(x)\\le f(x^{1})+S\\right\\}$ for all $k\\ \\geq\\ K$ and thus jsties the boundedness of $\\{x^{k}\\}$ ", "page_idx": 17}, {"type": "text", "text": "Taking lim inf $\\left\\|\\nabla f(x^{k})\\right\\|=0$ into account gives us an infinite set $J\\subset\\mathbb{N}$ such that $\\left\\|\\nabla f(x^{k})\\right\\|\\overset{J}{\\rightarrow}0$ $\\{x^{k}\\}$ $\\{x^{k}\\}_{k\\in J}$   \n$I\\subset J$ and $\\bar{x}\\in\\mathbb{R}^{n}$ such that $x^{k}\\xrightarrow{I}\\bar{x}$ By ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\in I}\\left\\|\\nabla f(x^{k})\\right\\|=\\operatorname*{lim}_{k\\in J}\\left\\|\\nabla f(x^{k})\\right\\|=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the continuity of $\\nabla f$ , we get that $\\nabla f({\\bar{x}})=0$ ensuring that $\\textstyle{\\bar{x}}$ is a global minimizer of $f$ with the optimal value $f^{*}:=f(\\bar{x})$ . Since the sequence $\\{f(x^{k})\\}$ is convergent and since $\\textstyle{\\bar{x}}$ is an accumulation point of $\\{x^{k}\\}$ , we conclude that $f^{*}=f(\\bar{x})$ is the limit of $\\{f(x^{k})\\}$ . Now take any accumulation point $\\widetilde{x}$ of $\\{x^{k}\\}$ and find an infnite set $J^{\\prime}\\subset\\mathbb{N}$ with $x^{k}\\xrightarrow[]{J^{\\prime}}\\tilde{x}$ .As $\\{f(x^{k})\\}$ converges to $f^{*}$ ,we deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\nf({\\widetilde{x}})=\\operatorname*{lim}_{k\\in J^{\\prime}}f(x^{k})=\\operatorname*{lim}_{k\\in\\mathbb{N}}f(x^{k})=f^{*},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that $\\widetilde{x}$ is also a global minimizer of $f$ Assuming in addition that $f$ has a unique minimizer $\\textstyle{\\bar{x}}$ and taking any accumulation point $\\widetilde{x}$ of $\\{x^{k}\\}$ , we get that $\\widetilde{x}$ is a minimizer of $f$ i.e., $\\widetilde{x}\\,=\\,\\bar{x}$ . This means that $\\textstyle{\\bar{x}}$ is the unique accumulation point of $\\{x^{k}\\}$ , and therefore $x^{k}\\ \\to\\ \\bar{x}$ as $k\\rightarrow\\infty$ \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C.2 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. By (8), we find some $c_{1}>0,c_{2}\\in(0,1)$ and $K\\in\\mathbb{N}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac12(2-L t_{k}-\\varepsilon_{k}+L t_{k}\\varepsilon_{k})\\ge c_{1},\\quad\\frac12(1-L t_{k})+\\frac{L t_{k}\\varepsilon_{k}}{2}\\le c_{2},\\,\\,\\,\\mathrm{and}\\,\\,\\,\\,\\,L t_{k}<1\\,\\,\\,\\mathrm{for}\\,\\,\\mathrm{all}\\,\\,\\,k\\ge K.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let us first verify the estimate ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(x^{k+1})\\leq f(x^{k})-c_{1}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}+c_{2}t_{k}\\varepsilon_{k}\\;{\\mathrm{~whenever~}}\\;k\\geq K.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To proceed, fix $k\\in\\mathbb{N}$ and deduce from the Cauchy-Schwarz inequality that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle g^{k},\\nabla f(x^{k})\\right\\rangle=\\left\\langle g^{k}-\\nabla f(x^{k}),\\nabla f(x^{k})\\right\\rangle+\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\geq-\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|\\cdot\\left\\|\\nabla f(x^{k})\\right\\|+\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\geq-\\varepsilon_{k}\\left\\|\\nabla f(x^{k})\\right\\|+\\left\\|\\nabla f(x^{k})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\nabla f$ is Lipschitz continuous with constant $L$ , it follows from the descent condition in (4) and the estimate (29) that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x}^{k+1})\\leq f(\\mathbf{x}^{k})+\\left\\langle\\nabla f(\\mathbf{x}^{k}),\\mathbf{z}^{k+1}-\\mathbf{x}^{k}\\right\\rangle+\\frac{L}{2}\\left\\Vert\\mathbf{z}^{k+1}-\\mathbf{x}^{k}\\right\\Vert^{2}}\\\\ &{\\quad=f(\\mathbf{x}^{k})-t_{k}\\left\\langle\\nabla f(\\mathbf{z}^{k}),\\mathbf{z}^{k}\\right\\rangle+\\frac{L U_{k}^{2}}{2}\\left\\Vert\\mathbf{z}^{k}\\right\\Vert^{2}}\\\\ &{\\quad=f(\\mathbf{x}^{k})-t_{k}(1-L\\mathbf{z}_{k})\\left\\langle\\nabla f(\\mathbf{z}^{k}),\\mathbf{z}^{k}\\right\\rangle+\\frac{L t_{k}^{2}}{2}\\left\\Vert\\mathbf{z}^{k}-\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}-\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}}\\\\ &{\\quad\\leq f(\\mathbf{x}^{k})-t_{k}(1-L t_{k})\\left(-\\varepsilon_{k}\\right\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert+\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}\\right)+\\frac{L t_{k}^{2}\\varepsilon_{k}^{2}}{2}-\\frac{L t_{k}^{2}}{2}\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}}\\\\ &{\\quad=f(\\mathbf{x}^{k})-\\frac{t_{k}}{2}(2-L t_{k})\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}+t_{k}(1-L t_{k})\\varepsilon_{k}\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert+\\frac{L t_{k}^{2}\\varepsilon_{k}^{2}}{2}}\\\\ &{\\quad\\leq f(\\mathbf{x}^{k})-\\frac{L t}{2}(2-L t_{k})\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}+\\frac{1}{2}t_{k}(1-L t_{k})\\varepsilon_{k}\\left(1+\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}\\right)+\\frac{L t_{k}^{2}\\varepsilon_{k}^{2}}{2}}\\\\ &{\\quad=f(\\mathbf{x}^{k})-\\frac{L}{2}(2-L t_{k}-\\varepsilon_{k}+L t_{k}\\varepsilon_{k})\\left\\Vert\\nabla f(\\mathbf{z}^{k})\\right\\Vert^{2}+ \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining this with 27) gives us 28).Dening $\\begin{array}{r}{u_{k}:=c_{2}\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}}\\end{array}$ for $k\\in\\mathbb{N}$ we get that $u_{k}\\to0$ as $k\\rightarrow\\infty$ and $u_{k}-u_{k+1}=t_{k}\\varepsilon_{k}$ for all $k\\in\\mathbb{N}$ . Then (28) can be rewritten as ", "page_idx": 18}, {"type": "equation", "text": "$$\nf({x}^{k+1})+{u}_{k+1}\\leq f({x}^{k})+{u}_{k}-c_{1}t_{k}\\left\\|\\nabla f({x}^{k})\\right\\|^{2},\\quad k\\geq K.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To proceed now with the proof of (i), we deduce from (30) combined with inf $f(x^{k})>-\\infty$ and $u_{k}\\rightarrow0$ as $k\\rightarrow\\infty$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{c_{1}\\displaystyle\\sum_{k=K}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\leq\\displaystyle\\sum_{k=K}^{\\infty}(f(x^{k})-f(x^{k+1})+u_{k}-u_{k+1})}\\\\ {\\leq f(x^{K})-\\displaystyle\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})+u_{K}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next we employ Lemma B.1 with $\\alpha_{k}:=\\|\\nabla f(x^{k})\\|$ \uff0c $\\beta_{k}:=L t_{k}$ , and $\\gamma_{k}:=L t_{k}\\varepsilon_{k}$ for all $k\\in\\mathbb{N}$ to derive $\\nabla f(x^{k})\\to0$ . Observe first that condition (a) is satisfied due to the the estimates ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{k+1}-\\alpha_{k}=\\left\\|\\nabla f(x^{k+1})\\right\\|-\\left\\|\\nabla f(x^{k})\\right\\|\\leq\\left\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})\\right\\|}\\\\ &{\\qquad\\qquad\\leq L\\left\\|x^{k+1}-x^{k}\\right\\|=L t_{k}\\left\\|g^{k}\\right\\|}\\\\ &{\\qquad\\qquad\\leq L t_{k}(\\left\\|\\nabla f(x^{k})\\right\\|+\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|)}\\\\ &{\\qquad\\qquad\\leq L t_{k}(\\left\\|\\nabla f(x^{k})\\right\\|+\\varepsilon_{k})}\\\\ &{\\qquad=\\beta_{k}\\alpha_{k}+\\gamma_{k}\\;\\mathrm{~for~all~}k\\in\\mathbb{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further, the conditions in (b) hold by (8) and $\\begin{array}{r}{\\sum_{k=1}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}<\\infty}\\end{array}$ As all the assumptions (a), (b) are satisfied, Lemma B.1 tells us that $\\left\\|\\nabla f(x^{k})\\right\\|=\\alpha_{k}\\to0$ as $k\\rightarrow\\infty$ ", "page_idx": 19}, {"type": "text", "text": "To verify (i), deduce from (30) that $\\left\\{f(x^{k})+u_{k}\\right\\}$ is nonincreasing. As $\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})>-\\infty$ and $u_{k}\\,\\rightarrow\\,0$ , we get that $\\left\\{f(x^{k})+u_{k}\\right\\}$ is bounded from below, and thus is convergent. Taking into account that $u_{k}\\rightarrow0$ , it follows that $f(\\boldsymbol x^{k})$ is convergent as well. Since $\\textstyle{\\bar{x}}$ is an accumulation point of $\\{x^{k}\\}$ , the continuity of $f$ tells us that $f({\\bar{x}})$ is also an accumulation point of $\\{f(x^{k})\\}$ , which immediately yields $f(x^{k})\\to f({\\bar{x}})$ due to the convergence of $\\{f(x^{k})\\}$ ", "page_idx": 19}, {"type": "text", "text": "It remains to verify (ii). By the $\\mathrm{KL}$ property of $f$ at $\\textstyle{\\bar{x}}$ , we find some $\\eta>0$ ,aneighborhood $U$ of $\\textstyle{\\bar{x}}$ ; and a desingularizing concave continuous function $\\varphi:[0,\\eta)\\rightarrow[0,\\infty)$ such that $\\varphi(0)=0$ $\\varphi$ is $\\mathcal{C}^{1}$ -smooth on $(0,\\eta)$ \uff0c $\\varphi^{\\prime}>0$ on $(0,\\eta)$ , and we have for all $x\\in U$ with $0<f(x)-f(\\bar{x})<\\eta$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varphi^{\\prime}(f(x)-f({\\bar{x}}))\\left\\|\\nabla f(x)\\right\\|\\geq1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let ${\\bar{K}}>K$ be natural number such that $f(x^{k})>f({\\bar{x}})$ for all $k\\ge\\bar{K}$ . Define $\\Delta_{k}:=\\varphi(f(x^{k})\\,-$ $f(\\bar{x})+u_{k})$ for all $k\\ge\\bar{K}$ , and let $R>0$ be such that $\\mathbb{B}(\\bar{x},R)\\subset U$ . Taking the number $C$ from Assumption 2.3, remembering that $\\textstyle{\\bar{x}}$ is an accumulation point of $\\{x^{k}\\}$ , and using $f(x^{k})+u_{k}\\downarrow f({\\bar{x}})$ $\\Delta_{k}\\downarrow0$ as $k\\rightarrow\\infty$ together with condition (9), we get by choosing a larger $\\bar{K}$ that $f(x^{\\bar{K}})+u_{\\bar{K}}<$ $f(\\bar{x})+\\eta$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|x^{\\bar{K}}-\\bar{x}\\right\\|+\\frac{1}{C c_{1}}\\Delta_{\\bar{K}}+\\sum_{k=\\bar{K}}^{\\infty}t_{k}\\varphi^{\\prime}\\left(\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}\\right)^{-1}+\\sum_{k=\\bar{K}}^{\\infty}t_{k}\\varepsilon_{k}<R.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us now show by induction that $x^{k}\\,\\in\\,\\mathbb{B}(\\bar{x},R)$ for all $k\\,\\geq\\,{\\bar{K}}$ . The assertion obviously holds for $k={\\bar{K}}$ due to (32). Take some ${\\widehat{K}}\\geq{\\bar{K}}$ and suppose that $x^{k}\\in U$ for all ${\\boldsymbol{k}}={\\boldsymbol{\\bar{K}}},\\ldots,{\\boldsymbol{\\widehat{K}}}$ We intend to show that $x^{\\hat{K}+1}\\in\\mathbb{B}(\\bar{x},R)$ as well. To proceed, fi some $k\\in\\left\\{\\bar{K},\\ldots,\\widehat{K}\\right\\}$ and get by $f({\\bar{x}})<f(x^{k})<f(x^{k})+u_{k}<f({\\bar{x}})+\\eta$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varphi^{\\prime}(f(x^{k})-f({\\bar{x}}))\\left\\|\\nabla f(x^{k})\\right\\|\\geq1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining this with $u_{k}>0$ and $f(x^{k})-f({\\bar{x}})>0$ gives us ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k}-\\Delta_{k+1}\\ge\\varphi^{\\prime}(f(x^{k})-f(\\bar{x})+u_{k})(f(x^{k})+u_{k}-f(x^{k+1})-u_{k+1})}\\\\ &{\\qquad\\qquad\\ge\\varphi^{\\prime}(f(x^{k})-f(\\bar{x})+u_{k})c_{1}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\ge\\frac{C}{\\left(\\varphi^{\\prime}(f(x^{k})-f(\\bar{x}))\\right)^{-1}+\\left(\\varphi^{\\prime}(u_{k})\\right)^{-1}}c_{1}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\ge\\frac{C}{\\left\\|\\nabla f(x^{k})\\right\\|+\\left(\\varphi^{\\prime}(u_{k})\\right)^{-1}}c_{1}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (34a) follows from the concavity of $\\varphi$ , (34b) follows from (30), (34c) follows from Assumption 2.3, and (34d) follows from (33). Taking the square root of both sides in (34d) and employing the AM-GM inequality yield ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle_{k}\\left\\|\\nabla f(x^{k})\\right\\|=\\sqrt{t_{k}}\\cdot\\sqrt{t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}}\\leq\\sqrt{\\frac{1}{C C_{1}}(\\Delta_{k}-\\Delta_{k+1})t_{k}(\\|\\nabla f(x^{k})\\|+(\\varphi^{\\prime}(u_{k}))^{-1})}}&{}\\\\ {\\displaystyle\\leq\\frac{1}{2C c_{1}}(\\Delta_{k}-\\Delta_{k+1})+\\frac{1}{2}t_{k}\\left((\\varphi^{\\prime}(u_{k}))^{-1}+\\|\\nabla f(x^{k})\\|\\right).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the nonincreasing property of $\\varphi^{\\prime}$ due to the concavity of $\\varphi$ and the choice of $c_{2}\\in(0,1)$ ensures that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\varphi^{\\prime}(u_{k})\\right)^{-1}=\\left(\\varphi^{\\prime}(c_{2}\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i})\\right)^{-1}\\leq\\left(\\varphi^{\\prime}(\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i})\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Rearranging terms and taking the sum over ${\\boldsymbol{k}}={\\boldsymbol{\\bar{K}}},\\ldots,{\\boldsymbol{\\widehat{K}}}$ of inequality (35) gives us ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=\\bar{K}}^{\\hat{K}}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|\\leq\\frac{1}{C c_{1}}\\sum_{k=\\bar{K}}^{\\hat{K}}(\\Delta_{k}-\\Delta_{k+1})+\\sum_{k=\\bar{K}}^{\\hat{K}}t_{k}\\varphi^{\\prime}(u_{k})^{-1}}}\\\\ &{=\\frac{1}{C c_{1}}(\\Delta_{\\bar{K}}-\\Delta_{\\bar{K}})+\\displaystyle\\sum_{k=\\bar{K}}^{\\hat{K}}t_{k}\\varphi^{\\prime}\\left(c_{2}\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}\\right)^{-1}}\\\\ &{\\leq\\displaystyle\\frac{1}{C c_{1}}\\Delta_{\\bar{K}}+\\sum_{k=\\bar{K}}^{\\hat{K}}t_{k}\\varphi^{\\prime}\\left(\\displaystyle\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The latter estimate together with the triangle inequality and (32) tells us that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|x^{\\hat{K}+1}-\\bar{x}\\right\\|=\\left\\|x^{\\hat{K}}-\\bar{x}\\right\\|+\\displaystyle\\sum_{k=K}^{\\hat{K}}\\|x^{k+1}-x^{k}\\|}\\\\ {\\displaystyle=\\left\\|x^{\\hat{K}}-\\bar{x}\\right\\|+\\displaystyle\\sum_{k=K}^{\\hat{K}}t_{k}\\|\\hat{\\mathcal x}^{k}\\|}\\\\ {\\displaystyle\\leq\\left\\|x^{\\hat{K}}-\\bar{x}\\right\\|+\\displaystyle\\sum_{k=K}^{\\hat{K}}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|+\\displaystyle\\sum_{k=K}^{\\hat{K}}t_{k}\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|}\\\\ {\\displaystyle\\leq\\left\\|x^{\\hat{K}}-\\bar{x}\\right\\|+\\displaystyle\\sum_{k=K}^{\\hat{K}}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|+\\displaystyle\\sum_{k=K}^{\\hat{K}}t_{k}\\varepsilon_{k}}\\\\ {\\displaystyle\\leq\\left\\|x^{\\hat{K}}-\\bar{x}\\right\\|+\\displaystyle\\frac{1}{C\\hat{C}_{1}}\\Delta\\varepsilon+\\displaystyle\\sum_{k=K}^{\\hat{K}}t_{k}\\varepsilon^{\\prime}\\left(\\displaystyle\\sum_{k=K}^{\\hat{K}}t_{k}\\varepsilon_{k}\\right)^{-1}+\\displaystyle\\sum_{k=K}^{\\infty}t_{k}\\varepsilon_{k}<R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By induction, this means that $x^{k}\\in\\mathbb{B}(\\bar{x},R)$ for all $k\\geq\\bar{K}$ . Then a similar device brings us to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=\\bar{K}}^{\\hat{K}}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|\\leq\\frac{1}{C c_{1}}\\Delta_{\\bar{K}}+\\sum_{k=\\bar{K}}^{\\infty}t_{k}\\varphi^{\\prime}\\left(\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}\\right)^{-1}\\;\\mathrm{for}\\;\\mathrm{all}\\;\\widehat{K}\\geq\\bar{K},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which yields $\\begin{array}{r}{\\sum_{k=1}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|<\\infty}\\end{array}$ Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k=1}^{\\infty}\\left\\|x^{k+1}-x^{k}\\right\\|=\\displaystyle\\sum_{k=1}^{\\infty}t_{k}\\left\\|g^{k}\\right\\|\\leq\\displaystyle\\sum_{k=1}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|+\\displaystyle\\sum_{k=1}^{\\infty}t_{k}\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|}\\\\ {\\displaystyle\\leq\\displaystyle\\sum_{k=1}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|+\\displaystyle\\sum_{k=1}^{\\infty}t_{k}\\varepsilon_{k}<\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which justifies the convergence of $\\{x^{k}\\}$ and thus completes the proof of the theorem. ", "page_idx": 20}, {"type": "text", "text": "C.3Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. Using $\\left\\|\\nabla f(x^{k})-g^{k}\\right\\|\\leq\\nu\\left\\|\\nabla f(x^{k})\\right\\|$ gives us the estimates ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert g^{k}\\right\\rVert^{2}=\\left\\lVert\\nabla f(x^{k})-g^{k}\\right\\rVert^{2}-\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}+2\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle}&{}\\\\ {\\leq\\nu^{2}\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}-\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}+2\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle}&{}\\\\ {=-(1-\\nu^{2})\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}+2\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle,}&{}\\\\ {\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle=\\left\\langle\\nabla f(x^{k}),g^{k}-\\nabla f(x^{k})\\right\\rangle+\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}}&{}\\\\ {\\leq\\left\\lVert\\nabla f(x^{k})\\right\\rVert\\cdot\\left\\lVert g^{k}-\\nabla f(x^{k})\\right\\rVert+\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}}&{}\\\\ {\\leq(1+\\nu)\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2},}&{}\\\\ {-\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle=-\\left\\langle\\nabla f(x^{k}),g^{k}-\\nabla f(x^{k})\\right\\rangle-\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}}&{}\\\\ {\\leq\\left\\lVert\\nabla f(x^{k})\\right\\rVert\\cdot\\left\\lVert g^{k}-\\nabla f(x^{k})\\right\\rVert-\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2}}&{}\\\\ {\\leq-(1-\\nu)\\left\\lVert\\nabla f(x^{k})\\right\\rVert^{2},}&{}\\\\ {\\left\\lVert\\nabla f(x^{k})\\right\\rVert-\\left\\lVert g^{k}-\\nabla f(x^{k})\\right\\rVert\\leq\\left\\lVert\\nabla f(x^{k})\\right\\rVert,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which in turn imply that ", "page_idx": 21}, {"type": "equation", "text": "$$\n(1-\\nu)\\left\\|\\nabla f(x^{k})\\right\\|\\leq\\left\\|g^{k}\\right\\|\\leq(1+\\nu)\\left\\|\\nabla f(x^{k})\\right\\|{\\mathrm{~for~all~}}k\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using condition (12), we find $N\\in\\mathbb{N}$ so that $2-2\\nu-L t_{k}(1+\\nu)^{2}\\geq\\delta$ for all $k\\geq N$ . Select such a natural number $k$ and use the Lipschitz continuity of $\\nabla f$ with constant $L$ to deduce from the descent condition (4),the relationship $x^{\\mathbf{\\dot{k}}+1}=x^{k}-t_{k}g^{\\mathbf{\\check{k}}}$ , and the estimates (36)-(38) that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(x^{k+1})\\leq f(x^{k})+\\left\\langle\\nabla f(x^{k}),x^{k+1}-x^{k}\\right\\rangle+\\displaystyle\\frac{L}{2}\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}}&{}\\\\ {=f(x^{k})-t_{k}\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle+\\displaystyle\\frac{L t_{k}^{2}}{2}\\left\\Vert g^{k}\\right\\Vert^{2}}&{}\\\\ {\\leq f(x^{k})-t_{k}\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle+L t_{k}^{2}\\left\\langle\\nabla f(x^{k}),g^{k}\\right\\rangle-\\displaystyle\\frac{L t_{k}^{2}(1-\\nu^{2})}{2}\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}}&{}\\\\ {\\leq f(x^{k})-t_{k}(1-\\nu)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+L t_{k}^{2}(1+\\nu)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}-\\displaystyle\\frac{L t_{k}^{2}(1-\\nu^{2})}{2}\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}}&{}\\\\ {=f(x^{k})-\\displaystyle\\frac{t_{k}}{2}\\left(2-2\\nu-L t_{k}(1+\\nu)^{2}\\right)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}}&{}\\\\ {\\leq f(x^{k})-\\displaystyle\\frac{\\delta t_{k}}{2}\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\mathrm{~for~all~}k\\geq N.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It follows from the above that the sequence $\\left\\{f(x^{k})\\right\\}_{k\\geq N}$ is nonincreasing, and hence the condition $\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})>-\\infty$ ensures the convergence of $\\{f(x^{k})\\}$ . This allows us to deduce from (40) that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\delta}{2}\\sum_{k=N}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\leq\\sum_{K=N}^{\\infty}\\left(f(x^{k})-f(x^{k+1})\\right)\\leq f(x^{K})-\\operatorname*{inf}_{k\\in\\mathbb{N}}f(x^{k})<\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining the latter with (39) and $x^{k+1}=x^{k}-t_{k}g^{k}$ gives us ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\left\\|x^{k+1}-x^{k}\\right\\|\\cdot\\left\\|g^{k}\\right\\|=\\sum_{k=1}^{\\infty}t_{k}\\left\\|g^{k}\\right\\|^{2}\\leq(1+\\nu)^{2}\\sum_{k=1}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we are ready to verify all the assertions of the theorem. Let us start with (i) and show that O in an accumulation point of $\\left\\{g^{k}\\right\\}$ . Indeed, supposing the contrary gives us $\\varepsilon>0$ and $K\\in\\mathbb{N}$ such that $\\|g^{k}\\|\\geq\\varepsilon$ for all $k\\geq K$ , and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\infty>\\sum_{k=K}^{\\infty}t_{k}\\left\\|g^{k}\\right\\|^{2}\\geq\\sum_{k=K}^{\\infty}t_{k}=\\infty,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is a contradiction justifying that O is an accumulation point of $\\{g^{k}\\}$ . If $\\textstyle{\\bar{x}}$ is an accumulation point of $\\{x^{k}\\}$ , then by Lemma B.2 and (42), we find an infinite set $J\\subset N$ such that $x^{k}\\xrightarrow[]{J}\\bar{x}$ and $g^{k}\\xrightarrow[]{J}0$ The latter being combined with (39) gives us $\\nabla f(x^{k})\\xrightarrow{J}\\,0$ which yields the stationary condition $\\nabla f({\\bar{x}})=0$ ", "page_idx": 22}, {"type": "text", "text": "To verity (ii), let $\\textstyle{\\bar{x}}$ be an accumulation point of $\\{x^{k}\\}$ and find an infinite set $J\\subset\\mathbb{N}$ such that $x^{k}\\xrightarrow[]{J}\\bar{x}$ Combining thiswiththecontinityof $f$ and the fact that $\\{f(x^{k})\\}$ is convergent, we arrive at the equalities ", "page_idx": 22}, {"type": "equation", "text": "$$\nf({\\bar{x}})=\\operatorname*{lim}_{k\\in J}f(x^{k})=\\operatorname*{lim}_{k\\in\\mathbb{N}}f(x^{k}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which therefore justify assertion (ii) ", "page_idx": 22}, {"type": "text", "text": "To proceed with the proof of the next assertion (ii), assume that $\\nabla f$ is Lipschitz continuous with constant $L>0$ and employ Lemma B.1 with $\\alpha_{k}:=\\left\\|\\nabla f(x^{k})\\right\\|$ \uff0c $\\beta_{k}:=L t_{k}(1+\\nu)$ , and $\\gamma_{k}:=0$ for all $k\\in\\mathbb{N}$ to derive that $\\nabla f(x^{k})\\to0$ . Observe first that condition (a) of this lemma is satisfied due to the the estimates ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{k+1}-\\alpha_{k}=\\left\\|\\nabla f(x^{k+1})\\right\\|-\\left\\|\\nabla f(x^{k})\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})\\right\\|\\leq L\\left\\|x^{k+1}-x^{k}\\right\\|}\\\\ &{\\qquad\\qquad=L t_{k}\\left\\|g^{k}\\right\\|\\leq L t_{k}(1+\\nu)\\left\\|\\nabla f(x^{k})\\right\\|=\\beta_{k}\\alpha_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The conditions in (b) of the lemma are satisfied since $\\{t_{k}\\}$ is bounded, $\\textstyle\\sum_{k=1}^{\\infty}t_{k}\\,=\\,\\infty$ by (12), $\\gamma_{k}=0$ ,and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\beta_{k}\\alpha_{k}^{2}=L(1+\\nu)\\sum_{k=1}^{\\infty}t_{k}\\left\\|\\nabla f(x^{k})\\right\\|^{2}<\\infty,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality follows from (41). Thus applying Lemma B.1 gives us $\\nabla f(x^{k})\\to0$ as $k\\rightarrow\\infty$ To prove (iv), we verify the assumptions of Proposition B.3 for the sequences generated by Algorithm 2. It folows from (40) and $x^{\\hat{k}+1}=x^{k}-\\dot{t_{k}}g^{k}$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f({x}^{k+1})\\leq f({x}^{k})-\\displaystyle\\frac{\\delta t_{k}}{2(1+\\nu)}\\left\\|\\nabla f({x}^{k})\\right\\|\\cdot\\left\\|{g}^{k}\\right\\|}\\\\ {\\displaystyle=f({x}^{k})-\\frac{\\delta}{2(1+\\nu)}\\left\\|\\nabla f({x}^{k})\\right\\|\\cdot\\left\\|{x}^{k+1}-{x}^{k}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which justify $(\\mathrm{H}1)$ with $\\begin{array}{r}{\\sigma=\\frac{\\delta}{2(1+\\nu)}}\\end{array}$ . Regarding condition (H2), assume that $f(x^{k+1})\\,=\\,f(x^{k})$ and get by (40) that $\\nabla f(x^{k})=0$ , which implies by $\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|\\leq\\nu\\left\\|\\nabla f(x^{k})\\right\\|$ that $g^{k}=0$ Combining this with $x^{k+1}\\,=\\,x^{k}\\,-\\,t_{k}g^{k}$ gives us $x^{k+1}\\,=\\,x^{k}$ , which verifies $\\left(\\mathrm{H}2\\right)$ .Therefore, Proposition B.3 tells us that $\\{x^{k}\\}$ is convergent. ", "page_idx": 22}, {"type": "text", "text": "Let us now verify the final assertion (v) of the theorem. It is nothing to prove if $\\{x^{k}\\}$ stops at a stationary point after a finite number of iterations. Thus we assume that $\\nabla f(x^{k})\\neq0$ for all $k\\in\\mathbb{N}$ The assumptions in (v) give us $\\bar{t}>0$ and $N\\in\\mathbb{N}$ such that $t_{k}\\geq\\bar{t}$ for all $k\\geq N$ . Let us check that the assumptions of Proposition B.4 hold for the sequences generated by Algorithm 2 with $\\tau_{k}:=t_{k}$ and $d^{k}:=-g^{k}$ for all $k\\,\\in\\,\\mathbb{N}$ The iterative procedure $\\overline{{x^{k+1}}}\\,=\\,x^{k^{\\'}}-\\,t_{k}^{\\'}g^{k}$ can be rewritten as $x^{k+1}=x^{k}+\\'t_{k}d^{k}$ Using the frst condition in (39) and taking into account that $\\nabla f(x^{k})\\neq0$ for all $k\\in\\mathbb{N}$ , we get that $g^{k}\\neq0$ for all $k\\in\\mathbb{N}$ Combining this with $x^{k+1}=x^{k}-t_{k}g^{k}$ and $t_{k}\\geq\\bar{t}$ for all $k\\geq N$ tlls us that $x^{k+1}\\neq x^{k}$ for all $k\\geq N$ It follows from (40) and (39) that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(x^{k+1})\\leq f(x^{k})-{\\frac{\\delta t_{k}}{2(1+\\nu)^{2}}}\\left\\|g^{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This estimate together withthe second inequality in (39) verife 21) with \u03b2 = 2(1+r) , \u03b1 = As all the assumptions are verified, Proposition B.4 gives us the assertions: ", "page_idx": 22}, {"type": "text", "text": "\u00b7If $q\\in(0,1/2]$ , then the sequence $\\{x^{k}\\}$ converges linearly to $\\textstyle{\\bar{x}}$ ", "page_idx": 22}, {"type": "text", "text": "\u00b7If $q\\in(1/2,1)$ , then we have the estimate ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|{x}^{k}-{\\bar{x}}\\right\\|={\\mathcal{O}}\\left(k^{-{\\frac{1-q}{2q-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The convergence rates of $\\{f(x^{k})\\}$ and $\\{\\|\\nabla f(x^{k})\\|\\}$ follow now from Proposition B.5, and thus we are done. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.4 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Prof Let $\\{x^{k}\\}$ be the sequence generated by Algorithm 2a. Defining $g^{k}:=\\nabla f(x^{k}+\\rho_{k}\\nabla f(x^{k}))$ and utilizing $\\rho_{k}\\overset{\\cdot}{\\leq}\\frac{\\nu}{L}$ we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|=\\left\\|\\nabla f(x^{k}+\\rho_{k}\\nabla f(x^{k}))-\\nabla f(x^{k})\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq L\\left\\|\\rho_{k}\\nabla f(x^{k})\\right\\|\\leq\\nu\\left\\|\\nabla f(x^{k})\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which verifies the inexact condition in Step 2 of Algorithm 2. Therefore, all the convergence properties in Theorem 4.1 hold for Algorithm 2a. The proof for the convergence properties of Algorithm 2b can be conducted similarly. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.5 Proof of Corollary 3.5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Prof Considering Algorimlaandeng $\\begin{array}{r}{g^{k}=\\nabla f\\left(x^{k}+\\rho_{k}\\frac{d^{k}}{\\|d^{k}\\|}\\right)}\\end{array}$ we dedue that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|\\leq L\\left\\|x^{k}+\\rho_{k}{\\frac{d^{k}}{\\|d^{k}\\|}}-x^{k}\\right\\|=L\\rho_{k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, Algorithm la is a specialization of Algorithm 1 with $\\varepsilon_{k}=L\\rho_{k}$ Combining thiswith (11) also gives us (8), thereby verifying all the assumptions in Theorem 3.3. Consequently, all the convergence properties outlined in Theorem 3.3 hold for Algorithm 1a. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D Efficient normalized variants of SAM ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we list several efficient normalized variants of SAM from [Foret et al., 2021, Liu et al., 2022, Li and Giannakis, 2023, Li et al., 2024] that are special cases of Algorithm 1a. As a consequence, all the convergence properties in Theorem 3.3 are satisfied for these methods. ", "page_idx": 23}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/9219bedeab407249f11e9340644bcb1f30796cb387f26cc4e94402c5b9f45431.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Algorithm 2d [Liu et al., 2022] Random Sharpness-Aware Minimization (RSAM) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Step O. Choose $x^{1}\\in\\mathbb{R}^{n}$ \uff0c $\\{\\rho_{k}\\}\\subset[0,\\infty)$ , and $\\{t_{k}\\}\\subset[0,\\infty)$ . For $k=1,2,\\dots$ , do the following: Step 1. Construct a random vector $\\Delta^{k}\\in\\mathbb{R}^{n}$ and set $g^{k}=\\nabla f(x^{k}+\\Delta^{k})$ Step2. Set+1=-tf(x+pT\u25b3) ", "page_idx": 23}, {"type": "text", "text": "Remark D.1. It is clear that Algorithms 2c-2f are specializations of Algorithm la with $d^{k}=\\nabla f(x^{k})$ in Algorithm 2c, $d^{k}=\\Delta^{k}+\\lambda g^{k}$ in Algorithm 2d, and $d^{k}$ constructed inductively for Algorithm 2e and Algorithm 2f. ", "page_idx": 24}, {"type": "text", "text": "E  Numerical experiments on SAM constant and SAM almost constant ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we present numerical results to support our claim in Remark 3.6 that SAM with an almost constant perturbation radius $\\begin{array}{r}{\\rho_{k}\\,=\\,\\frac{C}{k^{p}}}\\end{array}$ for $p$ close to O, e.g, $p\\,=\\,0.001$ , generates similar results to_SAM with a constant perturbation radius $\\rho\\,=\\,C$ . To do so, we consider the function $\\begin{array}{r}{f(x)\\,=\\,\\sum_{i=1}^{n}\\log(1+(A x-b)_{i}^{\\!\\!2})}\\end{array}$ ,where $A$ is an $n\\times n$ matrix, and $b$ is a vector in $\\mathbb{R}^{n}$ . In the experiment, we construct $A$ and $b$ randomly with n E 2, 20, 50, 100. The methods considered in the experiment are GD with a diminishing step size, SAM with a diminishing step size and a constant perturbation radius of 0.1, and lastly, SAM with a diminishing step size and a variable radius $\\begin{array}{r}{\\rho_{k}=\\frac{C}{k^{p}}}\\end{array}$ for $p\\in{1,0.1,0.001}$ We refer to the case $p=0.001$ as the \"almost constant\" case, as $\\begin{array}{r}{\\rho_{k}=\\frac{C}{k^{p}}}\\end{array}$ is numerically similar to when we consider a small number of iterations. The diminishing step size is chosen as $t_{k}=(0.1/n)/k$ at the $k^{\\mathrm{th}}$ iteration, where $n$ is the dimension of the problem. To make the plots clearer, we choose the initial point $x^{1}$ near the solution, which is $x^{1}=\\dot{x}^{\\infty}+(0.1/n^{2})\\mathbf{1}_{n}$ where $x^{\\infty}$ is a solution of $A x\\,=\\,b$ , and ${\\mathbf{1}}_{n}$ is the all-ones vector in $\\mathbb{R}^{n}$ . All the algorithms are executed for $100n$ iterations. The results presented in Figure 3 show that SAM with a constant perturbation and SAM with an almost constant perturbation have the same behavior regardless of the dimension of the problem.This is simply because $\\frac{C}{k^{0.001}}$ is almost the same as $C$ This also tells us that the convergence rate of these two versions of SAM is similar. Since SAM with a constant perturbation radius is always preferable in practice [Foret et al., 2021, Dai et al., 2023], this highlights the practicality of our development. ", "page_idx": 24}, {"type": "image", "img_path": "PuXYI4HOQU/tmp/0750be4858932d12d0deaf3f41ec04bb287b44e154a527266ff62935662fb2a6.jpg", "img_caption": ["Figure 3: SAM with constant perturbation and SAM almost constant perturbation "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "F  Numerical experiments on SAM with SGD without momentum as base optimizer ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "CIFAR-10, CIFAR-100, and Tiny ImageNet. The training configurations for these datasets follow a similar structure to Section 5, excluding momentum, which we set to zero. The results in Table 5 report test accuracy on CIFAR-10 and CIFAR-100. Table 6 shows the performance of SAM on momentum and without momentum settings. Each experiment is run once, and the highest accuracy for each column is highlighted in bold. ", "page_idx": 26}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/a073cf7b0114240e73e8bd9023147e3e2704349bfbf66781ccd28abf90c88382.jpg", "table_caption": ["Table 5: Additional Numerical Results on CIFAR-10, CIFAR-100 for SAM without momentum "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "PuXYI4HOQU/tmp/5c26a162975c3221d42700644c4226f87c51b67c8fdc8604a666505416ad1733.jpg", "table_caption": ["Table 6: Additional Numerical Results on Tiny ImageNet [Le and Yang, 2015] for SAM with and without momentum "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G Additional Remarks ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Remark G.1. Assumption 2.3 is satisfied with constant $C=1$ for $\\varphi(t)=M t^{1-q}$ with $M>0$ and $q\\in[0,1)$ . Indeed, taking any $x,y>0$ with $x+y<\\eta$ ,wededucethat $(x+y)^{q}\\leq x^{q}+y^{q}$ , and hence ", "page_idx": 26}, {"type": "equation", "text": "$$\n[\\varphi^{\\prime}(x+y)]^{-1}={\\frac{1}{M(1-q)}}(x+y)^{q}\\leq{\\frac{1}{M(1-q)}}(x^{q}+y^{q})=(\\varphi^{\\prime}(x))^{-1}+(\\varphi^{\\prime}(y))^{-1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Remark G.2. Construct an example to demonstrate that the conditions in (11) do not require that $\\rho_{k}$ converges to 0. Let $L>0$ be a Lipschitz constant of $\\nabla f$ , let $C$ be a positive constant such that $C<2/L$ ,let $P\\subset\\mathbb{N}$ be the set of all perfect squares, lt $\\begin{array}{r}{t_{k}=\\frac{1}{k}}\\end{array}$ for all $\\bar{k}\\in\\mathbb{N},p>0$ and let $\\{\\rho_{k}\\}$ be constructed as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\rho_{k}=\\left\\{C,\\quad k\\in P,\\right.\\mathrm{~which~yields~}\\left.t_{k}\\rho_{k}=\\left\\{\\begin{array}{l l}{C}\\\\ {\\frac{C}{k^{p}},}&{k\\notin P,}\\end{array}\\right.\\mathrm{~which~}\\mathrm{~with~yields~}\\left.t_{k}\\rho_{k}\\right.=\\left\\{\\begin{array}{l l}{\\frac{C}{k},}&{k\\in P,}\\\\ {\\frac{C}{k^{p+1}},}&{k\\notin P.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It is clear from the construction of $\\{\\rho_{k}\\}$ that lim $\\mathrm{sup}_{k\\to\\infty}\\,\\rho_{k}=C>0$ , which implies that $\\{\\rho_{k}\\}$ does not convergence to 0. We also immediately deduce that $\\textstyle\\sum_{k=1}^{\\infty}t_{k}=\\infty,\\ t_{k}\\downarrow0$ and lim sup $\\rho_{k}=$ $\\begin{array}{r}{C<\\frac{2}{L}}\\end{array}$ , which verifies the first three conditions in (11). The last condition in (11) follows from the estimates ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k=1}^{\\infty}t_{k}\\rho_{k}=\\sum_{k\\in P}t_{k}\\rho_{k}+\\sum_{k\\notin P}t_{k}\\rho_{k}\\leq\\sum_{k\\in P}\\frac{C}{k}+\\sum_{k\\in\\mathbb{N}}\\frac{C}{k^{p+1}}}}\\\\ &{}&{=\\displaystyle\\sum_{k\\in\\mathbb{N}}\\frac{C}{k^{2}}+\\sum_{k\\in\\mathbb{N}}\\frac{C}{k^{p+1}}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Remark G.3 (on Assumption (9)). Supposing that $\\varphi(t)=M t^{1-q}$ with $M>0,q\\in(0,1)$ and letting $C=1/(M(1-q))$ , we get that $(\\varphi^{\\prime}(\\dot{t}))^{-1}\\stackrel{\\textstyle-}{=}C t^{\\dot{q}}$ for $t>0$ is an inereaing function If $\\begin{array}{r}{t_{k}:=\\frac{\\intercal}{k}}\\end{array}$ and $\\begin{array}{r}{\\varepsilon_{k}:=\\frac{1}{k^{p}}}\\end{array}$ with $p>0$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}=\\sum_{i=k}^{\\infty}{\\frac{1}{i^{1+p}}}\\leq\\int_{k}^{\\infty}{\\frac{1}{x^{1+p}}}d x=-{\\frac{1}{p x^{p}}}{\\big|}_{k}^{\\infty}={\\frac{1}{p k^{p}}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which yields the relationships ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\varphi^{\\prime}\\left(\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}\\right)\\right)^{-1}\\leq\\left(\\varphi^{\\prime}\\left({\\frac{1}{p(k+1)^{p}}}\\right)\\right)^{-1}={\\frac{C}{p^{q}k^{p q}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we arrive at the claimed conditions ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}t_{k}\\left(\\varphi^{\\prime}\\left(\\sum_{i=k}^{\\infty}t_{i}\\varepsilon_{i}\\right)\\right)^{-1}\\leq\\sum_{k=1}^{\\infty}\\frac{1}{k}\\frac{C}{p^{q}k^{p q}}=\\sum_{k=1}^{\\infty}\\frac{C}{p^{q}k^{1+p q}}<\\infty.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Remark G.4. Let us finally compare the results presented in Theorem 4.1 with that in Andriushchenko and Flammarion [2022]. All the convergence properties in Andriushchenko and Flammarion [2022] are considered for the class of $\\mathcal{C}^{1,L}$ functions, which is more narrow than the class of $L$ -descent functions examined in Theorem 4.1(i). Under the convexity of the objective function, the convergence of the sequences of the function values at averages of iteration is established in [Andriushchenko and Flammarion, 2022, Theorem 11], which does not yield the convergence of either the function values, or the iterates, or the corresponding gradients. In the nonconvex case, we derive the stationarity of accumulation points, the convergence of the function value sequence, and the convergence of the gradient sequence in Theorem 4.1. Under the strong convexity of the objective function, the linear convergence of the sequence of iterate values is established Andriushchenko and Flammarion [2022, Theorem 11]. On the other hand, our Theorem 4.1 derives the convergence rates for the sequence of iterates, sequence of function values, and sequence of gradient under the KL property only, which covers many classes of nonconvex functions. Our convergence results address variable stepsizes and bounded radi, which also cover the case of constant stepsize and constant radi considered in Andriushchenko and Flammarion [2022]. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction about convergence properties of SAM and its variants are presented in Section 3 and Section 4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the assumptions are given in the main text while the full proofs are provided in the appendices. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Section 5 ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 30}, {"type": "text", "text": "Justification: See the supplementary material. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See Section 5 Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See Section 5 Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of themean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We mentioned an RTX 3090 computer worker. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Our paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which are specifically highlighted here. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper is a theoretical study. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA]   \nJustification: Our paper is a theoretical study. Guidelines:   \n\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]