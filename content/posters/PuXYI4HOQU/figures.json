[{"figure_path": "PuXYI4HOQU/figures/figures_5_1.jpg", "caption": "Figure 1: SAM with constant stepsize does not converge to optimal solution", "description": "This figure shows a comparison of the convergence behavior of Gradient Descent (GD) and Sharpness-Aware Minimization (SAM) with a constant stepsize when applied to a strongly convex quadratic function.  The x-axis represents the number of iterations, and the y-axis represents the function value.  The plot demonstrates that while GD converges smoothly to the optimal solution, SAM with a constant stepsize fails to converge and oscillates around a suboptimal point. This highlights the need for alternative stepsize rules, such as diminishing stepsizes, in SAM to ensure convergence.", "section": "3 SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/figures/figures_8_1.jpg", "caption": "Figure 2: Training loss on CIFAR-10 (first two graphs) and CIFAR-100 (last two graphs)", "description": "This figure displays the training loss curves for four different deep neural network models (ResNet18, ResNet34, WideResNet28-10) trained on the CIFAR-10 and CIFAR-100 datasets using SAM with various stepsize strategies.  The x-axis represents the training epoch, and the y-axis represents the training loss. Each line corresponds to a different stepsize schedule: 'Constant', 'Diminish 1', 'Diminish 2', and 'Diminish 3'. The plot visualizes how different stepsize strategies affect the convergence behavior of the training loss during the training process of deep neural networks.", "section": "Numerical Experiments"}, {"figure_path": "PuXYI4HOQU/figures/figures_25_1.jpg", "caption": "Figure 3: SAM with constant perturbation and SAM almost constant perturbation", "description": "This figure presents the results of numerical experiments comparing the performance of SAM with a constant perturbation radius and SAM with an almost constant perturbation radius.  The experiments were conducted on the function f(x) = \u2211i=1log(1 + (Ax \u2013 b)\u00b2) with different dimensions (n = 2, 20, 50, 100).  The results show that SAM with an almost constant perturbation radius (pk = C/kp) performs similarly to SAM with a constant perturbation radius (p = C), supporting the claim in Remark 3.6 that these two approaches have similar convergence behavior. The plots show the function value over iterations for both approaches, along with Gradient Descent (GD) as a comparison.", "section": "E Numerical experiments on SAM constant and SAM almost constant"}]