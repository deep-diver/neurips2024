[{"type": "text", "text": "Contextual Active Model Selection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuefeng Liu1\u2217, Fangfang $\\mathbf{Xia}^{2}$ , Rick L. Stevens1,2, Yuxin Chen1 1Department of Computer Science, University of Chicago 2Argonne National Laboratory ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While training models and labeling data are resource-intensive, a wealth of pretrained models and unlabeled data exists. To effectively utilize these resources, we present an approach to actively select pre-trained models while minimizing labeling costs. We frame this as an online contextual active model selection problem: At each round, the learner receives an unlabeled data point as a context. The objective is to adaptively select the best model to make a prediction while limiting label requests. To tackle this problem, we propose CAMS, a contextual active model selection algorithm that relies on two novel components: (1) a contextual model selection mechanism, which leverages context information to make informed decisions about which model is likely to perform best for a given context, and (2) an active query component, which strategically chooses when to request labels for data points, minimizing the overall labeling cost. We provide rigorous theoretical analysis for the regret and query complexity under both adversarial and stochastic settings. Furthermore, we demonstrate the effectiveness of our algorithm on a diverse collection of benchmark classification tasks. Notably, CAMS requires substantially less labeling effort (less than $10\\%$ ) compared to existing methods on CIFAR10 and DRIFT benchmarks, while achieving similar or better accuracy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As pre-trained models become increasingly prevalent in a variety of real-world machine learning applications [2, 11, 53], there is a growing demand for label-efficient approaches for model selection, especially when facing varying data distributions and contexts at run time. Oftentimes, no single pre-trained model achieves the best performance for every context, and a proper approach is to construct a policy for adaptively selecting models for specific contexts [48]. For instance, in medical diagnosis and drug discovery, accurate predictions are of paramount importance. The diagnosis of diseases through pathologist or the determination of compound chemical properties through lab testing can be costly and time-consuming. Different models may excel in analyzing different types of pathological images [1, 3, 23] or chemical compounds [17, 32, 46]. Furthermore, in many real-world applications, the collection of labels for model evaluation can be expensive and data instances may arrive as a stream rather than all at once. This scenario necessitates cost-effective and robust online algorithms capable of determining the most efficient model selection policy even when faced with a limited supply of labels, a scenario not fully addressed by previous works that typically assume access to all labels [6, 7, 27, 54]. ", "page_idx": 0}, {"type": "text", "text": "Recently, the problem of online model selection with the consideration of label acquisition costs was studied in a context-free setting by Karimi et al. [39]. However, this approach doesn\u2019t fully capture the dynamics of data contexts that are essential in many applications. Recognizing this gap, in this paper, we consider a more general problem setting that incorporates context information for adaptive model selection. We introduce CAMS, an algorithm for active model selection that dynamically adapts to the data context to choose the most suitable models for an arbitrary data stream. As highlighted in ", "page_idx": 0}, {"type": "table", "img_path": "ZizwgYErtQ/tmp/bd244fe5f9427919a10dd9bc5984891888f9aa1768144f1b8f2bc3a5fbc6f4dc.jpg", "table_caption": ["Table 1: Comparing CAMS against related work in terms of problem setup. "], "table_footnote": ["We regard \u201carms\u201d as \u201cmodels\u201d when comparing CAMS against bandit algorithms, such as EXP3/EXP4. Online ensemble learning aims to build a composite model by aggregating multiple models rather than selecting the best model (for a given context). "], "page_idx": 1}, {"type": "text", "text": "Table 1, CAMS aims to address the need for adaptive and effective model selection, by bridging the gap between contextual bandits, online learning, and active learning. ", "page_idx": 1}, {"type": "text", "text": "Our key contributions are summarized as follows:   \n\u2022 We investigate a novel problem which we refer to as contextual active model selection, and introduce a novel principled algorithm that features two key technical components: (1) a contextual online model selection procedure, designed to handle both stochastic and adversarial settings, and (2) an active query strategy. The proposed algorithm is designed to be robust to heterogeneous data streams, accommodating both stochastic and adversarial online data streaming scenarios.   \n\u2022 We provide rigorous theoretical analysis on the regret and query complexity of the proposed algorithms. We establish regret upper bounds for both adversarial and stochastic data streams under limited label costs. Our regret upper bounds are within constant factors of the existing lower bounds for online learning problems with expert advice under the full information setting.   \n\u2022 Empirically, we demonstrate the effectiveness and robustness of our approach on a variety of online model selection tasks spanning different application domains (from generic ML benchmarks such as CIFAR10 to domain-specific tasks in biomedical analysis), data scales (ranging from 80 to 10K), data modalities (i.e., tabular, image, and graph-based data), and label types (binary or multiclass labels). For the tasks evaluated, (1) CAMS outperforms all competing baselines by a significant margin. (2) Asymptotically, CAMS performs no worse than the best single model. (3) CAMS is not only robust to adversarial data streams but also can efficiently recover from \u201cmalicious experts\u201d (i.e. inferior pre-trained models). ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Contextual bandits. Classical bandit algorithms (e.g., [6, 7]) aim to find the best arm(s) through a sequence of actions. When side information (e.g., user profile for recommender systems or environmental context for experimental design) is available, many bandit algorithms can be lifted to the contextual setting: For example, EXP4 [7, 9, 52] considers the bandit setting with expert advice: At each round, experts announce their predictions of which actions are the most promising for the given context, and the goal is to construct a expect selection policy that competes with the best expert from hindsight. In bandit problems, the learner only gets to observe the reward for each action taken. In contrast, for the online model selection problem considered in this work\u2014where an action corresponds to choosing a model to make prediction on an incoming data point\u2014we get to see the loss/reward of all models on the labeled data point. By utilizing the information from unchosen arms, it could significantly reduce the cumulative regret. In this regard, this work aligns more closely with online learning with full information setting, where the learner has access to the loss of all the arms at each round (e.g. as considered in the Hedge algorithm [13, 14, 27, 36]). ", "page_idx": 1}, {"type": "text", "text": "Online learning with full information. A clear distinction between our work and online learning is that we assume the labels of the online data stream are not readily available but can be acquired at each round with a cost. In addition, the learner only observes the loss incurred by all models on a data point when it decides to query its label. In contrast, in the canonical online learning setting, labels arrive with the data and one gets to observe the loss of all candidate models at each round. Similar setting also applies to other online learning problems, such as online boosting or bagging. A related work to ours is online learning with label-efficient prediction [16], which proposes an online learning algorithm with matching upper and lower bounds on the regret. However, they consider a fixed query probability that leads to a linear query complexity. Our algorithm, inspired by uncertainty sampling in active learning, achieves an improved query complexity with the adaptive query strategy while maintaining a comparable regret. ", "page_idx": 1}, {"type": "text", "text": "Stream-based Active learning. Active learning aims to achieve a target learning performance with fewer training examples [64]. The active learning framework closest to our setting is queryby-committee (QBC) [65], in particular under the stream-based setting [35, 47]. QBC maintains a committee of hypotheses; each committee member votes on the label of an instance, and the instances with the maximal disagreement among the committee are considered the most informative labels. Note that existing stream-based QBC algorithms are designed and analyzed assuming i.i.d. data streams. In comparison, our work uses a different query strategy as well as a novel model recommendation strategy, which also applies to the adversarial setting. ", "page_idx": 2}, {"type": "text", "text": "Active model selection. Active model selection captures a broad class of problems where model evaluations are expensive, either due to (1) the cost of evaluating (or \u201cprobing\u201d) a model, or (2) the cost of annotating a training example. Existing works under the former setting [49, 59] and online setting [21, 66] often ignore context information and data annotation cost, and only consider partial feedback on the models being evaluated/ probed on i.i.d. data. The goal is to identify the best model with as few model probes as possible. This is quite different from our problem setting which considers the full information setting as well as non-negligible data annotation cost. [71] proposes that the optimal model choice is influenced by the sample size rather than any individual sample feature. [44] addresses the active model selection problem, however both works do not adopt a stream-based approach. For the later, apart from Karimi et al. [39], an online contextual-free model selection work, as shown in Table 1, most existing works assume a pool-based setting where the learner can choose among the pool of unlabeled data [4, 29, 43, 49, 60, 61, 68, 76], and the goal is to identify the best model with a minimal set of labels. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Let $\\mathcal{X}$ be the input domain and $\\mathcal{V}:=\\{0,\\ldots,c-1\\}$ be the set of $c$ possible class labels for each input instance. Let $\\mathcal{F}=\\{f_{1},\\ldots,f_{k}\\}$ be a set of $k$ pre-trained classifiers over $\\mathcal X\\times\\mathcal X$ . A model selection policy $\\pi:\\mathcal{X}\\to\\Delta^{k-1}$ maps any input instance $\\pmb{x}\\in\\mathcal{X}$ to a distribution over the pre-trained classifiers $\\mathcal{F}$ , specifying the probability $\\pi\\left(x\\right)$ of selecting each classifier under input $\\pmb{x}$ . Here, $\\Delta^{k-1}$ denotes the $k$ -dimensional probability simplex $\\left\\{w\\in\\bar{\\mathbb{R}^{k}}:|w|=1,w\\geq0\\right\\}$ . One can interpret a policy $\\pi$ as an \u201cexpert\u201d that suggests which model to select for a given context $\\pmb{x}$ . ", "page_idx": 2}, {"type": "text", "text": "Let $\\Pi$ be a collection of model selection policies. In this paper, we propose an extended policy set $\\Pi^{*}:=\\Pi\\cup\\{\\pi_{1}^{\\mathrm{const}},\\dots,\\pi_{k}^{\\mathrm{const}}\\}$ which also includes constant policies that always suggest a fixed model. Here, $\\pi_{j}^{\\mathrm{const}}(\\cdot):=e_{j}$ , and $\\pmb{e}_{j}\\in\\Delta^{k-1}$ denotes the canonical basis vector with $e_{j}=1$ . Unless otherwise specified, we assume $\\Pi$ is finite with $|\\Pi|=n$ , and $|\\Pi^{*}|\\leq n+k$ . As a special case, when $\\Pi=\\emptyset$ , our problem reduces to the contextual-free setting. ", "page_idx": 2}, {"type": "text", "text": "The contextual active model selection protocol. Assume that the learner knows the set of classifiers $\\mathcal{F}$ as well as the set of model selection policies $\\Pi$ . At round $t$ , the learner receives a data instance $\\pmb{x}_{t}\\in\\mathcal{X}$ as the context for the current round, and computes the predicted label $\\hat{y}_{t,j}=f_{j}\\left(\\pmb{x}_{t}\\right)$ for each pre-trained classifier indexed by $j\\in[k]$ . Denote the vector of predicted labels by all $k$ models by $\\hat{\\mathbf{y}}_{t}:=\\left[\\hat{y}_{t,1},\\ldots,\\hat{y}_{t,k}\\right]^{\\top}$ . Based on previous observations, the learner identifies a model/classifier $f_{j_{t}}$ and makes a prediction $\\hat{y}_{t,j_{t}}$ for the instance $\\pmb{x}_{t}$ . Meanwhile, the learner can obtain the true label $y_{t}$ only $i f$ it decides to query $\\pmb{x}_{t}$ . Upon observing $y_{t}$ , the learner incurs a query cost, and receives a (full) loss vector $\\pmb{\\ell}_{t}=\\mathbb{I}_{\\{\\hat{\\pmb{y}}_{t}\\neq y_{t}\\}}$ , where the $j$ th entry $\\ell_{t,j}:=\\mathbb{I}_{\\{\\hat{y}_{t,j}\\neq y_{t}\\}}$ corresponds to the 0-1 loss for model $j\\in[k]$ at round $t$ . The learner can then use the queried labels to adjust its model selection criterion for future rounds. ", "page_idx": 2}, {"type": "text", "text": "Performance metric. If $\\pmb{x}_{t}$ is misclassified by the model $j_{t}$ selected by learner at round $t$ , i.e. $\\hat{y}_{t,j_{t}}\\neq y_{t}$ , it will be counted towards the cumulative loss of the learner, regardless of the learner making a query. Otherwise, no loss will be incurred for that round. For a learning algorithm $\\boldsymbol{\\mathcal{A}}$ , its cumulative loss over $T$ rounds is defined as $\\begin{array}{r}{L_{T}^{A}:=\\sum_{t=1}^{T}\\ell_{t,j t}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "In practice, the choice of model $j_{t}$ at round $t$ by the learner $\\boldsymbol{\\mathcal{A}}$ could be random: For stochastic data streams where $(\\pmb{x},y)$ arrives i.i.d., the learner may choose different models for different random realizations of $\\left({\\pmb x}_{t},y_{t}\\right)$ . For the adversarial setting where the data stream $\\{(\\pmb{x}_{t},y_{t})\\}_{t\\ge1}$ is chosen by an adversary before each round, the learner may randomize its choice of model to avoid a constant loss at each round [33]. Therefore, due to the randomness of $L_{T}^{A}$ , we consider the expected cumulative loss $\\mathbb{E}[L_{T}^{A}]$ as a key performance measure of the learner $\\boldsymbol{\\mathcal{A}}$ . To characterize the progress of $\\boldsymbol{\\mathcal{A}}$ , we consider the regret\u2014formally defined as follows\u2014 as the difference between the cumulative loss received by the learner and the loss if the learner selects the \u201cbest policy\u201d $\\pi^{*}\\in\\Pi^{*}$ in hindsight. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "For stochastic data streams, we assume that each policy $i$ recommends the most probable model w.r.t. $\\pi_{i}(\\pmb{x}_{t})$ for context $\\pmb{x}_{t}$ . We use maxind $\\pmb{\\langle w\\rangle}:=\\arg\\operatorname*{max}_{j:w_{j}\\in w}w_{j}$ to denote the index of the maximalvalue entry2 of ${\\pmb w}$ . Since $(\\pmb{x},y)$ are drawn i.i.d., we define $\\begin{array}{r}{\\underline{{\\mu_{i}}}=\\frac{1}{T}\\sum_{t=\\underline{{1}}}^{T}\\mathbb{E}_{\\pmb{x}_{t},\\underline{{y}}_{t}}\\left[\\ell_{t,\\mathrm{maxind}\\left(\\pi_{i}\\left(\\pmb{x}_{t}\\right)\\right)}\\right]}\\end{array}$ . This leads to the pseudo-regret for the stochastic setting over $T$ rounds, defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\mathcal{R}}}_{T}\\left(A\\right)=\\mathbb{E}[L_{T}^{A}]-T\\operatorname*{min}_{i\\in\\left[\\left|\\Pi^{*}\\right|\\right]}\\mu_{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In an adversarial setting, since the data stream (and hence the loss vector) is determined by an adversary, we consider the reference best policy to be the one that minimizes the loss on the adversarial data stream, and the expected regret ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}\\left(A\\right)=\\mathbb{E}[L_{T}^{A}]-\\operatorname*{min}_{i\\in\\left[\\left|\\Pi^{*}\\right|\\right]}\\sum_{t=1}^{T}\\tilde{\\ell}_{t,i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{\\ell}_{t,i}:=\\left\\langle\\pi_{i}\\left(\\pmb{x}_{t}\\right),\\pmb{\\ell}_{t}\\right\\rangle$ denotes the expected loss if the learner commits to policy $\\pi_{i}$ , randomizes and selects $j_{t}\\sim\\pi_{i}\\left(\\pmb{x}_{t}\\right)$ (and receives loss $\\ell_{t,j_{t}}$ ) at round $t$ . Our goal is to devise a principled online active model selection strategy to minimize the regret as defined in (1) or (2), while maintaining a low total query cost. For convenience, we refer the readers to App. B for a summary of the notations used in this paper. ", "page_idx": 3}, {"type": "text", "text": "4 Contextual Active Model Selection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce our main algorithm for both stochastic and adversarial data streams. ", "page_idx": 3}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/98601e22be5c1198fb7c2ece679c7d54bd6947523e7bc170709a11c7930a9f51.jpg", "img_caption": ["Figure 1: The Contextual Active Model Selection (CAMS) algorithm "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Contextual model selection. Our key insight underlying the contextual model selection strategy extends from the online learning with expert advice framework [13, 27]. We start by appending the constant policies that always pick single pre-trained models to form the extended policy set $\\Pi^{*}$ (Line 3, in Fig. 1). This allows CAMS to be at least as competitive as the best model. Then, at each round, CAMS maintains a probability distribution over the (extended) policy set $\\Pi^{*}$ , and updates those according to the observed loss for each policy. We use $\\pmb{q}_{t}:=(q_{t,i})_{i\\in|\\Pi^{*}|}$ to denote the probability distribution over $\\Pi^{*}$ at $t$ . Specifically, the probability $q_{t,i}$ is computed based on the exponentially weighted cumulative loss, i.e. $q_{t,i}\\propto\\exp\\left(-\\eta_{t}\\tilde{L}_{t-1,i}\\right)$ where $\\begin{array}{r}{\\tilde{L}_{t,i}:=\\sum_{\\tau=1}^{t}\\tilde{\\ell}_{\\tau,i}}\\end{array}$ denotes the cumulative loss of policy $i$ . ", "page_idx": 3}, {"type": "text", "text": "For adversarial data streams, it is natural for both the online learner and the model selection policies to randomize their actions to avoid linear regret [33]. Following this insight, CAMS randomly samples a policy $i_{t}\\sim\\pmb q_{t}$ , and\u2014based on the current context $\\pmb{x}_{t}$ \u2014samples a classifier $j_{t}\\sim\\pi_{i_{t}}\\left(\\pmb{x}_{t}\\right)$ to recommend at round $t$ . ", "page_idx": 4}, {"type": "text", "text": "Under the stochastic setting, CAMS adopts a weighted majority strategy [45] when selecting models. The vector of each model\u2019s weighted votes from the policies, $\\begin{array}{r}{\\dot{\\pmb{w_{t}}}=\\sum_{i\\in|\\Pi^{*}|}q_{t,i}\\pi_{i}(\\pmb{x_{t}})}\\end{array}$ , is interpreted as a distribution induced by the weighted policy. The model $j_{t}=\\mathrm{maxind}(\\pmb{w}_{t})$ which receives the highest probability becomes the recommended model at round $t$ . This deterministic model selection strategy is commonly used in stochastic online optimization [33]. An alternative strategy is to take a randomized approach as in the adversarial setting, or take a Follow-the-Leader approach [42] and go with the most probable model recommended by the most probable policy (i.e. use ${\\pmb w}_{t}=\\pi_{\\mathrm{maxind}({\\pmb q}_{t})}({\\pmb x}_{t}))$ .As shown in experimental results section and further discussed in Appendix (outperformance over the best policy/expert section), CAMS outperforms these policies in a wide range of practical applications. The model selection steps are detailed in Line 5-9 in Fig. 1. ", "page_idx": 4}, {"type": "text", "text": "Active queries. Under a limited budget, we intend to query the labels of those instances that exhibit significant disagreement among the pre-trained models $\\mathcal{F}$ . To achieve this goal, we design an adaptive query strategy with query probability $z_{t}$ . Concretely, given context $\\pmb{x}_{t}$ , model predictions $\\hat{\\mathbf{y}}_{t}$ and model distribution $w_{t}$ , we denote by $\\bar{\\ell}_{t}^{y}:=\\langle\\pmb{w}_{t},\\mathbb{I}\\left\\{\\hat{\\pmb{y}}_{t}\\neq\\pmb{\\dot{y}}\\right\\}\\rangle$ as the expected loss if the true label is $y$ . We characterize the model disagreement as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathfrak{E}\\left(\\pmb{\\hat{y}}_{t},\\pmb{w}_{t}\\right):=\\frac{1}{c}\\sum_{y\\in\\mathcal{Y},\\bar{\\ell}_{t}^{y}\\in(0,1)}\\bar{\\ell}_{t}^{y}\\log_{c}\\frac{1}{\\bar{\\ell}_{t}^{y}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, when $\\bar{\\ell}_{t}^{y}$ is close to 0 or 1, there is little disagreement among the models in labeling $\\pmb{x}_{t}$ as $y$ , otherwise there is significant disagreement. We capture this insight with function $h(x)=-\\bar{x}\\log x$ . Since the label $y_{t}$ is unknown upfront when receiving $\\pmb{x}_{t}$ , we iterate through all the possible labels $y\\in\\mathcal{V}$ and take the average value as in Eq. (3). Note that $\\mathfrak{E}$ takes a similar algebraic form to the entropy function, although it does not inherit the information-theoretic interpretation. ", "page_idx": 4}, {"type": "text", "text": "With the model disagreement term defined above, we consider an adaptive query probability ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{t}=\\mathrm{max}\\left\\{\\delta_{0}^{t},\\mathfrak{E}\\left(\\hat{\\mathbf{y}}_{t},\\boldsymbol{w}_{t}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta_{0}^{t}=\\frac{1}{\\sqrt{t}}\\in(0,1]}\\end{array}$ is an adaptive lower bound on the query probability to encourage exploration at an early stage. The query strategy is summarized in Line 10-14 in Fig. 1. ", "page_idx": 4}, {"type": "text", "text": "Model updates. Now define $U_{t}\\,\\sim\\,{\\mathrm{Ber}}\\left(z_{t}\\right)$ as a binary query indicator that is sampled from a Bernoulli distribution parametrized by $z_{t}$ . Upon querying the label $y_{t}$ , one can calculate the loss for each model $f_{j}\\in\\mathcal{F}$ as $\\mathcal{\\ell}_{t,j}=\\mathbb{I}\\left\\{\\hat{y}_{t,j}\\neq y_{t}\\right\\}$ . Since CAMS does not query all the i.i.d. examples, we introduce an unbiased loss estimator for the models, defined as $\\begin{array}{r}{\\hat{\\ell}_{t,j}\\,=\\,\\frac{\\ell_{t,j}}{z_{t}}U_{t}}\\end{array}$ . The unbiased loss of policy $\\pi_{i}\\in\\Pi^{*}$ can then be computed as $\\tilde{\\ell}_{t,i}=\\langle\\pi_{i}(\\pmb{x}_{t}),\\hat{\\ell}_{t,j}\\rangle$ . In the end, CAMS computes the (unbiased) cumulative loss of policy $\\pi_{i}$ as $\\begin{array}{r}{\\tilde{L}_{T,i}=\\sum_{t=1}^{T}\\tilde{\\ell}_{t,i},}\\end{array}$ , which is used to update the policy probability distribution in next round. Pseudocode  for the model update steps is summarized in Line 15-21 in Fig. 1. ", "page_idx": 4}, {"type": "text", "text": "Remark. CAMS runs efficiently with time complexity $O\\left(n k\\right)$ per round and space complexity $O\\left(\\left(n+k\\right)\\cdot k\\right)$ . At each round, each model selection policy specifies a probability distribution over the models for the given context. When these distributions correspond to constant Dirac delta distributions (regardless of the context), the problem reduces to the context-free problem investigated by Karimi et al. [39]. ", "page_idx": 4}, {"type": "text", "text": "5 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present theoretical bounds on the regret (defined in Eq. (1) and Eq. (2), respectively) and the query complexity of CAMS for both the stochastic and the adversarial settings. ", "page_idx": 4}, {"type": "text", "text": "5.1 Stochastic setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Under the stochastic setting, the cumulative loss of CAMS over $\\mathrm{T}$ rounds\u2014as specified by the RECOMMEND procedure\u2014is $\\begin{array}{r}{L_{T}^{\\mathrm{CAMS}}=\\sum_{t=1}^{T}\\hat{\\ell}_{t,\\mathrm{maxind}(w_{t})}}\\end{array}$ where recall $\\begin{array}{r}{{\\pmb w}_{t}=\\sum_{i\\in|\\Pi^{*}|}q_{t,i}\\pi_{i}({\\pmb x}_{t})}\\end{array}$ is the probability distribution over $\\mathcal{F}$ induced by the weighted policy. ", "page_idx": 5}, {"type": "text", "text": "Let $i^{*}=\\arg\\operatorname*{min}_{i\\in[|\\Pi^{*}|]}\\mu_{i}$ be the index of the best policy ( $\\textstyle\\left.\\mu_{i}\\right.$ denotes the expected loss of policy $i$ as defined in problem statement section. The cumulative expected loss of policy $i^{*}$ is $T\\mu_{i^{*}}$ ; therefore the expected pseudo-regret (Eq. (1)) is $\\begin{array}{r}{\\mathcal{\\overline{{R}}}_{T}\\left(\\mathbf{CAMS}\\right)=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\hat{\\ell}_{t,\\mathrm{maxind}\\left(w_{t}\\right)}\\right]-T\\mu_{i^{*}}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Define $\\begin{array}{r}{\\Delta:=\\operatorname*{min}_{i\\neq i^{*}}(\\mu_{i}-\\mu_{i^{*}})}\\end{array}$ as the minimal sub-optimality $\\mathrm{gap^{4}}$ in terms of the expected loss against the best policy $i^{*}$ . Furthermore, let $w_{i^{*}}^{t}:=\\pi_{i^{*}}$ $\\left({\\bf{\\boldsymbol{x}}}_{t}\\right)$ be probability distribution over $\\mathcal{F}$ induced by policy $i^{*}$ at round $t$ . We define $\\begin{array}{r}{\\gamma:=\\operatorname*{min}_{x_{t}}\\{\\operatorname*{max}_{w_{j}\\in w_{i^{*}}^{t}}w_{j}-\\operatorname*{max}_{w_{j}\\in w_{i^{*}}^{t},j\\neq\\operatorname*{maxind}(w_{i^{*}}^{t})}w_{j}\\}}\\end{array}$ (5) as the minimal probability gap between the most probable model and the rest (assuming no ties) induced by the best policy $i^{*}$ . We further define $\\bar{b}=\\bar{p}_{\\mathrm{min}}\\log_{c}\\left(1/p_{\\mathrm{min}}\\right)$ , where $\\begin{array}{r}{p_{\\mathrm{min}}=\\operatorname*{min}_{s,i}\\pi(\\mathbf{x}_{s})}\\end{array}$ denotes the minimal model selection probability by any policy5. As our first main theoretical result, we show that, without exhaustively querying the labels of the stochastic stream, CAMS achieves constant expected regret. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. (Regret) In the stochastic environment, with probability at least $1-\\delta$ , CAMS achieves constant expected pseudo regret $\\begin{array}{r}{\\overline{{\\mathcal{R}}}_{T}\\left(\\mathbf{CAMS}\\right)=\\left(\\frac{\\ln\\frac{|\\Pi^{*}|-1}{\\gamma}+\\sqrt{\\ln|\\Pi^{*}|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}}{\\sqrt{\\ln|\\Pi^{*}|}\\Delta}\\right)^{2}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Note that in the stochastic setting, a lower bound of $\\Omega\\left(\\left(\\log\\Pi^{*}\\right)/\\Delta\\right)$ was shown in Mourtada and Ga\u00efffas [50] for online learning problems with expert advice under the full information setting (i.e. assuming labels are given for all data points in the stochastic stream). To establish the proof of Theorem 1, we consider a novel procedure to connect the weighted policy by CAMS to the best policy $\\pi_{i^{*}}$ . Conceptually, we would like to show that, after a constant number of rounds $\\tau_{\\mathrm{const}}$ , with high probability, the model selected by CAMS (Line 32) will be the same as the one selected by the best policy $i^{*}$ . In that way, the expected pseudo regret will be dominated by the maximal cumulative loss up to $\\tau_{\\mathrm{const}}$ . Toward this goal, we first bound the weight of the best policy $w_{t,i^{*}}$ as a function of $t$ , by choosing a proper learning rate $\\eta_{t}$ (CAMS, Line 23). Then, we identify a constant threshold $\\tau_{\\mathrm{const}}$ beyond which CAMS exhibits the same behavior as $\\pi_{i^{*}}$ with high probability. Finally, we obtain the regret bound by inspecting the regret at the two stages separately. The formal statement of Theorem 1 and the detailed proof are deferred to App. E.1. ", "page_idx": 5}, {"type": "text", "text": "Next, we provide an upper bound on the query complexity in the stochastic setting. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. (Query Complexity). For $c$ -class classification problems, with probability at least $1\\,-\\,\\delta$ , the expected number of queries made by CAMS over $T$ rounds is upper bounded by $\\begin{array}{r}{\\left(\\left(\\frac{\\ln\\frac{|\\Pi^{*}|-1}{\\gamma}+\\sqrt{\\ln|\\Pi^{*}|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}}{\\sqrt{\\ln|\\Pi^{*}|\\Delta}}\\right)^{2}+T\\mu_{i^{*}}\\right)\\frac{\\ln T}{c\\ln c}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 is built upon Theorem 1, where the the key idea behind the proof is to relate the number of updates to the regret. When $T\\mu_{i^{*}},\\tilde{L}_{T,*}$ are regarded as constants (given by an oracle), the query-complexity bound is then sub-linear w.r.t. $T$ . Note that the number of class labels $c$ affects the quality of the query complexity bound. The intuition behind this result is, with larger number of classes, each query may carry more information upon observation. For instance, in an extreme case where only one expert always recommends the best model and others gives random recommendations of models (and predicts random labels), having more classes lowers the chance of a model making the correct guess, and therefore helps to \"fliter out\" those suboptimal experts in fewer rounds\u2014hence being more query efficient. We defer the proof of Theorem 2 to App. E.2. ", "page_idx": 5}, {"type": "text", "text": "5.2 Adversarial setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Now we consider the adversarial setting. Let $\\begin{array}{r}{\\tilde{L}_{T,*}\\;:=\\;\\operatorname*{min}_{i\\in[|\\Pi^{*}|]}\\sum_{t=1}^{T}\\tilde{\\ell}_{t,i}}\\end{array}$ be the cumulative loss of the best policy. The expected regret (Eq. (2)) for CAMS equals to $\\mathcal{R}_{T}\\left(\\mathbf{CAMS}\\right)\\ =$ $\\begin{array}{r}{\\mathbb{E}\\big[\\sum_{t=1}^{T}\\langle\\pmb{q}_{t},\\tilde{\\ell}_{t}\\rangle\\big]-\\tilde{L}_{T,*}}\\end{array}$ . We show that under the adversarial setting, CAMS achieves sub-linear regret in $T$ without accessing all labels. ", "page_idx": 5}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/55a4bc565a5807c56cb0193c220ca232f094b69d88ba2b06d38080f74778d11a.jpg", "img_caption": ["Figure 2: Main results. Comparison of CAMS with 7 baselines across 4 diverse benchmarks in terms of cost effectiveness. We plot the cumulative loss as we increase the query cost for a fixed number of rounds $T$ and maximal query cost $B$ (from left to right: $T=10000,3000,80,4000$ , and $B=1200$ , 2000, 80, 2000). CAMS outperforms all baselines. Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). $90\\%$ confident interval are indicated in shades. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3. (Regret) Let c be the number of classes and $\\rho_{t}$ be specified as Line 26-27 in the SETRATE procedure. Under the adversarial setting, the expected regret of CAMS is bounded by $2c\\sqrt{\\ln{c}/\\mathrm{max}\\{\\rho_{T},\\sqrt{1/T}\\}}\\cdot\\sqrt{T\\log{|\\Pi^{*}|}}.$ . ", "page_idx": 6}, {"type": "text", "text": "The proof is provided in App. F.1. Assuming $\\rho_{t}$ to be a constant, our regret upper bound in Theorem 3 matches (up to constants) the lower bound of $\\Omega\\left(\\sqrt{T\\ln\\left|\\Pi^{*}\\right|}\\right)$ for online learning problems with expert advice under the full information setting [15, 63] (i.e. assuming labels are given for all data points). Her\u221aeby, the decaying learning rate $\\eta_{t}$ as specified in Line 27 is based on two parameters, where $1/\\sqrt{t}$ corresponds to the lower bound $\\delta_{0}^{t}$ on the query probability, and $\\rho_{t}\\triangleq1-\\operatorname*{max}_{\\tau\\in[t-1]}\\langle\\pmb{w}_{\\tau},\\mathbb{I}\\,\\{\\hat{\\pmb{y}}_{\\tau}=\\pmb{y}_{\\tau}\\}\\rangle$ (6) is a (data-dependent) term that is chosen to reduce the impact of the randomized query strategy on the regret bound (especially when $t$ is large). Intuitively, $\\rho_{t}$ relates to the skewness of the policy where the max term corresponds to the maximal probability of most probable mispredicted label over $t$ rounds. Note that in theory $\\rho_{t}$ can be small (e.g. CAMS may choose a constant policy $\\pi_{i}^{\\mathrm{const}}\\in\\Pi^{*}$ that mispredict the label for $\\pmb{x}_{t}$ , w\u221ahich leads to $\\rho_{t}=0,$ ); in such cases, our result still translates to a sublinear regret bound of $O(c{\\sqrt{\\log c}}\\cdot T^{\\frac{3}{4}}{\\sqrt{\\log|\\Pi^{*}|}})$ . Furthermore, in practice, we consider to \u201cregularize\u201d the policies (App. D.4) to ensure that probability a policy selecting any model is bounded away from 0. ", "page_idx": 6}, {"type": "text", "text": "Finally, the following theorem (proof in App. F.2) establishes a query complexity bound of CAMS. Theorem 4. (Query Complexity). Under the adversarial setting, the expected query complexity over $T$ rounds is $\\begin{array}{r}{O\\left(\\ln T\\left(\\sqrt{\\frac{T\\log|\\Pi^{*}|}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}_{T,*}\\right)\\right).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We evaluate our approach using five datasets: (1) CIFAR10 [41] contains 60,000 images from 10 different balanced classes. (2) DRIFT [73] is a tabular dataset with 128-dimensional features, based on 13,910 chemical sensor measurements of 6 types of gases at various concentration levels. (3) VERTEBRAL [5] is a biomedical tabular dataset which classifies 310 patients into three classes (Normal, Spondylolisthesis, Disk Hernia) based on 6 attributes. (4) HIV [74] contains over 40,000 compounds annotated with molecular graph features and binary labels (active, inactive) indicating their ability to inhibit HIV replication. (5) CovType [24] has 580K samples and contains details including slope, aspect, elevation, measurements of area, and type of forest cover. ", "page_idx": 6}, {"type": "text", "text": "Policy sets. We construct the policy sets $\\Pi$ for each dataset following a procedure similar to Meta-selector [48]. In this approach, a set of recommender algorithms is considered, and Meta-selector assigns varying ratings to these algorithms based on the specific user. Concretely, we first construct a set of models trained on different subsamples from each dataset. We then construct a set of policies, which include malicious, normal, random, and biased policy types for each dataset based on different models and features. Details on the classifiers and policies are provided in the supplemental materials. The malicious policy provides contrary advice; the random policy provides random advice; the biased policy provides biased advice by training on a biased distribution for classifying specific classes. The normal policy gives reasonable advice, being trained under a standard process on the training set. We represent the output of the $i_{t h}$ policy as $\\pi_{i}\\left(\\pmb{x}_{t}\\right)$ , indicating the rewards distribution of all the base classifiers on $\\pmb{x}_{t}$ . In total, we create 80, 10, 6, 4 classifiers and 85, 11, 17, 20 policies for CIFAR10, DRIFT, VERTEBRAL, and HIV, respectively. ", "page_idx": 6}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/3a899a583c625c362e0373fa1ce072cbfd39434a29bdaf668184fc940c31da5b.jpg", "img_caption": ["", "Figure 3: Ablation studies. (a) Comparing three query strategies $\\{{\\bf C}{\\bf A}{\\bf M}{\\bf S}$ , variance-based, random} under same model selection policy. (b) Comparing the increasing rate of CAMS\u2019 query cost over other baselines. (c) Comparing CAMS with MP in context-free environment. (d) Evaluating the performance of CAMS under a pure adversarial setting. (e) Large dataset. (f,g) Adjustable query probability. (h) CAMS outperforms the best single policy. The ablation study (a)-(d) is conducted on CIFAR10. For additional results on other benchmarks, please refer to the supplemental material. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Baselines. We evaluate CAMS against both contextual and non-contextual active model selection baselines. We consider four non-contextual baselines: (1) Random Query Strategy (RS) which queries the instance label with a fixed probability $\\frac{b}{T}$ ; (2) Model Picker (MP) [39] that employs variancebased active sampling with a coin-flip query probability max $\\{v\\left(\\hat{\\mathbf{y}}_{t},\\pmb{w}_{t}\\right),\\eta_{t}\\}$ , where the variance term is defined as $\\begin{array}{r}{v\\left(\\hat{\\pmb y}_{t},\\pmb w_{t}\\right)=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\bar{\\ell}_{t}^{y}\\left(1-\\bar{\\ell}_{t}^{y}\\right)}\\end{array}$ ; (3) Query by Committee (QBC) implementing committee-based sampling [22]; and (4) Importance Weighted Active Learning (IWAL) [8] that calculates query probability based on labeling disagreements of surviving classifiers. Since no contextual baselines exist yet, we propose contextual versions of QBC and IWAL as (5) CQBC and (6) CIWAL. Both extensions maintain their respective original query strategies but incorporate the context into the cumulative rewards. For model selection, CAMS, MP, CQBC, and CIWAL recommend the classifier with the highest probability. The other baselines use Follow-the-Leader (FTL), recommending the model with the minimum cumulative loss for past queried instances. Finally, we add (7) Oracle to represent the best single policy with the minimum cumulative loss, with the same query strategy as CAMS. ", "page_idx": 7}, {"type": "text", "text": "6.1 Main results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Fig. 2 visualizes the cost effectiveness of CAMS and the baselines. Here, we define cost effectiveness as the measure of how quickly the cumulative loss decreases in response to an increase in query cost. Fig. 2 demonstrates that CAMS outperforms all the comparison methods across all benchmarks. Remarkably, it outperforms even the oracle on the VERTEBRAL (Fig. 2c) and HIV (Fig. 2d) benchmarks with fewer than 10 and 20 queries, respectively. In the case of the VERTEBRAL benchmark, CAMS outperforms the best baseline in query cost by a margin of $20\\%$ , despite the fact that 11 out of the 17 experts provided malicious or random advice. This level of performance is attained by utilizing an active query strategy to retrieve highly informative data, thereby maximizing the differentiation between models and policies within the constraints of a limited budget. Additionally, the model selection strategy allows for effectively combining the expertise among the experts. ", "page_idx": 7}, {"type": "text", "text": "6.2 Ablation studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effectiveness of active querying. In Fig. 3a and Fig. 3b, we perform ablation studies to demonstrate the effectiveness of our active query strategy. We fix the model recommendation strategy as the one used by CAMS, and compare three query strategies: (1) CAMS, (2) the state-of-the-art variancebased query strategy from Model Picker [39] (referred to as \u201cvariance\u201d), and (3) a random query strategy. Figure 3a demonstrates that CAMS has the fastest convergence rate in terms of cumulative loss on CIFAR10, implying effective use of queried labels. Furthermore, CAMS not only achieves the minimum cumulative loss but also incurs significantly lower query costs, with reductions of $71\\%$ and $95\\%$ compared to the variance and random strategies respectively as showed in Fig. 3b. This suggests that CAMS selectively queries data to optimize policy improvement, whereas the other strategies may query unnecessary labels, including potentially noisy or uninformative ones, which impede policy improvement and convergence. ", "page_idx": 8}, {"type": "text", "text": "Robustness. In Fig. 2, 3c, 3d,3e, 3f, and 3g, CAMS exhibits robustness in a variety of environmental settings. Firstly, As shown in Fig. 2, CAMS outshines other methods in a contextual environment, whereas in Fig. 3c, a non-contextual (no experts) environment, it achieves comparable performance to the state-of-the-art Model Picker in identifying the best classifier. Secondly, CAMS is robust in both stochastic and adversarial environments. As demonstrated in Fig. 2, CAMS surpasses other methods in a stochastic environment. Additionally, as illustrated in Fig. 3d, in a worst-case adversarial environment, CAMS effectively recovers from adversarial actions and approaches the performance of the best classifier (see App. G.5). We further observe that CAMS demonstrates robustness to varying scales of data, where the online stream sizes range from 80 to 10K (Fig. 2) to 100K (Fig. 3e, where we randomly sample 100K samples from the CovType dataset [24]). ", "page_idx": 8}, {"type": "text", "text": "In Fig. 2, we assume that the stream length $T$ is hidden and not used as input to CAMS. Under the stochastic setting, however, knowing $T$ can provide additional information that one can leverage to optimize the query probability, thereby giving an advantage to some of the baseline algorithms (e.g. random). As an ablation study, in Fig. 3f and Fig. $3\\mathrm{g}$ , we assume the stochastic setting where the total length $T$ of the online stream is given. Given the stream length $T$ and query budget $b$ , we may optimize each algorithm by scaling their query probabilities, so that each algorithm allocates its query budget to the top $b$ informative labels in the entire online stream based on its own query criterion. CAMS still ourperform the baselines under the setting. ", "page_idx": 8}, {"type": "text", "text": "Improvement over the best classifier and policy. Fig. 3h demonstrates that when provided with good policies, CAMS formulates a stronger policy which incurs no regret. CAMS has the potential to outperform an oracle, especially in rounds where the oracle does not make the optimal recommendation. For instance, in the stochastic version of CAMS (as shown in lines 22-23 and 30-32 of Fig. 1), CAMS recommends a model using a weighted majority vote among all policies, enabling the formation of a new policy in each round by amalgamating the strengths of each sub-optimal policy. This adaptive strategy can potentially outperform any single policy. Moreover, in most real-world scenarios and conducted experiments (as depicted in App. G.6), data streams may not be strictly stochastic, and therefore no single policy consistently performs the best. In such cases, CAMS\u2019s weighted policy may find an enhanced combination of \u201cadvices\u201d, leading to improved performance. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduced CAMS, an online contextual active model selection framework based on a novel model selection and active query strategy. The algorithm was motivated by many real-world use cases that need to make decision by taking both contextual information and the cost into consideration. We have demonstrated CAMS\u2019s compelling performance of using the minimum query cost to learn the optimal contextual model selection policy on several diverse online model selection tasks. In addition to the promising empirical performance, we also provided rigorous theoretical guarantees on the regret and query complexity for both stochastic and adversarial settings. We hope our work can inspire future works to handle more complex real-world model selection tasks (e.g. beyond classification or non-uniform loss functions, etc. where our analysis does not readily apply). ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the RadBio-AI project (DE-AC02-06CH11357), U.S. Department of Energy Office of Science, Office of Biological and Environment Research, the IMPROVE project under contract (75N91019F00134, 75N91019D00024, 89233218CNA000001, DE-AC02-06- CH11357, DE-AC52-07NA27344, DE-AC05-00OR22725), the Laboratory Directed Research and Development (LDRD) funding from Argonne National Laboratory provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DE-AC02-06CH11357, the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration, the University of Chicago Joint Task Force Initiative, the AI-Assisted Hybrid Renewable Energy, Nutrient, and Water Recovery project (DOE DE-EE0009505), and the National Science Foundation under Grant No. IIS 2313131 and IIS 2332475. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Mahmoud Khaled Abd-Ellah, Ali Ismail Awad, Ashraf AM Khalaf, and Hesham FA Hamed. Two-phase multi-model automatic brain tumour diagnosis system from magnetic resonance images using convolutional neural networks. EURASIP Journal on Image and Video Processing, 2018(1):1\u201310, 2018. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Ravi Aggarwal, Viknesh Sounderajah, Guy Martin, Daniel SW Ting, Alan Karthikesalingam, Dominic King, Hutan Ashrafian, and Ara Darzi. Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis. NPJ digital medicine, 4(1):65, 2021. [4] Alnur Ali, Rich Caruana, and Ashish Kapoor. Active learning with model selection. In Proceedings of the AAAI conference on artificial intelligence, volume 28, 2014.   \n[5] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.   \n[6] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235\u2013256, 2002. [7] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002. [8] Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In Proceedings of the 26th annual international conference on machine learning, pages 49\u201356, 2009. [9] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 19\u201326. JMLR Workshop and Conference Proceedings, 2011.   \n[10] Leo Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001.   \n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[12] S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends\u00ae in Machine Learning, 5(1):1\u2013122, 2012.   \n[13] Giuseppe Burtini, Jason Loeppky, and Ramon Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015.   \n[14] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n[15] Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427\u2013485, 1997.   \n[16] Nicolo Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient prediction. IEEE Transactions on Information Theory, 51(6):2152\u20132162, 2005.   \n[17] Austin Clyde, Xuefeng Liu, Thomas Brettin, Hyunseung Yoo, Alexander Partin, Yadu Babuji, Ben Blaiszik, Jamaludin Mohd-Yusof, Andre Merzky, Matteo Turilli, et al. Ai-accelerated protein-ligand docking for sars-cov-2 is 100-fold faster with no significant change in detection. Scientific Reports, 13(1):2105, 2023.   \n[18] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3): 273\u2013297, 1995.   \n[19] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information theory, 13(1):21\u201327, 1967.   \n[20] Jan Salomon Cramer. The origins of logistic regression. 2002.   \n[21] Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, and Manish Purohit. Dynamic balancing for model selection in bandits and rl. In International Conference on Machine Learning, pages 2276\u20132285. PMLR, 2021.   \n[22] Ido Dagan and Sean P Engelson. Committee-based sampling for training probabilistic classifiers. In Machine Learning Proceedings 1995, pages 150\u2013157. Elsevier, 1995.   \n[23] Jiao Du, Weisheng Li, Ke Lu, and Bin Xiao. An overview of multi-modal medical image fusion. Neurocomputing, 215:3\u201320, 2016.   \n[24] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive. ics.uci.edu/ml.   \n[25] Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42 (6):1273\u20131280, 2002.   \n[26] Ronald A Fisher. The statistical utilization of multiple measurements. Annals of eugenics, 8(4): 376\u2013386, 1938.   \n[27] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997.   \n[28] Yoav Freund, Robert Schapire, and Naoki Abe. A short introduction to boosting. JournalJapanese Society For Artificial Intelligence, 14(771-780):1612, 1999.   \n[29] Jacob R Gardner, Gustavo Malkomes, Roman Garnett, Kilian Q Weinberger, Dennis Barbour, and John P Cunningham. Bayesian active model selection with an application to automated audiometry. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2, pages 2386\u20132394, 2015.   \n[30] Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine learning, 63(1):3\u201342, 2006.   \n[31] David J Hand and Keming Yu. Idiot\u2019s bayes\u2014not so stupid after all? International statistical review, 69(3):385\u2013398, 2001.   \n[32] Katja Hansen, Franziska Biegler, Raghunathan Ramakrishnan, Wiktor Pronobis, O Anatole Von Lilienfeld, Klaus-Robert Muller, and Alexandre Tkatchenko. Machine learning predictions of molecular properties: Accurate many-body potentials and nonlocality in chemical space. The journal of physical chemistry letters, 6(12):2326\u20132331, 2015.   \n[33] Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.   \n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[35] Shen-Shyang Ho and Harry Wechsler. Query by transduction. IEEE transactions on pattern analysis and machine intelligence, 30(9):1557\u20131571, 2008.   \n[36] Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive survey. Neurocomputing, 459:249\u2013289, 2021.   \n[37] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.   \n[38] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[39] Mohammad Reza Karimi, Nezihe Merve G\u00fcrel, Bojan Karla\u0161, Johannes Rausch, Ce Zhang, and Andreas Krause. Online active model selection for pre-trained classifiers. In International Conference on Artificial Intelligence and Statistics, pages 307\u2013315. PMLR, 2021.   \n[40] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[41] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[42] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[43] Rui Leite and Pavel Brazdil. Active testing strategy to predict the best classification algorithm via sampling and metalearning. In ECAI, pages 309\u2013314, 2010.   \n[44] Weizhi Li, Gautam Dasarathy, Karthikeyan Natesan Ramamurthy, and Visar Berisha. Finding the homology of decision boundaries with active learning. Advances in Neural Information Processing Systems, 33:8355\u20138365, 2020.   \n[45] Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and computation, 108(2):212\u2013261, 1994.   \n[46] Ying-tao Liu, Yi Li, Zi-fu Huang, Zhi-jian Xu, Zhuo Yang, Zhu-xi Chen, Kai-xian Chen, Ji-ye Shi, and Wei-liang Zhu. Multi-algorithm and multi-model based drug target prediction and web server. Acta Pharmacologica Sinica, 35(3):419\u2013431, 2014.   \n[47] Chen Change Loy, Timothy M Hospedales, Tao Xiang, and Shaogang Gong. Stream-based joint exploration-exploitation active learning. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 1560\u20131567. IEEE, 2012.   \n[48] Mi Luo, Fei Chen, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Jiashi Feng, and Zhenguo Li. Metaselector: Meta-learning for recommendation with user-level adaptive model selection. In Proceedings of The Web Conference 2020, pages 2507\u20132513, 2020.   \n[49] Omid Madani, Daniel J Lizotte, and Russell Greiner. Active model selection. arXiv preprint arXiv:1207.4138, 2012.   \n[50] Jaouad Mourtada and St\u00e9phane Ga\u00efffas. On the optimality of the hedge algorithm in the stochastic regime. Journal of Machine Learning Research, 20:1\u201328, 2019.   \n[51] Mohamad T Musavi, Wahid Ahmed, Khue Hiang Chan, Kathleen B Faris, and Donald M Hummels. On the training of radial basis function classifiers. Neural networks, 5(4):595\u2013603, 1992.   \n[52] Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. arXiv preprint arXiv:1506.03271, 2015.   \n[53] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[54] Nikunj C Oza and Stuart J Russell. Online bagging and boosting. In International Workshop on Artificial Intelligence and Statistics, pages 229\u2013236. PMLR, 2001.   \n[55] J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81\u2013106, 1986.   \n[56] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on machine learning, pages 63\u201371. Springer, 2003.   \n[57] Ryan M Rifkin and Ross A Lippert. Notes on regularized least squares. 2007.   \n[58] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5):742\u2013754, 2010.   \n[59] Marlesson RO Santana, Luckeciano C Melo, Fernando HF Camargo, Bruno Brand\u00e3o, Anderson Soares, Renan M Oliveira, and Sandor Caetano. Contextual meta-bandit for recommender systems selection. In Fourteenth ACM Conference on Recommender Systems, pages 444\u2013449, 2020.   \n[60] Christoph Sawade, Niels Landwehr, Steffen Bickel, and Tobias Scheffer. Active risk estimation. In ICML, 2010.   \n[61] Christoph Sawade, Niels Landwehr, and Tobias Scheffer. Active comparison of prediction models. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.   \n[62] Nick Schneider, Florian Piewak, Christoph Stiller, and Uwe Franke. Regnet: Multimodal sensor registration using deep neural networks. In 2017 IEEE intelligent vehicles symposium (IV), pages 1803\u20131810. IEEE, 2017.   \n[63] Yevgeny Seldin and G\u00e1bor Lugosi. A lower bound for multi-armed bandits with expert advice. In 13th European Workshop on Reinforcement Learning (EWRL), 2016.   \n[64] Burr Settles. Active learning literature survey. 2009.   \n[65] H Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Proceedings of the fifth annual workshop on Computational learning theory, pages 287\u2013294, 1992.   \n[66] Naman Shukla, Arinbj\u00f6rn Kolbeinsson, Lavanya Marla, and Kartik Yellepeddi. Adaptive model selection framework: An application to airline pricing. arXiv preprint arXiv:1905.08874, 2019.   \n[67] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[68] Masashi Sugiyama and Neil Rubens. A batch ensemble approach to active learning with model selection. Neural Networks, 21(9):1278\u20131286, 2008.   \n[69] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.   \n[70] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[71] St\u00e9phanie Van Der Pas and Peter Gr\u00fcnwald. Almost the best of three worlds: Risk, consistency and optional stopping for the switch criterion in nested model selection. Statistica Sinica, pages 229\u2013253, 2018.   \n[72] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[73] Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ram\u00f3n Huerta. Chemical gas sensor drift compensation using classifier ensembles. Sensors and Actuators B: Chemical, 166:320\u2013329, 2012.   \n[74] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513\u2013530, 2018.   \n[75] Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry, 63(16):8749\u20138760, 2019.   \n[76] Chicheng Zhang and Kamalika Chaudhuri. Beyond disagreement-based agnostic active learning. Advances in Neural Information Processing Systems, 27:442\u2013450, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Impact Statements ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This paper introduces a novel framework for adaptive model selection in label-efficient learning. By integrating robust online learning with active query strategies, our algorithm effectively adapts to varying data contexts and minimizes labeling efforts, crucial in domains requiring swift and accurate decisions, such as disease identification and financial predictions. Ethically, the framework\u2019s design promotes efficient and context-aware model selection, reducing potential biases associated with context-ignorant model selections. No major ethical concerns are anticipated, given the algorithm\u2019s generality and focus on solving practical problems. ", "page_idx": 13}, {"type": "text", "text": "B Table of Notations Defined in the Main Paper ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "ZizwgYErtQ/tmp/4fb14d68349973627ee82e5d2581d6261f63823cd074055c72e52e47572e6276.jpg", "table_caption": ["Table 2: Notations used in the main paper "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Summary of Regret and Query Complexity Bounds ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We summarize the regret and query complexity bounds (if applicable) of related algorithms in Table 3. ", "page_idx": 14}, {"type": "table", "img_path": "ZizwgYErtQ/tmp/780a7b3eda234748c2f341819aeae5b0edf9fd1a3e516da0ddd8880e7b92c266.jpg", "table_caption": [], "table_footnote": ["Table 3: Regret and query complexity bounds. For the notations in this table: $i^{*}$ is the model with the highest expected accuracy; $\\theta_{j}\\bar{=}\\mathbb{P}\\left[\\bar{\\ell}_{.,j}\\neq\\ell_{.,i^{*}}\\right]$ is the probability that exactly one of $j$ and $i^{*}$ correctly classifies a sample; $\\gamma$ and $\\rho_{T}$ are defined in Eq. (5) and (6), respectively. $b=p_{\\mathrm{min}}\\log_{c}\\left(1/p_{\\mathrm{min}}\\right)$ , where $\\begin{array}{r}{p_{\\mathrm{min}}=\\operatorname*{min}_{s,i}\\pi(\\mathbf{x}_{s})}\\end{array}$ denotes the minimal model selection probability by any policy. "], "page_idx": 14}, {"type": "text", "text": "Remark 5. When $T\\mu_{i^{*}},\\tilde{L}_{T,*}$ are regarded as constants (given by an oracle), the query-complexity bound is then sub-linear w.r.t. $T$ . ", "page_idx": 14}, {"type": "text", "text": "Remark 6. Note that the number of class labels $c$ affects the quality of the query complexity bound. The intuition behind this result is, with larger number of classes, each query may carry more information upon observation. For instance, in an extreme case where only one expert always recommends the best model and others gives random recommendations of models (and predicts random labels), having more classes lowers the chance of a model making the correct guess, and therefore helps to \"filter out\" those suboptimal experts in fewer rounds\u2014hence being more query efficient. ", "page_idx": 14}, {"type": "text", "text": "Remark 7. To prove the practical feasibility of CAMS, we have analyzed its time and space complexity. Our analysis shows that CAMS has a time complexity of $O\\left(T n k\\right)$ in total or $O(n k)$ per round (due to the RECOMMEND procedure under the stochastic setting), and a space complexity of $O\\left(\\left(n+k\\right)\\cdot k\\right)$ . Here, $T$ refers to the online horizon, $n$ denotes the number of policies, and $k$ denotes the number of models. Taking into account these complexities, we can confirm that CAMS is practically feasible. ", "page_idx": 14}, {"type": "text", "text": "D Supplemental Materials on Experimental Setup ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Model Picker (MP) Model Picker [39] is a context-free online active model selection method inspired by EXP3. Model Picker aims to find the best classifier in hindsight while making a small number of queries. For query strategy, it uses a variance-based active learning sampling method to select the most informative label to query to differentiate a pool of models, where the variance is defined as $\\begin{array}{r}{v\\left(\\hat{\\pmb{y}}_{t},\\pmb{w}_{t}\\right)=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\bar{\\ell}_{t}^{y}\\left(\\hat{1}-\\bar{\\ell}_{t}^{y}\\right)}\\end{array}$ . The coin-flip query probability is defined as max $\\{v\\left(\\hat{\\mathbf{y}}_{t},\\pmb{w}_{t}\\right),\\eta_{t}\\}$ when $v\\left(\\hat{\\mathbf{y}}_{t},\\mathbf{w}_{t}\\right)\\neq0$ , or 0 otherwise. For model recommendation, it uses an exponential weight algorithm to recommend the model with minimal exponential cumulative loss based on the past queried labels at each round. ", "page_idx": 14}, {"type": "text", "text": "Query by Committee (QBC) For query strategy, we have adapted the method of [22] as a disagreement-based selective sampling query strategy for online streaming data. We treat each classifier as a committee member and compute the query probability by measuring disagreement between models for each instance. The query function is coin-flip by vote entropy probability $-{\\frac{1}{\\log\\operatorname*{min}\\left(k,|C|\\right)}}\\sum_{c}{\\frac{V(c,x)}{k}}\\log{\\frac{V(c,x)}{k}}$ , where $V\\left(c,x\\right)$ stands for the number of committee members assigning a class c for input context $\\mathbf{X}$ $\\boldsymbol{\\mathrm{k}}$ is the number of committee. For the model recommendation part, we use the method of Follow-the-Leader (FTL) [42], which greedily recommends the model with the minimum cumulative loss for past queried instances. ", "page_idx": 14}, {"type": "text", "text": "Importance Weighted Active Learning (IWAL) We have implemented [8] as the IWAL baseline. For the query strategy part, IWAL computes an adaptive rejection threshold for each instance and assigns an importance weight to each classifier in the hypothesis space $\\mathcal{H}_{t}$ . IWAL retains the classifiers in the hypothesis space according to their weighted error versus the current best classifier\u2019s weighted error at round $t$ . The query probability is calculated based on labeling disagreements of surviving classifiers through function $\\begin{array}{r}{\\operatorname*{max}_{i,j\\in\\mathcal{H}_{t},y\\in[c]}\\ell_{t,i}^{(y)}-\\ell_{t,j}^{(y)}}\\end{array}$ . For model recommendation, we also adopt the Follow-the-Leader (FTL) strategy. ", "page_idx": 15}, {"type": "text", "text": "Random Query Strategy (RS) The RS method queries the label of incoming instances by the coin-flip fixed probability $\\frac{b}{T}$ . It also uses the FTL strategy based on queried instances for model recommendation. ", "page_idx": 15}, {"type": "text", "text": "Contextual Query by Committee (CQBC) We have created a contextual variant of QBC termed CQBC, which has the same entropy query strategy as the original QBC. For model recommendation, we combine two model selection strategies. The first strategy calculates the cumulative reward of each classifier based on past queries and normalizes it as a probability simplex vector. We also adopt Exp4\u2019s arm recommending vector to use contextual information. Finally, we compute the element-wise product of the two vectors and normalize it to be CQBC\u2019s model recommendation vector. At each round, CQBC would recommend the top model based on the classifiers\u2019 historical performance on queried instances and the online advice matrix for streaming data. ", "page_idx": 15}, {"type": "text", "text": "Contextual Importance Weighted Active Learning (CIWAL) We have created a variant version of importance-weighted active learning. Similar to CQBC, CIWAL adopts the query strategy from IWAL and converts the model selection strategy to be contextual. For model selection, we incorporate Exp4\u2019s arm recommendation strategy based on the side-information advice matrix and each classifier\u2019s historical performance according to queried instances. We compute the element-wise product of the two vectors as the model selection vector of CIWAL and normalize it as a weighted vector. Finally, CIWAL recommends the classifier with the highest weight. ", "page_idx": 15}, {"type": "text", "text": "Oracle: Among all the given policies, oracle represents the best single policy that achieves the minimum cumulative loss, and it has the same query strategy as CAMS. ", "page_idx": 15}, {"type": "text", "text": "D.2 Details on policies and classifiers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We constructed different expert-model configurations to reflect the cases in real-world applications6.   \nThis section lists the collection of policies and models used in our experiments. ", "page_idx": 15}, {"type": "text", "text": "CIFAR10: We have constructed 80 diversified classifiers based on VGG [67], ResNet [34], DenseNet [38], GoogLeNet [69]. We have also used EfficientNet [70], MobileNets [37], RegNet [62], and ResNet to construct 85 diversified policies. ", "page_idx": 15}, {"type": "text", "text": "DRIFT: We have constructed ten classifiers using Decision Tree [55], SVM [18], AdaBoost [28], Logistic Regression [20], KNN [19] models. We have also created 8 diversified policies with multilayer perceptron (MLP) models of different layer configurations: (128, 30, 10); (128, 60, 30, 10); (128, 120, 30, 10); (128, 240, 120, 30, 10). ", "page_idx": 15}, {"type": "text", "text": "VERTEBRAL: We have built six classifiers using Random Forest [10], Gaussian Process [56], linear discriminant analysis [26], Naive Bayes [31] algorithms. We have constructed policies by using standard scikit-learn built-in models including Random Forest Classifier, Extra Trees Classifier [30], Decision Tree Classifier, Radius Neighbors Classifier [51], Ridge Classifier [57] and K-NearestNeighbor classifiers. ", "page_idx": 15}, {"type": "text", "text": "HIV: We have used graph convolutional networks (GCN) [40], Graph Attention Networks (GAT) [72], AttentiveFP [75], and Random Forest to construct 4 classifiers. We have also used various feature representations of molecules such as MACCS key [25], ECFP2, ECFP4, and ECFP6 [58] molecular fingerprints to build 6 MLP-based policies, respectively. ", "page_idx": 16}, {"type": "text", "text": "CovType: We have built 6 classifiers using Random Forest, Gaussian Process, linear discriminant analysis, Naive Bayes algorithms. We have constructed 17 policies by using standard scikit-learn built-in models including Random Forest Classifier, Extra Trees Classifier, Decision Tree Classifier, Radius Neighbors Classifier, Ridge Classifier and K-Nearest-Neighbor classifiers. ", "page_idx": 16}, {"type": "text", "text": "D.3 Implementation details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We build our evaluation pipeline on top of prior work [39] around the four benchmark datasets. Specifically, ", "page_idx": 16}, {"type": "text", "text": "\u2022 Context $\\pmb{x}_{t}$ is the raw context of the data (e.g., the $32\\mathtt{x32}$ image for CIFAR10).   \n\u2022 Predictions $\\hat{\\mathbf{y}}_{t}$ contain the predicted label vector of all the classifiers\u2019 predictions according to the online context $x_{t}$ .   \n\u2022 Oracle contains the true label $y_{t}$ of $\\pmb{x}_{t}$ .   \n\u2022 Advice matrix contains all policies\u2019 probability distribution $\\lambda$ over all the classifiers on context $x_{t}$ . ", "page_idx": 16}, {"type": "text", "text": "To adapt to an online setting, we sequentially draw random $T$ i.i.d. instances $x_{1:T}$ from the test pool and define it as a realization. For a fair comparison, all algorithms receive data instances in the same order within the same realization. ", "page_idx": 16}, {"type": "table", "img_path": "ZizwgYErtQ/tmp/64209d98a0cef219364ecfc8f8c8fb78b354078cc7d8a1c474775f6c3da40979.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.4 Regularized policy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As discussed in adversarial section, we wish to ensure that the probability a policy selecting any model is bounded away from 0 so that the regret bound in Theorem 3 is non vacuous. In our experiments, we achieve this goal by applying a regularized policy $\\overline{{\\pi}}$ as shown in Algorithm 1. ", "page_idx": 16}, {"type": "text", "text": "D.5 Summary of datasets and models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We summarize the attributes of datasets, the models, and the model selection policies as follows. ", "page_idx": 16}, {"type": "table", "img_path": "ZizwgYErtQ/tmp/d08042348272e4460ccf8f81e59b66d1ca907fcb1b838eb933a24f98be7e3e42.jpg", "table_caption": ["Table 4: Attributes of benchmark datasets "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.6 Hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We performed our experiments on a Linux server with 80 Intel(R) Xeon(R) Gold 6148 CPU $@$ 2.40GHz and total 528 Gigabyte memory. ", "page_idx": 16}, {"type": "text", "text": "By considering the resource of server, We set 100 realizations and 3000 stream-size for DRIFT, 20 realizations and 10000 stream-size for CIFAR10, 200 realizations and 4000 stream size for HIV, ", "page_idx": 16}, {"type": "text", "text": "300 realization and 80 stream-size for VERTEBRAL. In each realization, we randomly selected stream-size aligned data from testing-set and make it as online streaming data which is the input of each algorithm. Thus, we got independent result for each realization. ", "page_idx": 17}, {"type": "text", "text": "A small realization number would increase the variance of the results due to the randomness of stream order. A large realization number would make the result be more stable but at the cost of increasing computational cost (time, memory, etc.). We chose the realization number by balancing both aspects. ", "page_idx": 17}, {"type": "text", "text": "E Proofs for the Stochastic Setting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we focus on the stochastic setting. We first prove the regret bound presented in Theorem 1 and then prove the query complexity presented in Theorem 2 for Algorithm 1. ", "page_idx": 17}, {"type": "text", "text": "E.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Before providing the proof of Theorem 1, we first introduce the following lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma 8. Fix $\\tau\\,\\in\\,(0,1)$ . Let $q_{t,i^{*}}$ be the probability of the optimal policy $i^{*}$ maintained by Algorithm $^{l}$ at $t$ , and let $b=p_{\\mathrm{min}}\\log_{c}\\left(1/p_{\\mathrm{min}}\\right)$ , where $\\begin{array}{r}{p_{\\mathrm{min}}=\\operatorname*{min}_{s,i}\\pi(\\mathbf{x}_{s})}\\end{array}$ denotes the minimal $\\begin{array}{r}{t\\geq\\left(\\frac{\\ln\\frac{\\left(\\left|\\Pi^{*}\\right|-1\\right)\\tau}{1-\\tau}}{\\sqrt{\\ln\\left|\\Pi^{*}\\right|}\\left(\\Delta-\\sqrt{\\frac{2b^{2}}{t}\\ln\\frac{2}{\\delta}}\\right)}\\right)^{\\frac{\\cdot}{2}}}\\end{array}$ 2 model selection probability by any policy7. When , with probability at least $1-\\delta$ , it holds that $q_{t,i^{*}}\\geq\\tau$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 8. W.l.o.g, we assume $\\mu_{1}~\\le~\\mu_{2}~\\le~\\dots\\mu_{n+k}$ . Recall that we define $\\Delta\\ =$ $\\begin{array}{r}{\\operatorname*{min}_{i\\neq i^{*}}\\Delta_{i}=\\mu_{2}-\\mu_{1}=\\frac{\\mathbb{E}\\left[\\widetilde{L}_{t,2}-\\widetilde{L}_{t,1}\\right]}{t}}\\end{array}$ , and $\\pi_{1}$ is the policy with the minimal expected loss. ", "page_idx": 17}, {"type": "text", "text": "Define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{t}\\triangleq\\tilde{\\ell}_{t-1,i^{\\prime}}-\\tilde{\\ell}_{t-1,1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{i^{\\prime}\\triangleq\\arg\\operatorname*{min}_{i\\neq1}\\tilde{L}_{t-1,i}}\\end{array}$ denotes the index of the best empirical policy up to $t-1$ other than $\\pi_{1}$ . Therefore for $i\\geq2$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widetilde{L}_{t-1,i^{\\prime}}-\\widetilde{L}_{t-1,i}=\\sum_{s=1}^{t-1}\\delta_{s}\\leq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have qt,i\u2217= qt,1 = $\\begin{array}{r}{q_{t,i^{*}}=q_{t,1}=\\frac{\\exp\\bigl(-\\eta_{t}\\widetilde{L}_{t-1,1}\\bigr)}{\\sum_{i=1}^{|\\Pi^{*}|}\\exp\\bigl(-\\eta_{t}\\widetilde{L}_{t-1,i}\\bigr)}}\\end{array}$ as the weight of optimal expert at round $t$ . Therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\eta_{t,i^{\\star}}=q_{t,1}=\\frac{\\exp\\big(-\\eta_{t}\\tilde{L}_{t-1,i^{\\star}}\\big)}{\\sum_{i=1}^{\\lfloor\\Pi\\rfloor}\\exp\\big(-\\eta_{t}\\tilde{L}_{t-1,i^{\\star}}\\big)}}\\\\ {\\overset{(a)}{=}\\frac{\\exp\\big(-\\eta_{t}\\tilde{L}_{t-1,1}+\\eta_{t}\\tilde{L}_{t-1,i^{\\star}}\\big)}{\\sum_{i=1}^{\\lfloor\\Pi\\rfloor}\\exp\\big(-\\eta_{t}\\tilde{L}_{t-1,i^{\\star}}+\\eta_{t}\\tilde{L}_{t-1,i^{\\star}}\\big)}}\\\\ {\\overset{(b)}{=}\\frac{\\exp\\big(\\eta_{t}\\sum_{s=1}^{t}\\delta_{s}\\big)}{\\exp\\big(\\eta_{t}\\sum_{s=1}^{t}\\delta_{s}\\big)+\\sum_{i=2}^{\\lfloor\\Pi\\rfloor}\\exp\\big(-\\eta_{t}\\tilde{L}_{t-1,i^{\\star}}+\\eta_{t}\\tilde{L}_{t-1,i^{\\prime}}\\big)}}\\\\ {\\geq\\frac{\\exp\\big(\\eta_{t}\\sum_{s=1}^{t}\\delta_{s}\\big)}{\\exp\\big(\\eta_{t}\\sum_{s=1}^{t}\\delta_{s}\\big)+\\prod^{\\bullet}\\left|-1\\right|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where step $(a)$ is by dividing the cumulative loss of sub-optimal policy $\\pi_{i^{\\prime}}$ and step (b) is by the definition of $\\delta_{t}$ in Equation (7). ", "page_idx": 17}, {"type": "text", "text": "Let $\\tau\\in(0,1)$ , such that \u2265exp(\u03b7t tst=1 \u03b4ss=)1+ |s\u03a0\u2217|\u22121 \u2265\u03c4. Plugging in \u03b7t = $\\begin{array}{r}{\\eta_{t}=\\sqrt{\\frac{\\ln|\\Pi^{*}|}{t}}}\\end{array}$ and define $\\begin{array}{r}{\\overline{{\\delta_{t}}}=\\frac{1}{t}\\sum_{s=1}^{t}\\delta_{s}}\\end{array}$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\exp\\left(\\sqrt{\\ln|\\Pi^{*}|}\\sqrt{t}\\cdot\\overline{{{\\delta_{t}}}}\\right)}{\\exp\\left(\\sqrt{\\ln|\\Pi^{*}|}\\sqrt{t}\\cdot\\overline{{{\\delta_{t}}}}\\right)+|\\Pi^{*}|-1}\\geq\\tau\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we obtain $\\begin{array}{r}{\\exp\\left(\\sqrt{\\ln|\\Pi^{*}|}\\sqrt{t}\\cdot\\overline{{\\delta_{t}}}\\right)\\geq\\frac{(|\\Pi^{*}|-1)\\tau}{1-\\tau}}\\end{array}$ . Rearranging the terms, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\nt\\geq\\left(\\frac{\\ln\\frac{(\\vert\\Pi^{*}\\vert-1)\\tau}{1-\\tau}}{\\sqrt{\\ln\\left\\vert\\Pi^{*}\\right\\vert\\cdot\\overline{{\\delta_{t}}}}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we seek a high probability upper bound on $\\overline{{\\delta}}_{t}$ . Denote $\\Delta_{i}\\triangleq\\mu_{i}-\\mu_{1}$ for $i\\in1,\\dots,|\\Pi^{*}|$ . We know ", "page_idx": 18}, {"type": "equation", "text": "$$\nP(\\overline{{\\delta}}_{t}\\leq\\Delta_{2}-\\epsilon)\\stackrel{\\mathrm{(a)}}{\\leq}P(\\overline{{\\delta}}_{t}\\leq\\Delta_{i^{\\prime}}-\\epsilon)=P(\\frac{1}{t}\\sum_{s=1}^{t}\\delta_{s}-\\Delta_{i^{\\prime}}\\leq-\\epsilon)\\stackrel{\\mathrm{(b)}}{\\leq}e^{-\\frac{t\\epsilon^{2}}{2b^{2}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, step (9a) is by the fact that $\\Delta_{2}=\\mathrm{min}_{i\\neq1}\\,\\Delta_{i}\\leq\\Delta_{i^{\\prime}}$ , and step (9b) is by Hoeffding\u2019s inequality where $b$ denotes the upper bound on $|\\delta_{s}|$ . Further note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{s+1}=\\tilde{\\ell}_{s,i^{\\prime}}-\\tilde{\\ell}_{s,1}=\\displaystyle\\frac{U_{s}}{z_{s}}\\langle\\pi_{i^{\\prime}}(\\mathbf{x}_{s})-\\pi_{1}(\\mathbf{x}_{s}),\\mathbb{I}\\left\\{\\hat{\\mathbf{y}}_{s}\\neq y_{s}\\right\\}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\langle\\pi_{i^{\\prime}}(\\mathbf{x}_{s}),\\mathbb{I}\\left\\{\\hat{\\mathbf{y}}_{s}\\neq y_{s}\\right\\}\\rangle}{z_{s}}}\\\\ &{\\qquad\\qquad\\qquad\\overset{\\mathrm{Eq.~}(4)}{\\leq}U_{s}\\frac{\\langle\\pi_{i^{\\prime}}(\\mathbf{x}_{s}),\\mathbb{I}\\left\\{\\hat{\\mathbf{y}}_{s}\\neq y_{s}\\right\\}\\rangle}{\\frac{1}{c}\\sum_{y\\in\\mathcal{Y}}\\langle\\boldsymbol{w}_{s},\\mathbb{I}\\left\\{\\hat{\\mathbf{y}}_{s}\\neq y\\right\\}\\rangle\\log_{c}\\frac{1}{\\langle\\boldsymbol{w}_{s},\\mathbb{I}\\left\\{\\hat{\\mathbf{y}}_{s}\\neq y\\right\\}\\rangle}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given $\\begin{array}{r l r}{p_{\\mathrm{min}}}&{{}=}&{\\operatorname*{min}_{s,i}\\pi(\\mathbf{x}_{s})}\\end{array}$ , we obtain $\\begin{array}{r l r}{\\delta_{s+1}}&{{}\\le}&{\\frac{1}{p_{\\mathrm{min}}\\log_{c}(1/p_{\\mathrm{min}})}}\\end{array}$ and similarly, $\\delta_{s+1}~~\\ge$ $\\begin{array}{r}{-\\frac{\\langle\\pi_{1}(\\mathbf{x}_{s}),\\mathbb{I}\\{\\hat{y}_{s}\\neq y_{s}\\}\\rangle}{z_{s}}\\geq-\\frac{1}{p_{\\mathrm{min}}\\log_{c}(1/p_{\\mathrm{min}})}}\\end{array}$ \u2212pmin logc1(1/pmin). We hence conclude that |\u03b4s+1| \u2264b. ", "page_idx": 18}, {"type": "text", "text": "Let $2e^{-\\frac{t\\epsilon^{2}}{2b^{2}}}=\\delta$ . Therefore, when $\\begin{array}{r}{t\\geq\\left(\\frac{\\ln\\frac{(|\\Pi^{*}|-1)\\tau}{1-\\tau}}{\\sqrt{\\ln|\\Pi^{*}|(\\Delta-\\epsilon)}}\\right)^{2}=\\left(\\frac{\\ln\\frac{(|\\Pi^{*}|-1)\\tau}{1-\\tau}}{\\sqrt{\\ln|\\Pi^{*}|}\\left(\\Delta-\\sqrt{\\frac{2b^{2}}{t}\\ln\\frac{2}{\\delta}}\\right)}\\right)^{2}}\\end{array}$ , it holds that $q_{t,i^{*}}\\geq\\tau$ with probability at least $1-\\delta$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 9. At round $t$ , when $\\begin{array}{r}{t\\,\\geq\\,\\left(\\frac{\\ln\\frac{|\\Pi^{*}|-1}{\\gamma}+\\sqrt{\\ln|\\Pi^{*}|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}}{\\sqrt{\\ln|\\Pi^{*}|\\Delta}}\\right)^{2}}\\end{array}$ , it holds that the arm chosen by the best policy $i^{*}$ will be the arm chosen by Algorithm $I$ with probability at least $1-\\delta$ . That is, arg max $\\begin{array}{r}{\\left\\{\\sum_{i\\in[|\\Pi^{*}|]}q_{t,i}\\pi_{i}(\\mathbf{x}_{t})\\right\\}=\\arg\\operatorname*{max}\\left\\{\\pi_{i^{*}}(\\mathbf{x}_{t})\\right\\}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 9. At round t, for Algorithm 1, we have loss $\\begin{array}{r}{\\sum_{j=1}^{k}\\mathbb{I}\\left\\{j=\\arg\\operatorname*{max}\\left\\{\\sum_{i\\in[|\\Pi^{*}|]}q_{t,i}\\pi_{i}(\\pmb{x}_{t})\\right\\}\\right\\}\\widehat{\\ell}_{t,j}}\\end{array}$ . Let $q_{t,i^{*}}\\mathrm{~\\ensuremath~{~\\geq~}~}\\tau$ . At round $t$ , the best policy $i^{*}$ \u2019s top weight arm $j_{t,i^{*}}$ \u2019s probability max $\\{\\pi_{i^{*}}(\\pmb{x}_{t})\\}$ is at least $\\frac{1}{k}$ . The second rank probability of $\\pi_{i^{*}}(\\pmb{x}_{t})$ is $\\begin{array}{r}{\\operatorname*{max}_{j}\\left[\\pi_{i^{*}}\\left(\\pmb{x}_{t}\\right)\\right]_{j\\neq\\operatorname*{maxind}\\left(\\pi_{i^{*}}\\left(\\pmb{x}_{t}\\right)\\right)}.}\\end{array}$ Let us define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\gamma:=\\underset{x_{t}}{\\operatorname*{min}}\\left\\lbrace\\underset{w_{j}\\in w_{i^{*}}^{t}}{\\operatorname*{max}}\\;w_{j}-\\underset{w_{j}\\in w_{i^{*}}^{t},j\\neq\\operatorname*{max}\\lbrace n\\rbrace}{\\operatorname*{max}}w_{j}\\right\\rbrace}\\\\ &{=\\operatorname*{max}\\left\\lbrace\\pi_{i^{*}}\\left(\\pmb{x}_{t}\\right)\\right\\rbrace-\\underset{j}{\\operatorname*{max}}\\left\\lbrace\\big[\\pi_{i^{*}}\\left(\\pmb{x}_{t}\\right)\\big]_{j\\neq\\operatorname*{max}\\lbrace n\\rbrace}(\\pi_{i^{*}}\\left(\\pmb{x}_{t}\\right))\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as the minimal gap in model distribution space of best policy. The arm recommended by the best policy $i^{*}$ of CAMS will dominate CAMS\u2019s selection, when we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nq_{t,i^{*}}\\cdot\\operatorname*{max}\\left\\{\\pi_{i^{**}}(\\pmb{x}_{t})\\right\\}\\geq(1-q_{t,i^{*}})+q_{t,i^{*}}\\left(\\operatorname*{max}_{j}\\left[\\pi_{i^{*}}\\left(\\pmb{x}_{t}\\right)\\right]_{j\\neq\\operatorname*{maxind}\\left(\\pi_{i^{*}}\\left(\\pmb{x}_{t}\\right)\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Rearranging the terms, and by ", "page_idx": 19}, {"type": "equation", "text": "$$\nq_{t,i^{*}}\\cdot\\gamma\\stackrel{\\mathrm{Eq.}\\left(10\\right)}{=}q_{t,i^{*}}\\left(\\operatorname*{max}\\left\\{\\pi_{i^{*}}(\\mathbf{x}_{t})\\right\\}-\\operatorname*{max}_{j}\\left[\\pi_{i^{*}}\\left(\\mathbf{x}_{t}\\right)\\right]_{j\\neq\\operatorname*{maxind}\\left(\\pi_{i^{*}}\\left(\\mathbf{x}_{t}\\right)\\right)}\\right)\\geq\\left(1-q_{t,i^{*}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we get $\\tau\\cdot(\\gamma)\\geq(1-\\tau)$ , and thus $\\begin{array}{r}{\\tau\\geq\\frac{1}{\\gamma+1}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Set $\\begin{array}{r}{\\tau\\geq\\frac{1}{\\gamma+1}}\\end{array}$ . By Lemma 8, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t\\geq\\left(\\frac{\\ln\\frac{|\\Pi^{*}-1|\\tau}{1-\\tau}}{\\sqrt{\\ln|\\Pi^{*}|}\\left(\\Delta-\\epsilon\\right)}\\right)^{2}}\\\\ &{\\ \\ \\geq\\left(\\frac{\\ln\\left(\\frac{|\\Pi^{*}|-1}{\\gamma}\\right)}{\\sqrt{\\ln|\\Pi^{*}|}\\left(\\Delta-\\epsilon\\right)}\\right)^{2}}\\\\ &{\\ \\ {\\overset{(c)}{\\geq}}\\left(\\frac{\\ln\\frac{|\\Pi^{*}|-1}{\\gamma}}{\\sqrt{\\ln|\\Pi^{*}|}\\Delta-\\sqrt{\\ln|\\Pi^{*}|\\cdot\\frac{2b^{2}}{t}\\ln\\frac{2}{\\delta}}}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last step is by applying $2e^{-\\frac{t\\epsilon^{2}}{2b^{2}}}=\\delta$ , thus, $\\begin{array}{r}{\\epsilon=\\sqrt{\\frac{2b^{2}}{t}\\ln\\frac{2}{\\delta}}}\\end{array}$ . Dividing both sides by $t$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad1\\stackrel{\\mathrm{(d)}}{\\geq}\\left(\\frac{\\ln\\left|\\frac{\\ln^{\\ast}\\left|-1\\right|}{\\gamma}\\right|}{\\sqrt{\\ln\\left|\\Pi^{\\ast}\\right|\\cdot t\\Delta-\\sqrt{\\ln\\left|\\Pi^{\\ast}\\right|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}\\right)^{2}}}\\right)^{2}}\\\\ &{\\ln\\frac{\\left|\\Pi^{\\ast}\\right|-1}{\\gamma}\\leq\\sqrt{t}\\sqrt{\\ln\\left(|\\Pi^{\\ast}|\\right)}\\Delta-\\sqrt{\\ln\\left(|\\Pi^{\\ast}|\\right)\\cdot2b^{2}\\ln\\frac{2}{\\delta}}}\\\\ &{\\qquad\\qquad t\\geq\\left(\\frac{\\ln\\left|\\frac{\\mathbb{H}^{\\ast}\\right|-1}{\\gamma}+\\sqrt{\\ln\\left|\\Pi^{\\ast}\\right|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}}{\\sqrt{\\ln\\left|\\Pi^{\\ast}\\right|\\Delta}}\\right)^{2}.}\\\\ &{\\qquad\\qquad\\qquad+\\sqrt{\\ln\\left|\\frac{\\mathbb{H}^{\\ast}\\left|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}{\\sqrt{\\ln\\left|\\Pi^{\\ast}\\right|\\Delta}}\\right)^{2}},\\,\\,\\mathrm{if}\\,\\,\\mathrm{holds}\\,\\,\\mathrm{that}\\,\\,\\mathrm{arg}\\operatorname*{max}\\left\\{\\sum_{i\\in[\\left|\\Pi^{\\ast}\\right|\\,q_{i},\\bar{\\pi}_{i}(x_{i})\\right|)}\\right\\}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So, when arg max $\\{\\pi_{i^{*}}(\\pmb{x}_{t})\\}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 1. Therefore, with probability at least $1\\ -\\ \\delta$ , we get constant regret $\\left(\\frac{\\ln\\frac{|\\Pi^{*}|-1}{\\gamma}+\\sqrt{\\ln|\\Pi^{*}|\\!\\cdot\\!2b^{2}\\ln\\frac{2}{\\delta}}}{\\sqrt{\\ln|\\Pi^{*}|\\Delta}}\\right)^{2}.$ ", "page_idx": 19}, {"type": "text", "text": "Furthermore, with probability at most $\\delta$ , the regret is upper bounded by $T$ . Thus, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathcal{R}}}\\left(T\\right)\\leq\\left(1-\\delta\\right)\\left(\\frac{\\ln\\frac{\\left|\\Pi^{*}\\right|-1}{\\gamma}+\\sqrt{\\ln\\left|\\Pi^{*}\\right|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}}{\\sqrt{\\ln\\left|\\Pi^{*}\\right|\\Delta}}\\right)^{2}+\\delta T}\\\\ &{\\qquad\\stackrel{(a)}{\\leq}\\left(1-\\frac{1}{T}\\right)\\left(\\frac{\\ln\\frac{\\left|\\Pi^{*}\\right|-1}{\\gamma}+b\\sqrt{\\ln\\left|\\Pi^{*}\\right|\\cdot\\left(2\\ln T+2\\ln2\\right)}}{\\sqrt{\\ln\\left|\\Pi^{*}\\right|\\Delta}}\\right)^{2}+1}\\\\ &{\\qquad=O\\left(\\frac{b\\ln T}{\\Delta^{2}}+\\left(\\frac{\\ln\\frac{\\left|\\Pi^{*}\\right|-1}{\\gamma}}{\\sqrt{\\ln\\left|\\Pi^{*}\\right|\\Delta}}\\right)^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "E.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we analyze the query complexity of CAMS in the stochastic setting, where we take a similar approach as proposed by Karimi et al. [39] for the context-free model selection problem. Our main idea is to derive from query indicator $U_{t}$ and query probability $z_{t}$ . We first used Lemma 10 to bound the expected number of queries $\\textstyle\\sum_{t=1}^{T}U_{t}$ by the sum of query probability as $\\begin{array}{r}{\\sum_{t=1}^{T}\\delta_{0}^{t}+\\sum_{t=1}^{T}\\mathfrak{E}\\left(\\hat{\\pmb{y}}_{t},\\pmb{w}_{t}\\right)}\\end{array}$ . Then we used emma 11  to bound the first item (which corresponds to the lower bound of query probability over $T$ rounds) and applied Lemma 12 to bound the second term (which characterizes the model disagreement). Finally, we combined the upper bounds on the two parts to reach the desired result. ", "page_idx": 20}, {"type": "text", "text": "Lemma 10. The query complexity of Algorithm $^{l}$ is upper bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\frac{1}{\\sqrt{t}}+\\frac{\\sum_{y\\in\\mathcal{Y}}\\langle\\mathbf{w}_{t},\\ell_{t}^{y}\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{\\langle\\mathbf{w}_{t},\\ell_{t}^{y}\\rangle}}{|\\mathcal{Y}|}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Now we have model disagreement defined in Eq. (3), the query probability defined in Eq. (4), and the query indicator $U$ . Let us assume, at each round, we have query probability $z_{t}>0$ , which indicates we will not process the instance that all the models\u2019 prediction are the same. ", "page_idx": 20}, {"type": "text", "text": "At round $t$ , from query probability Eq. (4), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{t}=\\operatorname*{max}\\left\\{\\delta_{0}^{t},\\mathfrak{E}\\left(\\hat{\\mathbf{y}}_{t},\\pmb{w}_{t}\\right)\\right\\}}\\\\ &{\\quad\\leq\\delta_{0}^{t}+\\mathfrak{E}\\left(\\hat{\\mathbf{y}}_{t},\\pmb{w}_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the inequality is by applying that $\\forall A,B\\geq0,\\operatorname*{max}\\{A,B\\}\\leq A+B.$ ", "page_idx": 20}, {"type": "text", "text": "Thus, in total round $T$ , we could get the following equation as the cumulative query cost, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}U_{t}\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\frac{1}{\\sqrt{t}}+\\frac{\\sum_{y\\in\\mathcal{Y}}\\langle w_{t},\\ell_{t}^{y}\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{\\langle w_{t},\\ell_{t}^{y}\\rangle}}{|\\mathcal{Y}|}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the inequality is by inputting $\\begin{array}{r}{\\delta_{0}^{t}=\\frac{1}{\\sqrt{t}}}\\end{array}$ and Eq. (3). ", "page_idx": 20}, {"type": "text", "text": "Proof. We can bound the LHS as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\frac{1}{\\sqrt{t}}=\\sum_{t=1}^{\\lfloor\\sqrt{T}\\rfloor}\\displaystyle\\frac{1}{\\sqrt{t}}+\\sum_{t=\\lfloor\\sqrt{T}\\rfloor+1}^{T}\\displaystyle\\frac{1}{\\sqrt{t}}}\\\\ &{\\qquad\\quad\\le\\sqrt{T}+\\displaystyle\\sum_{t=\\lfloor\\sqrt{T}\\rfloor+1}^{T}\\displaystyle\\frac{1}{\\sqrt{T}}}\\\\ &{\\qquad=\\sqrt{T}+\\left(T-\\sqrt{T}\\right)\\displaystyle\\frac{1}{\\sqrt{T}}}\\\\ &{\\qquad\\quad\\le2\\sqrt{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 12. Denote the true label at round $t$ by $y_{t}$ , and define $\\begin{array}{r}{p_{t,y}\\;:=\\;\\sum_{j\\in[k]}\\mathbb{I}\\left\\{\\hat{y}_{t,j}=y\\right\\}w_{j}}\\end{array}$ . Further define $\\begin{array}{r}{R_{t}:=\\sum_{t}1-p_{t,y_{t}}}\\end{array}$ as the expected cumulative loss of Algorithm $I$ at $t$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{\\sum_{y\\in\\mathcal{Y}}\\langle\\mathbf{w}_{t},\\ell_{t}^{y}\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{\\langle\\mathbf{w}_{t},\\ell_{t}^{y}\\rangle}}{|\\mathcal{Y}|}\\leq\\frac{R_{T}\\cdot\\left(\\log_{|\\mathcal{Y}|}\\frac{T^{2}(|\\mathcal{Y}|-1)}{R_{T}^{2}}\\right)}{|\\mathcal{Y}|}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma $^{12}$ . Suppose at round $t$ , the true label is $y_{t}$ . $\\begin{array}{r}{\\sum_{y\\ne y_{t}}p_{t,y}\\;=\\;1\\,-\\,p_{t,y_{t}}\\;=\\;1\\,-\\,}\\end{array}$ $\\begin{array}{r}{\\left\\langle\\sum_{i\\in|\\Pi^{*}|}q_{t,i}\\pi_{i}(\\pmb{x}_{t}),\\pmb{\\ell}_{t}\\right\\rangle=r_{t},}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\sum_{y\\in\\mathcal{Y}}\\langle w_{t},\\ell_{t}^{(\\ell)}\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{|w_{t},\\ell_{t}^{(\\ell)}|}}{|\\mathcal{Y}|}=\\frac{(1-p_{t,y_{t}})\\log_{|\\mathcal{Y}|}\\frac{1}{1-p_{t,y_{t}}}}{|\\mathcal{Y}|}+\\frac{\\sum_{y\\notin\\mathcal{Y}_{y}}(1-p_{t,y})\\log_{|\\mathcal{Y}|}\\frac{1}{1-p_{t,y_{t}}}}{|\\mathcal{Y}|}}&{}\\\\ {\\overset{(a)}{\\leq}\\frac{(1-p_{t,y_{t}})\\log_{|\\mathcal{Y}|}\\frac{1}{1-p_{t,y_{t}}}}{|\\mathcal{Y}|}+(|y|-1)\\frac{\\frac{(1-p_{t,y_{t}})}{|\\mathcal{Y}|-1}\\log_{|\\mathcal{Y}|}\\frac{|y|-1}{|y_{t}-y_{t,y_{t}}}}{|\\mathcal{Y}|}}&{}\\\\ {\\leq\\frac{(1-p_{t,y_{t}})\\log_{|\\mathcal{Y}|}\\frac{1}{|y|}-\\frac{1}{p_{t,y_{t}}}}{|\\mathcal{Y}|}+\\frac{(1-p_{t,y_{t}})\\log_{|\\mathcal{Y}|}\\frac{|y|-1}{1-p_{t,y_{t}}}}{|\\mathcal{Y}|}}&{}\\\\ {=\\frac{(1-p_{t,y_{t}})\\log_{|\\mathcal{Y}|}\\frac{|y|-1}{(1-p_{t,y_{t}})^{2}}}{|\\mathcal{Y}|}}&{}\\\\ {\\overset{(b)}{\\leq}\\frac{p_{t}\\log_{|\\mathcal{Y}|}\\frac{|y|-1}{|y|}}{|\\mathcal{Y}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "rwehplearcei sntge tph $(a)$ xips ebctye ad plpolsysi nb\u2019sy  iitnse qshuoalritt-yh aanndd  nuostiantigo $\\begin{array}{r}{1-p_{t,y}=\\frac{1-p_{t,y_{t}}}{|y|-1}}\\end{array}$ , and step $(b)$ is by $1-p_{t,y_{t}}$ $r_{t}$ ", "page_idx": 21}, {"type": "text", "text": "Recall that we define the expected cumulative loss as $\\begin{array}{r}{R_{T}\\,=\\,\\sum_{t=1}^{T}r_{t}}\\end{array}$ . Since when $r_{t}\\,\\in\\,[0,1]$ $\\frac{r_{t}\\log_{|\\mathcal{V}|}\\frac{|\\mathcal{V}|-1}{r_{t}^{2}}}{|\\mathcal{V}|}$ is concave, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{\\sum_{y\\in y}\\langle w_{t},\\ell_{t}^{y}\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{\\langle w_{t},\\ell_{t}^{y}\\rangle}}{|\\mathcal{Y}|}\\leq\\frac{T\\left(\\frac{\\sum r_{t}}{T}\\right)\\left(\\log_{|\\mathcal{Y}|}\\frac{|y|-1}{\\frac{\\sum r_{t}\\sum r_{t}}{T}}\\right)}{|\\mathcal{Y}|}=\\frac{R_{T}\\left(\\log_{|\\mathcal{Y}|}\\frac{T^{2}(|\\mathcal{Y}|-1)}{R_{T}^{2}}\\right)}{|\\mathcal{Y}|}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $R_{T}$ is the cumulative loss up to round $T$ , $T$ \u2019s incremental rate is no less than $R_{T}$ \u2019s incremental rate. Thus, $R_{T}\\leq T$ and $\\begin{array}{r}{\\frac{T_{t}}{R_{t}}\\leq\\frac{\\bar{T_{t+1}}}{R_{t+1}}}\\end{array}$ \u2264 TRt+1 . So we get Eq. (14). \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Now we are ready to prove Theorem 2. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 2. From Lemma 10, we get the following equation as the cumulative query cost ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}U_{t}\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\frac{1}{\\sqrt{t}}+\\frac{\\sum_{y\\in\\mathcal{Y}}\\langle w_{t},\\ell_{t}^{y}\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{\\langle w_{t},\\ell_{t}^{y}\\rangle}}{|\\mathcal{Y}|}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let us assume the expected total loss of best policy is $T\\mu_{i^{*}}$ . From Theorem 1, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{T}\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}r_{t}\\right]\\leq\\left(\\frac{\\ln\\frac{|\\Pi^{*}|-1}{\\gamma}+\\sqrt{\\ln\\left|\\Pi^{*}\\right|\\cdot2b^{2}\\ln\\frac{2}{\\delta}}}{\\sqrt{\\ln\\left|\\Pi^{*}\\right|\\Delta}}\\right)^{2}+T\\mu_{i^{*}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plugging this result into the query complexity bound given by Lemma $11$ and Lemma 12, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon\\left[\\displaystyle\\sum_{t=1}^{T}U_{t}\\right]\\leq2\\sqrt{T}+\\frac{\\left(\\left(\\frac{\\ln\\frac{|\\mathbf{x}+1-\\lambda_{\\mathbf{t}}|}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}\\ln\\frac{\\hat{\\mathbf{t}}}{t}}\\right)^{2}}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}}\\right)^{2}+T\\mu_{t+\\varepsilon}}{|\\nabla|\\cdot\\left(\\Delta^{2}\\right)}\\log_{\\frac{1}{T})|}\\frac{T^{2}\\left(|\\mathcal{Y}|-1\\right)}{\\left(\\left(\\frac{\\ln\\frac{|\\mathbf{x}+1-\\lambda_{\\mathbf{t}}|}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}\\ln\\frac{\\hat{\\mathbf{t}}}{t}}\\right)}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}}\\right)}}\\\\ &{\\leq\\frac{\\left(\\left(\\frac{\\ln\\frac{|\\mathbf{x}+1-\\lambda_{\\mathbf{t}}|}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}\\ln\\frac{\\hat{\\mathbf{t}}}{t}}\\right)^{2}}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}}\\right)^{2}+T\\mu_{t+\\varepsilon}\\left(\\log_{\\|\\mathcal{Y}\\|}(T)|\\right)}{|\\nabla|\\cdot\\left(\\frac{\\ln\\frac{|\\mathbf{x}-1}{t}}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}}\\right)^{2}+T\\mu_{t+\\varepsilon}\\right)\\ln\\left(T\\right)}}\\\\ &{=\\frac{\\left(\\left(\\frac{\\ln\\frac{|\\mathbf{x}+1-\\sqrt{n}\\ln|\\mathcal{Y}|}{\\sqrt{n}\\ln|\\mathcal{Z}|}\\right)^{2}}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}}+T\\mu_{t+\\varepsilon}\\right)\\ln\\left(T\\right)}{|\\nabla|\\ln\\left(\\frac{|\\mathbf{x}-1-\\mathbf{t}|}{\\sqrt{n}\\ln|\\mathcal{Z}|}\\right)^{2}+T\\mu_{t}\\right)\\ln\\left(T\\right)}}\\\\ &{\\overset{(a)}{=}\\frac{\\sqrt{\\left(\\frac{\\ln\\frac{|\\mathbf{x}+1-\\lambda_{\\mathbf{t}}|}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}\\ln\\frac{\\hat{\\mathbf{t}}}{t}}\\right)^{2}}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}}+T\\mu_{t}\\right)\\ln\\left(T\\right)}{\\sqrt{n}\\ln|\\Gamma|\\cdot\\Delta^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\gamma$ is defined as Eq. (10) and step (a) by applying $c=|y|$ . ", "page_idx": 22}, {"type": "text", "text": "F Proofs for the Adversarial Setting ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we first prove the regret bound presented in Theorem 3 and then prove the query complexity bound presented in Theorem 4 for Algorithm 1 in the adversarial setting. Lemma 13 builds upon the proof of the hedge algorithm [27], but with an adaptive learning rate. ", "page_idx": 22}, {"type": "text", "text": "F.1 Proof of Theorem 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 13. Consider the setting of Algorithm 1, Let us define $h_{t,i}=\\exp\\left(-\\eta_{t}\\tilde{L}_{t-1,i}\\right)\\forall i\\in|\\Pi^{*}|$ as exponential cumulative loss of policy i, $\\eta_{t}$ is the adaptive learning rate and $\\mathbf{q}_{t}$ is the probability distribution of policies, then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\log\\frac{\\sum_{i\\in[|\\Pi^{*}|]}h_{T+1,i}}{\\sum_{i\\in[|\\Pi^{*}|]}h_{1,i}}\\leq-\\sum_{t=1}^{T}\\eta_{t}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}+\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We first bound the following term ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underbrace{\\sum_{i\\in[[\\Pi^{*}]]}h_{t+1,i}}_{\\geq_{i\\in[[\\Pi^{*}]]}}=\\sum_{i=1}^{|\\Pi^{*}|}\\underbrace{h_{t+1,i}}_{\\geq_{i\\in[[\\Pi^{*}]]}}}\\\\ &{=\\displaystyle\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\exp\\left(-\\eta_{t}\\tilde{\\ell}_{t,i}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\left(1-\\eta_{t}\\tilde{\\ell}_{t,i}+\\frac{\\eta_{t}^{2}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2}}{2}\\right)}\\\\ &{=1-\\eta_{t}\\displaystyle\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}+\\frac{\\eta_{t}^{2}\\left[\\Pi^{*}\\right]}{2}q_{t,i}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality is by applying that for $x\\le0$ , we have $\\begin{array}{r}{e^{x}\\leq1+x+\\frac{x^{2}}{2}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "By taking log on both side, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\frac{\\sum_{i\\in[|\\Pi^{*}|]}h_{t+1,i}}{\\sum_{i\\in[|\\Pi^{*}|]}h_{t,i}}\\leq\\log\\left(1-\\eta_{t}\\displaystyle\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}+\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2}\\right)}\\\\ &{\\overset{(a)}{\\leq}-\\eta_{t}\\displaystyle\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}+\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where step $(a)$ is by applying that $\\log\\left(1+x\\right)\\leq x$ , when $x\\geq-1$ . ", "page_idx": 23}, {"type": "text", "text": "Now summing over $t=1:T$ yields: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\frac{\\sum_{i\\in[|\\Pi^{*}|]}h_{T+1,i}}{\\sum_{i\\in[|\\Pi^{*}|]}h_{1,i}}=\\displaystyle\\sum_{t=1}^{T}\\log\\frac{\\sum_{i\\in[|\\Pi^{*}|]}h_{t+1,i}}{\\sum_{i\\in[|\\Pi^{*}|]}h_{t,i}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq-\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\displaystyle\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}+\\displaystyle\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 14. Consider the setting of Algorithm 1. Let $\\begin{array}{r}{p_{t,y}\\,=\\,\\sum_{j\\in[k]}{\\mathbb{I}\\left\\{{\\hat{y}_{t,j}=y}\\right\\}w_{j}}}\\end{array}$ . The query probability $z_{t}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\nz_{t}\\geq\\frac{1}{|\\mathcal{Y}|\\ln|\\mathcal{Y}|}\\left(p_{t,y_{t}}\\left(1-p_{t,y_{t}}\\right)+p_{t,y}\\left(1-p_{t,y}\\right)\\right),\\forall y\\neq y_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We first bound the query probability term ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tau_{i}=\\operatorname*{max}\\{\\delta_{0}^{i},\\displaystyle\\frac{\\mathfrak{s}(\\hat{\\psi}_{i},\\psi_{i},\\psi_{i})}{\\sqrt{\\mathcal{V}_{i}}\\displaystyle\\left|\\nabla_{y}\\varphi\\right|}\\log_{\\|\\mathcal{S}\\|}\\frac{1}{\\left|\\mathfrak{w}_{i},\\psi_{i}\\right|}\\}}&{}\\\\ {=\\operatorname*{max}\\{\\delta_{0}^{i},\\displaystyle\\frac{1}{|\\mathcal{V}_{i}|}\\sum_{y\\in\\mathcal{Y}}\\left(1-p_{t,y}\\right)\\cdot\\ln\\frac{1}{1-p_{t,y}}\\frac{1}{\\ln{|\\mathcal{Y}|}}\\}}&{}\\\\ {\\overset{(a)}{\\geq}\\operatorname*{max}\\{\\delta_{0}^{i},\\displaystyle\\frac{1}{|\\mathcal{V}_{i}|}\\sum_{y\\in\\mathcal{Y}}\\left(1-p_{t,y}\\right)\\cdot p_{t,y}\\cdot\\frac{1}{\\ln{|\\mathcal{Y}|}}\\}}&{}\\\\ {=\\operatorname*{max}\\{\\delta_{0}^{i},\\displaystyle\\frac{1}{|\\mathcal{V}|}\\ln\\left|\\mathcal{Y}\\right|\\sum_{y\\in\\mathcal{Y}}\\left(1-p_{t,y}\\right)\\cdot p_{t,y}\\}}&{}\\\\ {\\overset{(b)}{\\geq}\\frac{1}{|\\mathcal{V}_{i}|\\ln{|\\mathcal{Y}|}}\\left(p_{t,y_{t}}\\left(1-p_{t,y_{t}}\\right)+p_{t,y}\\left(1-p_{t,y}\\right)\\right),\\forall y\\neq y_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where step $(a)$ is by applying $\\begin{array}{r}{\\ln\\left(1+x\\right)\\geq\\frac{x}{1+x}}\\end{array}$ for $x>-1$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ln{\\frac{1}{1-p_{t,y}}}=\\ln\\left(1+{\\frac{p_{t,y}}{1-p_{t,y}}}\\right)\\geq{\\frac{\\frac{p_{t,y}}{1-p_{t,y}}}{\\frac{1}{1-p_{t,y}}}}=p_{t,y},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and where step $(b)$ is by applying $\\forall a,b\\in\\mathbb{R}$ , max $\\{a,b\\}\\geq a$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 3. By applying Lemma 13, we got ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\log\\frac{\\sum_{i\\in[|\\Pi^{*}|]}h_{T+1,i}}{\\sum_{i\\in[|\\Pi^{*}|]}h_{1,i}}\\leq-\\sum_{t=1}^{T}\\eta_{t}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}+\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any policy $s$ , we have a lower bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\frac{\\sum_{i\\in[|\\Pi^{*}|]}h_{T+1,i}}{\\sum_{i\\in[|\\Pi^{*}|]}h_{1,i}}\\geq\\log\\frac{h_{T+1,s}}{\\sum_{i\\in[|\\Pi^{*}|]}h_{1,i}}}&{}\\\\ {\\ }&{\\stackrel{(a)}{=}\\log\\frac{h_{T+1,s}}{|\\Pi^{*}|}}\\\\ {\\ }&{=-\\log\\left(n+k\\right)\\!-\\!\\eta_{T}\\sum_{t=1}^{T}\\widetilde{\\ell}_{t,s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where step $(a)$ in Eq. (15) is by initializing $\\widetilde{\\cal L}_{0}=0$ , $e^{0}=1$ , and $\\begin{array}{r}{\\sum_{i\\in[|\\Pi^{*}|]}h_{1}=e^{\\left(-\\eta_{t}\\widetilde{L}_{0}\\right)}=|\\Pi^{*}|}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle-\\sum_{t=1}^{T}\\eta_{t}\\sum_{i=1}^{\\lfloor\\pi\\rfloor-1}q_{t,i}\\tilde{\\ell}_{t,i}+\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{\\lfloor\\pi\\rfloor^{*}\\rfloor}q_{t,i}\\left(\\tilde{\\ell}_{t,i}\\right)^{2}\\ge-\\log\\left(n+k\\right)-\\eta_{T}\\sum_{t=1}^{T}\\tilde{\\ell}_{t,s}}\\\\ &{\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\sum_{i=1}^{\\lfloor\\pi\\rfloor}q_{t,i}\\tilde{\\ell}_{t,i}-\\eta_{T}\\sum_{t=1}^{T}\\tilde{\\ell}_{t,s}\\le\\log\\left(n+k\\right)+\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}\\prod^{*}}{2}q_{t,i}\\left(\\tilde{\\ell}_{t,i}\\right)^{2}}\\\\ &{\\displaystyle\\eta_{T}\\sum_{t=1}^{T}\\sum_{i=1}^{\\lfloor\\pi\\rfloor}q_{t,i}\\tilde{\\ell}_{t,i}-\\eta_{T}\\sum_{t=1}^{T}\\tilde{\\ell}_{t,s}\\overset{(b)}{\\le}\\log\\left(n+k\\right)+\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{\\lfloor\\pi\\rfloor}q_{t,i}\\left(\\tilde{\\ell}_{t,i}\\right)^{2}}\\\\ &{\\displaystyle\\quad\\sum_{t=1}^{T}\\sum_{i=1}^{\\lfloor\\pi\\rfloor}q_{t,i}\\tilde{\\ell}_{t,i}-\\sum_{t=1}^{T}\\widetilde{\\ell}_{t,s}\\overset{(c)}{\\le}\\frac{\\log\\left\\vert\\prod^{*}\\right\\vert}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}\\prod^{*}}{2}q_{t,i}\\left(\\tilde{\\ell}_{t,i}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where step $(b)$ is by applying ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\eta_{T}\\sum_{t=1}^{T}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}-\\eta_{T}\\sum_{t=1}^{T}\\widetilde{\\ell}_{t,s}\\leq\\sum_{t=1}^{T}\\eta_{t}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}-\\eta_{T}\\sum_{t=1}^{T}\\widetilde{\\ell}_{t,s},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and step $(c)$ is by dividing $\\eta_{T}$ on both side. ", "page_idx": 24}, {"type": "text", "text": "Because we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{T}\\bigg[q_{t,i}\\left(\\widetilde{\\ell}_{t,i}\\right)^{2}\\bigg]=q_{t,i}\\mathbb{E}_{T}\\bigg[\\Big(\\pi_{i}\\left(x_{t}\\right)\\cdot\\widehat{\\ell}_{t}\\Big)^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=q_{t,i}\\left(P\\left(U_{t}=1\\right)\\left(\\pi_{i}\\left(x_{t}\\right)\\cdot\\frac{\\ell_{t}}{\\widehat{\\lambda}_{t}}\\right)^{2}+P\\left(U_{t}=0\\right)\\cdot0\\right)}\\\\ &{\\qquad\\qquad\\quad=q_{t,i}\\left(z_{t}\\left(\\pi_{i}\\left(x_{t}\\right)\\cdot\\frac{\\ell_{t}}{\\widehat{\\lambda}_{t}}\\right)^{2}\\right)}\\\\ &{\\qquad\\qquad\\quad=\\frac{q_{t,i}}{\\widehat{\\ell}_{t}}\\left(\\pi_{i}\\left(x_{t}\\right)\\cdot\\ell_{t}\\right)^{2}}\\\\ &{\\qquad\\qquad\\quad\\leq\\frac{q_{t,i}}{\\widehat{\\ell}_{t}}\\pi_{i}\\left(x_{t}\\right)\\cdot\\ell_{t}}\\\\ &{\\qquad\\qquad\\quad=\\frac{q_{t,i}}{\\widehat{\\ell}_{t}}\\pi_{i}\\left(\\pi_{i}\\left(x_{t}\\right),\\ell_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "it leads to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\widetilde{\\ell}_{t,i}-\\sum_{t=1}^{T}\\widetilde{\\ell}_{t,s}\\leq\\frac{\\log|\\Pi^{*}|}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\sum_{i=1}^{|\\Pi^{*}|}\\frac{q_{t,i}}{z_{t}}\\langle\\pi_{i}\\left(\\pmb{x}_{t}\\right),\\ell_{t}\\rangle}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\overset{(d)}{\\leq}\\frac{\\log|\\Pi^{*}|}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\frac{\\langle\\pmb{w}_{t},\\ell_{t}\\rangle}{z_{t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where step $(d)$ is by applying $\\begin{array}{r}{\\sum_{i=1}^{|\\Pi^{*}|}q_{t,i}\\langle\\pi_{i}\\left(\\pmb{x}_{t}\\right),\\pmb{\\ell}_{t}\\rangle=\\langle\\pmb{w}_{t},\\pmb{\\ell}_{t}\\rangle}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "So we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{[\\Pi^{*}]}q_{t,i}\\tilde{\\ell}_{t,i}-\\displaystyle\\sum_{t=1}^{T}\\widetilde{\\ell}_{t,s}\\leq\\frac{\\log|\\Pi^{*}|}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\displaystyle\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\frac{\\langle w_{t},\\ell_{t}\\rangle}{z_{t}}}\\\\ {\\displaystyle}&{\\stackrel{(e)}{\\leq}\\frac{\\log|\\Pi^{*}|}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\displaystyle\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\frac{1-p_{t,y_{t}}}{z_{t}}}\\\\ {\\displaystyle}&{\\stackrel{(f)}{\\leq}\\frac{\\log|\\Pi^{*}|}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\displaystyle\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\frac{1-p_{t,y_{t}}}{\\mathcal{Y}_{0}\\left((1-p_{t,y_{t}})p_{t,y_{t}}+(1-p_{t,y})p_{t,y}\\right)}}\\\\ {\\displaystyle}&{\\leq\\frac{\\log|\\Pi^{*}|}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\displaystyle\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}}{2}\\frac{1}{\\mathcal{Y}_{0}\\left(p_{t,y_{t}}+\\frac{1-p_{t,y_{t}}}{1-p_{t,y_{t}}}p_{t,y}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where step $(e)$ is by using $\\langle\\pmb{w}_{t},\\pmb{\\ell}_{t}\\rangle=1-p_{t,y_{t}}$ and step $(f)$ by using Lemma 14 and get lower bound of $z_{t}$ as $\\begin{array}{r}{\\frac{1}{|\\mathcal{y}|\\ln|\\mathcal{y}|}\\left(p_{t,y_{t}}\\left(1-p_{t,y_{t}}\\right)+p_{t,y}\\left(1-p_{t,y}\\right)\\right)}\\end{array}$ and applying $\\begin{array}{r}{\\frac{1}{|\\mathcal{D}|\\ln|\\mathcal{D}|}=\\mathcal{V}_{0}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "If pt,yt \u2265 |Y1|, ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{t,y_{t}}+\\frac{1-p_{t,y}}{1-p_{t,y_{t}}}p_{t,y}\\geq\\frac{1}{|\\mathcal{V}|}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If $\\begin{array}{r}{p_{t,y_{t}}\\,<\\,\\frac{1}{|\\mathcal{D}|}}\\end{array}$ , $\\exists y,p_{t,y}\\,\\to\\,1$ , $\\delta_{1}^{t}\\,=\\,1\\,-\\,\\mathrm{max}_{y,\\tau\\in[t]}\\,p_{\\tau,y}$ . Let $p_{t,\\hat{y}}\\,=\\,\\operatorname*{max}_{y}p_{t,y}$ . Thus, we have $\\begin{array}{r}{w_{\\hat{y}}>\\frac{1}{|\\mathcal{y}|}}\\end{array}$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{t,y_{t}}+\\frac{1-p_{t,y}}{1-p_{t,y_{t}}}p_{t,y}\\geq p_{t,y_{t}}+w_{\\hat{y}}\\frac{\\delta_{1}^{t}}{1-p_{t,y_{t}}}\\geq0+\\frac{1}{|\\mathcal{Y}|}\\frac{\\delta_{1}^{t}}{1}=\\frac{\\delta_{1}^{t}}{|\\mathcal{Y}|}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{p_{t,y_{t}}+p_{t,y}\\frac{1-p_{t,y}}{1-p_{t,y_{t}}}\\}=\\left\\{\\begin{array}{l l}{\\frac{1}{|\\mathcal{V}|}}&{\\mathrm{~if~}p_{t,y_{t}}\\geq\\frac{1}{|\\mathcal{V}|},}\\\\ {\\frac{\\delta_{1}^{t}}{|\\mathcal{V}|}}&{\\mathrm{~if~}p_{t,y_{t}}<\\frac{1}{|\\mathcal{V}|}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{\\lfloor\\lfloor\\mathtt{m}\\rfloor}q_{i,\\vec{t}}\\tilde{t}_{t,i}-\\displaystyle\\sum_{t=1}^{T}\\tilde{t}_{t,s}\\leq\\frac{\\log[\\Pi^{*}]}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{i=1}^{T}\\frac{\\eta_{i}^{2}}{2}\\frac{1}{y_{0}\\left(p_{i,\\mathtt{t}_{t}}+\\frac{1-\\kappa_{\\mathtt{p},\\vec{p}_{t}}}{1-p_{i,\\mathtt{t}_{t}}}p_{t,y}\\right)}}&{}\\\\ {\\displaystyle\\leq\\frac{\\Theta}{\\eta_{T}}\\frac{\\log[\\Pi^{*}]}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{i=1}^{T}\\frac{\\eta_{i}^{2}}{2}\\frac{1}{\\operatorname*{max}\\{y_{0}^{*}\\}_{\\xi_{0}^{[i]},\\vec{t}_{0}^{*}\\}}}&{}\\\\ &{=\\frac{\\log[\\Pi^{*}]}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{i=1}^{T}\\frac{\\eta_{i}^{2}}{2}\\frac{|y|^{2}\\ln[y]}{\\operatorname*{max}\\{\\xi_{i}^{[i]},\\xi_{0}^{[j]}\\}\\Pi|y|}}&{}\\\\ {\\displaystyle\\overset{(b)}{\\le}\\frac{\\log[\\Pi^{*}]}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{i=1}^{T}\\frac{\\eta_{i}^{2}}{2}\\frac{|y|^{2}\\ln[y]}{\\le\\frac{\\xi_{i}^{[i+1]}y^{2}\\operatorname*{min}[y]}{2}}}&{}\\\\ &{=\\frac{\\log[\\Pi^{*}]}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{i=1}^{T}\\eta_{i}^{2}\\frac{1}{\\delta_{i}^{[i+1]}y^{[j]}\\ln[y]}\\cdot|y|^{2}\\ln[y].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where step (g) is by getting the lower bound of zt as |\u03b4Yt1| \u2264 |Y1|, \u03b4t0 \u2264 1\u2212\u03b4pt0t,yt and step $(h)$ is by applying $\\begin{array}{r}{\\operatorname*{max}\\{A,B\\}\\geq\\frac{A+B}{2}}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Let us define $\\begin{array}{r}{\\rho_{t}\\triangleq\\operatorname*{min}_{\\tau\\in[t]}\\delta_{1}^{\\tau}=1-\\operatorname*{max}_{c,\\tau\\in[t]}p_{t,y}^{\\tau}}\\end{array}$ . We get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{T}[\\mathcal{R}_{T}]\\leq\\frac{\\log{|\\Pi^{*}|}}{\\eta_{T}}+\\frac{1}{\\eta_{T}}\\sum_{t=1}^{T}\\log{|\\Pi^{*}|}\\cdot\\frac{1}{T}\\leq\\frac{2\\log{|\\Pi^{*}|}}{\\eta_{T}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let \u03b7t = $\\begin{array}{r}{\\eta_{t}=\\sqrt{\\frac{\\rho_{t}+\\delta_{0}^{t}|\\mathcal{V}|^{2}\\ln|\\mathcal{V}|}{|\\mathcal{V}|^{2}\\ln|\\mathcal{V}|}}\\cdot\\sqrt{\\frac{\\log|\\Pi^{*}|}{T}}}\\end{array}$ we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{T}[\\mathcal{R}_{T}]\\leq\\frac{2\\sqrt{\\log{|\\Pi^{*}|}\\cdot\\sqrt{T}\\cdot\\sqrt{|\\mathcal{V}|^{2}\\ln{|\\mathcal{V}|}}}}{\\sqrt{\\rho_{T}+\\delta_{0}^{T}|\\mathcal{V}|^{2}\\ln{|\\mathcal{V}|}}}}\\\\ &{\\qquad\\qquad\\leq2|\\mathcal{V}|\\sqrt{\\frac{T\\ln{|\\mathcal{V}|\\log{|\\Pi^{*}|}}}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality is due to the fact that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\rho_{T}+\\delta_{0}^{T}|\\mathcal{V}|^{2}\\ln|\\mathcal{V}|>\\operatorname*{max}\\{\\rho_{T},\\delta_{0}^{T}\\}=\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which completes the proof. ", "page_idx": 26}, {"type": "text", "text": "F.2 Proof of Theorem 4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 4. From Lemma 10, we get the following equation as the cumulative query cost ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}U_{t}\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\frac{1}{\\sqrt{t}}+\\frac{\\sum_{y\\in\\mathcal{Y}}\\langle w_{t},\\ell_{t}^{y}\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{\\langle w_{t},\\ell_{t}^{y}\\rangle}}{|\\mathcal{Y}|}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let us assume the expected total loss of best policy is $\\tilde{L}_{T,*}$ . Thus, from Theorem 3, we get the expected cumulative loss ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{T}\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}r_{t}\\right]\\leq2|y|\\sqrt{\\frac{T\\ln|y|\\log|\\Pi^{*}|}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}_{T,*}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now plugging the regret bound $\\mathcal{R}_{T}$ proved in Theorem 3 into the query complexity bound given by Lemma 12, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underbrace{\\frac{T}{\\sum_{\\substack{=1}}}\\frac{\\sum_{y\\in\\mathcal{Y}}\\left\\langle w_{t},\\ell_{t}^{y}\\right\\rangle\\log_{|\\mathcal{Y}|}\\frac{1}{\\left\\langle w_{t},\\ell_{t}^{y}\\right\\rangle}}{|\\mathcal{Y}|}\\leq\\frac{\\left(2|\\mathcal{Y}|\\sqrt{\\frac{T\\ln{|y|\\log[\\Pi^{*}]}}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}_{T,\\star}\\right)\\left(\\log_{|\\mathcal{Y}|}\\frac{T^{2}(|y|-1)}{\\left(2|y|\\sqrt{\\frac{T\\ln{|y|\\log[\\Pi^{*}]}}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}+\\tilde{L}_{T}\\right)}}-\\right.}{|\\mathcal{Y}|}}\\\\ &{\\left.\\leq\\frac{\\left(2|\\mathcal{Y}|\\sqrt{\\frac{T\\ln{|y|\\log[\\Pi^{*}]}}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}_{T,\\star}\\right)\\left(\\log_{|\\mathcal{Y}|}T|\\mathcal{Y}|\\right)}{|\\mathcal{Y}|}\\right.}\\\\ &{=\\frac{\\left(2|\\mathcal{Y}|\\sqrt{\\frac{T\\ln{|y|\\log[\\Pi^{*}]}}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}_{T,\\star}\\right)\\left(\\log_{|\\mathcal{Y}|}T+1\\right)}{|\\mathcal{Y}|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, by applying query complexity upper bound of Lemma 11, we got ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}U_{t}\\right]\\leq2\\sqrt{T}+\\frac{\\left(2|\\mathcal{Y}|\\sqrt{\\frac{T\\ln|\\mathcal{Y}|\\log|\\Pi^{*}|}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}_{T,*}\\right)\\left(\\log_{|\\mathcal{Y}|}T+1\\right)}{|\\mathcal{Y}|}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since the second term on the RHS dominates the upper bound, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n)\\left(\\mathbb{E}\\left[\\sum_{t=1}^{T}U_{t}\\right]\\right)=O\\left(\\frac{\\left(\\sqrt{\\frac{T\\log|\\Pi^{*}|}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}T_{T,*}\\right)(\\ln T)}{\\sqrt{\\ln\\left(|\\mathcal{V}|\\right)}}\\right)\\overset{(a)}{=}O\\left(\\left(\\sqrt{\\frac{T\\log|\\Pi^{*}|}{\\operatorname*{max}\\{\\rho_{T},\\sqrt{1/T}\\}}}+\\tilde{L}T_{T,*}\\right)\\frac{1}{\\operatorname*{sup}\\left(\\Pi^{*}|\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where step (a) is obtained by suppressing constant coefficients involving $\\lvert\\mathcal{V}\\rvert$ into the $O$ notation. ", "page_idx": 27}, {"type": "text", "text": "G Additional Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we further evaluate CAMS and provide additional experimental results (complementary to the main results presented in Fig. 2) under the following scenarios: ", "page_idx": 28}, {"type": "text", "text": "1. In App. G.1, we demonstrate that CAMS outperforms the baselines on a large scale dataset as well.   \n2. In App. G.2, we perform ablation study of three query strategies CAMS (entropy), variance and random strategy. CAMS (Entropy) achieves the minimum cumulative loss for CIFAR10, DRIFT, and VERTEBRAL under the same query cost and outperform the other query strategies.   \n3. In a mixture of experts environment, CAMS converges to the best policy and outperforms all others (App. G.3);   \n4. In a non-contextual (no experts) environment, CAMS has approximately equal performance as Model Picker to reach the best classifier effectively (App. G.4);   \n5. In an adversarial environment, CAMS can efficiently recover from the adversary and approach the performance of the best classifier (App. G.5);   \n6. In a complete sub-optimal expert environment, a variant of the CAMS algorithm, namely CAMS-MAX, which deterministically picks the most probable policy and selects the most probable model, outperforms CAMS-Random-Policy, which randomly samples a policy and selects the most probable model (App. G.7 & App. G.8). However, CAMS-MAX at most approaches the performance of the best policy. In contrast, perhaps surprisingly, CAMS is able to outperform the best policy on both VERTEBRAL and HIV (App. G.6).   \n7. In App. G.9, we summarize the maximum query cost under a fixed number of realizations with its associated cumulative loss for all baselines (exclude oracle) on all benchmarks in experiment section.   \n8. In App. G.10, we compare the query complexity of each baselines and demonstrate that CAMS has the lowest query cost increasing rate on CIFAR10, DRIFT and VERTEBRAL dataset.   \n9. In previous studies, we assume that the data comes in an online format. In App. G.11, we assume we know the data stream length ahead and applying the scaling parameter to each algorithm to query their top data points from hindsight. CAMS still outperforms all the baselines.   \n10. In App. G.12, we compare CAMS with CAMS-nonactive, a greedy version (query label for each incoming data point). Although CAMS query much less data, it still performs equally well or even better than the greedy version.   \n11. In App. G.13, we demonstrates that CAMS can achieve negative RCL on all benchmarks, which means it outperforms any algorithms that chase the best classifier where the horizontal 0 line represents the performance benchmark of best classifier. ", "page_idx": 28}, {"type": "text", "text": "G.1 Performance of CAMS at scale: Experimental results on CovType ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We scaled up our experiments on a larger dataset, CovType [24]. The CovType dataset offers details about different types of forest cover in the United States. It contains details including slope, aspect, elevation, measurements of the wilderness area, and the type of forest cover. CovType has 580K samples, of which 100K instances were chosen at random as online stream for testing. Fig. 4 demonstrated that CAMS outperforms all baselines which is consistent with the existing results in experiment section. ", "page_idx": 28}, {"type": "text", "text": "G.2 Query strategies ablation comparison ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Using the same CAMS model recommendation section, we compare three query strategies: the adaptive model-disagreement-based query strategy in Line 10-14 of Fig. 1 (referred to as entropy in the following), the variance-based query strategy from Model Picker [39] (referred to as variance), and a random query strategy. Fig. 5 shows that CAMS\u2019s adaptive query strategy has the sharpest converge rate on cumulative loss, which demonstrates the effectiveness of the queried labels. Moreover, entropy achieves the minimum cumulative loss for CIFAR10, DRIFT, and VERTEBRAL under the same query cost. For the HIV dataset, there is no clear winner between entropy and variance since the mean of their performance lie within the error bar of each other for the most part. ", "page_idx": 28}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/7913d9ff28967d106793eeb71530c7e2b8ba59f816eb339fb2b535ba2161c7b0.jpg", "img_caption": ["Figure 4: Comparing CAMS with 7 baselines on CovType in terms of relative cumulative loss, query complexity, and cost effectiveness. CAMS outperforms all baselines. (Left) Performance measured by relative cumulative loss (i.e. loss against the best classifier) under a fixed query cost $B$ (where $B=1000]$ ). (Middle) Number of queries and (Right) Performance of cumulative loss by increasing the query cost, for a fixed number of rounds $T$ (where $T=100,000)$ and maximal query cost $B$ (where $B=5000$ ). Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). $90\\%$ confident interval are indicated in shades. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/e3809fe70923c283f97dc144cc4db32f935e16d230d54e5a57189b2a34010eab.jpg", "img_caption": ["Figure 5: Ablation study of three query strategies (entropy, variance, random) for 4 diverse benchmarks based on the same model recommendation strategy. Under the same query cost constraint, CAMS\u2019s entropy-based strategy exceeds the performance of the other two strategies on non-binary benchmarks in terms of query cost and cumulative lost. $90\\%$ confident intervals are indicated in shades. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "G.3 Comparing CAMS with each individual expert ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We evaluate CAMS by comparing it with all the policies available in various benchmarks. The policies in each benchmark are summarized in App. D.2 and Table D.5. The empirical results in Fig. 6 demonstrate that CAMS could efficiently outperform all policies and converge to the performance of the best policy with only slight increase in query cost in all benchmarks. In particular, on the VERTEBRAL and HIV benchmarks, CAMS even outperforms the best policy. ", "page_idx": 29}, {"type": "text", "text": "G.4 Comparing CAMS against Model Picker in a context-free environment ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "CAMS outperforms Model Picker in Fig. 2, by leveraging the context information for adaptive model selection. In a context-free environment, $\\Pi=\\bar{\\{}\\emptyset\\bar{\\}}$ , so $\\Pi^{*}:=\\{\\pi_{1}^{\\mathrm{const}},\\ldots,\\pi_{k}^{\\mathrm{const}}\\}$ , where $\\pi_{j}^{\\mathrm{const}}(\\cdot):=e_{j}$ represents a policy that only recommends a fixed model. In this case, selecting the best policy to CAMS equals selecting the best single model. Fig. 7 demonstrates that the mean of CAMS and Model Picker lies in the shades of each other, which means CAMS has approximately the same performance as model picker considering the randomness on all benchmarks. ", "page_idx": 29}, {"type": "text", "text": "G.5 Robustness against malicious experts in adversarial environments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "When given only malicious and random advice policies, the conventional contextual online learning from experts advice framework will be trapped in the malicious or random advice. In contrast, CAMS could efficiently identify these policies and avoid taking advice from them. Meanwhile, it also successfully identifies the best classifier to learn to reach its best performance. ", "page_idx": 29}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/d1f60b7abf630cd25ba0ac2b91d47c8610390a1f2427ed4ae6f2fe7a09000b7c.jpg", "img_caption": ["Figure 6: Comparing CAMS with every single policy (only plotted top performance policies in Figure). CAMS could approach the best expert and exceed all others with limited queries. In particular, on VERTEBRAL and HIV Benchmarks, CAMS outperforms the best expert. $90\\%$ confident intervals are indicated in shades. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/87f05424c61fe8c213ee13ab46e9e725bf2e6d17d092b89024861fbab5fb723f.jpg", "img_caption": ["Figure 7: Comparing the model selection strategy of CAMS and Model Picker baseline based on the same variance-based query strategy in a context-free environment. CAMS has approximately the same performance as Model Picker on all the benchmarks. $90\\%$ confident intervals are indicated in shades. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "The novelty in CAMS that enables this robustness is that we add the constant policies $\\left\\{\\pi_{1}^{\\mathrm{const}},\\ldots,\\pi_{k}^{\\mathrm{const}}\\right\\}$ into the policy set $\\Pi$ to form the new set as $\\Pi^{*}$ . To illustrate the performance difference, we have created a variant of CAMS by adapting to the conventional approach (named CAMS-conventional). Fig. 8 demonstrates that CAMS could outperform all the malicious and random policies and converge to the performance of the best classifier. CAMS-conventional: We create the CAMS-conventional algorithm as the CAMS using policy set $\\Pi$ , not $\\Pi^{*}$ . ", "page_idx": 30}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/c7e789bd380f9219025fa0927cbaebfaa147b9724c8046aabca1b1f5a068e0f2.jpg", "img_caption": ["Figure 8: Evaluating the robustness of CAMS compared to the conventional learning from experts\u2019 advice (CAMS-conventional) in a complete malicious and random policies environment. When no good policy is available, CAMS could recover from malicious advice and successfully approach the performance of the best classifier. In contrast, the conventional approach will be trapped in malicious advice. $90\\%$ confident intervals are indicated in shades. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "G.6 Outperformance over the best policy/expert ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We also observe that CAMS does not stop at approaching the best policy or classifier performance. Sometimes, it even outperforms all the policies and classifiers, and Fig. 9 demonstrates such a case. To demonstrate the advantage of CAMS, we create two variant versions of CAMS: (1) CAMS-MAX (App. G.7), (2) CAMS-Random-Policy (App. G.8). CAMS-MAX and CAMS-Random-Policy use the same algorithm as CAMS in adversarial settings but have different model selection strategies for ablation study in the stochastic settings. ", "page_idx": 31}, {"type": "text", "text": "We evaluate the three algorithms on VERTEBRAL and HIV benchmarks in terms of (a) normal policies (Fig. 9 Left), (b) classifiers (Fig. 9 Middle), and (c) malicious and random policies (Fig. 9 Right). In the normal policies column, we only compare the policies with regular policies giving helpful advice. In the classifier column, we compare them with the performance of classifiers only. In the malicious and random policies column, we compare them with unreasonable policies only. ", "page_idx": 31}, {"type": "text", "text": "Fig. 9 demonstrates that all three algorithms could outperform the malicious/random policies. However, CAMS-Random-Policy does not outperform the best classifier while both CAMS and CAMSMAX can on both benchmarks. CAMS-MAX approaches the performance of the best policy but does not outperform the best policy on both benchmarks. Finally, perhaps surprisingly, CAMS outperforms the best policy (Oracle) on both benchmarks and continues to approach the hypothetical, optimal policy (with 0 cumulative loss). ", "page_idx": 31}, {"type": "text", "text": "This surprising factor is contributed by the adaptive weighted policy of CAMS, which adaptively creates a better policy by combining the advantage of each sub-optimal policy and classifier to reach the performance of the hypothetical, optimal policy (defined as tT= 1 mini\u2208[n+k]\u2113 t,i). The second reason could be that the benchmark we created, or any real-world cases, will n ot be strictly in a stochastic setting (in which a single policy outperforms all others or has lower $\\mu$ in every round). The weight policy strategy can make a better combination of advice for this case. ", "page_idx": 31}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/6de74d822e309885c89c1e0b73def7f2af81f747074b431ec8009acda9b22dd1.jpg", "img_caption": ["Figure 9: Comparing CAMS, CAMS-MAX and CAMS-RANDOM-POLICY with top policies and classifiers in the VERTEBRA and HIV benchmarks. They outperform all the malicious/random policies. Moreover, CAMS and CAMS-MAX outperform the best classifier. Finally, only CAMS even exceeds the best policy (Oracle) in both benchmarks and continues approaching the hypothetical, optimal policy (0 cumulative loss). $90\\%$ confident intervals are indicated in shades. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "G.7 The CAMS-MAX algorithm ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "CAMS-MAX is a variant of CAMS. In an adversarial setting, they share the same algorithm. However, in a stochastic setting, CAMS-MAX gets the index $i^{*}$ of max value in the probability distribution of policy $\\pmb q$ , and selects the model with the max value in $\\pi_{i^{*}}$ $\\left(\\pmb{x}_{t}\\right)$ to recommendation. The difference is marked as blue color in Fig. 10. ", "page_idx": 32}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/b64071e4f8295ee2ded7f2a09810122fe7d0817fb338b5dd521866faa77d64b8.jpg", "img_caption": ["Figure 10: The CAMS-MAX Algorithm "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "G.8 The CAMS-Random-Policy algorithm ", "text_level": 1, "page_idx": 32}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/241ed8f5e8eb6e85f92876de51010501f1ec51c15b60e768523bd37734f2c53e.jpg", "img_caption": ["Figure 11: The CAMS-Random-Policy Algorithm "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "CAMS-Random-Policy is a variant of CAMS. It shares the same algorithm with CAMS in an adversarial environment. However, it uses a random sampling policy method in a stochastic setting. ", "page_idx": 32}, {"type": "text", "text": "It randomly samples the policy from the probability distribution of policy $\\pmb q$ , and selects the model with max value in $\\pi_{i^{*}}$ $\\left({\\bf{\\boldsymbol{x}}}_{t}\\right)$ to recommendation. The difference is marked as blue color in Fig. 11. ", "page_idx": 33}, {"type": "text", "text": "G.9 Maximal queries from experiments ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Table 6 in this section summarizes the maximum query cost for a given data stream (of fixed total size), with its associated cumulative loss for all baselines (exclude Oracle) on all benchmarks in experiment section. The result in this table is slightly different from the query complexity curves of Fig. 2 (Middle). The curve in Fig. 2 (Middle) takes the average value, while the table takes the maximal value from a fixed number of simulations. CAMS wins over all baselines (other than Oracle) in terms of query cost on CIFAR10, DRIFT, and VERTEBRAL benchmarks. CAMS outperforms all baselines in terms of cumulative loss on DRIFT, VERTEBRAL, and HIV benchmarks. In particular, CAMS outperforms both cumulative loss and query cost on the DRIFT and VERTEBRAL benchmarks. ", "page_idx": 33}, {"type": "table", "img_path": "ZizwgYErtQ/tmp/4af32cd2309a9037b4e8f7335658e41437e73a4f9b1d64da8afe15b84ce8e5d2.jpg", "table_caption": [], "table_footnote": ["Table 5: Maximal queries from experiments "], "page_idx": 33}, {"type": "text", "text": "G.10 Query complexity ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To achieve the same level of prediction accuracy (measured by average cumulative loss over a fixed number of rounds), CAMS incurs less than $10\\%$ of the label cost of the best competing baselines on CIFAR10 (10K examples), and $68\\%$ the cost on VERTEBRAL (see Fig. 12); Fig. $12^{8}$ and Table 6 also demonstrate the compelling effectiveness of CAMS\u2019s query strategy outperforming all baselines in terms of query cost in VERTEBRAL, DRIFT, and CIFAR10 benchmarks, which is consistent with our query complexity bound in Theorem 2. ", "page_idx": 33}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/4a53a5a6fa72e132cac788f355155660b221af3c72ffdcc31a671704934f800b.jpg", "img_caption": ["Figure 12: Comparing CAMS with 7 baselines on 4 diverse benchmarks in terms of query complexity (Number of queries). CAMS outperforms all baselines for a fixed number of rounds $T$ (where $T=$ 10000, 3000, 80, 4000 from left to right) and maximal query cost $B$ (where $B=1200,2000,80,2000$ from left to right). Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). $90\\%$ confident interval are indicated in shades. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "G.11 Fine-tuning the query probabilities for stochastic streams ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "For the experimental results we reported in the main paper, we consider a streaming setting where the data arrives online in an arbitrary order and arbitrary length. Therefore, for both CAMS and the baselines, we used the exact off-the-shelf query criteria as described in experiment setup section without fine-tuning the query probabilities, which could be otherwise desirable in certain scenarios (e.g. for stochastic streams, where the query probability can be further optimized). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "In this section, we consider such scenarios, and conduct an additional set of experiments to further demonstrate the performance of CAMS assuming stochastic data streams. Given the stream length $T$ and query budget $b$ , we may optimize each algorithm by scaling their query probabilities, so that each algorithm allocates its query budget to the top $b$ informative labels in the entire online stream based on its own query criterion. Note that in practice, finding the exact scaling parameter is infeasible, as we do not know the online performance unless we observe the entire data stream. While it is challenging to determine the scaling factor for each algorithm under the adversarial setting, one can effectively estimate the scaling factor for stochastic streams, where the context arrives i.i.d.. ", "page_idx": 34}, {"type": "text", "text": "Concretely, we use the early budget to decide the scaling parameter in our following evaluation: Firstly, we use a small fraction (i.e. $T/10,$ ) of the online stream and see how much queries $b_{\\mathrm{early}}$ each algorithm consumed. Then we calculate the scaling parameter s = (Tb \u2212\u2212Tbea/rl1y0) \u00b7 Tbe/ar1ly0 and multiply the scaling factor with the query probability of each algorithm for the remaining 190 \u00b7 T rounds. The results in Fig. 13 demonstrate that CAMS still outperforms all the baselines (excluding Oracle) when all algorithms select the top $b$ data of the whole online stream to query. The improvement of CAMS over the baseline approaches does not differ much between the two versions (with or without scaling) of the experiments as shown in the bottom plots of Fig. 2 and Fig. 13. ", "page_idx": 34}, {"type": "text", "text": "For a head-to-head comparison between the bottom plots of Fig. 2 and Fig. 13, note that the total number of rounds stays the same for DRIFT $T\\,=\\,3000,$ ), VERTEBRAL $~T\\,=\\,80)$ ), and HIV $T=4000)$ ); while we used half the rounds and half the maximal budget for CIFAR10 $T=5000)$ ) for the version with scaling. Roughly speaking, the cumulative regret plots for the baselines were \"streched out\" to cover the full allocated budget after scaling, but we do not observe a significant difference in terms of the absolute gain in terms of the cumulative loss. Another way to read the difference between the two plots is to compare the cumulative losses at the budget range where all algorithms were not cut off early: e.g., for DRIFT, when Query Cost is 250, the cumulative losses for the competing algorithm stay roughly the same under the two evaluation scenarios. ", "page_idx": 34}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/523d3d5c46e51a5029f0c530bdc369baae8834226fe47ab2f8d7077c17e5ff39.jpg", "img_caption": ["Figure 13: Comparing CAMS with 7 model selection baselines on 4 diverse benchmarks in terms of cost effectiveness after applying the scaling parameter to each algorithm. CAMS outperforms all baselines (excluding Oracle). Performance of cumulative loss by increasing the query cost, for a fixed number of rounds $T$ (where $T=5000$ , 3000, 80, 4000 from left to right) and maximal query cost $B$ (where $B=600$ , 800, 60, 600 from left to right). Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). $90\\%$ confident interval are indicated in shades. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "G.12 Ablation study on the active query strategy ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we compare the performance of CAMS and its non-active variant (CAMS-nonactive), which queries the label for each incoming data point. As shown in Fig. 14, CAMS performs equally well or better than CAMS-nonactive, even though it queries significantly less data. Surprisingly, on the DRIFT dataset, CAMS significantly outperforms CAMS-nonactive, even when using less than 10 percent of the query budget (Fig. 14b). This demonstrates that CAMS selectively choose the data to query to maximal optimize policy improvement, while CAMS queries all data points, regardless of their usefulness or noise, which hampers policy improvement and convergence. ", "page_idx": 34}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/696365252a2445e36e62217ff942b025f700ffc8fd15c1402807ffb79ed8ab91.jpg", "img_caption": ["Figure 14: Comparing CAMS (in red) with CAMS-nonactive (in blue) on 4 diverse benchmarks in terms of query complexity, and cost effectiveness. CAMS outperforms or performs equally well to CAMS-nonactive with much less queried labels for all benchmarks. (Top) Number of queries and (Bottom) Performance of cumulative loss by increasing the query cost, for a fixed number of rounds $T$ (where $T=5000,3000,80,4000$ from left to right) and maximal query cost $B$ (where $B=T=5000$ , 3000, 80, 4000 from left to right). $90\\%$ confident interval are indicated in shades. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "G.13 Relative Cumulative Loss ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Relative cumulative loss (RCL). At round $t$ , we define RCL as $L_{t,j_{i}}-L_{t,j^{*}}$ , where $L_{t,j^{*}}$ stands for the cumulative loss (CL) of the policy always selecting the best classifier, and $L_{t,j_{i}}$ stands for the CL of policy $i$ . ", "page_idx": 35}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/590ddd5c2dcc50d89c413c77514fdf04b01bde1c05ca6618b4d02adbb8fd0803.jpg", "img_caption": ["Figure 15: Comparing CAMS with 7 baselines on 4 diverse benchmarks in terms of loss trajectory. CAMS outperforms all baselines. Performance measured by relative cumulative loss (i.e. loss against the best classifier) under a fixed query cost $B$ (where $B\\,=\\,200,400,30,400$ from left to right). Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). $90\\%$ confident interval are indicated in shades. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "The RCL under the same query cost for all baselines is shown in Fig. 15. The loss trajectory demonstrates that CAMS efficiently adapts to the best policy after only a few rounds and outperforms all baselines in all benchmarks. The result also demonstrates that CAMS can achieve negative RCL on all benchmarks, which means it outperforms any algorithms that chase the best classifier, as the horizontal 0 line represents the performance benchmark of best classifier. This empirical result aligns with Theorem 1 that, in the worst scenario, if the best classifier is the best policy, CAMS will achieve its performance. Otherwise, CAMS will reach a better policy and incurs no regret. ", "page_idx": 35}, {"type": "text", "text": "CAMS could achieve such performance because when an Oracle fails to achieve 0 loss over all instances and contexts, CAMS has the opportunity to outperform the Oracle in those rounds Oracle does not make the best recommendation. For instance, the stochastic version of CAMS (Line 22-23; Line 30-32 in Fig. 1) may achieve this by recommending a model using the weighted majority vote among all policies. Therefore, one can view CAMS as adaptively constructing a new policy at each round by combining the advantages of each sub-optimal policy, which may outperform any single expert/policy. Furthermore, for the experiments we ran (or in most real-world scenarios), the data streams are not strictly in a stochastic setting (in which a single policy outperforms all others or has a lower expected loss in every round). The weighted policy strategy may find a better combination of \"advices\" in such cases (see Fig. 2 and App. G.6). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "Global Rebuttal Response ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "H Experiments on ImageNet ", "text_level": 1, "page_idx": 36}, {"type": "image", "img_path": "ZizwgYErtQ/tmp/6608e5da5c9f349adf1c1fd71cd8003b3b1ad7541a7ce0f18238b47b2a80cb4f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 16: Comparison of CAMS with 7 baselines on IMAGENET benchmark in terms of cost effectiveness. We plot the cumulative loss as we increase the query cost for a fixed number of rounds $T$ and maximal query cost $B$ ( $T=3000$ , and $B=2500)$ ). CAMS outperforms all baselines. Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included. $90\\%$ confident interval are indicated in shades. ", "page_idx": 36}, {"type": "table", "img_path": "ZizwgYErtQ/tmp/1bf3c4008bff0ab1a948a82ec2e3a221a8106d4f1d4df390ad2df027533d4f2d.jpg", "table_caption": ["I Comparing CAMS against recent works in active learning "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "Table 6: Selective comparison against recent works in active learning. Among these algorithms, Coreset (Sener & Savarese, 2017) is a diversity sampling strategy for deep active learning; BatchBALD is an uncertainty sampling strategy; BADGE (Ash et al., 2019), VAAL (Sinha et al., 2019) ClusterMargin (Citovsky et al., 2021), and VeSSAL (Saran et al., 2023) represent strategies that combine both; GLISTER (Killamsetty et al., 2020) and BALANCE (Zhang et al., 2023) represent decision-theoretic approaches that directly optimize the utility of queries. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We list the paper\u2019s contribution and scope clearly at the last paragraph of Section 1 ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We have discussed our limitation at Section 7 future direction. It summarizes the limitations of our approach and we hope to address in the future work. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have listed all the details of modeling approaches, proof assumptions, and complete proofs in Section 5, and Appendix E, F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide the code and data in the supplementary material with a readme.txt for reproducing the results. Experiment details are listed in Section 6 and Appendix G, D.6. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide the code and data in the supplementary material with a readme.txt for reproducing the results. Experiment details are listed in Section 6 and Appendix G, D.6. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Experiment details are listed in Section 6 and Appendix G, D.6. In addition, we provide the code and data in the supplementary material with a readme.txt for reproducing the results. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide standard deviation in tables and confidence interval in figures. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: It can be found at the supplementary Appendix D.6. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The codes are anonymous as well. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: It has been discussed at Abstract and Section 1. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 41}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]