{"references": [{"fullname_first_author": "Scott Fujimoto", "paper_title": "Off-policy deep reinforcement learning without exploration", "publication_date": "2019-07-01", "reason": "This paper is foundational to offline RL, introducing a method to learn policies from offline data without the need for online exploration, which is directly relevant to the multi-agent offline RL problem addressed in the target paper."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-07-01", "reason": "This paper proposes a conservative Q-learning algorithm for offline RL, addressing the extrapolation error problem which is a major challenge in offline RL and is also relevant to multi-agent settings."}, {"fullname_first_author": "Lili Chen", "paper_title": "Decision Transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-07-01", "reason": "This paper introduces a novel approach to offline RL using sequence modeling, which the target paper builds upon and extends to the multi-agent case."}, {"fullname_first_author": "Michael Janner", "paper_title": "Offline reinforcement learning as one big sequence modeling problem", "publication_date": "2021-07-01", "reason": "This paper connects offline RL to sequence modeling, a perspective that the target paper adopts and extends to the multi-agent case."}, {"fullname_first_author": "Ling Pan", "paper_title": "Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification", "publication_date": "2022-07-01", "reason": "This paper tackles the offline multi-agent RL problem directly and proposes the OMAR algorithm, which is used as a baseline in the target paper."}]}