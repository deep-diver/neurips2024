[{"figure_path": "PvoxbjcRPT/tables/tables_6_1.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents the average scores achieved by different multi-agent offline reinforcement learning methods across various tasks and datasets.  The scores are normalized to account for variations in difficulty.  The shaded columns highlight the performance of the MADIFF models, showcasing their improved performance compared to existing baselines.", "section": "5 Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_7_1.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents a comparison of the average scores achieved by different multi-agent offline reinforcement learning methods across various tasks.  The scores represent the performance of each algorithm on different datasets and tasks, categorized by environment (MPE, MA Mujoco, SMAC) and task difficulty (Spread, Tag, World, etc.).  The shaded columns highlight the performance of the proposed MADIFF method (MADIFF-D and MADIFF-C), showing its comparative advantage.  The mean and standard error are calculated across five different random seeds to provide a measure of statistical significance.", "section": "5 Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_13_1.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents the average scores achieved by different multi-agent offline reinforcement learning methods across various tasks and datasets.  The scores represent the performance of each algorithm on several benchmark environments, categorized by type (MPE, MuJoCo, SMAC) and dataset quality (Expert, Md-Replay, Medium, Random).  Shaded columns highlight the results obtained using the MADIFF method (both centralized and decentralized versions), allowing for comparison against other state-of-the-art methods such as MA-ICQ, MA-TD3+BC, MA-CQL, and OMAR. The mean and standard error are calculated from results obtained across five independent experimental runs, indicating the statistical significance of the reported results.", "section": "5 Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_18_1.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents the average scores achieved by different multi-agent offline reinforcement learning methods on various tasks.  The shaded columns highlight the performance of the MADIFF methods (MADIFF-D and MADIFF-C).  The table includes results for several benchmark algorithms and different datasets, providing a comprehensive comparison of the proposed approach against existing state-of-the-art methods.  The scores are averages calculated across five different random seeds to ensure statistical reliability. The standard error is also reported for each entry.", "section": "5 Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_18_2.jpg", "caption": "Table 5: Hyperparameters of MADIFF on MA Mujoco datasets.", "description": "This table shows the hyperparameters used for training the MADIFF model on the MA Mujoco datasets.  It details the settings used for each task (2halfcheetah, 2ant, 4ant) and dataset quality (Good, Medium, Poor). The parameters listed include the return scale, learning rate, guidance scale, planning horizon, history horizon, batch size, diffusion steps, reward discount, and the optimizer used. These hyperparameters were tuned for optimal performance on each specific task and dataset.", "section": "5. Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_19_1.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents the average scores achieved by different multi-agent offline reinforcement learning methods across various tasks and datasets.  The scores represent the performance of each algorithm, with shaded columns indicating the results obtained using the MADIFF methods proposed in the paper. The mean and standard error are calculated across five different random seeds for each entry, providing a measure of the statistical reliability of the results.", "section": "5 Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_19_2.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents the average scores achieved by different multi-agent offline reinforcement learning (MARL) methods across various tasks and datasets.  The scores are calculated as the episodic return in online rollouts. The table includes both baseline methods and the proposed method (MADIFF), with results shown for different dataset qualities (Expert, Md-Replay, Medium, and Random). Shaded columns highlight the performance of MADIFF, indicating its superior performance. The mean and standard error are calculated across five different random seeds for each method and dataset combination.", "section": "5 Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_20_1.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents the average scores achieved by different offline multi-agent reinforcement learning (MARL) methods across various tasks and datasets.  The methods are evaluated using the episodic return obtained in online rollout as a performance measure.  The shaded columns highlight the performance of the proposed MADIFF methods (MADIFF-D and MADIFF-C).  The results are averaged over five different random seeds to provide a measure of statistical significance. Each dataset represents a different level of data quality (Expert, Md-Replay, Medium, Random) that was used for training the algorithms.", "section": "5 Experiments"}, {"figure_path": "PvoxbjcRPT/tables/tables_20_2.jpg", "caption": "Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds.", "description": "This table presents the average scores achieved by different offline multi-agent reinforcement learning (MARL) methods across various tasks and datasets. The shaded columns highlight the performance of the proposed methods (MADIFF-D and MADIFF-C).  The results are averaged over 5 different random seeds, and the standard error is included to indicate the variability of the results.  The table allows for a comparison of the proposed method's performance against existing state-of-the-art MARL algorithms.", "section": "5 Experiments"}]