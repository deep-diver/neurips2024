[{"figure_path": "pc4GSBi1Hx/tables/tables_3_1.jpg", "caption": "Table 1: Dataset details of long-text-image retrieval and short-text-image retrieval tasks. We use BERT tokenizer for tokenization. ShareGPT4V-1k and 10k are selected from the ShareGPT4V dataset. For DCI and IIW, all images with human-authored long descriptions are used while evaluating.", "description": "This table presents the details of datasets used for both long-text and short-text image retrieval tasks.  It shows the number of images and texts, the average number of sub-captions per text, and the average number of tokens per text for each dataset.  The datasets are categorized into long-text and short-text datasets, highlighting the differences in text length used for training and evaluation. The tokenizer used is BERT.", "section": "4 Long Texts in Language-Image Pre-training"}, {"figure_path": "pc4GSBi1Hx/tables/tables_6_1.jpg", "caption": "Table 2: Analyze the effectiveness of LoTLIP in language-image pre-training with long texts. The architecture of the image encoder is ViT-B/16. I2T and T2I indicate R@1 on text and image retrieval, respectively. We use 3M scale dataset for pre-training. \u201c\u2713\u201d indicates we add long texts in the training stage.", "description": "This table compares the performance of different models on image and text retrieval and image classification tasks.  The models are trained with and without long texts. The impact of adding long texts to the pre-training stage on different metrics (I2T and T2I R@1) is evaluated across three different datasets (DCI, IIW, and ShareGPT4v-10k). The results show that using long texts in training improves performance in these tasks, particularly in retrieval tasks, demonstrating the effectiveness of LoTLIP, particularly when compared to other models with and without long texts in their training.", "section": "5.2 Ablation Studies"}, {"figure_path": "pc4GSBi1Hx/tables/tables_7_1.jpg", "caption": "Table 3: Analyze the influence of the number of corner tokens and the attention mask mechanism. We use 3M scale dataset for training. The architecture of the image encoder is ViT-B/16.", "description": "This table presents an ablation study on the impact of the number of corner tokens and the attention mask mechanism used in the LoTLIP model.  It shows the performance of the model on various tasks (long-text-image retrieval, short-text-image retrieval, and image classification) with different configurations of corner tokens and the attention mask. The experiment uses a 3M scale dataset for training, and the image encoder is a ViT-B/16 architecture.", "section": "5.2 Ablation Studies"}, {"figure_path": "pc4GSBi1Hx/tables/tables_7_2.jpg", "caption": "Table 4: Zero-shot evaluation of different models on long-text-image retrieval tasks. I2T and T2I indicate R@1 on text and image retrieval, respectively.", "description": "This table presents the results of a zero-shot evaluation of various models on long-text image retrieval tasks.  It compares the performance of different models (FILIP, LaCLIP, SigLIP, LiT, LoTLIP, CLIP, ALIGN, SigLIP, Long-CLIP) across several datasets (DCI, IIW, ShareGPT4V-1k, ShareGPT4V-10k) using two metrics: Recall@1 (R@1) for image-to-text (I2T) retrieval and text-to-image (T2I) retrieval. The datasets vary in size and source, and the models differ in their architecture and training methodologies. The table allows for a comparison of the effectiveness of different models in handling long-text image retrieval tasks.", "section": "5.2 Ablation Studies"}, {"figure_path": "pc4GSBi1Hx/tables/tables_8_1.jpg", "caption": "Table 5: Zero-shot evaluation of different models on short-text-image retrieval and classification tasks.", "description": "This table presents the results of a zero-shot evaluation of various models on short-text image retrieval and image classification tasks.  It compares the performance of different models (FILIP, LaCLIP, SigLIP, LiT, LoTLIP, CLIP, ALIGN, and Long-CLIP) across different datasets (MSCOCO and Flickr30k) and metrics (R@1, R@5, accuracy). The models were evaluated on both short and long-text inputs to assess their ability to handle different text lengths.  The table shows the impact of various models and different amounts of training data on the performance of these tasks. ", "section": "5. Experiments"}, {"figure_path": "pc4GSBi1Hx/tables/tables_11_1.jpg", "caption": "Table 6: Data statistic of LoTLIP dataset and other text-image paired dataset. Our dataset is the largest dataset consisting of long texts for multi-modal learning.", "description": "This table presents a comparison of the LoTLIP dataset with other publicly available image-text datasets.  It shows the number of images and texts, the average number of sub-captions per text, and the average number of tokens per text.  The table highlights that LoTLIP is significantly larger than other datasets and contains substantially longer texts, making it unique for multi-modal learning involving long text understanding.", "section": "8.1 Data Statistics"}, {"figure_path": "pc4GSBi1Hx/tables/tables_12_1.jpg", "caption": "Table 7: Compare corner tokens with register tokens. The models are trained with 3M scale dataset.", "description": "This table compares the performance of using corner tokens versus register tokens in the LoTLIP model.  Both types of tokens are added to the text encoder to improve the handling of long and short texts. The table shows the Recall@1 (R@1) scores for image-to-text (I2T) and text-to-image (T2I) retrieval tasks on four datasets (DCI, IIW, ShareGPT4v-10k, MSCOCO), as well as the accuracy (Acc.) of ImageNet classification.  The results demonstrate the superior performance of corner tokens compared to register tokens across all tasks and metrics. ", "section": "8.2 More Experimental Analysis"}, {"figure_path": "pc4GSBi1Hx/tables/tables_12_2.jpg", "caption": "Table 5: Zero-shot evaluation of different models on short-text-image retrieval and classification tasks.", "description": "This table presents the results of zero-shot evaluations performed on three different downstream tasks: short-text image retrieval using MSCOCO and Flickr30k datasets, and image classification using the ImageNet dataset.  The table compares the performance of various models (CLIP, LiT, LOTLIP and others) across these tasks, showing Recall@1 and Recall@5 for retrieval tasks and accuracy for classification. The data scale used for training the models (3M, 12M, 30M, 100M) is also shown, highlighting the impact of training data size on model performance.  The \"Long Texts\" column indicates whether long texts were used during the pre-training phase.  This allows for a comparison of model performance with and without long-text pre-training.", "section": "5.2 Ablation Studies"}, {"figure_path": "pc4GSBi1Hx/tables/tables_13_1.jpg", "caption": "Table 9: Utilizing long captions generated by different MLLMs in the training stage.", "description": "This table presents the results of experiments using three different large multi-modal language models (MLLMs) to generate long captions for images in the language-image pre-training process.  It compares the performance of the LoTLIP model when trained with captions generated by each of the three MLLMs individually and when trained using captions from all three MLLMs. The metrics evaluated include I2T and T2I Recall@1 for several datasets, and accuracy on an ImageNet classification task. The goal was to assess the impact of MLLM diversity on the overall effectiveness of long-caption-based training.", "section": "8.2.4 Utilizing Long Captions from Different MLLMs"}, {"figure_path": "pc4GSBi1Hx/tables/tables_15_1.jpg", "caption": "Table 2: Analyze the effectiveness of LoTLIP in language-image pre-training with long texts. The architecture of the image encoder is ViT-B/16. I2T and T2I indicate R@1 on text and image retrieval, respectively. We use 3M scale dataset for pre-training. \u201c\u2713\u201d indicates we add long texts in the training stage.", "description": "This table compares the performance of different models on image and text retrieval tasks, with and without the use of long texts in pre-training. The models are evaluated on three datasets (DCI, IIW, and ShareGPT4V-10k) for long-text image retrieval and MSCOCO for short-text image retrieval. ImageNet classification accuracy is also reported.  The table shows that incorporating long texts in the pre-training stage, as done in LoTLIP, significantly improves performance on long-text retrieval tasks, but may slightly reduce performance on short-text retrieval and image classification tasks.", "section": "5.2 Ablation Studies"}, {"figure_path": "pc4GSBi1Hx/tables/tables_16_1.jpg", "caption": "Table 2: Analyze the effectiveness of LoTLIP in language-image pre-training with long texts. The architecture of the image encoder is ViT-B/16. I2T and T2I indicate R@1 on text and image retrieval, respectively. We use 3M scale dataset for pre-training. \u201c\u2713\u201d indicates we add long texts in the training stage.", "description": "This table compares the performance of different methods on image classification, short-text image retrieval, and long-text image retrieval tasks.  The methods include LiT (a baseline), LiT with Long-CLIP (a competitor method that fine-tunes a pre-trained model with long texts), and LoTLIP (the proposed method). The results show the Recall@1 (R@1) metric for image-to-text (I2T) and text-to-image (T2I) retrieval, and Accuracy@1 (Acc@1) for image classification.  The impact of adding long texts to the training process is also demonstrated, highlighting the effectiveness of the proposed LoTLIP approach.", "section": "5.2 Ablation Studies"}]