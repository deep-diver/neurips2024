{"references": [{"fullname_first_author": "J. Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-04", "reason": "This paper introduces BERT, a foundational model for language understanding, which is highly relevant to the advancements in language-image pre-training discussed in the target paper."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a highly influential model for language-image pre-training that directly inspires and is built upon by the research in the target paper."}, {"fullname_first_author": "J. Deng", "paper_title": "Imagenet: A large-scale hierarchical image database", "publication_date": "2009-01-01", "reason": "This paper introduces ImageNet, a crucial dataset for training and evaluating image-related models, foundational to the datasets used in the target paper's research."}, {"fullname_first_author": "S. Changpinyo", "paper_title": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "publication_date": "2021-01-01", "reason": "This paper introduces Conceptual 12M, a large-scale dataset for language-image pre-training, which is directly used and referenced in the target paper's methodology."}, {"fullname_first_author": "X. Zhai", "paper_title": "LiT: Zero-shot transfer with locked-image text tuning", "publication_date": "2022-01-01", "reason": "This paper introduces LiT, a significant model in language-image pre-training that is directly compared to and built upon by the research in the target paper."}]}