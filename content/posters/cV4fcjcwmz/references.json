{"references": [{"fullname_first_author": "Viraj Bagal", "paper_title": "MolGPT: molecular generation using a transformer-decoder model", "publication_date": "2021-09-01", "reason": "This paper introduces MolGPT, a significant model in molecular generation using transformer architecture, directly relevant to the current research on molecular pre-trained models."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-01", "reason": "This is a crucial reference for understanding large language models, which are foundational to the concept of pre-trained models and data extraction attacks."}, {"fullname_first_author": "Anna Gaulton", "paper_title": "ChEMBL: a large-scale bioactivity database for drug discovery", "publication_date": "2012-01-01", "reason": "ChEMBL, as a large-scale bioactivity database, is a key resource for drug discovery and is foundational to the availability of molecular datasets for pre-training."}, {"fullname_first_author": "Xu Han", "paper_title": "Pre-trained models: Past, present and future", "publication_date": "2021-01-01", "reason": "This paper provides a comprehensive overview of pre-trained models, contextualizing the current work within the broader field of pre-trained models across various domains."}, {"fullname_first_author": "Zhenyu Hou", "paper_title": "GraphMAE: Self-supervised masked graph autoencoders", "publication_date": "2022-01-01", "reason": "GraphMAE is a state-of-the-art molecular pre-trained model using a masked autoencoder approach, directly relevant to the paper's focus on molecular graph data extraction attacks."}]}