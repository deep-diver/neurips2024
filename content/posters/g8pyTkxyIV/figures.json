[{"figure_path": "g8pyTkxyIV/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of our method. We first initialize 3D Gaussians as static, modeling their motion linearly. During optimization, dynamic and static objects are separated based on the amount of predicted motion, and the 3D Gaussians between the selected keyframes are interpolated and rendered.", "description": "This figure illustrates the overall process of the proposed Explicit 4D Gaussian Splatting (Ex4DGS) method. It starts by initializing 3D Gaussians as static, whose motion is modeled linearly.  During the optimization process, the algorithm separates dynamic and static objects based on their motion.  The dynamic Gaussians' position and rotation are interpolated between keyframes to achieve temporally continuous motion, while static Gaussians maintain linear motion. Finally, a point backtracking technique is used to refine the results by identifying and pruning erroneous Gaussians. The interpolated dynamic Gaussians are then rendered to generate the final output.", "section": "4 Methodology"}, {"figure_path": "g8pyTkxyIV/figures/figures_4_1.jpg", "caption": "Figure 2: Effectiveness of our keyframe interpolation.", "description": "The figure demonstrates the effectiveness of the proposed keyframe interpolation method.  It shows three subfigures. (a) shows a rendered image at a specific time (t). (b) illustrates the interpolated dynamic points that have been calculated based on the keyframes and their translations and rotations. (c) shows the rendered image at a subsequent time (t+I). The interpolated dynamic points smoothly transition between keyframes, resulting in a visually consistent and temporally coherent representation of dynamic motions. This visual representation strongly supports the method's ability to efficiently model dynamic motions.", "section": "4.2 Dynamic Gaussians"}, {"figure_path": "g8pyTkxyIV/figures/figures_4_2.jpg", "caption": "Figure 3: Comparison between the single Gaussian, Gaussian mixture, and our model for temporal opacity modeling.", "description": "This figure compares three different approaches to modeling temporal opacity: a single Gaussian, Gaussian mixtures, and the proposed method.  The x-axis represents time, and the y-axis represents opacity (\u03c3t).  The red line shows the actual opacity change over time, while the blue line shows the estimated opacity. (a) shows the limitations of using a single Gaussian to model complex opacity changes. (b) demonstrates the use of Gaussian mixtures to handle more complex changes. (c) illustrates how the authors' proposed method addresses the limitations of the previous approaches and achieves better results.", "section": "4.2.3 Temporal Opacities"}, {"figure_path": "g8pyTkxyIV/figures/figures_5_1.jpg", "caption": "Figure 1: Overview of our method. We first initialize 3D Gaussians as static, modeling their motion linearly. During optimization, dynamic and static objects are separated based on the amount of predicted motion, and the 3D Gaussians between the selected keyframes are interpolated and rendered.", "description": "This figure illustrates the overall pipeline of the proposed Explicit 4D Gaussian Splatting (Ex4DGS) method.  It starts by initializing all Gaussian points as static, assuming linear motion.  During optimization, it dynamically separates static and dynamic objects based on predicted motion.  The dynamic Gaussians' positions and rotations are then interpolated between keyframes, resulting in smooth and continuous motion representation.  The final step involves rendering the interpolated Gaussians to generate the output frames. The figure highlights the key stages: initialization, static/dynamic separation, progressive learning, point backtracking, and rendering.", "section": "4 Methodology"}, {"figure_path": "g8pyTkxyIV/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison of our Ex4DGS with other the state-of-the-art dynamic Gaussian splatting methods on Neural 3D Video [7] dataset.", "description": "This figure compares the visual results of Ex4DGS against other state-of-the-art dynamic Gaussian splatting methods on the Neural 3D Video dataset.  It shows example renderings from several scenes, allowing a visual comparison of rendering quality across different models. The goal is to highlight Ex4DGS's superior performance in generating high-quality images, even in challenging scenarios with dynamic motion.", "section": "5.1 Neural 3D Video Dataset"}, {"figure_path": "g8pyTkxyIV/figures/figures_7_2.jpg", "caption": "Figure 6: Visualization of our static and dynamic point separation on Coffee Martini, Flame Steak and Fabien scene in Neural 3D Video [7] and Technicolor [20] datasets.", "description": "This figure visualizes the separation of static and dynamic points achieved by the proposed method, Ex4DGS.  It demonstrates the model's ability to distinguish between static and dynamic components within a scene. The images show the ground truth, rendered static points only, rendered dynamic points only, and the combination of both.  The examples are taken from the Neural 3D Video and Technicolor datasets, highlighting the method's effectiveness across different scenes and datasets.", "section": "5.3 Separation of Dynamic and Static Points"}, {"figure_path": "g8pyTkxyIV/figures/figures_14_1.jpg", "caption": "Figure 7: Comparison between (b) handling color changes without dynamic points and (c) our complete model.", "description": "This figure compares the results of handling color changes in a video scene with two different methods. (b) shows the result when only color changes are considered, without differentiating between static and dynamic objects. The results show that static points cannot effectively handle these changes. (c) shows the result with the complete model where dynamic objects are treated differently, leading to better visual quality.", "section": "B.1 Without Handling Color Components"}, {"figure_path": "g8pyTkxyIV/figures/figures_15_1.jpg", "caption": "Figure 9: Qualitative comparison of the repeatedly occluded objects in the Technicolor Train scene over a sequence of 100 frames (frame #170 to #269). All models are trained with the point cloud data from the frame #170.", "description": "This figure compares the performance of different dynamic view synthesis methods (4DGS, 4D Gaussians, STG, and the proposed Ex4DGS) on a sequence of frames from the Technicolor Train scene where objects are repeatedly occluded.  The models are all trained using only point cloud data from the initial frame (#170). The figure aims to demonstrate the ability of each method to handle occlusions and maintain rendering quality. The highlighted areas show the main area of comparison and focus.", "section": "C.1 Handling Occlusion"}, {"figure_path": "g8pyTkxyIV/figures/figures_16_1.jpg", "caption": "Figure 1: Overview of our method. We first initialize 3D Gaussians as static, modeling their motion linearly. During optimization, dynamic and static objects are separated based on the amount of predicted motion, and the 3D Gaussians between the selected keyframes are interpolated and rendered.", "description": "This figure illustrates the workflow of the proposed Explicit 4D Gaussian Splatting (Ex4DGS) method.  It starts by initializing 3D Gaussians as static, assuming linear motion. During optimization, the method distinguishes between static and dynamic objects based on the predicted motion.  Dynamic Gaussians are then interpolated between keyframes to create temporally continuous motion and finally rendered.", "section": "4 Methodology"}, {"figure_path": "g8pyTkxyIV/figures/figures_16_2.jpg", "caption": "Figure 11: Evaluation of the extremely long video on Flame Salmon scene in Neural 3D Video dataset. Best viewed at Adobe Acrobat Reader.", "description": "This figure visualizes the results of an experiment using an extremely long video sequence (1000 frames, 20000 images) on the Flame Salmon scene from the Neural 3D Video dataset. It showcases the rendering quality and robustness of the Ex4DGS model over an extended duration and illustrates how well it handles long sequences, maintaining high quality even with many frames. The figure compares the rendered results with the actual video frames.", "section": "5.1 Neural 3D Video Dataset"}]