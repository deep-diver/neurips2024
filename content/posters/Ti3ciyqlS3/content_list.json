[{"type": "text", "text": "Improving Temporal Link Prediction via Temporal Walk Matrix Projection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaodong Lu   \nCCSE Lab, Beihang University Beijing, China   \nxiaodonglu@buaa.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Leilei Sun \u2217 CCSE Lab, Beihang University Beijing, China leileisun@buaa.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Tongyu Zhu CCSE Lab, Beihang University Beijing, China zhutongyu@buaa.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Weifeng Lv CCSE Lab, Beihang University Beijing, China lwf@buaa.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Temporal link prediction, aiming at predicting future interactions among entities based on historical interactions, is crucial for a series of real-world applications. Although previous methods have demonstrated the importance of relative encodings for effective temporal link prediction, computational efficiency remains a major concern in constructing these encodings. Moreover, existing relative encodings are usually constructed based on structural connectivity, where temporal information is seldom considered. To address the aforementioned issues, we first analyze existing relative encodings and unify them as a function of temporal walk matrices. This unification establishes a connection between relative encodings and temporal walk matrices, providing a more principled way for analyzing and designing relative encodings. Based on this analysis, we propose a new temporal graph neural network called TPNet, which introduces a temporal walk matrix that incorporates the time decay effect to simultaneously consider both temporal and structural information. Moreover, TPNet designs a random feature propagation mechanism with theoretical guarantees to implicitly maintain the temporal walk matrices, which improves the computation and storage efficiency. Experimental results on 13 benchmark datasets verify the effectiveness and efficiency of TPNet, where TPNet outperforms other baselines on most datasets and achieves a maximum speedup of $33.3\\times$ compared to the SOTA baseline. Our code can be found at https://github.com/lxd99/TPNet. ", "page_idx": 0}, {"type": "text", "text": "1 Intorduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many real-world dynamic systems can be abstracted as a temporal graph [1], where entities and interactions among them are denoted as nodes and edges with timestamps respectively. Temporal link prediction, aiming at predicting future interactions based on historical interactions, is a fundamental task for temporal graph learning, which can not only help us understand the evolution pattern of the temporal graph but also is crucial for a series of real-world tasks such as recommendations for online platforms [2, 3] and information diffusion prediction [4, 5]. ", "page_idx": 0}, {"type": "text", "text": "Relative encodings have become an indispensable module for effective temporal link prediction [6\u20139] where, without them, node representations computed independently by neighbor aggregation will fail to capture the pairwise information. As the toy example shown in Figure 1, A and F will have the same node representation due to sharing the same local structure. Thus it can not be determined whether D will interact with A or F at $t_{3}$ according to their representations. However, by assigning nodes with relative encodings (i.e., additional node features) specific to the target link before computing the node representation, we can highlight the importance of each node and guide the representation learning process to extract pairwise information. For example, in Figure 1, we can infer from the relative encoding of E (in red circle) that D is more likely to interact with F than with A since D and F share a common neighbor, E (detailed discussed in Section 2.2). Although achieving remarkable success, injecting pairwise information based on relative encodings is still far from satisfactory. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1) On Concept, existing ways of constructing relative encodings are fragmented as they are derived from different heuristics. There still lacks a unified view on the connections between different relative encodings, which may allow a more principled way to design new relative encodings. 2) On Method Design, most existing relative encodings are constructed based on structural connectivity between nodes (e.g., whether two nodes are neighbors), while the temporal information is ignored. 3) On Computation, existing methods for relative encodings are inefficient, which usually involve time-consuming graph query operations and need to re-extract the relative encoding from scratch for each target link, making them even become the main computational bottleneck for some methods (shown in Section 4.2). ", "page_idx": 1}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/7cdb40d984183710ef8cba209522dfa0a71f350ee1cb6184c457f3684e7496a1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "To tackle the above issues, this paper makes the following three technical contributions. 1) A Unified View (Concept). We analyze the construction of existing relative encodings and find that they can be uniformly viewed as a function of temporal random walk matrices, where different relative encodings essentially correspond to the construction of a series of temporal walk matrices based on temporal walk weighting. The presented function provides a unified view to analyze existing methods and may allow a more principal way to design new relative encodings. 2) A Effective and Efficient Method (Method Design and Computation). Based on our analysis, we propose a Time decay matrix Projection-based graph neural Network for temporal link prediction, named TPNet for short. TPNet introduces a new temporal walk matrix that incorporates the time decay effect of the temporal graph, simultaneously considering both temporal and structural information. Moreover, TPNet encodes the temporal walk matrix into a series of node representations by random feature propagation, which can be efficiently updated when the graph structure changes and is storage-efficient. Importantly, such node representations can be shared by different link likelihood computation processes to decode the pairwise information, reducing the redundancy computation of re-extracting the relative encodings and avoiding time-consuming graph query operations. 3) Theoretical and Empirical Analysis. We conduct a theoretical analysis of the node representations obtained through random feature propagation. Our analysis demonstrates that these representations stem from the random projection of the proposed temporal walk matrix while preserving the inner product of different rows of the matrix. Moreover, we discuss the conditions under which random feature propagation can be applied to improve the computational and storage efficiency of other temporal walk matrices and provide the corresponding propagation mechanisms for temporal walk matrices of existing methods. Empirically, we conduct experiments on 13 benchmark datasets to verify the effectiveness and efficiency of the proposed method, where TPNet outperforms other baselines on most datasets and achieves a maximum speedup of $33.3\\times$ compared to the SOTA baseline. Detailed ablation studies also verify the effectiveness of the proposed submodules. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we will first formally define some important concepts in this paper and then give a unified formulation of existing relative encodings. ", "page_idx": 1}, {"type": "text", "text": "2.1 Definitons ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 1 (Temporal Graph). A temporal graph can be considered as a sequence of non-decreasing chronological interactions $\\bar{\\mathcal{G}}=\\left[\\left(\\{u_{1},\\bar{v}_{1}\\},\\bar{t}_{1}\\right)\\bar{,}\\left(\\{u_{2},v_{2}\\},t_{2}\\right),\\cdot\\cdot\\cdot\\right]$ with $0\\leq t_{1}\\leq t_{2}\\leq\\cdots$ , where $(\\{u_{i},v_{i}\\},\\bar{t}_{i})$ can be considered as a undirected link between $u_{i}$ and $v_{i}$ with timestamp $t_{i}$ . Each node $u$ can be associated with node feature $\\pmb{x}_{u}\\in\\mathbb{R}^{d_{N}}$ , and each interaction $(\\{u,v\\},t)$ has link feature $\\pmb{e}_{u,v}^{t}\\in\\mathbb{R}^{d_{E}}$ , where $d_{N}$ and $d_{E}$ denote the dimensions of the node feature and link feature. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Temporal Link Prediction). The interaction sequence reflects the graph dynamics, and thus the ability of a model to capture the evolution pattern of a dynamic graph can be evaluated by how accurately it predicts the future interactions based on historical interactions. Formally, given the interactions before $t$ (i.e., $\\{(\\{u^{\\prime},v^{\\prime}\\},t^{\\prime})|t^{\\prime}<t\\})$ and two nodes $u,\\,v$ , the temporal link prediction task aims to predict whether there will be an interaction between $u$ and $v$ at $t$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (K-hop Subgraph). We use the notation $\\mathcal{G}(t)\\;=\\;(\\mathcal{V}(t),\\mathcal{E}(t))$ to denote the graph snapshot at $t$ , where $\\mathcal{E}(t)$ includes all the interactions that happen before $t$ and $\\mathcal{V}(t)$ includes all the nodes appear in $\\mathcal{E}(t)$ . Besides, we defined the $\\mathbf{k}$ -hop subgraph of node $u$ as $\\mathcal{G}_{u}^{k}(t)=(\\mathcal{V}_{u}^{k}(t),\\mathcal{E}_{u}^{k}(t)$ , where $\\mathcal{V}_{u}^{k}(t)\\subset\\mathcal{N}(t)$ is the set of nodes whose shortest path distance to $u$ is less than $k$ on $\\mathcal{G}(t)$ and $\\mathcal{E}_{u}^{k}(t)\\subset\\mathcal{E}(t)$ is the set of interactions between $\\mathcal{V}_{u}^{k}(t)$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 4 (Temporal Walk). A $\\boldsymbol{\\mathrm{k}}$ -step temporal walk $W$ on $\\mathcal{G}(t)$ is a sequence of node-time pair with decreased temporal order [6], which can be denoted as $\\dot{W_{\\mathrm{~=~}}}[(w_{0},\\dot{t}_{0}),\\cdots,(w_{k},t_{k})]$ with $t=t_{0}>t_{1}>\\cdot\\cdot\\cdot>t_{k}$ and $(\\{w_{i},w_{i+1}\\},t_{i+1})$ is an edge on $\\mathcal{G}(t)$ for $0\\leq i\\leq k-1$ . Figure 2 shows a visual illustration of a temporal walk. Here, we stipulate the first timestamp $t_{0}$ as the current time $t$ to avoid undefinedness of $t_{0}$ . We use the notation $\\dot{\\mathcal{M}}_{u,v}^{k}(t)$ to denote the set of all $\\boldsymbol{\\mathrm{k}}$ -step temporal walks from $u$ to $v$ on $\\mathcal{G}(t)$ . Specially, $\\mathcal{M}_{u,w}^{0}(t)=\\{[(u,t)]\\}$ if $u=w$ and $\\mathcal{M}_{u,w}^{0}(t)=\\emptyset$ otherwise. When there is no ambiguity, we replace $\\mathcal{M}_{u,v}^{k}(t)$ with $\\mathcal{M}_{u,v}^{k}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Relative Encoding for Dynamic Link Prediction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.2.1 Unified Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a future link $(u,v,t)$ to be predicted, existing temporal link prediction methods (referred as node-wise methods) usually first learn the node representations $h_{u}(t)$ and $h_{v}(t)$ independently, which are obtained by encoding their $\\boldsymbol{\\mathrm{k}}$ -hop subgraphs, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{u}(t)=f_{\\mathrm{enc}}(\\mathcal{G}_{u}^{k}(t),\\pmb{X}_{u,k}^{N},\\pmb{X}_{u,k}^{E}),\\quad\\pmb{h}_{v}(t)=f_{\\mathrm{enc}}(\\mathcal{G}_{v}^{k}(t),\\pmb{X}_{v,k}^{N},\\pmb{X}_{v,k}^{E}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $X_{u,k}^{N}$ and $X_{u,k}^{E}$ are the features of nodes and edges in $\\mathcal{G}_{u}^{k}(t)$ respectively 2. The $f_{\\mathrm{enc}}(\\cdot)$ here can be any encoding function that maps a graph into a representation such as a -layer GNN with a pooling layer Then the link likelihood $p_{u,v}^{\\overline{{t}}}$ is given $p_{u,v}^{t^{\\star}}\\,=\\,f_{\\mathrm{dec}}(h_{u}(t),h_{v}(t))$ . The $f_{\\mathrm{dec}}(\\cdot)$ is a decoding function that maps the node representations into link likelihood such as an MLP with a Sigmoid output layer. Detailed discussion about existing methods can be found in Appendix F.2. ", "page_idx": 2}, {"type": "text", "text": "As mentioned in the Section 1, learning representations independently might fail to capture the pairwise information of the given link. Thus recent methods (referred as link-wise methods) inject the pairwise information by constructing a relative encoding $\\boldsymbol{r}^{w|(u,v)}$ for each node $w\\in\\mathcal{V}_{u}^{k}(t)\\,\\bar{\\cup}\\,\\mathcal{V}_{v}^{k}(t)$ as an additional node feature (Detailed construction way for different methods will be introduced in Section 2.2.2). Then Equation (1) will be changed into ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{u}(t)=f_{\\mathrm{enc}}(\\mathcal{G}_{u}^{k}(t),{\\boldsymbol X}_{u,k}^{N}\\oplus{\\boldsymbol X}_{u,k}^{R},{\\boldsymbol X}_{u,k}^{E}),\\ \\ \\ h_{v}(t)=f_{\\mathrm{enc}}(\\mathcal{G}_{v}^{k}(t),{\\boldsymbol X}_{v,k}^{N}\\oplus{\\boldsymbol X}_{v,k}^{R},{\\boldsymbol X}_{v,k}^{E}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $X_{u,k}^{R}$ is the relative encodings for nodes in $\\mathcal{G}_{u}^{k}(t)$ and $X_{u,k}^{N}\\oplus X_{u,k}^{R}$ indicate the new node features obtained by concatenating $X_{u,k}^{N}$ and $X_{u,k}^{R}$ . Intuitively, the relative encoding $r^{w|(u,v)}$ for each node $w$ reflects its importance to predicted link $(u,v,t)$ , which can guide the representation learning process to extract the pairwise information specific to the predicted link from the subgraph. For the detailed construction way, the relative encoding ${\\pmb r}^{w|(u,v)}$ is the concatenation of two similarity features $r^{w|u}$ and $r^{w|v}$ , where $r^{w|u}/r^{w|v}$ encodes some form of similarity between $u/v$ and $w$ (e.g., the number of $\\mathbf{k}$ -step temporal walks from $u$ to $w$ ). Although the designs of similarity feature $r^{w|u}$ for different methods are induced from different heuristics, we find that they can be unified in a function about the temporal walk matrix, which is ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nr^{w|u}=g([A_{u,w}^{(0)}(t),A_{u,w}^{(1)}(t),\\cdot\\cdot\\cdot\\,,A_{u,w}^{(k)}(t)]),\\quad A_{u,w}^{(i)}(t)=\\sum_{W\\in\\cal M_{u,w}^{i}}s(W)\\;\\;\\mathrm{for}\\;\\;0\\leq i\\leq k.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The $s(\\cdot)$ here is a score function that maps a temporal walk to a scalar and $A^{(i)}(t)$ denotes an i-hop temporal walk matrix whose each entry $A_{u,w}^{(i)}$ is the sum of the score of all i-step temporal walks from $u$ to $w$ . $g(\\cdot)$ is a function applied on the vector of $[A_{u,w}^{(0)}(t),A_{u,w}^{(1)}(t),\\cdot\\cdot\\cdot\\,,A_{u,w}^{(k)}(t)]\\in\\mathbb{R}^{k+1}$ to extract similarity feature. The above equation shows that each relative encoding implies a construction of the temporal walk matrix based on weighting each temporal walk (i.e., $s(\\cdot){\\mathrm{.}}$ ). Next, we will analyze existing relative encodings and show how they can be represented in the form of Equation (3). ", "page_idx": 3}, {"type": "text", "text": "2.2.2 Analysis of Existing Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our analysis focuses on the four representative link-wise methods DyGFormer, PINT, NAT, and CAW, which cover all the link-wise methods in the benchmark of dynamic link prediction [9]. ", "page_idx": 3}, {"type": "text", "text": "DyGFormer [9]. The $r^{w\\vert u}$ of DyGFormer is a one-dimensional vector representing the number of links between $w$ and $u$ . For DyGFormer, we can first set the $s(\\cdot)$ to be a one-const function (i.e., $s(W)=1$ in for any $W$ ), which will make $A_{u,w}^{(k)}$ be the number of the $\\mathbf{k}$ -step temporal walks from $u$ to $w$ . Then, setting $g(\\cdot)$ to be a function that selects the second number of a vector (i.e., $g([x_{0},x_{1},..,x_{k}])=x_{1})$ ) will make Equation (3) generate the similarity feature of DyGFormer. ", "page_idx": 3}, {"type": "text", "text": "PINT [8]. The $r^{w|u}$ of PINT is a $(k+1)$ -dimensional vector, where $r_{i}^{w|u}$ is the number of $(i-1)$ -step temporal walks from $u$ to $w$ for $1\\leq i\\leq k+1$ . Setting $s(\\cdot)$ and $g(\\cdot)$ can be set to a one-const function and an identity function respectively will make Equation (3) outputs the similarity feature of PINT. ", "page_idx": 3}, {"type": "text", "text": "NAT [7]. NAT maintains a series of fix-sized hash maps $s_{u}^{(0)},...,s_{u}^{(k)}$ and generates the $r^{w|u}$ based on the hash maps. As proved in Appendix A.1, if the size of the hash maps becomes infinite, the $r^{w|u}$ is equivalent to a $(k+1)$ -dimensional vector, where, for $1\\leq i\\leq k+1$ , $r_{i}^{w|u}=1$ if the shortest temporal walk from $u$ to $w$ is less than $i$ ; otherwise, riw|u= 0. Let h(\u00b7) be a binary function where $h(x)=1$ o iff $\\begin{array}{r}{g([x_{0},x_{1},...,x_{k}])=[h(\\sum_{i=0}^{0}x_{i}),h(\\sum_{i=0}^{1}x_{i}),...,h(\\sum_{i=0}^{k}x_{i})]}\\end{array}$ $x>0$ $h(x)=0$ $s(\\cdot)$ s tc afnu nmctaikoen  tahned $g(\\cdot)$ ttioo na (3) generate the similarity feature of NAT. ", "page_idx": 3}, {"type": "text", "text": "CAWN [6]. The similarity feature $r^{w|u}$ of CAWN reflects the probability of a node $w$ being visited when performing a temporal walk from $u$ . Specifically, CAWN first samples a set of temporal walks beginning from $u$ based on a causal sampling strategy. Then, for each node $w$ , the similarity feature $r^{w|u}$ is extracted based on its occurrence in the sampled walks. As proved in Appendix A.2, the similarity feature $r^{w|u}$ is the estimation of a $(k+1)$ -dimensional vector, where, for $1\\leq i\\leq k+1$ , $r_{i}^{w|u}$ is the probability of visiting $w$ through a $(i\\!-\\!1)$ -step temporal walk matrix based on the sampling strategy of CAWN, which can be represented as $\\begin{array}{r}{r_{i}^{w|u}=\\sum_{W\\in\\mathcal{M}_{u,w}^{i-1}}s^{\\prime}(W)}\\end{array}$ . The value $s^{\\prime}(W)$ for a given tem$W=[(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{k},t_{k})]$ $\\begin{array}{r}{\\prod_{i=0}^{k-1}\\frac{\\exp\\left(-\\alpha\\left(t_{i}-t_{i+1}\\right)\\right)}{\\sum_{\\left(\\left\\{w^{\\prime},w\\right\\},t^{\\prime}\\right)\\in\\mathcal{E}_{w_{i},t_{i}}}\\exp\\left(-\\alpha\\left(t_{i}-t^{\\prime}\\right)\\right)}}\\end{array}$ where $\\alpha$ is hyperparameter to control the sampling process, $\\mathcal{E}_{w_{i},t_{i}}=\\big\\{(\\{w^{\\prime},w\\},t^{\\prime})|\\dot{t}^{\\prime}<t_{i}\\big\\}$ is the set of interactions attached to $w_{i}$ before $t_{i}$ , and $\\frac{\\exp(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{\\left(\\left\\{w^{\\prime},w\\right\\},t^{\\prime}\\right)\\in\\mathcal{E}_{w_{i},t_{i}}}}$ can be considered as the probability of moving from $(w_{i},t_{i})$ to $(w_{i+1},t_{i+1})$ in the sampling process. Setting $s(\\cdot)$ to $s^{\\prime}(\\cdot)$ and $g(\\cdot)$ to be an identity function can make Equation (3) generate the similarity feature of CAWN. ", "page_idx": 3}, {"type": "text", "text": "Conclusion. According to the above analysis, we can conclude that (i) Equation (3) provides a unified view to consider the injection of pairwise information from the construction of temporal walk matrix, where different relative encodings (implicitly or explicitly) correspond to a kind of temporal walk matrix. (ii) Examining existing relative encodings from the unified view reveals their limitations. First, the relative encodings of existing methods (except CAWN) ignore the temporal information carried by each temporal walk, where their score function $s(\\cdot)$ always yield 1 and the entries of the temporal walk matrix is just the count of the temporal walks. Next, although CAWN considers temporal information, they estimate the temporal walk matrix from the sampled temporal ", "page_idx": 3}, {"type": "text", "text": "1 Initialize $\\pmb{H}^{(0)},\\pmb{H}^{(1)},...,\\pmb{H}^{(k)}\\in\\mathbb{R}^{n\\times d_{R}}$ as zero matrix ;   \n2 Fill $H^{(0)}$ with entries independently drawn from $\\textstyle{N(0,{\\frac{1}{d_{R}}})}$ ;   \n3 for $(u,v,t)\\in\\mathcal{G}$ do ", "page_idx": 4}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/8a523f26cb1d201eea9e6fc7435107a086d5674619d0f15fc8c2435913e13fc0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/c90c3cf8b5456926f2211bda8ff3b54f5c1491e54f7598a862cd826a5efad265.jpg", "img_caption": ["Figure 2: A illustration of the temporal walk. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "walks, which needs time-consuming graph sampling and may introduce large estimation errors. In the next section, we will present a new temporal walk matrix to simultaneously consider the temporal and structural information and show how to efficiently maintain the proposed temporal walk matrix. ", "page_idx": 4}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "TPNet mainly consists of two modules: Node Representation Maintaining (NRM) and Link Likelihood Computing (LLC). The NRM is responsible for encoding the pairwise information, which maintains a series of node representations. Such representations will be updated when new interaction occurs and can be used to decode the temporal walk information between two nodes. The LLC module is a prediction module, which utilizes the maintained node representations and auxiliary information (e.g., like features) to compute the likelihood of the link to be predicted. ", "page_idx": 4}, {"type": "text", "text": "3.1 Node Representation Maintaining ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Temporal Matrix Construction. Based on the Equation (3), designing a temporal walk matrix relies on the definition of the score function $s(\\cdot)$ , where the element of a temporal walk matrix is $\\begin{array}{r}{A_{u,v}^{(k)}(t)=\\sum_{W\\in M_{u,v}^{k}}s(W)}\\end{array}$ . Unlike most previous methods that only count the number of temporal walks, we consider the temporal information carried by a temporal walk in $s(\\cdot)$ to simultaneously consider the temporal and structural information. Formally, let $t$ be the current time, given a temporal walk $W=[(w_{0},t_{0}),(w_{1},t_{1}),..,(w_{k},t_{k})]$ , the value of the score function is $\\begin{array}{r}{s(W)=\\prod_{i=1}^{k}e^{-\\lambda(t-t_{i})}}\\end{array}$ where $\\lambda>0$ is a hyperparameter to control the time decay weight. As the curren t time $t$ goes on, for each interaction $(\\{w_{i},w_{i+1}\\},t_{i+1})$ in the temporal walk $W$ , its weight $e^{-\\lambda(t-t_{i})}$ will decay exponentially. The design of the score function is motivated by the widely observed time decay effect [1, 10] on the temporal graph, where the importance of interactions will decay as time goes on, benefiting better modeling the graph evolution patterns. In the following part of Section 3, the notation of $s(\\cdot)$ and $A^{(k)}(t)$ will specifically refer to the score function and temporal walk matrix proposed in this part. Besides, for $A^{(0)}(t)$ , we stipulate it as an identity matrix constantly. ", "page_idx": 4}, {"type": "text", "text": "Node Representation Maintaining. Directly computing the temporal walk matrices is impractical since we need to enumerate the temporal walks and each matrix needs expensive $O(n\\times n)$ space complexity to store. Thus, we implicitly maintain the temporal walk matrices by maintaining a series of node representations $\\pmb{H}^{(0)}(t),\\mathbf{\\dot{H}}^{(1)}\\mathbf{\\dot{(}}t),...,\\pmb{H}^{(k)}(t)\\in\\mathbb{R}^{n\\times d_{R}}$ , where $d_{R}\\ll n$ is the dimension of node representations and $\\pmb{H}_{u}^{(l)}(t)\\in\\mathbb{R}^{d_{R}}$ encodes the information about the $l$ -step temporal walks beginning from $u$ for $0\\leq l\\leq k$ . The node representations will be updated when a new interaction occurs. Specifically, we first construct a random feature matrix $P\\in\\dot{\\mathbb{R}}^{n\\times d_{R}}$ , where each entry of $_{P}$ is independently drawn from Gaussian distribution with mean 0 and varianc e 1 . Then we initialize $H^{(0)}(t)$ as $_{P}$ and ${\\pmb H}^{(1)}(t),{\\pmb H}^{(2)}(t),...,{\\pmb H}^{(k)}(t)$ as zero matrix. When a new interaction $(u,v,t)$ occurs, for $1\\leq l\\leq k$ , we update the representations of $u$ and $v$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{H}_{u}^{(l)}(t^{+})=\\pmb{H}_{u}^{(l)}(t)+e^{\\lambda t}*\\pmb{H}_{v}^{(l-1)}(t),\\quad\\pmb{H}_{v}^{(l)}(t^{+})=\\pmb{H}_{v}^{(l)}(t)+e^{\\lambda t}*\\pmb{H}_{u}^{(l-1)}(t),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the $t^{+}$ denotes the time right after $t$ . A pseudocode for maintaining the node representations is shown in Algorithm 1. The maintaining mechanism here can be considered as a random feature propagation mechanism on the temporal graph, where we initialize the zero layer\u2019s representation of each node as a random feature and repeatedly propagate these features among nodes from the low layer to the high layer as interactions continuously appear. The following theorem shows the relationship between the node representations and the temporal walk matrices. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 1. If any two interactions on temporal graph $\\mathcal{G}$ have different timestamps, the obtained representations $\\pmb{H^{(0)}}(t),\\pmb{H^{(1)}}(t),...,\\pmb{H^{(k)}}(\\dot{t})$ satisfy $e^{-\\lambda l t}*H^{(l)}\\ddot{(t)}=A^{(l)}(t)P$ for $0\\leq l\\leq k$ . ", "page_idx": 5}, {"type": "text", "text": "For simplicity, we assume the timestamps of the interaction are different, and we show how to update the representations when multiple interactions have the same timestamps in Appendix C. We leave the proof in the Appendix B.1. The above theorem shows that the obtained node representation is the projection (i.e. linear transformation) of the temporal walk matrices, where the transform matrix is the initial random feature matrix $_{P}$ . The next theorem shows that the node representations preserve the inner product of the temporal walk matrices. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Given any \u03f5 \u2208 (0, 1), let \u2225\u00b7 \u22252 denote the L2 norm, clu1,,vl2 denote $\\frac{1}{2}(\\|\\mathbf{A}_{u}^{(l_{1})}(t)\\|_{2}^{2}+$ $\\|A_{v}^{(l_{2})}(t)\\|_{2}^{2})$ , and $\\bar{H}^{(l)}(t)$ denote $e^{-\\lambda l t}*H^{(l)}(t)$ , if dimension $d_{R}$ of node representations satisfy $\\begin{array}{r}{d_{R}\\geq\\frac{24}{\\epsilon^{2}}\\log(4^{1/3}(k+1)n).}\\end{array}$ , then for any $1\\leq u,v\\leq n$ , and $0\\le l_{1},l_{2}\\le k$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\langle\\bar{H}_{u}^{(l_{1})}(t),\\bar{H}_{v}^{(l_{2})}(t)\\rangle-\\langle A_{u}^{(l_{1})}(t),A_{v}^{(l_{2})}(t)\\rangle\\right|\\leq\\epsilon c_{u,v}^{l_{1},l_{2}}\\right)\\geq1-\\frac{1}{(k+1)n},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ denotes the inner product and $\\big|\\cdot\\big|$ denotes taking absolute value. ", "page_idx": 5}, {"type": "text", "text": "We leave the proof in Appendix B.2. The above theorem shows that the inner product of the node representations is approximately equal to the inner product of the temporal walk matrices (i.e., $\\langle\\bar{H}_{u}^{(l_{1})}\\mathring{(t)},\\bar{H}_{v}^{(l_{2})}(t)\\rangle\\approx\\langle\\bar{A}_{u}^{(l_{1})}(t)),\\bar{A}_{v}^{(l_{2})}(t)\\rangle)$ . Specifically, given any error rate $\\epsilon$ , if the dimension of the node representations satisfies a certain condition, the difference between $\\langle\\bar{H}_{u}^{(l_{1})}(t),\\bar{H}_{v}^{(l_{2})}(t)\\rangle$ and $\\langle\\mathbf{A}_{u}^{(l_{1})}(t)\\rangle,\\mathbf{A}_{v}^{(l_{2})}(t)\\rangle$ for any $u,v,l_{1},l_{2}$ will be less than $\\epsilon c_{u,v}^{l_{1},l_{2}}$ with high probability $\\textstyle(\\geq1-{\\frac{1}{(k+1)n}})$ . The error rate can approach 0 infinitely and thus the inner product of the node representations can approach that of temporal walk matrices infinitely, at the cost of increasing the dimension $d_{R}$ . As we will see in Section 4.3, a small dimension $(\\ll n)$ in practice is enabled to make the inner product of node representations a good estimation of that of temporal walk matrices. Additionally, due to the i-th row of $A^{(0)}(t)$ being the one-hot encoding of i (since it is an identity matrix), we have $\\langle\\mathbf{A}_{u}^{(l)}(t),\\mathbf{A}_{w}^{(0)}(t)\\rangle\\,=\\,\\ddot{A}_{u,w}^{(l)}(t)$ . Thus, we can obtain $[A_{u,w}^{(0)}(t),\\cdot\\cdot\\cdot\\;,A_{u,w}^{(k)}(t)]$ in Equation (3) by calculating the inner product between all layers of $u$ \u2019s representation and the zero layer of $w$ \u2019s representation, expressed as $[\\langle\\bar{H}_{u}^{(0)}(t),\\bar{H}_{w}^{(0)}(t)\\rangle,\\cdot\\cdot\\cdot\\:,\\langle\\bar{H}_{u}^{(k)}\\bar{(t)},\\bar{H}_{w}^{(0)}(t)\\rangle].$ . ", "page_idx": 5}, {"type": "text", "text": "Remark. Compared to directly computing the temporal walk matrices $\\pmb{A}^{(0)}(t),..,\\pmb{A}^{(k)}(t)$ , which needs to enumerate the temporal walks and $O((k+\\dot{1})n^{2})$ space complexity to store, maintaining the node representations largely improve the computation and storage efficiency, which only needs $O((k+$ $1)n d_{R})$ space complexity to store and $O(k\\bar{d}_{R})$ time complexity to update when a new interaction occurs. Actually, Theorem 2 is the direct result of Theorem 1 based on Johnson-Lindenstrauss Lemma [11], where the random projection can preserve the inner product and norm [12]. Notably, the method for implicitly maintaining temporal walk matrices via random feature propagation can be extended to other types of temporal walk matrices, provided they meet a specific condition (i.e., the updating function of the temporal walk matrix can be written as the linear combination of its rows). This condition is not restrictive, and all the temporal walk matrices discussed in Section 2 fulflil it. We show their propagation mechanism and related discussion in Appendix D. In conclusion, the unified function in Equation (3), combined with methods of implicitly maintaining the temporal walk matrices, provides a new way to design a more effective and efficient way to inject pairwise information. ", "page_idx": 5}, {"type": "text", "text": "3.2 Link Likelihood Computing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given an interaction $(u,v,t)$ to be predicted, we compute its happening likelihood based on the obtained node representations and auxiliary features. Specifically, we first decode a pairwise feature ${\\pmb f}_{u,v}(t)$ from the node representations obtained in Section 3.1. Then we compute the node embeddings $\\pmb{h}_{u}(t)$ and $h_{v}(t)$ for node $u$ and $v$ respectively based on their historical interactions. Finally, we give the link likelihood based on $h_{u}(t),h_{v}(t),f_{u,v}(t)$ . For notation simplicity, we omit the suffix of $\\bar{\\pmb{h}}_{u}(t),\\pmb{h}_{v}(t),\\pmb{f}_{u,v}(t)$ and denote them as $h_{u},h_{v},f_{u,v}$ in the following part. ", "page_idx": 5}, {"type": "text", "text": "Pairwise Feature Decoding. Although we can obtain the $(k+1)$ -dimensional feature in Equation (3) by calculating the inner product between the zero-layer representation of one node and all layers of the other node\u2019s representation, this method does not consider the correlation between all layers of both nodes. Therefore, we use representations from all layers to decode the pairwise ivwn.hf ioTcrhhm eacntai now nbe. e c Sdopenenccoaittfeeicnda alatlesy  , $\\pmb{F_{*}}\\,=\\,[e^{-\\lambda0t}\\pmb{H}_{*}^{(0)},...,e^{-\\bar{\\lambda}k t}\\pmb{H}_{*}^{(k)}]\\,\\in\\,\\mathbb{R}^{(k+1)\\times d_{R}}$ \\*da ifcnfodeu rloednb ttb aliean y ute hroser, $F_{u,v}\\;=\\;[F_{u},F_{v}]\\;\\in\\;\\mathbb{R}^{2(k+1)\\times d_{R}}$ raw pairwise feature $\\tilde{\\pmb f}_{u,v}$ by computing the inner product among different rows of $\\boldsymbol{F}_{u,v}$ , which is $\\tilde{\\pmb f}_{u,v}=\\mathrm{flat}(\\pmb F_{u,v}\\pmb F_{u,v}^{T})$ with $\\operatorname{flat}(\\cdot)$ means flatten a matrix of $\\mathbb{R}^{2(k+1)\\times2(k+1)}$ into a vector of $\\mathbb{R}^{4(k+1)^{2}}$ . Finally, we feed the raw pairwise feature $\\tilde{\\pmb f}_{u,v}$ into an MLP to get the pairwise feature $\\scriptstyle f_{u,v}$ , which is $f_{u,v}=\\mathrm{MLP}(\\log(\\mathrm{ReLU}(\\Tilde{f}_{u,v})+1))$ . The $\\mathrm{\\sfReLU}(\\cdot)$ here is used to reduce estimation error, where the inner product of temporal walk matrices should be larger than zero and we thus set it to be zero if the inner product of the node representations is negative. The $\\log(\\cdot)$ is used to scale the raw pairwise feature, where the range of the inner product between different layers varies greatly and the $+1$ is the shift term to avoid the undefined value of $\\log(0)$ . We will see in Section 4.3 that these two operations can improve the training stability. ", "page_idx": 6}, {"type": "text", "text": "Auxiliary Feature Learning. The auxiliary features such as link features also provide rich information for modeling the evolution patterns of the temporal graph. In this part, we follow the previous methods [13, 9] and consider the auxiliary feature learning as a sequential learning problem. Specifically, for node $u$ , we take its recent $m$ interactions $S_{u}=\\bar{[}(\\{u,w_{1}\\},t_{1}),...,(\\{u,\\bar{w_{m}}\\},t_{m})]$ before $t$ and learn node embedding $h_{u}$ from this sequence. We first fetch the node features $\\pmb{X}_{u,N}\\,=\\,[\\pmb{x}_{w_{1}},...,\\pmb{x}_{w_{m}}]\\,\\in\\,\\mathbb{R}^{m\\times d_{N}}$ and edge features $\\mathbf{\\widehat{X}}_{u,E}\\,=\\,[e_{u,w_{1}}^{t_{1}},..,e_{u,w_{1}}^{t_{m}}]\\,\\in\\,\\mathbb{R}^{m\\times d_{E}}$ . For timestamps, we map the timestamps into temporal features $\\pmb{X}_{u,T}=[\\phi(t-t_{1}),..,\\phi(t-t_{n})]\\in\\mathbb{R}^{m\\times d_{T}}$ like [14], where $\\phi(\\Delta t)~=~[\\cos(w_{1}\\Delta t),..,\\cos(w_{d_{T}}\\Delta t)]$ is a time encoding function to learn the periodic temporal pattern. Besides, we construct a relative encoding sequence $X_{u,F}\\ =$ $[\\pmb{f}_{u,w_{1}}\\oplus\\pmb{f}_{v,w_{1}},...,\\pmb{f}_{u,w_{m}}\\oplus\\pmb{f}_{v,w_{m}}]\\,\\in\\,\\mathbb{R}^{m\\times8(k+1)^{2}}$ to inject the pairwise features, where $f_{u,w_{m}}$ denote the pairwise feature of $u$ and $w_{m}$ and $\\bigoplus$ is the concatenation operation. After obtaining the above feature sequence, we concatenate them together and feed it into an MLP to get the final feature sequence $Z_{u}^{(0)}=\\mathrm{MLP}([X_{u,N},X_{u,E},X_{u,T},X_{u,F}])\\in\\mathbb{R}^{m\\times d}$ . Subsequently, we stack $l$ layers of MLP-Mixer [15] to capture the temporal and structural dependencies within the feature sequence, which is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\pmb Z}_{u}^{(l)}=\\pmb Z_{u}^{(l-1)}+\\pmb W_{1}^{(l)}\\mathrm{GeLU}(\\pmb W_{2}^{(l)}\\mathrm{LayerNorm}(\\pmb Z_{u}^{(l-1)}))}\\\\ &{\\pmb Z_{u}^{(l)}=\\tilde{\\pmb Z}_{u}^{(l)}+\\pmb W_{3}^{(l)}\\mathrm{GeLU}(\\pmb W_{4}^{(l)}\\mathrm{LayerNorm}(\\tilde{\\pmb Z}_{u}^{(l)})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Finally, we get the node embedding by mean pooling $h_{u}=\\mathbf{MEAN}(Z_{u}^{(l)})$ . The procedure to get node embedding $h_{v}$ is similar and for the node that does not have $m$ interactions, we pad the feature sequence with zero. Then, the likelihood of the link $(u,v,t)$ is given by $p_{u,v}^{t}=\\mathrm{MLP}([h_{u},h_{v},f_{u,v}])$ , where $\\mathbf{MLP}(\\cdot)$ is a 2-layer MLP model with Sigmoid activation function in its output layer. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and Baselines. We conduct experiments on 13 benchmark datasets for temporal link prediction, which are Wikipedia, Reddit, MOOC, LastFM, Enron, Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote and Contact. Eleven popular temporal graph learning methods are selected as baselines including JODIE, DyRep, TGAT, TGN, CAWN, EdgeBank, TCL, GraphMixer, NAT, PINT, and DyGFormer. Details about datasets and baselines can be found in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "Task Settings. The task settings strictly follow [9]. Specifically, we conduct experiments under two settings: 1) the transductive setting that predicts links between nodes that have been seen during training and 2) the inductive setting that predicts links between nodes that are not seen during training. Three different negative sampling strategies introduced by [16] are used to sample the negative links and the Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are adopted as the evaluation metrics. For dataset splitting, we chronologically split each dataset with $70\\%/15\\%/15\\%$ for training/validating/testing. The training and testing pipeline is the same as that in [9]. ", "page_idx": 6}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/6c0cf0fd1fb6e88d1b27455b999e52ce1243027b32629d8dfda46981518aa74e.jpg", "table_caption": ["Table 1: Transductive results for different baselines under the random negative sampling strategy. blod and underline highlight the best and second best result respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Model Configuration. For TPNet, the layer $l$ of node representations, the number of recent interactions $m$ , and dimension $d_{R}$ of the node representations are set to 3, 20 and $10*\\log(2\\mathrm{E})$ , where E is the number of the interactions. We find the best time decay weight $\\lambda$ via grid search within a range of $10^{-4}$ to $10^{-7}$ . Specifically, the best $\\lambda$ for Contact is $10^{-\\bar{4}}$ , the best $\\lambda$ for MOOC, Can. Parl. and UN Vote is $10^{-5}$ , the best $\\lambda$ for Wikipedia, Reddit, Enron, Social Evo., Flights and US Legis. is $10^{-6}$ , the best $\\lambda$ for LastFM, UCI and UN Trade is $10^{-7}$ . ", "page_idx": 7}, {"type": "text", "text": "Implementatoin Details. For baselines, we use the implementation of DyGLib [9], which is a unified temporal graph learning library, and has tuned the best hyperparameters for each baseline. For baselines that are not included in DyGLib (i.e., NAT and PINT), we use their official implementation and find the best hyperparameters via grid search. Experiments are conducted on a Ubuntu server, whose CPU and GPU devices are one Intel(R) Xeon(R) Gold 6226R CPU $@$ 2.9GHz with 64 CPU cores and four GeForce RTX 3090 GPUs with 24 GB memory respectively. We run each experiment five times and report the average. ", "page_idx": 7}, {"type": "text", "text": "4.2 Performance and Efficiency Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Performance comparison among baselines. The performance of TPNet and baselines is shown in Table 1. Due to space limit, Table 1 only shows the results under the transductive setting with random negative sampling strategy and more similar results can be found in Appendix G.1. As shown in Table 1, TPNet achieves the best performance among all the methods on most datasets, verifying its effectiveness. Besides, the link-wise methods (CAWN, NAT, PINT, and DyGFormer) perform better than the node-wise methods, indicating the importance of injecting the pairwise feature. Compared to the baselines, TPNet simultaneously considers the temporal and structural correlations between nodes and encodes the temporal walk matrix into node representations with theoretical guarantees, which contributes to its superior performance. ", "page_idx": 7}, {"type": "text", "text": "Efficiency Analysis. We compare the relative inference time of different methods to TPNet to evaluate their efficiency. The results on LastFM and MOOC are shown in Figure 4 and more results can be found in Appendix G.2. As shown in Figure 4, TPNet not only achieves the best performance but is also more efficient than other link-wise methods, where TPNet achieves a $33\\times$ and $71.5\\times$ speedups compared to the SOTA baseline DyGFormer and CAWN respectively on LastFM. The CAWN and DyGFormer models utilize time-consuming graph queries (e.g., temporal walk sampling) to construct relative encodings, which constitute the main computational bottleneck and consume over $70\\%$ of the running time. In contrast, TPNet caches historical interactions into node representations and constructs pairwise features based on these representations, thereby enhancing efficiency. ", "page_idx": 7}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/467457be22a8aabfcdd2c731fdc10c0bb44a6a7e1ad3c523cb8e24e500ad1fa5.jpg", "table_caption": ["Table 2: Ablation study results, where N/A indicates the numerical overflow error. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/489d24a760a4ac58903e93ce386b4f82febc3b0d785a215692723998ead75d15.jpg", "img_caption": ["Figure 3: Influence of node representation dimension. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Scalability Analysis. To further verify the scalability of TPNet, we generated a series of random graphs with an average degree fixed at 100 and the number of edges varying from 1e5 to 1e8. Figure 4 shows the change of running time and GPU memory of TPNet, where the growth of the running time and GPU memory is close to and less than the linear growth curve respectively, showing the good scalability of TPNet. In contrast, PINT explicitly stores the temporal walk matrices and encounters out-of-memory error when the edge number reaches 1e7 (shown inAppendix G.3), verifying the impracticability of explicitly storing temporal walk matrices. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Proposed Components. To verify the effectiveness of the proposed components in TPNet, we compare TPNet with the following variants: 1) w/o NR that remove the node representations and the corresponding features that decoded from them. 2) w/o Time that only considers the structural information by setting the time decay weight $\\lambda$ to be zero. 3) w/o Scale that remove the $\\log(\\cdot)$ and ReLU(\u00b7) in the pairwise feature decoding. As shown in Table 2, there is a dramatic performance drop of w/o NR, which shows that the pairwise information carried by the node representations plays a vital role in the performance of TPNet. There is also an obvious performance drop of w/o Time, which confirms the necessity of incorporating temporal information in temporal walk matrix construction. Besides, the unreasonable poor performance of the w/o Scale is due to the various distribution of node representations across different layers, where, without scaling the raw pairwise features, the training will be unstable, and numerical overflow errors may even occur on some datasets. Further details on the distribution of node representations from different layers are provided in Appendix G.5. ", "page_idx": 8}, {"type": "text", "text": "Dimension Change. To verify the influence of the node representation dimension. We vary the dimensions from 1 to 128 and report the performance of TPNet (denoted as TPNet-d). As shown in Figure 3, the required dimension of node representations is small, where only 1-dimensional and 16-dimensional node representations can achieve satisfactory performance on MOOC and LastFM respectively. For different datasets, we observe that the average degree may be a main influence of the required dimension, where sparse graphs (like MOOC and Wikipedia) only need a small dimension, and dense graphs (like LastFM and Enron) may require a larger dimension. Empirically, setting the dimension to be $10*\\log(2\\mathrm{E})$ is enough to achieve satisfactory performance on all datasets, where $E$ is the number of edges. Results on more datasets can be found in Appendix G.4. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Temporal link prediction [17] aims at predicting future interactions based on historical topology, which is crucial for a series of real-world applications [2, 3, 18]. Earlier methods considered the temporal graph as a series of graph snapshots that are sampled at regularly-spaced timestamps [19, 20], which will lose the fine-grained temporal information due to ignoring the temporal orders of interactions in a graph snapshot. Recently, some continuous-time temporal graph learning methods have been proposed [21, 3, 14, 13], which consider the temporal graph as a sequence of interactions with irregular time intervals to fully capture the graph dynamics. For example, TGN [21] maintained a dynamic memory vector for each node and generated node representations by aggregating memory vectors via temporal graph attention to capture the evolution pattern of the temporal graph. However, capturing pairwise information by merely aggregating representations of neighboring nodes [22] is challenging. To address this issue, the link-wise method was proposed, which constructs relative encodings as additional node features to inject the pairwise information into the representation learning process [6, 7, 9, 8]. For example, Souza et al. [8] proposed a relative encoding based on temporal walk counting and theoretically showed that constructing the relative encodings can improve the expressive power of models in distinguishing different links. Despite these advances, existing ways to construct the relative encodings are still far from satisfactory, where computation efficiency is a main concern and temporal information is seldom considered. In this paper, we unify existing relative encodings into a function of temporal walk matrices and explore encoding the pairwise information effectively and efficiently by temporal walk matrix projection. ", "page_idx": 8}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/58a2c1bdea070a38fd7bc8b39d320dee6bdca0697faff89a1afa2d88ef4ddef1.jpg", "img_caption": ["Figure 4: Relative running time of different methods. The proportion of construction relative encoding time to all running time is marked in brackets for link-wise methods. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/4f2e550de8a2d7326af6523048eeff8c1de28dd90b18c0c1127f649d420927cd.jpg", "img_caption": ["Figure 5: Scalability analysis of TPNet on synthetic datasets with different sizes. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "One limitation of our method is that the matrix construction approach requires manual predefined settings. Different networks may necessitate distinct construction methods, potentially leading to additional human effort in experimenting with various approaches. For instance, the proposed temporal walk matrix that incorporates the time decay effect may not be optimal for networks characterized by long-term dependencies. Developing an adaptive matrix construction technique will be an interesting direction for future research. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the problem of pairwise information injection for temporal link prediction. We unify existing construction ways of relative encodings into a unified function, which reveals a connection between the relative encoding and temporal walk matrix. Then we propose a new temporal link prediction model, TPNet, to address the computational inefficiencies and the ignorance of temporal information in previous methods. TPNet introduces a new temporal walk matrix to simultaneously consider the temporal and structural information and a random feature propagation mechanism to maintain the temporal walk matrices efficiently. Theoretically, TPNet preserves the inner product of the maintained temporal walk matrices and empirically outperforms other link-wise methods in both effectiveness and efficiency. An interesting future direction may be designing an adaptive feature propagation mechanism. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (No. 62272023 and No. 62276015). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Petter Holme and Jari Saram\u00e4ki. Temporal networks. Physics reports, 519(3):97\u2013125, 2012. ", "page_idx": 10}, {"type": "text", "text": "[2] Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng, and Philip S. Yu. Continuous-time sequential recommendation with temporal graph collaborative transformer. In International Conference on Information and Knowledge Management, pages 433\u2013442. ACM, 2021.   \n[3] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In International Conference on Knowledge Discovery & Data Mining, pages 1269\u20131278. ACM, 2019.   \n[4] Fan Zhou, Xovee Xu, Goce Trajcevski, and Kunpeng Zhang. A survey of information cascade analysis: Models, predictions, and recent advances. ACM Comput. Surv., 54(2):27:1\u201327:36, 2022.   \n[5] Xiaodong Lu, Shuo Ji, Le Yu, Leilei Sun, Bowen Du, and Tongyu Zhu. Continuous-time graph learning for cascade popularity prediction. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 2224\u20132232, 2023.   \n[6] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation learning in temporal networks via causal anonymous walks. In 9th International Conference on Learning Representations, 2021.   \n[7] Yuhong Luo and Pan Li. Neighborhood-aware scalable temporal network representation learning. In Learning on Graphs Conference, 2022.   \n[8] Amauri H. Souza, Diego Mesquita, Samuel Kaski, and Vikas K. Garg. Provably expressive temporal graph networks. In Advances in Neural Information Processing Systems, 2022.   \n[9] Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. Towards better dynamic graph learning: New architecture and unified library. In Advances in Neural Information Processing Systems, 2023.   \n[10] Giang Hoang Nguyen, John Boaz Lee, Ryan A. Rossi, Nesreen K. Ahmed, Eunyee Koh, and Sungchul Kim. Continuous-time dynamic network embeddings. In Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, 2018.   \n[11] D Sivakumar. Algorithmic derandomization via complexity theory. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 619\u2013626, 2002.   \n[12] Santosh S Vempala. The random projection method, volume 65. American Mathematical Soc., 2005.   \n[13] Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, and Mehrdad Mahdavi. Do we really need complicated model architectures for temporal networks? In The Eleventh International Conference on Learning Representations, 2023.   \n[14] Da Xu, Chuanwei Ruan, Evren K\u00f6rpeoglu, Sushant Kumar, and Kannan Achan. Inductive representation learning on temporal graphs. In 8th International Conference on Learning Representations. OpenReview.net, 2020.   \n[15] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. In Advances in Neural Information Processing Systems, 2021.   \n[16] Farimah Poursafaei, Shenyang Huang, Kellin Pelrine, and Reihaneh Rabbany. Towards better evaluation for dynamic link prediction. In Advances in Neural Information Processing Systems, 2022.   \n[17] Meng Qin and Dit-Yan Yeung. Temporal link prediction: A unified framework, taxonomy, and review. ACM Comput. Surv., 56(4):89:1\u201389:40, 2024. doi: 10.1145/3625820. URL https://doi.org/10.1145/3625820.   \n[18] Le Yu, Zihang Liu, Leilei Sun, Bowen Du, Chuanren Liu, and Weifeng Lv. Continuous-time user preference modelling for temporal sets prediction. IEEE Trans. Knowl. Data Eng., 36(4): 1475\u20131488, 2024.   \n[19] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. Dysat: Deep neural representation learning on dynamic graphs via self-attention networks. In WSDM \u201920: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020, pages 519\u2013527. ACM, 2020.   \n[20] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao B. Schardl, and Charles E. Leiserson. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.   \n[21] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael M. Bronstein. Temporal graph networks for deep learning on dynamic graphs. CoRR, abs/2006.10637, 2020.   \n[22] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.   \n[23] Roman Vershynin. High-dimensional probability. University of California, Irvine, 10:11, 2020.   \n[24] Moses Charikar. Similarity estimation techniques from rounding algorithms. In John H. Reif, editor, Proceedings on 34th Annual ACM Symposium on Theory of Computing, 2002.   \n[25] Ping Li, Trevor Hastie, and Kenneth Ward Church. Very sparse random projections. In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Philadelphia, PA, USA, August 20-23, 2006, 2006.   \n[26] John Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles, computation, and applications. Cambridge University Press, 2022.   \n[27] James W Pennebaker, Martha E Francis, and Roger J Booth. Linguistic inquiry and word count: Liwc 2001. Mahway: Lawrence Erlbaum Associates, 71(2001):2001, 2001.   \n[28] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. Dyrep: Learning representations over dynamic graphs. In International Conference on Learning Representations, 2019.   \n[29] Peter J Diggle. Spatio-temporal point processes: methods and applications. Monographs on Statistics and Applied Probability, 107:1, 2006.   \n[30] Walter Rudin. Fourier analysis on groups. Courier Dover Publications, 2017.   \n[31] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.   \n[32] Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He, Le Song, Jingren Zhou, and Hongxia Yang. TCL: transformer-based dynamic graph modelling via contrastive learning. CoRR, abs/2105.07944, 2021.   \n[33] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In Annual Conference on Neural Information Processing Systems, 2021.   \n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Annual Conference on Neural Information Processing Systems, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Analysis of Existing Relative Encodings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 NAT ", "page_idx": 12}, {"type": "text", "text": "In Algorithm 1 of NAT, it maintains a series of hash maps $\\mathbf{\\boldsymbol{s}}_{u}^{(0)},\\mathbf{\\boldsymbol{s}}_{u}^{(1)},..,\\mathbf{\\boldsymbol{s}}_{u}^{(k)}$ for each node $u$ , which are sets of node ids. Next, we will first prove the statement holds if the size of the hash maps is infinite. ", "page_idx": 12}, {"type": "text", "text": "Statement. $\\mathbf{\\boldsymbol{s}}^{(i)}$ is a set of nodes that $u$ can approach through a temporal walk whose length is less than $i+1$ . ", "page_idx": 12}, {"type": "text", "text": "Initially, the $s_{u}^{(0)}$ is set as $\\{u\\}$ and $s_{u}^{(1)},..,s_{u}^{(k)}$ are set as empty set. The state . When a new interaction $(\\{u,v\\},t)$ occurs and if the size of the hash maps is infinite, for $1\\leq i\\leq k$ , the updating function of hash maps can be represented as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\bar{s}_{u}^{(i)}=s_{u}^{(i)}\\cup s_{v}^{(i-1)},\\ \\ \\ \\ \\bar{s}_{v}^{(i)}=s_{v}^{(i)}\\cup s_{u}^{(i-1)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\bar{s}_{u}^{(i)}$ and $s_{u}^{(i)}$ denote the hash map after and before adding the interaction respectively. Then if the timestamp of $(\\{u,v\\},t)$ is larger than that of previous interactions, the newly generated temporal walks beginning from $u$ must first visit $v$ through $(\\{u,v\\},t)$ (proved in Section B.1), thus the newly added nodes that $u$ can visit through a temporal walk with length less than $i+1$ must belong to s(vi\u22121). So s\u00af(ui) will contain all the nodes that u can approach through a temporal walk with length less than $i+1$ after adding $(\\{u,v\\},t)$ . The statement holds. ", "page_idx": 12}, {"type": "text", "text": "For a node $w$ , its similarity feature $r^{w|u}$ is a $(k+1)$ -dimensional vector, where for $1\\leq i\\leq k+1$ , riw|u= 1 if w \u2208s(ui and riw|u= 0 otherwise. Considering that the above statement holds, the similarity feature $r^{w|u}$ is equivalent to a $(k+1)$ -dimensional vector, where riw|u= 1 if the shortest temporal walk from $u$ to $w$ is less than $i$ ; otherwise, $r_{i}^{w|u}=0$ ", "page_idx": 12}, {"type": "text", "text": "Algorithm 2: Temporal Walk Extraction $(\\mathcal{G}(t),\\alpha,k,u)$ 1 Initialize $W$ to be $\\{(u,t)\\}$ ; 2 for $i$ from 1 to $k$ do 3 $(w_{\\mathrm{p}},t_{\\mathrm{p}})\\gets$ the last (node, time) pair in $W$ ; 4 Sample one $(\\{w_{p},w^{\\prime}\\},t^{\\prime})\\in\\bar{\\mathcal{E}_{w_{\\mathrm{p}},t_{\\mathrm{p}}}}$ with prob. $\\propto$ $\\exp(-\\alpha(t_{\\mathrm{p}}-t^{\\prime}))$ ; 5 $W_{i}\\gets W_{i}\\oplus(w^{\\prime},t^{\\prime})$ ; 6 Return W; ", "page_idx": 12}, {"type": "text", "text": "A.2 CAWN ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For constructing $r^{w|u}$ , CAWN first repeatedly samples $m$ temporal walks of length $k$ begging at $u$ according to the sampling strategy in Algorithm 2. Then $r^{w\\vert u}$ is set to be a $(k+1)$ -dimensional vector, where $r_{i}^{w|u}$ will be the number of walks whose i-th visited node is $w$ for $1\\leq i\\leq k+1$ , which can be represented as, ", "page_idx": 12}, {"type": "equation", "text": "$$\nr_{i}^{w|u}=\\sum_{j=1}^{m}\\mathbf{1}_{W_{j}[i][0]=w},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $W_{j}$ is the $\\mathrm{j}$ -th sampled walks and ${\\bf1}_{W_{j}[i][0]=w}$ is 1 if $W_{j}[i][0]\\,=\\,w$ ; otherwise, ${\\bf1}_{W_{j}\\,[i]\\,[0]=w}$ is 0. Due to each temporal walk being sampled independently, $\\mathbf{1}_{W_{1}[i][0]=w},\\cdot\\cdot\\cdot\\mathbf{\\cdot\\cdot\\nabla},\\mathbf{1}_{W_{m}[i][0]=w}$ are $m$ independent and identically distributed Bernoulli random variables $\\mathrm{Ber}(\\mu_{w}^{i-1})$ , where $\\mu_{w}^{i-1}$ is the probability of reaching $w$ from $u$ after performing a $(i-1)$ -step temporal walk according to Algorithm 2. And according to the strong law of large numbers (Theorem 1.3.1 of [23]), the mean of these random variables (i.e., $\\begin{array}{r}{\\frac{1}{m}\\sum_{j=1}^{m}\\mathbf{1}_{W_{j}[i][0]=w}\\iff\\frac{1}{m}r_{i}^{w|u})}\\end{array}$ will coverage to the mean as the number of sampled walks $m\\rightarrow\\infty$ . The mean of the $\\mathrm{Ber}(\\mu_{w}^{i-1})$ is $\\mu_{w}^{i-1}$ . Expand all $(i-1)$ -step temporal walks from $u$ to $w$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mu_{w}^{i-1}=\\sum_{W\\in\\mathcal{M}_{u,w}^{i-1}}f(W),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/956d61e066c6bfe4985e5b13ae89c63ff03f84d30e93bd90e2aa2e90cbfe7eb5.jpg", "img_caption": ["Figure 6: Illustration of the newly generated 3-step temporal walks beginning from $D$ after adding a new interaction $(\\{C,D\\},t_{4})$ There is a one-to-one map between $\\Delta\\bar{M_{D,*}^{3}}$ and $M_{C,*}^{2}(t_{4})$ . "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "where $f(\\cdot)$ is the sampled probability of a walk according to Algorithm 2. For Algorithm 2, if we are current at $(w_{i},t_{i})$ , then the probability of moving to $(w_{i+1},t_{i+1})$ through an interaction $(\\{w_{i},w_{i+1}\\},t_{i+1})$ is proportional to $\\exp(-\\alpha(t_{i}\\!-\\!t_{i+1}))$ , which is $\\frac{\\exp\\!\\big(\\!-\\!\\alpha\\big(t_{i}\\!-\\!t_{i+1}\\big)\\big)}{\\sum_{(w^{\\prime},t^{\\prime})\\in\\mathcal{E}_{w_{i},t_{i}}}\\exp\\!\\big(\\!-\\!\\alpha\\big(t_{i}\\!-\\!t^{\\prime}\\big)\\big)}$ . Thus the probability of a temporal walk $W=[(w_{0},t_{0}),(w_{1},t_{1}),..,(w_{k},t_{k})]$ is ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(W)=\\prod_{i=0}^{k-1}\\frac{\\exp(-\\alpha(t_{i}-t_{i+1}))}{\\sum_{(\\{w^{\\prime},w\\},t^{\\prime})\\in\\mathcal{E}_{w_{i},t_{i}}}\\exp(-\\alpha(t_{i}-t^{\\prime}))}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is the same as the score function $s^{\\prime}(\\cdot)$ of CAWN we shown in Section 2.2.2. Thus, $r_{i}^{w|u}$ (multiplied with a const $\\frac{1}{m}$ ) is the same as $\\sum_{W\\in\\mathcal{M}_{u,w}^{i-1}}s^{\\prime}(W)$ . ", "page_idx": 13}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The equation in Theorem 1 can be rewritten as $\\pmb{H}^{(l)}(t)\\ =\\ e^{\\lambda l t}\\,*\\,\\pmb{A}^{(l)}(t)\\pmb{P}$ . We can consider $\\bar{e^{\\lambda l t}}*A^{(l)}(t)$ as a new temporal walk matrix whose score function for a temporal walk $[(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{l},t_{l})]$ at time $t$ is $e^{\\lambda l t}*s(W)$ . Expanding it, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\ne^{\\lambda l t}\\ast s(W)=e^{\\lambda l t}\\ast\\prod_{j=1}^{l}e^{-\\lambda(t-t_{j})}=\\prod_{j=1}^{l}e^{-\\lambda(t-t_{j})}\\ast e^{\\lambda t}=\\prod_{j=1}^{l}e^{\\lambda t_{j}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We use the notation $\\bar{\\pmb{A}}^{(l)}(t)$ to denote $e^{\\lambda l t}\\,*\\,A^{(l)}(t)$ and $\\bar{s}(W)$ to denote $e^{\\lambda l t}\\ast s(W)$ for a $l$ - step temporal walk in the following proof. The original problem is transformed into proving that $\\bar{\\pmb{H}^{(l)}}(t)\\bar{\\pmb{\\Delta}}=\\bar{\\pmb{A}}^{(l)}(t)\\pmb{P}$ . Note that for a given temporal walk $[(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{l},t_{l})]$ , each term of $\\bar{s}(W)$ (i.e., $\\textstyle\\prod_{j=1}^{l}e^{\\lambda t_{j}})$ will no change as time $t$ goes on. Thus the temporal walk matrices $\\bar{\\pmb{A}}^{(l)}(t)$ will only change when a new interaction occurs. Next, we inspect how the $\\bar{\\pmb{A}}^{(l)}(t)$ changes when a new interaction $(u,v,t)$ occurs. For each element $\\bar{A}_{i,j}^{(l)}(t)$ , its change is caused by the newly generated $l$ -step temporal walks from $i$ to $j$ after adding the interaction $(u,v,t)$ , which can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\nA_{i,j}^{(l)}(t^{+})=A_{i,j}^{(l)}(t)+\\sum_{W\\in\\Delta M_{i,j}^{l}}\\bar{s}(W),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $t^{+}$ denotes the timestamps right after $t$ and $\\Delta M_{i,j}^{l}$ denote the newly generated $l$ -step temporal walks from $i$ to $j$ . According to the definition of the temporal walk, the new $l.$ - step temporal walks must begin from $u$ or $v$ . Because if there is a temporal walk that does not begin from $u$ or $v$ , it can be represented as $[(w_{0},t_{0}),..,(w_{i},t_{i}),(u,t_{i+1}),(v,t),..,(w_{l},t_{l}))]$ or $\\left[(w_{0},t_{0}),..,(w_{i},t_{i}),(v,t_{i+1}),(u,t),..,(w_{l},t_{l})\\right)\\right]$ , which means that there is an interaction $(\\{w_{i},u\\},t_{i+1})$ or $(\\{w_{i},v\\},t_{i+1})$ whose timestamp $t_{i}$ is larger than $t$ . (Since the timestamps of a temporal walk are decreasing). This is impossible since $(u,v,t)$ is a newly happened interaction. Thus only the $u$ -th row and $v$ -th row of the $\\bar{\\pmb{A}}^{(l)}(t)$ will change. Besides for each newly generated temporal walk from $u$ , it must be $[(u,t),(v,t_{1}),..,(w_{l},t_{l}))]$ , where $[(v,t_{1}),..,(w_{l},t_{l}))]$ corresponds to a $(l-1)$ -step temporal walk beginning from $v$ . And for any $(l-1)$ -step temporal walk beginning from $v$ , we can add a prefix $(u,t)$ to make it become a $l$ -step temporal walk beginning from $u$ . Thus for any $1\\leq i\\leq n$ , there is a one-to-one mapping between $\\Delta M_{u,i}^{l}$ and $M_{v,i}^{l-1}(t)$ with $M_{v,i}^{l-1}(t)$ denote the set of $(l-1)$ -th temporal walks from $v$ to $i$ before $t$ , and we can rewritten Equation (12) as ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nA_{u,i}^{(l)}(t^{+})=A_{u,i}^{(l)}(t)+\\sum_{W\\in M_{v,i}^{l-1}(t)}e^{-\\lambda t}*\\bar{s}(W)=A_{u,i}^{(l)}(t)+e^{-\\lambda t}*A_{v,i}^{(l-1)}(t),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The updating function for $\\bar{A}_{v,i}^{(l)}(t)$ is also similar. We give a visual illustration of the new temporal walks in Figure 6 for better understanding. Finally, writing the update formula in vector form, for any $1\\leq l\\leq k$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bar{A}_{u}^{(l)}(t^{+})=\\bar{A}_{u}^{(l)}(t)+e^{\\lambda t}*\\bar{A}_{v}^{(l-1)}(t),\\quad\\bar{A}_{v}^{(l)}(t^{+})=\\bar{A}_{v}^{(l)}(t)+e^{\\lambda t}*\\bar{A}_{u}^{(l-1)}(t),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is the same as Equation (14)! Thus if $\\pmb{H}^{(l)}(t)=\\Bar{\\pmb{A}}^{(l)}(t)\\pmb{P}$ for $0\\leq l\\leq k$ , after adding a new interaction $(u,v,t)$ , for $0\\leq l\\leq k$ , $\\pmb{H}^{(l)}(t^{+})=\\bar{\\pmb{A}}^{(l)}(t^{+})\\pmb{P}$ still holds. Because we have ", "page_idx": 14}, {"type": "text", "text": "$\\pmb{H}_{u}^{(l)}(t^{+})=\\pmb{H}_{u}^{(l)}(t)+e^{\\lambda t}*\\pmb{H}_{v}^{(l-1)}(t)=(\\bar{\\pmb{A}}_{u}^{(l)}(t)+e^{\\lambda t}*\\bar{\\pmb{A}}_{v}^{(l-1)}(t))\\pmb{P}=\\bar{\\pmb{A}}_{u}^{(l)}(t^{+})\\pmb{P}$ (15) Note that the equation in Equation (1) holds at initialization, thus the equation always holds and the theorem is proved. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Theorem 2 can be considered as a special case of the Johnson-Lindenstrauss Lemma [11], where the random projection [12, 24, 25] can preserve the norm and inner product. For the convenience of readers lacking relevant background, we follow [26] to provide the proof under the given conditions of this paper. We begin the proof by the following lemma. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 ((Lemma 3.18 of [26])). Let $\\pmb{x}\\in\\mathbb{R}^{d}$ be a $d$ -dimensional random vector whose entries are independent $\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d}})$ . Then for any $\\epsilon\\in[0,1]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\|\\pmb{x}\\|_{2}^{2}-1\\right|>\\epsilon\\right)\\le2\\exp(\\frac{-\\epsilon^{2}d}{8})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following part can be divided into 1) We first give two corollaries and their proofs. 2) Then we give the proof of Theorem 2 based on the corollaries. ", "page_idx": 14}, {"type": "text", "text": "B.2.1 Two Corolarries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Based on the above lemma, we can get the following corollaries. ", "page_idx": 14}, {"type": "text", "text": "Corollary 1. Given any $\\epsilon\\in(0,1),\\mathbf{{x}}\\in\\mathbb{R}^{m}$ . Let $\\b{P}\\in\\mathbb{R}^{d\\times m}$ be a random matrix whose entries are independent $\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d}})$ , if $\\begin{array}{r}{\\dot{d}\\geq\\frac{8}{\\epsilon^{2}}\\log(\\frac{1}{\\delta})}\\end{array}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((1-\\epsilon)\\|\\pmb{x}\\|_{2}^{2}\\leq\\|\\pmb{P}\\pmb{x}\\|_{2}^{2}\\leq(1+\\epsilon)\\|\\pmb{x}\\|_{2}^{2}\\right)\\geq1-2\\delta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For the above corollary, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left((1-\\epsilon)\\|\\pmb{x}\\|_{2}^{2}\\leq\\|\\pmb{P}\\pmb{x}\\|_{2}^{2}\\leq(1+\\epsilon)\\|\\pmb{x}\\|_{2}^{2}\\right)\\geq1-2\\delta}\\\\ {\\iff\\mathbb{P}\\left(\\left|\\frac{\\|\\pmb{P}\\pmb{x}\\|_{2}^{2}}{\\|\\pmb{x}\\|_{2}^{2}}-1\\right|\\leq\\epsilon\\right)\\geq1-2\\delta}\\\\ {\\iff\\mathbb{P}\\left(\\left|\\frac{\\|\\pmb{P}\\pmb{x}\\|_{2}^{2}}{\\|\\pmb{x}\\|_{2}^{2}}-1\\right|>\\epsilon\\right)\\leq2\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that each entry of $_{P}$ is an independent $\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d}})$ , thus $_{P x}$ is a random vector whose each entry $\\begin{array}{r}{(P x)_{k}=\\sum_{i=1}^{m}P_{k,i}*x_{i}}\\end{array}$ is an independent $\\mathcal{N}(0,\\frac{||\\mathbf{\\boldsymbol{x}}||_{2}^{2}}{d})$ and each entry of $\\frac{P x}{\\|x\\|_{2}}$ is an independent $\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d}})$ . Substituting it to Lemma 1 and taking $\\begin{array}{r}{d\\geq\\frac{8}{\\epsilon^{2}}\\log(\\frac{1}{\\delta})}\\end{array}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|\\frac{\\|\\pmb{P}\\pmb{x}\\|_{2}^{2}}{\\|\\pmb{x}\\|_{2}^{2}}-1\\right|>\\epsilon\\right)\\leq2\\exp(\\frac{-\\epsilon^{2}d}{8})}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\leq2\\exp(\\frac{-\\epsilon^{2}}{8}*\\frac{8}{\\epsilon^{2}}*\\log(\\frac{1}{\\delta}))\\leq2\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Based on Corollary 1, we can get the following corollary. ", "page_idx": 14}, {"type": "text", "text": "Corollary 2. Given any n m-dimensional vectors $\\pmb{x}_{1},...,\\pmb{x}_{n}$ , $\\epsilon\\in(0,1)$ , let $P\\in\\mathbb{R}^{d\\times m}$ be a random matrix whose entries are independent $\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d}})$ , if $\\begin{array}{r}{d\\geq\\frac{24}{\\epsilon^{2}}\\log(4^{1/3}n).}\\end{array}$ , for any $1\\leq i,j\\leq n$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\langle P x_{i},P x_{j}\\rangle-\\langle x_{i},x_{j}\\rangle|\\leq\\frac{\\epsilon}{2}(\\|x_{i}\\|_{2}^{2}+\\|x_{j}\\|_{2}^{2})\\right)\\geq1-\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "let $E_{i,j}^{+}$ and $E_{i,j}^{-}$ denote the event of $\\{||P(x_{i}+x_{j})||_{2}^{2}-||x_{i}+x_{j}||_{2}^{2}|\\ \\leq\\ \\epsilon||x_{i}+x_{j}||_{2}^{2}\\}$ and $\\{|\\|P(\\mathbf{\\bar{x}}_{i}-\\mathbf{\\bar{x}}_{j})\\|_{2}^{2}-\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\|_{2}^{2}\\}\\leq\\epsilon\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\|_{2}^{2}\\}$ respectively. Since $\\mathbf{\\boldsymbol{x}}_{i}+\\mathbf{\\boldsymbol{x}}_{j}$ and $\\mathbf{{x}}_{i}-\\mathbf{{x}}_{j}$ can also be consider two $m$ -dimensional vectors. Thus take it and $\\begin{array}{r}{d\\ge\\frac{24}{\\epsilon^{2}}\\log(4^{1/3}n)}\\end{array}$ into Corollary 1, we have $\\begin{array}{r}{\\mathbb{P}(E_{i,j}^{+})\\ge1-\\frac{1}{2n^{3}}}\\end{array}$ and $\\begin{array}{r}{\\mathbb{P}(E_{i,j}^{-})\\ge1-\\frac{1}{2n^{3}}}\\end{array}$ . Then let $C_{i,j}=E_{i,j}^{+}\\cap E_{i,j}^{-}$ denote that $E_{i,j}^{+}$ and $E_{i,j}^{-}$ hold simultaneously, according to the union bound, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(C_{i,j})=1-\\mathbb{P}(\\overline{{E_{i,j}^{+}}}\\cup\\overline{{E_{i,j}^{-}}})\\ge1-\\big(\\mathbb{P}(\\overline{{E_{i,j}^{+}}})+\\mathbb{P}(\\overline{{E_{i,j}^{-}}})\\big)\\ge1-\\frac{1}{n^{3}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\overline{{E_{i,j}^{+}}}$ denote that $E_{i,j}^{+}$ does not hold and $\\overline{{E_{i,j}^{+}}}\\cup\\overline{{E_{i,j}^{-}}}$ denotes that $\\overline{{E_{i,j}^{+}}}$ or $\\overline{{E_{i,j}^{-}}}$ holds. According to the union bound, we further have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\cap_{i,j}C_{i,j})=1-\\mathbb{P}(\\cup_{i,j}\\overline{{C_{i,j}}})\\ge1-\\sum_{i,j}\\mathbb{P}(\\overline{{C_{i,j}}})\\ge1-n^{2}*\\frac{1}{n^{3}}\\ge1-\\frac{1}{n},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\cap_{i,j}C_{i,j}$ denote that $C_{i,j}$ holds for any $i,j$ and $\\cup_{i,j}\\overline{{C_{i,j}}}$ denote that there exist $i,j$ that $\\overline{{C_{i,j}}}$ holds. If $C_{i,j}$ holds, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{(1-\\epsilon)\\|x_{i}+x_{j}\\|_{2}^{2}\\leq\\|P(x_{i}+x_{j})\\|_{2}^{2}\\leq(1+\\epsilon)\\|x_{i}+x_{j}\\|_{2}^{2}}\\\\ &{(1-\\epsilon)\\|x_{i}-x_{j}\\|_{2}^{2}\\leq\\|P(x_{i}-x_{j})\\|_{2}^{2}\\leq(1+\\epsilon)\\|x_{i}-x_{j}\\|_{2}^{2}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Multiplying Equation (24) with $-1$ and adding it to (23), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\langle P x_{i},P x_{j}\\rangle-\\langle x_{i},x_{j}\\rangle|\\leq\\frac{\\epsilon}{2}(\\|\\pmb{x}_{i}\\|_{2}^{2}+\\|\\pmb{x}_{j}\\|_{2}^{2})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\cap_{i,j}C_{i,j})\\ge1-\\frac{1}{n}\\implies\\mathbb{P}(|\\langle P x_{i},P x_{j}\\rangle-\\langle x_{i},x_{j}\\rangle|\\le\\frac{\\epsilon}{2}(\\|x_{i}\\|_{2}^{2}+\\|x_{j}\\|_{2}^{2}))\\ge1-\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds for any $1\\leq i,j\\leq n$ . ", "page_idx": 15}, {"type": "text", "text": "B.2.2 Proof based on Corollaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The matrix of $A^{(l)}(t)$ in Theorem 2 can be considered as $n$ vectors with $n$ dimensions, where each row of $A^{(l)}(t)$ is a $n$ -dimensional vector. Similarly, we can consider $\\mathbf{\\boldsymbol{A}}^{(0)}(t),\\cdot\\cdot\\cdot\\mathbf{\\boldsymbol{A}}^{(k)}(t)$ together as $n(k+1)$ vectors with $n$ dimensions. Considering that $P\\in\\mathbb{R}^{n\\times d_{R}}$ is a random matrix where each entry is an independent $\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d_{R}}})$ and $e^{-\\lambda l t}*H^{(l)}(t)$ is the projection of $A^{(l)}(t)$ , substitute it into Corollary 2 and taking the number of vectors as $(k+1)n$ , we can get Theorem 2. ", "page_idx": 15}, {"type": "text", "text": "C Batch Updating Mechanism ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For the situation where multiple interactions happen simultaneously, we can first compute each interaction\u2019s contribution independently and sum them together to update the node representations. The maintaining mechanism is shown in Algorithm 3, where we packed the interactions that happen at the same time into a set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and sum the independent contribution of each interaction in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ into $\\Delta H^{(1)},\\cdot\\cdot\\cdot,\\Delta H^{(k)}$ . For simplicity, we initialize $\\Delta H^{(1)},\\cdot\\cdot\\cdot,\\Delta H^{(k)}$ each time and it can be replaced by some efficient implementation such as scatter_add operation in pytorch. ", "page_idx": 15}, {"type": "text", "text": "1 Initialize $\\pmb{H}^{(0)},\\pmb{H}^{(1)},...,\\pmb{H}^{(k)}\\in\\mathbb{R}^{n\\times d_{R}}$ as zero matrix ;   \n2 Fill $H^{(0)}$ with entries independently drawn from $\\textstyle{N(0,{\\frac{1}{d_{R}}})}$ ; ", "page_idx": 16}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/b3fd6e3c07ea2a2eae6690518ce08fc96764aba13b164d3ed686e22f1a0344f9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Propagation Mechanism for Other Matrices ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For simplicity, here we only consider the situation where the timestamp of each interaction is different and we can adopt a similar batch updating mechanism in Section C to handle the situation where multiple interactions happen simultaneously. For the updating of other types of temporal walk matrices, let $\\pmb{A}(t)=[\\pmb{A}^{(\\hat{0})}(t),\\cdot\\cdot\\cdot\\cdot,\\pmb{A}^{(k)}(t)]\\stackrel{\\cdot}{\\in}\\mathbb{R}^{n(k+1)\\times\\hat{d}}$ denotes the concatenation of the temporal walk matrices. If the following two situations can be satisfied simultaneously, we can apply the random feature propagation mechanism to implicitly maintain the temporal walk matrices. ", "page_idx": 16}, {"type": "text", "text": "Condition 1. After adding a new interaction $(u,v,t)$ , the change of each row can be written as the linear combination of other rows, which is for each $1\\leq i\\leq n(k+1)$ , there exists $k_{1},...,k_{m}$ and $l_{1},...,l_{m}$ satisfying. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{A}_{i}(t^{+})=k_{1}\\pmb{A}_{l_{1}}(t)+\\cdot\\cdot\\cdot+k_{m}\\pmb{A}_{l_{m}}(t),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $t^{+}$ is the time right after $t$ . ", "page_idx": 16}, {"type": "text", "text": "Condition 2. After time moving $\\Delta t$ without adding new interactions, the change of each row can be written as the linear combination of other rows, which is for each $1\\leq i\\leq n(\\bar{k}+1)$ , there exists $k_{1},...,k_{m}$ and $l_{1},...,l_{m}$ satisfying. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{A}_{i}(t+\\Delta t)=k_{1}\\pmb{A}_{l_{1}}(t)+\\cdot\\cdot\\cdot+k_{m}\\pmb{A}_{l_{m}}(t)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The motivation behind the conditions is that the projection is a linear operation. If the node representations are the projection of the temporal walk matrix at $t$ (i.e., $\\pmb{H}(t)=\\pmb{A}(t)\\pmb{P})$ and the updating function of the temporal walk matrix is the linear combination of other rows, then we can apply the same updating function on $H(t)$ , which will make $H(t^{+})$ still be the projection of the temporal walk matrix. For example, applying (27) to $H(t)$ will get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{H}_{i}(t^{+})=k_{1}\\pmb{H}_{l_{1}}(t)+\\cdot\\cdot\\cdot+k_{m}\\pmb{H}_{l_{m}}(t)}\\\\ &{\\qquad\\quad=(k_{1}\\pmb{A}_{l_{1}}(t)+\\cdot\\cdot\\cdot+k_{m}\\pmb{A}_{l_{m}}(t))\\pmb{P}=\\pmb{A}_{i}(t^{+})\\pmb{P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we can ensure that $H(t)$ is always the random projection of $\\mathbf{\\boldsymbol{A}}(t)$ and thus preserve the inner product of the $A(t)$ . ", "page_idx": 16}, {"type": "text", "text": "D.1 Detailed Updating Mechanism ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Based on the above analysis, we give the detailed propagation mechanism of methods in Section 2. For NAT, PINT and DyGFormer, their temporal matrix element $A_{u,v}^{(l)}(t)$ is the number of the $l_{\\cdot}$ -step temporal walks from $u$ to $v$ and their feature propagation mechanism is shown in Algorithm 4, where the obtained node representations are the projection of the corresponding temporal walk matrix. ", "page_idx": 16}, {"type": "text", "text": "For CAWN, its score function for a temporal walk $W=[(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{l},t_{l})]$ is defined as $\\begin{array}{r}{s(W)=\\prod_{i=0}^{l-1}\\frac{\\exp\\left(-\\alpha\\left(t_{i}-t_{i+1}\\right)\\right)}{\\sum_{\\left(\\left\\{w^{\\prime},w\\right\\},t^{\\prime}\\right)\\in\\mathcal{E}_{w_{i},t_{i}}}\\exp\\left(-\\alpha\\left(t_{i}-t^{\\prime}\\right)\\right)}}\\end{array}$ . As time goes on, its element of temporal walk ", "page_idx": 16}, {"type": "text", "text": "1 Initialize $\\pmb{H}^{(0)},\\pmb{H}^{(1)},...,\\pmb{H}^{(k)}\\in\\mathbb{R}^{n\\times d_{R}}$ as zero matrix ;   \n2 Fill $H^{(0)}$ with entries independently drawn from $\\textstyle{N(0,{\\frac{1}{d_{R}}})}$ ;   \n3 for do   \n4 for $l=k$ to 1 do   \n5 $H_{u}^{(l)}=H_{u}^{(l)}+H_{v}^{(l-1)}$   \n6 $H_{v}^{(l)}=H_{v}^{(l)}+H_{u}^{(l-1)}$ ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Algorithm 5: Propagation Mechanism for CAWN $(\\mathcal{G},\\alpha,k,n,d_{R})$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1 Initialize $\\pmb{H}^{(0)},\\pmb{H}^{(1)},...,\\pmb{H}^{(k)}\\in\\mathbb{R}^{n\\times d_{R}}$ as zero matrix ;   \n2 Fill $H^{(0)}$ with entries independently drawn from $\\textstyle{N(0,{\\frac{1}{d_{R}}})}$ ;   \n3 Initialize $\\pmb{d}\\in\\mathbb{R}^{n}$ as zero vector ;   \n4 Set $t_{\\mathrm{prev}}\\in\\mathbb{R}$ to be zero ;   \n5 for $(\\dot{\\boldsymbol{u}},\\boldsymbol{v},t)\\in\\boldsymbol{\\mathcal{G}}$ do   \n6 $d=d*\\exp(-\\alpha(t-t_{\\mathrm{prev}}))$ ;   \n7 for $l=k$ to 1 do   \n8 $\\begin{array}{r}{H_{u}^{(l)}=\\frac{d_{u}}{d_{u}+1}*H_{u}^{(l)}+\\frac{1}{d_{u}+1}*H_{v}^{(l-1)}}\\end{array}$   \n9 $\\begin{array}{r}{H_{v}^{(l)}=\\frac{d_{v}}{d_{v}+1}*H_{v}^{(l)}+\\frac{1}{d_{v}+1}*H_{u}^{(l-1)}}\\end{array}$   \n10 $d_{u}=d_{u}+1$ ;   \n11 $d_{v}=d_{v}+1$ ;   \n12 $t_{\\mathrm{prev}}=t$ ;   \n13 Return ${H^{(0)},H^{(1)},..,H^{(k)}}$ ; ", "page_idx": 17}, {"type": "text", "text": "matrices will not change. When a new interaction $(u,v,t)$ happens, for a temporal walk matrix $A^{(l)}(t)$ , its $u$ -th row and $v$ -th row will change. Formally, let $\\pmb{d}(t)\\in\\mathbb{R}^{n}$ denotes a time decay degree vector, where for each node $u$ , $d_{u}(t)$ is defined as $\\begin{array}{r}{d_{u}(t)=\\sum_{(\\{(u,v^{\\prime}\\},t^{\\prime})\\in\\mathcal{E}_{u,t}}\\exp(-\\alpha(t^{\\prime}-t))}\\end{array}$ with $\\mathcal{E}_{u,t}$ denoting the set of interactions attached to $u$ before $t$ , then the updating function of the temporal walk matrix $A^{(k)}(t)$ can be represented as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{A}_{u}^{(l)}(t^{+})=\\frac{d_{u}(t)}{d_{u}(t)+1}\\ast\\pmb{A}_{u}^{(l)}(t)+\\frac{1}{d_{u}(t)+1}\\ast\\pmb{A}_{v}^{(l-1)}(t),}\\\\ {\\pmb{A}_{v}^{(l)}(t^{+})=\\frac{d_{v}(t)}{d_{v}(t)+1}\\ast\\pmb{A}_{v}^{(l)}(t)+\\frac{1}{d_{v}(t)+1}\\ast\\pmb{A}_{u}^{(l-1)}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\frac{d_{u}(t)}{d_{u}(t){+}1}*A_{u}^{(l)}(t)$ corresponds to the change in score of the old $l$ -step temporal walks begging from $u$ and $\\begin{array}{r}{\\frac{1}{d_{u}(t)+1}*A_{v}^{(l-1)}(t)}\\end{array}$ correspond to change caused by the newly generated $l$ -step temporal walk from $u$ (the same for $v$ and similar analysis about the updating of temporal walk matrix can be found in Appendix B.1). Then we can give the propagation mechanism for CAWN in Algorithm 5 based on the above analysis, where $^d$ corresponds to the time decay degree vector. ", "page_idx": 17}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We proposed an effective and efficient temporal link prediction method, which may advance realworld scenarios that rely on link prediction as a cornerstone, such as recommendation systems. For potential negative impacts, overly accurate link prediction in some contexts may lead to imbalanced outcomes such as reduced diversity in recommendation systems. ", "page_idx": 17}, {"type": "text", "text": "F Experimental Settings ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Experiments are conducted on the following 13 benchmark datasets 3 collected by [16]. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Wikipedia is a bipartite interaction graph that records the edits on Wikipedia pages over a month. Nodes represent users and pages, and links denote the editing behaviors with timestamps. Each link is associated with a 172-dimensional Linguistic Inquiry and Word Count (LIWC) feature [27].   \n\u2022 Reddit is a bipartite graph capturing user posts under subreddits over a month. Nodes represent users and subreddits, while links denote timestamped posts. Each link carries a 172-dimensional LIWC feature.   \n\u2022 MOOC is a bipartite interaction network of online courses, where nodes represent students and course content units (e.g., videos, problem sets). Links indicate students\u2019 access to specific content units and have a 4-dimensional feature.   \n\u2022 LastFM is a bipartite network detailing song-listening behaviors of users over one month. Nodes are users and songs, and links represent listening activities.   \n\u2022 Enron records email communications between employees of the Enron Energy Corporation over three years.   \n\u2022 Social Evo. is a mobile phone proximity network tracking daily activities within an undergraduate dormitory for eight months, with each link having a 2-dimensional feature.   \n\u2022 UCI is an online communication network with nodes representing university students and links representing posted messages.   \n\u2022 Flights is a dynamic flight network showing air traffic development during the COVID-19 pandemic. Nodes represent airports, and links denote tracked filghts. Each link has a weight indicating the number of flights between two airports per day.   \n\u2022 Can. Parl. is a dynamic political network recording interactions between Canadian Members of Parliament (MPs) from 2006 to 2019. Nodes represent MPs from electoral districts, and links are formed when two MPs vote \"yes\" on a bill. The weight of each link represents the number of times one MP voted \"yes\" in support of another MP in a year.   \n\u2022 US Legis. is a senate co-sponsorship network tracking social interactions between US Senators. The weight of each link indicates the number of times two senators co-sponsored a bill in a given congress.   \n\u2022 UN Trade captures food and agriculture trade between 181 nations over more than 30 years. The weight of each link represents the total normalized agriculture import or export values between two countries.   \n\u2022 UN Vote records roll-call votes in the United Nations General Assembly. A link between two nations increases in weight each time both vote \"yes\" on an item.   \n\u2022 Contact tracks the evolution of physical proximity among approximately 700 university students over a month. Each student has a unique identifier, and links indicate close proximity, with weights revealing the extent of physical closeness between students. ", "page_idx": 18}, {"type": "text", "text": "The statistics of the datasets are shown in Table 3, where #N&L Feat stands for the dimensions of node and link features. ", "page_idx": 18}, {"type": "text", "text": "F.2 Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We select the following eleven popular baselines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 JODIE [3] designs a recurrent architecture to maintain a memory vector for each node and a projection layer to map the node memories into future representation trajectories. \u2022 DyRep [28] considers each link as a temporal point process [29] and designs a deep temporal point process model to capture the dynamics of the observed process. ", "page_idx": 18}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/a053ad00856f3da404e84f77c0c4d08b100265f05a6051d263111c5bf225ee62.jpg", "table_caption": ["Table 3: Statistics of the datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "\u2022 TGAT [14] proposes a time encoding function based on Bochner\u2019s Theorem [30] and combines it with the graph attention mechanism [31] to learn dynamic node representations.   \n\u2022 TGN [21] proposes a general framework for temporal graph learning, which includes a memory module to maintain node memories and an embedding module to aggregate node memories.   \n\u2022 CAWN [6] captures the evolution pattern of the temporal graph by causal anonymous walks. For a given link, CAWN first sampled a set of temporal walks beginning from the two end nodes respectively and constructs relative encodings. The sampled walks together with relative encodings are then mapped into node representations via a sequential model.   \n\u2022 EdgeBank [16] is a statistical method, that gives the likelihood of a link based on historical interactions between the two end nodes.   \n\u2022 TCL [32] propose a graph transformer architecture [33] to learning node representations, which samples neighbor nodes based on BFS and maps them into the node representation.   \n\u2022 NAT [7] propose a dictionary-type neighborhood representation to efficiently capture the correlation information between nodes, which maintains a series of N-caches to store the neighborhood information and use them to decode the pairwise information between nodes.   \n\u2022 PINT [8] proposes an injective temporal message passing mechanism to learn node representations and relative positional features constructed based on temporal walk counting to inject pairwise information between nodes.   \n\u2022 GraphMixer [13] proposes a simplified temporal graph learning architecture, which employs MLP-Mixer to learn the representation of a node from its historical interaction sequences.   \n\u2022 DyGFormer [9] proposes a transformer architecture [34] to learning node representations, which include a patching technique to capture the long-term histories of a node to and a neighbor co-occurrence encoding scheme to capture the correlation between nodes. ", "page_idx": 19}, {"type": "text", "text": "G Additional Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "G.1 Performance Comparison ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The results under other settings are shown in Table 6, Table 7, Table 8, Table 9 and Table 10. ", "page_idx": 19}, {"type": "text", "text": "G.2 Efficicency Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Efficiency analysis results on Reddit, UCI, and Wikipedia are shown in Figure 7. ", "page_idx": 19}, {"type": "text", "text": "G.3 Scalability Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The scalability analysis result of PINT is shown in Table 4, where OOM indicates the out-of-memory error. ", "page_idx": 19}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/710fff18f484dcad734752f63bcb1649e3585c95bfd19ccd62f0642808babbdf.jpg", "img_caption": ["Figure 7: Efficiency analysis on more datasets. "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/db2164670c04d3d6e6ad0665a802740b63c03a045e8fb8db060fc7374da151a8.jpg", "table_caption": ["Table 4: Scalability analysis of PINT. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/24584b6fde143da51c139339b7b49ae1fa0ebd7f7496c524fe7dd2c50a8f8216.jpg", "table_caption": ["Table 5: Average norm of node representations from different layers. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "G.4 Influence of Node Representation Dimension ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Results on Wikipedia, Enron, and UCI are shown in Figure 8. ", "page_idx": 20}, {"type": "text", "text": "G.5 Statistic Analysis of Node Representations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 5 shows the mean of node representation norm at different layers, where aEb indicates $a\\times10^{b}$ . ", "page_idx": 20}, {"type": "image", "img_path": "Ti3ciyqlS3/tmp/a23d654cc6a2a53c79023b5ca6151c33bbfc666112d78a8fe4a8334171dc1bb9.jpg", "img_caption": ["Figure 8: Influence of Node Representation Dimension "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/a7e879b3d66764f0b923048acbf8526c62f58163f7ee00a99f083c0685039d3e.jpg", "table_caption": ["Table 6: Inductive results for random negative sampling. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/ce68e13632cfa95517c48a47206f6be4f7e98b732ec0e0f0483eee1701638066.jpg", "table_caption": ["Table 7: Transductive results for historical negative sampling "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/311e6b3df738c18f23bfc3ed2520f29a8b88e21de61a15a405b599e288be942a.jpg", "table_caption": ["Table 8: Inductive results for historical negative sampling "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/09d271ef48b3de4480bdd56ab7e80a8b7fe52a26d651f6f8d91b83408c494f43.jpg", "table_caption": ["Table 9: Transductive results for inductive negative sampling "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Ti3ciyqlS3/tmp/47ae0f5cbe4f026967285fbeaadd4d2e939005e6c69efaa10213ba72a720dabe.jpg", "table_caption": ["Table 10: Inductive results for inductive negative sampling. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The abstract and introduction list the limitations of existing relative encodings, and the goal of this paper is to tackle such limitations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitation in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All theorems include the full sets of assumptions and proof. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the implementation details of our method in the appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We include our codes in the supplemental material. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Implementation details are shown in the appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report the mean and standard deviation in tables that record the experimental results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We list the compute resources of our experiments in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper follows the NeurIPS Code of Ethics in every respect. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the potential societal impacts in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The proposed method does not have a high risk for misuse. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We follow the license of each asset and cite them properly. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not provide new assets in this paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include crowdsourcing experiments and research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include crowdsourcing experiments and research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]