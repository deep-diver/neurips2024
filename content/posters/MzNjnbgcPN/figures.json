[{"figure_path": "MzNjnbgcPN/figures/figures_3_1.jpg", "caption": "Figure 1: An illustration of OptEx at iteration t.", "description": "This figure illustrates the OptEx framework at a given iteration *t*. It shows the parallel processes involved in approximating the gradients using the kernelized gradient estimation and subsequently using these gradients to execute the standard FOO algorithms in parallel. Process 1 begins with the previous iteration's result (\u03b8t-1). Each process involves using the kernelized gradient estimation, applying the FO-OPT algorithm, sampling *f* to evaluate the gradient, and updating the history of gradients. The output of the last process in the parallel loop is taken to be the input for the next iteration.", "section": "4 The Optex Framework"}, {"figure_path": "MzNjnbgcPN/figures/figures_3_2.jpg", "caption": "Figure 1: An illustration of OptEx at iteration t.", "description": "This figure illustrates the OptEx framework at iteration *t*.  It shows how, unlike standard first-order optimization methods, OptEx uses a kernelized gradient estimation to predict gradients for multiple iterations. This enables parallel processing of these iterations (represented by the parallel processes 1 through *N*).  The output of each parallel process is used to refine the gradient estimation and inform the next set of parallel iterations. The ultimate outcome is a final parameter update \u03b8*t* at the end of iteration *t*. This approach improves efficiency by approximating parallelization of inherently sequential steps.", "section": "4 The Optex Framework"}, {"figure_path": "MzNjnbgcPN/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison of the number of sequential iterations T (x-axis) required by different methods to achieve the same optimality gap F(\u03b8) \u2013 inf\u03b8 F(\u03b8) (y-axis) for various synthetic functions. The parallelism N is set to 5 and each curve denotes the mean from 5 independent runs.", "description": "This figure compares the number of sequential iterations needed to achieve a certain optimality gap for four different synthetic functions using three different optimization methods: Vanilla (standard FOO), Target (idealized parallelized FOO), and OptEx (the proposed method). The x-axis represents the number of sequential iterations, and the y-axis shows the optimality gap.  The parallelism (N) is set to 5 for all methods.  Each data point is the average of 5 independent runs.  The figure demonstrates that OptEx significantly reduces the number of iterations compared to the Vanilla method, approaching the performance of the idealized Target method.", "section": "6.1 Synthetic Function Minimization"}, {"figure_path": "MzNjnbgcPN/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of the cumulative average reward (y-axis) achieved by different methods to train DQN on RL tasks under various parameter dimension d and a varying number of sequential episodes T (x-axis). The parallelism N is set to 4 and each curve denotes the mean from 3 independent runs.", "description": "This figure compares the performance of Vanilla, Target, and OptEx in training Deep Q-Networks (DQNs) on four different reinforcement learning tasks from the OpenAI Gym suite.  The y-axis represents the cumulative average reward, while the x-axis shows the number of sequential iterations (episodes).  Each task has a different parameter dimension (d), indicated in the title of each subplot.  The parallelism (N) is fixed at 4 for all methods, and each data point is the mean reward over 3 independent runs.  OptEx consistently outperforms Vanilla and performs comparably to Target (which has access to perfect gradient information, but is unrealistic in a real-world setting). This demonstrates OptEx's ability to improve the efficiency of reinforcement learning.", "section": "6.2 Reinforcement Learning"}, {"figure_path": "MzNjnbgcPN/figures/figures_8_2.jpg", "caption": "Figure 4: Comparison of the test error or training loss (y-axis) achieved by different optimizers when training deep neural networks on (a) CIFAR-10 and (b) Shakespeare Corpus with a varying number T of sequential iterations or a varying wallclock time (x-axis). The parallelism N is set to 4 and each curve denotes the mean from 5 (for CIFAR-10) or 3 (for Shakespeare corpus) independent runs. The wallclock time is evaluated on a single NVIDIA RTX 4090 GPU.", "description": "This figure compares the performance of Vanilla, Target, and OptEx optimizers on CIFAR-10 and Shakespeare Corpus datasets.  It shows test error and training loss (log scale) against sequential iterations and wall-clock time.  Parallelism (N) is fixed at 4.  The results highlight OptEx's efficiency in reducing the number of iterations needed for convergence compared to Vanilla, performing comparably to the ideal Target method.", "section": "6.3 Neural Network Training"}, {"figure_path": "MzNjnbgcPN/figures/figures_23_1.jpg", "caption": "Figure 1: An illustration of OptEx at iteration t.", "description": "This figure illustrates the OptEx framework at a given iteration *t*. It compares three approaches: Vanilla, OptEx, and Target.  Vanilla represents the standard sequential first-order optimization. OptEx introduces approximate parallelization by using a kernelized gradient estimation to predict future gradients, allowing parallel computation of multiple steps. The Target approach is an ideal (but impractical) version where perfectly parallel updates are possible using true gradients. The figure shows how OptEx aims to bridge the gap between the efficiency of perfectly parallel updates (Target) and the efficiency of standard, sequential optimization (Vanilla).", "section": "4 The OptEx Framework"}, {"figure_path": "MzNjnbgcPN/figures/figures_24_1.jpg", "caption": "Figure 6: Ablation studies on the Rosenbrock synthetic function.", "description": "This figure presents four ablation studies on the Rosenbrock synthetic function.  (a) compares the parallel and sequential versions of OptEx, demonstrating the effect of parallel processing. (b) illustrates how different methods of selecting the next input (\u03b8t) impact performance.  (c) shows the sensitivity of OptEx to the length of the local gradient history (To), and (d) explores the effect of varying the level of parallelism (N). Each subplot shows the optimality gap (y-axis) plotted against the number of sequential iterations (T) required to achieve that gap.", "section": "B.3 Ablation Studies on Synthetic Function"}, {"figure_path": "MzNjnbgcPN/figures/figures_25_1.jpg", "caption": "Figure 2: Comparison of the number of sequential iterations T (x-axis) required by different methods to achieve the same optimality gap F(\u03b8) \u2013 info F(\u03b8) (y-axis) for various synthetic functions. The parallelism N is set to 5 and each curve denotes the mean from 5 independent runs.", "description": "This figure compares the number of sequential iterations needed by Vanilla, Target, and OptEx methods to reach the same level of optimality for various synthetic functions.  The x-axis represents the number of sequential iterations (T), and the y-axis represents the optimality gap (F(\u03b8) - info F(\u03b8)). OptEx consistently requires fewer iterations than Vanilla, demonstrating its improved efficiency, although it does not quite reach the ideal performance of the Target method which leverages perfect gradient information.", "section": "6.1 Synthetic Function Minimization"}, {"figure_path": "MzNjnbgcPN/figures/figures_25_2.jpg", "caption": "Figure 2: Comparison of the number of sequential iterations T (x-axis) required by different methods to achieve the same optimality gap F(\u03b8) \u2013 inf\u03b8 F(\u03b8) (y-axis) for various synthetic functions. The parallelism N is set to 5 and each curve denotes the mean from 5 independent runs.", "description": "This figure compares the number of sequential iterations needed by Vanilla, Target, and OptEx methods to reach the same optimality gap for four different synthetic functions (Ackley, Ackley (higher dimension), Sphere, and Rosenbrock).  The x-axis represents the number of sequential iterations (T), and the y-axis represents the optimality gap. OptEx consistently requires fewer iterations than Vanilla, demonstrating its improved efficiency. The parallelism (N) is fixed at 5 for all experiments. Each data point is an average across 5 independent runs.", "section": "6.1 Synthetic Function Minimization"}, {"figure_path": "MzNjnbgcPN/figures/figures_25_3.jpg", "caption": "Figure 2: Comparison of the number of sequential iterations T (x-axis) required by different methods to achieve the same optimality gap F(\u03b8) \u2013 info F(\u03b8) (y-axis) for various synthetic functions. The parallelism N is set to 5 and each curve denotes the mean from 5 independent runs.", "description": "This figure compares the number of sequential iterations needed by different optimization methods (Vanilla, Target, and OptEx) to achieve a similar optimality gap on various synthetic functions.  OptEx significantly reduces the number of iterations compared to Vanilla, showcasing its efficiency. The Target line represents an ideal scenario with perfect parallelization, providing an upper bound for OptEx's performance.", "section": "6.1 Synthetic Function Minimization"}, {"figure_path": "MzNjnbgcPN/figures/figures_25_4.jpg", "caption": "Figure 2: Comparison of the number of sequential iterations T (x-axis) required by different methods to achieve the same optimality gap F(\u03b8) \u2013 info F(\u03b8) (y-axis) for various synthetic functions. The parallelism N is set to 5 and each curve denotes the mean from 5 independent runs.", "description": "This figure compares the number of sequential iterations needed by Vanilla, Target, and OptEx to reach a specific optimality gap for four different synthetic functions (Ackley, Ackley with higher dimensions, Sphere, and Rosenbrock).  OptEx consistently requires fewer iterations than Vanilla, demonstrating its efficiency improvement. Although OptEx doesn't quite match the ideal Target (which uses perfect gradient information), it achieves a significant reduction in iterations, supporting the claims of approximate parallelization.", "section": "6.1 Synthetic Function Minimization"}, {"figure_path": "MzNjnbgcPN/figures/figures_26_1.jpg", "caption": "Figure 6: Ablation studies on the Rosenbrock synthetic function.", "description": "This figure presents ablation studies conducted on the Rosenbrock synthetic function to analyze different aspects of the OptEx algorithm.  Panel (a) compares the performance of OptEx using parallel vs. sequential iterations.  Panel (b) investigates the impact of different selection methods for choosing proxy updates. Panel (c) explores the effect of varying the size of the local gradient history (To). Panel (d) examines the influence of varying the level of parallelism (N).  The results help demonstrate the effectiveness of using intermediate gradient calculations and illustrate the optimal settings of certain parameters in the algorithm.", "section": "B.3 Ablation Studies on Synthetic Function"}, {"figure_path": "MzNjnbgcPN/figures/figures_26_2.jpg", "caption": "Figure 2: Comparison of the number of sequential iterations T (x-axis) required by different methods to achieve the same optimality gap F(\u03b8) \u2013 info F(\u03b8) (y-axis) for various synthetic functions. The parallelism N is set to 5 and each curve denotes the mean from 5 independent runs.", "description": "This figure compares the number of sequential iterations needed by Vanilla, Target, and OptEx to achieve a similar optimality gap on four different synthetic functions.  OptEx consistently requires fewer iterations, demonstrating its efficiency improvement. The parallelism (N) was set to 5.", "section": "6.1 Synthetic Function Minimization"}]