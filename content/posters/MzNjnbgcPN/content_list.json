[{"type": "text", "text": "OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yao $\\mathbf{Shu^{\\#\\dagger}}$ , Jiongfeng Fang\u2021, Ying Tiffany $\\mathbf{H}\\mathbf{e}^{\\ddag}$ , Fei Richard $\\mathbf{Y}\\mathbf{u}^{\\ddagger\\S}$ ", "page_idx": 0}, {"type": "text", "text": "\u2020Guangdong Lab of AI and Digital Economy (SZ), China \u2021College of Computer Science and Software Engineering, Shenzhen University, China \u00a7School of Information Technology, Carleton University, Canada ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "First-order optimization (FOO) algorithms are pivotal in numerous computational domains, such as reinforcement learning and deep learning. However, their application to complex tasks often entails significant optimization inefficiency due to their need of many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first general framework that enhances the optimization efficiency of FOO by leveraging parallel computing to directly mitigate its requirement of many sequential iterations for convergence. To achieve this, OptEx utilizes a kernelized gradient estimation that is based on the history of evaluated gradients to predict the gradients required by the next few sequential iterations in FOO, which helps to break the inherent iterative dependency and hence enables the approximate parallelization of iterations in FOO. We further establish theoretical guarantees for the estimation error of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming that the estimation error diminishes to zero as the history of gradients accumulates and that our SGD-based OptEx enjoys an effective acceleration rate of $\\Theta({\\sqrt{N}})$ over standard SGD given parallelism of $N$ , in terms of the sequential iterations required for convergence. Finally, we provide extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training on various datasets, to underscore the substantial efficiency improvements achieved by OptEx in practice. Our implementation is available at https://github.com/youyve/OptEx. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "First-order optimization (FOO) algorithms, such as stochastic gradient descent (SGD) [1], Nesterov Accelerated Gradient (NGA) [2], AdaGrad [3], Adam [4] etc., have already been the cornerstone of many computational disciplines, driving advancements in areas ranging from reinforcement learning [5] to machine learning [6]. These algorithms, which are widely known for their straightforward form of iterative gradient-based updates, are fundamental in solving both simple and intricate optimization problems. However, their applications usually encounter substantial optimization inefficiency, especially when addressing complex functions that not only are expensive in evaluating their function values and gradients but also necessitate a large number of sequential iterations to converge in practice, e.g., deep reinforcement learning [7] and neural network training [8]. ", "page_idx": 0}, {"type": "text", "text": "To this end, parallel computing has been widely used in the literature to considerably enhance the optimization (e.g., time) efficiency of FOO by reducing the evaluation cost of function and gradient per iteration in FOO [9]. For instance, in the field of neural network training, techniques that are based on parallel computing, e.g., data parallelism [8, 10, 11, 12], model parallelism [13], and pipeline parallelism [14, 15], have been employed to reduce the evaluation time of loss function and parameter gradient by processing multiple input samples and network components concurrently. However, to the best of our knowledge, few efforts have been devoted to leveraging parallel computing to reduce the number of sequential iterations required for convergence to mitigate the optimization inefficiency in FOO. Different from the methods of reducing the evaluation time per iteration during optimization, which requires specialized human efforts in a specific domain (e.g., neural network training), the reduction of sequential iterations is likely to be more general since no such specialized domain efforts are required and thus shall enjoy a wider application in practice. This underscores the need to explore the potential of parallelizing sequential iterations in standard FOO. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, the inherent iterative dependency in FOO where the output of each iteration servers as the input of the next iteration, poses a significant barrier to independent and concurrent iteration execution, thereby making it nearly impossible to realize iteration parallelism within standard FOO. To this end, we develop a novel framework called first-order optimization expedited with approximately parallelized iterations (OptEx) that is capable of bypassing the challenge of inherent iterative dependency in standard FOO and therefore make parallelized iterations in FOO possible. Specifically, our framework begins with a novel kernelized gradient estimation strategy, which uses the history of gradients during optimization to predict the gradients for any input within the domain such that these estimated gradients can be used in standard FOO algorithms to determine the inputs for the next few iterations. We further introduce the techniques of separable kernel function and local history of gradients to enhance the computational efficiency of this gradient estimation (Sec. 4.1). We then apply standard FOO algorithms with this kernelized gradient estimation to determine the inputs for the next $N$ sequential iterations efficiently (namely proxy updates), aiming to approximate the ground-truth sequential updates and bypass the iteration dependency in standard FOO (Sec. 4.2). Lastly, we complete our approximately parallelized iterations for standard FOO by leveraging parallel computing with parallelism of $N$ to concurrently execute standard FOO algorithms over these $N$ inputs obtained from our proxy updates using the ground-truth gradients (Sec. 4.3). ", "page_idx": 1}, {"type": "text", "text": "Apart from proposing our innovative OptEx framework, we further establish rigorous theoretical guarantees and extensive empirical studies underpinning its efficacy. Specifically, we give a theoretical bound for the estimation error of our kernelized gradient estimation. Remarkably, this error approaches zero asymptotically as the number of historical gradients increases, ranging across a broad spectrum of kernel functions. This suggests that our kernelized gradient estimation can facilitate effective proxy updates to help parallelize sequential iterations in FOO (Sec.5.1). Building on this, we delineate both upper and lower bounds for the sequential iteration complexity of our SGD-based OptEx, showing that our SGD-based $\\mathrm{{OptEx}}$ is able to reduce the sequential iteration complexity of standard FOO algorithms at a rate of $\\Theta({\\sqrt{N}})$ with parallelism of $N$ (Sec.5.2). Finally, through extensive empirical studies, including the optimization of synthetic functions, reinforcement learning tasks, and neural network training on both image and text datasets, we demonstrate the consistent advantages of our OptEx in expediting existing FOO algorithms (Sec. 6). ", "page_idx": 1}, {"type": "text", "text": "To summarize, our contribution to this work includes: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to develop a general framework (i.e., OptEx) that can leverage parallel computing to approximately parallelize the sequential iterations in FOO, thereby considerably reducing the sequential iteration complexity of FOO algorithms.   \n\u2022 We provide the first upper and lower iteration complexity bound for SGD-based OptEx, which gives an effective acceleration rate of $\\Theta({\\sqrt{N}})$ with parallelism of $N$ .   \n\u2022 We conduct extensive empirical studies, including the optimization of synthetic function, reinforcement learning tasks, and neural network training on both image and text datasets, to support the efficacy of our OptEx framework. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reduction of Iteration Complexity. In the literature, various techniques have been developed to enhance the optimization efficiency of FOO by improving their sequential iteration complexity. For example, variance reduction strategies [16, 17, 18] have been proposed to accelerate stochastic optimization by effectively reducing the gradient variance and therefore aligning the iteration complexity of SGD with that of gradient descent (GD) in expectation. These strategies usually yield significant improvements in high-variance problems whereas their compelling performance is hard to extend to low-variance scenarios and deterministic contexts. Meanwhile, adaptive gradient methods, e.g., AdaGrad [3], Adam [4], and AdaBelief [19], have been introduced to employ an adaptive learning rate for a better-performing optimization where fewer iterations are required for convergence. Furthermore, acceleration techniques like the Nesterov method [2] and momentum-based updates [20] have also been proven to be capable of reducing the sequential iteration complexity for GD and SGD efficiently. Orthogonal to these established methodologies, our paper introduces parallel computing as a distinct and innovative strategy to further decrease the sequential iteration complexity of FOO. Of note, such an approach not only stands independently but also offers potential for synergistic integration with existing methods, promising enhanced optimization outcomes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Reduction of Time Complexity Per Iteration using Parallel Computing. In the realm of enhancing the computational efficiency of FOO, parallel computing has emerged as a rescue by reducing the time complexity per iteration in FOO. Particularly in the field of neural network training, data parallelism [8, 10, 11, 12] has been introduced to evaluate the gradients of model parameters w.r.t mini-batch input samples simultaneously. In addition to data parallelism, model parallelism [13] has been developed to process various neural network components concurrently. Furthermore, pipeline parallelism [14, 15] divides the neural network into stages and assigns each stage to a different device, allowing different stages of the computation to be executed in parallel across the pipeline. However, the tailored nature of these methods constrains their application to wider contexts. Contradictory to these case-specified solutions, this paper proposes a general framework that can leverage parallel computing to enhance the optimization efficiency of FOO in wide practical applications. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we aim to enhance the optimization efficiency of the following stochastic minimization problem by leveraging parallel computing with parallelism of $N$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\pmb{\\theta}}\\in\\mathbb{R}^{d}}F({\\pmb{\\theta}})\\triangleq\\mathbb{E}\\left[f({\\pmb{\\theta}})\\right]~.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\nabla f(\\pmb\\theta)$ is assumed to follow a specific Gaussian distribution, i.e., $\\nabla f(\\pmb\\theta)\\sim\\mathcal{N}(\\nabla F(\\pmb\\theta),\\sigma^{2}\\mathbf I)$ for any $\\pmb\\theta\\in\\mathbb{R}$ , which has already been widely used in the literature [21, 22, 23]. Besides, we adopt a common assumption that $\\nabla F$ is sampled from a Gaussian process, i.e., $\\nabla F\\,\\sim\\,\\mathcal{G P}(\\mathbf{0},\\mathbf{K}(\\cdot,\\cdot))$ [24, 25, 26]. Of note, (1) has found extensive applications in practice, e.g., neural network training [27] and reinforcement learning [28]. Importantly, although our primary focus is on this stochastic optimization, our method can also be applied to deterministic optimization (evidenced in Sec. 6.1). ", "page_idx": 2}, {"type": "text", "text": "Standard FOO algorithms commonly optimize (1) in an iterative and sequential manner: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\operatorname{FO-OPT}(\\pmb{\\theta}_{t},\\nabla f(\\pmb{\\theta}_{t}))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t$ is the iteration number. Ideally, if parallel computing can be used to parallelize the sequential iterations in FOO (i.e., to execute several sequential iterations simultaneously), it will be able to lead to a noticeable improvement in its optimization efficiency since fewer sequential iterations will be required for convergence. Unfortunately, there is an inherent iterative dependency in standard FOO, that is, the output of each iteration $t$ (e.g., $\\mathbf{\\boldsymbol{\\theta}}_{t}^{\\mathrm{~\\,~}}$ ) is the input of the next iteration $t+1$ . Such an iterative and sequential process makes it nearly impossible to attain $\\pmb\\theta_{t}$ and $\\pmb{\\theta}_{t+1}$ concurrently, and therefore parallelize the iterations for established FOO algorithms. ", "page_idx": 2}, {"type": "text", "text": "4 The OptEx Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To this end, we introduce the first general framework in Algo. 1 with a detailed illustration in Fig. 1, namely first-order optimization expedited with approximately parallelized iterations (OptEx), to overcome the aforementioned inherent iterative dependency in FOO and facilitate the realization of parallelized iterations therein. To achieve this, we first propose a kernelized gradient estimation with the technique of separable kernel function and local history of gradient to efficiently and effectively estimate the gradient at any input in the domain (Sec. 4.1). We then follow standard FOO algorithms with this kernelized gradient estimation to approximate the inputs for the next $N$ sequential iterations to be parallelized (Sec.4.2), aiming to overcome the inherent iterative dependency in FOO. Lastly, we finish our approximately parallelized iterations by leveraging parallel computing to run standard FOO algorithms on these $N$ inputs concurrently using the ground-truth gradients (Sec. 4.3). ", "page_idx": 2}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/676ddb5c43ab0f66d4bab8b236448b29acdb1ac5fed3c72faa8012c0f2bfcc01.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4.1 Kernelized Gradient Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Process 1 Process 2 Process 3 Process N-1 Process N ", "page_idx": 3}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/fcc69d49da3d5bc67a8d95d0d93a3dc4430c94688fe0ac8c427e87886cb676c8.jpg", "img_caption": ["Figure 1: An illustration of $\\mathrm{{OptEx}}$ at iteration $t$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "As mentioned in our Sec. 3, $\\nabla F$ is assumed to be sampled from a Gaussian process, i.e., $\\nabla F\\sim$ $\\mathcal{G P}(\\mathbf{0},\\mathbf{K}(\\cdot,\\cdot))$ with kernel function $\\mathbf{K}$ . Then, for every sequential iteration $t$ of Algo. 1, conditioned on the history of gradients during optimization $\\mathcal{G}\\triangleq\\{(\\pmb{\\theta}_{\\tau},\\nabla f(\\pmb{\\theta}_{\\tau})\\}_{\\tau=1}^{N(t-1)}:$ }\u03c4N=(t1\u22121)1 , \u2207F then follows the posterior Gaussian process: $\\nabla F\\;\\sim\\;\\mathcal{G P}\\left(\\pmb{\\mu}_{t}(\\cdot),\\pmb{\\Sigma}_{t}^{2}(\\cdot,\\cdot)\\right)$ with the mean function $\\pmb{\\mu}_{t}(\\cdot)$ and the covariance function $\\Sigma_{t}^{2}(\\cdot,\\cdot)$ defined as below [24]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\,\\mu_{t}(\\pmb\\theta)\\triangleq\\mathbf{V}_{t}^{\\top}(\\pmb\\theta)\\left(\\mathbf{U}_{t}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\mathrm{vec}(\\mathbf{G}_{t}^{\\top})\\,,}\\\\ &{\\sum_{t}^{2}(\\pmb\\theta,\\pmb\\theta^{\\prime})\\triangleq\\mathbf{K}\\left(\\pmb\\theta,\\pmb\\theta^{\\prime}\\right)-\\mathbf{V}_{t}^{\\top}(\\pmb\\theta)\\left(\\mathbf{U}_{t}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\mathbf{V}_{t}\\left(\\pmb\\theta^{\\prime}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\operatorname{vec}(\\cdot)$ vectorizes a matrix into a column vector, $\\mathbf{G}_{t}\\triangleq[\\nabla f(\\pmb{\\theta}_{\\tau})]_{\\tau=1}^{N(t-1)}$ is a $d\\times N(t-1)$ dimensional matrix, ${\\mathbf{V}}_{t}^{\\top}(\\pmb{\\theta})\\,\\triangleq\\,[{\\mathbf{K}}(\\pmb{\\theta},\\pmb{\\theta}_{\\tau})]_{\\tau=1}^{N(t-1)}$ is a $d\\times N(t-1)d$ -dimensional matrices, and $\\mathbf{U}_{t}\\triangleq\\left[\\mathbf{K}(\\pmb{\\theta}_{\\tau},\\pmb{\\theta}_{\\tau^{\\prime}})\\right]_{\\tau,\\tau^{\\prime}=1}^{N(t-1)}$ is a $N(t-1)d\\times N(t-1)d$ -dimensional matrices. We therefore propose to use $\\pmb{\\mu}_{t}(\\cdot)$ to estimate the gradient at any input $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ , that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla F(\\pmb\\theta)\\approx\\mu_{t}(\\pmb\\theta)\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and covariance $\\Sigma^{2}(\\pmb{\\theta})\\triangleq\\Sigma^{2}(\\pmb{\\theta},\\pmb{\\theta})$ to measure the quality of this gradient estimation in a principled way, which will be further theoretically supported in our Sec. 5.1. ", "page_idx": 3}, {"type": "text", "text": "However, for every sequential iteration $t$ of Algo. 1 with (3), it will incur a computational complexity of $\\mathcal{O}(N^{3}(t-1)^{3}d^{3})$ , along with a space complexity of $\\mathcal{O}(N(t-1)d)$ . Practically, this presents a significant challenge in scenarios with a large input dimension $d$ or requiring a substantial number $T$ of sequential iterations for convergence, such as in neural network training [8]. To mitigate these complexity issues, we introduce two techniques: the separable kernel function and the local history of gradients, to reduce both the computational and space complexities associated with our kernelized gradient estimation, thereby enhancing its efficiency and practical applicability. ", "page_idx": 3}, {"type": "text", "text": "Separable Kernel Function. Let $\\mathbf{K}(\\cdot,\\cdot)=k(\\cdot,\\cdot)\\,\\mathbf{I}$ where $k(\\cdot,\\cdot)$ produces a scalar value and I is a $d\\times d$ identity matrix, and define the $N(t-1)$ -dimensional vector $\\pmb{k}_{t}^{\\top}(\\pmb{\\theta})\\triangleq[k(\\pmb{\\theta},\\pmb{\\theta}_{\\tau})]_{\\tau=1}^{N(t-1)}$ , and $N(t-1)\\times N(t-1)$ -dimensional matrix $\\mathbf{K}_{t}\\triangleq[k(\\pmb{\\theta}_{\\tau},\\pmb{\\theta}_{\\tau^{\\prime}})]_{\\tau=\\tau^{\\prime}=1}^{N(t-1)}$ , we can prove that the Gaussian process in (3) can be simplified as the Gaussian process in Prop. 4.1 (line 3 of Algo. 1). ", "page_idx": 3}, {"type": "text", "text": "Proposition 4.1. Let $\\mathbf{K}(\\cdot,\\cdot)=k(\\cdot,\\cdot)\\,\\mathbf{I},$ , the posterior mean and covariance in (3) become ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mu_{t}(\\pmb\\theta)=\\left[\\left(k_{t}^{\\top}(\\pmb\\theta)\\left(\\mathbf K_{t}+\\sigma^{2}\\mathbf I\\right)^{-1}\\right)\\mathbf G_{t}\\right]^{\\top}~,}&&{}\\\\ {\\displaystyle\\Sigma_{t}^{2}(\\pmb\\theta,\\pmb\\theta^{\\prime})=\\left(k(\\pmb\\theta,\\pmb\\theta^{\\prime})-k_{t}^{\\top}(\\pmb\\theta)\\left(\\mathbf K_{t}+\\sigma^{2}\\mathbf I\\right)^{-1}k_{t}(\\pmb\\theta^{\\prime})\\right)\\mathbf I\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "1We slightly abuse the notation $f$ to denote the different functions that are randomly sampled per iteration and $(\\pmb{\\theta}_{\\tau},\\nabla f(\\pmb{\\theta}_{\\tau})$ to denote a historical evaluation till sequential iteration $t-1$ with parallelism of $N$ . ", "page_idx": 3}, {"type": "text", "text": "Its proof is in Appx. A.1. Prop. 4.1 shows that with a separable kernel function $\\mathbf{K}(\\cdot,\\cdot)=k(\\cdot,\\cdot)\\,\\mathbf{I}$ , the multi-output Gaussian process in a $d$ -dimensional space can be effectively decoupled into $d$ independent single-output Gaussian processes. Each of these processes results from the same scalar kernel function $k$ , leading to a uniform posterior form shared by all these processes, i.e., the expression ${\\boldsymbol{k}}_{t}^{\\top}(\\theta)\\left({\\bf K}_{t}+\\sigma^{2}{\\bf I}\\right)^{-1}$ in ${\\pmb{\\mu}}_{t}({\\pmb{\\theta}})$ and $\\overline{{k(\\pmb{\\theta},\\pmb{\\theta}^{\\prime})\\!-\\!k_{t}^{\\top}(\\pmb{\\theta})\\left(\\mathbf{K}_{t}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\overline{{k}}_{t}(\\pmb{\\theta}^{\\prime})}}$ in $\\Sigma_{t}^{2}(\\theta,\\theta^{\\prime})$ . This thus considerably diminishes the computational complexity, now quantified as $\\mathcal{O}(N^{3}(t{-}1)^{3}{+}N(t{-}1)d)$ , resulting in a more computationally efficient gradient estimation in practice. ", "page_idx": 4}, {"type": "text", "text": "Local History of Gradients. Conventional FOO algorithms predominantly operate by optimizing within a localized region neighboring the initial input $\\theta_{0}$ [29]. This therefore indicates that our Algo. 1 only requires precise gradient estimation within a local region. In this context, the use of a local gradient history is posited as sufficiently informative for effective kernelized gradient estimation, which can be supported by the theoretical results in [30] and the empirical evidence in our Sec. 6. As a result, rather than relying on a complete gradient history, we propose to use a localized gradient history of size $T_{0}$ that neighbors $\\pmb{\\theta}$ to estimate the gradient at $\\pmb{\\theta}$ . This strategic modification results in a substantial reduction of computational complexity to $\\mathcal{O}(T_{0}^{3}+T_{0}d)$ as well as a corresponding decrease in space complexity to ${\\mathcal{O}}(T_{0}d)$ , which is especially beneficial in the situations where $T_{0}$ is considerably smaller than $N(t-1)$ for $t\\in[T]$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Multi-Step Proxy Updates ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The ability of our kernelized gradient estimation to provide gradient estimation at any input $\\pmb{\\theta}$ then enables the application of a multi-step gradient estimation. This helps to approximate the inputs for the next $N$ sequential iterations $\\{\\bar{\\theta}_{\\tau+i}\\}_{i=0}^{N-1}$ to be parallelized in standard FOO, given $\\theta\\tau$ . Specifically, in the context of our Algo. 1, for every sequential iteration $t\\in[T]$ , by employing a first-order optimizer (FO-OPT), we can approximate the inputs required by our parallelized iteration in Sec. 4.3 sequentially as below through our multi-step proxy updates (line 4-5 of Algo. 1). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\theta}_{t,s}=\\mathrm{FO-OPT}(\\pmb{\\theta}_{t,s-1},\\mu_{t}(\\theta_{t,s-1})),\\forall s\\in[N-1]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, these proxy updates imitate the sequential iterations in standard FOO by using only the estimated gradients in our Sec. 4.1. We will show that these proxy updates can provide a reasonably good approximation of the ground-truth updates in Sec. 5.1. Meanwhile, despite the iterative and sequential nature of (5), our proxy updates based on operations on relatively small-sized matrices (refer to the Prop. 4.1) will still be able to provide significantly enhanced computational efficiency compared to the ground-truth updates based on expensive evaluation of function values and gradients in complex tasks, like neural network training. This effectiveness and efficiency of (5) thus render it an essential foundation for achieving parallelized iterations and improved the optimization efficiency in FOO. ", "page_idx": 4}, {"type": "text", "text": "4.3 Approximately Parallelized Iterations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Upon obtaining the inputs $\\{\\pmb{\\theta}_{t,s-1}\\}_{s=1}^{N}$ for the next $N$ sequential iterations to be parallelized, we then finish our approximately parallelized iteration by executing standard FOO algorithms over each of $\\{\\pmb{\\theta}_{t,s-1}\\}_{s=1}^{N}$ based on the ground-truth gradients $\\bar{\\{\\nabla\\,f(\\pmb{\\theta}_{t,s-1})\\}_{s=1}^{N}}$ in parallel (line 6-9 of Algo. 1, see also the processes in Fig. 1). That is, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\theta}_{t}^{(i)}=\\mathrm{FO-OPT}(\\pmb{\\theta}_{t,i-1},\\nabla f(\\pmb{\\theta}_{t,i-1})),\\,\\forall i\\in[N]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After that, the final input $\\pmb{\\theta}_{t}=\\pmb{\\theta}_{t}^{(N)}$ will be used in the next sequential iteration (line 10 of Algo. 1). Of note, central to the approximately parallelized iterations in our OptEx framework is the necessity of evaluating the gradients \u2207f(\u03b8t,s\u22121) sN=1 in our Algo. 1. These evaluations in fact play pivotal roles in reducing the estimation error of our kernelized gradient estimation and consequently improving the performance of our $\\mathrm{{OptEx}}$ by augmenting the gradient history near the input $\\pmb{\\theta}_{t}$ with $N$ more evaluations, which will be supported by the theoretical results in our Sec. 5 and the empirical evidence in our Appx. B.3. ", "page_idx": 4}, {"type": "text", "text": "5 Theoretical Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To begin with, we formally present the assumptions mentioned in our Sec. 3 as below. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. $\\nabla f(\\pmb\\theta)-\\nabla F(\\pmb\\theta)$ follows ${\\mathcal{N}}\\left(\\mathbf{0},\\sigma^{2}\\mathbf{I}\\right)$ for any $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. $\\nabla F$ is sampled from a Gaussian process $\\mathcal{G P}\\left(\\mathbf{0},\\mathbf{K}(\\cdot,\\cdot)\\right)$ where $\\mathbf{K}(\\cdot,\\cdot)=k(\\cdot,\\cdot)\\,\\mathbf{I}$ and $|k(\\theta,\\theta)|\\leq\\kappa$ for any $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ . ", "page_idx": 5}, {"type": "text", "text": "Note that the Assump. 1 has already been widely employed in the literature [21, 22, 23]. Meanwhile, it is also common to assume that $F$ is sampled from a Gaussian process [24, 31], implying that $\\nabla F$ follows a Gaussian process as well [24, 25, 26] (Assump. 2), i.e., $\\nabla F$ can be any function in this prior. The inclusion of a separable kernel function in Assump. 2 aims to enhance the efficiency of our kernelized gradient estimation in Sec. 4.1 and simplify our theoretical analyses below, whereas our conclusions apply to non-separable kernel functions as well by following our proof techniques. ", "page_idx": 5}, {"type": "text", "text": "5.1 Gradient Estimation Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following the principled idea in kernelized bandit [32, 33] and Bayesian Optimization [34, 31], we define the maximal information gain as below ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma_{n}\\triangleq\\operatorname*{max}_{\\{\\pmb{\\theta}_{j}\\}_{j=1}^{n}\\subset\\mathbb{R}^{d}}I\\left(\\mathrm{vec}(\\mathbf{G}_{n});\\mathrm{vec}(\\pmb{\\nabla}_{n})\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $I(\\mathrm{vec}(\\mathbf G_{n});\\mathrm{vec}(\\mathbf{\\nabla}\\nabla_{n}))$ is the mutual information between $\\mathbf{G}_{n}\\,\\triangleq\\,\\left[\\nabla f(\\pmb{\\theta}_{i})\\right]_{i=1}^{n}$ and $\\nabla_{n}\\ \\triangleq$ $[\\nabla F(\\pmb\\theta_{i})]_{i=1}^{n}$ . In essence, $\\gamma_{n}$ encapsulates the maximum amount of information about $\\nabla F$ that can be gleaned from observing any set of $n$ evaluated gradients, represented as ${\\bf G}_{n}$ , which is known to be problem dependent measure that is highly related to the kernel function $k(\\cdot,\\cdot)$ [32]. Built on this notation, we then provide the following theoretical result for our gradient estimation. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Gradient Estimation Error). Let $\\delta\\,\\in\\,(0,1)$ and $\\alpha\\triangleq\\,d+(\\sqrt{d}+1)\\ln(1/\\delta)$ . Given Assump. 1 and 2, let $|\\mathcal{G}|=T_{0}-1$ for any sequential iteration t in Algo. 1, then for any $\\pmb{\\theta}\\in\\mathbb{R}^{d},t>0,$ , with a probability of at least $1-\\delta$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\nabla F(\\theta)-\\mu_{t}(\\theta)\\|\\le\\sqrt{\\alpha\\left\\|\\Sigma^{2}(\\theta)\\right\\|}\\;\\;w h e r e\\;\\frac{\\kappa}{(\\kappa+1/\\sigma^{2})^{T_{0}-1}}\\le\\left\\|\\Sigma^{2}(\\theta)\\right\\|\\le\\frac{4\\operatorname*{max}\\{\\kappa,\\sigma^{2}\\}\\gamma_{T_{0}}}{T_{0}d}\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is in Appx. A.2. It is important to note that since FOO pertains to local optimization, the global fulfillment of Assump. 2 is not a prerequisite. That is, the assumption that $\\nabla F$ is sampled from $\\mathcal{G P}(\\mathbf{0},\\mathbf{K})$ within a local region will already be sufficient for our kernelized gradient estimation in Sec. 4.1 to achieve accurate gradient estimation in practice. Our Sec. 6 will later evidence this empirically. Thm. 1 with upper bound on $\\left\\|\\Sigma^{2}(\\pmb{\\theta})\\right\\|$ illustrates that the efficacy of our kernelized gradient estimation in the worst case will enjo y a poly nomial error rate of $\\mathcal{O}\\left(\\sqrt{\\gamma_{T_{0}}/T_{0}}\\right)$ . This means that if $\\gamma_{T_{0}}/T_{0}$ will asymptotically approach zero w.r.t. $T_{0}$ , the error of our kernelized gradient estimation method will become significantly small given a large number of evaluated gradients $T_{0}$ . This consequently facilitates the effectiveness of our proxy updates in (5) built on our kernelized gradient estimation to approximate the ground-truth updates when $|\\mathcal G|$ is sufficiently large. Meanwhile, Thm. 1 with lower bound on $\\left\\|\\Sigma^{2}(\\pmb{\\theta})\\right\\|$ illustrates that our kernelized gradient estimation in the best case may achieve an exponen tial erro r rate of $\\mathcal{O}\\left(\\kappa/(\\kappa+1/\\sigma^{2})^{T_{0}-1}\\right)$ , which thus further elaborates the efficacy of kernelized gradient estimation in Sec. 4.1 and proxy updates in Sec. 4.2. ", "page_idx": 5}, {"type": "text", "text": "It is important to note that the ratio $\\gamma_{T_{0}}/T_{0}$ has been demonstrated to asymptotically approach zero for a range of kernel functions, as evidenced in existing literature [35]. This therefore underpins the establishment of concrete error bounds for our kernelized gradient estimation where notationO is applied to hide the logarithmic factors, delineated as follows: ", "page_idx": 5}, {"type": "text", "text": "Corollary 1 (Concrete Error Bounds). Let $k(\\cdot,\\cdot)$ be the radial basis function (RBF) kernel, then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla F(\\pmb{\\theta})-\\pmb{\\mu}_{t}(\\pmb{\\theta})\\right\\|=\\widetilde{\\mathcal{O}}\\left(T_{0}^{-1/2}\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $k(\\cdot,\\cdot)$ be the Mat\u00e9rn kernel where $\\nu$ is the smoothness parameter, then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F(\\pmb{\\theta})-\\pmb{\\mu}_{t}(\\pmb{\\theta})\\|=\\widetilde{\\mathcal{O}}\\left(T_{0}^{-\\nu/(2\\nu+d(d+1))}\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Cor. 1 elucidates that with kernel functions such as RBF and Mat\u00e9rn kernel, the error in our kernelized gradient estimation indeed will diminish asymptotically w.r.t. $T_{0}$ . That is, as $T_{0}$ increases, the estimation error $\\|\\nabla F(\\pmb\\theta)-\\pmb\\mu_{t}(\\pmb\\theta)\\|$ decreases and consequently the proxy updates in (5) become closer to the ground-truth updates. It is important to note that this reduction typically follows a nonlinear trajectory, suggesting that the effect of an increasing $T_{0}$ on our kernelized gradient estimation diminishes when $T_{0}$ is reasonably large. This consequently affirms the reasonability of our utility of local history for gradient estimation in Sec. 4.1, which leads to not only accurate but also efficient gradient estimations. ", "page_idx": 6}, {"type": "text", "text": "5.2 Iteration Complexity Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first introduce Assump. 3, which has been widely applied in stochastic optimization [16, 36], to underpin the analysis of sequential iteration complexity of our OptEx framework. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. $F$ is $L$ -Lipschitz smooth: $\\left\\|\\nabla F(\\pmb\\theta)-\\nabla F(\\pmb\\theta^{\\prime})\\right\\|\\leq L\\left\\|\\pmb\\theta-\\pmb\\theta^{\\prime}\\right\\|$ for any $\\pmb{\\theta},\\pmb{\\theta}^{\\prime}\\in\\mathbb{R}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "To simplify the analysis, we primarily prove the sequential iteration complexity of our SGD-based OptEx where we use $\\mathrm{min}_{\\tau\\in[N T)}\\left\\lVert\\nabla F(\\pmb{\\theta}_{\\tau})\\right\\rVert^{2}$ to denote the minimal gradient norm we can achieve within the whole optimization process when applying our $\\mathrm{{OptEx}}$ with $T$ sequential iterations and parallelism of $N$ for a clear and fair comparison with standard SGD. Notably, our analysis can also be extended to other FOO-based OptEx by following similar proof idea. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Upper Bound). Let $\\delta\\,\\in\\,(0,1)$ , $\\Delta\\triangleq F(\\pmb\\theta)-\\operatorname*{inf}_{\\pmb\\theta}F(\\pmb\\theta),\\,\\triangleq\\operatorname*{max}\\{\\kappa,\\sigma^{2}\\}$ and $\\rho\\triangleq$ (1 \u2212 N1 )4\u03c3\u03b22\u03b3TT00 + N1 . Under Assump. 1\u20133, by choosing T \u2265N $\\begin{array}{r}{T\\ge\\frac{2\\Delta L}{N\\sigma^{2}\\rho}}\\end{array}$ , $\\begin{array}{r}{\\eta=\\sqrt{\\frac{2\\Delta}{N L T\\sigma^{2}\\rho}}}\\end{array}$ and $\\vert\\mathcal{G}\\vert=T_{0}-1$ for our SGD-based Algo. $^{\\,I}$ , with a probability of at least $1-\\delta$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\leq T,s\\leq N}\\left\\|\\nabla F(\\pmb{\\theta}_{t,s})\\right\\|^{2}\\leq2\\sigma\\sqrt{\\frac{2\\Delta L\\rho}{N T}}+\\frac{4\\beta\\ln(1/2\\delta)}{N T}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Thm. 2 is in Appx. A.3. Of note, our Thm. 2 with $N=1$ aligns with the established upper bound for standard SGD, as discussed in [36]. Importantly, our Thm. 2 elucidates that with parallelism $N>1$ , our SGD-based OptEx algorithm can expedite the standard SGD by a factor of at least $\\sqrt{N/\\rho}$ , where $1/\\rho$ quantifies the impact of the error introduced by our kernelized gradient estimation. This efficiency gain can be further amplified as the accuracy of our kernelized gradient estimation increases (i.e., a decrease in $\\dot{\\rho_{,}}$ ), which can be achieved by augmenting the number $T_{0}$ as discussed in our Sec. 5.1. In addition, Thm. 2 also demonstrates that for a fixed learning rate $\\eta$ , there exists a constant $N_{\\mathrm{max}}$ , e.g., $N_{\\operatorname*{max}}=2\\Delta/(\\eta^{2}L T\\sigma^{2}\\rho)$ in Thm. 2, the parallelism $N$ should roughly remain below to ensure the fastest convergence of function $F$ to a stationary point. In contrast, if $N$ exceeds $N_{\\mathrm{max}}$ , our SGD-based OptEx will underperform due to the increased gradient estimation error. This observation is further supported by the results presented in Appx. B.3. However, when the learning rate $\\eta$ is relatively small (e.g., during fine-tuning in practice), the parallelism $N$ can be significantly larger to achieve a further improved speedup. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Lower Bound). Let $\\delta~\\in~(0,1)$ , $\\Delta\\ \\triangleq\\ F(\\pmb\\theta)\\,-\\,\\operatorname*{inf}_{\\pmb\\theta}F(\\pmb\\theta),\\ \\beta\\ \\triangleq\\ \\operatorname*{max}\\{\\kappa,\\sigma^{2}\\}.$ , and $\\widetilde{\\beta}\\triangleq\\operatorname*{min}\\{\\kappa/(\\kappa+1/\\sigma^{2})^{T_{0}-1},\\sigma^{2}\\}$ . Then, for any $L>0,\\Delta>0,N\\ge1,T\\ge1$ and $\\eta\\in[0,1/L)$ , t here exists a $F$ on $\\mathbb{R}^{d}\\left(\\forall d\\!>\\!d_{0}\\!=\\!\\mathcal{O}\\left(\\beta/(\\Delta L^{2})\\ln N T/\\delta\\right)\\right)$ satisfying Assump. $^{I-3}$ and having the following with a probability of at least $1-\\delta$ when applying SGD-based Algo. 1 with $\\vert\\mathcal{G}\\vert=T_{0}-1$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\leq T,s\\leq N}\\left\\|\\nabla F(\\pmb{\\theta}_{t,s})\\right\\|^{2}\\geq\\frac{d_{0}\\operatorname*{min}\\{\\Delta L,\\widetilde{\\beta},1\\}}{4\\sqrt{N T}}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Thm. 3 is in Appx. A.4. Note that when $N=1$ , Thm. 3 aligns with the recognized lower bound for SGD, as elucidated in [37]. Thm. 3 illustrates that, with parallelism of $N$ , our SGD-based OptEx can potentially accelerate standard SGD by up to $\\sqrt{N}/(\\kappa\\bar{/}(\\sigma^{2}(1\\!+\\!1/\\sigma^{2})^{T_{0}-1}))$ , under the condition that $\\kappa/(1+1/\\sigma^{2})^{T_{0}-1}\\leq\\operatorname*{min}\\{\\Delta L,1,\\sigma^{2}\\}$ . This upper limit in fact corresponds with the lower bound of the variance in our kernelized gradient estimation, as established in Thm. 1. Essentially, the agreement between Thm. 2 and Thm. 3, in the aspect of parallelism $N$ , demonstrates the tightness of our sequential complexity analysis for SGD-based Algo. 1. Finally, the combination of Thm. 2 and Thm. 3 enables us to specify the effective acceleration that can be achieved by our SGD-based OptEx tightly, as shown in our Cor. 2 below. ", "page_idx": 6}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/88be58b686d85b037ceb1b445a4a308b2d5f9612f533df883ba1e3e78d6d3529.jpg", "img_caption": ["Figure 2: Comparison of the number of sequential iterations $T$ ( $x$ -axis) required by different methods to achieve the same optimality gap $F({\\bar{\\pmb\\theta}})-\\operatorname*{inf}_{\\pmb\\theta}F(\\pmb\\theta)$ ( $y$ -axis) for various synthetic functions . The parallelism $N$ is set to 5 and each curve denotes the mean from 5 independent runs. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Corollary 2 (Acceleration Rate). With parallelism of $N$ , the effective acceleration rate achieved by our SGD-based OptEx over standard SGD is $\\Theta({\\sqrt{N}})$ . ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we use extensive experiments to show that our OptEx framework can considerably enhance the efficiency of FOO with parallel computing, including synthetic experiments (Sec. 6.1), reinforcement learning (Sec. 6.2) and neural network training on various datasets (Sec. 6.3). ", "page_idx": 7}, {"type": "text", "text": "6.1 Synthetic Function Minimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we utilize synthetic functions to demonstrate the enhanced performance of our OptEx framework compared to existing baselines, including the standard FOO algorithm, namely Vanilla, and FOO with ideally parallelized iterations, namely Target, which ideally but impractically utilizes the ground-truth gradient to obtain the inputs for the iterations to be parallelized. More specifically, the Vanilla baseline is equivalent to Algo. 1 with parallelism of $N\\!=\\!1$ , and the Target baseline is equivalent to Algo. 1 with $\\mu_{t}(\\pmb{\\theta}_{t,s-1})$ being replaced with $\\nabla f(\\pmb{\\theta}_{t,s-1})$ , indicating the desired parallelized iteration we aim to approximate. We have also provided a comprehensive illustration of these baselines in Appx. B.1 and detailed experimental setup applied here in Appx. B.2.1. ", "page_idx": 7}, {"type": "text", "text": "The results in Fig. 2 with ${\\sigma}^{2}=0$ and $N=5$ have demonstrated the efficacy of our OptEx framework for deterministic optimization (i.e., $\\sigma^{2}\\,=\\,0\\$ ). Specifically, Fig. 2 shows that OptEx consistently achieves a notable speedup in optimization efficiency measured by the number of sequential iterations, which is at least $2\\times$ more efficient than the Vanilla baseline, when optimizing with parallelism of $N=5$ to reach an equivalent level of optimality gap. This is roughly in line with the result of our Cor. 2, implying the validity of our Cor. 2. Meanwhile, although our OptEx framework slightly underperforms the Target baseline, such a phenomenon is in fact quite reasonable since the Target baseline can leverage the ground-truth gradient whereas OptEx relies on the kernelized gradient estimation with estimation error bounded in Thm. 1 to parallelize sequential iterations. This also aligns with the insight from our iteration complexity analysis in Thm. 2. Overall, the results in Fig. 2 have provided strong empirical support for the efficacy of our OptEx in expediting FOO, as theoretically justified in our Sec. 5.2. We also present a number of ablation studies as well as analyses in Appx. B.3 to examine the effects of different components in our proposed OptEx framework on its effectiveness. ", "page_idx": 7}, {"type": "text", "text": "6.2 Reinforcement Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We proceed to compare our OptEx framework with previously established baselines under various reinforcement learning tasks with different parameter dimension $d$ from the OpenAI Gym suite [38], with the deployment of DQN agents [39]. Here, the parallelism parameter is set to be $N=4$ and a detailed experimental setup is provided in Appx. B.2.2. The results are presented in Fig. 3. As illustrated in Fig. 3, the integration of parallel computing techniques, including Target and $\\mathrm{{OptEx}}$ , considerably outperforms the traditional Vanilla baseline in terms of the optimization efficiency quantified by the number of sequential iterations. More importantly, amongst these methodologies, OptEx consistently demonstrates a more stable and superior improvement on the optimization efficiency compared with other baselines, which consequently well corroborates the efficacy of OptEx in improving the efficiency of established FOO algorithms. Interestingly, our OptEx framework can even enjoy an improved efficiency over the Target baseline where the ground-truth gradient $\\nabla f(\\cdot)$ is applied. This is likely because the gradient variance (i.e., $\\|\\pmb{\\Sigma}^{2}(\\pmb{\\theta})\\|)$ ) in our OptEx framework can asymptotically approach zero by using a large number of history of gradient (refer to our Sec. 4.1), whereas the gradient variance in the Target baseline remains the same. ", "page_idx": 7}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/3045296c478021698c39fa57c3a67895cb23e23a74fa7660062d8a42716218c9.jpg", "img_caption": ["Figure 3: Comparison of the cumulative average reward $y$ -axis) achieved by different methods to train DQN on RL tasks under various parameter dimension $d$ and a varying number of sequential episodes $T$ $x$ -axis). The parallelism $N$ is set to 4 and each curve denotes the mean from 3 independent runs. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/71fdd6c7799d6039f457ff750f74cae4183fe23acf863fd063a922dbef635881.jpg", "img_caption": ["Figure 4: Comparison of the test error or training loss ( $y$ -axis) achieved by different optimizers when training deep neural networks on (a) CIFAR-10 and (b) Shakespeare Corpus with a varying number $T$ of sequential iterations or a varying wallclock time $x$ -axis) . The parallelism $N$ is set to 4 and each curve denotes the mean from 5 (for CIFAR-10) or 3 (for Shakespeare corpus) independent runs. The wallclock time is evaluated on a single NVIDIA RTX 4090 GPU. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.3 Neural Network Training ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "At last, we examine the efficacy of our OptEx in expediting the optimization (i.e., training) of deep neural networks, specifically for image classification and text autoregression tasks. Specifically, we apply our OptEx and the aforementioned baselines to (a) train a 10-layer MLP model $\\!\\!\\!\\!d=2412298\\!\\!\\!$ ) with residual connections [40] on CIFAR-10 [41], and (b) train an autoregressive transformer model $'d=1626496)$ ) borrowed from the Haiku library [42] on a curated collection of works from Shakespeare with parallelism of $N=4$ . Comprehensive details for the experimental setup are provided in Appx. B.2.3 and the final results are illustrated in Fig. 4 where both the number of sequential iterations and wallclock time are used to quantify the optimization efficiency of different optimizers. Intriguingly, as evidenced by Fig. 4, OptEx consistently outperforms Vanilla by a large margin in terms of both training and testing errors across the image and text datasets, given an equal number of sequential iterations $T$ or alternatively the same wallclock time budget. Remarkably, the efficiency of $\\mathrm{{OptEx}}$ approaches that of the theoretically ideal algorithm \u2013 the Target baseline, which therefore further verifies the efficacy of our OptEx framework. More results are in Appx. B.3. Overall, these empirical results have well verified the capability of OptEx in significantly expediting FOO algorithms as justified by our theorems in Sec. 5, even in the context of deep neural network training. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In conclusion, our OptEx framework represents a significant advancement in FOO. By leveraging kernelized gradient estimation to enable approximately parallelized iterations, OptEx effectively reduces the number of sequential iterations required for convergence and thus addresses the traditional inefficiencies of FOO. Theoretical analyses and extensive empirical studies validate the reliability and efficacy of OptEx, confirming its potential to expedite optimization processes across various applications. Of note, a limitation of $\\mathrm{{OptEx}}$ is the additional storage and computational cost introduced by the kernelized gradient estimation, which we aim to mitigate further in the future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the Guangdong Lab of AI and Digital Economy (SZ) under the Guangming Laboratory Genius Nova Programme (Award No: 24410002). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.   \n[2] Yurii Evgen\u2019evich Nesterov. A method of solving a convex programming problem with convergence rate o\\bigl(k\u02c62\\bigr). In Doklady Akademii Nauk, volume 269, pages 543\u2013547. Russian Academy of Sciences, 1983.   \n[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12(7), 2011.   \n[4] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. ICML, 2014.   \n[5] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.   \n[6] Guanghui Lan. First-order and stochastic optimization methods for machine learning, volume 1. Springer, 2020.   \n[7] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. arXiv:1312.5602, 2013.   \n[8] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In Proc. NIPS, 2012.   \n[9] Mahmoud Assran, Arda Aytekin, Hamid Reza Feyzmahdavian, Mikael Johansson, and Michael G. Rabbat. Advances in asynchronous parallel and distributed optimization. Proc. IEEE, 2020.   \n[10] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. In Proc. NeurIPS, 2011.   \n[11] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proc. ICML, 2016.   \n[12] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proc. AAAI, 2019.   \n[13] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc\u2019Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In Proc. NIPS, 2012.   \n[14] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377, 2018.   \n[15] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline parallelism. In Proc. NeurIPS, 2019.   \n[16] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Proc. NIPS, 2013.   \n[17] Kaiwen Zhou, Fanhua Shang, and James Cheng. A simple stochastic variance reduced algorithm with fast convergence rates. In Proc. ICML, 2018.   \n[18] Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis R. Bach, and Robert M. Gower. Towards closing the gap between the theory and practice of SVRG. In Proc. NeurIPS, 2019.   \n[19] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha C. Dvornek, Xenophon Papademetris, and James S. Duncan. AdaBelief optimizer: Adapting stepsizes by the belief in observed gradients. In Proc. NeurIPS, 2020.   \n[20] Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with momentum. In Proc. NeurIPS, 2020.   \n[21] Rui Luo, Jianhong Wang, Yaodong Yang, Jun Wang, and Zhanxing Zhu. Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for bayesian learning. In Proc. NeurIPS, 2018.   \n[22] Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize well: Theoretical and empirical evidence. In Proc. NeurIPS, 2019.   \n[23] Yixin Wu, Rui Luo, Chen Zhang, Jun Wang, and Yaodong Yang. Revisiting the characteristics of stochastic gradient noise and dynamics. arXiv:2109.09833, 2021.   \n[24] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. Adaptive computation and machine learning. MIT Press, 2006.   \n[25] Yao Shu, Zhongxiang Dai, Weicong Sng, Arun Verma, Patrick Jaillet, and Bryan Kian Hsiang Low. Zeroth-order optimization with trajectory-informed derivative estimation. In Proc. ICLR, 2023.   \n[26] Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, and Bryan Kian Hsiang Low. Federated zerothorder optimization using trajectory-informed surrogate gradients. arXiv:2308.04077, 2023.   \n[27] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016.   \n[28] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018.   \n[29] L\u00e9on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Rev., 60(2):223\u2013311, 2018.   \n[30] Armin Lederer, Jonas Umlauft, and Sandra Hirche. Posterior variance analysis of Gaussian processes with application to average learning curves. arXiv:1906.01404, 2019.   \n[31] Zhongxiang Dai, Yao Shu, Bryan Kian Hsiang Low, and Patrick Jaillet. Sample-then-optimize batch neural Thompson sampling. In Proc. NeurIPS, 2022.   \n[32] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In Proc. ICML, 2017.   \n[33] Zhongxiang Dai, Yao Shu, Arun Verma, Flint Xiaofeng Fan, Bryan Kian Hsiang Low, and Patrick Jaillet. Federated neural bandit. In Proc. ICLR, 2023.   \n[34] Sayak Ray Chowdhury and Aditya Gopalan. No-regret algorithms for multi-task Bayesian optimization. In Proc. AISTATS, 2021.   \n[35] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proc. ICML, 2010.   \n[36] Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Nguyen. High probability convergence of stochastic gradient methods. In Proc. ICML, 2023.   \n[37] Yoel Drori and Ohad Shamir. The complexity of finding stationary points with stochastic gradient descent. In Proc. ICML, 2020.   \n[38] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016.   \n[39] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[40] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016.   \n[41] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.   \n[42] Tom Hennigan, Trevor Cai, Tamara Norman, Lena Martens, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020.   \n[43] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302\u20131338, 2000.   \n[44] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - From Theory to Algorithms. Cambridge University Press, 2014.   \n[45] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.   \n[46] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-Mnist: A novel image dataset for benchmarking machine learning algorithms. arXiv:1708.07747, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Appendix A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Recall that we have defined $\\pmb{k}_{t}^{\\top}(\\pmb{\\theta})\\triangleq[k(\\pmb{\\theta},\\pmb{\\theta}_{\\tau})]_{\\tau=1}^{N(t-1)}$ , and $\\mathbf{K}_{t}\\triangleq[k(\\pmb{\\theta}_{\\tau},\\pmb{\\theta}_{\\tau^{\\prime}})]_{\\tau=\\tau^{\\prime}=1}^{N(t-1)}$ . Let $\\otimes$ denote the Kronecker product, by introducing the fact that $\\mathbf{K}(\\cdot,\\cdot)=k(\\cdot,\\cdot)\\,\\mathbf{I}$ into $\\mathbf{V}_{t}^{\\top}(\\pmb\\theta)$ and $\\mathbf{U}_{t}$ from the Gaussian process posterior (3), we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\bf V}_{t}^{\\top}(\\pmb\\theta)=[k(\\pmb\\theta,\\pmb\\theta_{1}){\\bf I}\\quad\\cdot\\cdot\\cdot\\quad k(\\pmb\\theta,\\pmb\\theta_{\\tau}){\\bf I}\\quad\\cdot\\cdot\\cdot\\quad k(\\pmb\\theta,\\pmb\\theta_{t-1}){\\bf I}]=k_{t}^{\\top}(\\pmb\\theta)\\otimes{\\bf I}\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{U}_{t}=\\left[\\begin{array}{c c c c c c}{k(\\theta_{1},\\theta_{1})\\mathbf{I}}&{\\cdot\\cdot\\cdot}&{k(\\theta_{1},\\theta_{\\tau^{\\prime}})\\mathbf{I}}&{\\cdot\\cdot\\cdot}&{k(\\theta_{1},\\theta_{t-1})\\mathbf{I}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {k(\\theta_{\\tau},\\theta_{1})\\mathbf{I}}&{\\cdot\\cdot}&{k(\\theta_{\\tau},\\theta_{\\tau^{\\prime}})\\mathbf{I}}&{\\cdot\\cdot}&{k(\\theta_{\\tau},\\theta_{t-1})\\mathbf{I}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {k(\\theta_{t-1},\\theta_{1})\\mathbf{I}}&{\\cdot\\cdot}&{k(\\theta_{t-1},\\theta_{\\tau^{\\prime}})\\mathbf{I}}&{\\cdot\\cdot}&{k(\\theta_{t-1},\\theta_{t-1})\\mathbf{I}}\\end{array}\\right]=\\mathbf{K}_{t}\\otimes\\mathbf{I}\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By introducing the results above into the posterior mean and variance in (3), we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t}(\\theta)\\overset{(a)}{=}\\mathbf{V}_{t}^{\\top}(\\theta)\\left(\\mathbf{U}_{t}+\\sigma^{2}\\right)^{-1}\\mathbf{vrc}(\\mathbf{G}_{t}^{\\top})}\\\\ &{\\quad\\quad\\overset{(b)}{=}\\left(k_{t}^{\\top}(\\theta)\\otimes\\mathbf{I}\\right)\\left(\\mathbf{K}_{t}\\otimes\\mathbf{I}+\\sigma^{2}\\right)^{-1}\\mathbf{vc}(\\mathbf{G}_{t}^{\\top})}\\\\ &{\\quad\\quad\\overset{(c)}{=}\\left(k_{t}^{\\top}(\\theta)\\otimes\\mathbf{I}\\right)\\left[\\left(\\mathbf{K}_{t}+\\sigma^{2}\\right)\\otimes\\mathbf{I}\\right]^{-1}\\mathbf{vc}(\\mathbf{G}_{t}^{\\top})}\\\\ &{\\quad\\quad\\overset{(d)}{=}\\left(k_{t}^{\\top}(\\theta)\\otimes\\mathbf{I}\\right)\\left[\\left(\\mathbf{K}_{t}+\\sigma^{2}\\right)^{-1}\\otimes\\mathbf{I}\\right]\\mathbf{w}_{t}\\otimes\\left(\\mathbf{G}_{t}^{\\top}\\right)}\\\\ &{\\quad\\stackrel{(c)}{=}\\left(\\left[k_{t}^{\\top}(\\theta)\\left(\\mathbf{K}_{t}+\\sigma^{2}\\right)^{-1}\\right]\\otimes\\mathbf{I}\\right)\\mathbf{wc}\\left(\\mathbf{G}_{t}^{\\top}\\right)}\\\\ &{\\quad\\quad\\overset{(d)}{=}\\left(\\mathbf{K}_{t}^{\\top}\\left(\\theta\\right)\\left(\\mathbf{K}_{t}+\\sigma^{2}\\right)^{-1}\\right)\\otimes\\mathbf{I}\\right)\\mathbf{w}_{t}\\left(\\mathbf{G}_{t}^{\\top}\\right)}\\\\ &{\\quad\\quad\\overset{(d)}{=}\\left(\\mathbf{G}_{t}^{\\top}\\left[k_{t}^{\\top}(\\theta)\\left(\\mathbf{K}_{t}+\\sigma^{2}\\right)^{-1}\\right]\\right)^{-1}\\right)}\\\\ &{\\quad\\quad\\vdots}\\\\ &{\\quad\\quad\\quad\\mathcal{Q}\\left[\\left(k_{t}^{\\top}(\\theta)\\left(\\mathbf{K}_{t}+\\sigma^{2}\\right)^{-1}\\right)\\mathbf{G}_{t}\\right]^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $(c)$ come from the bi-linearity of the Kronecker product, i.e., $\\left(\\mathbf{A}\\!+\\!\\mathbf{B}\\right)\\!\\otimes\\!\\mathbf{C}=\\mathbf{A}\\otimes\\mathbf{C}\\!+\\!\\mathbf{B}\\otimes\\mathbf{C}$ while $(d)$ is from the inverse of the Kronecker product, i.e., $\\left(\\mathbf{A}\\otimes\\mathbf{B}\\right)^{-1}=\\mathbf{A}^{-1}\\otimes\\mathbf{B}^{-1}$ . In addition, $(e)$ is due to the mixed-product property of the Kronecker product, i.e., $(\\mathbf{A}\\otimes\\mathbf{B})(\\mathbf{C}\\otimes\\mathbf{D})=(\\mathbf{AC})\\otimes$ (BD), and $(f)$ results from the mixed Kronecker matrix-vector product of the Kronecker product, i.e., $(\\mathbf{A}\\otimes\\mathbf{B})\\mathrm{vec}(\\mathbf{C})=\\mathrm{vec}(\\mathbf{BCA}^{\\top})$ . ", "page_idx": 12}, {"type": "text", "text": "Similarly, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\Sigma}_{t}^{2}(\\theta,\\theta^{\\prime})\\stackrel{(a)}{=}\\mathbf{K}\\left(\\theta,\\theta^{\\prime}\\right)-\\mathbf{V}_{t}^{\\top}(\\theta)\\left(\\mathbf{U}_{t}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\Phi_{n}\\left(\\theta^{\\prime}\\right)}\\\\ &{\\stackrel{(b)}{=}k(\\theta,\\theta^{\\prime})\\mathbf{I}-\\left(\\left[k_{t}^{\\top}(\\theta)\\left(\\mathbf{K}_{t}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\right]\\otimes\\mathbf{I}\\right)\\left(k_{t}(\\theta^{\\prime})\\otimes\\mathbf{I}\\right)}\\\\ &{\\stackrel{(c)}{=}k(\\theta,\\theta^{\\prime})\\mathbf{I}-\\left(k_{t}^{\\top}(\\theta)\\left(\\mathbf{K}_{t}+\\sigma^{2}\\mathbf{I}\\right)^{-1}k_{t}(\\theta^{\\prime})\\right)\\mathbf{I}}\\\\ &{\\stackrel{(d)}{=}\\left(k(\\theta,\\theta^{\\prime})-k_{t}^{\\top}(\\theta)\\left(\\mathbf{K}_{t}+\\sigma^{2}\\mathbf{I}\\right)^{-1}k_{t}(\\theta^{\\prime})\\right)\\mathbf{I}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where (b) comes from the result in (12), $(c)$ results from the mixed-product property of the Kronecker product and the fact that $\\left(k_{t}^{\\top}(\\pmb\\theta)\\left(\\dot{\\mathbf K}_{t}+\\sigma^{2}\\mathbf I\\right)^{-1}k_{t}(\\pmb\\theta^{\\prime})\\right)$ is a scalar. This finally concludes our proof. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To begin with, we introduce the following lemmas: ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1 ([43]). Let $\\zeta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d})$ and $\\delta\\in(0,1)$ then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\|\\pmb{\\zeta}\\|_{2}\\leq\\sqrt{d+2(\\sqrt{d}+1)\\ln(1/\\delta)}\\right)\\geq1-\\delta\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.2 (Lemma 2 in Appx. B of [34]). For any $\\sigma\\in\\mathbb R$ and any matrix A, the following hold ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{I}-\\mathbf{A}^{\\top}\\left(\\mathbf{A}\\mathbf{A}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}=\\sigma^{2}\\left(\\mathbf{A}^{\\top}\\mathbf{A}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\mathbf{\\Omega}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.3 (Sherman-Morrison formula). For any invertible square matrix A and column vectors ${\\pmb u},{\\pmb v}$ , suppose A + uv\u22a4is invertible, then the following holds ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\mathbf{A}+\\boldsymbol{u v}^{\\top}\\right)^{-1}=\\mathbf{A}^{-1}-\\frac{\\mathbf{A}^{-1}\\boldsymbol{u v}^{\\top}\\mathbf{A}^{-1}}{1+\\boldsymbol{v}^{\\top}\\mathbf{A}^{-1}\\boldsymbol{u}}\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.4 (Non-Increasing Variance Norm). Define variance $\\Sigma_{n}^{2}(\\pmb{\\theta})\\triangleq\\Sigma_{n}^{2}(\\pmb{\\theta},\\pmb{\\theta})$ with n being the number of gradients employed to evaluate the mean and covariance in Prop. 4.1. Then for any $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ and $n\\geq1$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\Sigma_{n}^{2}(\\pmb\\theta)\\right\\|\\leq\\left\\|\\Sigma_{n-1}^{2}(\\pmb\\theta)\\right\\|~.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. We follow the idea in [34] and [25] to prove it. Specifically, we firstly define $k(\\pmb\\theta,\\pmb\\theta^{\\prime})\\,=$ $\\boldsymbol{\\phi}(\\pmb{\\theta})^{\\top}\\boldsymbol{\\phi}(\\pmb{\\theta}^{\\prime})$ and $\\phi_{n}\\triangleq[\\phi(\\pmb{\\theta}_{i})]_{i=1}^{n}$ . Then the matrix $\\mathbf{K}_{n}$ in Prop. 4.1 can be reformulated as ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathbf K}_{n}=\\boldsymbol\\phi_{n}^{\\top}\\boldsymbol\\phi_{n}\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and based on the definition of ${\\boldsymbol{\\Phi}}_{n}\\triangleq\\phi_{n}\\phi_{n}^{\\intercal}+\\sigma^{2}\\mathbf{I}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{t}^{2}(\\theta)\\overset{(a)}{=}\\left(\\phi(\\theta)^{\\top}\\phi(\\theta)-\\phi(\\theta)^{\\top}\\phi_{n}\\left(\\phi_{n}^{\\top}\\phi_{n}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\phi_{n}^{\\top}\\phi(\\theta)\\right)\\mathbf{I}}\\\\ &{\\overset{(b)}{=}\\left(\\phi(\\theta)^{\\top}\\left(\\mathbf{I}-\\phi_{n}\\left(\\phi_{n}^{\\top}\\phi_{n}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\phi_{n}^{\\top}\\right)\\phi(\\theta)\\right)\\mathbf{I}}\\\\ &{\\overset{(c)}{=}\\left(\\sigma^{2}\\phi(\\theta)^{\\top}\\left(\\phi_{n}\\phi_{n}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)^{-1}\\phi(\\theta)\\right)\\mathbf{I}}\\\\ &{\\overset{(d)}{=}\\left(\\sigma^{2}\\phi(\\theta)^{\\top}\\Phi_{n}^{-1}\\phi(\\theta)\\right)\\mathbf{I}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(c)$ comes from Lemma A.2 by replacing the matrix A in Lemma A.2 with the matrix $\\phi_{n}^{\\top}$ . ", "page_idx": 13}, {"type": "text", "text": "As a result, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{n}^{2}(\\theta)}\\\\ &{\\overset{\\mathrm{a}}{=}\\left(\\sigma^{2}\\phi(\\theta)^{\\top}\\Phi_{n}^{-1}\\phi(\\theta)\\right)\\mathbf{I}}\\\\ &{\\overset{\\mathrm{b}}{=}\\left(\\sigma^{2}\\phi(\\theta)^{\\top}\\left(\\phi_{n-1}\\phi_{n-1}^{\\top}+\\sigma^{2}\\mathbf{I}+\\phi(\\theta_{n})\\phi(\\theta_{n})^{\\top}\\right)^{-1}\\phi(\\theta)\\right)\\mathbf{I}}\\\\ &{\\overset{\\mathrm{c}}{=}\\left(\\sigma^{2}\\phi(\\theta)^{\\top}\\left(\\Phi_{n-1}+\\phi(\\theta_{n})\\phi(\\theta_{n})^{\\top}\\right)^{-1}\\phi(\\theta)\\right)\\mathbf{I}}\\\\ &{\\overset{\\mathrm{d}}{=}\\left(\\sigma^{2}\\phi(\\theta)^{\\top}\\left(\\Phi_{n-1}+\\phi(\\theta_{n})\\phi(\\theta_{n})^{\\top}\\right)^{-1}\\phi(\\theta)\\right)\\mathbf{I}}\\\\ &{\\overset{\\mathrm{d}}{=}\\left(\\sigma^{2}\\phi(\\theta)^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta)-\\sigma^{2}\\left(1+\\phi(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta_{n})\\right)^{-1}\\phi(\\theta)^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta_{n})\\phi(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta)\\right)}\\\\ &{\\overset{\\mathrm{c}}{=}\\sum_{n-1}^{2}(\\theta)-\\sigma^{2}\\left(1+\\phi(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta_{n})\\right)^{-1}\\phi(\\theta)^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta_{n})\\phi(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta)\\,\\mathbf{I}}\\\\ &{\\overset{\\mathrm{D}}{=}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(b)$ is due to the fact that $\\Phi_{n}\\Phi_{n}^{\\top}=\\Phi_{n-1}\\Phi_{n-1}^{\\top}+\\phi(\\pmb\\theta_{n})\\phi(\\pmb\\theta_{n})^{\\top}$ , and $(d)$ is from Lemma A.3. Finally, $(f)$ derives from the positive semi-definite property of $\\boldsymbol{\\Phi}_{n-1}^{-1}\\boldsymbol{\\phi}(\\boldsymbol{\\theta}_{t})\\boldsymbol{\\phi}(\\boldsymbol{\\theta}_{t})^{\\top}\\boldsymbol{\\Phi}_{n-1}^{-1}$ and $\\Phi_{n-1}^{-1}$ , leading to the conclusion of our proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma A.5 (lower Bound of Variance Norm). Following the definition in Lemma A.4, for any $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ and $n\\geq1$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\Sigma_{n}^{2}(\\pmb\\theta)\\right\\|\\geq\\frac{1}{\\left(\\kappa+1/\\sigma^{2}\\right)}\\left\\|\\Sigma_{n-1}^{2}(\\pmb\\theta)\\right\\|~.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Again, we follow the idea in [34] and [25] to prove it. we first prove the following inequality ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\Phi_{n-1}^{-1/2}\\phi(\\theta_{n})\\phi(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1/2}\\right\\Vert\\stackrel{(a)}{=}\\left\\Vert\\phi(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1/2}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\left(\\theta_{n}\\right)\\phi(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1}\\phi(\\theta_{n})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\stackrel{(c)}{\\leq}\\phi(\\theta_{n})^{\\top}\\Phi_{n-2}^{-1}\\phi(\\theta_{n})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\stackrel{(d)}{\\leq}\\sigma^{2}\\phi(\\theta_{n})^{\\top}\\phi(\\theta_{n})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\stackrel{(e)}{\\leq}\\kappa\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(c)$ comes from the fact that $\\begin{array}{r}{\\Phi_{n-1}=\\Phi_{n-2}^{-1}+\\phi(\\pmb{\\theta}_{n-1}\\phi(\\pmb{\\theta}_{n-1})^{\\top}\\succcurlyeq\\Phi_{n-2}\\succcurlyeq\\cdot\\cdot\\cdot\\succcurlyeq\\sigma^{2}\\mathbf{I}}\\end{array}$ and $(e)$ is due to the fact that $\\begin{array}{r}{\\boldsymbol{\\phi}(\\boldsymbol{\\theta}_{n})^{\\top}\\boldsymbol{\\phi}(\\boldsymbol{\\theta}_{n})=k(\\boldsymbol{\\theta}_{n},\\boldsymbol{\\theta}_{n})\\leq\\kappa}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Then, based on the reformulation of $\\Sigma_{n}^{2}(\\pmb\\theta)$ in (20), we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\Sigma}_{n}^{2}(\\theta)\\overset{(a)}{=}\\left(\\sigma^{2}\\boldsymbol{\\phi}(\\theta)^{\\top}\\left(\\Phi_{n-1}+\\boldsymbol{\\phi}(\\theta_{n})\\boldsymbol{\\phi}(\\theta_{n})^{\\top}\\right)^{-1}\\boldsymbol{\\phi}(\\theta)\\right)\\textbf{I}}\\\\ &{\\quad\\quad\\overset{(b)}{=}\\left(\\sigma^{2}\\boldsymbol{\\phi}(\\theta)^{\\top}\\Phi_{n-1}^{-1/2}\\left(\\mathbf{I}+\\Phi_{n-1}^{-1/2}\\boldsymbol{\\phi}(\\theta_{n})\\boldsymbol{\\phi}(\\theta_{n})^{\\top}\\Phi_{n-1}^{-1/2}\\right)^{-1}\\Phi_{n-1}^{-1/2}\\boldsymbol{\\phi}(\\theta)\\right)\\textbf{I}}\\\\ &{\\quad\\overset{(c)}{\\sim}\\frac{\\sigma^{2}}{1+\\kappa\\sigma^{2}}\\boldsymbol{\\phi}(\\theta)^{\\top}\\Phi_{n-1}^{-1}\\boldsymbol{\\phi}(\\theta)\\,\\mathbf{I}}\\\\ &{\\quad\\overset{(d)}{=}\\frac{\\sigma^{2}}{1+\\kappa\\sigma^{2}}\\boldsymbol{\\Sigma}_{n-1}^{2}(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(c)$ comes from (22). This finally concludes our proof. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.6 (Information Gain). Define $\\mathbf{G}_{n}\\ \\triangleq\\ [\\nabla f(\\pmb{\\theta}_{i})]_{i=1}^{n}$ , $\\pmb{\\nabla}_{n}\\,\\triangleq\\,[\\nabla F(\\pmb{\\theta}_{i})]_{i=1}^{n}$ , and $\\mathbf{K}_{n}$ \u225c k(\u03b8i, \u03b8j) i,j=1. The information gain $I(\\mathrm{vec}(\\mathbf{G}_{n});\\mathrm{vec}(\\mathbf{\\nabla}\\nabla_{n}))$ has the following form with Assump. 1, 2: ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(\\mathrm{vec}(\\mathbf{G}_{n});\\mathrm{vec}(\\mathbf{\\nabla}\\mathbf{v}_{n}))=\\frac{d}{2}\\ln\\left(\\operatorname*{det}(\\mathbf{I}+\\sigma^{-2}\\mathbf{K}_{n})\\right)\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Based on our Assump. 1, 2, the following holds respectively: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{vec}(\\mathbf G_{n})\\mid\\mathrm{vec}(\\nabla_{n})\\sim\\mathcal N(\\mathbf0,\\sigma^{2}\\,\\mathbf I_{n d})\\,,\\mathrm{~and~}\\,\\mathrm{vec}(\\mathbf G_{n})\\sim\\mathcal G P\\left(\\mathbf0,\\left(\\mathbf K_{n}+\\sigma^{2}\\mathbf I_{n}\\right)\\otimes\\mathbf I_{d}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Due to the fact that $\\begin{array}{r}{H(\\mathbf{z})=\\frac{1}{2}\\ln(\\operatorname*{det}(2\\pi e\\mathbf{\\Sigma}))}\\end{array}$ if $\\mathbf{z}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},\\mathbf{\\Sigma}\\Sigma)$ , the following holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{(vec}(\\mathbf{G}_{n});\\mathrm{vec}(\\nabla_{n}))\\overset{(a)}{=}H(\\mathrm{vec}(\\mathbf{G}_{n}))-H(\\mathrm{vec}(\\mathbf{G}_{n})\\mid\\mathrm{vec}(\\nabla_{n}))}&{}\\\\ {\\overset{(b)}{=}\\frac{1}{2}\\ln\\left(\\operatorname*{det}\\left(2\\pi e\\left(\\mathbf{K}_{n}+\\sigma^{2}\\mathbf{I}_{n}\\right)\\otimes\\mathbf{I}_{d}\\right)\\right)-\\frac{1}{2}\\ln\\left(\\operatorname*{det}\\left(2\\pi e\\sigma^{2}\\mathbf{I}_{n d}\\right)\\right)}&{}\\\\ {\\overset{(c)}{=}\\frac{1}{2}\\ln\\left(\\left[\\operatorname*{det}\\left(2\\pi e\\left(\\mathbf{K}_{n}+\\sigma^{2}\\mathbf{I}_{n}\\right)\\right)\\right]^{d}\\left(\\operatorname*{det}\\left(\\mathbf{I}_{d}\\right)\\right)^{n}\\right)-\\frac{1}{2}\\ln\\left(\\operatorname*{det}\\left(2\\pi e\\sigma^{2}\\mathbf{I}_{n d}\\right)\\right.}&{}\\\\ {\\overset{(d)}{=}\\frac{1}{2}\\ln\\left(\\frac{\\operatorname*{det}\\left(2\\pi e\\left(\\mathbf{K}_{n}+\\sigma^{2}\\mathbf{I}_{n}\\right)\\right)}{\\operatorname*{det}\\left(2\\pi e\\sigma^{2}\\mathbf{I}_{n}\\right)}\\right)^{d}}&{}\\\\ {\\overset{(e)}{=}\\frac{d}{2}\\ln\\left(\\operatorname*{det}(\\mathbf{I}+\\sigma^{-2}\\mathbf{K}_{n})\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(a)$ comes from the definition of information gain, $(b)$ derives from the results in (25), and $(c)$ is due to the fact that $\\operatorname*{det}(\\mathbf{A}\\otimes\\mathbf{B})=\\left(\\operatorname*{det}(\\mathbf{A})\\right)^{b}\\left(\\operatorname*{det}(\\mathbf{B})\\right)^{a}$ given the $a\\times a$ -dimensional matrix A and $b\\times b$ -dimensional matrix $\\mathbf{B}$ . In addition, $(e)$ follows from $\\operatorname*{det}(\\mathbf{A}\\mathbf{B}^{-1})=\\operatorname*{det}(\\mathbf{A})/\\operatorname*{det}(\\mathbf{B})$ . This then concludes our proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.7 (Sum of Variance). Define the maximal information gain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{n}\\triangleq\\operatorname*{max}_{\\{\\pmb{\\theta}_{j}\\}_{j=1}^{n}\\subset\\mathbb{R}^{d}}I(\\mathrm{vec}(\\mathbf{G}_{n});\\mathrm{vec}(\\pmb{\\nabla}_{n}))\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "the following then holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac1n\\sum_{i=0}^{n-1}\\left\\|\\Sigma_{i}^{2}(\\pmb\\theta)\\right\\|\\leq\\frac{2\\sigma^{2}\\gamma_{n}}d\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. To begin with, we show the following inequalities resulting from the matrix determinant lemma: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}\\left(\\boldsymbol{\\Phi}_{i+1}\\right)=\\operatorname*{det}\\left(\\boldsymbol{\\Phi}_{i}+\\phi(\\pmb{\\theta})\\phi(\\pmb{\\theta})^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\quad=\\operatorname*{det}\\left(\\boldsymbol{\\Phi}_{i}\\right)\\left(1+\\phi(\\pmb{\\theta})^{\\top}\\pmb{\\Phi}_{i}^{-1}\\phi(\\pmb{\\theta})\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given $\\kappa~\\leq~\\sigma^{2}$ , since $\\Big\\|\\Sigma_{n}^{2}(\\pmb\\theta)\\Big\\|\\ \\leq\\ \\Big\\|\\Sigma_{n-1}^{2}(\\pmb\\theta)\\Big\\|\\ \\leq\\ \\cdots\\ \\leq\\ \\Big\\|\\Sigma_{0}^{2}(\\pmb\\theta)\\Big\\|\\ =\\ |k(\\pmb\\theta,\\pmb\\theta)|\\ \\leq\\ \\kappa$ from Lemma A.4, we then hav e $\\phi(\\pmb\\theta)^{\\top}\\pmb{\\Phi}_{i}^{-1}\\phi(\\pmb\\theta)\\leq1$ . A s a result, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{2}\\displaystyle\\sum_{i=0}^{n-1}\\left\\|\\mathbf{x}_{i}^{\\alpha}(\\theta)\\right\\|\\ U_{i}^{\\alpha}\\rightleftharpoons\\operatorname{c}_{i}^{\\infty}\\operatorname{c}_{j=0}^{\\infty-1}\\operatorname{arg}\\left(\\theta\\right)^{\\alpha}\\operatorname{c}_{i}^{-1}\\mathcal{S}_{i}^{\\alpha}(\\theta)}\\\\ {\\overset{(i)}{\\underset{s\\to}{\\leq}}\\frac{\\operatorname{c}_{j=1}^{\\infty}}{\\operatorname{c}_{j=0}^{\\infty-1}}\\ln\\left(1+\\phi(\\theta)^{\\alpha}\\Psi_{i}^{-1}\\phi(\\theta)\\right)}\\\\ {\\overset{(i i)}{\\underset{s\\to}{\\leq}}\\frac{\\operatorname{c}_{j=1}^{\\infty-1}}{\\operatorname{c}_{j=1}^{\\infty}}\\ln\\left(\\frac{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{+}\\right)}{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{+}\\right)}\\right)}\\\\ {\\overset{(i i)}{\\underset{s\\to}{\\leq}}\\frac{\\operatorname{c}_{j=1}^{\\infty-1}}{\\operatorname{drg}\\left(\\frac{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{+}\\right)}{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{+}\\right)}\\right)}}\\\\ {\\overset{(i i)}{\\underset{s\\to}{\\leq}}\\frac{\\operatorname{c}_{j}^{\\infty}}{\\operatorname{drg}\\left(\\frac{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{+}\\right)}{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{+}\\right)}+\\frac{\\operatorname{c}_{j}^{\\infty-1}}{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{-}\\right)}\\right)}}\\\\ {\\mathcal{Q}\\mathcal{Q}^{\\prime}\\ln\\left(\\frac{\\operatorname{drg}\\left(\\phi^{-}-^{\\alpha}\\phi_{\\theta,\\alpha}^{+}\\in\\mathcal{Q}_{N}^{-1}\\right)}{\\operatorname{drg}\\left(\\phi_{\\theta,\\alpha}^{+}\\right)}\\right)}\\\\ {\\vdots}\\\\ {\\mathcal{Q}\\mathcal{Q}^{\\prime}\\ln\\left(\\phi((1+\\phi^{-2}\\phi_{\\theta,\\alpha}^{+})\\phi_{\\theta,\\alpha})\\right)}\\\\ {\\frac{\\partial\\mathcal{Q}^{\\prime}}{\\partial t}\\frac{\\partial\\mathcal{Q}^{\\prime}}{\\partial t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(a)$ follows from the reformulation of $\\Sigma_{i}^{2}(\\pmb\\theta)$ in (19), $(b)$ results from the fact that $x/2\\le$ $\\ln(1+x)$ for any $x\\in(0,1)$ , $(c)$ derives from (29), $(d)$ is from the telescoping sum, $(e)$ is due to the fact that $\\operatorname*{det}(\\boldsymbol{\\Phi}_{0})=\\operatorname*{det}(\\sigma^{2}\\mathbf{I})$ , $(f)$ is from the fact that $\\operatorname*{det}(\\mathbf{A}\\mathbf{B}^{-1})=\\operatorname*{det}(\\mathbf{A})/\\operatorname*{det}(\\mathbf{B}),(g)$ comes from the Sylvester\u2019s determinant identity, i.e., $\\operatorname*{det}(\\boldsymbol{\\Phi}_{i})=\\operatorname*{det}(\\mathbf{K}_{i}\\!+\\!\\sigma^{2}\\mathbf{I}_{i})$ according to the definition of $\\Phi_{i}$ , and $(h)$ results from the fact that ${\\bf K}_{n}=\\phi_{n}^{\\top}\\phi_{n}$ in (18), the conclusion in Lemma A.6, and the definition of $\\gamma_{n}$ . ", "page_idx": 16}, {"type": "text", "text": "Following the same idea, given $\\kappa>\\sigma^{2}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2\\kappa}\\displaystyle\\sum_{i=0}^{n-1}\\left\\|\\Sigma_{i}^{2}(\\theta)\\right\\|\\overset{(a)}{=}\\displaystyle\\sum_{i=0}^{n-1}\\frac{\\sigma^{2}}{2\\kappa}\\phi(\\theta)^{\\top}\\Phi_{i}^{-1}\\phi(\\theta)}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\sum_{i=0}^{n-1}\\ln\\left(1+\\frac{\\sigma^{2}}{\\kappa}\\phi(\\theta)^{\\top}\\Phi_{i}^{-1}\\phi(\\theta)\\right)}\\\\ &{\\overset{(c)}{\\leq}\\displaystyle\\sum_{i=0}^{n-1}\\ln\\left(1+\\phi(\\theta)^{\\top}\\Phi_{i}^{-1}\\phi(\\theta)\\right)}\\\\ &{\\overset{(d)}{\\leq}\\displaystyle\\sum_{d=0}^{2\\gamma_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the results in (30) and (31), we conclude our proof by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac1n\\sum_{i=0}^{n-1}\\left\\|\\Sigma_{i}^{2}(\\pmb\\theta)\\right\\|\\le\\frac{4\\operatorname*{max}\\{\\kappa,\\sigma^{2}\\}\\gamma_{n}}{d n}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of our Thm. 1. Since $\\Sigma_{n}^{-1}(\\pmb\\theta)\\left[\\pmb\\mu_{n}(\\pmb\\theta)-\\nabla F(\\pmb\\theta)\\right]\\sim\\mathcal{N}(\\pmb0,\\mathbf I_{d})$ , according to Lemma A.1, for any $\\delta\\in(0,1)$ and $\\alpha\\triangleq d+2(\\sqrt{d}+1)\\ln(1/\\delta)$ , with a probability of at least $1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla F(\\pmb{\\theta})-\\pmb{\\mu}_{n}(\\pmb{\\theta})\\|\\overset{(a)}{\\le}\\|\\Sigma_{n}(\\pmb{\\theta})\\|\\,\\Big\\|\\Sigma_{n}^{-1}(\\pmb{\\theta})\\,[\\pmb{\\mu}_{n}(\\pmb{\\theta})-\\nabla F(\\pmb{\\theta})]\\Big\\|}&{}\\\\ {\\overset{(b)}{\\le}\\sqrt{\\alpha}\\,\\Big\\|\\Sigma_{n}^{2}(\\pmb{\\theta})\\Big\\|^{1/2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(a)$ is from CauchySchwarz inequality and $(b)$ is from Lemma A.1. By introducing the results in Lemma A.5 and Lemma A.7 into the result above and letting $T_{0}=n+1$ , we conclude our proof. ", "page_idx": 17}, {"type": "text", "text": "A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In general, we follow the idea in [36] to give a high probability convergence for our OptEx algorithm.   \nTo begin with, we introduce the following definition and lemma. ", "page_idx": 17}, {"type": "text", "text": "Definition A.1 (Sub-Gaussian Random Variable). A random variable $\\Chi$ is $\\sigma$ -sub-Gaussian if $\\mathbb{E}\\left[\\exp\\left(\\lambda^{2}\\mathbf{X}^{2}\\right)\\right]\\leq\\exp\\left(\\lambda^{2}\\sigma^{2}\\right)\\forall\\lambda$ such that $|\\lambda|\\leq\\frac{1}{\\sigma}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma A.8 (Bound for Sub-Gaussian Random Variable). Suppose $\\Chi$ is a $\\sigma$ -sub-Gaussian random variable, then for any a \u2208R, 0 \u2264b \u226421\u03c3, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(a{\\mathbf{X}}+b^{2}{\\mathbf{X}}^{2}\\right)\\right]\\leq\\exp\\left((a^{2}+b^{2})\\sigma^{2}+{\\frac{1}{4}}\\right)~.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(a\\mathbf{X}+b^{2}\\mathbf{X}^{2}\\right)\\right]\\stackrel{(a)}{\\leq}\\mathbb{E}\\left[\\exp\\left(a^{2}\\sigma^{2}+\\frac{\\mathbf{X}^{2}}{4\\sigma^{2}}+b^{2}\\mathbf{X}^{2}\\right)\\right]}\\\\ &{\\stackrel{(b)}{=}\\exp\\left(a^{2}\\sigma^{2}\\right)\\mathbb{E}\\left[\\exp\\left(\\left(\\frac{1}{4\\sigma^{2}}+b^{2}\\right)\\mathbf{X}^{2}\\right)\\right]}\\\\ &{\\stackrel{(c)}{\\leq}\\exp\\left(a^{2}\\sigma^{2}\\right)\\exp\\left(\\left(\\frac{1}{4\\sigma^{2}}+b^{2}\\right)\\sigma^{2}\\right)}\\\\ &{\\stackrel{(d)}{=}\\exp\\left((a^{2}+b^{2})\\sigma^{2}+\\frac{1}{4}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(c)$ comes from the definition of $\\sigma$ -sub-Gaussian random variable. ", "page_idx": 17}, {"type": "text", "text": "Proof of Thm. 2. Define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}(\\pmb{\\theta}_{t,s})\\triangleq\\left\\{\\left\\|\\sum^{2}(\\pmb{\\theta}_{t,s},\\pmb{\\theta}_{t,s})\\right\\|\\quad\\mathrm{if~}s<N-1}\\\\ &{\\qquad\\qquad\\sigma^{2}\\qquad\\qquad\\mathrm{if~}s=N-1\\,,}\\\\ &{\\varepsilon(\\pmb{\\theta}_{t,s})\\triangleq\\left\\{\\nabla F(\\pmb{\\theta}_{t,s})-\\nabla\\mu_{t}(\\pmb{\\theta}_{t,s})\\quad\\mathrm{if~}s<N-1\\right.}\\\\ &{\\qquad\\varepsilon(\\pmb{\\theta}_{t,s})=\\left\\{\\nabla F(\\pmb{\\theta}_{t,s})-\\nabla f(\\pmb{\\theta}_{t,s})\\quad\\mathrm{if~}s=N-1\\,,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{X}_{t,s}\\triangleq w\\left(\\eta(1-\\eta L)\\nabla F(\\pmb{\\theta}_{t,s-1})^{\\top}\\varepsilon_{t}(\\pmb{\\theta}_{t,s-1})+\\frac{\\eta^{2}L}{2}\\left\\|\\varepsilon_{t}(\\pmb{\\theta}_{t,s-1})\\right\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad-\\left.w^{2}\\eta^{2}(1-\\eta L)^{2}\\left\\|\\nabla F(\\pmb{\\theta}_{t,s-1})\\right\\|^{2}\\sigma^{2}(\\pmb{\\theta}_{t,s-1})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "According to our Lemma A.8 and the fact that each dimension of $\\varepsilon_{t}(\\pmb\\theta_{t,s})$ follows an independent Gaussian distribution given Assump. 2, the following holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbb{\\xi}\\left[\\exp\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\mathbf{X}_{t,s}\\right)\\right]\\leq\\exp\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\left(w^{2}\\eta^{2}(1-\\eta L)^{2}\\left\\lVert\\nabla F(\\theta_{t,s-1})\\right\\rVert^{2}+\\displaystyle\\frac{w\\eta^{2}L}{2}\\right)\\sigma^{2}(\\theta_{t,s-1})\\right.\\right.}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\left.\\left.-\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}w^{2}\\eta^{2}(1-\\eta L)^{2}\\left\\lVert\\nabla F(\\theta_{t,s-1})\\right\\rVert^{2}\\sigma^{2}(\\theta_{t,s-1})+\\frac{1}{4}\\right)}\\\\ {\\displaystyle=\\exp\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\frac{w\\eta^{2}L}{2}\\sigma^{2}(\\theta_{t,s-1})+\\frac{1}{4}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Based on Markov inequality, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\exp\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\mathbf{X}_{t,s}\\right)>\\frac{1}{2\\delta}\\exp\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\frac{w\\eta^{2}L}{2}\\sigma^{2}(\\theta_{t,s-1})\\right)\\right]}\\\\ &{\\leq\\displaystyle\\frac{\\mathbb{E}\\left[\\exp\\left(\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\mathbf{X}_{t,s}\\right)\\right]}{\\exp\\left(\\sum_{t=1}^{T}\\sum_{s=1}^{N}w\\eta^{2}L\\sigma^{2}(\\theta_{t,s-1})/2\\right)/(2\\delta)}}\\\\ &{\\leq\\delta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, with a probability of at least $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\Chi_{t,s}\\leq\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\frac{w\\eta^{2}L}{2}\\sigma^{2}(\\pmb{\\theta}_{t,s-1})+\\ln\\left(\\frac{1}{2\\delta}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which leads to the following inequality with $w=w$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\substack{i=1}}^{T}\\sum_{s=1}^{N}\\left(\\eta(1-\\eta L)\\nabla F(\\theta_{t,s-1})^{\\top}\\varepsilon_{t}(\\theta_{t,s-1})+\\frac{\\eta^{2}L}{2}\\left\\Vert\\varepsilon_{t}(\\theta_{t,s-1})\\right\\Vert^{2}\\right)\\leq}&{}\\\\ {\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\left(w\\eta^{2}(1-\\eta L)^{2}\\left\\Vert\\nabla F(\\theta_{t,s-1})\\right\\Vert^{2}\\sigma^{2}(\\theta_{t,s-1})+\\frac{\\eta^{2}L}{2}\\sigma^{2}(\\theta_{t,s-1})\\right)+\\frac{1}{w}\\ln\\left(\\frac{1}{2\\delta}\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Of note, for every proxy step based on SGD: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad F(\\pmb\\theta_{t,s})}\\\\ &{\\stackrel{a}{\\leq}F(\\pmb\\theta_{t,s-1})+\\nabla F(\\pmb\\theta_{t,s-1})^{\\top}(\\pmb\\theta_{t,s}-\\pmb\\theta_{t,s-1})+\\frac{L}{2}\\left\\|\\pmb\\theta_{t,s}-\\pmb\\theta_{t,s-1}\\right\\|^{2}}\\\\ &{\\stackrel{b}{=}F(\\pmb\\theta_{t,s-1})-\\eta\\nabla F(\\pmb\\theta_{t,s-1})^{\\top}(\\nabla F(\\pmb\\theta_{t,s-1})-\\pmb\\varepsilon_{t}(\\pmb\\theta_{t,s-1}))+\\frac{\\eta^{2}L}{2}\\left\\|\\nabla F(\\pmb\\theta_{t,s-1})-\\pmb\\varepsilon_{t}(\\pmb\\theta_{t,s-1})\\right\\|^{2}}\\\\ &{\\stackrel{c}{=}F(\\pmb\\theta_{t,s-1})+\\eta(1-\\eta L)\\nabla F(\\pmb\\theta_{t,s-1})^{\\top}\\varepsilon_{t}(\\pmb\\theta_{t,s-1})+\\left(\\frac{\\eta^{2}L}{2}-\\eta\\right)\\left\\|\\nabla F(\\pmb\\theta_{t,s-1})\\right\\|^{2}+\\frac{\\eta^{2}L}{2}\\left\\|\\pmb\\varepsilon_{t}(\\pmb\\theta_{t,s-1})\\right\\|^{2}}\\\\ &{\\stackrel{a}{=}F(\\pmb\\theta_{t,s-1})+\\eta(1-\\eta L)\\nabla F(\\pmb\\theta_{t,s-1})^{\\top}\\varepsilon_{t}(\\pmb\\theta_{t,s-1})+\\left(\\frac{\\eta^{2}L}{2}-\\eta\\right)\\left\\|\\nabla F(\\pmb\\theta_{t,s-1})\\right\\|^{2}+\\frac{\\eta^{2}L}{2}\\left\\|\\pmb\\varepsilon_{t}(\\pmb\\theta_{t,s-1})\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(a)$ derives from the Lipschitz smoothness of function $F$ (i.e., Assump. 3), $(b)$ comes from the standard SGD update and the definition of $\\varepsilon_{t}(\\pmb\\theta_{t,s})$ , and $(d)$ is a rearrangement of the results in $(c)$ . ", "page_idx": 18}, {"type": "text", "text": "By introducing the results above into (42) and choosing $w^{-1}\\,=\\,2\\beta\\eta$ with $\\beta\\ \\triangleq\\ \\operatorname*{max}\\{\\kappa,\\sigma^{2}\\}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{T}\\sum_{s=1}^{N}F(\\theta_{t,s})\\overset{(a)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\left(F(\\theta_{t,s-1})+\\left(w\\eta^{2}(1-\\eta L)^{2}\\sigma^{2}(\\theta_{t,s-1})-\\eta(1-\\frac{\\eta L}{2})\\right)\\|\\nabla F(\\theta_{t,s-1})\\right)}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\left(F(\\theta_{t,s-1})+\\frac{\\eta^{2}L}{2}\\sigma^{2}(\\theta_{t,s-1})\\right)+\\frac{1}{w}\\ln\\left(\\frac{1}{2\\delta}\\right)}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\left(F(\\theta_{t,s-1})+\\left(\\frac{1}{2}\\eta(1-\\eta L)^{2}-\\eta(1-\\frac{\\eta L}{2})\\right)\\|\\nabla F(\\theta_{t,s-1})\\|^{2}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\frac{\\eta^{2}L}{2}\\sigma^{2}(\\theta_{t,s-1})\\right)+2\\beta\\eta\\ln\\left(\\frac{1}{2\\delta}\\right)}\\\\ &{\\overset{(c)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\left(F(\\theta_{t,s-1})-\\frac{\\eta}{2}\\|\\nabla F(\\theta_{t,s-1})\\|^{2}+\\frac{\\eta^{2}L}{2}\\sigma^{2}(\\theta_{t,s-1})\\right)+2\\beta\\eta\\ln\\left(\\frac{1}{2\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(a)$ comes from $\\eta\\leq1/L,\\,(b)$ is due to the fact that $\\sigma^{2}(\\pmb{\\theta}_{t,s-1})\\leq\\operatorname*{max}\\{\\kappa,\\sigma^{2}\\}=\\beta$ , and $(c)$ is due to the fact that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\eta}{2}(1-\\eta L)^{2}-\\eta(1-\\frac{\\eta L}{2})=\\frac{\\eta}{2}\\left(1-2\\eta L+\\eta^{2}L^{2}-2+\\eta L\\right)}\\\\ {\\displaystyle=\\frac{\\eta}{2}\\left(\\eta^{2}L^{2}-\\eta L-1\\right)}\\\\ {\\displaystyle\\leq-\\frac{\\eta}{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By rearranging the result in (43) and defining \u03c1 \u225c(1 \u2212N1 )4\u03c3\u03b22\u03b3TT0d + N1 , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\left\\|\\nabla F(\\theta_{t,s-1})\\right\\|^{2}\\leq\\frac{2}{\\eta N T}\\left(F(\\theta_{0})-F(\\theta_{T})\\right)+\\frac{\\eta L}{N T}\\sum_{t=1}^{T}\\sum_{s=1}^{N}\\sigma^{2}(\\theta_{t,s-1})+\\frac{4\\beta}{N T}\\ln\\left(\\frac{1}{2\\delta}\\right)}}\\\\ &{\\leq\\frac{2}{\\eta N T}\\left(F(\\theta_{0})-\\operatorname*{inf}_{\\theta}F(\\theta)\\right)+\\eta L\\rho\\sigma^{2}+\\frac{4\\beta}{N T}\\ln\\left(\\frac{1}{2\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality comes from the fact that $F(\\pmb\\theta_{T})\\ \\ \\leq\\ \\ \\operatorname{inf}_{\\pmb\\theta}F(\\pmb\\theta)$ and $\\sigma^{2}(\\pmb{\\theta}_{t,s-1})\\stackrel{.}{\\leq}$ $4\\beta\\gamma_{T_{0}}/(T_{0}d)$ in (33). ", "page_idx": 19}, {"type": "text", "text": "By choosing $\\begin{array}{r}{T\\,\\ge\\,\\frac{2\\Delta L}{N\\sigma^{2}\\rho}}\\end{array}$ and \u03b7 $\\begin{array}{r}{\\eta\\,=\\,\\sqrt{\\frac{2\\Delta}{N T L{\\sigma}^{2}\\rho}}}\\end{array}$ where $\\Delta\\,\\triangleq\\,F(\\pmb\\theta_{0})-\\operatorname*{inf}_{\\pmb\\theta}F(\\pmb\\theta)$ , we conclude our proof. ", "page_idx": 19}, {"type": "text", "text": "Remark 1. The speedup achieved by OptEx matches that of basic sample averaging (i.e., data parallelism) for stochastic optimization with noisy gradients. However, the speedup from OptEx comes from reduced sequential iterations (first term on the RHS in (45)), while sample averaging derives from reduced gradient variance (second term on the RHS in (45)). When gradient noise is already small or in deterministic optimization (e.g., the experiments in Sec. 6.1), data parallelism may not provide noticeable speedup, but OptEx can still contribute significantly. Overall, OptEx works in a complementary direction to existing parallelization methods, including sample averaging, to speed up first-order optimization, especially when other methods are not applicable or underperforming as discussed in our Sec. 2). ", "page_idx": 19}, {"type": "text", "text": "A.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We follow the idea in [37] to prove our Thm. 3. We first introduce the following lemma: ", "page_idx": 19}, {"type": "text", "text": "Lemma A.9 (Lemma B.12 in [44]). Let $\\mathbf{X}_{i}\\sim{\\mathcal{N}}(0,1)$ independently, $\\textstyle\\mathbf{Z}\\triangleq\\sum_{i=1}^{n}\\mathbf{X}_{i}^{2}$ , and $\\epsilon\\in(0,1)$ then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbf{Z}\\leq(1-\\epsilon)n\\right)\\leq\\exp\\left(-\\frac{n\\epsilon^{2}}{6}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Thm. 3. When $\\eta\\in\\left[1/(\\sqrt{N T}L),1/L\\right]$ , We consider the function ", "page_idx": 19}, {"type": "equation", "text": "$$\nF(\\pmb\\theta)=\\frac{L}{2}\\left\\lVert\\pmb\\theta\\right\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\theta_{0}$ is initialized with $\\mathcal{N}(\\mathbf{0},\\frac{\\Delta}{L}\\mathbf{I})$ . ", "page_idx": 19}, {"type": "text", "text": "We abuse $\\varepsilon(\\pmb\\theta_{\\tau})$ to denote the $\\varepsilon(\\pmb\\theta_{t,s-1})$ defined in our (36). Based on the update rule of stochastic gradient descent, we then have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\theta}_{\\tau}=\\pmb{\\theta}_{\\tau-1}-\\eta\\left(\\pmb{L}\\pmb{\\theta}_{\\tau-1}+\\pmb{\\varepsilon}(\\pmb{\\theta}_{\\tau-1})\\right)}\\\\ &{\\quad=(1-\\eta L)\\pmb{\\theta}_{\\tau-1}-\\eta\\pmb{\\varepsilon}(\\pmb{\\theta}_{\\tau-1})}\\\\ &{\\quad=(1-\\eta L)^{\\tau}\\pmb{\\theta}_{0}+\\displaystyle\\sum_{i=0}^{\\tau-1}\\eta(1-\\eta L)^{\\tau-i-1}\\pmb{\\varepsilon}(\\pmb{\\theta}_{i})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\varepsilon(\\pmb\\theta_{i})$ follows $\\mathcal{N}(\\mathbf{0},\\sigma^{2}(\\pmb{\\theta}_{i}))$ independently where we abuse $\\sigma^{2}(\\pmb\\theta_{i})$ to denote the $\\sigma^{2}(\\theta_{t,s-1})$ defined in (36) and $\\theta_{0}$ is initialized with $\\begin{array}{r}{\\mathcal{N}(\\mathbf{0},\\frac{\\Delta}{L}\\mathbf{I})}\\end{array}$ , we then have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{\\tau}\\sim\\mathcal{N}\\left(\\mathbf{0},\\left((1-\\eta L)^{2\\tau}\\frac{\\Delta}{L}+\\sum_{i=0}^{\\tau-1}\\eta^{2}(1-\\eta L)^{2(\\tau-i-1)}\\sigma^{2}(\\pmb{\\theta}_{i})\\right)\\mathbf{I}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\delta\\in(0,1)$ and $\\widetilde{\\beta}\\triangleq\\operatorname*{min}\\{1/(1+1/\\sigma^{2})^{T_{0}}\\kappa,\\sigma^{2}\\}$ , since $\\left\\|\\nabla F(\\pmb{\\theta}_{\\tau})\\right\\|^{2}=L^{2}\\left\\|\\pmb{\\theta}_{\\tau}\\right\\|^{2}$ , by introducing the results above i n to Lemma A.9 with $\\epsilon=1/2$ and $d\\geq d_{0}\\triangleq24\\ln(N T/\\delta)$ , with a probability of at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{s\\longrightarrow\\infty}{\\operatorname*{min}}\\ \\vert\\nabla F(\\theta)\\vert^{2}=1\\frac{1}{N T}\\underset{=1}{\\overset{n}{\\sum}}\\Bigg[\\nabla F(\\theta)\\vert^{2}}\\\\ &{\\geq\\frac{4}{2}\\Bigg((1-\\eta/2)^{\\frac{n}{2}}\\frac{1}{L^{\\frac{1}{\\alpha}}}+\\underset{=1}{\\overset{n}{\\sum}}\\frac{1}{\\eta^{2}}(1-\\eta/2)^{n(1-1)}\\sigma^{2}(\\theta)}\\\\ &{\\geq\\frac{4}{2}\\Bigg((1-\\eta/2)^{\\frac{n}{2}}\\frac{1}{L}+\\underset{=1}{\\overset{n}{\\sum}}\\frac{1}{\\eta^{2}}(1-\\eta/2)^{n(1-1)}\\sigma^{2}(\\theta)}\\\\ &{-\\frac{4\\cdot2}{2}\\Bigg((1-\\eta/2)^{\\frac{n}{2}}\\frac{1}{L}+\\frac{1}{1-(1-\\eta/2)^{n}}\\tau^{\\frac{n}{2}}\\Bigg)}\\\\ &{-\\frac{4}{2}\\Bigg((1-\\eta/2)^{n}\\Delta L+\\left(1-(1-\\eta/2)^{n}\\right)\\frac{\\eta}{2-\\eta}\\frac{\\sqrt{2}}{L^{\\frac{1}{\\alpha}}}\\Bigg)}\\\\ &{\\geq\\frac{4}{2}\\underset{=1}{\\overset{n}{\\sum}}\\Bigg\\{\\Delta L\\frac{\\eta}{2}\\frac{\\sqrt{2}}{2N T}\\Bigg\\}}\\\\ &{\\geq\\frac{4}{2}\\underset{=1}{\\overset{n}{\\sum}}\\Bigg\\{\\Delta L\\frac{\\eta}{2}\\frac{\\sqrt{2}}{2N T}\\Bigg\\}}\\\\ &{\\geq\\frac{4\\cdot2}{3}\\underset{=1}{\\overset{n}{\\sum}}\\Bigg(\\frac{\\Delta L\\eta}{2N}\\underset{=1}{\\overset{n}{\\sum}}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $\\eta\\in\\,\\Bigl[0,1/(\\sqrt{N T}L)\\Bigr].$ , we consider the function ", "page_idx": 20}, {"type": "equation", "text": "$$\nF(\\pmb\\theta)=\\frac{1}{4\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\left\\|\\pmb\\theta^{\\top}\\pmb e_{1}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\theta_{0}$ is initialized with $\\begin{array}{r}{\\pmb{\\theta}_{0}^{\\top}=\\left[\\sqrt{d\\Delta\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}},0,\\cdots\\,,0\\right].}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Similarly, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{\\tau}=\\left(1-\\frac{1}{2\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\right)^{\\tau}\\theta_{0}+\\sum_{i=0}^{\\tau-1}\\eta\\left(1-\\frac{1}{2\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\right)^{\\tau-i-1}\\varepsilon(\\theta_{i})\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta_{\\tau}\\sim\\mathcal{N}\\left(\\left(1-\\frac{1}{2\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\right)^{\\tau}\\theta_{0},\\left(\\sum_{i=0}^{\\tau-1}\\eta^{2}\\left(1-\\frac{1}{2\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\right)^{2(\\tau-i-1)}\\alpha^{2}\\right)^{\\tau-i-1}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, let $\\pmb{\\theta}_{\\tau}^{(1)}$ denote the first element of $\\theta_{\\tau}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\theta_{\\tau}^{(1)}\\right]\\stackrel{(a)}{=}\\left(1-\\frac{1}{2\\operatorname*{max}\\big\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\big\\}}\\right)^{\\tau}\\sqrt{d\\Delta\\operatorname*{max}\\left\\{1/L,\\displaystyle\\sum_{\\tau=1}^{N T}\\eta\\right\\}}}\\\\ &{\\stackrel{(b)}{\\geq}\\sqrt{d\\Delta\\operatorname*{max}\\left\\{1/L,\\displaystyle\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\exp\\left(\\left(\\ln\\frac{1}{2}\\right)\\displaystyle\\sum_{\\tau=1}^{N T}\\frac{\\eta}{\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\right)}\\\\ &{\\stackrel{(c)}{\\geq}\\frac{1}{2}\\sqrt{d\\Delta\\operatorname*{max}\\left\\{1/L,\\displaystyle\\sum_{\\tau=1}^{N T}\\eta\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(b)$ comes from the fact that $1-z/2\\geq\\exp(\\ln(1/2)z)$ for all $z\\in[0,1]$ . ", "page_idx": 21}, {"type": "text", "text": "In addition, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{var}\\left[\\pmb{\\theta}_{\\tau}^{(1)}\\right]=\\displaystyle\\sum_{i=0}^{\\tau-1}\\eta^{2}\\left(1-\\frac{1}{2\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\right)^{2(\\tau-i-1)}\\sigma^{2}(\\pmb{\\theta}_{i}^{(1)})}\\\\ &{\\leq\\displaystyle\\sum_{i=0}^{\\tau-1}\\eta^{2}\\beta}\\\\ &{=\\frac{\\beta}{L^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\beta\\triangleq\\operatorname*{max}\\{\\kappa,\\sigma^{2}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "Let $\\Psi$ denote the CDF of standard normal distribution and follow the idea in [37], by choosing ", "page_idx": 21}, {"type": "equation", "text": "$$\nd>d_{0}\\triangleq\\frac{16\\beta/L^{2}\\left(\\Psi^{-1}\\left(1-\\frac{\\delta}{N T}\\right)\\right)^{2}}{\\Delta\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}=\\mathcal{O}\\left(\\beta/(\\Delta L^{2})\\ln N T/\\delta\\right)\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\frac{\\theta_{\\tau}^{(1)}-\\mathbb{E}\\left[\\theta_{\\tau}^{(1)}\\right]}{\\sqrt{\\operatorname{var}\\left[\\theta_{\\tau}^{(1)}\\right]}}\\geq-\\frac{\\frac{1}{4}\\sqrt{d\\Delta\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}}{\\sqrt{\\beta/L^{2}}}\\right)}\\\\ &{\\geq\\mathbb{P}\\left(\\frac{\\theta_{\\tau}^{(1)}-\\mathbb{E}\\left[\\theta_{\\tau}^{(1)}\\right]}{\\sqrt{\\operatorname{var}\\left[\\theta_{\\tau}^{(1)}\\right]}}\\geq-\\frac{\\frac{1}{4}\\sqrt{d\\Delta\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}}{\\sqrt{\\beta/L^{2}}}\\right)}\\\\ &{=1-N T\\delta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "That is, with a probability of at least $1-\\delta/(N T)$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\theta_{\\tau}^{(1)}\\geq\\frac{1}{4}\\sqrt{d\\Delta\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\begin{array}{r}{\\|\\nabla F(\\pmb{\\theta}_{\\tau})\\|^{2}=\\left(\\frac{1}{2\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}\\right)^{2}\\left\\|\\pmb{\\theta}^{\\top}\\pmb{e}_{1}\\right\\|^{2}}\\end{array}$ , we conclude our proof by applying union bound on (58) as below ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\tau\\in[N T]}{\\mathrm{min}}\\,\\|\\nabla F(\\pmb{\\theta}_{\\tau})\\|^{2}=\\frac{1}{N T}\\sum_{\\tau=1}^{N T}\\|\\nabla F(\\pmb{\\theta}_{\\tau})\\|^{2}}&{}\\\\ {\\pm\\,\\frac{d_{0}\\Delta}{4\\operatorname*{max}\\left\\{1/L,\\sum_{\\tau=1}^{N T}\\eta\\right\\}}}&{}\\\\ {\\pm\\,\\frac{d_{0}\\Delta L}{4\\sqrt{N T}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality comes from the fact that $\\eta\\in[0,1/(\\sqrt{N T}L)]$ . This finally concludes our proof. ", "page_idx": 22}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/da87f20f4c7f1f86316627059c292ecafdabf307fe80d9960e33430a970d1492.jpg", "img_caption": ["Figure 5: An illustrated comparison among our OptEx and all the baselines at iteration $t$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Appendix B Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "B.1 Baselines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide an illustrated comparison between our $\\mathrm{{OptEx}}$ and all the baselines at iteration $t$ in Fig. 5. Notably, the Target baseline represents an ideal parallelization of the Vanilla baseline. However, this is impractical because the ground-truth gradient (i.e., $\\nabla f(\\cdot))$ required by a process $i\\,\\in\\,[N]$ to produce the update can not be obtained before the start of this process. More specifically, this gradient is the outcome at the end of the corresponding process. In contrast, our OptEx framework makes use of the kernelized gradient estimation (i.e., $\\pmb{\\mu}_{t}(\\cdot))$ to achieve approximated parallelized iterations for FOO as illustrated in our Fig. 5, which is more practical and useful. ", "page_idx": 23}, {"type": "text", "text": "B.2 Settings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "B.2.1 Optimization of Synthetic Functions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let input $\\pmb{\\theta}=[\\theta_{i}]_{i=1}^{d}$ , the Ackley, Sphere, and Rosenbrock functions applied in our synthetic experiments are given below, which have been slightly modified compared with the standard ones. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle F(\\theta)=-20\\exp\\left(-0.2\\sqrt{\\frac{1}{d}\\displaystyle\\sum_{i=1}^{d}\\theta_{i}^{2}}\\right)-\\exp(\\frac{1}{d}\\displaystyle\\sum_{i=1}^{d}\\cos{(2\\pi\\theta_{i})})+20+\\exp(1),\\mathrm{(Ackley)}}}\\\\ {~~}\\\\ {{\\displaystyle F(\\theta)=\\sqrt{\\frac{1}{d}\\displaystyle\\sum_{i=1}^{d}\\theta_{i}^{2}},\\mathrm{(Sphere)}}}\\\\ {{\\displaystyle F(\\theta)=\\frac{1}{d}\\displaystyle\\sum_{i=1}^{d-1}\\left[100\\big(\\theta_{i+1}-\\theta_{i}\\big)^{2}+\\left(1-\\theta_{i}\\right)^{2}\\right],\\mathrm{(Rosenbrock)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that both Ackley and Sphere function achieve their minimum (i.e., $\\operatorname*{min}F(\\pmb{\\theta})=0)$ ) at ${\\boldsymbol{\\theta}}^{*}=\\mathbf{0}$ , whereas Rosenbrock function achieves its minimum (i.e., $\\operatorname*{min}F(\\pmb{\\theta})=0)$ ) at $\\boldsymbol{\\theta}^{*}=\\mathbf{\\dot{1}}$ . ", "page_idx": 23}, {"type": "text", "text": "In this experiment, the parallelism of $N=5$ is applied and all the baselines introduced in Sec. 6.1 as well as our OptEx are based on Adam [4] with a learning rate of 0.1, $\\beta_{1}=0.9$ , and $\\beta_{2}=0.999$ . In addition, we employ a Mat\u00e9rn kernel-based gradient estimation in our OptEx with $T_{0}=20$ . ", "page_idx": 23}, {"type": "text", "text": "B.2.2 Optimization of Reinforcement Learning Tasks ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our experimental framework is built on the Deep Q-Network (DQN) algorithm, as outlined in [39], and implemented within the OpenAI Gym environment [38]. This study investigates the effectiveness of different optimizer configurations across classical discrete control tasks provided by Gym. ", "page_idx": 23}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/b466f642ff38966e1e39ef563c2c09abaae7d27b9a5e7cf4415a0ba16b2370bb.jpg", "img_caption": ["Figure 6: Ablation studies on the Rosenbrock synthetic function. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Each trial is conducted on a dedicated CPU to maintain consistency in computational conditions. The DQN architecture consists of dual fully connected layers, with 64 or 128 neurons tailored to each task\u2019s requirements. Hyperparameters, including a learning rate of 0.001, a reward discount factor of 0.95, and a batch size of 256, are applied for fairness and consistency across experiments. ", "page_idx": 24}, {"type": "text", "text": "Performance evaluation of the optimizer-enhanced DQN agents is systematically carried out over 100 to 200 episodes per game, employing an $\\epsilon$ -greedy policy with a minimum epsilon of 0.1 and an exponential epsilon decay with a rate of 2\u22121500 . A preliminary warm-up phase of either 30 or 50 episodes, depending on the task, is incorporated to stabilize initial learning dynamics. Besides, all baselines introduced in Sec.6.1 and our OptEx are based on Adam[4] with a learning rate of 0.001, $\\beta_{1}=0.9$ , and $\\beta_{2}=0.999$ . For OptEx, we utilize a Mat\u2019ern kernel-based gradient estimation, with $T_{0}=150$ to accommodate the variance in RL tasks, and parallelism of $N=4$ is applied. ", "page_idx": 24}, {"type": "text", "text": "B.2.3 Optimization of Neural Network Training ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this experiment, we compared our OptEx with other baselines using both image classification and text autoregression tasks. Here, we simply make use of the jax.vmap function to simulate parallel computing and measure the wallclock time for each sequential iteration. We believe that the time efficiency of our OptEx can be further improved when it is more properly implemented on a parallel computing platform. Besides, to reduce the computational cost of our kernelized gradient estimation in these high-dimensional optimization problems, we propose to use a randomly sampled subset of dimensions (e.g., $\\widetilde d=10^{4}$ for image classification and $\\bar{\\tilde{d}}=10^{5}$ for text autoregression) from the total $d$ dimension  t o compute the kernel value $k(\\cdot,\\cdot)$ in e a ch sequential iteration of our OptEx. ", "page_idx": 24}, {"type": "text", "text": "Image Classification. In this image classification task, we train a 9-layer MLP (including input and output layer) with skip connections on MNIST [45], Fashion MNIST [46] and a 10-layer MLP (including input and output layer) with skip connections on CIFAR-10 [41] datasets, which have a parameter size of $d\\,=\\,978186$ for (fashion-)MNIST and $d\\,=\\,2412298$ for CIFAR-10. Both our OptEx and other baselines are based on SGD [1] with a learning rate of 0.001, a batch size of 512, and parallelism of $N=4$ . For OptEx, we employ a Mat\u00e9rn kernel-based gradient estimation with $T_{0}=6$ . ", "page_idx": 24}, {"type": "text", "text": "Text Autoregression. In addition, we further train a simple transformer from Haiku library [42] with a parameter size of $d=1626496$ on the corpus of \u201cHarry Potter and the Sorcerers Stone\u201d and a subset work from Shakespeare. In both tasks, all the baselines introduced in Sec. 6.1 and our OptEx are based on SGD [1] with a learning rate of 0.01, batch size of 256 and parallelism of $N=4$ . For OptEx, we employ a Mat\u00e9rn kernel-based gradient estimation, where $T_{0}=10$ . ", "page_idx": 24}, {"type": "text", "text": "B.3 More Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Ablation Studies on Synthetic Function. To better understand our OptEx algorithm, we have conducted a number of ablation studies on the Rosenbrock synthetic function with a dimension of $d=\\mathrm{10^{5}}$ . The results are in Fig. 6, in which there are 4 different types of comparisons: (a) We have acto emvpearrye idt eorautri oOn , Edxe nwoittihn gv sa. s wpiathroault leevla launadti nsge qthuee inntteiraml erdeisaptee cgtirvaedliye nitns ,F ii.ge.. ,6 $\\{\\nabla f(\\pmb{\\theta}_{t,i-1})\\}_{i=1}^{N-1}$ $t$ ", "page_idx": 24}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/37da9692cf24841dc49fb6e423439efdbd7d5ef144f3052712f8a614a0d4aa48.jpg", "img_caption": ["(a) Sequential Iterations "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/3271c02dcf0da8e9a38c6706e802dc67a4c068470db082910af1965d2a2207cb.jpg", "img_caption": ["(b) Wallclock Time "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 7: Comparison of the train and test error (i.e., 1 - accuracy in log scale for $y$ -axis) achieved by different optimizers when training MLP with residual connections on MNIST dataset with (a) a varying number $T$ of sequential iteration and (b) a varying wallclock time $x$ -axis). The parallelism $N$ is set to 4 and each curve denotes the mean from 5 independent runs. The wallclock time is evaluated on an AMD EPYC 7763 CPU. ", "page_idx": 25}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/1c1128070fd4f3585056722f64ce50d42992a718347869a49323b751d6f7bc63.jpg", "img_caption": ["(a) Sequential Iterations "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/68da3a322121307b72aa62eacd1890f8d5a90c831f6351ac59cd6f92f9d59f47.jpg", "img_caption": ["(b) Wallclock Time "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 8: Comparison of the train and test error (i.e., 1 - accuracy in log scale for $y$ -axis) achieved by different optimizers when training MLP with residual connections on the fashion-MNIST dataset with (a) a varying number $T$ of sequential iteration and (b) a varying wallclock time ( $x$ -axis). The parallelism $N$ is set to 4 and each curve denotes the mean from 5 independent runs. Similarly, the wallclock time is evaluated on an AMD EPYC 7763 CPU. ", "page_idx": 25}, {"type": "text", "text": "show the importance of these intermediate gradients on an accurate gradient estimation and therefore improved convergence of our OptEx as justified in our Sec. 4.3. (b) We have compared our OptEx using different principles to choose $\\pmb\\theta_{t}$ from $\\{\\pmb{\\theta}_{t}^{(i)}\\}_{i=1}^{N}$ , including using function value (denoted as func in Fig. 6 (b)) via $\\pmb{\\theta}_{t}=\\arg\\operatorname*{min}_{\\pmb{\\theta}\\in\\{\\pmb{\\theta}_{t}^{(i)}\\}_{i=1}^{N}}f(\\pmb{\\theta})$ , using $\\pmb{\\theta}$ from the process $N$ (denoted as last in Fig. 6 (b), i.e., the standard principle in Algo. 1) with $\\pmb{\\theta}_{t}\\:=\\:\\pmb{\\theta}_{t}^{(N)}$ , and using gradient norm (denoted as grad in Fig. 6 (b)) via $\\pmb{\\theta}_{t}=\\arg\\operatorname*{min}_{\\pmb{\\theta}\\in\\{\\pmb{\\theta}_{t}^{(i)}\\}_{i=1}^{N}}\\|\\nabla f(\\pmb{\\theta})\\|$ . (c) We have compared our OptEx with varying $T_{0}$ in Fig. 6 (c). (d) We have compared our OptEx with varying in Fig. 6 (d). All the other experimental settings follow from the same ones in our Appx. B.2.1. ", "page_idx": 25}, {"type": "text", "text": "The results presented in Fig. 6 (a) indicate that evaluating intermediate gradients $\\left\\{\\nabla f(\\pmb{\\theta}_{t,i-1})\\right\\}_{i=1}^{N-1}$ at each iteration is crucial for achieving better convergence with our OptEx. This improved performance likely stems from these evaluations being more aligned with the gradient approximations required at point $\\pmb{\\theta}$ in our OptEx, which is essential to achieve accurate gradient estimation and therefore well-performing convergence in our $\\mathrm{{OptEx}}$ . Consequently, these findings underscore the importance and necessity of line 7 in Algo. 1, as discussed in Sec. 4.3. Further, Fig. 6 (b) shows that utilizing $\\pmb{\\theta}$ from the final process $N$ (denoted as last) where $\\pmb{\\theta}_{t}=\\pmb{\\theta}_{t}^{(N)}$ , typically results in marginally better convergence. This approach maximizes the benefits of parallelism within $N$ processes, unlike the other methods which often operate under reduced parallelism due to constraints in optimizing $\\pmb{\\theta}_{t}^{(N)}$ . Additionally, Fig. 6 (c) reveals that maintaining a gradient history length of $T_{0}\\leq10$ generally improves convergence. Extending $T_{0}$ beyond 10, however, does not significantly improve outcomes, which thereby validates our theoretical insights from Sec. 5.1. Finally, Fig. 6 (d) shows that increasing the number of processes when $N\\leq10$ improves the iteration complexity of our OptEx. However, as $N$ increases to 20, convergence deteriorates. This observation aligns with the theoretical insights in our Sec. 5.2, which posits that while increasing $N$ up to an optimal point $N_{\\mathrm{opt}}=\\Delta\\eta^{2}/(L T\\bar{\\sigma^{2}}\\rho)$ enhances convergence, further increases can degrade performance. ", "page_idx": 25}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/67e2e28b5edd2debe7d24eb13aaf6c4a5d768739216c28389eb1dfa21faf0e61.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 9: Comparison of the train and test error (i.e., 1 - accuracy in log scale for $y$ -axis) achieved by different optimizers when training MLP with residual connections on CIFAR-10 dataset with (a) a varying number $T$ of sequential iteration and (b) a varying wallclock time $x$ -axis). The parallelism $N$ is set to 4 and each curve denotes the mean from 5 independent runs. Similarly, the wallclock time is evaluated on a single NVIDIA RTX 4090 GPU. ", "page_idx": 26}, {"type": "image", "img_path": "MzNjnbgcPN/tmp/9b6b17197411d2a3c3d95ef199fe396bb6ddb2cfd0a95665c060a1d8ee688582.jpg", "img_caption": ["Figure 10: Comparison of the training loss $y$ -axis) achieved by different optimizers when training transformer on the corpus of \u201cHarry Potter and the Sorcerer\u2019s Stone\u201d with a varying number $T$ of sequential iteration and a varying wallclock time $x$ -axis). The parallelism $N$ is set to 4 and each curve denotes the mean from 3 independent experiments. The wallclock time is evaluated on a single NVIDIA RTX 4090 GPU. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Image Classification on MNIST and Fashion-MNIST. We have also compared the training and test errors achieved by different optimizers when training an MLP with residual connections on the MNIST and Fashion-MNIST datasets. The results in Fig.7 and Fig.8 indicate that our OptEx consistently improves over the Vanilla baseline and performs comparably to the Target baseline in terms of the number of sequential iteration required to achieve the same level of training or test error. Furthermore, our OptEx significantly improves the time efficiency of training the MLP on both datasets, as evidenced by the results in Fig.7 and Fig.8. These findings hence also validate the efficacy of our OptEx in accelerating FOO across various image classification tasks. ", "page_idx": 26}, {"type": "text", "text": "Image Classification on CIFAR-10. Besides the test errors presented in Sec. 6.3, we also provide a comprehensive comparison of both training and test errors achieved by different optimizers when training an MLP with residual connections on the CIFAR-10 dataset. This comparison, shown in Fig.9, considers varying numbers $T$ of sequential iterations and varying wallclock time. Due to the computational cost of training deep neural networks on CIFAR-10, we evaluate the wallclock time for all optimizers using a single NVIDIA RTX 4090 GPU. Notably, similar to the results in Sec. 6.3, our OptEx consistently outperforms the Vanilla baseline and performs comparably to the Target baseline in both training and test errors. Additionally, our OptEx considerably enhances the time efficiency of training the MLP on CIFAR-10, as demonstrated by the results in Fig.9. These results therefore adequately verify the efficacy of our OptEx in expediting FOO. ", "page_idx": 26}, {"type": "text", "text": "Autoregression on Text Corpus. In addition to the training results on the Shakespeare corpus, we also present the training loss achieved by different optimizers when training the same transformer on the corpus of \u201cHarry Potter and the Sorcerer\u2019s Stone\u201d in Fig.10. Remarkably, Fig.10 shows that our OptEx still consistently outperforms the Vanilla baseline and performs comparably to the Target baseline. Moreover, the results in Fig. 10 demonstrate that our OptEx significantly enhances the time efficiency of training the transformer on the Harry Potter corpus. These findings thus further validate the efficacy of our OptEx in accelerating FOO across various types of learning tasks. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The main claims provided in the abstract and introduction can reflect the paper\u2019s contributions and scope. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The limitation of the work is provided in our Sec. 7 ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The assumptions are summarized in Sec. 5 and the proofs are provided in the Appx. A. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All the information is in Appx. B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The data and code are provided in the supplemental material. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the information is in Appx. B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the results are reported with mean of multiple independent runs in the figures. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See our Sec. B for the details. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The research conducted in the paper indeed conforms with the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not see any societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: All the data and codes used in the paper are open-sourced. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]