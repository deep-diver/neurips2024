[{"figure_path": "7gf6oGdKPU/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Training process of the Masked Precursor Completion (MPC) retriever. (b) Training process of the Neural Reaction Energy (NRE) retriever.", "description": "This figure illustrates the training processes of two crucial components in the Retrieval-Retro model: the Masked Precursor Completion (MPC) retriever and the Neural Reaction Energy (NRE) retriever.  (a) shows the MPC retriever's training, focusing on learning dependencies between precursors by masking some precursor information and reconstructing it. (b) depicts the NRE retriever's two-step training. The first step pre-trains the model on DFT-calculated formation energies, while the second step fine-tunes the model with experimental formation energies to enhance the model's accuracy in predicting the thermodynamic relationships between materials and their precursors.  These retrievers work together to identify relevant reference materials for retrosynthesis prediction.", "section": "3 Proposed Method: Retrieval-Retro"}, {"figure_path": "7gf6oGdKPU/figures/figures_4_1.jpg", "caption": "Figure 3: The overall framework of Retrieval-Retro.", "description": "The figure illustrates the overall framework of the proposed Retrieval-Retro model for inorganic retrosynthesis planning. It starts with a target material's composition, which is then encoded using a Graph Neural Network (GNN). Simultaneously, two retrievers, the Masked Precursor Completion (MPC) retriever and the Neural Reaction Energy (NRE) retriever, identify relevant reference materials from a knowledge base.  The retrieved materials are then processed using self-attention and cross-attention mechanisms to implicitly extract precursor information. Finally, a classifier predicts the precursors for the target material using this implicitly extracted information.", "section": "3 Proposed Method: Retrieval-Retro"}, {"figure_path": "7gf6oGdKPU/figures/figures_8_1.jpg", "caption": "Figure 4: Sensitivity Analysis results. KB refers to the knowledge base.", "description": "This figure shows the sensitivity analysis of Retrieval-Retro's performance to the size of the knowledge base (KB) and the number of reference materials (K).  The left plot (a) illustrates how the model's top-K accuracy improves as the size of the KB increases from 0% to 100% (Full). The right plot (b) shows that the accuracy generally improves as K increases until it reaches an optimal point, after which adding more references does not further enhance performance. This indicates there's an optimal number of references to consider for best performance, and that an overly large KB may introduce noise.", "section": "4 Experiments"}, {"figure_path": "7gf6oGdKPU/figures/figures_17_1.jpg", "caption": "Figure 5: Performance Improvements across Various GNN Backbones", "description": "This figure demonstrates the consistent improvement achieved by integrating the proposed Retrieval-Retro framework with various Graph Neural Network (GNN) architectures.  The bar chart visually compares the performance (Top-1, Top-3, Top-5, and Top-10 accuracy) of different GNNs (GCN, GIN, GAT, Graph Network) both with and without the Retrieval-Retro enhancement.  The results highlight that Retrieval-Retro consistently boosts the performance of each GNN architecture across all evaluation metrics, showcasing its effectiveness as a general enhancement technique for inorganic retrosynthesis planning regardless of the underlying GNN.", "section": "E.1 Performance on Various Backbone Architecture"}]