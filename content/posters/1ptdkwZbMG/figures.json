[{"figure_path": "1ptdkwZbMG/figures/figures_1_1.jpg", "caption": "Figure 1: Motivation. The proposed CLOVER is inspired by the classic closed-loop control in automation systems (a). Our framework (b) employs a visual planner to predetermine a sequence of sub-goals (Section 3.1). Then these goals guide the policy to generate actions with an error measurement strategy (Section 3.2). Within the feedback loop, it automatically replans when the sub-goal is infeasible, and adapts to to the next one upon achievement (Section 3.3).", "description": "This figure illustrates the core idea of the CLOVER framework by comparing it to the classic closed-loop control system.  Panel (a) shows a simple closed-loop system with a controller adjusting actions based on error measurements to achieve a desired value. Panel (b) shows the CLOVER framework, which incorporates a visual planner to generate a sequence of sub-goals.  A feedback-driven policy then uses error measurements (comparing the current observation to the sub-goals) to generate actions. The system also includes mechanisms for adaptive transitions between sub-goals and replanning if necessary.", "section": "1 Introduction"}, {"figure_path": "1ptdkwZbMG/figures/figures_3_1.jpg", "caption": "Figure 2: Architecture of our feedback-driven policy. 1) The state encoder takes in both current observation along with the synthesized sub-goal. A shared multimodal encoder generates fused RGB-D features, followed by two queries extracting informative features as the current and goal embeddings respectively. 2) The discrepancy of the two state embeddings is explicitly modeled as errors. 3) The resultant residual in error measurement is ultimately decoded to the final action.", "description": "This figure shows the architecture of the feedback-driven policy used in CLOVER. It consists of three main components: a state encoder that processes both the current observation and the planned sub-goal to generate fused RGB-D features and separate current and goal state embeddings; an error measurement module that calculates the difference between the current and goal state embeddings to quantify the error; and an action decoder that takes the error as input and outputs the action for the robot.  The multimodal encoder uses both RGB and depth information to generate a more comprehensive representation of the robot's state.", "section": "3.2 Feedback-Driven Policy"}, {"figure_path": "1ptdkwZbMG/figures/figures_4_1.jpg", "caption": "Figure 3: Comparison on the measurement ability of different embeddings. We visualize the cosine distance between embeddings of observations and generated sub-goals during a roll-out process. (a) CLIP feature [23] and (b) state embeddings trained without error measuring do not hold clear interrelations among frames. While (c) state embeddings obtained from our policy distribute reasonably in the latent space which benefits measuring the errors in feedback loops.", "description": "This figure compares the ability of different embedding methods to measure the error between observed and planned states for robotic manipulation.  It shows the cosine distance between embeddings of observed states and generated sub-goals over a sequence of actions.  CLIP features and state embeddings trained without an explicit error-measuring mechanism show poor correlation and low variability, hindering accurate error quantification. In contrast, the state embeddings learned by the proposed CLOVER framework exhibit a strong correlation and high variability, enabling accurate error measurement and facilitating adaptive control.", "section": "3.2 Feedback-Driven Policy"}, {"figure_path": "1ptdkwZbMG/figures/figures_5_1.jpg", "caption": "Figure 1: Motivation. The proposed CLOVER is inspired by the classic closed-loop control in automation systems (a). Our framework (b) employs a visual planner to predetermine a sequence of sub-goals (Section 3.1). Then these goals guide the policy to generate actions with an error measurement strategy (Section 3.2). Within the feedback loop, it automatically replans when the sub-goal is infeasible, and adapts to to the next one upon achievement (Section 3.3).", "description": "This figure illustrates the core idea of CLOVER, a closed-loop visuomotor control framework.  Panel (a) shows a classic closed-loop control system, highlighting the reference input, error measurement, and controller. Panel (b) depicts CLOVER, which adapts the closed-loop concept for robotic manipulation. CLOVER uses a visual planner to generate a sequence of visual sub-goals, a feedback-driven policy to generate actions based on these sub-goals and error measurements, and a feedback loop to handle infeasible sub-goals and transition to the next sub-goal upon achievement.", "section": "1 Introduction"}, {"figure_path": "1ptdkwZbMG/figures/figures_6_1.jpg", "caption": "Figure 4: Real-world robot setting. We propose a long-horizon task encompassing three consecutive sub-tasks, where the failure of a prequel task will inevitably lead to failure of subsequent tasks. The additional single tasks are designed to validate the generalizability of CLOVER of all aspects.", "description": "The figure shows a real-world robot performing a long-horizon task composed of three sub-tasks and two single tasks.  The long-horizon task involves lifting a pot lid, putting a fish into the pot, and putting the lid back on. The two single tasks involve pouring shrimp onto a plate and stacking two bowls.  The setup aims to test the robot's ability to handle multi-step tasks and to assess the generalizability of the CLOVER system across different task types.", "section": "4.1 Experimental Setup"}, {"figure_path": "1ptdkwZbMG/figures/figures_7_1.jpg", "caption": "Figure 5: Experiment setting of the generalization evaluation. We place entirely new objects absent from training, alongside the interaction object to introduce visual distraction. We test policies under dynamic conditions by randomly placing and picking up a doll to create unpredictable visual changes.", "description": "This figure shows the experimental setup for evaluating the generalization capabilities of the proposed CLOVER framework.  Three different settings are shown: the original setting, a setting with visual distractions (additional objects), and a dynamic scene setting (additional objects and a moving doll).  The purpose is to assess how well the robot performs under conditions that differ from those seen during training.", "section": "4.2 Main Results"}, {"figure_path": "1ptdkwZbMG/figures/figures_8_1.jpg", "caption": "Figure 6: Generated videos of diverse tasks conditioned on the same initial frame. CLOVER can generate precise visual plans corresponding to the tasks, facilitating low-level executor guidance. We downsample the video by 2 and exclude depth results in visualizations for simplicity.", "description": "This figure shows examples of videos generated by CLOVER for four different tasks, all starting from the same initial frame.  The model generates a sequence of images (a visual plan) for each task that accurately reflects the task's goal.  The visual plans are detailed enough to guide a low-level robotic control system in performing the task successfully.  The videos are downsampled (2x) and only RGB images are shown to simplify visualization; depth information is omitted.", "section": "4.2 Main Results"}, {"figure_path": "1ptdkwZbMG/figures/figures_8_2.jpg", "caption": "Figure 7: Analysis and comparisons on closed-loop and open-loop roll-out on CALVIN. (a) Comparative analysis of performance (Avg. Len.) through varying step lengths in open-loop control. Evaluations are conducted using identical models but employing different roll-out strategies. (b) The distribution of action steps taken in closed-loop roll-out to achieve each sub-goal. (c) Examination of the robustness of closed-loop control employing various visual encoders and distance thresholds.", "description": "This figure compares the performance of closed-loop and open-loop control strategies on the CALVIN benchmark.  It shows the impact of varying the time interval for sub-goal transitions in open-loop control, the distribution of action steps in closed-loop control, and the robustness of closed-loop control to different visual encoders and distance thresholds.", "section": "3.3 CLOVER"}, {"figure_path": "1ptdkwZbMG/figures/figures_9_1.jpg", "caption": "Figure 3: Comparison on the measurement ability of different embeddings. We visualize the cosine distance between embeddings of observations and generated sub-goals during a roll-out process. (a) CLIP feature [23] and (b) state embeddings trained without error measuring do not hold clear interrelations among frames. While (c) state embeddings obtained from our policy distribute reasonably in the latent space which benefits measuring the errors in feedback loops.", "description": "This figure compares the effectiveness of different embedding methods for measuring errors in a closed-loop visuomotor control system.  It shows that using the proposed state embeddings, trained with an explicit error measuring mechanism, leads to a clearer and more measurable representation of the error between the observed and planned states compared to using CLIP features or state embeddings trained without such a mechanism. This improved measurement ability is crucial for the effectiveness of the feedback control system.", "section": "3.2 Feedback-Driven Policy"}, {"figure_path": "1ptdkwZbMG/figures/figures_14_1.jpg", "caption": "Figure 9: Automatic identification of unreliable sub-goals generated. We set the distance threshold for replanning as DR = 1.0 under all circumstances. Ideally, the distances between generated frames remain consistent and relatively small, whereas significant variations occur during unstable generation. Examining the distance within two adjacent frames, we can detect erroneous plans generated by the diffusion model before passing them to the subsequent policy model for execution.", "description": "This figure shows two examples of how the system identifies unreliable sub-goals generated by the diffusion model.  In Case 1 (Unreachability Detection), a large spike in the cosine distance between consecutive frames indicates an unreachable sub-goal. In Case 2 (Inconsistency Detection), inconsistent distances between frames signal an unreliable plan. The threshold for identifying these issues is set at a cosine distance of 1.0. The generated sub-goals are visualized alongside each graph, showing the visual inconsistencies associated with the high cosine distances.", "section": "3.3 CLOVER"}, {"figure_path": "1ptdkwZbMG/figures/figures_15_1.jpg", "caption": "Figure 3: Comparison on the measurement ability of different embeddings. We visualize the cosine distance between embeddings of observations and generated sub-goals during a roll-out process. (a) CLIP feature [23] and (b) state embeddings trained without error measuring do not hold clear interrelations among frames. While (c) state embeddings obtained from our policy distribute reasonably in the latent space which benefits measuring the errors in feedback loops.", "description": "This figure compares the ability of different embedding methods to measure the error between observed and planned states for robotic manipulation.  It shows that using CLIP features or state embeddings trained without an explicit error-measuring component results in inconsistent and less informative distance metrics between sub-goals. In contrast, the proposed state embeddings show a clear, monotonic relationship reflecting progress towards the sub-goals, making them suitable for closed-loop control.", "section": "3.2 Feedback-Driven Policy"}, {"figure_path": "1ptdkwZbMG/figures/figures_15_2.jpg", "caption": "Figure 11: Task completion score given by our value function. As the robot approaches each sub-goal and eventually completes the task described in the text, the learned value function monotonically increments, which could be helpful for task completion assessment.", "description": "This figure shows how a learned value function can be used to assess task completion.  As the robot successfully completes sub-goals in a task, the value function increases monotonically. This provides a way to automatically determine task completion without relying on external signals, which is especially useful in real-world scenarios.", "section": "3.3 CLOVER"}, {"figure_path": "1ptdkwZbMG/figures/figures_16_1.jpg", "caption": "Figure 12: The architecture of our visual planner. We augment the original UNet proposed in Imagen [33] with casual temporal attention to improve intra-frame consistency and additional cross-attention-based language conditioning. Combining a lightweight ContextNet introduced in RAFT [40], we estimate optical flows with a correspondence map of diffusion latent embeddings.", "description": "This figure illustrates the architecture of the visual planner used in the CLOVER framework.  It shows how a UNet architecture is augmented with components for handling temporal consistency (casual temporal attention), incorporating language information (language conditioning and text cross-attention), and leveraging optical flow estimation for improved accuracy (ContextNet in RAFT). The visual planner takes RGB images and depth maps as input and generates a sequence of future visual plans.", "section": "3.1 Visual Planner"}, {"figure_path": "1ptdkwZbMG/figures/figures_17_1.jpg", "caption": "Figure 13: Quantitative performance of video diffusion model with and without the flow-based regularization term. The optical flow-based regularization endows the video diffusion model with more efficient training convergence and notable performance improvement on CALVIN.", "description": "This figure shows the quantitative performance comparison of the video diffusion model with and without optical flow-based regularization.  Three metrics are plotted against training steps: Structural Similarity Index (SSIM), Fr\u00e9chet Inception Distance (FID), and Root Mean Square Error (RMSE) for depth.  The results indicate that optical flow regularization leads to significantly better performance in all three metrics, demonstrating improved training convergence and quality of the generated videos.", "section": "B.2 Training Protocol"}, {"figure_path": "1ptdkwZbMG/figures/figures_18_1.jpg", "caption": "Figure 14: Visualization on attention maps of the policy model. The policy model demonstrates the ability to direct attention towards the end-effector and the object it interacts with, without the need for explicit supervision during the learning process.", "description": "The figure shows attention maps generated by the policy model during execution. The maps highlight the areas of the input image that the model focuses on when making decisions. The fact that the model focuses on the end-effector and the object it is interacting with demonstrates the model's ability to implicitly learn relevant visual features.", "section": "Extended Visualizations"}, {"figure_path": "1ptdkwZbMG/figures/figures_19_1.jpg", "caption": "Figure 15: Visualization on generated RGB-D visual plans. Our model can generate reliable sub-goals with high consistency between RGB and depth.", "description": "This figure shows examples of RGB-D visual plans generated by the model for various tasks. The high consistency between the RGB and depth information in the generated plans demonstrates the model's ability to produce reliable and coherent visual plans that can effectively guide the robot's actions.", "section": "4.4 Ablation Studies"}, {"figure_path": "1ptdkwZbMG/figures/figures_19_2.jpg", "caption": "Figure 4: Real-world robot setting. We propose a long-horizon task encompassing three consecutive sub-tasks, where the failure of a prequel task will inevitably lead to failure of subsequent tasks. The additional single tasks are designed to validate the generalizability of CLOVER of all aspects.", "description": "This figure shows the experimental setup for real-world robot experiments.  A long-horizon task consisting of three consecutive sub-tasks is presented, demonstrating the robot's ability to perform multiple actions in sequence, with failure in one step affecting subsequent steps.  Additional single tasks further demonstrate CLOVER's generalizability to various scenarios.", "section": "4.1 Experimental Setup"}, {"figure_path": "1ptdkwZbMG/figures/figures_20_1.jpg", "caption": "Figure 5: Experiment setting of the generalization evaluation. We place entirely new objects absent from training, alongside the interaction object to introduce visual distraction. We test policies under dynamic conditions by randomly placing and picking up a doll to create unpredictable visual changes.", "description": "This figure shows the experimental setup for evaluating the model's generalization capabilities.  Two scenarios are presented: visual distraction and dynamic scenes. The visual distraction scenario includes adding new objects to the scene that were not present during training. In the dynamic scene experiment, an additional object (a doll) is introduced and manipulated to simulate a more dynamic environment. The images depict the setup for each scenario.", "section": "4.2 Main Results"}]